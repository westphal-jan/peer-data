{"id": "0910.5761", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2009", "title": "Which graphical models are difficult to learn?", "abstract": "we consider the problem driver learning the signature of noisy sequences ( pairwise binary valued random messages ) from i. i. pi. c. overall several methods have been utilized to promote this hypothesis, and relative merits and properties always continuously variable. by analyzing a number of concrete functions, we show that d - complexity detectors systematically fail otherwise finite markov random network develops long - range components. extremely easily, this phenomenon appears initially largely dependent from the ising experimental phase transition ( although it does not coincide with it ).", "histories": [["v1", "Fri, 30 Oct 2009 01:10:44 GMT  (25kb)", "http://arxiv.org/abs/0910.5761v1", null]], "reviews": [], "SUBJECTS": "stat.ML cond-mat.stat-mech cs.LG", "authors": ["andrea montanari", "jose ayres pereira"], "accepted": true, "id": "0910.5761"}, "pdf": {"name": "0910.5761.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jbento@stanford.edu", "montanari@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :0\n91 0.\n57 61\nv1 [\nst at\n.M L\n] 3\n0 O"}, {"heading": "1 Introduction and main results", "text": "Given a graph G = (V = [p], E), and a positive parameter \u03b8 > 0 the ferromagnetic Ising model on G is the pairwise Markov random field\n\u00b5G,\u03b8(x) = 1\nZG,\u03b8\n\u220f\n(i,j)\u2208E\ne\u03b8xixj (1)\nover binary variables x = (x1, x2, . . . , xp). Apart from being one of the most studied models in statistical mechanics, the Ising model is a prototypical undirected graphical model, with applications in computer vision, clustering and spatial statistics. Its obvious generalization to edge-dependent parameters \u03b8ij , (i, j) \u2208 E is of interest as well, and will be introduced in Section 1.2.2. (Let us stress that we follow the statistical mechanics convention of calling (1) an Ising model for any graph G.)\nIn this paper we study the following structural learning problem: Given n i.i.d. samples x(1), x(2),. . . , x(n) with distribution \u00b5G,\u03b8( \u00b7 ), reconstruct the graph G. For the sake of simplicity, we assume that the parameter \u03b8 is known, and that G has no double edges (it is a \u2018simple\u2019 graph).\nThe graph learning problem is solvable with unbounded sample complexity, and computational resources [1]. The question we address is: for which classes of graphs and values of the parameter \u03b8 is the problem solvable under appropriate complexity constraints? More precisely, given an algorithm Alg, a graph G, a value \u03b8 of the model parameter, and a small \u03b4 > 0, the sample complexity is defined as\nnAlg(G, \u03b8) \u2261 inf { n \u2208 N : Pn,G,\u03b8{Alg(x(1), . . . , x(n)) = G} \u2265 1\u2212 \u03b4 } , (2)\nwhere Pn,G,\u03b8 denotes probability with respect to n i.i.d. samples with distribution \u00b5G,\u03b8. Further, we let \u03c7Alg(G, \u03b8) denote the number of operations of the algorithm Alg, when run on nAlg(G, \u03b8) samples.1\n1For the algorithms analyzed in this paper, the behavior of nAlg and \u03c7Alg does not change significantly if we require only \u2018approximate\u2019 reconstruction (e.g. in graph distance).\nThe general problem is therefore to characterize the functions nAlg(G, \u03b8) and \u03c7Alg(G, \u03b8), in particular for an optimal choice of the algorithm. General bounds on nAlg(G, \u03b8) have been given in [2, 3], under the assumption of unbounded computational resources. A general charactrization of how well low complexity algorithms can perform is therefore lacking. Although we cannot prove such a general characterization, in this paper we estimate nAlg and \u03c7Alg for a number of graph models, as a function of \u03b8, and unveil a fascinating universal pattern: when the model (1) develops long range correlations, low-complexity algorithms fail. Under the Ising model, the variables {xi}i\u2208V become strongly correlated for \u03b8 large. For a large class of graphs with degree bounded by \u2206, this phenomenon corresponds to a phase transition beyond some critical value of \u03b8 uniformly bounded in p, with typically \u03b8crit \u2264 const./\u2206. In the examples discussed below, the failure of low-complexity algorithms appears to be related to this phase transition (although it does not coincide with it)."}, {"heading": "1.1 A toy example: the thresholding algorithm", "text": "In order to illustrate the interplay between graph structure, sample complexity and interaction strength \u03b8, it is instructive to consider a warmup example. The thresholding algorithm reconstructs G by thresholding the empirical correlations\nC\u0302ij \u2261 1\nn\nn\u2211\n\u2113=1\nx (\u2113) i x (\u2113) j for i, j \u2208 V . (3)\nTHRESHOLDING( samples {x(\u2113)}, threshold \u03c4 ) 1: Compute the empirical correlations {C\u0302ij}(i,j)\u2208V\u00d7V ; 2: For each (i, j) \u2208 V \u00d7 V 3: If C\u0302ij \u2265 \u03c4 , set (i, j) \u2208 E;\nWe will denote this algorithm by Thr(\u03c4). Notice that its complexity is dominated by the computation of the empirical correlations, i.e. \u03c7Thr(\u03c4) = O(p2n). The sample complexitynThr(\u03c4) can be bounded for specific classes of graphs as follows (the proofs are straightforward and omitted from this paper). Theorem 1.1. If G has maximum degree \u2206 > 1 and if \u03b8 < atanh(1/(2\u2206)) then there exists \u03c4 = \u03c4(\u03b8) such that\nnThr(\u03c4)(G, \u03b8) \u2264 8\n(tanh \u03b8 \u2212 12\u2206)2 log\n2p\n\u03b4 . (4)\nFurther, the choice \u03c4(\u03b8) = (tanh \u03b8 + (1/2\u2206))/2 achieves this bound. Theorem 1.2. There exists a numerical constant K such that the following is true. If \u2206 > 3 and \u03b8 > K/\u2206, there are graphs of bounded degree \u2206 such that for any \u03c4 , nThr(\u03c4) = \u221e, i.e. the thresholding algorithm always fails with high probability.\nThese results confirm the idea that the failure of low-complexity algorithms is related to long-range correlations in the underlying graphical model. If the graph G is a tree, then correlations between far apart variables xi, xj decay exponentially with the distance between vertices i, j. The same happens on bounded-degree graphs if \u03b8 \u2264 const./\u2206. However, for \u03b8 > const./\u2206, there exists families of bounded degree graphs with long-range correlations."}, {"heading": "1.2 More sophisticated algorithms", "text": "In this section we characterize \u03c7Alg(G, \u03b8) and nAlg(G, \u03b8) for more advanced algorithms. We again obtain very distinct behaviors of these algorithms depending on long range correlations. Due to space limitations, we focus on two type of algorithms and only outline the proof of our most challenging result, namely Theorem 1.6.\nIn the following we denote by \u2202i the neighborhood of a node i \u2208 G (i /\u2208 \u2202i), and assume the degree to be bounded: |\u2202i| \u2264 \u2206."}, {"heading": "1.2.1 Local Independence Test", "text": "A recurring approach to structural learning consists in exploiting the conditional independence structure encoded by the graph [1, 4, 5, 6].\nLet us consider, to be definite, the approach of [4], specializing it to the model (1). Fix a vertex r, whose neighborhood we want to reconstruct, and consider the conditional distribution of xr given its neighbors2: \u00b5G,\u03b8(xr |x\u2202r). Any change of xi, i \u2208 \u2202r, produces a change in this distribution which is bounded away from 0. Let U be a candidate neighborhood, and assume U \u2286 \u2202r. Then changing the value of xj , j \u2208 U will produce a noticeable change in the marginal of Xr, even if we condition on the remaining values in U and in any W , |W | \u2264 \u2206. On the other hand, if U * \u2202r, then it is possible to find W (with |W | \u2264 \u2206) and a node i \u2208 U such that, changing its value after fixing all other values in U \u222aW will produce no noticeable change in the conditional marginal. (Just choose i \u2208 U\\\u2202r and W = \u2202r\\U ). This procedure allows us to distinguish subsets of \u2202r from other sets of vertices, thus motivating the following algorithm.\nLOCAL INDEPENDENCE TEST( samples {x(\u2113)}, thresholds (\u01eb, \u03b3) ) 1: Select a node r \u2208 V ; 2: Set as its neighborhood the largest candidate neighbor U of\nsize at most \u2206 for which the score function SCORE(U) > \u01eb/2; 3: Repeat for all nodes r \u2208 V ;\nThe score function SCORE( \u00b7 ) depends on ({x(\u2113)},\u2206, \u03b3) and is defined as follows,\nmin W,j max xi,xW ,xU ,xj\n|P\u0302n,G,\u03b8{Xi = xi|XW = xW , XU = xU}\u2212\nP\u0302n,G,\u03b8{Xi = xi|XW = xW , XU\\j = xU\\j , Xj = xj}| . (5) In the minimum, |W | \u2264 \u2206 and j \u2208 U . In the maximum, the values must be such that\nP\u0302n,G,\u03b8{XW = xW , XU = xU} > \u03b3/2, P\u0302n,G,\u03b8{XW = xW , XU\\j = xU\\j , Xj = xj} > \u03b3/2\nP\u0302n,G,\u03b8 is the empirical distribution calculated from the samples {x(\u2113)}. We denote this algorithm by Ind(\u01eb, \u03b3). The search over candidate neighbors U , the search for minima and maxima in the computation of the SCORE(U) and the computation of P\u0302n,G,\u03b8 all contribute for \u03c7Ind(G, \u03b8).\nBoth theorems that follow are consequences of the analysis of [4].\nTheorem 1.3. Let G be a graph of bounded degree \u2206 \u2265 1. For every \u03b8 there exists (\u01eb, \u03b3), and a numerical constant K , such that\nnInd(\u01eb,\u03b3)(G, \u03b8) \u2264 100\u2206\n\u01eb2\u03b34 log\n2p\n\u03b4 , \u03c7Ind(\u01eb,\u03b3)(G, \u03b8) \u2264 K (2p)2\u2206+1 log p .\nMore specifically, one can take \u01eb = 14 sinh(2\u03b8), \u03b3 = e \u22124\u2206\u03b8 2\u22122\u2206.\nThis first result implies in particular that G can be reconstructed with polynomial complexity for any bounded \u2206. However, the degree of such polynomial is pretty high and non-uniform in \u2206. This makes the above approach impractical.\nA way out was proposed in [4]. The idea is to identify a set of \u2018potential neighbors\u2019 of vertex r via thresholding: B(r) = {i \u2208 V : C\u0302ri > \u03ba/2} , (6) For each node r \u2208 V , we evaluate SCORE(U) by restricting the minimum in Eq. (5) overW \u2286 B(r), and search only over U \u2286 B(r). We call this algorithm IndD(\u01eb, \u03b3, \u03ba). The basic intuition here is that Cri decreases rapidly with the graph distance between vertices r and i. As mentioned above, this is true at small \u03b8.\nTheorem 1.4. Let G be a graph of bounded degree \u2206 \u2265 1. Assume that \u03b8 < K/\u2206 for some small enough constant K . Then there exists \u01eb, \u03b3, \u03ba such that\nnIndD(\u01eb,\u03b3,\u03ba)(G, \u03b8) \u2264 8(\u03ba2 + 8\u2206) log 4p\n\u03b4 , \u03c7IndD(\u01eb,\u03b3,\u03ba)(G, \u03b8) \u2264 K \u2032p\u2206\u2206\nlog(4/\u03ba) \u03b1 +K \u2032\u2206p2 log p .\nMore specifically, we can take \u03ba = tanh \u03b8, \u01eb = 14 sinh(2\u03b8) and \u03b3 = e \u22124\u2206\u03b8 2\u22122\u2206.\n2If a is a vector and R is a set of indices then we denote by aR the vector formed by the components of a with index in R."}, {"heading": "1.2.2 Regularized Pseudo-Likelihoods", "text": "A different approach to the learning problem consists in maximizing an appropriate empirical likelihood function [7, 8, 9, 10, 13]. To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].\nAs a specific low complexity implementation of this idea, we consider the \u21131-regularized pseudolikelihood method of [7]. For each node r, the following likelihood function is considered\nL(\u03b8; {x(\u2113)}) = \u2212 1 n\nn\u2211\n\u2113=1\nlogPn,G,\u03b8(x (\u2113) r |x (\u2113) \\r ) (7)\nwhere x\\r = xV \\r = {xi : i \u2208 V \\ r} is the vector of all variables except xr and Pn,G,\u03b8 is defined from the following extension of (1),\n\u00b5G,\u03b8(x) = 1\nZG,\u03b8\n\u220f\ni,j\u2208V\ne\u03b8ijxixj (8)\nwhere \u03b8 = {\u03b8ij}i,j\u2208V is a vector of real parameters. Model (1) corresponds to \u03b8ij = 0, \u2200(i, j) /\u2208 E and \u03b8ij = \u03b8, \u2200(i, j) \u2208 E.\nThe function L(\u03b8; {x(\u2113)}) depends only on \u03b8r,\u00b7 = {\u03b8rj , j \u2208 \u2202r} and is used to estimate the neighborhood of each node by the following algorithm, Rlr(\u03bb),\nREGULARIZED LOGISTIC REGRESSION( samples {x(\u2113)}, regularization (\u03bb)) 1: Select a node r \u2208 V ; 2: Calculate \u03b8\u0302r,\u00b7 = arg min\n\u03b8r,\u00b7\u2208R p\u22121\n{L(\u03b8r,\u00b7; {x(\u2113)}) + \u03bb||\u03b8r,\u00b7||1};\n3: If \u03b8\u0302rj > 0, set (r, j) \u2208 E;\nOur first result shows that Rlr(\u03bb) indeed reconstructs G if \u03b8 is sufficiently small.\nTheorem 1.5. There exists numerical constants K1, K2, K3, such that the following is true. Let G be a graph with degree bounded by \u2206 \u2265 3. If \u03b8 \u2264 K1/\u2206, then there exist \u03bb such that\nnRlr(\u03bb)(G, \u03b8) \u2264 K2 \u03b8\u22122 \u2206 log 8p2\n\u03b4 . (9)\nFurther, the above holds with \u03bb = K3 \u03b8\u2206\u22121/2.\nThis theorem is proved by noting that for \u03b8 \u2264 K1/\u2206 correlations decay exponentially, which makes all conditions in Theorem 1 of [7] (denoted there by A1 and A2) hold, and then computing the probability of success as a function of n, while strenghtening the error bounds of [7].\nIn order to prove a converse to the above result, we need to make some assumptions on \u03bb. Given \u03b8 > 0, we say that \u03bb is \u2018reasonable for that value of \u03b8 if the following conditions old: (i) Rlr(\u03bb) is successful with probability larger than 1/2 on any star graph (a graph composed by a vertex r connected to \u2206 neighbors, plus isolated vertices); (ii) \u03bb \u2264 \u03b4(n) for some sequence \u03b4(n) \u2193 0. Theorem 1.6. There exists a numerical constant K such that the following happens. If \u2206 > 3, \u03b8 > K/\u2206, then there exists graphs G of degree bounded by \u2206 such that for all reasonable \u03bb, nRlr(\u03bb)(G) = \u221e, i.e. regularized logistic regression fails with high probability.\nThe graphs for which regularized logistic regression fails are not contrived examples. Indeed we will prove that the claim in the last theorem holds with high probability when G is a uniformly random graph of regular degree \u2206.\nThe proof Theorem 1.6 is based on showing that an appropriate incoherence condition is necessary for Rlr to successfully reconstruct G. The analogous result was proven in [14] for model selection using the Lasso. In this paper we show that such a condition is also necessary when the underlying model is an Ising model. Notice that, given the graph G, checking the incoherence condition is NP-hard for general (non-ferromagnetic) Ising model, and requires significant computational effort\neven in the ferromagnetic case. Hence the incoherence condition does not provide, by itself, a clear picture of which graph structure are difficult to learn. We will instead show how to evaluate it on specific graph families.\nUnder the restriction \u03bb \u2192 0 the solutions given by Rlr converge to \u03b8\u2217 with n [7]. Thus, for large n we can expand L around \u03b8\u2217 to second order in (\u03b8 \u2212 \u03b8\u2217). When we add the regularization term to L we obtain a quadratic model analogous the Lasso plus the error term due to the quadratic approximation. It is thus not surprising that, when \u03bb \u2192 0 the incoherence condition introduced for the Lasso in [14] is also relevant for the Ising model."}, {"heading": "2 Numerical experiments", "text": "In order to explore the practical relevance of the above results, we carried out extensive numerical simulations using the regularized logistic regression algorithm Rlr(\u03bb). Among other learning algorithms, Rlr(\u03bb) strikes a good balance of complexity and performance. Samples from the Ising model (1) where generated using Gibbs sampling (a.k.a. Glauber dynamics). Mixing time can be very large for \u03b8 \u2265 \u03b8crit, and was estimated using the time required for the overall bias to change sign (this is a quite conservative estimate at low temperature). Generating the samples {x(\u2113)} was indeed the bulk of our computational effort and took about 50 days CPU time on Pentium Dual Core processors (we show here only part of these data). Notice that Rlr(\u03bb) had been tested in [7] only on tree graphs G, or in the weakly coupled regime \u03b8 < \u03b8crit. In these cases sampling from the Ising model is easy, but structural learning is also intrinsically easier.\nFigure reports the success probability of Rlr(\u03bb) when applied to random subgraphs of a 7 \u00d7 7 two-dimensional grid. Each such graphs was obtained by removing each edge independently with probability \u03c1 = 0.3. Success probability was estimated by applying Rlr(\u03bb) to each vertex of 8 graphs (thus averaging over 392 runs of Rlr(\u03bb)), using n = 4500 samples. We scaled the regularization parameter as \u03bb = 2\u03bb0\u03b8(log p/n)1/2 (this choice is motivated by the algorithm analysis and is empirically the most satisfactory), and searched over \u03bb0.\nThe data clearly illustrate the phenomenon discussed. Despite the large number of samples n \u226b log p, when \u03b8 crosses a threshold, the algorithm starts performing poorly irrespective of \u03bb. Intriguingly, this threshold is not far from the critical point of the Ising model on a randomly diluted grid \u03b8crit(\u03c1 = 0.3) \u2248 0.7 [15, 16].\nFigure 2 presents similar data when G is a uniformly random graph of degree \u2206 = 4, over p = 50 vertices. The evolution of the success probability with n clearly shows a dichotomy. When \u03b8 is below a threshold, a small number of samples is sufficient to reconstruct G with high probability. Above the threshold even n = 104 samples are to few. In this case we can predict the threshold analytically, cf. Lemma 3.3 below, and get \u03b8thr(\u2206 = 4) \u2248 0.4203, which compares favorably with the data."}, {"heading": "3 Proofs", "text": "In order to prove Theorem 1.6, we need a few auxiliary results. It is convenient to introduce some notations. If M is a matrix and R,P are index sets then MR P denotes the submatrix with row indices in R and column indices in P . As above, we let r be the vertex whose neighborhood we are trying to reconstruct and define S = \u2202r, Sc = V \\ \u2202r \u222a r. Since the cost function L(\u03b8; {x(\u2113)}) + \u03bb||\u03b8||1 only depend on \u03b8 through its components \u03b8r,\u00b7 = {\u03b8rj}, we will hereafter neglect all the other parameters and write \u03b8 as a shorthand of \u03b8r,\u00b7.\nLet z\u0302\u2217 be a subgradient of ||\u03b8||1 evaluated at the true parameters values, \u03b8\u2217 = {\u03b8rj : \u03b8ij = 0, \u2200j /\u2208 \u2202r, \u03b8rj = \u03b8, \u2200j \u2208 \u2202r}. Let \u03b8\u0302 n be the parameter estimate returned by Rlr(\u03bb) when the number of samples is n. Note that, since we assumed \u03b8\u2217 \u2265 0, z\u0302\u2217S = 1. Define Qn(\u03b8, ; {x(\u2113)}) to be the Hessian of L(\u03b8; {x(\u2113)}) and Q(\u03b8) = limn\u2192\u221e Qn(\u03b8, ; {x(\u2113)}). By the law of large numbers Q(\u03b8) is the Hessian of EG,\u03b8 logPG,\u03b8(Xr|X\\r) where EG,\u03b8 is the expectation with respect to (8) and X is a random variable distributed according to (8). We will denote the maximum and minimum eigenvalue of a symmetric matrix M by \u03c3max(M) and \u03c3min(M) respectively.\nWe will omit arguments whenever clear from the context. Any quantity evaluated at the true parameter values will be represented with a \u2217, e.g. Q\u2217 = Q(\u03b8\u2217). Quantities under a \u2227 depend on n. Throughout this section G is a graph of maximum degree \u2206."}, {"heading": "3.1 Proof of Theorem 1.6", "text": "Our first auxiliary results establishes that, if \u03bb is small, then ||Q\u2217ScSQ\u2217SS\u22121z\u0302\u2217S ||\u221e > 1 is a sufficient condition for the failure of Rlr(\u03bb).\nLemma 3.1. Assume [Q\u2217ScSQ \u2217 SS \u22121z\u0302\u2217S ]i \u2265 1+\u01eb for some \u01eb > 0 and some row i \u2208 V , \u03c3min(Q\u2217SS) \u2265 Cmin > 0, and \u03bb < \u221a C3min\u01eb/2 9\u22064. Then the success probability of Rlr(\u03bb) is upper bounded as\nPsucc \u2264 4\u22062e\u2212n\u03b4 2 A + 2\u2206 e\u2212n\u03bb 2\u03b42B (10)\nwhere \u03b4A = (C2min/100\u2206 2)\u01eb and \u03b4B = (Cmin/8\u2206)\u01eb.\nThe next Lemma implies that, for \u03bb to be \u2018reasonable\u2019 (in the sense introduced in Section 1.2.2), n\u03bb2 must be unbounded. Lemma 3.2. There exist M = M(K, \u03b8) > 0 for \u03b8 > 0 such that the following is true: If G is the graph with only one edge between nodes r and i and n\u03bb2 \u2264 K , then\nPsucc \u2264 e\u2212M(K,\u03b8)p + e\u2212n(1\u2212tanh \u03b8) 2/32 . (11)\nFinally, our key result shows that the condition ||Q\u2217ScSQ\u2217SS\u22121z\u0302\u2217S ||\u221e \u2264 1 is violated with high probability for large random graphs. The proof of this result relies on a local weak convergence result for ferromagnetic Ising models on random graphs proved in [17]. Lemma 3.3. Let G be a uniformly random regular graph of degree \u2206 > 3, and \u01eb > 0 be sufficiently small. Then, there exists \u03b8thr(\u2206, \u01eb) such that, for \u03b8 > \u03b8thr(\u2206, \u01eb), ||Q\u2217ScSQ\u2217SS\u22121z\u0302\u2217S ||\u221e \u2265 1 + \u01eb with probability converging to 1 as p \u2192 \u221e. Furthermore, for large \u2206, \u03b8thr(\u2206, 0+) = \u03b8\u0303\u2206\u22121(1 + o(1)). The constant \u03b8\u0303 is given by \u03b8\u0303 = tanh h\u0304)/h\u0304 and h\u0304 is the unique positive solution of h\u0304 tanh h\u0304 = (1 \u2212 tanh2 h\u0304)2. Finally, there exist Cmin > 0 dependent only on \u2206 and \u03b8 such that \u03c3min(Q\u2217SS) \u2265 Cmin with probability converging to 1 as p \u2192 \u221e.\nThe proofs of Lemmas 3.1 and 3.3 are sketched in the next subsection. Lemma 3.2 is more straightforward and we omit its proof for space reasons.\nProof. (Theorem 1.6) Fix \u2206 > 3, \u03b8 > K/\u2206 (where K is a large enough constant independent of \u2206), and \u01eb, Cmin > 0 and both small enough. By Lemma 3.3, for any p large enough we can choose a \u2206-regular graph Gp = (V = [p], Ep) and a vertex r \u2208 V such that |Q\u2217ScSQ\u2217SS\u221211S |i > 1 + \u01eb for some i \u2208 V \\ r. By Theorem 1 in [4] we can assume, without loss of generality n > K \u2032\u2206log p for some small constant K \u2032. Further by Lemma 3.2, n\u03bb2 \u2265 F (p) for some F (p) \u2191 \u221e as p \u2192 \u221e and the condition of Lemma 3.1 on \u03bb is satisfied since by the \u201dreasonable\u201d assumption \u03bb \u2192 0 with n. Using these results in Eq. (10) of Lemma 3.1 we get the following upper bound on the success probability\nPsucc(Gp) \u2264 4\u22062p\u2212\u03b4 2 AK \u2032\u2206 + 2\u2206 e\u2212nF (p)\u03b4 2 B . (12)\nIn particular Psucc(Gp) \u2192 0 as p \u2192 \u221e."}, {"heading": "3.2 Proofs of auxiliary lemmas", "text": "Proof. (Lemma 3.1) We will show that under the assumptions of the lemma and if \u03b8\u0302 = (\u03b8\u0302S , \u03b8\u0302SC ) = (\u03b8\u0302S , 0) then the probability that the i component of any subgradient ofL(\u03b8; {x(\u2113)})+\u03bb||\u03b8||1 vanishes for any \u03b8\u0302S > 0 (component wise) is upper bounded as in Eq. (10). To simplify notation we will omit {x(\u2113)} in all the expression derived from L.\nLet z\u0302 be a subgradient of ||\u03b8|| at \u03b8\u0302 and assume \u2207L(\u03b8\u0302) + \u03bbz\u0302 = 0. An application of the mean value theorem yields \u22072L(\u03b8\u2217)[\u03b8\u0302 \u2212 \u03b8\u2217] = Wn \u2212 \u03bbz\u0302 +Rn , (13) where Wn = \u2212\u2207L(\u03b8\u2217) and [Rn]j = [\u22072L(\u03b8\u0304(j))\u2212\u22072L(\u03b8\u2217)]Tj (\u03b8\u0302\u2212 \u03b8\u2217) with \u03b8\u0304 (j) a point in the line from \u03b8\u0302 to \u03b8\u2217. Notice that by definition \u22072L(\u03b8\u2217) = Qn\u2217 = Qn(\u03b8\u2217). To simplify notation we will omit the \u2217 in all Qn\u2217. All Qn in this proof are thus evaluated at \u03b8\u2217.\nBreaking this expression into its S and Sc components and since \u03b8\u0302SC = \u03b8 \u2217 SC = 0 we can eliminate \u03b8\u0302S \u2212 \u03b8\u2217S from the two expressions obtained and write [WnSC \u2212RnSC ]\u2212QnSCS(QnSS)\u22121[WnS \u2212RnS ] + \u03bbQnSCS(QnSS)\u22121z\u0302S = \u03bbz\u0302SC . (14)\nNow notice that QnSCS(Q n SS) \u22121 = T1 + T2 + T3 + T4 where\nT1 = Q \u2217 SCS [(Q n SS) \u22121 \u2212 (Q\u2217SS)\u22121] , T2 = [QnSCS \u2212Q\u2217SCS ]Q\u2217SS \u22121 ,\nT3 = [Q n SCS \u2212Q\u2217SCS ][(QnSS)\u22121 \u2212 (Q\u2217SS)\u22121] , T4 = Q\u2217SCSQ\u2217SS \u22121 .\nWe will assume that the samples {x(\u2113)} are such that the following event holds E \u2261 {||QnSS \u2212Q\u2217SS ||\u221e < \u03beA, ||QnSCS \u2212Q\u2217SCS ||\u221e < \u03beB , ||WnS /\u03bb||\u221e < \u03beC} , (15) where \u03beA \u2261 C2min\u01eb/(16\u2206), \u03beB \u2261 Cmin\u01eb/(8 \u221a \u2206) and \u03beC \u2261 Cmin\u01eb/(8\u2206). Since EG,\u03b8(Qn) = Q\u2217 and EG,\u03b8(Wn) = 0 and noticing that both Qn and Wn are sums of bounded i.i.d. random variables, a simple application of Azuma-Hoeffding inequality upper bounds the probability of E as in (10). From E it follows that \u03c3min(QnSS) > \u03c3min(Q\u2217SS) \u2212 Cmin/2 > Cmin/2. We can therefore lower bound the absolute value of the ith component of z\u0302SC by |[Q\u2217SCSQ\u2217SS \u221211S ]i|\u2212||T1,i||\u221e\u2212||T2,i||\u221e\u2212||T3,i||\u221e\u2212\u2223\u2223\u2223Wni\n\u03bb \u2223\u2223\u2223\u2212 \u2223\u2223\u2223 Rni \u03bb \u2223\u2223\u2223\u2212 \u2206 Cmin (\u2223\u2223\u2223 \u2223\u2223\u2223 WnS \u03bb \u2223\u2223\u2223 \u2223\u2223\u2223 \u221e + \u2223\u2223\u2223 \u2223\u2223\u2223 RnS \u03bb \u2223\u2223\u2223 \u2223\u2223\u2223 \u221e ) ,\nwhere the subscript i denotes the i-th row of a matrix.\nThe proof is completed by showing that the event E and the assumptions of the theorem imply that each of last 7 terms in this expression is smaller than \u01eb/8. Since |[Q\u2217SCSQ\u2217SS \u22121]Ti z\u0302 n S | \u2265 1 + \u01eb by assumption, this implies |z\u0302i| \u2265 1 + \u01eb/8 > 1 which cannot be since any subgradient of the 1-norm has components of magnitude at most 1.\nThe last condition on E immediately bounds all terms involving W by \u01eb/8. Some straightforward manipulations imply (See Lemma 7 from [7])\n||T1,i||\u221e \u2264 \u2206\nC2min ||QnSS \u2212Q\u2217SS ||\u221e , ||T2,i||\u221e \u2264\n\u221a \u2206\nCmin ||[QnSCS \u2212Q\u2217SCS ]i||\u221e ,\n||T3,i||\u221e \u2264 2\u2206\nC2min ||QnSS \u2212Q\u2217SS ||\u221e||[QnSCS \u2212Q\u2217SCS ]i||\u221e ,\nand thus all will be bounded by \u01eb/8 when E holds. The upper bound of Rn follows along similar lines via an mean value theorem, and is deferred to a longer version of this paper.\nProof. (Lemma 3.3.) Let us state explicitly the local weak convergence result mentioned in Sec. 3.1. For t \u2208 N, let T(t) = (VT, ET) be the regular rooted tree of t generations and define the associated Ising measure as\n\u00b5+T,\u03b8(x) = 1\nZT,\u03b8\n\u220f\n(i,j)\u2208ET\ne\u03b8xixj \u220f\ni\u2208\u2202T(t)\neh \u2217xi . (16)\nHere \u2202T(t) is the set of leaves of T(t) and h\u2217 is the unique positive solution of h = (\u2206 \u2212 1) atanh {tanh \u03b8 tanhh}. It can be proved using [17] and uniform continuity with respect to the \u2018external field\u2019 that non-trivial local expectations with respect to \u00b5G,\u03b8(x) converge to local expectations with respect to \u00b5+T,\u03b8(x), as p \u2192 \u221e.\nMore precisely, let Br(t) denote a ball of radius t around node r \u2208 G (the node whose neighborhood we are trying to reconstruct). For any fixed t, the probability that Br(t) is not isomorphic to T(t) goes to 0 as p \u2192 \u221e. Let g(xBr(t)) be any function of the variables in Br(t) such that g(xBr(t)) = g(\u2212xBr(t)). Then almost surely over graph sequences Gp of uniformly random regular graphs with p nodes (expectations here are taken with respect to the measures (1) and (16))\nlim p\u2192\u221e\nEG,\u03b8{g(XBr(t))} = ET(t),\u03b8,+{g(XT(t))} . (17)\nThe proof consists in considering [Q\u2217ScSQ \u2217 SS \u22121z\u0302\u2217S ]i for t = dist(r, i) finite. We then write (Q\u2217SS)lk = E{gl,k(XBr(t))} and (Q \u2217 ScS)il = E{gi,l(XBr(t))} for some functions g\u00b7,\u00b7(XBr(t)) and apply the weak convergence result (17) to these expectations. We thus reduced the calculation of [Q\u2217ScSQ \u2217 SS\n\u22121z\u0302\u2217S]i to the calculation of expectations with respect to the tree measure (16). The latter can be implemented explicitly through a recursive procedure, with simplifications arising thanks to the tree symmetry and by taking t \u226b 1. The actual calculations consist in a (very) long exercise in calculus and we omit them from this outline.\nThe lower bound on \u03c3min(Q\u2217SS) is proved by a similar calculation."}, {"heading": "Acknowledgments", "text": "This work was partially supported by a Terman fellowship, the NSF CAREER award CCF-0743978 and the NSF grant DMS-0806211 and by a Portuguese Doctoral FCT fellowship."}], "references": [{"title": "Learning factor graphs in polynomial time and sample complexity", "author": ["P. Abbeel", "D. Koller", "A. Ng"], "venue": "Journal of Machine Learning Research.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Information-theoretic limits on sparsity recovery in the high-dimensional and noisy setting", "author": ["M. Wainwright"], "venue": "[math.ST],", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Information-theoretic limits of selecting binary graphical models in high dimensions", "author": ["N. Santhanam", "M. Wainwright"], "venue": "[cs.IT],", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Reconstruction of Markov Random Fields from Samples: Some Observations and Algorithms\u201d,Proceedings of the 11th international workshop, APPROX 2008, and 12th international workshop", "author": ["G. Bresler", "E. Mossel", "A. Sly"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Consistent estimation of the basic neighborhood structure of Markov random fields", "author": ["Csiszar", "Z. Talata"], "venue": "The Annals of Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Learning Bayesian network structure from massive datasets: The sparse candidate algorithm", "author": ["N. Friedman", "I. Nachman", "D. Peer"], "venue": "In UAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "High-Dimensional Ising Model Selection Using l1-Regularized Logistic Regression", "author": ["P. Ravikumar", "M. Wainwright", "J. Lafferty"], "venue": "[math.ST],", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Inferring graphical model structure using l1regularized pseudolikelihood", "author": ["M.Wainwright", "P. Ravikumar", "J. Lafferty"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Estimation of Sparse Binary Pairwise Markov Networks using Pseudo-likelihoods", "author": ["H. H\u00f6fling", "R. Tibshirani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "d\u2019Aspremont, \u201cModel Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data", "author": ["O.Banerjee", "A.L. El Ghaoui"], "venue": "Journal of Machine Learning Research, March 2008,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Model Selection and Estimation in Regression with Grouped Variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Royal. Statist. Soc B,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "B\u00fcuhlmann, \u201cHigh dimensional graphs and variable selection with the lasso", "author": ["P.N. Meinshausen"], "venue": "Annals of Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "Journal of Machine. Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Critical behavior of the bond-dilute two-dimensional Ising model", "author": ["D. Zobin"], "venue": "Phys. Rev.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1978}, {"title": "Critical Temperatures of Anisotropic Ising Lattices", "author": ["M. Fisher"], "venue": "II. General Upper Bounds\u201d, Phys. Rev", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1967}, {"title": "Ising Models on Locally Tree Like Graphs", "author": ["A. Dembo", "A. Montanari"], "venue": "Ann. Appl. Prob", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "The graph learning problem is solvable with unbounded sample complexity, and computational resources [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "General bounds on nAlg(G, \u03b8) have been given in [2, 3], under the assumption of unbounded computational resources.", "startOffset": 48, "endOffset": 54}, {"referenceID": 2, "context": "General bounds on nAlg(G, \u03b8) have been given in [2, 3], under the assumption of unbounded computational resources.", "startOffset": 48, "endOffset": 54}, {"referenceID": 0, "context": "A recurring approach to structural learning consists in exploiting the conditional independence structure encoded by the graph [1, 4, 5, 6].", "startOffset": 127, "endOffset": 139}, {"referenceID": 3, "context": "A recurring approach to structural learning consists in exploiting the conditional independence structure encoded by the graph [1, 4, 5, 6].", "startOffset": 127, "endOffset": 139}, {"referenceID": 4, "context": "A recurring approach to structural learning consists in exploiting the conditional independence structure encoded by the graph [1, 4, 5, 6].", "startOffset": 127, "endOffset": 139}, {"referenceID": 5, "context": "A recurring approach to structural learning consists in exploiting the conditional independence structure encoded by the graph [1, 4, 5, 6].", "startOffset": 127, "endOffset": 139}, {"referenceID": 3, "context": "Let us consider, to be definite, the approach of [4], specializing it to the model (1).", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "Both theorems that follow are consequences of the analysis of [4].", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "A way out was proposed in [4].", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "A different approach to the learning problem consists in maximizing an appropriate empirical likelihood function [7, 8, 9, 10, 13].", "startOffset": 113, "endOffset": 130}, {"referenceID": 7, "context": "A different approach to the learning problem consists in maximizing an appropriate empirical likelihood function [7, 8, 9, 10, 13].", "startOffset": 113, "endOffset": 130}, {"referenceID": 8, "context": "A different approach to the learning problem consists in maximizing an appropriate empirical likelihood function [7, 8, 9, 10, 13].", "startOffset": 113, "endOffset": 130}, {"referenceID": 9, "context": "A different approach to the learning problem consists in maximizing an appropriate empirical likelihood function [7, 8, 9, 10, 13].", "startOffset": 113, "endOffset": 130}, {"referenceID": 12, "context": "A different approach to the learning problem consists in maximizing an appropriate empirical likelihood function [7, 8, 9, 10, 13].", "startOffset": 113, "endOffset": 130}, {"referenceID": 6, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 7, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 8, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 9, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 10, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 11, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 12, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 6, "context": "As a specific low complexity implementation of this idea, we consider the l1-regularized pseudolikelihood method of [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "This theorem is proved by noting that for \u03b8 \u2264 K1/\u2206 correlations decay exponentially, which makes all conditions in Theorem 1 of [7] (denoted there by A1 and A2) hold, and then computing the probability of success as a function of n, while strenghtening the error bounds of [7].", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "This theorem is proved by noting that for \u03b8 \u2264 K1/\u2206 correlations decay exponentially, which makes all conditions in Theorem 1 of [7] (denoted there by A1 and A2) hold, and then computing the probability of success as a function of n, while strenghtening the error bounds of [7].", "startOffset": 273, "endOffset": 276}, {"referenceID": 13, "context": "The analogous result was proven in [14] for model selection using the Lasso.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "Under the restriction \u03bb \u2192 0 the solutions given by Rlr converge to \u03b8 with n [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 13, "context": "It is thus not surprising that, when \u03bb \u2192 0 the incoherence condition introduced for the Lasso in [14] is also relevant for the Ising model.", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "Notice that Rlr(\u03bb) had been tested in [7] only on tree graphs G, or in the weakly coupled regime \u03b8 < \u03b8crit.", "startOffset": 38, "endOffset": 41}, {"referenceID": 14, "context": "7 [15, 16].", "startOffset": 2, "endOffset": 10}, {"referenceID": 15, "context": "7 [15, 16].", "startOffset": 2, "endOffset": 10}, {"referenceID": 16, "context": "The proof of this result relies on a local weak convergence result for ferromagnetic Ising models on random graphs proved in [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 3, "context": "By Theorem 1 in [4] we can assume, without loss of generality n > K \u2206log p for some small constant K .", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "Some straightforward manipulations imply (See Lemma 7 from [7]) ||T1,i||\u221e \u2264 \u2206 C2 min ||QSS \u2212QSS ||\u221e , ||T2,i||\u221e \u2264 \u221a \u2206 Cmin ||[QnSCS \u2212Q\u2217SCS ]i||\u221e , ||T3,i||\u221e \u2264 2\u2206 C2 min ||QSS \u2212QSS ||\u221e||[QSCS \u2212QSCS ]i||\u221e , and thus all will be bounded by \u01eb/8 when E holds.", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "It can be proved using [17] and uniform continuity with respect to the \u2018external field\u2019 that non-trivial local expectations with respect to \u03bcG,\u03b8(x) converge to local expectations with respect to \u03bc+T,\u03b8(x), as p \u2192 \u221e.", "startOffset": 23, "endOffset": 27}], "year": 2009, "abstractText": "We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity algorithms systematically fail when the Markov random field develops long-range correlations. More precisely, this phenomenon appears to be related to the Ising model phase transition (although it does not coincide with it).", "creator": "gnuplot 4.2 patchlevel 2 "}}}