{"id": "1001.0879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2010", "title": "Linear Probability Forecasting", "abstract": "multi - coefficient estimation is one potentially the most important consideration in dynamic design. in this paper we consider simple online scala - class validation problems : classification inside a marko model and by our kernelized model. overall quality of predictions is decided by the brier sampling filter. we evaluate two task efficient algorithms constantly work with these problems before identify conceptual guarantees on that complexity. we kernelize one of the algorithms not prove formal guarantees on its loss. we perform scaling and verify approximation algorithms with logistic regression.", "histories": [["v1", "Wed, 6 Jan 2010 12:40:13 GMT  (44kb)", "http://arxiv.org/abs/1001.0879v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fedor zhdanov", "yuri kalnishkan"], "accepted": false, "id": "1001.0879"}, "pdf": {"name": "1001.0879.pdf", "metadata": {"source": "CRF", "title": "Linear Probability Forecasting", "authors": ["Fedor Zhdanov", "Yuri Kalnishkan"], "emails": ["fedor@cs.rhul.ac.uk", "yura@cs.rhul.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 1.\n08 79\nv1 [\ncs .L\nG ]\nMulti-class classification is one of the most important tasks in machine learning. In this paper we consider two online multi-class classification problems: classification by a linear model and by a kernelized model. The quality of predictions is measured by the Brier loss function. We suggest two computationally efficient algorithms to work with these problems and prove theoretical guarantees on their losses. We kernelize one of the algorithms and prove theoretical guarantees on its loss. We perform experiments and compare our algorithms with logistic regression."}, {"heading": "1 Introduction", "text": "Online prediction is a wide area of machine learning (see Cesa-Bianchi and Lugosi, 2006). Its algorithms can be applied to different data mining problems (see for example Freund and Schapire, 1997). Online prediction provides efficient algorithms which adapt to a predicted process \u201con fly\u201d. In online regression framework we assume the existence of some input at each step and try to predict an outcome on this input. This process is repeated step by step. We consider multi-dimensional Brier game where outcomes and predictions come from a simplex and can be thought of as probability distributions on the vertices of the simplex. If the outcomes are identified with vertices of the simplex this problem can be thought of as the multi-class classification problem of the given input.\nIn the simple case the dependence between the input and its outcome is assumed to be linear; linear regression minimising the expected loss is studied in statistics. As opposite to the traditional statistical setting, the learner in online prediction does not make any statistical assumptions about the data generating process. Its goal is to predict as well as the best linear function on input. Instead of looking for the best linear function, our learner considers all linear functions and makes his prediction by mixing them in a certain way at each prediction step. We prove theoretical bounds on the cumulative loss of the learner in comparison with the cumulative loss of the best linear function (we\nsay the learner competes with these functions). We consider the square loss: mean square error is one of the benchmark measures for classification algorithms (see Brier, 1950).\nWe use Vovk\u2019s Aggregating Algorithm (a generalization of the Bayesian mixture) to mix functions (as in Aggregating Algorithm Regression, AAR: see Vovk, 2001). This method has previously been applied to the case when possible outcomes lie in a segment of the real line, and so the prediction was one-dimensional. We develop two algorithms to solve the problem of multi-dimensional prediction. The first algorithm applies a variant of AAR to predict each coordinate of the outcome separately, and then combines these predictions in a certain way to get probability prediction. The other algorithm is designed to give probability predictions directly; these are first computationally efficient online regression algorithm designed to solve linear and non-linear multi-class classification problems. We derive theoretical bounds on the losses of both algorithms. We come to an unexpected conclusion that the component-wise algorithm is better than the second one asymptotically, but worse in the beginning of the prediction process. Their performance on benchmark data sets is very similar.\nOne component of the prediction of the second algorithm has the meaning of a remainder. In practice this situation is quite common. For example, in a football match either one team wins or the other, and the remainder is a draw (see Vovk and Zhdanov (2008) for online prediction experiments in football). When we analyse a precious metal alloy we may look for a description of the following kind: the alloy has 40% of gold, 35% of silver, and some addition (e.g., copper and palladium). It is common for financial applications to predict the direction of the price: the price can go up, down, or stay close to the current value. We perform classification experiments with linear algorithms and compare them with logistic regression.\nA description of the framework can be found in Section 2, description of the algorithms can be found in Section 3, and derivation of the theoretical bounds can be found in Section 4.\nWe look for a way to extend the class of experts using the kernel trick. We kernelize the second algorithm and prove a theoretical bound on its loss. The cumulative loss of the kernelized algorithm is compared with the cumulative loss of any finite set of functions from the RKHS given by a kernel parameter it uses. Kernelization process is described in Section 5. Our experiments are shown in Section 6. Section 7 makes the conclusions and shows some possibilities for prospective work."}, {"heading": "2 Framework", "text": "A game of prediction contains three components: a space \u2126 of outcomes, a decision space \u0393, and a loss function \u03bb : \u2126 \u00d7 \u0393 \u2192 R. We are interested in the generalisation of the Brier game from Brier (1950) where the space of outcomes \u2126 = P(\u03a3) is the set of all probability measures on a finite set \u03a3 with d elements, \u0393 := {(\u03b31, . . . , \u03b3d) : \u2211d i=1 \u03b3i = 1, \u03b3i \u2208 R} is a hyperplane in d-dimensional space\ncontaining all the outcomes, and for any y \u2208 \u2126 we define the loss\n\u03bb(y, \u03b3) = \u2211\n\u03c3\u2208\u03a3\n(\u03b3{\u03c3} \u2212 y{\u03c3})2 .\nFor example, if \u2126 = {1, 2, 3}, \u03c9 = 1, \u03b3{1} = 1/2, \u03b3{2} = 1/4, and \u03b3{3} = 1/4, \u03bb(\u03c9, \u03b3) = (1/2\u22121)2+(1/4\u22120)2+(1/4\u22120)2 = 3/8. Brier loss is one of the most important loss functions used to assess the quality of classification algorithms. The game of prediction is being played repeatedly by a learner receiving some input vectors xt \u2208 X \u2286 Rn, and follows prediction protocol 1.\nProtocol 1 Protocol of forecasting game\nL0 := 0. for t = 1, 2, . . . do Reality announces a signal xt \u2208 X \u2286 Rn. Learner announces \u03b3t \u2208 \u0393 \u2286 Rd. Reality announces yt \u2208 \u2126 \u2286 Rd. Lt := Lt\u22121 + \u03bb(yt, \u03b3t). end for\nWe find an algorithm which is capable of competing with all linear functions (we call them experts) \u03bet = (\u03be 1 t , . . . , \u03be d t ) \u2032 on x:\n\u03be1t = 1/d+ \u03b1 \u2032 1xt\n. . .\n\u03bed\u22121t = 1/d+ \u03b1 \u2032 d\u22121xt (1)\n\u03bedt = 1\u2212 \u03be1 \u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03bed\u22121 = 1/d\u2212 ( d\u22121\u2211\ni=1\n\u03b1i )\u2032 xt,\nwhere \u03b1i = (\u03b1 1 i , . . . , \u03b1 n i ) \u2032, i = 1, . . . , d\u2212 1. In the model (1) the prediction for the last component of an outcome is calculated from the predictions for other components. Denote \u03b1 = (\u03b1\u20321, . . . , \u03b1 \u2032 d\u22121)\n\u2032 \u2208 \u0398 = Rn(d\u22121). Then any expert can be presented as \u03bet = \u03bet(\u03b1). Let also LT (\u03b1) = \u2211T t=1 \u03bb(yt, \u03bet(\u03b1)) be the cumulative loss of an expert \u03b1 over T trials."}, {"heading": "3 Derivation of the algorithms", "text": "In this section we describe how we apply the Aggregating Algorithm (AA) proposed in Vovk (1990) to mix experts and make predictions. The algorithm keeps weights Pt\u22121(d\u03b1) for the experts at each prediction step t, and updates them by the exponential weighting scheme after the actual outcomes is announced:\nPt(d\u03b1) = \u03b2 \u03bb(yt,\u03bet(\u03b1))Pt\u22121(d\u03b1), \u03b2 \u2208 (0, 1). (2)\nHere \u03b2 = e\u2212\u03b7, where \u03b7 \u2208 (0,\u221e) is a learning rate parameter. This weight update ensures that the experts which predict badly at the step t receive less weight. The weights are then normalized P \u2217t (d\u03b1) = Pt(d\u03b1) Pt(\u0398) .\nThe prediction of the algorithm is a combination of the experts\u2019 predictions. It is suggested in Kivinen and Warmuth (1999) that the prediction is simply the weighted average of the experts\u2019 predictions with weights Pt(d\u03b1). The Aggregating Algorithm uses more sophisticated prediction scheme, and sometimes achieves better theoretical performance. It first defines a generalised prediction at any step t as a function gt : \u2126 \u2192 R such that\ngt(y) = log\u03b2\n\u222b\n\u0398\n\u03b2\u03bb(y,\u03bet(\u03b1))P \u2217t\u22121(d\u03b1) (3)\nfor all y \u2208 \u2126. It is a weighted average (in a general sense) of the experts\u2019 losses for each possible outcome. It then predicts any \u03b3t such that\n\u03bb(y, \u03b3t) \u2264 gt(y) (4)\nfor all possible y \u2208 \u2126. If such prediction can be found for any weights distribution on experts the game is called perfectly mixable. Perfectly mixable games and other types of games are analyzed in Vovk (1998). It is also shown there that for countable (and thus finite) number of experts the AA achieves the best possible theoretical guarantees."}, {"heading": "3.1 Proof of mixability", "text": "In this section we prove that our game is perfectly mixable and show a function that can be used to give predictions satisfying (4).\nIt is shown in Theorem 1 Vovk and Zhdanov (2008) that the Brier game with finite number of outcomes is perfectly mixable iff \u03b7 \u2208 (0, 1]. The two authors of that paper consider the outcome space of d probability measures concentrated in points of \u03a3. We denote this space by R(\u03a3). They consider experts giving predictions from all probability measures P(\u03a3). We need to prove that the inequality (4) holds for our experts (1) (who can give predictions outside of the probability simplex) and our outcome space \u2126 (the whole probability simplex, not only its vertices). Lemma 2 describes the first part, but first we need to state an additional statement. The following lemma shows that any vector from R d can be projected into simplex without increasing the Brier loss.\nLemma 1. For any \u03be = (\u03be1, . . . , \u03bed) \u2208 Rd there exists \u03b8 = (\u03b81, . . . , \u03b8d) \u2208 P(\u03a3) such that for any y \u2208 \u2126 we have \u03bb(y, \u03b8) \u2264 \u03bb(y, \u03be).\nProof. The Brier loss of a prediction \u03b3 is a square Euclidean distance between \u03b3 and the actual outcome y in a d-dimensional space. The proof follows from the fact that \u2126 is a convex and closed set in Rd.\nLemma 2. Let P (d\u03b1) be any probability distribution on \u0398. Then for any \u03b7 \u2208 (0, 1] there exists \u03b3 \u2208 \u0393 such that for any y \u2208 R(\u03a3) we have\n\u03bb(y, \u03b3) \u2264 log\u03b2 \u222b\n\u0398\n\u03b2\u03bb(y,\u03be(\u03b1))P (d\u03b1).\nProof. By Lemma 1 for any \u03be(\u03b1) we can find \u03b8(\u03b1) \u2208 P(\u03a3) such that the loss of experts decreases: \u03bb(y, \u03b8(\u03b1)) \u2264 \u03bb(y, \u03be(\u03b1)) for any y \u2208 R(\u03a3). Thus we have\nlog\u03b2\n\u222b\n\u0398\n\u03b2\u03bb(y,\u03b8(\u03b1))P (d\u03b1) \u2264 log\u03b2 \u222b\n\u0398\n\u03b2\u03bb(y,\u03be(\u03b1))P (d\u03b1)\nfor any y \u2208 R(\u03a3). We can take the same prediction \u03b3 \u2208 \u0393 that satisfies the necessary inequality with \u03b8 instead of \u03be. By Theorem 1 in Vovk and Zhdanov (2008) such prediction exists for any \u03b7 \u2208 (0, 1] (\u03b2 \u2208 [e\u22121, 1)).\nA way to convert the generalised prediction into the prediction of AA is called a substitution function. We prove that we can use the same substitution function and the same learning rate parameter \u03b7 as for the case of finite number of possible outcomes. Such a function is proposed in Vovk and Zhdanov (2008). This is an extension of Lemma 4.1 from Haussler et al. (1998).\nLemma 3. Let P (d\u03b1) be a probability distribution on \u0398 and put\nf(y) = log\u03b2\n\u222b\n\u0398\n\u03b2\u03bb(y,\u03be(\u03b1))P (d\u03b1)\nfor every y \u2208 \u2126. Then if \u03b3 is such a prediction that \u03bb(z, \u03b3) \u2264 f(z) for any z \u2208 R(\u03a3) then \u03bb(y, \u03b3) \u2264 f(y) for any y \u2208 \u2126.\nProof. For the typographical reasons we will write \u03be instead of \u03be(\u03b1). It is easy to ensure that \u03bb(y, \u03b3)\u2212 \u03bb(y, \u03be) = \u2211\u03c3\u2208\u03a3 y{\u03c3}[\u03bb(z\u03c3, \u03b3)\u2212 \u03bb(z\u03c3, \u03be)] for z\u03c3{\u03c1} = 0 if \u03c3 6= \u03c1 and z\u03c3{\u03c1} = 1 if \u03c3 = \u03c1. We also have that \u03bb(y, \u03b3) \u2212 f(y) \u2264 0 is equivalent to \u222b \u0398 \u03b2\n\u03bb(y,\u03be)\u2212\u03bb(y,\u03b3)P (d\u03b1) \u2264 1. Thus due to the convexity of the exponent function \u222b \u0393 \u03b2 \u2211 \u03c3\u2208\u03a3 y{\u03c3}[\u03bb(z\u03c3,\u03be)\u2212\u03bb(z\u03c3,\u03b3)]P (d\u03b1) \u2264 \u2211\u03c3\u2208\u03a3 y{\u03c3} = 1.\nLet us denote the i-th possible outcome from R(\u03a3) by y{i}, i = 1, . . . , d. We use the substitution function defined by the following proposition:\nProposition 1. Let ri = g(y{i}), and x+ = max(x, 0). Define s \u2208 R by the requirement\nd\u2211\ni=1\n(s\u2212 ri)+ = 2.\nIf the prediction of the Aggregating Algorithm is given by\n\u03b3i = (s\u2212 ri)+\n2 , i = 1, . . . , d\nthen (4) holds.\nThis function allows us to avoid weights normalization in calculating the generalized prediction at each step (avoid \u2217 in the weights distribution), which would be computationally inefficient. Suppose we can get only r = gt(y) + C instead of gt(y), where C is the same for all y. Then predictions \u03b3t defined by the substitution function from Proposition 1 will be the same as if we calculated the generalized prediction with weights normalization."}, {"heading": "3.2 Algorithm for multidimensional outcomes", "text": "We set the prior weights distribution P0 over the set \u0398 = R n(d\u22121) of experts \u03b1 to have the Gaussian density with a parameter a > 0:\n(a\u03b7/\u03c0)n(d\u22121)/2e\u2212a\u03b7\u2016\u03b1\u2016 2 d\u03b1.\nInstead of taking the integral in (3) we get a shifted generalised prediction r by calculating ri = gT (y{i}) \u2212 gT (y{d}) (we omit the index T in r for brevity). Each component of r = (r1, . . . , rd) corresponds to one of the possible outcomes, so rd = 0. Other components, i = 1, . . . , d\u2212 1:\nri = log\u03b2 \u03b2gT (y{i})+\n\u2211T\u22121 t=1 gt(yt)\n\u03b2gT (y{d})+ \u2211T\u22121 t=1 gt(yt) = log\u03b2\n\u222b \u0398 e\n\u2212\u03b7Q(\u03b1,y{i})d\u03b1\u222b \u0398 e\u2212\u03b7Q(\u03b1,y{d})d\u03b1\nwhere by Q(\u03b1, y) we denote the quadratic form:\nQ(\u03b1, y) =\nT\u2211\nt=1\nd\u2211\ni=1\n((yit \u2212 \u03bei(xt))2.\nHere yt = (y 1 t , . . . , y d t ) are the outcomes on the steps before T and yT = (y1T , . . . , y d T ) is a possible outcome on the step T .\nLet C = \u2211T\nt=1 xtx \u2032 t be n\u00d7 n matrix. The quadratic form Q can be divided\ninto a quadratic part, a linear part, and a remainder: Q = Q1 +Q2 +Q3. Here\nQ1(\u03b1, y) = \u03b1 \u2032A\u03b1\nis a quadratic part of Q(\u03b1, y). Here A is a square matrix with n(d \u2212 1) rows (see the expression for A in the algorithm below). The linear part is equal to\nQ2(\u03b1, y) = h \u2032\u03b1\u2212 2\nd\u22121\u2211\ni=1\n(yiT \u2212 ydT )\u03b1\u2032ixT ,\nwhere hi = \u22122 \u2211T\u22121 t=1 (y i t \u2212 ydt )xt, i = 1, . . . , d \u2212 1 make up a big vector h = (h\u20321, . . . , h \u2032 d\u22121) \u2032. The remainder is equal to\nQ3(\u03b1, y) =\nT\u22121\u2211\nt=1\nd\u2211\ni=1\n(yit \u2212 1/d)2 + d\u2211\ni=1\n(yiT \u2212 1/d)2.\nRatio for ri can be calculated using the following lemmas. The integral evaluates as follows:\nLemma 4. Let Q(\u03b1) = \u03b1\u2032A\u03b1+ b\u2032\u03b1+ c, where \u03b1, b \u2208 Rn, c is a scalar and A is a symmetric positive definite n\u00d7 n matrix. Then\n\u222b\nRn\ne\u2212Q(\u03b1)d\u03b1 = e\u2212Q0 \u03c0n/2\u221a detA ,\nwhere Q0 = min\u03b1\u2208Rn Q(\u03b1).\nThe proof of this lemma can be found in Harville (1997, Theorem 15.12.1). Following this lemma, we can rewrite ri as ri = F (A, bi, zi), i = 1, . . . , d \u2212 1, where\nF (A, bi, zi) = min \u03b1\u2208\u0398 Q(\u03b1, yi)\u2212min \u03b1\u2208\u0398 Q(\u03b1, yd).\nVariables bi, zi and the precise formula for F are defined by the following lemma\nLemma 5. Let\nF (A, b, z) = min \u03b1\u2208Rn (\u03b1\u2032A\u03b1 + b\u2032\u03b1+ z\u2032\u03b1)\u2212 min \u03b1\u2208Rn (\u03b1\u2032A\u03b1+ b\u2032\u03b1\u2212 z\u2032\u03b1),\nwhere b, z \u2208 Rn and A is a symmetric positive definite n \u00d7 n matrix. Then F (A, b, z) = \u2212b\u2032A\u22121z.\nProof. This lemma is proven by taking the derivative of the quadratic forms in F by \u03b1 and calculating the minimum: min\u03b1\u2208Rn(\u03b1 \u2032A\u03b1 + c\u2032\u03b1) = \u2212 (A \u22121c)\u2032\n4 c for any c \u2208 Rn (see Harville, 1997, Theorem 19.1.1).\nWe can see that bi = h+(x \u2032 T , . . . , x \u2032 T ,0, x \u2032 T , . . . , x \u2032 T ) \u2032 \u2208 Rn(d\u22121), where 0 is a zero-vector from Rn. We also have zi = (\u2212x\u2032T , . . . ,\u2212x\u2032T ,\u22122x\u2032T ,\u2212x\u2032T , . . . ,\u2212x\u2032T )\u2032. Thus we can calculate d \u2212 1 differences ri, assign rd = 0, and then apply the substitution function from proposition 1 to get predictions. The resulting algorithm is Algorithm 1. We will further call it mAAR (multi-dimensional Aggregating Algorithm for Regression)."}, {"heading": "3.3 Component-wise algorithm", "text": "In this section we derive the component-wise algorithm. It gives predictions for each component of the outcome separately, and then combines them in a special way.\nFirst we explain why we should not directly use the algorithm and the theoretical bound proposed in Vovk (2001). Vovk\u2019s experts do not allow us to take advantage of the fact that only one outcome is possible to happen at each moment. They are more suitable for the case when each input vector x can belong to many classes simultaneously in case of classification. In other words, they are centered around the center 1/2 of the prediction interval [0, 1]: \u03bei = 1/2 + \u03b1ix. Assume that the number of outcomes is very large and the distribution on experts is normal N(0, \u03c32) with small \u03c3. Then the average experts\u2019 prediction is (1/2, . . . , 1/2, 1\u2212(d\u22121)/2)), and the average loss of the experts on trials with the same outcome y = y{i} (we can take y = (1, 0, . . . , 0)) is (d\u22121)/22+(d\u22121)2/22.\nAlgorithm 1 mAAR for the Brier game\nFix n, a > 0. C = 0, h = 0. for t = 1, 2, . . . do Read new xt \u2208 X.\nC = C + xtx \u2032 t, A = aI +   2C \u00b7 \u00b7 \u00b7 C ... . . . ...\nC \u00b7 \u00b7 \u00b7 2C\n \nSet bi = h + (x \u2032 t, . . . , x \u2032 t, 0, x \u2032 t, . . . , x \u2032 t) \u2032, where 0 is a zero-vector from Rn is placed at i-th position, i = 1, . . . , d\u2212 1. Set zi = (\u2212x\u2032t, . . . ,\u2212x\u2032t,\u22122x\u2032t,\u2212x\u2032t, . . . ,\u2212x\u2032t)\u2032, where \u22122x\u2032t is placed at i-th position, i = 1, . . . , d\u2212 1. Calculate ri := \u2212b\u2032iA\u22121zi, rd := 0, i = 1, . . . , d\u2212 1. Solve \u2211d i=1(s\u2212 ri)+ = 2 in s \u2208 R. Set \u03b3it := (s\u2212 ri)+/2, \u03c9 \u2208 \u2126, i = 1, . . . , d. Output prediction \u03b3t \u2208 P(\u2126). Read observation yt. hi = hi \u2212 2(yit \u2212 ydt )xt, h = (h\u20321, . . . , h\u2032d\u22121)\u2032.\nend for\nComponents of experts (1) concentrate around the point 1/d, and so experts have the average loss (d\u2212 1)/d2 + (1\u2212 1/d)2. This loss is smaller than the loss of Vovk\u2019s experts for large values of d.\nOur component-wise experts are expressed by\n\u03beit = 1/d+ \u03b1 \u2032 ixt, i = 1, . . . , d. (5)\nThe derivation of the component-wise algorithm (further cAAR stands for componentwise Aggregating Algorithm Regression) is similar to the derivation of Algorithm 1 for two outcomes. The initial distribution on each component of experts (5) is given by\n(a\u03b7\u0303/\u03c0)n/2e\u2212a\u03b7\u0303\u2016\u03b1i\u2016 2\nd\u03b1i.\nNote that the value for \u03b7\u0303 here will be different from 1 since the loss function by each component is half of the Brier loss \u03bb(y, \u03b3) = (y \u2212 \u03b3)2 + (1\u2212 y \u2212 (1\u2212 \u03b3))2. We will further see that \u03b7\u0303 = 2. The loss of expert \u03be(\u03b1i) over the first T trials is\nT\u2211\nt=1\n(yit\u22121/d\u2212\u03b1\u2032ixt)2 = \u03b1\u2032i\n( T\u2211\nt=1\nxtx \u2032 t ) \u03b1i\u22122\u03b1\u2032i ( T\u2211\nt=1\n(yit \u2212 1/d)xt ) + T\u2211\nt=1\n(yit\u22121/d)2.\nInstead of the substitution function from Proposition 1 we use the substitution function suggested in Vovk (2001) for the one-dimensional game:\n\u03b3iT = 1\n2 + gT (0)\u2212 gT (1) 2\nTherefore, the substitution function can be represented as\n\u03b3iT = 1\n2 +\n1 2 log\u03b2\u0303\n\u03b2\u0303gT (0)\n\u03b2\u0303gT (1)\n= 1\n2 +\n1 2 log\u03b2\u0303\n\u222b Rn e\u2212\u03b7\u0303\u03b1 \u2032 iB\u03b1i+2\u03b7\u0303\u03b1 \u2032 i(E+(0\u22121/d)xT )\u2212\u03b7\u0303(W+1/d\n2)d\u03b1i\u222b Rn e\u2212\u03b7\u0303\u03b1 \u2032 i B\u03b1i+2\u03b7\u0303\u03b1\u2032i(E+(1\u22121/d)xT )\u2212\u03b7\u0303(W+(1\u22121/d) 2)d\u03b1i\n= 1\nd +\n1 2 F\n( B,\u22122E \u2212 d\u2212 2\nd xT , xT\n)\n= 1\nd +\n( T\u22121\u2211\nt=1\n(yit \u2212 1/d)x\u2032t + d\u2212 2 2d x\u2032T\n)( aI + T\u2211\nt=1\nxtx \u2032 t )\u22121 xT (6)\nfor i = 1, . . . , d. Here B = aI + \u2211T\nt=1 xtx \u2032 t, E = \u2211T\u22121 t=1 (y\ni t \u2212 1/d)xt, W =\u2211T\u22121\nt=1 (y i t \u2212 1/d)2, \u03b2\u0303 = e\u2212\u03b7\u0303. The transitions are justified using Lemma 4 and Lemma 5. Then this method projects its prediction onto the prediction simplex such that the loss does not increase. We use the projection algorithm suggested in Michelot (1986).\nAlgorithm 2 Projection of a point from Rn onto probability simplex.\nInitialize I = \u2205, x = 1 \u2208 Rd. Let \u03b3T be the prediction vector and |I| is the dimension of the set I. while 1 do \u03b3T = \u03b3T \u2212 \u2211d i=1 \u03b3iT\u22121\nd\u2212|I| ;\n\u03b3iT = 0, \u2200i \u2208 I; If \u03b3iT \u2265 0 for all i = 1, . . . , d then break; I = I\n\u22c3{i : \u03b3iT < 0}; If \u03b3iT < 0 for some i then \u03b3 i T = 0;\nend while"}, {"heading": "4 Theoretical bound", "text": "We derive the theoretical bounds for the losses of Algorithm 1 and of a naive component-wise algorithm predicting in the same framework."}, {"heading": "4.1 Component-wise algorithm", "text": "We prove here the theoretical bound for the loss of cAAR. The following lemma is the main tool helping us to prove our theorems. It is easy to prove the following statement (Lemma 1 from Vovk (2001)):\nLemma 6. If the learner follows the Aggregating Algorithm in a perfectly mixable game, then for every positive integer T , every sequence of outcomes of the\nlength T , and any initial weights distribution on experts P0(d\u03b1) it suffers loss satisfying for any \u03b1 \u2208 \u0398\nLT (AA(\u03b7, P0)) \u2264 log\u03b2 \u222b\n\u0398\n\u03b2LT (\u03b1)P0(d\u03b1). (7)\nProof. We proceed by induction in T : for T = 0 the inequality is obvious, and for T > 0 we have:\nLT (AA(\u03b7, P0)) \u2264 LT\u22121(AA(\u03b7, P0)) + gT (\u03c9T )\n= log\u03b2\n\u222b\n\u0398\n\u03b2L \u03b8 T\u22121P0(d\u03b8) + log\u03b2\n\u222b\n\u0398\n\u03b2\u03bb(\u03c9T ,\u03be \u03b8 t )\n\u03b2L \u03b8 T\u22121\n\u222b \u0398 \u03b2L \u03b8 T\u22121P0(d\u03b8) P0(d\u03b8)\n= log\u03b2\n\u222b\n\u0398\n\u03b2L \u03b8 TP0(d\u03b8) .\nHere the second equality follows from the inductive assumption, the definition (3) of gT , and (2).\nThe loss of the component-wise algorithm by one component is bounded as in the following theorem.\nTheorem 1. Let the outcome space in the prediction game be [A,B], A,B \u2208 R. Assume experts\u2019 predictions at each step are \u03bet = C + \u03b1\n\u2032xt, where \u03b1 \u2208 Rn, C \u2208 R is the same for all the experts \u03b1, and \u2016xt\u2016\u221e \u2264 X, \u2200t. There exists a prediction algorithm producing \u03b3i \u2208 R, i = 1, . . . , d such that for any a > 0, every positive integer T , every sequence of input vectors and outcomes of the length T and any \u03b1 \u2208 Rn we have\nT\u2211\nt=1\n(\u03b3t \u2212 yt)2 \u2264 T\u2211\nt=1\n(\u03bet \u2212 yt)2 + a\u2016\u03b1\u201622 + n(B \u2212A)2\n4 ln\n( TX2\na + 1\n) . (8)\nProof. We need to prove that the game is perfectly mixable (see (4)) and find the optimal parameter \u03b7 for the algorithm. Implications similar to the ones in the proof of Lemma 2 from Vovk (2001) lead to the inequality \u03b7 \u2264 2(B\u2212A)2 . Clearly, Lemma 6 holds for our case, so we need only to calculate the difference between the right-hand side of (7)\nlog\u03b2\n\u222b\nRn\nd\u03b1(a\u03b7/\u03c0)n/2 exp [ \u2212\u03b7\u03b1\u2032 ( aI + T\u2211\nt=1\nxtx \u2032 t\n) \u03b1\n+ \u03b7 2\u03b1\u2032\n( T\u2211\nt=1\n(yt \u2212 C)xt ) \u2212 \u03b7 T\u2211\nt=1\n(yt \u2212 C)2 ] .\nand the loss of the best expert \u03b1\u20320\n( aI + \u2211T t=1 xtx \u2032 t ) \u03b10\u22122\u03b1\u20320 (\u2211T t=1(yt \u2212 C)xt ) +\n\u2211T t=1(yt \u2212C)2. Here \u03b10 is the point where the minimum of the quadratic form\nis attained. Then due to Lemma 4 this difference will be equal to\n1\n2\u03b7 ln det\n( I + 1\na\nT\u2211\nt=1\nxtx \u2032 t\n) \u2264 n(B \u2212A) 2\n4 ln\n( TX2\na + 1\n) .\nWe bound the determinant of a symmetric positive definite matrix by the product of its diagonal elements (see Beckenbach and Bellman (1961), Chapter 2, Theorem 7) and use \u03b7 = 2(B\u2212A)2 .\nInterestingly, the theoretical bound for the regression algorithm depends only on the size of the prediction interval but not on the location of it. It also does not depend on the concentration point of experts. We use the component-wise algorithm to predict each component separately.\nTheorem 2. If \u2016xt\u2016\u221e \u2264 X, \u2200t, then for any a > 0, every positive integer T , every sequence of outcomes of the length T , and any \u03b1 \u2208 Rn(d\u22121) the loss LT of the component-wise algorithm satisfies\nLT \u2264 LT (\u03b1) + da\u2016\u03b1\u201622 + nd\n4 ln\n( TX2\na + 1\n) . (9)\nProof. We extend the class of experts in (1) in (5). The algorithm predicts each component of the outcome separately. Summing theoretical bounds (8) for d components of the outcome, taking \u03b1d = \u2212 \u2211d\u22121 i=1 \u03b1 \u2032 i, and using the Cauchy inequality \u2016\u2211d\u22121i=1 \u03b1i\u201622 \u2264 (d\u2212 1) \u2211d\u22121\ni=1 \u2016\u03b1i\u201622 we get the bound. To give probability forecasts we can project prediction points on the prediction simplex using Algorithm 2. The bound will then hold by Lemma 1."}, {"heading": "4.2 Linear forecasting", "text": "The theoretical bound for the loss of the Algorithm 1 is\nTheorem 3. If \u2016xt\u2016\u221e \u2264 X, \u2200t, then for any a > 0, every positive integer T , every sequence of outcomes of the length T , and any \u03b1 \u2208 Rn(d\u22121) mAAR(2a) satisfies\nLT (mAAR(2a)) \u2264 LT (\u03b1) + 2a\u2016\u03b1\u201622 + n(d\u2212 1)\n2 ln\n( TX2\na + 1\n) . (10)\nProof. We apply mAAR with the parameter b = 2a. Recall that C = \u2211T\nt=1 xtx \u2032 t.\nFollowing the line of the proof of Theorem 1 with \u03b7 = 1 we get the theoretical bound.\nWe can derive a slightly better theoretical bound: in the determinant of A one should subtract the second block raw from the first one and then add the first block column to the second one, then repeat this d\u2212 2 times.\nProposition 2. In the conditions of Theorem 3 mAAR(a) satisfies\nLT (mAAR(a)) \u2264 LT (\u03b1) + a\u2016\u03b1\u201622\n+ n(d\u2212 2)\n2 ln\n( TX2\na + 1\n) + n\n2 ln\n( TX2d\na + 1\n) . (11)\nThe theoretical bound (10) is worse asymptotically by d than the bound (9) of the component-wise algorithm, but it is better in the beginning, especially when the norm of the best expert \u2016\u03b1\u2016 is large. This can happen in the important case when the dimension of the input vector is larger than the size of the prediction set: n >> T ."}, {"heading": "5 Kernelization", "text": "In some cases the linear model can be considered not rich enough to describe data well, and a more complicated model is needed. We use a popular in computer learning kernel trick, firstly applied to the AAR in Gammerman et al. (2004). We derive an algorithm competing with all sets of functions from an RKHS with d\u2212 1 elements."}, {"heading": "5.1 Derivation of the algorithm", "text": "Definition 1. Let us take x1, . . . , xn \u2208 X. A kernel function is a nonnegative function K : Rn \u00d7 Rn \u2192 R satisfying \u2211ni,j=1 K(xi, xj)\u03bei\u03bej \u2265 0 for all positive integers n, all x1, . . . , xn \u2208 X, and \u03be1, . . . , \u03ben \u2208 R.\nAn RKHS contains all linear regressors \u3008\u03a6(\u00b7), h\u3009H defined by means of a feature map (for all the definitions see Scho\u0308lkopf and Smola, 2002). It can also be defined in a different equivalent way as a functional Hilbert space with continuous evaluation functional \u03d5 : f \u2208 F 7\u2192 f(x) for each x \u2208 X. We will use the notation cF (x) for the norm of this functional: cF(x) := supf :\u2016f\u2016F\u22641 |f(x)| and for the embedding constant cF := supx\u2208X cF(x) and assume cF < \u221e.\nOur algorithm competes with the following experts:\n\u03be1t = 1/d+ f1(xt)\n. . .\n\u03bed\u22121t = 1/d+ fd\u22121(xt) (12)\n\u03bedt = 1\u2212 \u03be1 \u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03bed\u22121. Here f1, . . . , fd\u22121 \u2208 F are any functions from some RKHS F . We start by rewriting mAAR in the dual form. Denote\nY\u0303i = \u22122(yi1 \u2212 yd1 , . . . , yiT\u22121 \u2212 ydT\u22121,\u22121/2), Y i = \u22122(yi1 \u2212 yd1 , . . . , yiT\u22121 \u2212 ydT\u22121, 0)\nk\u0303(xT ) = (x \u2032 1xT , . . . , x \u2032 TxT ) \u2032,\nK\u0303 = (x\u2032s, xt)s,t is the matrix of scalar products\nfor i = 1, . . . , d\u2212 1, s, t = 1, . . . , T . We show that the predictions of mAAR can be represented in terms of variables defined above. We will need the following matrix property.\nProposition 3. Let B,C be matrices such that the number of rows in B equals to the number of columns in C, and identity matrices I. If aI+CB and aI+BC are nonsingular then\nB(aI + CB)\u22121 = (aI +BC)\u22121B. (13)\nProof. This is equivalent to (aI + BC)B = B(aI + CB). That is true because of distributivity of matrix multiplication.\nLet us set A =  aI +   2K\u0303 \u00b7 \u00b7 \u00b7 K\u0303 ... . . . ...\nK\u0303 \u00b7 \u00b7 \u00b7 2K\u0303\n   .\nLemma 7. On trial T values ri for i = 1, . . . , d\u22121 in mAAR can be represented as\nri = ( Y\u03031 \u00b7 \u00b7 \u00b7 Y i \u00b7 \u00b7 \u00b7 Y\u0303d\u22121 )\n\u00b7A\u22121 ( k\u0303(xT ) \u2032 \u00b7 \u00b7 \u00b7 2k\u0303(xT )\u2032 \u00b7 \u00b7 \u00b7 k\u0303(xT )\u2032 )\u2032 . (14)\nProof. By M = (x1, . . . , xT ) denote a matrix n \u00d7 T of column input vectors. Let us set\nB =   2M \u00b7 \u00b7 \u00b7 M ... . . . ...\nM \u00b7 \u00b7 \u00b7 2M\n  , C =   M \u2032 \u00b7 \u00b7 \u00b7 0 ... . . . ...\n0 \u00b7 \u00b7 \u00b7 M \u2032\n  .\nThen hi from the algorithm mAAR equals hi = MY i \u2208 Rn. Decompose b\u2032i =( Y\u03031 \u00b7 \u00b7 \u00b7 Y i \u00b7 \u00b7 \u00b7 Y\u0303d\u22121 ) C, where only the i-th block uses Y i. The matrix A is equal A = aI +BC. Using proposition 3\nri = \u2212b\u2032iA\u22121zi = \u2212 ( Y\u03031 \u00b7 \u00b7 \u00b7 Y i \u00b7 \u00b7 \u00b7 Y\u0303d\u22121 )\n\u00b7 (aI + CB)\u22121C ( \u2212x\u2032T \u00b7 \u00b7 \u00b7 \u22122x\u2032T \u00b7 \u00b7 \u00b7 \u2212 x\u2032T )\u2032 .\nNote that K\u0303 = M \u2032M and k\u0303(xT ) = M \u2032xT , thus (14) holds.\nIf instead of dot product in K\u0303, k\u0303(xT ) we can choose a different kernel (clas-\nsical examples of kernels are Gaussian (RBF): K(xi, xj) = e \u2212\n\u2016xi\u2212xj\u2016 2\n2\u03c32 , Vapnik\u2019s polynomial K(xi, xj) = (xi \u00b7 xj + 1)d, etc.). To get predictions one can use the same substitution function from Proposition 1. We call this algorithm mKAAR (K for Kernelized)."}, {"heading": "5.2 Theoretical bound for the kernelized algorithm", "text": "To derive a theoretical bound for the loss of mKAAR we will use the following matrix determinant identity lemma.\nLemma 8 (Matrix determinant identity). Let B,C are as in Proposition 3, and a is a real number. Then det(aI +BC) = det(aI + CB).\nProof. The proof is by considering a block matrix identity.\nThe main theorem follows from the property of RKHS called Representer theorem (see Scho\u0308lkopf and Smola, 2002, Theorem 4.2).\nTheorem 4 (Representer theorem). Denote by g : [0,\u221e) \u2192 R a strictly monotonic increasing function. Assume X is an arbitrary set, and F is a Reproducing Kernel Hilbert Space of functions on X with the given kernel K : X2 \u2192 R. Assume we also have a positive integer T and an arbitrary loss function c : (X\u00d7 R2)T \u2192 R\u22c3{\u221e}. Then each minimizer f \u2208 F of\nc ((x1, y1, f(x1)), . . . , (xT , yT , f(xT ))) + g(\u2016f\u2016F)\nadmits a representation of the form f(x) = \u2211T\ni=1 \u03b1iK(xi, x) for any x \u2208 X and reals \u03b1i, i = 1, . . . , T .\nThe theoretical bound for the loss of mKAAR is proven in the following theorem.\nTheorem 5. Assume X is an arbitrary set of inputs and F is a Reproducing Kernel Hilbert Space of functions on X with the given kernel K : X2 \u2192 R. Then for any a > 0, any f1, . . . , fd\u22121 \u2208 F , any positive integer T , and any sequence of inputs and outputs (x1, y1), . . . , (xT , yT )\nLT (mKAAR) \u2264 LT (f) + a d\u22121\u2211\ni=1\n\u2016fi\u20162F + 1\n2 ln detA (15)\nHere the matrix K\u0303 is a matrix of kernel values K(xi, xj), i, j = 1, . . . , T .\nProof. The bound follows from Theorem 3 for mAAR and the Representer theorem. Let us first consider the case with scalar product kernel. Denote C = \u2211T t=1 xtx \u2032 t. By Lemma 8 and calculations similar to ones in the proof of Lemma 7 we have the equality of determinants. So we can use any other kernel instead of scalar product to get the term with the determinant. The Representer theorem assures that the minimum of the expression LT (f) + a \u2211d\u22121 i=1 \u2016fi\u20162F by f -s is reached on a linear regressor.\nWe can represent the bound (15) in another form which is more familiar from the on-line prediction literature:\nCorollary 1. Under assumptions of Theorem 5 and if we know the number of steps T in advance and are given F > 0, the mKAAR reaches the performance\nLT (mKAAR) \u2264 LT (f) + 2cFF \u221a (d\u2212 1)T , (16)\nfor any f1, . . . , fd\u22121 \u2208 F : \u2211d\u22121\ni=1 \u2016fi\u20162F \u2264 F . Proof. Bounding the logarithm of the determinant we have ln detA \u2264 (d \u2212 1)T ln ( 1 +\n2c2F a\n) .We can choose the value for a where the minimum is achieved:\na = cF \u221a (d\u22121)T\nF ."}, {"heading": "6 Experiments", "text": "We run our algorithms on six real world time-series data sets. In the time series we consider there are no signals attached to the outcomes. However we can take vectors consisting of previous observations (we shall take ten of those) and use them as signals. Data set DEC-PKT1 contains an hours worth of all wide-area traffic between Digital Equipment Corporation and the rest of the world. Data set LBL-PKT-41 consists of observations of another hour of traffic between the Lawrence Berkeley Laboratory and the rest of the world. We transformed both the data sets in such a way that each observation is the number of packets in the corresponding network during a fixed time interval of one second. The other four datasets2 (C4,C9,E5,E8) relate to transportation data. Two of them (C9,C11) contain low-frequency monthly traffic measures. Two of them (E5,E8) contain high-frequency day traffic measures. On each of these data sets the following operations were performed: subtraction of the mean value and division by the maximum absolute value. The resulting time series are shown in Figure 1.\nWe used ten previous observations as an input vector for tested algorithms at each prediction step. We are solving the 3-class classification problem: we predict whether the next value in a time series will be more than the previous value plus a precision parameter \u01eb, less than that value, or lies in the 2\u01eb tube around the previous value. The precision \u01eb is chosen to be the median of all the changes in a data set. In order to assess the quality of predictions, we calculate the cumulative square loss at the last two thirds of each time series (test set) and divide it by the number of examples (MSE). Since we are considering the online setting, we could calculate the cumulative loss from the beginning of each time series. However our approach is not sensitive to starting effects, it allows us to choose the ridge parameter a fairly on the training set, and it allows us to compare the performance of our algorithms with batch algorithms, which would be normally used to solve this problem.\nThe square loss on the test set takes into account the quality of an algorithm only at the very end of the prediction process, and does not consider the quality during the process. We introduce another quality measure: at each step in the\n1Data sets can be found http://ita.ee.lbl.gov/html/traces.html. 2Data sets can be found http://www.neural-forecasting-competition.com/index.htm.\ntest set we calculate MSE of an algorithm until this step. After all the steps we average these MSEs (AMSE). Clearly, if one algorithm is better than another on the whole test set (its total MSE is smaller) but was often worse on many parts of the test set (total MSEs of many parts of the set is larger), this measure takes it into account.\nWe compare the performance of our algorithms with the multinomial logistic regression (mLog), because it is a standard classification algorithm which gives probability predictions:\n\u03b3imLog = e\u03b8\nix\n\u2211d i=1 e \u03b8ix\nfor all the components of the outcome i = 1, . . . , d. In our case d = 3. Here parameters \u03b81, . . . , \u03b8d are estimated from the training set. We apply this algorithm in two regimes: batch regime, where the algorithm learns only on the training set and is tested on the test set (and thus \u03b8 is not updated on the test set); and in the online regime, where at each step new parameters \u03b8 are found, and only one next outcome is predicted. The second regime is more fair to compare with online algorithms, but the first one is standard and faster. In both regimes logistic regression does not have theoretical guarantees on the square loss.\nWe also compare our algorithms with the simple predictor predicting the average of the ten previous outcomes (and thus it always gives probability predictions).\nWe are not aware of other efficient algorithms for online probability prediction, and thus logistic regression and simple predictor as the only baselines. Component-wise algorithms which could be used for online prediction (e.g., Gradient Descent, Kivinen and Warmuth 1997, Ridge Regression, Hoerl and Kennard 2000), have to use normalization by Algorithm 2. Thus they have to be applied in a different way than they are described in the corresponding papers, and can not be fairly compared with our algorithms.\nThe ridge for our algorithms is chosen to achieve the best MSE on the training set: the first third of each series. The results are shown in Table 1. We highlight the most precise algorithms for different data sets. We also show time needed to make predictions on the whole data set. The algorithms were implemented in Matlab R2007b and run on the laptop with 2Gb RAM and processor Intel Core 2, T7200, 2.00GHz.\nAs we can see from the table, online methods perform better than the batch method. Online logistic regression performs well, but is very slow. Our algorithms perform similar to each other and comparable to the online logistic regression, but are much faster."}, {"heading": "7 Discussion", "text": "We consider an important generalization of the online classification problem. We presented new algorithms which give probability predictions in the Brier game. Both algorithms do not involve any numerical integration, and can be easily computed. Both algorithms have theoretical guarantees on their cumulative losses. One of the algorithms is kernelized and a theoretical bound is proven for the kernelized algorithm. We performed experiments with linear algorithms and showed that they perform relatively well. We compared them with the logistic regression: the benchmark algorithm giving probability predictions.\nCompeting with linear experts in the case where possible outcomes lie in a more than 2-dimensional simplex was not widely considered by other researchers, so the comparison of theoretical bounds can not be performed. Kivinen and Warmuth\u2019s work Kivinen and Warmuth (2001) includes the case when the possible outcomes lie in a more than 2-dimensional simplex and their algorithm competes with all logistic regression functions. They use the relative entropy loss function L and get a regret term of the order O( \u221a LT (\u03b1)) which is upper unbounded in the worst case. Their prediction algorithm is not computationally efficient and it is not clear how to extend their results for the case when the predictors lie in an RKHS.\nWe can prove lower bounds for the regret term of the order O(d\u22121d lnT ) for the case of the linear model (1) using methods similar to ones described in Vovk (2001), and lower bounds for the regret term of the order O( \u221a T ) for the case of RKHS. Thus we can say that the order of our bounds by time step is optimal. Multiplicative constants may possibly be improved though."}, {"heading": "Acknowledgments", "text": "Authors are grateful for useful comments and discussions to Alexey Chernov, Vladimir Vovk, and Alex Gammerman. This work has been supported by EPSRC grant EP/F002998/1 and ASPIDA grant from the Cyprus Research Promotion Foundation."}], "references": [{"title": "Verification of forecasts expressed in terms of probability", "author": ["Glenn W. Brier"], "venue": "Monthly Weather Review,", "citeRegEx": "Brier.,? \\Q1950\\E", "shortCiteRegEx": "Brier.", "year": 1950}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "J. Comput. System Sci.,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "On-line prediction with kernels and the complexity approximation principle", "author": ["Alexander Gammerman", "Yuri Kalnishkan", "Vladimir Vovk"], "venue": "In UAI,", "citeRegEx": "Gammerman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gammerman et al\\.", "year": 2004}, {"title": "Matrix algebra from a statistician\u2019s perspective", "author": ["David A. Harville"], "venue": null, "citeRegEx": "Harville.,? \\Q1997\\E", "shortCiteRegEx": "Harville.", "year": 1997}, {"title": "Sequential prediction of individual sequences under general loss functions", "author": ["David Haussler", "Jyrki Kivinen", "Manfred K. Warmuth"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Haussler et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1998}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["Arthur E. Hoerl", "Robert W. Kennard"], "venue": null, "citeRegEx": "Hoerl and Kennard.,? \\Q2000\\E", "shortCiteRegEx": "Hoerl and Kennard.", "year": 2000}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["Jyrki Kivinen", "Manfred K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1997}, {"title": "Averaging expert predictions", "author": ["Jyrki Kivinen", "Manfred K. Warmuth"], "venue": "In Computational learning theory (Nordkirchen,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1999\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1999}, {"title": "Relative loss bounds for multidimensional regression problems", "author": ["Jyrki Kivinen", "Manfred K. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Kivinen and Warmuth.,? \\Q2001\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 2001}, {"title": "A finite algorithm for finding the projection of a point onto the canonical simplex of rn", "author": ["C Michelot"], "venue": "J. Optim. Theory Appl.,", "citeRegEx": "Michelot.,? \\Q1986\\E", "shortCiteRegEx": "Michelot.", "year": 1986}, {"title": "Learning with kernels: Support Vector Machines, regularization, optimization, and beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Aggregating strategies", "author": ["Vladimir Vovk"], "venue": "In Proceedings of the Third Annual Workshop on Computational Learning Theory,", "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "A game of prediction with expert advice", "author": ["Vladimir Vovk"], "venue": "J. Comput. System Sci.,", "citeRegEx": "Vovk.,? \\Q1998\\E", "shortCiteRegEx": "Vovk.", "year": 1998}, {"title": "Competitive on-line statistics", "author": ["Vladimir Vovk"], "venue": "International Statistical Review,", "citeRegEx": "Vovk.,? \\Q2001\\E", "shortCiteRegEx": "Vovk.", "year": 2001}, {"title": "Prediction with expert advice for the Brier game", "author": ["Vladimir Vovk", "Fedor Zhdanov"], "venue": "Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Vovk and Zhdanov.,? \\Q2008\\E", "shortCiteRegEx": "Vovk and Zhdanov.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "We consider the square loss: mean square error is one of the benchmark measures for classification algorithms (see Brier, 1950). We use Vovk\u2019s Aggregating Algorithm (a generalization of the Bayesian mixture) to mix functions (as in Aggregating Algorithm Regression, AAR: see Vovk, 2001). This method has previously been applied to the case when possible outcomes lie in a segment of the real line, and so the prediction was one-dimensional. We develop two algorithms to solve the problem of multi-dimensional prediction. The first algorithm applies a variant of AAR to predict each coordinate of the outcome separately, and then combines these predictions in a certain way to get probability prediction. The other algorithm is designed to give probability predictions directly; these are first computationally efficient online regression algorithm designed to solve linear and non-linear multi-class classification problems. We derive theoretical bounds on the losses of both algorithms. We come to an unexpected conclusion that the component-wise algorithm is better than the second one asymptotically, but worse in the beginning of the prediction process. Their performance on benchmark data sets is very similar. One component of the prediction of the second algorithm has the meaning of a remainder. In practice this situation is quite common. For example, in a football match either one team wins or the other, and the remainder is a draw (see Vovk and Zhdanov (2008) for online prediction experiments in football).", "startOffset": 115, "endOffset": 1473}, {"referenceID": 0, "context": "We are interested in the generalisation of the Brier game from Brier (1950) where the space of outcomes \u03a9 = P(\u03a3) is the set of all probability measures on a finite set \u03a3 with d elements, \u0393 := {(\u03b31, .", "startOffset": 47, "endOffset": 76}, {"referenceID": 12, "context": "In this section we describe how we apply the Aggregating Algorithm (AA) proposed in Vovk (1990) to mix experts and make predictions.", "startOffset": 84, "endOffset": 96}, {"referenceID": 7, "context": "It is suggested in Kivinen and Warmuth (1999) that the prediction is simply the weighted average of the experts\u2019 predictions with weights Pt(d\u03b1).", "startOffset": 19, "endOffset": 46}, {"referenceID": 12, "context": "Perfectly mixable games and other types of games are analyzed in Vovk (1998). It is also shown there that for countable (and thus finite) number of experts the AA achieves the best possible theoretical guarantees.", "startOffset": 65, "endOffset": 77}, {"referenceID": 11, "context": "It is shown in Theorem 1 Vovk and Zhdanov (2008) that the Brier game with finite number of outcomes is perfectly mixable iff \u03b7 \u2208 (0, 1].", "startOffset": 25, "endOffset": 49}, {"referenceID": 12, "context": "By Theorem 1 in Vovk and Zhdanov (2008) such prediction exists for any \u03b7 \u2208 (0, 1] (\u03b2 \u2208 [e, 1)).", "startOffset": 16, "endOffset": 40}, {"referenceID": 11, "context": "Such a function is proposed in Vovk and Zhdanov (2008). This is an extension of Lemma 4.", "startOffset": 31, "endOffset": 55}, {"referenceID": 5, "context": "1 from Haussler et al. (1998). Lemma 3.", "startOffset": 7, "endOffset": 30}, {"referenceID": 12, "context": "First we explain why we should not directly use the algorithm and the theoretical bound proposed in Vovk (2001). Vovk\u2019s experts do not allow us to take advantage of the fact that only one outcome is possible to happen at each moment.", "startOffset": 100, "endOffset": 112}, {"referenceID": 12, "context": "Instead of the substitution function from Proposition 1 we use the substitution function suggested in Vovk (2001) for the one-dimensional game:", "startOffset": 102, "endOffset": 114}, {"referenceID": 10, "context": "We use the projection algorithm suggested in Michelot (1986).", "startOffset": 45, "endOffset": 61}, {"referenceID": 12, "context": "It is easy to prove the following statement (Lemma 1 from Vovk (2001)):", "startOffset": 58, "endOffset": 70}, {"referenceID": 12, "context": "Implications similar to the ones in the proof of Lemma 2 from Vovk (2001) lead to the inequality \u03b7 \u2264 2 (B\u2212A) .", "startOffset": 62, "endOffset": 74}, {"referenceID": 3, "context": "We use a popular in computer learning kernel trick, firstly applied to the AAR in Gammerman et al. (2004). We derive an algorithm competing with all sets of functions from an RKHS with d\u2212 1 elements.", "startOffset": 82, "endOffset": 106}, {"referenceID": 0, "context": "We presented new algorithms which give probability predictions in the Brier game. Both algorithms do not involve any numerical integration, and can be easily computed. Both algorithms have theoretical guarantees on their cumulative losses. One of the algorithms is kernelized and a theoretical bound is proven for the kernelized algorithm. We performed experiments with linear algorithms and showed that they perform relatively well. We compared them with the logistic regression: the benchmark algorithm giving probability predictions. Competing with linear experts in the case where possible outcomes lie in a more than 2-dimensional simplex was not widely considered by other researchers, so the comparison of theoretical bounds can not be performed. Kivinen and Warmuth\u2019s work Kivinen and Warmuth (2001) includes the case when the possible outcomes lie in a more than 2-dimensional simplex and their algorithm competes with all logistic regression functions.", "startOffset": 70, "endOffset": 808}, {"referenceID": 0, "context": "We presented new algorithms which give probability predictions in the Brier game. Both algorithms do not involve any numerical integration, and can be easily computed. Both algorithms have theoretical guarantees on their cumulative losses. One of the algorithms is kernelized and a theoretical bound is proven for the kernelized algorithm. We performed experiments with linear algorithms and showed that they perform relatively well. We compared them with the logistic regression: the benchmark algorithm giving probability predictions. Competing with linear experts in the case where possible outcomes lie in a more than 2-dimensional simplex was not widely considered by other researchers, so the comparison of theoretical bounds can not be performed. Kivinen and Warmuth\u2019s work Kivinen and Warmuth (2001) includes the case when the possible outcomes lie in a more than 2-dimensional simplex and their algorithm competes with all logistic regression functions. They use the relative entropy loss function L and get a regret term of the order O( \u221a LT (\u03b1)) which is upper unbounded in the worst case. Their prediction algorithm is not computationally efficient and it is not clear how to extend their results for the case when the predictors lie in an RKHS. We can prove lower bounds for the regret term of the order O( d lnT ) for the case of the linear model (1) using methods similar to ones described in Vovk (2001), and lower bounds for the regret term of the order O( \u221a T ) for the case of RKHS.", "startOffset": 70, "endOffset": 1420}], "year": 2010, "abstractText": "Multi-class classification is one of the most important tasks in machine learning. In this paper we consider two online multi-class classification problems: classification by a linear model and by a kernelized model. The quality of predictions is measured by the Brier loss function. We suggest two computationally efficient algorithms to work with these problems and prove theoretical guarantees on their losses. We kernelize one of the algorithms and prove theoretical guarantees on its loss. We perform experiments and compare our algorithms with logistic regression.", "creator": "LaTeX with hyperref package"}}}