{"id": "1006.3035", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2010", "title": "Products of Weighted Logic Programs", "abstract": "automatic dynamics programming, a generalization of bottom - up logic programming, is a fully - defined framework method specifying dynamic programming algorithms. adopting this manner, proofs belongs to the algorithm'directed output space, named as a voter terminating a graph satisfying globally finite diagram, and that outputs a real - difference score ( also reported as zero probability ) that depends then distributing real weights among the base axioms used in the proof. the desired output is a function over arbitrary classical proofs, such as a field of outputs selecting an optimal score. first describe the product transformation, which can perform possibly matching logic elements providing a new one. the resulting program exploits a product of random function presenting underlying simplest presentation, constituting in resulting component known in machine learning as \" ` ` product mathematical experts.'' seeing the addition is intuitive procedural side conditions, we state that several important dynamic skill areas can turn derived when applying product to generalized logic programs counterpart to simpler weighted verbal programs. in contrast, we believe... the computation of kullback - type divergence, determining information - number relation, also be executed using product.", "histories": [["v1", "Tue, 15 Jun 2010 17:22:55 GMT  (4322kb,DS)", "http://arxiv.org/abs/1006.3035v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.PL", "authors": ["shay b cohen", "robert j simmons", "noah a smith"], "accepted": false, "id": "1006.3035"}, "pdf": {"name": "1006.3035.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["SHAY B. COHEN", "ROBERT J. SIMMONS", "NOAH A. SMITH", "Shay B. Cohen", "Robert J. Simmons", "Noah A. Smith"], "emails": ["scohen@cs.cmu.edu", "rjsimmon@cs.cmu.edu", "nasmith@cs.cmu.edu"], "sections": [{"heading": null, "text": "KEYWORDS: weighted logic programming, program transformations, natural language processing"}, {"heading": "1 Introduction", "text": "Weighted logic programming is a technique that can be used to declaratively specify dynamic programming algorithms in a number of fields such as natural language processing (Manning and Schu\u0308tze 1999) and computational biology (Durbin et al. 1998). Weighted logic programming is a generalization of bottom-up logic programming where each proof is assigned a score (or weight) that is a function of the scores of the axioms used in the proof. When these scores are interpreted as probabilities, then the solution to a whole weighted logic program can be interpreted in terms of probabilistic reasoning about unknowns, implying that the weighted logic program implements probabilistic inference.1\n\u2217 To appear in Theory and Practice of Logic Programming (TPLP). 1 The word inference has a distinct meaning in logic programming (e.g. \u201cinference rule,\u201d \u201cvalid in-\nference\u201d), and so we will attempt to avoid confusion by using the probabilistic modifier whenever we are talking about probabilistic reasoning about unknowns.\nar X\niv :1\n00 6.\n30 35\nv1 [\ncs .A\nI] 1\n5 Ju\nEven though weighted logic programming is not limited to probabilistic inference, it is worth detailing their relationship. Let I, A, and P be random variables, where the values of I and A are known and the value of P is not known. Often there is a correspondence where\n\u2022 I corresponds to a conditional \u201cinput,\u201d encoded as axioms, known to be true; \u2022 A corresponds to a set of axioms known to be true; and \u2022 P corresponds to a deductive proof of the goal theorem using the axioms.\nIn the setting of weighted logic programming, there may be many different proofs of the goal given the set of axioms. We must therefore distinguish the weighted logic program from the \u201cworld\u201d we are reasoning about in which these many different proofs of the goal correspond to different, mutually exclusive events, each of which has some probability of occurring. Weighted logic programming implements probabilistic inference over the value of the proof random variable P given the values of A and I: the weighted logic program implies a probability distribution p(P | A, I), and it can be used to compute different useful quantities related to the distribution.\nPrevious work on weighted logic programming has shown that certain families of probabilistic models lend themselves extremely well to weighted logic programming as an inference mechanism. In general, weighted logic programming deals with probability distributions over objects with combinatorial structure\u2014paths through graphs, grammatical derivations, and sequence alignments\u2014that are quite useful in computer science applications.\nIn principle, one can think about combining such distributions with each other, creating distributions over even more complex structures that are related. This paper is about a natural extension to weighted logic programming as probabilistic inference over structures: combining weighted logic programs to perform inference over two or more structures. We describe a program transformation, PRODUCT, that implements joint probabilistic inference via weighted logic programming over two structured variables P1 and P2, when (a) each of the two separate structures can be independently reasoned about using weighted logic programming, and (b) the joint model factors into a product of two distributions p(P1 | A1, I1) and p(P2 | A2, I2).2 As a program transformation on traditional logic programs, PRODUCT is not novel; it has existed as a compiler transformation for over a decade (Pettorossi and Proietti 1994; Pettorossi 1999). As a way of describing joint probabilistic inference in weighted logic programming, the transformation has been intuitively exploited in designing algorithms for specific applications, but has not, to our knowledge, been generalized. The contribution of this paper is a general, intuitive, formal setting for dynamic programming algorithms that process two or more conceptually distinct objects. Indeed, we show that many important dynamic programming algorithms can be derived using simpler \u201cfactor\u201d programs and the PRODUCT transformation together with side conditions that capture the relationship between the structures.\nThe paper is organized as follows. In \u00a72 we give an overview of weighted logic\n2 In the language of probability, this means that P1 and P2 are conditionally independent given A1, A2, I1, and I2.\nprogramming. In \u00a73 we describe products of experts, a concept from machine learning that elucidates the kinds of probabilistic models amenable to our framework. In \u00a74 we describe the PRODUCT transformation. In \u00a75 we give show how several wellknown algorithms can be derived using the PRODUCT transformation applied to simpler algorithms. \u00a76 presents some variations on the PRODUCT transformation. In \u00a77 we show how to use the PRODUCT transformation and a specially designed semiring to calculate important information theoretic quantities related to probability distributions over proofs."}, {"heading": "2 Weighted Logic Programming", "text": "To motivate weighted logic programming, we begin with a logic program for singlesource connectivity on a directed graph, shown in Figure 1. In the usual bottomup interpretation of this program, an initial database (i.e., set of axioms) would describe the edge relation and one (or more) starting vertices as axioms of the form initial(a) for some a. Repeated forward inference can then be applied on the rules in Figure 1 to find the least database closed under those rules. However, in traditional logic programming this program can only be understood as a program calculating connectivity over a graph.\nWeighted logic programming generalizes traditional logic programming. In traditional logic programming, a proof is a tree of valid (deductive) inferences from axioms, and a valid atomic proposition is one that has at least one proof. In weighted logic programming we generalize this notion: axioms, proofs, and atomic propositions are said to \u201chave values\u201d rather than just \u201cbe valid.\u201d Traditional logic programs can be understood as weighted logic programs with Boolean values: axioms all have the value \u201ctrue,\u201d as do all valid propositions. The single-source connectivity program would describe the graph in Figure 2 by assigning t as the value of all the existing edges and the proposition initial(a)."}, {"heading": "2.1 Non-Boolean Programs", "text": "With weighted logic programming, the axioms and propositions can be understood as having non-Boolean values. In Figure 3, each axiom of the form edge(X, Y) is given a value corresponding to the cost associated with that edge in the graph, and the axiom initial(a) is given the value 0. If we take the value or \u201cscore\u201d of a proof to be the the sum of the values the axioms at its leaves and take the value of a proposition to be the minimum score over all possible proofs, then the program from Figure 1 gives a declarative specification of the single-source shortest path problem. Multiple uses of an axiom in a proof are meaningful: if a proof includes the edge(d, d) axiom once, it corresponds to a single traversal of the loop from d to d and adds a cost of 2, and if a proof includes the axiom twice, it corresponds to two distinct traversals and adds a cost of 4.\nWe replace the connectives :- (disjunction) and , (conjunction) with min = and +, respectively, and interpret the WLP over the non-negative numbers. With a specific execution model, the result is Dijkstra\u2019s single-source shortest-path algorithm.\nIn addition to the cost-minimization interpretation in Figure 3, we can interpret weights on edges as probabilities and restate the problem in terms of probability maximization. In Figure 4, the outgoing edges from each vertex sum to at most 1. If we assign the missing 0.1 probability from vertex b to a \u201cstopping\u201d event\u2014either implicitly or explicitly by modifying the axioms\u2014then each vertex\u2019s outgoing edges sum to exactly one and the graph can be seen as a Markov model or probabilistic finite-state network over which random walks are well-defined. If we replace the connectives :- (disjunction) and , (conjunction) with max = and \u00d7, then the value of reachable(X) for any X is the probability of the most likely path from a to X. For instance, reachable(a) ends up with the value 1, and reachable(b) ends up with value 0.16, corresponding to the path a \u2192 d \u2192 b, whose weight is (value of initial(a)\u00d7 value of edge(a, d)\u00d7 value of edge(d, b)).\nIf we keep the initial database from Figure 4 but change our operators from max = and \u00d7 to += and \u00d7, the result is a program for summing over the probabilities of all distinct paths that start in a and lead to X, for each vertex X. This quantity is known as the \u201cpath sum\u201d (Tarjan 1981). The path sum for reachable(b), for\ninstance, is 10\u2014this is not a probability, but rather an infinite sum of probabilities of many paths, some of which are prefixes of each other.3\nThese three related weighted logic programs are useful generalizations of the reachability logic program in Figure 1. Figure 5 gives a generic representation of all four algorithms in the Dyna language (Eisner et al. 2005). The key difference among them is the semiring in which we interpret the weights. An algebraic semiring consists of five elements \u3008K,\u2295,\u2297,0,1\u3009, where K is a domain closed under \u2295 and \u2297, \u2295 is a binary, associative, commutative operator, \u2297 is a binary, associative operator that distributes over \u2295, 0 \u2208 K is the \u2295-identity, and 1 \u2208 K is the \u2297-identity. We require, following Goodman (1999), that the semirings we use be complete. Complete semirings are semirings with the additional property that they are closed under finite products and infinite sums\u2014in our running example, this corresponds to the idea that there may be infinitely many paths through a graph, all with finite length. Complete semirings also have the property that infinite sums behave like finite ones\u2014they are associative and commutative, and the multiplicative operator distributes over them.\nIn our running example, reachability uses the Boolean semiring \u3008{t, f},\u2228,\u2227, f,t\u3009, single-source shortest path uses \u3008R\u22650\u222a{\u221e},min,+,\u221e, 0\u3009, the most-probable-path\n3 Clearly \u201c10\u201d is not a meaningful probability, but that is a result of the loop from b to b with probability 0.9\u2014in fact, one informal way of looking at the result is simply to observe that 10 = 1 + 0.9 + (0.9)2 + (0.9)3 + . . ., corresponding to proofs of reachable(b) that include edge(b, b) zero, one, two, three, . . . times. If we added an axiom edge(b, final) with weight 0.1 representing the 10% probability of stopping at any step in state b, then the path sum for reachable(final) would be 10 \u00d7 0.1 = 1, which is a reasonable probability that corresponds to the fact that a graph traversal can be arbitrarily long but has a 100% chance of eventually reaching b and then stopping.\nvariant uses \u3008[0, 1],max,\u00d7, 0, 1\u3009, and the probabilistic path-sum variant uses the so-called \u201creal\u201d semiring \u3008R\u22650 \u222a {\u221e},+,\u00d7, 0, 1\u3009.\nWeighted logic programming developed primarily within the computational linguistics community. Building upon the observations of Shieber, Schabes, and Pereira (1995) and Sikkel (1997) that many parsing algorithms for nondeterministic grammars could be represented as deductive logic programs, Goodman (1999) showed that the structure of the parsing algorithms was amenable to interpretation on a number of semirings. McAllester (1999) additionally showed that this representation facilitates reasoning about asymptotic complexity. Other developments include a connection between weighted logic programs and hypergraphs (Klein and Manning 2004), optimal A\u2217 search for maximizing programs (Felzenszwalb and McAllester 2007), semiring-general agenda-based implementations (Eisner et al. 2005), improved k-best algorithms (Huang and Chiang 2005), and program transformations to improve efficiency (Eisner and Blatz 2007)."}, {"heading": "2.2 Formal Definition", "text": "Eisner and Blatz (2007) describe the semantics of weighted logic programs in detail; we summarize their discussion in this section and point the reader to that paper for further detail. A weighted logic program is a set of Horn equations describing a set of declarative, usually recursive equations over an abstract semiring. Horn equations, which we will refer to by the shorter and more traditional term rules, take the form\nconsequent(U)\u2295= antecedent1(W1)\u2297 \u00b7 \u00b7 \u00b7 \u2297 antecedentn(Wn).\nHere U and the Wi are sequences of terms which include free variables. If the variables in U are a subset of the variables in W1 . . .Wn for every rule, then the program is range restricted or fully grounded.\nWe can also give rules side conditions. Side conditions are additional constraints that are added to a rule to remove certain proofs from consideration. For example, side conditions could allow us to modify rule 4 in Figure 5 to disallow self-loops and only allow traversal of an edge when there was another edge in the opposite direction:\nreachable(Q) \u2295= reachable(P)\u2297 edge(P, Q) if edge(Q, P) \u2227 Q 6= P. (5)\nSide conditions do not change the value of any individual proof, they only filter out any proof that does not satisfy the side conditions. In this paper, we use mostly side conditions that enforce equality between variables. For a more thorough treatment of side conditions see Goodman (1999) or Eisner and Blatz (2007).\nA weighted logic program is specified on an arbitrary semiring, and can be interpreted in any semiring \u3008K,\u2295,\u2297,0,1\u3009 as previously described. The meaning of a weighted logic program is determined by the rules together with a set of fully grounded axioms (or facts in the Prolog setting). Each axiom is assigned a value from the set K that is interpreted as a weight or score. A common idiom in weighted logic programming is to specify the query as a\ndistinguished predicate goal that takes no arguments. A computationally uninteresting (because are no intermediate computation steps) but otherwise legitimate way to present a weighted logic program is as a single rule of the form\ngoal \u2295= axiom1(W1)\u2297 \u00b7 \u00b7 \u00b7 \u2297 axiomn(Wn).\nIn this degenerate case, each distinct way of satisfying the premises using axioms in the database would correspond to a distinct proof of goal. The score of each proof would be given by the semiring-product of the scores of the axioms, and the value of goal would be determined by the semiring-sum of the scores of all the proofs.\nIn the general case, the value of the proposition/theorem goal is a semiring-sum over all of its proofs, starting from the axioms, where the value of any single proof is the semiring-product of the axioms involved. This is effectively encoded using the inference rules as a sum of products of sums of products of ... sums of products, exploiting distributivity and shared substructure for efficiency. This inherent notion of shared substructure means that weighted logic programming can give straightforward declarative specifications for problems that are typically solved by dynamic programming. The Dyna programming language implements a particular dynamic programming strategy for implementing these declarative specifications (Eisner et al. 2005), though the agenda algorithm that it implements may potentially have significantly different behavior, in terms of time and space complexity, than other dynamic programming algorithms that meet the same specification.\nIn many practical applications, as in our reachability example in \u00a72.1, values are interpreted as probabilities to be maximized or summed or costs to be minimized."}, {"heading": "3 Weighted Logic Programs and Probabilistic Reasoning", "text": "In this section, we will return focus to the probabilistic interpretation of weighted logic programs that we first described in the introduction. In \u00a73.1 we will describe in more detail how the results of weighted logic programs are interpreted as probabilities\u2014readers with a background in statistics and machine learning can probably skip or skim this section. In \u00a73.2, we will introduce the notion of a product of experts that motivates the PRODUCT transformation.\nOur running example for this section is a probabilistic finite-state automaton\nover the alphabet {0, 1}, shown in Figure 6. The most probable path through the graph is the one that recognizes the string \u201c01\u201d by going through states a, b, and c, and that the probability of this path is 0.4. Other than the labels on the edges, this is the same setup used in the graph-reachability example from Figure 4. The edge predicate from the previous section is now called arc and has been augmented to carry a third argument representing an output character."}, {"heading": "3.1 The Probabilistic Interpretation of Weighted Logic Programming", "text": "Recall from the introduction that, in the context of weighted logic programming, we have random variables I, A, and P , where\n\u2022 I corresponds to a set of conditional \u201cinput\u201d axioms known to be true; \u2022 A corresponds to a set of axioms known to be true; and \u2022 P corresponds to a deductive proof of the goal theorem using the axioms.\nIn this case, I corresponds to one of the various possible sentences recognized by the FSA (i.e., 00, 01, 10, and 11). A corresponds to a particular directed graph with weighted edges, encoded by a set of axioms. P corresponds to an individual proof/path through the graph. In Figure 7, which is the straightforward adaptation of the reachability program in Figure 5 to labeled edges, the value of goal in the most-probable-path semiring is maxproof p(P = proof , I = sentence | A = graph)\u2014 the value of the most probable path emitting any possible sentence I.\nIn order to talk about the input sentences I, we first add a set of axioms that describe I. If we are interested in the sentence \u201c01\u201d we would add axioms string(1, 0), string(2, 1), and length(2), whereas if we were interested in the sentence \u201chey\u201d we would add axioms string(1, h), string(2, e), string(3, y), and length(3). These axioms are all given the value 1 (the multiplicative unit of the semiring), and so they could equivalently be treated as side conditions. With these new axioms, we modify\nFigure 7 to obtain Figure 8, a weighted logic program that limits the proofs/paths to the ones which represent recognition of the input string I.4\nNow, Figure 8 interpreted over the most-probable-path semiring does allow us to find the proof that, given the edge weights and a specific sentence, maximizes p(P = proof | I = sentence, A = graph). It does not, however, give us p(P = proof | I = sentence, A = graph), but rather p(P = proof , I = sentence | A = graph), the joint probability of a path and a sentence given the weights on the edges.\nConcretely, in our running example there are five possible proofs of goal in Figure 7 whose probabilities sum to 1, but there are only two parses that also recognize the string \u201c01,\u201d which are a0b1c with weight 0.4 and a0d1c with weight 0.2\u2014the route through b is twice as likely. The value of goal in Figure 8 interpreted in the most-probable-path semiring would be 0.4 (the joint probability of obtaining the proof a0b1c and of recognizing the string \u201c01\u201d) not 0.6 (the probability of the proof a0b1c given the sentence \u201c01\u201d). In other words, we have: p(P = a0b1c, I = 01 | A = Fig. 6) = 0.4, p(P = a0d1c, I = 01 | A = Fig. 6) = 0.2, p(P = a0b1c | I = 01, A = Fig. 6) = 0.6.\nThe solution for correctly discovering the conditional probability lies in the fact\nthat the joint and conditional probabilities are related in the following way:\np(P | A, I) = p(P, I | A) p(I | A)\nThis, combined with the knowledge that the marginal probability p(I | A) is the result of evaluating Figure 8 over the path-sum semiring (i.e., \u3008R\u22650\u222a{\u221e},+,\u00d7, 0, 1\u3009), allows us to correctly calculate not only the most probable proof P of a given sentence but also the probability of that proof given the sentence. The marginal probability in our running example is 0.6, and 0.4/0.6 = 0.6, which is the desired result.\nTo restate this in a way that is more notationally consistent with other work in machine learning, we first take the weighted axioms A as implicit. Then, instead of proofs P we talk about values y for a random variable Y drawn out of a domain Y (the space of possible structures, which in our setting corresponds to the space of possible proofs), and instead of inputs I we talk about values x for a random variable X drawn out of a domain X (the space of all possible inputs). Then, to predict the most likely observed value for y, denoted y\u0302, we have the following formula:\ny\u0302 = argmax y\u2208Y p(Y = y | X = x) = argmax y\u2208Y\np(Y = y,X = x)\np(X = x) (12)\nBecause p(X = x) does not depend on y, if we only want to know y\u0302 it suffices to find the y that maximizes p(Y = y,X = x) (which was written as p(P = proof , I =\n4 Rule 11 in this figure uses \u201cI\u2212 1\u201d in a premise: we assume that our formalism includes natural numbers that support increment/decrement operations, and our simple uses can be understood as syntactic shorthand either for structured terms (z, s(z), etc.) or for the use of primitive side conditions such as inc(I, I\u2032).\nsentence | A = axioms) above). One way to do this is to execute a weighted logic program in the most-probable-path semiring."}, {"heading": "3.2 Products of Experts", "text": "Of recent interest are probability models p that take a factored form, for example:\np(Y = y | X = x) \u221d p1(Y = y | X = x)\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 pn(Y = y | X = x) (13)\nwhere \u221d signifies \u201cproportional to\u201d and suppresses the means by which the probability distribution is renormalized to sum to one. This kind of model is called a product of experts (Hinton 2002). Intuitively, the probability of an event under p can only be relatively large if \u201call the experts concur,\u201d i.e., if the probability is large under each of the pi. Any single expert can make an event arbitrarily unlikely (even impossible) by giving it very low probability, and the solution to Equation 12 for a product of experts model will be the y \u2208 Y (here, a proof) least objectionable to all experts.\nThe attraction of such probability distributions is that they modularize complex systems (Klein and Manning 2003; Liang et al. 2008). They can also offer computational advantages when solving Equation 12 (Chiang 2007). Further, the expert factors can often be trained (i.e., estimated from data) separately, speeding up expensive but powerful machine learning methods (Smith and Smith 2004; Sutton and McCallum 2005; Smith et al. 2005; Cohen and Smith 2007).\nTo the best of our knowledge, there has been no attempt to formalize the following intuitive idea about products of experts: algorithms for reasoning about mutually constrained product proof values should resemble the individual algorithms for each of the two separate \u201cfactor\u201d proofs\u2019 values. Our formalization is intended to aid in algorithm development as new kinds of complex random variables are coupled, with a key practical advantage: the expert factors are known because they fundamentally underlie the main algorithm. Indeed, we call our algorithms \u201cproducts\u201d because they are derived from \u201cfactors,\u201d analogous to the product of expert probability models that are derived from factor expert probability models.\nTo relate this observation to the running example from this section, imagine we created two copies of Figure 8 which operated over the same sentence (as described by string and length predicates) but which had different predicates and axioms goal1, path1, final1, initial1, and arc1 (and likewise goal2, path2, etc.). Consider a combined goal predicate goal1\u20222 defined by the rule\ngoal1\u20222 \u2295= goal1 \u2297 goal2. (14)\nNow we have two experts (goal1 and goal2), and we literally take the (semiring) product of them, but this is still not quite the \u201cproduct of experts,\u201d because the proofs of the goals are allowed to be independent. In other words, what we have is the following:\np(Y1 = y1, Y2 = y2 | X = x) \u221d p1(Y1 = y1 | X = x)\u00d7 p2(Y2 = y2 | X = x).\nThe PRODUCT transformation is a meaning-preserving transformation on weighted\nlogic programs that exposes the joint structure in such a way that\u2014depending on our domain-specific understanding of what it means for the two proofs y1 and y2 to match\u2014allows us to add constraints that result in a weighted logic program that forces the structures to match, as required by the specification in Equation 13."}, {"heading": "4 Products of Weighted Logic Programs", "text": "In this section, we will motivate products of weighted logic programs in the context of the running example of generalized graph reachability. We will then define the PRODUCT transformation precisely and describe the process of specifying new algorithms as constrained versions of product programs.\nThe PRODUCT transformation can be seen as an instance of the tupling program transformation combined with an unfold/fold transformation (Pettorossi and Proietti 1994; Pettorossi 1999) that preserves the meaning of programs. However, we are interested in this transformation not for reasons of efficiency, but because it has the effect of exposing the shared structure of the two individual programs in such a way that, by the manual addition of constraints, we can force the two original programs to optimize over the same structures, thereby implementing optimization over the product of experts as described in the previous section. The addition of constraints requires an understanding of the problem at hand, as we show in \u00a75 by presenting a number of examples."}, {"heading": "4.1 The Product of Graph Reachability Experts", "text": "Figure 9 defines two experts, copies of the graph-reachability program from Figure 5. We are interested in a new predicate reachable1\u20222(Q1, Q2), which for any particular Q1 and Q2 should be equal to the product of reachable1(Q1) and reachable2(Q2). Just as we did in our thought experiment with goal1\u20222 in the previous section, we could define the predicate by adding the following rule to the program in Figure 9:\nreachable1\u20222(Q1, Q2) \u2295= reachable1(Q1)\u2297 reachable2(Q2).\nThis program is a bit simplistic, however; it merely describes calculating the experts independently and then combining them at the end.\nThe predicate reachable1\u20222 can alternatively be calculated by adding the fol-\nlowing four rules to Figure 9:\nreachable1\u20222(Q1, Q2) \u2295= initial1(Q1)\u2297 initial2(Q2). reachable1\u20222(Q1, Q2) \u2295= initial1(Q1)\u2297 reachable2(P2)\u2297 edge2(P2, Q2). reachable1\u20222(Q1, Q2) \u2295= reachable1(P1)\u2297 edge1(P1, Q1)\u2297 initial2(Q2). reachable1\u20222(Q1, Q2) \u2295= reachable1(P1)\u2297 edge1(P1, Q1)\u2297\nreachable2(P2)\u2297 edge2(P2, Q2).\nThis step is described as an unfold by Pettorossi (1999). This unfold can then be followed by a fold: because reachable1\u20222(Q1, Q2) was defined above to be the product of reachable1(Q1) and reachable2(Q2), we can replace each instance of the two premises reachable1(Q1) and reachable2(Q2) with the single premise reachable1\u20222(Q1, Q2).\nreachable1\u20222(Q1, Q2) \u2295= initial1(Q1)\u2297 initial2(Q2). (19) reachable1\u20222(Q1, Q2) \u2295= reachable2(P2)\u2297 edge2(P2, Q2)\u2297 initial1(Q1). (20) reachable1\u20222(Q1, Q2) \u2295= reachable1(P1)\u2297 edge1(P1, Q1)\u2297 initial2(Q2). (21) reachable1\u20222(Q1, Q2) \u2295= reachable1\u20222(P1, P2)\u2297 edge1(P1, Q1)\u2297 edge2(P2, Q2). (22)\nThe PRODUCT program transformation is shown in Figure 11. For each desired product of experts, where one expert, the predicate p, is defined by n rules and the other expert q by m rules, the transformation defines the product of experts for p\u2022q with n \u00d7m new rules, the cross product of inference rules from the first and second experts. The value of a coupled proposition p\u2022q in P \u2032 will be equal to the semiring product of p\u2019s value and q\u2019s value in P (or, equivalently, in P \u2032).\nNote that lines 6\u20138 are nondeterministic under certain circumstances, because if the antecedent of the combined program is a(X)\u2297 a(Y)\u2297 b(Z) and the algorithm is computing the product of a and b, then the resulting antecedent could be either a\u2022b(X, Z)\u2297 a(Y) or a\u2022b(Y, Z)\u2297 a(X). This nondeterminism usually does not arise, and when it does, as in \u00a75.2, there is usually an obvious preference.\nThe PRODUCT transformation is essentially meaning preserving: if the program\nP \u2032 is the result of the PRODUCT transformation on P, then the following is true:\n\u2022 Any ground instance p(X) that is given a value in P is given the same value in P \u2032. This is immediately apparent because the program P \u2032 is stratified: none of the new rules are ever used to compute values of the form p(X), so their\nvalue is identical to their value in P. \u2022 Any ground instance p\u2022q(X,Y) in P \u2032 has the same value as p(X) \u2297 q(Y).\nThis is the result of the following theorem:\nTheorem 1 Let P be a weighted logic program over a set of predicates R, and let S be a set of pairs of predicates from P. Then after applying PRODUCT on (P,S), resulting in a new program P \u2032, for every (p, q) \u2208 S, the value p\u2022q(X,Y) in P \u2032 is p(X)\u2297 q(Y).\nProof: By distributivity of the semiring, we know that p(X) \u2297 q(Y) is the sum:\u2295 t,r v(t)\u2297 v(r) where t and r range over proofs of p(X) and q(Y) respectively, with their values being v(t) and v(r). This implies that we need to show that there is a bijection between the set A of proofs for p\u2022q(X,Y) in P \u2032 and the set B of pairs of proofs for p(X) and q(Y) such that for every s \u2208 A and (t, r) \u2208 B we have v(s) = v(t)\u2297 v(r).\nUsing structural induction over the proofs, we first show that every pair of proofs (t, r) \u2208 B has a corresponding proof s \u2208 A with the needed value. In the base case, where the proofs t and r include a single step, the correspondence follows trivially. Let (t, r) \u2208 B. Without loss of generality, we will assume that both t and r contain more than a single step in their proofs. In the last step of its proof, t used a rule of the form\np(X)\u2295= a1(X1)\u2297 \u00b7 \u00b7 \u00b7 \u2297 an(Xn) (23)\nand r used a rule in its last step of the form\nq(Y)\u2295= b1(Y1)\u2297 \u00b7 \u00b7 \u00b7 \u2297 bm(Ym) (24)\nLet ti be the subproofs of ai(Xi) and let rj be subproofs of bj(Yj). It follows that PRODUCT creates from those two rules a single inference rule of the form:\np\u2022q(X,Y)\u2295= c1(W1)\u2297 \u00b7 \u00b7 \u00b7 \u2297 cp(Wp) (25)\nwhere ci(Wi) is either ak(Yk) for some k, or bl(Yl) for some k, or ak \u2022 b`(Xk,Y`) for some k, `.\nWe resolve each case as following:\n1. If ci(Wi) = ak(Yk) then we set si = tk. 2. If ci(Wi) = bk(Yk) then we set si = rk. 3. If ci(Wi) = ak \u2022 b`(Xk,Y`) then according to the induction hypothesis, we have a proof for ak \u2022 b`(Xk,Y`) such that its value is v(tk)\u2297 v(r`). We set si to be that proof.\nSince we have shown there is a proof for each antecedent of p \u2022 q(X,Y), we have shown that there is a proof for p \u2022 q(X,Y). That its value is indeed p(X) \u2297 q(Y) is concluded trivially from the induction steps.\nThe reverse direction for constructing the bijection is similar, again using struc-\ntural induction over proofs.\n4.3 From PRODUCT to a Product of Experts\nThe output of the PRODUCT transformation is a starting point for describing dynamic programming algorithms that perform two actions\u2014traversing a graph, scanning a string, parsing a sentence\u2014at the same time and in a coordinated fashion. Exactly what \u201ccoordinated fashion\u201d means depends on the problem, and answering that question determines how the problem is constrained.\nIf we return to the running example of generalized graph reachability, the program as written has eight rules, four from Figure 9 and four from Figure 10. Two\nexamples of constrained product programs are given in Figures 12\u201314. In the first example in Figure 12, the only change is that all but two rules have been removed from the program in Figures 9 and 10. Whereas in the original product program reachable1\u20222(Q1, Q2) corresponded to the product of the weight of the best path from the initial state of graph one to Q1 and the weight of the best path from the initial state of graph two to Q2, the new program computes the best paths from the two origins to the two destinations with the additional requirement that the paths be the same length\u2014the rules that were deleted allowed for the possibility of a prefix on one path or the other.\nIf our intent is for the two paths to not only have the same length but to visit exactly the same sequence of vertices, then we can further constrain the program to only define reachable1\u20222(Q1, Q2) where Q1 = Q2, as shown in Figure 13. After adding this side condition, it is no longer necessary for reachable1\u20222 to have two arguments that are always the same, so we can simply further as shown in Figure 14. For simplicity\u2019s sake, we will usually collapse arguments that have been forced by equality constraints to agree.\nThe choice of paired predicates S is important for the final weighted logic program that PRODUCT returns and it also limits the way we can add constraints to derive a new weighted logic program. Future research might consider a machine learning setting for automatically deriving S from data, to minimize some cost (e.g., observed runtime). When PRODUCT is applied on two copies of the same weighted logic program (concatenated together to a single program), a natural schema for selecting paired predicates arises, in which we pair a predicate from one program with the same predicate from the other program. This \u201cnatural\u201d pairing leads to the derivation of several useful, known algorithms, to which we turn in \u00a75."}, {"heading": "5 Examples", "text": "In this section, we give several examples of constructing weighted logic programs as constrained products of simpler weighted logic programs."}, {"heading": "5.1 Finite-State Algorithms", "text": "We have already encountered weighted finite-state automata (WSFAs) in \u00a73.1. Like WFSAs, weighted finite-state transducers (WFSTs) are a generalization of the graph-reachability problem: in WFSAs the edges are augmented with a symbol and represented as arc(P, Q, A), whereas in WFSTs edges are augmented with a pair of input-output symbols and represented as arc(P, Q, A, B). Weighted finite-state machines are widely used in speech and language processing (Mohri 1997; Pereira and Riley 1997). They are used to compactly encode many competing string hypotheses, for example in speech recognition, translation, and morphological (word-structure) disambiguation. Many sequence labeling and segmentation methods can also be seen as weighted finite-state models.\nWeighted finite-state automata. Our starting point for weighted finite-state automata will be the weighted logic program for WFSAs described in Figure 7, which is usually interpreted as a probabilistic automaton in the most-probable-path semiring (i.e., \u3008[0, 1],max,\u00d7, 0, 1\u3009). If the PRODUCT of that algorithm with itself is taken, we can follow a series of steps similar to the ones described in \u00a74.3. First, we remove rules that would allow the two WFSAs to consider different prefixes, and then we add a constraint to rule 33 that requires the two paths\u2019 symbols to be identical. The result is a WFSA describing the (weighted) intersection of the two WFSAs. The intersection of two WFSAs is itself a WFSA, though it is a WFSA where states are described by two terms\u2014Q1 and Q2 in path1\u20222(Q1, Q2)\u2014instead of a single term.\nWeighted intersection generalizes intersection and has a number of uses. For instance, consider an FSA that is \u201cprobabilistic\u201d but that only accepts the single string \u201c01\u201d because the transitions are all deterministic and have probability 1:\n211.0,101.0,0\nIf we consider the program in Figure 15 with axioms describing the above FSA and the probabilistic FSA given in Figure 6, then the resulting program is functionally equivalent to the weighted logic program in Figure 8 describing a WFSA specialized to a particular string. Alternatively, if we consider the program in Figure 15 with axioms describing the probabilistic FSA in Figure 6 and the following single-state\nprobabilistic FSA, the result will be a probabilistic FSA biased towards edges with the \u201c1\u201d symbol and against edges with the \u201c0\u201d symbol.\ns00.1, 10.9,\nBoth of the above examples can be understood as instances of the product of experts pattern discussed in \u00a73.2. In the first case, the additional expert eliminates certain possibilities by assigning zero probability to them, and in the second case the additional expert merely modifies probabilities by preferring the symbol \u201c1\u201d to the symbol \u201c0.\u201d\nWeighted finite-state transducers. Suppose we take the PRODUCT transformation of the WFST recognition algorithm in Figure 16 with itself and constrain the result by removing all but the three interesting rules (as before) and requiring that B1 (the \u201coutput\u201d along the first edge) always be equal to A2 (the \u201cinput\u201d along the second edge). The result is shown in Figure 17; this is the recognition algorithm for the WFST resulting from composition of two WFSTs. Composition permits small, understandable components to be cascaded and optionally compiled, forming complex but efficient models of string transduction (Pereira and Riley 1997)."}, {"heading": "5.2 Context-Free Parsing", "text": "Parsing natural languages is a difficult, central problem in computational linguistics (Manning and Schu\u0308tze 1999). Consider the sentence \u201cAlice saw Bob with binoculars.\u201d One analysis (the most likely in the real world) is that Alice had the binoculars and saw Bob through them. Another is that Bob had the binoculars, and Alice saw the binocular-endowed Bob. Figure 18 shows syntactic parses into noun phrases (NP), verb phrases (VP), etc., corresponding to these two meanings. It also shows some of the axioms that could be used to describe a context-free grammar describing English sentences in Chomsky normal form (Hopcroft and Ullman 1979).5 A proof corresponds to a derivation of the given sentence in a context-free grammar, i.e., a parse tree.\nShieber et al. (1995) show that parsing with CFGs can be formalized as a logic program, and in Goodman (1999) this framework is extended to the weighted case. If weights are interpreted as probabilities, then the \u3008[0, 1],max,\u00d7, 0, 1\u3009 semiring interpretation finds the probability of the parse with maximum probability and the \u3008R\u22650 \u222a {\u221e},+,\u00d7, 0, 1\u3009 semiring interpretation finds the total weight of all parse trees (a measure of the \u201ctotal grammatically\u201d of a sentence). In Figure 19, we give the specification of the weighted CKY algorithm (Cocke and Schwartz 1970; Kasami\n5 Chomsky normal form (CNF) means that the rules in the grammar are either binary with two nonterminals or unary with a terminal. We do not allow rules, which in general are allowed in CNF grammars.\n1965; Younger 1967), which is a dynamic programming algorithm for parsing using a context-free grammar in Chomsky normal form.6\nFigure 19 suggestively has a subscript attached to all but the length and string inputs. In our description of the product of experts framework in \u00a73.2, the axioms length and string correspond to the conditional input sentence I. The unconstrained result of the PRODUCT transformation on the combination of the rules in Figure 19 and a second copy that has \u201c2\u201d subscripts is given in Figure 20. Under the most-probable-path probabilistic interpretation, the value of goal1\u20222 is the probability of the given string being generated twice, once by each of the two probabilistic grammars, in each case by the most probable tree in that grammar. By constraining Figure 20, we get the more interesting program in Figure 21 that adds the additional requirement that the two parse trees in the two different grammars have the same structure. In particular, in all cases the constraints I1 = I2, J1 = J2, K1 = K2, N1 = N2 are added, so that instead of writing c1\u20222(X1, I1, J1, X2, I2, J2) we just write c1\u20222(X1, X2, I, J).\nLexicalized CFG parsing. An interesting variant of the previous rule involves lexicalized grammars, which are motivated in Figure 22. Instead of describing a gram-\n6 Strictly speaking, the CKY parsing algorithm corresponds to a na\u0308\u0131ve bottom-up evaluation strategy for this program.\nmar using nonterminals denoting phrases (e.g., NP and VP), which is called a constituent-structure grammar we can define a (context-free) dependency grammar (Gaifman 1965) that encodes the syntax of a sentence in terms of parent-child relationships between words. In the case of the example of Figure 22, the arrows below the sentence in the middle establish \u201csaw\u201d as the root of the sentence; the word \u201csaw\u201d has three children (arguments and modifiers), one of which is the word \u201cwith,\u201d which in turn has the child \u201cbinoculars.\u201d\nA simple kind of dependency grammar is a Chomsky normal form CFG where the nonterminal set is equivalent to the set of terminal symbols (so that the terminal \u201cwith\u201d corresponds to a unique nonterminal with, and so on) and where all rules have the form P\u2192 P C, P\u2192 C P, and W \u2192 w (where P is the \u201cparent\u201d word, C is the \u201cchild\u201d word that is dependent on the parent, and W is the nonterminal version of terminal word w).\nIf we encode the constituent-structure grammar in the unary1 and binary1 relations and encode a dependency grammar in the unary2 and binary2 relations, then the product is a lexicalized grammar, like the third example from Figure 22. In particular, it describes a lexicalized context-free grammar with a product of experts probability model (Klein and Manning 2003), because the weight given to any production A-X\u2192 B-X C-Y is the semiring-product of the weight given to the production A\u2192 B C and the weight given to the dependency based production X\u2192 X Y. This was an important distinction for Klein and Manning\u2014they were interested in factored lexicalized grammars that Figure 21 can describe. These are only a small (but interesting) subset of all possible lexicalized grammars. Standard lexicalized CFGs assign weights directly to grammar productions of the form A-X\u2192 B-X C-Y, not indirectly (as we do) by assigning weights to a constituentstructure and a dependency grammar. We will return to this point in \u00a76.2 when we consider the \u201caxiom generalization\u201d pattern that allows us to describe general lexicalized CKY parsing (Eisner 1997; Eisner and Satta 1999).\nNondeterminism and rule binarization. The result of the PRODUCT transformation shown in Figure 20 was the first time the nondeterminism inherent in lines 6-8 of the description of the PRODUCT transformation (Figure 11) has come into play. Because there were two c1 premises and two c2 premises, they could have been merged in more than one way. For example, the following would have been a potential alternative to rule 46:\nc1\u20222(X1, I1, K1, X2, I2, K2) \u2295= binary1(X1, Y1, Z1)\u2297 binary2(X2, Y2, Z2) \u2297 (50) c1\u20222(Y1, I1, J1, Z2, K2, J2)\u2297 c1\u20222(Z1, J1, K1, Y2, I2, J2).\nHowever, this would have broken the correspondence between I1 and I2 and made it impossible to constrain the resulting program as we did. An alternative to CKY is the binarized variant of CKY where rule 41 is split into two rules by introducing a new, temporary predicate (rules 51 and 52):\ntemp1(X, Y, J, K) \u2295= binary1(X, Y, Z)\u2297 c1(Z, J, K). (51) c1(X, I, K) \u2295= c1(Y, I, J)\u2297 temp1(X, Y, J, K). (52)\nIn this variant, the nondeterministic choice in the PRODUCT transformation disappears. The choice that we made in pairing was consistent with the choice that is forced in the binarized CKY program."}, {"heading": "5.3 Translation Algorithms", "text": "Another example of two probabilistic models that play the role of experts arises in translation of sentences from one natural language to another. We will summarize how the PRODUCT transformation was applied to a simple form of phrase-to-phrase translation (Koehn et al. 2003) by Lopez (2009).\nLopez (2009) suggested a deductive view of algorithms for machine translation, similar to the view of parsing given in Shieber et al. (1995). Lopez used the PRODUCT transformation to derive an algorithm for phrase translation from two different factor programs, one which attempts to enforce fluency (a measure of the grammaticality of a sentence) in the translated sentence and one which attempts to enforce adequacy (a measure of how much of the meaning of an original sentence is preserved in the translation.)\nIf fluency is a measure of the grammaticality of a sentence, then it would seem that the CKY algorithm for parsing context-free grammars would be a candidate. While such models have been used in translation (Charniak et al. 2003), Lopez\u2019s example uses a simpler notion of fluency based on an n-gram language model (Manning and Schu\u0308tze 1999, Chapter 6). An n-gram model assigns the probability of a sentence to be the product of probabilities of each word following the (n\u2212 1)-word\nsequence immediately preceding it. As a concrete example, let us say that n = 3 (called a \u201ctrigram\u201d model) and work with the program in Figure 23. If we were estimating our trigram model based on the relative frequencies of sequences in Shakespeare\u2019s Othello, we would note that the phrase \u201cif it\u201d appears eight times in the text. Three of these are from the sequence \u201cif it be\u201d and one is from the sequence \u201cif it prove,\u201d so the axiom trigram(\u201cif\u201d, \u201cit\u201d, \u201cbe\u201d) should have a probability that is three times the probability given to trigram(\u201cif\u201d, \u201cit\u201d, \u201cprove\u201d). If we then stared the program with the initial sentence fragment predict(\u201cif\u201d, \u201cit\u201d, 3), we could derive predict(\u201cit\u201d, \u201cbe\u201d, 4) with the aforementioned axiom and then predict(\u201cbe\u201d, \u201cdemanded\u201d, 5) with the axiom trigram(\u201cit\u201d, \u201cbe\u201d, \u201cdemanded\u201d), a sequence occurring once in the text. The result so far is a sequence \u201cif it be demanded\u201d that does not appear in Othello, but which perhaps sounds like it could (which is an informal way of describing the criterion for fluency).\nThe weighted logic program Lopez uses to enforce adequacy is the \u201cmonotone decoding\u201d logic program presented in Figure 24. The program is slightly contrived in order to interact with the PRODUCT transformation correctly. The atomic proposition trans(I, Es) refers to a particular point, I, in the source-language string and a list Es of unprocessed words in the target language.7 Each deduction consumes a single word (EJ) in the target language\u2014indeed, this is the only function of rule 57. When there are no words to remove, then either the entire source-language string has been translated (rule 55), or else progress can continue by translating some chunk of the source-language sentence starting from position I and ending at position I\u2032 as the non-empty list of target-language words EJ :: Es and applying rule 56. This translation of a sequence of the source-language words is captured by the premise phrase(I, I\u2032, Es), corresponding to the source subsequence from position I to position I\u2032 being translated as Es (a target-language phrase). The meaning of phrase could be defined by a set of axioms or by a rule. In the latter case, if we enumerate all the substrings Ds in the source-language sentence as axioms substr(I, I\u2032, Ds) and provide axioms ptranslate(Ds, Es) describing sourcelanguage to target-language phrase translation, then phrase(I, I\u2032, Es) may be defined by the following rule:\nphrase(I, I\u2032, Es) \u2295= substr(I, I\u2032, Ds)\u2297 ptranslate(Ds, Es). (61)\nNote that substr might be provided as an axiom, or derived from axioms encoding the source sentence through another inference rule.\nFigure 25 displays Lopez\u2019s phrase translation program by constraining the product of the n-gram model and monotone decoding programs. Lopez describes this for any n, but for simplicity we continue using a trigram model (n = 3). The combined predicate simultaneously tracks a position in the source-language sentence I and the target-language sentence J. The word EJ that was discarded at each step in Figure 24 is given relevance by the trigram model. The combination of these two programs uses the monotone decoding program\u2019s capabilities to make sure that the\n7 We use a standard syntactic shorthand for lists; \u201c[]\u201d can be read as the constant nil and \u201cE :: Es\u201d can be read as cons(E, Es).\nphrase-by-phrase meaning of the source-language string D1 . . . DN is preserved in the destination language string E1 . . . EM (adequacy) while simultaneously using the trigram model\u2019s capabilities to ensure that the result is a plausible sentence in the destination language (fluency).\nOur presentation of machine translation algorithms through the PRODUCT transformation is simplistic. Lopez (2009) discusses more powerful translation algorithms that permit, for example, reordering of phrases.\n6 Variations on PRODUCT\nUp to this point, we have viewed our use of the PRODUCT transformation as one that solves a problem of joint optimization: we take two logic programs that describe structures (such as strings, paths, or trees), relate them to one another by adding constraints, and then optimize over the two original structures simultaneously (one instance of this is when we use weighted logic programming to describe a product of experts.) This is a useful pattern, but it is not the only interesting use of the fold/unfold transformation underlying the PRODUCT transformation. In this section we consider two other variants: in the first we only optimize over one of the two structures and fix the other one, and in the second we take the output of PRODUCT as describing not joint optimization over two simple structures but over one complex structure."}, {"heading": "6.1 Fixing One of the Factor Structures", "text": "The usual use of the PRODUCT transformation is to joint optimization on two structures, but general side conditions can be used to take the additional step of fixing one of the two structures and having the weighted logic program perform optimization on the other structure, subject to constraints imposed through the pairing.\nIn the setting where we consider weights to be probabilities, this is useful for solving certain probabilistic inference problems. Using the path-sum semiring (i.e., \u3008R\u22650 \u222a{\u221e},+,\u00d7, 0, 1\u3009), the result is a program calculating the marginalized quantity p(x) = \u2211 y p(x, y) (where x corresponds to one program\u2019s proof and y to the other program\u2019s proof). This is a useful quantity in learning; for example, the expectation-maximization (EM) algorithm (Dempster et al. 1977) for optimizing the marginalized log-likelihood of observed structures requires calculating sufficient statistics which are based on marginal quantities. Using the mostprobable-path semiring (i.e., \u3008[0, 1],max,\u00d7, 0, 1\u3009), the result is a program for solving argmaxy p(y | x)\u2014that is, for finding the most probable y given the fixed x.\nThe transformation of the constrained result of the PRODUCT transformation to a program with one proof fixed is essentially mechanical. We consider the example of lexicalized parsing from Figure 22. We take the constituent-structure parse as the structure we want to fix in order optimize over the possible matching parses from the dependency grammar. The shape of the constituent-structure parse tree can be represented by a series of new axioms that mirror the structure of the c1(X, I, J) pred-\nicate defining the constituent-structure grammar: proof1(s, 0, 5), proof1(np, 0, 1), proof1(vp, 1, 5), proof1(vp, 1, 3), proof1(pp, 3, 5), and so on.\nThen we take the constrained PRODUCT of CKY that we used to describe lexicalized parsing (Figure 21) and, wherever there was a conclusion derived from c1, we add a matching side condition that references proof1. The critical rule (49) ends up looking like this:\nc1\u20222(X1, X2, I, K) \u2295= binary1(X1, Y1, Z1)\u2297 binary2(X2, Y2, Z2)\u2297 (62) c1\u20222(Y1, Y2, I, J)\u2297 c1\u20222(Z1, Z2, J, K) if proof1(X1, I, K).\nThe effect of this additional constraint is to disqualify any proof that does not match the constituent-structure grammar which we have fixed and encoded as proof1 axioms. The idea of partially constraining CFG derivations with some bracketing structure was explored by Pereira and Schabes (1992)."}, {"heading": "6.2 Axiom Generalization", "text": "Axiom generalization is another way of manipulating products of weighted logic programs in a way that reveals the simple structures underlying a complex structure. Figure 26, which is intended to describe a weighted finite-state transducer, is close to the weighted logic program in Figure 15 that describes the intersection of two finite-state machines, but there are two differences. First, we have not forced the two symbols to be the same; instead, we wish to interpret A1 from the first expert as the transducer\u2019s input symbol and A2 as the transducer\u2019s output symbol. Second, we have merged initial1(Q1)\u2297 initial2(Q2) to the single product predicate initial1\u20222(Q1, Q2), and likewise for arc. As a first approximation, we can just\ndefine arc1\u20222 (and, similarly, initial1\u20222) by a single rule of this form:\narc1\u20222(P1, P2, Q1, Q2, A1, A2)\u2295= arc1(P1, Q1, A1)\u2297 arc2(P2, Q2, A2) (65)\nAn example is given in Figure 27. Two finite-state machines, one with two states (a and b) and one with three states (x, y, and z), are shown\u2014we are working over the Boolean semiring, so each arc in the figure corresponds to a true-valued arc axiom. The PRODUCT of these two experts in the manner of Figure 26 is a single finite-state transducer with six states.\nHowever, we can only describe a certain subset of finite-state transducers as the direct product of finite-state machines in this way. If we consider all possible Boolean-valued finite-state transducers with two symbols and one state, we have 16 possible transducers, but only 10 that can be \u201cfactored\u201d as two independent finite-state machines, such as these three:\n(0,0) (1,1)\n(0,1)(1,0)\n0\n1\n0\n1(0,1)1\n0 (1,1)\n(0,1)1\n0\n1\nSix others, like the NOT transducer that outputs 1 given the input 0 and outputs 0 given the input 1, cannot be represented as the product of two FSMs.\nIn many settings, limiting ourselves to the \u201cfactorable\u201d finite-state transducers (or lexicalized grammars) can have conceptual or computational advantages. When this does not suffice, we can perform axiom generalization, which amounts to removing the requirement of Eq. 65 that the value of atomic propositions of the form arc1\u20222 be the product of an atomic proposition of the form arc1 and an atomic proposition of the form arc2. If we directly define axioms of the form arc1\u20222, we can describe transducers in their full generality.\nThis represents a new way of thinking about the PRODUCT transformation. Thus far, we have considered the result of the PRODUCT transformation as a way of describing programs that work over two different structures. Axiom generalization suggests that we can consider the PRODUCT transformation as a way of taking two programs that work over individual structures and deriving a new program that works over a single more complicated structure that, in special cases, can be factored into two different structures. This is particularly relevant in the area of lexicalized grammars and parsing where the general, more complicated structure is what came first and the factored models which we have considered thus far arose later as special cases.\nParsing algorithms and the PRODUCT transformation. Many parsing algorithms can be derived by using the PRODUCT transformation as a way of deriving programs that do not neatly factor into two parts. Lexicalized parsing is a simple example; Figure 28 derives a lexicalized parser by performing axiom generalization on Figure 21. The grammar production \u201cP-with\u2192 with\u201d can be represented by including the axiom unary1\u20222(p, \u201cwith\u201d), and the binary production S-saw\u2192 NP-Alice VP-saw can be represented by the axiom binary1\u20222(s, \u201csaw\u201d, np, \u201calice\u201d, vp, \u201csaw\u201d).\nSynchronous grammars are another instance in which the axiom generalization\nview is interesting. A synchronous grammar can be thought of as parsing two different sentences in two different languages with two different grammars using a single parse tree. For example, if X\u2192 YZ is a grammar production in one language and A\u2192 BC is a grammar production in another language, then X-A\u2192 Y-B Z-C is a possible grammar production in the synchronous grammar.\nA transduction grammar (Wu 1997), is a synchronous grammar which generates two isomorphic derivations with a trivial alignment between the nodes of those two derivations. We can describe a parser for a transduction grammar with the program\nin Figure 30. Synchronous grammars need to be able to deal with situations in which a word in one language does not appear in the matching sentence in the other language; this is done by starting from the enriched CKY program in Figure 29 that can handle grammar productions of the form X\u2192 . In practice, transduction grammars do a bad job of aligning two sentences in different natural languages that are translations of each other, because it is often the case that two parts of a pair of sentences need to be in opposite positions relative to one another\u2014in language one, the verb phrase might precede a prepositional phrase, and in language two, the corresponding verb phrase might follow the corresponding prepositional phrase. An inversion transduction grammar describes an alternate form of grammar production, which Wu (1997) writes as X\u2192 \u3008YZ\u3009. This grammar production declares that if A1 and A2 simultaneously parse as Y in languages one and two (respectively) and B1 and B2 simultaneously parse as Z in languages one and two (respectively), then A1B1 and B2A2 simultaneously parse as Z.\nSomewhat surprisingly, this inversion production rule can be described using the alternate allowable way of merging the premises when the PRODUCT transformation is performed on two copies of the CKY algorithm, as discussed in \u00a75.2 (see rule 50). By adding this alternate form as given in Figure 31, we can describe the algorithm for parsing with inversion transduction grammars described by Wu (1997)."}, {"heading": "7 The Entropy Semiring and Kullback-Leibler Divergence", "text": "An important construct in information theory and machine learning is the KullbackLeibler (KL) divergence (Kullback and Leibler 1951). KL divergence is a function of two probability distributions over the same event space. It measures their dissimilarity, though it is not, strictly speaking, a distance (it is not symmetric). For two distributions p and q for random variable X ranging over events x \u2208 X , KL divergence is defined as\nKL(p\u2016q) = \u2211 x\u2208X p(X = x) log p(X = x) q(X = x) (79)\n= \u2211 x\u2208X\np(X = x) log p(X = x)\ufe38 \ufe37\ufe37 \ufe38 \u2212H(p)\n\u2212 \u2211 x\u2208X\np(X = x) log q(X = x)\ufe38 \ufe37\ufe37 \ufe38 CE(p\u2016q) (80)\nwhere H(p) denotes the Shannon entropy of the distribution p (Shannon 1948), a measure of uncertainty, and CE(p\u2016q) denotes the cross-entropy between p and q.8 A full discussion of these information-theoretic quantities is out of scope for this paper; we note that they are widely used in statistical machine learning (Koller\n8 In brief, the Shannon entropy of distribution p is the expected number of bits required to send a message drawn according to p under an optimal coding scheme. Cross-entropy is the average number of bits required to encode a message in the optimal coding scheme for q when messages are actually distributed according to p. Hence KL(p\u2016q) = CE(p\u2016q)\u2212H(p) is the average number of extra bits required when the true distribution of messages is p but the coding scheme is based on q. Note that KL(p\u2016p) = 0. If there is an event x \u2208 X such that p(x) > 0 and q(x) = 0, then KL(p\u2016q) = +\u221e.\nand Friedman 2009). In this section, we first show how the entropy of p(P ), with P ranging over proofs of goal (the axioms corresponding to random variables A and I are suppressed here, for clarity), can be calculated using a weighted logic program, following Hwa (2004). We then describe a generalization of a result of Cortes et al. (2006) to show how to use PRODUCT to produce a weighted logic program for calculating the KL divergence between the two distributions induced by the WLPs."}, {"heading": "7.1 Generalized Entropy Semiring", "text": "The domain of the generalized entropy semiring is (R \u222a {+\u221e,\u2212\u221e})3. The multiplication and addition operations are defined as follows:\n\u3008x1, y1, z1\u3009 \u2295 \u3008x2, y2, z2\u3009 = \u3008x1 + x2, y1 + y2, z1 + z2\u3009 (81) \u3008x1, y1, z1\u3009 \u2297 \u3008x2, y2, z2\u3009 = \u3008x1x2, x1y2 + x2y1, z1z2\u3009 (82)\nThese operations have the required closure, associativity, and commutativity properties previously discussed for semirings. See Cortes et al. (2006) for a proof which can be extended trivially to our generalized semiring.\nSuppose we have a weighted logic program such that the path-sum (in the \u3008R\u22650\u222a {\u221e},+,\u00d7, 0, 1\u3009 semiring) is 1 (i.e., the value of the goal theorem is 1). If we map the weights of all axioms in the original program to new values in the generalized entropy semiring, we can use the new semiring to calculate the Shannon entropy of the distribution over proofs of goal:\n\u2212 \u2211 proof p(P = proof ) log p(P = proof ) (83)\nwhere x ranges over proofs of goal. The mapping is simply w 7\u2192 \u3008w,\u2212w logw, 0\u3009. (The third element of the semiring value is not needed here.) If we solve the new weighted logic program and achieve value \u3008w\u2032, h\u2032, 0\u3009 for the goal theorem, then under our assumption that w\u2032 = 1 (the value of goal in the original program in the real semiring), h\u2032 is the entropy of the distribution over the proof random variable (given the axioms and goal). The formal result is given as a corollary in \u00a77.2. This semiring can be used, for example, with the CKY algorithm from Figure 19. It makes the derivation of the tree entropy for context-free grammars (i.e., the entropy over the context-free derivations for an ambiguous string) automatic, and obviates the design of a specific algorithm for computing the tree entropy for probabilistic context-free grammars, as described in Hwa (2004). With the CKY algorithm, a proof proof in Eq. 83 represents a derivation in the grammar. Similarly, a weighted logic program describing a finite-state transducer (Figure 16) can be used to compute the entropy of hidden sequences for hidden Markov models as described by Hernando et al. (2005).\nWe now relax the assumption that the sum of all proof scores is 1. Suppose that the value of the goal theorem in the generalized entropy semiring is (w\u2032, h\u2032, 0), with w\u2032 6= 1. In this case, h\u2032 is not the entropy of a proper probability distribution.\nWe can renormalize the scores of the proofs, u(proof ), by dividing by w\u2032, treating them as a proper conditional distribution (conditioning on the truth of the goal theorem); then the entropy of this conditional distribution, u(proof )\nw\u2032 , is\n\u2212 \u2211 proof u(proof ) w\u2032 log u(proof ) w\u2032 (84)\n= 1\nw\u2032 \u2212\u2211 proof u(proof )(log u(proof )\u2212 logw\u2032)  = 1\nw\u2032 h\u2032 + (logw\u2032) \u2211 proof u(proof )  = 1 w\u2032 (h\u2032 + w\u2032 logw\u2032) = h\u2032 w\u2032 + logw\u2032\nTherefore, whenever we can use weighted logic programming (in the real semiring) to calculate sums of proof scores, we can use the generalized entropy semiring to find the Shannon entropy of the (possibly renormalized) distribution over proofs. The renormalization uses w\u2032 and h\u2032, two quantities that are calculated directly when we use the generalized entropy semiring.\n7.2 KL Divergence Between Proof Distributions and PRODUCT\nCortes et al. (2006) showed how to compute the KL divergence (also called relative entropy) between two distributions over strings defined by probabilistic FSA, using a construct similar to our generalized entropy semiring. We generalize that result to KL divergence over two proof distributions p(P ) and q(P ) given by a weighted logic program P. We assume in this discussion that the set of axioms with non-zero weights are identical under p and under q; the general setting where this does not hold is correctly handled, using a log 0 = \u2212\u221e and 0 log a = 0 for all a > 0. We abuse notation slightly and use p and q to denote the values of axioms, theorems, and proofs in the real semiring weighted logic programs used to calculate the sum of proof-scores for goal under axioms weighted according to p and q. Let Proofs(t) denote the set of logical proofs of a theorem t, and for x \u2208 Proofs(t), let p(t)\u2014respectively, q(t)\u2014denote the score of the proof x:\np(t) = \u2211\nx\u2208Proofs(t)\np(x) (85)\nq(t) = \u2211\nx\u2208Proofs(t)\nq(x) (86)\n(87)\nWe seek the KL divergence:\nKL(p\u2016q) = \u2211\nx\u2208Proofs(goal)\np(x) log p(x)\nq(x) (88)\nIn order to accomplish this calculation, we will first map the weights of axioms\nunder p and q into the generalized entropy semiring as follows, for any axiom a:\n\u3008p(a), q(a)\u3009 7\u2192 \u3008p(a), p(a) log q(a), q(a)\u3009 (89)\nFor a theorem t, let\nR(t) = \u2211\nx\u2208Proofs(t)\np(x) log q(x) (90)\nTheorem 2 Solving P in the generalized entropy semiring with weights defined as above results in goal having value \u3008p(goal), R(goal), q(goal)\u3009.\nProof: We will treat the weighted logic program as a set of equations with all lefthand-side variables grounded. We will use upper-case to refer to free variables (e.g., Z = \u3008Z1, . . .\u3009) and lower-case to refer to grounded values (e.g., z = \u3008z1, . . .\u3009). The range of values that variables Z can get is denoted by Rng(Z). The weighted logic program can be seen as a set of equations:\nc(w) = \u2295\n[c(w)\u2295= ai(w\u2032,Z)\u2297bi(w\u2032\u2032,Z)]\u2208P,w\u2032\u2286w,w\u2032\u2032\u2286w \u2295 z\u2208Rng(Z) ai(w \u2032, z)\u2297 bi(w\u2032\u2032, z) (91)\n(Note that any of w, w\u2032, w\u2032\u2032, and z may be empty.)\nWe now show that the value achieved for c(w) when solving in the semiring is \u3008p(c(w)), \u2211\nx\u2208Proofs(c(w))\np(x) log q(x), q(c(w))\u3009 (92)\nwhere Proofs(c(w)) denotes the set of proofs for c(w). We will show that the solution of Equations 91 is the value in Equation 92 for c(w).\nFor the first and third coordinates, this equality follows naturally because of the definition of the generalized entropy semiring: the first and third coordinates are equivalent to the non-negative real semiring used for summing over proof scores under the two value assignments p and q, respectively.\nConsider a particular \u2295-addend to the value of c(w),\nai(w \u2032, z)\u2297 bi(w\u2032\u2032, z) (93)\n= \u3008p(ai(w\u2032, z)), R(ai(w\u2032, z)), q(ai(w\u2032, z))\u3009 \u2297 \u3008p(bi(w\u2032\u2032, z)), R(bi(w\u2032\u2032, z)), q(bi(w\u2032\u2032, z))\u3009 (94)\n= \u2329 p(ai(w\u2032, z))p(bi(w\u2032\u2032, z)), p(ai(w \u2032, z))R(bi(w \u2032\u2032, z)) + p(bi(w \u2032\u2032, z))R(ai(w \u2032, z)),\nq(ai(w \u2032, z)))q(bi(w \u2032\u2032, z))\n\u232a (95)\nConsider the second coordinate.\np(ai(w \u2032, z))R(bi(w \u2032\u2032, z)) + p(bi(w \u2032\u2032, z))R(ai(w \u2032, z)) (96)\n= p(ai(w\u2032, z)) \u2211 x\u2208Proofs(bi(w\u2032\u2032,z)) p(x) log q(x) \nProducts of Weighted Logic ProgramsTo appear in Theory and Practice of Logic Programming (TPLP).31\n+ p(bi(w\u2032\u2032, z)) \u2211 x\u2032\u2208Proofs(ai(w\u2032,z)) p(x\u2032) log q(x\u2032)  (97) =\n \u2211 x\u2032\u2208Proofs(ai(w\u2032,z)) p(x\u2032) \u2211 x\u2208Proofs(bi(w\u2032\u2032,z)) p(x) log q(x)  +\n \u2211 x\u2208Proofs(bi(w\u2032\u2032,z)) p(x) \u2211 x\u2032\u2208Proofs(ai(w\u2032,z)) p(x\u2032) log q(x\u2032)  (98) =\n\u2211 x\u2208Proofs(bi(w\u2032\u2032,z)) \u2211 x\u2032\u2208Proofs(ai(w\u2032,z)) p(x)p(x\u2032) log(q(x)q(x\u2032)) (99)\nEmbedding the above in a \u2295-summation over z and a \u2295-summation over inference rule instantiations gives a \u2295-summation over proofs of c(w),\u2211\nx\u2208Proofs(c(w))\np(x) log q(x) (100)\nwhich is R(c(w)) as desired. Denote by (p\u0304, R\u0304, q\u0304) the value for goal in the generalized entropy semiring as discussed above, i.e., p\u0304 = p(goal), R\u0304 = R(goal), and q\u0304 = q(goal). If we wish to renormalize p by p\u0304 and q by q\u0304 to give proper distributions over proofs of goal (given axioms and goal), then\nCE\n( 1\np\u0304 p \u2225\u2225\u2225\u2225 1q\u0304 q ) = R\u0304 p\u0304 \u2212 log q\u0304 (101)\nNoting that \u2212H(p) = CE(p\u2016p),\nKL\n( 1\np\u0304 p \u2225\u2225\u2225\u2225 1q\u0304 q ) = CE(p\u2016p)\u2212 CE(p\u2016q) (102)\nwe can solve for the KL divergence of two (possibly renormalized) distributions p and q using the above results. Alternatively, if the generalized KL divergence between unnormalized distributions is preferred (O\u2019Sullivan 1998), note that (in the notation of the above):\u2211 x\u2208Proofs(goal) ( p(x) log p(x) q(x) \u2212 p(x) + q(x) ) = R\u0304\u2212 p\u0304+ q\u0304 (103)\nCortes et al. describe how to compute KL divergence between two probabilistic finite-state automata with a single path per string (\u201cunambiguous\u201d automata). The authors make use of finite-state intersection (discussed above in \u00a75.1). This suggests an analogous interpretation of the PRODUCT transformation for computing KL divergence between two weighted logic programs.\nLet P and Q be two instances of a weighted logic program, with possibly different different axiom weights. Assume we set the values of the axioms of P (ranging over a) to be \u3008p(a), 0, 1\u3009, and for Q we set them to \u30081, log q(a), q(a)\u3009. If we take a PRODUCT of P and Q, using the \u201cnatural\u201d pairing, then we end up with a program that computes \u3008p(goal), R(goal), q(goal)\u3009 in the generalized entropy semiring, where\nR(\u00b7) is specified in Eq. 90. These quantities can be used to compute KL divergence as specified in Eq. 102. This is a direct result of Theorem 2."}, {"heading": "7.3 KL Divergence and Projections", "text": "We can use PRODUCT to calculate KL divergence between proof distributions even when P and Q are not two instances of the same program. We consider cases where the proofs of P and the proofs of Q have a shared semantics, that is, each proof of either P or Q maps to an event in some \u201cinterpretation space.\u201d\nAs an example, consider the WLP in Figure 16 describing a weighted finite-state transducer. In a more general formulation, where each state depends on the previous N states visited, rather than just the single most recent state. This modification is reflected in Figure 32 for N = 2. The axiom biarc(P\u2032, Q, P, A, B) is to be interpreted as: \u201cif the last two states were Q and P\u2032, transfer to state P while reading symbol A and emitting the symbol B.\u201d Since the two programs have different axioms, the spaces of their respective proofs are different. However, both programs have identical semantics to a proof: a proof (in either program) corresponds to a sequence of states that the transducers go through together with the reading of a symbol and the emission of another symbol.\nRunning PRODUCT on the WFST in Figure 16 (we call it P) and the WFST in Figure 32 (we call it Q) with a particular pairing and constraints (such that the paths are identical) yields the program in Figure 33. If we let the axioms a in P have the values \u3008p(a), 0, 1\u3009 and the axioms a in Q the values \u30081, log q(a), q(a)\u3009, then the resulting PRODUCT program in Figure 33, as implied by Theorem 2, calculates the KL divergence between two distributions over the set of state paths: one which is defined using a finite-state transducer with N = 1 and the other with N = 2.\nWe now generalize this idea for two different programs P and Q. We assume\nthat PRODUCT is applied in such a way that axioms from P are paired only with axioms from Q, and vice versa. Further, each proof in the PRODUCT program must decompose into exactly one proof in P and one proof in Q.9 For a proof in the PRODUCT program, y, we define \u03c0P(y) (\u03c0Q(y)) to be the projection of y to a proof in P (Q). The \u201cprojection\u201d of a proof is a separation of the proof which uses coupled theorems and axioms into theorems and axioms of only one of the programs. For example, projecting a proof y in the product program in Figure 33 yields two proofs: \u03c0P(y) describes a sequence of transitions through the transducer with N = 1 and \u03c0Q(y) describes a sequence of transitions through the transducer with N = 2; yet both proofs correspond to the same sequence of states.\nIn the generalized entropy semiring, we set the values of the axioms of P to be \u3008p(a), 0, 1\u3009, and for Q we set them to \u30081, log q(a), q(a)\u3009. The PRODUCT program computes \u3008p(goal1), R(goal1\u00b72), q(goal2)\u3009. This time, the summation in R(goal1\u00b72) is over proofs which are implicitly paired:\nR(goal1\u00b72) = \u2211\ny\u2208Proofs(goal1\u00b72)\np(\u03c0P(y)) log q(\u03c0Q(y)) (110)\nThe quantities p(goal1), R(goal1\u00b72), and q(goal2) can be used as before to compute the KL divergence between the distributions over the shared \u201cinterpretation space\u201d of the proofs in the two programs. This technique is only correct when interpretations are in a one-to-one correspondence with the proofs in P and with the proofs in Q, and PRODUCT is applied so that equivalently-interpretable proofs in the two programs are paired.\nWe note that in the general case, the problem of computing KL divergence between two arbitrary distributions is hard. For example, with Markov networks, there are restrictions, which resemble the restrictions we pose, of clique decomposition (Koller and Friedman 2009)."}, {"heading": "8 Conclusion", "text": "We have described a framework for dynamic programming algorithms whose solutions correspond to proof values in two constrained weighted logic programs. Our framework includes a program transformation, PRODUCT, which combines the two weighted logic programs that compute over two structures into a single weighted logic program for a joint proof. Appropriate constraints, encoded intuitively as variable unification or side conditions in the weighted logic program, are then added manually. The framework naturally captures and permits generalization of many existing algorithms. We have shown how variations on the the program transformation enable to include a larger set of algorithms as the result of the program transformation. We have concluded by showing how the program transformation\n9 Note that these constraints are satisfied in the case of two identical programs with the \u201cnatural\u201d pairing, as in \u00a77.2.\ncan be used to interpret the computation of Kullback-Leibler divergence for two weighted logic programs which are defined over an identical interpretation space."}, {"heading": "Acknowledgments", "text": "The authors acknowledge helpful comments from the two anonymous reviewers, Jason Eisner, Rebecca Hwa, Adam Lopez, Alberto Pettorossi, Frank Pfenning, Sylvia Rebholz and David Smith. This research was supported by an NSF graduate fellowship to the second author and NSF grant IIS-0713265 and an IBM faculty award to the third author."}], "references": [], "referenceMentions": [], "year": 2010, "abstractText": "Weighted logic programming, a generalization of bottom-up logic programming, is a wellsuited framework for specifying dynamic programming algorithms. In this setting, proofs correspond to the algorithm\u2019s output space, such as a path through a graph or a grammatical derivation, and are given a real-valued score (often interpreted as a probability) that depends on the real weights of the base axioms used in the proof. The desired output is a function over all possible proofs, such as a sum of scores or an optimal score. We describe the PRODUCT transformation, which can merge two weighted logic programs into a new one. The resulting program optimizes a product of proof scores from the original programs, constituting a scoring function known in machine learning as a \u201cproduct of experts.\u201d Through the addition of intuitive constraining side conditions, we show that several important dynamic programming algorithms can be derived by applying PRODUCT to weighted logic programs corresponding to simpler weighted logic programs. In addition, we show how the computation of Kullback-Leibler divergence, an information-theoretic measure, can be interpreted using PRODUCT.", "creator": "LaTeX with hyperref package"}}}