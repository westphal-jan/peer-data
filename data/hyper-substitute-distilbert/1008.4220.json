{"id": "1008.4220", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2010", "title": "Structured sparsity-inducing norms through submodular functions", "abstract": "sparse methods for linear operations function at finding reliable linear predictors sized as two variables as many, i. her., with small cardinality towards their supports. a consistent type problem reduces often turned : a convex optimization problem by rendering the random argument following its convex envelope ( tightest approximation lower bound ), in complexity problem the l1 - norm. in this paper, we request finer general domain - analytic than the cardinality, this may incorporate prior reconstruction or robust matching which remains important in similar tasks : namely, we say that for nonincreasing linear set - analyses, the theoretical implicit boundary can be assembled from classical lovasz theorem, enabling common p. functional analysis. t allows minimal family of polyhedral norms, for which we provide generic algorithmic expressions ( subgradients termed proximal rules ) and theoretical results ( conditions for support constraint or high - dimensional inference ). despite selecting specific submodular networks, we invariably apply a broad expression identifying known norms, such efficient models occurring on rank - statistics or examining them with non overlapping groups ; wilkinson also introduced new regions, permitting modern algorithms since instead be used as non - factorial priors for supervised learning.", "histories": [["v1", "Wed, 25 Aug 2010 07:28:08 GMT  (123kb)", "https://arxiv.org/abs/1008.4220v1", null], ["v2", "Wed, 22 Sep 2010 03:11:25 GMT  (123kb)", "http://arxiv.org/abs/1008.4220v2", null], ["v3", "Fri, 12 Nov 2010 14:51:23 GMT  (123kb)", "http://arxiv.org/abs/1008.4220v3", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["francis r bach"], "accepted": true, "id": "1008.4220"}, "pdf": {"name": "1008.4220.pdf", "metadata": {"source": "CRF", "title": "Structured sparsity-inducing norms through submodular functions", "authors": ["Francis Bach"], "emails": ["francis.bach@ens.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 8.\n42 20\nv3 [\ncs .L\nG ]"}, {"heading": "1 Introduction", "text": "The concept of parsimony is central in many scientific domains. In the context of statistics, signal processing or machine learning, it takes the form of variable or feature selection problems, and is commonly used in two situations: First, to make the model or the prediction more interpretable or cheaper to use, i.e., even if the underlying problem does not admit sparse solutions, one looks for the best sparse approximation. Second, sparsity can also be used given prior knowledge that the model should be sparse. In these two situations, reducing parsimony to finding models with low cardinality turns out to be limiting, and structured parsimony has emerged as a fruitful practical extension, with applications to image processing, text processing or bioinformatics (see, e.g., [1, 2, 3, 4, 5, 6, 7] and Section 4). For example, in [4], structured sparsity is used to encode prior knowledge regarding network relationship between genes, while in [6], it is used as an alternative to structured nonparametric Bayesian process based priors for topic models.\nMost of the work based on convex optimization and the design of dedicated sparsity-inducing norms has focused mainly on the specific allowed set of sparsity patterns [1, 2, 4, 6]: if w \u2208 Rp denotes the predictor we aim to estimate, and Supp(w) denotes its support, then these norms are designed so that\npenalizing with these norms only leads to supports from a given family of allowed patterns. In this paper, we instead follow the approach of [8, 3] and consider specific penalty functions F (Supp(w)) of the support set, which go beyond the cardinality function, but are not limited or designed to only forbid certain sparsity patterns. As shown in Section 6.2, these may also lead to restricted sets of supports but their interpretation in terms of an explicit penalty on the support leads to additional insights into the behavior of structured sparsity-inducing norms (see, e.g., Section 4.1). While direct greedy approaches (i.e., forward selection) to the problem are considered in [8, 3], we provide convex relaxations to the function w 7\u2192 F (Supp(w)), which extend the traditional link between the \u21131-norm and the cardinality function.\nThis is done for a particular ensemble of set-functions F , namely nondecreasing submodular functions. Submodular functions may be seen as the set-function equivalent of convex functions, and exhibit many interesting properties that we review in Section 2\u2014see [9] for a tutorial on submodular analysis and [10, 11] for other applications to machine learning. This paper makes the following contributions:\n\u2212We make explicit links between submodularity and sparsity by showing that the convex envelope of the function w 7\u2192 F (Supp(w)) on the \u2113\u221e-ball may be readily obtained from the Lova\u0301sz extension of the submodular function (Section 3).\n\u2212 We provide generic algorithmic tools, i.e., subgradients and proximal operators (Section 5), as well as theoretical guarantees, i.e., conditions for support recovery or high-dimensional inference (Section 6), that extend classical results for the \u21131-norm and show that many norms may be tackled by the exact same analysis and algorithms.\n\u2212 By selecting specific submodular functions in Section 4, we recover and give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups [1, 2, 7], and we define new norms, in particular ones that can be used as non-factorial priors for supervised learning (Section 4). These are illustrated on simulation experiments in Section 7, where they outperform related greedy approaches [3].\nNotation. For w \u2208 Rp, Supp(w) \u2282 V = {1, . . . , p} denotes the support of w, defined as Supp(w) = {j \u2208 V, wj 6= 0}. For w \u2208 Rp and q \u2208 [1,\u221e], we denote by \u2016w\u2016q the \u2113q-norm of w. We denote by |w| \u2208 Rp the vector of absolute values of the components of w. Moreover, given a vector w and a matrix Q, wA and QAA are the corresponding subvector and submatrix of w and Q. Finally, for w \u2208 Rp and A \u2282 V , w(A) = \u2211k\u2208A wk (this defines a modular set-function)."}, {"heading": "2 Review of submodular function theory", "text": "Throughout this paper, we consider a nondecreasing submodular function F defined on the power set 2V of V = {1, . . . , p}, i.e., such that:\n\u2200A,B \u2282 V, F (A) + F (B) > F (A \u222aB) + F (A \u2229B), (submodularity) \u2200A,B \u2282 V, A \u2282 B \u21d2 F (A) 6 F (B). (monotonicity)\nMoreover, we assume (without loss of generality) that F (\u2205) = 0. These set-functions are often referred to as polymatroid set-functions [12] or \u03b2-functions [13]. Also, without loss of generality, we may assume that F is strictly positive on singletons, i.e., for all k \u2208 V , F ({k}) > 0. Indeed, if F ({k}) = 0, then by submodularity and monotonicity, if A \u220b k, F (A) = F (A\\{k}) and thus we can simply consider V \\{k} instead of V . Classical examples are the cardinality function (which will lead to the \u21131-norm) and, given a partition of V into B1 \u222a \u00b7 \u00b7 \u00b7 \u222a Bk = V , the set function A 7\u2192 F (A) which is equal to the number of groups B1, . . . , Bk with non empty intersection with A (which will lead to the grouped \u21131/\u2113\u221e-norm [1, 14]).\nLova\u0301sz extension. Given any set-function F , one can define its Lova\u0301sz extension [15] (a.k.a. Choquet integral [16]) f : Rp+ \u2192 R, as follows: given w \u2208 Rp+, we can order the components of w in decreasing order wj1 > \u00b7 \u00b7 \u00b7 > wjp > 0; the value f(w) is then defined as:\nf(w) =\np \u2211\nk=1\nwjk [F ({j1, . . . , jk})\u2212 F ({j1, . . . , jk\u22121})]. (1)\nNote that if some of the components of w are equal, all orderings lead to the same value of f(w). The Lova\u0301sz extension f is always piecewise-linear, and when F is submodular, it is also convex (see, e.g., [15, 12]). Moreover, for all \u03b4 \u2208 {0, 1}p, f(\u03b4) = F (Supp(\u03b4)): f is indeed an extension from vectors in {0, 1}p (which can be identified with indicator vectors of sets) to all vectors in Rp+. Moreover, it turns out that minimizing F over subsets, i.e., minimizing f over {0, 1}p is equivalent to minimizing f over [0, 1]p [15, 13].\nSubmodular polyhedron and greedy algorithm. We denote by P the submodular polyhedron [12], defined as the set of s \u2208 Rp+ such that for all A \u2282 V , s(A) 6 F (A), i.e., P = {s \u2208 R\np +, \u2200A \u2282 V, s(A) 6 F (A)}, where we use the notation s(A) = \u2211\nk\u2208A sk. One important result in submodular analysis is that if F is a nondecreasing submodular function, then we have a representation of f as a maximum of linear functions [12, 15], i.e., for all w \u2208 Rp+,\nf(w) = max s\u2208P\nw\u22a4s. (2)\nInstead of solving a linear program with p + 2p contraints, a solution s may be obtained by the following \u201cgreedy algorithm\u201d: order the components of w in decreasing order wj1 > \u00b7 \u00b7 \u00b7 > wjp , and then take for all k \u2208 {1, . . . , p}, sjk = F ({j1, . . . , jk})\u2212 F ({j1, . . . , jk\u22121}). Stable sets. A set A is said stable if it cannot be augmented without increasing F , i.e., if for all sets B \u2283 A, B 6= A \u21d2 F (B) > F (A). If F is strictly increasing, then all sets are stable. Stable sets are also sometimes referred to as flat or closed [13]. The set of stable sets is closed by intersection [13], and will correspond to the set of allowed sparsity patterns (see Section 6.2). For the cardinality function, all sets are stable.\nSeparable sets. A set A is separable if we can find a partition of A into A = B1 \u222a \u00b7 \u00b7 \u00b7 \u222a Bk such that F (A) = F (B1) + \u00b7 \u00b7 \u00b7 + F (Bk). A set A is inseparable if it is not separable. As shown in [13], the submodular polytope P has full dimension p as soon as F is strictly positive on all singletons, and its faces are exactly the sets {sk = 0} for k \u2208 V and {s(A) = F (A)} for stable and inseparable sets. We let denote T the set of such sets. This implies that P = {s \u2208 Rp+, \u2200A \u2208 T , s(A) 6 F (A)}. These stable inseparable sets will play a role when describing extreme points of unit balls of our new norms (Section 3) and for deriving concentration inequalities in Section 6.3. For the cardinality function, stable and inseparable sets are singletons.\nSubmodular function minimization. Submodular functions are particularly interesting because they can be minimized in polynomial time. In this paragraph, we consider a non-monotonic submodular function G (otherwise finding the minimum is trivial). Most algorithms for minimizing submodular functions rely on the following strong duality principle [13, 12]:\nmin A\u2282V G(A) = max s\u2208B(G)\n\u2211 k\u2208V min{0, sk}, (3)\nwhere B(G) = {s \u2208 Rp, \u2200A \u2282 V, s(A) 6 G(A), s(V ) = G(V )} is referred to as the base polyhedron. Moreover, algorithms for minimizing G will usually output A and s such that G(A) = \u2211\nk\u2208V min{0, sk} as a certificate for optimality. The two main types of algorithms are combinatorial algorithms (that explicitly looks for A) and ones based on convex optimization (that explicitly\nlooks for s). The first type of algorithm leads to strongly polynomial algorithms with best known complexity O(p6) [17], while the minimum point algorithm of [12] has no worst-time complexity bounds but is usually much faster in practice [12] and is based on the equivalent problem of finding the minimum-norm point in B(G), i.e., mins\u2208B(G) \u2016s\u201622. Note that in this case, the minimum point algorithm also outputs a particular s solution of Eq. (3)\u2014which has several solutions in general."}, {"heading": "3 Definition and properties of structured norms", "text": "We define the function \u2126(w) = f(|w|), where |w| is the vector in Rp composed of absolute values of w and f the Lova\u0301sz extension of F . We have the following properties (see proof in the appendix), which show that we indeed define a norm and that it is the desired convex envelope:\nProposition 1 (Convex envelope, dual norm) Assume that the set-function F is submodular, nondecreasing, and strictly positive for all singletons. Define \u2126 : w 7\u2192 f(|w|). Then: (i) \u2126 is a norm on Rp,\n(ii) \u2126 is the convex envelope of the function g : w 7\u2192 F (Supp(w)) on the unit \u2113\u221e-ball,\n(iii) the dual norm (see, e.g., [18]) of \u2126 is equal to \u2126\u2217(s) = maxA\u2282V \u2016sA\u20161 F (A) = maxA\u2208T \u2016sA\u20161 F (A) .\nWe provide examples of submodular set-functions and norms in Section 4, where we go from setfunctions to norms, and vice-versa. From the definition of the Lova\u0301sz extension in Eq. (1), we see that \u2126 is a polyhedral norm (i.e., its unit ball is a polyhedron). The following proposition gives the set of extreme points of the unit ball (see proof in the appendix and examples in Figure 1):\nProposition 2 (Extreme points of unit ball) The extreme points of the unit ball of \u2126 are the vectors 1F (A)s, with s \u2208 {\u22121, 0, 1}p, Supp(s) = A and A a stable inseparable set.\nThis proposition shows, that depending on the number and cardinality of the inseparable stable sets, we can go from 2p (only singletons) to 3p \u2212 1 extreme points (all possible sign vectors). We show in Figure 1 examples of balls for p = 2, as well as sets of extreme points. These extreme points will play a role in concentration inequalities derived in Section 6."}, {"heading": "4 Examples of nondecreasing submodular functions", "text": "We consider three main types of submodular functions with potential applications to regularization for supervised learning. Some existing norms are shown to be examples of our frameworks (Section 4.1, Section 4.3), while other novel norms are designed from specific submodular functions (Section 4.2). Other examples of submodular functions, in particular in terms of matroids and entropies, may be found in [12, 10, 11] and could also lead to interesting new norms. Note that set covers, which are common examples of submodular functions are subcases of set-functions defined in Section 4.1 (see, e.g., [9])."}, {"heading": "4.1 Norms defined with non-overlapping or overlapping groups", "text": "We consider grouped norms defined with potentially overlapping groups [1, 2], i.e., \u2126(w) = \u2211 G\u2282V d(G)\u2016wG\u2016\u221e where d is a nonnegative set-function (with potentially d(G) = 0 when G should not be considered in the norm). It is a norm as soon as \u222aG,d(G)>0G = V and it corresponds to the nondecreasing submodular function F (A) = \u2211\nG\u2229A 6=\u2205 d(G). In the case where \u2113\u221e-norms are replaced by \u21132-norms, [2] has shown that the set of allowed sparsity patterns are intersections of complements of groups G with strictly positive weights. These sets happen to be the set of stable sets for the corresponding submodular function; thus the analysis provided in Section 6.2 extends the result of [2] to the new case of \u2113\u221e-norms. However, in our situation, we can give a reinterpretation through a submodular function that counts the number of times the support A intersects groups G with non zero weights. This goes beyond restricting the set of allowed sparsity patterns to stable sets. We show later in this section some insights gained by this reinterpretation. We now give some examples of norms, with various topologies of groups.\nHierarchical norms. Hierarchical norms defined on directed acyclic graphs [1, 5, 6] correspond to the set-function F (A) which is the cardinality of the union of ancestors of elements in A. These have been applied to bioinformatics [5], computer vision and topic models [6].\nNorms defined on grids. If we assume that the p variables are organized in a 1D, 2D or 3D grid, [2] considers norms based on overlapping groups leading to stable sets equal to rectangular or convex shapes, with applications in computer vision [19]. For example, for the groups defined in the left side of Figure 2 (with unit weights), we have F (A) = p \u2212 2 + range(A) if A 6= \u2205 and F (\u2205) = 0 (the range of A is equal to max(A) \u2212min(A) + 1). From empty sets to non-empty sets, there is a gap of p \u2212 1, which is larger than differences among non-empty sets. This leads to the undesired result, which has been already observed by [2], of adding all variables in one step, rather than gradually, when the regularization parameter decreases in a regularized optimization problem. In order to counterbalance this effect, adding a constant times the cardinality function has the effect of making the first gap relatively smaller. This corresponds to adding a constant times the \u21131-norm and, as shown in Figure 3, solves the problem of having all variables coming together. All patterns are then allowed, but contiguous ones are encouraged rather than forced.\nAnother interesting new norm may be defined from the groups in the right side of Figure 2. Indeed, it corresponds to the function F (A) equal to |A| plus the number of intervals of A. Note that this also favors contiguous patterns but is not limited to selecting a single interval (like the norm obtained from groups in the left side of Figure 2). Note that it is to be contrasted with the total variation (a.k.a. fused Lasso penalty [20]), which is a relaxation of the number of jumps in a vector w rather than in its support. In 2D or 3D, this extends to the notion of perimeter and area, but we do not pursue such extensions here."}, {"heading": "4.2 Spectral functions of submatrices", "text": "Given a positive semidefinite matrix Q \u2208 Rp\u00d7p and a real-valued function h from R+ \u2192 R, one may define tr[h(Q)] as\n\u2211p i=1 h(\u03bbi) where \u03bb1, . . . , \u03bbp are the (nonnegative) eigenvalues of Q [21]. We\ncan thus define the set-function F (A) = trh(QAA) for A \u2282 V . The functions h(\u03bb) = log(\u03bb + t) for t > 0 lead to submodular functions, as they correspond to entropies of Gaussian random variables (see, e.g., [12, 9]). Thus, since for q \u2208 (0, 1), \u03bbq = q sin q\u03c0\u03c0 \u222b\u221e 0\nlog(1 + \u03bb/t)tq\u22121dt (see, e.g., [22]), h(\u03bb) = \u03bbq for q \u2208 (0, 1] are positive linear combinations of functions that lead to nondecreasing submodular functions. Thus, they are also nondecreasing submodular functions, and, to the best of our knowledge, provide novel examples of such functions.\nIn the context of supervised learning from a design matrix X \u2208 Rn\u00d7p, we naturally use Q = X\u22a4X . If h is linear, then F (A) = trX\u22a4AXA = \u2211 k\u2208A X \u22a4 k Xk (where XA denotes the submatrix of X with columns in A) and we obtain a weighted cardinality function and hence and a weighted \u21131-norm, which is a factorial prior, i.e., it is a sum of terms depending on each variable independently.\nIn a frequentist setting, the Mallows CL penalty [23] depends on the degrees of freedom, of the form trX\u22a4AXA(X \u22a4 AXA + \u03bbI)\n\u22121. This is a non-factorial prior but unfortunately it does not lead to a submodular function. In a Bayesian context however, it is shown by [24] that penalties of the form log det(X\u22a4AXA + \u03bbI) (which lead to submodular functions) correspond to marginal likelihoods associated to the set A and have good behavior when used within a non-convex framework. This highlights the need for non-factorial priors which are sub-linear functions of the eigenvalues ofX\u22a4AXA, which is exactly what nondecreasing submodular function of submatrices are. We do not pursue the extensive evaluation of non-factorial convex priors in this paper but provide in simulations examples with F (A) = tr(X\u22a4AXA) 1/2 (which is equal to the trace norm of XA [18])."}, {"heading": "4.3 Functions of cardinality", "text": "For F (A) = h(|A|) where h is nondecreasing, such that h(0) = 0 and concave, then, from Eq. (1), \u2126(w) is defined from the rank statistics of |w| \u2208 Rp+, i.e., if |w(1)| > |w(2)| > \u00b7 \u00b7 \u00b7 > |w(p)|, then\n\u2126(w) = \u2211p k=1[h(k)\u2212h(k\u22121)]|w(k)|. This includes the sum of the q largest elements, and might lead to interesting new norms for unstructured variable selection but this is not pursued here. However, the algorithms and analysis presented in Section 5 and Section 6 apply to this case."}, {"heading": "5 Convex analysis and optimization", "text": "In this section we provide algorithmic tools related to optimization problems based on the regularization by our novel sparsity-inducing norms. Note that since these norms are polyhedral norms with unit balls having potentially an exponential number of vertices or faces, regular linear programming toolboxes may not be used.\nSubgradient. From \u2126(w) = maxs\u2208P s\u22a4|w| and the greedy algorithm1 presented in Section 2, one can easily get in polynomial time one subgradient as one of the maximizers s. This allows to use subgradient descent, with, as shown in Figure 4, slow convergence compared to proximal methods.\nProximal operator. Given regularized problems of the form minw\u2208Rp L(w) + \u03bb\u2126(w), where L is differentiable with Lipschitz-continuous gradient, proximal methods have been shown to be particularly efficient first-order methods (see, e.g., [25]). In this paper, we consider the methods \u201cISTA\u201d and its accelerated variants \u201cFISTA\u201d [25], which are compared in Figure 4.\nTo apply these methods, it suffices to be able to solve efficiently problems of the form: minw\u2208Rp 1 2\u2016w\u2212 z\u201622 + \u03bb\u2126(w). In the case of the \u21131-norm, this reduces to soft thresholding of z, the following proposition (see proof in the appendix) shows that this is equivalent to a particular algorithm for submodular function minimization, namely the minimum-norm-point algorithm, which has no complexity bound but is empirically faster than algorithms with such bounds [12]:\nProposition 3 (Proximal operator) Let z \u2208 Rp and \u03bb > 0, minimizing 12\u2016w \u2212 z\u201622 + \u03bb\u2126(w) is equivalent to finding the minimum of the submodular function A 7\u2192 \u03bbF (A) \u2212 |z|(A) with the minimum-norm-point algorithm.\nIn the proof, it is shown how a solution for one problem may be obtained from a solution to the other problem. Moreover, any algorithm for minimizing submodular functions allows to get directly the support of the unique solution of the proximal problem and that with a sequence of submodular function minimizations, the full solution may also be obtained. Similar links between convex optimization and minimization of submodular functions have been considered (see, e.g., [26]). However, these are dedicated to symmetric submodular functions (such as the ones obtained from graph cuts) and are thus not directly applicable to our situation of non-increasing submodular functions.\nFinally, note that using the minimum-norm-point algorithm leads to a generic algorithm that can be applied to any submodular functions F , and that it may be rather inefficient for simpler subcases (e.g., the \u21131/\u2113\u221e-norm, tree-structured groups [6], or general overlapping groups [7])."}, {"heading": "6 Sparsity-inducing properties", "text": "In this section, we consider a fixed design matrixX \u2208 Rn\u00d7p and y \u2208 Rn a vector of random responses. Given \u03bb > 0, we define w\u0302 as a minimizer of the regularized least-squares cost:\nminw\u2208Rp 1 2n\u2016y \u2212Xw\u201622 + \u03bb\u2126(w). (4)\n1The greedy algorithm to find extreme points of the submodular polyhedron should not be confused with the greedy algorithm (e.g., forward selection) that we consider in Section 7.\nWe study the sparsity-inducing properties of solutions of Eq. (6), i.e., we determine in Section 6.2 which patterns are allowed and in Section 6.3 which sufficient conditions lead to correct estimation. Like recent analysis of sparsity-inducing norms [27], the analysis provided in this section relies heavily on decomposability properties of our norm \u2126."}, {"heading": "6.1 Decomposability", "text": "For a subset J of V , we denote by FJ : 2 J \u2192 R the restriction of F to J , defined for A \u2282 J by FJ (A) = F (A), and by F J : 2J\nc \u2192 R the contraction of F by J , defined for A \u2282 Jc by F J(A) = F (A\u222a J)\u2212F (A). These two functions are submodular and nondecreasing as soon as F is (see, e.g., [12]).\nWe denote by \u2126J the norm on R J defined through the submodular function FJ , and \u2126 J the pseudonorm defined on RJ c\ndefined through F J (as shown in Proposition 4, it is a norm only when J is a stable set). Note that \u2126Jc (a norm on J\nc) is in general different from \u2126J . Moreover, \u2126J(wJ ) is actually equal to \u2126(w\u0303) where w\u0303J = wJ and w\u0303Jc = 0, i.e., it is the restriction of \u2126 to J .\nWe can now prove the following decomposition properties, which show that under certain circumstances, we can decompose the norm \u2126 on subsets J and their complements:\nProposition 4 (Decomposition) Given J \u2282 V and \u2126J and \u2126J defined as above, we have: (i) \u2200w \u2208 Rp, \u2126(w) > \u2126J(wJ ) + \u2126J(wJc), (ii) \u2200w \u2208 Rp, if minj\u2208J |wj | > maxj\u2208Jc |wj | , then \u2126(w) = \u2126J(wJ ) + \u2126J(wJc), (iii) \u2126J is a norm on RJ c if and only if J is a stable set."}, {"heading": "6.2 Sparsity patterns", "text": "In this section, we do not make any assumptions regarding the correct specification of the linear model. We show that with probability one, only stable support sets may be obtained (see proof in the appendix). For simplicity, we assume invertibility of X\u22a4X , which forbids the high-dimensional situation p > n we consider in Section 6.3, but we could consider assumptions similar to the ones used in [2].\nProposition 5 (Stable sparsity patterns) Assume y \u2208 Rn has an absolutely continuous density with respect to the Lebesgue measure and that X\u22a4X is invertible. Then the minimizer w\u0302 of Eq. (6) is unique and, with probability one, its support Supp(w\u0302) is a stable set."}, {"heading": "6.3 High-dimensional inference", "text": "We now assume that the linear model is well-specified and extend results from [28] for sufficient support recovery conditions and from [27] for estimation consistency. As seen in Proposition 4, the norm \u2126 is decomposable and we use this property extensively in this section. We denote by \u03c1(J) = minB\u2282Jc F (B\u222aJ)\u2212F (J)\nF (B) ; by submodularity and monotonicity of F , \u03c1(J) is always between\nzero and one, and, as soon as J is stable it is strictly positive (for the \u21131-norm, \u03c1(J) = 1). Moreover, we denote by c(J) = supw\u2208Rp \u2126J(wJ )/\u2016wJ\u20162, the equivalence constant between the norm \u2126J and the \u21132-norm. We always have c(J) 6 |J |1/2 maxk\u2208V F ({k}) (with equality for the \u21131-norm). The following propositions allow us to get back and extend well-known results for the \u21131-norm, i.e., Propositions 6 and 8 extend results based on support recovery conditions [28]; while Propositions\n7 and 8 extend results based on restricted eigenvalue conditions (see, e.g., [27]). We can also get back results for the \u21131/\u2113\u221e-norm [14]. As shown in the appendix, proof techniques are similar and are adapted through the decomposition properties from Proposition 4.\nProposition 6 (Support recovery) Assume that y = Xw\u2217 + \u03c3\u03b5, where \u03b5 is a standard multivariate normal vector. Let Q = 1nX\n\u22a4X \u2208 Rp\u00d7p. Denote by J the smallest stable set containing the support Supp(w\u2217) of w\u2217. Define \u03bd = minj,w\u2217j 6=0 |w\u2217j | > 0, assume \u03ba = \u03bbmin(QJJ) > 0 and that for \u03b7 > 0, (\u2126J)\u2217[(\u2126J (Q \u22121 JJQJj))j\u2208Jc ] 6 1 \u2212 \u03b7. Then, if \u03bb 6 \u03ba\u03bd2c(J) , the minimizer w\u0302 is unique and has support equal to J , with probability larger than 1\u2212 3P ( \u2126\u2217(z) > \u03bb\u03b7\u03c1(J) \u221a n\n2\u03c3\n)\n, where z is a multivariate normal with covariance matrix Q.\nProposition 7 (Consistency) Assume that y = Xw\u2217 + \u03c3\u03b5, where \u03b5 is a standard multivariate normal vector. Let Q = 1nX\n\u22a4X \u2208 Rp\u00d7p. Denote by J the smallest stable set containing the support Supp(w\u2217) of w\u2217. Assume that for all \u2206 such that \u2126J(\u2206Jc) 6 3\u2126J(\u2206J ), \u2206\u22a4Q\u2206 > \u03ba\u2016\u2206J\u201622. Then we have \u2126(w\u0302 \u2212 w\u2217) 6 24c(J)\n2\u03bb \u03ba\u03c1(J)2 and 1 n\u2016Xw\u0302 \u2212Xw\u2217\u201622 6 36c(J)2\u03bb2 \u03ba\u03c1(J)2 , with probability larger than\n1\u2212 P ( \u2126\u2217(z) > \u03bb\u03c1(J) \u221a n\n2\u03c3\n)\nwhere z is a multivariate normal with covariance matrix Q.\nProposition 8 (Concentration inequalities) Let z be a normal variable with covariance matrix Q. Let T be the set of stable inseparable sets. Then P (\u2126\u2217(z) > t) 6 \u2211\nA\u2208T 2 |A| exp ( \u2212 t 2F (A)2/2 1\u22a4QAA1 ) ."}, {"heading": "7 Experiments", "text": "We provide illustrations on toy examples of some of the results presented in the paper. We consider the regularized least-squares problem of Eq. (6), with data generated as follows: given p, n, k, the design matrix X \u2208 Rn\u00d7p is a matrix of i.i.d. Gaussian components, normalized to have unit \u21132-norm columns. A set J of cardinality k is chosen at random and the weights w\u2217J are sampled from a standard multivariate Gaussian distribution and w\u2217Jc = 0. We then take y = Xw\n\u2217+n\u22121/2\u2016Xw\u2217\u20162 \u03b5 where \u03b5 is a standard Gaussian vector (this corresponds to a unit signal-to-noise ratio).\nProximal methods vs. subgradient descent. For the submodular function F (A) = |A|1/2 (a simple submodular function beyond the cardinality) we compare three optimization algorithms described in Section 5, subgradient descent and two proximal methods, ISTA and its accelerated version FISTA [25], for p = n = 1000, k = 100 and \u03bb = 0.1. Other settings and other set-functions would lead to similar results than the ones presented in Figure 4: FISTA is faster than ISTA, and much faster than subgradient descent.\nRelaxation of combinatorial optimization problem. We compare three strategies for solving the combinatorial optimization problem minw\u2208Rp 1 2n\u2016y \u2212 Xw\u201622 + \u03bbF (Supp(w)) with F (A) = tr(X\u22a4AXA) 1/2, the approach based on our sparsity-inducing norms, the simpler greedy (forward selection) approach proposed in [8, 3], and by thresholding the ordinary least-squares estimate. For all methods, we try all possible regularization parameters. We see in the right plots of Figure 4 that for hard cases (middle plot) convex optimization techniques perform better than other approaches, while for easier cases with more observations (right plot), it does as well as greedy approaches.\nNon factorial priors for variable selection. We now focus on the predictive performance and compare our new norm with F (A) = tr(X\u22a4AXA)\n1/2, with greedy approaches [3] and to regularization by \u21131 or \u21132 norms. As shown in Table 1, the new norm based on non-factorial priors is more robust than the \u21131-norm to lower number of observations n and to larger cardinality of support k."}, {"heading": "8 Conclusions", "text": "We have presented a family of sparsity-inducing norms dedicated to incorporating prior knowledge or structural constraints on the support of linear predictors. We have provided a set of common algorithms and theoretical results, as well as simulations on synthetic examples illustrating the good behavior of these norms. Several avenues are worth investigating: first, we could follow current practice in sparse methods, e.g., by considering related adapted concave penalties to enhance sparsity-inducing norms, or by extending some of the concepts for norms of matrices, with potential applications in matrix factorization or multi-task learning (see, e.g., [29] for application of submodular functions to dictionary learning). Second, links between submodularity and sparsity could be studied further, in particular by considering submodular relaxations of other combinatorial functions, or studying links with other polyhedral norms such as the total variation, which are known to be similarly associated with symmetric submodular set-functions such as graph cuts [26].\nAcknowledgements. This paper was partially supported by the Agence Nationale de la Recherche (MGA Project) and the European Research Council (SIERRA Project). The author would like to thank Edouard Grave, Rodolphe Jenatton, Armand Joulin, Julien Mairal and Guillaume Obozinski for discussions related to this work."}, {"heading": "A Properties of the norm", "text": ""}, {"heading": "A.1 Proof of Proposition 1", "text": "(i) \u2126 is positively homogeneous by definition of the Lova\u0301sz extension in Eq. (1), convex because of the representation in Eq. (2) as the maximum of s\u22a4|w| for some s \u2208 P \u2282 Rp+, and it is a norm as soon as \u2126(w) = 0 implies that w = 0, which is true since \u2126(w) > mink F ({k})\u2016w\u2016\u221e. (ii) We denote by g\u2217 the Fenchel conjugate of g on the domain {w \u2208 Rp, \u2016w\u2016\u221e 6 1}, and g\u2217\u2217 its bidual [18]. By definition of the Fenchel conjugate, we have:\ng\u2217(s) = max \u2016w\u2016\u221e61 w\u22a4s\u2212 g(w)\n= max \u03b4\u2208{0,1}p max \u2016w\u2016\u221e61\n(\u03b4 \u25e6 w)\u22a4s\u2212 f(\u03b4)\n= max \u03b4\u2208{0,1}p\n\u03b4\u22a4|s| \u2212 f(\u03b4)\n= max \u03b4\u2208[0,1]p\n\u03b4\u22a4|s| \u2212 f(\u03b4) because F \u2212 |s| is submodular.\nThus, for all w such that \u2016w\u2016\u221e 6 1,\ng\u2217\u2217(w) = max s\u2208Rp s\u22a4w \u2212 g\u2217(s)\n= max s\u2208Rp min \u03b4\u2208[0,1]p\ns\u22a4w \u2212 \u03b4\u22a4|s|+ f(\u03b4)\n= min \u03b4\u2208[0,1]p max s\u2208Rp\ns\u22a4w \u2212 \u03b4\u22a4|s|+ f(\u03b4) by strong duality and Slater\u2019s condition [18]\n= min \u03b4\u2208[0,1]p,\u03b4>|w|\nf(\u03b4) = f(|w|) because F is nonincreasing.\nNote that F non-increasing implies that f is non-increasing with respect to all of its components. (ii) We have \u2126(w) = f(|w|) = max\ns\u2208P s\u22a4|w| = max |s|\u2208P s\u22a4w = max \u2016sA\u201616F (A), A\u2282V s\u22a4w = max\nmaxA\u2282V \u2016sA\u20161 F (A) 61\ns\u22a4w,\nwhich implies the desired result. Note that the maximization may indeed be limited to the stable inseparable sets A \u2208 T ."}, {"heading": "A.2 Proof of Proposition 2", "text": "We have seen in Section 2 that for A \u2208 T (set of stable inseparable sets), then {x(A) = F (A)} is a face of P (and those sets are the only ones for which this happens). We get to the desired result by considering potential different signs."}, {"heading": "B Convex optimization results", "text": "We first prove an additional result related to decomposition of subdifferentials. Note that the exact subdifferential for the non-zero components of w is rather complicated when w has components with equal magnitude. If this is not the case, i.e., |wj1 | > \u00b7 \u00b7 \u00b7 > |wjk | > 0, where k = |J |, then the subdifferential \u2202\u2126J(wJ ) is reduced to a point s such that sjk = F ({j1, . . . , jk})\u2212F ({j1, . . . , jk\u22121}). For more details on the subdifferential for nonzero components, see [12].\nLemma 1 (Decomposition of subdifferential) Let w \u2208 Rp, with support J = Supp(w) and with H equal to the smallest stable set containing J . The subdifferential \u2202\u2126(w) at w, can then be decomposed as follows on RV = RJ\u00d7RH\\J\u00d7RHc : \u2202\u2126(w) = \u2202\u2126J(wJ )\u00d7{0}\u00d7{sHc, (\u2126H)\u2217(sHc) 6 1}.\nProof For all sufficient small \u2206 \u2208 Rp, the components in (w+\u2206)J have all greater absolute values than the ones in (w + \u2206)Jc . Thus, from Proposition 4, \u2126(w + \u2206) = \u2126J(wJ + \u2206J ) + \u2126\nJ (\u2206Jc) = \u2126J(wJ + \u2206J ) + \u2126\nH(\u2206Hc ), and thus the subdifferential decomposes as \u2202\u2126J(wJ ) \u00d7 {0} \u00d7 \u2202\u2126H(0). The subdifferential of a norm at zero is exactly the unit ball of the dual norm, which leads to the desired result."}, {"heading": "B.1 Proof of Proposition 3", "text": "Following [6], without loss of generality, we assume that z has nonnegative components. We have by convex duality (which is applicable here because of Slater\u2019s condition):\nmin w\u2208Rp\n1 2 \u2016w \u2212 z\u201622 + \u03bb\u2126(w) = min w\u2208Rp max \u2126\u2217(s)61 1 2 \u2016w \u2212 z\u201622 + \u03bbs\u22a4w\n= max \u2126\u2217(s)61 min w\u2208Rp\n1 2 \u2016w \u2212 z\u201622 + \u03bbs\u22a4w\n= max \u2126\u2217(s)61\n1 2 \u2016z\u201622 \u2212 1 2 \u2016\u03bbs\u2212 z\u201622,\nwhere the (unique) optimal w is obtained from the optimal s by w = z\u2212\u03bbs. s is defined constrained to satisfy \u2126\u2217(s) 6 1, which is equivalent to |s| \u2208 P . Since z has nonnegative components, the minimum restricted to |s| \u2208 P is the same as the minimum restricted to s \u2208 P , and also the same as the one restricted to the submodular polyhedron without constraints on positivity, i.e., our problem reduces to min\u2200A\u2282V,s(A)\u2282F (A) \u2016s\u2212 z/\u03bb\u201622, which is also equivalent to\nmin \u2200A\u2282V,t(A)\u2282F (A)\u2212\u03bb\u22121z(A)\n\u2016t\u201622. (5)\nUp to the constraints s(V ) = F (V ) \u2212 \u03bb\u22121z(V ), this is the minimum-norm point problem for the submodular function G : A 7\u2192 F (A) \u2212 \u03bb\u22121z(A). We can then follow two approaches: the first one is to apply directly the minimum-norm point algorithm to the problem in Eq. (5), which we have followed in simulations. The second approach is to consider the regular minimum point algorithm; we can then follow [12, Lemma 7.4]: if t is the minimum-norm solution for the submodular function G, then we can obtain s as \u03bb\u22121z plus the negative part of t. From s we then get w through w = z\u2212\u03bbs. If another algorithm is used for submodular function minimization, then, following [12, Lemma 7.4], we know which components of the (unique) optimal value t\u2217 are negative and which of them are equal to zero (which corresponds to zero components of w\u2217). Then, following [26], if we add a constant vector with components equal to \u03b1 to z, we may obtain level sets of w\u2217. With several values of \u03b1, we can then obtain the full solution w\u2217. However, the minimum norm point algorithm remains the most efficient and allows to obtain directly the solution of the proximal problem."}, {"heading": "C Sparse estimation", "text": "In this section, we consider a design X \u2208 Rn\u00d7p be a fixed design and y \u2208 Rn a set of random responses. Given \u03bb > 0, we define w\u0302 as a minimizer of the regularized least-squares cost:\nmin w\u2208Rp\n1\n2n \u2016y \u2212Xw\u201622 + \u03bb\u2126(w). (6)"}, {"heading": "C.1 Proof of Proposition 4", "text": "(i) for s \u2208 Rp+, if \u2200B \u2282 J , s(B) 6 F (B) and \u2200C \u2282 Jc, s(C) 6 F (C \u2229 J) \u2212 F (J), then \u2200A \u2282 V , s(A) = s(A \u2229 J) + s(A \u2229 Jc) 6 F (A \u2229 J) + F (A \u222a J) \u2212 F (J) 6 F (A) by submodularity. This implies that the desired result by considering the representation of the Lova\u0301sz extension in Eq. (1) and the fact that we have just prove that P contains the product of the two submodular polyhedra associated to F J and FJ .\n(ii) This is immediate from the expression of the Lova\u0301sz extension in Eq. (1). Indeed, the order within J and the one within Jc do not interact. Note that this case includes cases where we some of the components of |wJ | are equal to some of |wJc |. (iii) \u2126J corresponds to the submodular function obtained as the contraction of F by J . It is thus a norm as soon as F J is positive on all singletons, which is itself equivalent to the stability of J . The equivalence of being a norm with stability of the set J is then straightforward."}, {"heading": "C.2 Proof of Proposition 5", "text": "Let Q = 1nX \u22a4X \u2208 Rp\u00d7p and r = 1nX\u22a4y \u2208 Rp. The unicity of the minimizer w\u0302 is a consequence of the invertibility of Q = 1nX \u22a4X . Let J \u2282 V . We will show that if Supp(w\u0302) = J , then w\u0302J is an affine function of r (and hence y), of the form w\u0302J = (Q \u22121 JJ \u2212 AJJ )rJ + bJ , where 0 4 AJJ 4 Q\u22121JJ and (AJJ , bJ) belongs to a finite set independent of r.\nIf J is not a stable set, then, by Proposition 1, this will implies that there exists j \u2208 Jc such that QjJ [(Q \u22121 JJ \u2212AJJ )rJ + bJ ]\u2212 rj) = 0, i.e.,\n0 = QjJ [(Q \u22121 JJ \u2212AJJ )rJ + bJ ]\u2212 rj =\n1 n [QjJQ \u22121 JJX \u22a4 J \u2212X\u22a4j \u2212QjJAJJX\u22a4J ]y +QjJbJ .\nThe row vector QjJQ \u22121 JJX \u22a4 J \u2212X\u22a4j \u2212QjJAJJX\u22a4J cannot be equal to zero, otherwise,\n0 = 1\nn [QjJQ\n\u22121 JJX \u22a4 J \u2212X\u22a4Jc \u2212QjJAJJX\u22a4J ]Xj = QjJQ\u22121JJQJi\u2212Qjj \u2212QjJAJJQJj 6 QjJQ\u22121JJQJi\u2212Qjj\nwhich is a contradiction because of the invertibility of Q and the Schur complement lemma [30] (which implies that the previous quantity must be strictly negative). Thus, we have shown that if Supp(w\u0302) = J and J is not a stable subset, then for a finite number of non zero (c, d) \u2208 Rn \u00d7R, then c\u22a4y is constant. This occurs with probability zero.\nWhat remains to be shown is the affine representation of w\u0302J when the support is given; it is essentially equivalent to showing that the path is piecewise affine, which is not surprising for a polyhedral norm [31]. We use the representation \u2126J (wJ ) = maxz\u2208B z\u22a4wJ where B is the finite set of z such that |z| in an extreme point of the submodular polyhedron associated with \u2126J . Necessary optimality conditions [32] for such the problem in Eq. (6) is the existence of \u03b7z > 0 (for each z \u2208 B) such that (1) \u2211z\u2208B \u03b7z = 1, (2) \u03b7z = 0 if z is not a maximizer of maxz\u2208B z\u22a4wJ , and\n(3) wJ is a minimizer of 1 2w \u22a4 J QJJwJ \u2212 r\u22a4J wJ + \u03bbw\u22a4 \u2211 z\u2208A \u03b7zz, i.e., QJJwJ + \u03bb \u2211\nz\u2208A \u03b7zz = rJ . Moreover, by Carathe\u0301odory\u2019s theorem [32], the number k of non-zero \u03b7 may be taken to be less than |J |+ 1. This thus implies that, if consider the vector \u03b6 \u2208 Rk of non-zero \u03b7, and the matrix Z \u2208 R|J|\u00d7k of corresponding z\u2019s, then we have\nQJJwJ + \u03bbZ\u03b6 = rJ\n\u03b6\u22a41 = 1\n\u2203c \u2208 R such that Z\u22a4wJ = c1. In matrix form, this can be written as:\n\n QJJ \u03bbZ 0 \u03bbZ\u22a4 0 \u2212\u03bb1 0 \u2212\u03bb1\u22a4 0\n\n\n\n wJ \u03b6 c\n\n =\n\n rJ 0 \u2212\u03bb\n\n .\nIt is then a simple linear algebra exercise to show that if k 6 |J |+1, then wJ is of the desired form."}, {"heading": "C.3 Proof of Proposition 6", "text": "Let q = 1nX \u22a4\u03b5 \u2208 Rp, which is normal with mean zero and covariance matrix \u03c32Q/n. We have \u2126(x) > \u2126J(xJ ) + \u2126 J(xJc) > \u2126J (xJ ) + \u03c1(J)\u2126Jc(xJc) > \u03c1(J)\u2126(x). This implies that \u2126\n\u2217(q) > \u03c1(J)\u22121 max{\u2126\u2217J(qJ ), (\u2126J)\u2217(qJc)}. Moreover, qJc \u2212 QJcJQ\u22121JJqJ is normal with covariance matrix \u03c32/n(QJcJc \u2212 QJcJQ\u22121JJQJJc) 4 \u03c32/nQJcJc . This implies that with probability larger than 1 \u2212 3P (\u2126\u2217(q) > \u03bb\u03c1(J)\u03b7/2), we have \u2126\u2217J(qJ ) 6 \u03bb/2 and (\u2126\nJ )\u2217(qJc \u2212QJcJQ\u22121JJqJ) 6 \u03bb\u03b7/2. We denote by w\u0303 the unique (because QJJ is invertible) minimum of 1 2n\u2016y\u2212Xw\u201622 +\u03bb\u2126(w), subject to wJc = 0. w\u0303J is defined through QJJ(w\u0303J \u2212wJ\u2217)\u2212 qJ = \u2212\u03bbsJ where sJ \u2208 \u2202\u2126J(w\u0303J ) (which implies that \u2126\u2217J(sJ ) 6 1) , i.e., w\u0303J \u2212 w\u2217J = Q\u22121JJ(qJ \u2212 \u03bbsJ). We have:\n\u2016w\u0303J \u2212 w\u2217J\u2016\u221e 6 max j\u2208J |\u03b4\u22a4j Q\u22121JJ (qJ \u2212 \u03bbsJ)|\n6 max j\u2208J\n\u2126J(Q \u22121 JJ\u03b4j)\u2126 \u2217 J (qJ \u2212 \u03bbsJ)|\n6 max j\u2208J\nc(J)\u2016Q\u22121JJ\u03b4j\u20162[\u2126\u2217J (qJ) + \u03bb\u2126\u2217J (sJ)] 6 3 2 \u03bbc(J)\u03ba\u22121\nThus if 2\u03bbc(J)\u03ba\u22121 6 \u03bd, then Supp(w\u0303) \u2283 Supp(w\u2217). We now show that since we have (\u2126J )\u2217(qJc \u2212 QJcJQ\u22121JJqJ ) 6 \u03bb\u03b7/2, w\u0303 is the unique minimizer of Eq. (6). For that it suffices to show that (\u2126J )\u2217(QJcJ(w\u0303J \u2212 w\u2217J)\u2212 qJc) < \u03bb. We have:\n(\u2126J )\u2217(QJcJ(w\u0303J \u2212 w\u2217J )\u2212 qJc) = (\u2126J )\u2217(QJcJQ\u22121JJ (qJ \u2212 \u03bbsJ )\u2212 qJc) 6 (\u2126J )\u2217(QJcJQ \u22121 JJqJ \u2212 qJc) + \u03bb(\u2126J )\u2217(QJcJQ\u22121JJsJ)\n6 (\u2126J )\u2217(QJcJQ \u22121 JJqJ \u2212 qJc) + \u03bb(\u2126J )\u2217[(\u2126J(Q\u22121JJQJj))j\u2208Jc ] 6 \u03bb\u03b7/2 + \u03bb(1\u2212 \u03b7) < \u03bb\nwhich leads to the desired result."}, {"heading": "C.4 Proof of Proposition 7", "text": "Like for the proof of Proposition 6, we have \u2126(x) > \u2126J (xJ ) + \u2126 J (xJc) > \u2126J(xJ ) + \u03c1(J)\u2126Jc(xJc) > \u03c1(J)\u2126(x). Thus, if we assume \u2126\u2217(q) 6 \u03bb\u03c1(J)/2, then \u2126\u2217J(qJ ) 6 \u03bb/2 and (\u2126 J )\u2217(qJc) 6 \u03bb/2. Let \u2206 = w\u0302 \u2212 w\u2217. We follow the proof from [33] by using the decomposition property of the norm \u2126. We have, by optimality of w\u0302:\n1 2 \u2206\u22a4Q\u2206+ \u03bb\u2126(w\u2217 +\u2206) + q\u22a4\u2206 6 \u03bb\u2126(w\u2217 +\u2206) + q\u22a4\u2206 6 \u03bb\u2126(w\u2217)\nUsing the decomposition property,\n\u03bb\u2126J ((w \u2217 +\u2206)J ) + \u03bb\u2126 J ((w\u2217 +\u2206)Jc) + q \u22a4 J \u2206J + q \u22a4 Jc\u2206Jc 6 \u03bb\u2126J(w \u2217 J )\n\u03bb\u2126J(\u2206Jc) 6 \u03bb\u2126J (w \u2217 J )\u2212 \u03bb\u2126J (w\u2217J +\u2206J ) + \u2126\u2217J (qJ )\u2126J(\u2206J ) + (\u2126J)\u2217(qJc)\u2126J(\u2206Jc)\n(\u03bb\u2212 (\u2126J )\u2217(qJc))\u2126J (\u2206Jc) 6 (\u03bb+\u2126\u2217J(qJ ))\u2126J (\u2206J ) Thus \u2126J (\u2206Jc) 6 3\u2126J(\u2206J), which implies \u2206\n\u22a4Q\u2206 > \u03ba\u2016\u2206J\u201622 (we have assumed a restricted eigenvalue condition). Moreover, we have:\n\u2206\u22a4Q\u2206 = \u2206\u22a4(Q\u2206) 6 \u2126(\u2206)\u2126\u2217(Q\u2206)\n6 \u2126(\u2206)(\u2126\u2217(q) + \u03bb) 6 3\u03bb\n2 \u2126(\u2206) by optimality of w\u0302\n\u2126(\u2206) 6 \u2126J (\u2206J ) + \u03c1(J) \u22121\u2126J(\u2206Jc)\n6 \u2126J (\u2206J )(3 + 1\n\u03c1(J) ) 6\n4\n\u03c1(J) \u2126J(\u2206J )\nThis implies that \u03bac(J)2\u2126J(\u2206J ) 2 6 \u03ba\u2016\u2206J\u201622 6 \u2206\u22a4Q\u2206 6 6\u03bb\u03c1(J)\u2126J(\u2206J ), and thus \u2126J(\u2206J ) 6 6c(J)2\u03bb \u03ba\u03c1(J) , which leads to the desired result, given the previous inequalities."}, {"heading": "C.5 Proof of Proposition 8", "text": "We have \u2126\u2217(z) = max\u2126(w)61 w \u22a4z; the maximum can be taken over the set of extreme points of the unit ball, which leads to the desired result given Proposition 2."}], "references": [{"title": "Grouped and hierarchical model selection through composite absolute penalties", "author": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "Annals of Statistics, 37(6A):3468\u20133497", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Structured variable selection with sparsity-inducing norms", "author": ["R. Jenatton", "J.Y. Audibert", "F. Bach"], "venue": "Technical report, arXiv:0904.3523", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning with structured sparsity", "author": ["J. Huang", "T. Zhang", "D. Metaxas"], "venue": "Proc. ICML", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Group Lasso with overlaps and graph Lasso", "author": ["L. Jacob", "G. Obozinski", "J.-P. Vert"], "venue": "Proc. ICML", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Tree-guided group Lasso for multi-task regression with structured sparsity", "author": ["S. Kim", "E. Xing"], "venue": "Proc. ICML", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "Proc. ICML", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Network flow algorithms for structured sparsity", "author": ["J. Mairal", "R. Jenatton", "G. Obozinski", "F. Bach"], "venue": "Adv. NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Signal reconstruction from noisy random projections", "author": ["J. Haupt", "R. Nowak"], "venue": "IEEE Transactions on Information Theory, 52(9):4036\u20134048", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex analysis and optimization with submodular functions: a tutorial", "author": ["F Bach"], "venue": "Technical Report 00527714, HAL", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["A. Krause", "C. Guestrin"], "venue": "Proc. UAI", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Submodularity cuts and applications", "author": ["Y. Kawahara", "K. Nagano", "K. Tsuda", "J.A. Bilmes"], "venue": "Adv. NIPS", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Submodular Functions and Optimization", "author": ["S. Fujishige"], "venue": "Elsevier", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Submodular functions", "author": ["J. Edmonds"], "venue": "matroids, and certain polyhedra. In Combinatorial optimization - Eureka, you shrink!, pages 11\u201326. Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Joint support recovery under high-dimensional scaling: Benefits and perils of l1-l\u221e-regularization", "author": ["S. Negahban", "M.J. Wainwright"], "venue": "Adv. NIPS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Submodular functions and convexity", "author": ["L. Lov\u00e1sz"], "venue": "Mathematical programming: the state of the art, Bonn, pages 235\u2013257", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1982}, {"title": "Theory of capacities", "author": ["G. Choquet"], "venue": "Ann. Inst. Fourier, 5:131\u2013295", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1954}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["J.B. Orlin"], "venue": "Mathematical Programming, 118(2):237\u2013251", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Structured sparse principal component analysis", "author": ["R. Jenatton", "G. Obozinski", "F. Bach"], "venue": "Proc. AISTATS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparsity and smoothness via the fused Lasso", "author": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "J. Roy. Stat. Soc. B, 67(1):91\u2013108", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Matrix analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": "Cambridge Univ. Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1990}, {"title": "Concavity of certain maps on positive definite matrices and applications to hadamard products", "author": ["T. Ando"], "venue": "Linear Algebra and its Applications, 26:203\u2013241", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1979}, {"title": "Some comments on  Cp", "author": ["C.L. Mallows"], "venue": "Technometrics, 15(4):661\u2013675", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1973}, {"title": "Sparse estimation using general likelihoods and non-factorial priors", "author": ["D. Wipf", "S. Nagarajan"], "venue": "Adv. NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, 2(1):183\u2013202", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "On total variation minimization and surface evolution using parametric maximum flows", "author": ["A. Chambolle", "J. Darbon"], "venue": "International Journal of Computer Vision, 84(3):288\u2013307", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "A unified framework for highdimensional analysis of M-estimators with decomposable regularizers", "author": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Adv. NIPS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "Journal of Machine Learning Research, 7:2541\u20132563", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Submodular dictionary selection for sparse representation", "author": ["A. Krause", "V. Cevher"], "venue": "Proc. ICML", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "Johns Hopkins University Press", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Piecewise linear regularized solution paths", "author": ["S. Rosset", "J. Zhu"], "venue": "Ann. Statist., 35(3):1012\u2013 1030", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Convex Analysis and Nonlinear Optimization: Theory and Examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": "Springer", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["P. Bickel", "Y. Ritov", "A. Tsybakov"], "venue": "Annals of Statistics, 37(4):1705\u20131732", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 1, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 2, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 3, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 4, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 5, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 6, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 3, "context": "For example, in [4], structured sparsity is used to encode prior knowledge regarding network relationship between genes, while in [6], it is used as an alternative to structured nonparametric Bayesian process based priors for topic models.", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "For example, in [4], structured sparsity is used to encode prior knowledge regarding network relationship between genes, while in [6], it is used as an alternative to structured nonparametric Bayesian process based priors for topic models.", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "Most of the work based on convex optimization and the design of dedicated sparsity-inducing norms has focused mainly on the specific allowed set of sparsity patterns [1, 2, 4, 6]: if w \u2208 R denotes the predictor we aim to estimate, and Supp(w) denotes its support, then these norms are designed so that", "startOffset": 166, "endOffset": 178}, {"referenceID": 1, "context": "Most of the work based on convex optimization and the design of dedicated sparsity-inducing norms has focused mainly on the specific allowed set of sparsity patterns [1, 2, 4, 6]: if w \u2208 R denotes the predictor we aim to estimate, and Supp(w) denotes its support, then these norms are designed so that", "startOffset": 166, "endOffset": 178}, {"referenceID": 3, "context": "Most of the work based on convex optimization and the design of dedicated sparsity-inducing norms has focused mainly on the specific allowed set of sparsity patterns [1, 2, 4, 6]: if w \u2208 R denotes the predictor we aim to estimate, and Supp(w) denotes its support, then these norms are designed so that", "startOffset": 166, "endOffset": 178}, {"referenceID": 5, "context": "Most of the work based on convex optimization and the design of dedicated sparsity-inducing norms has focused mainly on the specific allowed set of sparsity patterns [1, 2, 4, 6]: if w \u2208 R denotes the predictor we aim to estimate, and Supp(w) denotes its support, then these norms are designed so that", "startOffset": 166, "endOffset": 178}, {"referenceID": 7, "context": "In this paper, we instead follow the approach of [8, 3] and consider specific penalty functions F (Supp(w)) of the support set, which go beyond the cardinality function, but are not limited or designed to only forbid certain sparsity patterns.", "startOffset": 49, "endOffset": 55}, {"referenceID": 2, "context": "In this paper, we instead follow the approach of [8, 3] and consider specific penalty functions F (Supp(w)) of the support set, which go beyond the cardinality function, but are not limited or designed to only forbid certain sparsity patterns.", "startOffset": 49, "endOffset": 55}, {"referenceID": 7, "context": ", forward selection) to the problem are considered in [8, 3], we provide convex relaxations to the function w 7\u2192 F (Supp(w)), which extend the traditional link between the l1-norm and the cardinality function.", "startOffset": 54, "endOffset": 60}, {"referenceID": 2, "context": ", forward selection) to the problem are considered in [8, 3], we provide convex relaxations to the function w 7\u2192 F (Supp(w)), which extend the traditional link between the l1-norm and the cardinality function.", "startOffset": 54, "endOffset": 60}, {"referenceID": 8, "context": "Submodular functions may be seen as the set-function equivalent of convex functions, and exhibit many interesting properties that we review in Section 2\u2014see [9] for a tutorial on submodular analysis and [10, 11] for other applications to machine learning.", "startOffset": 157, "endOffset": 160}, {"referenceID": 9, "context": "Submodular functions may be seen as the set-function equivalent of convex functions, and exhibit many interesting properties that we review in Section 2\u2014see [9] for a tutorial on submodular analysis and [10, 11] for other applications to machine learning.", "startOffset": 203, "endOffset": 211}, {"referenceID": 10, "context": "Submodular functions may be seen as the set-function equivalent of convex functions, and exhibit many interesting properties that we review in Section 2\u2014see [9] for a tutorial on submodular analysis and [10, 11] for other applications to machine learning.", "startOffset": 203, "endOffset": 211}, {"referenceID": 0, "context": "\u2212 By selecting specific submodular functions in Section 4, we recover and give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups [1, 2, 7], and we define new norms, in particular ones that can be used as non-factorial priors for supervised learning (Section 4).", "startOffset": 208, "endOffset": 217}, {"referenceID": 1, "context": "\u2212 By selecting specific submodular functions in Section 4, we recover and give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups [1, 2, 7], and we define new norms, in particular ones that can be used as non-factorial priors for supervised learning (Section 4).", "startOffset": 208, "endOffset": 217}, {"referenceID": 6, "context": "\u2212 By selecting specific submodular functions in Section 4, we recover and give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups [1, 2, 7], and we define new norms, in particular ones that can be used as non-factorial priors for supervised learning (Section 4).", "startOffset": 208, "endOffset": 217}, {"referenceID": 2, "context": "These are illustrated on simulation experiments in Section 7, where they outperform related greedy approaches [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 11, "context": "These set-functions are often referred to as polymatroid set-functions [12] or \u03b2-functions [13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "These set-functions are often referred to as polymatroid set-functions [12] or \u03b2-functions [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": ", Bk with non empty intersection with A (which will lead to the grouped l1/l\u221e-norm [1, 14]).", "startOffset": 83, "endOffset": 90}, {"referenceID": 13, "context": ", Bk with non empty intersection with A (which will lead to the grouped l1/l\u221e-norm [1, 14]).", "startOffset": 83, "endOffset": 90}, {"referenceID": 14, "context": "Given any set-function F , one can define its Lov\u00e1sz extension [15] (a.", "startOffset": 63, "endOffset": 67}, {"referenceID": 15, "context": "Choquet integral [16]) f : Rp+ \u2192 R, as follows: given w \u2208 Rp+, we can order the components of w in decreasing order wj1 > \u00b7 \u00b7 \u00b7 > wjp > 0; the value f(w) is then defined as:", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": ", [15, 12]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 11, "context": ", [15, 12]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 0, "context": ", minimizing f over {0, 1}p is equivalent to minimizing f over [0, 1] [15, 13].", "startOffset": 63, "endOffset": 69}, {"referenceID": 14, "context": ", minimizing f over {0, 1}p is equivalent to minimizing f over [0, 1] [15, 13].", "startOffset": 70, "endOffset": 78}, {"referenceID": 12, "context": ", minimizing f over {0, 1}p is equivalent to minimizing f over [0, 1] [15, 13].", "startOffset": 70, "endOffset": 78}, {"referenceID": 11, "context": "We denote by P the submodular polyhedron [12], defined as the set of s \u2208 Rp+ such that for all A \u2282 V , s(A) 6 F (A), i.", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "One important result in submodular analysis is that if F is a nondecreasing submodular function, then we have a representation of f as a maximum of linear functions [12, 15], i.", "startOffset": 165, "endOffset": 173}, {"referenceID": 14, "context": "One important result in submodular analysis is that if F is a nondecreasing submodular function, then we have a representation of f as a maximum of linear functions [12, 15], i.", "startOffset": 165, "endOffset": 173}, {"referenceID": 12, "context": "Stable sets are also sometimes referred to as flat or closed [13].", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "The set of stable sets is closed by intersection [13], and will correspond to the set of allowed sparsity patterns (see Section 6.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "As shown in [13], the submodular polytope P has full dimension p as soon as F is strictly positive on all singletons, and its faces are exactly the sets {sk = 0} for k \u2208 V and {s(A) = F (A)} for stable and inseparable sets.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Most algorithms for minimizing submodular functions rely on the following strong duality principle [13, 12]:", "startOffset": 99, "endOffset": 107}, {"referenceID": 11, "context": "Most algorithms for minimizing submodular functions rely on the following strong duality principle [13, 12]:", "startOffset": 99, "endOffset": 107}, {"referenceID": 16, "context": "The first type of algorithm leads to strongly polynomial algorithms with best known complexity O(p) [17], while the minimum point algorithm of [12] has no worst-time complexity bounds but is usually much faster in practice [12] and is based on the equivalent problem of finding the minimum-norm point in B(G), i.", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "The first type of algorithm leads to strongly polynomial algorithms with best known complexity O(p) [17], while the minimum point algorithm of [12] has no worst-time complexity bounds but is usually much faster in practice [12] and is based on the equivalent problem of finding the minimum-norm point in B(G), i.", "startOffset": 143, "endOffset": 147}, {"referenceID": 11, "context": "The first type of algorithm leads to strongly polynomial algorithms with best known complexity O(p) [17], while the minimum point algorithm of [12] has no worst-time complexity bounds but is usually much faster in practice [12] and is based on the equivalent problem of finding the minimum-norm point in B(G), i.", "startOffset": 223, "endOffset": 227}, {"referenceID": 17, "context": ", [18]) of \u03a9 is equal to \u03a9\u2217(s) = maxA\u2282V \u2016sA\u20161 F (A) = maxA\u2208T \u2016sA\u20161 F (A) .", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "Other examples of submodular functions, in particular in terms of matroids and entropies, may be found in [12, 10, 11] and could also lead to interesting new norms.", "startOffset": 106, "endOffset": 118}, {"referenceID": 9, "context": "Other examples of submodular functions, in particular in terms of matroids and entropies, may be found in [12, 10, 11] and could also lead to interesting new norms.", "startOffset": 106, "endOffset": 118}, {"referenceID": 10, "context": "Other examples of submodular functions, in particular in terms of matroids and entropies, may be found in [12, 10, 11] and could also lead to interesting new norms.", "startOffset": 106, "endOffset": 118}, {"referenceID": 8, "context": ", [9]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "We consider grouped norms defined with potentially overlapping groups [1, 2], i.", "startOffset": 70, "endOffset": 76}, {"referenceID": 1, "context": "We consider grouped norms defined with potentially overlapping groups [1, 2], i.", "startOffset": 70, "endOffset": 76}, {"referenceID": 1, "context": "In the case where l\u221e-norms are replaced by l2-norms, [2] has shown that the set of allowed sparsity patterns are intersections of complements of groups G with strictly positive weights.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "2 extends the result of [2] to the new case of l\u221e-norms.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "Hierarchical norms defined on directed acyclic graphs [1, 5, 6] correspond to the set-function F (A) which is the cardinality of the union of ancestors of elements in A.", "startOffset": 54, "endOffset": 63}, {"referenceID": 4, "context": "Hierarchical norms defined on directed acyclic graphs [1, 5, 6] correspond to the set-function F (A) which is the cardinality of the union of ancestors of elements in A.", "startOffset": 54, "endOffset": 63}, {"referenceID": 5, "context": "Hierarchical norms defined on directed acyclic graphs [1, 5, 6] correspond to the set-function F (A) which is the cardinality of the union of ancestors of elements in A.", "startOffset": 54, "endOffset": 63}, {"referenceID": 4, "context": "These have been applied to bioinformatics [5], computer vision and topic models [6].", "startOffset": 42, "endOffset": 45}, {"referenceID": 5, "context": "These have been applied to bioinformatics [5], computer vision and topic models [6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "If we assume that the p variables are organized in a 1D, 2D or 3D grid, [2] considers norms based on overlapping groups leading to stable sets equal to rectangular or convex shapes, with applications in computer vision [19].", "startOffset": 72, "endOffset": 75}, {"referenceID": 18, "context": "If we assume that the p variables are organized in a 1D, 2D or 3D grid, [2] considers norms based on overlapping groups leading to stable sets equal to rectangular or convex shapes, with applications in computer vision [19].", "startOffset": 219, "endOffset": 223}, {"referenceID": 1, "context": "This leads to the undesired result, which has been already observed by [2], of adding all variables in one step, rather than gradually, when the regularization parameter decreases in a regularized optimization problem.", "startOffset": 71, "endOffset": 74}, {"referenceID": 19, "context": "fused Lasso penalty [20]), which is a relaxation of the number of jumps in a vector w rather than in its support.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": ", \u03bbp are the (nonnegative) eigenvalues of Q [21].", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": ", [12, 9]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 8, "context": ", [12, 9]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 21, "context": ", [22]), h(\u03bb) = \u03bb for q \u2208 (0, 1] are positive linear combinations of functions that lead to nondecreasing submodular functions.", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": "In a frequentist setting, the Mallows CL penalty [23] depends on the degrees of freedom, of the form trX\u22a4 AXA(X \u22a4 AXA + \u03bbI) \u22121.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "In a Bayesian context however, it is shown by [24] that penalties of the form log det(X\u22a4 AXA + \u03bbI) (which lead to submodular functions) correspond to marginal likelihoods associated to the set A and have good behavior when used within a non-convex framework.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "We do not pursue the extensive evaluation of non-factorial convex priors in this paper but provide in simulations examples with F (A) = tr(X\u22a4 AXA) 1/2 (which is equal to the trace norm of XA [18]).", "startOffset": 191, "endOffset": 195}, {"referenceID": 24, "context": ", [25]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": "In this paper, we consider the methods \u201cISTA\u201d and its accelerated variants \u201cFISTA\u201d [25], which are compared in Figure 4.", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "In the case of the l1-norm, this reduces to soft thresholding of z, the following proposition (see proof in the appendix) shows that this is equivalent to a particular algorithm for submodular function minimization, namely the minimum-norm-point algorithm, which has no complexity bound but is empirically faster than algorithms with such bounds [12]: Proposition 3 (Proximal operator) Let z \u2208 R and \u03bb > 0, minimizing 1 2\u2016w \u2212 z\u20162 + \u03bb\u03a9(w) is equivalent to finding the minimum of the submodular function A 7\u2192 \u03bbF (A) \u2212 |z|(A) with the minimum-norm-point algorithm.", "startOffset": 346, "endOffset": 350}, {"referenceID": 25, "context": ", [26]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 5, "context": ", the l1/l\u221e-norm, tree-structured groups [6], or general overlapping groups [7]).", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": ", the l1/l\u221e-norm, tree-structured groups [6], or general overlapping groups [7]).", "startOffset": 76, "endOffset": 79}, {"referenceID": 26, "context": "Like recent analysis of sparsity-inducing norms [27], the analysis provided in this section relies heavily on decomposability properties of our norm \u03a9.", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": ", [12]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "3, but we could consider assumptions similar to the ones used in [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 27, "context": "We now assume that the linear model is well-specified and extend results from [28] for sufficient support recovery conditions and from [27] for estimation consistency.", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "We now assume that the linear model is well-specified and extend results from [28] for sufficient support recovery conditions and from [27] for estimation consistency.", "startOffset": 135, "endOffset": 139}, {"referenceID": 27, "context": ", Propositions 6 and 8 extend results based on support recovery conditions [28]; while Propositions", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": ", [27]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "We can also get back results for the l1/l\u221e-norm [14].", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "For the submodular function F (A) = |A|1/2 (a simple submodular function beyond the cardinality) we compare three optimization algorithms described in Section 5, subgradient descent and two proximal methods, ISTA and its accelerated version FISTA [25], for p = n = 1000, k = 100 and \u03bb = 0.", "startOffset": 247, "endOffset": 251}, {"referenceID": 7, "context": "We compare three strategies for solving the combinatorial optimization problem minw\u2208Rp 1 2n\u2016y \u2212 Xw\u20162 + \u03bbF (Supp(w)) with F (A) = tr(X\u22a4 AXA) , the approach based on our sparsity-inducing norms, the simpler greedy (forward selection) approach proposed in [8, 3], and by thresholding the ordinary least-squares estimate.", "startOffset": 253, "endOffset": 259}, {"referenceID": 2, "context": "We compare three strategies for solving the combinatorial optimization problem minw\u2208Rp 1 2n\u2016y \u2212 Xw\u20162 + \u03bbF (Supp(w)) with F (A) = tr(X\u22a4 AXA) , the approach based on our sparsity-inducing norms, the simpler greedy (forward selection) approach proposed in [8, 3], and by thresholding the ordinary least-squares estimate.", "startOffset": 253, "endOffset": 259}, {"referenceID": 2, "context": "We now focus on the predictive performance and compare our new norm with F (A) = tr(X\u22a4 AXA) , with greedy approaches [3] and to regularization by l1 or l2 norms.", "startOffset": 117, "endOffset": 120}, {"referenceID": 28, "context": ", [29] for application of submodular functions to dictionary learning).", "startOffset": 2, "endOffset": 6}, {"referenceID": 25, "context": "Second, links between submodularity and sparsity could be studied further, in particular by considering submodular relaxations of other combinatorial functions, or studying links with other polyhedral norms such as the total variation, which are known to be similarly associated with symmetric submodular set-functions such as graph cuts [26].", "startOffset": 338, "endOffset": 342}, {"referenceID": 17, "context": "(ii) We denote by g\u2217 the Fenchel conjugate of g on the domain {w \u2208 R, \u2016w\u2016\u221e 6 1}, and g\u2217\u2217 its bidual [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 0, "context": "= max \u03b4\u2208[0,1]p \u03b4\u22a4|s| \u2212 f(\u03b4) because F \u2212 |s| is submodular.", "startOffset": 8, "endOffset": 13}, {"referenceID": 0, "context": "= max s\u2208Rp min \u03b4\u2208[0,1]p s\u22a4w \u2212 \u03b4\u22a4|s|+ f(\u03b4)", "startOffset": 17, "endOffset": 22}, {"referenceID": 0, "context": "= min \u03b4\u2208[0,1]p max s\u2208Rp s\u22a4w \u2212 \u03b4\u22a4|s|+ f(\u03b4) by strong duality and Slater\u2019s condition [18]", "startOffset": 8, "endOffset": 13}, {"referenceID": 17, "context": "= min \u03b4\u2208[0,1]p max s\u2208Rp s\u22a4w \u2212 \u03b4\u22a4|s|+ f(\u03b4) by strong duality and Slater\u2019s condition [18]", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "= min \u03b4\u2208[0,1]p,\u03b4>|w| f(\u03b4) = f(|w|) because F is nonincreasing.", "startOffset": 8, "endOffset": 13}, {"referenceID": 11, "context": "For more details on the subdifferential for nonzero components, see [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "Following [6], without loss of generality, we assume that z has nonnegative components.", "startOffset": 10, "endOffset": 13}, {"referenceID": 25, "context": "Then, following [26], if we add a constant vector with components equal to \u03b1 to z, we may obtain level sets of w\u2217.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "which is a contradiction because of the invertibility of Q and the Schur complement lemma [30] (which implies that the previous quantity must be strictly negative).", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "What remains to be shown is the affine representation of \u0175J when the support is given; it is essentially equivalent to showing that the path is piecewise affine, which is not surprising for a polyhedral norm [31].", "startOffset": 208, "endOffset": 212}, {"referenceID": 31, "context": "Necessary optimality conditions [32] for such the problem in Eq.", "startOffset": 32, "endOffset": 36}, {"referenceID": 31, "context": "Moreover, by Carath\u00e9odory\u2019s theorem [32], the number k of non-zero \u03b7 may be taken to be less than |J |+ 1.", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": "We follow the proof from [33] by using the decomposition property of the norm \u03a9.", "startOffset": 25, "endOffset": 29}], "year": 2010, "abstractText": "Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the l1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lov\u00e1sz extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.", "creator": "LaTeX with hyperref package"}}}