{"id": "1008.5287", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2010", "title": "Lexical Co-occurrence, Statistical Significance, and Word Association", "abstract": "tandem information - likelihood poses an important mechanism for knowing word associations. adults present ample novel construct for discovering visually significant random co - incidence from a limited era. tandem reasoning with our most accounts today giving feedback to unigram descriptions, infants report only regarding the documents containing of the collections ( of such candidate bigram ). we experience disagreements while span distributions of associated words, while being reconstructed from changes in underlying unigram data. our perspective has the fidelity to distinguish different classes of lexical co - occurrences, based on measurement of the document and corpuslevel cues of co - occurrence in archival data. economists construct selective surveys on benchmark data often to study optimal dependence of various co - occurrence measures they are sometimes known to detail. laboratory find that in relatively obscure measure specifies posterior, and a newly introduced measure csa capture the notion of retrieval co - occurrence best, performing next through llr, dice, whereas ttest, while another mysterious measure, pmi, odds, indicates poorly in causal context of lexical co - localities.", "histories": [["v1", "Tue, 31 Aug 2010 11:37:32 GMT  (82kb,D)", "http://arxiv.org/abs/1008.5287v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["dipak chaudhari", "om p damani", "srivatsan laxman"], "accepted": true, "id": "1008.5287"}, "pdf": {"name": "1008.5287.pdf", "metadata": {"source": "CRF", "title": "Lexical Co-occurrence, Statistical Significance, and Word Association", "authors": ["Dipak Chaudhari", "Om P. Damani", "Srivatsan Laxman"], "emails": ["dipakc@cse.iitb.ac.in", "damani@cse.iitb.ac.in", "slaxman@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The notion of word association is important for numerous NLP applications, like, word sense disambiguation, optical character recognition, speech recognition, parsing, lexicography, natural language generation, and machine translation. Lexical cooccurrence is an important indicator of word association and this has motivated several frequencybased measures for word association (Church and Hanks, 1989; Dunning, 1993; Dice, 1945; Washtell\nand Markert, 2009). In this paper, we present a theoretical basis for detection and classification of lexical co-occurrences1. In general, a lexical cooccurrence could refer to a pair of words that occur in a large number of documents; or it could refer to a pair of words that, although appear only in a small number of documents, occur frequently very close to each other within each document. We formalize these ideas and construct a significance test for co-occurrences that will allow us to detect different kinds of co-occurrences within a single unified framework (a feature which is absent in current measures for co-occurrence). As a by-product, our framework also leads to a better understanding of existing measures for word co-occurrence.\nAs pointed out in (Kilgarriff, 2005), language is never random - which brings us to the question of what model of random chance can give us a good statistical test for lexical co-occurrences. We need a null hypothesis that can account for an observed cooccurrence as a pure chance event and this in-turn requires a corpus generation model. It is often reasonable to assume that documents in the corpus are generated independent of each other. Existing frequecybased association measures like PMI (Church and Hanks, 1989), LLR (Dunning, 1993) etc. further assume that each document is drawn from a multinomial distribution based on global unigram frequencies. The main concern with such a null model is the overbearing influence of unigram frequencies on the detection of word associations. For example,\n1Note that we are interested in co-occurrence, not collocation, i.e., pairs of words that co-occur in a document with an arbitrary number of intervening words. Also, we use the term bigram to mean bigram at-a-distance or spanned-bigram \u2013 again, other words can occur in-between the constituents of a bigram.\nar X\niv :1\n00 8.\n52 87\nv1 [\ncs .C\nL ]\n3 1\nA ug\n2 01\nthe association between anomochilidae (dwarf pipe snakes) and snake would go undetected in our wikepedia corpus, since less than 0.1% of the pages containing snake also contained anomochilidae. Similarly, under current models, the expected span (interword distance) of a bigram is also very sensitive to the associated unigram frequencies: the expected span of a bigram composed of low frequency unigrams is much larger than that with high frequency unigrams. This is contrary to how word associations appear in language, where semantic relationships manifest with small inter-word distances irrespective of the underlying unigram distributions.\nThese considerations motivate our search for a more direct relationship between words, one that can potentially be detected using careful statistical characterization of inter-word distances, while minimizing the influence of the associated unigram frequencies. We focus on only the documents containing both the terms (of a candidate bigram) since in NLP applications, we often have to chose from a set of alternatives for a given word. Hence, rather than ask the abstract question of whether words x and y are related, our approach is to ask, given that y is a candidate for pairing with x, how likely is it that x and y are lexically correlated. For example, probability that anomochilidae is found in the vicinity of snake is higher if we knew that anomochilidae and snake appear in the same context.\nWe consider a null model that represents each document as a bag of words 2. Then, a random permutation of the associated bag of words gives a linear representation for the document. An arbitrary relation between a pair of words will result in the locations of these words to be randomly distributed in the documents in which they co-occur. If the observed span distribution of a bigram resembles that under the (random permutation) null model, then the relation between the words is not strong enough for one word to influence the placement of the other. However, if the words are found to occur closer together than explainable by our null model, then we hypothesize existence of a more direct association between these words.\n2There can be many ways to associate a bag of words with a document. Details of this association are not important for us, except that the bag of words provides some kind of quantitative summary of the words within the document.\nIn this paper, we formalize the notion of statistically significant lexical co-occurrences by introducing a null model that can detect biases in span distributions of word associations, while being agnostic to variations in global unigram frequencies. Our framework has the fidelity to distinguish different classes of lexical co-occurrences, based on strengths of the document and corpus-level cues of co-occurrence in the data. We perform extensive experiments on benchmark data sets to study the performance of various co-occurrence measures that are currently known in literature. We find that a relatively obscure measure called Ochiai, and a newly introduced measure CSA, capture the notion of lexical co-occurrence best, followed next by LLR, Dice, and TTest, while another popular measure, PMI, suprisingly, performs poorly in the context of lexical co-occurrence."}, {"heading": "2 Lexically significant co-occurrences", "text": "Consider a bigram \u03b1. Let D = {D1, . . . , DK} denote the set of K documents (from out of the entire corpus) that contain at least one occurrence of \u03b1. The frequency of \u03b1 in document Di, fi, is the maximum number of non-overlapped occurrences of \u03b1 in Di. A set of occurrences of a bigram are called nonoverlapping if the words corresponding to one occurrence from the set do not appear in-between the words corresponding to any other occurrence from the set.\nThe span of an occurrence of \u03b1 is the \u2018unsigned distance\u2019 between the first and last textual units of interest associated with that occurrence. We mostly use words as the unit of distance, but in general, distance can be measured in words, sentences, or even paragraphs (e.g. an occurrence comprising two adjacent words in a sentence has a word-span of one and a sentence-span of zero). Likewise, the size of a document Di, denoted as `i, is correspondingly measured in units of words, sentences or paragraphs. Finally, let f\u0302i denote the maximum number of nonoverlapped occurrences of \u03b1 in Di with span less than a given threshold x. We refer to f\u0302i as the spanconstrained frequency of \u03b1 in Di. Note that f\u0302i cannot exceed fi.\nTo assess the statistical significance of the bigram \u03b1 we ask if the span-constrained frequency f\u0302i (of \u03b1)\nis more than what we would expect for it in a document of size `i containing fi \u2018random\u2019 occurrences of \u03b1. Our intuition is that if two words are semantically related, they will often appear close to each other in the document and so the distribution of the spans will typically exhibit a prominent bias toward values less than a small x.\nConsider the null hypothesis that a document is generated as a random permutation of the bag of words associated with the document. Let \u03c0x(f\u0302 , f, `) denote the probability of observing a span-constrained frequency (for \u03b1) of at least f\u0302 in a document of length ` that contains a maximum of f non-overlapped occurrences of \u03b1. Observe that \u03c0x(0, f, `) = 1 for any x > 0; also, for x \u2265 ` we have \u03c0x(f, f, `) = 1 (i.e. all f occurrences will always have span less than x for x \u2265 `). However, for typical values of x (i.e. for x `) the probability \u03c0x(f\u0302 , f, `) decreases with increasing f\u0302 . For example, consider a document of length 400 with 4 non-overlapped occurrences of \u03b1. The probabilities of observing at least 4, 3, 2, 1 and 0 occurrences of \u03b1 within a span of 20 words are 0.007, 0.09, 0.41, 0.83, and 1.0 respectively. Since \u03c020(3, 4, 400) = 0.09, even if 3 of the 4 occurrences of \u03b1 (in the example document) have span less than 20 words, there is 9% chance that the occurrences were a consequence of a random event (under our null model). As a result, if we desired a confidence-level of at least 95%, we would have to declare \u03b1 as insignificant.\nGiven an (0 < < 1) and a span upperbound x (\u2265 0) the document Di is said to support the hypothesis \u201c\u03b1 is a -significant bigram\u201d if \u03c0x(f\u0302i, fi, `) < . We refer to as the documentlevel lexical co-occurrence of \u03b1. Define indicator variables zi, i = 1, . . . ,K as:\nzi = { 1 if \u03c0x(f\u0302i, fi, `) < 0 otherwise\n(1) Let Z = \u2211K\ni=1 zi; Z models the number of documents (out of K) that support the hypothesis \u201c\u03b1 is a -significant bigram.\u201d The expected value of Z is given by\nE(Z) = K\u2211 i=1 E(zi) (2)\n= K\u2211 i=1 \u03c0x(g (fi, `i), fi, `i) (3)\nwhere g (fi, `i) denotes the smallest f\u0302 for which we can get \u03c0x(f\u0302 , fi, `i) < (This quantity is well-defined since \u03c0x(f\u0302 , fi, `i) is non-increasing with respect to f\u0302 ). For the example given earlier, g0.2(4, 400) = 3 and g0.05(4, 400) = 4.\nUsing Hoeffding\u2019s Inequality, for t > 0,\nP [Z \u2265 E(Z) +Kt] \u2264 exp(\u22122Kt2) (4)\nTherefore, we can bound the deviation of the observed value of Z from its expectation by chosing t appropriately. For example, in our corpus, the bigram (canyon, landscape) occurs in K = 416 documents. For = 0.1, we find that Z = 33 documents (out of 416) have -significant occurrences, while E(Z) is 14.34. Let \u03b4 = .01. By setting t =\u221a ln \u03b4/(\u22122K) = .07, we get E(Z) +Kt = 43.46, which is greater than the observed value of Z (=33). Thus, we cannot be 99% sure that the occurrences of (canyon, landscape) in the 33 documents were a consequence of non-random phenomena. Hence, our test declares (canyon, landscape) as insignificant at = .1, \u03b4 = .01. We formally state the significance test for lexical co-occurrences next:\nDefinition 1 (Significant lexical co-occurrence) Consider a bigram \u03b1 and a set of K documents containing at least one occurrence of \u03b1. Let Z denote the number of documents (out of K) that support the hypothesis \u201c\u03b1 is an -significant bigram (for a given > 0, x > 0)\u201d. The K occurrences of the bigram \u03b1 are regarded -significant with confidence (1 \u2212 \u03b4) (for some user-defined \u03b4 > 0) if we have [Z \u2265 E(Z) + Kt], where t = \u221a log \u03b4/(\u22122K) and E(Z) is given by Eq. (3). The ratio [Z/(E(Z) + Kt)] is called the Co-occurrence Significance Ratio (CSR) for \u03b1.\nWe now describe how to compute \u03c0x(f\u0302i, fi, `i) for \u03b1 in Di. Let N(fi, `i) denote the number of ways of embedding fi non-overlapped occurrences of \u03b1 in a document of length `i. Similarly, let Nx(f\u0302i, fi, `i) denote the number of ways of embedding fi non-overlapped occurrences of \u03b1 in a document of length `i, in such a way that, at least f\u0302i of the fi occurrences have span less than x. Recall that \u03c0x(f\u0302i, fi, `i) denotes the probability of observing a span-constrained frequency (for \u03b1) of at least f\u0302i in a document of length `i that contains a maximum of fi non-overlapped occurrences of \u03b1. Thus,\nwe can assign the probability \u03c0x(f\u0302i, fi, `i) in terms of N(fi, `i) and Nx(f\u0302i, fi, `i) as follows:\n\u03c0x(f\u0302i, fi, `i) =\n( Nx(f\u0302i, fi, `i)\nN(fi, `i)\n) (5)\nTo compute N(fi, `i) and Nx(f\u0302i, fi, `i), we essentially need the histogram for f\u0302 given f and `. Let histf,`[f\u0302 ] denote the number of ways to embed f non-overlapped occurrences of a bigram in a document of length ` in such a way that exactly f\u0302 of the f occurrences satisfy the span constraint x. We can obtain N(fi, `i) and Nx(f\u0302i, fi, `i) from histfi,`i using\nNx(f\u0302i, fi, `i) = fi\u2211 k=f\u0302i histfi,`i [k] (6)\nN(fi, `i) = fi\u2211 k=0 histfi,`i [k] (7)\nAlgorithm 1 ComputeHist(f, `) Input: ` - length of document; f - number of non-\noverlapped occurrences to be embedded; x - span constraint for occurrences Output: histf,`[\u00b7] - histogram of f\u0302 when f occurrences are embedded in a document of length `\n1: Initialize histf,`[f\u0302 ]\u2190 0 for f\u0302 = 0, . . . , f 2: if f > ` then 3: return histf,` 4: if f = 0 then 5: histf,`[0]\u2190 1; 6: return histf,` 7: for i\u2190 1 to (`\u2212 1) do 8: for j \u2190 (i+ 1) to ` do 9: histf\u22121,`\u2212j \u2190 ComputeHist(f \u2212 1, `\u2212 j)\n10: for k \u2190 0 to f \u2212 1 do 11: if (j \u2212 i) < x then 12: histf,`[k + 1] \u2190 histf,`[k + 1] + histf\u22121,`\u2212j [k] 13: else 14: histf,`[k]\u2190 histf,`[k] + histf\u22121,`\u2212j [k] 15: return histf,`\nAlgorithm 1 lists the pseudocode for computing the histogram hf,`. It enumerates all possible ways of embedding f non-overlapped occurrences of a bigram in a document of length `. The main steps in\nthe algorithm involve selecting a start and end position for embedding the very first occurrence (lines 7- 8) and then recursively calling ComputeHist(\u00b7, \u00b7) (line 9). The i-loop selects a start position for the first occurrence of the bigram, and the j-loop selects the end position. The task in the recursion step is to now compute the number of ways to embed the remaining (f \u2212 1) non-overlapped occurrences in the remaining (` \u2212 j) positions. Once we have histf\u22121,`\u2212j , we need to check whether the occurrence introduced at positions (i, j) will contribute to the f\u0302 count. If (j \u2212 i) < x, whenever there are k span-constrained occurrences in positions (j +1) to `, there will be (k+1) span-constrained occurrences in positions 1 to `. Thus, we increment histf,`[k+1] by the quantity histf\u22121,`\u2212j [k] (lines 10-12). However, if (j \u2212 i) > x, there is no contribution to the span-constrained frequency from the (i, j) occurrence, and so we increment histf,`[k] by the quantity histf\u22121,`\u2212j [k] (lines 10-11, 13-14).\nThis algorithm is exponential in f and l, but it does not depend explicitly on the data. This allows us to populate the histogram off-line, and publish the \u03c0x(f\u0302 , f, `) tables for various x, f\u0302 , f and `. (If the paper is accepted, we will make an interface to this table publicly available)."}, {"heading": "3 Utility of CSR test", "text": "Evidence for significant lexical co-occurrences can be gathered at two levels in the data \u2013 documentlevel and corpus-level. First, at the document level, we may find that a surprisingly high proportion of occurrences within a document (of a pair of words) have smaller spans than they would by random chance. Second, at the corpus-level, we may find a pair of words appearing closer-than-random in an unusually high number of documents in the corpus. The significance test of Definition 1 is capable of gathering both kinds of evidence from data in carefully calibrated amounts. Prescribing essentially fixes the strength of the document-level hypothesis in our test. A small corresponds to a strong document-level hypothesis and vice-versa. The second parameter in our test, \u03b4, controls the confidence of our decision given all the documents in the data corpus. A small \u03b4 represents a high confidence test (in the sense that there are a surprisingly large num-\nber of documents in the corpus, each of which, individually have some evidence of relatedness for the pair of words). By running the significance test with different values of and \u03b4, we can detect different types of lexically significant co-occurrences. We illustrate the utility of our test of significance by considering the 4 types of lexical significant cooccurrences\nType A: These correspond to the strongest lexical co-occurrences in the data, with strong documentlevel hypotheses (low ) as well as high corpus-level confidence (low \u03b4). Intuitively, if a pair of words appear close together several times within a document, and if this pattern is observed in a large number of documents, then the co-occurrence is of Type A.\nType B: These are co-occurrences based on weak document-level hypotheses (high ) but because of repeated observation in a substantial number of documents in the corpus, we can still detect them with high confidence (low \u03b4). We expect many interesting lexical co-occurrences in text corpora to be of Type B pairs of words that appear close to each other only a small number of times within a document, but they appear together in a large number of documents.\nType C: Sometimes we may be interested in words that are strongly correlated within a document, even if we observe the strong correlation only in a relatively small number of documents in the corpus. These correspond to Type C co-occurrences. Although they are statistically weaker inferences than those of Type A and Type B (since confidence (1\u2212\u03b4) is lower) Type C co-occurrences represent an important class of relationships between words. If the document corpus contains a very small of number documents on some topic, then strong co-occurrences (i.e. those found with low ) which are unique to that topic may not be detected at low values of \u03b4. By relaxing the confidence parameter \u03b4, we may be able to detect such occurrences (possibly at the cost of some extra false positives).\nType D: These co-occurrences represent the weakest correlations found in the data, since they neither employ a strong document-level hypothesis nor enforce a high corpus-level confidence. In most applications, we expect Type D co-occurrences to be of little use, with their best case utility being to provide a baseline for disambiguating Type C cooccurrences.\nand \u03b4 for the different Types as per Table 1. Finally, we note that Types B and C subsume Type A; similarly, Type D subsumes all three other types. Thus, to detect co-occurrences that are exclusively of (say) Type B, we would have to run the test with a high and low \u03b4 and then remove from the output, those co-occurrences that are also part of Type A."}, {"heading": "4 Experimental Results", "text": ""}, {"heading": "4.1 Datasets and Text Corpus", "text": "Since similarity and relatedness are different kinds of word associations (Budanitsky and Hirst, 2006), in (Agirre et al., 2009) two different data sets, namely 203 words sim (the union of similar and unrelated pairs) and 252 words rel (the union of related and unrelated pairs) datasets are derived from wordsim (Finkelstein et al., 2002). We use these two data sets in our experiments. These datasets are symmetric in that the order of words in a pair is not expected to matter. As some of our chosen co-occurrence measures are asymmetric, we also report results on the asymmetric 272-words esslli dataset for the \u2018free association\u2019 task at (ESSLLI, 2008).\nWe use the Wikipedia (Wikipedia, April 2008) corpus in our experiments. It contains 2.7 million articles for a total size of 1.24 Gigawords. We did not pre-process the corpus - no lemmatization, no function-word removal. When counting document size in words, punctuation symbols were ignored. Documents larger than 1500 words were partitioned keeping the size of each part to no greater than 1500 words.\nIn Table 4.1, we present some examples of different types of co-occurrences observed in the data."}, {"heading": "4.2 Performance of different co-occurrence measures", "text": "We now compare the performance of various frequency-based measures in the context of lexical significance. Given the large numbers of measures\nproposed in the literature (Pecina and Schlesinger, 2006), we need to identify a subset of measures to compare. Inspired by (Janson and Vegelius, 1981) and (Tan et al., 2006) we identify three properties of co-occurrence measure which may be useful for language processing applications. First is Symmetry - does the measure yield the same association score for (x,y) and (y,x)? Second is Null Addition - does addition of data containing neither x nor y affect the association score for (x,y)? And, finally, Homogenity - if we replicate the corpus several times and merge them to construct a larger corpus, does the association score for (x,y) remain unchanged? Note that the concept of homogenity conflicts with the notion of statistical support, as support increases in direct proportion with the absolute amount of evidence. Different applications may need co-occurrence measures having different combinations of these properties.\nTable 3 shows the characteristics of our chosen co-occurrence measures, which were selected from several domains like ecology, psychology, medicine, and language processing. Except Ochiai (Ochiai, 1957), (Janson and Vegelius, 1981), and the recently introduced measure CWCD (Washtell and Markert, 2009)3, all other selected measures are well-known in the NLP community (Pecina and Schlesinger, 2006). Based on our extensive study of theoretical and empirical properties of CSR, we also introduce a new bigram frequency based measure called CSA (Co-occurrence Significance Approximated), which approximates the behaviour of CSR over a wide range of parameter settings.\n3From various so-called windowless measures introduced in (Washtell and Markert, 2009), we chose the best-performing variant Cue-Weighted Co-Dispersion (CWCD) and implemented a window based version of it with harmonic mean. We note that any of windowless (or spanless) measure can easily be thought of as a special case of a window-based measure where the windowless formulation corresponds to a very large window (or span in our terminology).\nIn our experiments, we found that Ochiai and Chi-Square have almost identical performance, differing only in 3rd decimal digits. This can be be explained easily. In our context, for any word x, as defined in Table 3, f(x) << N and therefore p(x) << 1. With this, Chi-Square reduces to square of Ochiai. Similarly Jaccard and Dice coincide, since f(x, y) << f(x) and f(x, y) << f(y). Hence we do not report further results for ChiSquare and Jaccard.\nIn our first set of experiments, we compared the performance of various frequency-based measures in terms of their suitability for detecting lexically significant co-occurrences (cf. Definition 1). A high Spearman correlation coefficient between the ranked list produced by a given measure and the list produced by CSR with respect to some choice of and \u03b4 would imply that the measure is effective in detecting the corresponding type of lexically significant co-occurrences.\nThe Table 4 lists for each measure and for each data set, the different types of lexically significant co-occurrences that the measure is able to detect effectively \u2013 if the corresponding Spearman correlation coefficient exceeds 0.90, we consider the measure to be effective for the given type. Results are shown for three different span constraints \u2013 small\nIn our next experiment, we examine which of the four types of co-occurrences are best captured by each measure. Results for the sim data set are listed in Table 5 (Similar results were obtained on the other data sets). For each measure and for each span constraint, the table describes the best performing parameters ( and \u03b4), the corresponding co-occurrence Type and the associated \u2018best\u2019 correlation achieved with respect to the test of Definition 1 . The results show that, irrespective of the span constraint, most measures perform best on Type A co-occurrences. This is reasonable because Type A essentially represents the strongest correlations in the data and one would expect the measures to capture the strong correlations better than weaker ones. There are how-\never, two exceptions to this rule, namely PMI and CWCD, which instead peak at Types C or D. The best correlations for these two measures are also typically lower than the other measures. We now summarize the main findings from our study:\n\u2022 The relatively obscure Ochiai, and the newly introduce CSA are the best performing measure, in terms of detecting all types of lexical co-occurrences in all data sets and for a wide range of span constraints.\n\u2022 Dice, LLR and TTest are the other measures that effectively track lexically significant cooccurrences (although, all three are less effective as the span constraints become larger).\n\u2022 SCI, CWCD, and the popular PMI measure are ineffective at capturing any notion of lexically significant co-occurrences, even for small span constraints. In fact, the best result for PMI is the detection of Type C co-occurrences in the sim data set. The low and high \u03b4 setting of Type C suggests that PMI does a poor job of detecting the strongest co-occurrences in the data, overlooking both strong document-level as well as corpus-level cues for lexical significance.\nNote that our results do not contradict the utility of PMI, SCI, or, CWCD as word-association measures. We only observe their poor performance in context of detecting lexical co-occurrences. Also, our notion of lexical co-occurrence is symmetric. It is possible that asymmetric SCI may have competi-\ntive performance for certain asymmetric tasks compared to the better performing symmetric measures. Finally, to give a qualitative feel about the differences in the correlations preferred by different methods, in Table 6, we show the top 10 bigrams picked by PMI and Ochiai for all three datasets."}, {"heading": "5 Relation between lexical co-occurrence and human judgements", "text": "While the focus of our work is on characterizing the statistically significant lexical co-occurrence, as illustrated in in Table 7, human judgement of word association is governed by many factors in addition to lexical co-occurrence considerations, and many non co-occurrence based measures have been designed to capture semantic word association. Notable among them are distributional similarity based measures (Agirre et al., 2009; Bollegala et al., 2007; Chen et al., 2006) and knowledge-based measures (Milne and Witten, 2008; Hughes and Ramage, 2007; Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Wandmacher et al., 2008). Since our focus is on frequency based measures alone, we do not discuss these other measures.\nThe lexical co-occurrence phenomenon and the human judgement of semantic association are related but different dimensions of relationships between words and different applications may prefer one over the other. For example, suppose, given one word (say dirt), the task is to choose from among a number of alternatives for the second(say grime\nand filth). Human judgment scores for (dirt, grime) and (dirt, filth) are 5.4 and 6.1 respectively. However, their lexical co-occurrence scores (CSR) are 1.49 and 0.84 respectively. This is because filth is often used in a moral context as well. Grime is usually used only in a physical sense. Dirt is used mostly in a physical sense, but is a bit more generic and may be used in a moral sense occasionally. Hence (dirt, grime) is more correlated in corpus than (dirt, filth). This shows that human judgement is fallible and annotators may ignore the subtleties of meanings that may be picked up by a statistical techniques like ours.\nIn general, for association with a given word, all synonyms of a second word will be given similar semantic relatedness score by human judges but they may have very different lexical association scores.\nFor applications where the notion of statistical lexical co-occurrence is potentially more relevant than semantic relatedness, our method can be used to generate a gold-standard of lexical association (against which other association measures can be evaluated). In this context, it is interesting to note that contrary to the human judgement, each one of the co-occurrence measures studied by us finds (dirt, grime) more associated than (dirt, filth).\nHaving explained that significant lexical cooccurrence is a fundamentally different notion than human judgement of word association, we also want\nto emphasize that the two are not completely different notions either and they correlate reasonably well with each-other. For sim, rel, and essli datasets, CSR\u2019s best correlations with human judgment are 0.74, 0.65, and 0.46 respectively. Note that CSR is a symmetric notion and hence correlates far more with human judgement for symmetric sim and rel datasets than for the asymmetric essli dataset. Also, at first glance, it is little counter-intuitive that the notion of lexical co-occurrence yields better correlations with the sim (based on similarity) data set when compared to the rel(based on relatedness) data set. This can essentially be explained by our observation that similar words tend to co-occur less frequently bychance than the related words."}, {"heading": "6 Conclusions", "text": "In this paper, we introduced the notion of statistically significant lexical co-occurrences. We detected skews in span distributions of bigrams to assess significance and showed how our method allows classification of co-occurrences into different types. We performed experiments to assess the performance of various frequency-based measures for detecting lexically signficant co-occurrences. We believe lexical co-occurrence can play a critical role in several applications, including sense disambiguation, mutliword spotting, etc. We will address some of these in our future work."}], "references": [{"title": "Marius Pasca", "author": ["Agirre", "Eneko", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova"], "venue": "and Aitor Soroa.", "citeRegEx": "Agirre et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Yutaka Matsuo", "author": ["Bollegala", "Danushka"], "venue": "and Mitsuru Ishizuka.", "citeRegEx": "Bollegala et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Evaluating wordnet-based measures of lexical semantic relatedness", "author": ["Budanitsky", "Hirst2006] Budanitsky", "Alexander", "Graeme Hirst"], "venue": "Computational Linguists,", "citeRegEx": "Budanitsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Budanitsky et al\\.", "year": 2006}, {"title": "Ming-Shun Lin", "author": ["Chen", "Hsin-Hsi"], "venue": "and Yu-Chuan Wei.", "citeRegEx": "Chen et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Word association norms, mutual information and lexicography", "author": ["Church", "Hanks1989] Church", "Kenneth Ward", "Patrick Hanks"], "venue": "In ACL,", "citeRegEx": "Church et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Church et al\\.", "year": 1989}, {"title": "L", "author": ["Dice"], "venue": "R.", "citeRegEx": "Dice1945", "shortCiteRegEx": null, "year": 1945}, {"title": "2008", "author": ["ESSLLI"], "venue": "Free association task at lexical semantics workshop esslli", "citeRegEx": "ESSLLI2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Gadi Wolfman", "author": ["Finkelstein", "Lev", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan"], "venue": "and Eytan Ruppin.", "citeRegEx": "Finkelstein et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis", "author": ["Gabrilovich", "Markovitch2007] Gabrilovich", "Evgeniy", "Shaul Markovitch"], "venue": null, "citeRegEx": "Gabrilovich et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2007}, {"title": "Lexical semantic relatedness with random graph walks", "author": ["Hughes", "T Ramage2007] Hughes", "D Ramage"], "venue": null, "citeRegEx": "Hughes et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hughes et al\\.", "year": 2007}, {"title": "The distribution of the flora of the alpine zone", "author": ["P. Jaccard"], "venue": "New Phytologist,", "citeRegEx": "Jaccard,? \\Q1912\\E", "shortCiteRegEx": "Jaccard", "year": 1912}, {"title": "Measures of ecological association", "author": ["Janson", "Vegelius1981] Janson", "Svante", "Jan Vegelius"], "venue": "Oecologia,", "citeRegEx": "Janson et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Janson et al\\.", "year": 1981}, {"title": "Language is never ever ever random. Corpus Linguistics and Linguistic Theory, 1(2):263\u2013276", "author": ["Kilgarriff", "Adam"], "venue": null, "citeRegEx": "Kilgarriff and Adam.,? \\Q2005\\E", "shortCiteRegEx": "Kilgarriff and Adam.", "year": 2005}, {"title": "David and Ian H", "author": ["Milne"], "venue": "Witten.", "citeRegEx": "Milne and Witten2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Combining association measures for collocation extraction", "author": ["Pecina", "Schlesinger2006] Pecina", "Pavel", "Pavel Schlesinger"], "venue": null, "citeRegEx": "Pecina et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pecina et al\\.", "year": 2006}, {"title": "Wikirelate! computing semantic relatedness using wikipedia", "author": ["Strube", "Ponzetto2006] Strube", "Michael", "Simone Paolo Ponzetto"], "venue": "In AAAI,", "citeRegEx": "Strube et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strube et al\\.", "year": 2006}, {"title": "Michael Steinbach", "author": ["Tan", "Pang-Ning"], "venue": "and Vipin Kumar.", "citeRegEx": "Tan et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and T", "author": ["T. Wandmacher", "E. Ovchinnikova"], "venue": "Alexandrov.", "citeRegEx": "Wandmacher et al.2008", "shortCiteRegEx": null, "year": 2008}, {"title": "A comparison of windowless and window-based computational association measures as predictors of syntagmatic human associations", "author": ["Washtell", "Markert2009] Washtell", "Justin", "Katja Markert"], "venue": null, "citeRegEx": "Washtell et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Washtell et al\\.", "year": 2009}, {"title": "Eneko Agirre", "author": ["Yeh", "Eric", "Daniel Ramage", "Chris Manning"], "venue": "and Aitor Soroa.", "citeRegEx": "Yeh et al.2009", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [], "year": 2010, "abstractText": "Lexical co-occurrence is an important cue for detecting word associations. We present a theoretical framework for discovering statistically significant lexical co-occurrences from a given corpus. In contrast with the prevalent practice of giving weightage to unigram frequencies, we focus only on the documents containing both the terms (of a candidate bigram). We detect biases in span distributions of associated words, while being agnostic to variations in global unigram frequencies. Our framework has the fidelity to distinguish different classes of lexical co-occurrences, based on strengths of the document and corpuslevel cues of co-occurrence in the data. We perform extensive experiments on benchmark data sets to study the performance of various co-occurrence measures that are currently known in literature. We find that a relatively obscure measure called Ochiai, and a newly introduced measure CSA capture the notion of lexical co-occurrence best, followed next by LLR, Dice, and TTest, while another popular measure, PMI, suprisingly, performs poorly in the context of lexical co-occurrence.", "creator": "LaTeX with hyperref package"}}}