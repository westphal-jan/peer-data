{"id": "1103.5034", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2011", "title": "On Understanding and Machine Understanding", "abstract": "regarding the latter paper, bell then next propose a less - similar convergence theory surrounding the basic understanding. by extending certain natural languages to a kind simply so called idealy sufficient language, we best proceed a few steps to facilitate investigation as the software modelling and query language understanding of ai.", "histories": [["v1", "Thu, 24 Mar 2011 03:35:24 GMT  (8kb)", "http://arxiv.org/abs/1103.5034v1", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tong chern"], "accepted": false, "id": "1103.5034"}, "pdf": {"name": "1103.5034.pdf", "metadata": {"source": "CRF", "title": "On Understanding and Machine Understanding", "authors": ["Tong Chern"], "emails": ["tongchen@ihep.ac.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 3.\n50 34\nv1 [\ncs .A\nI] 2\n4 M\nIn the present paper, we try to propose a self-similar network theory for the basic understanding. By extending the natural languages to a kind of so called idealy sufficient language, we can proceed a few steps to the investigation of the language searching and the language understanding of AI.\nImage understanding, and the familiarity of the brain to the surrounding environment are also discussed. Group effects are discussed by addressing the essense of the power of influences, and constructing the influence network of a society. We also give a discussion of inspirations.\n1tongchen@ihep.ac.cn\nContents"}, {"heading": "1 Introduction 2", "text": ""}, {"heading": "2 The Self-Similar Neural Network Of Understanding 3", "text": "2.1 The self-similar network for basic understanding . . . . . . . . 3 2.2 The Self-Similar Network For Understanding At Large Scale\n\u2013 On Knowledge Managment System . . . . . . . . . . . . . . 4"}, {"heading": "3 The Theory of Self-Similar Neural-Network for Idealy Sufficient Language 5", "text": "3.1 The Idealy Sufficient Languages For Knowledge Representation 5 3.2 Sentance And Text . . . . . . . . . . . . . . . . . . . . . . . . 6 3.3 Stories and Inspirations . . . . . . . . . . . . . . . . . . . . . . 7 3.4 For living languages . . . . . . . . . . . . . . . . . . . . . . . . 8 3.5 To Construct The Network Mathematically . . . . . . . . . . . 8"}, {"heading": "4 Picture Understanding and Familiarity 9", "text": "4.1 Picture Understanding . . . . . . . . . . . . . . . . . . . . . . 9 4.2 Familiarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10"}, {"heading": "5 From Understanding to Influence 12", "text": "5.1 The Power of Influence Is Amount of Computations (of the\nhuman group) Influenced By The Individual . . . . . . . . . . 12 5.2 Stories And Inspirations . . . . . . . . . . . . . . . . . . . . . 13\n6 On Machine Understanding 14"}, {"heading": "1 Introduction", "text": "What is the principle behind the human brain? of understanding the world. What is the mechanism of the inter- influences of a group of people in everyday life? How? the ideas and thinkings are exchanged. Talking, reading and then understanding, then generating influences, and then changing the infrastructure of the brain. Then express, talk, write and read, the development of the infrastructure will develop the whole future.\nAt the present paper, we try to develop a neatly theory for the basic\nunderstanding of the brain, especially for the understanding of language.\nWe\u2019ll begin from an investigation to the understanding process of the human brain, and then proceed to investigate the inter-influences of a group of people (such as a society). To measure and to investigate the neural processes of the brain, one need to tomograghy it by utilizing some suitable neural networks of artificial intelligence. We suggest that the self-similar networks are suitable for the neural understanding process. This consists section 2.\nWe then translate the self-similar network for basic understanding to the knowledge representations, by defining a kind of idealy sufficient language. After that, we continue to consider the conceptral structures of this language, and the representation of sentance, sentances and text on the network. The patterns of stories and inspirations are also discussed at the same section, section 3.\nAt section 4, we proceed to investigate the image understanding, and the\nfamiliarity of the brain to the surrounding environment.\nAt section 5, we\u2019ll understand that the essense of the power of influences of an individual, is at the amount of computations influenced by that people. Where, the amount of computation refers to the amount of logical computations of thinking processes, when measured by using some suitable neural networks of artificial intelligence (AI) and after a suitable tomography to the brain.\nHaving quantified the power of influence. We can proceed to consider the exchanges of thinkings and ideas, and the influences of these exchangments. Thus, we can investigate the network of the inter-influences of a society.\nAt the last section, we apply our investigations to basic understanding and the language understanding to machine understanding, especially for the O(1) sentance understanding, which is central both to the language searching of AI and to the machine translation (for O(1) sentance conversation)."}, {"heading": "2 The Self-Similar Neural Network Of Un-", "text": "derstanding"}, {"heading": "2.1 The self-similar network for basic understanding", "text": "As a process of the brain, not a logical program processing on a determine computer, understanding is hard to be or cannot be translated into a logical computational process. But by using all kinds of suitable neural networks of artifical intelligence, e.g. self similar neural networks or any kinds of complex networks, to tomograph the whole state of the creating, breaking out and annihilating neuralcircuits of the human brain. One can translate a neural process of thinking into a logical computational process.\nSelf-similar neural networks may be the most natural choice to tomograph any suitable basic understanding processes, which concerning the understanding of basic concepts and the inter-relationships between them, since basic understanding is natural and universal to everyone. And the network (for basic understanding) should be naturally scaled according to the power of awarenese.\nTo some extance, all these claims can be proofed by the observations and\nexperiments of cognitive neuroscience.\nAn irrelevant but interesting and promoting example, from the article posted by Steve Hsu [http://infoproc.blogspot.com], The city buildings of a big domain of the earth emerge the property of self-similarity.\nTo the understanding of natural language, the meaning of words, sen-\ntances and maybe O(1) sentance texts, what we just said is that it can be viewed as a process taking place at the self-similar network. This claim, if confirmed, will be important to the language searching, for it said that the language searching of AI is a P problem. We\u2019ll continue the investigation to this problem, in much more detail, at section 3."}, {"heading": "2.2 The Self-Similar Network For Understanding At", "text": "Large Scale \u2013 On Knowledge Managment System\nFrom other hand side, let\u2019s consider the understanding taking place at large scale. Let\u2019s consider the optimal knowledge management of an individual. The conclusion is clear, it is a self-similar network scaled according to the vision of values of individual (with Jia). The optimal thinking process of an individual is a searching process, on the self-similar network, according to the importance of values.\nIn the vision of values of every individual, one may rescale the values of different disciplines, e.g. in my reflection, the value of pure theoretical physics weights 1, the value of poetry weights 0.85, and other subjects take small weights, say, weights 0.6, 0.5 e.t.c. In another word, in the vision of values of me, to count the values of multidisciplines, i\u2019ll time different factor of weights to different subjects. Different people holds different systems of values. The subjects at the center of systems of values, and the weight factors of different subjects, both are different for different people. The knowledge systems and thinking modes of different people will put different subject at the center, according to their personal values (with Jia).\nWe now can conclude that it is a natural conseqense that, the self-similar network of understanding at large scale is scaled according to personal vision of values.\nLet\u2019s compare the personal knowledge systems with the knowledge systems created by the cooperations of many people, such as the Wikipedia. One will find that: 1, the knowledge systems of Wikipedia will slowly evolve to a self-similar network. 2, the self-similar network of Wikipedia consists of\nthe nodes of knowledges and their correlations reflected in the connections of the network. 3, the reason behind this evolution is the self-similar knowledge systems of the brain of optimal management to the knowledges. This, is the deep reason of the existence and self stablization of the Wikipedia, since the human brain is self stablized by the evolution.\nLet\u2019s summarize what we have said, in a few words: The basic understanding of human brain can be tomographied into a self-similar network, by translating it, one can get the self-similar network of concepts. We\u2019ll investigate this network of words in detail at next section."}, {"heading": "3 The Theory of Self-Similar Neural-Network", "text": "for Idealy Sufficient Language"}, {"heading": "3.1 The Idealy Sufficient Languages For Knowledge", "text": "Representation\nOur theory of self-similar network is for a kind of idealy sufficient language for knowledge representation. This language, besides complete and consistent, also must be sufficient for any expressions, communications, with sufficient accuracy, and can be used to conceptualization to any given measure (Thus an infinite numbers of words must be required)\nOne can construct a network for this language, by using the fuzzy mathematical relationships between words (including concepts that we will define at below), which we will further discuss in some detail at the subsection 3 of this section.\nThe network for our idealy sufficient language is obviously large than the semantic network of words. And a most natural assumption about it is that:\nIt is a self-similar network with many glomerate words structured like Mandelblot sets! Or in another words, the network looks like the true neural network of the brain.\nIn our network, the glomerate words may be used to represent concept\u2013 One may compare this description of concept with the containment model of\nconceptual structure.\nBut we did not divide concepts into prior concepts and experimental concepts (deduced from experiences), or divide into basic concepts and induced concepts (defined by the basic ones). But, in our description, small concepts can accumulate up, to form a glomeration of concepts, to define a bigger one. And this, is the basic meaning of the Mandelblot like behavior of the glomeration of concepts.\nIt may need special notices that one can value an abstract word as a node of the network to symbolize any specific bigger concept. But the core hided behand a specific concept is not at such a word, but at the accumulation of the concepts used to define this concept, in another word, at the glomeration of relevant concepts. Especially, for small concepts, the accumulation of relevants is just accumulation of words (relevant to these concepts). It is this accumulation, of concepts or words, that defines the concept.\nBut for completion, to stand for a concept, we need to evalue an abstract word into the accumulated glomeration for the defined concept, by using the inter- relationships between the definitional words and the definition!\nThe procedure of this evaluation is somewhat arbitrary, which is a reflection of the ordinary arbitrary of the different specific definitions of the same essense of a concept. Thus, the accumulation of concepts is essential but the specific evaluation is not essential.\nOne may call the accumulated glomeration as the module of the defined concept. In this expression, The term of concept is essentially refered to the module of it, and the stand for word of the concept is not essential."}, {"heading": "3.2 Sentance And Text", "text": "We now turn to the charactrization of a sentance and an O(n) sentances\ntext on the self similar network defined at above subsections.\n1. Sentance on the net-work of words. On our network, a sentance is a string of words conneted by relationships, it is a simple pattern of trace on the network, begins from or passes at least one small glomeration of words,\nbut not many glomerations on its trace.\nThe trace of sentance may be self intersected or may not, but must be\nregular to some certain degree, for sentance must be meaningful.\nAre there some certain logical algorithms to generate an arbitrary such\ntrace? and how to distinguish two such traces (thus two sentances)?\n2. O(n) Sentances text on the net-work of words. For an O(n) sentances text, the corresponding pattern on the network must be much more complicated. We now encounter the problems of, how to represent the relationship of two sentances in a O(1) sentances text? and whether it\u2019s possible, or not, to represent the relationships between the sentances of a general O(n) sentances text, since there are many obstructures of social intelligence!\n3. Before understanding and after understanding (with Jia). Before the understanding, a text is a set of fragements of words. The meaningful relationships between words, sentances or between words and sentance are meaningless and lost. Only after understanding the text becomes living and fruitful, all the relationships become meaningful!"}, {"heading": "3.3 Stories and Inspirations", "text": "Little children like listening stroies, for story can help them to understand. From the viewpoint of brain science, it is because a story emerges as a meaningful and fruitful structure in the true neural network of the brain.\nThus, after tomographing the human brain and translating the neural processes into the self-similar network of idealy sufficient languages (representing the understanding), stories will emerge as structures, or patterns on the network.\nDid these structures or these patterns can be generated by the ant algorithm or by the bee algorithm? (suggested by Jia) or be generated by some kinds of more complicated algorithms?\nWe now turn to another problem that have partially been discussed by the brain scientist and me (at some times ago): the logical expansions of inspirations.\nIn the inspired state of the brain, the problem to be solved, the key elements and the full solution are emerged at the same time. But in the logical generations of them, it may need a text with many stentances interrelated logically, or even it cannot be expressed as a logical process. This, is the logical expansions of inspirations.\nWe now can proceed a further step, from the viewpoint of tomographing the neural processes of the brain. An inspiration is a structure or a pattern on the self-similar network. To create this pattern logically, the logical program will be many many steps or even super long. But benefits from the tomographing, all we have said can now be quantified and measured to some extance.\nBut, this brings a big problem, that the inspiration that can be measured from the brain tomographing, may be not the highly top creative neural process, but only a process that needs super many logical computations.\nThe highly top inspirations, the lights of highly top talents, may cannot be tomographied. Since it hides in the infinite correlations between infinite many tomographing neural networks of AI, any specific tomography will beak it into pieces and lost. The highly top inspirations, it comes from the heaven! a lightening crystal!"}, {"heading": "3.4 For living languages", "text": "The living languages are obviously not so idea and sufficient, but since they are used in ordinary life, we may view any one of it as an approximation, to some degree of accuracy, to the idealy sufficient language defined by the self-similar network of it. Thus, what we have said may be useful for the natural languages of ordinary life, such as chinese."}, {"heading": "3.5 To Construct The Network Mathematically", "text": "We know turn to the logical or mathematical construction of the neural network of words for the idealy sufficient language, this network we previously got by translating the neural circuits.\nTo construct the network (for the idealy sufficient language, or in another word, to construct the self-similar neural network of the understanding at basic level), one needs concepts and its properties, as the objects and its artributes of the network, respectively. And then, One also needs to investigate the relationships between these concepts(as the relationships of the objects). The typical relationships include belongs to and cooperation relationship and basic logical relationships, such as iterations and choices. The weight consideration and the probability thinking are also improtant.\nIt is obvious, fuzzy mathematica is an appropriate consistent framework\nto incorporate all these elements.\nWhat elements are important to the construction of a self-similar network? It looks like that objects, weights of objects and the relationships of corporation and belongs to and iterations, are important. Where, the weights are used to rescale the whole network, thus have no absolute meanings.\nBy using fuzzy mathematica to the idealy sufficient language, one can construct the needed neural network of words and concepts. Given a sentance or some sentances or a text of the idealy sufficient language, one can represent them as patterns at the network of words\u2013by using the fuzzy mathematica to deal with various relationships . e.g. The represented sentance will be a pattern of trace, that we have discussed.\nBy reversing the above procedure, one can reconstruct sentance, sentances and maybe text from given patterns on the neural network. Although the specific algorithms of the construction is obscure to the author presently. Thus, at the existence level, we have answered the question of creating a sentance, of section 3 subsection 1."}, {"heading": "4 Picture Understanding and Familiarity", "text": ""}, {"heading": "4.1 Picture Understanding", "text": "We now turn to investigate the principle of understanding images! Firstly, i think that there must be a quite ordinary process, just like the\nordinary operations to the image processing, such as the compressed sensing. In some details, the brain will cut the whole image into small pieces, every piece is a simple unit characterized by some simple properties, a zoo factor, a quite simple color property and its intensity and texture of pixels e.t.c.\nAs in the compressed sensing, with a holding fixed fidelity, a zoo factor is needed to reduce the amount of datas, since one need only zoo out the more complicated part of the image to watch it more clearly.\nIn the understanding of image, effectively, the brain will apply the selfsimilar networks of the basic understanding to these simple units to structure them. One then have the image self-similar network, which is scaled according to the power of awarenese also.\nOn this network, the small pieces of the image are as objective nodes, and the relationships between any two of these pieces may be induced from the natural color relations, relations of pixels and zoo factors e.t.c.\nThere is a somewhat confusing problem. How to preserve the space position structures of the image? The answer is quite simple: the brain understands and checks the position structure by using the usual logical relationships of space positions. e.g. the small pieces may value space coordinates, and the brain can order the position structure according to these coordinates."}, {"heading": "4.2 Familiarity", "text": "We now turn to another problem, named familiarity! For a familiar environment people are used to, quite an amount of the detail information of the environment that stay static or only changing slowly \u2013 such as the trees, the color of the leaves \u2013 will be registed in a \u201dregister\u201d of the brain, and persisting for seconds, months, or even years, just like Cowan\u2019s theory for \u201dattention and memory\u201d [Cowan, N. (1995). Attention and memory: An integrated framework. New York: Oxford University Press; Cowan, N. (2005). Working memory capacity. New York, NY: Psychology Press]. An especial test is that, in his memory one cannot recall any person clearly, but can see clear or even colorful figures in the dream. This is because,\nin the dream the brain takes out the needed details from the register.\nThe brain behavior like this for reducing the amount of computations. The brain deal with those detail and static information only once and then put it into the register for the future utilizing, since every re-computation to those information requires a huge amount of computational utilities.\nAt every moment, the brain will focus on more microscopic and changing fast parts of the world, such as the riple of the water after the breeze, and the waves of the leaves. The brain symbolize these parts at the psycological domain, keep them immediatly responding to the environment, and transfer them into the other domain of the brain to be understood.\nBy evaluating these responding symblozations with the details from the register, the brain can repair the simplized microscopic and fast changing parts after a simple understanding. And after a further understanding, the brain can \u201dsee\u201d the livingly riple and the colorful waving leaves.\nAll the responding symblozations belong to awarenesses. The brain also symbolize things (at the psycological domain) that have been understood, such as the sense of humor . These symbolizations are much more static, comparing with the responding symblozations. Both the static symbolizations and the details of register contain familiar contents, thus belong to the Familiarities of the brain.\nAll the familiarities will be structured in the understanding of the brain,\nto form a global undstand to the whole environment.\nBy calling the symbolizations to call the Familiarities, is to reduce the amount of computations, which is a kind of self protection of the brain. For a creator \u2013 an artist for example \u2013, to overcome this self protectional mechanism of the brain is a central dogma (of trains and practices) to become an artist [Thinking Like An Artist]. That is: watch the world as the world itself, not the world in our brain!\nNote added: The idea of symbolization came from the discusses with the student of the brain scientist. It\u2019s used to describe the psycological states, means that the\npsycological states are indexes of the neural processes of psycological domain of the brain.\nBoth the lacking of the be present and the collapsing of the memory are a kind of symolization (with Jia) . The memory collapsing may be related to the register of details, that is to say that the memory does not deal with the details in register, they are lacking.\nThis moment and this place are the whole infinity!"}, {"heading": "5 From Understanding to Influence", "text": "After understanding, the understood texts or stories or knowledges will influence the thinking of human brain. We now turn to investigate the quantification of influences and influences network of a group of people\u2013the society. The power of influences of individuals consists the nodes of the influence network, The inter-influences between two of the peoples give us the relationship needed for the network construction."}, {"heading": "5.1 The Power of Influence Is Amount of Computa-", "text": "tions (of the human group) Influenced By The In-\ndividual\nFirstly, the power of influences of an individual in a society is amount of computations influenced by this person. The amount of computations is a measurement of thinkings of the influenced people.\nIt is obvious that to measure the amount of thinkings, one need to tomograph the true neural processes by some appropriate AI networks. After that, the power of influences can be measured as amount of logical computations of the thinking which is measured by the tomographing AI networks, see section 2.\nThus, we can conclude that the power of influences of an individual in a\nsociety is the total amount of computations influenced by this guy.\nA point that may deserve a clarification is that, the social processes itself\nare computational processes, which need an appropriate tomographing since the complexity of it is typically at NP order. In the present paper, all we consider and need to consider are the inter-influences of people, thus we need only tomograph the social process as a network of influences. The influences of individuals are the weights of this influence network.\nObviously, besides the objects of power of influences, all other fuzzy mathematical elements, relationships (corporation, belongs to, e.t.c.) and various fuzzy logics are encoded in the influence network naturally."}, {"heading": "5.2 Stories And Inspirations", "text": "At section 4, we have discussed the representation and the understanding\nof the stories and inspirations. Now, we turn to the group effects of them.\nCorresponding to the patterns on the self-similar network of understanding, the stories, especially the stories read and transfered by the society, or the stories created by groups, such as the mud game, will also create structures on the network of influences. It\u2019s a reflection of the group effects!\nBut one cannot find the social correspondences of inspirations. This is\nbecause:\nFirstly, as we have discussed, the highly top inspirations are hided in the infinite correlations of infinite neural networks of artificial intelligence, and cannot be tomographied and measured. And thus, they have no correspondence in the influence network of a society. The influences of it cannot be confined at this moment and this place, it belongs to infinity!\nIn fact, even more ordinary creative inspirations, even the inspirations that can be logically expansed, are difficult to find social correspondence in the influence network.\nIt is because that, the lightening of the light in the attic, belongs to not\nonly this moment, not only this place!"}, {"heading": "6 On Machine Understanding", "text": "Now, let\u2019s image an application of our principle of understanding \u2013 to the machine understanding! How to understand a person for a machine? e.g. How to understand the natural language of the human being? and How to translate these words into the logical computer program?\nHow to understand a sentance of the human being for a machine? The first step is quite similar to the natural understanding discussed in section 3. Machine will break the sentance into pieces of words and concepts, and the relationships between them, according to the network for understanding. \u2013That is, to represent the sentance on the self-similar neural network of the idealy sufficient language (with the natural language included in).\nThe second step is by translating the pieces into computer orders, according to a logical algorithm, for example, a kind of searching algorithms that appropriate to the whole network (its objects, encoded relationships e.t.c), This algorithm must be existance as we have addressed, but we do not know a concrete algorithm as an illustrating example, presently.\nBut for an O(n) sentances text, things may be quite different, since the\nproblems of social intelligence are now become relevant and important.\nAcknowledgement See acknowledgements of the series of papers in chinese posted at my web\npages: http://mozartchern.blogspot.com http://mozartchern.wordpress.com."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "In the present paper, we try to propose a self-similar network theory for the basic understanding. By extending the natural languages to a kind of so called idealy sufficient language, we can proceed a few steps to the investigation of the language searching and the language understanding of AI. Image understanding, and the familiarity of the brain to the surrounding environment are also discussed. Group effects are discussed by addressing the essense of the power of influences, and constructing the influence network of a society. We also give a discussion of inspirations.", "creator": "LaTeX with hyperref package"}}}