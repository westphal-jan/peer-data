{"id": "1108.6211", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2011", "title": "Transfer from Multiple MDPs", "abstract": "transfer reinforcement learning ( sip ) methods use on the experience collected on a set of mixed tasks to drive - up rl algorithms. a simple only basic approach is to transfer strengths before desired tasks and include them into the training room alone to solve this given reinforcement task. in this paper, we found general theoretical variation of this transfer method and we introduce similarity validation by the transfer process owes the basis its residual similarity between copy and target tasks. finally, we reject illustrative experimental results versus complex balanced search problem.", "histories": [["v1", "Wed, 31 Aug 2011 12:46:11 GMT  (70kb)", "https://arxiv.org/abs/1108.6211v1", "2011"], ["v2", "Thu, 1 Sep 2011 09:19:00 GMT  (70kb)", "http://arxiv.org/abs/1108.6211v2", "2011"]], "COMMENTS": "2011", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["alessandro lazaric", "marcello restelli"], "accepted": true, "id": "1108.6211"}, "pdf": {"name": "1108.6211.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["alessandro.lazaric@inria.fr", "restelli@elet.polimi.it"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 8.\n62 11\nv2 [\ncs .A\nI] 1"}, {"heading": "1 Introduction", "text": "The objective of transfer in reinforcement learning (RL) [12] is to speed-up RL algorithms by reusing knowledge (e.g., samples, value function, features, parameters) obtained from a set of source tasks. The underlying assumption of transfer methods is that the source tasks (or a suitable combination of these) are somehow similar to the target task, so that the transferred knowledge can be useful in learning its solution. A wide range of scenarios and methods for transfer in RL have been studied in the last decade (see [14, 9] for a thorough survey). In this paper, we focus on the simple transfer approach where trajectory samples are transferred from source MDPs to increase the size of the training set used to solve the target MDP. This approach is particularly suited in problems (e.g., robotics, applications involving human interaction) where it is not possible to interact with the environment long enough to collect samples to solve the task at hand. If samples are available from other sources (e.g., simulators in case of robotic applications), the solution of the target task can benefit from a larger training set that includes also some source samples. This approach has been already investigated in the case of transfer between tasks with different state-action spaces in [13], where the source samples are used to build a model of the target task whenever the number of target samples is not large enough. A more sophisticated sample-transfer method is proposed in [8]. The authors introduce an algorithm which estimates the similarity between source and target tasks and selectively transfers from the source tasks which are more likely to provide samples similar to those generated by the target MDP. Although the empirical results are encouraging, the proposed method is based on heuristic measures and no theoretical analysis of its performance is provided. On the other hand, in supervised learning a number of theoretical works investigated the effectiveness of transfer in reducing the sample complexity of the learning process. In domain adaptation, a solution learned on a source task is transferred to a target task and its performance depends on how similar the two tasks are. In [2] and [10] different distance measures are proposed and are shown to be connected to the performance of the transferred solution. The case of transfer of samples from multiple source tasks is studied in [3]. The most interesting finding is that the transfer performance benefits from using a larger training set at the cost of an additional error due to the average distance between source and target tasks. This implies the existence of a transfer tradeoff between transferring as many samples as possible and limiting the transfer to sources which are similar to the target task. As a result, the\ntransfer of samples is expected to outperform single-task learning whenever negative transfer (i.e., transfer from source tasks far from the target task) is limited w.r.t. to the advantage of increasing the size of the training set. This also opens the question whether it is possible to design methods able to automatically detect the similarity between tasks and adapt the transfer process accordingly.\nIn this paper, we investigate the transfer of samples in RL from a more theoretical perspective w.r.t. previous works. The main contributions of this paper can be summarized as follows:\n\u2022 Algorithmic contribution. We introduce three sample-transfer algorithms based on fitted Qiteration [4]. The first algorithm (AST in Section 3) simply transfers all the source samples. We also design two adaptive methods (BAT and BTT in Section 4 and 5) whose objective is to solve the transfer tradeoff by identifying the best combination of source tasks.\n\u2022 Theoretical contribution. We formalize the setting of transfer of samples and we derive a finite-sample analysis of AST which highlights the importance of the average MDP obtained by the combination of the source tasks. We also report the analysis for BAT which shows both the advantage of identifying the best combination of source tasks and the additional cost in terms of auxiliary samples needed to compute the similarity between tasks.\n\u2022 Empirical contribution. We report results (in Section 6) on a simple chain problem which confirm the main theoretical findings and support the idea that sample transfer can significantly speed-up the learning process and that adaptive methods are able to solve the transfer tradeoff and avoid negative transfer effects.\nThe rest of the paper is organized as follows. In Section 2 we introduce the notation and we define the transfer problem. Section 3 reports the theoretical analysis of AST. BAT is described in Section 4 along with its theoretical analysis. A more challenging setting is introduced in Section 5 together with BTT. Section 6 reports the experimental results and Section 7 concludes the paper. Finally, in the appendix we report the proofs and some additional experimental analysis."}, {"heading": "2 Preliminaries", "text": "In this section we introduce the notation and the transfer problem considered in the rest of the paper.\nNotation for MDPs. We define a discounted Markov decision process (MDP) as a tuple M = \u3008X ,A,R,P , \u03b3\u3009 where the state space X is a bounded closed subset of the Euclidean space, A is a finite (|A| < \u221e) action space, the deterministic1 reward function R : X \u00d7A \u2192 R is uniformly bounded by Rmax, the transition kernel P is such that for all x \u2208 X and a \u2208 A, P(\u00b7|x, a) is a distribution over X , and \u03b3 \u2208 (0, 1) is a discount factor. We denote by S(X \u00d7A) the set of probability measures overX \u00d7A and by B(X \u00d7A;Vmax= Rmax1\u2212\u03b3 ) the space of bounded measurable functions with domain X \u00d7A and bounded in [\u2212Vmax, Vmax]. We define the optimal action-value function Q\u2217 as the unique fixed-point of the optimal Bellman operator T : B(X \u00d7A;Vmax) \u2192 B(X \u00d7A;Vmax) defined by\n(T Q)(x, a) = R(x, a) + \u03b3 \u222b\nX\nmax a\u2032\u2208A\nQ(y, a\u2032)P(dy|x, a).\nNotation for function spaces. For any measure \u00b5 \u2208 S(X \u00d7A) obtained from the combination of a distribution \u03c1 \u2208 S(X ) and a uniform distribution over the discrete set A, and a measurable function f : X \u00d7A \u2192 R, we define the L2(\u00b5)-norm of f as ||f ||2\u00b5 = 1|A| \u2211 a\u2208A \u222b X f(x, a)2\u03c1(dx). The supremum norm of f is defined as ||f ||\u221e = supx\u2208X |f(x)|. Finally, we define the standard L2-norm for a vector \u03b1 \u2208 Rd as ||\u03b1||2 = \u2211d i=1 \u03b1 2 i . We denote by \u03c6(\u00b7, \u00b7) = ( \u03d51(\u00b7, \u00b7), . . . , \u03d5d(\u00b7, \u00b7) )\u22a4 a feature vector with features \u03d5i : X \u00d7A \u2192 [\u2212C,C], and by F = {f\u03b1(\u00b7, \u00b7) = \u03c6(\u00b7, \u00b7)\u22a4\u03b1} the linear space of action-value functions spanned by the basis functions in \u03c6. Given a set of state-action pairs {(Xl, Al)}Ll=1, let \u03a6 = [\u03c6(X1, A1)\u22a4; . . . ;\u03c6(XL, AL)\u22a4] be the corresponding feature matrix. We define the orthogonal projection operator \u03a0 : B(X \u00d7A;Vmax) \u2192 F as \u03a0Q = argminf\u2208F ||Q \u2212 f ||\u00b5. Finally, by T (Q) we denote the truncation of a function Q in the range [\u2212Vmax, Vmax].\n1The extension to stochastic reward functions is straightforward.\nProblem setup. We consider the transfer problem in which M tasks {Mm}Mm=1 are available and the objective is to learn the solution for the target task M1 transferring samples from the source tasks {Mm}Mm=2. We define an assumption on how the training sets are generated. Definition 1. (Random Tasks Design) An input set {(Xl, Al)}Ll=1 is built with samples drawn from an arbitrary sampling distribution \u00b5 \u2208 S(X \u00d7 A), i.e. (Xl, Al) \u223c \u00b5. For each task m, one transition and reward sample is generated in each of the state-action pairs in the input set, i.e. Y ml \u223c P(\u00b7|Xl, Al), and Rml = R(Xl, Al). Finally, we define the random sequence {Ml}Ll=1 where the indexes Ml are drawn i.i.d. from a multinomial distribution with parameters (\u03bb1, . . . , \u03bbM ). The training set available to the learner is {(Xl, Al, Yl, Rl)}Ll=1 where Yl = Yl,Ml and Rl = Rl,Ml .\nThis is an assumption on how the samples are generated but in practice, a single realization of samples and task indexes Ml is available. We consider the case in which \u03bb1 \u226a \u03bbm (m = 2, . . . ,M ). This condition implies that (on average) the number of target samples is much less than the source samples and it is usually not enough to learn an accurate solution for the target task. We will also consider the pure transfer case in which \u03bb1 = 0 (i.e., no target sample is available). Finally, we notice that Def. 1 implies the existence of a generative model for all the MDPs, since the stateaction pairs are generated according to an arbitrary sampling distribution \u00b5."}, {"heading": "3 All-Sample Transfer Algorithm", "text": "We first consider the case when the source samples are generated beforehand according to Def. 1 and the designer has no access to the source tasks. We study the algorithm called All-Sample Transfer (AST) (Fig. 1) which simply runs FQI with a linear space F on the whole training set {(Xl, Al, Yl, Rl)}Ll=1. At each iteration k, given the result of the previous iteration Q\u0303k\u22121 = T (Q\u0302k\u22121), the algorithm returns\nQ\u0302k = argmin f\u2208F\n1\nL\nL\u2211\nl=1\n( f(Xl, Al)\u2212 (Rl + \u03b3max\na\u2032\u2208A Q\u0303k\u22121(Yl, a \u2032))\n)2 . (1)\nIn the case of linear spaces, the minimization problem is solved in closed form as in Fig. 1. In the following we report a finite-sample analysis of the performance of AST. Similar to [11], we first study the prediction error in each iteration and we then propagate it through iterations."}, {"heading": "3.1 Single Iteration Finite-Sample Analysis", "text": "We define the average MDP M\u03bb as the average of the M MDPs at hand. We define its reward function R\u03bb and its transition kernel P\u03bb as the weighted average of reward functions and transition kernels of the basic MDPs with weights determined by the proportions\u03bb of the multinomial distribution in the definition of the random tasks design (i.e., R\u03bb = \u2211M m=1 \u03bbmRm, P\u03bb = \u2211M m=1 \u03bbmPm). The resulting average Bellman operator is\n(T \u03bbQ)(x, a) = ( M\u2211\nm=1\n\u03bbmT mQ ) (x, a) = R(x, a) + \u03b3\n\u222b\nX\nmax a\u2032\nQ(y, a\u2032)P(dy|x, a). (2)\nIn the random tasks design, the average MDP plays a crucial role since the implicit target function of the minimization of the empirical loss in Eq. 1 is indeed T \u03bbQ\u0303k\u22121. At each iteration k, we prove the following performance bound for AST.\nTheorem 1. Let M be the number of tasks {Mm}Mm=1, with M1 the target task. Let the training set {(Xl, Al, Yl, Rl)}Ll=1 be generated as in Def. 1, with a proportion vector \u03bb = (\u03bb1, . . . , \u03bbM ). Let f\u03b1k\n\u2217\n= \u03a0T1Q\u0303k\u22121 = arg inff\u2208F ||f \u2212 T1Q\u0303k\u22121||\u00b5, then for any 0 < \u03b4 \u2264 1, Q\u0302k (Eq. 1) satisfies\n||T (Q\u0302k)\u2212 T1Q\u0303k\u22121||\u00b5 \u2264 4||f\u03b1k \u2217\n\u2212 T1Q\u0303k\u22121||\u00b5 + 5 \u221a E\u03bb(Q\u0303k\u22121)\n+ 24(Vmax + C||\u03b1k\u2217 ||) \u221a 2\nL log\n9 \u03b4 + 32Vmax\n\u221a 2\nL log\n( 27(12Le2)2(d+1)\n\u03b4\n) .\nwith probability 1\u2212 \u03b4 (w.r.t. samples), where ||\u03d5i||\u221e\u2264C and E\u03bb(Q\u0303k\u22121) = \u2016(T1 \u2212 T \u03bb)Q\u0303k\u22121\u20162\u00b5.\nRemark 1 (Analysis of the bound). We first notice that the previous bound reduces (up to constants) to the standard bound for FQI when M = 1 (see Section B). The bound is composed by three main terms: (i) approximation error, (ii) estimation error, and (iii) transfer error. The approximation error ||f\u03b1k\n\u2217 \u2212 T1Q\u0303k\u22121||\u00b5 is the smallest error of functions in F in approximating the target function T1Q\u0303k\u22121 and it is independent from the transfer algorithm. The estimation error (third and fourth terms in the bound) is due to the finite random samples used to learn Q\u0302k and it depends on the dimensionality d of the function space and it decreases with the total number of samples L with the fast rate of linear spaces (O(d/L) instead of O( \u221a d/L)). Finally, the transfer error E\u03bb accounts for the difference between source and target tasks. In fact, samples from source tasks different from the target might bias Q\u0302k towards a wrong solution, thus resulting in a poor approximation of the target function T1Q\u0303k\u22121. It is interesting to notice that the transfer error depends on the difference between the target task and the average MDP M\u03bb obtained by taking a linear combination of the source tasks weighted by the parameters \u03bb. This means that even when each of the source tasks is very different from the target, if there exists a suitable combination which is similar to the target task, then the transfer process is still likely to be effective. Furthermore, E\u03bb considers the difference in the result of the application of the two Bellman operators to a given function Q\u0303k\u22121. As a result, when the two operators T1 and T \u03bb have the same reward functions, even if the transition distributions are different (e.g., the total variation ||P1(\u00b7|x, a)\u2212P\u03bb(\u00b7|x, a)||TV is large), their corresponding averages of Q\u0303k\u22121 might still be similar (i.e., \u222b maxa\u2032 Q\u0303(y, a \u2032)P1(dy|x, a) similar to \u222b maxa\u2032 Q\u0303(y, a \u2032)P\u03bb(dy|x, a)).\nRemark 2 (Comparison to single-task learning). Let Q\u0302ks be the solution obtained by solving one iteration of FQI with only samples from the source task, the performance bounds of Q\u0302k and Q\u0302ks can be written as (up to constants and logarithmic factors)\n\u2016T (Q\u0302k)\u2212 T1Q\u0303k\u22121\u2016\u00b5 \u2264 ||f\u03b1k \u2217 \u2212 T1Q\u0303k\u22121||\u00b5 + (Vmax + C||\u03b1k\u2217 ||) \u221a 1\nL + Vmax\n\u221a d\nL + \u221a E\u03bb ,\n\u2016T (Q\u0302ks)\u2212 T1Q\u0303k\u22121\u2016\u00b5 \u2264 ||f\u03b1k \u2217 \u2212 T1Q\u0303k\u22121||\u00b5 + (Vmax + C||\u03b1k\u2217 ||) \u221a 1\nN1 + Vmax\n\u221a d\nN1 ,\nwith N1 = \u03bb1L (on average). Both bounds share exactly the same approximation error. The main difference is that Q\u0302ks uses only N1 samples and, as a result, has a much bigger estimation error than Q\u0302k, which takes advantage of all the L samples transferred from the source tasks. At the same time, Q\u0302k suffers from an additional transfer error which does not exist in the case of Q\u0302ks . Thus, we can conclude that AST is expected to perform better than single-task learning whenever the advantage of using more samples is greater than the bias due to samples coming from tasks different from the target task. This introduces a transfer tradeoff between including many source samples, so as to reduce the estimation error, and finding source tasks whose combination leads to a small transfer error. In Section 4 we show how it is possible to define an adaptive transfer algorithm which selects proportions \u03bb so as to keep the transfer error E\u03bb as small as possible. Finally, in Section 5 we consider a different setting where the maximum number of samples in each source is fixed."}, {"heading": "3.2 Propagation Finite-Sample Analysis", "text": "We now study how the previous error is propagated through iterations. Let \u03bd be the evaluation norm (i.e., in general different from the sampling distribution \u00b5). We first report two assumptions. 2\nAssumption 1. [11] Given \u00b5, \u03bd, p \u2265 1, and an arbitrary sequence of policies {\u03c0p}p\u22651, we assume that the future-state distribution \u00b5P1\u03c01 \u00b7 \u00b7 \u00b7 P1\u03c0p is absolutely continuous w.r.t. \u03bd. We assume that c(p) = sup\u03c01\u00b7\u00b7\u00b7\u03c0p ||d(\u00b5P1\u03c01 \u00b7 \u00b7 \u00b7 P1\u03c0p)/\u03bd||\u221e satisfies C\u00b5,\u03bd = (1 \u2212 \u03b32)2 \u2211 p p\u03b3 p\u22121c(p) < \u221e.\nWe also need the features \u03d5i to be linearly independent w.r.t. \u00b5. Assumption 2. Let G \u2208 Rd\u00d7d be the Gram matrix with [G]ij = \u222b \u03d5i(x, a)\u03d5j(x, a)\u00b5(dx, a). We assume that its smallest eigenvalue \u03c9 is strictly positive (i.e., \u03c9 > 0).\nUsing the two previous assumptions we derive the following performance bound for AST.\nTheorem 2. Let Assumptions 1 and 2 hold and the setting be as in Theorem 1. After K iterations, AST returns an action-value function Q\u0303K , whose corresponding greedy policy \u03c0K satisfies\n||Q\u2217 \u2212Q\u03c0K ||\u03bd \u2264 2\u03b3 (1\u2212 \u03b3)3/2 \u221a C\u00b5,\u03bd [ 4 sup g\u2208F inf f\u2208F ||f \u2212 T1g||\u00b5 + 5 sup \u03b1 \u2016(T1 \u2212 T \u03bb)T (f\u03b1)\u2016\u00b5\n+ 56(Vmax + Vmax\u221a\n\u03c9 )\n\u221a 2\nL log\n9K\n\u03b4 + 32Vmax\n\u221a 2\nL log\n( 27K(12Le2)2(d+1)\n\u03b4\n) +\n2Vmax\u221a C\u00b5,\u03bd \u03b3K\n] .\nRemark (Analysis of the bound). The bound reported in the previous theorem displays few differences w.r.t. to the single-iteration bound. The additional term O(\u03b3K) accounts for the error due to the finite number of iterations of FQI and it decreases exponentially with base \u03b3. The approximation error is now supg inff ||f \u2212 T 1g||\u00b5. This term is referred to as the inherent Bellman error [11] of the space F and it is related to how well the Bellman images of functions in F can be approximated by F itself. It is possible to show that for particular classes of MDPs (e.g., Lipschitz), if a large enough number of carefully designed features is available, then this term is small. In the estimation error, the norm ||\u03b1k\u2217 || is bounded using the linear independency between features (Assumption 2) and the boundedness of the functions Q\u0303k returned at each iteration. The resulting term has an inverse dependency on the smallest eigenvalue \u03c9 which tends to be small whenever the Gram matrix is not well-defined (i.e., the features are almost linearly dependent). The transfer error sup\u03b1 \u2016(T1 \u2212 T \u03bb)T (f\u03b1)\u2016\u00b5 characterizes the difference between the target and average Bellman operators through the space F . As a result, even MDPs with significantly different rewards and transitions might have a small transfer error because of the functions in F . This introduces a tradeoff in the design of F between a \u201clarge\u201d enough space containing functions able to approximate T1Q (i.e., small approximation error) and a small function space where the Q-functions induced by T1 and T \u03bb can be closer (i.e., small transfer error). This term also displays interesting similarities with the notion of discrepancy introduced in [10] in domain adaptation."}, {"heading": "4 Best Average Transfer Algorithm", "text": "As discussed in the previous section, the transfer error E\u03bb plays a crucial role in the comparison with single-task learning. In particular, E\u03bb is related to the proportions \u03bb inducing the average Bellman operator T \u03bb which defines the target function approximated at each iteration. We now consider the case where the designer has direct access to the source tasks (i.e., it is possible to choose how many samples to draw from each source) and can define an arbitrary proportion \u03bb. In particular, we propose a method that adapts \u03bb at each iteration so as to minimize the transfer error E\u03bb. We consider the case in which L is fixed as a parameter of the algorithm and \u03bb1 = 0 (i.e., no target samples are used in the learning training set). At each iteration k, we need to estimate the quantity E\u03bb(Q\u0303k\u22121). We assume that for each task additional samples available. Let {(Xs, As, Rs,1, . . . , Rs,M )}Ss=1 be an auxiliary training set where (Xs, As) \u223c \u00b5 and Rs,m =\n2We refer to [11] for a thorough explanation of the concentrability terms.\nRm(Xs, As). In each state-action pair, we generate T next states for each task, that is Y ts,m \u223c Pm(\u00b7|Xs, As) with t = 1, . . . , T . Thus, for any function Q we define the estimated transfer error as\nE\u0302\u03bb(Q)= 1\nS\nS\u2211\ns=1\n[ Rs,1\u2212 M\u2211\nm=2\n\u03bbmRs,m + \u03b3\nT\nT\u2211\nt=1\n( max a\u2032 Q(Y ts,1, a \u2032)\u2212 M\u2211\nm=2\n\u03bbm max a\u2032\nQ(Y ts,m, a \u2032) )]2 . (3)\nAt each iteration, the algorithm Best Average Transfer (BAT) (Fig. 2) first computes \u03bb\u0302k = argmin\u03bb\u2208\u039b E\u0302\u03bb(Q\u0303k\u22121), where \u039b is the (M -2)-dimensional simplex, and then runs an iteration of AST with samples generated according to the proportions \u03bb\u0302k . We denote by \u03bbk\u2217 the best combination at iteration k, that is\n\u03bbk\u2217 = argmin \u03bb\u2208\u039b E\u03bb(Q\u0303k\u22121) = argmin \u03bb\u2208\u039b E\u00b5\n[( M\u2211\nm=2\n\u03bbm(T mQ\u0303k\u22121)(x, a) \u2212 (T 1Q\u0303k\u22121)(x, a) )2 ] . (4)\nThe following performance guarantee can be proved for BAT.\nLemma 1. Let {(Xs, As, R1s, . . . , RMs )}Ss=1 be a training set where (Xs, As) iid\u223c \u00b5 and Rms = Rm(Xs, As) and for each state-action pair and for each task m, T next states Y ms,t \u223c Pm(\u00b7|Xs, As) with t = 1, . . . , T are available. For any fixed bounded function Q \u2208 B(X \u00d7 A;Vmax), the \u03bb\u0302 returned by minimizing Eq. 3 is such that\nE\u03bb\u0302(Q)\u2212 E\u03bb\u2217(Q) \u2264 2Vmax \u221a (M \u2212 2) log 4S/\u03b4 S + 16V 2max log 4SM/\u03b4 T (5)\nwith probability 1\u2212 \u03b4.\nFrom the previous lemma the approximation performance of BAT at each iteration follows.\nTheorem 3. Let Q\u0303k\u22121 be the function returned at the previous iteration and Q\u0302kBAT the function returned by the BAT algorithm (Fig. 2). Then for any 0 < \u03b4 \u2264 1, Q\u0302kBAT satisfies\n||T (Q\u0302kBAT)\u2212 T1Q\u0303k\u22121||\u00b5 \u2264 4||f\u03b1k \u2217 \u2212 T1Q\u0303k\u22121||\u00b5 + 5 \u221a E\u03bbk \u2217 (Q\u0303k\u22121)\n+ 5 \u221a 2Vmax\n( (M \u2212 2) log 8S/\u03b4\nS\n)1/4 + 20Vmax \u221a log 8SM/\u03b4\nT\n+ 24(Vmax + C||\u03b1k\u2217 ||) \u221a 2\nL log\n18\n\u03b4 + 32Vmax\n\u221a 2\nL log\n( 54(12Le2)2(d+1)\n\u03b4\n) .\nwith probability 1\u2212 \u03b4.\nRemark 1 (Comparison with AST and single-task learning). The analysis of the bound shows that BAT outperforms AST whenever the advantage in achieving the smallest possible transfer error E\u03bbk\n\u2217\nis larger than the additional estimation error due to the auxiliary training set. It is also interesting\nto compare BAT to single-task learning. In fact, BAT performs better than single-task learning whenever the best possible combination of source tasks has a small transfer error and the additional estimation error related to the auxiliary training set is smaller than the estimation error in singletask learning. In particular, this means that O((M/S)1/4) + O((1/T )1/2) should be smaller than O((d/N)1/2) (with N the number of target samples). The number of calls to the generative model for BAT is ST . In order to have a fair comparison with single-task learning we set S = N2/3 and T = N1/3, then we obtain the condition M \u2264 d2N\u22124/3 that constrains the number of tasks to be smaller than the dimensionality of the function space F . We remark that the dependency of the auxiliary estimation error on M is due to the fact that the \u03bb vectors (over which the transfer error is optimized) belong to the simplex \u039b of dimensionality M -2. Hence, the previous condition suggests that, in general, adaptive transfer methods may significantly improve the transfer performance (i.e., in this case a smaller transfer error) at the cost of additional sources of errors which depend on the dimensionality of the search space used to adapt the transfer process (i.e., in this case \u039b).\nRemark 2 (Iterations). BAT recomputes the proportions \u03bb\u0302k at each iteration k. In fact a combination \u03bb1 approximating well the reward function R1 at the first iteration (i.e., R1 \u2248 R\u03bb1 ) does not necessarily have a small transfer error ||(T1 \u2212 T\u03bb1)Q\u03031||\u00b5 at the second iteration. We further investigate how the best source combination changes through iterations in the experiments of Section 6.\nRemark 3 (Best source combination). The previous theorem shows that BAT achieves the smallest transfer error E\u03bbk\n\u2217 (Q\u0303k\u22121) at the cost of an additional estimation error which scales with the size of the auxiliary training set as O((M/S)1/4)+O((1/T )1/2). We notice that the first term of the estimation error depends on how well the \u00b5 is approximated by using a finite number S of state-action pairs and it has a slower rate w.r.t. the other terms. The second term depends on the number of next states T simulated at each state-action pair which are used to estimate the value of the Bellman operators. As a result, in order to reduce the estimation error we need to increase both S and the number of next states T in each state-action pair. It is interesting to notice that similar estimation errors appear in FVI [11] where the optimal Bellman operator is approximated by Monte-Carlo estimation.\nRemark 4 (Training set). The implicit assumption in the definition of the auxiliary training set is that it is possible to generate a series of next states and rewards for all the tasks at the same stateaction pairs. If the source training sets are fixed in advance and the designer has no access to the source tasks, then this assumption is not verified and it is not possible to test the similarity between the MDP M and the target task. Nonetheless, if the generative model for the source tasks is available at learning time, the auxiliary training set could be generated before the learning phase actually begins. Furthermore, in the theoretical analysis, BAT does not use the samples in the auxiliary training set at learning time. A trivial improvement is to include the auxiliary samples to the training set.\nRemark 5 (Comparison to other transfer methods). In [8] a method to compute the similarity between MDPs is proposed. In particular, the authors introduce the definition of compliance as the average probability of the target samples to be generated from an sample-based estimation of the source MDPs. The compliance is later used to determine the proportion of samples to be transferred from each of the source tasks. Although this algorithm shares a similar objective as BAT, they use different notions of similarity. In particular, the method in [8] tries to identify source tasks which are individually similar to the target task, while the transfer error minimized in BAT considers the average MDP obtained by the transfer process. Furthermore, the notion of compliance tries to measures the overall distance between two MDPs, while E\u03bb(Q) always measures the distance of the images of a function Q through two different Bellman operators.\nRemark 6 (Computational complexity). Finally, we notice that the minimization of E\u0302\u03bb is a convex quadratic problem since the objective function is convex in \u03bb and \u03bb belongs to the (M -2)- dimensional simplex."}, {"heading": "5 Best Transfer Trade-off Algorithm", "text": "The previous algorithm is proved to successfully estimate the combination of source tasks which better approximates the Bellman operator of the target task. Nonetheless, BAT relies on the implicit\nassumption that L samples can always be generated from any source task 3 and it cannot be applied to the case where the number of source samples is limited. Here we consider the more challenging case where the designer has still access to the source tasks but only a limited number of samples is available in each of them. In this case, an adaptive transfer algorithm should solve a tradeoff between selecting as many samples as possible, so as to reduce the estimation error, and choosing the proportion of source samples properly, so as to control the transfer error. The solution of this tradeoff may return non-trivial results, where source tasks similar to the target task but with few samples are removed in favor of a pool of tasks whose average roughly approximate the target task but can provide a larger number of samples.\nHere we introduce the Best Tradeoff Transfer (BTT) algorithm (see Figure 3). Similar to BAT, it relies on an auxiliary training set to solve the tradeoff. We denote by Nm the maximum number of samples available for source task m. Let \u03b2 \u2208 [0, 1]M be a weight vector, where \u03b2m is the fraction of samples from task m used in the transfer process. We denote by E\u03b2 (E\u0302\u03b2) the transfer error (the estimated transfer error) with proportions \u03bb where \u03bbm = (\u03b2mNm)/ \u2211 m\u2032(\u03b2m\u2032Nm\u2032). At each iteration k, BTT returns the vector \u03b2 which optimizes the tradeoff between estimation and transfer errors, that is\n\u03b2\u0302k = arg min \u03b2\u2208[0,1]M\n( E\u0302\u03b2(Q\u0303k\u22121) + \u03c4\n\u221a d\n\u2211M m=1 \u03b2mNm\n) , (6)\nwhere \u03c4 is a parameter. While the first term accounts for the transfer error induced by \u03b2, the second term is the estimation error due to the total amount of samples used by the algorithm.\nUnlike AST and BAT, BTT is a heuristic algorithm motivated by the performance bound in Theorem 1 and we do not provide any theoretical guarantee about its performance. The main technical difficulty w.r.t. the previous algorithms is that the setting considered here does not match the random task design assumption (see Def. 1) since the number of source samples is constrained by Nm. As a result, given a proportion vector \u03bb, we cannot assume samples to be drawn at random according to a multinomial of parameters \u03bb. Without this assumption, it is an open question whether a similar bound to AST and BAT could be derived. Nonetheless, the experimental results reported in Section 6 show the effectiveness of BTT in solving the transfer tradeoff."}, {"heading": "6 Experiments", "text": "In this section, we report and discuss preliminary experimental results of the transfer algorithms introduced in the previous sections. The main objective is to illustrate the functioning of the algorithms and compare their results with the theoretical findings. Thus, we focus on a simple problem and we leave more challenging problems for future work.\nWe consider a continuous extension of the 50-state variant of the chain walk problem proposed in [6]. The state space is described by a continuous state variable x and two actions are available: one that\n3If \u03bbm = 1 for task m, then the algorithm would generate all the L training samples from task m.\nTable 1: Parameters for the first set of tasks\ntasks p l \u03b7 Reward\nM1 0.9 1 0.1 +1 in [\u221211,\u22129] \u222a [9, 11]\nM2 0.9 2 0.1 \u22125 in [\u221211,\u22129] \u222a [9, 11] M3 0.9 1 0.1 +5 in [\u221211,\u22129] \u222a [9, 11] M4 0.9 1 0.1 +1 in [\u22126,\u22124] \u222a [4, 6] M5 0.9 1 0.1 \u22121 in [\u22126,\u22124] \u222a [4, 6]\nTable 2: Parameters for the second set of tasks\ntasks p l \u03b7 Reward\nM1 0.9 1 0.1 +1 in [\u221211,\u22129] \u222a [9, 11]\nM6 0.7 1 0.1 +1 in [\u221211,\u22129] \u222a [9, 11] M7 0.1 1 0.1 +1 in [\u221211,\u22129] \u222a [9, 11] M8 0.9 1 0.1 \u22125 in [\u221211,\u22129] \u222a [9, 11] M9 0.7 1 0.5 +5 in [\u221211,\u22129] \u222a [9, 11]\n-0.2\n-0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0 2000 4000 6000 8000 10000\nA ve\nra ge\nr ew\nar d\npe r\nst ep\nNumber of target samples\nwithout transfer BAT with 1000 samples BAT with 5000 samples BAT with 10000 samples AST with 10000 samples\n-0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n1 2 3 4 5 6 7 8 9 10 11 12 13 P\nro ba\nbi lit y Number of iterations\n\u03bb2 \u03bb3 \u03bb4 \u03bb5\nFigure 4: Transfer from M2, M3, M4, M5. Left: Comparison between single-task learning, AST with L = 10000, BAT with L = 1000, 5000, 10000. Right: Source task probabilities estimated by BAT algorithm as a function of FQI iterations.\nmoves toward left and the other toward right. With probability p each action makes a step of length l, affected by a noise \u03b7, in the intended direction, while with probability 1\u2212 p it moves in the opposite direction. For the target task M1, the state\u2013transition model is defined by the following parameters: p = 0.9, l = 1, and \u03b7 is uniform in the interval [\u22120.1, 0.1]. The reward function provides +1 when the system state reaches the regions [\u221211,\u22129] and [9, 11] and 0 elsewhere. Furthermore, to evaluate the performance of the transfer algorithms previously described, we considered eight source tasks {M2, . . . ,M9} whose state\u2013transition model parameters and reward functions are reported in Tab. 1 and 2. To approximate the Q-functions, we use a linear combination of 20 radial basis functions. In particular, for each action, we consider 9 Gaussians with means uniformly spread in the interval [\u221220, 20] and variance equal to 16, plus a constant feature. The number of iterations for the FQI algorithm has been empirically fixed to 13. Samples are collected through a sequence of episodes, each one starting from the state x0 = 0 with actions chosen uniformly at random. For all the experiments, we average over 100 runs and we report standard deviation error bars.\nWe first consider the pure transfer problem where no target samples are actually used in the learning training set (i.e., \u03bb1 = 0). The objective is to study the impact of the transfer error due to the use of source samples and the effectiveness of BAT in finding a suitable combination of source tasks. The left plot in Fig. 4 compares the performances of FQI with and without the transfer of samples from the first four tasks listed in Tab. 1. In case of single-task learning, the number of target samples refers to the samples used at learning time, while for BAT it represents the size S of the auxiliary training set used to estimate the transfer error. Thus, while in single-task learning the performance increases with the target samples, in BAT they just make estimation of E\u03bb more accurate. The number of source samples added to the auxiliary set for each target sample was empirically fixed to one (T = 1). We first run AST with L = 10000 and \u03bb2 = \u03bb3 = \u03bb4 = \u03bb5 = 0.25 (which on average corresponds to 2500 samples from each source). As it can be noticed by looking at the models in Tab. 1, this combination is very different from the target model and AST does not learn any good policy. On the other hand, even with a small set of auxiliary target samples, BAT is able to learn good policies. Such result is due to the existence of linear combinations of source tasks which closely approximate the target task M1 at each iteration of FQI. An example of the proportion coefficients computed at each iteration of BAT is shown in the right plot in Fig. 4. At the first iteration, FQI produces an approximation of the reward function. Given the first four source tasks,\nBAT finds a combination (\u03bb \u2243 (0.2, 0.4, 0.2, 0.2)) that produces the same reward function as R1. However, after a few FQI iterations, such combination is no more able to accurately approximate functions T1Q\u0303. In fact, the state\u2013transition model of task M2 is different from all the other ones (the step length is doubled). As a result, the coefficient \u03bb2 drops to zero, while a new combination among the other source tasks is found. Note that BAT significantly improves single-task learning, in particular when very few target samples are available.\nIn the general case, the target task cannot be obtained as any combination of the source tasks, as it happens by considering the second set of source tasks (M6, M7, M8, M9). The impact of such situation on the learning performance of BAT is shown in the left plot in Fig. 5. Note that, when a few target samples are available, the transfer of samples from a combination of the source tasks using the BAT algorithm is still beneficial. On the other hand, the performance attainable by BAT is bounded by the transfer error corresponding to the best source task combination (which in this case is large). As a result, single-task FQI quickly achieves a better performance.\nResults presented so far for the BAT transfer algorithm assume that FQI is trained only with the samples obtained through combinations of source tasks. Since a number of target samples is already available in the auxiliary training set, a trivial improvement is to include them in the training set together with the source samples (selected according to the proportions computed by BAT). As shown in the plot in the right side of Fig. 5 this leads to a significant improvement. From the behavior of BAT it is clear that with a small set of target samples, it is better to transfer as many samples as possible from source tasks, while as the number of target samples increases, it is preferable to reduce the number of samples obtained from a combination of source tasks that actually does not match the target task. In fact, for L = 10000, BAT has a much better performance at the beginning but it is then outperformed by single-task learning. On the other hand, for L = 1000 the initial advantage is small but the performance remains close to single-task FQI for large number of target samples. This experiment highlights the tradeoff between the need of samples to reduce the estimation error and the resulting transfer error when the target task cannot be expressed as a combination of source tasks (see Section 5). BTT algorithm provides a principled way to address such tradeoff, and, as shown by the right plot in Fig. 5, it exploits the advantage of transferring source samples when a few target samples are available, and it reduces the weight of the source tasks (so as to avoid large transfer errors) when samples from the target task are enough. It is interesting to notice that increasing the number of samples available for each source task from 5000 to 10000 improves the performance in the first part of the graph, while keeping unchanged the final performance. This is due to the capability of the BTT algorithm to avoid the transfer of source samples when there is no need for them, thus avoiding negative transfer effects."}, {"heading": "7 Conclusions", "text": "In this paper, we formalized and studied the sample-transfer problem. We first derived a finitesample analysis of the performance of a simple transfer algorithm which includes all the source samples into the training set used to solve a given target task. At the best of our knowledge, this is the first theoretical result for a transfer algorithm in RL showing the potential benefit of transfer over single-task learning. Then, in the case when the designer has direct access to the source tasks, we introduced an adaptive algorithm which selects the proportion of source tasks so as to minimize the bias due to the use of source samples. Finally, we considered a more challenging setting where the number of samples available in each source task is limited and a tradeoff between the amount of transferred samples and the similarity between source and target tasks must be solved. For this setting, we proposed a principled adaptive algorithm. Finally, we report a detailed experimental analysis on a simple problem which confirms and supports the theoretical findings.\nThis work opens several directions for future work.\n\u2022 Transfer with transformations. In many problems, there exist simple transformations to the source tasks dynamics and reward which would increase their similarity w.r.t. the target task, thus making the transfer process more effective. How affine transformations could be used in the adaptive transfer algorithms presented in this paper is an interesting direction for future work. In particular, it is an open question whether the cost (in terms of samples) of finding a suitable transformation would be effectively counter-balanced by transferring more similar samples.\n\u2022 Transfer between tasks with different state-action spaces. In many real applications source and target tasks might have a different number of state variables and different actions. Thus, the current work should be extended to the more general case of tasks with different stateaction spaces and it should be integrated with inter-task mapping transfer methods (see [14]).\n\u2022 Transfer with fixed tasks design. Definition 1 prescribes the process used to generate the training set used in learning the target task. At each state-action pair, the sample is generated from a source task chosen at random according to a multinomial distribution. When the designer has no access to the source tasks and their samples are generated beforehand, this generative model is not reasonable. A different model (fixed tasks design) should be defined where each sample is coming from a specific source which is fixed in advance. An interesting direction for future work is to understand how this different generative model affects the performance of the transfer algorithm and whether it is possible to define effective adaptive algorithms for this case.\nAcknowledgments This work was supported by French National Research Agency through the projects EXPLO-RA n\u25e6 ANR-08-COSI-004, by Ministry of Higher Education and Research, NordPas de Calais Regional Council and FEDER through the \u201ccontrat de projets e\u0301tat region 2007\u20132013\u201d, and by PASCAL2 European Network of Excellence. The research leading to these results has also received funding from the European Community\u2019s Seventh Framework Programme (FP7/20072013) under grant agreement n 231495."}, {"heading": "A Additional Notation", "text": "Besides the notation introduced in Section 2, here we introduce additional symbols used in the proofs. We define two empirical norms on functions and vectors. Given a set of N state-action pairs {(Xn, An)}Nn=1 drawn i.i.d. from \u00b5 we define the empirical norm ||f ||\u00b5\u0302 as\n||f ||2\u00b5\u0302 = 1\nN\nN\u2211\nn=1\nf(Xn, An) 2.\nSimilarly, given a vector y \u2208 RN we define the empirical norm ||y||N as\n||y||2N = 1\nN\nN\u2211\nn=1\ny2n.\nGiven a set of N state-action pairs {(Xn, An)}Nn=1, let \u03a6 = [\u03c6(X1, A1)\u22a4; . . . ;\u03c6(XN , AN )\u22a4] be the feature matrix defined at the states {(Xn, An)}Nn=1, and Fn = {\u03a6\u03b1, \u03b1 \u2208 Rd} \u2282 RN be the corresponding vector space. We denote by \u03a0\u0302 : RN \u2192 FN the empirical orthogonal projection onto FN , defined by\n\u03a0\u0302y = argmin z\u2208FN\n||y \u2212 z||N . (7)\nNote that the orthogonal projection \u03a0\u0302y of any y \u2208 RN always exists and is unique."}, {"heading": "B Fitted Q-iteration with Linear Spaces", "text": "Although fitted iterative methods have been already analyzed in detail in [11] and [1], at the best of our knowledge no explicit finite-sample bounds for FQI with linear spaces is available. Since at each iteration, FQI solves an explicit regression problem, the derivation is mostly a straightforward application of regression bounds for linear spaces and quadratic loss. Here we just report the result and the proof of the single iteration error for the so-called fixed and random samples design settings.\nIn Algorithm 6 we report the structure of the algorithm."}, {"heading": "B.1 Fixed Samples Design", "text": "Similar to the analysis of LSTD in [7] we first derive the fixed design bound (i.e., the performance is evaluated exactly on the states in the training set).\nTheorem 4. Let F = {\u03c6(\u00b7, \u00b7)\u22a4\u03b1, \u03b1 \u2208 Rd} be a d-dimensional linear space. Let {(xn, an, Yn, Rn)}Nn=1 be the training set where {(xn, an)}Nn=1 is an arbitrary sequence of stateaction pairs, Yn \u223c P(\u00b7|xn, an), and Rn = R(xn, an). Given a function Q \u2208 B(X \u00d7A, Vmax), let q \u2208 RN be the vector whose components are qn = (T Q)(xn, an) and q\u0302 be the solution of a single iteration of fitted value iteration. Then with probability 1 \u2212 \u03b4 (w.r.t. the random next states Yn), q\u0302\nsatisfies\n||q\u0302 \u2212 q||N \u2264 ||\u03a0\u0302q \u2212 q||N + 4Vmax\n\u221a 2\nN log\n( 3(9Ne2)d+1\n\u03b4\n) . (8)\nProof. We denote by u \u2208 RN the orthogonal projection of the target vector q onto the vector space FN , that is u = \u03a0\u0302q. By the definition of orthogonal projection and the Pythagorean theorem we decompose the error ||q\u0302 \u2212 q||N as\n||q\u0302 \u2212 q||2N = ||q\u0302 \u2212 u||2N + ||u\u2212 q||2N , (9) where the first term represents the estimation error and the second term is the approximation error (see Fig. 7). We denote by \u03ben = pn \u2212 qn the noise in the observations p w.r.t. q. It is easy to notice that\nE [\u03ben] = EY\u223cP(\u00b7|xn,an) [ R(xn, an, Y ) + \u03b3max\na\u2032\u2208A Q(Y, a\u2032)\n] \u2212 (T Q)(xn, an) = 0, (10)\nand that |\u03ben| \u2264 2Vmax. We also define the projected noise \u03be\u0302n = q\u0302n \u2212 un, that is \u03be\u0302 = \u03a0\u0302\u03be. Thus, we can rewrite the estimation error as\n||q\u0302 \u2212 u||2N = ||\u03be\u0302||2N = \u3008\u03be\u0302, \u03be\u0302\u3009 = \u3008\u03be, \u03be\u0302\u3009, (11)\nwhere the last equality follows from the fact that \u03be\u0302 is the orthogonal projection of \u03be. Since \u03be\u0302 \u2208 FN , let f\u03b2 \u2208 F be any function such that f\u03b2(xn, an) = \u03be\u0302n, and by a straightforward application of a variation of Pollard\u2019s inequality [5] we obtain\n\u3008\u03be, \u03be\u0302\u3009 = 1 N\nN\u2211\nn=1\n\u03benf\u03b2(xn, an) \u2264 4Vmax ( 1\nN\nN\u2211\nn=1\nf\u03b2(xn, an) 2\n)1/2\u221a 2\nN log\n( 3(9Ne2)d+1\n\u03b4\n)\n= 4Vmax||\u03be\u0302||N\n\u221a 2\nN log\n( 3(9Ne2)d+1\n\u03b4\n) (12)\nwith probability 1\u2212 \u03b4. Thus from equation 11 we bound the estimation error by\n||q\u0302 \u2212 u||N \u2264 4Vmax\n\u221a 2\nN log\n( 3(9Ne2)d+1\n\u03b4\n) . (13)\nPutting together the estimation error bound and the approximation error term, the statement of the theorem follows."}, {"heading": "B.2 Random Samples Design", "text": "While in the previous section we analyzed the performance of FQI on the very same state-action pairs in the training set, we now focus on the generalization (i.e., prediction) performance on the whole state-action space.\nLet Q\u0302 be any function f\u03b1\u0302 \u2208 F satisfying \u03a6\u03b1\u0302 = q\u0302, where q\u0302 is the vector defined in the previous section. Then we derive the following theorem.\nTheorem 5. Let F = {\u03c6(\u00b7, \u00b7)\u22a4\u03b1, \u03b1 \u2208 Rd} be a d-dimensional linear space. Let {(Xn, An, Yn, Rn)}Nn=1 be the training set where (Xn, An)\niid\u223c \u00b5, Yn \u223c P(\u00b7|Xn, An), and Rn = R(Xn, An). Given a function Q \u2208 B(X \u00d7A, Vmax), let Q\u0302 be the solution of a single iteration of fitted value iteration. Then with probability 1\u2212 \u03b4 (w.r.t. the samples and the next states),"}, {"heading": "Q\u0302 satisfies", "text": "||T (Q\u0302)\u2212 T Q||\u00b5 \u2264 4 inf f\u03b1\u2208F ||f\u03b1 \u2212 T Q||\u00b5\n+ 24(Vmax + L||\u03b1\u2217||) \u221a 2\nN log\n9\n\u03b4\n+ 32Vmax\n\u221a 2\nN log\n( 27(12Ne2)2(d+1)\n\u03b4\n) . (14)\nProof. The proof mainly relies on the application of concentration of measures inequalities for linear spaces to the deterministic design bound in Theorem 4.\nLet f\u03b1\u0302\u2217 \u2208 F be any function such that f\u03b1\u0302\u2217(Xn) = (\u03a0\u0302q)n, thus the approximation error ||\u03a0\u0302q\u2212 q||N can be rewritten as ||f\u03b1\u0302\u2217 \u2212 T Q||\u00b5\u0302. Furthermore we denote by f\u03b1\u2217 = \u03a0(T Q), that is the best approximation of the target function T Q onto F w.r.t. the distribution \u00b5. Since f\u03b1\u0302\u2217 is the minimizer of the empirical squared error, any function in F different from f\u03b1\u0302\u2217 has a bigger empirical loss, thus we obtain\n||f\u03b1\u0302\u2217 \u2212 T Q||\u00b5\u0302 \u2264 ||f\u03b1\u2217 \u2212 T Q||\u00b5\u0302 \u2264 2||f\u03b1\u2217 \u2212 T Q||\u00b5 + 12(Vmax + L||\u03b1\u2217||) \u221a 2\nN log\n3 \u03b4\u2032 , (15)\nwith probability 1\u2212 \u03b4\u2032, where the second inequality is an application of a variation of Theorem 11.2 in [5] with a bound ||f\u03b1\u2217 \u2212 T Q||\u221e \u2264 Vmax + L||\u03b1\u2217||. Similar, we notice that the left hand side of Eq. 8 is ||q\u0302 \u2212 q||N = ||Q\u0302\u2212 T \u2217Q||\u00b5\u0302 and we obtain\n2||Q\u0302\u2212 T Q||\u00b5\u0302 \u2265 2||T (Q\u0302)\u2212 T Q||\u00b5\u0302 \u2265 ||T (Q\u0302)\u2212 T Q||\u00b5 \u2212 24Vmax\n\u221a 2\nN log\n( 9(12eN)2(d+1)\n\u03b4\u2032\n)\n(16)\nwith probability 1\u2212 \u03b4\u2032, where the second inequality is an application of a variation of Theorem 11.2 in [5]. Putting together Eqs 8, 15, and 16 we obtain\n||T (Q\u0302)\u2212 T Q||\u00b5 \u22642 ( 2||f\u03b1\u2217 \u2212 T Q||\u00b5 + 12(Vmax + L||\u03b1\u2217||) \u221a 2\nN log\n3 \u03b4\u2032 +\n+ 4Vmax\n\u221a 2\nN log\n( 3(9Ne2)d+1\n\u03b4\u2032\n)) + 24Vmax \u221a 2\nN log\n( 9(12eN)2(d+1)\n\u03b4\u2032\n)\nFinally, by setting \u03b4 = 3\u03b4\u2032 the statement follows."}, {"heading": "C Analysis of AST", "text": ""}, {"heading": "C.1 Proof of Theorem 1", "text": "Proof. Since the proof follows similar steps as in the proof of Theorem 5, we discuss here only the fixed samples design bound. We define the vector p \u2208 RL such that for any l = 1, . . . , L, pl = \u2211M m=1 I {Ml = m} (Rml + \u03b3maxa\u2032 Q(Y ml , a\u2032)). The target vector q \u2208 RL is the image of the function Q through the average optimal Bellman operator. In fact, by defining ql = (T \u03bbQ)(Xl, Al) we obtain a zero-mean noise vector \u03bel = pl \u2212 ql such that E [\u03bel] = 0 and |\u03bel| \u2264 2Vmax. 4\n4The expectation is taken w.r.t. both the random realization of the reward Rml and next state Y m l and task\nindex Ml.\nThe statement of the theorem simply follows by decomposing the prediction error of Q\u0302 as\n\u2016T (Q\u0302)\u2212 T1Q\u2016\u00b5 \u2264 \u2016T (Q\u0302)\u2212 T \u03bbQ\u2016\u00b5 + \u2016T \u03bbQ\u2212 T1Q\u2016\u00b5. (17)\nBy substituting \u2016T (Q\u0302)\u2212 T \u03bbQ\u2016\u00b5 with a FQI bound w.r.t. the target function T \u03bbQ we obtain\n\u2016T (Q\u0302)\u2212 T1Q\u2016\u00b5 \u2264 4||f\u03b1 \u2212 T \u03bbQ||\u00b5 + \u2016T \u03bbQ \u2212 T1Q\u2016\u00b5 (18)\n+ 24(Vmax + C||\u03b1||) \u221a 2\nL log\n9\n\u03b4\n+ 32Vmax\n\u221a 2\nL log\n( 27(12Le2)2(d+1)\n\u03b4\n) . (19)\nBy rewriting the approximation error as ||f\u03b1 \u2212 T \u03bbQ||\u00b5 \u2264 ||f\u03b1 \u2212 T 1Q||\u00b5 + ||T 1Q \u2212 T \u03bbQ||\u00b5 and using \u03b1 = \u03b1\u2217 the final bound follows."}, {"heading": "C.2 Proof of Theorem 2", "text": "Proof. [Sketch] The main structure of the proof is exactly the same as in [11]. The main differences are due to the use of linear spaces and the transfer error. Following the passages in the proof of Theorem 2 in [11], we obtain\n||Q\u2217 \u2212Q\u03c0K ||\u03bd \u2264 2\u03b3\n(1\u2212 \u03b3)3/2\n[ \u221a C\u00b5,\u03bd max\nk ||T (Q\u0302k)\u2212 T 1Q\u0303k||\u00b5 + 2Vmax\u03b3K\n] .\nThus, we need to study all the terms in the statement of Theorem 1 affected by the maximization over the iterations.\nApproximation error. The approximation term becomes\nmax k min f\u2208F ||f \u2212 T 1Q\u0303k||\u00b5 \u2264 sup g\u2208F min f\u2208F ||f \u2212 T 1g||\u00b5.\nThis term is referred to as the inherent Bellman error of the space F and it is related to how well the Bellman images of functions in F can be approximated by F itself. Estimation error. The second relevant term is the term ||\u03b1k\u2217 || appearing in the estimation error. We recall that f\u03b1k\n\u2217 = \u03a0T1Q\u0303k\u22121 is the projection on F of the Bellman image of the function returned at the previous iteration. The function Q\u0303k\u22121 is truncated in the interval [\u2212Vmax, Vmax] and its Bellman image T1Q\u0303k\u22121 is still bounded in the same interval. Since the projection operator \u03a0 is a non-expansion, we finally have that ||f\u03b1k\n\u2217 ||\u221e \u2264 Vmax. Using Assumption 2, for any f\u03b1 \u2208 F , it is possible to relate the norm of the function to the norm of the vector \u03b1 as\n||f\u03b1||2\u00b5 = ||\u03c6\u22a4\u03b1||2\u00b5 = \u03b1\u22a4G\u03b1 \u2265 \u03c9\u03b1\u22a4\u03b1 = \u03c9||\u03b1||2. By combining the bound on \u03b1 with the bound on f\u03b1, we obtain that\nmax k ||\u03b1k\u2217 || \u2264 max k\n||f\u03b1k \u2217 ||\u00b5\u221a \u03c9 \u2264 Vmax\u221a \u03c9\nTransfer error. Since Q\u0303k is the truncation of a function f\u03b1\u0302k = Q\u0302 k belonging to F , the transfer error is\nmax k ||(T1 \u2212 T \u03bb)Q\u0303k||\u00b5 = sup \u03b1 \u2016(T1 \u2212 T \u03bb)T (f\u03b1)\u2016\u00b5.\nFinally, the statement of the theorem follows by taking a union bound over K iterations."}, {"heading": "D Analysis of BAT", "text": ""}, {"heading": "D.1 Proof of Theorem 3", "text": "Lemma 2. Let {(Xs, As, R1s, . . . , RMs )}Ss=1 be a training set where (Xs, As) iid\u223c \u00b5 and Rms = Rm(Xs, As) and for each state-action pair and for each task m, T next states Y ms,t \u223c Pm(\u00b7|Xs, As) with t = 1, . . . , T are available. For any fixed bounded function Q \u2208 B(X \u00d7 A;Vmax), the \u03bb\u0302 returned by minimizing Eq. 3 is such that\nE\u03bb\u0302(Q)\u2212 E\u03bb\u2217(Q) \u2264 2Vmax \u221a (M \u2212 2) log 4S/\u03b4 S + 16V 2max log 4SM/\u03b4 T (20)\nwith probability 1\u2212 \u03b4."}, {"heading": "Proof. [Lemma 2]", "text": "The sketch of the proof is as follows. For any state-action pair Xs, As, we define\nE\u0302\u03bb(Xs, As) = R1s \u2212 M\u2211\nm=2\n\u03bbmR m s + \u03b3\n1\nT\nT\u2211\nt=1\n( max a\u2032 Q\u0303k\u22121(Y 1s,t, a \u2032)\u2212 M\u2211\nm=2\n\u03bbm max a\u2032\nQ\u0303k\u22121(Y ms,t, a \u2032) ) ,\nand\nE\u03bb(Xs, As) = (T1Q\u0303k\u22121)(Xs, As)\u2212 M\u2211\nm=2\n\u03bbm(T mQ\u0303k\u22121)(Xs, As).\nAs a result, E\u03bb = E\u00b5 [ E\u03bb(x, a)2 ] and E\u0302\u03bb = 1S \u2211S s=1 E\u0302\u03bb(Xs, As)2. By Pollard\u2019s inequality on the (M -2)-dimensional simplex \u039b, we have for any \u03bb \u2208 \u039b\n|E\u00b5 [ E\u03bb(x, a)2 ] \u2212 1\nS\nS\u2211\ns=1\nE\u03bb(Xs, As)2| \u2264 Vmax \u221a\n(M \u2212 2) logS/\u03b4\u2032 S\n(21)\nwith probability 1 \u2212 \u03b4\u2032. Using Chernoff-Hoeffding inequality we now bound the distance between the true Bellman operators in E\u03bb(Xs, As) and their estimates in E\u0302\u03bb(Xs, As). By triangle inequality and the previous definitions, we obtain the following series of inequlities\n| 1 S\nS\u2211\ns=1\nE\u03bb(Xs, As)2 \u2212 1\nS\nS\u2211\ns=1\nE\u0302\u03bb(Xs, As)2| \u2264 | 1\nS\nS\u2211\ns=1\n(E\u03bb(Xs, As)\u2212 E\u0302\u03bb(Xs, As))2|\n\u2264 max s\n( E\u03bb(Xs, As)\u2212 1\nS\nS\u2211\ns=1\nE\u0302\u03bb(Xs, As) )2\n\u2264 2max s max m\n( (T mQ\u0303k\u22121)(Xs, Ax)\u2212Rms \u2212 \u03b3 1\nT\nT\u2211\nt=1\nmax a\u2032\nQ(Y ms,t, a \u2032) )2\n\u2264 2 ( 2Vmax \u221a logSM/\u03b4\u2032\nT\n)2 (22)\nBy using Eqs 21 and 22, we have for any \u03bb \u2208 \u039b\n|E\u03bb \u2212 E\u0302\u03bb| \u2264 Vmax \u221a\n(M \u2212 2) logS/\u03b4\u2032 S + 8V 2max logSM/\u03b4\u2032 T ,\nwith probability 1\u2212 2\u03b4\u2032. Finally, we can prove the following sequence of inequalities E\u03bb\u0302 \u2212 E\u03bb\u2217 = E\u03bb\u0302 \u2212 E\u0302\u03bb\u0302 + E\u0302\u03bb\u0302 \u2212 E\u0302\u03bb\u2217 + E\u0302\u03bb\u2217 \u2212 E\u03bb\u2217\n\u2264 2 sup \u03bb\u2208\u039b\n|E\u03bb \u2212 E\u0302\u03bb| \u2264 2Vmax \u221a\n(M \u2212 2) logS/\u03b4\u2032 S + 16V 2max logSM/\u03b4\u2032 T ,\nwith probability 1\u2212 4\u03b4\u2032. By setting \u03b4 = 4\u03b4\u2032 the statement follows."}, {"heading": "E Additional Experimental Analysis", "text": "In this section, we provide additional experimental results related to the BTT algorithm."}, {"heading": "E.1 Analysis of parameters \u03b2", "text": "In order to have a better understanding on how BTT trades off between the need for samples and the risk of introducing a large transfer error, in Figure 8 we show the values of parameters \u03b2 (which represent the percentage of samples transferred from each task) as optimized by the BTT algorithm at each FQI iteration. The tasks considered are the target task M1 and the source tasks M6,M7,M8,M9, each one with 5000 samples available. Figure 8 compares the values of \u03b2 in two scenarios: when the available target samples are 100 (left pane) and 10000 (right pane). Obviously, BTT always exploits all the target samples (\u03b21 = 1). When few target samples are available, BTT transfers high percentages of samples from the source tasks. In particular, it transfers all the samples available from task M9 in each iteration, and also the percentage of samples taken from task M8 is almost constant (about 0.7). The percentage of samples transferred from tasks M6 and M7 starts from 100% and decreases (with different rates) through iterations reaching zero after iteration 10. This behavior can be explained by the attempt to include as many samples as possible at the earlier iterations when it is still possible to find combinations of sources with a small transfer error. As the iterations continue, no suitable combination of sources is possible and the algorithm is forced to reduce the number of samples from the more different source tasks. On the other hand, when the number of target samples is large enough, we notice that the percentage of samples transferred from all the source tasks drop down after the first FQI iterations. In fact, in this case, BTT exploits a lot of source samples to produce a more accurate approximations only when a very small error is introduced. On the other hand, as the iterations progress, the samples from the source tasks (even when optimally combined) provide a poor approximation of the Q-functions and, as a result, BTT, given the large number of target samples (10000), prefers to reduce the number of samples transferred from the source tasks.\nIn Figure 9 we show the proportions \u03bb induced by the weights \u03b2 computed by BTT. When only 100 target samples are available, BTT tries to compensate the lack of target samples by transferring a large amount of samples from a suitable combination of source tasks, while, when many target samples are available, it considers source samples only when they can guarantee a good approximation of the target Q-functions, otherwise the proportions are changed in favor of the target samples.\nFinally, in Figure 10, we consider the total number of samples used to train FQI at each iteration under the two scenarios. As expected, at the first iterations, due to the similarity between source tasks and target task, the number of samples provided to FQI by BTT is very large and then it decreases through iterations. It is interesting to notice that the total number of samples selected in the two scenarios are quite similar (in particular starting from the third iteration), which is an effect of the tradeoff realized by the BTT algorithm."}, {"heading": "E.2 Analysis of parameter \u03c4", "text": "The tradeoff realized by the BTT algorithm is tuned by the parameter \u03c4 multiplying the estimation error. In Figure 11 we analyze the effect of \u03c4 on the learning performances. Different values of the tradeoff parameter have been tried (\u03c4 = 0.25, 0.50, 0.75, 1.0) when both 5000 samples (left pane) and 10000 samples (right pane) are available for each source task. As we can notice, BTT is quite robust w.r.t. the choice of the tradeoff parameter. The main differences appear when a small number of target samples is available. In this case, low values of \u03c4 make BTT more concerned about the transfer error and, as a result, it tends to avoid transferring source samples, even if target samples are not enough. On the other hand, with high values of \u03c4 , BTT is pushed to use more source samples, and this may negatively affect the performance when several target tasks are available and no combination of source tasks provides a good target approximation."}], "references": [{"title": "Fitted Q-iteration in continuous actionspace MDPs", "author": ["Andras Antos", "R\u00e9mi Munos", "Csaba Szepesvari"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "A theory of learning from different domains", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Vaughan"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Learning from multiple sources", "author": ["Koby Crammer", "Michael Kearns", "Jennifer Wortman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Tree-based batch mode reinforcement learning", "author": ["Damien Ernst", "Pierre Geurts", "Louis Wehenkel"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "A distribution-free theory of nonparametric regression", "author": ["L. Gy\u00f6rfi", "M. Kohler", "A. Krzy\u017cak", "H. Walk"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Finite-sample analysis of LSTD", "author": ["A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "Technical Report inria-00482189,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Transfer of samples in batch reinforcement learning", "author": ["A. Lazaric", "M. Restelli", "A. Bonarini"], "venue": "In Proceedings of the Twenty-Fifth Annual International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Knowledge Transfer in Reinforcement Learning", "author": ["Alessandro Lazaric"], "venue": "PhD thesis, Poltecnico di Milano,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Proceedings of the 22nd Conference on Learning Theory (COLT\u201909),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Finite time bounds for fitted value iteration", "author": ["R. Munos", "Cs. Szepesv\u00e1ri"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Transferring instances for model-based reinforcement learning", "author": ["Matthew E. Taylor", "Nicholas K. Jong", "Peter Stone"], "venue": "In Proceedings of the European Conference on Machine Learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}], "referenceMentions": [{"referenceID": 11, "context": "The objective of transfer in reinforcement learning (RL) [12] is to speed-up RL algorithms by reusing knowledge (e.", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "A wide range of scenarios and methods for transfer in RL have been studied in the last decade (see [14, 9] for a thorough survey).", "startOffset": 99, "endOffset": 106}, {"referenceID": 12, "context": "This approach has been already investigated in the case of transfer between tasks with different state-action spaces in [13], where the source samples are used to build a model of the target task whenever the number of target samples is not large enough.", "startOffset": 120, "endOffset": 124}, {"referenceID": 7, "context": "A more sophisticated sample-transfer method is proposed in [8].", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "In [2] and [10] different distance measures are proposed and are shown to be connected to the performance of the transferred solution.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "In [2] and [10] different distance measures are proposed and are shown to be connected to the performance of the transferred solution.", "startOffset": 11, "endOffset": 15}, {"referenceID": 2, "context": "The case of transfer of samples from multiple source tasks is studied in [3].", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "We introduce three sample-transfer algorithms based on fitted Qiteration [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 10, "context": "Similar to [11], we first study the prediction error in each iteration and we then propagate it through iterations.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "[11] Given \u03bc, \u03bd, p \u2265 1, and an arbitrary sequence of policies {\u03c0p}p\u22651, we assume that the future-state distribution \u03bcP1 \u03c01 \u00b7 \u00b7 \u00b7 P1 \u03c0p is absolutely continuous w.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "This term is referred to as the inherent Bellman error [11] of the space F and it is related to how well the Bellman images of functions in F can be approximated by F itself.", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "This term also displays interesting similarities with the notion of discrepancy introduced in [10] in domain adaptation.", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": ", Rs,M )}s=1 be an auxiliary training set where (Xs, As) \u223c \u03bc and Rs,m = We refer to [11] for a thorough explanation of the concentrability terms.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "It is interesting to notice that similar estimation errors appear in FVI [11] where the optimal Bellman operator is approximated by Monte-Carlo estimation.", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "In [8] a method to compute the similarity between MDPs is proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In particular, the method in [8] tries to identify source tasks which are individually similar to the target task, while the transfer error minimized in BAT considers the average MDP obtained by the transfer process.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "do Compute \u03b2\u0302 = argmin\u03b2\u2208[0,1]M \u00ca\u03b2 + c \u221a d \u2211 M m=1 \u03b2mNm Run one iteration of AST (Fig.", "startOffset": 24, "endOffset": 29}, {"referenceID": 0, "context": "Let \u03b2 \u2208 [0, 1] be a weight vector, where \u03b2m is the fraction of samples from task m used in the transfer process.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "\u03b2\u0302 = arg min \u03b2\u2208[0,1]M ( \u00ca\u03b2(Q\u0303) + \u03c4 \u221a d \u2211M m=1 \u03b2mNm ) , (6)", "startOffset": 15, "endOffset": 20}, {"referenceID": 5, "context": "We consider a continuous extension of the 50-state variant of the chain walk problem proposed in [6].", "startOffset": 97, "endOffset": 100}, {"referenceID": 8, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M2 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M2 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "1 \u22125 in [\u221211,\u22129] \u222a [9, 11] M3 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 \u22125 in [\u221211,\u22129] \u222a [9, 11] M3 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "1 +5 in [\u221211,\u22129] \u222a [9, 11] M4 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 +5 in [\u221211,\u22129] \u222a [9, 11] M4 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 3, "context": "1 +1 in [\u22126,\u22124] \u222a [4, 6] M5 0.", "startOffset": 18, "endOffset": 24}, {"referenceID": 5, "context": "1 +1 in [\u22126,\u22124] \u222a [4, 6] M5 0.", "startOffset": 18, "endOffset": 24}, {"referenceID": 3, "context": "1 \u22121 in [\u22126,\u22124] \u222a [4, 6] Table 2: Parameters for the second set of tasks", "startOffset": 18, "endOffset": 24}, {"referenceID": 5, "context": "1 \u22121 in [\u22126,\u22124] \u222a [4, 6] Table 2: Parameters for the second set of tasks", "startOffset": 18, "endOffset": 24}, {"referenceID": 8, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M6 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M6 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M7 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M7 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M8 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M8 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "1 \u22125 in [\u221211,\u22129] \u222a [9, 11] M9 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 \u22125 in [\u221211,\u22129] \u222a [9, 11] M9 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "5 +5 in [\u221211,\u22129] \u222a [9, 11]", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "5 +5 in [\u221211,\u22129] \u222a [9, 11]", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "The reward function provides +1 when the system state reaches the regions [\u221211,\u22129] and [9, 11] and 0 elsewhere.", "startOffset": 87, "endOffset": 94}, {"referenceID": 10, "context": "The reward function provides +1 when the system state reaches the regions [\u221211,\u22129] and [9, 11] and 0 elsewhere.", "startOffset": 87, "endOffset": 94}], "year": 2011, "abstractText": "Transfer reinforcement learning (RL) methods leverage on the experience collected on a set of source tasks to speed-up RL algorithms. A simple and effective approach is to transfer samples from source tasks and include them in the training set used to solve a target task. In this paper, we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks. Finally, we report illustrative experimental results in a continuous chain problem.", "creator": "gnuplot 4.4 patchlevel 0"}}}