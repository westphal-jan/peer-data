{"id": "1202.3505", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2012", "title": "Near-optimal Coresets For Least-Squares Regression", "abstract": "fuzzy functional coreset is a subset of the data which contains effectively all the key answers. we propose deterministic, low order polynomial - time procedures to construct complete coresets for maximum and multiple response linear design, together using lower bounds enabling that type s not excess variation above improvement upon our database.", "histories": [["v1", "Thu, 16 Feb 2012 03:07:35 GMT  (41kb)", "http://arxiv.org/abs/1202.3505v1", "15 pages; working paper"], ["v2", "Fri, 21 Jun 2013 20:58:43 GMT  (46kb)", "http://arxiv.org/abs/1202.3505v2", "To appear in IEEE Transactions on Information Theory"]], "COMMENTS": "15 pages; working paper", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["christos boutsidis", "petros drineas", "malik magdon-ismail"], "accepted": false, "id": "1202.3505"}, "pdf": {"name": "1202.3505.pdf", "metadata": {"source": "CRF", "title": "Rich Coresets For Constrained Linear Regression", "authors": ["Christos Boutsidis"], "emails": ["cboutsi@us.ibm.com", "drinep@cs.rpi.edu", "magdon@cs.rpi.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 2.\n35 05\nv1 [\ncs .D\nS] 1"}, {"heading": "1 Introduction", "text": "Linear regression is an important technique in data analysis (Seber and Lee, 1977). Research in the area ranges from numerical techniques (A. Bjo\u0308rck, 1996) to robustness of the prediction error to noise (e.g. using feature selection (Guyon and Elisseeff, 2003)).\nIs it possible to efficiently identify a small subset of the data that contains all the essential information of a learning problem? Such a subset is called a \u201crich\u201d coreset. We show that the answer is yes, for linear regression. Such a rich coreset is analogous to the support vectors in support vector machines (Cristianini and Shawe-Taylor, 2000). Such rich coresets contain the meaningful or important points in the data and can be used to find good approximate solutions to the full problem by solving a (much) smaller problem. When the constraints are complex (e.g. non-convex constraints), solving a much smaller regression problem could be a significant saving (Gao, 2007).\nWe present coreset constructions for constrained regression (both simple and multiple response), as well as lower bounds for the size of \u201crich\u201d coresets. In addition to potential computational savings, a rich coreset identifies the important core of a machine learning problem and is of considerable interest in applications with huge data where incremental approaches are necessary (eg. chunking) and applications where data is distributed and bandwith is costly (hence communicating only the essential data is imperative).\nOur first contribution is a deterministic, polynomial-time algorithm for constructing a rich coreset for arbitrarily constrained linear regression. Let k be the \u201ceffective dimension\u201d of the data and let \u01eb > 0 be the desired accuracy parameter. Our algorithm constructs a rich coreset of size O ( k/\u01eb2 )\n, which achieves a (1 + \u01eb)-relative error performance guarantee. In other words, solving the regression problem on the coreset results in a solution which fits all the data with an error which is at most (1 + \u01eb) worse than the best possible fit to all the data. We extend our results to the setting of multiple response regression using more\nsophisticated techniques. Underlying our proofs are two sparsification tools from linear algebra (Batson et al., 2009; Boutsidis et al., 2011), which may be of general interest to the machine learning community."}, {"heading": "1.1 Problem Setup", "text": "Assume the usual setting with n data points (z1, y1), . . . , (zn, yn); zi \u2208 Rd are feature vectors (which could have been obtained by applying a non-linear feature transform to raw data) and yi \u2208 R are targets (responses). The linear regression problem asks to determine a vector xopt \u2208 D \u2286 Rd that minimizes\nE(x) = n \u2211\ni=1\nwi(z T i \u00b7 x\u2212 yi)2\nover x \u2208 D, where wi are positive weights. So, E(xopt) \u2264 E(x), for all x \u2208 D. The domain D represents the constraints on the solution, e.g., in non-negative least squares (NNLS) (Lawson and Hanson, 1974; Bellavia et al., 2006), D = Rd+, the nonnegative orthant. Our results hold for arbitrary D.\nA coreset of size r < n is a subset of the data, (zi1 , yi1), . . . , (zir , yir). The coreset regression problem considers the squared error on the coreset with a, possibly different, set of weights sj > 0,\nE\u0303(x) = r \u2211\nj=1\nsj(z T ij \u00b7 x\u2212 yij) 2.\nSuppose that E\u0303 is minimized at x\u0303opt, so E\u0303(x\u0303opt) \u2264 E\u0303(x), for all x \u2208 D. The coreset is rich if, for some set of weights sj, x\u0303opt is nearly as good as xopt for the original regression problem on all the data, i.e., for some small \u01eb > 0, E(xopt) \u2264 E(x\u0303opt) \u2264 (1 + \u01eb)E(xopt). The algorithm which constructs the coreset should also provide the weights sj. For the remainder of the paper, we switch to an equivalent matrix formulation of the problem. We give some linear algebra background in the Appendix.\nMatrix Formulation. Let A \u2208 Rn\u00d7d be the data matrix whose rows are the weighted data points\u221a wiz T i and b \u2208 Rn is the similarly weighted target vector, bi = \u221a wiyi. The effective dimension of the data can be measured by the rank of A; let k = rank(A).Our results hold for arbitrary d, however, in most applications, n \u226b d and rank(A) \u2248 d. We can rewrite the squared error as E(x) = \u2016Ax\u2212 b\u201622, so,\nxopt = argmin x\u2208D\n\u2016Ax\u2212 b\u201622.\nA coreset of size r < n is a subset C \u2208 Rr\u00d7d of the rows of A and the corresponding elements bc \u2208 Rr of b. Let D \u2208 Rr\u00d7r be a positive diagonal matrix for the coreset regression (the weights sj of the coreset regression will depend onD). The weighted squared error on the coreset is given by E\u0303(x) = \u2016D(Cx\u2212bc)\u201622, so the coreset regression seeks x\u0303opt defined by\nx\u0303opt = argmin x\u2208D\n\u2016D (Cx\u2212 bc) \u201622.\nWe say that the coreset is (1 + \u01eb)-rich if the solution obtained by fitting the coreset data can fit all the data almost optimally. Formally,\n\u2016Axopt \u2212 b\u201622 \u2264 \u2016Ax\u0303opt \u2212 b\u201622 \u2264 (1 + \u01eb)\u2016Axopt \u2212 b\u201622."}, {"heading": "1.2 Our contributions", "text": "Constrained Linear Regression (Section 2). Our main result for constrained simple regression is Theorem 1, which describes a deterministic polynomial time algorithm that constructs a (1+\u01eb)-rich coreset of size O ( k/\u01eb2 )\n. Prior to our work, the best result achieving comparable relative error performance guarantees is Theorem 1 of (Boutsidis and Drineas, 2009) for constrained regression, and the work of (Drineas et al., 2006) for unconstrained regression. Both of these prior results construct coresets of size O ( k log k/\u01eb2 ) and they are randomized, so, with some probability, the fit on all the data can be arbitrarily bad (despite the coreset being a logarithmic factor larger). Our methods have comparable, low order polynomial running times and provide deterministic guarantees. The results in (Drineas et al., 2006) and (Boutsidis and Drineas, 2009) are achieved using the matrix concentration results in (Rudelson and Vershynin, 2007). However, these concentration bounds break unless the coreset size is at least \u2126 ( k log(k)/\u01eb2 )\n. We give the first algorithms that break the k log k barrier.\nWe extend our results to multiple response regression, where the target is a matrix B \u2208 Rn\u00d7\u03c9 with \u03c9 \u2265 1. Each column of B is a seperate target (or response) that we wish to predict. We seek to minimize \u2016AX\u2212B\u2016 over allX \u2208 D \u2286 Rd\u00d7\u03c9, and some matrix norm \u2016 \u00b7 \u2016. Multiple response regression has numerous applications, but is perhaps most common in multivariate time series analysis; see for example (Hamilton, 1994; Breiman and Friedman, 1997). To illustrate, consider prediction of time series data: let Z \u2208 R(n+1)\u00d7d be a set of d time series, where each column is a time series with n+1 time steps; we wish to predict time step t+ 1 from time step t. Let A contain the first n rows of Z and let B contain the last n rows. Then, we seek X that minimizes \u2016AX\u2212B\u2016, which is exactly the multiple response regression problem. In our work, we focus on the spectral norm \u2016 \u00b7 \u20162 and the Frobenius norm \u2016 \u00b7 \u2016F, the two most common norms in matrix analysis.\nMulti-Objective Regression (Section 3.1). An important variant of multiple regression is the socalled multi-objective regression. Let B = [b1, . . . ,b\u03c9] \u2208 Rn\u00d7\u03c9, where we explicitly identify each column in B as a target response bj . We seek to simultaneously fit multiple target vectors with the same x, i.e. to simultaneously minimize \u2016Ax\u2212 bj\u201622 where j \u2208 {1, 2, . . . , \u03c9}. This is common when the goal is to trade off different quality criteria simultaneously. Writing X = [x,x, . . . ,x] \u2208 Rd\u00d7\u03c9 (\u03c9 copies of x), we consider minimizing \u2016AX\u2212B\u2016F, which is equivalent to multiple regression with a strong constraint on X. We present results for coreset constructions for the Frobenius-norm multi-objective regression problem in Theorem 4, which describes a deterministic algorithm to construct (1+ \u01eb)-rich coresets of size O ( k/\u01eb2 )\n. Theorem 4 emerges by applying Theorem 1 after converting the Frobenius-norm multi-objective regression problem to a simple response regression problem.\nArbitrarily-Constrained Multiple-Response Regression (Section 3.2). Using the same approach, converting the problem to a single response regression, we construct a (1 + \u01eb)-rich coreset for Frobeniusnorm arbitrarily-constrained regression in Section 3.2. The coreset size here is O ( k\u03c9/\u01eb2 ) .\nUnconstrained Multiple-Response Regression (Section 4). In Section 4, we consider rich coresets for unconstrained multiple regression for both the spectral and Frobenius norms. The sizes of the coresets are smaller than the constrained case, and our main results are presented in Theorems 6 and 7. Theorem 6 presents a (2+ \u01eb)-rich coreset of size O((k+\u03c9)/\u01eb2) for spectral norm regression, while Theorem 7 presents a (2 + \u01eb)-rich coreset of size O(k/\u01eb2) for Frobenius norm regression.\nLower Bounds (Section 5). Finally, in Section 5, we present lower bounds on coreset sizes. In the single response regression setting, we note that our algorithms need to look at the target vector b. We show that this is unavoidable, by arguing that no b-agnostic deterministic coreset construction algorithm\ncan construct rich coresets which are small (Theorem 11). We also present similar results for b-agnostic randomized coreset constructions (Theorem 12). Having shown that we cannot (in general) be b-agnostic, we present lower bounds on the size of rich coresets for spectral and Frobenius norm multiple response regression that apply in the non b-agnostic setting (Theorems 13 and 14)."}, {"heading": "2 Constrained Linear Regression", "text": "We define constrained linear regression as follows: given A \u2208 Rn\u00d7d of rank k, b \u2208 Rn, and D \u2286 Rd, we seek xopt \u2208 D for which \u2016Axopt\u2212b\u201622 \u2264 \u2016Ax\u2212b\u201622, for all x \u2208 D (the domain D represents the constraints on x and can be arbitrary). To construct a coreset C \u2208 Rr\u00d7d (i.e., C consists of a few rows of A) and bc \u2208 Rr (i.e., bc consists of a few elements of b), we introduce sampling and rescaling matrices S and D respectively. More specifically, we define the row-sampling matrix S \u2208 Rr\u00d7n whose rows are basis vectors eTi1 , . . . , e T ir . Our coreset C is now equal to SA; clearly, C is a matrix whose rows are the rows of A corresponding to indices i1, . . . , ir. Similarly, bc = Sb contains the corresponding elements of the target vector. Next, let D \u2208 Rr\u00d7r be a positive diagonal rescaling matrix and define the D-weighted regression problem on the coreset as follows:\nx\u0303opt = argmin x\u2208D \u2016D (Cx\u2212 bc) \u201622 = argmin x\u2208D \u2016DS (Ax\u2212 b) \u201622. (1)\nIn the above, the operator DS first samples and then rescales rows of A and b. Theorem 1 is the main result in this section and presents a deterministic algorithm to select a rich coreset by constructing the matrices D and S. (All algorithms are given in the Appendix.)\nTheorem 1. Given A \u2208 Rn\u00d7d of rank k, b \u2208 Rn, and D \u2286 Rd, Algorithm 1 constructs matrices S \u2208 Rr\u00d7n and D \u2208 Rr\u00d7r (for any r > k + 1) such that x\u0303opt of eqn. (1) satisfies\n\u2016Ax\u0303opt \u2212 b\u201622 \u2016Axopt \u2212 b\u201622 \u2264 r + k + 1 + 2 \u221a r(k + 1) r + k + 1\u2212 2 \u221a r(k + 1) = 1 + 4\n\u221a\nk r + o ( \u221a k/r ) .\nThe running time of the proposed algorithm is T ( U[A,b] ) +O ( rnk2 ) , where T ( U[A,b] ) is the time needed to compute the left singular vectors of the matrix [A,b] \u2208 Rn\u00d7(d+1).\nFor any 0 < \u01eb < 1, we can set r = k/\u01eb2 to get an approximation ratio roughly equal to 1 + 4\u01eb. This result considerably improves the result in (Boutsidis and Drineas, 2009), which needs r = O(k log(k)/\u01eb2) to achieve the same approximation. Additionally, our bound is deterministic, whereas the bound in (Boutsidis and Drineas, 2009) fails with constant probability. (Boutsidis and Drineas, 2009) requires an SVD computation in the first step, so its running time is comparable to ours.\nIn order to prove the above theorem, we need a linear algebraic sparsification result from (Batson et al., 2009), which we restate using our notation.\nLemma 2 (Single-set Spectral Sparsification (Batson et al., 2009)). Given U \u2208 Rn\u00d7\u2113 satisfying UTU = I\u2113 and r > \u2113, we can deterministically construct sampling and rescaling matrices S and D such that, for all y \u2208 R\u2113:\n(\n1\u2212 \u221a \u2113/r )2 \u2016Uy\u201622 \u2264 \u2016DSUy\u201622 \u2264 ( 1 + \u221a \u2113/r )2 \u2016Uy\u201622.\nThe algorithm runs in O(rn\u21132) time and we denote it as [D,S] = SimpleSampling(U, r).\nProof. (of Theorem 1) Let Y = [A,b] \u2208 Rn\u00d7(d+1) and compute its SVD: Y = U\u03a3VT. Let \u2113 be the rank of Y (\u2113 \u2264 k + 1, since rank(A) = k) and note that U \u2208 Rn\u00d7\u2113, \u03a3 \u2208 R\u2113\u00d7\u2113, and V \u2208 R(d+1)\u00d7\u2113. Let [D,S] = SimpleSampling(U, r) and define y1,y2 \u2208 R\u2113 as follows:\ny1 = \u03a3V T\n[\nxopt \u22121\n]\n, and y2 =\n[\nx\u0303opt \u22121\n]\n.\nNote thatUy1 = Axopt\u2212b, Uy2 = Ax\u0303opt\u2212b,DSUy1 = DS (Axopt \u2212 b), andDSUy2 = DS (Ax\u0303opt \u2212 b). We will bound \u2016Uy2\u2016 in terms of \u2016Uy1\u2016:\n(\n1\u2212 \u221a \u2113/r )2 \u2016Uy2\u201622 (a) \u2264 \u2016DSUy2\u201622 (b) \u2264 \u2016DSUy1\u201622 (c) \u2264 ( 1 + \u221a \u2113/r )2 \u2016Uy1\u201622.\n(a) and (c) follow from Lemma 2; (b) follows from the optimality of x\u0303opt for the coreset regression in eqn. (1). Using \u2113 \u2264 k+1 and manipulating the above expression concludes the proof of the theorem. The running time of the algorithm is equal to the time needed to compute U and the time needed to run the algorithm of Lemma 2 with \u2113 \u2264 k + 1."}, {"heading": "3 Constrained Multiple-Response Regression", "text": "Constrained multiple-response regression in the Frobenius norm can be reduced to simple regression. So, we can apply the results of the previous section to this setting."}, {"heading": "3.1 Multi-Objective Regression", "text": "The task is to minimize, over all x \u2208 D, the Frobenius-norm error \u2016A[x, . . . ,x]\u2212B\u20162F. Let bavg = 1\u03c9B1\u03c9 (here 1\u03c9 is a vector of all ones and thus bavg is the average of the columns in B). Recall that A \u2208 Rn\u00d7d, B \u2208 Rn\u00d7\u03c9, and let X = [x, . . . ,x] \u2208 Rd\u00d7\u03c9.\nLemma 3. For X = [x, . . . ,x] \u2208 Rd\u00d7\u03c9, \u2016AX\u2212B\u20162F = \u03c9\u2016Ax\u2212 bavg\u2016 2 2 +\n\u03c9 \u2211\ni=1\n\u2016bavg \u2212B(i)\u2016 2\n2.\nIn the above B(i) denotes the i-th column of B as a column vector. Note that the second term in Lemma 3 does not depend on x and thus the generalized multi-objective regression can be reduced to simple regression on A and bavg. Using Theorem 1, we can get a coreset: let x\u0303opt minimize \u2016DS (Ax\u2212 bavg) \u20162, where S and D are obtained via Theorem 1 applied to A and bavg . If X\u0303opt = [x\u0303opt, . . . , x\u0303opt], then, by Lemma 3, X\u0303opt minimizes \u2016DS (AX\u2212B) \u2016F. Similarly, if xopt minimizes \u2016Ax \u2212 bavg\u20162 and Xopt = [xopt, . . . ,xopt], then Xopt minimizes \u2016AX\u2212B\u2016F . Theorem 4 says that X\u0303opt is approximates Xopt (the proof is in the appendix).\nTheorem 4. Given A \u2208 Rn\u00d7d of rank k and B \u2208 Rn\u00d7\u03c9, we can construct matrices S \u2208 Rr\u00d7n and D \u2208 Rr\u00d7r (for any r > k+1) such that the matrix X\u0303opt = [x\u0303opt, . . . , x\u0303opt] that minimizes \u2016DS (AX\u2212B) \u2016F over all matrices X = [x,x, . . . ,x] satisfies:\n\u2016AX\u0303opt \u2212B\u2016 2 F \u2264 ( 1 +O ( \u221a k/r )) \u2016AXopt \u2212B\u20162F.\nThe run time of the proposed algorithm is T ( U[A,bavg] ) +O ( n\u03c9 + rnk2 ) , where T ( U[A,bavg] ) is the time needed to compute the left singular vectors of the matrix [A,bavg ] \u2208 Rn\u00d7(d+1).\nWe note that the coreset size depends only on the rank of A and not on the size of B."}, {"heading": "3.2 Arbitrarily-Constrained Multiple-Response Regression", "text": "Multi-objective regression is a special case of constrained multiple-response regression for which we can efficiently obtain the coresets. In the general case, the problem still reduces to simple regression, but the coresets are now larger. We wish to minimize \u2016AX\u2212B\u2016F over X \u2208 D \u2286 Rd\u00d7\u03c9. Since Rd\u00d7\u03c9 is isomorphic to Rd\u03c9, we can view X \u2208 Rd\u00d7\u03c9 as a \u201cstretched out\u201d vector X\u0302 \u2208 Rd\u03c9; corresponding to the domain D is the domain D\u0302 \u2286 Rd\u03c9. Similarly, we can stretch out B \u2208 Rn\u00d7\u03c9 to B\u0302 \u2208 Rn\u03c9. To complete the transformation to simple linear regression, we build a transformed block-diagonal data matrix A\u0302 from A, by repeating \u03c9 copies of A along the diagonal:\nA\u0302 =\n\n   \nA A\n. . .\nA\n\n    \u2208 Rn\u03c9\u00d7d\u03c9, X\u0302 =\n\n   \nX(1) X(2)\n...\nX(\u03c9)\n\n    \u2208 Rd\u03c9, B\u0302 =\n\n   \nB(1) B(2)\n...\nB(\u03c9)\n\n    \u2208 Rn\u03c9\nLemma 5. For all A, X and B of appropriate dimensions, \u2016AX\u2212B\u20162F = \u2016A\u0302X\u0302\u2212 B\u0302\u201622.\nTheorem 1 gives us coresets for this equivalent regression. Note that rank(A\u0302) \u2264 \u03c9 \u00b7 rank(A). The coreset will identify the important rows of A (the same row may get identified multiple times as different rows of A\u0302), and the important elements of B, because the entries in B\u0302 are elements of B, not rows of B. Let X\u0302opt be the solution constructed from the coreset, which minimizes \u2016A\u0302X\u0302\u2212 B\u0302\u2016 over X\u0302 \u2208 D\u0302, and let X\u0303opt \u2208 D be the corresponding solution in the original domain D. If r is the size of the coreset and rank(A) = k, then, by Theorem 1,\n\u2016AX\u0303opt \u2212B\u2016 2 F \u2264 ( 1 +O ( \u221a k\u03c9/r )) \u2016AXopt \u2212B\u20162F.\nSo, for the approximation ratio to be 1+O(\u01eb), we set r = O ( k\u03c9/\u01eb2 ) . The running time would involve the\ntime needed to compute the SVD of [A\u0302, B\u0302]. Notice that the coresets are large and somewhat costly to compute and they only work for the Frobenius norm. In the next section, using more sophisticated techniques, we will get smaller coresets for unconstrained regression in both the Frobenius and spectral norms."}, {"heading": "4 Unconstrained Multiple-Response Regression", "text": "Consider the following problem: given a matrix A \u2208 Rn\u00d7d with rank exactly k and a matrix B \u2208 Rn\u00d7\u03c9, we seek to identify the matrix Xopt \u2208 Rd\u00d7\u03c9 that minimizes (\u03be = 2 and \u03be = F)\nXopt = arg min X\u2208Rd\u00d7\u03c9\n\u2016AX\u2212B\u20162\u03be .\nWe can compute Xopt via the pseudoinverse of A, namely Xopt = A +B. If S and D are sampling and rescaling matrices respectively, then the coreset regression problem is:\nX\u0303opt = arg min X\u2208Rd\u00d7\u03c9\n\u2016DSAX\u2212DSB\u20162\u03be . (2)\nThe solution of the coreset regression problem is X\u0303opt = (DSA) +DSB. The main results in this section are presented in Theorems 6 and 7.\nTheorem 6 (Spectral norm). Given a matrix A \u2208 Rn\u00d7d with rank exactly k and a matrix B \u2208 Rn\u00d7\u03c9, Algorithm 2 deterministically constructs matrices S and D such that solving the problem of eqn. (2) satisfies (for any r such that k + 1 < r \u2264 n):\n\u2016AX\u0303opt \u2212B\u201622 \u2264 \u2016AXopt \u2212B\u201622 + ( 1 + \u221a \u03c9/r\n1\u2212 \u221a k/r\n)2\n\u2016AXopt \u2212B\u201622.\nThe running time of the proposed algorithm is T (UA)+O ( rn ( k2 + \u03c92 ))\n, where T (UA) is the time needed to compute the left singular vectors of A.\nAsymptotically, for large \u03c9, the approximation ratio of the above theorem is O (\u03c9/r). We will argue that this is nearly optimal by providing a matching lower bound in Theorem 13.\nTheorem 7 (Frobenius norm). Given matrices A \u2208 Rn\u00d7d of rank k and B \u2208 Rn\u00d7\u03c9, Algorithm 3 deterministically constructs a sampling matrix S and a rescaling matrix D such that solving the problem of eqn. (2) satisfies (for any r such that k + 1 < r \u2264 n):\n\u2016AX\u0303opt \u2212B\u20162F \u2264 \u2016AXopt \u2212B\u20162F + 1 (\n1\u2212 \u221a k/r )2 \u2016AXopt \u2212B\u2016 2 F.\nThe running time of the proposed algorithm is T (UA) + O ( rnk2 )\n, where T (UA) is the time needed to compute the left singular vectors of A.\nThe approximation ratio in the above theorem is 2 + O( \u221a\nk/r). In Theorem 14, we will give a lower bound for the approximation ratio which is 1 + \u2126(k/r). We conjecture that our lower bound can be achieved, perhaps by a more sophisticated algorithm.\nFinally, we note that the B-agnostic randomized construction of Drineas et al. (2008) achieves a (1+\u01eb) approximation ratio using a significantly larger coreset, r = O(k log(k)/\u01eb2). Importantly, they do not need any access to B in order to construct the coreset, whereas our approach constructs coresets by carefully choosing important data points with respect to the particular target response matrix B. We will also discuss B-agnostic algorithms in Section 4.2 (Theorem 10) and we will present matching lower bounds in Section 5."}, {"heading": "4.1 Proofs of Theorems 6 and 7", "text": "We will make heavy use of facts from Section A. We start with a few simple lemmas.\nLemma 8. Let E = AXopt \u2212B be the regression residual. Then, rank(E) \u2264 min{\u03c9, n\u2212 k}.\nProof. Using our notation, AXopt \u2212 B = ( In \u2212UAUTA ) B = U\u22a5 A ( U\u22a5 A )T B. To conclude notice that rank(XY) \u2264 min{rank(X), rank(Y)}.\nWe now give our main tool for obtaining approximation guarantees for coreset regression. The proof is deferred to the appendix.\nLemma 9. Assume that the rank of the matrix DSUA \u2208 Rr\u00d7k is equal to k (i.e., the matrix has full rank). Then, for \u03be = 2,F,\n\u2016AX\u0303opt \u2212B\u2016 2 \u03be \u2264 \u2016AXopt \u2212B\u20162\u03be + \u2016(DSUA)+DS (AXopt \u2212B) \u20162\u03be .\nThis lemma provides a framework for coreset construction: all we need are sampling and rescaling matrices S and D, such that rank(DSUA) = k and \u2016 (DSUA)+DS (AXopt \u2212B) \u20162\u03be is small. The final ingredients for the proofs of Theorems 6 and 7 are two matrix sparsification results, Lemmas 16 and 17 in the Appendix.\nProof. (of Theorem 6) Theorem 6 follows from Lemmas 9 and 16. First, compute the SVD of A to obtain UA, and let E = AXopt \u2212 B = UAUTAB \u2212 B. Next, run the algorithm of Lemma 16 to obtain [\u2126,S] = MultipleSpectralSampling (UA,E, r). This algorithm will run in time TSVD (E) + O ( rn ( k2 + \u03c12 E ))\n, where k is the rank of UA and A. The total running time of the algorithm is T (UA) + TSVD (E) + O ( rn (\nk2 + \u03c12 E\n)) = T (UA) +O ( rn ( k2 + \u03c92 ))\n. Lemma 16 guarantees that D and S satisfy the rank assumption of Lemma 9. To conclude the proof,\nwe bound the second term of Lemma 9, using the bounds of Lemma 16 and \u03c1E \u2264 min{\u03c9, n\u2212 k} \u2264 \u03c9:\n\u2016(DSUA)+DS (AXopt \u2212B) \u201622 \u2264 \u2016 (DSUA)+ \u201622\u2016DS (AXopt \u2212B) \u201622 \u2264 ( 1 + \u221a \u03c9/r )2 ( 1\u2212 \u221a k/r )\u22122\n\u2016AXopt \u2212B\u201622.\nProof. (of Theorem 7) Similar to Theorem 6, using Lemma 17 instead of Lemma 16."}, {"heading": "4.2 B-Agnostic Coreset Construction", "text": "All the coreset construction algorithms that we presented so far carefully construct the coreset using knowledge of the response vector. If the algorithm does not need knowledge of B to construct the coreset, and yet can provide an approximation guarantee for every B, then the algorithm is B-agnostic. A Bagnostic coreset construction algorithm is appealing because the coreset, as specified by the sampling and rescaling matrices S and D, can be computed off-line and applied to any B. We briefly digress to show how our methods can be extended to develop B-agnostic coreset constructions.\nTheorem 10 (B-Agnostic Coresets). Given a matrix A \u2208 Rn\u00d7d with rank exactly k and a matrix B \u2208 R n\u00d7\u03c9, there exists an algorithm to deterministically construct a sampling matrix S and a rescaling matrix D such that for any B \u2208 Rn\u00d7\u03c9, the matrix X\u0303opt that solves the problem of eqn. (2) satisfies (for any r such that k < r \u2264 n):\n\u2016AX\u0303opt \u2212B\u20162\u03be \u2264 \u2016AXopt \u2212B\u20162\u03be + ( 1 + \u221a n/r\n1\u2212 \u221a k/r\n)2\n\u2016AXopt \u2212B\u20162\u03be .\nThe running time of the proposed algorithm is T (UA) + O ( rnk2 )\n, where T (UA) is the time needed to compute the left singular vectors of A.\nProof. The proof is similar to the proof of Theorem 6, except we now construct the sampling and rescaling matrices as [D,S] = MultipleSpectralSampling (UA, In, r). To bound the second term in Lemma 9, we use\n\u2016 (DSUA)+DS (AXopt \u2212B) \u20162\u03be = \u2016 (DSUA)+DSIn (AXopt \u2212B) \u20162\u03be \u2264 \u2016 (DSUA)+ \u201622\u2016DSIn\u201622\u2016 (AXopt \u2212B) \u20162\u03be ,\nand the bounds of Lemma 16.\nThe above bound decreases with r and holds for any B, guaranteeing a constant-factor approximation with a constant fraction of the data. The approximation ratio is O(n/r), which seems quite weak. In the next section, we show that this result is tight."}, {"heading": "5 Lower Bounds on Coreset Size", "text": "We have just seen a B-agnostic coreset construction algorithm with a rather weak worst case guarantee of O(n/r) approximation error. We will now show that no deterministic B-agnostic coreset construction algorithm can guarantee a better error (Theorem 11).\n(Drineas et al., 2008) provides anotherB-agnostic coreset construction algorithm with r = O(k log(k)/\u01eb2). For a fixed B, the method in (Drineas et al., 2008) delivers a probabilistic bound on the approximation error. However, there are target matrices B for which the bound fails by an arbitrarily large amount. The probabilistic algorithms get away with this by brushing all these (possibly large) errors into a low probability event, with respect to random choices made in the algorithm. So, in some sense, these algorithms are not B-agnostic, in that they do not construct a coreset which works well for all B with some (say) constant probability. Nevertheless, the fact that they give a constant probability of success for a fixed but unknown B makes these algorithms interesting and useful. We will give a lower bound on the approximation ratio of such algorithms as well, for a given probability of success (Theorem 12). Finally, we will give lower bounds on the size of the coreset for the general (non-agnostic) multiple regression setting (Theorems 13 and 14)."}, {"heading": "5.1 An Impossibility Result for B-Agnostic Coreset Construction", "text": "We first present the lower bound for simple regression. Recall that a coreset construction algorithm is b-agnostic if it constructs a coreset without knowledge of b, and then provides an approximation guarantee for every b. We show that no coreset can work for every b; therefore a b-agnostic coreset will be bad for some vector b. In fact, there exists a matrix A such that every coreset has an associated \u201cbad\u201d b.\nTheorem 11 (Deterministic b-Agnostic coresets). There exists a matrix A \u2208 Rn\u00d7d such that for every coreset C \u2208 Rr\u00d7d of size r \u2264 n, there exists b \u2208 Rn (depending on C) for which\n\u2016Ax\u0303opt \u2212 b\u201622 \u2265 n\nr \u2016Axopt \u2212 b\u201622.\nProof. Let A be any matrix with orthonormal columns whose first column is 1n/ \u221a n, and consider any\ncoreset C of size r. Let b = 1 C / \u221a n\u2212 r, where 1 C is the n-vector of 1\u2019s except at the coreset locations. So for the coreset regression, bc = 0, and so x\u0303opt = 0d\u00d71. Therefore, \u2016Ax\u0303opt \u2212 b\u201622 = \u2016b\u201622 = 1. Let PA project onto the columns of A and PA(1) project onto the first column of A. The following sequence establishes the result:\n\u2016Axopt \u2212 b\u201622 = \u2016(I \u2212PA)b\u201622 \u2264 \u2016(I \u2212PA(1))b\u201622 = r\nn\nWe now consider randomized algorithms that construct a coreset without looking at b (e.g. (Drineas et al., 2008)). These algorithms work for any fixed (but unknown) b, and deliver a probabilistic approximation guarantee for any single fixed b; in some sense they are b-agnostic. By the previous discussion, the returned coreset must fail for some b, i.e., the probabilistic guarantee does not hold for all b) and, when it fails, it could do so with very bad error. We will now present a lower bound on the approximation accuracy of such existing randomized algorithms for coreset construction, even for a single b.\nFirst, we define randomized coreset construction algorithms. Let C1,C2, . . . ,C(nr ) be the\n(\nn r\n)\ndifferent\ncoresets of size r. A randomized algorithm assigns probabilities p1, p2, . . . , p(nr ) to each coreset, and selects one according to these probabilities. The probabilities pi may depend on A. The algorithm is b-agnostic if the probabilities pi do not depend on b.\nTheorem 12 (Probabilistic b-Agnostic Coresets). For any randomized b-agnostic coreset construction algorithm, and any integer 0 \u2264 \u2113 \u2264 n \u2212 r, there exists A \u2208 Rn\u00d7d and b \u2208 Rn, such that, with probability at least\n(n\u2212r \u2113 ) / (n \u2113 ) ,\n\u2016Ax\u0303opt \u2212 b\u201622 \u2265 n\nn\u2212 \u2113\u2016Axopt \u2212 b\u2016 2 2.\nProof. Let A be any matrix with orthonormal columns whose first column is 1n/ \u221a n, as in the proof of Theorem 11. Let T be a set of size \u2113 \u2264 n \u2212 r. The neighborhood N(T) is the set of coresets that have non-empty intersection with T. Every coreset appears in (\nn \u2113\n) \u2212 ( n\u2212r \u2113 )\nsuch neighborhoods (the number of sets of size \u2113 which intersect with a coreset of size r). Let Pr [T] be the probability that the coreset selected by the algorithm is in N (T); then, Pr [T] = \u2211\nCi\u2208N(T) Pr [Ci]. Therefore,\n\u2211\nT\nPr [T] = \u2211\nT\n\u2211\nCi\u2208N(T)\nPr [Ci] = (n\n\u2113\n) \u2212 ( n\u2212 r \u2113 ) ,\nwhere the last equality follows because each coreset appears exactly (n \u2113 ) \u2212 (n\u2212r \u2113 )\ntimes in the summation and \u2211\niPr [Ci] = 1. Thus, there is at least one set T \u2217 for which\nPr [C \u2208 N(T\u2217)] \u2264 (n \u2113 ) \u2212 (n\u2212r \u2113 ) (\nn l\n) = 1\u2212 (n\u2212r \u2113 ) (\nn r\n) .\nSo with probability at least (n\u2212r\n\u2113\n) / (n \u2113 ) , the selected coreset does not intersect withT\u2217. Select b = 1T\u2217 (the\nunit vector which is 1/ \u221a \u2113 at the indices corresponding to T\u2217). Now, with probability at least\n(n\u2212r \u2113 ) / (n \u2113 )\n, x\u0303opt = 0, and the analysis in the proof of Theorem 11 shows that \u2016Ax\u0303opt \u2212 b\u201622 \u2265 nn\u2212\u2113\u2016Axopt \u2212 b\u201622.\nBy Stirling\u2019s formula, after some algebra, the probability (n\u2212r\n\u2113\n) / (n \u2113 ) is asymptotic to e\u22122r\u2113/n. Setting \u2113 = \u0398(n/r) gives a success probability that is \u0398(1) (a constant), then the approximation ratio cannot be better than 1 + \u2126(1/r). With regard to high probability (approaching one) algorithms, consider \u2113 = n log n/2r to conclude that if the success probability is at least 1 \u2212 1/n, the approximation ratio is no better than 1 + log(n)/(2r \u2212 log n)."}, {"heading": "5.2 Lower Bounds for Non-Agnostic Multiple Regression", "text": "For both the spectral and the Frobenius norm, we now consider non-agnostic unconstrained multiple regression, and give lower bounds for coresets of size r > d = rank(A) (for simplicity, we set rank(A) = d). The results are presented in Theorems 13 and 14.\nTheorem 13 (Spectral Norm). There exists A \u2208 Rn\u00d7d and B \u2208 Rn\u00d7\u03c9 such that for any r > d and any sampling and rescaling matrices S \u2208 Rr\u00d7n and D \u2208 Rr\u00d7r, the solution to the coreset regression X\u0303opt = (DSA) +DSB \u2208 Rd\u00d7\u03c9 satisfies\n\u2016AX\u0303opt \u2212B\u2016 2 2 \u2265 w\nr + 1 \u2016AXopt \u2212B\u201622.\nProof. First, we need some results from (Boutsidis et al., 2011). Boutsidis et al. (2011) exhibits a matrix B \u2208 R(\u03c9\u22121)\u00d7\u03c9 such that for any sampling matrix S \u2208 Rr\u00d7(\u03c9\u22121) and rescaling matrix D \u2208 Rr\u00d7r, with C = DSB (rescaled sampled coreset of B),\n\u2016B\u2212\u03a0C,k(B)\u201622 \u2265 \u03c9\nr + 1 \u2016B\u2212Bk\u201622,\nwhere \u03a0C,k(B) is the best rank-k approximation to B whose rows lie in the span of all the rows in C (the row-space of C); and, Bk is the best rank-k approximation to B (which could be computed via the truncated SVD of B). Actually, D is irrelevant here because the row-space of SB is not changed by a positive diagonal rescaling matrix D.\nSince \u03a0C,k(B) is the best rank-k approximation to B in the row-space of C, it follows that \u2016B \u2212 \u03a0C,k(B)\u201622 \u2264 \u2016B\u2212XC\u201622 for any X\u2208 R(\u03c9\u22121)\u00d7r with rank at most k (because XC will have rank at most k and is in the row space of C). Set X = UB,k(DSUB,k)\n+, where UB,k \u2208 R(\u03c9\u22121)\u00d7k has k columns which are the top-k left singular vectors of B. It is easy to verify that X has the correct dimensions and rank at most k. Since C = DSB, we have that\n\u2016B\u2212\u03a0C,k(B)\u201622 \u2264 \u2016B\u2212UB,k(DSUB,k)+DSB\u201622.\nWe now construct the regression problem. Let A = UB,d \u2208 R(\u03c9\u22121)\u00d7d (i.e., we choose k = d in the above discussion and n = \u03c9\u2212 1. Suppose a coreset construction algorithm gives sampling and rescaling matrices S and D, for a coreset of size r. So, the coreset regression is with A\u0303 = C = DSA and B\u0303 = DSB. The solution to the coreset regression is\nX\u0303opt = A\u0303 +B\u0303 = C+DSB = (DSA)+DSB = (DSUB,d) +DSB,\nwhich means that\n\u2016AX\u0303opt \u2212B\u2016 2 2 = \u2016UB,d(DSUB,d) +DSB\u2212B\u201622 \u2265 \u2016\u03a0C,d(B)\u2212B\u201622 \u2265\n\u03c9\nr + 1 \u2016Bd \u2212B\u201622.\nTo conclude the proof, observe that Bd = UB,dU T B,dB = AA +B = AXopt.\nTheorem 14 (Frobenius Norm). There exists A \u2208 Rn\u00d7d and B \u2208 Rn\u00d7\u03c9 such that for any r > d and any sampling and rescaling matrices S \u2208 Rn\u00d7r and D \u2208 Rr\u00d7r, the solution to the coreset regression X\u0303opt = (DSA) +DSB \u2208 Rd\u00d7\u03c9 satisfies\n\u2016AX\u0303opt \u2212B\u20162F \u2265 ( 1 + d\nr\n)\n\u2016AXopt \u2212B\u20162F.\nProof. The proof of this Frobenius norm lower bound follows the same argument as in the proof of Theorem 13, with \u03c9/(r+1) replaced by 1+d/r, providing that there is a matrix B for which \u2016B\u2212\u03a0C,d(B)\u20162F \u2265 (1+d/r)\u2016B\u2212Bd\u20162F. Indeed, the construction of such a matrix was presented in (Boutsidis et al., 2011)."}, {"heading": "6 Open problems", "text": "Can one determine the minimum size of a coreset that provides a (1+\u01eb) relative-error guarantee for simple linear regression? We conjecture that \u2126 (k/\u01eb) is a lower bound, which will make our results almost tight. Certainly, rich coresets of size exactly k cannot be guaranteed: consider two data points (1, 1), (\u22121, 1). The optimal regression is 0; however any coreset of size one will give non-zero regression. Is it possible to get strong guarantees on small corsets for other learning problems?"}, {"heading": "A Linear Algebra Background", "text": "The Singular Value Decomposition (SVD) of a matrix A \u2208 Rn\u00d7d of rank k is a decomposition A = UA\u03a3AV T A . The singular values \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3k > 0 are contained in the diagonal matrix \u03a3A \u2208 Rk\u00d7k; UA \u2208 Rn\u00d7k contains the left singular vectors of A; and VA \u2208 Rd\u00d7k contains the right singular vectors. The SVD of A can be computed in deterministic O(ndmin{n, d}) time.\nThe Moore-Penrose pseudo-inverse of A is equal to A+ = VA\u03a3 \u22121 A UT A . Given an orthonormal matrix UA \u2208 Rn\u00d7k, the perpendicular matrix U\u22a5A \u2208 Rn\u00d7(n\u2212k) to UA satisfies: (U\u22a5A)TU\u22a5A = In\u2212k, UTAU\u22a5A = 0k\u00d7(n\u2212k), and UAU T A + U\u22a5 A (U\u22a5 A )T = In. All the singular values of both UA and U \u22a5 A are equal to one. Given UA, U \u22a5 A can be computed in deterministic O ( n (n\u2212 k)2 ) time via the QR factorization.\nWe remind the reader of the Frobenius and spectral matrix norms: \u2016A\u20162F = \u2211 i,j A 2 ij = \u2211k i=1 \u03c3 2 i and \u2016A\u201622 = \u03c321. We will sometimes use the notation \u2016A\u2016\u03be to indicate that an expression holds for both \u03be = 2 or \u03be = F. For any two matrices X and Y, \u2016X\u20162 \u2264 \u2016X\u2016F \u2264 \u221a\nrank(X)\u2016X\u20162; \u2016XY\u2016F \u2264 \u2016X\u2016F\u2016Y\u20162; \u2016XY\u2016F \u2264 \u2016X\u20162\u2016Y\u2016F. These are stronger variants of the standard submultiplicativity property \u2016XY\u2016\u03be \u2264 \u2016X\u2016\u03be\u2016Y\u2016\u03be and we will refer to them as spectral submultiplicativity. It follows that, if Q is orthonormal, then \u2016QX\u2016\u03be \u2264 \u2016X\u2016\u03be and \u2016YQT\u2016\u03be \u2264 \u2016Y\u2016\u03be. Finally,\nLemma 15 (matrix-Pythagoras). Let X and Y be two n\u00d7 d matrices. If XYT = 0n\u00d7n or XTY = 0d\u00d7d, then \u2016X+Y\u20162\u03be \u2264 \u2016X\u20162\u03be + \u2016Y\u20162\u03be .\nA.1 Sparsification Results\nWe now state two recent results on matrix sparsification ((Boutsidis, 2011, Lemmas 71 and 72, p. 132),(Boutsidis et al., 2011)) using our notation.\nLemma 16 (Spectral Sparsification). Let Y \u2208 Rn\u00d7\u21131 and \u03a8 \u2208 Rn\u00d7\u21132 with respective ranks \u03c1Y, and \u03c1\u03a8. Given r > \u03c1Y, there exists a deterministic algorithm that runs in time TSVD (Y)+TSVD (\u03a8)+O(rn(\u03c1 2 Y + \u03c12\u03a8)) and constructs sampling and rescaling matrices S \u2208 Rr\u00d7n, D \u2208 Rr\u00d7r satisfying:\nrank (DSY) = rank (Y) ; \u2016 (DSY)+ \u20162 < 1\n1\u2212 \u221a \u03c1Y/r \u2016Y+\u20162; \u2016DS\u03a8\u20162 <\n(\n1 +\n\u221a\n\u03c1\u03a8 r\n)\n\u2016\u03a8\u20162.\nIf \u03a8 = In, the running time of the algorithm reduces to TSV D (Y) + O ( rn\u03c12 Y )\n. We write [D,S] = MultipleSpectralSampling (Y,\u03a8, r) to denote such a deterministic procedure.\nLemma 17 (Spectral-Frobenius Sparsification). Let Y \u2208 Rn\u00d7\u21131 and \u03a8 \u2208 Rn\u00d7\u21132 with respective ranks \u03c1Y, and \u03c1\u03a8. Given r > \u03c1Y, there exists a deterministic algorithm that runs in time TSVD(Y)+O(rn\u03c1 2 Y + \u21132n) and constructs sampling and rescaling matrices S \u2208 Rr\u00d7n, D \u2208 Rr\u00d7r satisfying:\nrank (DSY) = rank (Y) ; \u2016 (DSY)+ \u20162 < 1\n1\u2212 \u221a \u03c1Y/r \u2016Y+\u20162; \u2016DS\u03a8\u2016F \u2264 \u2016\u03a8\u2016F.\nIf \u03a8 = In, the running time of the algorithm reduces to TSV D (Y) + O ( rn\u03c12 Y )\n. We write [D,S] = MultipleFrobeniusSampling (Y,\u03a8, r) to denote such a deterministic procedure."}, {"heading": "B Algorithms", "text": "Input: A \u2208 Rn\u00d7d of rank k, b \u2208 Rn, and r > k + 1. Output: sampling matrix S and rescaling matrix D.\n1: Compute the SVD of Y = [A,b]. Let Y = U\u03a3VT, where U \u2208 Rn\u00d7\u2113, \u03a3 \u2208 R\u2113\u00d7\u2113 and V \u2208 Rd\u00d7\u2113, with \u2113 \u2264 k + 1 (the rank of Y). 2: Return [\u2126,S] = SimpleSampling(U, r) (see Lemma 2)\nAlgorithm 1: Deterministic coreset construction for constrained linear regression.\nInput: A \u2208 Rn\u00d7d of rank k, B \u2208 Rn\u00d7\u03c9, and r > k. Output: sampling matrix S and rescaling matrix D.\n1: Compute the SVD of A: A = UA\u03a3AV T A , where UA \u2208 Rn\u00d7k, \u03a3A \u2208 Rk\u00d7k, and VA \u2208 Rd\u00d7k;\ncompute E = UAU T A B\u2212B.\n2: return [S,D] = MultipleSpectralSampling(UA,E, r) (see Lemma 16)\nAlgorithm 2: Deterministic coresets for multiple regression in spectral norm.\nInput: A \u2208 Rn\u00d7d of rank k, B \u2208 Rn\u00d7\u03c9, and r > k. Output: sampling matrix S and rescaling matrix D.\n1: Compute the SVD of A: A = UA\u03a3AV T A , where UA \u2208 Rn\u00d7k, \u03a3A \u2208 Rk\u00d7k, and VA \u2208 Rd\u00d7k;\ncompute E = UAU T A B\u2212B.\n2: return [S,D] = MultipleFrobeniusSampling(UA,E, r) (see Lemma 17)\nAlgorithm 3: Deterministic coresets for multiple regression in Frobenius norm."}, {"heading": "C Technical Proofs", "text": "Proof. (Theorem 4) We first construct D and S via Theorem 1 applied to A and bavg . The running time is O (n\u03c9) (the time needed to compute bavg) plus the running time of Theorem 1. The result is immediate from the following derivation:\n\u2016AX\u0303opt \u2212B\u2016 2\nF\n(a) = \u03c9\u2016Ax\u0303opt \u2212 bavg\u20162 +\n\u03c9 \u2211\ni=1\n\u2016bavg \u2212B(i)\u2016 2\n(b) \u2264 ( 1 +O ( \u221a k/r ))2 \u03c9\u2016Axopt \u2212 bavg\u20162 + \u03c9 \u2211\ni=1\n\u2016bavg \u2212B(i)\u2016 2\n\u2264 ( 1 +O ( \u221a k/r ))2\n(\n\u03c9\u2016Axopt \u2212 bavg\u20162 + \u03c9 \u2211\ni=1\n\u2016bavg \u2212B(i)\u2016 2\n)\n(a) =\n( 1 +O ( \u221a k/r ))2\n\u2016AXopt \u2212B\u20162F.\n(a) follows by Lemma 3; (b) follows because x\u0303opt is the output of a coreset regression as in Theorem 1. Finally, r > k + 1 implies that ( 1 +O ( \u221a k/r ))2 = 1 +O ( \u221a k/r ) .\nProof. (Lemma 9) To simplify notation, let W = DS. Using the SVD of A, A = UA\u03a3AV T A , we get:\n\u2016B\u2212AX\u0303opt\u2016 2 \u03be = \u2016B\u2212UA\u03a3AVTA(WUA\u03a3AVTA)+WB\u2016 2 \u03be = \u2016B\u2212UA(WUA)+WB\u2016 2 \u03be ,\nwhere the last equality follows from properties of the pseudo-inverse and the fact that WUA is a full-rank matrix. Using B = (\nUAU T A +U\u22a5 A\n(\nU\u22a5 A\n)T )\nB, we get\n\u2016B\u2212AX\u0303opt\u2016 2 \u03be = \u2016B\u2212UA (WUA) +W\n(\nUAU T A +U \u22a5 A\n(\nU\u22a5A\n) T ) B\u2016 2\n\u03be\n= \u2016B\u2212UA (WUA)+WUAUTAB+UA (WUA)+WU\u22a5A ( U\u22a5A ) T B\u2016 2\n\u03be\n(a) = \u2016U\u22a5A(U\u22a5A)TB+UA(WUA)+WU\u22a5A(U\u22a5A)TB\u20162\u03be (b) = \u2016U\u22a5A(U\u22a5A)TB\u20162\u03be + \u2016UA(WUA)+WU\u22a5A(U\u22a5A)TB\u2016\u03be.\n(a) follows from the assumption that the rank of WUA is equal to k and thus (WUA) +WUA = Ik and (b) follows by matrix-Pythagoras (Lemma 15). To conclude, we use spectral submultiplicativity on the second term and the fact that U\u22a5 A ( U\u22a5 A )T B = AXopt \u2212B."}], "references": [{"title": "Numerical Methods for Least Squares Problems", "author": ["A. Bj\u00f6rck"], "venue": "Twice-ramanujan sparsifiers. In Proc. 41st Annual ACM", "citeRegEx": "Bj\u00f6rck.,? \\Q1996\\E", "shortCiteRegEx": "Bj\u00f6rck.", "year": 1996}], "referenceMentions": [], "year": 2017, "abstractText": "A rich coreset is a subset of the data which contains nearly all the essential information. We give deterministic, low order polynomial-time algorithms to construct rich coresets for simple and multiple response linear regression, together with lower bounds indicating that there is not much room for improvement upon our results.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}