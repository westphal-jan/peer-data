{"id": "1202.3706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "A Framework for Optimizing Paper Matching", "abstract": "before the table of many scientific conferences is prepared method of matching submitted bids together suitable reviewers. arriving at a critical assignment is particular major and important challenge this trainee conference organizer. in this paper they propose fit framework to achieve criteria - to - reviewer relationship. tailored framework prepares suitability scores to support pairwise affinity scoring papers and reviewers. we show quantitative learning possibilities be used to obtain suitability scores from a small subset of internal factors, simply pulling the burden on reviewers and organizers. \u2022 frame your comparative problem as determining integer program and propose compression procedures for the paper - to - reviewer matching domain. researches also explore differential learning and matching interact. experiments relating two conference data sets examine the performance following one editing procedures as well thereby yield predictions of three candidate formulations.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (385kb)", "http://arxiv.org/abs/1202.3706v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["laurent charlin", "richard s zemel", "craig boutilier"], "accepted": false, "id": "1202.3706"}, "pdf": {"name": "1202.3706.pdf", "metadata": {"source": "CRF", "title": "A Framework for Optimizing Paper Matching", "authors": ["Laurent Charlin", "Richard Zemel"], "emails": ["lcharlin@cs.toronto.edu", "zemel@cs.toronto.edu", "cebly@cs.toronto.edu"], "sections": [{"heading": null, "text": "At the heart of many scientific conferences is the problem of matching submitted papers to suitable reviewers. Arriving at a good assignment is a major and important challenge for any conference organizer. In this paper we propose a framework to optimize paper-to-reviewer assignments. Our framework uses suitability scores to measure pairwise affinity between papers and reviewers. We show how learning can be used to infer suitability scores from a small set of provided scores, thereby reducing the burden on reviewers and organizers. We frame the assignment problem as an integer program and propose several variations for the paper-to-reviewer matching domain. We also explore how learning and matching interact. Experiments on two conference data sets examine the performance of several learning methods as well as the effectiveness of the matching formulations."}, {"heading": "1 Introduction", "text": "The assignment of papers to reviewers is one of the most important tasks facing the organizers of scientific conferences. Assigning submitted papers to their most suitable reviewers is essential to the success of any conference, indeed to the functioning of many scientific fields, since it is reviewer assessments that determine the conference program and, to some extent, the shape of a discipline. However, this is not a simple task: large conferences often receive well over 1000 submissions that must be assigned to many hundreds of reviewers in short amount of time. Apart from ensuring the suitability of assigned reviewers, constraints imposed by reviewer load limits, conflicts of interest, and other factors push this assignment problem beyond the reach of a single program chair, and generally prevent the process from being distributed in a fully satisfactory way.\nMany large conferences in computer science (CS), and especially artificial intelligence (AI), allow reviewers to bid on papers, basically providing their \u201cpreferences\u201d\u2014which we interpret as reflecting a reviewer\u2019s suitability to review particular submitted papers\u2014after which a centralized matching process takes place to find the most suitable assignment. Preferences collected this way are, unfortunately, inherently noisy for two key reasons: (a) it is difficult for reviewers to offer reasonable assessments of all but a small fraction of the papers, given the numbers involved; and (b) reviewers have access to limited information about each paper (e.g., only title and abstract). The latter factor fundamentally limits how well a reviewer can judge her own suitability, while the former means reviewers are, in some sense, semi-randomly choosing papers on which they express interest.\nOne response to this problem is to associate simple features (keywords being most common) with both papers and reviewers, and use some measure of overlap as a reflection of suitability. Unfortunately, this simple method is crude at best, and relies on a common understanding of this (usually limited) vocabulary by all reviewers and authors. A more sophisticated response involves the use of machine learning techniques to help predict reviewer expertise [1, 16, 6]. By using specific features of both reviewers (e.g., previously written articles, co-authorship relations) and submitted papers (e.g., words or keywords), we can relieve reviewers of the burden of bidding. Ideally, a combination using this information as well as self-declared reviewer expertise (or bids) can be leveraged to predict reviewer suitability using collaborative filtering methods [10]. Ultimately, however, the primary goal is not to accurately predict expertise, but to find a good matching.\nIn this paper, we propose and test various instantiations of a flexible framework for optimizing paper matching. We investigate approaches that use incomplete information in the form of a limited number of suitability scores: our basic framework predicts missing scores using learning techniques and then finds optimal matchings using both observed and predicted scores. Within this framework, we\nexplore several learning models which leverage (one or both of) two sources of information\u2014reviewer/paper features and self-reported suitability\u2014to predict the unknown scores: these include regression, collaborative filtering and language modeling methods. We then describe several desirable properties for paper-to-reviewer assignments. We frame the assignment problem as an integer program [24], and explore several variations that reflect different desiderata, and how these interact with various learning methods. We test our framework on two data sets collected from a large AI conference, measuring predictive accuracy with respect to both reviewer suitability and matching performance, exploring several different matching objectives and how they can be traded off against one another.\nAlthough we focus on reviewer matching, our methods are applicable to any constrained matching domain where: (a) user preferences for a set of items can be predicted using user and/or item features; (b) preferences can be used to improve matching quality; (c) it is infeasible or undesirable for users to express preferences over all items; and (d) capacity or other constraints limit the min/max number of users-per-item (or vice versa). Examples include facility location, school/college admissions, certain forms of scheduling and time-tabling, and many others."}, {"heading": "2 Related Work", "text": "Deep bodies of related work exist for each of the two components that comprise our framework for reviewer matching: prediction of suitabilities or preferences for unobserved reviewer-paper (or user-item) pairs; and computing matchings given (known or predicted) suitabilities. Past work has either explored the score prediction problem or different approaches to matching but, to the best of our knowledge, ours is the first that examines suitability prediction relative to different matching objectives, and examines the interactions between learning and matching.\nThere has been significant research on the use of information retrieval and learning techniques to determine suitability of reviewers for papers. These include the use of latent semantic indexing [7] or term frequency, inverse document frequency (TF-IDF) methods [12, 2] that exploit the content of abstracts of papers authored by reviewers and those of submitted papers. Other have utilized co-authorship graphs, using the references of a submitted paper as a starting point to generate potential referees [18]. Balog et al. [1] used language models to determine the suitability of experts for various topics/tasks, and more recently topic models have been applied to the problem of modeling expertise based on authored documents [25], with Mimno and McCallum [16] applying their topic model to the assessment of reviewer suitability (we discuss this further below).\nWhile the models above predict suitability using contentbased features of papers and/or reviewers, other methods exploit elicited suitability scores from reviewers for a sub-\nset of papers to make predictions for other papers. This can be treated as a collaborative filtering (CF) problem. CF methods leverage known preference information for a subset of user-item to generate predictions for unobserved pairs. Recent CF techniques have performed extremely well in a variety of domains, especially where available content features are not especially predictive of preference (or suitability) [21, 22, 15]. Conry et al. [6] applied an ensemble CF approach, combining side information about the papers and reviewers with several CF predictors to estimate reviewer suitabilities, and then used a simple matching program to determine assignments based on these suitabilities. This work is closest to ours; however, it does not explore variants of the matching objective, nor interactions between learning and matching. While CF is typically framed in terms of preference prediction, recent extensions instead use CF for optimization w.r.t. a specific target task. Weimer et al. [26] use CF data for optimization in a ranking task, while Petterson et al. [17] frame ranking as finding the weights that lead to an optimal matching in a bipartite graph. Our work has a similar motivation, trying to optimize suitability predictions w.r.t. a matching objective.\nA second body of work focuses on the matching problem itself. Benferhat and Lang [3], Goldsmith and Sloan [11], and Garg et al. [9] discuss various optimization criteria, and some of the practices used by program chairs and existing conference management software. Taylor [24] shows how these criteria can be formulated as an integer program (IP). Tang et al. [23] propose several extensions to the IP. This work assumes reviewer suitability for each paper is known, and deals exclusively with specific matching criteria. There is a rich literature on more general matching problems in economics and theoretical CS. Examples include the well-known stable marriage problem [8]; resident matching (of residency candidates to hospitals) [19]; and (one-sided) matching in housing markets [13]. In economic models, a key focus is on stability of the matches and minimizing incentives for participants in the matching market to misreport their preferences. We do not consider such strategic issues here."}, {"heading": "3 Matching Framework and Instantiations", "text": "We begin by outlining our basic problem definition, then elaborate on several specific instantiations of the framework we develop. These include the use of various learning methods for predicting unknown suitabilities, a range of objectives and constraints on the matching process reflecting different desiderata for the reviewing process, and interactions between the two."}, {"heading": "3.1 Problem Definition", "text": "Our approach to the matching problem relies on suitability scores, which describe the relevance of a reviewer to a given paper. The matching procedure uses these scores to\nform a set of assignments of items to users. For reasons discussed above, the suitabilities will not be fully specified. Since we do not wish to limit the matching process to reviewer-paper pairs that are known (i.e., have been directly elicited), these need to be predicted in some fashion.\nWe formalize the matching problem as follows. Let r \u2208 R refer to users or reviewers, p \u2208 P to items or papers, and let |R| = N and |P| = M . Every user-item pair has a suitability score srp. The set of all scores can be viewed as a suitability matrix S \u2208 RN\u00d7M . Only a subset of the suitabilities are observed, namely, those collected from reviewers during an elicitation process. Denote this by So, and denote the observed scores for a particular reviewer r and paper p by Sor and S o p , respectively. S u, Sur , S u p are the analogous collections of unobserved scores.\nWe may have access to additional side information about individual reviewers and papers which may come in different forms. In our setting, side information about submitted papers could include author-specified keywords, citations, and word usage in the paper. For reviewers, we may have stated preferences for keywords, citations, or other descriptions of reviewer expertise. Our data sets also include an archive, containing a set of papers written by each reviewer, providing information about their expertise. This is represented as a word count vector wr summarizing r\u2019s own papers. Similarly, we summarize each submitted paper p as a word count vector wp.\nGiven this information, our goal is to find a \u201cgood\u201d matching of papers to reviewers in the presence of incomplete information about reviewer suitabilities, possibly exploiting the side information available. The problem can be broken into two main components: predicting unknown suitabilities using some combination of known scores and side information; and matching papers to reviewers based on known and predicted suitabilities. Notice that predicting suitability scores is, however, not a goal in and of itself: it is subservient to the primary goal of good matching performance. Many different factors may be used to define the quality of a matching, as we discuss below."}, {"heading": "3.2 Learning Methods", "text": "We have explored a range of learning methods for predicting suitabilities of reviewers for papers. Here we focus on three methods, each exploiting the different information available for prediction: a language model (LM); linear regression (LR); and Bayesian probabilistic matrix factorization (BPMF). LM uses the content of submitted papers and archived papers for prediction, but does not use reviewer bids; BPMF uses reported suitabilities/bids, but no document/archive side information; and LR uses bids and the content submissions, but not the archive.\nLanguage Model: Several previous approaches to reviewer matching have used simple language models to represent distributions over words of papers and reviewers\n[25, 16]. Our language model (LM) is based on these, and predicts suitabilities without using stated reviewer preferences; rather it builds a model in word (feature) space, assuming that distance in this space correlates with distance in suitability space. LM constructs a distribution over words for each reviewer, based on the archive of papers written by the reviewer wr (the reviewer side information). The starting point for LM is a multinomial Pr(w|d) over words w in a document d. The maximum likelihood estimate of Pr(w|d) is the number of occurrences of this word divided by the total number of words in the document (Prml(w|d) = |wd|/Td). Using Dirichlet smoothing to account for the fact that most words do not appear in a given document, this estimate can be written as:\nPr(w|d) = Td Td + \u00b5 Prml(w|d) + \u00b5Td+\u00b5 Pr(w) (1)\nwhere Pr(w) is the probability of the word across all documents and \u00b5 is the smoothing parameter. This distribution can be formed in various ways from the user side information (i.e., the collected papers of a reviewer). We adopt a variant of an approach [16] in which the word vectors of reviewer-authored papers are averaged to form a single document dr per reviewer. LM encodes each submitted paper as a word count vector wp, and predicts suitabilities srp to be log Pr(wp|dr) = \u2211 w\u2208wp Pr(w|dr). This language model has outperformed sophisticated topic models in some settings [16].\nRegression: Linear regression (LR) predicts suitabilities directly using the side information associated with the items. Each reviewer has a set of parameters \u03b8r, which is applied to item information wp to form an estimate of srp: s\u0302rp = \u03b8r \u00b7wp. Stated reviewer preferences are used as training observations, and LR minimizes the mean-squared error (MSE) w.r.t. observed suitabilities:\nCLR(S o) =\n1 |So| \u2211\nsrp\u2208So (s\u0302rp \u2212 srp)2 (2)\nCollaborative Filtering: Given observed suitabilities, prediction of unobserved suitabilities can be tackled using collaborative filtering. Probabilistic matrix factorization (PMF) [21] is a popular CF method that finds a lowrank factorization of the suitability matrix S \u2248 UTV , where S \u2208 RN\u00d7M , U \u2208 RK\u00d7N and V \u2208 RK\u00d7M and K << min(M,N). The columns Ur of U and Vp of V represent latent reviewer and paper factors. The full S matrix, including unobserved suitabilities, can be estimated by taking the product of U and V . Under this model, the conditional distribution over suitabilities is:\nPr(S|U, V, \u03c32) = M\u220f r N\u220f p N (srp|UTr Vp, \u03c32)Irp\nwhere I is an indicator matrix and entry Irp is 1 if it was observed and 0 otherwise. Assuming zero-mean Gaussian priors over the parameters U and V : Pr(U |\u03c32U ) = N\u220f r=1 N (Ur|0, \u03c32U ); Pr(V |\u03c32V ) = M\u220f p=1 N (Vp|0, \u03c32V ),\nfinding the MAP solution involves minimizing MSE between UTV and the known suitabilities.\nIn a Bayesian version of PMF (BPMF), the parameters U and V have non-zero mean and full-covariance priors [22]. The predictive distribution cannot be calculated analytically because the posteriors over U and V are intractable, but a Markov Chain Monte Carlo sampler can be used to approximate sufficient statistics. Integrating over the parameters has been shown to produce performance advantages w.r.t. root mean squared error (RMSE) on the Netflix task [22].\nOther Methods: In addition to the three methods above, we investigated several other algorithms, including supervised and unsupervised topic models [4, 5], conditional restricted Boltzmann machines (RBMs) [14], and inference using co-reference graphs. Preliminary experiments showed that these methods did not match the performance of those above. We also tried leveraging the different sources of information by using combinations of the various learning models without success. Incorporating each reviewer\u2019s archive as extra training papers for LR did not offer any improvement either. Finally, we explored a formulation of the problem in which the training objective explicitly optimizes matchings J(x) rather than optimizing RMSE w.r.t. predicted suitabilities. However, experiments with this approach failed to demonstrate improved matching performance."}, {"heading": "3.3 Matching Objectives", "text": "We articulate several different criteria that may influence the definition of a \u201cgood\u201d matching and explore different formulations of the optimization problem that can be used to accommodate these criteria. We also discuss how these criteria may interact with our learning methods.\nNaturally, one would like to assign submitted papers to their most suitable reviewers; of course, this is almost never possible since some reviewers will be most suited to far more papers than others. In general, load balancing is enforced by placing an upper limit or maximum on the number of papers per reviewer. Similarly, we may impose a minimum to ensure reasonable load equity or load fairness across reviewers. However, limiting the paper load increases the probability that certain papers will be assigned to very unsuitable reviewers. This suggests only making assignments involving pairs with score srp above some minimum score threshold. This ensures that every paper is reviewed by a minimally suitable reviewer, but may sacrifice load equity (indeed, it may sacrifice feasibility). One may also desire suitability fairness across reviewers; that is, reviewers should have similar score distributions over their assigned papers (so on average no reviewer is assigned papers to which she is significantly more ill-suited than any other). Finally, when multiple reviewers are assigned to papers, it may be desirable to assign complementary review-\ners to a paper so as to cover the range of topics spanned by a submission. Related is the desire to ensure each paper is reviewed by at least one \u201csenior\u201d reviewer with significant expertise.\nThe intricacies of different conferences prevent us from establishing an exhaustive list of matching desiderata (see [3, 11, 9] for further discussion). We now explore matching mechanisms that will account for several of these criteria: we frame the matching procedure as an optimization problem and show how several properties can be formulated as constraints or modifications of the objective function. We formulate the basic matching problem as an IP, where each paper is assigned to its best-suited reviewers [24]:\nmaximize Jbasic(x) = \u2211 r \u2211 p srpxrp (3) subject to xrp \u2208 {0, 1}, \u2200r, p (4)\u2211 r xrp = Rtarget , \u2200p\nThe binary variable xrp encodes the matching of item p to user r; a match is an instantiation of these variables. Rtarget is the desired number of reviewers per paper. Minimum and maximum reviewer load, Pmin and Pmax respectively, can be incorporated as constraints [24]:\u2211\np xrp \u2265 Pmin, \u2211 p xrp \u2264 Pmax, \u2200r. (5)\nThis IP, including constraints (5), is our basic formulation (Basic IP). Its solution, the optimal match, maximizes total reviewer suitability given the constraints. Although IPs can be computationally difficult, our constraint matrix is totally unimodular, so the linear program (LP) relaxation (allowing xrp \u2208 [0, 1]) does not affect the integrality of the optimal solution; hence the problem can be solved as an LP.\nAlthough not mentioned above it is essential for the matching to prevent assignments of reviewers to submitted papers for which they have conflicts of interest (COI). The above formulation can easily enforce known COI by directly constraining the conflicting assignments xrp\u2019s to be 0, alternatively, we can set the relevant scores srp\u2019s to \u2212\u221e. To capture additional matching desirata, we can modify the objective or the constraints of this IP. Load balancing can be controlled by manipulating Pmin and Pmax: a small range ensures each reviewer is assigned the same number of papers at the expense of match quality, while a larger range does the converse. We can instead enforce load equity by making the tradeoff explicit in the objective with \u201csoft constraints\u201d on load:\nJbalance(x) = \u2211 r \u2211 p srpxrp + \u2211 r \u03bbf ((\u2211 p xrp ) \u2212 x\u0304 ) (6)\nwhere x\u0304 is the average number of papers per reviewer (M/N ) and f is a penalty function (e.g., f(x) = |x| or f(x) = x2). The parameter \u03bb controls the tradeoff between load equity and match quality. The Jbalance objective (Eq. 6) along with the constraints expressed in Eq. 4 comprise our Balance IP.\nThe Jbasic objective (Eq. 3) maximizes the overall suitability of the assignments, equating \u201cutility\u201d with suitability. However, the utility of a specific match xrp may not be linear in suitability srp. For example, utility may be more \u201cbinary\u201d: as long as a paper is assigned to a reviewer whose suitability is above a certain threshold, then the assignment is good, otherwise it is not. This can be realized by applying some non-linear transformation g to the scores in the matching objective (e.g., a matched pair with score srp \u2208 {2, 3} may be greatly preferred to srp \u2208 {0, 1}):\nJ tfm(x) = \u2211 r \u2211 p g(srp)xrp. (7)\nIn this transformed objective J tfm , if g is a logistic function then score are softly \u201cbinarized.\u201d Finally, we note that some of these matching objectives can also be incorporated into the suitability prediction model. For example, the nonlinear transformation g can be directly used in the LR training objective (cf. Eq. 2):\nCLR-TFM(S o) =\n1 |So| \u2211\nsrp\u2208So (s\u0302rp \u2212 g(srp))2. (8)"}, {"heading": "4 Experimental Results", "text": "We start by describing the data sets used in our experiments. The rest of the section is divided into three parts. The first considers score predictions with the different learning models. The second turns to matching quality and explores the soft constraints on the number of papers matched per reviewer. Finally, the third part evaluates a transformation of the matching objective and shows how using a transformed learning objective can enhance performance on the transformed matching problem."}, {"heading": "4.1 Data", "text": "Experiments are run using two data sets, N10 and N09, from the 2010 and 2009 editions, respectively, of the NIPS conference, one of the leading conferences in machine learning.1 For both data sets, side information for each reviewer comprises a self-selected set of papers taken representative of her areas of expertise; these were summarized as word count vectors wr. Side information about submitted papers consisted of document word counts wp for each p. The total vocabulary used by submissions (across both sets) contained over 21,000 words; We used only the top 1000 words for our experiments as ranked using TF-IDF (|wp| = |wr| = 1000). Reviewer suitability scores ranged from 0 to 3, with 0 meaning \u201cpaper lies outside my expertise;\u201d 1 means \u201ccan review if necessary;\u201d 2 means \u201cqualified to review;\u201d and 3 means \u201cvery qualified to review.\u201d As discussed above, these scores are intended to reflect reviewer expertise, not desire. We focus on the area chair\n1See http://nips.cc. We are currently investigating mechanisms by which we can make both data sets available to the community.\n(or meta-reviewer) assignment problem, where the matching task is to assign a single area chair to each paper. We use the term reviewer below to refer to such area chairs.\nN10 comprises 1250 submitted papers to be assigned to 48 reviewers. Suitabilities on a subset of papers were elicited from reviewers using a rather involved two-stage process. This process utilized the language model (LM) to estimate the suitability of each reviewer for each paper, and then queried each reviewer on the papers on which his estimated suitability was maximal. The output of the first round was fine-tuned using a combination of a hybrid discriminative/generative RBM [14] with replicated softmax input units [20] trained on the initial scores, and LM, which then determined the second round of queries. In total, each reviewer provided score on an average of 143 queried papers (excluding one extreme outlier), and each paper received an average of 3.3 suitability assessments (with a std. dev. of 1.3). The mean suitability score was 1.1376 (std. dev. 1.1); a histogram of the scores is shown in Figure 1(a). Note that since the querying process was biased towards asking about pairs with high predicted suitability, the unobserved scores are not missing at random, but rather tended toward pairs with low suitability. We do not distinguish the data acquired in the two phases of elicitation; both took place within a short time frame, so we assume suitabilities for any one reviewer are stable.\nN09 comprises 1079 submitted papers and 30 area chairs. Reviewer scores were not elicited, but instead provided by the conference program chairs for every reviewersubmission pair. A histogram of the scores is shown in Figure 1(b). The mean suitability score was 0.19 (std. dev. 0.57)."}, {"heading": "4.2 Score Predictions", "text": "We first analyze performance using the root mean squared error (RMSE) metric, as is common in collaborative filtering research. We are especially interested in how the different approaches behave as the size of the training set increases. An understanding of this dynamic is vital if one is to strike a balance between the demands of eliciting suitability scores from the user and increased accuracy of predictions.\nFor learning, we are given a set of training instances, Str \u2261 So. We split this set into a training and validation set. The trained model predicts all unobserved scores Su.\nSince we do not have true suitability values for all unobserved scores, we distinguish Su as being the union of test instances Ste (for which we have scores in the data set), and missing instances Sm. We denote a model\u2019s estimates of the test instances as S\u0302te, and evaluate RMSE over these test instances ( \u2211 rp\u2208Ste(s\u0302 te rp \u2212 sterp)/|Ste|)1/2.\nWe report results averaged over 5 different splits of the data in all experiments. In each split, the data is divided into training, validation and test sets in 60/20/20 proportions. There is no overlap in the test sets across the 5 splits. When reporting results across splits, we report the mean and standard error. Training LR is naturally slightly faster than training BPMF 2, for which we used 330 MCMC samples including 30 burn-in samples, but both methods are trained within minutes on both of our data sets.\nFig. 2 shows results for training sets of different sizes, simulating the effect of additional elicitation. To facilitate comparison, the test set size is fixed across different training set sizes. We compare LR and BPMF to a baseline which predicts the mean training score. Recall that BPMF learns to predict Su by using So only while LR also utilizes paper features wp. The strong performance of LR suggests that the information contained in the paper features is extremely useful in predicting user preferences. Interestingly, BPMF, a state-of-the-art method in CF, performs worse than LR in all but the extremely small trainingset sizes. Recall that BPMF attempts to exploit similarities across reviewers and across papers. In this case, the (meta-)reviewers were specifically chosen by the conference organizers to span the field, providing expertise across the multitude of topics typically represented at this conference. We conjecture that, compared to other popular domains for CF (e.g., movie recommendation), there are far fewer commonalities across users (w.r.t. paper topics); thus it is difficult for BPMF to attain very good performance. Furthermore, although there are probably significant commonalities between papers, each paper receives an average of fewer than four ratings, which makes it difficult to discover those commonalities from the preference data alone.\nNote that the language model is not included here. LM\u2019s outputs values represent log-probabilities and thus do not fall into the [0, 3] score range. A linear mapping of the predicted suitabilities into the score range did not produce sensible results.\nThe behavior of the different learning methods on N09 are similar to that observed for N10. Since they provide no additional insight, we do not report results here."}, {"heading": "4.3 Match Quality", "text": "We now turn our attention to the matching framework. We first elaborate on how we perform the matching. We then\n2We use an implementation of BPMF provided by its authors: http://www.mit.edu/~rsalakhu/BPMF.html\nevaluate the performance of the different learning methods on the matching objective. Finally we introduce soft constraints into the matching objective and analyze the tradeoffs they introduce.\nMatching Experimental Procedures: The matching IPs discussed above assume access to fully known (or predicted) suitability scores. Since we learn estimates of the unknown scores, we denote a model\u2019s estimates of the test instances as S\u0302te, and impute a value for all suitability values that are missing, using a constant imputation of \u03c4 \u2208 R. Since missing scores are likely to reflect, on average, lower suitability than their observed counterparts, we use \u03c4 = 1 in all experiments (recall that N10\u2019s mean was 1.1376 and N09 has no missing scores).\nGiven the estimate S\u0302te computed by one of our learning methods, we perform a matching with S = Str \u222a S\u0302te \u222a (Sm = \u03c4). Note that this permits missing values to be matched, which is important in the regime of sparse knownsuitability scores. Table 1 summarizes this procedure. For data set N10 we set Pmin and Pmax to 20 and 30, respectively, while the range is 30\u201340 for data set N09.\nBaseline: We adopt a baseline method that provides an absolute comparison across methods. The baseline has access to Str and imputes \u03c4 for any element of Ste. To allow meaningful comparison to other methods, it employs the same imputation for missing scores, Sm = \u03c4 .\nA note on LM: Although the output of LM can be directly used for matching, it does not exploit observed suitabilities in its usual formulation. However LM can make use of some of the training data Str by incorporating submitted papers assessed as \u201csuitable\u201d by some reviewer r into her word vector wr. Specifically, we include all papers in wr for which r offered a score of 3 (only if this score is in Str).\nFor all methods, once an optimal match x\u2217 is found, we\nevaluate it using all observed and unobserved scores, with the same constant imputation for the missing scores, where match quality is measured using Jbasic (see Equation 3):\u2211\nr \u2211 p x\u2217rp(S tr \u222a Ste \u222a Sm = 1) (9)\nMatching Performance using Basic IP: We now report on the quality of the matchings that result from using the predictions of the different methods. Again we consider dynamic matching performance as the amount of training data per user increases. Note that the optimal match value is 3053 for N10 and 2172 for N09, which occurs when S\u0302te = Ste.\nFigure 3 shows how matching quality varies as the amount of training data per user increases. Since training scores are also observed at matching time (Eq. 9), all methods benefit from having a larger training set. Figure 3 leads to the following three observations. Firstly, when no observed data is available, i.e., using only the archive, LM does very well, with a matching score of 2247 \u00b1 32, nearly identical to the quality of LR and BPMF with 10 bids per user, and much better than the match quality of 1262 obtained using constant scores ((Ste \u222a Sm) = \u03c4). Secondly, when very few scores are available, LR and LM perform best (and do equally well). As mentioned above, LM is able to exploit observed suitabilities by adding relevant papers to the user corpus, but this attenuates the impact of elicited scores: we see LM is outperformed by all other methods when sufficient data is available. Thirdly, LR outperforms all other methods as data is added. We also see that as the number of observed scores increases, unsurprisingly, the gain in matching performance (value of information) from additional scores decreases.\nIt is also interesting to note that a total matching score of over 2500 implies that, on average, each reviewer is assigned papers on which her average preference is greater than 2 (out of 3). LR reaches this level of performance with less than 30 observed scores per user, while other methods\nneed 30% more data per user to reach the same level of performance.\nFurther insight into matching quality on N10 induced by the different learning methods can be gained by examining the distribution of scores associated with matched papers (Figure 4) or under different sizes of the training set (Figure 5). Figure 4 displays the number of scores of each value (0\u20133) that get assigned with a training set size of 40. Not surprisingly, LR and BPMF assign significantly more 2s and 3s combined than all other methods. LM is very good at picking the top scores which reinforces the fact that word-level features, from reviewer and submitted papers, contain useful information for matching reviewers. Similar results were obtained on N09 and thus LM\u2019s performance is not only a consequence of the data collection method used for N10. In addition, Baseline assigns few zeros, since all missing and test scores are imputed to be \u03c4 = 1.\nFigure 5 provides another perspective on assignment quality. Here we plot results for the best performing method, LR, on both N10 and N09, for 3 different training set sizes. We first note that the extreme imbalance in N09 leads LR to assign many zeros even with 80 training scores per user. Overall, both data sets show that as the number of training scores increases, more 2s and 3s, and fewer 0s and 1s, are assigned.\nOur remaining results deal exclusively with N10 since experimental results with N09 were similar.\nLoad Balancing Balance IP: The experiments above all constrain the number of papers per reviewer to be within a specific range (Pmin\u2013Pmax). There is no good indication as to how to set these two extrema. Instead we now use the Balance IP, both for matching and evaluation (see Eq. 9), setting f to be the absolute value function.\nFigure 6 shows the histogram of assigned papers per re-\nviewer given by the optimal solution to the IP for different \u03bb \u2208 {0, 0.1, 1}. When \u03bb = 0 load equity is ignored, and almost all reviewers either get assigned the minimum (Rmin) or the maximum (Rmax) number of papers; withinreviewer variance ( \u2211 p(xrp \u2212 x\u0304)2/M ) is extremely high. When a \u201csoft constraint\u201d on load equity is introduced, assignments become more balanced as the \u03bb increases (i.e., the balance constraint becomes \u201charder\u201d). The following table reports the matching objective versus the variance, averaged across users, for different values of \u03bbwith a training set size of 40 (other training sizes yielded similar results):\n\u03bb 0 0.1 0.25 0.5 0.75 1 Jbasic 2625 2615 2600 2573 2569 2569\nVariance 4.62 3.28 2.61 0.89 0.37 0.33\nNot surprisingly, larger penalties \u03bb for deviating from the mean reviewer load give rise to greater load balance (lower load variance) and worse matching performance. Generally, an appropriate \u03bb will be chosen by the conference organizers, that nicely trades off performance versus load balance across reviewers (here, perhaps around \u03bb = 0.5)."}, {"heading": "4.4 Transformed Matching and Learning", "text": "We now consider a non-linear transformation of the scores, reflecting the view that it is much better to assign reviewerpaper pairs with suitabilities of 2 and 3, than pairs with 0 and 1; as discussed above this can be accomplished by allowing \u201cutility\u201d xrp to be non-linear in suitability score srp. We adopt the following sigmoid function to effect this nonlinear transformation: \u03c3(s) = 1/(1 + exp(\u2212(s\u2212 1.5)\u03b2)); here 1.5 is the middle of the scores\u2019 range. We set \u03b2 = 4.5, which gives: \u03c3(0) = 0.001; \u03c3(1) = 0.095; \u03c3(2) = 0.90; \u03c3(3) = 1.0. We first show how this transformation impacts matching performance without learning; then we discuss how one can incorporate the transformation into the learning objective itself.\nWe first test how matching using the transformed objectives affects results without using learning to infer missing scores (consequently, Su = \u03c4 ), by examining difference in matching performance when varying the percentage of\nobserved scores. Figure 7(a) shows the difference when matching with the transformed objective (J tfm ) versus the basic objective (Jbasic). In both cases the resulting matches are evaluated using J tfm . Although a minor gain is observed when most of the known data is observed, there is, overall, very little difference in performance when matching with either objective. Recall that the mean number of scores per paper is less than 4. Hence, when matching using a small fraction of the data, the matching procedure has very little flexibility to assign high scoring pairs unless learning is used to predict unobserved scores.\nWe can modify the learning objective to take into account the nonlinearity introduced in the matching objective. We do this by transforming all labels using the same sigmoidal transformation as in the matching objective (Eq.8). This allows learning to better predict the transformed scores by explicitly training on them Figure 7(b) shows the transformed matching performance of both LR on the nontransformed data, and LR-TFM, a linear regression model trained using the transformed learning objective. Not surprisingly, LR-TFM outperforms LR across all training set sizes, since it is trained for the modified objective J tfm . The difference is especially pronounced with smaller training sets\u2014when enough data is available, both methods will naturally assign many 2s and 3s. (We also verified that LRTFM outperforms BPMF trained on the transformed objective)."}, {"heading": "5 Conclusion", "text": "We have developed a framework for paper-to-reviewer assignment in the context of scientific conferences. We showed how by eliciting only a small subset of scores from reviewers and inferring unobserved scores, using one of several learning methods, we are able to determine high quality matchings. Interestingly, in the field of collaborative filtering, side-information is often perceived to be useful only in the cold-start condition, where few or no scores are available. The performance of both LM and LR, which leverage word-level features from reviewers and submitted papers, show that this is not the case in our domain. We also explored the trade-off between matching quality and paper load balancing, which helps one avoid the need to manually set limits on the reviewer load. Finally we showed that using the realistic assumption that utility is non-linear in suitability score, we discover better matches using the same nonlinear transformation in the learning objective.\nGiven how matching benefits from an interaction with learning, we are developing ways to strengthen this interaction by making the learning methods sensitive to the final matching objective. We have obtained good results using this approach in an active learning setting where the system chooses which reviewer scores to query. We are also interested in exploring how asking meta-queries, about general aspects of papers rather than a single paper, may be\nexploited to reduce the number of reviewer queries while maintaining strong matching performance.\nAcknowledgements: Special thanks to the NIPS Foundation and to the NIPS 2009 program chairs, Chris Williams and John Lafferty for their assistance in collecting the N09 data set. Thanks to the reviewers for helpful suggestions. This research was supported by NSERC and CIFAR."}], "references": [{"title": "Formal models for expert finding in enterprise corpora", "author": ["Krisztian Balog", "Leif Azzopardi", "Maarten de Rijke"], "venue": "Proceedings of the 29th Annual International ACM SIGIR Conference on Research and development in Information Retrieval", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Recommending papers by mining the web. In Working Notes of the Workshop on Machine Learning for Information Filtering in the 16th International Joint Artificial Intelligence", "author": ["C. Basu", "H. Hirsh", "W. Cohen", "C. Nevill-Manning"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Latent Dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan", "John Lafferty"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Recommender systems for the conference paper assignment problem", "author": ["Don Conry", "Yehuda Koren", "Naren Ramakrishnan"], "venue": "In Proceedings of the Third ACM Conference on Recommender Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Automating the assignment of submitted manuscripts to reviewers", "author": ["Susan T. Dumais", "Jakob Nielsen"], "venue": "In Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "College admissions and the stability of marriage", "author": ["David Gale", "Lloyd S. Shapley"], "venue": "American Mathematical Monthly,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1962}, {"title": "Using collaborative filtering to weave an information tapestry", "author": ["David Goldberg", "David Nichols", "Brian M. Oki", "Douglas Terry"], "venue": "Communications of the ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1992}, {"title": "The AI conference paper assignment problem", "author": ["Judy Goldsmith", "Robert H. Sloan"], "venue": "In 22nd National Conference on Artificial Intelligence (AAAI-07) Workshop on Preference Handling in AI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Mining for proposal reviewers: lessons learned at the National Science Foundation", "author": ["Seth Hettich", "Michael J. Pazzani"], "venue": "In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-06),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "The efficient allocation of individuals to positions", "author": ["Aanund Hylland", "Richard J. Zeckhauser"], "venue": "Journal of Political Economy,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1979}, {"title": "Classification using discriminative restricted Boltzmann machines", "author": ["Hugo Larochelle", "Yoshua Bengio"], "venue": "In Proceedings of the 25th International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Non-linear matrix factorization with Gaussian processes", "author": ["Neil D. Lawrence", "Raquel Urtasun"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Expertise modeling for matching papers with reviewers", "author": ["David M. Mimno", "Andrew McCallum"], "venue": "Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "An algorithm to determine peer-reviewers", "author": ["Marko A. Rodriguez", "Johan Bollen"], "venue": "In Proceeding of the 17th ACM Conference on Information and Knowledge Management (CIKM-08),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "The evolution of the labor market for medical interns and residents: A case study in game theory", "author": ["Alvin E. Roth"], "venue": "Journal of Political Economy,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1984}, {"title": "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo", "author": ["Ruslan Salakhutdinov", "Andriy Mnih"], "venue": "In Proceedings of the 25th International Conference on Machine Learning (ICML),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Expertise matching via constraint-based optimization", "author": ["Wenbin Tang", "Jie Tang", "Chenhao Tan"], "venue": "In IEEE/WIC/ACM International Conference on Web Intelligence (WI-10) and Intelligent Agent Technology (IAT-10),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "On the optimal assignment of conference papers to reviewers", "author": ["Camillo J. Taylor"], "venue": "Technical Report MS-CIS-08-30, University of Pennsylvania,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "LDA-based document models for ad-hoc retrieval", "author": ["Xing Wei", "W. Bruce Croft"], "venue": "In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "A more sophisticated response involves the use of machine learning techniques to help predict reviewer expertise [1, 16, 6].", "startOffset": 113, "endOffset": 123}, {"referenceID": 12, "context": "A more sophisticated response involves the use of machine learning techniques to help predict reviewer expertise [1, 16, 6].", "startOffset": 113, "endOffset": 123}, {"referenceID": 3, "context": "A more sophisticated response involves the use of machine learning techniques to help predict reviewer expertise [1, 16, 6].", "startOffset": 113, "endOffset": 123}, {"referenceID": 6, "context": "Ideally, a combination using this information as well as self-declared reviewer expertise (or bids) can be leveraged to predict reviewer suitability using collaborative filtering methods [10].", "startOffset": 187, "endOffset": 191}, {"referenceID": 17, "context": "We frame the assignment problem as an integer program [24], and explore several variations that reflect different desiderata, and how these interact with various learning methods.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "These include the use of latent semantic indexing [7] or term frequency, inverse document frequency (TF-IDF) methods [12, 2] that exploit the content of abstracts of papers authored by reviewers and those of submitted papers.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "These include the use of latent semantic indexing [7] or term frequency, inverse document frequency (TF-IDF) methods [12, 2] that exploit the content of abstracts of papers authored by reviewers and those of submitted papers.", "startOffset": 117, "endOffset": 124}, {"referenceID": 1, "context": "These include the use of latent semantic indexing [7] or term frequency, inverse document frequency (TF-IDF) methods [12, 2] that exploit the content of abstracts of papers authored by reviewers and those of submitted papers.", "startOffset": 117, "endOffset": 124}, {"referenceID": 13, "context": "Other have utilized co-authorship graphs, using the references of a submitted paper as a starting point to generate potential referees [18].", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "[1] used language models to determine the suitability of experts for various topics/tasks, and more recently topic models have been applied to the problem of modeling expertise based on authored documents [25], with Mimno and McCallum [16] applying their topic model to the assessment of reviewer suitability (we discuss this further below).", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[1] used language models to determine the suitability of experts for various topics/tasks, and more recently topic models have been applied to the problem of modeling expertise based on authored documents [25], with Mimno and McCallum [16] applying their topic model to the assessment of reviewer suitability (we discuss this further below).", "startOffset": 205, "endOffset": 209}, {"referenceID": 12, "context": "[1] used language models to determine the suitability of experts for various topics/tasks, and more recently topic models have been applied to the problem of modeling expertise based on authored documents [25], with Mimno and McCallum [16] applying their topic model to the assessment of reviewer suitability (we discuss this further below).", "startOffset": 235, "endOffset": 239}, {"referenceID": 15, "context": "Recent CF techniques have performed extremely well in a variety of domains, especially where available content features are not especially predictive of preference (or suitability) [21, 22, 15].", "startOffset": 181, "endOffset": 193}, {"referenceID": 11, "context": "Recent CF techniques have performed extremely well in a variety of domains, especially where available content features are not especially predictive of preference (or suitability) [21, 22, 15].", "startOffset": 181, "endOffset": 193}, {"referenceID": 3, "context": "[6] applied an ensemble CF approach, combining side information about the papers and reviewers with several CF predictors to estimate reviewer suitabilities, and then used a simple matching program to determine assignments based on these suitabilities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Benferhat and Lang [3], Goldsmith and Sloan [11], and Garg et al.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "Taylor [24] shows how these criteria can be formulated as an integer program (IP).", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "[23] propose several extensions to the IP.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Examples include the well-known stable marriage problem [8]; resident matching (of residency candidates to hospitals) [19]; and (one-sided) matching in housing markets [13].", "startOffset": 56, "endOffset": 59}, {"referenceID": 14, "context": "Examples include the well-known stable marriage problem [8]; resident matching (of residency candidates to hospitals) [19]; and (one-sided) matching in housing markets [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 9, "context": "Examples include the well-known stable marriage problem [8]; resident matching (of residency candidates to hospitals) [19]; and (one-sided) matching in housing markets [13].", "startOffset": 168, "endOffset": 172}, {"referenceID": 18, "context": "Language Model: Several previous approaches to reviewer matching have used simple language models to represent distributions over words of papers and reviewers [25, 16].", "startOffset": 160, "endOffset": 168}, {"referenceID": 12, "context": "Language Model: Several previous approaches to reviewer matching have used simple language models to represent distributions over words of papers and reviewers [25, 16].", "startOffset": 160, "endOffset": 168}, {"referenceID": 12, "context": "We adopt a variant of an approach [16] in which the word vectors of reviewer-authored papers are averaged to form a single document dr per reviewer.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "This language model has outperformed sophisticated topic models in some settings [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "In a Bayesian version of PMF (BPMF), the parameters U and V have non-zero mean and full-covariance priors [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "root mean squared error (RMSE) on the Netflix task [22].", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "Other Methods: In addition to the three methods above, we investigated several other algorithms, including supervised and unsupervised topic models [4, 5], conditional restricted Boltzmann machines (RBMs) [14], and inference using co-reference graphs.", "startOffset": 148, "endOffset": 154}, {"referenceID": 10, "context": "Other Methods: In addition to the three methods above, we investigated several other algorithms, including supervised and unsupervised topic models [4, 5], conditional restricted Boltzmann machines (RBMs) [14], and inference using co-reference graphs.", "startOffset": 205, "endOffset": 209}, {"referenceID": 7, "context": "The intricacies of different conferences prevent us from establishing an exhaustive list of matching desiderata (see [3, 11, 9] for further discussion).", "startOffset": 117, "endOffset": 127}, {"referenceID": 17, "context": "We formulate the basic matching problem as an IP, where each paper is assigned to its best-suited reviewers [24]:", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "Minimum and maximum reviewer load, Pmin and Pmax respectively, can be incorporated as constraints [24]: \u2211", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "Although IPs can be computationally difficult, our constraint matrix is totally unimodular, so the linear program (LP) relaxation (allowing xrp \u2208 [0, 1]) does not affect the integrality of the optimal solution; hence the problem can be solved as an LP.", "startOffset": 146, "endOffset": 152}, {"referenceID": 10, "context": "The output of the first round was fine-tuned using a combination of a hybrid discriminative/generative RBM [14] with replicated softmax input units [20] trained on the initial scores, and LM, which then determined the second round of queries.", "startOffset": 107, "endOffset": 111}], "year": 2011, "abstractText": "At the heart of many scientific conferences is the problem of matching submitted papers to suitable reviewers. Arriving at a good assignment is a major and important challenge for any conference organizer. In this paper we propose a framework to optimize paper-to-reviewer assignments. Our framework uses suitability scores to measure pairwise affinity between papers and reviewers. We show how learning can be used to infer suitability scores from a small set of provided scores, thereby reducing the burden on reviewers and organizers. We frame the assignment problem as an integer program and propose several variations for the paper-to-reviewer matching domain. We also explore how learning and matching interact. Experiments on two conference data sets examine the performance of several learning methods as well as the effectiveness of the matching formulations.", "creator": "LaTeX with hyperref package"}}}