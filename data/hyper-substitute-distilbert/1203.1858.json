{"id": "1203.1858", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2012", "title": "Distributional Measures of Semantic Distance: A Survey", "abstract": "the ability actually mimic human notions of kinship similarity has widespread applications. some measures rely only on attribute sequences ( distributional measures ) and some prefer on knowledge sources such digital wordnet. although extensive scholars have been initiated to link wordnet - based findings with human judgment, morphological indicators representing human functions as proxies with distinguish semantic diversity recently reached little effectiveness. even though they have traditionally improved poorly when compared to group - based measures, they successfully claim - some uniquely attractive features, useful as their applicability : field - mapped languages and diagnostic ability help mimic compare semantic similarity and semantic consistency. moreover, it paper presents a detailed study of distributional measures. particular individuals specifically paid to flesh out the strengths associated limitations of distinguishing sentence - based and distributional criteria, and how distributional calculations by distance can operate brought more tightly line through human norms of causal distance. we discuss via his brief discussion upon recent work establishing hybrid sampling.", "histories": [["v1", "Thu, 8 Mar 2012 17:29:33 GMT  (69kb)", "http://arxiv.org/abs/1203.1858v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["saif m mohammad", "graeme hirst"], "accepted": false, "id": "1203.1858"}, "pdf": {"name": "1203.1858.pdf", "metadata": {"source": "CRF", "title": "Distributional Measures of Semantic Distance: A Survey", "authors": ["Saif Mohammad", "Graeme Hirst"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n20 3.\n18 58\nv1 [\ncs .C\nL ]\n8 M\nar 2\n01 2\nThis is an unpublished manuscript, first made available on the Web in 2006. It is a much-revised version of an earlier manuscript entitled \u201cDistributional Measures as Proxies for Semantic Relatedness\u201d, which was first made available on the Web in 2005.\nDistributional Measures of Semantic\nDistance: A Survey\nSaif Mohammad University of Toronto\nGraeme Hirst University of Toronto\nThe ability to mimic human notions of semantic distance has widespread applications. Some\nmeasures rely only on raw text (distributional measures) and some rely on knowledge sources\nsuch as WordNet. Although extensive studies have been performed to compare WordNet-based\nmeasures with human judgment, the use of distributional measures as proxies to estimate\nsemantic distance has received little attention. Even though they have traditionally performed\npoorly when compared to WordNet-based measures, they lay claim to certain uniquely attractive\nfeatures, such as their applicability in resource-poor languages and their ability to mimic both\nsemantic similarity and semantic relatedness. Therefore, this paper presents a detailed study of\ndistributional measures. Particular attention is paid to flesh out the strengths and limitations of\nboth WordNet-based and distributional measures, and how distributional measures of distance\ncan be brought more in line with human notions of semantic distance. We conclude with a brief\ndiscussion of recent work on hybrid measures."}, {"heading": "1. Introduction", "text": "Semantic distance is a measure of how close or distant the meanings of two units of language are. The units of language may be words, phrases, sentences, paragraphs, or documents. The nouns dance and choreography, for example, are closer in meaning than the nouns clown and bridge, and so are said to be semantically closer. The semantic distances betweenwords (ormore precisely, between concepts) can be used as fundamental building blocks for measuring semantic distance between larger units of language. The ability to mimic human judgments of semantic distance is useful in numerous natural language tasks including machine translation, word sense disambiguation, thesaurus creation, information retrieval, text summarization, and identifying discourse structure. This paper describes the state-of-the-art in corpus-based measures of semantic distance between these fundamental units of language. It identifies the significant challenges that existing approaches to semantic distance face and in the process fleshes out questions that lead to a better understanding of why two concepts are considered semantically close. The paper concludes with a discussion of new hybrid approaches, that show the potential to address these challenges.\nUnits of language, especially words, may have more than one possible meaning. However, their context may be used to determine the intended senses. For example, star can mean both CELESTIAL BODY and CELEBRITY; however, star in\u2014Stars are powered by nuclear fusion\u2014refers only to CELESTIAL BODY and is much closer to sun than to famous. Thus, semantic distance between words in context is in fact the distance between their underlying senses or lexical concepts. Therefore, in this paper, we take word senses to be a particular kind of concept. When we refer directly to a concept (written in small capitals), it is with the understanding that it is the sense of one or more words,\n\u00a9 2006 Saif Mohammad and Graeme Hirst.\nas reflected in the name that we give the concept. We do not, in this paper, consider concepts that are unlexicalized.\nHumans consider two concepts to be semantically close if there is a sharing of some meaning. Specifically, two lexical concepts are semantically close if there is a lexical semantic relation between the concepts. Putting it differently, the reason why two concepts are considered semantically close can be attributed to a lexical semantic relation that binds them. According to Cruse (1986), a lexical semantic relation is a relation between lexical units\u2014a surface form along with a sense. As he points out, the number of semantic relations that bind concepts is innumerable; but certain relations, such as hyponymy, meronymy, antonymy, and troponymy, are more systematic and have enjoyed more attention in the linguistics community. However, as Morris and Hirst (2004) point out, these relations are far out-numbered by others, which they call non-classical relations. Here are a few of the kinds of non-classical relations they observed: positive qualities (BRILLIANT, KIND), concepts pertaining to a concept (KIND, CHIVALROUS, FORMAL pertaining to GENTLEMANLY), and commonly co-occurring words (locations such as HOMELESS, SHELTER; problem\u2013solution pairs such as HOMELESS, SHELTER)."}, {"heading": "1.1 Semantic relatedness and semantic similarity", "text": "Semantic distance is of two kinds: semantic similarity and semantic relatedness. The former is a subset of the latter, but the two terms may be used interchangeably in certain contexts, making it even more important to be aware of their distinction. Two concepts are considered to be semantically similar if there is a synonymy (or nearsynonymy), hyponymy (hypernymy), antonymy, or troponymy relation between them (examples include APPLES\u2013BANANAS, DOCTOR\u2013SURGEON, DARK\u2013BRIGHT). Two word senses are considered to be semantically related if there is any lexical semantic relation at all between them\u2014classical or non-classical (examples include APPLES\u2013BANANAS, SURGEON\u2013SCALPEL, TREE\u2013SHADE)."}, {"heading": "1.2 Human judgments of semantic distance", "text": "Humans are adept at estimating semantic distance; but consider the following questions: How strongly will two people agree/disagree on distance estimates? Will the agreement vary over different sets of concepts? Are we equally good at estimating semantic similarity and semantic relatedness? In our minds, is there a clear distinction between related and unrelated concepts or are concept-pairs spread across the whole range from synonymous to unrelated? Some of the earliest work that begins to address these questions is by Rubenstein and Goodenough (1965). They conducted quantitative experiments with human subjects (51 in all) who were asked to rate 65 English word pairs on a scale from 0.0 to 4.0 as per their semantic distance. The word pairs chosen ranged from almost synonymous to unrelated. However, they were all noun pairs and those that were semantically close were also semantically similar; the dataset did not contain word pairs that are semantically related but not semantically similar. The subjects repeated the annotation after two weeks and the new distance values had a Pearson\u2019s correlation r of 0.85 with the old ones. Miller and Charles (1991) also conducted a similar study on 30 word pairs taken from the Rubenstein-Goodenough pairs. These annotations had a high correlation (r = 0.97) with the mean annotations of Rubenstein and Goodenough (1965). Resnik (1999) repeated these experiments and found the inter-annotator correlation (r) to be 0.90.\n2\nMohammad and Hirst Distributional Measures of Semantic Distance\nNote: Rubenstein and Goodenough (1965) do not report inter-subject correlation, but determine intra-subject correlation to be 0.85 for 36 (out of the 65) word pairs for which similarity judgments were repeated by 15 (of the 51) subjects.\nResnik and Diab (2000) conducted annotations of 48 verb pairs and found inter-annotator correlation (r) to be 0.76 (when the verbs were presented without context) and 0.79 (when presented in context). Gurevych (2005) and Zesch, Gurevych, and M\u00fchlh\u00e4user (2007) asked native German speakers to mark two different sets of German word pairs with distance values. Set 1 was a German translation of the Rubenstein and Goodenough (1965) dataset. It had 65 noun\u2013noun word pairs. Set 2 was a larger dataset containing 350 word pairs made up of nouns, verbs, and adjectives. The semantically close word pairs in the 65-word set were mostly synonyms or hypernyms (hyponyms) of each other, whereas those in the 350-word set had both classical and non-classical relations with each other. Details of these semantic distance benchmarks are summarized in Table 1. Inter-subject correlations (last column in Table 1) are indicative of the degree of ease in annotating the datasets.\nIt should be noted here that even though the annotators were presented with wordpairs and not concept-pairs, it is reasonable to assume that they were annotated as per their closest senses. For example, given the noun pair bank and interest, most if not all will identify it as semantically related even though both words have more than one sense and many of the sense\u2013sense combinations are unrelated (for example, the RIVER BANK sense of bank and the SPECIAL ATTENTION sense of interest). The high agreement and correlation values suggest that humans are quite good and consistent at estimating semantic distance of noun-pairs; however, annotating verbs and adjectives and a combination of parts of speech is harder. This alsomeans that estimating semantic relatedness is harder than estimating semantic similarity.\nApart from showing that humans can indeed estimate semantic distance, these datasets act as \u201cgold standards\" to evaluate automatic distance measures. However, lack of large amounts of data from human subject experimentation limits the reliability of this mode of evaluation. Therefore automatic distance measures are also evaluated by their usefulness in natural language tasks such as those mentioned earlier."}, {"heading": "1.3 Automatic measures of semantic distance", "text": "Automatic measures of semantic distance quantify the semantic distance between word pairs. They give values within a certain range (for example, 0 to 1), such that one end of this range represent maximal closeness or synonymy, while the other end represents\n3\nmaximal distance. Depending onwhich end is which, measures of semantic distance can be classified as measures of distance (larger values indicate greater distance and less closeness) andmeasures of closeness (larger values indicate shorter distance and more closeness).1 Ameasure of closeness can be easily converted to a measure of distance by applying a suitable inverse function, or vice versa.\nTwo classes of automatic methods have been traditionally used to determine semantic distance. Knowledge-rich measures of concept-distance, such as those of Jiang and Conrath (1997), Leacock and Chodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it.2 Distributional measures of word-distance (knowledgelean measures), such as cosine and \u03b1-skew divergence (Lee 2001), rely on the distributional hypothesis, which states that two words tend to be semantically close if they occur in similar contexts (Firth 1957). Distributional measures rely simply on text (and possibly some shallow syntactic processing) and can give the distance between any two words that occur at least a few times.\nThe various WordNet-based measures have been widely studied (Budanitsky and Hirst 2006; Patwardhan, Banerjee, and Pedersen 2003). The study of distributional measures on the whole has received much less attention.3 Even though, as Weeds (2003) and Mohammad and Hirst (2006b) show, they perform poorly when compared to WordNet-based measures, the distributional measures of word-distance have many attractive features, including their ability to measure both semantic similarity and semantic relatedness. Further, they are not dependent on costly knowledge sources that do not exist for most languages. This paper therefore focuses on distributional measures and analyzes their strengths and limitations. Particular attention is paid to the different kinds of distributional measures and their components. The motivation is that a better understanding of distributional measures will lead to bringing them more in line with human notions of semantic distance, while still maintaining their applicability to resource-poor languages and their ability to mimic both semantic similarity and semantic distance."}, {"heading": "2. Knowledge-rich approaches to semantic distance", "text": "Before we begin our examination of distributional measures, we look briefly at the resource-based measures. In some ways they are complementary to distributional measures and so the discussion will set the context for the analysis of distributional measures.\nCreation of electronically available ontologies and semantic networks such as WordNet has allowed their use to help solve numerous natural language problems including the measurement of semantic distance. Budanitsky and Hirst (2006), Hirst and Budanitsky (2005), and Patwardhan, Banerjee, and Pedersen (2003) have done an extensive survey of the various WordNet-based measures, their comparisons\n1 A note about terminology: In many contexts, the term distance measures refers to the complete set of measures (irrespective of what the different ends of the range signify). In certain other contexts (as in this paragraph), distance measures refers only to those measures that give larger values to signify greater distance. The context, usually by its reference to this numeric property or lack thereof will make clear the intended meaning of the term. 2 The nodes in WordNet (synsets) represent word senses and edges between nodes represent semantic relations such as hyponymy and meronymy. 3 See Curran (2004) and Weeds, Weir, and McCarthy (2004) for other work that compares various distributional measures.\n4\nMohammad and Hirst Distributional Measures of Semantic Distance\nwith human judgment on selected word pairs, and their usefulness in applications such as real-word spelling correction and word sense disambiguation. Hence, this section provides only a brief summary of the major knowledge-rich measures of semantic distance."}, {"heading": "2.1 Measures that exploit WordNet\u2019s semantic network", "text": "A number of WordNet-based measures consider two concepts to be close if they are close to each other inWordNet. One of the earliest and simplest measures is Rada et al.\u2019s (1989) edge-countingmethod. The shortest path in the network between the two target concepts (target path) is determined. The more edges there are between two words, the more distant they are. Elegant as it may be, the measure hinges on the largely incorrect assumption that all the network edges correspond to identical semantic distance.\nNodes in a network may be connected by different kinds of lexical relations such as hyponymy, meronymy, and so on. Edge counts apart, Hirst and St-Onge\u2019s (1998) measure takes into account the fact that if the target path consists of edges that belong to many different relations, then the target concepts are likely more distant. The idea is that if we start from a particular node c1 and take a path via a particular relation (say, hyponymy), to a certain extent the concepts reached will be semantically related to c1. However, if during the way we take edges belonging to different relations (other than hyponymy), very soon we may reach words that are unrelated. Hirst and St-Onge\u2019s measure of semantic relatedness is:\nHS(c1, c2) = C\u2212 path length\u2212 k\u00d7 d (1)\nwhere c1 and c2 are the target concepts, d is the number of times an edge pertaining to a relation different from that of the preceding edge is taken, and C and k are empirically determined constants. More recently, Yang and Powers (2005) proposed a weighted edge-counting method to determine semantic relatedness using the hypernymy/hyponymy, holonymy/meronymy, and antonymy links in WordNet.\nLeacock and Chodorow (1998) used just one relation (hyponymy) and modified the path length formula to reflect the fact that edges lower down in the hyponymy hierarchy correspond to smaller semantic distance than the ones higher up. For example, synsets pertaining to sports car and car (low in the hierarchy) are much more similar than those pertaining to transport and instrumentation (higher up in the hierarchy) even though both pairs of nodes are separated by exactly one edge in the hierarchy. Their formula is:\nLC(c1, c2) = \u2212 log len(c1, c2)\n2D (2)\nwhere D is the maximum depth of the taxonomy. Resnik (1995) suggested a measure that combines corpus statistics with WordNet. He proposed that since the lowest common subsumer or lowest superordinate (lso) of the target nodes represents what is similar between them, the semantic similarity between the two concepts is directly proportional to how specific the lso is. The more general the lso is, the larger the semantic distance between the target nodes. This specificity is measured by the formula for information content (IC)\u2014the negative logarithm of the probability of the lso:\nRes(c1, c2) = IC(lso(c1, c2)) = \u2212 logP(lso(c1, c2)) (3)\n5\nObserve that using information content has the effect of inherently scaling the semantic similarity measure by the depth of the taxonomy. Usually, the lower the lowest superordinate, the lower the probability of occurrence of the lso and the concepts subsumed by it, and hence, the higher its information content.\nAs per Resnik\u2019s formula, given a particular lowest superordinate, the exact positions of the target nodes below it in the hierarchy do not have any effect on the semantic similarity. Intuitively, we would expect that word pairs closer to the lso are more semantically similar than those that are distant. Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms. The Jiang and Conrath (1997) measure (JC) determines how dissimilar each target concept is from the lso (IC(c1)\u2212 IC(lso(c1, c2)) and IC(c2)\u2212 IC(lso(c1, c2))). The final semantic distance between the two concepts is then taken to be the sum of these differences. Lin (1997) (like Resnik) points out that the lso is what is common between the two target concepts and that its information content is the common information between the two concepts. His formula (Lin) can be thought of as taking the Dice coefficient of the information in the two target concepts.\nJC(c1, c2) = 2 log p(lso(c1, c2))\u2212 (log(p(c1)) + (log(p(c2))) (4)\nLin(c1, c2) = 2\u00d7 log p(lso(c1, c2))\nlog(p(c1)) + (log(p(c2)) (5)\nBudanitsky and Hirst (2006) showed that the Jiang-Conrath measure has the highest correlation (0.850) with the Miller and Charles noun pairs and performs better than all these measures in a spelling correction task. Patwardhan, Banerjee, and Pedersen (2003) achieved similar results using the measure for word sense disambiguation.\nAll of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs. Resnik and Diab (2000) ported several measures which are traditionally applied on the noun hypernymy/hyponymy network (edge counting, and the measures of Resnik (1995), and Lin (1997)) to the relatively shallow verb troponymy network. The two information content\u2013based measures ranked a carefully chosen set of 48 verbs best in order of their semantic distance.4 Yang and Powers (2006) ported their earlier work on nouns (Yang and Powers 2005) to verbs. In order to compensate for the relatively shallow verb troponymy hierarchy and the lack of a corresponding holonymy/meronymy hierarchy, they proposed several back-off models\u2014the most useful one being the distance between a noun pair that has the same lexical form as the verb pair. However, the approach has too many tuned parameters (9 in all) and performed poorly on a set of 36 TOEFL word-choice questions involving verb targets and alternatives."}, {"heading": "2.2 Measures that rely on dictionaries and thesauri", "text": "Lesk (1986) introduced a method to perform word sense disambiguation using word glosses (definitions). The glosses of the senses of a target word are compared with those of its context and the number of word overlaps is determined. The sense with the\n4 Only those verbs were selected which require a theme, and the sub-categorization frames had to match.\n6\nMohammad and Hirst Distributional Measures of Semantic Distance\ngreatest number of overlaps is chosen as the intended sense of the target. Inspired by this approach, Banerjee and Pedersen (2003) proposed a semantic relatedness measure that deems two concepts to be more semantically related if there is more overlap in their glosses. Notably, they overcome the problem of short glosses by considering the glosses of concepts related to the target concepts through the WordNet lexical semantic relations such as hyponymy/hypernymy. They also give more weight to larger overlap sequences. Patwardhan and Pedersen (2006) proposed another gloss-based semantic relatedness measure which performed slightly worse than the extended gloss overlap measure in a word sense disambiguation task, but markedly better at ranking the Miller and Charles (1991) word pairs. They create aggregate co-occurrence vectors for a WordNet sense by adding the co-occurrence vectors of the words in its WordNet gloss. The distance between two senses is then determined by the cosine of the angle between their aggregate vectors. Such aggregate co-occurrence vectors are expected to be noisy because they are created from data that is not sense-annotated.\nJarmasz and Szpakowicz (2003) use the taxonomic structure of Roget\u2019s Thesaurus to determine semantic similarity. Two words are considered maximally similar if they occur in the same semicolon group in the thesaurus. Then on, decreasing in similarity are word pairs in the same paragraph, words pairs in different paragraphs belonging to the same part of speech andwithin the same category, word pairs in the category, and so on until word pairs which have nothing in common except that they are in the thesaurus (maximally distant). They show that this simple approach performs remarkably well at ranking word pairs and determining the correct answer in sets of TOEFL, ESL, and Reader\u2019s Digestword choice problems."}, {"heading": "2.3 Challenges", "text": "In this section, we review some of the shortcomings of resource-basedmeasures in order to motivate and to compare them with distributional measures that we will introduce in Section 3.\n2.3.1 Lack of high-quality WordNet-like knowledge sources. Ontologies, WordNets, and semantic networks are available for a few languages such as English, German, and Hindi. Creating them requires human experts and it is time intensive. Thus, for most languages, we cannot use WordNet-based measures simply due to the lack of a WordNet in that language. Further, even if created, updating an ontology is again expensive and there is usually a lag between the current state of language usage/comprehension and the semantic network representing it. Further, the complexity of human languages makes creation of even a near-perfect semantic network of its concepts impossible. Thus in many ways the ontology-based measures are only as good as the networks on which they are based.\nOn the other hand, distributional measures require only text. Large corpora, billions of words in size, may now be collected by a simple web crawler. Large corpora of moreformal writing are also available (for example, the Wall Street Journal or the American Printing House for the Blind (APHB) corpus). This makes distributional measures very attractive.\n2.3.2 Poor estimation of semantic relatedness.As Morris and Hirst (2004) pointed out, a large number of concept pairs, such as STRAWBERRY\u2013CREAM and DOCTOR\u2013SCALPEL, have a non-classical relation between them (STRAWBERRIES are usually eaten with CREAM and a DOCTOR uses a SCALPEL to make an incision). These words are not\n7\nsemantically similar, but rather semantically related. An ontology- or WordNet-based measure will correctly identify the amount of semantic relatedness only if such relations are explicitly coded into the knowledge source. Further, the most accurate WordNetbased measures rely only on its extensive is-a hierarchy. This is because networks of other lexical-relations such as meronymy are much less developed. Further, the networks for different parts of speech are not well connected. All this means that, while WordNet-based measures accurately estimate semantic similarity between nouns, their estimation of semantic relatedness, especially in pairs other than noun\u2013noun, is at best poor and at worse non-existent. On the other hand, distributional measures can be used to determine both semantic relatedness and semantic similarity.\n2.3.3 Inability to cater to specific domains. Given a concept pair, measures that rely only on WordNet and no text, such as that of Rada et al. (1989), give just one distance value. However, two concepts may be very close in a certain domain but not so much in another. For example, SPACE and TIME are close in the domain of quantum mechanics but not so much in most others. Ontologies have beenmade for specific domains, which may be used to determine semantic similarity specific to these domains. However, the number of such ontologies is very limited. Some of the more successful WordNet-based measures, such as that of Jiang and Conrath (1997), that rely also on text, do indeed capture domain-specificity to some extent, but the distance values are still largely shaped by the underlying network, which is not domain-specific. On the other hand, distributional measures rely primarily (if not completely) on text, and large amounts of corpora specific to particular domains can easily be collected.\n2.3.4 Computational complexity and storage requirements. As applications for linguistic distance become more sophisticated and demanding, it becomes attractive to pre-compute and store the distance values between all possible pairs of words or senses. However both WordNet-based and distributional measures have large space requirements to do this, requiring matrices of size N \u00d7 N, where N is very large. In case of WordNet-based measures, N is the number of senses (81,000 just for nouns). In case of distributional measures, N is the size of the vocabulary (at least 100,000 for most languages). Given that the above matrices tend to be sparse5 and that computational capabilities are continuing to improve, the above limitation may not seem hugely problematic, but as we see more and more natural language applications in embedded systems and hand-held devices, such as cell phones, iPods, and medical equipment, memory and computational power become serious constraints.\n2.3.5 Reluctance to cross the language barrier. Both WordNet-based and distributional measures have largely been used in a monolingual framework. Even though semantic distance seems to hold promise in tasks such as machine translation and multi-lingual text summarization that inherently involve two or more languages, automatic measures of semantic distance have rarely been applied. With the development of the EuroWordNet, involving interconnected networks of seven different languages, it is possible that we shall see more cross-lingual work using WordNet-based measures in the future. However, such an interconnected network will be very hard to create for more-different language pairs such as English and Chinese or English and Arabic.\n5 Even though WordNet-based and distributional measures give non-zero closeness values to a large number of term pairs, values below a suitable threshold can be reset to 0.\n8\nMohammad and Hirst Distributional Measures of Semantic Distance"}, {"heading": "3. Knowledge-lean, distributional approaches to semantic distance", "text": ""}, {"heading": "3.1 The distributional hypotheses: the original and the revised", "text": "Distributional measures are inspired by the maxim \u201cYou shall know a word by the company it keeps\u201d (Firth 1957). These measures rely simply on raw text and possibly some shallow syntactic processing. They are much less resource-hungry than the semantic measures, but they measure the distance between words rather than wordsenses or concepts. Two words are considered close if they occur in similar contexts. The context of a target word is usually taken to be the set of words within a certain window around it, for example, \u00b15 words or the complete sentence. The set of contexts of a target word is usually represented by the set of words in these contexts, their strength of association (SoA) with the target word, and possibly their syntactic relation with the target, for example verb\u2013object, subject\u2013verb, and so on. The strength of cooccurrence association between the target and another word quantifies howmuch more (or less) than chance the two words occur together in text. Commonly used measures of association are conditional probability (CP) and pointwise mutual information (PMI). The distance between the sets of contexts of two target words can be used as a proxy for their semantic distance, as words found in similar contexts tend to be semantically similar\u2014the distributional hypothesis (Firth 1957; Harris 1968).\nThe hypothesis makes intuitive sense, as Budanitsky and Hirst (2006) point out: If two words have many co-occurring words in common, then similar things are being said about both of them and so they are likely to be semantically similar. Conversely, if two words are semantically similar, then they are likely to be used in a similar fashion in text and thus end up with many common co-occurrences. For example, the semantically similar bug and insect are expected to have a number of common co-occurring words such as crawl, squash, small, woods, and so on, in a large-enough text corpus.\nThe distributional hypothesis only mentions semantic similarity and not semantic relatedness. This, coupled with the fact that the difference between semantic relatedness and semantic similarity is somewhat nuanced and can be missed, meant that almost all work employing the distributional hypothesis was labeled as estimating semantic similarity. However, it should be noted that distributional measures can be used to estimate both semantic similarity and semantic relatedness. Even though Sch\u00fctze and Pedersen (1997) and Landauer, Foltz, and Laham (1998), for example, use the term similarity and not relatedness, their LSA-based distance measures in fact estimate semantic relatedness and not semantic similarity. We propose more-specific distributional hypotheses that make clear how distributional measures can be used to estimate semantic similarity and how they can be used tomeasure semantic relatedness:\nHypothesis of the distributionally close and semantically related: Two target words are distributionally close and semantically related if they have many common strongly co-occurring words. (For example, doctor\u2013surgeon and doctor\u2013scalpel. See example co-occurring words in Table 2.)\nHypothesis of the distributionally close and semantically similar: Two target words are distributionally close and semantically similar if they have many common strongly co-occurring words that each have the same syntactic relation with the two targets. (For example, doctor\u2013surgeon, but not doctor\u2013scalpel. See syntactic relations with example co-occurring words in Table 2.)\n9\nThe idea is that both semantically similar and semantically related word pairs will have many common co-occurring words. However, words that are semantically similar belong to the same broad part of speech (noun, verb, etc.), but the same need not be true for words that are semantically related. Therefore, words that are semantically similar will tend to have the same syntactic relation, such as verb\u2013object or subject\u2013 verb, with most common co-occurring words. Thus, the two words are considered semantically related simply if they have many common co-occurring words. But to be semantically similar as well, the words must have the same syntactic relation with cooccurring words. Consider the word pair doctor\u2013operate. In a large enough body of text, the two words are likely to have the following common co-occurring words: patient, scalpel, surgery, recuperate, and so on. All these words will contribute to a high score of relatedness. However, they do not have the same syntactic relation with the two targets. (The word doctor is almost always used as a noun while operate is a verb.) Thus, as per the two revised distributional hypotheses, doctor and operate will correctly be identified as semantically related but not semantically similar. The word pair doctor\u2013nurse, on the other hand, will be identified as both semantically related and semantically similar.\nIn order to clearly differentiate from the distance as calculated by a WordNet-based semantic measure (described earlier in Section 2.1), the distance calculated by a corpusbased distributional measure will be referred to as distributional distance."}, {"heading": "3.2 Corpus-based measures of distributional distance", "text": "We now describe specific distributional measures that rely on the distributional hypotheses; depending on which specific hypothesis they use, they mimic either semantic similarity or semantic relatedness.\n3.2.1 SpatialMetrics: Cos, L1, L2.Consider a multidimensional space in which the number of dimensions is equal to the size of the vocabulary. A word w can be represented by a point in this space such that the component of ~w in a dimension (corresponding to word x, say) is equal to the strength of association (SoA) of w with x (SoA(w, x)). Thus, the vectors corresponding to two words are close together, and thereby get a low distributional distance score, if they share many co-occurring words and the co-occurring\n10\nMohammad and Hirst Distributional Measures of Semantic Distance\nwords havemore or less the same strength of association with the two target words. The distance between two vectors can be calculated in different ways as described below.\nCosine. The cosinemethod (denoted by Cos) is one of the earliest and most widely used distributional measures. Given two words w1 and w2, the cosine measure calculates the cosine of the angle between ~w1 and ~w2. If a large number of words co-occur with both w1 and w2, then ~w1 and ~w2 will have a small angle between them and the cosine will be large; signifying a large relatedness/similarity between them. The cosine measure gives scores in the range from 0 (unrelated) to 1 (synonymous). So the higher the value, the less distant the target word-pair is.\nCos(w1,w2) = \u2211w\u2208C(w1)\u222aC(w2) (P(w|w1)\u00d7 P(w|w2)) \u221a\n\u2211w\u2208C(w1) P(w|w1) 2 \u00d7\n\u221a\n\u2211w\u2208C(w2) P(w|w2) 2\n(6)\nwhere C(w) is the set of words that co-occur (within a certain window) with the word w in a corpus. In this instantiation of the cosine measure, conditional probability of the co-occurring words given the target words is used as the strength of association.\nThe cosine was used, among others, by Sch\u00fctze and Pedersen (1997) and Yoshida, Yukawa, and Kuwabara (2003), who suggest methods of automatically generating distributional thesauri from text corpora. Sch\u00fctze and Pedersen (1997) use the Tipster category B corpus (Harman 1993) (450,000 unique terms) and the Wall Street Journal to create a large but sparse co-occurrence matrix of 3,000 medium-frequency words (frequency rank between 2,000 and 5,000). Latent semantic indexing (singular value decomposition) (Sch\u00fctze and Pedersen 1997) is used to reduce the dimensionality of the matrix and get for each term a word vector of its 20 strongest co-occurrences. The cosine of a target word\u2019s vector with each of the other word vectors is calculated and the words that give the highest scores comprise the thesaurus entry for the target word.\nYoshida, Yukawa, and Kuwabara (2003) believe that words that are closely related for one person may be distant for another. They use around 40,000 HTML documents to generate personalized thesauri for six different people. Documents used to create the thesaurus for a person are retrieved from the subject\u2019s home page and a web crawler which accesses linked documents. The authors also suggest a root-meansquared method to determine the similarity of two different thesaurus entries for the same word.\nManhattan and Euclidean Distances.Distance between two points (words) in vector space can also be calculated using the formulae for Manhattan distance a.k.a. the L1 norm (denoted by L1) or Euclidean distance a.k.a. the L2 norm (denoted by L2). In the Manhattan distance (7) (Dagan, Lee, and Pereira (1997), Dagan, Lee, and Pereira (1999), and Lee (1999)), the difference in strength of association of w1 and w2 with each word that they co-occur with is summed. The greater the difference, the greater is the distributional distance between the two words. Euclidean distance (8) (Lee 1999) employs the root mean square of the difference in association to get the final distributional distance. Both the L1 and L2 norms give scores in the range between 0 (zero distance or synonymous) and infinity (maximally distant or unrelated).\nL1(w1,w2) = \u2211 w\u2208C(w1)\u222aC(w2) | P(w|w1)\u2212 P(w|w2) | (7)\n11\nL2(w1,w2) =\n\u221a\n\u2211 w\u2208C(w1)\u222aC(w2)\n(P (w|w1)\u2212 P (w|w2)) 2 (8)\nThe above formulae use conditional probability of the co-occurring words given a target word as the strength of association.\nLee (1999) compared the ability of all three spatial metrics to determine the probability of an unseen (not found in training data) word pair. The measures in order of their performance (from better to worse) were: L1 norm, cosine, and L2 norm. Weeds (2003) determined the correlation of word pair ranking as per a handful of distributional measures with human rankings (Miller and Charles (1991) word pairs). She used verbobject pairs from the British National Corpus (BNC) and found the correlation of L1 norm with human rankings to be 0.39.\n3.2.2 Mutual information\u2013based measures: Hindle, Lin. Hindle (1990) was one of the first to factor the strength of association of co-occurring words into a distributional similarity measure.6 Consider the nouns nj and nk that exist as objects of verb vi in different instances within a text corpus. Hindle used the following formula to determine the distributional similarity of nj and nk solely from their occurrences as object of vi:\nHinobj(vi, nj, nk) =\n\n   \n   \nmin(I(vi, nj), I(vi, nk)), if I(vi, nj) > 0 and I(vi, nk) > 0 | max(I(vi, nj), I(vi, nk)) |, if I(vi, nj) < 0 and I(vi, nk) < 0 0, otherwise\n(9)\nI(n, v) stands for the pointwise mutual information (PMI) between the noun n and verb v (note that in case of negative PMI values, the maximum function captures the PMI which is lower in absolute value). The measure follows from the distributional hypothesis\u2014the more similar the associations of co-occurring words with the two target words, the more semantically similar they are. Hindle used PMI7 as the strength of association. Using the minimum of the two PMIs captures the similarity in the strength of association of vi with each of the two nouns.\nHindle used an analogous formula to calculate distributional similarity (Hinsubj) using the subject\u2013verb relation. The overall distributional similarity between any two nouns is calculated by the formula:\nHin(n1, n2) = N\n\u2211 i=0\n( Hinobj(vi, n1, n2) +Hinsubj(vi, n1, n2) )\n(10)\nThemeasure gives similarity scores from 0 (maximally dissimilar) to infinity (maximally similar or synonymous). Note that in Hindle\u2019s measure, the set of co-occurring words used is restricted to include only those words that have the same syntactic relation with both target words (either verb\u2013object or verb\u2013subject). This is therefore a measure that\n6 See Grefenstette (1992) for an approach that does not incorporate strength of association of co-occurring words. He, like Hindle (1990), uses syntactic dependencies to to characterize the set of contexts of a target word. The Jaccard coefficient is used to determine how similar the two sets of contexts are. 7 Hindle (1990) and Lin (1998b) both refer to pointwise mutual information as mutual information.\n12\nMohammad and Hirst Distributional Measures of Semantic Distance\nmimics semantic similarity and not semantic relatedness. A form of Hindle\u2019s measure where all co-occurring words are used, making it a measure that mimics semantic relatedness, is shown below:\nHinrel(w1,w2) = \u2211 w\u2208C(w)\n\n    \n    \nmin(I(w,w1), I(w,w2)), if I(w,w1) > 0 and I(w,w2) > 0 | max(I(w,w1), I(w,w2)) |, if I(w,w1) < 0 and I(w,w2) < 0 0, otherwise\n(11)\nwhere C(w) is the set of words that co-occur with word w. Lin (1998b) suggests a different measure derived from his information-theoretic definition of similarity (Lin 1998a). Further, he uses a broad set of syntactic relations apart from just subject\u2013verb and verb\u2013object relations and shows that using multiple relations is beneficial even for Hindle\u2019s measure. He first extracts triples of the form (x, r, y) from the partially parsed text, where the word x is related to y by the syntactic relation r. Lin defines the distributional similarity between two words, w1 and w2, as follows:\nLin(w1,w2) = \u2211(r,w) \u2208 T(w1) \u2229 T(w2) (I(w1, r,w) + I(w2, r,w))\n\u2211(r,w\u2032) \u2208 T(w1) I(w1, r,w \u2032) + \u2211(r,w\u2032\u2032) \u2208 T(w2) I(w2, r,w\n\u2032\u2032) (12)\nwhere T(x) is the set of all word pairs (r, y) such that the pointwise mutual information I(x, r, y), is positive. Note that this is different from Hindle (1990) where even the cases of negative PMI were considered. Church and Hanks (1990) showed that it is hard to accurately predict negative word association ratios with confidence, and so, co-occurrence pairs with negative PMI are ignored. The measure gives similarity scores from 0 (maximally dissimilar) to 1 (maximally similar).\nLike Hindle\u2019s measure, Lin\u2019s is a measure of distributional similarity. However, it distinguishes itself from that of Hindle in two respects. First, Lin normalizes the similarity score between two words (numerator of (12)) by their cumulative strengths of association with the rest of the co-occurring words (denominator of (12)). This is a significant improvement as now high PMI of the target words with shared co-occurring words alone does not guarantee a high distributional similarity score. As an additional requirement, the target words must have low PMI with words they do not both cooccur with. Second, Hindle uses the minimum of the PMI between each of the target words and the shared co-occurring word, while Lin uses the sum. Taking the sum has the drawback of not penalizing for a mismatch in strength of co-occurrence, as long as w1 and w2 both co-occur with a word.\nHindle (1990) used a portion of the Associated Press news stories (6 million words) to classify the nouns into semantically related classes. Lin (1998b) used his measure to generate a distributional thesaurus from a 64-million-word corpus of theWall Street Journal, San Jose Mercury, and AP Newswire. He also provides a framework for evaluating such automatically generated thesauri by comparing them with WordNet-based and Rogetbased thesauri. He shows that the distributional thesaurus created with his measure is closer to the WordNet and Roget-based thesauri than that created using Hindle\u2019s measure.\n3.2.3 Relative Entropy\u2013BasedMeasures: KLD, ASD, JSD.\n13\nKullback-Leibler divergence. Given two probability mass functions p(x) and q(x), their relative entropy D(p\u2016q) is:\nD(p\u2016q) = \u2211 x\u2208X\np(x) log p(x)\nq(x) for q(x) 6= 0 (13)\nIntuitively, if p(x) is the accurate probability mass function corresponding to a random variable X, then D(p\u2016q) is the information lost when approximating p(x) by q(x). In other words, D(p\u2016q) is indicative of how different the two distributions are. Relative entropy is also called the Kullback-Leibler divergence or the Kullback-Leibler distance (denoted by KLD).\nPereira, Tishby, and Lee (1993) and Dagan, Lee, and Pereira (1994) point out that words have probabilistic distributions with respect to neighboring syntactically related words. For example, there exists a certain probabilistic distribution (d1(P(v|n1)), say) of a particular noun n1 being the object of any verb. This distribution can be estimated by corpus counts of parsed or chunked text. Let d2 (P(v|n2)) be the corresponding distribution for noun n2. These distributions (d1 and d2) define the contexts of the two nouns (n1 and n2, respectively). As per the distributional hypothesis, the more these contexts are similar, the more n1 and n2 are semantically similar. Thus the KullbackLeibler distance between the two distributions is indicative of the semantic distance between the nouns n1 and n2.\nKLD(n1, n2) = D(d1\u2016d2)\n= \u2211v\u2208Vb P(v|n1) log P(v|n1) P(v|n2)\nfor P(v|n2) 6= 0\n= \u2211v\u2208Vb\u2032(n1)\u2229Vb\u2032(n2) P(v|n1) log P(v|n1) P(v|n2) for P(v|n2) 6= 0\n(14)\nwhere Vb is the set of all verbs and Vb\u2032(x) is the set of verbs that have x as the object. Note again that the set of co-occurring words used is restricted to include only verbs that each have the same syntactic relation (verb\u2013object) with both target nouns. This too is therefore a measure that mimics semantic similarity and not semantic relatedness.\nIt should be noted that the verb\u2013object relationship is not inherent to the measure and that one or more of any other syntactic relations may be used. One may also estimate semantic relatedness by using all words co-occurring with the target words. Thus a more generic expression of the Kullback-Leibler divergence is as follows:\nKLD(w1,w2) = D(d1\u2016d2)\n= \u2211w\u2208V P(w|w1) log P(w|w1) P(w|w2)\nfor P(w|w2) 6= 0\n= \u2211w\u2208C(w1)\u222aC(w2) P(w|w1) log P(w|w1) P(w|w2) for P(w|w2) 6= 0\n(15)\nwhere V is the vocabulary (all the words found in a corpus). C(w), as mentioned earlier, is the set of words occurring (within a certain window) with word w.\nIt should be noted that the Kullback-Leibler distance is not symmetric; that is, the distance from w1 to w2 is not necessarily, and even not likely, the same as the distance from w2 to w1. This asymmetry is counterintuitive to the general notion of semantic similarity of words, althoughWeeds (2003) has argued in favor of asymmetric measures. Further, it is very likely that there are instances such that P(w1|v) is greater than 0 for a particular verb v, while due to data sparseness or grammatical and semantic constraints,\n14\nMohammad and Hirst Distributional Measures of Semantic Distance\nthe training data has no sentence where v has the object w2. This makes P(w2|v) equal to 0 and the ratio of the two probabilities infinite. Kullback-Leibler divergence is not defined in such cases, but approximations may be made by considering smoothed values for the denominator.\nPereira, Tishby, and Lee (1993) used KLD to create clusters of nouns from verbobject pairs corresponding to the thousand most frequent nouns in the Grolier\u2019s Encyclopedia, June 1991 version (10 million words). Dagan, Lee, and Pereira (1994) used KLD to estimate the probabilities of bigrams that were not seen in a text corpus. They point out that a significant number of possible bigrams are not seen in any given text corpus. The probabilities of such bigrams may be determined by taking a weighted average of the probabilities of bigrams composed of distributionally similar words. Use of Kullback-Leibler distance as the semantic distance metric yielded a 20% improvement in perplexity on the Wall Street Journal and dictation corpora provided by ARPA\u2019s HLT program (Paul 1991).\nIt should be noted here that the use of distributionally similar words to estimate unseen bigram probabilities will likely lead to erroneous results in case of less-preferred and strongly-preferred collocations (word pairs). Inkpen and Hirst (2002) point out that even though words like task and job are semantically very similar, the collocations they form with other words may have varying degrees of usage. While daunting task is a strongly-preferred collocation, daunting job is rarely used. Thus using the probability of one bigram to estimate that of another will not be beneficial in such cases.\n\u03b1-skew divergence. The \u03b1-skewdivergence (ASD) is a slight modification of the KullbackLeibler divergence that obviates the need for smoothed probabilities. It has the following formula:\nASD(w1,w2) = \u2211 w\u2208C(w1)\u222aC(w2)\nP(w|w1) log P(w|w1)\n\u03b1P(w|w2) + (1\u2212 \u03b1)P(w|w1) (16)\nwhere \u03b1 is a parameter that may be varied but is usually set to 0.99. Note that the denominator within the logarithm is never zero with a non-zero numerator. Also, the measure retains the asymmetric nature of the Kullback-Leibler divergence. Lee (2001) shows that \u03b1-skew divergence performs better than Kullback-Leibler divergence in estimating word co-occurrence probabilities. Weeds (2003) achieves a correlation of 0.48 and 0.26 with human judgment on the Miller and Charles word pairs using ASD(w1,w2) and ASD(w2,w1), respectively.\nJensen-Shannon divergence. A relative entropy\u2013based measure that overcomes the problem of asymmetry in Kullback-Leibler divergence is the Jensen-Shannon divergence a.k.a. total divergence to the average a.k.a. information radius. It is denoted by JSD and has the following formula:\nJSD(w1,w2) = D\n(\nd1\u2016 1\n2 (d1 + d2)\n)\n+ D\n(\nd2\u2016 1\n2 (d1 + d2)\n)\n(17)\n= \u2211 w\u2208C(w1)\u222aC(w2)\n(\nP(w|w1) log P(w|w1)\n1 2 (P(w|w1) + P(w|w2))\n+\n15\nP(w|w2) log P(w|w2)\n1 2 (P(w|w1) + P(w|w2))\n)\n(18)\nThe Jensen-Shannon divergence is the sum of the Kullback-Leibler divergence between each of the individual co-occurrence distributions d1 and d2 of the target words with the average distribution ( d1+d22 ). Further, it can be shown that the Jensen-Shannon divergence avoids the problem of zero denominator. The Jensen-Shannon divergence is therefore always well defined and, like \u03b1-skew divergence, obviates the need for smoothed estimates.\nThe Kullback-Leibler divergence, \u03b1-skew divergence, and Jensen-Shannon divergence all give distributional distance scores from 0 (synonymous) to infinity (unrelated).\n3.2.4 Latent Semantic Analysis. Landauer, Foltz, and Laham (1998) proposed Latent semantic analysis (LSA), which can be used to determine distributional distance betweenwords or between sets of words.8 Unlike the various approaches described earlier where a word\u2013word co-occurrence matrix is created, the first step of LSA involves the creation of a word\u2013paragraph, word\u2013document, or similar such word-passage matrix, where a passage is some grouping of words. A cell for wordw and passage p is populated with the number of times w occurs in p or, for even better results, a function of this frequency that captures how much information the occurrence of the word in a text passage carries.\nNext, the dimensionality of this matrix is reduced by applying singular value decomposition (SVD), a standard matrix decomposition technique. This smaller set of dimensions represents abstract (unknown) concepts. Then the original word\u2013passage matrix is recreated, but this time from the reduced dimensions. Landauer, Foltz, and Laham (1998) point out that this results in new matrix cell values that are different from what they were before. More specifically, words that are expected to occur more often in a passage than what the original cell values reflect are incremented. Then a standard vector distance measure, such as cosine, that captures the distance between distributions of the two target words is applied.\nLSA was used by Sch\u00fctze and Pedersen (1997), Turney (2001) and Rapp (2003) to measure distributional distance, with encouraging results. However, there is no nonheuristic way to determine when the dimension reduction should stop. Further, the generic concepts represented by the reduced dimensions are not interpretable; that is, one cannot determine which concepts they represent in a given sense inventory. This means that LSA cannot directly be used for tasks such as unsupervised sense disambiguation or estimating semantic similarity of known concepts. LSA is computationally expensive as singular value decomposition, a key component for dimensionality reduction, requires computationally intensive matrix operations. This makes LSA less scalable to large amounts of text (Gorman and Curran 2006). Finally, it too, like other distributional word-distance measures conflates the many senses of a word (see Section 4.6.1 ahead for more discussion on sense conflation).\n3.2.5 Co-occurrence Retrieval Models. The distributional measures suggested by Weeds (2003) are based on a notion of substitutability: the more appropriate it is to\n8 Landauer, Foltz, and Laham (1998) describe it as a measure of similarity, but in fact it is a distributional measure that mimics semantic relatedness.\n16\nMohammad and Hirst Distributional Measures of Semantic Distance\nsubstitute word w1 in place of word w2 in a suitable natural language task, the more semantically similar they are. She uses co-occurrence retrieval (the retrieval of words that co-occur with a target word from text) to determine the degree to which one word is substitutable by another. The degree of substitutability of w2 with w1 is dependent on how the proportion of co-occurrences of w1 that are also co-occurrences of w2 and the proportion of co-occurrences of w2 that are also co-occurrences of w1. Thus Weeds\u2019s distributional measures have a precision component and a recall component (which may or may not incorporate the strength of co-occurrence association). The final score is a weighted sum of the precision, recall, and standard F measure (see equation (19)9). The weights determine the importance of precision and recall and are determined empirically. If precision and recall are equally important, then it results in a symmetric measure which gives identical scores for the distributional similarity of w1 with w2 and w2 with w1. Otherwise, we get an asymmetric measure which assigns different scores to the two cases.\nCRM(w1,w2) = \u03b3\n[\n2\u00d7 P\u00d7 R\nP+ R\n]\n+ (1\u2212 \u03b3)\n[\n\u03b2[P] + (1\u2212 \u03b2)[R]\n]\n(19)\n\u03b3 and \u03b2 are tuned parameters that lie between 0 and 1. Both precision and recall can be considered as the product of a core formula (denoted by core) and a penalty function (denoted by penalty). Weeds03 provides six (three times two) distinct formulae for precision and recall, depending on the strength of cooccurrence (three alternatives) and whether or not a penalty is applied for differences in strength of association of common co-occurring words (two alternatives).\nDepending on the strength of association, the CRMs are classified as type-based, token-based, and mutual information\u2013based. The CRMs that use simple counts of the common co-occurrences and not the strength of associations as core precision and recall values are called type-based CRMs (denoted by the superscript type). The CRMs that use conditional probabilities of the shared co-occurring words with the target words are called token-based CRMs (denoted by the superscript token). The CRMs that use pointwise mutual information of the shared co-occurring words with target words are called mutual information\u2013based CRMs (denoted by the superscript mi). The core precision and recall formulae for type, token, and mutual information\u2013based CRMs are listed below:\ncore type P (w1,w2) =\n| C(w1) \u2229 C(w2) |\n| C(w1) | (20)\ncore type R (w1,w2) =\n| C(w1) \u2229 C(w2) |\n| C(w2) | (21)\ncoretokenP (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) P(w|w1) (22)\ncoretokenR (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) P(w|w2) (23)\n9 P is short for P(w1,w2), while R is short for R(w1,w2). The abbreviations are made due to space constraints and to improve readability.\n17\ncoremiP (w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) I(w,w1)\n\u2211w\u2208C(w1) I(w,w1) (24)\ncoremiR (w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) I(w,w2)\n\u2211w\u2208C(w2) I(w,w2) (25)\nwhere C(x) is the set of words that co-occur with x. Depending on the penalty function, the CRMs are classified as additive and difference-weighted. The CRMs that do not penalize difference in strength of cooccurrence are called additive CRMs (denoted by the subscript add); those that do penalize are called difference-weighted CRMs (subscript dw). The penalty is a conditional probability\u2013based function (26, 27) for the token- and type-based CRMs, and a mutual information\u2013based function (28, 29) for the mutual information\u2013based CRM.\npenalty type P = penalty token P =\nmin(P(w|w1), P(w|w2))\nP(w|w1) (26)\npenalty type R = penalty token R =\nmin(P(w|w1), P(w|w2))\nP(w|w2) (27)\npenaltymiP = min(I(w,w1), I(w,w2))\nI(w,w1) (28)\npenaltymiR = min(I(w,w1), I(w,w2))\nI(w,w2) (29)\nThe six pairs of precision and recall difference-weighted CRMs are thus as follows:\nP type add (w1,w2) =\n| C(w1) \u2229 C(w2) |\n| C(w1) | (30)\nR type add (w1,w2) =\n| C(w1) \u2229 C(w2) |\n| C(w2) | (31)\nP type dw (w1,w2) =\n\u2211|C(w1)\u2229C(w2)| min(P(w|w1),P(w|w2))\nP(w|w1)\n| C(w1) | (32)\nR type dw (w1,w2) =\n\u2211|C(w1)\u2229C(w2)| min(P(w|w1),P(w|w2))\nP(w|w2)\n| C(w2) | (33)\nPtokenadd (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) P(w|w1) (34)\nRtokenadd (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) P(w|w2) (35)\nPtokendw (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) min(P(w|w1), P(w|w2)) (36)\n18\nMohammad and Hirst Distributional Measures of Semantic Distance\nRtokendw (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) min(P(w|w2), P(w|w1)) (37)\nPmiadd(w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) I(w,w1)\n\u2211w\u2208C(w1) I(w,w1) (38)\nRmiadd(w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) I(w,w2)\n\u2211w\u2208C(w2) I(w,w2) (39)\nPmidw(w1,w2) = \u2211w\u2208C(w1)\u2229C(w2)min(I(w,w1), I(w,w2))\n\u2211w\u2208C(w1) I(w,w1) (40)\nRmidw(w1,w2) = \u2211w\u2208C(w1)\u2229C(w2)min(I(w,w1), I(w,w2))\n\u2211w\u2208C(w2) I(w,w2) (41)\nNote that in case of the difference-weighted token and mutual information\u2013based precision and recall formulae, there is a cancellation of a pair of terms obtained from the core formulae and the penalty.\nAsymmetry in substitutability is intuitive, as in many cases it may be acceptable to substitute a word, say dog, with another, say animal, but the reverse is not acceptable as often. SinceWeeds uses substitutability as a measure of semantic similarity, she believes that distributional similarity between two words should reflect this property as well. Hence, like the Kullback-Leibler divergence, all her distributional similarity models are asymmetric.\nWeeds (2003) extracted verb\u2013object pairs of 2,000 nouns from the British National Corpus (BNC). The verbs related to the target words by the verb\u2013object relation were used. Thus each of the co-occurring verbs is related to the target nouns by the same syntactic relation and therefore the measures mimic semantic similarity, not relatedness. Correlation with human judgment (Miller and Charles word pairs) showed that difference-weighted (r = 0.61) and additive mutual information\u2013based measures (r = 0.62) performed far better than the other CRMs."}, {"heading": "4. The anatomy of a distributional measure", "text": "Even though there are numerous distributional measures, many of which may seem dramatically different from each other, all distributional measures perform two functions: (1) create distributional profiles (DPs), and (2) calculate the distance between two DPs.\nThe distributional profile of aword is the strength of association between it and each of the lexical, syntactic, and/or semantic units that co-occur with it. Commonly used measures of strength of association are conditional probability (0 to 1) and pointwise mutual information (\u2212\u221e to \u221e). Commonly used units of co-occurrence with the target are other words, and so we speak of the lexical distributional profile of a word (lexical DPW). The co-occurring words may be all those in a predetermined window around the target, or may be restricted to those that have a certain syntactic (e.g., verb\u2013object) or semantic (e.g., agent\u2013theme) relation with the target word. We will refer to the former kind of DPs as relation-free. Usually in the latter case, separate association values are calculated for each of the different relations between the target and the co-occurring\n19\nunits. We will refer to such DPs as relation-constrained. Typical relation-free DPs are those of Sch\u00fctze and Pedersen (1997) and Yoshida, Yukawa, and Kuwabara (2003). Typical relation-constrained DPs are those of Lin (1998a) and Lee (2001). Below are contrived, but plausible, examples of each for the word pulse; the numbers are conditional probabilities:\nrelation-free DP pulse: beat .28, racing .2, grow .13, beans .09, heart .04, . . .\nrelation-constrained DP pulse: \u3008beat, subject\u2013verb\u3009 .34, \u3008racing, noun\u2013qualifying adjective\u3009 .22, \u3008grow, subject\u2013verb\u3009 .14, . . .\nSince the DPs represent the contexts of the two target words, the distance between the DPs is the distributional distance and, as per the distributional hypothesis, a proxy for semantic distance. Ameasure of DP distance, such as cosine, calculates the distance between two distributional profiles. While any of the measures of DP distance may be used with any of the measures of strength of association, in practice only certain combinations are used (see Table 3) and certain other combinations might not be meaningful (for example, Kullback-Leibler divergence with \u03c6 coefficient). Observe from Table 3 that\n20\nMohammad and Hirst Distributional Measures of Semantic Distance\nall standard-combination distributional measures (or at least those that are described in this paper) use either conditional probability or PMI as the measure of association."}, {"heading": "4.1 Simple co-occurrences versus syntactically related words", "text": "Harris (1968), one of the early proponents of the distributional hypothesis, used syntactically related words to represent the context of a word. However, the strength of association of any word appearing in the context of the target words may be used to determine their distributional similarity. Dagan, Lee, and Pereira (1997), Lee (1999), and Weeds (2003) represent the context of a noun with verbs whose object it is (single syntactic relation), Hindle (1990) represents the context of a noun with verbs with which it shares the verb-object or subject-verb relation, while Lin (1998b) uses words related to a noun by any of the many pre-decided syntactic relations to determine distributional similarity. Sch\u00fctze and Pedersen (1997) and Yoshida, Yukawa, and Kuwabara (2003) use all co-occurring words in a pre-decided window size. Although Lin (1998b) shows that the use of multiple syntactic relations is more beneficial as compared to just one, McCarthy et al. (2007) show that results obtained using just word co-occurrences produced almost as good results as those obtained using syntactically related words. Further, use of syntactically related words entails the requirement of chunking or parsing the data."}, {"heading": "4.2 Compositionality", "text": "The various measures of distributional similarity may be divided into two kinds as per their composition. In certain measures, each co-occurring word contributes to some finite calculable distributional distance between the target words. The final score of distributional distance is the sum of these contributions. We will call such measures compositional measures. The relative entropy\u2013based measures, L1 norm and L2 norm, fall in this category. On the other hand, the cosine measure, along with Hindle\u2019s and Lin\u2019s mutual information\u2013based measures, belong to the category of what we call non-compositional measures. Each co-occurring word shared by both target words contributes a score to the numerator and the denominator of the measures\u2019 formula. Words that co-occur with just one of the two target words contribute scores only to the denominator. The ratio is calculated once all co-occurring words are considered. Thus the distributional distance contributed by individual co-occurrences is not calculable and the final semantic distance cannot be broken down into compositional distances contributed by each of the co-occurrences. It is not clear as to which of the two kinds of measures (compositional or non-compositional) resembles human judgment more closely and how much they differ in their ranking of word pairs.\n4.2.1 Primary Compositional Measures. The compositional measures of distributional similarity (or relatedness) capture the contribution to distance between the target words (w1 and w2) due to a co-occurring word by three primary mathematical manipulations of the co-occurrence distributions (d1 and d2): the difference, denoted by Dif (as in L1 norm), division, denoted by Div (as in the relative entropy\u2013based measures), and product, denoted by Pdt (as in the conditional probability or mutual information\u2013 based cosine method). We will call the three types of compositional measures primary compositional measures (PCM). Their form is depicted below:\n21\nDif = \u2211 w\u2208C(w1)\u222aC(w2) |P(w|w1)\u2212 P(w|w2)| (42)\nDiv = \u2211 w\u2208C(w1)\u222aC(w2)\n\u2223 \u2223 \u2223 \u2223 log P(w|w1)\nP(w|w2)\n\u2223 \u2223 \u2223 \u2223\n(43)\nPdt = \u2211 w\u2208C(w1)\u222aC(w2)\nP(w|w1)\u00d7 P(w|w2)\nScaling Factor (44)\nObserve that by taking absolute values in expressions (42) and (43), the variation in the distributions for different co-occurring words has an additive affect rather than one of cancellation. This corresponds to our distributional hypothesis \u2014 themore the disparity in distributions, the more is the semantic distance between the target words. The product form (44) also achieves this and is based on this theorem: The product of any two numbers will always be less than or equal to the square of their average. In other words, the more two numbers are close to each other in value, the higher is the ratio of their product to a suitable scaling factor (for example, the square of their average). Note that the difference and division measures give higher values when there is large disparity between the strength of association of co-occurring words with the target words. They are therefore measures of distributional distance and not distributional similarity. The product method gives higher values when the strengths of association are closer, and is a measure of distributional relatedness.\nAlthough all three methods seem intuitive, each produces different distributional similarity values and more importantly, given a set of word pairs, each is likely to rank them differently. For example, consider the division and difference expressions applied to word pairs (w1, w2) and (w3, w4). For simplicity, let there be just one word w\n\u2032 in the context of all the words. Given:\nP(w\u2032|w1) = 0.91\nP(w\u2032|w2) = 0.80\nP(w\u2032|w3) = 0.60\nP(w\u2032|w4) = 0.50\nThe distributional distance between word pairs as per the difference PCM:\nDif (w1,w2) = |0.91\u2212 0.8| = 0.11\nDif (w3,w4) = |0.6\u2212 0.5| = 0.1\nThe distributional distance between word pairs as per the division PCM:\nDiv(w1,w2) =\n\u2223 \u2223 \u2223 \u2223 log 0.91\n0.8\n\u2223 \u2223 \u2223 \u2223 = 0.056\n22\nMohammad and Hirst Distributional Measures of Semantic Distance\nDiv(w3,w4) =\n\u2223 \u2223 \u2223 \u2223 log 0.6\n0.5\n\u2223 \u2223 \u2223 \u2223 = 0.079\nObserve that for the same set of co-occurrence probabilities, the difference-based measure ranks the (w3,w4) pair more distributionally similar (lower distributional distance), while the division-based measure gives lower distributional similarity values for word pairs having large co-occurrence probabilities. This behavior is not intuitive and it remains to be seen, by experimentation, as to which of the three, difference, division or product, yields distributional similarity measures closest to human notions of semantic similarity.\nThe L1 norm is a basic implementation of the difference method. A simple productbased measure of distributional similarity is as proposed below:\nPdtAvg(w1,w2) = \u2211 w\u2208C(w1)\u222aC(w2)\nP(w|w1)\u00d7 P(w|w2)\n( 12 (P(w|w1) + P(w|w2))) 2\n(45)\nThe scaling factor used is the square of the average probability. It can be proved that if the sum of two variables is equal to a constant (k, say), their values must be equal to k/2 in order to get the largest product. Now, let k be equal to the sum of P(w|w1)/(P(w|w1) + P(w|w2)) and P(w|w2)/(P(w|w1) + P(w|w2)). This sum will always be equal to 1 and hence the product (Z) will be largest only when the two numbers are equal i.e. P(w|w1) is equal to P(w|w2). In other words, the farther P(w|w1) and P(w|w2) are from their average, the smaller is the product Z. Therefore, the measure gives high scores for low disparity in strengths of co-occurrence and low scores otherwise. The incorporation of 1/2 in the scaling factor results in a measure that ranges between 0 and 1.\nThe relative entropy\u2013based methods use a weighted division method. Observe that both Kullback-Leibler divergence (formula repeated below for convenience \u2014 equation (46)) and Jensen-Shannon divergence do not take absolute values of the division of cooccurrence probabilities. This will mean that if P(w|w1) > P(w|w2), the logarithm of their ratio will be positive and if P(w|w1) < P(w|w2), the logarithm will be a negative number. Therefore, therewill be a cancellation of contributions to distributional distance by words that have higher co-occurrence probability with respect to w1 and words that have a higher co-occurrence probability with respect to w2. Observe however that the weight P(w|w1)multiplying the logarithmmeans that in general the positive logarithm values receive higher weight than the negative ones, resulting in a net positive score. Therefore, with no absolute value of the logarithm, as in the KLD, the weight plays a crucial role. A modified Kullback-Leibler divergence (DAbs) which incorporates the absolute value is suggested in equation (47):10\nKLD(w1,w2) = D(d1\u2016d2) = \u2211 w\u2208C(w1)\u222aC(w2)\nP(w|w1) log P(w|w1)\nP(w|w2) (46)\n10 It should be noted that any changes to the formula for Kullback-Leibler divergence means that the resulting measure is no longer Kullback-Leibler divergence; these measures are denoted by KLD (and a suitable subscript and/or superscript simply to indicate that they are derived from the Kullback-Leibler divergence.\n23\nKLDAbs(w1,w2) = D Abs(d1\u2016d2) = \u2211\nw\u2208C(w1)\u222aC(w2)\nP(w|w1)\n\u2223 \u2223 \u2223 \u2223 log P(w|w1)\nP(w|w2)\n\u2223 \u2223 \u2223 \u2223\n(47)\nThe updated Jensen-Shannon divergence measure will remain the same as in equation (17), except that it is a manipulation of DAbs and not the original Kullback-Leibler divergence (relative entropy).\nJSDAbs(w1,w2) = D Abs(d1\u2016\n1 2 (d1 + d2)) + D Abs(d2\u2016 1 2 (d1 + d2)) (48)\nNote that once the absolute value of the logarithm is taken, it no longer makes much sense to use an asymmetric weight (P(w|w1)) as in the KLD or as necessary to use a weight at all. Equation (49) shows a simple division-based measure. It is an unweighted form of KLDAbs(w1,w2) and so we will call it KLD Abs Unw.\nKLDAbsUnw(w1,w2) = Div(w1,w2) = \u2211 w\u2208C(w1)\u222aC(w2)\n\u2223 \u2223 \u2223 \u2223 log P(w|w1)\nP(w|w2)\n\u2223 \u2223 \u2223 \u2223\n(49)\nExperimental evaluation of these suggested modifications of Kullback-Leibler divergence will be interesting.\n4.2.2 Weighting the PCMs. The performance of the primary compositional measures may be improved by adding suitable weights to the distributional distance contributed by each co-occurrence. The idea is that some co-occurrences may be better indicators of semantic distance than others. Usually, a formulation of the strength of association of the co-occurring word with the target words is used as weight, the hypothesis being that a strong co-occurrence is likely to be a strong indicator of semantic closeness.\nWeighting the primary compositional measures results in some of the existing measures. For example, as pointed out earlier, the Kullback-Leibler divergence is a weighted form of the division measure (not considering the absolute value). Here, the conditional probability of a co-occurring wordwith respect to the first word (P(w|w1)) is used as the weight. Since the absolute value of the logarithm is not taken and because the weight (P(w|w1)) is dependent on the first word and not the other, Kullback-Leibler divergence is asymmetric. Below is a symmetric weight function:\nweightAvgWt(w1,w2) = 1\n2 (P(w|w1) + P(w|w2)) (50)\nL2 norm is a weighted version of the L1 norm, the weight being P(w|w1)\u2212 P(w|w2). A simple product measure with weights is shown below:\nPdt Avg AvgWt = \u2211\nw\u2208C(w1)\u222aC(w2)\n1 2 (P(w|w1) + P(w|w2))\nP(w|w1)\u00d7 P(w|w2)\n( 12 (P(w|w1) + P(w|w2))) 2\n= \u2211 w\u2208C(w1)\u222aC(w2) P(w|w1)\u00d7 P(w|w2) 1 2 (P(w|w1) + P(w|w2))\n(51)\n24\nMohammad and Hirst Distributional Measures of Semantic Distance\nA possibly better weight function (which is also symmetric) hinges on the following hypothesis: The stronger the association of a co-occurring word with a target word, the better the indicator of semantic properties of the target word it is. Equation (52) shows the corresponding weight function:\nweightMaxWt(w1,w2) = max (P(w|w1), P(w|w2))\n\u2211w\u2032\u2208C(w1)\u222aC(w2)max (P(w \u2032|w1), P(w\u2032|w2))\n(52)\nThe co-occurring word is likely to have different strengths of associations with the two target words. Taking the maximum of the two as the weight (Dagan, Marcus, and Markovitch (1995)) will mean that more weight is given to a cooccurring word if it has high strength of association with any of the two target words. As Dagan, Marcus, and Markovitch (1995) point out, there is strong evidence for dissimilarity if the strength of association with the other target word is much lower than the maximum, and strong evidence of similarity if the strength of association with both target words is more or less the same."}, {"heading": "4.3 Predictors of Semantic Relatedness", "text": "Given a pair of target words, the vocabulary may be divided into three sets: (1) the set of words that co-occur with both target words (common); (2) words that co-occur with exactly one of the two target words (exclusive); (3) words that do not co-occur with either of the two target words. Hindle (1990) uses evidence only from words that cooccur with both target words to determine the distributional similarity. All the other measures discussed in this paper so far also use words that co-occur with just one target word.\nOne can argue that the more there are common co-occurrences between two words, the more they are related. For example, drink and sip may be considered related as they have a number of common co-occurrences such as water, tea and so on. Similarly, drink and chess can be deemed unrelated as words that co-occur with one do not with the other. For example, water and tea do not usually co-occur with chess, while castle and move are not found close to drink. Measures that use all co-occurrences (common and exclusive) tap into this intuitive notion. However, certain strong exclusive co-occurrences can adversely effect the measure. Consider the classic strong coffee vs powerful coffee example (Halliday (1966)). The words strong and powerful are semantically very related. However, the word coffee is likely to co-occur with strong but not with powerful. Further, strong and coffee can be expected to have a large value of association as given by a suitable measure, say PMI. This large PMI value, if used in the distributional relatedness formula, can greatly reduce the final value. Thus it is not clear if the benefit of using all co-occurrences is outweighed by the drawback pointed out.\nA further advantage of using only common co-occurrences is that the KullbackLeibler divergence can now be used without the need of smoothed probabilities.\nKLDCom(w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2)\nP(w|w1) log P(w|w1)\nP(w|w2) (53)\nObserve that we are taking the intersection of the set of co-occurring words instead of union as in the original formula (15).\n25"}, {"heading": "4.4 Capitalizing on asymmetry", "text": "Given a hypernym-hyponym pair (automobile-car, say) asymmetric distributional measures such as the Kullback-Leibler divergence, \u03b1 skew divergence, and the CRMs generate different values as the distributional distance of w1 with w2 as compared to that of w2 with w1. Usually, if w1 is a more generic concept than w2, the measures find w1 to be more distributionally similar to w2 than the other way round (see (Mirkin, Dagan, and Geffet 2007) for work on lexical entailment using the KullbackLeibler divergence).Weeds (2003) argues that this behavior is intuitive as it is more often acceptable to substitute a generic concept in place of a specific one than vice versa, and substitutability is a indicator of semantic similarity.\nOn the other hand, in most cases the notion of asymmetric semantic similarity is counterintuitive, and possibly detrimental. In many natural language tasks, one needs the distance between two words and there is no order information. Further, in case two words share a hypernym-hyponym relation, they are likely to be highly semantically similar. Thus given two words, it may make sense to always choose the higher of the two distributional similarity values suggested by an asymmetric measure as the final distributional similarity between the two. This way an asymmetric measure (SimAsym) can easily be converted into a symmetric one (SimMax), while still capitalizing on the asymmetry to generate more suitable distributional distance values for hypernymhyponym word pairs. Equation (54) states the formula for the proposed conversion. A specific implementation of the KL divergence formula is given in equation (55):\nSimMax(w1,w2) = max(SimAsym(w1,w2), SimAsym(w2,w1)) (54)\nKLDMax(w1,w2) = max(KLD(w1,w2),KLD(w2,w1)) (55)\nAnother way to convert an asymmetric measure of distributional distance into a symmetric one is by taking the average (formula 56) of the two possible similarity values. A specific implementation on the KL divergence formula is given in equations (57) through (60):"}, {"heading": "4.5 Summarizing the distributional measures", "text": "26\nM o h am m ad an d H irst\nD istrib u tio n al M easu res o f S em an tic D istan ce\n2 7\nDistributional measures and their properties (continued). distributional measure compo- symm- strength of measure type sitional PCM formula etric association\nJSD distance division \u2211w\u2208C(w1)\u222aC(w2)\n(\nP(w|w1) log P(w|w1)\n1 2 (P(w|w1)+P(w|w2))\n+ CP\nP(w|w2) log P(w|w2)\n1 2 (P(w|w1)+P(w|w2))\n)\nKLD distance div. \u2211w\u2208C(w1)\u222aC(w2) P(w|w1) log P(w|w1) P(w|w2)\nX CP\nKLDCom distance div. \u2211w\u2208C(w1)\u2229C(w2) P(w|w1) log P(w|w1) P(w|w2)\nX CP\nKLDAbs distance div. \u2211w\u2208C(w1)\u222aC(w2) P(w|w1) \u2223 \u2223 \u2223 log P(w|w1) P(w|w2) \u2223 \u2223 \u2223 X CP\nKLDAvg distance div. 1 2 \u2211w\u2208C(w1)\u222aC(w2) (P(w|w1)\u2212 P(w|w2)) log P(w|w1) P(w|w2)\nCP\nKLDMax distance div. max(KLD(w1,w2),KLD(w2,w1)) CP\nL2 distance difference \u221a \u2211w\u2208C(w1)\u222aC(w2) (P (w|w1)\u2212 P (w|w2)) 2\nCP\nLin closeness X n.a. \u2211(r,w)\u2208 T(w1)\u2229 T(w2)\n(I(w1,r,w)+I(w2,r,w))\n\u2211(r,w\u2032)\u2208 T(w1) I(w1,r,w\u2032)+\u2211(r,w\u2032\u2032)\u2208 T(w2)\nI(w2,r,w\u2032\u2032) PMI\nPdtAvg closeness pdt. \u2211w\u2208C(w1)\u222aC(w2) P(w|w1)\u00d7P(w|w2)\n( 12 (P(w|w1)+P(w|w2))) 2 CP\nPdt Avg AvgWt closeness pdt. \u2211w\u2208C(w1)\u222aC(w2) P(w|w1)\u00d7P(w|w2) 1 2 (P(w|w1)+P(w|w2))\nCP\nNote: For measures that are not compositional, the type of PCM is not applicable.\n2 8\nMohammad and Hirst Distributional Measures of Semantic Distance"}, {"heading": "4.6 Challenges", "text": "4.6.1 Conflation of word senses. The distributional hypothesis (Firth 1957) states that words that occur in similar contexts tend to be semantically close. But when words have more than one sense, it is not at all clear what semantic distance between them actually means. Further, a word in each of its senses is likely to co-occur with different sets of words. For example, bank in the FINANCIAL INSTITUTION sense is likely to co-occur with interest, money, accounts, and so on, whereas the RIVER BANK sense might have words such as river, erosion, and silt around it. Since words that occur together in text tend to refer to senses that are closest in meaning to one another, in most natural language applications, what is needed is the distance between the closest senses of the two target words. However, because distributional measures calculate distance from occurrences of the target word in all its occurrences and hence all its senses, they fail to get the desired result. Also note that the dimensionality reduction inherent to latent semantic analysis, a special kind of distributional measure, has the effect of making the predominant senses of the words more dominant while de-emphasizing the other senses. Therefore, an LSA-based approach will also conflate information from the different senses, and even more emphasis will be placed on the predominant senses. Given the semantically close target nouns play and actor, for example, a distributional measurewill give a score that is some sort of a dominance-based average of the distances between their senses. The noun play has the predominant sense of CHILDREN\u2019S RECREATION (and not DRAMA), so a distributional measure will tend to give the target pair a large (and thus erroneous) distance score. WordNet-based measures do not suffer from this problem as they give distance between concepts, not words.\n4.6.2 Lack of explicitly-encodedworld knowledge and data sparseness. It is becoming increasingly clear that more-accurate results can be achieved in a large number of natural language tasks, including the estimation of semantic distance, by combining corpus statistics with a knowledge source, such as a dictionary, published thesaurus, or WordNet. This is because such knowledge sources capture semantic information about concepts and, to some extent, world knowledge. For example, WordNet, as discussed earlier, has an extensive is-a hierarchy. If it lists one concept, say GERMAN SHEPHERD as a hyponym of another, say DOG, then we can be sure that the two are semantically close. On the other hand, distributional measures do not have access to such explicitly encoded information. Further, unless the corpus used by a distributional measure has sufficient instances of GERMAN SHEPHERD and DOG, it will be unable to deem them semantically close. Since Zipf\u2019s law seems to hold even for the largest of corpora, there will always be words that occur too few times to accurately determine their distributional distance from others.\n4.6.3 Limitations shared with WordNet-based measures. In addition to the limitations described above, which are unique to the knowledge-lean distributional measures, like the knowledge-richmeasures they also suffer fromproblems of requiring the calculation of large distance matrices (as described in Section 2.3.4 earlier) and the reluctance to cross the language barrier (Section 2.3.5)."}, {"heading": "5. A hybrid approach: Distributional measures of concept-distance", "text": "So far we have looked at approaches that exploit the structure of a resource such as WordNet, and corpus-based distributional approaches that make use of co-occurrence statistics. A hybrid approach to semantic distance is one that reconciling the two,\n29\ncombining the information about concepts, explicitly encoded in a linguistic resource, with the information about words, implicitly encoded in text by co-occurrence. Mohammad and Hirst (2006b) and Mohammad et al. (2007) have proposed one such approach that combines corpus statistics with a published thesaurus to give the semantic distance between concepts (rather than words)."}, {"heading": "5.1 The distributional hypothesis for concepts", "text": "As pointed out in Section 4.6.1, words when used in different senses tend to keep different \u201ccompany\" (co-occurring words). For example, consider the contrived but plausible distributional profile of star:\nstar : space 0.21, movie 0.16, famous 0.15, light 0.12, constellation 0.11, heat 0.08, rich 0.07, hydrogen 0.07, . . .\nObserve that it haswords that co-occur both with star\u2019s CELESTIAL BODY sense and star\u2019s CELEBRITY sense. Thus, it is clear that different senses of a wordmay have very different distributional profiles. Using a single DP for the word will mean the union of those profiles. While this might be useful for certain applications, Mohammad and Hirst (2006b) argue that in a number of tasks (including estimating semantic distance), acquiring different DPs for the different senses is not only more intuitive, but also, as they show through experiments, more useful. They show that the closer the distributional profiles of two concepts, the smaller is their semantic distance. Below are example distributional profiles of two senses of star:\nCELESTIAL BODY: space 0.36, light 0.27, constellation 0.11, hydrogen 0.07, . . . CELEBRITY: famous 0.24, movie 0.14, rich 0.14, fan 0.10, . . .\nThe values are the strength of association (usually pointwise mutual information or conditional probability) of the target concept with co-occurring words.\nWe have seen that creating distributional profiles of words involves simple word\u2013 word co-occurrence counts. The creation of DPCs, on the other hand, requires: (1) a concept inventory that list all the concepts and words that refer to them, and (2) counts of how often a concept co-occurs with a word in text. These two aspects will be discussed in the next two sub-sections; however once created, any of the many distributional measures can be used to estimate the distance between the DPs of two target concepts (just as in the case of traditional word-distancemeasures, where distributional measures are used to estimate the distance between the DPs of two target words). For example, here is how Mohammad and Hirst adapt the formula for cosine (described earlier in Section 3.2.1) to estimate distributional distance between two concepts:\nCoscp(c1, c2) = \u2211w\u2208C(c1)\u222aC(c2) (P(w|c1)\u00d7 P(w|c2)) \u221a\n\u2211w\u2208C(c1) P(w|c1) 2 \u00d7\n\u221a\n\u2211w\u2208C(c2) P(w|c2) 2\n(61)\nC(x) is now the set of words that co-occur with concept x within a pre-determined window. The conditional probabilities in the formula are taken from the distributional profiles of concepts.\n30\nMohammad and Hirst Distributional Measures of Semantic Distance"}, {"heading": "5.1.1 A suitable knowledge source and concept inventory.", "text": "Mohammad and Hirst (2006b) use the categories in the Macquarie Thesaurus, 812 in all, as very coarse-grained word senses or concepts, in contrast to approaches that use WordNet or other similarly fine-grained sense inventories.11 Their approach to determining word\u2013concept co-occurrence counts (described in the next sub-section) requires a set of possibly ambiguous words that together unambiguously represent each concept\u2014for which a thesaurus is a natural choice. The use of categories in a thesaurus as concepts means that this approach requires a concept\u2013concept distance matrix of size only 812 \u00d7 812\u2014much smaller than (less than 0.01% of) the matrix required by the WordNet-based and distributional measures."}, {"heading": "5.1.2 Estimating distributional profiles of concepts. A word\u2013category co-occurrence", "text": "matrix (WCCM) is created having word types w as one dimension and thesaurus categories c as another.\nc1 c2 . . . cj . . . w1 m11 m12 . . . m1j . . . w2 m21 m22 . . . m2j . . . ... ... ... . . . ...\n... wi mi1 mi2 . . . mij . . . ... ... ... . . . ... . . .\nThe matrix is populated with co-occurrence counts from a large corpus. A particular cell mij, corresponding to word wi and category or concept cj, is populated with the number of times wi co-occurs (they use a window of \u00b15 words) with any word that has cj as one of its senses (i.e., wi co-occurs with any word listed under concept cj in the thesaurus). This matrix, created after a first pass of the corpus, is called the base word\u2013category co-occurrence matrix (base WCCM). A contingency table for any particular word w and category c can be easily generated from the WCCM by collapsing cells for all other words and categories into one and summing up their frequencies.\nc \u00acc w nwc nw\u00ac \u00acw n\u00acc n\u00ac\u00ac\nA suitable statistic, such as pointwise mutual information or conditional probability, will then yield the strength of association between the word and the category.\nAs the base WCCM is created from unannotated text, it will be noisy but nonetheless capture strong word\u2013category co-occurrence associations reasonably accurately. This is because the errors in determining the true category that a word co-occurs with will be distributed thinly across a number of other categories. (For more discussion of the general principle see Resnik (1998).) A second pass of the corpus is made and the base WCCM is used to disambiguate the words in it. A new bootstrapped WCCM is created such that each cell mij, corresponding to word wi and concept cj, is populated with the number of times wi co-occurs with any word used in sense cj.\n11 It has been suggested for some time now that WordNet is much too fine-grained for certain natural language applications (Agirre and Lopez de Lacalle Lekuona (2003) and citations therein).\n31\nMohammad and Hirst (2006a) showed that this WCCM, created after simple sense disambiguation, better captures word\u2013concept co-occurrence values, and hence strengths of association values, than the base WCCM.\n5.1.3 Mimicking semantic relatedness and semantic similarity. The distributional profiles created by the above methodology are relation-free. This is because (1) all co-occurring words (not just those that are related to the target by certain syntactic relations) are used, and (2) the WCCM, as described above, does not maintain separate counts for the different syntactic relations between the target and co-occurring words. Thus, distributional measures that use these WCCMs will estimate semantic relatedness between concepts. Distributional measures that mimic semantic similarity, which require relation-constrained DPCs, can easily be created from WCCMs that have rows for each word\u2013syntactic relation pair (rather than just for words).\n5.1.4 Performance. Mohammad and Hirst (2006b) evaluate this approach on two tasks: ranking word pairs in order of their semantic distance and correcting real-word spelling errors. On both tasks, distributional concept-distancemeasuresmarkedly outperformed distributional word-distancemeasures. TheWordNet-basedmeasures performed better in the word-pair ranking task, but in the spelling correction task three of the four distributional measures outperformed all WordNet-based measures except the Jiang\u2013 Conrath measure. It should be noted, however, that these experiments evaluated only semantic similarity of noun\u2013noun pairs\u2014for all other part-of-speech combinations and semantic relatedness estimates, the WordNet-based measures are markedly less accurate."}, {"heading": "5.2 Multilinguality", "text": "Some of the best algorithms for semantic distance cannot be applied to most languages because of a lack of high-quality linguistic resources. Mohammad et al. (2007) showed how text in one language L1 can be combined with a knowledge source in another L2 using a bilingual lexicon L1\u2013L2 and a bootstrapping and concept-disambiguation algorithm to create cross-lingual distributional profiles of concepts. These cross-lingual DPCs model co-occurrence distributions of concepts, as per a knowledge source in one language, with words from another language, and obtain semantic distance in a resource-poor language using a knowledge source from a resource-rich one. Crosslingual semantic distance and cross-lingual DPCs are also useful in tasks that inherently involve two or more languages, such as machine translation, multilingual multidocument tasks of clustering, coreference resolution, and information retrieval. We summarize their approach here using German as L1 and English as L2; however, the algorithm is language-pair independent.\n5.2.1 Cross-lingual senses, cross-lingual distributional profiles, and cross-lingual distributional distance. Given a German word wde in context, Mohammad et al. (2007) use a German\u2013English bilingual lexicon to determine its different possible English translations. Each English translation wen may have one or more possible coarse senses, as listed in an English thesaurus. These English thesaurus concepts (cen) will be referred\n32\nto as the cross-lingual candidate senses of the German word wde. Figure 1 depicts examples.12\nAs in the monolingual estimation of distributional concept-distance, the distance between two concepts is calculated by first determining their DPs. However, a concept is now glossed by near-synonymous words in an English thesaurus, whereas its profile is made up of the strengths of association with co-occurring German words. Here are constructed example cross-lingual distributional profiles of the two cross-lingual candidate senses of the German word Stern:13\nCELESTIAL BODY (celestial body, sun, . . . ): Raum 0.36, Licht 0.27, Konstellation 0.11, . . . CELEBRITY (celebrity, hero, . . . ): ber\u00fchmt 0.24, Film 0.14, reich 0.14, . . .\nThe cross-lingual DPCs are created from a cross-lingual word\u2013category co-occurrence matrix without the use of any word-aligned parallel corpora or sense-annotated data (as described in the next subsection). Just as in the case of monolingual distributional concept-distancemeasures, distributional measures can be used to estimate the distance between the cross-lingual DPs of two target concepts. For example, the cosine formula can be adapted to estimate cross-lingual distributional distance between two concepts as shown below:\nCos(cen1 , c en 2 ) =\n\u2211wde\u2208C(cen1 )\u222aC(c en 2 )\n(\nP(wde|cen1 )\u00d7 P(w de|cen2 )\n)\n\u221a\n\u2211wde\u2208C(cen1 ) P(wde|cen1 )\n2 \u00d7 \u221a\n\u2211wde\u2208C(cen2 ) P(wde|cen2 )\n2 (62)\nC(x) is now the set of German words that co-occur with English concept x within a predetermined window. The conditional probabilities in the formula are taken from the cross-lingual DPCs.\n5.2.2 Creating cross-lingual word\u2013category co-occurrence matrix. A German\u2013English cross-lingual word\u2013category co-occurrence matrix has German word types wde as one\n12 They are called \u201ccandidate senses\" because some of the senses of wen might not really be senses of wde. For example, CELESTIAL BODY and CELEBRITY are both senses of the English word star, but the German word Stern can only mean CELESTIAL BODY and not CELEBRITY. Similarly, the German Bank can mean FINANCIAL INSTITUTION or FURNITURE, but not RIVER BANK or JUDICIARY. An automated system has no\nstraightforward method of teasing out the actual cross-lingual senses of wde from those that are an artifact of the translation step.\n13 Vocabulary of German words needed to understand this discussion: Bank: 1. financial institution, 2. bench (furniture); ber\u00fchmt: famous; Film: movie (motion picture); Himmelsk\u00f6rper: heavenly body; Konstellation: constellation; Licht: light; Morgensonne: morning sun; Raum: space; reich: rich; Sonne: sun; Star: star (celebrity); Stern: star (celestial body)\n33\ndimension and English thesaurus concepts cen as another.\ncen1 c en 2 . . . c en j . . .\nwde1 m11 m12 . . . m1j . . . wde2 m21 m22 . . . m2j . . . ... ... ... . . . ... ... wdei mi1 mi2 . . . mij . . . ... ... ... . . . ... . . .\nThe matrix is populated with co-occurrence counts from a large German corpus. A particular cell mij, corresponding to word w de i and concept c en j , is populated with the number of times the German word wdei co-occurs (say a window of \u00b15 words) with any German word having cenj as one of its cross-lingual candidate senses. For example, the Raum\u2013CELESTIAL BODY cell will have the sum of the number of times Raum cooccurs with Himmelsk\u00f6rper, Sonne, Morgensonne, Star, Stern, and so on (see Figure 2). This matrix, created after a first pass of the corpus, is called the cross-lingual base WCCM. A contingency table for any particular German word wde and English category cen can be easily generated from the WCCM by collapsing cells for all other words and categories into one and summing up their frequencies. A suitable statistic, such as PMI or conditional probability, will yield the strength of association between the German word and the English category. Then a new bootstrapped cross-lingual WCCM is created, just as in the monolingual case.\n5.2.3 Performance. Mohammad et al. (2007) evaluated the cross-lingual measures of semantic distance on two tasks: (1) estimating semantic distance between words and ranking the word pairs according to semantic distance, and (2) solving Reader\u2019s Digest \u2018Word Power\u2019 problems. They compared these results with those obtained by conventional state-of-the-art monolingual approaches with and without a knowledge source in the target language L1 (GermaNet). The cross-lingual approach obtained much better results than monolingual approaches that do not use a knowledge source. Further, in both tasks, the cross-lingual measures performed as well if not slightly better than the GermaNet-based monolingual approaches, as well. This shows that the cross-lingual approach is able to keep losses due to the translation step at a minimum, while allowing the use of a superior knowledge source in another language to get better results."}, {"heading": "5.3 Challenges", "text": "Distributional measures of concept-distance have many desirable features of both knowledge-rich approaches and strictly corpus-based approaches\u2014they have the high\n34\nMohammad and Hirst Distributional Measures of Semantic Distance\naccuracies of knowledge-rich approaches, they can measure both semantic relatedness and semantic similarity, and they have a strong corpus-reliance making them domain adaptable. Further, with the cross-lingual approach, a lack of high-quality knowledge source in the target language is no longer a problem. However, certain issues remain.\n5.3.1 Integrating domain-specific terminology. The reliance on a knowledge source means that the approach cannot measure the distance between words not listed in the thesaurus. This is especially a problem for domain-specific jargon, which might not find place in a general purpose knowledge source. Automatic ways of integrating domainspecific terminology into a general purpose knowledge source will be valuable to this end.\n5.3.2 Choosing the right concept-granularity. Mohammad and Hirst (2006b) and Mohammad et al. (2007) have reported results using the categories of the thesaurus as very coarse word senses. This level of granularity has worked well for the tasks they experimentedwith; however, a relatively finer sense inventory may bemore suitable for other tasks. Words within a thesaurus category are grouped into paragraphs; and using them (instead of categories) and determining when this finer-grained sense-inventory is more suitable for use remains to be explored.\n5.3.3 Identifying lexical semantic relations. Word pairs can be semantically close because of any of the classical lexical semantic relations, such as hypernymy, nearsynonymy, antonymy, troponymy, and meronymy, or the innumerable non-classical relations. The various distributional approaches discussed in this paper determine semantic distance without explicitly identifying the nature of the relationship. Already, there is some work on determining lexical entailment (Mirkin, Dagan, and Geffet 2007) and determining near-synonymy (Lin et al. 2003). Identifying antonymy (or more generally, contrasting word-pairs) is especially useful in many natural language tasks, even if it is simply to discard them from a list of distributionally close terms. Also, it will be interesting for measures of semantic distance to characterize the nature of any nonclassical relationship shared by two words\u2014not only determining if two terms are close but also specifying (in some intuitive way) the aspect of meaning they share."}, {"heading": "6. Conclusion", "text": "A large number of important natural language problems, including machine translation, information retrieval, and word sense disambiguation, can be viewed in part as semantic distance problems. Numerous measures of semantic distance exist\u2014those that use a knowledge source and those that rely on corpora. Yet, their use in real-world applications has been limited. In this paper, we investigated how automatic measures can be brought more in line with human notions of semantic distance, how they can be made applicable to a large number of natural language tasks, and how they can be used even for languages deficient in high-quality linguistic resources.\nEven though corpus-based distributional measures of distance have traditionally performed poorly when compared to WordNet-based measures, we have shown that (1) there are a number of reasons that make distributional measures uniquely attractive, and (2) that their potential is yet to be fully realized. Distributional measures can be easily applied to most languages (they can make do even with just raw text) and they can be used to mimic both semantic similarity and semantic relatedness. With this in mind, the paper presented a detailed study of many important distributional\n35\nmeasures, analyzed their limitations, and explained why their performance has been relatively poor so far. Understanding these limitations is crucial in the development of new and better approaches, whether they have a distributional base or otherwise. We concluded the paper with the discussion of a hybrid, yet distinctly distributional approach, that presents one way to more accurately measure distributional distance without compromising too much on essential properties such as the applicability to resource-poor languages."}, {"heading": "Acknowledgments", "text": "We thank Suzanne Stevenson, Gerald Penn, and the Computational Linguistics group at the University of Toronto for helpful discussions. This research is financially supported by the Natural Sciences and Engineering Research Council of Canada and the University of Toronto."}], "references": [{"title": "Clustering WordNet word senses", "author": ["Agirre", "Eneko", "Oier Lopez de Lacalle Lekuona"], "venue": "In Proceedings of the 1st International Conference on Recent", "citeRegEx": "Agirre et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2003}, {"title": "Extended gloss overlaps as a measure of semantic relatedness", "author": ["Banerjee", "Pedersen2003]Banerjee", "Satanjeev", "Ted Pedersen"], "venue": "In Proceedings of the Eighteenth International Joint Conference", "citeRegEx": "Banerjee et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2003}, {"title": "Evaluating WordNet-based measures of semantic distance", "author": ["Budanitsky", "Hirst2006]Budanitsky", "Alexander", "Graeme Hirst"], "venue": "Computational Linguistics,", "citeRegEx": "Budanitsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Budanitsky et al\\.", "year": 2006}, {"title": "Word association norms, mutual information and lexicography", "author": ["Church", "Hanks1990]Church", "Kenneth W", "Patrick Hanks"], "venue": "Computational Linguistics,", "citeRegEx": "Church et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Church et al\\.", "year": 1990}, {"title": "From Distributional to Semantic Similarity", "author": ["R. James"], "venue": "Ph.D. thesis, School of Informatics,", "citeRegEx": "James,? \\Q2004\\E", "shortCiteRegEx": "James", "year": 2004}, {"title": "Similarity-based estimation of word cooccurrence probabilities", "author": ["Dagan", "Lee", "Pereira1994]Dagan", "Ido", "Lillian Lee", "Fernando Pereira"], "venue": "In Proceedings of the 32nd Annual Meeting of the Association", "citeRegEx": "Dagan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1994}, {"title": "Similarity-based methods for word sense disambiguation", "author": ["Dagan", "Lee", "Pereira1997]Dagan", "Ido", "Lillian Lee", "Fernando Pereira"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1997}, {"title": "Similarity-based models of cooccurrence probabilities.Machine Learning, 34(1\u20133):43\u201369", "author": ["Dagan", "Lee", "Pereira1999]Dagan", "Ido", "Lillian Lee", "Fernando Pereira"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1999}, {"title": "Contextual word similarity and estimation from sparse data", "author": ["Dagan", "Marcus", "Markovitch1995] Dagan", "Ido", "Shaul Marcus", "Shaul Markovitch"], "venue": "Computer Speech and Language,", "citeRegEx": "Dagan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1995}, {"title": "A synopsis of linguistic theory 1930\u201355", "author": ["R. John"], "venue": null, "citeRegEx": "John,? \\Q1957\\E", "shortCiteRegEx": "John", "year": 1957}, {"title": "Scaling distributional similarity to large corpora", "author": ["Gorman", "Curran2006]Gorman", "James", "James R. Curran"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics", "citeRegEx": "Gorman et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gorman et al\\.", "year": 2006}, {"title": "Sextant: Exploring unexplored contexts for semantic extraction from syntactic analysis", "author": ["Grefenstette1992]Grefenstette", "Gregory"], "venue": "In Proceedings of the 30th Annual Meeting of the Association", "citeRegEx": "Grefenstette1992.Grefenstette and Gregory.,? \\Q1992\\E", "shortCiteRegEx": "Grefenstette1992.Grefenstette and Gregory.", "year": 1992}, {"title": "Using the structure of a conceptual network in computing semantic relatedness", "author": ["Gurevych2005]Gurevych", "Iryna"], "venue": "In Proceedings of the 2nd International Joint Conference on Natural Language Processing", "citeRegEx": "Gurevych2005.Gurevych and Iryna.,? \\Q2005\\E", "shortCiteRegEx": "Gurevych2005.Gurevych and Iryna.", "year": 2005}, {"title": "Lexis as a linguistic level", "author": ["K. Michael A"], "venue": "In memory of J.R. Firth,", "citeRegEx": "A.,? \\Q1966\\E", "shortCiteRegEx": "A.", "year": 1966}, {"title": "Overview of the first text retrieval conference", "author": ["Harman1993]Harman", "Donna"], "venue": "In Proceedings of the 16th Annual International Association for Computing Machinery Special Interest Group", "citeRegEx": "Harman1993.Harman and Donna.,? \\Q1993\\E", "shortCiteRegEx": "Harman1993.Harman and Donna.", "year": 1993}, {"title": "Noun classification from predicate-argument structures", "author": ["Hindle1990]Hindle", "Donald"], "venue": "In Proceedings of the 28th Annual Meeting of the Association of Computational Linguistics", "citeRegEx": "Hindle1990.Hindle and Donald.,? \\Q1990\\E", "shortCiteRegEx": "Hindle1990.Hindle and Donald.", "year": 1990}, {"title": "Correcting real-word spelling errors by restoring lexical cohesion", "author": ["Hirst", "Budanitsky2005]Hirst", "Graeme", "Alexander Budanitsky"], "venue": "Natural Language Engineering,", "citeRegEx": "Hirst et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hirst et al\\.", "year": 2005}, {"title": "Lexical chains as representations of context for the detection and correction of malapropisms", "author": ["Hirst", "St-Onge1998]Hirst", "Graeme", "David St-Onge"], "venue": null, "citeRegEx": "Hirst et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hirst et al\\.", "year": 1998}, {"title": "Acquiring collocations for lexical choice between near-synonyms", "author": ["Inkpen", "Hirst2002]Inkpen", "Diana", "Graeme Hirst"], "venue": "In SIGLEXWorkshop on Unsupervised Lexical Acquisition,", "citeRegEx": "Inkpen et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Inkpen et al\\.", "year": 2002}, {"title": "Roget\u2019s Thesaurus and semantic similarity", "author": ["Jarmasz", "Szpakowicz2003]Jarmasz", "Mario", "Stan Szpakowicz"], "venue": "In Proceedings of the International Conference on Recent Advances in Natural Language", "citeRegEx": "Jarmasz et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Jarmasz et al\\.", "year": 2003}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["Jiang", "Conrath1997]Jiang", "Jay J", "David W. Conrath"], "venue": "In Proceedings of International Conference on Research", "citeRegEx": "Jiang et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 1997}, {"title": "Introduction to latent semantic analysis", "author": ["Landauer", "Foltz", "Laham1998]Landauer", "Thomas K", "Peter W. Foltz", "Darrell Laham"], "venue": "Discourse Processes,", "citeRegEx": "Landauer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "Combining local context and WordNet similarity for word sense identification", "author": ["Leacock", "Chodorow1998]Leacock", "Claudia", "Martin Chodorow"], "venue": null, "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Measures of distributional similarity", "author": ["Lee1999]Lee", "Lillian"], "venue": "In Proceedings of the 37th conference on Association for Computational Linguistics", "citeRegEx": "Lee1999.Lee and Lillian.,? \\Q1999\\E", "shortCiteRegEx": "Lee1999.Lee and Lillian.", "year": 1999}, {"title": "On the effectiveness of the skew divergence for statistical language analysis", "author": ["Lee2001]Lee", "Lillian"], "venue": "In Proceedings of the Eigth International Workshop on Artificial Intelligence and Statistics", "citeRegEx": "Lee2001.Lee and Lillian.,? \\Q2001\\E", "shortCiteRegEx": "Lee2001.Lee and Lillian.", "year": 2001}, {"title": "Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone", "author": ["Lesk1986]Lesk", "Michael"], "venue": "In Proceedings of the 5th Annual International Conference", "citeRegEx": "Lesk1986.Lesk and Michael.,? \\Q1986\\E", "shortCiteRegEx": "Lesk1986.Lesk and Michael.", "year": 1986}, {"title": "Identifying synonyms among distributionally similar words", "author": ["Lin et al.2003]Lin", "Dekang", "Shaojun Zhao", "Lijuan Qin", "Ming Zhou"], "venue": "In Proceedings of the 18th International Joint", "citeRegEx": "al.2003.Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al.2003.Lin et al\\.", "year": 2003}, {"title": "Unsupervised acquisition of predominant word senses", "author": ["McCarthy et al.2007]McCarthy", "Diana", "Rob Koeling", "Julie Weeds", "John Carroll"], "venue": "Computational linguistics,", "citeRegEx": "al.2007.McCarthy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al.2007.McCarthy et al\\.", "year": 2007}, {"title": "Contextual correlates of semantic similarity", "author": ["Miller", "Charles1991]Miller", "George A", "Walter G. Charles"], "venue": "Language and Cognitive Processes,", "citeRegEx": "Miller et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1991}, {"title": "Integrating pattern-based and distributional similarity methods for lexical entailment acquisition", "author": ["Mirkin", "Dagan", "Geffet2007]Mirkin", "Shachar", "Ido Dagan", "Maayan Geffet"], "venue": null, "citeRegEx": "Mirkin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mirkin et al\\.", "year": 2007}, {"title": "Cross-lingual distributional profiles of concepts for measuring semantic distance", "author": ["Mohammad et al.2007]Mohammad", "Saif", "Iryna Gurevych", "Graeme Hirst", "Torsten Zesch"], "venue": null, "citeRegEx": "al.2007.Mohammad et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al.2007.Mohammad et al\\.", "year": 2007}, {"title": "Determining word sense dominance using a thesaurus", "author": ["Mohammad", "Hirst2006a]Mohammad", "Saif", "Graeme Hirst"], "venue": "In Proceedings of the 11th Conference of the European Chapter of the Association", "citeRegEx": "Mohammad et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2006}, {"title": "Distributional measures of concept-distance: A task-oriented evaluation", "author": ["Mohammad", "Hirst2006b]Mohammad", "Saif", "Graeme Hirst"], "venue": "In Proceedings of the Conference", "citeRegEx": "Mohammad et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2006}, {"title": "Non-classical lexical semantic relations", "author": ["Morris", "Hirst2004]Morris", "Jane", "Graeme Hirst"], "venue": "In Proceedings of the Workshop on Computational Lexical Semantics, Human Language Technology", "citeRegEx": "Morris et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Morris et al\\.", "year": 2004}, {"title": "Using measures of semantic relatedness for word sense disambiguation", "author": ["Patwardhan", "Banerjee", "Pedersen2003] Patwardhan", "Siddharth", "Satanjeev Banerjee", "Ted Pedersen"], "venue": null, "citeRegEx": "Patwardhan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Patwardhan et al\\.", "year": 2003}, {"title": "Experience with a stack decoder-based HMM CSR and back-off n-gram language models", "author": ["B. Douglas"], "venue": "In Proceedings of the Speech and Natural Language Workshop,", "citeRegEx": "Douglas,? \\Q1991\\E", "shortCiteRegEx": "Douglas", "year": 1991}, {"title": "Distributional clustering of English words", "author": ["Pereira", "Tishby", "Lee1993]Pereira", "Fernando", "Naftali Tishby", "Lillian Lee"], "venue": "In Proceedings of the 31st Annual Meeting of the Association of Computational", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Development and application of a metric on semantic nets", "author": ["Rada et al.1989]Rada", "Roy", "Hafedh Mili", "Ellen Bicknell", "Maria Blettner"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "al.1989.Rada et al\\.,? \\Q1989\\E", "shortCiteRegEx": "al.1989.Rada et al\\.", "year": 1989}, {"title": "Word sense discovery based on sense descriptor dissimilarity", "author": ["Rapp2003]Rapp", "Reinhard"], "venue": "In Proceedings of the Machine Translation Summit IX,", "citeRegEx": "Rapp2003.Rapp and Reinhard.,? \\Q2003\\E", "shortCiteRegEx": "Rapp2003.Rapp and Reinhard.", "year": 2003}, {"title": "Using information content to evaluate semantic similarity", "author": ["Resnik1995]Resnik", "Philip"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence", "citeRegEx": "Resnik1995.Resnik and Philip.,? \\Q1995\\E", "shortCiteRegEx": "Resnik1995.Resnik and Philip.", "year": 1995}, {"title": "WordNet and class-based probabilities", "author": ["Resnik1998]Resnik", "Philip"], "venue": null, "citeRegEx": "Resnik1998.Resnik and Philip.,? \\Q1998\\E", "shortCiteRegEx": "Resnik1998.Resnik and Philip.", "year": 1998}, {"title": "Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language", "author": ["Resnik1999]Resnik", "Philip"], "venue": null, "citeRegEx": "Resnik1999.Resnik and Philip.,? \\Q1999\\E", "shortCiteRegEx": "Resnik1999.Resnik and Philip.", "year": 1999}, {"title": "Measuring verb similarity", "author": ["Resnik", "Diab2000]Resnik", "Philip", "Mona Diab"], "venue": "In Proceedings of the 22nd Annual Meeting of the Cognitive Science Society (CogSci", "citeRegEx": "Resnik et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Resnik et al\\.", "year": 2000}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "Goodenough1965] Rubenstein", "Herbert", "John B. Goodenough"], "venue": "Communications of the", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "A cooccurrence-based thesaurus and two applications to information retreival", "author": ["Sch\u00fctze", "Pedersen1997]Sch\u00fctze", "Hinrich", "Jan O. Pedersen"], "venue": "Information Processing and Management,", "citeRegEx": "Sch\u00fctze et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Sch\u00fctze et al\\.", "year": 1997}, {"title": "Mining the Web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["Turney2001]Turney", "Peter"], "venue": "In Proceedings of the Twelfth European Conference on Machine Learning", "citeRegEx": "Turney2001.Turney and Peter.,? \\Q2001\\E", "shortCiteRegEx": "Turney2001.Turney and Peter.", "year": 2001}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Weeds", "Weir", "McCarthy2004]Weeds", "Julie", "David Weir", "Diana McCarthy"], "venue": "In Proceedings of the 20th International Conference", "citeRegEx": "Weeds et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}, {"title": "Measures and Applications of Lexical Distributional Similarity", "author": ["E. Julie"], "venue": "Ph.D. thesis, Department of Informatics,", "citeRegEx": "Julie,? \\Q2003\\E", "shortCiteRegEx": "Julie", "year": 2003}, {"title": "Measuring semantic similarity in the taxonomy of WordNet", "author": ["Yang", "Powers2005]Yang", "Dongqiang", "David Powers"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2005}, {"title": "Verb similarity on the taxonomy of WordNet", "author": ["Yang", "Powers2006]Yang", "Dongqiang", "David Powers"], "venue": "In Proceedings of the Third International WordNet Conference", "citeRegEx": "Yang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2006}, {"title": "Constructing and examining personalized cooccurrence-based thesauri on web", "author": ["Yoshida", "Yukawa", "Kuwabara2003] Yoshida", "Sen", "Takashi Yukawa", "Kazuhiro Kuwabara"], "venue": null, "citeRegEx": "Yoshida et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yoshida et al\\.", "year": 2003}, {"title": "Comparing Wikipedia and German WordNet by evaluating semantic relatedness on multiple datasets", "author": ["Zesch", "Gurevych", "M\u00fchlh\u00e4user2007] Zesch", "Torsten", "Iryna Gurevych", "Max M\u00fchlh\u00e4user"], "venue": null, "citeRegEx": "Zesch et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zesch et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 13, "context": "According to Cruse (1986), a lexical semantic relation is a relation between lexical units\u2014a surface form along with a sense.", "startOffset": 0, "endOffset": 26}, {"referenceID": 13, "context": "According to Cruse (1986), a lexical semantic relation is a relation between lexical units\u2014a surface form along with a sense. As he points out, the number of semantic relations that bind concepts is innumerable; but certain relations, such as hyponymy, meronymy, antonymy, and troponymy, are more systematic and have enjoyed more attention in the linguistics community. However, as Morris and Hirst (2004) point out, these relations are far out-numbered by others, which they call non-classical relations.", "startOffset": 0, "endOffset": 406}, {"referenceID": 13, "context": "Humans are adept at estimating semantic distance; but consider the following questions: How strongly will two people agree/disagree on distance estimates? Will the agreement vary over different sets of concepts? Are we equally good at estimating semantic similarity and semantic relatedness? In our minds, is there a clear distinction between related and unrelated concepts or are concept-pairs spread across the whole range from synonymous to unrelated? Some of the earliest work that begins to address these questions is by Rubenstein and Goodenough (1965). They conducted quantitative experiments with human subjects (51 in all) who were asked to rate 65 English word pairs on a scale from 0.", "startOffset": 212, "endOffset": 559}, {"referenceID": 13, "context": "Humans are adept at estimating semantic distance; but consider the following questions: How strongly will two people agree/disagree on distance estimates? Will the agreement vary over different sets of concepts? Are we equally good at estimating semantic similarity and semantic relatedness? In our minds, is there a clear distinction between related and unrelated concepts or are concept-pairs spread across the whole range from synonymous to unrelated? Some of the earliest work that begins to address these questions is by Rubenstein and Goodenough (1965). They conducted quantitative experiments with human subjects (51 in all) who were asked to rate 65 English word pairs on a scale from 0.0 to 4.0 as per their semantic distance. The word pairs chosen ranged from almost synonymous to unrelated. However, they were all noun pairs and those that were semantically close were also semantically similar; the dataset did not contain word pairs that are semantically related but not semantically similar. The subjects repeated the annotation after two weeks and the new distance values had a Pearson\u2019s correlation r of 0.85 with the old ones. Miller and Charles (1991) also conducted a similar study on 30 word pairs taken from the Rubenstein-Goodenough pairs.", "startOffset": 212, "endOffset": 1170}, {"referenceID": 13, "context": "Humans are adept at estimating semantic distance; but consider the following questions: How strongly will two people agree/disagree on distance estimates? Will the agreement vary over different sets of concepts? Are we equally good at estimating semantic similarity and semantic relatedness? In our minds, is there a clear distinction between related and unrelated concepts or are concept-pairs spread across the whole range from synonymous to unrelated? Some of the earliest work that begins to address these questions is by Rubenstein and Goodenough (1965). They conducted quantitative experiments with human subjects (51 in all) who were asked to rate 65 English word pairs on a scale from 0.0 to 4.0 as per their semantic distance. The word pairs chosen ranged from almost synonymous to unrelated. However, they were all noun pairs and those that were semantically close were also semantically similar; the dataset did not contain word pairs that are semantically related but not semantically similar. The subjects repeated the annotation after two weeks and the new distance values had a Pearson\u2019s correlation r of 0.85 with the old ones. Miller and Charles (1991) also conducted a similar study on 30 word pairs taken from the Rubenstein-Goodenough pairs. These annotations had a high correlation (r = 0.97) with the mean annotations of Rubenstein and Goodenough (1965). Resnik (1999) repeated these experiments and found the inter-annotator correlation (r) to be 0.", "startOffset": 212, "endOffset": 1376}, {"referenceID": 13, "context": "Humans are adept at estimating semantic distance; but consider the following questions: How strongly will two people agree/disagree on distance estimates? Will the agreement vary over different sets of concepts? Are we equally good at estimating semantic similarity and semantic relatedness? In our minds, is there a clear distinction between related and unrelated concepts or are concept-pairs spread across the whole range from synonymous to unrelated? Some of the earliest work that begins to address these questions is by Rubenstein and Goodenough (1965). They conducted quantitative experiments with human subjects (51 in all) who were asked to rate 65 English word pairs on a scale from 0.0 to 4.0 as per their semantic distance. The word pairs chosen ranged from almost synonymous to unrelated. However, they were all noun pairs and those that were semantically close were also semantically similar; the dataset did not contain word pairs that are semantically related but not semantically similar. The subjects repeated the annotation after two weeks and the new distance values had a Pearson\u2019s correlation r of 0.85 with the old ones. Miller and Charles (1991) also conducted a similar study on 30 word pairs taken from the Rubenstein-Goodenough pairs. These annotations had a high correlation (r = 0.97) with the mean annotations of Rubenstein and Goodenough (1965). Resnik (1999) repeated these experiments and found the inter-annotator correlation (r) to be 0.", "startOffset": 212, "endOffset": 1391}, {"referenceID": 13, "context": "1 Ameasure of closeness can be easily converted to a measure of distance by applying a suitable inverse function, or vice versa. Two classes of automatic methods have been traditionally used to determine semantic distance. Knowledge-rich measures of concept-distance, such as those of Jiang and Conrath (1997), Leacock and Chodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it.", "startOffset": 2, "endOffset": 310}, {"referenceID": 13, "context": "1 Ameasure of closeness can be easily converted to a measure of distance by applying a suitable inverse function, or vice versa. Two classes of automatic methods have been traditionally used to determine semantic distance. Knowledge-rich measures of concept-distance, such as those of Jiang and Conrath (1997), Leacock and Chodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it.", "startOffset": 2, "endOffset": 339}, {"referenceID": 13, "context": "1 Ameasure of closeness can be easily converted to a measure of distance by applying a suitable inverse function, or vice versa. Two classes of automatic methods have been traditionally used to determine semantic distance. Knowledge-rich measures of concept-distance, such as those of Jiang and Conrath (1997), Leacock and Chodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it.", "startOffset": 2, "endOffset": 358}, {"referenceID": 13, "context": "1 Ameasure of closeness can be easily converted to a measure of distance by applying a suitable inverse function, or vice versa. Two classes of automatic methods have been traditionally used to determine semantic distance. Knowledge-rich measures of concept-distance, such as those of Jiang and Conrath (1997), Leacock and Chodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it.2 Distributional measures of word-distance (knowledgelean measures), such as cosine and \u03b1-skew divergence (Lee 2001), rely on the distributional hypothesis, which states that two words tend to be semantically close if they occur in similar contexts (Firth 1957). Distributional measures rely simply on text (and possibly some shallow syntactic processing) and can give the distance between any two words that occur at least a few times. The various WordNet-based measures have been widely studied (Budanitsky and Hirst 2006; Patwardhan, Banerjee, and Pedersen 2003). The study of distributional measures on the whole has received much less attention.3 Even though, as Weeds (2003) and Mohammad and Hirst (2006b) show, they perform poorly when compared to WordNet-based measures, the distributional measures of word-distance have many attractive features, including their ability to measure both semantic similarity and semantic relatedness.", "startOffset": 2, "endOffset": 1163}, {"referenceID": 13, "context": "1 Ameasure of closeness can be easily converted to a measure of distance by applying a suitable inverse function, or vice versa. Two classes of automatic methods have been traditionally used to determine semantic distance. Knowledge-rich measures of concept-distance, such as those of Jiang and Conrath (1997), Leacock and Chodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it.2 Distributional measures of word-distance (knowledgelean measures), such as cosine and \u03b1-skew divergence (Lee 2001), rely on the distributional hypothesis, which states that two words tend to be semantically close if they occur in similar contexts (Firth 1957). Distributional measures rely simply on text (and possibly some shallow syntactic processing) and can give the distance between any two words that occur at least a few times. The various WordNet-based measures have been widely studied (Budanitsky and Hirst 2006; Patwardhan, Banerjee, and Pedersen 2003). The study of distributional measures on the whole has received much less attention.3 Even though, as Weeds (2003) and Mohammad and Hirst (2006b) show, they perform poorly when compared to WordNet-based measures, the distributional measures of word-distance have many attractive features, including their ability to measure both semantic similarity and semantic relatedness.", "startOffset": 2, "endOffset": 1194}, {"referenceID": 13, "context": "1 A note about terminology: In many contexts, the term distance measures refers to the complete set of measures (irrespective of what the different ends of the range signify). In certain other contexts (as in this paragraph), distance measures refers only to those measures that give larger values to signify greater distance. The context, usually by its reference to this numeric property or lack thereof will make clear the intended meaning of the term. 2 The nodes in WordNet (synsets) represent word senses and edges between nodes represent semantic relations such as hyponymy and meronymy. 3 See Curran (2004) and Weeds, Weir, and McCarthy (2004) for other work that compares various distributional measures.", "startOffset": 2, "endOffset": 615}, {"referenceID": 13, "context": "1 A note about terminology: In many contexts, the term distance measures refers to the complete set of measures (irrespective of what the different ends of the range signify). In certain other contexts (as in this paragraph), distance measures refers only to those measures that give larger values to signify greater distance. The context, usually by its reference to this numeric property or lack thereof will make clear the intended meaning of the term. 2 The nodes in WordNet (synsets) represent word senses and edges between nodes represent semantic relations such as hyponymy and meronymy. 3 See Curran (2004) and Weeds, Weir, and McCarthy (2004) for other work that compares various distributional measures.", "startOffset": 2, "endOffset": 652}, {"referenceID": 13, "context": "As per Resnik\u2019s formula, given a particular lowest superordinate, the exact positions of the target nodes below it in the hierarchy do not have any effect on the semantic similarity. Intuitively, we would expect that word pairs closer to the lso are more semantically similar than those that are distant. Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms.", "startOffset": 0, "endOffset": 330}, {"referenceID": 13, "context": "As per Resnik\u2019s formula, given a particular lowest superordinate, the exact positions of the target nodes below it in the hierarchy do not have any effect on the semantic similarity. Intuitively, we would expect that word pairs closer to the lso are more semantically similar than those that are distant. Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms.", "startOffset": 0, "endOffset": 345}, {"referenceID": 13, "context": "As per Resnik\u2019s formula, given a particular lowest superordinate, the exact positions of the target nodes below it in the hierarchy do not have any effect on the semantic similarity. Intuitively, we would expect that word pairs closer to the lso are more semantically similar than those that are distant. Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms. The Jiang and Conrath (1997) measure (JC) determines how dissimilar each target concept is from the lso (IC(c1)\u2212 IC(lso(c1, c2)) and IC(c2)\u2212 IC(lso(c1, c2))).", "startOffset": 0, "endOffset": 469}, {"referenceID": 13, "context": "As per Resnik\u2019s formula, given a particular lowest superordinate, the exact positions of the target nodes below it in the hierarchy do not have any effect on the semantic similarity. Intuitively, we would expect that word pairs closer to the lso are more semantically similar than those that are distant. Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms. The Jiang and Conrath (1997) measure (JC) determines how dissimilar each target concept is from the lso (IC(c1)\u2212 IC(lso(c1, c2)) and IC(c2)\u2212 IC(lso(c1, c2))). The final semantic distance between the two concepts is then taken to be the sum of these differences. Lin (1997) (like Resnik) points out that the lso is what is common between the two target concepts and that its information content is the common information between the two concepts.", "startOffset": 0, "endOffset": 713}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs.", "startOffset": 0, "endOffset": 218}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs.", "startOffset": 0, "endOffset": 245}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs. Resnik and Diab (2000) ported several measures which are traditionally applied on the noun hypernymy/hyponymy network (edge counting, and the measures of Resnik (1995), and Lin (1997)) to the relatively shallow verb troponymy network.", "startOffset": 0, "endOffset": 313}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs. Resnik and Diab (2000) ported several measures which are traditionally applied on the noun hypernymy/hyponymy network (edge counting, and the measures of Resnik (1995), and Lin (1997)) to the relatively shallow verb troponymy network.", "startOffset": 0, "endOffset": 458}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs. Resnik and Diab (2000) ported several measures which are traditionally applied on the noun hypernymy/hyponymy network (edge counting, and the measures of Resnik (1995), and Lin (1997)) to the relatively shallow verb troponymy network.", "startOffset": 0, "endOffset": 474}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs. Resnik and Diab (2000) ported several measures which are traditionally applied on the noun hypernymy/hyponymy network (edge counting, and the measures of Resnik (1995), and Lin (1997)) to the relatively shallow verb troponymy network. The two information content\u2013based measures ranked a carefully chosen set of 48 verbs best in order of their semantic distance.4 Yang and Powers (2006) ported their earlier work on nouns (Yang and Powers 2005) to verbs.", "startOffset": 0, "endOffset": 676}, {"referenceID": 13, "context": "As Morris and Hirst (2004) pointed out, a large number of concept pairs, such as STRAWBERRY\u2013CREAM and DOCTOR\u2013SCALPEL, have a non-classical relation between them (STRAWBERRIES are usually eaten with CREAM and a DOCTOR uses a SCALPEL to make an incision).", "startOffset": 0, "endOffset": 27}, {"referenceID": 13, "context": "For example, SPACE and TIME are close in the domain of quantum mechanics but not so much in most others. Ontologies have beenmade for specific domains, which may be used to determine semantic similarity specific to these domains. However, the number of such ontologies is very limited. Some of the more successful WordNet-based measures, such as that of Jiang and Conrath (1997), that rely also on text, do indeed capture domain-specificity to some extent, but the distance values are still largely shaped by the underlying network, which is not domain-specific.", "startOffset": 15, "endOffset": 379}, {"referenceID": 13, "context": "The set of contexts of a target word is usually represented by the set of words in these contexts, their strength of association (SoA) with the target word, and possibly their syntactic relation with the target, for example verb\u2013object, subject\u2013verb, and so on. The strength of cooccurrence association between the target and another word quantifies howmuch more (or less) than chance the two words occur together in text. Commonly used measures of association are conditional probability (CP) and pointwise mutual information (PMI). The distance between the sets of contexts of two target words can be used as a proxy for their semantic distance, as words found in similar contexts tend to be semantically similar\u2014the distributional hypothesis (Firth 1957; Harris 1968). The hypothesis makes intuitive sense, as Budanitsky and Hirst (2006) point out: If two words have many co-occurring words in common, then similar things are being said about both of them and so they are likely to be semantically similar.", "startOffset": 132, "endOffset": 841}, {"referenceID": 13, "context": "The set of contexts of a target word is usually represented by the set of words in these contexts, their strength of association (SoA) with the target word, and possibly their syntactic relation with the target, for example verb\u2013object, subject\u2013verb, and so on. The strength of cooccurrence association between the target and another word quantifies howmuch more (or less) than chance the two words occur together in text. Commonly used measures of association are conditional probability (CP) and pointwise mutual information (PMI). The distance between the sets of contexts of two target words can be used as a proxy for their semantic distance, as words found in similar contexts tend to be semantically similar\u2014the distributional hypothesis (Firth 1957; Harris 1968). The hypothesis makes intuitive sense, as Budanitsky and Hirst (2006) point out: If two words have many co-occurring words in common, then similar things are being said about both of them and so they are likely to be semantically similar. Conversely, if two words are semantically similar, then they are likely to be used in a similar fashion in text and thus end up with many common co-occurrences. For example, the semantically similar bug and insect are expected to have a number of common co-occurring words such as crawl, squash, small, woods, and so on, in a large-enough text corpus. The distributional hypothesis only mentions semantic similarity and not semantic relatedness. This, coupled with the fact that the difference between semantic relatedness and semantic similarity is somewhat nuanced and can be missed, meant that almost all work employing the distributional hypothesis was labeled as estimating semantic similarity. However, it should be noted that distributional measures can be used to estimate both semantic similarity and semantic relatedness. Even though Sch\u00fctze and Pedersen (1997) and Landauer, Foltz, and Laham (1998), for example, use the term similarity and not relatedness, their LSA-based distance measures in fact estimate semantic relatedness and not semantic similarity.", "startOffset": 132, "endOffset": 1882}, {"referenceID": 13, "context": "The set of contexts of a target word is usually represented by the set of words in these contexts, their strength of association (SoA) with the target word, and possibly their syntactic relation with the target, for example verb\u2013object, subject\u2013verb, and so on. The strength of cooccurrence association between the target and another word quantifies howmuch more (or less) than chance the two words occur together in text. Commonly used measures of association are conditional probability (CP) and pointwise mutual information (PMI). The distance between the sets of contexts of two target words can be used as a proxy for their semantic distance, as words found in similar contexts tend to be semantically similar\u2014the distributional hypothesis (Firth 1957; Harris 1968). The hypothesis makes intuitive sense, as Budanitsky and Hirst (2006) point out: If two words have many co-occurring words in common, then similar things are being said about both of them and so they are likely to be semantically similar. Conversely, if two words are semantically similar, then they are likely to be used in a similar fashion in text and thus end up with many common co-occurrences. For example, the semantically similar bug and insect are expected to have a number of common co-occurring words such as crawl, squash, small, woods, and so on, in a large-enough text corpus. The distributional hypothesis only mentions semantic similarity and not semantic relatedness. This, coupled with the fact that the difference between semantic relatedness and semantic similarity is somewhat nuanced and can be missed, meant that almost all work employing the distributional hypothesis was labeled as estimating semantic similarity. However, it should be noted that distributional measures can be used to estimate both semantic similarity and semantic relatedness. Even though Sch\u00fctze and Pedersen (1997) and Landauer, Foltz, and Laham (1998), for example, use the term similarity and not relatedness, their LSA-based distance measures in fact estimate semantic relatedness and not semantic similarity.", "startOffset": 132, "endOffset": 1920}, {"referenceID": 13, "context": "As an additional requirement, the target words must have low PMI with words they do not both cooccur with. Second, Hindle uses the minimum of the PMI between each of the target words and the shared co-occurring word, while Lin uses the sum. Taking the sum has the drawback of not penalizing for a mismatch in strength of co-occurrence, as long as w1 and w2 both co-occur with a word. Hindle (1990) used a portion of the Associated Press news stories (6 million words) to classify the nouns into semantically related classes.", "startOffset": 0, "endOffset": 398}, {"referenceID": 13, "context": "As an additional requirement, the target words must have low PMI with words they do not both cooccur with. Second, Hindle uses the minimum of the PMI between each of the target words and the shared co-occurring word, while Lin uses the sum. Taking the sum has the drawback of not penalizing for a mismatch in strength of co-occurrence, as long as w1 and w2 both co-occur with a word. Hindle (1990) used a portion of the Associated Press news stories (6 million words) to classify the nouns into semantically related classes. Lin (1998b) used his measure to generate a distributional thesaurus from a 64-million-word corpus of theWall Street Journal, San Jose Mercury, and AP Newswire.", "startOffset": 0, "endOffset": 537}, {"referenceID": 13, "context": "Use of Kullback-Leibler distance as the semantic distance metric yielded a 20% improvement in perplexity on the Wall Street Journal and dictation corpora provided by ARPA\u2019s HLT program (Paul 1991). It should be noted here that the use of distributionally similar words to estimate unseen bigram probabilities will likely lead to erroneous results in case of less-preferred and strongly-preferred collocations (word pairs). Inkpen and Hirst (2002) point out that even though words like task and job are semantically very similar, the collocations they form with other words may have varying degrees of usage.", "startOffset": 166, "endOffset": 447}, {"referenceID": 13, "context": "Also, the measure retains the asymmetric nature of the Kullback-Leibler divergence. Lee (2001) shows that \u03b1-skew divergence performs better than Kullback-Leibler divergence in estimating word co-occurrence probabilities.", "startOffset": 0, "endOffset": 95}, {"referenceID": 13, "context": "Also, the measure retains the asymmetric nature of the Kullback-Leibler divergence. Lee (2001) shows that \u03b1-skew divergence performs better than Kullback-Leibler divergence in estimating word co-occurrence probabilities. Weeds (2003) achieves a correlation of 0.", "startOffset": 0, "endOffset": 234}, {"referenceID": 13, "context": "4 Latent Semantic Analysis. Landauer, Foltz, and Laham (1998) proposed Latent semantic analysis (LSA), which can be used to determine distributional distance betweenwords or between sets of words.", "startOffset": 18, "endOffset": 62}, {"referenceID": 13, "context": "4 Latent Semantic Analysis. Landauer, Foltz, and Laham (1998) proposed Latent semantic analysis (LSA), which can be used to determine distributional distance betweenwords or between sets of words.8 Unlike the various approaches described earlier where a word\u2013word co-occurrence matrix is created, the first step of LSA involves the creation of a word\u2013paragraph, word\u2013document, or similar such word-passage matrix, where a passage is some grouping of words. A cell for wordw and passage p is populated with the number of times w occurs in p or, for even better results, a function of this frequency that captures how much information the occurrence of the word in a text passage carries. Next, the dimensionality of this matrix is reduced by applying singular value decomposition (SVD), a standard matrix decomposition technique. This smaller set of dimensions represents abstract (unknown) concepts. Then the original word\u2013passage matrix is recreated, but this time from the reduced dimensions. Landauer, Foltz, and Laham (1998) point out that this results in new matrix cell values that are different from what they were before.", "startOffset": 18, "endOffset": 1029}, {"referenceID": 13, "context": "4 Latent Semantic Analysis. Landauer, Foltz, and Laham (1998) proposed Latent semantic analysis (LSA), which can be used to determine distributional distance betweenwords or between sets of words.8 Unlike the various approaches described earlier where a word\u2013word co-occurrence matrix is created, the first step of LSA involves the creation of a word\u2013paragraph, word\u2013document, or similar such word-passage matrix, where a passage is some grouping of words. A cell for wordw and passage p is populated with the number of times w occurs in p or, for even better results, a function of this frequency that captures how much information the occurrence of the word in a text passage carries. Next, the dimensionality of this matrix is reduced by applying singular value decomposition (SVD), a standard matrix decomposition technique. This smaller set of dimensions represents abstract (unknown) concepts. Then the original word\u2013passage matrix is recreated, but this time from the reduced dimensions. Landauer, Foltz, and Laham (1998) point out that this results in new matrix cell values that are different from what they were before. More specifically, words that are expected to occur more often in a passage than what the original cell values reflect are incremented. Then a standard vector distance measure, such as cosine, that captures the distance between distributions of the two target words is applied. LSA was used by Sch\u00fctze and Pedersen (1997), Turney (2001) and Rapp (2003) to measure distributional distance, with encouraging results.", "startOffset": 18, "endOffset": 1452}, {"referenceID": 13, "context": "4 Latent Semantic Analysis. Landauer, Foltz, and Laham (1998) proposed Latent semantic analysis (LSA), which can be used to determine distributional distance betweenwords or between sets of words.8 Unlike the various approaches described earlier where a word\u2013word co-occurrence matrix is created, the first step of LSA involves the creation of a word\u2013paragraph, word\u2013document, or similar such word-passage matrix, where a passage is some grouping of words. A cell for wordw and passage p is populated with the number of times w occurs in p or, for even better results, a function of this frequency that captures how much information the occurrence of the word in a text passage carries. Next, the dimensionality of this matrix is reduced by applying singular value decomposition (SVD), a standard matrix decomposition technique. This smaller set of dimensions represents abstract (unknown) concepts. Then the original word\u2013passage matrix is recreated, but this time from the reduced dimensions. Landauer, Foltz, and Laham (1998) point out that this results in new matrix cell values that are different from what they were before. More specifically, words that are expected to occur more often in a passage than what the original cell values reflect are incremented. Then a standard vector distance measure, such as cosine, that captures the distance between distributions of the two target words is applied. LSA was used by Sch\u00fctze and Pedersen (1997), Turney (2001) and Rapp (2003) to measure distributional distance, with encouraging results.", "startOffset": 18, "endOffset": 1467}, {"referenceID": 13, "context": "4 Latent Semantic Analysis. Landauer, Foltz, and Laham (1998) proposed Latent semantic analysis (LSA), which can be used to determine distributional distance betweenwords or between sets of words.8 Unlike the various approaches described earlier where a word\u2013word co-occurrence matrix is created, the first step of LSA involves the creation of a word\u2013paragraph, word\u2013document, or similar such word-passage matrix, where a passage is some grouping of words. A cell for wordw and passage p is populated with the number of times w occurs in p or, for even better results, a function of this frequency that captures how much information the occurrence of the word in a text passage carries. Next, the dimensionality of this matrix is reduced by applying singular value decomposition (SVD), a standard matrix decomposition technique. This smaller set of dimensions represents abstract (unknown) concepts. Then the original word\u2013passage matrix is recreated, but this time from the reduced dimensions. Landauer, Foltz, and Laham (1998) point out that this results in new matrix cell values that are different from what they were before. More specifically, words that are expected to occur more often in a passage than what the original cell values reflect are incremented. Then a standard vector distance measure, such as cosine, that captures the distance between distributions of the two target words is applied. LSA was used by Sch\u00fctze and Pedersen (1997), Turney (2001) and Rapp (2003) to measure distributional distance, with encouraging results.", "startOffset": 18, "endOffset": 1483}, {"referenceID": 13, "context": "Asymmetry in substitutability is intuitive, as in many cases it may be acceptable to substitute a word, say dog, with another, say animal, but the reverse is not acceptable as often. SinceWeeds uses substitutability as a measure of semantic similarity, she believes that distributional similarity between two words should reflect this property as well. Hence, like the Kullback-Leibler divergence, all her distributional similarity models are asymmetric. Weeds (2003) extracted verb\u2013object pairs of 2,000 nouns from the British National Corpus (BNC).", "startOffset": 0, "endOffset": 468}, {"referenceID": 13, "context": "Although Lin (1998b) shows that the use of multiple syntactic relations is more beneficial as compared to just one, McCarthy et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "Although Lin (1998b) shows that the use of multiple syntactic relations is more beneficial as compared to just one, McCarthy et al. (2007) show that results obtained using just word co-occurrences produced almost as good results as those obtained using syntactically related words.", "startOffset": 0, "endOffset": 139}, {"referenceID": 13, "context": "As Dagan, Marcus, and Markovitch (1995) point out, there is strong evidence for dissimilarity if the strength of association with the other target word is much lower than the maximum, and strong evidence of similarity if the strength of association with both target words is more or less the same.", "startOffset": 0, "endOffset": 40}, {"referenceID": 13, "context": "All the other measures discussed in this paper so far also use words that co-occur with just one target word. One can argue that the more there are common co-occurrences between two words, the more they are related. For example, drink and sip may be considered related as they have a number of common co-occurrences such as water, tea and so on. Similarly, drink and chess can be deemed unrelated as words that co-occur with one do not with the other. For example, water and tea do not usually co-occur with chess, while castle and move are not found close to drink. Measures that use all co-occurrences (common and exclusive) tap into this intuitive notion. However, certain strong exclusive co-occurrences can adversely effect the measure. Consider the classic strong coffee vs powerful coffee example (Halliday (1966)).", "startOffset": 0, "endOffset": 821}, {"referenceID": 31, "context": "Mohammad and Hirst (2006b) and Mohammad et al. (2007) have proposed one such approach that combines corpus statistics with a published thesaurus to give the semantic distance between concepts (rather than words).", "startOffset": 31, "endOffset": 54}, {"referenceID": 13, "context": "Observe that it haswords that co-occur both with star\u2019s CELESTIAL BODY sense and star\u2019s CELEBRITY sense. Thus, it is clear that different senses of a wordmay have very different distributional profiles. Using a single DP for the word will mean the union of those profiles. While this might be useful for certain applications, Mohammad and Hirst (2006b) argue that in a number of tasks (including estimating semantic distance), acquiring different DPs for the different senses is not only more intuitive, but also, as they show through experiments, more useful.", "startOffset": 63, "endOffset": 353}, {"referenceID": 13, "context": "11 It has been suggested for some time now that WordNet is much too fine-grained for certain natural language applications (Agirre and Lopez de Lacalle Lekuona (2003) and citations therein).", "startOffset": 124, "endOffset": 167}, {"referenceID": 31, "context": "Mohammad et al. (2007) showed how text in one language L1 can be combined with a knowledge source in another L2 using a bilingual lexicon L1\u2013L2 and a bootstrapping and concept-disambiguation algorithm to create cross-lingual distributional profiles of concepts.", "startOffset": 0, "endOffset": 23}, {"referenceID": 31, "context": "Given a German word wde in context, Mohammad et al. (2007) use a German\u2013English bilingual lexicon to determine its different possible English translations.", "startOffset": 36, "endOffset": 59}, {"referenceID": 31, "context": "Mohammad et al. (2007) evaluated the cross-lingual measures of semantic distance on two tasks: (1) estimating semantic distance between words and ranking the word pairs according to semantic distance, and (2) solving Reader\u2019s Digest \u2018Word Power\u2019 problems.", "startOffset": 0, "endOffset": 23}, {"referenceID": 31, "context": "Mohammad and Hirst (2006b) and Mohammad et al. (2007) have reported results using the categories of the thesaurus as very coarse word senses.", "startOffset": 31, "endOffset": 54}], "year": 2012, "abstractText": "The ability to mimic human notions of semantic distance has widespread applications. Some measures rely only on raw text (distributional measures) and some rely on knowledge sources such as WordNet. Although extensive studies have been performed to compare WordNet-based measures with human judgment, the use of distributional measures as proxies to estimate semantic distance has received little attention. Even though they have traditionally performed poorly when compared to WordNet-based measures, they lay claim to certain uniquely attractive features, such as their applicability in resource-poor languages and their ability to mimic both semantic similarity and semantic relatedness. Therefore, this paper presents a detailed study of distributional measures. Particular attention is paid to flesh out the strengths and limitations of both WordNet-based and distributional measures, and how distributional measures of distance can be brought more in line with human notions of semantic distance. We conclude with a brief discussion of recent work on hybrid measures.", "creator": "LaTeX with hyperref package"}}}