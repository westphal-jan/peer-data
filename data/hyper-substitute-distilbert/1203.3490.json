{"id": "1203.3490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Anytime Planning for Decentralized POMDPs using Expectation Maximization", "abstract": "jc pomdps provide an additional device reflecting mixed - user sequential decision making. given n - horizon decpomdps have enjoyed signifcant success, progress remains slow for the n - horizon case mainly due to growing simulation complexity by optimizing stochastic controllers governing agent functions. we present a far new class of algorithms probing the infnite - horizon case, successfully yields the substitution protocol as predicted above a mixture of laws. an architectural feature of this approach is because straightforward adoption that existing experimental techniques in addressing properly solving dec - pomdps and supporting candidate representations transition from input or mixed states to actions. we quickly derive the expectation maximization ( aim ) requirements automatically optimize the joint policy response as allocation. agile modelling consensus sampling find weak em prefer favorably less stable state - s - n - art systems.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (547kb)", "http://arxiv.org/abs/1203.3490v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["akshat kumar", "shlomo zilberstein"], "accepted": false, "id": "1203.3490"}, "pdf": {"name": "1203.3490.pdf", "metadata": {"source": "CRF", "title": "Anytime Planning for Decentralized POMDPs using Expectation Maximization", "authors": ["Akshat Kumar"], "emails": ["akshat@cs.umass.edu", "shlomo@cs.umass.edu"], "sections": [{"heading": null, "text": "Decentralized POMDPs provide an expressive framework for multi-agent sequential decision making. While finite-horizon DECPOMDPs have enjoyed significant success, progress remains slow for the infinite-horizon case mainly due to the inherent complexity of optimizing stochastic controllers representing agent policies. We present a promising new class of algorithms for the infinite-horizon case, which recasts the optimization problem as inference in a mixture of DBNs. An attractive feature of this approach is the straightforward adoption of existing inference techniques in DBNs for solving DEC-POMDPs and supporting richer representations such as factored or continuous states and actions. We also derive the Expectation Maximization (EM) algorithm to optimize the joint policy represented as DBNs. Experiments on benchmark domains show that EM compares favorably against the state-of-the-art solvers."}, {"heading": "1 Introduction", "text": "Decentralized partially observable MDPs (DECPOMDPs) have emerged in recent years as an important framework for modeling sequential decision making by a team of agents [5]. Their expressive power makes it possible to tackle coordination problems in which agents must act based on different partial information about the environment and about each other to maximize a global reward function. Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13]. However, the rich model comes with a price\u2013optimally solving a finite-horizon DEC-POMDP\nis NEXP-Complete [5]. In contrast, finite-horizon POMDPs are PSPACE-complete [12], a strictly lower complexity class that highlights the difficulty of solving DEC-POMDPs.\nRecently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon DEC-POMDPs [11, 8, 16]. However, unlike their point-based counterparts in POMDPs ([15, 17]), they cannot be easily adopted for the infinite-horizon case due to a variety of reasons. For example, POMDP algorithms represent the policy compactly as \u03b1-vectors, whereas all DEC-POMDP algorithms explicitly store the policy as a mapping from observation sequences to actions, making them unsuitable for the infinitehorizon case. In POMDPs, the Bellman equation forms the basis of most point-based solvers, but as Bernstein et. al. [4] highlight, no analogous equation exists for DEC-POMDPs.\nTo alleviate such problems, most infinite-horizon algorithms represent agent policies as finite-state controllers [1, 4]. So far, only two algorithms have shown promise for effectively solving infinite-horizon DEC-POMDPs\u2013decentralized bounded policy iteration (DEC-BPI) [4] and a non-linear programming based approach (NLP) [1]. However, both of these algorithms have significant drawbacks in terms of the representative class of problems that can be handled. For example, solving DEC-POMDPs with continuous state or action spaces is not supported by either of these approaches. Scaling up to structured representations such as factored or hierarchical state-space is difficult due to convergence issues in DEC-BPI and a potential increase in the number of non-linear constraints in the NLP solver. Further, none of the above approaches have been shown to work for more than 2 agents, a significant bottleneck for solving practical problems.\nTo address these shortcomings, we present a promising new class of algorithms which amalgamates planning with probabilistic inference and opens the door\nto the application of rich inference techniques to solving infinite-horizon DEC-POMDPs. Our technique is based on Toussaint et. al.\u2019s approach of transforming the planning problem to its equivalent mixture of dynamic Bayes nets (DBNs) and using likelihood maximization in this framework to optimize the policy value [20, 19]. Earlier work on planning by probabilistic inference can be found in [2]. Such approaches have been successful in solving MDPs and POMDPs [19]. They also easily extend to factored or hierarchical structures [18] and can handle continuous action and state spaces thanks to advanced probabilistic inference techniques [10]. We show how DEC-POMDPs, which are much harder to solve than MDPs or POMDPs, can also be reformulated as a mixture of DBNs. We then present the Expectation Maximization algorithm (EM) to maximize the reward likelihood in this framework. The EM algorithm naturally has the desirable anytime property as it is guaranteed to improve the likelihood (and hence the policy value) with each iteration. We also discuss its extension to large multiagent systems. Our experiments on benchmark domains show that EM compares favorably against the state-of-the-art algorithms, DEC-BPI and NLP-based optimization. It always produces better quality policies than DEC-BPI and for some instances, it nearly doubles the solution quality of the NLP solver. Finally, we discuss potential pitfalls, which are inherent in the EM based approach."}, {"heading": "2 The DEC-POMDP model", "text": "In this section, we introduce the DEC-POMDP model for two agents [5]. Note that finite-horizon DECPOMDPs are NEXP complete even for two agents.\nThe set S denotes the set of environment states, with a given initial state distribution b0. The action set of agent 1 is denoted by A and agent 2 by B. The state transition probability P (s\u2032|s, a, b) depends upon the actions of both the agents. Upon taking the joint action \u3008a, b\u3009 in state s, agents receive the joint reward R(s, a, b). Y is the finite set of observations for agent 1 and Z for agent 2. O(s, ab, yz) denotes the probability P (y, z|s, a, b) of agent 1 observing y \u2208 Y and agent 2 observing z \u2208 Z when the joint action \u3008a, b\u3009 was taken and resulted in state s.\nTo highlight the differences between a single agent POMDP and a DEC-POMDP, we note that in a POMDP an agent can maintain a belief over the environment state. However, in a DEC-POMDP, an agent is not only uncertain about the environment states but also about the actions and observations of the other agent. Therefore in a DEC-POMDP a belief over the states cannot be maintained during execution time.\nThis added uncertainty about other agents in the system make DEC-POMDPs NEXP complete [5].\nWe are concerned with solving infinite-horizon DECPOMDPs with a discount factor \u03b3. We represent the stationary policy of each agent using a fixed size, stochastic finite-state controller (FSC) similar to [1]. An FSC can be described by a tuple \u3008N, \u03c0, \u03bb, \u03bd\u3009. N denotes a finite set of controller nodes n; \u03c0 : N \u2192 \u2206A represents the actions selection model or the probability \u03c0an = P (a|n); \u03bb : N \u00d7 Y \u2192 \u2206N represents the node transition model or the probability \u03bbn\u2032ny = P (n\n\u2032|n, y); \u03bd : N \u2192 \u2206N represents the initial node distribution \u03bdn = P (n). We adopt the convention that nodes of agent 1\u2019s controller are denoted by p and agent 2\u2019s by q. Other problem parameters such as observation function P (y, z|s, a, b) are represented using subscripts as Pyzsab. The value for starting the controllers in nodes \u3008p, q\u3009 at state s is given by:\nV (p, q, s) = \u2211 a,b \u03c0ap\u03c0bq [ Rsab+\n\u03b3 \u2211 s\u2032 Ps\u2032sab \u2211 y,z Pyzs\u2032ab \u2211 p\u2032,q\u2032 \u03bbp\u2032py\u03bbq\u2032qzV (p \u2032, q\u2032, s\u2032) ] .\nThe goal is to set the parameters \u3008\u03c0, \u03bb, \u03bd\u3009 of the agents\u2019 controllers (of some given size) that maximize the expected discounted reward for the initial belief b0:\nV (b0) = \u2211 p,q,s \u03bdp\u03bdqb0(s)V (p, q, s)"}, {"heading": "3 DEC-POMDPs as mixture of DBNs", "text": "In this section, we describe how DEC-POMDPs can be reformulated as a mixture of DBNs such that maximizing the reward likelihood (to be defined later) in this framework is equivalent to optimizing the joint policy. Our approach is based on the framework proposed in [19, 20] to solve Markovian planning problems using probabilistic inference. First we informally describe the intuition behind this reformulation (for details please refer to [19]) and then we describe in detail the steps specific to DEC-POMDPs.\nA DEC-POMDP can be described using a single DBN where the reward is emitted at each time step. However, in our approach, it is described by an infinite mixture of a special type of DBNs where reward is emitted only at the end. For example, Fig. 1(a) describes the DBN for time t = 0. The key intuition is that for the reward emitted at any time step T , we have a separate DBN with the general structure as in Fig. 1(b). Further, to simulate the discounting of rewards, probability of time variable T is set as P (T = t) = \u03b3t(1\u2212 \u03b3). This ensures that \u2211\u221e t=0 pt = 1. In addition, the random variable r shown in Fig. 1(a,b)\n1\n1\n1\nis a binary variable with its conditional distribution (for any time T ) described using the normalized immediate reward as R\u0302sab = P (r = 1|sT = s, aT = a, bT = b) = (Rsab \u2212 Rmin)/(Rmax \u2212 Rmin). This scaling of the reward is the key to transforming the optimization problem from the realm of planning to likelihood maximization as stated below. \u03b8 denotes the parameters \u3008\u03c0, \u03bb, \u03bd\u3009 for each agent\u2019s controller.\nTheorem 1. Let the CPT of binary rewards r be such that R\u0302sab \u221d Rsab and the discounting time prior be set as P (T ) = \u03b3T (1\u2212\u03b3). Then, maximizing the likelihood L\u03b8 = P (r = 1; \u03b8) in the mixture of DBNs is equivalent to optimizing the DEC-POMDP policy. Furthermore, the joint-policy value relates linearly to the likelihood as V \u03b8 = (Rmax \u2212Rmin)L\u03b8/(1\u2212 \u03b3) + \u2211 T \u03b3 TRmin\nThe proof is omitted as it is very similar to that of MDPs and POMDPs [19]. Before detailing the EM algorithm, we describe the DBN representation of DECPOMDPs\u2013the basis for any inference technique.\nThe DBN for any time step T is shown in Fig. 1(b). Every node is a random variable with subscripts indicating time. pi denotes controller nodes for agent 1 and qi for agent 2. The remaining nodes represent the states, actions, and observations. There are four kinds of dependencies induced by the DEC-POMDP model that the DBN must represent:\n\u2022 State transitions: State transitions as a result of the joint action of both agents and the previous state, shown by the DBN\u2019s middle layer.\n\u2022 Controller node transitions (\u03bb): These transitions depend on the last controller state and the most recent individual observation received. They are shown in the top and bottom layers.\n\u2022 Action probabilities (\u03c0): The action taken at any time step t depends on the current controller state. The links between controller nodes (pi or qi) and action nodes (ai or bi) model this. \u2022 Observation probabilities: First, the probability of receiving joint observation yizi depends on the joint action of both agents and the domain\nstate. This relationship is modeled by the DBN nodes labeled yizi. Second, the individual observation each agent receives is a deterministic function of the joint observation. That is Pyy\u2032z\u2032 = P (y|y\u2032z\u2032) = 1 if y = y\u2032 else 0. This is modeled by a link between yizi and the nodes yi and zi.\nTo highlight the differences from a POMDP, Fig. 1(c) shows the DBN for a POMDP. The sheer scale of interactions present in a DEC-POMDP DBN become clear from this comparison, also highlighting the difficulty of solving DEC-POMDPs even approximately. In a POMDP, an agent receives the observation which is affected by the environment state, whereas in a DECPOMDP agents only perceive the individual part of the joint observation yizi. Such differences in the interaction structure make the E and M steps of a DECPOMDP EM very different from that of a POMDP, despite sharing the same high-level principles."}, {"heading": "4 EM algorithm for DEC-POMDPs", "text": "This section describes the EM algorithm [7] for maximizing the reward likelihood in the mixture of DBNs representing DEC-POMDPs. In the corresponding DBNs, only the binary reward is treated as observed (r = 1); all other variables are latent. While maximizing the likelihood, EM yields the DEC-POMDP joint-policy parameters \u03b8. EM also possesses the desirable anytime characteristic as the likelihood (and the policy value which is proportional to the likelihood) is guaranteed to increase per iteration until convergence. We note that EM is not guaranteed to converge to the global optima. However, in the experiments we show that EM almost always achieves similar values as the state-of-the-art NLP based solver [1] and much better than DEC-BPI [4]. The main advantage of using EM lies in its ability to easily generalize to much richer representations than currently possible for DECPOMDPs such as factored or hierarchical controllers, continuous state and action spaces. Another important advantage is the ability to generalize the solver to larger multi-agent systems with more than 2 agents.\nThe E step we derive next is generic as any probabilistic inference technique can be used."}, {"heading": "4.1 E-step", "text": "In the E-step, for the fixed parameter \u03b8, forward messages \u03b1 and backward messages \u03b2 are propagated. First, we define the following Markovian transitions on the (p, q, s) state in the DBN of Fig. 1(b). These transitions are independent of the time t due to the stationary joint policy. We also adopt the convention that for any random variable v, v\u2032 refers to the next time slice and v\u0304 refers to the previous time slice. For any group of variables v, Pt(v,v \u2032) refers to P (vt = v,vt+1 = v \u2032). P (p\u2032, q\u2032, s\u2032|p, q, s) =\u2211 aby\u2032z\u2032 \u03bbp\u2032py\u2032\u03bbq\u2032qz\u2032Py\u2032z\u2032abs\u2032\u03c0ap\u03c0bqPs\u2032sab (1)\n\u03b1t is defined as Pt(p, q, s; \u03b8). It might appear that we need to propagate \u03b1 messages for each DBN separately, but as pointed out in [19], only one sweep is required as the head of the DBN is shared among all the mixture components. That is, \u03b12 is the same for all the T-step DBNs with T \u2265 2. We will omit using \u03b8 as long as it is unambiguous.\n\u03b10(p, q, s) = \u03bdp\u03bdqb0(s)\n\u03b1t(p \u2032, q\u2032, s\u2032) = \u2211 p,q,s P (p\u2032, q\u2032, s\u2032|p, q, s)\u03b1t\u22121(p, q, s)\nIntuitively, \u03b1 messages compute the probability of visiting a particular (p, q, s) state in the DBN as per the current policy. The \u03b2 messages are similar to computing the value of starting the controllers in nodes \u3008p, q\u3009 at state s using dynamic programming. They are propagated backwards and are defined as Pt(r = 1|p, q, s). However, this particular definition would require separate inference for each DBN as for T and T \u2032 step DBN, \u03b2t will be different due to difference in the time-to-go (T \u2212 t and T \u2032 \u2212 t). To circumvent this problem, \u03b2 messages are indexed backward in time as \u03b2\u03c4 (p, q, s) = PT\u2212\u03c4 (r = 1|p, q, s) using the index \u03c4 such that \u03c4 = 0 denotes the time slice t = T . Hence we get:\n\u03b20(p, q, s) = \u2211 ab Rsab\u03c0ap\u03c0bq\n\u03b2\u03c4 (p, q, s) = \u2211 p\u2032,q\u2032,s\u2032 \u03b2\u03c4\u22121(p \u2032, q\u2032, s\u2032)P (p\u2032, q\u2032, s\u2032|p, q, s)\nBased on the \u03b1 and \u03b2 messages we also calculate two more quantities \u03b1\u0302(p, q, s) = \u2211 t P (T = t)\u03b1(p, q, s) and\n\u03b2\u0302(p, q, s) = \u2211 t P (T = t)\u03b2(p, q, s), which will be used in the M-step. The cut-off time for message propagation can either be fixed a priori or be more flexible based on the likelihood accumulation. If \u03b1 messages\nare propagated for t-steps and \u03b2-messages for \u03c4 steps, then the likelihood for T = t+ \u03c4 is given by\nL\u03b8t+\u03c4 = P (r=1|T = t+ \u03c4 ; \u03b8) = \u2211 p,q,s \u03b1t(p, q, s)\u03b2\u03c4 (p, q, s)\nIf both \u03b1 and \u03b2 messages are propagated for k steps and L\u03b82k \u22112k\u22121 T=0 \u03b3\nTL\u03b8T , then the message propagation can be stopped."}, {"heading": "4.1.1 Complexity", "text": "Calculating the Markov transitions on the (p, q, s) chain has complexity O(N4S2A2Y 2), where N is the maximum number of nodes for a controller. The message propagation has complexity O(TmaxN\n4S2). Techniques to effectively reduce this complexity without sacrificing accuracy will be discussed later."}, {"heading": "4.2 M-step", "text": "In the DBNs of Fig. 1(a,b) every variable is hidden except the reward variable. After each M-step, EM provides better estimates of these variables, improving the likelihood L\u03b8 and hence the policy value. For details of EM, we refer to [7]. The parameters to estimate are \u3008\u03c0, \u03bb, \u03bd\u3009 for each agent. For a particular DBN for time T , let L\u0303 = (P,Q,A,B, S) denote the latent variables, where each variable denotes a sequence of length T . That is, P = p0:T . EM maximizes the following expected complete log-likelihood for the DEC-POMDP DBN mixture. \u03b8 denotes the previous parameters and \u03b8? denotes new parameters.\nQ(\u03b8, \u03b8?) = \u2211 T \u2211 L\u0303 P (r=1, L\u0303, T ; \u03b8) logP (r=1, L\u0303, T ; \u03b8?)\nIn the rest of the section, all the derivations refer to the general DBN structure of the DEC-POMDP as in Fig. 1(b). The joint probability of all the variables is: P (r = 1, L\u0303, T ; \u03b8) = P (T ) [ Rsab ] t=T [ T\u220f t=1 \u03c0ap\u03c0bqPss\u0304a\u0304b\u0304\nPyyzPzyzPyzsa\u0304b\u0304\u03bbpp\u0304y\u03bbqq\u0304z ][ \u03c0ap\u03c0bq\u03bdp\u03bdqb0(s) ] t=0\n(2)\nwhere brackets indicate the time slices, i.e.,[ Rsab ] t=T = R(sT , aT , bT ). Taking the log, we get:\nlogP (r = 1, L\u0303, T ) = . . .+ T\u2211 t=0 log \u03c0atpt + T\u2211 t=0 log \u03c0btqt\n+ T\u2211 t=1 log \u03bbptpt\u22121yt + T\u2211 t=1 log \u03bbqtqt\u22121zt\n+ log \u03bdp0 + log \u03bdq0 (3)\nwhere the missing terms represents the quantities independent of \u03b8. As all the policy parameters \u3008\u03c0, \u03bb, \u03bd\u3009 get separated out for each agent in the log above, we first derive the action updates for an agent by substituting Eq. 3 in Q(\u03b8, \u03b8?)"}, {"heading": "4.2.1 Action updates", "text": "The update for action parameters \u03c0?ap for agent 1 can be derived by simplifying Q(\u03b8, \u03b8?) as follows:\nQ(\u03b8, \u03b8?) = \u221e\u2211 T=0 P (T ) T\u2211 t=0 \u2211 a,p [ P (r=1, a, p|T ; \u03b8) ] t log \u03c0?ap\nBy breaking the above summation between t = T and t = 0 to T \u2212 1, we get \u221e\u2211 T=0 P (T ) \u2211 apqbs Rsab\u03c0ap\u03c0bq\u03b1T (p, q, s) log \u03c0 ? ap+ \u221e\u2211 T=0 P (T )\nT\u22121\u2211 t=0 \u2211 app\u2032q\u2032s\u2032 \u03b2T\u2212t\u22121(p \u2032, q\u2032, s\u2032)Pt(a, p, p \u2032, q\u2032, s\u2032) log \u03c0?ap\nIn the above equation, we marginalized the last time slice over the variables (q, b, s). For the intermediate time slice t, we condition upon the variables (p\u2032, q\u2032, s\u2032) in the next time slice t+ 1. We now use the definition of \u03b1\u0302 and move the summation over time T inside for the last time slice and further marginalize over the remaining variables (q, s) in the intermediate slice t: = \u2211\na,p,q,b,s\nRsab\u03c0ap\u03c0bq\u03b1\u0302(p, q, s) log \u03c0 ? ap +\n\u221e\u2211 T=0 P (T ) T\u22121\u2211 t=0 \u2211 ap log \u03c0?ap \u2211 p\u2032q\u2032s\u2032sq \u03b2T\u2212t\u22121(p \u2032, q\u2032, s\u2032)\u03c0ap\nP (p\u2032, q\u2032, s\u2032|a, p, q, s)\u03b1t(p, q, s)\nUpon further marginalizing over the joint observations y\u2032z\u2032 and simplifying we get: = \u2211 ap \u03c0ap log \u03c0 ? ap \u2211 qs [\u2211 b Rsab\u03c0bq\u03b1\u0302(p, q, s) +\n\u2211 p\u2032q\u2032s\u2032y\u2032z\u2032 \u221e\u2211 T=0 P (T ) T\u22121\u2211 t=0 \u03b2T\u2212t\u22121(p \u2032, q\u2032, s\u2032)P (s\u2032|a, q, s)\n\u03bbp\u2032py\u2032\u03bbq\u2032qz\u2032P (y \u2032z\u2032|a, q, s\u2032)\u03b1t(p, q, s) ] We resolve the above time summation, as in [19], based\non the fact that \u2211\u221e T=0 \u2211T\u22121 t=0 f(T \u2212 t \u2212 1)g(t) can be\nrewritten as \u2211\u221e t=0 \u2211\u221e T=t+1 f(T \u2212 t \u2212 1)g(t) and then\nsetting \u03c4 = T \u2212 t\u2212 1 to get \u2211\u221e t=0 g(t) \u2211\u221e \u03c4=0 f(\u03c4). Finally we get: = \u2211 ap \u03c0ap log \u03c0 ? ap \u2211 qs \u03b1\u0302(p, q, s) [\u2211 b Rsab\u03c0bq + \u03b3\n1\u2212 \u03b3\u2211 p\u2032q\u2032s\u2032y\u2032z\u2032 \u03b2\u0302(p\u2032, q\u2032, s\u2032)\u03bbp\u2032py\u2032\u03bbq\u2032qz\u2032P (s \u2032|a, q, s)P (y\u2032z\u2032|a, q, s\u2032) ]\nThe product P (s\u2032|a, q, s)P (y\u2032z\u2032|a, q, s\u2032) can be further simplified by marginalizing out over actions b of agent 2 as follows: = \u2211 ap \u03c0ap log \u03c0 ? ap \u2211 qs \u03b1\u0302(p, q, s) [\u2211 b Rsab\u03c0bq + \u03b3\n1\u2212 \u03b3\u2211 p\u2032q\u2032s\u2032y\u2032z\u2032 \u03b2\u0302(p\u2032, q\u2032, s\u2032)\u03bbp\u2032py\u2032\u03bbq\u2032qz\u2032 \u2211 b Py\u2032z\u2032s\u2032ab\u03c0bqPs\u2032sab ]\nThe above expression is maximized by setting the parameter \u03c0?ap to be:\n\u03c0?ap = \u03c0ap Cp \u2211 qs \u03b1\u0302(p, q, s) [\u2211 b Rsab\u03c0bq + \u03b3\n1\u2212 \u03b3\u2211 p\u2032q\u2032s\u2032y\u2032z\u2032 \u03b2\u0302(p\u2032, q\u2032, s\u2032)\u03bbp\u2032py\u2032\u03bbq\u2032qz\u2032 \u2211 b Py\u2032z\u2032s\u2032ab\u03c0bqPs\u2032sab ] (4)\nwhere Cp is a normalization constant. The action parameters \u03c0?bq of the other agent can be found similarly by the analogue of the previous equation."}, {"heading": "4.2.2 Controller node transition updates", "text": "The update for controller node transition parameters \u03bbpp\u0304y for agent 1 can be found by maximizing Q(\u03b8, \u03b8\n?) w.r.t \u03bb?pp\u0304y as follows.\nQ(\u03b8, \u03b8?)= \u221e\u2211 T=0 P (T ) T\u2211 t=1 \u2211 pp\u0304y [ P (r=1, p, p\u0304, y|T ; \u03b8) ] t log \u03bb?pp\u0304y\nBy marginalizing over the variables (q, s) for the current time slice t, we get\n= \u221e\u2211 T=0 P (T ) T\u2211 t=1 \u2211 pp\u0304ysq log \u03bb?pp\u0304y\u03b2T\u2212t(p, q, s)Pt(p, p\u0304, y, s, q|T ; \u03b8)\nBy further marginalizing over the variables (s\u0304, q\u0304) for the previous time slice of t and over the observations z of the other agent, we get = \u2211 pp\u0304y \u03bbpp\u0304y log \u03bb ? pp\u0304y \u221e\u2211 T=0 P (T ) T\u2211 t=1 \u2211 sqs\u0304q\u0304z \u03b2T\u2212t(p, q, s)\u03bbqq\u0304z\nP (yz|p\u0304, q\u0304, s)P (s|p\u0304, q\u0304, s\u0304)\u03b1t\u22121(p\u0304, q\u0304, s\u0304)\nThe above equation can be further simplified by marginalizing the product P (yz|p\u0304, q\u0304, s)P (s|p\u0304, q\u0304, s\u0304) over actions a and b of both the agents as follows: = \u2211 pp\u0304y \u03bbpp\u0304y log \u03bb ? pp\u0304y \u221e\u2211 T=0 P (T ) T\u2211 t=1 \u2211 sqs\u0304q\u0304z \u03b2T\u2212t(p, q, s)\u03bbqq\u0304z\n\u03b1t\u22121(p\u0304, q\u0304, s\u0304) \u2211 ab PyzsabPss\u0304ab\u03c0ap\u0304\u03c0bq\u0304\nUpon resolving the time summation as before, we get the final M-step estimate:\n\u03bb?pp\u0304y = \u03bbpp\u0304y Cp\u0304y \u2211 sqs\u0304q\u0304z\n\u03b1\u0302(p\u0304, q\u0304, s\u0304)\u03b2\u0302(p, q, s)\u03bbqq\u0304z\u2211 ab PyzsabPss\u0304ab\u03c0ap\u0304\u03c0bq\u0304 (5)\nThe parameters \u03bb?qq\u0304z for the other agent can be found in an analogous way."}, {"heading": "4.2.3 Initial node distribution", "text": "The initial node distribution \u03bd for controller nodes of agent 1 and 2 can be updated as follows. We do not show the complete derivation as it is similar to that of the other parameters.\n\u03bd?p = \u03bdp Cp \u2211 qs \u03b2\u0302(p, q, s)\u03bdqPsb0(s) (6)"}, {"heading": "4.2.4 Complexity and implementation issues", "text": "The complexity of updating all action parameters is O(N4S2AY 2). Updating node transitions requires O(N4S2Y 2 + N2S2Y 2A2). This is relatively high when compared to the POMDP updates requiring O(N2S2AY ) mainly due to the scale of the interactions present in DEC-POMDPs.\nIn our experimental settings, we observed that having a relatively small sized controller (N \u2264 5) suffices to yield good quality solutions. The main contributor to the complexity is the factor S2 as we experimented with large domains having nearly 250 states. The good news is that the structure of the E and M-step equations provides a way to effectively reduce this complexity by significant factor without sacrificing accuracy. For a given state s, joint action \u3008a, b\u3009 and joint observation \u3008y, z\u3009, the possible next states can be calculated as follows: succ(s, a, b, y, z) = {s\u2032|P (s\u2032|s, a, b)P (y, z|s\u2032, a, b) > 0}. For most of the problems, the size of this set is typically a constant k < 10. Such simple reachability analysis and other techniques could speed up the EM algorithm by more than an order of magnitude for large problems. The effective complexity reduces to O(N4SAY 2k) for the action updates and O(N4SY 2k+N2SY 2A2k) for node transitions. Other enhancements of the EM implementation are discussed in Section 6."}, {"heading": "5 Experiments", "text": "We experimented with several standard 2-agent DECPOMDP benchmarks with discount factor 0.9. Complete details of these problems can be found in [1, 4].\nWe compare our approach with the decentralized bounded policy iteration (DEC-BPI) algorithm [4] and a non-convex optimization solver (NLP) [1]. The DEC-BPI algorithm iteratively improves the parameters of a node using a linear program while keeping the other nodes\u2019 parameters fixed. The NLP approach recasts the policy optimization problem as a non-linear program and uses an off-the-shelf solver, Snopt [9], to obtain a solution. We implemented the EM algorithm in JAVA. All our experiments were on a Mac with 4GB RAM and 2.4GHz CPU. Each data point is an average of 10 runs with random initial controller parameters. In terms of solution quality, EM is always better than DEC-BPI and it achieves similar or higher solution quality than NLP. We note that our current implementation is mainly a proof-of-concept; we have not yet implemented several enhancements (discussed later) that could improve the performance of the EM approach. In contrast, the NLP solver [9] is an optimized package and therefore for larger problems is currently faster than the EM approach. The fact that a crude implementation of the EM approach works so well is very encouraging.\nTable 1 shows results for the broadcast channel problem, which has 4 states, 2 actions per agent and 5 observations. This is a networking problem where agents must decide whether or not to send a message on a shared channel and must avoid collision to get a reward. We tested with different controller sizes. On this problem, all the algorithms compare reasonably well, with EM being better than DEC-BPI and very close in value to NLP. The time for NLP is also \u2248 1s.\nFig. 2(a) compares the solution quality of the EM approach against DEC-BPI and NLP for varying controller sizes on the recycling robots problem. In this problem, two robots have the task of picking up cans in an office building. They can search for a small can, a big can or recharge the battery. The large item is only retrievable by the joint action of the two robots. Their goal is to coordinate their actions to maximize the joint reward. EM(2) and NLP(2) show the results with controller size 2 for both agents in Fig. 2(a). For this problem, EM works much better than both DEC-BPI and the NLP approach. EM achieves a value of \u2248 62 for all controller sizes, providing nearly 12% improvement over DEC-BPI (= 55) and 20% improvement over NLP (= 51). Fig. 2(b) shows the time comparisons for EM with different controller sizes. Both the NLP and DEC-BPI take nearly 1s to converge. EM\nwith controller size 2 has comparable performance, but as expected, EM with 4-node controllers takes longer as the complexity of EM is proportional to O(N4).\nFig. 2(c) compares the solution quality of EM on the meeting on a grid problem. In this problem, agents start diagonally across in a 2 \u00d7 2 grid and their goal is to take actions such that they meet each other (i.e., share the same square) as much as possible. As the figure shows, EM provides much better solution quality than the NLP approach. EM achieves a value of \u2248 7, which nearly doubles the solution quality achieved by NLP (= 3.3). DEC-BPI results are not plotted as it performs much worse and achieves a solution quality of 0, essentially unable to improve the policy at all even for large controllers. Both DEC-BPI and NLP take around 1s to converge. Fig. 2(d) shows the time comparison for EM versions. EM with 2-node controllers is very fast and takes < 1s to converge (50 iterations). Also note that in both the cases, EM could run with much larger controller sizes (\u224810), but the increase in size did not provide tangible improvement in solution quality.\nFig. 3 shows the results for the multi-agent tiger problem, involving two doors with a tiger behind one door and a treasure behind the other. Agents should coordinate to open the door leading to the treasure [1]. Fig. 3(a) shows the quality comparisons. EM does not perform well in this case; even after increasing the controller size, it achieves a value of \u221219. NLP works better with large controller sizes. However, this experiment presents an interesting insight into the workings of EM as related to the scaling of the rewards. Recalling the relation between the likelihood and the policy value from Theorem 1, the equation for this problem\nis: V \u03b8 = 1210L\u03b8 \u2212 1004.5. For EM to achieve the same solution as the best NLP setting (= \u22123), the likelihood should be .827. Fig. 3(b) shows that the likelihood EM converges to is .813. Therefore, from EM\u2019s perspective, it is finding a really good solution. Thus, the scaling of rewards has a significant impact (in this case, adverse) on the policy value. This is a potential drawback of the EM approach, which applies to other Markovian planning problems too when using the technique of [19]. Incidently, DEC-BPI performs much worse on this problem and gets a quality of \u221277.\nFig. 4 shows the results for the two largest DECPOMDP domains\u2013box pushing and Mars rovers. In the box pushing domain, agents need to coordinate and push boxes into a goal area. In the Mars rovers domain, agents need to coordinate their actions to perform experiments at multiple sites. Fig. 4(a) shows that EM performs much better than DEC-BPI for every controller size. For controller size 2, EM achieves better quality than NLP with comparable runtime (Fig. 4(b), 500 iterations). However, for the larger controller size (= 3), it achieves slightly lower quality than NLP. For the largest Mars rovers domain (Fig. 4(c)), EM achieves better solution quality (= 9.9) than NLP (= 8.1). However, EM also takes many more iterations to converge than for previous problems and hence, requires more time than NLP. EM is also much better than DEC-BPI, which achieves a quality of \u22121.18 and takes even longer to converge (Fig. 4(d))."}, {"heading": "6 Conclusion and future work", "text": "We present a new approach to solve DEC-POMDPs using inference in a mixture of DBNs. Even a simple implementation of the approach provides good results. Extensive experiments show that EM is always better than DEC-BPI and compares favorably with the stateof-the-art NLP solver. The experiments also highlight two potential drawbacks of the EM approach: the adverse effect of reward scaling on solution quality and slow convergence rate for large problems. We are currently addressing the runtime issue by parallelizing the algorithm. For example, \u03b1 and \u03b2 can be propagated in parallel. Even updating each node\u2019s parameters can\nbe done in parallel for each iteration. Furthermore, the structure of EM\u2019s update equations is very amenable to Google\u2019s Map-Reduce paradigm [6], allowing each parameter to be computed by a cluster of machines in parallel using Map-Reduce. Such scalable techniques will certainly make our approach many times faster than the current serial implementation. We are also investigating how a different scaling of rewards affects the convergence properties of EM.\nThe main benefit of the EM approach is that it opens up the possibility of using powerful probabilistic inference techniques to solve decentralized planning problems. Using a graphical DBN structure, EM can easily generalize to richer representations such as factored or hierarchical controllers, or continuous state and action spaces. Unlike the existing techniques, EM can easily extend to larger multi-agent systems with more than 2 agents. The ND-POMDP model [13] is a class of DECPOMDPs specifically designed to support large multiagent systems. It makes some restrictive yet realistic assumptions such as locality of interaction among agents, and transition and observation independence. EM can naturally exploit such independence structure in the DBN and scale to larger multi-agent systems, something that current infinite-horizon algorithms fail to achieve. Hence the approach we introduce offers great promise to overcome the shortcomings of the prevailing approaches to multi-agent planning."}, {"heading": "Acknowledgments", "text": "Support for this work was provided in part by the National Science Foundation Grant IIS-0812149 and by the Air Force Office of Scientific Research Grant FA9550-08-1-0181."}], "references": [{"title": "Optimizing fixed-size stochastic controllers for POMDPs and decentralized POMDPs", "author": ["C. Amato", "D.S. Bernstein", "S. Zilberstein"], "venue": "JAAMAS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Planning by probabilistic inference", "author": ["H. Attias"], "venue": "In Workshop on AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Solving transition independent decentralized markov decision processes", "author": ["R. Becker", "S. Zilberstein", "V. Lesser", "C.V. Goldman"], "venue": "JAIR, 22:423\u2013455,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Policy iteration for decentralized control of Markov decision processes", "author": ["D.S. Bernstein", "C. Amato", "E.A. Hansen", "S. Zilberstein"], "venue": "JAIR, 34:89\u2013132,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "J. MOR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "MapReduce: a flexible data processing", "author": ["J. Dean", "S. Ghemawat"], "venue": "tool. CACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical society, Series B,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "Point-based incremental pruning heuristic for solving finite-horizon DEC-POMDPs", "author": ["J.S. Dibangoye", "A.-I. Mouaddib", "B. Chaib-draa"], "venue": "In AAMAS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "SNOPT: An SQP algorithm for large-scale constrained optimization. SIOPT", "author": ["P.E. Gill", "W. Murray", "M.A. Saunders"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "New inference strategies for solving Markov decision processes using reversible jump MCMC", "author": ["M. Hoffman", "H. Kueck", "N. de Freitas", "A. Doucet"], "venue": "In UAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Point based backup for decentralized POMDPs: Complexity and new algorithms", "author": ["A. Kumar", "S. Zilberstein"], "venue": "In AAMAS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Complexity of finite-horizon Markov decision process problems", "author": ["M. Mundhenk", "J. Goldsmith", "C. Lusena", "E. Allender"], "venue": "J. ACM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs", "author": ["R. Nair", "P. Varakantham", "M. Tambe", "M. Yokoo"], "venue": "In AAAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Optimal and approximate Q-value functions for decentralized POMDPs", "author": ["F.A. Oliehoek", "M.T.J. Spaan", "N.A. Vlassis"], "venue": "JAIR, 32:289\u2013353,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Anytime pointbased approximations for large POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "JAIR, 27:335\u2013380,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Memory-bounded dynamic programming for DEC-POMDPs", "author": ["S. Seuken", "S. Zilberstein"], "venue": "In IJCAI, pages 2009\u20132015,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Heuristic search value iteration for POMDPs", "author": ["T. Smith", "R. Simmons"], "venue": "In UAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Hierarchical POMDP controller optimization by likelihood maximization", "author": ["M. Toussaint", "L. Charlin", "P. Poupart"], "venue": "In UAI,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Probabilistic inference for solving (PO)MDPs", "author": ["M. Toussaint", "S. Harmeling", "A. Storkey"], "venue": "Technical Report EDIINF-RR-0934,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Probabilistic inference for solving discrete and continuous state markov decision processes", "author": ["M. Toussaint", "A.J. Storkey"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}], "referenceMentions": [{"referenceID": 4, "context": "Decentralized partially observable MDPs (DECPOMDPs) have emerged in recent years as an important framework for modeling sequential decision making by a team of agents [5].", "startOffset": 167, "endOffset": 170}, {"referenceID": 2, "context": "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].", "startOffset": 94, "endOffset": 97}, {"referenceID": 13, "context": "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].", "startOffset": 166, "endOffset": 169}, {"referenceID": 12, "context": "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].", "startOffset": 217, "endOffset": 221}, {"referenceID": 4, "context": "However, the rich model comes with a price\u2013optimally solving a finite-horizon DEC-POMDP is NEXP-Complete [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 11, "context": "In contrast, finite-horizon POMDPs are PSPACE-complete [12], a strictly lower complexity class that highlights the difficulty of solving DEC-POMDPs.", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "Recently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon DEC-POMDPs [11, 8, 16].", "startOffset": 117, "endOffset": 128}, {"referenceID": 7, "context": "Recently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon DEC-POMDPs [11, 8, 16].", "startOffset": 117, "endOffset": 128}, {"referenceID": 15, "context": "Recently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon DEC-POMDPs [11, 8, 16].", "startOffset": 117, "endOffset": 128}, {"referenceID": 14, "context": "However, unlike their point-based counterparts in POMDPs ([15, 17]), they cannot be easily adopted for the infinite-horizon case due to a variety of reasons.", "startOffset": 58, "endOffset": 66}, {"referenceID": 16, "context": "However, unlike their point-based counterparts in POMDPs ([15, 17]), they cannot be easily adopted for the infinite-horizon case due to a variety of reasons.", "startOffset": 58, "endOffset": 66}, {"referenceID": 3, "context": "[4] highlight, no analogous equation exists for DEC-POMDPs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "To alleviate such problems, most infinite-horizon algorithms represent agent policies as finite-state controllers [1, 4].", "startOffset": 114, "endOffset": 120}, {"referenceID": 3, "context": "To alleviate such problems, most infinite-horizon algorithms represent agent policies as finite-state controllers [1, 4].", "startOffset": 114, "endOffset": 120}, {"referenceID": 3, "context": "So far, only two algorithms have shown promise for effectively solving infinite-horizon DEC-POMDPs\u2013decentralized bounded policy iteration (DEC-BPI) [4] and a non-linear programming based approach (NLP) [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "So far, only two algorithms have shown promise for effectively solving infinite-horizon DEC-POMDPs\u2013decentralized bounded policy iteration (DEC-BPI) [4] and a non-linear programming based approach (NLP) [1].", "startOffset": 202, "endOffset": 205}, {"referenceID": 19, "context": "\u2019s approach of transforming the planning problem to its equivalent mixture of dynamic Bayes nets (DBNs) and using likelihood maximization in this framework to optimize the policy value [20, 19].", "startOffset": 185, "endOffset": 193}, {"referenceID": 18, "context": "\u2019s approach of transforming the planning problem to its equivalent mixture of dynamic Bayes nets (DBNs) and using likelihood maximization in this framework to optimize the policy value [20, 19].", "startOffset": 185, "endOffset": 193}, {"referenceID": 1, "context": "Earlier work on planning by probabilistic inference can be found in [2].", "startOffset": 68, "endOffset": 71}, {"referenceID": 18, "context": "Such approaches have been successful in solving MDPs and POMDPs [19].", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "They also easily extend to factored or hierarchical structures [18] and can handle continuous action and state spaces thanks to advanced probabilistic inference techniques [10].", "startOffset": 63, "endOffset": 67}, {"referenceID": 9, "context": "They also easily extend to factored or hierarchical structures [18] and can handle continuous action and state spaces thanks to advanced probabilistic inference techniques [10].", "startOffset": 172, "endOffset": 176}, {"referenceID": 4, "context": "In this section, we introduce the DEC-POMDP model for two agents [5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "This added uncertainty about other agents in the system make DEC-POMDPs NEXP complete [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "We represent the stationary policy of each agent using a fixed size, stochastic finite-state controller (FSC) similar to [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 18, "context": "Our approach is based on the framework proposed in [19, 20] to solve Markovian planning problems using probabilistic inference.", "startOffset": 51, "endOffset": 59}, {"referenceID": 19, "context": "Our approach is based on the framework proposed in [19, 20] to solve Markovian planning problems using probabilistic inference.", "startOffset": 51, "endOffset": 59}, {"referenceID": 18, "context": "First we informally describe the intuition behind this reformulation (for details please refer to [19]) and then we describe in detail the steps specific to DEC-POMDPs.", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "The proof is omitted as it is very similar to that of MDPs and POMDPs [19].", "startOffset": 70, "endOffset": 74}, {"referenceID": 6, "context": "This section describes the EM algorithm [7] for maximizing the reward likelihood in the mixture of DBNs representing DEC-POMDPs.", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "However, in the experiments we show that EM almost always achieves similar values as the state-of-the-art NLP based solver [1] and much better than DEC-BPI [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "However, in the experiments we show that EM almost always achieves similar values as the state-of-the-art NLP based solver [1] and much better than DEC-BPI [4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 18, "context": "It might appear that we need to propagate \u03b1 messages for each DBN separately, but as pointed out in [19], only one sweep is required as the head of the DBN is shared among all the mixture components.", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "For details of EM, we refer to [7].", "startOffset": 31, "endOffset": 34}, {"referenceID": 18, "context": "We resolve the above time summation, as in [19], based on the fact that \u2211\u221e T=0 \u2211T\u22121 t=0 f(T \u2212 t \u2212 1)g(t) can be rewritten as \u2211\u221e t=0 \u2211\u221e T=t+1 f(T \u2212 t \u2212 1)g(t) and then setting \u03c4 = T \u2212 t\u2212 1 to get \u2211\u221e t=0 g(t) \u2211\u221e \u03c4=0 f(\u03c4).", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "Complete details of these problems can be found in [1, 4].", "startOffset": 51, "endOffset": 57}, {"referenceID": 3, "context": "Complete details of these problems can be found in [1, 4].", "startOffset": 51, "endOffset": 57}, {"referenceID": 3, "context": "We compare our approach with the decentralized bounded policy iteration (DEC-BPI) algorithm [4] and a non-convex optimization solver (NLP) [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "We compare our approach with the decentralized bounded policy iteration (DEC-BPI) algorithm [4] and a non-convex optimization solver (NLP) [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "The NLP approach recasts the policy optimization problem as a non-linear program and uses an off-the-shelf solver, Snopt [9], to obtain a solution.", "startOffset": 121, "endOffset": 124}, {"referenceID": 8, "context": "In contrast, the NLP solver [9] is an optimized package and therefore for larger problems is currently faster than the EM approach.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "Agents should coordinate to open the door leading to the treasure [1].", "startOffset": 66, "endOffset": 69}, {"referenceID": 18, "context": "This is a potential drawback of the EM approach, which applies to other Markovian planning problems too when using the technique of [19].", "startOffset": 132, "endOffset": 136}, {"referenceID": 5, "context": "Furthermore, the structure of EM\u2019s update equations is very amenable to Google\u2019s Map-Reduce paradigm [6], allowing each parameter to be computed by a cluster of machines in parallel using Map-Reduce.", "startOffset": 101, "endOffset": 104}, {"referenceID": 12, "context": "The ND-POMDP model [13] is a class of DECPOMDPs specifically designed to support large multiagent systems.", "startOffset": 19, "endOffset": 23}], "year": 2010, "abstractText": "Decentralized POMDPs provide an expressive framework for multi-agent sequential decision making. While finite-horizon DECPOMDPs have enjoyed significant success, progress remains slow for the infinite-horizon case mainly due to the inherent complexity of optimizing stochastic controllers representing agent policies. We present a promising new class of algorithms for the infinite-horizon case, which recasts the optimization problem as inference in a mixture of DBNs. An attractive feature of this approach is the straightforward adoption of existing inference techniques in DBNs for solving DEC-POMDPs and supporting richer representations such as factored or continuous states and actions. We also derive the Expectation Maximization (EM) algorithm to optimize the joint policy represented as DBNs. Experiments on benchmark domains show that EM compares favorably against the state-of-the-art solvers.", "creator": "TeX"}}}