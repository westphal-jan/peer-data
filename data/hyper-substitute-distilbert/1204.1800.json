{"id": "1204.1800", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Apr-2012", "title": "On Power-Law Kernels, Corresponding Reproducing Kernel Hilbert Space and Applications", "abstract": "constructive role via simulations include direct unto numerical learning. motivated applying the instability of power law distributions in arithmetic, simulation and learning, applying this paper, the propose a vector - law generalization unto the maxim kernel. this integral is termed, q - gaussian curvature, which specifies a power - law topology assumed somewhere context including nonextensive statistical mechanics. we prove that the proposed boundary is positive definite, and provide some insights regarding the extended generic kernel in processes ( rkhs ). we also study practical computing some taylor - curve kernels encompassing inference, regression and aggregation, we present internationally notable results.", "histories": [["v1", "Mon, 9 Apr 2012 05:53:27 GMT  (775kb,D)", "http://arxiv.org/abs/1204.1800v1", "7 pages, 3 figures, 4 tables"], ["v2", "Mon, 1 Apr 2013 07:12:43 GMT  (191kb,D)", "http://arxiv.org/abs/1204.1800v2", "7 pages, 3 figures, 3 tables"]], "COMMENTS": "7 pages, 3 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT stat.ML", "authors": ["debarghya ghoshdastidar", "ambedkar dukkipati"], "accepted": true, "id": "1204.1800"}, "pdf": {"name": "1204.1800.pdf", "metadata": {"source": "CRF", "title": "On q-Gaussian kernel and its Reproducing Kernel Hilbert Space", "authors": ["Debarghya Ghoshdastidar", "Ambedkar Dukkipati"], "emails": ["gdebarghya@ee.iisc.ernet.in", "ambedkar@csa.iisc.ernet.in"], "sections": [{"heading": "1. INTRODUCTION", "text": "Power-law distributions were first studied in economics (Pareto, 1906) in the context of distribution of wealth. Later power-law behavior was observed in various fields such as physics, biology, computer science etc. Baraba\u0301si & Albert (1999) observed this behavior in the World Wide Web. Goldwater et al. (2011) used these distributions to study language models.\nIn recent years, interest in generalized information measures has increased dramatically, one reason being while maximization of Shannon entropy gives rise to exponential distributions, these measures give power-law distributions. One such generalization is known as nonextensive entropy introduced by Havrda & Charva\u0301t (1967), and then studied by Tsallis (1988) in statistical mechanics. The Shannon-Khinchin axioms of Shannon entropy have been generalized to this case (Suyari, 2004), and this entropy functional has been studied in information theory, statistics and many other fields. Tsallis entropy has been used to study power-law behavior in different cases like earthquakes and network traffic (Abe & Suzuki, 2003, 2005).\nIn kernel based machine learning (Scholko\u0308pf & Smola, 2002), positive definite kernels are considered as a measure of similarity between points. The choice of kernel is critical to the performance of the learning algorithms, and hence, many kernels have been studied in literature (Cristianini & Shawe-Taylor, 2004). One of the most common kernel used in practical applications is the Gaussian kernel. Nonextensive kernels on probability measures, based on Tsallis divergences, have been proposed by Martins et al. (2009).\nIn this paper, we propose a new kernel based on q-Gaussian, which is a generalization of the Gaussian distribution, obtained\nby maximizing Tsallis entropy under certain moment constraints. The power-law nature of this distribution has been studied by Sato (2010). In fact, the value of q controls the nature of the power-law tails.\nWe prove that the proposed kernel is positive definite over a range of values of q. We demonstrate the effect of this kernel by applying it to various machine learning problems, like SVMs, regression and kernel k-means clustering. We provide results indicating that in some cases, the q-Gaussians may outperform the Gaussian kernel for certain values of q. Further, we also discuss about the RKHS corresponding to this kernel.\nThe rest of the paper is organized as follows. Some of the preliminaries regarding q-Gaussian distributions and kernels have been discussed in Section 2. In Section 3, the q-Gaussian kernel is proposed, and its important properties are presented. Section 4 presents numerical evidence regarding the RKHS of the proposed kernel. Some results comparing q-Gaussian kernel with Gaussian kernel for different classification tasks are presented in Section 5. Finally, Section 6 provides the concluding remarks."}, {"heading": "2. BACKGROUND AND PRELIMINARIES", "text": "A. q-Gaussian distribution\nTsallis entropy can be obtained by generalizing the information of a single event in the definition of Shannon entropy as shown by Tsallis (1988), where logarithm is replaced with q-logarithm defined as lnq x = x 1\u2212q\u22121 1\u2212q q \u2208 R, q > 0, q 6= 1. Tsallis entropy in the continuous case is defined as (Dukkipati et al., 2007)\nHq(p) =\n1\u2212 \u222b X [p(x)]qdx\nq \u2212 1 , q \u2208 R, q > 0, q 6= 1, (1)\nThis function produces the differential Shannon entropy functional as q \u2192 1. It is called nonextensive because of its pseudo-additive nature (Tsallis, 1988).\nKullback\u2019s minimum discrimination theorem (Kullback, 1959) establishes important connections between statistics and information theory. A special case is Jaynes\u2019 maximum entropy principle (Jaynes, 1957), by which exponential distributions can be obtained by maximizing Shannon entropy functional, subject to some moment constraints. Using the\nar X\niv :1\n20 4.\n18 00\nv1 [\ncs .L\nG ]\n9 A\npr 2\n01 2\nsame principle, maximizing Tsallis entropy under the following constraints\nq-mean \u3008x\u3009q :=\n\u222b R x[p(x)]qdx\u222b\nR [p(x)]qdx\n= \u00b5, and (2)\nq-variance \u3008x2\u3009q :=\n\u222b R (x\u2212 \u00b5)2[p(x)]qdx\u222b\nR [p(x)]qdx\n= \u03c32, (3)\nresults in a distribution known as q-Gaussian distribution (Prato & Tsallis, 1999). This is of the form\nGq,\u03b2(x) = \u039bq\n\u03c3 \u221a 3\u2212 q expq\n( \u2212 (x\u2212 \u00b5) 2\n(3\u2212 q)\u03c32\n) , (4)\nwhere the q-exponential, expq(z), is expressed as\nexpq(z) = [ 1 + (1\u2212 q)z ] 1 1\u2212q + , (5)\nand \u039bq is the normalizing constant of the form\n\u039bq =  \u221a 1\u2212 q\u221a \u03c0 \u0393 ( 5\u22123q 2(1\u2212q) ) \u0393 ( 2\u2212q 1\u2212q ) for q < 1, 1\u221a \u03c0 for q = 1, and \u221a q \u2212 1\u221a \u03c0 \u0393 ( 1 q\u22121 ) \u0393 (\n3\u2212q 2(1\u2212q)\n) for 1 < q < 3. (6)\nThe condition y+ = max(y, 0) in (5) is called the Tsallis cut-off condition, which ensures the existence of the qexponential. As a special case, Gaussian distribution is a special case of (4) as q \u2192 1."}, {"heading": "B. Kernels in Machine Learning", "text": "One of the fundamental problems in machine learning is to obtain a map between an input space X and an output space Y . The objective varies depending on the nature of the problem.\nIn linear methods of learning, the Euclidian distance between data points is used to distinguish them. In other words, the dot product between two vectors is used as a measure of similarity between them. But this approach does not work well when the data is not linearly separable.\nIn such cases, a better method, known as kernel based approach (Scholko\u0308pf & Smola, 2002), is to transform the data into a higher dimensional space H through a mapping \u03a6 : X 7\u2192 H, such that the data is linearly separable in H. The similarity between two points in this transformed space, given by a kernel function K : X \u00d7 X 7\u2192 R defined as\nK(x, y) = \u03a6(x)T\u03a6(y) x, y \u2208 X . (7)\nBerg et al. (1984) has shown that for any symmetric function K, there exists a mapping \u03a6 such that (7) holds if and only\nif K is positive definite (p.d.), i.e., given any set of points {x1, x2, . . . , xn} \u2282 X , the n \u00d7 n matrix K, such that Kij = K(xi, xj), is positive semi-definite. This implies that any p.d. kernel can be used in learning theory."}, {"heading": "3. THE PROPOSED KERNEL", "text": "Based on multi-dimensional expression of q-Gaussian (4), proposed by Vignat & Plastino (2007), for a given q \u2208 R, we define the q-Gaussian kernel Kq : X \u00d7 X 7\u2192 R as\nKq(x, y) = expq\n( \u2212 \u2016x\u2212 y\u2016 2\n(3\u2212 q)\u03c32\n) for all x, y \u2208 X , (8)\nwhere X \u2282 RN , and q, \u03c3 \u2208 R are two parameters controlling the behavior of the kernel, satisfying the conditions q 6= 1, q 6= 3 and \u03c3 6= 0. For 1 < q < 3, the term inside the bracket is non-negative and hence, the kernel can be written as\nKq(x, y) =\n( 1 +\n(q \u2212 1) (3\u2212 q)\u03c32\n\u2016x\u2212 y\u20162 ) 1 1\u2212q . (9)\nDue to the power-law tail of the q-Gaussian, for the above kernel, similarity decreases at a slower rate than the Gaussian kernel with increasing distance. The rate of decrease is controlled by the parameter q, and it leads to better performance in some machine learning tasks, as shown in Section 5.\nWe now show that for certain values of q, the proposed kernel satisfies the property of positive definiteness, which is essential for it to be useful in learning theory."}, {"heading": "A. Positive Definiteness", "text": "For q < 1 and q > 3, various examples can be generated, where the kernel is not positive definite (p.d.), but for other values we have the following theorem.\nTheorem 3.1. For 1 < q < 3, the q-Gaussian kernel, as defined in (9), is positive definite.\nWe first state some of the results presented in (Berg et al., 1984), which are required to prove Theorem 3.1.\nLemma 3.2. For a p.d. kernel \u03d5 : X \u00d7 X 7\u2192 R, \u03d5 > 0, the following conditions are equivalent:\n(i) \u2212 log\u03d5 is negative definite (n.d.), and (ii) \u03d5t is p.d. for all t > 0.\nLemma 3.3. If \u03d5 : X \u00d7 X 7\u2192 R is n.d. and satisfies \u03d5(x, x) > 0 for all x \u2208 X , then log(1 + \u03d5) is also n.d.\nLemma 3.4. Let \u03d5 : X \u00d7 X 7\u2192 R be a n.d. kernel, which is strictly positive, then 1\u03d5 is p.d.\nProof of Theorem 3.1: It is easy to verify that \u2016x\u2212 y\u20162 is n.d. So,(\n1 + (q \u2212 1)\n(3\u2212 q)\u03c32 \u2016x\u2212 y\u20162\n) is n.d. for all 1 < q < 3. (10)\nAlso, for all x, y \u2208 X ,( (q \u2212 1)\n(3\u2212 q)\u03c32 \u2016x\u2212 y\u20162\n) > 0 (11)\nwith equality if and only if x = y. Hence, from Lemma 3.3,\nlog ( 1 +\n(q \u2212 1) (3\u2212 q)\u03c32\n\u2016x\u2212 y\u20162 ) is n.d. (12)\nLet \u03c6(x, y) = 1( 1 + (q\u22121)(3\u2212q)\u03c32 \u2016x\u2212 y\u20162 ) . It follows from (10) and (11) that( 1\n\u03c6\n) is n.d. and ( 1\n\u03c6\n) > 1.\nHence, by Lemma 3.4, \u03c6 is p.d. Also, from (12),\nlog\n( 1\n\u03c6\n) is n.d., i.e.,\u2212 log \u03c6 is n.d.\nApplying Lemma 3.2 we obtain, \u03c6t is p.d. for all t > 0. So, ( 1 + (q \u2212 1)\n(3\u2212 q)\u03c32 \u2016x\u2212 y\u20162\n)k is p.d. for all k < 0.\nThe claim follows as (\n1 1\u2212q\n) < 0 for all 1 < q < 3."}, {"heading": "B. Relation with common kernels", "text": "Here, we show that two popular kernels can be obtained as special cases of q-Gaussian kernel.\n(i) Gaussian Kernel: The Gaussian kernel is defined as\n\u03c81(x, y) = exp\n( \u2212\u2016x\u2212 y\u2016 2\n2\u03c32\n) , (13)\nwhere \u03c3 \u2208 R, \u03c3 > 0. We can retrieve the Gaussian kernel (13) when q \u2192 1 in the q-Gaussian kernel (9).\n(ii) Rational Quadratic Kernel: The Rational Quadratic kernel is defined as:\n\u03c82(x, y) =\n( 1\u2212 \u2016x\u2212 y\u2016 2\n\u2016x\u2212 y\u20162 + c\n) , (14)\nwhere c \u2208 R, c > 0. Putting q = 2 in (9), we obtain (14) with c = \u03c32."}, {"heading": "4. NOTE ON REPRODUCING KERNEL HILBERT SPACE", "text": "Regression using kernel models has been widely used in statistics (Parzen, 1963), where estimating a function is equivalent to a solving a variational problem in the RKHS. Smola et al. (1998) showed that the significance of RKHS for support vector kernels using Bochner\u2019s theorem (Bochner, 1959), which provides a RKHS in Fourier space for translation invariant kernels. Other approaches also exist which lead to explicit description of the Gaussian kernel (Steinwart et al., 2006). But such an approach does not work for the q-Gaussian case as binomial series expansion of q-Gaussian does not converge for q > 1. So, we follow Bochner\u2019s approach."}, {"heading": "A. Realization of RKHS", "text": "We state Bochner\u2019s theorem, and then use the method presented in (Hofmann et al., 2008) to show how it can be used to construct the RKHS for a p.d. kernel.\nDefinition 4.1. A function \u03c6 : RN 7\u2192 C is called a positive definite function if \u03d5(x, y) = \u03c6(x\u2212y) is a p.d. kernel on RN .\nTheorem 4.2 (Bochner). Let \u03c6 be a continuous function on RN . Then, \u03c6 is positive definite if and only if there is a finite non-negative Borel measure \u00b5 on RN such that\n\u03c6(x) = \u222b RN e\u2212i\u3008t,x\u3009d\u00b5(t) (15)\nAssuming that d\u00b5(t) = \u03c1(t)dt, it immediately follows that \u03c1(t) is the inverse Fourier transform of \u03c6(x). Then, the RKHS of the kernel \u03d5 is given by\nH\u03d5 = { f \u2208 L2(RN ,dx) \u2223\u2223\u2223\u2223 \u222b RN |f\u0302(t)|2 \u03c1(t) dt <\u221e } (16)\nwith the inner product defined as \u3008f, g\u3009\u03d5 = \u222b RN f\u0302(t)g\u0302(t) \u03c1(t) dt, (17)\nwhere f\u0302(t) is the Fourier transform of f(x) and L2(RN ,dx) is set of all measurable functions over RN .\nClaim 4.3. For a given q \u2208 (1, 3), the RKHS of Kq is the set\nHKq =\n{ f \u2208 L2(RN ,dx) \u2223\u2223\u2223\u2223 \u222b RN |f\u0302(t)|2dt \u03b4q(t) <\u221e } with the inner product\n\u3008f, g\u3009Kq = \u222b RN f\u0302(t)g\u0302(t) \u03b4q(t) dt,\nwhere \u03b4q(t) = exp\n( \u2212\u03c3 Aq |t|Aq\nB Aq q\n) , Aq and Bq being con-\nstants depending on the value of q.\nThe above claim cannot be proved directly. But we show some numerical results justifying the claim.\nB. Inverse Fourier Transform (Numerical Evidence)\nWe define \u03c6q : RN 7\u2192 R such that \u03c6q(x \u2212 y) = Kq(x, y) for all x, y \u2208 X \u2282 RN . From Definition 4.1, \u03c6q is a positive definite function, and hence, Theorem 4.2 should hold. But, it is difficult to validate this fact since a closed form expression of the inverse Fourier transform of \u03c6 given by\n\u03c1q(t) = 1\n2\u03c0 \u222b RN ei\u3008t,x\u3009 ( 1 + (q \u2212 1) (3\u2212 q)\u03c32 \u2016x\u20162 ) 1 1\u2212q dt (18)\ncannot be computed easily. However, some intuition can be obtained from the Gaussian (q = 1) and the Rational Quadratic (q = 2) cases, where the\nclosed form solutions exist. For simplicity, we consider the one-dimensional case where\n\u03c61(x) = exp\n( \u2212 x 2\n2\u03c32\n) \u21d4 \u03c11(t) =\n\u03c3\u221a 2\u03c0 exp\n( \u2212\u03c3 2t2\n2 ) and \u03c62(x) = \u03c32\n\u03c32 + x2 \u21d4 \u03c12(t) =\n\u03c3 2 exp (\u2212\u03c3|t|) .\nFrom the above two cases, we can make the following assumption.\nAssumption 4.4. The inverse Fourier transform of \u03c6q is of the form\n\u03c1q(t) = \u03c3\n2\u03c0 Cq exp\n( \u2212\u03c3 Aq |t|Aq\nB Aq q\n) , (19)\nwhere Aq, Bq, Cq \u2208 R are positive constants.\nIn special cases, for q = 1, A1 = 2, B1 = \u221a\n2 and C1 =\u221a 2\u03c0, whereas for q = 2, A2 = B2 = 1 and C2 = \u03c0. Claim 4.3 follows if we assume Assumption 4.4 to hold. Since, the inverse Fourier transform is not explicitly determined, hence, the exact relationship of Aq , Bq and Cq with q could not be found. But, a necessary condition can be obtained using Parseval\u2019s identity, as stated in the following proposition.\nProposition 4.5. If Assumption 4.4 holds, then for a given q \u2208 (1, 3), the constants Aq , Bq and Cq should satisfy\n\u221a 3\u2212 q\u221a q \u2212 1\n\u0393 (\n5\u2212q 2(q\u22121) ) \u0393 (\n2 q\u22121\n) = 2\u22121/Aq \u03c03/2 C2qBq Aq \u0393\n( 1\nAq\n) (20)\nProof: Parseval\u2019s identity states that\n1\n2\u03c0 \u222b \u221e \u2212\u221e \u03c6q(x) 2dx = \u222b \u221e \u2212\u221e \u03c1q(t) 2dt. (21)\nThe right hand side in (21) is\u222b \u221e \u2212\u221e \u03c1q(t) 2dt = \u03c32C2q 2\u03c02 \u222b \u221e 0 exp ( \u22122\u03c3 Aq tAq B Aq q ) dt,\nwhich can be evaluated by substituting z = 2\u03c3 Aq tAq\nB Aq q to obtain\u222b \u221e \u2212\u221e \u03c1q(t) 2dt = \u03c3 2\u03c02 1 21/Aq C2qBq Aq \u0393 ( 1 Aq ) . (22)\nConsidering the left hand side in (21), we can see that for 1 < q < 3,\n\u03c6q(x) 2 = ( 1 +\n(q\u2032 \u2212 1) (3\u2212 q\u2032) x2 (\u03c3\u2032)2\n) 1 1\u2212q\u2032\nwhere q\u2032 = q+12 and \u03c3 \u2032 = \u03c3 \u221a 2\u2212q\u2032 3\u2212q\u2032 . So, integrating \u03c6q(x) 2 leads to the normalizing constant in (6) as\u222b \u221e \u2212\u221e \u03c6q(x) 2dx = \u03c3\u2032 \u221a 3\u2212 q\u2032 1 \u039bq\u2032 = \u03c3 \u221a 3\u2212 q 2 1 \u039b( q+12 ) .\n(23)\nThe claim follows from by equating (22) and (23).\nWe have performed numerical integration to calculate the inverse Fourier transform given in (18) for various values of q. The constants Aq , Bq and Cq are obtained by minimizing the least squares error between the obtained numerical solution and (19) at uniformly sampled data points. The constants obtained are shown in the Table I. Using these constants, we have numerically computed the Fourier transform of (19), and compared the result with \u03c6q in Figure 1."}, {"heading": "1.25 1.696 1.329 2.635", "text": ""}, {"heading": "1.50 1.428 1.234 2.782", "text": ""}, {"heading": "1.75 1.200 1.127 2.947", "text": ""}, {"heading": "2.25 0.805 0.826 3.420", "text": ""}, {"heading": "2.50 0.613 0.595 3.825", "text": ""}, {"heading": "2.75 0.453 0.375 4.031", "text": ""}, {"heading": "2.90 0.364 0.293 3.403", "text": "It can be verified that the values of the constants in Table 1 satisfy Proposition 4.5. Moreover, the above figure clearly validates Assumption 4.4."}, {"heading": "5. PERFORMANCE COMPARISON", "text": "In this section, we apply the q-Gaussian kernel in various learning algorithms. We provide some insights regarding the behavior of q-Gaussian through illustrative examples. We also compare the performance of the kernel for different values of q, and also with the Gaussian kernel, using various data sets from UCI repository (Frank & Asuncion, 2010)."}, {"heading": "A. Kernel SVM", "text": "Support Vector Machines (SVMs) are one of the most important class of kernel machines. While linear SVMs, using inner product as similarity measure, are quite common, other variants using various kernel functions, mostly Gaussian, are also used in practice. Use of kernels leads to nonlinear separating hyperplanes, which sometimes provide better\nclassification. We formulate a SVM based on the proposed kernel, which would lead to an optimization problem with the following dual form:\nmin \u03b1\u2208Rn n\u2211 i=1 \u03b1i \u2212 1 2 n\u2211 i,j=1 \u03b1i\u03b1jyiyj expq ( \u2212\u2016xi \u2212 xj\u2016 2 (3\u2212 q)\u03c32 ) s.t. \u03b1i > 0, i = 1, 2, . . . , n, and\nn\u2211 i=1 \u03b1iyi = 0,\nwhere, {x1, . . . , xn} \u2282 X are the training data points and {y1, . . . , yn} \u2282 {\u22121, 1} are the true classes.\nThe following two-dimensional example illustrates the nature of hyperplanes that can be obtained using Gaussian and q-Gaussian kernels. The decision boundaries tends to be more flexible as q increases.\nWe compare the performance of linear, Gaussian and qGaussian kernel SVMs. For each of the data sets, 10% of the samples are used for training, and the rest as test data.\nThe training set is randomly chosen. Table II presents the percentage of correct classification averaged over 20 independent trials. For each data set, we have fixed \u03c3 for both Gaussian and q-Gaussian kernels. The data sets with multiple classes have been considered as two class problems by grouping related classes based on data description.\nIt can be observed that the performance of q-Gaussian kernel is close to that of Gaussian kernel, and much better than linear SVM. The optimal values of q (highlighted) varies for different data sets. However, in most of the cases, the performance either steadily increases or decreases, considering the fact the Gaussian corresponds to q = 1. This can be justified by the flexibility of the separating hyperplane required for the given data. It has been noticed that for very high or very low values of \u03c3, Gaussian and q-Gaussian kernels give similar results, which happens because the power-law and the exponential natures cannot be distinguished in these cases."}, {"heading": "B. Kernel k-means clustering", "text": "A major drawback of k-means clustering is that it cannot separate clusters that are non-linearly separable in input space. In order to avoid this problem, the data may be mapped to a higher-dimensional feature space using a nonlinear function, and then k-means clustering is applied in that space. This method is known as kernel k-means.\nThe q-Gaussian kernels have the ability to produce complex decision boundaries as shown in Figure 2. This can be used in kernel k-means to cluster challenging data sets. We compare the performance of q-Gaussian kernel k-means with Gaussian and ordinary k-means. We use the purity of the clusters as a measure of performance, which is defined as\npurity(\u2126, C) = 1\nN m\u2211 k=1 max j\u2208{1,..,m} |\u03c9k \u2229 cj |,\nwhere, \u2126 = {\u03c91, \u03c92, . . . , \u03c9m} are the output class labels, C = {c1, c2, . . . , cm} are the true classes, N is the number of data\npoints and m is the number of clusters. During comparison, the initial cluster means set are randomly chosen from the given data points. Table III presents the percentage purity of the clusters averaged over 20 independent trials. As in the case of SVMs, we have chosen \u03c3 according to the data.\nIt can be observed from the table that q = 2.95 gives optimal results in a considerable number of cases. But for the other sets, the optimal value of q varies arbitrarily. In some cases, ordinary k-means perform better than kernel k-means. These cases have been marked. In certain data sets, Gaussian and q-Gaussian kernels give same purity for all values of \u03c3, and hence, those results have been excluded from Table III."}, {"heading": "C. Regression", "text": "In linear basis function models for regression, given a set of data points, the output function is approximated as a linear combination of fixed non-linear functions of input points as\nf(X) = w0 + M\u2211 j=1 wj\u03c6j(X),\nwhere {\u03c61(.), \u03c62(.), . . . , \u03c6M (.)} are the basis functions. The constants {w0, w1, . . . , wM} are determined by the least square solution of a set of equations of the form\nf(Xj) = yj , j = 1, 2, . . . ,M,\nwhere X1, X2, . . . , XM are the given data points, and their corresponding function values are y1, y2, . . . , yM .\nGaussian basis functions are commonly used for such a model. We use (9) as the q-Gaussian basis functions to obtain\n\u03c6j(X) = expq\n( \u2212\u2016X \u2212Xj\u2016 2\n(3\u2212 q)\u03c32\n) , j = 1, 2, . . . ,M.\nAnother regression model is the Nadaraya-Watson estimator (Nadaraya, 1964), more commonly known as kernel regression, where the estimated function is\nf(X) = \u2211M j=1K(X,Xj)yj\u2211M j=1K(X,Xj) .\nThe kernel defined in (9) can also be used in this case as shown in the following example, where both the above methods have been used to reconstruct a sine wave from uniformly spaced sampled data points.\nThe performance of Gaussian and q-Gaussian kernels, for both linear and kernel regression, have been compared using two data sets from UCI repository . We have chosen 10% of the data randomly for the basis. The root mean squared difference between the estimated and true function values are shown, averaged over 20 independent iterations. The value of \u03c3 chosen depends on the data set and the method. It can be observed that for kernel regression, the error is a convex function of q, and the optimal value of q depends on the data. But, for linear regression, no such trend can be noticed."}, {"heading": "6. CONCLUSION", "text": "In this paper, we proposed a generalization of the Gaussian kernel. The main motivation comes form the generalization of Gaussian distribution to power-law tailed Gaussians called qGaussians, studied in statistical mechanics. Due to its powerlaw nature, the tails of the q-Gaussian fall at a slower rate than Gaussian. This fact can be used in learning algorithms as distant data points have more similarity when q-Gaussian kernels are used.\nWe showed that the proposed kernel is positive definite for all q \u2208 (1, 3). We also gave some results pertaining to the RKHS of q-Gaussian kernel using Bochner\u2019s theorem, and showed that this is a generalization of the RKHS for Gaussian and Rational Quadratic kernels. We also demonstrated the use of the proposed kernel in SVM, regression and k-means clustering.\nThe power-law behavior was recognized long time back in many problems in the context of statistical analysis. Recently power-law distributions have been studied in machine learning communities. The present work further looks in this direction. A possible future work can be to learn the optimal value of q, based on the learning algorithm and the nature of the data."}], "references": [{"title": "Itineration of the internet over nonequilibrium stationary states in Tsallis statistics", "author": ["S. Abe", "N. Suzuki"], "venue": "Physical Review E,", "citeRegEx": "Abe and Suzuki,? \\Q2003\\E", "shortCiteRegEx": "Abe and Suzuki", "year": 2003}, {"title": "Scale-free statistics of time interval between successive earthquakes", "author": ["S. Abe", "N. Suzuki"], "venue": "Physica A: Statistical Mechanics and its Applications,", "citeRegEx": "Abe and Suzuki,? \\Q2005\\E", "shortCiteRegEx": "Abe and Suzuki", "year": 2005}, {"title": "Emergence of scaling in random networks", "author": ["A.L. Barab\u00e1si", "R. Albert"], "venue": "Science, 286:509\u2013512,", "citeRegEx": "Barab\u00e1si and Albert,? \\Q1999\\E", "shortCiteRegEx": "Barab\u00e1si and Albert", "year": 1999}, {"title": "Lectures on Fourier Integral", "author": ["S. Bochner"], "venue": null, "citeRegEx": "Bochner,? \\Q1959\\E", "shortCiteRegEx": "Bochner", "year": 1959}, {"title": "Kernel methods for Pattern Analysis", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "Cristianini and Shawe.Taylor,? \\Q2004\\E", "shortCiteRegEx": "Cristianini and Shawe.Taylor", "year": 2004}, {"title": "On measuretheoretic aspects of nonextensive entropy functionals and corresponding maximum entropy prescriptions", "author": ["A. Dukkipati", "S. Bhatnagar", "M.N. Murty"], "venue": "Physica A: Statistical Mechanics and its Applications,", "citeRegEx": "Dukkipati et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dukkipati et al\\.", "year": 2007}, {"title": "Producing power-law distributions and damping word frequencies with two-stage language models", "author": ["S. Goldwater", "T.L. Griffiths", "M. Johnson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Goldwater et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2011}, {"title": "Quantification method of classification processes: Concept of structural a-entropy", "author": ["J. Havrda", "F. Charv\u00e1t"], "venue": null, "citeRegEx": "Havrda and Charv\u00e1t,? \\Q1967\\E", "shortCiteRegEx": "Havrda and Charv\u00e1t", "year": 1967}, {"title": "Kernel methods in machine learning", "author": ["T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Annals of Statistics,", "citeRegEx": "Hofmann et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2008}, {"title": "Information theory and statistical mechanics", "author": ["E.T. Jaynes"], "venue": "The Physical Review,", "citeRegEx": "Jaynes,? \\Q1957\\E", "shortCiteRegEx": "Jaynes", "year": 1957}, {"title": "Information theory and statistics", "author": ["S. Kullback"], "venue": null, "citeRegEx": "Kullback,? \\Q1959\\E", "shortCiteRegEx": "Kullback", "year": 1959}, {"title": "Nonextensive information theoretic kernels on measures", "author": ["A.F.T. Martins", "N.A. Smith", "E.P. Xing", "P.M.Q. Aguiar", "M.A.T. Figueiredo"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Martins et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "On estimating regression", "author": ["E.A. Nadaraya"], "venue": "Theory of Probability and its Applications,", "citeRegEx": "Nadaraya,? \\Q1964\\E", "shortCiteRegEx": "Nadaraya", "year": 1964}, {"title": "Manuale di economica politica", "author": ["V. Pareto"], "venue": "Societa Editrice Libraria,", "citeRegEx": "Pareto,? \\Q1906\\E", "shortCiteRegEx": "Pareto", "year": 1906}, {"title": "Probability density functionals and reproducing kernel Hilbert spaces", "author": ["E. Parzen"], "venue": "In Proceedings of the Symposium on Time Series Analysis,", "citeRegEx": "Parzen,? \\Q1963\\E", "shortCiteRegEx": "Parzen", "year": 1963}, {"title": "Nonextensive foundation of L\u00e9vy distributions", "author": ["D. Prato", "C. Tsallis"], "venue": "Physical Review E.,", "citeRegEx": "Prato and Tsallis,? \\Q1999\\E", "shortCiteRegEx": "Prato and Tsallis", "year": 1999}, {"title": "q-Gaussian distributions and multiplicative stochastic processes for analysis of multiple financial time series", "author": ["A.H. Sato"], "venue": "Journal of Physics: Conference Series,", "citeRegEx": "Sato,? \\Q2010\\E", "shortCiteRegEx": "Sato", "year": 2010}, {"title": "Learning with Kernels", "author": ["B. Scholk\u00f6pf", "A.J. Smola"], "venue": null, "citeRegEx": "Scholk\u00f6pf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Scholk\u00f6pf and Smola", "year": 2002}, {"title": "The connection between regularization operators and support vector kernels", "author": ["A.J. Smola", "B. Sch\u00f6lkopf", "K. M\u00fcller"], "venue": "Neural Networks,", "citeRegEx": "Smola et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Smola et al\\.", "year": 1998}, {"title": "An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels", "author": ["I. Steinwart", "D.R. Hush", "C. Scovel"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Steinwart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2006}, {"title": "Generalization of Shannon-Khinchin axioms to nonextensive systems and the uniqueness theorem for the nonextensive entropy", "author": ["H. Suyari"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Suyari,? \\Q2004\\E", "shortCiteRegEx": "Suyari", "year": 2004}, {"title": "Possible generalization of Boltzmann-Gibbs statistics", "author": ["C. Tsallis"], "venue": "Journal of Statiscal Physics,", "citeRegEx": "Tsallis,? \\Q1988\\E", "shortCiteRegEx": "Tsallis", "year": 1988}, {"title": "Central limit theorem and deformed exponentials", "author": ["C. Vignat", "A. Plastino"], "venue": "Journal of Physics A: Mathematical and Theoretical,", "citeRegEx": "Vignat and Plastino,? \\Q2007\\E", "shortCiteRegEx": "Vignat and Plastino", "year": 2007}], "referenceMentions": [{"referenceID": 13, "context": "INTRODUCTION Power-law distributions were first studied in economics (Pareto, 1906) in the context of distribution of wealth.", "startOffset": 69, "endOffset": 83}, {"referenceID": 20, "context": "The Shannon-Khinchin axioms of Shannon entropy have been generalized to this case (Suyari, 2004), and this entropy functional has been studied in information theory, statistics and many other fields.", "startOffset": 82, "endOffset": 96}, {"referenceID": 11, "context": "INTRODUCTION Power-law distributions were first studied in economics (Pareto, 1906) in the context of distribution of wealth. Later power-law behavior was observed in various fields such as physics, biology, computer science etc. Barab\u00e1si & Albert (1999) observed this behavior in the World Wide Web.", "startOffset": 70, "endOffset": 255}, {"referenceID": 6, "context": "Goldwater et al. (2011) used these distributions to study language models.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "Goldwater et al. (2011) used these distributions to study language models. In recent years, interest in generalized information measures has increased dramatically, one reason being while maximization of Shannon entropy gives rise to exponential distributions, these measures give power-law distributions. One such generalization is known as nonextensive entropy introduced by Havrda & Charv\u00e1t (1967), and then studied by Tsallis (1988) in statistical mechanics.", "startOffset": 0, "endOffset": 401}, {"referenceID": 6, "context": "Goldwater et al. (2011) used these distributions to study language models. In recent years, interest in generalized information measures has increased dramatically, one reason being while maximization of Shannon entropy gives rise to exponential distributions, these measures give power-law distributions. One such generalization is known as nonextensive entropy introduced by Havrda & Charv\u00e1t (1967), and then studied by Tsallis (1988) in statistical mechanics.", "startOffset": 0, "endOffset": 437}, {"referenceID": 6, "context": "Goldwater et al. (2011) used these distributions to study language models. In recent years, interest in generalized information measures has increased dramatically, one reason being while maximization of Shannon entropy gives rise to exponential distributions, these measures give power-law distributions. One such generalization is known as nonextensive entropy introduced by Havrda & Charv\u00e1t (1967), and then studied by Tsallis (1988) in statistical mechanics. The Shannon-Khinchin axioms of Shannon entropy have been generalized to this case (Suyari, 2004), and this entropy functional has been studied in information theory, statistics and many other fields. Tsallis entropy has been used to study power-law behavior in different cases like earthquakes and network traffic (Abe & Suzuki, 2003, 2005). In kernel based machine learning (Scholk\u00f6pf & Smola, 2002), positive definite kernels are considered as a measure of similarity between points. The choice of kernel is critical to the performance of the learning algorithms, and hence, many kernels have been studied in literature (Cristianini & Shawe-Taylor, 2004). One of the most common kernel used in practical applications is the Gaussian kernel. Nonextensive kernels on probability measures, based on Tsallis divergences, have been proposed by Martins et al. (2009). In this paper, we propose a new kernel based on q-Gaussian, which is a generalization of the Gaussian distribution, obtained by maximizing Tsallis entropy under certain moment constraints.", "startOffset": 0, "endOffset": 1326}, {"referenceID": 6, "context": "Goldwater et al. (2011) used these distributions to study language models. In recent years, interest in generalized information measures has increased dramatically, one reason being while maximization of Shannon entropy gives rise to exponential distributions, these measures give power-law distributions. One such generalization is known as nonextensive entropy introduced by Havrda & Charv\u00e1t (1967), and then studied by Tsallis (1988) in statistical mechanics. The Shannon-Khinchin axioms of Shannon entropy have been generalized to this case (Suyari, 2004), and this entropy functional has been studied in information theory, statistics and many other fields. Tsallis entropy has been used to study power-law behavior in different cases like earthquakes and network traffic (Abe & Suzuki, 2003, 2005). In kernel based machine learning (Scholk\u00f6pf & Smola, 2002), positive definite kernels are considered as a measure of similarity between points. The choice of kernel is critical to the performance of the learning algorithms, and hence, many kernels have been studied in literature (Cristianini & Shawe-Taylor, 2004). One of the most common kernel used in practical applications is the Gaussian kernel. Nonextensive kernels on probability measures, based on Tsallis divergences, have been proposed by Martins et al. (2009). In this paper, we propose a new kernel based on q-Gaussian, which is a generalization of the Gaussian distribution, obtained by maximizing Tsallis entropy under certain moment constraints. The power-law nature of this distribution has been studied by Sato (2010). In fact, the value of q controls the nature of the power-law tails.", "startOffset": 0, "endOffset": 1590}, {"referenceID": 5, "context": "Tsallis entropy in the continuous case is defined as (Dukkipati et al., 2007)", "startOffset": 53, "endOffset": 77}, {"referenceID": 20, "context": "q-Gaussian distribution Tsallis entropy can be obtained by generalizing the information of a single event in the definition of Shannon entropy as shown by Tsallis (1988), where logarithm is replaced with q-logarithm defined as lnq x = x 1\u2212q\u22121 1\u2212q q \u2208 R, q > 0, q 6= 1.", "startOffset": 24, "endOffset": 170}, {"referenceID": 21, "context": "It is called nonextensive because of its pseudo-additive nature (Tsallis, 1988).", "startOffset": 64, "endOffset": 79}, {"referenceID": 10, "context": "Kullback\u2019s minimum discrimination theorem (Kullback, 1959) establishes important connections between statistics and information theory.", "startOffset": 42, "endOffset": 58}, {"referenceID": 9, "context": "A special case is Jaynes\u2019 maximum entropy principle (Jaynes, 1957), by which exponential distributions can be obtained by maximizing Shannon entropy functional, subject to some moment constraints.", "startOffset": 52, "endOffset": 66}, {"referenceID": 14, "context": "Regression using kernel models has been widely used in statistics (Parzen, 1963), where estimating a function is equivalent to a solving a variational problem in the RKHS.", "startOffset": 66, "endOffset": 80}, {"referenceID": 3, "context": "(1998) showed that the significance of RKHS for support vector kernels using Bochner\u2019s theorem (Bochner, 1959), which provides a RKHS in Fourier space for translation invariant kernels.", "startOffset": 95, "endOffset": 110}, {"referenceID": 19, "context": "Other approaches also exist which lead to explicit description of the Gaussian kernel (Steinwart et al., 2006).", "startOffset": 86, "endOffset": 110}, {"referenceID": 8, "context": "Realization of RKHS We state Bochner\u2019s theorem, and then use the method presented in (Hofmann et al., 2008) to show how it can be used to construct the RKHS for a p.", "startOffset": 85, "endOffset": 107}, {"referenceID": 12, "context": "Regression using kernel models has been widely used in statistics (Parzen, 1963), where estimating a function is equivalent to a solving a variational problem in the RKHS. Smola et al. (1998) showed that the significance of RKHS for support vector kernels using Bochner\u2019s theorem (Bochner, 1959), which provides a RKHS in Fourier space for translation invariant kernels.", "startOffset": 67, "endOffset": 192}, {"referenceID": 12, "context": "Another regression model is the Nadaraya-Watson estimator (Nadaraya, 1964), more commonly known as kernel regression, where the estimated function is", "startOffset": 58, "endOffset": 74}], "year": 2017, "abstractText": "The role of kernels is central to machine learning. Motivated by the importance of power law distributions in modeling, simulation and learning, in this paper, we propose a powerlaw generalization of the Gaussian kernel. This generalization is based on q-Gaussian distribution, which is a power-law distribution studied in context of nonextensive statistical mechanics. We prove that the proposed kernel is positive definite, and provide some insights regarding the corresponding Reproducing Kernel Hilbert Space (RKHS). We also study practical significance of qGaussian kernels in classification, regression and clustering, and present some simulation results.", "creator": "LaTeX with hyperref package"}}}