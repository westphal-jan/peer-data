{"id": "1204.3616", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2012", "title": "Large-Scale Automatic Labeling of Video Events with Verbs Based on Event-Participant Interaction", "abstract": "astronomy present an approach to capturing short words clips within english learners as event descriptions. a key distinguishing fact of this work is understanding it labels videos referencing verbs that describe the spatiotemporal interaction between event events, humans and subjects interacting with each other, carving away all object - centred information and wide - grained image characteristics, sometimes relying solely on abstract cloth - strewn task of representing particular participants. we apply our query using providing preliminary set examining 22 distributed data registers describing statistical corpus of 2, 870 videos, bearing two surprising outcomes. essentially, a survey accuracy showing brightness by 70 % on a 1 - out - x - 22 binary screen reports greater than 98 % on a variety of 1 - third - of - 10 subsets of this labeling task arrives independent of sampling choice of intersection of two random time - series regions we exclude. second, astronomers judge this margin of accuracy using a certain impoverished intermediate representation functioning solely of consecutive color boxes representing languages or two event participants as a function of outcome. this guarantee our generic event validation depends negatively on sheer choice of appropriate features that constitute the linguistic expressions of the following classes contingent on the different organizing algorithms.", "histories": [["v1", "Mon, 16 Apr 2012 19:59:15 GMT  (928kb,D)", "http://arxiv.org/abs/1204.3616v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["andrei barbu", "alexander bridge", "dan coroian", "sven dickinson", "sam mussman", "siddharth narayanaswamy", "dhaval salvi", "lara schmidt", "jiangnan shangguan", "jeffrey mark siskind", "jarrell waggoner", "song wang", "jinlian wei", "yifan yin", "zhiqi zhang"], "accepted": false, "id": "1204.3616"}, "pdf": {"name": "1204.3616.pdf", "metadata": {"source": "CRF", "title": "Large-Scale Automatic Labeling of Video Events with Verbs Based on Event-Participant Interaction", "authors": ["Andrei Barbu", "Alexander Bridge", "Dan Coroian", "Sven Dickinson", "Sam Mussman", "Siddharth Narayanaswamy", "Dhaval Salvi", "Lara Schmidt", "Jiangnan Shangguan", "Jeffrey Mark Siskind", "Jarrell Waggoner", "Song Wang", "Jinlian Wei", "Yifan Yin", "Zhiqi Zhang"], "emails": ["qobi@purdue.edu."], "sections": [{"heading": null, "text": "We present an approach to labeling short video clips with English verbs as event descriptions. A key distinguishing aspect of this work is that it labels videos with verbs that describe the spatiotemporal interaction between event participants, humans and objects interacting with each other, abstracting away all object-class information and fine-grained image characteristics, and relying solely on the coarse-grained motion of the event participants. We apply our approach to a large set of 22 distinct verb classes and a corpus of 2,584 videos, yielding two surprising outcomes. First, a classification accuracy of greater than 70% on a 1-out-of-22 labeling task and greater than 85% on a variety of 1-out-of10 subsets of this labeling task is independent of the choice of which of two different time-series classifiers we employ. Second, we achieve this level of accuracy using a highly impoverished intermediate representation consisting solely of the bounding boxes of one or two event participants as a function of time. This indicates that successful event recognition depends more on the choice of appropriate features that characterize the linguistic invariants of the event classes than on the particular classifier algorithms.\n\u2217 Corresponding author. Email: qobi@purdue.edu. Additional images and videos as well as all code and datasets are available at http://engineering.purdue. edu/\u02dcqobi/arxiv2012d."}, {"heading": "1 Introduction", "text": "People describe observed visual events using verbs. A common assumption in Linguistics (Jackendoff, 1983; Pinker, 1989) is that verbs typically characterize the interaction between event participants in terms of the gross changing motion of these participants. Object class and image characteristics of the participants are believed to be largely irrelevant to determining the appropriate verb label for an event. Participants simply fill roles (such as agent and patient) in the spatiotemporal structure of the event class described by a verb. For example, an event where one participant (the agent) picks up another participant (the patient) consists of a sequence of two subevents, where during the first subevent the agent moves towards the patient while the patient is at rest and during the second subevent the agent moves together with the patient away from the original location of the patient. It does not matter whether the agent is a human or a cat, or whether the patient is a ball or a cup. Moreover, the shapes, sizes, colors, textures, etc. of the participants are irrelevant. Additionally, only the gross motion characteristics are relevant; it is irrelevant whether the participants grow, shrink, bend, vibrate, etc. during a pick up event. The precise linear or angular velocities and accelerations are likewise irrelevant.\nThe objective of this paper is to evaluate this Linguistic assumption and its relevance to the computer-vision task of labeling video events with verbs. In order to evaluate this hypothesis, we focus our attention on methods that classify events solely on the basis of the gross changing motion of the event participants. In doing do, we often expressly discard other sources of information such as object class,\nar X\niv :1\n20 4.\n36 16\nv1 [\ncs .C\nchanging human body posture, and low-level image characteristics such as shape, size, color, and texture. We do this not because we believe that such information could not help event recognition but rather to allow us to strongly evaluate the above hypothesis. The surprising result of this endeavor is that gross changing motion of event participants attains greater than 70% accuracy on a 1-out-of-22 forced-choice labeling task, significantly outperforming chance (4.5%), and greater than 85% accuracy on a variety of 1-out-of10 subsets of this labeling task, again significantly outperforming chance (10%).\nAs this paper focusses on labeling video events with verbs, both the methods and datasets commonly used in prior event-classification efforts are not appropriate. Such work typically classifies events using object and image characteristics and fine-grained shape and motion features, such as spatiotemporal volumes (Blank et al., 2005; I. Laptev and Rozenfeld, 2008; Rodriguez et al., 2008) and tracked feature points (Liu et al., 2009; Schuldt et al., 2004; Wang and Mori, 2009). Moreover, many of the datasets commonly used in such work do not involve people interacting with objects or other people and contain event classes that do not depict common verbs. For example, the distinctions between wave1 and wave2 or jump and pjump in the WEIZMANN dataset (Blank et al., 2005) or the distinctions between Golf-Swing-Back, Golf-Swing-Front, and Golf-Swing-Side; Kicking-Front and Kicking-Side; or Swing-Bench and Swing-SideAngle in the SPORTS ACTIONS dataset (Rodriguez et al., 2008) do not correspond to distinctions in verb semantics. The event classes side and jack in the WEIZMANN dataset, the event classes Swing-Bench and Swing-SideAngle in the SPORTS ACTIONS dataset, and the vast majority of the event classes in the UCF50 dataset (Liu et al., 2009) (e.g. Basketball, Billiards, BreastStroke, CleanAndJerk, HorseRace, HulaHoop, MilitaryParade, TaiChi, or YoYo, just to name a few) do not correspond to verbs in any language. The videos in the KTH dataset (Schuldt et al., 2004) do not reflect the true meanings of any verbs, let alone boxing or clapping or waving ones hands. Typical actions in specialized domains like ballet (c.f. the BALLET dataset (Wang and Mori, 2009)) are described by nouns, not verbs, and often are not part of common lay vocabulary. The distinction between the event classes golf swing, tennis swing, and swing in the YOUTUBE dataset (Liu et al., 2009) reflect distinctions in event participants, not the semantics of the verb swing.\nSiskind and Morris (1996) presented a technique for labeling video events with verbs based on the changing motion patterns of the event participants. However, they only applied their technique to a small number of event classes (six) and a small corpus of thirty-six videos, six per class.\nMoreover, they derived the changing motion patterns using a rudimentary tracker that was specific to color and motion using background subtraction. Thus the event participants were limited to people\u2019s hands interacting with colored blocks in uncluttered desktop environments with static backgrounds. In this paper, we employ the same technique for labeling video events with verbs but extend it to a much larger number of event classes (twenty two) and evaluate it on a much larger corpus of 2,584 videos ranging from 6 to 584 per class. Since the corpus used in the present effort exhibits a wide variety of natural event participants in a wide variety of cluttered environments with nonstationary backgrounds, this paper employs novel and more general-purpose techniques for deriving the changing motion patterns. Moreover, Siskind & Morris used only one algorithmic method, namely hidden Markov models (HMMs), to classify the time series that characterize the changing motion patterns. Thus one might conclude that the performance of this approach is somehow dependent on this choice of classifier. In this paper, we employ two distinct time-series classification methods, namely HMMs and dynamic time warping (DTW) and demonstrate that both achieve essentially identical performance. Thus it appears that the strength of the approach results from the general principle of classifying events based on gross changing motion patterns, not on the algorithmic particulars. Moreover, we demonstrate a surprising result. Our front-end tracker abstracts each video as one or two moving axisaligned rectangles. Despite such an extremely impoverished representation that passes only 4 or 8 small integers per frame between the front-end tracker and the back-end time-series classifier, and the fact that all training and classification is performed solely on this impoverished representation, both of our classifiers attain greater than 70% accuracy on a 1-out-of-22 forced-choice labeling task and greater than 85% accuracy on a variety 1-out-of-10 subsets of this task. This supports the common assumption in Linguistics that the meanings of many common verbs are sensitive only to gross changing motion patterns of the event participants and not the object class or image characteristics of those participants.\nThe paper is organized as follows. Section 2 describes the new corpus that we use for this effort. Section 3 describes the tracking methods that we employ to abstract each video in this corpus to one or two moving axis-aligned rectangles. Section 4 describes the feature vectors that we extract from this impoverished representation and the particulars of the training and classification paradigms. Section 5 describes our experimental results. Section 6 concludes with a discussion of potential improvements."}, {"heading": "2 The Mind\u2019s Eye Corpus", "text": "As part of the Mind\u2019s Eye program, DARPA has produced a video corpus that is specifically designed to support la-\nbeling of videos with common verbs. The particulars of this corpus were driven by the desire to ground the semantics of 48 specific English verbs. To date, several components of this corpus have been released to program participants. One portion, C-D1a, containing 2,584 videos, was released in late September 2010, while a second portion, CD1b, containing 1,564 videos, was released in late January 2011. The videos are provided at 720p@30fps and range from 21 frames to 1408 frames in length, with an average of 241 frames. The videos in C-D1a range from 21 frames to 809 frames in length, with an average of 141 frames. Each video is intended to depict one of the 48 specific English verbs and collectively all 48 verbs are represented in this combined corpus (with unequal numbers of exemplar videos). Each video comes labeled with the intended verb depiction. Because verbs often exhibit a range of polysemous and homonymous meanings and also may exhibit synonymy where the semantic space of one verb may include all or part of the semantics space of another verb, DARPA intends to eventually solicit human judgements for the association of verb labels with each video. Since such human labelings have not yet been produced, in this paper we simply take the \u2018correct\u2019 label for each video to be the intended verb label provided with the video. Moreover, this paper considers only the C-D1a portion that depicts 22 specific English verbs. Fig. 1 summarizes the distribution of verbs and exemplar videos in this portion of the corpus.\nConformant to the linguistic observation that object identity and class is tangential to the task of labeling a video with a verb, different exemplars for each of the verbs in C-D1a often have the participant roles played by different object instances and classes. The C-D1a corpus has a total of 26 distinct objects that play a role in the depicted verbs as enumerated in Fig. 2. (Note that there are far more distinct objects that do not play a role in the depicted verbs and serve solely to clutter the background.) Many of these objects, however, only appear in the corpus occupying a\nOur corpus- size measurements reflect only the videos in the SINGLE VERB directory of C-D1a, and eliminate from consideration those videos not labeled with a single verb from this list of 48 verbs.\nvery small portion of the field of view and are difficult for humans, let alone machines, to detect and classify reliably. The ones that are most difficult to detect and classify reliably are starred in Fig. 2. For each of the remaining ones, we manually cropped a collection of between 1,500 and 2,100 exemplars (combining both positive and negative samples) to train a part-based object detector (Felzenszwalb et al., 2010). It is important to stress that we use this object detector solely to produce bounding-box information for deriving the gross changing motion patterns of the event participants. During event classification, we expressly discard the object-class information and confidence scores provided by the object detector. In section 6, we discuss how one could extend our methods to make use of such information and achieve even higher classification accuracy."}, {"heading": "3 Tracking", "text": "We use Felzenszwalb et al.\u2019s (Felzenszwalb et al., 2010) part-based object detector as a detection source to produce axis-aligned rectangles (henceforth detection boxes or simply detections or boxes) as a function of time. However, it is unreliable alone as a means for characterizing gross participant-object motion because it simultaneously exhibits a high false-positive rate and a high false-negative rate. Moreover, there is no single detection threshold that properly trades off the false-positive and false-negative rates in a class- or video-independent fashion. Additionally, the raw detection-confidence values produced by the detector, or even their rank ordering, cannot be used on isolated frames to select the desired detection. Moreover, the detector alone cannot distinguish between false positives and multiple objects of the same class at close positions in the field of view. Likewise, the detector alone does not provide temporal-correspondence information in this situation. These problems are particularly exacerbated by occlusion, where objects enter and leave the field of view or pass in front of or behind other objects. In these circumstances, the\ndetection confidence becomes an even less reliable measure of the (partially occluded) presence or absence of an object. This is a particularly egregious limitation because verbs describe interaction among participants and such interaction most frequently involves occlusion."}, {"heading": "3.1 Optimal selection of object tracks", "text": "We address all of these issues with a novel technique that produces coherent object tracks across a video from collections of independent detections in each frame by simultaneously selecting among multiple detections in all frames of a video to find the combination of selections that leads to a global optimum of a cost function that characterizes the overall object-track coherence. While we employ this technique using Felzenszwalb et al.\u2019s part-based object detector as a detection source, it can be more generally applied to any alternate detection source that outputs boxes with confidence scores. The only requirement is that the confidence scores must provide a total ordering of the boxes. The confidence scores need not be normalized or lie in a particular interval. This lax requirement facilitates integrating boxes produced by different detection sources into a single coherent track, simply by providing a correspondence between the confidence values produced by the different detection sources and how they impact this total order. We avail ourselves of this potential in section 3.4 to provide resilience in the face of appearance change due to nonrigid motion and out-of-plane object rotation.\nOne can conceivably use an alternate detection source that does not rely on an object detector. For example, one might do some form of background subtraction or motion-based tracking to separate moving objects from the background or some form of bottom-up foreground/background segmentation or contour completion to segment salient objects. Any method that could reliably place bounding boxes around event participants as a function of time would suffice for our purposes. The sole reason that we employ an object detector as a detection source is that bottom-up methods are currently not sufficiently reliable, while methods based on background subtraction or motion detection fail to detect non-moving event participants (of which there are many in our corpus) and are unreliable in the presence of nonstationary backgrounds (such as occur frequently in our corpus).\nWe apply our detection source independently for each frame and each model, biasing this detection source to yield few false negatives at the expense of yielding a preponderance of false positives, and use our tracker to filter out the false positives. When using Felzenszwalb et al.\u2019s partbased object detector as a detection source, we do this by subtracting a fixed offset (which we take to be 1) from the learned detection threshold. The particular value of this offset is unimportant so long as it yields a sufficiently low\nfalse-negative rate, as our method reliably selects coherent tracks despite an extremely high false-positive rate. The only negative impact of choosing too high of an offset is an increase in run time.\nFelzenszwalb et al.\u2019s part-based object detector, by default, incorporates non-maxima suppression to remove detections that overlap more than 50% with detections of higher confidence. This tends to foil the above process for biasing the detector towards few false negatives and many false positives. To counter the effect of excessive non-maxima suppression, we raise the overlap threshold to 80%. This allows for much better object localization and reduces jitter considerably.\nWe have found that no amount of the above bias process will completely eliminate false negatives. To provide for robust production of coherent object tracks that are necessary for successful event classification, we compensate for the remaining false negatives by projecting each detection box in each frame forward a fixed number of frames using the Kanade-Lucas-Tomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker. We track the KLT features that reside inside each detection box for one frame and compute a single velocity vector and divergence vector for that detection by computing the average velocity and divergence of the KLT features tracked for that box. We use the aggregate velocity and divergence vectors to project the detection box forward one frame and repeat this process. We limit this projection process to 5 frames as it is subject to drift, and we need it only to compensate for false negatives which are relatively rare as a result of the above bias process. We augment the collection of detections to include the forward-projected boxes, taking the confidence score of a forward-projected box to be that of the original detection that was forward projected.\nTo select a coherent object track across multiple frames we construct a graph with one vertex for each detection in each frame and edges connecting all pairs of detections in adjacent frames. The edges are weighted with a cost that inversely measures coherence and we search for a path from the first to last frames with minimal total edge weight using a dynamic-programming algorithm (Viterbi, 1971) that finds a global optimum. This cost is formulated as a linear combination of two components, one being the detection confidence score and the other being consistency with optical flow. The latter is taken to be the Euclidean distance between the center of a detection box in a given frame and a projection of the center of the corresponding detection box from the previous frame forward using optical flow. This forward-projection process is analogous to the one performed to compensate for false negatives except that the average velocity vector is computed from dense optical flow instead of tracked KLT features.\nIn principle, one could use either KLT features or opti-\ncal flow for either forward-projection process. We find that, in practice, KLT features yield better results for the forward-projection process used to compensate for false negatives while optical flow yields better results for the forward-projection process used to compute track coherence. Also, our track-coherence measure uses only the distance between detection-box centers and thus does not need a divergence measure. While one could extend the track-coherence measure to incorporate such information, we find that it yields no improvement in performance. In our experiments, we weight the optical-flow component of track coherence ten times less than the detectionconfidence score. We bias the track-coherence measure towards detection confidence to prevent production of tracks that are consistent with optical flow but do not correspond to reliable object detections. Other than this general bias, we find that the object tracks produced are largely insensitive to the precise weighting value."}, {"heading": "3.2 Entering and leaving the field of view", "text": "The algorithm described thus far constructs tracks that span the entire video from the first frame to the last frame. We allow for objects that enter and leave the field of view simply by applying this algorithm to a subinterval of the video. The only difficulty in doing so is determining the subinterval boundaries. We take the subinterval to begin at the first frame with a detection confidence above a certain threshold, and end at the last such frame. To derive this threshold, we compute a (50 bin) histogram of the maximal detectionconfidence scores in each frame, over the entire video. One expects this histogram to be bimodal since frames in which the object is not present will have lower confidence scores, as all detections will be false positives. We take the threshold to be the minimum of the value that maximizes the between-class variance (Otsu, 1979) when bipartitioning this histogram and the learned detector-confidence threshold offset by a fixed, but small, amount (0.4). In practice, we find that proper selection of the subinterval is largely insensitive to the number of bins and the precise threshold offset."}, {"heading": "3.3 Multiple instances of the same object class", "text": "We detect multiple tracks of the same object class by repeated application of the above method. In doing so, we must prevent subsequent iterations from rediscovering tracks produced by earlier iterations. The na\u0131\u0308ve way of doing this would be to remove detections associated with earlier tracks. Detection boxes can be deemed to be associated with earlier tracks when their centers lie inside detection boxes included in those earlier tracks. However, removing all such detections runs the risk of precluding overlapping tracks, as would happen when objects pass each other in the field of view. So instead of removing detections, we rescore them with the maximal detection score in the lower\nquartile of scores for that frame. Given the biasing process towards false positives away from false negatives in the detection source, boxes in the lower quartile are likely to be false positives and undesirable to include in a coherent track. Rescoring detections in this fashion biases subsequent iterations to find distinct tracks while allowing tracks to briefly overlap.\nIf one is not careful, there can be crossover at such points of overlap, where the object identity is swapped between two distinct tracks. We use an object-appearance model to bias against such crossover. Color histograms are computed in the CIELAB (C.I.E., 1978) color space of the pixel values inside the detection boxes after shrinking those boxes by 60% to ameliorate the influence of background pixels on these histograms. We then augment the edge-weight function to include a coherence measure on object appearance, taking this coherence measure to be Earth Mover\u2019s distance (Peleg et al., 1989) between the corresponding histograms. We weight object appearance and detector confidence equally in the coherence measure, though in practice, we find that the object tracks produced are largely insensitive to the precise weighting."}, {"heading": "3.4 Nonrigid motion and out-of-plane rotation", "text": "Felzenszwalb et al.\u2019s part-based object detector is unreliable as a detection source when there is nonrigid motion and out-of-plane rotation. Our tracking framework can provide resilience in the face of such unreliability by integrating detection boxes from multiple detection sources. We do so by training multiple models for Felzenszwalb et al.\u2019s part-based object detector for varying object appearance under nonrigid motion and out-of-plane rotation and union the resulting detections. As discussed in section 3.1, we must insure that the confidence scores allow for comparison between detections produced by different detection sources. We do this by offsetting the confidence scores for each detection source by the threshold computed in section 3.2.\nThe C-D1a corpus has little out-of-plane rotation and therefore such does not impact the reliability of the detection source. But the corpus does contain one source of nonrigid motion, namely changing human body posture. For this corpus, it is sufficient to train detectors for three distinct postures: standing, crouching, and lying down.\nIntegrating multiple detection sources into a single object track allows annotation of the detections in that track with their source. In particular, this allows temporal annotation of human motion tracks with their changing posture. Conceivably one could use such information to support selection of the appropriate verb label. Because we wish to evaluate the hypothesis that verbs typically characterize the gross changing motion of the event participants, we expressly discard such information in the experiments per-\nformed in this paper."}, {"heading": "3.5 Smoothing", "text": "Boxes comprising the recovered object tracks suffer from jitter. We remove this jitter by fitting piecewise cubic splines to the widths, heights, and x and y center coordinates of the tracked boxes. A simple selection of smoothing parameters suffices for the C-D1a corpus. Since the videos in C-D1a have low frame length variance, a constant number of spline pieces is adequate. Box x and y center coordinates are smoothed with 10 pieces, as they can move significantly when tracking accelerating objects, for example a bouncing ball. Box widths and heights are smoothed with 5 pieces as object shape and size change less drastically."}, {"heading": "3.6 Results", "text": "Our tracker runs in time O(lm + lmn|df |2) to recover n tracks with m detection sources, each yielding d detections per frame, doing f frames of forward projection, on videos of length l. In practise, the run time is dominated by the detection process and the dynamic-programming step. Fig. 3 illustrates the operation of our tracker, rendering the output of each stage. From this video, one can clearly see the robustness of our tracker in light of cluttered nonstationary backgrounds, motion that is not perpendicular to the camera axis, an extremely high falsepositive biased detection rate of the detection source, occlusion that results from overlapping tracks corresponding to interacting objects, nonrigid motion that results from changing human body posture, objects entering and leaving the field of view, and multiple instances of the same object class. Moreover, as illustrated in Fig. 4, the fact that our tracker finds an optimal coherent track by processing the entire video allows it to robustly track objects that approach or recede from the camera by a large distance that would otherwise be too small in the field of view to reliably track by methods that did not process the entire video. Without the false-positive bias that such a whole-video approach allows, Felzenszwalb et al.\u2019s part-based object detector would not even detect such objects."}, {"heading": "4 Classification", "text": "We convert the collection of object tracks for a video to a time-series of real-valued feature vectors and formulate the problem of labeling a video with a verb as a time-series classification problem. In doing so, we discard all object identity and body posture information that is available in those tracks.\nFor each video, we designate one track as the agent and another track (if present) as the patient. The agent is determined using a heuristic: people are more likely to be agents\nthan inanimate objects are, and bicycles, motorcycles, and SUVs are more likely to be agents than other inanimate objects because they are driven by people that we might fail to detect due to occlusion. Another track (if present) is selected as the patient using the same heuristic. Ties are broken by selecting the track with highest track coherence as the agent and the one with second highest track coherence as the patient.\nFor all videos, we extract a feature vector for each frame representing the gross absolute motion of the agent:\n1. x-coordinate of the box center 2. y-coordinate of the box center 3. box aspect ratio 4. derivative of the box aspect ratio 5. magnitude of the velocity of the box center 6. direction of the velocity of the box center 7. magnitude of the acceleration of the box center 8. direction of the acceleration of the box center\nFor videos with two or more object tracks, we also extract a feature vector that includes the above absolute motion features representing the independent motion of each of the agent and patient along with additional features that describe their relative motion:\n1. distance between agent and patient box centers 2. orientation of vector from agent box center to patient\nbox center 3. derivative of the distance between agent and patient\nbox centers\nIn all of the above, temporal derivatives and corresponding velocities and accelerations are computed as a two-point finite difference. Note that we label videos with verbs using the gross changing motion patterns of at most two event participants. While we could, in principle, label videos on the basis of the motion patterns of more event participants, if present, by straightforward extension of the above feature-vector computation to include absolute features for all objects and relative features for all object pairs, we expressly refrain from doing so to evaluate the Linguistic hypothesis that verbs largely describe the interaction between an agent and a patient.\nThe verbs in C-D1a often have different senses, such as the causative/inchoative alternation (the agent bounces vs. the agent bounces the patient), that involve a different number of participants. In this case, we train two distinct classifiers, one on all videos characterizing the motion of just the agent and one on those videos that have both an agent and a patient characterizing the motion of both the agent and the patient. When classifying an unseen video with just a single object track we use models trained on just agents, while when classifying an unseen video with more than one object track we use models trained on both agents and patients.\nTo evaluate the hypothesis that it is possible to classify events solely on the basis of the gross changing motion of the event participants and demonstrate the insensitivity of this hypothesis to the choice of time-series classifier, we have run two parallel sets of experiments, one with HMMs (Baum and Petrie, 1966) and one with DTW (Ellis, 2003; Sakoe and Chiba, 1978). When using HMMs, we train models with 5 states and independent continuous output distributions for each feature. We use Gaussian distributions for those features that constitute linear quantities and Von Mises distributions for those features that constitute angular quantities. We found that increasing the number of states beyond 5 did not significantly improve accuracy. When using DTW, we employ Euclidean distance between feature vectors as the distance metric between frames and use DTW to extend this metric as a distance between frame sequences to construct a nearest-neighbor classifier between unseen videos and training exemplars."}, {"heading": "5 Results", "text": "We performed 5-fold cross-validation on the entire C-D1a corpus with a 1-out-of-22 forced-choice classification task using both HMMs and DTW. To do this, we independently partitioned the set of \u2018correct\u2019 exemplars for each verb into five random but equally sized components (up to quantization). For each of the five cross-validation runs we trained on the exemplars in four of the five partitions and tested on the exemplars in the remaining partition. Fig. 5 gives the recognition accuracy for each classification algorithm for each cross-validation run. Fig. 7 and Fig. 8 give the aggregate confusion matrices for each classification algorithm across all five cross-validation runs. Note the essentially identical performance of HMMs and DTW: HMMs exhibits an aggregate classification accuracy of 71.9% while DTW exhibits an aggregate classification accuracy of 71.3%. Moreover, we attain greater than 85% aggregate classification accuracy for three different 1-outof-10 subsets of this forced-choice classification task with both HMMs and DTW: arrive bounce dig drop exchange give jump kick pickup run (87.4% HMMs, 85.3% DTW), bounce dig drop exchange give jump kick pickup pull run (87.5% HMMs, 85.1% DTW), and bounce dig drop exchange give jump kick pass pickup pull (86.1% HMMs, 87.0% DTW). These results support the hypothesis that classification accuracy depends more on the correct choice of features than on the classification algorithm."}, {"heading": "6 Conclusion", "text": "Our focus in this paper is to evaluate the hypothesis that it is possible to label videos with verbs using information solely about the gross changing motion of the event participants. There are numerous places where our computational methods expressly discard information that is oth-\nerwise available in order to evaluate this hypothesis. Since such information might correlate with the underlying event, one could extend our classifiers to make use of such information. For example, one might expect that detector confidence scores would decrease with occlusion and thus correlate with the object interaction indicative of event class. Similarly, one might expect that object class would correlate with event class. Indeed, as shown in Fig. 6, such correlation significantly reduces the potential verb-label space, rendering the verb-labeling task almost trivial. Likewise, as discussed in section 3.4, one could augment the time series of feature vectors with human body-posture information that is extracted as a by-product of using multiple detection sources to provide resilience in the face of out-ofplane rotation and nonrigid motion. It is quite unexpected that we attain as good results as we have despite expressly discarding such information. This supports the common assumption in Linguistics that verbs typically characterize the interaction between event participants in terms of the gross changing motion of these participants."}, {"heading": "Acknowledgments", "text": "This work was supported, in part, by NSF grant CCF0438806, by the Naval Research Laboratory under Contract\nNumber N00173-10-1-G023, by the Army Research Laboratory accomplished under Cooperative Agreement Number W911NF-10-2-0060, and by computational resources provided by Information Technology at Purdue through its Rosen Center for Advanced Computing. Any views, opinions, findings, conclusions, or recommendations contained or expressed in this document or material are those of the author(s) and do not necessarily reflect or represent the views or official policies, either expressed or implied, of NSF, the Naval Research Laboratory, the Office of Naval Research, the Army Research Laboratory, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein."}], "references": [{"title": "Statistical inference for probabilistic functions of finite state Markov chains", "author": ["L.E. Baum", "T. Petrie"], "venue": "Ann. Math. Stat,", "citeRegEx": "Baum and Petrie.,? \\Q1966\\E", "shortCiteRegEx": "Baum and Petrie.", "year": 1966}, {"title": "Actions as space-time shapes", "author": ["M. Blank", "L. Gorelick", "E. Shechtman", "M. Irani", "R. Basri"], "venue": "In Proceedings of the Tenth IEEE International Conf. on Computer Vision,", "citeRegEx": "Blank et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blank et al\\.", "year": 2005}, {"title": "Dynamic time warp (DTW) in Matlab", "author": ["D. Ellis"], "venue": "http://www.ee.columbia.edu/ \u0303dpwe/ resources/matlab/dtw/,", "citeRegEx": "Ellis.,? \\Q2003\\E", "shortCiteRegEx": "Ellis.", "year": 2003}, {"title": "Object detection with discriminatively trained part based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Learning realistic human actions from movies", "author": ["C. Schmid I. Laptev", "M. Marszalek", "B. Rozenfeld"], "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "Laptev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Laptev et al\\.", "year": 2008}, {"title": "Semantics and Cognition", "author": ["R. Jackendoff"], "venue": null, "citeRegEx": "Jackendoff.,? \\Q1983\\E", "shortCiteRegEx": "Jackendoff.", "year": 1983}, {"title": "Recognizing realistic actions from videos \u201cin the wild", "author": ["J. Liu", "J. Luo", "M. Shah"], "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "A threshold selection method from gray-level histograms", "author": ["N. Otsu"], "venue": "IEEE Trans. on Systems, Man and Cybernetics,", "citeRegEx": "Otsu.,? \\Q1979\\E", "shortCiteRegEx": "Otsu.", "year": 1979}, {"title": "A unified approach to the change of resolution: Space and gray-level", "author": ["S. Peleg", "M. Werman", "H. Rom"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Peleg et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Peleg et al\\.", "year": 1989}, {"title": "Learnability and Cognition", "author": ["S. Pinker"], "venue": null, "citeRegEx": "Pinker.,? \\Q1989\\E", "shortCiteRegEx": "Pinker.", "year": 1989}, {"title": "Action MACH: A spatio-temporal maximum average correlation height filter for action recognition", "author": ["M.D. Rodriguez", "J. Ahmed", "M. Shah"], "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "Rodriguez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 2008}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "IEEE Trans. on Acoustics, Speech and Signal Processing,", "citeRegEx": "Sakoe and Chiba.,? \\Q1978\\E", "shortCiteRegEx": "Sakoe and Chiba.", "year": 1978}, {"title": "Recognizing human actions: A local SVM approach", "author": ["C. Schuldt", "I. Laptev", "B. Caputo"], "venue": "In Proceedings of the Seventeenth International Conf. on Pattern Recognition,", "citeRegEx": "Schuldt et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Schuldt et al\\.", "year": 2004}, {"title": "Good features to track", "author": ["J. Shi", "C. Tomasi"], "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "Shi and Tomasi.,? \\Q1994\\E", "shortCiteRegEx": "Shi and Tomasi.", "year": 1994}, {"title": "A maximum-likelihood approach to visual event classification", "author": ["J.M. Siskind", "Q. Morris"], "venue": "In Proceedings of the Fourth European Conf. on Computer Vision,", "citeRegEx": "Siskind and Morris.,? \\Q1996\\E", "shortCiteRegEx": "Siskind and Morris.", "year": 1996}, {"title": "Detection and tracking of point features", "author": ["C. Tomasi", "T. Kanade"], "venue": "Technical Report CMU-CS-91-132,", "citeRegEx": "Tomasi and Kanade.,? \\Q1991\\E", "shortCiteRegEx": "Tomasi and Kanade.", "year": 1991}, {"title": "Convolutional codes and their performance in communication systems", "author": ["A.J. Viterbi"], "venue": "IEEE Trans. Communication,", "citeRegEx": "Viterbi.,? \\Q1971\\E", "shortCiteRegEx": "Viterbi.", "year": 1971}, {"title": "Human action recognition by semilatent topic models", "author": ["Y. Wang", "G. Mori"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Wang and Mori.,? \\Q2009\\E", "shortCiteRegEx": "Wang and Mori.", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "A common assumption in Linguistics (Jackendoff, 1983; Pinker, 1989) is that verbs typically characterize the interaction between event participants in terms of the gross changing motion of these participants.", "startOffset": 35, "endOffset": 67}, {"referenceID": 9, "context": "A common assumption in Linguistics (Jackendoff, 1983; Pinker, 1989) is that verbs typically characterize the interaction between event participants in terms of the gross changing motion of these participants.", "startOffset": 35, "endOffset": 67}, {"referenceID": 1, "context": "Such work typically classifies events using object and image characteristics and fine-grained shape and motion features, such as spatiotemporal volumes (Blank et al., 2005; I. Laptev and Rozenfeld, 2008; Rodriguez et al., 2008) and tracked feature points (Liu et al.", "startOffset": 152, "endOffset": 227}, {"referenceID": 10, "context": "Such work typically classifies events using object and image characteristics and fine-grained shape and motion features, such as spatiotemporal volumes (Blank et al., 2005; I. Laptev and Rozenfeld, 2008; Rodriguez et al., 2008) and tracked feature points (Liu et al.", "startOffset": 152, "endOffset": 227}, {"referenceID": 6, "context": ", 2008) and tracked feature points (Liu et al., 2009; Schuldt et al., 2004; Wang and Mori, 2009).", "startOffset": 35, "endOffset": 96}, {"referenceID": 12, "context": ", 2008) and tracked feature points (Liu et al., 2009; Schuldt et al., 2004; Wang and Mori, 2009).", "startOffset": 35, "endOffset": 96}, {"referenceID": 17, "context": ", 2008) and tracked feature points (Liu et al., 2009; Schuldt et al., 2004; Wang and Mori, 2009).", "startOffset": 35, "endOffset": 96}, {"referenceID": 1, "context": "For example, the distinctions between wave1 and wave2 or jump and pjump in the WEIZMANN dataset (Blank et al., 2005) or the distinctions between Golf-Swing-Back, Golf-Swing-Front, and Golf-Swing-Side; Kicking-Front and Kicking-Side; or Swing-Bench and Swing-SideAngle in the SPORTS ACTIONS dataset (Rodriguez et al.", "startOffset": 96, "endOffset": 116}, {"referenceID": 10, "context": ", 2005) or the distinctions between Golf-Swing-Back, Golf-Swing-Front, and Golf-Swing-Side; Kicking-Front and Kicking-Side; or Swing-Bench and Swing-SideAngle in the SPORTS ACTIONS dataset (Rodriguez et al., 2008) do not correspond to distinctions in verb semantics.", "startOffset": 189, "endOffset": 213}, {"referenceID": 6, "context": "The event classes side and jack in the WEIZMANN dataset, the event classes Swing-Bench and Swing-SideAngle in the SPORTS ACTIONS dataset, and the vast majority of the event classes in the UCF50 dataset (Liu et al., 2009) (e.", "startOffset": 202, "endOffset": 220}, {"referenceID": 12, "context": "The videos in the KTH dataset (Schuldt et al., 2004) do not reflect the true meanings of any verbs, let alone boxing or clapping or waving ones hands.", "startOffset": 30, "endOffset": 52}, {"referenceID": 17, "context": "the BALLET dataset (Wang and Mori, 2009)) are described by nouns, not verbs, and often are not part of common lay vocabulary.", "startOffset": 19, "endOffset": 40}, {"referenceID": 6, "context": "golf swing, tennis swing, and swing in the YOUTUBE dataset (Liu et al., 2009) reflect distinctions in event participants, not the semantics of the verb swing.", "startOffset": 59, "endOffset": 77}, {"referenceID": 3, "context": "For each of the remaining ones, we manually cropped a collection of between 1,500 and 2,100 exemplars (combining both positive and negative samples) to train a part-based object detector (Felzenszwalb et al., 2010).", "startOffset": 187, "endOffset": 214}, {"referenceID": 3, "context": "\u2019s (Felzenszwalb et al., 2010) part-based object detector as a detection source to produce axis-aligned rectangles (henceforth detection boxes or simply detections or boxes) as a function of time.", "startOffset": 3, "endOffset": 30}, {"referenceID": 13, "context": "To provide for robust production of coherent object tracks that are necessary for successful event classification, we compensate for the remaining false negatives by projecting each detection box in each frame forward a fixed number of frames using the Kanade-Lucas-Tomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker.", "startOffset": 279, "endOffset": 326}, {"referenceID": 15, "context": "To provide for robust production of coherent object tracks that are necessary for successful event classification, we compensate for the remaining false negatives by projecting each detection box in each frame forward a fixed number of frames using the Kanade-Lucas-Tomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker.", "startOffset": 279, "endOffset": 326}, {"referenceID": 16, "context": "The edges are weighted with a cost that inversely measures coherence and we search for a path from the first to last frames with minimal total edge weight using a dynamic-programming algorithm (Viterbi, 1971) that finds a global optimum.", "startOffset": 193, "endOffset": 208}, {"referenceID": 7, "context": "We take the threshold to be the minimum of the value that maximizes the between-class variance (Otsu, 1979) when bipartitioning this histogram and the learned detector-confidence threshold offset by a fixed, but small, amount (0.", "startOffset": 95, "endOffset": 107}, {"referenceID": 8, "context": "We then augment the edge-weight function to include a coherence measure on object appearance, taking this coherence measure to be Earth Mover\u2019s distance (Peleg et al., 1989) between the corresponding histograms.", "startOffset": 153, "endOffset": 173}, {"referenceID": 0, "context": "To evaluate the hypothesis that it is possible to classify events solely on the basis of the gross changing motion of the event participants and demonstrate the insensitivity of this hypothesis to the choice of time-series classifier, we have run two parallel sets of experiments, one with HMMs (Baum and Petrie, 1966) and one with DTW (Ellis, 2003; Sakoe and Chiba, 1978).", "startOffset": 295, "endOffset": 318}, {"referenceID": 2, "context": "To evaluate the hypothesis that it is possible to classify events solely on the basis of the gross changing motion of the event participants and demonstrate the insensitivity of this hypothesis to the choice of time-series classifier, we have run two parallel sets of experiments, one with HMMs (Baum and Petrie, 1966) and one with DTW (Ellis, 2003; Sakoe and Chiba, 1978).", "startOffset": 336, "endOffset": 372}, {"referenceID": 11, "context": "To evaluate the hypothesis that it is possible to classify events solely on the basis of the gross changing motion of the event participants and demonstrate the insensitivity of this hypothesis to the choice of time-series classifier, we have run two parallel sets of experiments, one with HMMs (Baum and Petrie, 1966) and one with DTW (Ellis, 2003; Sakoe and Chiba, 1978).", "startOffset": 336, "endOffset": 372}], "year": 2012, "abstractText": "We present an approach to labeling short video clips with English verbs as event descriptions. A key distinguishing aspect of this work is that it labels videos with verbs that describe the spatiotemporal interaction between event participants, humans and objects interacting with each other, abstracting away all object-class information and fine-grained image characteristics, and relying solely on the coarse-grained motion of the event participants. We apply our approach to a large set of 22 distinct verb classes and a corpus of 2,584 videos, yielding two surprising outcomes. First, a classification accuracy of greater than 70% on a 1-out-of-22 labeling task and greater than 85% on a variety of 1-out-of10 subsets of this labeling task is independent of the choice of which of two different time-series classifiers we employ. Second, we achieve this level of accuracy using a highly impoverished intermediate representation consisting solely of the bounding boxes of one or two event participants as a function of time. This indicates that successful event recognition depends more on the choice of appropriate features that characterize the linguistic invariants of the event classes than on the particular classifier algorithms. \u2217 Corresponding author. Email: qobi@purdue.edu. Additional images and videos as well as all code and datasets are available at http://engineering.purdue. edu/ \u0303qobi/arxiv2012d.", "creator": "LaTeX with hyperref package"}}}