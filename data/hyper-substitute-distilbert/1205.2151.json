{"id": "1205.2151", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2012", "title": "A Converged Algorithm for Tikhonov Regularized Nonnegative Matrix Factorization with Automatic Regularization Parameters Determination", "abstract": "we present weakly converged argument like tikhonov regularized nonnegative matrix factorization ( nmf ). must never choose this problem or also is known that tikhonov regularized elliptic square ( atm ) is the more preferable form is solving linear euclidean arithmetic when the conventional matrix. because median absolute problem appears rarely decomposed like convex operations, it commonly be expected that uniformly regularized nmf will satisfy without corresponding appropriate approach in confronting linear problems. variance combination considers essential towards additive scaling parameter therefore notably been shown to facilitate identity guarantee. we produce the algorithm with specialized compiler to independently determine the regularization parameters based on the l - curve, a only - known concept in the inverse solving community, often is rather unknown in the stability approximation. the basic to efficient algorithm thus serves two inverse problems in tikhonov regularized nmf complexity applications, i. e., composition guarantee and log constant approximation.", "histories": [["v1", "Thu, 10 May 2012 03:31:39 GMT  (140kb,S)", "http://arxiv.org/abs/1205.2151v1", "Preliminary result without experimental result"]], "COMMENTS": "Preliminary result without experimental result", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andri mirzal"], "accepted": false, "id": "1205.2151"}, "pdf": {"name": "1205.2151.pdf", "metadata": {"source": "CRF", "title": "A Converged Algorithm for Tikhonov Regularized Nonnegative Matrix Factorization with Automatic Regularization Parameters Determination", "authors": ["Andri Mirzal"], "emails": ["andrimirzal@utm.my"], "sections": [{"heading": null, "text": "Index Terms\u2014converged algorithm, inverse problems, L-curve, nonnegative matrix factorization, Tikhonov regularization.\nI. INTRODUCTION\nTHE nonnegative matrix factorization (NMF) is a tech-nique that decomposes a nonnegative matrix into a pair of other nonnegative matrices. Given a nonnegative matrix A, the NMF seeks to find two nonnegative matrices B and C such that:\nA \u2248 BC, (1)\nwhere A \u2208 RM\u00d7N+ = [a1, . . . , aN ], B \u2208 R M\u00d7R + = [b1, . . . ,bR], C \u2208 R R\u00d7N + = [c1, . . . , cN ], R denotes the number of factors which usually is chosen so that R \u226a min(M,N), and RM\u00d7N+ denotes M by N matrix with nonnegative entries. The conventional method in computing B and C is by minimizing the distance between A and BC in Frobenius norm,\nmin B,C\nJ(B,C) = 1\n2 \u2016A\u2212BC\u20162F s.t. B \u2265 0,C \u2265 0. (2)\nIn addition, other criteria like Kullback-Leibler divergence [1], [2] and Csisza\u0301rs \u03d5-divergence [3] can also be used.\nPreviously, the NMF was studied under the term positive matrix factorization by Paatero et al. [4], [5]. The popularity of the NMF is due to the work of Lee and Seung [6] in which they introduced a simple yet powerful NMF algorithm, and then show its applicability in image processing and text analysis.\nA. Mirzal is with the Faculty of Computer Science and Information Systems, University of Technology Malaysia, 81310 Johor Bahru, Malaysia e-mail: andrimirzal@utm.my\nIn addition, the algorithm also produces sparser factors (thus requires less storage) [6]\u2013[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9]. Due to these reasons, many works explored the possibility of applying the NMF in some problem domains, e.g., document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]\u2013[17], and showed that the NMF can give better results.\nRecently, various works have been conducted to extend the standard NMF formulation (eq. 2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]\u2013[24]. These constraints are usually formulated based on inherent properties of the data and prior knowledge about the applications, so that computed solutions can be directed to have desired characteristics. Algorithms for solving the problems are mainly based on multiplicative update rules algorithms. This is due to the convenience of deriving algorithm directly from corresponding objective function. However, as multiplicative update rules based NMF algorithms do not have convergence guarantee [9], [18], [19], [25], the development of converged algorithms for various NMF objectives with auxiliary constraints is an open research problem. Note that even though some alternating nonnegativity-constrained least square based NMF algorithms (e.g., projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it\u2019s not always clear how to incorporate those auxiliary constraints into the algorithms.\nIn this work, we propose a converged algorithm for Tikhonov regularized NMF using additive update rules. The additive update rules based algorithm for standard NMF first appeared in the work of Lee & Seung [1], but the convergence proof was given by Lin in ref. [18]. As in the multiplicative update rules version, the additive update rules based algorithm can also be derived directly from corresponding NMF objective, thus providing a convenient way in deriving converged algorithms for various NMF objectives.\nWe choose Tikhonov regularization as the auxiliary constraint because this constraint has been used in many applications, e.g., text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.e., the Tikhonov regularization, instead of L1\n3 norm\u2014the more appropriate constraint for enforcing sparseness [31]), and showed that it can offer better results compared to the results of standard NMF. This constraint also can reduce influence of noise and other uncertainties in the data [9], [12], [13]. In addition, from the inverse problem study, it is known that Tikhonov regularized least square (LS) is the more preferable form in solving inverse problems because solutions of the conventional LS tend to be unstable and dominated by data and rounding errors [32]\u2013[34]. Moreover, in the presence of noise, frequently the conventional LS solutions are rather undesirable as it leads to amplification of noise in the direction of singular vectors with small singular values [35]. Since LS is the building block of the NMF,\n\u2016A\u2212BC\u20162F = N \u2211\nn=1\n\u2016an \u2212Bcn\u2016 2 F , (3)\nthen it can be expected that Tikhonov regularized NMF will be the more appropriate approach to solving NMF problem in eq. 1.\nIntroducing Tikhonov regularization into the NMF brings the issue of how to properly determine the regularization parameters. From the inverse problems study it is known that the effectiveness of regularization methods depends strongly on the parameters; too much regularization creates a loss of information, and too little regularization leads to a solution that is dominated by noise components and has similar problems as in the unregularized solutions. There are two methods that are usually be used in determining an appropriate value for the regularization parameter: the L-curve and Morozov discrepancy principles. In this paper we will utilize the L-curve since the Morozov discrepancy principles require knowledge of the error level in the data which is often inaccessible [36].\nThe L-curve is a graphical tool that displays the tradeoff between approximation error and solution size as the regularization parameter varies. In this curve, the proper value for the regularization parameter is the value associated with corner of the curve where both solution and approximation error have minimum norms [32], [34]. There are some methods proposed to find this L-corner, e.g., [32], [34], [37], [38]. We will use a method proposed by Oraintara et al. in ref. [34] in which they defined L-corner to be the point of tangency between L-curve with positive curvature and a straight line of negative slope. Fig. 1 shows such condition for L-corner. We choose this method because it has convergence guarantee and is relatively faster to compute than the standard method; the maximum curvature approach [32].\nII. TIKHONOV REGULARIZED NMF\nTikhonov regularization is a method for regularizing solutions of linear inverse problems in order to enhance stability of the solutions and reduce the observational errors. The method was developed independently by Phillips [39] and Tikhonov [40]. In statistic it is also known as ridge regression. Because Tikhonov regularization can smooth the solutions, it is often used as a smoothness constraint.\nGiven a linear inverse problem,\ny = Ax (4)\nFig. 1. The generic L-curve with positive curvature; a is the L-corner.\nwhere x \u2208 RM denotes unknown vector to be estimated, y \u2208 RN denotes observation data, and A \u2208 RN\u00d7M denotes distortion matrix. The classical approach is to use standard LS approach to estimate x,\nx = argmin x\n\u2016y \u2212Ax\u20162F . (5)\nTo improve the solution, usually Tikhonov regularized LS is used instead [32]:\nx\u03bb = argmin x\n\u2016y \u2212Ax\u20162F + \u03bb\u2016x\u2016 2 F (6)\nwhere \u03bb denotes nonnegative regularization parameter, \u2016y \u2212 Ax\u20162F denotes approximation error, and \u2016x\u2016 2 F denotes solution size. In [34], Oraintara et al. proposed an iterative algorithm to compute \u03bb based on the L-curve criterion depicted in fig. 1. In summary, x and \u03bb can be computed using algorithm 1:\nAlgorithm 1 Iterative algorithm for computing x and \u03bb.\nInitialize x(0) and \u03bb(0). Set k \u2190 0 repeat\nk \u2190 k + 1\nx(k) \u2190 argmin x \u2016y \u2212Ax\u20162F + \u03bb (k\u22121)\u2016x\u20162F (7) \u03bb(k) \u2190 |\u03b3| \u2016y\u2212Ax(k)\u20162F\n\u2016x(k)\u20162F (8)\nerror \u2190 \u2016x(k) \u2212 x(k\u22121)\u2016\nx(k\u22121)\nuntil error \u2264 \u01eb\nwhere \u03b3 is the slope of the straight line that is tangent to the Lcurve and \u01eb is a small nonnegative number that is set to 0.001 in the authors\u2019 work [34]. Note that the value of \u03b3 doesn\u2019t influence convergence property of sequence x(k) and \u03bb(k), and as long as \u03bb(0) is sufficiently small then \u03bb(k) converges to a stationary point [34]. The similar method for computing \u03bb can also be found in ref. [33], but the authors fixed \u03b3 value to one.\nWe will now derive formulation for Tikhonov regularized NMF. The NMF problem in eq. 2 is known to be nonconvex\n4 and may have several local mimima [25]. The common practice to deal with the nonconvexity of an optimization problem is by transforming it into convex subproblems [41]. In the case of the NMF, this can be done by employing the alternating strategy; fixing one matrix while solving for the other [25] (apparently, all NMF algorithms utilizing alternating strategy). This strategy transforms an NMF problem into a pair of convex subproblems. The following equations give convex subproblems of the NMF,\nB = argmin B\u22650\n1 2 \u2016A\u2212BC\u20162F (9)\nC = argmin C\u22650\n1 2 \u2016A\u2212BC\u20162F . (10)\nAlternatingly solving eq. 9 for B and eq. 10 for C is known as alternating nonnegativity-constrained LS (ANLS), and usually each of these subproblems is solved by decomposing it into a series of corresponding nonnegativity-constraint LS (NNLS) problems. The following equations are the NNLS versions of eq. 9 and eq. 10.\nbTm = arg min bm\u22650\n1 2 \u2016aTm \u2212C T bTm\u2016 2 F , \u2200m (11)\ncn = arg min cn\u22650\n1 2 \u2016an \u2212Bcn\u2016 2 F , \u2200n, (12)\nwhere bm and am denotes the m-th row of B and A respectively.\nAs shown, each of these NNLS problems is exactly the standard LS problem with additional nonnegativity constraint. Accordingly, Tikhonov regularization can be employed to improve the solutions. The following equations give Tikhonov regularized version of the above NNLS problems.\nbTm = arg min bm\u22650\n1 2 \u2016aTm \u2212C T bTm\u2016 2 F + 1 2 \u03b2m\u2016b T m\u2016 2 F \u2200m, (13)\ncn = arg min cn\u22650\n1 2 \u2016an \u2212Bcn\u2016 2 F + 1 2 \u03b1n\u2016cn\u2016 2 F , \u2200n, (14)\nwhere \u03b1n and \u03b2m denotes the corresponding nonnegative regularization parameters. By rearranging rows of B and columns of C back, Tikhonov regularized NMF can be written as:\nB = argmin B\u22650\n1 2 \u2016A\u2212BC\u20162F + 1 2 \u2016 \u221a \u03b2B\u20162F , (15)\nC = argmin C\u22650\n1 2 \u2016A\u2212BC\u20162F + 1 2 \u2016C \u221a \u03b1\u20162F , (16)\nwhere \u03b2 = diag(\u03b21, . . . , \u03b2M ) and \u03b1 = diag(\u03b11, . . . , \u03b1N ). The following gives a generic algorithm for Tikhonov regularized NMF where update rules for \u03b1n and \u03b2m are derived based on the work of Oraintara et al. [34], \u03b3Bm and \u03b3 C n are defined similarly as in algorithm 1, and \u01eb denotes small positive number.\nIII. A CONVERGED ALGORITHM FOR TIKHONOV REGULARIZED NMF\nWe will now present a converged algorithm for Tikhonov regularized NMF based on additive update rules. By combining update rules for B and C in eq. 15 and eq. 16, we define\nAlgorithm 2 A generic algorithm for Tikhonov regularized NMF.\nInitialize B(0), C(0), \u03b1(0), and \u03b2(0). Set k \u2190 0 repeat\nB(k+1) \u2190 argmin B\u22650\n1 2 \u2016A\u2212BC(k)\u20162F + 1 2 \u2016 \u221a \u03b2 (k) B\u20162F\nC(k+1) \u2190 arg min C\u22650\n1 2 \u2016A\u2212B(k+1)C\u20162F + 1 2 \u2016C \u221a \u03b1 (k) \u20162F\n\u03b2(k+1)m \u2190 |\u03b3 B m|\n\u2016aTm \u2212C (k)T b (k+1)T m \u20162F\n\u2016b (k+1)T m \u20162F\n, \u2200m\n\u03b1(k+1)n \u2190 |\u03b3 C n |\n\u2016an \u2212B (k+1)c (k+1) n \u20162F\n\u2016c (k+1) n \u20162F\n, \u2200n\nk \u2190 k + 1 until\nmax ( \u2207BJ ( B ) \u2299B ) \u2264 \u01eb &max ( \u2207CJ ( C ) \u2299C ) \u2264 \u01eb\nobjective function for Tikhonov regularized NMF as follows:\nmin B,C\nJ(B,C) = 1\n2 \u2016A\u2212BC\u20162F +\n1 2 \u2016 \u221a \u03b2B\u20162F + 1 2 \u2016C \u221a \u03b1\u20162F\n(17)\ns.t. B \u2265 0,C \u2265 0.\nThe Karush-Kuhn-Tucker (KKT) function of the objective can be written as:\nL(B,C) = J(B,C)\u2212 tr ( \u0393BB T ) \u2212 tr ( \u0393CC ) .\nwhere \u0393B \u2208 RM\u00d7R and \u0393C \u2208 RN\u00d7R denotes the KKT multipliers. Partial derivatives of L with respect to B and C are:\n\u2207BL(B) = \u2207BJ(B)\u2212 \u0393B, \u2207CL(C) = \u2207CJ(C)\u2212 \u0393 T C,\nwith\n\u2207BJ(B) = BCC T \u2212ACT + \u03b2B, \u2207CJ(C) = B TBC\u2212BTA+C\u03b1.\nBy results from optimization studies, (B\u2217,C\u2217) is a stationary point of eq. 17 if it satisfies the KKT optimality conditions [42], i.e.,\nB\u2217 \u2265 0, C\u2217 \u2265 0, (18)\n\u2207BJ(B \u2217) = \u0393B \u2265 0, \u2207CJ(C \u2217) = \u0393TC \u2265 0, (19)\n\u2207BJ(B \u2217)\u2299B\u2217 = 0, \u2207CJ(C \u2217)\u2299C\u2217 = 0, (20)\nwhere \u2299 denotes Hadamard products (component-wise multiplications), and eq. 20 is known as the complementary slackness.\n5 Multiplicative update rules based algorithm for Tikhonov regularized NMF can be derived by utilizing the complementary slackness:\n( BCCT \u2212ACT + \u03b2B )\n\u2299B = 0, (\nBTBC\u2212BTA+C\u03b1 ) \u2299C = 0.\nThese equations lead to the following update rules:\nbmr \u2190\u2212 bmr\n( ACT )\nmr (\nBCCT + \u03b2B )\nmr\n\u2200m, r (21)\ncrn \u2190\u2212 crn\n( BTA )\nrn (\nBTBC+C\u03b1 )\nrn\n\u2200r, n (22)\nwhere bmr and crn denote (m, r) entry of B and (r, n) entry of C respectively.\nAs stated by Lin [18], the above multiplicative update rules based algorithm can be modified into an equivalent converged algorithm by (1) using additive update rules, and (2) replacing zero entries that do not satisfy the KKT conditions with a small positive number to escape the zero locking.\nThe additive update rules version of the algorithm can be written as:\nbmr \u2190\u2212 bmr \u2212 bmr (\nBCCT + \u03b2B )\nmr\n\u2207BJ(B)mr,\ncrn \u2190\u2212 crn \u2212 crn (\nBTBC+C\u03b1 )\nrn\n\u2207CJ(C)rn.\nBy inspection it is clear that this algorithm also suffers from the zero locking, i.e.:\n\u2207BJ(B)mr < 0 & bmr = 0, or\n\u2207CJ(C)rn < 0 & crn = 0,\nsuch that when these conditions happened, the algorithm can no longer update the corresponding entries even though those entries haven\u2019t satisfied the KKT optimality conditions in eq. 19.\nAlgorithm 3 gives necessary modifications to avoid zero locking and\u2014as will be shown later\u2014also has convergence guarantee, where \u2298 denotes component-wise division,\nb\u0304(k)mr \u2261\n{\nb (k) mr if \u2207BJ ( B(k),C(k) )\nmr \u2265 0\nmax(b (k) mr, \u03c3) if \u2207BJ ( B(k+1),C(k) )\nmr < 0\n,\n(23)\nc\u0304(k)rn \u2261\n{\nc (k) rn if \u2207CJ ( B(k+1),C(k) )\nrn \u2265 0\nmax(c (k) rn , \u03c3) if \u2207CJ ( B(k+1),C(k) )\nrn < 0\n,\n(24)\ndenote the modifications to avoid the zero locking with \u03c3 is a small positive number, \u03b4B and \u03b4C denote small positive numbers that introduced to avoid division by zeros, B\u0304 and C\u0304 denote matrices that contain b\u0304mr and c\u0304rn respectively, and\n\u2207BJ(B (k),C(k)) = B(k)C(k)C(k)T \u2212AC(k)T\n+ \u03b2(k)B(k),\n\u2207CJ(B (k+1),C(k)) = B(k+1)TB(k+1)C(k) \u2212B(k+1)TA\n+C(k)\u03b1(k).\nAlgorithm 3 A converged algorithm for Tikhonov regularized NMF.\nInitialization B(0) \u2265 0, C(0) \u2265 0, \u03b2(0)m \u2265 0 \u2200m, and \u03b1 (0) n \u2265 0 \u2200n. k \u2190 0 repeat\nB(k+1) \u2190 B(k) \u2212 B\u0304(k) \u2299\u2207BJ(B (k),C(k))\u2298\n( B\u0304(k)C(k)C(k)T + \u03b2(k)B\u0304(k) + \u03b4B )\n(25)\nC(k+1) \u2190 C(k) \u2212 C\u0304(k) \u2299\u2207CJ(B (k+1),C(k))\u2298\n( B(k+1)TB(k+1)C\u0304(k) + C\u0304(k)\u03b1(k) + \u03b4C ) (26)\n\u03b2(k+1)m \u2190 |\u03b3 B m|\n\u2016aTm \u2212C (k)T b (k+1)T m \u20162F\n\u2016b (k+1)T m \u20162F + \u03b4B\n, \u2200m (27)\n\u03b1(k+1)n \u2190 |\u03b3 C n |\n\u2016an \u2212B (k+1)c (k+1) n \u20162F\n\u2016c (k+1) n \u20162F + \u03b4C\n, \u2200n (28)\nk \u2190 k + 1 until\nmax ( \u2207BJ ( B ) \u2299B ) \u2264 \u01eb &max ( \u2207CJ ( C ) \u2299C ) \u2264 \u01eb\nwhere 0 < step < 1. Note that since algorithm 3 is free from the zero locking, B and C can be initialized using nonnegative matrices. Theorem 1 explains this formally.\nTheorem 1: If B0 > 0 and C0 > 0, then Bk > 0 and Ck > 0, \u2200k \u2265 0. And if B0 \u2265 0 and C0 \u2265 0, then Bk \u2265 0 and Ck \u2265 0, \u2200k \u2265 0.\nProof: This statement is clear for k = 0, so we need only to prove for k > 0. Case 1: \u2207BJmr \u2265 0 \u21d2 b\u0304mr = bmr (see b\u0304mr definition in eq. 23).\nb(k+1)mr =\n( B(k)C(k)C(k)T + \u03b2(k)B(k) ) mr b (k) mr + \u03b4Bb k mr\n( B(k)C(k)C(k)T + \u03b2(k)B(k) )\nmr + \u03b4B\n\u2212\n( B(k)C(k)C(k)T \u2212AC(k)T + \u03b2(k)B(k) ) mr b (k) mr\n( B(k)C(k)C(k)T + \u03b2(k)B(k) )\nmr + \u03b4B\n=\n(\n\u03b4B +AC (k)T\n) mr b (k) mr\n( B(k)C(k)C(k)T + \u03b2(k)B(k) )\nmr + \u03b4B\n.\nThus \u2200k > 0, b(k)mr > 0 \u21d2 b (k+1) mr > 0 \u2200m, r, and b (k) mr \u2265 0 \u21d2 b (k+1) mr \u2265 0 \u2200m, r. Case 2: \u2207BJmr < 0 \u21d2 b\u0304mr 6= bmr.\nb(k+1)mr = b (k) mr \u2212\nmax ( b (k) mr, \u03c3 ) \u2207BJ ( B(k),C(k) )\nmr (\nB\u0304(k)C(k)C(k)T + \u03b2(k)B\u0304(k) )\nmr + \u03b4B\n.\nBecause max ( b (k) mr, \u03c3 ) > 0 and \u2207BJ ( B(k),C(k) )\nmr < 0,\nthen \u2200k > 0, b(k)mr > 0 \u21d2 b (k+1) mr > 0 \u2200m, r, and b (k) mr \u2265 0 \u21d2 b (k+1) mr > 0 \u2200m, r.\n6 Case 3: \u2207CJrn \u2265 0 \u21d2 c\u0304rn = crn.\nc(k+1)rn =\n( B(k+1)TB(k+1)C(k) +C(k)\u03b1(k) ) rn c (k) rn + \u03b4Cc (k) rn\n( B(k+1)TB(k+1)C(k) +C(k)\u03b1(k) )\nrn + \u03b4C\n\u2212\n( B(k+1)TB(k+1)C(k) \u2212B(k+1)TA+C(k)\u03b1(k) ) rn c (k) rn\n( B(k+1)TB(k+1)C(k) +C(k)\u03b1(k) )\nrn + \u03b4C\n=\n(\n\u03b4C +B (k+1)TA\n) rn c (k) rn\n( B(k+1)TB(k+1)C(k) +C(k)\u03b1(k) )\nrn + \u03b4C\n,\nThus \u2200k > 0, ckrn > 0 \u21d2 c (k+1) rn > 0 \u2200r, n, and ckrn \u2265 0 \u21d2 c (k+1) rn \u2265 0 \u2200r, n. Case 4: \u2207CJrn < 0 \u21d2 c\u0304rn 6= crn.\nc(k+1)rn = c (k) rn \u2212\nmax ( c (k) rn , \u03c3 ) \u2207CJ ( B(k+1),C(k) )\nrn (\nB(k+1)TB(k+1)C\u0304(k) + C\u0304(k)\u03b1(k) )\nrn + \u03b4C\n.\nBecause max ( c (k) rn , \u03c3 ) > 0 and \u2207CJ ( B(k+1),C(k) )\nrn < 0,\nthen \u2200k > 0, c(k)rn > 0 \u21d2 c (k+1) rn > 0 \u2200r, n, and c (k) rn \u2265 0 \u21d2 c (k+1) rn > 0 \u2200r, n.\nBy combining results for k = 0 and k > 0 in case 1-4, the proof is completed.\nAppendix A gives Matlab/Octave codes for implementing algorithm 3.\nIV. CONVERGENCE ANALYSIS\nThere are two type of update rules in algorithm 3. The first is the update rules for B(k) and C(k), and the second is the update rules for \u03b2(k) and \u03b1(k). Since the algorithm 3 uses alternating strategy in updating these variables, the convergence analysis can be carried out separately. This approach is known as the block-coordinate descent method [42].\nTo derive the convergence guarantee of solution sequence {\nB(k),C(k),\u03b2(k),\u03b1(k) }\n, we will first show the convergence of sequence { B(k),C(k) } , and then sequence { \u03b2(k),\u03b1(k) }\n. From convergence analysis study, the following conditions must be satisfied for sequence { B(k),C(k) }\nto have convergence guarantee [18], [25], [44].\n1) The nonincreasing property of sequence J ( B(k),C(k) )\n, i.e.,\na) J ( B(k+1) ) \u2264 J ( B(k) )\nand b) J ( C(k+1) ) \u2264 J ( C(k) ) .\n2) Any limit point of sequence { B(k),C(k) }\ngenerated by algorithm 3 is a stationary point.\n3) Sequence { B(k),C(k) } has at least one limit point.\nA. The nonincreasing property of sequence J ( B(k) )\nWe will utilize the auxiliary function approach introduced in [1] to prove this property. By using the auxiliary function as an intermediate function, the nonincreasing property of J ( B(k) ) can be restated with:\nJ ( B(k+1) ) = G ( B(k+1),B(k+1) ) \u2264 G ( B(k+1),B(k) )\n\u2264 G ( B(k),B(k) ) = J ( B(k) ) .\nTo define G, let\u2019s rearrange B into:\nBT \u2261\n\n    \nbT1\nbT2\n. . .\nbTM\n\n    \nwhere bm denotes the m-th row of B. And also let\u2019s define:\n\u2207BT J ( BkT ) \u2261\n\n   \n\u2207BJ ( B(k) )T\n1\n. . .\n\u2207BJ ( B(k) )T\nM\n\n   \nwhere \u2207BJ ( B(k) ) m denotes the m-th row of \u2207BJ(B(k)) = B(k)C(k)C(k)T \u2212AC(k)T + \u03b2(k)B(k). Then define,\nD \u2261 diag ( D1, . . . ,DM )\nwhere Dm denotes a diagonal matrix with its diagonal entries defined as:\ndmrr \u2261\n\n\n\n( B\u0304(k)C(k)C(k)T +\u03b2(k)B\u0304(k) )\nmr +\u03b4B\nb\u0304 (k) mr\nif r \u2208 Im\n\u22c6 if r /\u2208 Im\nwith\nIm \u2261 { r|b(k)mr > 0, \u2207BJ ( B(k) ) mr 6= 0, or\nb(k)mr = 0, \u2207BJ ( B(k) ) mr < 0 }\ndenotes the set of non-KKT indices in m-th row of B(k), and \u22c6 is defined so that \u22c6 \u2261 0 and \u22c6\u22121 \u2261 0.\nThe auxiliary function G can be defined as:\nG ( BT ,B(k)T ) \u2261 J ( B(k)T ) + tr {( B\u2212B(k) ) \u2207BT J ( B(k)T )}\n+ 1\n2 tr\n{( B\u2212B(k) ) D ( B\u2212B(k) )T} .\n(29)\nNote that J and G are equivalent to J and G with B is rearranged into BT , and other variables are reordered accordingly. And also whenever X(k+1) is a variable, we remove (k + 1) sign. And:\n\u2207BTG ( B T ,B(k)T ) = D ( B\u2212B(k) )T +\u2207BT J ( B (k)T ) .\nBy definition D is positive definite for all B(k) not satisfy the KKT conditions and positive semidefinite if and only if B(k) satisfies the KKT conditions. Thus G ( BT ,B(k)T )\nis a strict convex function, and consequently has a unique minimum, so that:\nD ( B\u2212B(k) )T +\u2207BT J ( B(k)T ) = 0, (30)\nBT = B(k)T \u2212D\u22121\u2207BT J ( B(k)T ) ,\nwhich is exactly the update rule for B in eq. 25. Lemma 1: J ( BT ) can be rewritten as:\nJ ( BT ) = J ( B(k)T ) + tr {( B\u2212B(k) ) \u2207BT J ( B(k)T )}\n+ 1\n2 tr\n{( B\u2212B(k) ) \u22072BJ ( B(k) )( B\u2212B(k) )T} .\n(31)\n7 where\n\u22072BJ ( Bk ) \u2261\n\n  \nC(k)C(k)T + \u03b21I\n. . .\nC(k)C(k)T + \u03b2MI\n\n  \nand I denotes corresponding compatible identity matrix. Proof: Let decompose J ( B ) into:\nJ ( B )\nm =\n1 2 \u2016aTm \u2212C (k)T bTm\u2016 2 F + 1 2 \u03b2m\u2016b T m\u2016 2 F \u2200m,\nso that J ( B ) = J ( B )\n1 . . . J\n( B )\nM . Then,\n\u2202J ( B )\nm\n\u2202bm = \u2212amC\n(k)T + bmC (k)C(k)T + \u03b2mbm\nand \u22022J ( B )\nm \u2202b2m = C(k)C(k)T + \u03b2mI.\nBy using the Taylor series expansion, J ( B )\nm can be rewritten\nas:\nJ ( B )\nm = J\n( B(k) ) m + ( bm \u2212 b (k) m )\n(\n\u2202J ( B )\nm\n\u2202bm\n)T\n+ 1\n2\n(\nbm \u2212 b (k) m\n)\n(\n\u22022J ( B )\nm\n\u2202b2m\n)\n(\nbm \u2212 b (k) m )T ,\nwhich is the m-th row of J ( BT )\n. To prove the nonincreasing property of J ( B(k) )\n, the following statements must be shown:\n1) G ( BT ,BT ) = J ( BT )\n, 2) G ( BkT ,BkT ) = J ( BkT )\n, 3) G ( BT ,BT ) \u2264 G ( BT ,BkT )\n, and 4) G ( BT ,BkT ) \u2264 G ( BkT ,BkT ) .\nThe first and second will be proven in theorem 2, the third in theorem 3, and the fourth in theorem 4.\nTheorem 2: G ( BT ,BT ) = J ( BT ) and G ( B(k)T , B(k)T )\n= J ( B(k)T )\n. Proof: These are obvious from the definition of G in\neq. 29. Theorem 3: G ( BT ,BT ) \u2264 G ( BT ,B(k)T )\n. Moreover if and only if Bk satisfies the KKT conditions in eq. 18\u201320, then G ( BT ,BT ) = G ( BT ,B(k)T )\n. Proof:\nG ( BT ,B(k)T ) \u2212G ( BT ,BT ) =\n1 2 tr { ( B\u2212B(k) ) ( D\u2212\u22072BJ ( B(k) ) ) ( B\u2212B(k) )T } =\n1\n2\nM \u2211\nm=1\n[\n(\nbm \u2212 b (k) m\n)\n(\nDm \u2212 \u22022J\n( B )\nm\n\u2202b2m\n)\n(\nbm \u2212 b (k) m\n)T\n]\nIf Dm \u2212 \u22022J\n( B )\nm\n\u2202b2 m\n\u2200m are all positive definite, then the\ninequality always holds except when bm = bkm \u2200m, where based on theorem 1 and update rule eq. 25 happened if and only if the point has reach a stationary point\u2014a point where the KKT conditions are satisfied. Thus, it is sufficient to prove the positive definiteness of Dm \u2212\u22072BJ ( Bk ) m \u2200m.\nLet vTm = bm \u2212 b (k) m 6= 0, then we must prove:\nvTm\n(\nDm \u2212 \u22022J\n( B )\nm\n\u2202b2m\n)\nvm > 0.\nNote that\ndmrr \u2261\n\n\n\n(\nb\u0304(k) m X(k) m\n)\nr +\u03b4B\nb\u0304 (k) mr\nif r \u2208 Im\n\u22c6 if r /\u2208 Im\nwhere b\u0304(k)m denotes the m-th row of B\u0304(k); and X (k) m = C(k)C(k)T + \u03b2mI and Dm are both symmetric matrix. Thus,\nvTm\n(\nDm \u2212 \u22022J\n( B )\nm\n\u2202b2m\n)\nvm =\nR \u2211\nr=1\nv2r \u03b4B\nb\u0304 (k) mr\n+ R \u2211\nr=1\nv2r\n( X(k)b\u0304 (k)T m )\nr\nb\u0304 (k) mr\n\u2212 R \u2211\nr,s=1\nvrvsx (k) rs >\nR \u2211\nr=1\nv2r\n\u2211R s=1 x (k) rs ( b\u0304 (k) m )\ns\nb\u0304 (k) mr\n\u2212\nR \u2211\nr=1\nR \u2211\ns=1\nvrvsx (k) rs =\n1\n2\nR \u2211\nr=1\nR \u2211\ns=1\nv2r x (k) rs ( b\u0304 (k) m ) s\nb\u0304 (k) mr\n+ 1\n2\nR \u2211\ns=1\nR \u2211\nr=1\nv2s x (k) sr ( b\u0304 (k) m ) r\nb\u0304 (k) ms\n\u2212\nR \u2211\nr=1\nR \u2211\ns=1\nvrvsx (k) rs =\n1\n2\nR \u2211\nr=1\nR \u2211\ns=1\nx(k)rs\n\n\n\u221a\nb\u0304 (k) ms b\u0304 (k) mr vr \u2212\n\u221a\nb\u0304 (k) mr b\u0304 (k) ms vs\n\n\n2\n\u2265 0\nwhere vr denotes the r-th entry of vm and x (k) rs denotes the (r, s) entry of X(k). Therefore, Dm \u2212 \u22022J ( B ) m\n\u2202b2 m \u2200m are all positive definite\nTheorem 4: G ( BT ,B(k)T ) \u2264 G ( B(k)T ,B(k)T )\n. Moreover, if and only if B satisfies the KKT conditions, then G ( BT ,B(k)T ) = G ( B(k)T ,B(k)T )\n. Proof:\nG\n( B(k)T ,B(k)T ) \u2212G ( BT ,B(k)T ) =\n\u2212 tr { ( B\u2212B(k) ) \u2207BT J ( B(k)T ) }\n\u2212 1\n2 tr\n{\n( B\u2212B(k) ) D ( B\u2212B(k) )T\n}\n.\nBy using eq. 30, and the fact that D is positive semi-definite:\nG\n( B(k)T ,B(k)T ) \u2212G ( BT ,B(k)T ) =\n1 2 tr { ( B\u2212B(k) ) D ( B\u2212B(k) )T } \u2265 0,\nwe proved that G ( BT ,B(k)T ) \u2264 G ( B(k)T ,B(k)T )\n. Now, let\u2019s prove the second part of the theorem. By the update rule eq. 25, if B(k) satisfies the KKT conditions, then B will be equal to B(k), and thus the equality holds. Now we need to prove that if the equality holds, then B(k) satisfies the KKT conditions. To prove this, let consider a contradiction where the equality holds but B(k) does not satisfy the KKT\n8 conditions. In this case, there exists at least an index (m, r) such that:\nbmr 6= b (k) mr and d m rr =\n(\nb\u0304 (k) m X (k) m\n)\nr + \u03b4B\nb\u0304 (k) mr\n\u2265 \u03b4B\nb\u0304 (k) mr\n.\nNote that by the definition in eq. 23, if b\u0304(k)mr = 0, then it satisfies the KKT conditions. Accordingly, bmr = b (k) mr which violates the condition for contradiction. So, b\u0304kmr cannot be equal to zero, and thus dmrr is well defined. Consequently,\nG\n( B(k)T ,B(k)T ) \u2212G ( BT ,B(k)T ) \u2265\n(\nbmr \u2212 b (k) mr\n)2\n\u03b4B\nb\u0304 (k) mr\n> 0,\nwhich violates the condition for contradiction. Thus, it is proven that if the equality holds, then B(k) satisfies the KKT conditions.\nThe following theorem summarizes the nonincreasing property of sequence J ( B(k) )\nTheorem 5: J ( B(k+1) ) \u2264 J ( B(k) )\n\u2200k \u2265 0 under update rule eq. 25 with the equality happens if and only if B(k) satisfies the KKT optimality conditions in eq. 18-20. Proof: This is the results of theorem 2, 3, and 4.\nB. The nonincreasing property of sequence J ( C(k) )\nTheorem 6: J ( C(k+1) ) \u2264 J ( C(k) )\n\u2200k \u2265 0 under update rule eq. 26 with the equality happens if and only if C(k) satisfies the KKT optimality conditions in eq. 18-20. Proof: This theorem can be proven similarly as in J ( B(k) ) case.\nC. The nonincreasing property of sequence J ( B(k),C(k) )\nTheorem 7: J ( B(k+1), C(k+1) ) \u2264 J ( B(k+1), C(k) )\n\u2264 J ( B(k), C(k) )\n\u2200k \u2265 0 under update rule eq. 25 and 26 with the equality happens if and only if ( B(k),C(k) )\nsatisfies the KKT optimality conditions in eq. 18-20.\nProof: This statement can be proven by combining the results in theorem 5 and 6.\nBy this theorem, we have proven the first conditions for the algorithm 3 to have convergence guarantee. The next subsection will give proofs for the second and the third condition.\nD. Limit points of sequence { B(k),C(k) }\nTheorem 8: Any limit point of sequence { B(k),C(k) }\ngenerated by algorithm 3 is a stationary point\nProof: By theorem 7, algorithm 3 produces strictly decreasing sequence J ( B(k),C(k) )\nuntil reaching a point that satisfies the KKT conditions. By update rules in eq. 25 and 26, after a point satisfies the KKT conditions, the algorithm will stop updating ( B(k),C(k) )\n, i.e., B(k+1) = B(k) and C(k+1) = C(k) \u2200k \u2265 \u2217, where \u2217 is the iteration where the KKT conditions are satisfied.\nTheorem 9: Sequence { B(k),C(k) }\nhas at least one limit point.\nProof: As stated by Lin [18], it suffices to prove that {\nB(k),C(k) } is in a closed and bounded set. The boundedness\nof this sequence is clear by the objective eq. 17; if there exists l such that lim blmr \u2192 \u221e or lim c l rn \u2192 \u221e, then lim J (\nB(l), C(l) ) \u2192 \u221e > J ( B(0), C(0) )\nwhich violates theorem 7. With nonnegativity guarantee from theorem 1, it is proven that { B(k),C(k) } is in closed and bounded set.\nE. The convergence of sequence { \u03b2(k),\u03b1(k) }\nThe convergence guarantee of this sequence has been established by Oraintara et al. in ref. [34], [43]. Here we will adopt their works and summarize the results in accord to our notations.\nTheorem 10 (Oraintara et al. [34]): The optimum \u03b2m corresponding to the L-corner must satisfy\n\u03b2m \u2225 \u2225bTm \u2225 \u2225 2 F = \u2223 \u2223\u03b3Bm \u2223 \u2223 \u2225 \u2225aTm \u2212C T bTm \u2225 \u2225 2 F .\nThe similar condition can also derived for \u03b1n. Since the update rules for \u03b2(k)m \u2200m and \u03b1 (k) n \u2200n are derived from this theorem, it implies that the update rules find the optimal solutions of these parameters for each iteration.\nNext we state the monotonicity property of sequence \u03b2(k)m and \u03b1(k)m .\nLemma 2 (Oraintara et al. [34]): The values of \u03b2(k)m \u2200m either strictly increase or decrease under update rule eq. 27 unless converged to a limit point. The similar conditions also apply to \u03b1(k)n \u2200n.\nAnd, the following theorem summarizes the convergence property of sequence \u03b2(k)m and \u03b1 (k) n .\nTheorem 11 (Oraintara et al. [34]): If the update rule eq. 27 converges, then it converges to the corresponding Lcorner. The same condition also applies to the update rule eq. 28\nThus, lemma 2 and theorem 11 state that while update rules eq. 27 and 28 generate monotonic updated values for \u03b2(k)m \u2200m and \u03b1(k)n \u2200n which if the sequences converge, then they converge to the corresponding L-corner; there is no guarantee that the sequences will converge. Fortunately, the convergence can be established by choosing appropriate initial values.\nDirectly from ref. [34], the followings summarize the strategy in choosing the initial values.\n1) If an is not in the range B, then choosing \u03b1 (0) n = 0 will\nproduce an increasing sequence of \u03b1(k)n converging to the nearest L-corner. 2) If an is in the range of B, then it is always possible to choose \u03b1(0)n > 0 small enough so that J ( \u03b1 (0) n )\nn > \u03b1\n(0) n\nand convergence occurs. 3) More generally, if lim\u03b1n\u2192\u221e J ( \u03b1n ) n /\u03b1n < 1 and\nJ ( 0 ) > 0, any initial value \u03b1(0)n produces a converging\nsequence \u03b1(k)n . 4) When lim\u03b1n\u2192\u221e J ( \u03b1n ) n /\u03b1n > 1, the only case when\nthe update rule eq. 28 will not generate a converged sequence is when \u03b1(0)n is chosen larger than the last intersection between J ( \u03b1n )\nn and the straight line \u03b1n =\nJ ( \u03b1n )\nn .\nThe same conditions can also be derived for sequence \u03b2 (k) m \u2200m. Thus, by simply choosing \u03b1n = 0 \u2200n and \u03b2m = 0 \u2200m, the update rules eq. 27 and 28 will generate sequence\n9 \u03b2 (k) m \u2200m and \u03b1 (k) n \u2200n that converge to the corresponding Lcorners.\nF. The convergence of the solution sequence\nThe following theorem gives the convergence guarantee of the solution sequence { B(k),C(k),\u03b2(k),\u03b1(k) }\ngenerated by algorithm 3\nTheorem 12: By choosing appropriate initial values \u03b2 (0) m \u2200m and \u03b1 (0) n \u2200n, algorithm 3 generates sequence {\nB(k),C(k) }\nthat converges to a point that satisfies the KKT optimality conditions (a stationary point), and sequence {\n\u03b2(k),\u03b1(k) }\nthat converges to the corresponding L-corners. Proof: By the results of theorem 7, 8, and 9, we know that\nsequence { B(k),C(k) }\nconverges to a point that satisfies the KKT conditions. By discussion in subsection IV-E we know that it is always possible to choose appropriate initial values for \u03b2 (0) m \u2200m and \u03b1 (0) n \u2200n so that sequence { \u03b2 (k),\u03b1(k) }\nconverges to the corresponding L-corners.\nV. DISCUSSION ON THE CONVERGENCE OF ALGORITHM 3\nAlgorithm 3 converges when it stops updating both sequence { B(k),C(k) } and sequence { \u03b2 (k),\u03b1(k) }\n. And as shown in eq. 27 and eq. 28, when { B(k),C(k) }\nhas converged then { \u03b2(k),\u03b1(k) }\nwould also have converged. With the boundedness of B(k) and C(k), \u03b2(k) and \u03b1(k) will also be bounded. Thus, algorithm 3 will be well-behaved through the update steps. This is an important fact since even though sequence J ( B(k),C(k) )\nhas the nonincreasing property, due to lemma 2, algorithm 3 may produce nondecreasing J (\nB(k), C(k), \u03b2(k), \u03b1(k) ) (we will refer this sequence as J ( \u00b7 )\n). This is because the nonincreasing property of J ( \u00b7 )\nshould be shown by proofing that:\n1) J ( B(k+1) ) \u2264 J ( B(k) )\n, 2) J ( C(k+1) ) \u2264 J ( C(k) )\n, 3) J ( \u03b2(k+1) ) \u2264 J ( \u03b2(k) )\n, and 4) J ( \u03b1(k+1) ) \u2264 J ( \u03b1(k) ) .\nThe first and second have been proven in theorem 5 and 6 respectively. But as stated in lemma 2, sequence \u03b2m \u2200m and \u03b1n \u2200n can either strictly increase or decrease until reaching the limit points, thus J ( \u03b2(k+1) ) > J ( \u03b2(k) ) and/or J ( \u03b1(k+1) ) > J ( \u03b1(k) )\ncases can occur. This, however, will not affect the convergence of algorithm 3 since as long as sequence {\n\u03b2(k),\u03b1(k) }\nconverges, then the update rules eq. 25 and 26 will eventually find the stationary point for { B(k),C(k) }\n. Numerically, it seems that \u03b3Bm and \u03b3 C n play the key role in\ndetermining whether sequence { \u03b2 (k) m } and { \u03b1 (k) n }\nwill strictly increase or decrease with the big values lead to the increasing sequences and vice versa.\nVI. CONCLUSION\nWe have presented a converged algorithm for NMF with Tikhonov regularization on its factor. There are two contributions that can be pointed out. The first is to show the connection between Tikhonov regularized linear inverse problems with constraint NMF which naturally leads to a\nmechanisme for determining the regularization parameter in the NMF automatically. And, the second is the development of a converged algorithm for NMF with Tikhonov regularized constraints.\nAPPENDIX A OCTAVE/MATLAB CODES FOR ALGORITHM 3\nfunction [B, C, newalpha, newbeta, iteration, olderror, errordiff, maxNablaB, maxNablaC, resNorm, solNorm] = TikhonovNMF3(A, r, B0, C0, oldalpha, oldbeta, gammaB, gammaC, maxiter, tol)\n%The converged version of the algorithm %Use complementary slackness as stopping\ncriterion\nformat long; %%Check the input matrix if min(min(A)) < 0 error(\u2019Input matrix cannot contain negative\nentries\u2019); return\nend\n[m,n] = size(A);\n%%Check input arguments\nif \u02dcexist(\u2019A\u2019) error(\u2019incorrect inputs!\u2019) end if \u02dcexist(\u2019r\u2019) error(\u2019incorrect inputs!\u2019) end if \u02dcexist(\u2019B0\u2019) B0 = rand(m,r); end if \u02dcexist(\u2019C0\u2019) C0 = rand(r,n); end if \u02dcexist(\u2019alpha0\u2019) oldalpha = zeros(n,1); end if \u02dcexist(\u2019beta0\u2019) oldbeta = zeros(m,1); end if \u02dcexist(\u2019gammaB\u2019) gammaB = ones(m,1)*0.1; %small values lead to better convergence property end if \u02dcexist(\u2019gammaC\u2019) gammaC = ones(n,1)*0.1; %small values lead to better convergence property end if \u02dcexist(\u2019maxiter\u2019) maxiter = 1000; end if \u02dcexist(\u2019tol\u2019) tol = 1.0e-9; end\nB = B0; clear B0; C = C0; clear C0; newalpha = oldalpha; newbeta = oldbeta; trAtA = trace(A\u2019*A);\n10\nolderror = zeros(maxiter+1,1);\nolderror(1) = 0.5*trAtA - trace(C\u2019*(B\u2019*A)) + 0.5*trace(C\u2019*(B\u2019*B*C)) + 0.5*trace(B\u2019*diag(newbeta)*B) + 0.5*trace(C\u2019*(C*diag(newalpha)));\nsigma = 1.0e-9; delta = sigma;\nfor iteration=1:maxiter CCt = C*C\u2019; gradB = B*CCt-A*C\u2019+diag(newbeta)*B; Bm = max(B,(gradB<0)*sigma); B = B - (Bm.*gradB./(Bm*CCt +\ndiag(newbeta)*Bm + delta));\nBtB = B\u2019*B; gradC = BtB*C-B\u2019*A+C*diag(newalpha); Cm = max(C,(gradC<0)*sigma); C = C - (Cm.*gradC./(BtB*Cm +\nCm*diag(newalpha) + delta));\nfor i = 1:m newbeta(i) = gammaB(i)*norm((A(i,:) -\nB(i,:)*C),\u2019fro\u2019)\u02c62/(norm(B(i,:),\u2019fro\u2019)\u02c62 + delta);\nend\nfor i = 1:n newalpha(i) = gammaC(i)*norm((A(:,i) -\nB*C(:,i)),\u2019fro\u2019)\u02c62/(norm(C(:,i),\u2019fro\u2019)\u02c62 + delta);\nend\nnewerror = 0.5*trAtA - trace(C\u2019*(B\u2019*A)) + 0.5*trace(C\u2019*(B\u2019*B*C)) + 0.5*trace(B\u2019*diag(newbeta)*B) + 0.5*trace(C\u2019*(C*diag(newalpha)));\nerrordiff = abs(newerror - olderror(iteration)); olderror(iteration+1) = newerror; NablaB = (B*C*C\u2019 - A*C\u2019 +\ndiag(newbeta)*B).*B; NablaC = (B\u2019*B*C - B\u2019*A +\nC*diag(newalpha)).*C;\nmaxNablaB = max(max(abs(NablaB))); maxNablaC = max(max(abs(NablaC))); if(maxNablaB < tol && maxNablaC < tol) break; end\nend resNorm = norm((A-B*C),\u2019fro\u2019)\u02c62; solNorm(1) = norm(B,\u2019fro\u2019)\u02c62; solNorm(2) = norm(C, \u2019fro\u2019)\u02c62;\nREFERENCES\n[1] D. Lee and H. Seung, \u201cAlgorithms for non-negative matrix factorization,\u201d Proc. Advances in Neural Processing Information Systems, pp. 556-62, 2000. [2] I.S. Dhillon and S. Sra, \u201cGeneralized nonnegative matrix approximation with Bregman divergences,\u201d UTCS Technical Reports, The University of Texas at Austin, 2005. [3] A. Cichocki, R. Zdunek, and S. Amari, \u201cCsisza\u0301rs divergences for nonnegative matrix factorization: Family of new algorithms,\u201d Proc. 6th Int\u2019l\nConf. on Independent Component Analysis and Blind Signal Separation, 2006. [4] P. Paatero and U. Tapper,\u201cPositive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values,\u201d Environmetrics, Vol. 5, pp. 111-26, 1994. [5] P. Anttila, P. Paatero, and U. Tapper, \u201cSource identification of bulk wet deposition in finland by positive matrix factorization,\u201d Atmospheric Environment, Vol. 29, No. 14, pp. 1705-18, 1995. [6] D. Lee and H. Seung, \u201cLearning the parts of objects by non-negative matrix factorization,\u201d Nature, 401(6755), pp. 788-91, 1999. [7] P.O. Hoyer, \u201cNon-negative matrix factorization with sparseness constraints,\u201d The Journal of Machine Learning Research, Vol. 5, pp. 1457-69, 2004. [8] S.Z. Li, X.W. Hou, H.J. Zhang, and Q.S. Cheng, \u201cLearning spatially localized, parts-based representation,\u201d Proc. IEEE Comp. Soc. Conf. on Computer Vision and Pattern Recognition, pp. 207-12, 2001. [9] M. Berry, M. Brown, A. Langville, P. Pauca, and R.J. Plemmons, \u201cAlgorithms and applications for approximate nonnegative matrix factorization,\u201d Computational Statistics and Data Analysis, 2006. [10] F. Shahnaz, M.W. Berry, V. Pauca, and R.J. Plemmons, \u201cDocument clustering using nonnegative matrix factorization,\u201d Information Processing & Management, Vol. 42, No. 2, pp. 373-86, 2006. [11] W. Xu, X. Liu and Y. Gong, \u201cDocument clustering based on nonnegative matrix factorization,\u201d Proc. ACM SIGIR, pp. 267-73, 2003. [12] V.P. Pauca, J. Piper, and R.J. Plemmons, \u201cNonnegative matrix factorization for spectral data analysis,\u201d Linear Algebra and Its Applications, Vol. 416, No. 1, pp. 29-47, 2006. [13] S. Jia and Y. Qian, \u201cConstrained Nonnegative Matrix factorization for hyperspectral unmixing,\u201d IEEE Transactions on Geoscience and Remote Sensing, Vol. 47, No. 1, pp. 161-73, 2009. [14] A. Cichocki, S. Amari, R. Zdunek, R. Kompass, G. Hori, and Z. He, \u201cExtended SMART algorithms for non-negative matrix factorization,\u201d Lecture Notes in Computer Science, Vol. 4029, pp. 548-62, 2006. [15] J.P. Brunet, P. Tamayo, T.R. Golub, and J.P. Mesirov, \u201cMetagenes and molecular pattern discovery using matrix factorization,\u201d Proc. Natl Acad. Sci. USA, Vol. 101, No. 12, pp. 4164-9, 2003. [16] Y. Gao and G. Church, \u201cImproving Molecular cancer class discovery through sparse non-negative matrix factorization,\u201d Bioinformatics, Vol. 21, No. 21, pp. 3970-5, 2005. [17] H. Kim and H. Park, \u201cSparse non-negative matrix factorizations via alternating non-negativity constrained least squares for microarray data analysis,\u201d Bioinformatics, Vol. 23, No. 12, pp. 1495-502, 2007. [18] C.J. Lin, \u201cOn the convergence of multiplicative update algorithms for nonnegative matrix factorization,\u201d IEEE Transactions on Neural Networks, Vol. 18, No. 6, pp. 1589-96, 2007. [19] A. Mirzal, Nonnegative Matrix Factorizations for Clustering and LSI, LAP Lambert Academic Publishing, 2011, chapter 4. [20] C. Ding, T. Li, W. Peng, and H. Park, \u201cOrthogonal nonnegative matrix t-factorizations for clustering,\u201d Proc. 12th ACM SIGKDD Int\u2019l Conf. on Knowledge Discovery and Data Mining, pp. 126-35, 2006. [21] J. Yoo and S. Choi, \u201cOrthogonal nonnegative matrix factorization: Multiplicative updates on Stiefel manifolds,\u201d Proc. 9th Int\u2019l Conf. Intelligent Data Engineering and Automated Learning, pp. 140-7, 2008. [22] J. Yoo and S. Choi, \u201cOrthogonal nonnegative matrix tri-factorization for co-clustering: Multiplicative updates on Stiefel manifolds,\u201d Information Processing & Management, Vol. 46, No. 5, pp. 559-70, 2010. [23] S. Choi, \u201cAlgorithms for orthogonal nonnegative matrix factorization,\u201d Proc. IEEE Int\u2019l Joint Conf. on Neural Networks, pp. 1828-32, 2008. [24] H. Li, T. Adali, W. Wang, and D. Emge, \u201cNon-Negative Matrix Factorization with Orthogonality Constraints for Chemical Agent Detection in Raman Spectra,\u201d Proc. IEEE Workshop on Machine Learning for Signal Processing, pp. 253-8, 2005. [25] C.J. Lin, \u201cProjected gradient methods for non-negative matrix factorization,\u201d Technical Report ISSTECH-95-013, Department of CS, National Taiwan University, 2005. [26] E.F. Gonzales and Y. Zhang, \u201cAccelerating the Lee-Seung algorithm for non-negative matrix factorization,\u201d Technical Report, Dept. Comput. Appl. Math., Rice Univ., Houston, 2005. [27] D. Kim, S. Sra, and I.S. Dhillon, \u201cFast projection-based methods for the least squares nonnegative matrix approximation problem,\u201d Stat. Anal. Data Min., Vol. 1, No. 1, pp. 38-51, 2008. [28] D. Kim, S. Sra, I.S. Dhillon, \u201cFast newton-type methods for the least squares nonnegative matrix approximation problem,\u201d Proc. SIAM Conference on Data Mining, pp. 343-54, 2007. [29] H. Kim and H. Park, \u201cNonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method,\u201d SIAM. J. Matrix Anal. & Appl., Vol. 30, No. 2, pp. 713-30, 2008.\n11\n[30] J. Kim and H. Park, \u201cToward faster nonnegative matrix factorization: A new algorithm and comparisons,\u201d Proc. 8th IEEE International Conference on Data Mining, pp. 353-62, 2008. [31] R. Tibshirani, \u201cRegression shrinkage and selection via the lasso,\u201d J. Royal Statistical Society B, Vol. 58, Issue 1, pp. 267-88, 1996. [32] P.C. Hansen, The L-curve and its use in the numerical treatment of inverse problems, Computational Inverse Problems in Electrocardiology, WIT Press, 2000, pp. 119-142. [33] P.R. Johnston and R.M. Gulrajani, \u201cA new method for regularization parameter determination in the inverse problem of electrocardiography,\u201d IEEE Transaction on Biomedical Engineering, Vol. 44, No. 1, pp. 19-39, 1997. [34] S. Oraintara, W.C. Karl, D.A. Castanon, and T.Q. Nguyen, \u201cA method for choosing the regularization parameter in generalized tikhonov regularized linear inverse problems,\u201d Proc. International Conference of Image Processing, pp. 93-96, 2000. [35] S.M. Tan and C. Fox, \u201cRegularization methods for linear inverse problems,\u201d Lecture Note: PHYSICS 707 Inverse Problems, Chapter 3, pp. 1-15, 1998. [36] K. Kunisch and J. Zhou, \u201cIterative choices of regularization parameters in linear inverse problems,\u201d Inverse Problems, Vol. 14, No. 5, pp. 1247- 64, 1998. [37] M. Belge, E. Miller, and M. Kilmer, \u201cSimultaneous multiple regulariza-\ntion parameter selection by means of the L-hypersurface with applications to linear inverse problems posed in the wavelet transform domain,\u201d SPIE Int\u2019l Symposium on Optical Science, Engineering, and Instrumentation: Bayesian Inference for Inverse Problems, pp. 328-36, 1998. [38] T. Reginska, \u201cA regularization parameter in discrete ill-posed problems,\u201d SIAM Journal of Scientific Computing, Vol. 17, No. 3, pp. 740-9, 1996. [39] D.L. Phillips, \u201cA technique for the numerical solution of certain integral equations of the first kind,\u201d J. Association for Computing Machinery, Vol. 9, Issue 1, pp. 84-97, 1997. [40] A.N. Tikhonov, \u201cSolution of incorrectly formulated problems and the regularization method,\u201d Soviet Math. Dokl. 4, pp. 1035-8, 1963. English translation of Dokl. Akad. Nauk. SSSR, 151, pp. 501-4, 1963. [41] H. Hindi, \u201cA tutorial on convex optimization,\u201d Proc. American Control Conference, pp. 3252-65, 2004. [42] D.P. Bertsekas, \u201cNonlinear Programming 2nd Ed.,\u201d Athena Scientific, 1999. [43] S. Oraintara, \u201cRegular Linear Phase Perfect Reconstruction Filter Banks for Image Compression,\u201d PhD Thesis, Boston University, 2000, Appendix A. [44] L. Grippo and M. Sciandrone, \u201cOn the convergence of the block nonlinear Gauss-Seidel method under convex constraints,\u201d Operation Research Letters, Vol. 26, pp. 127-36, 2000."}], "references": [{"title": "Algorithms for non-negative matrix factorization", "author": ["D. Lee", "H. Seung"], "venue": "Proc. Advances in Neural Processing Information Systems, pp. 556-62, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Generalized nonnegative matrix approximation with Bregman divergences", "author": ["I.S. Dhillon", "S. Sra"], "venue": "UTCS Technical Reports, The University of Texas at Austin, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Csisz\u00e1rs divergences for nonnegative matrix factorization: Family of new algorithms", "author": ["A. Cichocki", "R. Zdunek", "S. Amari"], "venue": "Proc. 6th Int\u2019l  Conf. on Independent Component Analysis and Blind Signal Separation, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Tapper,\u201cPositive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values,", "author": ["U.P. Paatero"], "venue": "Environmetrics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Source identification of bulk wet deposition in finland by positive matrix factorization", "author": ["P. Anttila", "P. Paatero", "U. Tapper"], "venue": "Atmospheric Environment, Vol. 29, No. 14, pp. 1705-18, 1995.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D. Lee", "H. Seung"], "venue": "Nature, 401(6755), pp. 788-91, 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Non-negative matrix factorization with sparseness constraints", "author": ["P.O. Hoyer"], "venue": "The Journal of Machine Learning Research, Vol. 5, pp. 1457-69, 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning spatially localized, parts-based representation", "author": ["S.Z. Li", "X.W. Hou", "H.J. Zhang", "Q.S. Cheng"], "venue": "Proc. IEEE Comp. Soc. Conf. on Computer Vision and Pattern Recognition, pp. 207-12, 2001.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Algorithms and applications for approximate nonnegative matrix factorization", "author": ["M. Berry", "M. Brown", "A. Langville", "P. Pauca", "R.J. Plemmons"], "venue": "Computational Statistics and Data Analysis, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Document clustering using nonnegative matrix factorization", "author": ["F. Shahnaz", "M.W. Berry", "V. Pauca", "R.J. Plemmons"], "venue": "Information Processing & Management, Vol. 42, No. 2, pp. 373-86, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Document clustering based on nonnegative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "Proc. ACM SIGIR, pp. 267-73, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Nonnegative matrix factorization for spectral data analysis", "author": ["V.P. Pauca", "J. Piper", "R.J. Plemmons"], "venue": "Linear Algebra and Its Applications, Vol. 416, No. 1, pp. 29-47, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Constrained Nonnegative Matrix factorization for hyperspectral unmixing", "author": ["S. Jia", "Y. Qian"], "venue": "IEEE Transactions on Geoscience and Remote Sensing, Vol. 47, No. 1, pp. 161-73, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Extended SMART algorithms for non-negative matrix factorization", "author": ["A. Cichocki", "S. Amari", "R. Zdunek", "R. Kompass", "G. Hori", "Z. He"], "venue": "Lecture Notes in Computer Science, Vol. 4029, pp. 548-62, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Metagenes and molecular pattern discovery using matrix factorization", "author": ["J.P. Brunet", "P. Tamayo", "T.R. Golub", "J.P. Mesirov"], "venue": "Proc. Natl Acad. Sci. USA, Vol. 101, No. 12, pp. 4164-9, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Improving Molecular cancer class discovery through sparse non-negative matrix factorization", "author": ["Y. Gao", "G. Church"], "venue": "Bioinformatics, Vol. 21, No. 21, pp. 3970-5, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Sparse non-negative matrix factorizations via alternating non-negativity constrained least squares for microarray data analysis", "author": ["H. Kim", "H. Park"], "venue": "Bioinformatics, Vol. 23, No. 12, pp. 1495-502, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "On the convergence of multiplicative update algorithms for nonnegative matrix factorization", "author": ["C.J. Lin"], "venue": "IEEE Transactions on Neural Networks, Vol. 18, No. 6, pp. 1589-96, 2007.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonnegative Matrix Factorizations for Clustering and LSI", "author": ["A. Mirzal"], "venue": "LAP Lambert Academic Publishing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "Proc. 12th ACM SIGKDD Int\u2019l Conf. on Knowledge Discovery and Data Mining, pp. 126-35, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Orthogonal nonnegative matrix factorization: Multiplicative updates on Stiefel manifolds", "author": ["J. Yoo", "S. Choi"], "venue": "Proc. 9th Int\u2019l Conf. Intelligent Data Engineering and Automated Learning, pp. 140-7, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Orthogonal nonnegative matrix tri-factorization for co-clustering: Multiplicative updates on Stiefel manifolds", "author": ["J. Yoo", "S. Choi"], "venue": "Information Processing & Management, Vol. 46, No. 5, pp. 559-70, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Algorithms for orthogonal nonnegative matrix factorization", "author": ["S. Choi"], "venue": "Proc. IEEE Int\u2019l Joint Conf. on Neural Networks, pp. 1828-32, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1828}, {"title": "Non-Negative Matrix Factorization with Orthogonality Constraints for Chemical Agent Detection in Raman Spectra", "author": ["H. Li", "T. Adali", "W. Wang", "D. Emge"], "venue": "Proc. IEEE Workshop on Machine Learning for Signal Processing, pp. 253-8, 2005.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Projected gradient methods for non-negative matrix factorization", "author": ["C.J. Lin"], "venue": "Technical Report ISSTECH-95-013, Department of CS, National Taiwan University, 2005.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Accelerating the Lee-Seung algorithm for non-negative matrix factorization", "author": ["E.F. Gonzales", "Y. Zhang"], "venue": "Technical Report, Dept. Comput. Appl. Math., Rice Univ., Houston, 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast projection-based methods for the least squares nonnegative matrix approximation problem", "author": ["D. Kim", "S. Sra", "I.S. Dhillon"], "venue": "Stat. Anal. Data Min., Vol. 1, No. 1, pp. 38-51, 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast newton-type methods for the least squares nonnegative matrix approximation problem", "author": ["D. Kim", "S. Sra", "I.S. Dhillon"], "venue": "Proc. SIAM Conference on Data Mining, pp. 343-54, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method", "author": ["H. Kim", "H. Park"], "venue": "SIAM. J. Matrix Anal. & Appl., Vol. 30, No. 2, pp. 713-30, 2008.  11", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Toward faster nonnegative matrix factorization: A new algorithm and comparisons", "author": ["J. Kim", "H. Park"], "venue": "Proc. 8th IEEE International Conference on Data Mining, pp. 353-62, 2008.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Royal Statistical Society B, Vol. 58, Issue 1, pp. 267-88, 1996.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1996}, {"title": "The L-curve and its use in the numerical treatment of inverse problems, Computational Inverse Problems in Electrocardiology", "author": ["P.C. Hansen"], "venue": "WIT Press,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2000}, {"title": "A new method for regularization parameter determination in the inverse problem of electrocardiography", "author": ["P.R. Johnston", "R.M. Gulrajani"], "venue": "IEEE Transaction on Biomedical Engineering, Vol. 44, No. 1, pp. 19-39, 1997.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1997}, {"title": "A method for choosing the regularization parameter in generalized tikhonov regularized linear inverse problems", "author": ["S. Oraintara", "W.C. Karl", "D.A. Castanon", "T.Q. Nguyen"], "venue": "Proc. International Conference of Image Processing, pp. 93-96, 2000.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2000}, {"title": "Regularization methods for linear inverse problems", "author": ["S.M. Tan", "C. Fox"], "venue": "Lecture Note: PHYSICS 707 Inverse Problems, Chapter 3, pp. 1-15, 1998.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "Iterative choices of regularization parameters in linear inverse problems", "author": ["K. Kunisch", "J. Zhou"], "venue": "Inverse Problems, Vol. 14, No. 5, pp. 1247- 64, 1998.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}, {"title": "Simultaneous multiple regulariza-  tion parameter selection by means of the L-hypersurface with applications to linear inverse problems posed in the wavelet transform domain", "author": ["M. Belge", "E. Miller", "M. Kilmer"], "venue": "SPIE Int\u2019l Symposium on Optical Science, Engineering, and Instrumentation: Bayesian Inference for Inverse Problems, pp. 328-36, 1998.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1998}, {"title": "A regularization parameter in discrete ill-posed problems", "author": ["T. Reginska"], "venue": "SIAM Journal of Scientific Computing, Vol. 17, No. 3, pp. 740-9, 1996.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1996}, {"title": "A technique for the numerical solution of certain integral equations of the first kind", "author": ["D.L. Phillips"], "venue": "J. Association for Computing Machinery, Vol. 9, Issue 1, pp. 84-97, 1997.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1997}, {"title": "Solution of incorrectly formulated problems and the regularization method", "author": ["A.N. Tikhonov"], "venue": "Soviet Math. Dokl. 4, pp. 1035-8, 1963. English translation of Dokl. Akad. Nauk. SSSR, 151, pp. 501-4, 1963.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1963}, {"title": "A tutorial on convex optimization", "author": ["H. Hindi"], "venue": "Proc. American Control Conference, pp. 3252-65, 2004.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}, {"title": "Nonlinear Programming 2nd Ed", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific, 1999.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1999}, {"title": "Regular Linear Phase Perfect Reconstruction Filter Banks for Image Compression", "author": ["S. Oraintara"], "venue": "PhD Thesis, Boston University, 2000, Appendix A.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2000}, {"title": "On the convergence of the block nonlinear Gauss-Seidel method under convex constraints", "author": ["L. Grippo", "M. Sciandrone"], "venue": "Operation Research Letters, Vol. 26, pp. 127-36, 2000.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "(2) In addition, other criteria like Kullback-Leibler divergence [1], [2] and Csisz\u00e1rs \u03c6-divergence [3] can also be used.", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "(2) In addition, other criteria like Kullback-Leibler divergence [1], [2] and Csisz\u00e1rs \u03c6-divergence [3] can also be used.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "(2) In addition, other criteria like Kullback-Leibler divergence [1], [2] and Csisz\u00e1rs \u03c6-divergence [3] can also be used.", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "[4], [5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4], [5].", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "The popularity of the NMF is due to the work of Lee and Seung [6] in which they introduced a simple yet powerful NMF algorithm, and then show its applicability in image processing and text analysis.", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "my In addition, the algorithm also produces sparser factors (thus requires less storage) [6]\u2013[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "my In addition, the algorithm also produces sparser factors (thus requires less storage) [6]\u2013[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "my In addition, the algorithm also produces sparser factors (thus requires less storage) [6]\u2013[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9].", "startOffset": 266, "endOffset": 269}, {"referenceID": 6, "context": "my In addition, the algorithm also produces sparser factors (thus requires less storage) [6]\u2013[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9].", "startOffset": 271, "endOffset": 274}, {"referenceID": 8, "context": "my In addition, the algorithm also produces sparser factors (thus requires less storage) [6]\u2013[8] and can give more intuitive results compared to other subspace approximation techniques like Principal Component Analysis (PCA) and Independent Component Analysis (ICA) [6], [7], [9].", "startOffset": 276, "endOffset": 279}, {"referenceID": 9, "context": ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]\u2013[17], and showed that the NMF can give better results.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]\u2013[17], and showed that the NMF can give better results.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]\u2013[17], and showed that the NMF can give better results.", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]\u2013[17], and showed that the NMF can give better results.", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]\u2013[17], and showed that the NMF can give better results.", "startOffset": 81, "endOffset": 84}, {"referenceID": 7, "context": ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]\u2013[17], and showed that the NMF can give better results.", "startOffset": 86, "endOffset": 89}, {"referenceID": 13, "context": ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]\u2013[17], and showed that the NMF can give better results.", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]\u2013[17], and showed that the NMF can give better results.", "startOffset": 142, "endOffset": 146}, {"referenceID": 16, "context": ", document clustering [10], [11], spectral analysis [12], [13], image processing [7], [8], blind source separation [14], and cancer detection [15]\u2013[17], and showed that the NMF can give better results.", "startOffset": 147, "endOffset": 151}, {"referenceID": 6, "context": "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]\u2013[24].", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]\u2013[24].", "startOffset": 65, "endOffset": 68}, {"referenceID": 15, "context": "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]\u2013[24].", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]\u2013[24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]\u2013[24].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]\u2013[24].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]\u2013[24].", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]\u2013[24].", "startOffset": 129, "endOffset": 133}, {"referenceID": 23, "context": "2) to also include auxiliary constraints such as sparseness [7], [8], [16], [17], smoothness [10], [12], [13], and orthogonality [19]\u2013[24].", "startOffset": 134, "endOffset": 138}, {"referenceID": 8, "context": "However, as multiplicative update rules based NMF algorithms do not have convergence guarantee [9], [18], [19], [25], the development of converged algorithms for various NMF objectives with auxiliary constraints is an open research problem.", "startOffset": 95, "endOffset": 98}, {"referenceID": 17, "context": "However, as multiplicative update rules based NMF algorithms do not have convergence guarantee [9], [18], [19], [25], the development of converged algorithms for various NMF objectives with auxiliary constraints is an open research problem.", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "However, as multiplicative update rules based NMF algorithms do not have convergence guarantee [9], [18], [19], [25], the development of converged algorithms for various NMF objectives with auxiliary constraints is an open research problem.", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "However, as multiplicative update rules based NMF algorithms do not have convergence guarantee [9], [18], [19], [25], the development of converged algorithms for various NMF objectives with auxiliary constraints is an open research problem.", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": ", projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it\u2019s not always clear how to incorporate those auxiliary constraints into the algorithms.", "startOffset": 29, "endOffset": 33}, {"referenceID": 26, "context": ", projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it\u2019s not always clear how to incorporate those auxiliary constraints into the algorithms.", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": ", projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it\u2019s not always clear how to incorporate those auxiliary constraints into the algorithms.", "startOffset": 70, "endOffset": 74}, {"referenceID": 28, "context": ", projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it\u2019s not always clear how to incorporate those auxiliary constraints into the algorithms.", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": ", projected gradient methods [25], [27], projected quasiNewton method [28], active set method [29], and block principal pivoting method [30]) do have convergence guarantee, due to the complexity of the algorithms, it\u2019s not always clear how to incorporate those auxiliary constraints into the algorithms.", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "The additive update rules based algorithm for standard NMF first appeared in the work of Lee & Seung [1], but the convergence proof was given by Lin in ref.", "startOffset": 101, "endOffset": 104}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": ", text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": ", text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": ", text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.", "startOffset": 49, "endOffset": 53}, {"referenceID": 28, "context": ", text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": ", text mining [10], spectral data analysis [12], [13], microarray data analysis [29], and cancer class discovery [16] (in some works, sparseness is enforced using L2 norm on the solution, i.", "startOffset": 113, "endOffset": 117}, {"referenceID": 30, "context": "norm\u2014the more appropriate constraint for enforcing sparseness [31]), and showed that it can offer better results compared to the results of standard NMF.", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "This constraint also can reduce influence of noise and other uncertainties in the data [9], [12], [13].", "startOffset": 87, "endOffset": 90}, {"referenceID": 11, "context": "This constraint also can reduce influence of noise and other uncertainties in the data [9], [12], [13].", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "This constraint also can reduce influence of noise and other uncertainties in the data [9], [12], [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 31, "context": "In addition, from the inverse problem study, it is known that Tikhonov regularized least square (LS) is the more preferable form in solving inverse problems because solutions of the conventional LS tend to be unstable and dominated by data and rounding errors [32]\u2013[34].", "startOffset": 260, "endOffset": 264}, {"referenceID": 33, "context": "In addition, from the inverse problem study, it is known that Tikhonov regularized least square (LS) is the more preferable form in solving inverse problems because solutions of the conventional LS tend to be unstable and dominated by data and rounding errors [32]\u2013[34].", "startOffset": 265, "endOffset": 269}, {"referenceID": 34, "context": "Moreover, in the presence of noise, frequently the conventional LS solutions are rather undesirable as it leads to amplification of noise in the direction of singular vectors with small singular values [35].", "startOffset": 202, "endOffset": 206}, {"referenceID": 35, "context": "In this paper we will utilize the L-curve since the Morozov discrepancy principles require knowledge of the error level in the data which is often inaccessible [36].", "startOffset": 160, "endOffset": 164}, {"referenceID": 31, "context": "In this curve, the proper value for the regularization parameter is the value associated with corner of the curve where both solution and approximation error have minimum norms [32], [34].", "startOffset": 177, "endOffset": 181}, {"referenceID": 33, "context": "In this curve, the proper value for the regularization parameter is the value associated with corner of the curve where both solution and approximation error have minimum norms [32], [34].", "startOffset": 183, "endOffset": 187}, {"referenceID": 31, "context": ", [32], [34], [37], [38].", "startOffset": 2, "endOffset": 6}, {"referenceID": 33, "context": ", [32], [34], [37], [38].", "startOffset": 8, "endOffset": 12}, {"referenceID": 36, "context": ", [32], [34], [37], [38].", "startOffset": 14, "endOffset": 18}, {"referenceID": 37, "context": ", [32], [34], [37], [38].", "startOffset": 20, "endOffset": 24}, {"referenceID": 33, "context": "[34] in which they defined L-corner to be the point of tangency between L-curve with positive curvature and a straight line of negative slope.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "We choose this method because it has convergence guarantee and is relatively faster to compute than the standard method; the maximum curvature approach [32].", "startOffset": 152, "endOffset": 156}, {"referenceID": 38, "context": "The method was developed independently by Phillips [39] and Tikhonov [40].", "startOffset": 51, "endOffset": 55}, {"referenceID": 39, "context": "The method was developed independently by Phillips [39] and Tikhonov [40].", "startOffset": 69, "endOffset": 73}, {"referenceID": 31, "context": "(5) To improve the solution, usually Tikhonov regularized LS is used instead [32]: x\u03bb = argmin x \u2016y \u2212Ax\u2016F + \u03bb\u2016x\u2016 2 F (6) where \u03bb denotes nonnegative regularization parameter, \u2016y \u2212 Ax\u2016F denotes approximation error, and \u2016x\u2016 2 F denotes solution size.", "startOffset": 77, "endOffset": 81}, {"referenceID": 33, "context": "In [34], Oraintara et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "001 in the authors\u2019 work [34].", "startOffset": 25, "endOffset": 29}, {"referenceID": 33, "context": "Note that the value of \u03b3 doesn\u2019t influence convergence property of sequence x and \u03bb, and as long as \u03bb is sufficiently small then \u03bb converges to a stationary point [34].", "startOffset": 163, "endOffset": 167}, {"referenceID": 32, "context": "[33], but the authors fixed \u03b3 value to one.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "and may have several local mimima [25].", "startOffset": 34, "endOffset": 38}, {"referenceID": 40, "context": "The common practice to deal with the nonconvexity of an optimization problem is by transforming it into convex subproblems [41].", "startOffset": 123, "endOffset": 127}, {"referenceID": 24, "context": "In the case of the NMF, this can be done by employing the alternating strategy; fixing one matrix while solving for the other [25] (apparently, all NMF algorithms utilizing alternating strategy).", "startOffset": 126, "endOffset": 130}, {"referenceID": 33, "context": "[34], \u03b3 m and \u03b3 C n are defined similarly as in algorithm 1, and \u01eb denotes small positive number.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "17 if it satisfies the KKT optimality conditions [42], i.", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "As stated by Lin [18], the above multiplicative update rules based algorithm can be modified into an equivalent converged algorithm by (1) using additive update rules, and (2) replacing zero entries that do not satisfy the KKT conditions with a small positive number to escape the zero locking.", "startOffset": 17, "endOffset": 21}, {"referenceID": 41, "context": "This approach is known as the block-coordinate descent method [42].", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "From convergence analysis study, the following conditions must be satisfied for sequence { B,C } to have convergence guarantee [18], [25], [44].", "startOffset": 127, "endOffset": 131}, {"referenceID": 24, "context": "From convergence analysis study, the following conditions must be satisfied for sequence { B,C } to have convergence guarantee [18], [25], [44].", "startOffset": 133, "endOffset": 137}, {"referenceID": 43, "context": "From convergence analysis study, the following conditions must be satisfied for sequence { B,C } to have convergence guarantee [18], [25], [44].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "We will utilize the auxiliary function approach introduced in [1] to prove this property.", "startOffset": 62, "endOffset": 65}, {"referenceID": 17, "context": "Proof: As stated by Lin [18], it suffices to prove that", "startOffset": 24, "endOffset": 28}, {"referenceID": 33, "context": "[34], [43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[34], [43].", "startOffset": 6, "endOffset": 10}, {"referenceID": 33, "context": "[34]): The optimum \u03b2m corresponding to the L-corner must satisfy \u03b2m \u2225 bTm \u2225", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34]): The values of \u03b2 m \u2200m either strictly increase or decrease under update rule eq.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34]): If the update rule eq.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34], the followings summarize the strategy in choosing the initial values.", "startOffset": 0, "endOffset": 4}], "year": 2012, "abstractText": "We present a converged algorithm for Tikhonov regularized nonnegative matrix factorization (NMF). We specially choose this regularization because it is known that Tikhonov regularized least square (LS) is the more preferable form in solving linear inverse problems than the conventional LS. Because an NMF problem can be decomposed into LS subproblems, it can be expected that Tikhonov regularized NMF will be the more appropriate approach in solving NMF problems. The algorithm is derived using additive update rules which have been shown to have convergence guarantee. We equip the algorithm with a mechanism to automatically determine the regularization parameters based on the L-curve, a well-known concept in the inverse problems community, but is rather unknown in the NMF research. The introduction of this algorithm thus solves two inherent problems in Tikhonov regularized NMF algorithm research, i.e., convergence guarantee and regularization parameters determination.", "creator": "LaTeX with hyperref package"}}}