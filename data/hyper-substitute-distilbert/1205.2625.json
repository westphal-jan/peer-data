{"id": "1205.2625", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Convergent message passing algorithms - a unifying view", "abstract": "message - oblivious algorithms sometimes emerged as search machines allowing improving inference sequence inference models. wherein bounded algorithms converge, they needs be used to if local ( nevertheless sometimes even global ) optima - approximate intervals form his inference problem. but many of the most popular algorithms that not supposed to converge. this has lead to recent interest in convergent conversation - passing programming. in this paper, we present a contrasting view of convergent message - passing issues. we present a simple derivation of an abstract algorithm, tree - consistency bound optimization ( tcbo ) to is efficiently convergent in both flow estimate and max product requirements. yamaha then discovered that many of the truly convergent algorithms lose instances on our tcbo algorithm, and approximate fully convergent algorithms \" for purposes \" by exchanging keys giving summations becoming exponential algorithms. most specifically, we show that mcdonald's bi - singular bit - critical complexity for avoiding parallel variational distributions, is thoroughly approximate with arguably right approximation order regarding the case where trees cross continuous chains.", "histories": [["v1", "Wed, 9 May 2012 17:21:25 GMT  (178kb)", "http://arxiv.org/abs/1205.2625v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["talya meltzer", "amir globerson", "yair weiss"], "accepted": false, "id": "1205.2625"}, "pdf": {"name": "1205.2625.pdf", "metadata": {"source": "CRF", "title": "Convergent message passing algorithms - a unifying view", "authors": ["Talya Meltzer", "Amir Globerson"], "emails": ["talyam@cs.huji.ac.il", "gamir@cs.huji.ac.il", "yweiss@cs.huji.ac.il"], "sections": [{"heading": null, "text": "Message-passing algorithms have emerged as powerful techniques for approximate inference in graphical models. When these algorithms converge, they can be shown to find local (or sometimes even global) optima of variational formulations to the inference problem. But many of the most popular algorithms are not guaranteed to converge. This has lead to recent interest in convergent message-passing algorithms.\nIn this paper, we present a unified view of convergent message-passing algorithms. We present a simple derivation of an abstract algorithm, tree-consistency bound optimization (TCBO) that is provably convergent in both its sum and max product forms. We then show that many of the existing convergent algorithms are instances of our TCBO algorithm, and obtain novel convergent algorithms \u201cfor free\u201d by exchanging maximizations and summations in existing algorithms. In particular, we show that Wainwright\u2019s non-convergent sum-product algorithm for tree based variational bounds, is actually convergent with the right update order for the case where trees are monotonic chains."}, {"heading": "1 Introduction", "text": "Probabilistic inference in graphical models is a key component in learning and using these models in practice. The two key inference problems are calculating the marginals of a model and calculating its most likely assignment (sometimes referred to as the MAP problem).\nOne approach to these generally intractable problems is to use a variational formulation, where approximate\ninference is cast as an optimization problem. For the marginals case this usually corresponds to minimization of a free energy functional, and for the MAP problem it corresponds to solving a linear programming (LP) relaxation [10].\nA key challenge in both the MAP and marginals case is to devise simple and scalable algorithms for solving the variational optimization problem. In recent years numerous algorithms have been introduced for both tasks. These algorithms typically have a \u201cmessage passing like\u201d structure.\nPerhaps the most widely used message-passing algorithms are \u201cbelief propagation\u201d and its generalizations [5, 8, 12, 14]. These algorithms typically have two variants: sum-product which is used to approximate the marginals, and max-product which is used to approximate the MAP. Fixed-points of these algorithms can be shown to be local (or sometimes even global) optima of the corresponding variational formulation. Yet despite the spectacular empirical success of these algorithms in real-world applications, they are not guaranteed to converge, and variants of \u201cdampening\u201d are often used to improve their convergence [8, 12, 14].\nThere has therefore been much recent work on convergent message passing algorithms [2, 5, 7, 13]. These algorithms are often very similar in structure to the nonconvergent algorithms and often include local maxproduct or sum-product operations. However, for each of these specific algorithms it has been possible to prove that the value of the variational problem (or its dual) improves at each iteration. Perhaps the most intriguing example of this is Kolmogorov\u2019s TRW-S algorithm [7] which is simply Wainwright\u2019s tree-reweighted max-product algorithm [11] with a different update schedule.\nHere we introduce a unifying framework which encompasses both marginals and MAP approximations, by exploiting the mathematical similarities between these approximations. Specifically, we provide an up-\nper bound on the optimum of the variational approximations, and give sufficient conditions that algorithms need to satisfy in order to decrease this bound in a monotone fashion. Any algorithm which satisfies these conditions is guaranteed to decrease the upper bound at every iteration. This property in turn guarantees that such algorithms converge to a global optimum of the variational problem in the marginals case and to a local optimum in the MAP LP approximation case.\nOur framework involves updating a subset of regions which form a tree in the region graph. A related approach was recently suggested by Sontag et al. [9] in the context of solving the MAP approximation. Their work gives an explicit algorithm for optimizing all edges corresponding to a tree in the original graph, such that an upper bound on the LP optimum is decreased at every iteration. Our formalism does not give an explicit update but rather conditions that guarantee an update to decrease the objective. However, we show that these conditions are satisfied by several known algorithms. Furthermore, since the condition is similar in the marginals and MAP case (specifically a condition of sum and max consistency respectively) it is easy to obtain algorithms for both these cases simultaneously, and to use results in one problem for obtaining algorithms for the other.\nFor instance, we consider the tree-reweighted (TRW) free energy minimization problem [12]. Recently two works have provided convergent algorithms for this problem [1, 3], but these were more involved than standard message passing algorithms. Here we show the surprising result that in fact the original algorithm provided for TRW by Wainwright et al. is convergent, if run with an appropriate schedule of message updates."}, {"heading": "2 Bounds for MAP and Log-Partition", "text": "We consider a graphical model where the joint probability over variables p(x) factors into a product over clique potentials p(x) = 1\nZ\n\u220f\n\u03b1 \u03a8\u03b1(x\u03b1) or equivalently, the energy function is a sum over clique energies p(x) = 1\nZ exp(\n\u2211\n\u03b1 \u03b8\u03b1(x\u03b1)). We also denote \u03b8(x) = \u2211\n\u03b1 \u03b8\u03b1(x\u03b1).\nThe problem of calculating marginals and approximation of the partition function Z can be recast as the following maximization problem of the function F (q) (the negative of the free energy):\nlogZ = max q F (q) = max q (\u3008\u03b8(x)\u3009q +H(q)) (1)\nwhere q is the set of probability distributions over x, \u3008\u03b8(x)\u3009q is the average energy with respect to q and H(q) is the entropy function. The maximizing argu-\nment is then the distribution p(x).\nThis maximization is in general intractable, so approximate free energies are often used. A class of approximate free energies, discussed in [14], is based on the concept of a region graph G whose nodes \u03b1 are regions of the original graph, and whose edges represent subregion relationships (i.e. an edge between \u03b1 in \u03b2 exists only if \u03b2 \u2282 \u03b1). The approximation replaces the joint entropy H(q) with a linear combination of local region entropies H\u03b1(q\u03b1) where each local entropy is weighted by a \u201cdouble-counting\u201d number c\u03b1:\nH\u0303G,c(q) = \u2211\n\u03b1\nc\u03b1H\u03b1(q\u03b1) , (2)\nwhere the subscript G, c indicates the dependence of the approximation on the region graph G and the counting numbers c\u03b1.\nWith this approximation of H(q) the free energy now only depends on local distributions, since the average energy is a simple function of the q\u03b1, namely \u3008\u03b8(x)\u3009q = \u2211\n\u03b1\u3008\u03b8\u03b1(x\u03b1)\u3009q\u03b1 . To optimize only over local distributions q\u03b1, we need to consider only distributions such that there exists a q(x) that has these as marginals. This set (called the marginal polytope [10]) cannot be expressed in a compact form, and is typically approximated. One popular approximation is the local polytope of the region graph L(G) defined as the set of local distributions that agree on the marginals for any two regions in the region graph that are subsets of each other:1\nL(G) =\n{\nq \u2265 0\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2200\u03b2 \u2282 \u03b1, x\u03b2 \u2211 x\u03b1\\x\u03b2 q\u03b1(x\u03b1) = q\u03b2(x\u03b2)\n\u2200\u03b1, \u2211\nx\u03b1 q\u03b1(x\u03b1) = 1\n}\nTake together, this results in the following standard variational approximation [10]:\nmax q\u2208L(G) F\u0303 (q) = max q\u2208L(G)\n\u2211\n\u03b1\n\u3008\u03b8\u03b1(x\u03b1)\u3009q\u03b1 + H\u0303G,c(q) (3)\nSimilarly the MAP problem is approximated via\nmax q\u2208L(G) F\u0303 (q) = max q\u2208L(G)\n\u2211\n\u03b1\n\u3008\u03b8\u03b1(x\u03b1)\u3009q\u03b1 (4)\nTo obtain a unified formalism for MAP and marginals, we use a temperature parameter T where T = 1 for marginals and T = 0 for MAP and the optimization is:\nmax q\u2208L(G) F\u0303 (q) = max q\u2208L(G)\n\u2211\n\u03b1\n\u3008\u03b8\u03b1(x\u03b1)\u3009q\u03b1 + TH\u0303G,c(q) (5)\n1Note that this is the local polytope of the region graph not the local polytope of the original graph"}, {"heading": "2.1 Positive Counting Numbers", "text": "The entropy approximation H\u0303G,c(q) in Eq. 2 is generally not a concave function of the local distributions q. Thus maximization of F\u0303 (q) may result in local optima. To avoid this undesirable property, several works (e.g., [5]) have focused on entropies which are obtained by considering only concave H\u0303G,c(q) functions. We focus on approximations where all the double counting numbers are non-negative. This is a strong restriction but since we are working with a region graph formulation, many approximate free energies which have negative double counting numbers can be transformed into ones with positive double counting numbers on a region graph. Perhaps the most important example are tree-reweighted free energies in which the entropy is approximated as HTRBP (q) = \u2211\n\u03c4 \u03c1\u03c4H\u03c4 (q) with \u03c1\u03c4 a probability distribution over trees in the graph and H\u03c4 (q) is the entropy of the distribution on \u03c4 with marginals given by q (more precisely, the projection of q on the tree \u03c4). If we consider a region graph with trees and their intersection (Fig. 5) the double counting numbers are non-negative. But HTRBP can also be rewritten HTRBP = \u2211 ij \u03c1ijHij+ \u2211\ni ciHi with ci = 1\u2212 \u2211\nj \u03c1ij and \u03c1ij is the edge appearance probability of the edge ij under the distribution \u03c1. In this case, the double-counting number for the singletons ci may be negative. However, we will show that it is sometimes advantageous to work in the representation that uses a non-negative mixture of trees, since nonnegativity of the counting numbers allows a simpler derivation of algorithms."}, {"heading": "2.2 Optimization and Reparameterization", "text": "The vast majority of methods for solving the variational approximation are based on two classes of constraints that local optima should satisfy. It is easy to show using Lagrange multipliers, that local optima of F\u0303 should satisfy two types of constraints [5, 6, 12, 14, 15]\n\u2022 Reparametrization (or admissibility, or \u201ce constraints\u201d). P (x) \u221d \u220f\n\u03b1 q\u03b1(x\u03b1) c\u03b1 , for every x.\n\u2022 sum-consistency (or \u201cm constraints\u201d), \u2211\nx\u03b1\\x\u03b2 q\u03b1(x\u03b1) = q\u03b2(x\u03b2) for all \u03b2 \u2282 \u03b1 and\nx\u03b2 .\nBy enforcing each of these constraints iteratively one obtains many of the popular sum-product algorithms. Replacing the sum-consistency constraint with maxconsistency gives many of the popular max-product algorithms. A simple example is ordinary BP, which maintains admissiblity at each iteration and a message from i to j enforces consistency between bij and bj .\nIn general, simply iteratively enforcing constraints is not guaranteed to give convergent algorithms. However, as we show in this paper, by iterating through the constraints in a particular order, we obtain monotonically convergent algorithms."}, {"heading": "2.3 Bound minimization and reparameterizations", "text": "We begin by providing an upper bound on the logpartition function whose minimization is equivalent to the maximization in Eq. 5.\nWe consider marginals b\u03b1 of the exponential form:\nb\u03b1(x\u03b1; \u03b8\u0303\u03b1) = 1\nZ\u03b8\u0303\u03b1 exp\n(\n\u03b8\u0303\u03b1(x\u03b1)/c\u03b1\n)\n(6)\nand require that these marginals will be admissible (maintain the \u201ce constraints\u201d). We obtain admissibility by requiring that the variables \u03b8\u0303 will satisfy the following for each x:\n\u2211\n\u03b1\n\u03b8\u03b1(x\u03b1) = \u2211\n\u03b1\n\u03b8\u0303\u03b1(x\u03b1) (7)\nThe algorithms we propose will optimize over the variables \u03b8\u0303 while keeping the constraint in Eq. 7 satisfied at all times. Moreover, they will monotonically decrease an upper bound on the optimum of Eq. 5. In the following two lemmas we provide this bound for the sum and max cases.\nLemma 1 The approximation to the log partition function is bounded above by:\nboundsum(\u03b8\u0303) = \u2211\n\u03b1\nc\u03b1 lnZ\u03b8\u0303\u03b1 (8)\nfor any reparameterization \u03b8\u0303 (i.e., any \u03b8\u0303 satisfying Eq. 7).\nMinimizing boundsum(\u03b8\u0303) over the set of reparameterizations \u03b8\u0303 would give the approximated log-partition function:\nmin \u03b8\u0303 boundsum(\u03b8\u0303) = max q\u2208L(G) F\u0303 (q) (9)\nThis is the optimum of Eq. 5 with T = 1.\nProof: Kolmogorov [7] showed that if \u03b8\u0303 is a reparameterization (i.e. keeping the constraint in Eq. 7), it also holds that \u3008\u03b8\u0303\u3009q = \u3008\u03b8\u3009q for any q \u2208 L(G). Using this property, we can see that the log-partition function is constant under reparameterization: F\u0303 (q; \u03b8) = F\u0303 (q; \u03b8\u0303) for any q \u2208 L(G), and in particular the maximum value\nwill remain the same. Now, using the new variables \u03b8\u0303 we have a trivial bound on the log-partition:\nmax q\u2208L(G) F\u0303 (q; \u03b8\u0303) = max q\u2208L(G)\n\u2211\n\u03b1\n( \u3008\u03b8\u0303\u03b1\u3009q\u03b1 + c\u03b1H\u03b1(q\u03b1) )\n\u2264 \u2211\n\u03b1\nmax q\u03b1\n( \u3008\u03b8\u0303\u03b1\u3009q\u03b1 + c\u03b1H\u03b1(q\u03b1) )\nSince the counting numbers c\u03b1 are non-negative, the marginals defined in Eq. 6 maximize each local functional F\u03b1(q\u03b1; \u03b8\u0303\u03b1, c\u03b1) = \u3008\u03b8\u0303\u03b1\u3009q\u03b1 + c\u03b1H\u03b1(q\u03b1), and the optimal value is c\u03b1 lnZ\u03b8\u0303\u03b1 . Thus,\nmax q\u2208L(G)\nF\u0303 (q; \u03b8\u0303) \u2264 \u2211\n\u03b1\nc\u03b1 lnZ\u03b8\u0303\u03b1 (10)\nThe bound is tight if there exists a reparameterization \u03b8\u0303 such that the marginals b\u03b1(x\u03b1; \u03b8\u0303\u03b1) are sumconsistent (i.e. b \u2208 L(G)). The existence of such a re-parameterization is guaranteed if the maximum of the approximated negative free energy F\u0303 (q; \u03b8) does not happen at an extreme point [14].\nA similar result may be obtained for the MAP case (this result or variants of it appeared in previous works, e.g., [7, 9, 13]).\nLemma 2 The value of the MAP is bounded above by:\nboundmax(\u03b8\u0303) = \u2211\n\u03b1\nmax x\u03b1 \u03b8\u0303\u03b1(x\u03b1) (11)\nfor any reparameterization \u03b8\u0303 (i.e., any \u03b8\u0303 satisfying Eq. 7).\nMinimizing boundmax(\u03b8\u0303) over the set of reparameterizations \u03b8\u0303 would give the optimal value for the regiongraph LP relaxation of the MAP:\nmin \u03b8\u0303 boundmax(\u03b8\u0303) = max q\u2208L(G)\n\u2211\n\u03b1\n\u3008\u03b8\u03b1(x\u03b1)\u3009q\u03b1 (12)\nThis is the optimum of Eq. 5 with T = 0.\nProof: The bound follows directly from the admissiblity constraint so that maxx \u2211\n\u03b1 \u03b8\u0303\u03b1(x\u03b1) \u2264 \u2211\n\u03b1 maxx\u03b1 \u03b8\u0303\u03b1(x\u03b1). The fact that the tightest bound coincides with the LP relaxation was proven in [13].\nWe note that the above two lemmas may also be viewed as an outcome of convex duality. In other words, the original variational maximization problem and the equivalent bound minimization problem are convex duals of each other.\nIn the following sections we provide a framework for deriving minimization algorithms for the above two bounds.\nAlgorithm 1 The tree consistency bound optimization (TCBO) algorithm\nIterate over sub-graphs T of the region graph that have a tree structure:\n1. Choose a tree T\n2. Update the values of \u03b8\u0303t+1\u03b1 for all \u03b1 \u2208 T such that:\n\u2022 re-parameterization is maintained:\n\u03b8(x) = \u2211\n\u03b1\u2208T\n\u03b8\u0303t+1\u03b1 (x\u03b1) + \u2211\n\u03b16\u2208T\n\u03b8\u0303t\u03b1(x\u03b1)\n\u2022 tree-consistency is enforced: Define the beliefs\nbt+1\u03b1 (x\u03b1; \u03b8\u0303 t+1 \u03b1 ) =\n1\nZt+1\u03b1 exp\n(\n\u03b8\u0303t+1\u03b1 (x\u03b1)/c\u03b1\n)\nFor each \u03b1 \u2208 T, \u03b2 \u2208 T, \u03b2 \u2282 \u03b1 and x\u03b2 , optimize the bound to the log-partition function by enforcing sum-consistency :\n\u2211\nx\u03b1\\\u03b2\nbt+1\u03b1 (x\u03b1; \u03b8\u0303 t+1 \u03b1 ) = b t+1 \u03b2 (x\u03b2 ; \u03b8\u0303 t+1 \u03b2 )\nor optimize the bound to the MAP by enforcing max-consistency :\nmax x\u03b1\\\u03b2\nbt+1\u03b1 (x\u03b1; \u03b8\u0303 t+1 \u03b1 ) = b t+1 \u03b2 (x\u03b2 ; \u03b8\u0303 t+1 \u03b2 )"}, {"heading": "3 Bound optimization and consistency", "text": "We propose the tree consistency bound optimization (TCBO) algorithm as a general framework for minimizing the bounds in Sec. 2.3 for the approximated log-partition and for the MAP, within a region graph with positive counting numbers c\u03b1.\nThe idea is to perform updates on trees that are subgraphs of the region-graph. The \u03b8\u0303 corresponding to each such tree will be updated simultaneously in a way that will achieve a monotone decrease in the bound.\nThe method we propose, as described in Algorithm 1 keeps the beliefs admissible with the positive counting numbers c\u03b1 (or equivalently, always maintains \u03b8\u0303(x) that reparameterize the original energy \u03b8(x)). The corresponding \u03b8\u0303 thus satisfy the conditions of the bound in Sec. 2.3. Furthermore, at each iteration, max or sum consistency of the beliefs is enforced for the subtree T .\nAs mentioned earlier, maintaining consistency on subsets does not generally result in convergent algorithms. However, as the following lemmas show, in our case enforcing consistency is equivalent to block coordinate\ndescent on the bound.\nLemma 3 The sum-consistency lemma: Consider the bound minimization problem for the logpartition function with positive counting numbers (Lemma 1), defined on a subset of regions and intersections T . The part of the bound which is influenced by the beliefs of the subset is:\nPBT (\u03b8\u0303T ) = \u2211\n\u03b1\u2208T\nc\u03b1 lnZ\u03b8\u0303\u03b1\nThe problem is to find {\u03b8\u0303\u03b1} for all \u03b1 \u2208 T that minimize PBT (\u03b8\u0303T ) subject to \u03b8\u0303 being reparameterizations of the energy {\u03b8\u03b1}. If for some reparameterization \u03b8\u0303 \u2217 the beliefs b(x\u03b1; \u03b8\u0303 \u2217 \u03b1) \u221d exp ( \u03b8\u0303\u2217\u03b1/c\u03b1 ) are sum-consistent, then it minimizes the bound.\nProof: The part of the bound which is dependent on \u03b8\u0303T the is bounded below:\nPBT (\u03b8\u0303T ) = \u2211\n\u03b1\u2208T\nmax q\u03b1 F\u03b1(q\u03b1; \u03b8\u0303\u03b1, c\u03b1)\n\u2265 max qT\u2208L(G)\n\u2211\n\u03b1\u2208T\nF\u03b1(q\u03b1; \u03b8\u0303\u03b1, c\u03b1)\nNow, if we find variables \u03b8\u0303\u03b1 for all \u03b1 \u2208 T such that they provide global re-parameterization \u03b8\u0303(x) = \u03b8(x) (so we can have a bound), and the marginals b\u03b1(x\u03b1; \u03b8\u0303\u03b1) \u221d exp(\u03b8\u0303\u03b1(x\u03b1)/c\u03b1) which maximize each term F\u03b1(q\u03b1; \u03b8\u0303\u03b1, c\u03b1) separately are also sum-consistent (bT \u2208 L(G)), then PBT (\u03b8\u0303T ) will achieve its optimal value, and thus we perform block coordinate descent on the bound.\nNote that for optimizing the bound to the logpartition, the subset T does not have to form a tree, and the sum-consistency of the beliefs is enough. Yet, it may be easier in practice to enforce sum-consistency on trees.\nLemma 4 The max-consistency lemma: Consider the bound minimization problem for MAP with positive counting numbers (Lemma 2), defined on a subset of regions and intersections that form a tree T . The part of the bound which is influenced by the beliefs of the tree is:\nPBT (\u03b8\u0303T ) = \u2211\n\u03b1\u2208T\nmax x\u03b1 \u03b8\u0303\u03b1(x\u03b1)\nThe problem is to find {\u03b8\u0303\u03b1} for all \u03b1 \u2208 T that minimize PBT (\u03b8\u0303T ) subject to \u03b8\u0303 being reparameterizations of the energy {\u03b8\u03b1}. If for some reparameterization \u03b8\u0303 \u2217 the beliefs b(x\u03b1; \u03b8\u0303 \u2217 \u03b1) \u221d exp ( \u03b8\u0303\u2217\u03b1/c\u03b1 ) are max-consistent, then it minimizes the bound.\nProof: The part of the bound which is dependent on \u03b8\u0303T is bounded below:\nPBT (\u03b8\u0303T ) \u2265 max xT \u03b8\u0303T (xT )\nwhere \u03b8\u0303T (xT ) . = \u2211\n\u03b1\u2208T\n\u03b8\u0303\u03b1(x\u03b1)\nso if we can find an assignment x\u2217T whose cost \u03b8\u0303T (x \u2217 T ) equals PBT (\u03b8\u0303T ), that means we have the tightest bound. Now, if for some reparameterization \u03b8\u0303\u2217T the beliefs b\u03b1(x\u03b1; \u03b8\u0303 \u2217 \u03b1) \u221d exp ( \u03b8\u0303\u2217\u03b1/c\u03b1 ) are max-marginalizable then we can always find an assignment x\u2217T that sits on the maxima of \u03b8\u0303\u2217\u03b1 because the subgraph is a tree (so there cannot be any frustrations). Hence, we obtain \u03b8\u0303T (x \u2217 T ) = PBT (\u03b8\u0303 \u2217 T ), and the bound achieves its optimal value for the coordinates in T .\nThe above two lemmas show that the TCBO algorithm monotonically decreases the bound after each update. In the log-partition case, the bound is strictly convex and thus this strategy finds the global minimum of the bound which is the global maximum of Eq. 5. In the MAP case, the function is not strictly convex and the algorithm may converge to values that are not its global optimum. This phenomenon is shared by most dual descent algorithms (e.g., [2, 7, 13]).\nTCBO is a general scheme and can be implemented for different choices of tree sub-graphs. In the next section we illustrate some possible choices and their relation to known algorithms."}, {"heading": "4 Existing bound minimizing algorithms", "text": "We identify some existing convergent algorithms as instances of TCBO: Heskes\u2019 algorithm [5] for approximating the marginals, and MPLP [2],TRW-S [7], maxsum diffusion (MSD) [13] for approximating MAP.\nFigures 2-5 show the reparametrization, region graph and the tree sub-graph updated at each iteration of these algorithms, for the simple example of a 2x2 grid shown in Fig. 1. Note that all algorithms use a reparameterization with positive double counting numbers. Furthermore, they update only a subtree at\nAlgorithm 2 The max sum diffusion (MSD) algorithm\nIterate over edges between regions < \u03b1, \u03b2 >:\n1. Set the message from \u03b1 to \u03b2:\nm t+1 \u03b1\u2192\u03b2(x\u03b2) = m t \u03b1\u2192\u03b2(x\u03b2) \u00b7\nv u u t maxx\u03b1\\\u03b2 b t \u03b1(x\u03b1)\nbt\u03b2(x\u03b2)\n2. Update the beliefs:\nb t+1 \u03b2 (x\u03b2) \u221d m t+1 \u03b1\u2192\u03b2(x\u03b2) \u00b7\nY\n\u03b1\u2032 6=\u03b1\nm t \u03b1\u2032\u2192\u03b2(x\u03b2)\nb t+1 \u03b1 (x\u03b1) \u221d\n\u03a8\u03b1(x\u03b1)\nm t+1 \u03b1\u2192\u03b2(x\u03b2) \u00b7 Q \u03b2\u2032 6=\u03b2 m t \u03b1\u2192\u03b2\u2032 (x\u03b2)\na time. What remains to be shown is that each iteration achieves consistency among the beliefs (in other words, it satisfies the conditions of TCBO framework and thus monotonically decreases the corresponding upper bound).\nHeskes\u2019 algorithm can be shown to be an instance of TCBO using direct substitution. The update rules are shown in algorithm 3. MPLP (algorithm 4) does not appear at first sight to use the region graph illustrated in Fig. 4, but rather works with edges and singletons. However, as we show in the appendix, there is a way to transform the messages used in the max-product version of Heskes\u2019s algorithm into messages of MPLP using the MPLP region graph. The max-consistency achieved by MSD (algorithm 2) can again be shown directly.\nIt is also possible to use tree graphs (or forests) as regions, and various existing methods indeed use this approach. We may consider a TCBO algorithm which iterates through all edges and nodes, and for each edge or node enforces consistency between it and all trees that contain it. This is illustrated in Fig. 5. A naive\nAlgorithm 3 Heskes\u2019 sum-product algorithm Iterate over intersection regions \u03b2:\n1. \u2200\u03b1 \u2283 \u03b2 set the message from \u03b1 to \u03b2:\nm t+1 \u03b1\u2192\u03b2(x\u03b2) =\nP x\u03b1\\\u03b2 bt\u03b1(x\u03b1)\nmt\u03b2\u2192\u03b1(x\u03b2)\n2. Update the belief of the intersection region:\nb t+1 \u03b2 (x\u03b2) \u221d\nY\n\u03b1\u2283\u03b2\n\u201c\nm t+1 \u03b1\u2192\u03b2(x\u03b2)\n\u201dc\u03b1/c\u0302\u03b2\n3. \u2200\u03b1 \u2283 \u03b2 set the messages to the parent regions and their beliefs:\nm t+1 \u03b2\u2192\u03b1(x\u03b2) =\nb t+1 \u03b2 (x\u03b2)\nm t+1 \u03b1\u2192\u03b2(x\u03b2)\nb t+1 \u03b1 (x\u03b1) \u221d \u03a8 1/c\u03b1 \u03b1 (x\u03b1) \u00b7 m t+1 \u03b2\u2192\u03b1(x\u03b1)\nY\n\u03b2\u2032 6=\u03b2\nm t \u03b2\u2032\u2192\u03b1(x \u2032 \u03b2)\nimplementation of such a scheme is costly, as it requires multiple tree updates for every edge. However, Kolmogorov [7] showed that there exists an efficient implementation (which he called TRW-S) of such a scheme in the MAP case. This implementation may only be applied if the trees are monotonic chains, defined as follows: given an ordering of the nodes in a graph, a set of chains is monotonic if the order of nodes in the chain respects the given ordering. This structure allows one to reuse messages in a way that simultaneously implements operations on multiple trees. The scheduling of messages is important for guaranteeing convergence in this case. It turns out that one needs to scan nodes along the pre-specified order, first forward and then backward.\nIn the marginals case, the TRW algorithm of Wainwright [12] corresponds to optimizing over tree regions but is not provably convergent. In the next section we show how to derive a convergent algorithm for this case using our formalism.\nAlgorithm 4 The max product linear programming (MPLP) algorithm\nIterate over pairs of neighbouring nodes < ij >:\n1. Set the message from i to < ij >:\nm t+1 i\u2192ij(xi) =\nY\nk\u2208N(i)\\j\nm t ik\u2192i(xi)\nand equivalently from j to < ij >\n2. Update the pairwise beliefs of < ij >:\nb t+1 ij (xi, xj) \u221d\nq\n\u03a8ij(xi, xj) \u00b7 m t+1 i\u2192ij(xi) \u00b7 m t+1 j\u2192ij(xj)\n3. Set the messages from < ij > to i (and equivalently from < ij > to j):\nm t+1 ij\u2192i(xi) \u221d\nv u u u t maxxj \u201c \u03a8ij(xi, xj) \u00b7 m t+1 j\u2192ij(xj) \u201d\nm t+1 i\u2192ij(xi)\n4. Set the beliefs of i (and same for j):\nb t+1 i (xi) \u221d m t+1 ij\u2192i(xi) \u00b7\nY\nk\u2208N(i)\\j\nm t ik\u2192i(xi)"}, {"heading": "5 New bound minimizing algorithms", "text": "By replacing the max with a sum (or vice versa) in the algorithms discussed in the previous section, we obtain algorithms that enforce a different type of consistency, and keep the same reparameterization and region graph as shown in the figures. Thus, the maxproduct version of Heskes\u2019 algorithm and the sumproduct versions of TRW-S, MPLP and MSD are convergent with respect to the relevant bound.\nThe TRW-S sum-product case is especially interesting. In this case the relevant bound becomes the treereweighted log-partition function bound introduced in [12]. The message passing algorithm suggested in [12] does not generally converge. In contrast, the TRWS sum-product algorithm is guaranteed to converge,\nsince it is an instance of a TCBO algorithm for the sum case. Furthermore, this TRW-S variant differs from the algorithm in [12] only in the scheduling of messages.\nAn additional algorithm that can be easily shown to be convergent is two way GBP [14] with all double counting numbers c\u03b1 = 1, both in the sum and in the max versions. At each iteration, two-way GBP updates only the beliefs of a region and one of its subregions, which is trivially a tree. The fact that it maintains reparameterization and enforces consistency can be shown directly. In fact, it can be shown that two way GBP with c\u03b1 = 1 is identical to MSD."}, {"heading": "6 Experiments", "text": "We present two experiments to illustrate the convergence of our sum and max algorithms. All algorithms were applied to an instance of a 10x10 \u201cspin glass\u201d with pairwise terms drawn randomly from [\u22129, 9] and field from [\u22121, 1]. In each case we tested the new TCBO algorithms.\nFor estimating the log-partition, we ran TRW and considered a uniform distribution over 2 spanning forests in the graph: all horizontal and all vertical chains. These chains are monotone with respect to the node ordering {1, 2, ..., 100}. We ran TRW-S by following the nodes order first forward and then backward, updating each time only the messages in the direction of\nAlgorithm 5 The sequential tree reweighted BP (TRW-"}, {"heading": "S) algorithm", "text": "1. Iterate over edges i \u2192 j in a certain updating order, and set the message from i to j:\nmi\u2192j(xj) \u221d max xi\n\u03a8i(xi)\u03a8 1/\u03c1ij ij (xi, xj)\nQ k\u2208N(i)\\j m \u03c1ik k\u2192i(xi)\nm 1\u2212\u03c1ij j\u2192i (xi)\n2. After each iteration over all edges, update all singleton and pairwise beliefs:\nbi(xi) \u221d \u03a8i(xi) Y\nj\u2208N(i)\nm \u03c1ij j\u2192i(xi)\nbij(xi, xj) \u221d \u03a8i(xi)\u03a8j(xj)\u03a8 1/\u03c1ij ij (xi, xj)\n\u00b7\nQ k\u2208N(i)\\j m \u03c1ik k\u2192i(xi)\nm 1\u2212\u03c1ij j\u2192i (xi)\nQ k\u2208N(j)\\i m \u03c1jk k\u2192j(xj)\nm 1\u2212\u03c1ij i\u2192j (xj)\nthe scan. Fig. 6 shows a comparison of this schedule to TRW where the node ordering is followed in a forward manner, and all outgoing messages are updated from each node. Both schedules keep a re-parameterization and provide a bound to the log-partition, yet only TRW-S monotonically decreases it at each iteration.\nFor the MAP case, we ran the max-product version of Heskes\u2019 algorithm using a region graph of pairs (with double counting numbers 1) and singletons (with double counting numbers 0). We also ran MPLP and MSD on the same problem. Fig. 7 shows the bounds obtained after each iteration. As can be seen, all three algorithms monotonically converged to the same value."}, {"heading": "7 Discussion", "text": "Despite the empirical success of max-product and sumproduct algorithms in applications, the original algorithms are not guaranteed to converge. Much research in recent years has therefore been devoted to devising convergent algorithms. Typically these recent algorithms are either max-product or sum-product and their proof of convergence is specific to the algorithm. Here we have presented a unified framework for convergent message passing algorithms and showed the importance of enforcing consistency in both sum-product and max-product algorithms. Not only does this analogy allow us to give a unified derivation for existing algorithms, it also gives an easy way to derive novel algorithms from existing ones by exchanging maximizations and summations.\nAlthough many convergent algorithms are instances of our framework, it is worth pointing out two convergent algorithms that are not. The first is Hazan and Shashua\u2019s recent algorithm [3, 4] which works for provably convex double counting numbers (not necessarily\npositive as we are assuming). The second is ordinary BP on a single cycle, which can be shown to be convergent in both its sum and max product forms. We emphasize that even negative counting numbers can be handled by us in some cases, by using larger regions.\nAll the algorithms we discussed here in fact only updated star graphs in the region graph. Our conditions for monotonicity apply to general tree updates. However, it seems less straightforward to obtain general (non-star) tree updates that achieve (max or sum) consistency and reparameterization simultaneously. Interestingly, the tree based updates in [9] do monotonically decrease an upper bound but seem not to satisfy maxconsistency. Thus, it remains an interesting challenge to find general tree updates that satisfy the consistency constraints, as these could be easily used interchangeably for MAP and marginals.\nPerhaps the most intriguing result of our analysis is the importance of update schedule for obtaining convergence \u2013 a non-convergent algorithm becomes convergent when the right update schedule is used. It will be interesting to see whether convergent update schedules can be derived for an even larger class of message-passing algorithms."}, {"heading": "Appendix A MPLP as a TCBO", "text": "algorithm\nMPLP can be derived as an instance of the maxproduct version to Heskes\u2019 algorithm, and thus to be shown as a TCBO. We derive it for the case where the potentials in the original graph are defined for pairs, and then the generalization to larger cliques is simple.\nWe consider the max-product version to Heskes\u2019 algorithm, applied to the region graph in Fig. 4. The region graph consists on a layer of all the stars Si in the original graph. Each star Si corresponds to a node i and all its neighbours in the original graph, with the edges between them. The layer of intersection regions would be all the edges < ij >. The counting numbers would be cSi = 1 for a star region and cij = 0 for an edge region. Assuming that the original potentials are \u03b8ij(xi, xj) for all pairs, we define the new potentials to be \u03b8\u0303(xi, xj) = 0 for the subsets, and \u03b8\u0303(xSi) = 1 2 \u2211 j\u2208Ni \u03b8ij(xi, xj) for the stars.\nThe messages passed in MPLP mij\u2192i(xi) are obtained from the messages \u00b5ij\u2192Si(xi, xj) passed in Heskes\u2019 algorithm through the transformation:\nmtij\u2192i(xi) = max xj\n(\n1 2 \u03b8ij(xi, xj) + \u00b5 t ij\u2192Si(xi, xj)\n)\nIt is easy to verify that this transformation yields the same update rule and beliefs of MPLP, and the same bound is minimized.\nIn order to generalize our derivation for clusters larger than pairs, we should replace the edges and stars by\nclusters and \u201chyper stars\u201d respectively, in which the centers are the intersections of the clusters."}], "references": [{"title": "Convergent propagation algorithms via oriented trees", "author": ["A. Globerson", "T. Jaakkola"], "venue": "In UAI", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Fixing max-product: Convergent message passing algorithms for MAP LPrelaxations", "author": ["A. Globerson", "T. Jaakkola"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Convergent messagepassing algorithms for inference over general graphs with convex free energies", "author": ["T. Hazan", "A. Shashua"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "The norm-product belief propagation: Primal-dual message-passing for lprelaxation and approximate inference", "author": ["T. Hazan", "A. Shashua"], "venue": "Technical report, Hebrew University of Jerusalem,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Convexity arguments for efficient minimization of the Bethe and Kikuchi free energies", "author": ["T. Heskes"], "venue": "JAIR, 26:153\u2013190,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Stochastic reasoning, free energy, and information geometry", "author": ["S. Ikeda", "T. Tanaka", "S. Amari"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "In UAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Tree block coordinate descent for MAP in graphical models", "author": ["D. Sontag", "T. Jaakkola"], "venue": "In AISTATS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Graphical models, exponential families and variational inference", "author": ["M. Wainwright", "M.I. Jordan"], "venue": "Technical report, UC Berkeley, Dept. of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Map estimation via agreement on trees: messagepassing and linear programming", "author": ["M.J. Wainwright", "T. Jaakkola", "A.S. Willsky"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "A new class of upper bounds on the log partition function", "author": ["M.J. Wainwright", "T. Jaakkola", "A.S. Willsky"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "A linear programming approach to maxsum problem: A review", "author": ["T. Werner"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "CCCP algorithms to minimize the bethe and kikuchi free energies: Convergent alternatives to belief propagation", "author": ["A.L. Yuille"], "venue": "Neural Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}], "referenceMentions": [{"referenceID": 9, "context": "For the marginals case this usually corresponds to minimization of a free energy functional, and for the MAP problem it corresponds to solving a linear programming (LP) relaxation [10].", "startOffset": 180, "endOffset": 184}, {"referenceID": 4, "context": "Perhaps the most widely used message-passing algorithms are \u201cbelief propagation\u201d and its generalizations [5, 8, 12, 14].", "startOffset": 105, "endOffset": 119}, {"referenceID": 7, "context": "Perhaps the most widely used message-passing algorithms are \u201cbelief propagation\u201d and its generalizations [5, 8, 12, 14].", "startOffset": 105, "endOffset": 119}, {"referenceID": 11, "context": "Perhaps the most widely used message-passing algorithms are \u201cbelief propagation\u201d and its generalizations [5, 8, 12, 14].", "startOffset": 105, "endOffset": 119}, {"referenceID": 13, "context": "Perhaps the most widely used message-passing algorithms are \u201cbelief propagation\u201d and its generalizations [5, 8, 12, 14].", "startOffset": 105, "endOffset": 119}, {"referenceID": 7, "context": "Yet despite the spectacular empirical success of these algorithms in real-world applications, they are not guaranteed to converge, and variants of \u201cdampening\u201d are often used to improve their convergence [8, 12, 14].", "startOffset": 203, "endOffset": 214}, {"referenceID": 11, "context": "Yet despite the spectacular empirical success of these algorithms in real-world applications, they are not guaranteed to converge, and variants of \u201cdampening\u201d are often used to improve their convergence [8, 12, 14].", "startOffset": 203, "endOffset": 214}, {"referenceID": 13, "context": "Yet despite the spectacular empirical success of these algorithms in real-world applications, they are not guaranteed to converge, and variants of \u201cdampening\u201d are often used to improve their convergence [8, 12, 14].", "startOffset": 203, "endOffset": 214}, {"referenceID": 1, "context": "There has therefore been much recent work on convergent message passing algorithms [2, 5, 7, 13].", "startOffset": 83, "endOffset": 96}, {"referenceID": 4, "context": "There has therefore been much recent work on convergent message passing algorithms [2, 5, 7, 13].", "startOffset": 83, "endOffset": 96}, {"referenceID": 6, "context": "There has therefore been much recent work on convergent message passing algorithms [2, 5, 7, 13].", "startOffset": 83, "endOffset": 96}, {"referenceID": 12, "context": "There has therefore been much recent work on convergent message passing algorithms [2, 5, 7, 13].", "startOffset": 83, "endOffset": 96}, {"referenceID": 6, "context": "Perhaps the most intriguing example of this is Kolmogorov\u2019s TRW-S algorithm [7] which is simply Wainwright\u2019s tree-reweighted max-product algorithm [11] with a different update schedule.", "startOffset": 76, "endOffset": 79}, {"referenceID": 10, "context": "Perhaps the most intriguing example of this is Kolmogorov\u2019s TRW-S algorithm [7] which is simply Wainwright\u2019s tree-reweighted max-product algorithm [11] with a different update schedule.", "startOffset": 147, "endOffset": 151}, {"referenceID": 8, "context": "[9] in the context of solving the MAP approximation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "For instance, we consider the tree-reweighted (TRW) free energy minimization problem [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "Recently two works have provided convergent algorithms for this problem [1, 3], but these were more involved than standard message passing algorithms.", "startOffset": 72, "endOffset": 78}, {"referenceID": 2, "context": "Recently two works have provided convergent algorithms for this problem [1, 3], but these were more involved than standard message passing algorithms.", "startOffset": 72, "endOffset": 78}, {"referenceID": 13, "context": "A class of approximate free energies, discussed in [14], is based on the concept of a region graph G whose nodes \u03b1 are regions of the original graph, and whose edges represent subregion relationships (i.", "startOffset": 51, "endOffset": 55}, {"referenceID": 9, "context": "This set (called the marginal polytope [10]) cannot be expressed in a compact form, and is typically approximated.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "Take together, this results in the following standard variational approximation [10]:", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": ", [5]) have focused on entropies which are obtained by considering only concave H\u0303G,c(q) functions.", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": "It is easy to show using Lagrange multipliers, that local optima of F\u0303 should satisfy two types of constraints [5, 6, 12, 14, 15]", "startOffset": 111, "endOffset": 129}, {"referenceID": 5, "context": "It is easy to show using Lagrange multipliers, that local optima of F\u0303 should satisfy two types of constraints [5, 6, 12, 14, 15]", "startOffset": 111, "endOffset": 129}, {"referenceID": 11, "context": "It is easy to show using Lagrange multipliers, that local optima of F\u0303 should satisfy two types of constraints [5, 6, 12, 14, 15]", "startOffset": 111, "endOffset": 129}, {"referenceID": 13, "context": "It is easy to show using Lagrange multipliers, that local optima of F\u0303 should satisfy two types of constraints [5, 6, 12, 14, 15]", "startOffset": 111, "endOffset": 129}, {"referenceID": 14, "context": "It is easy to show using Lagrange multipliers, that local optima of F\u0303 should satisfy two types of constraints [5, 6, 12, 14, 15]", "startOffset": 111, "endOffset": 129}, {"referenceID": 6, "context": "Proof: Kolmogorov [7] showed that if \u03b8\u0303 is a reparameterization (i.", "startOffset": 18, "endOffset": 21}, {"referenceID": 13, "context": "The existence of such a re-parameterization is guaranteed if the maximum of the approximated negative free energy F\u0303 (q; \u03b8) does not happen at an extreme point [14].", "startOffset": 160, "endOffset": 164}, {"referenceID": 6, "context": ", [7, 9, 13]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 8, "context": ", [7, 9, 13]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 12, "context": ", [7, 9, 13]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 12, "context": "The fact that the tightest bound coincides with the LP relaxation was proven in [13].", "startOffset": 80, "endOffset": 84}, {"referenceID": 1, "context": ", [2, 7, 13]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 6, "context": ", [2, 7, 13]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 12, "context": ", [2, 7, 13]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 4, "context": "We identify some existing convergent algorithms as instances of TCBO: Heskes\u2019 algorithm [5] for approximating the marginals, and MPLP [2],TRW-S [7], maxsum diffusion (MSD) [13] for approximating MAP.", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "We identify some existing convergent algorithms as instances of TCBO: Heskes\u2019 algorithm [5] for approximating the marginals, and MPLP [2],TRW-S [7], maxsum diffusion (MSD) [13] for approximating MAP.", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "We identify some existing convergent algorithms as instances of TCBO: Heskes\u2019 algorithm [5] for approximating the marginals, and MPLP [2],TRW-S [7], maxsum diffusion (MSD) [13] for approximating MAP.", "startOffset": 144, "endOffset": 147}, {"referenceID": 12, "context": "We identify some existing convergent algorithms as instances of TCBO: Heskes\u2019 algorithm [5] for approximating the marginals, and MPLP [2],TRW-S [7], maxsum diffusion (MSD) [13] for approximating MAP.", "startOffset": 172, "endOffset": 176}, {"referenceID": 6, "context": "However, Kolmogorov [7] showed that there exists an efficient implementation (which he called TRW-S) of such a scheme in the MAP case.", "startOffset": 20, "endOffset": 23}, {"referenceID": 11, "context": "In the marginals case, the TRW algorithm of Wainwright [12] corresponds to optimizing over tree regions but is not provably convergent.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "In this case the relevant bound becomes the treereweighted log-partition function bound introduced in [12].", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "The message passing algorithm suggested in [12] does not generally converge.", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "Furthermore, this TRW-S variant differs from the algorithm in [12] only in the scheduling of messages.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "An additional algorithm that can be easily shown to be convergent is two way GBP [14] with all double counting numbers c\u03b1 = 1, both in the sum and in the max versions.", "startOffset": 81, "endOffset": 85}, {"referenceID": 2, "context": "The first is Hazan and Shashua\u2019s recent algorithm [3, 4] which works for provably convex double counting numbers (not necessarily 0 10 20 30 40 50 60 860 870 880 890 900 910 920 930 940", "startOffset": 50, "endOffset": 56}, {"referenceID": 3, "context": "The first is Hazan and Shashua\u2019s recent algorithm [3, 4] which works for provably convex double counting numbers (not necessarily 0 10 20 30 40 50 60 860 870 880 890 900 910 920 930 940", "startOffset": 50, "endOffset": 56}, {"referenceID": 8, "context": "Interestingly, the tree based updates in [9] do monotonically decrease an upper bound but seem not to satisfy maxconsistency.", "startOffset": 41, "endOffset": 44}], "year": 2009, "abstractText": "Message-passing algorithms have emerged as powerful techniques for approximate inference in graphical models. When these algorithms converge, they can be shown to find local (or sometimes even global) optima of variational formulations to the inference problem. But many of the most popular algorithms are not guaranteed to converge. This has lead to recent interest in convergent message-passing algorithms. In this paper, we present a unified view of convergent message-passing algorithms. We present a simple derivation of an abstract algorithm, tree-consistency bound optimization (TCBO) that is provably convergent in both its sum and max product forms. We then show that many of the existing convergent algorithms are instances of our TCBO algorithm, and obtain novel convergent algorithms \u201cfor free\u201d by exchanging maximizations and summations in existing algorithms. In particular, we show that Wainwright\u2019s non-convergent sum-product algorithm for tree based variational bounds, is actually convergent with the right update order for the case where trees are monotonic chains.", "creator": null}}}