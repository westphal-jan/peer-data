{"id": "1205.2645", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Distributed Parallel Inference on Large Factor Graphs", "abstract": "as computer clusters become more common and population size of the problems encountered in the array encompassing java grows, there is consistently increasing needed for robust common inference algorithms. microsoft eliminate the problem surrounding similarity inference through the factor graphs in the distributed processing structures of computer clusters. soon develop brand new complexity data inference platform, dbrsplash, which performed over - par graph partitioning, belief residual scheduling, and processor work order operations. we empirically evaluate the dbrsplash algorithm combining existing specialized processor line and demonstrate alternatives to super - linear performance thus solving enormous factor graph models.", "histories": [["v1", "Wed, 9 May 2012 15:23:28 GMT  (505kb)", "http://arxiv.org/abs/1205.2645v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["joseph e gonzalez", "yucheng low", "carlos e guestrin", "david o'hallaron"], "accepted": false, "id": "1205.2645"}, "pdf": {"name": "1205.2645.pdf", "metadata": {"source": "CRF", "title": "Distributed Parallel Inference on Large Factor Graphs", "authors": ["Joseph E. Gonzalez", "Yucheng Low", "Carlos Guestrin"], "emails": ["jegonzal@cs.cmu.edu", "ylow@cs.cmu.edu", "guestrin@cs.cmu.edu", "david.ohallaron@intel.com"], "sections": [{"heading": null, "text": "As computer clusters become more common and the size of the problems encountered in the field of AI grows, there is an increasing demand for efficient parallel inference algorithms. We consider the problem of parallel inference on large factor graphs in the distributed memory setting of computer clusters. We develop a new efficient parallel inference algorithm, DBRSplash, which incorporates over-segmented graph partitioning, belief residual scheduling, and uniform work Splash operations. We empirically evaluate the DBRSplash algorithm on a 120 processor cluster and demonstrate linear to super-linear performance gains on large factor graph models."}, {"heading": "1 INTRODUCTION", "text": "A computer cluster is a large collection of processors connected by a fast reliable communication network and configured to achieve a common task. Here we define a processor as a single processing element with a unique instruction counter1. Cluster computing confers both the obvious increase in computational throughput and memory capacity as well as the less obvious increase in memory bandwidth and cache capacity. With the availability of affordable commodity hardware and high performance networking, the AI community has increasing access to computer clusters. Unfortunately, many computationally intensive tasks in AI are not directly able to efficiently utilize cluster resources.\nWork by Newman et al. [2007] and Asuncion et al. [2008] in parallel inference for latent topic models and by Paskin et al. [2005] and Funiak et al. [2006] in distributed inference for sensor networks adopt a message based asynchronous computation model to address the important task of distributed graphical model inference. However their approaches are specialized to particular models or settings. Alternatively, in Gonzalez et al. [2009] we explored\n1We treat each core on a multi-core computer as a separate processor.\nthe problem of general parallel inference in the multicore shared memory setting. However, the shared memory model does not efficiently scale to large clusters, and the algorithm we proposed, ResidualSplash, makes scheduling assumptions that fail on large irregular models.\nHere, we extend the ResidualSplash algorithm to large factor graphs in the distributed memory cluster setting and address several critical challenges to distributed parallel inference. We adopt the message passing computational model for cluster parallelism. In this model, the state of the algorithm is spread over p processors which only exchange information by passing \u201cmessages.\u201d This differs from the shared memory setting of Gonzalez et al. [2009], where every processor has direct access to all available memory.\nWhile the message passing model requires efficient work partitioning, and distributed reasoning, it also introduces several key advantages over the multi-core shared memory setting. Because processors are no longer tightly coupled, it is easier to construct substantially larger clusters. Because each processor has its own memory and dedicated bus, clusters provide increased memory capacity, memory bandwidth, and cache efficiency, permitting super-linear performance gains. Increased access to memory and memory bandwidth is critical to the performance of AI algorithms which often operate on large data-sets and quickly saturate the system bus.\nIn this paper, we outline the key challenges to efficient large scale distributed inference and address these challenges through the DBRSplash algorithm. The key contributions of this paper are:\n\u2022 A formalization of state partitioning as a weighted graph cut and an empirical analysis of an approximate cutting procedure which exploits over-partitioning to improve work-balance. \u2022 A belief residual scheduling and a work balanced Splash for improved scheduling on irregular graphs. \u2022 DBRSplash, a distributed inference algorithm which retains the ResidualSplash parallel optimality. \u2022 An empirical evaluation of DBRSplash on a 120 node cluster demonstrating linear to super-linear performance scaling for large factor graphs."}, {"heading": "2 BELIEF PROPAGATION", "text": "Many important probabilistic models may be represented by factorized distributions of the form:\nP (x1, . . . , xn) \u221d \u220f \u03b1\u2208C \u03c8\u03b1(x\u03b1), (2.1)\nwhere the set of factors F = {\u03c8\u03b1 : \u03b1 \u2208 C} correspond to un-normalized positive functions, \u03c8\u03b1 : x\u03b1 \u2192 R+ over subsets C = {\u03b1 : X\u03b1 \u2286 X} of the random variables. Here, we focus on discrete random variables Xi \u2208 {1, . . . , Ai} taking on some finite set of Ai possible values.\nDistributions of the form Eq. (2.1) are naturally represented as a Factor Graph G = ({X ,F} , E), where the vertices V = X \u222a F are the variables and factors, and the edges E = {{\u03c8j , Xi} : Xi \u2208 Xj} connect factors with the variables in their domain. Factor graphs provide a convenient representation of the dependencies between variables which we will later exploit to partition the distribution over processors. To simplify notation, we use \u03c8i, Xj \u2208 V to refer to vertices when we wish to distinguish between factors and variables, and i, j \u2208 V otherwise. We define \u0393i as the neighbors of i in the factor graph.\nEstimating marginal distributions is essential to learning and inference in factor graphs. While computing exact marginals is NP-hard in general, there are many popular approximate inference algorithms. Belief Propagation (BP), or the Sum-Product algorithm, is a commonly used approximate inference algorithm originally proposed by Pearl [1988]. In BP, \u201cmessages\u201d (parameters), are iteratively computed along edges in the factor graph until convergence and then used to estimate marginals. The message sent from variable Xi to factor \u03c8j along the edge {Xi, \u03c8j} is given in Eq. (2.2) and the message sent from factor \u03c8j to vertex Xi along the edge {\u03c8j , Xi} is given in Eq. (2.3),\nm Xi\u2192\u03c8j\n(xi) \u221d \u220f\nk\u2208\u0393i\\j\nm k\u2192i (xi) (2.2)\nm \u03c8j\u2192Xi (xi) \u221d \u2211 xj\\xi \u03c8j(xj) \u220f k\u2208\u0393j\\i m k\u2192j (xk) (2.3)\nwhere \u2211\nxj\\xi is a sum over all assignments to xj with xi restricted, and \u220f k\u2208\u0393j\\i is a product over all neighbors of the vertex \u03c8j excluding variable Xi.\nIn synchronous BP, all vertices simultaneously compute their outbound messages at every iteration using the messages from the previous iteration. In asynchronous BP, messages are updated sequentially using the most recent messages. Typically, message are sent until the maximum change in messages is bounded by a small constant \u03b2 \u2265 0:\nmax (i,j)\u2208E \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 mi\u2192j(new) \u2212 mi\u2192j(old) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\n1\n\u2264 \u03b2. (2.4)\nThe estimates of the marginal distributions are then, P (Xi = xi) \u2248 bXi(xi) \u221d \u220f j\u2208\u0393i m j\u2192i (xi) (2.5)\nP (Xi = xi) \u2248 bXi(xi) \u221d \u03c8i(xi) \u220f j\u2208\u0393i m j\u2192i (xj).\nWhile BP is guaranteed to converge to the exact marginals in acyclic graphs, there are few guarantees for convergence or correctness in general graphs. Nonetheless, BP on cyclic graphs is used extensively with great success as an approximate inference algorithm [McEliece et al., 1998, Sun et al., 2003, Yedidia et al., 2003, Yanover and Weiss, 2002].\nThe \u201cmessage\u201d passing formulation and embarrassingly parallel synchronous update schedule suggests that BP is an embarrassingly parallel algorithm. However, in Gonzalez et al. [2009] we showed that efficient parallel inference in the shared memory setting is limited by the sequential dependencies among messages. Furthermore, the natural synchronous parallel scheduling can be asymptotically slower than the optimal parallel asynchronous scheduling. We provided a general asynchronous parallel algorithm, ResidualSplash, for the shared memory setting and demonstrated its optimality in the sequentially limiting case of chain graphical models."}, {"heading": "2.1 THE RESIDUAL SPLASH ALGORITHM", "text": "Here, we briefly review the key points of the ResidualSplash algorithm which we extend in later sections. The Splash procedure, shown in Fig. 1, generalizes the optimal forward-backward sequential update ordering used in acyclic graphical models. When applied to a vertex v in the factor graph, the Splash procedure first constructs a fixed volume breadth first search (BFS) ordering rooted at v. Then, starting at the leaves, vertices are sequentially updated until the root is reached and then the process is reversed. When a vertex is updated, all outbound messages from a vertex are recomputed using the current inbound messages. In an acyclic subgraph, the Splash procedure is equivalent to running forward-backward BP.\nThe ResidualSplash algorithm applies a variation of the residual scheduling heuristic proposed by Elidan\net al. [2006] to determine the Splash ordering, prune the Splash BFS, and assess convergence. In particular the ResidualSplash algorithm assigns residual,\nrj = max i\u2208\u0393j \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 mi\u2192jnew \u2212 mi\u2192jold \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\n1\n(2.6)\nto each vertex. The value rj can be loosely interpreted as a measure of the value of updating vertex j. If rj = 0 then updating a vertex will waste processor cycles as outgoing messages will not change. The ResidualSplash algorithm repeatedly runs the Splash procedure on the vertex with highest residual. When constructing the BFS, the Splash procedure stops searching along a branch if it reaches a vertex with residual less than the termination threshold \u03b2. Finally, ResidualSplash terminates when the highest residual is less than \u03b2."}, {"heading": "3 DISTRIBUTING STATE", "text": "In this section, we address the challenges associated with distributing the state of the ResidualSplash algorithm over p processors. In the shared memory setting, each processor has efficient direct access to all data structures and memory, permitting a single shared scheduling queue and message set. Consequently, the time and resources required to access the scheduling queue and update messages are symmetric for all processors. Conversely, in the distributed memory setting, access times are not symmetric. Instead, each processor can only directly access its local memory and must pass messages to communicate with other processors."}, {"heading": "3.1 FACTOR GRAPH AND MESSAGES", "text": "We begin by partitioning the factor graph and messages. To maximize throughput and hide network latency, we must minimize communication and ensure that the data needed for message computations is locally available. We define a partitioning of the factor graph over p processors as a set B = {B1, ..., Bp} of disjoint sets of vertices Bk \u2286 V such that \u222apk=1Bk = V . Given a partitioning B we assign all the factor data associated with \u03c8i \u2208 Bk to the kth processor. Finally, we store each message on the processor containing the destination vertex.\nEach vertex update is therefore a local procedure. For instance, if vertex i is updated, the processor owning vertex i can read factors and all incoming messages without communication. To maintain the locality invariant, after the new outgoing message is computed, it is transmitted to the processor owning the message.\nBy imposing the above locality constraints, we define the storage, computation, and communication responsibilities of each processor under a particular partitioning. Therefore, we can frame the minimum communication load balancing objective in terms of a graph partitioning, which is a classical problem in high performance computing. We\nformally define the graph partitioning problem as:\nmin B\n\u2211 B\u2208B \u2211 (i\u2208B,j /\u2208B)\u2208E wij (3.1)\nsubj. to: \u2200B \u2208 B \u2211 i\u2208B wi \u2264 \u03b3 p \u2211 v\u2208v wv (3.2)\nwhere wij is the communication cost of the edge between vertex i and vertex j, wi is the total computation associated with the vertex i, and \u03b3 \u2265 1 is the balance coefficient. The objective in Eq. (3.1) minimizes communication while for small \u03b3, the constraint in Eq. (3.2) ensures work balance.\nTo define the communication and computation costs we introduce Ui the total number of updates to vertex i. We define the communication cost as wij = (Ui + Uj) \u00d7 (min(|Ai|, |Aj |) + Ccomm) the total number of times a message is sent across the edge (i, j) times the size of the message plus the fixed header cost Ccomm. We define the work associated with each variable as Ui \u00d7 |\u0393i| \u00d7 |Ai|, the number of updates, times the number of neighbors, times the size of that variable. Similarly, we define the work associated with each factor as Ui \u00d7 |\u0393i| \u00d7 \u220f j\u2208\u0393i |Aj |, the number of updates. times the number of neighbors. times the size of the factor. While the exponential dependence on degree may suggest factors are more costly, their degree is usually small compared to variables.\nUnfortunately, obtaining an optimal partitioning or near optimal partitioning is NP -Hard in general and the best approximation algorithms are generally slow. Fortunately, the there are several very fast heuristic approaches which typically produce reasonable partitions in timeO (|E|) linear in the number of edges. Here we use the collection of multilevel graph partitioning algorithms in the METIS [Karypis and Kumar, 1998] graph partitioning library. These algorithms, iteratively coarsen the underlying graph, apply high quality partitioning techniques to the small coarsened graph, and then iteratively refine the coarsened graph to obtain a high quality partitioning of the original graph. While there are no theoretical guarantees, these algorithms have been shown to perform well in practice and are commonly used in the parallel computing setting."}, {"heading": "3.1.1 Update Counts Ui", "text": "Unfortunately, due to dynamic scheduling, the update counts Ui for each vertex depend on the evidence, graph structure, and progress towards convergence, and are not known before running ResidualSplash. In practice we find that the ResidualSplash algorithm updates vertices in a highly non-uniform manner; a key property of the dynamic scheduling, which enables more frequent updates of slower converging messages.\nTo illustrate the difficulty involved in estimating the update counts for each vertex, we introduce the synthetic denoising task. The input, shown in Fig. 2(a), is a grayscale image\nwith independent Gaussian noise N ( 0, \u03c32 ) added to each pixel. The factor graph (Fig. 2(b)) corresponds to the pairwise grid Markov Random Field constructed by introducing a latent random variable for each pixel and connecting neighboring variables by factors that encode a similarity preference. The synthetic image was constructed to have a nonuniform update pattern (Fig. 2(d)) by making the top half more irregular than the bottom half. The distribution of vertex update frequencies (Fig. 2(f)) for the denoising task is nonuniform with a few vertices being updated orders of magnitude more frequently than the rest. The update patterns is temporally inconsistent frustrating attempts to estimate future update counts using past behavior. (Fig. 2(g))."}, {"heading": "3.1.2 Uninformed Partitioning", "text": "Surprisingly, in practice we find that an uninformed cut obtained by setting the number of updates to a constant (i.e., Ui = 1) achieves partitions with comparable communication cost and work balance as those obtained when using the true update counts. In Table 1 we construct uninformed p = 120 partitions B\u0302 with U\u0302i = 1 on several graphs and report the communication cost and balance\nRel. Com. Cost =\n\u2211 B\u2208B\u0302 \u2211 (u\u2208B,v/\u2208B)\u2208E wuv\u2211\nB\u2208B\u2217 \u2211 (u\u2208B,v/\u2208B)\u2208E wuv\nRel. Work Balance = p\u2211\nv\u2208V wv max B\u2208B\u0302 \u2211 v\u2208B wv\nrelative to the ideal cut B\u2217 obtained using the true update counts Ui. We find that uninformed cuts have lower communication costs at the expense of increased imbalance. This discrepancy arises from the need to satisfy the balance requirement with respect to the true Ui at the expense of a higher communication cost."}, {"heading": "3.1.3 Over-Partitioning", "text": "Because uninformed partitions tend to have reduced communication cost and greater work imbalance relative to informed partitions, we propose over-partitioning to improve the overall work balance with a small increase in communication cost. When partitioning the graph with an uninformed cut a frequently updated subgraph may be placed within a single partition. To lessen the chance of such an event, we can over-partition the graph into k \u00d7 p balanced partitions and then randomly redistribute the partitions to the original p processors. By partitioning the graph\nmore finely and randomly assigning regions to different processor, we more evenly distribute nonuniform update patterns improving the overall work balance. However, over-partitioning also increases the number of edges crossing the cut and therefore the communication cost. By overpartitioning in the denoise task we are able to improve the work balance (shown in Fig. 2(h)) at a small expense to the communication cost (shown in Fig. 2(i)).\nChoosing the optimal over-partitioning factor k is challenging and depends heavily on hardware, graph structure, and even factors. In situations where inference may be run repeatedly, standard search techniques may be used. We found that in practice when work balance is an issue small factors, e.g., k = 5 are typically sufficient. When using a recursive bisection partitioning algorithm where the true work split at each step is an unknown random variable, we can provide a theoretical bound on the ideal size of k. If at each split the work is divided into two parts of proportion X and 1 \u2212 X where E [X] = 12 and Var [X] = \u03c32 (\u03c3 \u2264 12 ]), Sanders [1994] shows that we can obtain work balance with high probability if we select\nk at least \u2126 ( p(log( 1 \u03c3+1/2 )) \u22121) ."}, {"heading": "3.2 DISTRIBUTING THE PRIORITY QUEUE", "text": "The ResidualSplash algorithm relies on a shared global priority queue. However, in the cluster computing setting, a centralized ordering is inefficient. Instead, in our approach, each processor constructs a local priority queue and iteratively applies the Splash operation to the top element in its local queue. On each round, the globally highest residual vertex will be at the top of one of the local queues. Unfortunately, the remaining p\u2212 1 highest vertices are not guaranteed to be at the top of the remaining queues and so we do not recover the original shared memory scheduling. However, any processor with vertices that have not yet converged, must eventually update those vertices and therefore can always make progress by updating the vertex at the top of its local queue. In Sec. 5.1 we show that the collection of local queues is sufficient to retain the original optimality properties of the ResidualSplash algorithm."}, {"heading": "3.3 DISTRIBUTED TERMINATION", "text": "In the distributed setting where there is no synchronized common state, it is difficult to identify the globally largest element and stop the algorithm when it falls below the termination bound. This is the well studied distributed termination problem [Matocha and Camp, Mattern, 1987]. We implement a variation of of the algorithm described in Misra [1983] by defining a token ring over all the nodes, in which a marker is passed in one direction around the ring. The marker is advanced once the node owning the marker converges, halting execution. A node may resume execution if it receives a message that causes its maximum residual to exceed the termination threshold. Global termination\nis achieved when the token completes two cycles in which all nodes remain converged and the number of messages received equals the number of messages sent."}, {"heading": "4 IMPROVED SCHEDULING", "text": "The fixed volume Splash operation and message based residual scheduling used in the ResidualSplash algorithm present several key challenges when scaling the algorithm to large factor graphs. In particular, both assume all vertices require the same amount of work to update. However, complex factors and variables that are involved in many factors often take much longer to update. Meanwhile, the message residual scheduling assumes that a significant change in one inbound message implies a significant change in the belief and outbound messages. Conversely, using message residuals as the convergence condition assumes that a small change in all inbound messages will induce only a small change in the belief and outbound messages. When the factor graph is large with high degree vertices this can result in an imbalanced convergence and an affinity for updating high degree vertices with little improvement in accuracy."}, {"heading": "4.1 BALANCED SPLASH", "text": "When scheduling Splash operations the residual heuristic assigns a \u201cvalue\u201d to each vertex update which ignores the cost of computing the Splash. When the graph structure is regular, the size of each Splash and resulting costs are likely to be similar, enabling the residual scheduling to focus on minimizing the residual. However, when the cost of computing a Splash is vastly different, as is the case in large irregular graphs, the residual heuristic will fail to account for the cost. Consequently, the residual heuristic will skip relatively high residual vertices with low cost in favor of the highest residual vertex with much greater cost.\nFurthermore, high degree, costly vertices are likely to be included in many BFS traversals and therefore updated disproportionately more often than other less heavily connected vertices. This problem is further frustrated in the cluster setting, by high degree vertices which are connected to vertices on many other processors, increasing network traffic substantially. We can resolve the imbalance in work by limiting the Splash size by the amount of work Wmax (as defined in Sec. 3.1) rather than the number of vertices. Consequently, the scheduling heuristic can safely ignore the cost of each Splash."}, {"heading": "4.2 NONUNIFORM CONVERGENCE", "text": "Using message residuals as the termination criterion leads to nonuniform convergence in beliefs. Small change to individual messages can combine at high degree vertices resulting in large changes in beliefs and asymmetric convergence. We demonstrate this behavior by considering a variable Xi with d = |\u0393i| incoming messages {m1, . . . ,md}. Suppose all the incoming messages are\nchanged to {m\u20321, . . . ,m\u2032d} such that the resulting residual is less than \u03b2 (i.e., \u2200k |m\u2032k\u2212mk|1 \u2264 \u03b2). Using the convergence criterion in Eq. (2.4) the messages have converged. However, the effective change in belief depends linearly on the degree, and therefore can be far from convergence.\nAssume {m1, . . . ,md} are binary uniform messages. Then the belief at that variable is also uniform (i.e., bi = [ 12 , 1 2 ]). If we then perturb the messages m\u2032k(0) = 1 2 \u2212 and m\u2032k(1) = 1 2 + by some small \u2264 \u03b2/2 the new belief is:\nb\u2032i(0) =\n( 1 2 \u2212 )d( 1 2 +\n)d + ( 12 \u2212 )d . The L1 belief residual due to the compounded change in each message is then:\n|b\u2032i(0)\u2212 bi(0)|1 = 1 2 \u2212\n( 1 2 \u2212 )d( 1 2 +\n)d + ( 12 \u2212 )d . A 2nd order Taylor expansion around = 0 obtains:\n|b\u2032i(0)\u2212 bi(0)|1 \u2248 d +O( 3).\nTherefore, the change in belief varies linearly in the degree of the vertex enabling small message residuals to translate into large d belief residuals."}, {"heading": "4.3 BELIEF RESIDUALS", "text": "The aim of BP is to estimate the marginal for each variable. However, ResidualSplash defines the scheduling and convergence using the change in messages rather than beliefs. In Sec. 4.2, we showed that small message changes do not imply small belief changes. Here we define a belief residual which addresses the problems associated with the message-centric approach.\nA natural definition of the belief residuals analogous to the message residuals defined in Eq. (2.6) is\nrj = \u2223\u2223\u2223\u2223bnewi \u2212 boldi \u2223\u2223\u2223\u22231 (4.1)\nwhere boldi is the belief at vertex i the last time vertex i was updated. Unfortunately, Eq. (4.1) has a surprising flaw that admits premature convergence on acyclic graphs with \u03b2 = 0 under a specially constructed scheduling. We will demonstrate this failure scenario and present a natural solution which is also computationally desirable.\nWithout loss of generality we consider a chain MRF of 5 vertices with the binary factors \u03c8Xi,Xi+1(xi, xi+1) = I[xi = xi+1] and unary factors:\n\u03c8X1 = [ 1 9 , 9 ] \u03c8X2 = [ 9 10 , 1 10 ] \u03c8X3 = [ 1 2 , 1 2 ] \u03c8X4 = [ 1 10 , 9 10 ] \u03c8X5 = [ 9, 19 ]\nWe begin by initalizing all vertex residuals to infinity, and all messages to uniform distributions. Then we perform the following update sequence marked in black:\nX1 X3 X5\nX1 X2 X4 X5\nX2 X4X3\nX1 X3 X5\nX2 X4X3\na)\nb)\nc)\nd)\ne)\nX2 X4\nX4X2\nX1\nX1\nX3\nX5\nX5\nAfter stage (b), X3 will have uniform belief and zero residual and m\n2\u21923 = \u03c8X2 and m 4\u21923 = \u03c8X4 . After stage (d),\nm 2\u21923 and m 4\u21923 will have swapped values. Therefore X3 will continue to have uniform belief and zero residual. At this point X2 and X4 also have zero residual since they were just updated. Stage (e) clears the residuals on X1 and X5. The residuals on X2 and X4 remain zero since messages m\n1\u21922 and m 5\u21924 haven\u2019t changed since state (c). By Eq. (4.1) with \u03b2 = 0 we have converged prematurely since no sequence of messages connects X1 and X5. The use of the naive belief residual in Eq. (4.1) will therefore converge to an erroneous solution.\nAn alternative formulation of the belief residual which does not suffer from premature convergence is given by:\nr (t) j \u2190 r (t\u22121) j + \u2223\u2223\u2223\u2223\u2223\u2223b(t)i \u2212 b(t\u22121)i \u2223\u2223\u2223\u2223\u2223\u2223 1\n(4.2)\nb (t) i (xi) \u221d\nb (t\u22121) i (xi) m\ni\u2192j (t)(xi)\nm i\u2192j\n(t\u22121)(xi) . (4.3)\nb (t\u22121) i is the belief after incorporating the last message and b (t) i is the belief after incorporating the new message. As each new message arrives, the belief can be efficiently recomputed using Eq. (4.3). Because Eq. (4.2) accumulates the change in belief with each new message, it will not lead to premature termination. Intuitively, it measures the cumulative effect of all message updates on the belief. Additionally, since Eq. (4.2) satisfies the triangle inequality, it is an upper bound on the total change in belief. This residual definition also has the advantage of not requiring previous versions of messages or beliefs to be stored."}, {"heading": "5 THE DBRSPLASH ALGORITHM", "text": "We now present our Distributed Belief Residual Splash algorithm (DBRSplash shown in Alg. 1) which combines the ideas presented in earlier sections. The execution can be divided into two phases, setup and inference.\nIn the setup phase, in Line 1 we over-segment the input factor graph into kp pieces using the METIS algorithms. Note that this could be accomplished in parallel using ParMETIS, however our implementation uses the sequential version for simplicity. Then in Line 2 we randomly assign k pieces to each of the p processors. In parallel each processor collects its factors and variables (Line 3). On\nAlgorithm 1: The DBRSplash Algorithm Btemp \u2190 OverSegment(G, p, k);1 B \u2190 RandomAssign(Btemp, p);2 forall Processors b \u2208 B do in parallel Collect(Fb,Xb);3 Initialize (Q);4 while TokenRing(Q, \u03b2) do5 v \u2190 Pop(Q) ; FixedWorkSplash(v, Wmax, \u03b2);6 RecvExternalMsgs();7 foreach u \u2208 Local changed vertices do8 Promote(Q, ||\u2206bv||1);9 SendExternalMsgs();10 Push(Q, v, 0);\nLine 4 the priorities of each variable and factor are set to infinity to ensure that every vertex is updated at least once.\nOn Line 5 we evaluate the top residual with respect to the \u03b2 convergence criterion and check for termination in the token ring. On Line 6, a splash of total work Wmax is applied to v. The fixed work Splash uses \u03b2 to prune subtrees that have sufficiently low belief residual. After completing the Splash all external messages from other processors are incorporated (Line 7). Any beliefs that changed during the Splash or after receiving external messages are promoted in the priority queue on Line 9. On Line 10, the external messages are transmitted across the network. Empirically, we find that accumulating external messages and transmitting only once every 10 loops reduces network overhead substantially and does not adversely affect convergence. The process repeats until termination at which point all beliefs are sent to the originating processor."}, {"heading": "5.1 PRESERVING SPLASH CHAIN OPTIMALITY", "text": "In Gonzalez et al. [2009] we introduced the \u03c4 notation as a theoretical measure for the effective distance \u03c4 at which vertices are assumed to be almost independent. More formally, for all vertices \u03c4 is the minimum radius for which running belief propagation on the subgraph centered at that vertex yields beliefs at most away from beliefs obtained using the entire graph. By increasing the value of , we decrease the sequential dependency structure, and increase the opportunity for parallelism. We showed that the ResidualSplash algorithm, when applied to chain graphs under the \u03c4 approximate inference setting, achieves the \u2126 (|V | /p+ \u03c4 ) optimal lower bound. We now show that DBRSplash retains the optimality in the distributed setting.\nTheorem 5.1 (Splash Chain Optimality). Given a chain graph with n = |V | vertices and p \u2264 n processes, the distributed DBRSplash algorithm with no oversegmentation, using a graph partitioning algorithm which\nreturns connected partitions, and with work Splash size at least 2 \u2211 v\u2208V wv/p will obtain a \u03c4 -approximation in ex-\npected running time O ( |V | p + \u03c4 ) .\nProof of Theorem 5.1. The proof is essentially identical to the method used in Gonzalez et al. [2009]. We assume that the chain graph is optimally sliced into p connected pieces of |V | /p vertices each. Since every vertex has at most 2 neighbors, the partion has at most 2 |V | /p work. A Splash anywhere within each partition will therefore cover the entire partition, performing the complete \u201cforwardbackward\u201d scheduling.\nBecause we send and receive all external messages after every splash, after d \u03c4 |V |/pe iterations, every vertex will have received messages from vertices a distance of at least \u03c4 away. The runtime will therefore be:\n2 |V | p \u00d7 \u2308 \u03c4 |V | /p \u2309 \u2264 2 |V | p + 2\u03c4\nSince each processor only send 2 external messages per iteration (one from each end of the partition), communication therefore only adds a constant to the total runtime."}, {"heading": "6 EXPERIMENTS", "text": "We implemented an MPI based version of DBRSplash in C++ using MPICH2. The splash size, over-partitioning factor, and scheduling method (i.e., message based and belief based) were parametrized for comparison. We invoked the weighted kmetis partitioning routine from the METIS software library for graph partitioning. All partitions were computed in under 10 seconds. To ensure numerical stability and convergence, log-space message calculations and 0.6 damping were used. The convergence bound was set to \u03b2 = 10\u22125. Cluster experiments were compiled using GCC 4.2.4 and tested on a cluster of 15 64Bit Linux Blades with dual Quad-Core Intel Xeon 2.33GHZ (E5345) processors connected with Gigabit Ethernet.\nWe assessed the performance of DBRSplash on Markov Logic Networks (MLNs) [Domingos et al., 2008], a probabilistic extension to first-order logic obtained by attaching weights to logical clauses. We used Alchemy to compile several MLNs into factor graphs. We constructed MLNs from the UW-CSE relational data-set [Domingos, 2009] and present results for the smallest uw-languages MLN with 1078 variables and 26598 factors and the largest uw-systems MLN with 7951 variables and 406389 factors. These MLNs have varied degree distributions as seen in Fig. 3. The large uw-systems model illustrates the scaling potential while uw-languages illustrates the limitations of our algorithm on small models."}, {"heading": "6.1 PARALLEL PERFORMANCE", "text": "The running time and speedup of DBRSplash were assessed on the uw-systems and uw-languages MLNs using\nvarious over-partitioning factors. In Fig. 4(a) and Fig. 4(b), DBRSplash achieves linear to super-linear running times and speedups up to 120 processors on the larger uw-systems MLN. The super-linear speedup may be attributed to increasing cache efficiency and memory bandwidth. Increasing the over-partitioning factor initially improves performance but performance gains are gradually attenuated by increased communication costs as more processors are used. Meanwhile, the much smaller uw-languages MLN only demonstrates linear to super-linear performance gains up to 20 processors (Fig. 5(a) and Fig. 5(b)). With a total running time under 10 seconds, there is insufficient work to efficiently use more than 20 processors."}, {"heading": "6.2 OVER-PARTITIONING", "text": "To directly assess the impact of over-partitioning on work balance and network traffic we used the denoising task (introduced in Sec. 3.1.1) on a 500\u00d7500 image with 5 colors. Using 60 processors we tested several over-partition factors and plotted both instantaneous CPU usage (Fig. 6(a)) and the cumulative network traffic (Fig. 6(b)). Without over-partitioning, the computation is unbalanced resulting in a gradual decrease in the number of active processors. Increasing the over-partitioning factor decreases the running time and ensures that all processor remain active up to convergence. However, as suggested, over-partitioning increases network activity.\nWe conduct a similar analysis on both MLNs. The cpu us-\nage Fig. 4(d) for the uw-systems MLN is consistent with results from the denoising task. Surprisingly, the network activity Fig. 4(e) for the uw-systems MLN after an initial increase shows a minor decrease with increased overpartitioning which we attribute to variability in partitioning and reduced running time. The cpu usage (Fig. 5(d)) for the smaller uw-languages MLN shows an increase in balance with increasing partitioning factor, but performance decreases going from over-partitioning factor of 5 to 10. This is due to the increased network activity (Fig. 5(e)) dominating the already short computation time."}, {"heading": "6.3 ACCURACY ASSESSMENT", "text": "To assess the accuracy of DBRSplash belief estimates we compare with belief estimates obtained through Gibbs sampling. We generated chains of 125 thousand samples, dropped the first 25 thousand samples (burn-in), and then used remaining samples to estimate the true beliefs. We compared belief estimates by computing the L1 difference averaged over all variables in the model. We found that repeated chains starting at different random states converged to beliefs that differed by less than 0.05 in average L1 per variable.\nIn Fig. 4(c), we plot the accuracy of DBRSplash with 60 processors on uw-systems as function of the vertex updates. In Fig. 5(c), we do the same for uw-languages but using a single processor since the running time is too short. In both cases DBRSplash quickly achieves high accuracy. In Fig. 4(f), we plot the accuracy of DBRSplash on uwsystems as a function of the number of processors for a fixed running time of one minute. We see that using 20 processors substantially improves the running time but the return quickly diminishes. In Fig. 5(f) we do the same for uw-languages, but for a running time of one second since it converges rapidly. In this case, going beyond 20 processors decreases the accuracy, due to increased running time."}, {"heading": "6.4 IMPROVED SCHEDULING", "text": "It is difficult to directly compare the convergence time of belief based scheduling and message based scheduling because they do not share the same convergence criterion. To provide a common basis for comparison, we assess the accuracy of the beliefs as discussed in Sec. 6.3 (Fig. 4(c), Fig. 5(c), Fig. 4(f) and Fig. 5(f)). These plots compare the accuracy of DBRSplash using Belief Residuals and Message Residuals. For uw-systems, the belief residuals achieve more rapid convergence to an accurate solution. The uw-languages MLN presents nearly identical accuracy convergence using both scheduling methods.\nAdditionally, we have found that for several MLNs, message based scheduling failed to converge while belief based scheduling converges consistently. One such MLN, cora-1, is characterized by extremely high degree variables (e.g., 59 variables with degree greater than 100 and 3 variables\nwith degree greater than 1000). To understand the behavior of our algorithm on cora-1, we plot the cumulative number of edge updates against the number of vertex updates in Fig. 6.4. We factor out the effect of different parts of DBRSplash. We observe that while the work balanced Splash (Sec. 4.1) slightly decreases the cumulative edge updates, the belief scheduling has a more substantially effect."}, {"heading": "7 CONCLUSIONS", "text": "We investigated the challenges involved in efficient distributed Belief Propagation on large factor graphs. We identified two primary challenges: efficient state partitioning and scheduling in complex irregular graphs.\nTo address the problem of state partitioning, we reduced the allocation of factors and messages and computation to graph cuts with edge and vertex weights. While estimating the weights exactly requires knowing the update scheduling, we showed that uninformed cuts perform reasonably well in practice. Because uniformed cuts tend to have lower communication costs and greater imbalance than informed cuts, we proposed over-partitioning to improve balance at the expense of increased communication costs. We found that over-partitioning can reduce the overall running time as long as the communication costs do not dominate.\nTo support the distributed memory setting and to improve performance on complex irregular graphs we proposed a new scheduling that addresses the limitations in ResidualSplash scheduling while retaining the parallel optimality property. Using a distributed collection of queues we decouple the scheduling across processors. By using fixed work sized Splash operations we ensure that high degree vertices are not updated disproportionately often. By switching from message based scheduling to belief based scheduling, we ensure more uniform convergence in the belief estimates. Experimentally, these changes resulted in improved performance, enabling rapid, accurate convergence on graphs which otherwise were intractable using previous belief propagation based techniques.\nWe tested our new algorithm, DBRSplash, on a cluster of 120 processors and found linear to super-linear performance gains on large factor graphs. For small factor graphs\nwhich run in minutes on a single processor, we obtained linear speedups only when using up to 20 processors. In conclusion we proposed an efficient parallel distributed algorithm, DBRSplash, which performs optimally on large factor graphs and demonstrates the potential capability of efficient parallel algorithms for the future of AI."}, {"heading": "Acknowledgements", "text": "This work is supported by ONR Young Investigator Program grant N00014-08-1-0752, the ARO under MURI W911NF0810242, DARPA IPTO FA8750-09-1-0141, the NSF under grants NeTS-NOSS and CNS-0625518 and Joseph Gonzalez is supported by the AT&T Labs Fellowship. We thank Intel Research for cluster time."}], "references": [{"title": "Distributed inference for latent dirichlet allocation", "author": ["D. Newman", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Newman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2007}, {"title": "Asynchronous distributed learning of topic models", "author": ["A. Asuncion", "P. Smyth", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Asuncion et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2008}, {"title": "A robust architecture for distributed inference in sensor networks", "author": ["M. Paskin", "C. Guestrin", "J. McFadden"], "venue": "In IPSN,", "citeRegEx": "Paskin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Paskin et al\\.", "year": 2005}, {"title": "Distributed inference in dynamical systems", "author": ["S. Funiak", "C. Guestrin", "M. Paskin", "R. Sukthankar"], "venue": "In NIPS,", "citeRegEx": "Funiak et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Funiak et al\\.", "year": 2006}, {"title": "Residual splash for optimally parallelizing belief propagation", "author": ["J. Gonzalez", "Y. Low", "C. Guestrin"], "venue": "In AISTATS,", "citeRegEx": "Gonzalez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gonzalez et al\\.", "year": 2009}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1988\\E", "shortCiteRegEx": "Pearl.", "year": 1988}, {"title": "Turbo decoding as an instance of Pearl\u2019s belief propagation algorithm", "author": ["R.J. McEliece", "D.J.C. MacKay", "J.F. Cheng"], "venue": "J-SAC,", "citeRegEx": "McEliece et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McEliece et al\\.", "year": 1998}, {"title": "Stereo matching using belief propagation", "author": ["J. Sun", "N.N. Zheng", "H.Y. Shum"], "venue": "ITPAM,", "citeRegEx": "Sun et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2003}, {"title": "Understanding belief propagation and its generalizations", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "In Exploring artificial intelligence in the new millennium,", "citeRegEx": "Yedidia et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2003}, {"title": "Approximate inference and protein folding", "author": ["C. Yanover", "Y. Weiss"], "venue": "In NIPS,", "citeRegEx": "Yanover and Weiss.,? \\Q2002\\E", "shortCiteRegEx": "Yanover and Weiss.", "year": 2002}, {"title": "Residual belief propagation: Informed scheduling for asynchronous message passing", "author": ["G. Elidan", "I. Mcgraw", "D. Koller"], "venue": "In UAI,", "citeRegEx": "Elidan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Elidan et al\\.", "year": 2006}, {"title": "Multilevel k-way partitioning scheme for irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "J. Parallel Distrib. Comput.,", "citeRegEx": "Karypis and Kumar.,? \\Q1998\\E", "shortCiteRegEx": "Karypis and Kumar.", "year": 1998}, {"title": "Randomized static load balancing for tree-shaped computations", "author": ["P. Sanders"], "venue": "In Workshop on Parallel Processing,", "citeRegEx": "Sanders.,? \\Q1994\\E", "shortCiteRegEx": "Sanders.", "year": 1994}, {"title": "Algorithms for distributed termination detection", "author": ["F. Mattern"], "venue": "Distributed Computing,", "citeRegEx": "Mattern.,? \\Q1987\\E", "shortCiteRegEx": "Mattern.", "year": 1987}, {"title": "Detecting termination of distributed computations using markers", "author": ["J. Misra"], "venue": "In SIGOPS,", "citeRegEx": "Misra.,? \\Q1983\\E", "shortCiteRegEx": "Misra.", "year": 1983}, {"title": "Markov logic: A unifying language for structural and statistical pattern recognition", "author": ["P. Domingos", "S. Kok", "D. Lowd", "H.F. Poon", "M. Richardson", "P. Singla", "M. Sumner", "J. Wang"], "venue": "SSPR,", "citeRegEx": "Domingos et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Domingos et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Work by Newman et al. [2007] and Asuncion et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "Work by Newman et al. [2007] and Asuncion et al. [2008] in parallel inference for latent topic models and by Paskin et al.", "startOffset": 8, "endOffset": 56}, {"referenceID": 0, "context": "Work by Newman et al. [2007] and Asuncion et al. [2008] in parallel inference for latent topic models and by Paskin et al. [2005] and Funiak et al.", "startOffset": 8, "endOffset": 130}, {"referenceID": 0, "context": "Work by Newman et al. [2007] and Asuncion et al. [2008] in parallel inference for latent topic models and by Paskin et al. [2005] and Funiak et al. [2006] in distributed inference for sensor networks adopt a message based asynchronous computation model to address the important task of distributed graphical model inference.", "startOffset": 8, "endOffset": 155}, {"referenceID": 0, "context": "Work by Newman et al. [2007] and Asuncion et al. [2008] in parallel inference for latent topic models and by Paskin et al. [2005] and Funiak et al. [2006] in distributed inference for sensor networks adopt a message based asynchronous computation model to address the important task of distributed graphical model inference. However their approaches are specialized to particular models or settings. Alternatively, in Gonzalez et al. [2009] we explored", "startOffset": 8, "endOffset": 441}, {"referenceID": 4, "context": "\u201d This differs from the shared memory setting of Gonzalez et al. [2009], where every processor has direct access to all available memory.", "startOffset": 49, "endOffset": 72}, {"referenceID": 5, "context": "Belief Propagation (BP), or the Sum-Product algorithm, is a commonly used approximate inference algorithm originally proposed by Pearl [1988]. In BP, \u201cmessages\u201d (parameters), are iteratively computed along edges in the factor graph until convergence and then used to estimate marginals.", "startOffset": 129, "endOffset": 142}, {"referenceID": 4, "context": "However, in Gonzalez et al. [2009] we showed that efficient parallel inference in the shared memory setting is limited by the sequential dependencies among messages.", "startOffset": 12, "endOffset": 35}, {"referenceID": 11, "context": "Here we use the collection of multilevel graph partitioning algorithms in the METIS [Karypis and Kumar, 1998] graph partitioning library.", "startOffset": 84, "endOffset": 109}, {"referenceID": 12, "context": "If at each split the work is divided into two parts of proportion X and 1 \u2212 X where E [X] = 12 and Var [X] = \u03c3 (\u03c3 \u2264 1 2 ]), Sanders [1994] shows that we can obtain work balance with high probability if we select k at least \u03a9 ( p(log( 1 \u03c3+1/2 )) \u22121) .", "startOffset": 124, "endOffset": 139}, {"referenceID": 13, "context": "This is the well studied distributed termination problem [Matocha and Camp, Mattern, 1987]. We implement a variation of of the algorithm described in Misra [1983] by defining a token ring over all the nodes, in which a marker is passed in one direction around the ring.", "startOffset": 76, "endOffset": 163}, {"referenceID": 4, "context": "In Gonzalez et al. [2009] we introduced the \u03c4 notation as a theoretical measure for the effective distance \u03c4 at which vertices are assumed to be almost independent.", "startOffset": 3, "endOffset": 26}, {"referenceID": 4, "context": "The proof is essentially identical to the method used in Gonzalez et al. [2009]. We assume that the chain graph is optimally sliced into p connected pieces of |V | /p vertices each.", "startOffset": 57, "endOffset": 80}, {"referenceID": 15, "context": "We assessed the performance of DBRSplash on Markov Logic Networks (MLNs) [Domingos et al., 2008], a probabilistic extension to first-order logic obtained by attaching weights to logical clauses.", "startOffset": 73, "endOffset": 96}], "year": 2009, "abstractText": "As computer clusters become more common and the size of the problems encountered in the field of AI grows, there is an increasing demand for efficient parallel inference algorithms. We consider the problem of parallel inference on large factor graphs in the distributed memory setting of computer clusters. We develop a new efficient parallel inference algorithm, DBRSplash, which incorporates over-segmented graph partitioning, belief residual scheduling, and uniform work Splash operations. We empirically evaluate the DBRSplash algorithm on a 120 processor cluster and demonstrate linear to super-linear performance gains on large factor graph models.", "creator": "TeX"}}}