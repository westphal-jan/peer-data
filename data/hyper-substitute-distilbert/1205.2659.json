{"id": "1205.2659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Deterministic POMDPs Revisited", "abstract": "we study a subclass include equations, presumably quantitative pomdps, whereby is characterized by similar actions and variables. these structures do barely provide inherently correct model regarding results yet they capture a number of relevant and crucial items, and permit potentially reliable algorithms. indeed, some of the estimation performance in telecommunication is built beside such assumptions explained by the quest of amenable predictions harder expressive than the classical deterministic models. theorists derive results comparing the exact properties by deterministic pomdps, their relation equations and / - search problems and predictions, consequently their computational strategies.", "histories": [["v1", "Wed, 9 May 2012 14:50:18 GMT  (212kb)", "http://arxiv.org/abs/1205.2659v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["blai bonet"], "accepted": false, "id": "1205.2659"}, "pdf": {"name": "1205.2659.pdf", "metadata": {"source": "CRF", "title": "Deterministic POMDPs Revisited", "authors": ["Blai Bonet"], "emails": ["bonet@ldc.usb.ve"], "sections": [{"heading": null, "text": "We study a subclass of POMDPs, called Deterministic POMDPs, that is characterized by deterministic actions and observations. These models do not provide the same generality of POMDPs yet they capture a number of interesting and challenging problems, and permit more efficient algorithms. Indeed, some of the recent work in planning is built around such assumptions mainly by the quest of amenable models more expressive than the classical deterministic models. We provide results about the fundamental properties of Deterministic POMDPs, their relation with AND/OR search problems and algorithms, and their computational complexity."}, {"heading": "1 Introduction", "text": "The simplest model for sequential decision making is the deterministic model with known initial and goal states. Solutions are sequences of actions that map the initial state into a goal state that can be computed with standard search algorithms. This model has been studied thoroughly in AI with important contributions such as A*, IDA*, and others [30, 34].\nThe deterministic model has strong limitations on the type of problems that can be represented: it is not possible to model situations where actions have nondeterministic outcomes or where states are not fully observable. In such cases, one must resort to more expressive formalisms such as Markov Decision Processes (mdps) and Partially Observable mdps (pomdps). The generality of these models comes with a cost since the computation of solutions increase in complexity, specially for pomdps, and thus one gains in generality but loses in the ability to solve problems. pomdps, for example, are widely used as they offer one of the most\ngeneral frameworks for sequential decision making [19], yet the known algorithms scale very poorly.\nHowever, we have seen that an important collection of problems that involve uncertainty and partial information have a common characteristic: they have actions with deterministic outcomes and the observations generated at each decision stage also behave deterministically. Indeed, these models have been used in recent proposals for planning with incomplete information [15, 16, 27], appear in works of more general scope [15, 20] and about causation [31], and are used for learning partially-observable action models [1].\nThese models were briefly considered in Littman\u2019s thesis [21] under the name of Deterministic POMDPs (det-pomdps) for which some important theoretical results were obtained. Among others, he showed that a det-pomdp can be mapped into an mdp with an exponential number of states and thus solved with standard MDP algorithms, and that optimal nonstationary policies of polynomial horizon can be computed in non-deterministic polynomial time. Unfortunately, det-pomdps briefly appeared as a curiosity of theoretical interest and then quickly fade out from consideration, to the point that, up to our knowledge, there are no publications on this subject neither from Littman or others.\nGiven the role of det-pomdps in recent investigations, motivated mainly by the quest of amenable models for decision making with uncertainty and partial information, we believe that det-pomdps should be further studied. In this paper, we carry out a systematic exploration of det-pomdps mainly from the complexity perspective yet we outline novel algorithms for them. We present three variants of the model: the fully observable, the unobservable and the general case, and two metrics of performance: worst- and expected-cost. As it will be shown, det-pomdps offer a tradeoff between the classical deterministic model and the general pomdp model. Furthermore, their characteristics permits the use of standard and novel AND/OR algo-\nrithms which are simpler and more efficient that the standard algorithms for pomdps [32, 36, 37], or the transformation proposed by Littman.\nThe paper is organized as follows. First, we give examples of challenging problems that help us to establish the relevance of det-pomdps. We then present the definition and variants of the model in Sect. 3, the relation with AND/OR graphs and algorithms in Sect. 4, complexity analyses in Sect. 5, and finish with a brief discussion in Sect. 6."}, {"heading": "2 Examples", "text": "Numerous det-pomdps problem had been used to evaluate and develop different algorithms for planning with uncertainty and partial information. For space reasons, we only provide few examples and brief descriptions for some of them.\nMastermind. There is a secret word of lengthm over an alphabet of n symbols. The goal is to discover the word by making guesses about it. Upon each guess, the number of exact matches and near matches is returned. The goal is to obtain a strategy to identify the secret.\nNavigation in Partially-Known Terrains. There is a robot in a n\u00d7 n grid that must navigate from an initial position to a goal position. Each cell of the grid is either traversable or untraversable. The robot has perfect sensing within its cell but the traversability of a subset of cells is unknown. The task is to obtain a strategy for guiding the robot to its destination [20].\nDiagnosis. There are n binary tests for finding out the state of a system among m possible states. An instance consists of a m\u00d7n binary matrix T such that Tij = 1 iff test j is positive when the state is i. The goal is to get a strategy for identifying the state [29].\nCoins. There are n coins from which one is a counterfeit of different weight, and there is a 2-pan balance scale. A strategy that spots the counterfeit and finds out whether is heavier or lighter is needed [30].\nDomains from IPC. The problems in the 2006 and 2008 International Planning Competition for the track on conformant planning consisted of domains covering topics such as Blocksworld, circuit synthesis, universal traversal sequences, sorting networks, communication protocols and others [11], all of which are instances of det-pomdps."}, {"heading": "3 Model and Variants", "text": "Formally, a det-pomdp model is a tuple made of:\n\u2013 a finite state space S = {1, . . . , n},\n\u2013 finite sets of applicable actions Ai \u2286 A for i \u2208 S,\n\u2013 a finite set of observations O,\n\u2013 an initial subset of states b0 \u2286 S, or alternatively an initial distribution of states b0 \u2208 \u2206S,\n\u2013 a subset T \u2286 S of goal (target) states,\n\u2013 a deterministic transition function f(i, a) \u2208 S, for i \u2208 S, a \u2208 Ai, that specifies the result of applying action a on state i,\n\u2013 a deterministic observation function o(i, a) \u2208 O, for i \u2208 S, a \u2208 A, that specifies the observation received when entering state i after the application of action a, and\n\u2013 positive costs c(i, a), for i \u2208 S, a \u2208 Ai that tells the immediate cost of applying a on i.\nFor simplicity, we assume that goal states are absorbing. That is, once a goal state is entered, the system remains there and incurs in no costs, hence At = A, f(t, a) = t and c(t, a) = 0 for t \u2208 T .\nThe two options for b0 depend on whether the interest is in minimizing the worst-case total accumulated cost, or the expected total accumulated cost (see below). In any case, b0 is called the initial belief state1 and describes the different possibilities for the initial state which is not known a priori. Its importance is crucial since, under the assumption of deterministic transitions, if the initial state were known, then all future states would also be known, and the model reduces to the well-known deterministic model in AI [34]. Hence, the only source of uncertainty in det-pomdp models comes from the initial state which further induces an uncertainty on the observations generated at each decision stage. Nonetheless, the model remains challenging, with respect to expressivity and computation, as exemplified in the previous section.\nAlthough the state of the system may not be fully observable, it is possible (and indeed useful) to consider preconditions for actions. The role of preconditions is to permit the knowledge engineer to design economical representations by leaving out from the specification unimportant details or undesirable situations. For example, if one does not want to model the effects of plugging a 120V artifact into a 240V outlet, then a simple precondition can be used to avoid such situations. Preconditions in the det-pomdp model are expressed through the sets of applicable actions Ai. As it is standard in planning, situations in which there are no actions available are called dead ends.\n1The term \u2018belief state\u2019 refers to a subset of states (or a probability distribution on states) that is deemed possible by the agent at a given point in time."}, {"heading": "3.1 Optimality Criteria", "text": "Our goal is to compute strategies that permit the agent to act optimally. We consider two optimality criteria, and three variants of the model. For optimality, we consider the minmax that minimizes the worst-case cost of a policy, and the minexp that minimizes the expected cost of a policy. The variants of the model depend on the observation model:\n\u2022 unobservable models in which O is a singleton and thus observations return no information about the state. This class is a subclass of the so-called conformant problems in planning [13].\n\u2022 fully-observable models in which o(i, a) = i. This class corresponds to fully observable mdps except that the initial state is unknown, but after the application of the first action the state is revealed.\n\u2022 models with no assumptions on the observations. This case corresponds to general det-pomdps and include the previous cases.\nTwo optimization criteria and three variants combine into six models: unobservable models with the minmax and minexp criteria, fully-observable models with the minmax and minexp criteria, and general models with minmax and minexp criteria. Among these, the unobservable and the general models are the interesting ones; fully-observable models are trivial and pathological and thus will not be considered again.\nFor partially-observable problems, the most general form of a policy is a function that maps belief states into actions. Beliefs are subsets of states for minmax models and probability distributions over states for minexp models. However, an optimal policy does not need to specify an action for all possible beliefs: it just need to consider the beliefs that could appear during the execution of the policy."}, {"heading": "3.2 Belief Dynamics and Closed Policies", "text": "In order to execute an action, the agent must be certain about its applicability no matter what is the current state of the environment. Otherwise, the model might end up in an inconsistent configuration with respect to the specification. Hence, the set Ab of applicable actions at b is Ab\n.= \u2229{Ai : i \u2208 sup(b)} where sup(b), called the support of b, is the collection of states that are deemed possible at b, i.e. sup(b) .= b for subsets of states and sup(b) .= {i : b(i) > 0} for distributions on states. Once an applicable action is executed, each state in sup(b) changes into a new state making up a new belief ba\n.= {f(i, a) : i \u2208 sup(b)} for subsets, and ba(j) .= \u2211 f(i,a)=j b(i) for distributions.\nOn the real environment though, the current state transforms into a new state and an observation is generated, which is used to filter out states inconsistent with it; i.e. the filtered belief is boa\n.= {i \u2208 ba : o(i, a) = o} for subsets, and, for distributions, is\nboa(i) .= {\n0 if ba(i) = 0 or o(i, a) 6= o, ba(i)/ba(o) otherwise,\nwhere ba(o) is a normalization constant.\nFrom the point of view of the agent, which does not know the current state of the environment, the set of observations that may appear after the application of a at b is o(b, a) .= {o(i, a) : i \u2208 sup(ba)}. Under the minexp criterion, each observation has probability ba(o) of being received once action a is applied at b.\nLittman proposes an interesting representation for the reachable beliefs in the probabilistic case [21], in which a reachable belief b is represented by a table tb : S \u2192 S \u222a {\u22a5} as follows. For b0, tb0(i) is i or \u22a5 whether b0(i) > 0 or not, and for boa where b is a reachable belief, a \u2208 Ab and o \u2208 o(b, a), the table tboa is given by tboa(i) = \u22a5 if tb(i) = \u22a5, ba(i) = 0 or o(i, a) 6= o, and tboa(i) = f(i, a) otherwise. In words, the entry tb(i) tells what would be the current state had the initial state been equal to i. The relation between b and tb is\nb(i) \u221d \u2211\nj\u2208S b0(j)|{j : tb(j) = i}|.\nWe are interested in optimal policies that are closed with respect to the initial belief b0. Namely, that the policy is defined over all the beliefs that may appear during the execution of the policy. We say that a policy \u03c0 is closed in a subset B of beliefs if \u03c0 is defined on each b \u2208 B, and bo\u03c0(b) \u2208 B for each b \u2208 B and o \u2208 o(b, \u03c0(b)), and that \u03c0 is closed with respect to b0, or just closed, if b0 \u2208 Dom(\u03c0) and \u03c0 is closed in Dom(\u03c0). We will only consider closed policies. The set of reachable beliefs from b0 using (closed) policy \u03c0 is the minimum subset R .= R(b0, \u03c0) \u2286 Dom(\u03c0) such that b0 \u2208 R and \u03c0|R is closed with respect to b0.2 A policy \u03c0 is minimal if \u03c0 = \u03c0|R(b0,\u03c0)."}, {"heading": "3.3 Graphs, Costs and Optimal Policies", "text": "Let us reason about the trajectories generated by a policy \u03c0. At the beginning, the agent applies the action a0 = \u03c0(b0). Then, the agent incurs in certain immediate cost, which depends on the current state and a0, and receives an observation o0, which also depends on the current state and action, that is used by the agent to update its initial belief into b1 = (b0)o0a0 . At the second decision stage, the agent applies the action a1 = \u03c0(b1), incurs in a cost and receives a new\n2f |A means the restriction of f to Dom(f) \u2229 A.\nobservations o1 that is used to update the belief into b2 = (b1)o1a1 , and so on. This process continues until the agent is certain that a goal state has been reached, i.e. until the current belief b is a target belief characterized by sup(b) \u2286 T . It is important to remark that during the execution of a policy the agent cannot predict with certainty neither the costs nor the observations since s/he is not certain about the current state of the environment.\nAll possibles trajectories of \u03c0 seeded at b0 form a labeled directed (multi-)graph G\u03c0 = (V,E, `) where V = R(b0, \u03c0) and there is an edge (b, b\u2032) labeled with o iff b is non-target and b\u2032 = bo\u03c0(b) for some o \u2208 o(b, \u03c0(b)). In probabilistic models, edges (b, b\u2032) labeled with o have attached probabilities equal to b\u03c0(b)(o).\nDeterministic actions imply |sup(ba)| \u2264 |sup(b)| with strict inequality only if f(i, a) = f(j, a) for some i 6= j in sup(b). Therefore, if a is applicable in b and {b1, . . . , bm} is the collection of possible beliefs after observations, then the collection {sup(bi)}mi=1 is a partition of sup(ba) and thus \u2211 o\u2208o(b,a) |sup(boa)| \u2264 |sup(b)| for each a \u2208 Ab. Furthermore, if b <\u03c0 b\u2032 denotes the existence of a path b b\u2032 in G\u03c0 and b <d\u03c0 b \u2032 the existence of a deterministic path, i.e. one in which each node has outdegree of one, then Theorem 1. Let G\u03c0 be the graph for \u03c0. Then, (a) if b <\u03c0 b\u2032 then |sup(b)| \u2265 |sup(b\u2032)|, (b) if b <\u03c0 b\u2032 and |sup(b)| = |sup(b\u2032)| then b <d\u03c0 b\u2032, and (c) for minexp, if b <\u03c0 b\n\u2032 and sup(b) = sup(b\u2032) then b(i) = b\u2032(\u03c3i) where \u03c3 is a permutation of sup(b).\nAn immediate consequence is that for minexp models the set of reachable beliefs is finite independently of the initial belief. This fact was also observed by Littman who gave the upper bound (1+|S|)|S| on the maximum number of reachable beliefs as this is the maximum number of tables. This is a marked difference with respect to standard pomdps in which the number of reachable beliefs is typically infinite as well as the size of the policy graphs. Theorem 2. The set R(b0, A) of reachable beliefs is finite, the graph G\u03c0 is finite for each policy \u03c0, and the number of minimal policies is also finite.\nTo define the cost of a policy, we consider the cases of whether G\u03c0 is a DAG or not. In the latter case, there is a cycle in G\u03c0 along a deterministic path and thus, for both optimization criteria, \u03c0 incurs in infinite cost once it gets trapped into the cycle.3 Therefore, if G\u03c0 has a cycle, its cost at b0 is set to infinity; i.e. V max\u03c0 (b0) = V exp \u03c0 (b0)\n.=\u221e. 3In mdps, it is customary to use a factor to discount future costs at a geometric rate so that every policy has finite cost. We do not consider such factors as often they are difficult to interpret and justify.\nIf G\u03c0 is a DAG, the cost assigned by \u03c0 at each belief is defined inductively bottom-up from the sink nodes up to the source b0. Indeed, being \u03c0 closed, the sinks are target beliefs which has zero cost under both criteria, i.e. V max\u03c0 (b) = V exp \u03c0 (b)\n.= 0 if sup(b) \u2286 T . Once all successors of b get values, the value at b is defined as\nV max\u03c0 (b) .= cmax(b, \u03c0(b)) + max\n(b,b\u2032)\u2208G\u03c0 V max\u03c0 (b \u2032),\nV exp\u03c0 (b) .= cexp(b, \u03c0(b)) + \u2211 o\u2208o(b,\u03c0(b)) ba(o) \u00b7 V exp\u03c0 (boa),\nwhere the costs over beliefs are defined as cmax(b, a) .= maxi\u2208b c(i, a) and cexp(b, a) = \u2211 i\u2208S b(i) \u00b7 c(i, a). The cost assigned by policy \u03c0 to belief b0, either finite or infinite, is called the cost of \u03c0. Clearly, if G\u03c0 is a DAG, then V exp\u03c0 (b0) \u2264 V max\u03c0 (b0) <\u221e.\nA policy \u03c0 is preferred to policy \u03c0\u2032 under optimization criterion \u2206 if V \u2206\u03c0 (b0) < V \u2206 \u03c0\u2032 (b0). A policy \u03c0 is optimal under \u2206 if no other policy is preferred to it. Hence, by Theorem 2, if there is a policy of finite cost, then there is an optimal policy \u03c0\u2217 whose graph is a DAG. From now on, when we say an optimal policy, we mean an optimal policy of finite cost. If there is no policy of finite cost, we say there is no optimal policy."}, {"heading": "3.4 Policy Forms and Sizes", "text": "Unobservable Models. As there is no information on which to branch, plans for unobservable models are linear sequences of actions that take the initial belief into a goal belief. Hence, G\u03c0 is a single path of form\nb0 \u2192 b1 \u2192 b2 \u2192 \u00b7 \u00b7 \u00b7 \u2192 bn\u22121 \u2192 bn.\nBy Theorem 1, |sup(b0)| \u2265 \u00b7 \u00b7 \u00b7 \u2265 1. Let us say that a \u201cjump\u201d occurs at bi if |sup(bi)| > |sup(bi+1)|, and that \u3008bi, . . . , bi+m\u3009 is a \u201cchunk\u201d if it is a maximal subsequence with |sup(bi)| = \u00b7 \u00b7 \u00b7 = |sup(bi+m)|. There are at most |S| jumps in the sequence so we need to bound the size of a largest chunk. Theorem 1 implies that the actions in the chunk map states in a 1-1 way, i.e. that actions behave like permutations over states. As it will be shown later, the size of a chunk can be exponential in some problems.\nLet us introduce the notion of diameter of a model that allow us to bound the length of a largest chunk. For a set of actions A and belief b, denote with R(b, A) the set of beliefs reachable from b using only actions in A, and with R\u2217(b, A) the set of beliefs with supports of size |sup(b)| that are reachable from b using only actions in A. A belief in R\u2217(b, A) is said to be k-reachable if it can be reached from b through the application of at most k actions from A. The diameter of R\u2217(b, A) is the least integer k such that every b\u2032 \u2208 R\u2217(b, A) is k-reachable, and the diameter\nof a model M is the maximum over the diameters of R\u2217(b, A) for all b \u2208 R(b0, A). The model has polynomial diameter if its diameter is O(poly(|S|, |A|)).\nIf R(b0, A) is of polynomial size, the model is of polynomial diameter but the converse is not necessarily true. If M is of polynomial diameter, the lengths of the chunks are of polynomial length. Thus, unobservable problems of polynomial diameter have optimal policies of polynomial size.\nGeneral Models. Optimal policies are DAGs in which paths correspond to sequences of beliefs with non-increasing supports. As before, Theorem 3. Models of polynomial diameter have optimal policies of polynomial size.\nProof. It remains to show the claim for general models. Let \u03c0 be an optimal policy. A subset B of beliefs is called an anti-chain iff there are no two different beliefs b, b\u2032 \u2208 B with b <\u03c0 b\u2032. An induction shows that if B is an anti-chain, then |sup(b0)| \u2265 \u2211 b\u2208B |sup(b)|. Thus, since |sup(b0)| \u2264 |S|, all anti-chains have size at most |S|. On the other hand, the assumption implies that all paths in G\u03c0 have polynomial length. Therefore, G\u03c0 is of polynomial size since the beliefs at depth k form an anti-chain and thus their number is at most |S|."}, {"heading": "4 AND/OR Graphs", "text": "We establish a relation between policies and AND/OR graphs that can be exploited by algorithms. An AND/OR graph is a tuple G = (V1 \u222a V2, E, T, n0, c) where V1 and V2 are finite sets of AND and OR nodes, E \u2286 (V1\u222aV 2)\\T\u00d7(V1\u222aV2) is a subset of directed edges between nodes, T \u2286 V2 is subset of terminal nodes, n0 \u2208 V1 \u222a V2 is the root node, and c : E \u222a T \u2192 R\u2217 is a cost function over edges and terminal nodes [26]. A solution is a subgraph H spanned by a subset of edges H(E) such that (1) n0 \u2208 H, (2) if n \u2208 H \\ T is AND node then all its outbound edges are in H(E), and (3) if n \u2208 H \\T is OR node then one of its outbound edges is in H(E). A solution is valid iff it is acyclic. Costs c(H) can be associated to valid solutionsH inductively from the sinks up to the root by\nVH(n) .=  c(n) n \u2208 Tmax(n,n\u2032)\u2208H(E) c(n, n\u2032) + VH(n\u2032) n \u2208 V1 c(n, n\u2032) + VH(n\u2032) n \u2208 V2\nA stochastic graph associates probabilities p(n, n\u2032) to the edges (n, n\u2032) incident at AND nodes n so that\u2211\n(n,n\u2032)\u2208E p(n, n \u2032) = 1. The cost VH is similarly defined except that VH(n) .= \u2211 (n,n\u2032)\u2208H(E)(c(n, n \u2032) + VH(n\u2032))p(n, n\u2032) for AND nodes. In any case, the cost c(H) is defined as VH(n0), and a solution is optimal if its cost is no larger than the cost of any other solution."}, {"heading": "4.1 From Models to Graphs and Algorithms", "text": "The relation between det-pomdps and AND/OR graphs is direct. For a model M , we construct an AND/OR graph GM such that the solutions of GM coincide with the solutions of M . Indeed, given a model M with minmax criterion, the graph GM is given by\n\u2013 V1 .= {ba : b \u2208 R(b0, A), a \u2208 Ab}, \u2013 V2 .= R(b0, A), \u2013 T .= {b \u2208 V2 : sup(b) \u2286 T}, \u2013 if b \u2208 V2 \\ T then (b, ba) \u2208 E for a \u2208 Ab, \u2013 if ba \u2208 V1 then (ba, boa) \u2208 E for o \u2208 o(b, a), \u2013 n0\n.= b0, \u2013 c(b) .= 0 for b \u2208 T , \u2013 c(b, ba)\n.= cmax(b, a) for b \u2208 V2 and a \u2208 Ab, \u2013 c(ba, boa) .= 0 for ba \u2208 V1 and o \u2208 o(b, a).\nFor minexp, the graph must be extended into a stochastic graph with labels p(ba, boa) = ba(o), and c(b, ba) must be replaced by cexp(b, a). Also observe that the beliefs in the graph can be represented with probability distributions or with Littman\u2019s tables.\nThis relation permits the use of diverse algorithms. Firstly, since the number of reachable beliefs is finite, then all reachable beliefs can be enumerated and Value or Policy Iteration applied on the resulting mdp over belief space (this is essentially Littman\u2019s proposal); both algorithms are guaranteed to converge in a finite number of steps. However, we can do better since optimal policies are acyclic and thus consistently improving policies exist. Therefore, more efficient algorithms can be used such as label setting methods and Dial\u2019s algorithm [3, 7].\nExplicit algorithms as the above require enough memory to compute the set of reachable beliefs. If no such memory is available, search-based algorithms that generate beliefs incrementally and that use admissible heuristics to focus the search must be used. Relevant algorithms in this class are the classical AO* [6, 23, 26], LAO* [14], RTDP-like algorithms [2, 4], and LDFS [5]. AO* can only be used on acyclic graphs; LAO*, RTDP and LDFS do not have this restriction."}, {"heading": "5 Complexity", "text": "The complexity of pomdps had been thoroughly studied. Results for different optimization criteria, probabilistic and non-deterministic variants, and so on are known [17, 22, 25, 28, 33]. Littman [21] obtained important complexity results about detpomdps for problems with non-negative costs (i.e. with non-positive rewards):\n\u2022 deciding the existence of a policy that incurs in zero cost for an infinite-horizon det-pomdp is PSPACEcomplete, and\n\u2022 deciding the existence of a policy of polynomial length that incurs in zero cost for a det-pomdp is NP-complete\nIn this section, we extend this results as follows:\n\u2022 deciding the existence of a policy of finite cost for det-pomdps of polynomial diameter is NPcomplete,\n\u2022 there are det-pomdps on which all policies of finite cost are of super-polynomial size,\n\u2022 give new class of problems on which the existence of policies can be checked in non-deterministic polynomial time, and\n\u2022 give some sufficient conditions for testing whether a det-pomdp is of polynomial diameter.\nAll these results, as well as Littman\u2019s results, are based in the assumption that the models are encoded in a flat language; that is, that the models are encoded with O(poly(|S|, |A|, h)) bits where h is the maximum number of bits needed to specify a cost or probability. The general question that we address here is whether there is a policy that reaches the goal with certainty. Namely,\nPolicy Existence for Flat Models (PEF): Is there a policy of finite cost for flat model M?\nThe fact that pef only considers flat models is important because with compact representations, an exponential number of states can be described and thus a double-exponential number of beliefs could be needed.\nTheorem 4. The PEF problem for unobservable models of polynomial diameter is NP-complete.\nProof. Inclusion is direct by guessing a polynomiallysized policy and checking it. For hardness, we reduce sat using the method in [25] almost verbatim. Let \u03c6 be a 3-CNF formula with variables x1, . . . , xn and clauses C1, . . . , Cm. A variable x 1-appears (0- appears) in clause C if x \u2208 C (x \u2208 C). We construct an unobservable modelM(\u03c6) as follows. The set of states is S = {[xi, Cj ] : 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 m}\u222a {t, f}. States t and f mean satisfiable and unsatisfiable respectively. The actions set(i, v), for 1 \u2264 i \u2264 n and v \u2208 {0, 1}, are used to set the value of variables: set(i, v) maps state [xi, Cj ] ([xn, Cj ]) to either t or [xi+1, Cj ] (to either t or f) whether xi v-appears in Cj or not. The initial belief is b0 = {[x1, Cj ] : 1 \u2264 j \u2264 m} and t is the unique target state. M(\u03c6) is of polynomial size and diameter, and has a policy of finite cost iff \u03c6 is satisfiable.\nCorollary 5. The PEF problem for general models of polynomial diameter is NP-complete.\nProof. Hardness is direct since unobservable models are a special case. Inclusion follows by guessing a policy of polynomial size (Theorem 3) and checking it.\nSometimes, it is easy to verify that a model is of polynomial diameter. Indeed, if |sup(b0)| = k the number of reachable beliefs is O(nk). If k is a constant then the number of reachable beliefs is polynomial as well as the diameter. For another case, consider the (global) transition graph TM that is a directed graph whose nodes are the states and there is an edge (i, j) iff there is action a \u2208 Ai such that j = f(i, a). If TM is acyclic, except for self-loops, the model has linear diameter. Examples are knowledge-gathering problems such as Mastermind, 12-coins and Diagnosis. The IPC problem for sorting networks is also of this type.\nTheorem 6. If |sup(b0)| is constant, then the diameter is polynomial. If TM is acyclic, except perhaps for self-loops, then the diameter is linear.\nAs noted in Theorem 1, there is a close connection between the diameter of a model and permutations over states. Thus, the study of permutations provides important insight for estimating the diameter of models."}, {"heading": "5.1 Permutation Groups and Their Diameter", "text": "The set of permutations with composition forms a multiplicative group. The order of a permutation \u03c3 is the least n such that \u03c3n is the identity permutation. It is well known that a permutation can be written in cycle notation as a product of disjoint cycles, and that the order of \u03c3 = C1C2 . . . Cm (in cycle notation) is the least common multiple for the lengths of the cycles.\nFor example, \u03c3 = ( 1 2 3 4 5 6 7 8 3 4 5 7 8 6 2 1 ) can be written as \u03c3 = (1, 3, 5, 8)(2, 4, 7)(6) and its order is ord(\u03c3) = lcm{4, 3, 1} = 12.\nConsider now an unobservable model with a single action a with empty precondition that corresponds to a permutation \u03c3 = C1C2 \u00b7 \u00b7 \u00b7Cm over states. Furthermore, suppose that b0 has m states one from each cycle Ci. Then, the repeated application of a over b0 generates a trajectory \u3008b0, . . . , bk\u3009 of different beliefs if k < ord(\u03c3). Since observations filter nothing, |sup(b0)| = \u00b7 \u00b7 \u00b7 = |sup(bk)| and there is a chunk of length k+1. If k is large, the size of an optimal policy could be large as well. We use this idea to proof\nTheorem 7. There is an unobservable model for which all policies are of super-polynomial size.\nProof. Let S = {1, . . . , n}. First, we show that there is a permutation \u03c3 over S with super-polynomial order. If \u03c3 = C1 . . . Cm in cycle notation, then |C1| + \u00b7 \u00b7 \u00b7 + |Cm| = n and ord(\u03c3) = lcm{|C1|, . . . , |Cm|}, so we need to show that there are integers {d1, . . . , dm} whose sum is n and lcm is super-polynomial in n.\nThe Prime Number Theorem says that the number of primes less than N is asymptotically equal to N/ logN . Hence, the number of primes in [n1/2, n3/4] is asymptotically equal to 4n3/4/3 log n\u2212 2n1/2/ log n which dominates n1/4. Therefore, for sufficiently large n, we can choose bn1/4c different primes in [n1/2, n3/4]. Their sum is bounded by n3/4 \u00d7 n1/4 = n and thus these primes can be extended into a collection {d1, . . . , dm} of integers whose sum is equal to n. Since all primes are different and at least n1/2, their lcm is at least n 1 2n 1/4 which dominates nk for any constant k. Estimations on the expected order of random permutations are also known [9, 12, 38].\nFor the model, let a be an action that is like \u03c3 and let b0 be the subset containing the \u201cfirst\u201d state from each cycle Ci. Let a\u2032 be another action that maps s into a target if s is the \u201clast\u201d state in a cycle, or into s otherwise. The optimal plan is the sequence of ord(\u03c3)\u2212 1 repetitions of a followed by a\u2032.\nHowever, even in cases of non-polynomial diameter, we can test in non-deterministic polynomial-time the existence of policies for some models (proof below).\nTheorem 8. If each action is a permutation with empty precondition, then the PEF problem is in NP, even if the model is not of polynomial diameter.\nThe group Sn of all permutations over n elements is called the symmetric group of degree n. Let A \u2286 Sn be a subset. Then, A generates the subgroup \u3008A\u3009 of all permutations that can be formed by finite compositions of elements from A. A permutation \u03c3 \u2208 \u3008A\u3009 is k-expressible if it is the product of at most k permutations from A. The diameter of \u3008A\u3009 is the least integer k such that every permutation in \u3008A\u3009 is k-expressible. Problems related to generated groups have a tradition in computer science. For example, Furst, Hopcroft and Luks [10] gave a polynomial-time algorithm for deciding whether a permutation \u03c3 is generated by the set A, and Jerrum [18] showed that computing the size of a smallest generator set is PSPACE-complete.\nProof Sketch for Theorem 8. We do it for unobservable models; the proof for general models is a bit more involved. Since actions are permutations, the size of beliefs do not change with actions. If there is a solution b0 \u2192 \u00b7 \u00b7 \u00b7 \u2192 bn where bn is a target belief, then by Theorem 1, there is a permutation \u03c3 such that sup(b0) is\nmapped 1-1 into sup(bn). Thus, it is enough to guess \u03c3 and then test, in polynomial-time using the algorithm in [10], if \u03c3 is generated by the actions.\nMore important to us are results about diameters of groups. Driscoll and Furst [8] showed that if the generators are all cycles of bounded length, the diameter is O(n2), while McKenzie [24] showed that if all generators move at most k elements, the diameter is O(nk).\nTheorem 9. If every action has empty precondition, and is a permutation that moves a constant number of states or all cycles are of bounded length, then the model is of polynomial diameter."}, {"heading": "6 Discussion", "text": "We have shown that the Deterministic POMDP model first studied by Littman is more relevant than previously thought for two reasons. First, a number of important and challenging problems in planning correspond to such models, and second since they provide a tradeoff between the restricted yet efficient classical deterministic model and the general but inefficient pomdp model. We have shown novel complexity results that show this tradeoff: while checking the existence of plans in classical deterministic (flat) problems is NL-complete (via st-Reachability [35]), and checking the existence of plans for pomdps is EXPTIME-complete [21], we have that checking the existence of plans for det-pomdps of polynomial diameter is NP-complete, and that for det-pomdps in which the actions are permutations with empty precondition is in NP. Furthermore, we give polynomialtime checkable conditions for polynomial diameter.\nWe also proposed a relation between det-pomdps and AND/OR graphs that permit the use of general AND/OR search algorithms. Although we did not perform an experimental evaluation, it is clear that such algorithms should outperform in practice the explicit mapping of a det-pomdp into an mdp.\nIt is important to emphasize that det-pomdps are not \u201cgeneral\u201d pomdps, and hence one should be careful when evaluating general algorithms with them. Due to the reduced complexity of the model, det-pomdps must be tackled as a more specialized class which is likely to scale better.\nIn the future, we would like to further study the relation between det-pomdps and permutation groups and to implement algorithms based of AND/OR search.\nAcknowledgments. Thanks to the reviewers for valuable comments and pointers to related work."}], "references": [{"title": "Learning partially-observable deterministic action models", "author": ["E. Amir", "A. Chang"], "venue": "J. of Artificial Intelligence Research, 33:349\u2013402,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning to act using real-time dynamic programming", "author": ["A. Barto", "S. Bradtke", "S. Singh"], "venue": "Artificial Intelligence, 72:81\u2013138,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Dynamic Programming and Optimal Control, (2 Vols)", "author": ["D. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Labeled RTDP: Improving the convergence of real-time dynamic programming", "author": ["B. Bonet", "H. Geffner"], "venue": "ICAPS, pages 12\u201321,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "An algorithm better than AO*", "author": ["B. Bonet", "H. Geffner"], "venue": "In AAAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Algorithms for searching explicit AND/OR graphs and their applications to problem reduction search", "author": ["P.P. Chakrabarti"], "venue": "Artificial Intelligence, 65(2):329\u2013345,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "Algorithm 360: Shortest path forest with topological ordering", "author": ["R. Dial"], "venue": "Comm. ACM, 12:632\u2013633,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1969}, {"title": "On the diameter of permutation groups", "author": ["J.R. Driscoll", "M. Furst"], "venue": "STOC, pages 152\u2013160,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1983}, {"title": "On some problems of a statistical group theory, IV", "author": ["P. Erd\u00f6s", "P. Tur\u00e1n"], "venue": "Acta Math. Acad. Sci. Hungar., 19:413\u2013435,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1969}, {"title": "Polynomial-time algorithms for permutation groups", "author": ["M. Furst", "J. Hopcroft", "E. Luks"], "venue": "FOCS, pages 36\u201341,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1980}, {"title": "The expected order of a random permutation", "author": ["W. Goh", "E. Schmutz"], "venue": "Bull. Lond. Math. Soc., 23:34\u2013 42,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1991}, {"title": "Expressive planning and explicit knowledge", "author": ["R.P. Goldman", "M.S. Boddy"], "venue": "AIPS, pages 110\u2013117,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "LAO*: A heuristic search algorithm that finds solutions with loops", "author": ["E. Hansen", "S. Zilberstein"], "venue": "Artificial Intelligence, 129:35\u201362,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Contingent planning via heuristic forward search with implicit belief states", "author": ["J. Hoffmann", "R. Brafman"], "venue": "ICAPS, pages 71\u201380,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Conformant planning via heuristic forward search: A new approach", "author": ["J. Hoffmann", "R. Brafman"], "venue": "Artificial Intelligence, 170:507\u2013541,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "What makes some POMDP problems easy to approximate", "author": ["D. Hsu", "W.S. Lee", "N. Rong"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "The complexity of finding minimumlength generator sequences", "author": ["M. Jerrum"], "venue": "Theoretical Computer Science, 36:265\u2013289,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1985}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence, 101:99\u2013134,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Ppcp: Efficient probabilistic planning with clear preferences in partiallyknown environments", "author": ["M. Likhachev", "A. Stentz"], "venue": "AAAI, pages 860\u2013867,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Algorithms for Sequential Decision Making", "author": ["M. Littman"], "venue": "PhD thesis, Brown University,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "The computational complexity of probabilistic planning", "author": ["M. Littman", "J. Goldsmith", "M. Mundhenk"], "venue": "J. of Artificial Intelligence Research, 9:1\u201336,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Additive AND/OR graphs", "author": ["A. Martelli", "U. Montanari"], "venue": "IJCAI, pages 1\u201311,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1973}, {"title": "Permutations of bounded degree generate groups of polynomial diameter", "author": ["P. McKenzie"], "venue": "Information Processing Letters, 19(5):253\u2013254,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1984}, {"title": "Complexity of finite-horizon markov decision process problems", "author": ["M. Mundhenk", "J. Goldsmith", "C. Lusena", "E. Allender"], "venue": "J. ACM, 47(4):681\u2013720,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Principles of Artificial Intelligence", "author": ["N. Nilsson"], "venue": "Tioga,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1980}, {"title": "From conformant into classical planning: Efficient translations that may be efficient too", "author": ["H. Palacios", "H. Geffner"], "venue": "ICAPS,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "The complexity of markov decision processses", "author": ["C.H. Papadimitriou", "J.N. Tsitsiklis"], "venue": "Math. of Operations Research, 12(3):441\u2013450,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1987}, {"title": "Applications of heuristic search and information theory to sequential fault diagnosis", "author": ["K. Pattipati", "M. Alexandridis"], "venue": "IEEE Trans. System, Man and Cybernetics, 20:872\u2013887,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1990}, {"title": "Heuristics", "author": ["J. Pearl"], "venue": "Morgan Kaufmann,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1983}, {"title": "Causality: Models, Rreasoning and Inferece", "author": ["J. Pearl"], "venue": "Cambridge University Press, New York,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Anytime point-based approximations for large POMDPs", "author": ["J. Pineau", "G.J. Gordon", "S. Thrun"], "venue": "J. of Artificial Intelligence Research, 27:335\u2013380,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Complexity of planning with partial observability", "author": ["J. Rintanen"], "venue": "ICAPS, pages 345\u2013354,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S. Russell", "P. Norvig"], "venue": "Prentice Hall,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1994}, {"title": "Introduction to Theory of Computation, 2nd Edition", "author": ["M. Sipser"], "venue": "Thomson Course Technology, Boston, MA,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Heuristic search value iteration for POMDPs", "author": ["T. Smith", "R. Simmons"], "venue": "UAI, pages 520\u2013527,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Perseus: Randomized point-based value iteration for POMDPs", "author": ["M.T.J. Spaan", "N.A. Vlassis"], "venue": "J. of Artificial Intelligence Research, 24:195\u2013220,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "The average order of a permutation", "author": ["R. Stong"], "venue": "Electronic Journal of Combinatorics, 5:#R41,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 28, "context": "This model has been studied thoroughly in AI with important contributions such as A*, IDA*, and others [30, 34].", "startOffset": 103, "endOffset": 111}, {"referenceID": 32, "context": "This model has been studied thoroughly in AI with important contributions such as A*, IDA*, and others [30, 34].", "startOffset": 103, "endOffset": 111}, {"referenceID": 17, "context": "pomdps, for example, are widely used as they offer one of the most general frameworks for sequential decision making [19], yet the known algorithms scale very poorly.", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "Indeed, these models have been used in recent proposals for planning with incomplete information [15, 16, 27], appear in works of more general scope [15, 20] and about causation [31], and are used for learning partially-observable action models [1].", "startOffset": 97, "endOffset": 109}, {"referenceID": 14, "context": "Indeed, these models have been used in recent proposals for planning with incomplete information [15, 16, 27], appear in works of more general scope [15, 20] and about causation [31], and are used for learning partially-observable action models [1].", "startOffset": 97, "endOffset": 109}, {"referenceID": 25, "context": "Indeed, these models have been used in recent proposals for planning with incomplete information [15, 16, 27], appear in works of more general scope [15, 20] and about causation [31], and are used for learning partially-observable action models [1].", "startOffset": 97, "endOffset": 109}, {"referenceID": 13, "context": "Indeed, these models have been used in recent proposals for planning with incomplete information [15, 16, 27], appear in works of more general scope [15, 20] and about causation [31], and are used for learning partially-observable action models [1].", "startOffset": 149, "endOffset": 157}, {"referenceID": 18, "context": "Indeed, these models have been used in recent proposals for planning with incomplete information [15, 16, 27], appear in works of more general scope [15, 20] and about causation [31], and are used for learning partially-observable action models [1].", "startOffset": 149, "endOffset": 157}, {"referenceID": 29, "context": "Indeed, these models have been used in recent proposals for planning with incomplete information [15, 16, 27], appear in works of more general scope [15, 20] and about causation [31], and are used for learning partially-observable action models [1].", "startOffset": 178, "endOffset": 182}, {"referenceID": 0, "context": "Indeed, these models have been used in recent proposals for planning with incomplete information [15, 16, 27], appear in works of more general scope [15, 20] and about causation [31], and are used for learning partially-observable action models [1].", "startOffset": 245, "endOffset": 248}, {"referenceID": 19, "context": "These models were briefly considered in Littman\u2019s thesis [21] under the name of Deterministic POMDPs (det-pomdps) for which some important theoretical results were obtained.", "startOffset": 57, "endOffset": 61}, {"referenceID": 30, "context": "rithms which are simpler and more efficient that the standard algorithms for pomdps [32, 36, 37], or the transformation proposed by Littman.", "startOffset": 84, "endOffset": 96}, {"referenceID": 34, "context": "rithms which are simpler and more efficient that the standard algorithms for pomdps [32, 36, 37], or the transformation proposed by Littman.", "startOffset": 84, "endOffset": 96}, {"referenceID": 35, "context": "rithms which are simpler and more efficient that the standard algorithms for pomdps [32, 36, 37], or the transformation proposed by Littman.", "startOffset": 84, "endOffset": 96}, {"referenceID": 18, "context": "The task is to obtain a strategy for guiding the robot to its destination [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 27, "context": "The goal is to get a strategy for identifying the state [29].", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "A strategy that spots the counterfeit and finds out whether is heavier or lighter is needed [30].", "startOffset": 92, "endOffset": 96}, {"referenceID": 32, "context": "Its importance is crucial since, under the assumption of deterministic transitions, if the initial state were known, then all future states would also be known, and the model reduces to the well-known deterministic model in AI [34].", "startOffset": 227, "endOffset": 231}, {"referenceID": 11, "context": "This class is a subclass of the so-called conformant problems in planning [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "Littman proposes an interesting representation for the reachable beliefs in the probabilistic case [21], in which a reachable belief b is represented by a table tb : S \u2192 S \u222a {\u22a5} as follows.", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "An AND/OR graph is a tuple G = (V1 \u222a V2, E, T, n0, c) where V1 and V2 are finite sets of AND and OR nodes, E \u2286 (V1\u222aV 2)\\T\u00d7(V1\u222aV2) is a subset of directed edges between nodes, T \u2286 V2 is subset of terminal nodes, n0 \u2208 V1 \u222a V2 is the root node, and c : E \u222a T \u2192 R\u2217 is a cost function over edges and terminal nodes [26].", "startOffset": 310, "endOffset": 314}, {"referenceID": 2, "context": "Therefore, more efficient algorithms can be used such as label setting methods and Dial\u2019s algorithm [3, 7].", "startOffset": 100, "endOffset": 106}, {"referenceID": 6, "context": "Therefore, more efficient algorithms can be used such as label setting methods and Dial\u2019s algorithm [3, 7].", "startOffset": 100, "endOffset": 106}, {"referenceID": 5, "context": "Relevant algorithms in this class are the classical AO* [6, 23, 26], LAO* [14], RTDP-like algorithms [2, 4], and LDFS [5].", "startOffset": 56, "endOffset": 67}, {"referenceID": 21, "context": "Relevant algorithms in this class are the classical AO* [6, 23, 26], LAO* [14], RTDP-like algorithms [2, 4], and LDFS [5].", "startOffset": 56, "endOffset": 67}, {"referenceID": 24, "context": "Relevant algorithms in this class are the classical AO* [6, 23, 26], LAO* [14], RTDP-like algorithms [2, 4], and LDFS [5].", "startOffset": 56, "endOffset": 67}, {"referenceID": 12, "context": "Relevant algorithms in this class are the classical AO* [6, 23, 26], LAO* [14], RTDP-like algorithms [2, 4], and LDFS [5].", "startOffset": 74, "endOffset": 78}, {"referenceID": 1, "context": "Relevant algorithms in this class are the classical AO* [6, 23, 26], LAO* [14], RTDP-like algorithms [2, 4], and LDFS [5].", "startOffset": 101, "endOffset": 107}, {"referenceID": 3, "context": "Relevant algorithms in this class are the classical AO* [6, 23, 26], LAO* [14], RTDP-like algorithms [2, 4], and LDFS [5].", "startOffset": 101, "endOffset": 107}, {"referenceID": 4, "context": "Relevant algorithms in this class are the classical AO* [6, 23, 26], LAO* [14], RTDP-like algorithms [2, 4], and LDFS [5].", "startOffset": 118, "endOffset": 121}, {"referenceID": 15, "context": "Results for different optimization criteria, probabilistic and non-deterministic variants, and so on are known [17, 22, 25, 28, 33].", "startOffset": 111, "endOffset": 131}, {"referenceID": 20, "context": "Results for different optimization criteria, probabilistic and non-deterministic variants, and so on are known [17, 22, 25, 28, 33].", "startOffset": 111, "endOffset": 131}, {"referenceID": 23, "context": "Results for different optimization criteria, probabilistic and non-deterministic variants, and so on are known [17, 22, 25, 28, 33].", "startOffset": 111, "endOffset": 131}, {"referenceID": 26, "context": "Results for different optimization criteria, probabilistic and non-deterministic variants, and so on are known [17, 22, 25, 28, 33].", "startOffset": 111, "endOffset": 131}, {"referenceID": 31, "context": "Results for different optimization criteria, probabilistic and non-deterministic variants, and so on are known [17, 22, 25, 28, 33].", "startOffset": 111, "endOffset": 131}, {"referenceID": 19, "context": "Littman [21] obtained important complexity results about detpomdps for problems with non-negative costs (i.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "For hardness, we reduce sat using the method in [25] almost verbatim.", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "Estimations on the expected order of random permutations are also known [9, 12, 38].", "startOffset": 72, "endOffset": 83}, {"referenceID": 10, "context": "Estimations on the expected order of random permutations are also known [9, 12, 38].", "startOffset": 72, "endOffset": 83}, {"referenceID": 36, "context": "Estimations on the expected order of random permutations are also known [9, 12, 38].", "startOffset": 72, "endOffset": 83}, {"referenceID": 9, "context": "For example, Furst, Hopcroft and Luks [10] gave a polynomial-time algorithm for deciding whether a permutation \u03c3 is generated by the set A, and Jerrum [18] showed that computing the size of a smallest generator set is PSPACE-complete.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "For example, Furst, Hopcroft and Luks [10] gave a polynomial-time algorithm for deciding whether a permutation \u03c3 is generated by the set A, and Jerrum [18] showed that computing the size of a smallest generator set is PSPACE-complete.", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "Thus, it is enough to guess \u03c3 and then test, in polynomial-time using the algorithm in [10], if \u03c3 is generated by the actions.", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": "Driscoll and Furst [8] showed that if the generators are all cycles of bounded length, the diameter is O(n), while McKenzie [24] showed that if all generators move at most k elements, the diameter is O(n).", "startOffset": 19, "endOffset": 22}, {"referenceID": 22, "context": "Driscoll and Furst [8] showed that if the generators are all cycles of bounded length, the diameter is O(n), while McKenzie [24] showed that if all generators move at most k elements, the diameter is O(n).", "startOffset": 124, "endOffset": 128}, {"referenceID": 33, "context": "We have shown novel complexity results that show this tradeoff: while checking the existence of plans in classical deterministic (flat) problems is NL-complete (via st-Reachability [35]), and checking the existence of plans for pomdps is EXPTIME-complete [21], we have that checking the existence of plans for det-pomdps of polynomial diameter is NP-complete, and that for det-pomdps in which the actions are permutations with empty precondition is in NP.", "startOffset": 181, "endOffset": 185}, {"referenceID": 19, "context": "We have shown novel complexity results that show this tradeoff: while checking the existence of plans in classical deterministic (flat) problems is NL-complete (via st-Reachability [35]), and checking the existence of plans for pomdps is EXPTIME-complete [21], we have that checking the existence of plans for det-pomdps of polynomial diameter is NP-complete, and that for det-pomdps in which the actions are permutations with empty precondition is in NP.", "startOffset": 255, "endOffset": 259}], "year": 2009, "abstractText": "We study a subclass of POMDPs, called Deterministic POMDPs, that is characterized by deterministic actions and observations. These models do not provide the same generality of POMDPs yet they capture a number of interesting and challenging problems, and permit more efficient algorithms. Indeed, some of the recent work in planning is built around such assumptions mainly by the quest of amenable models more expressive than the classical deterministic models. We provide results about the fundamental properties of Deterministic POMDPs, their relation with AND/OR search problems and algorithms, and their computational complexity.", "creator": "TeX"}}}