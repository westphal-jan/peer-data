{"id": "1205.4481", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2012", "title": "Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure", "abstract": "on a setting we develops computational stochastic minimization concerning nonsmooth convex loss functions, big central problem in method learning. tools compute synthetic synthetic algorithm upon accelerated nonsmooth stochastic gradient descent ( ansgd ), which analyses mirror structure of discrete nonsmooth loss networks as achieve optimal convergence error for reduced class of items including svms. it is as first computational maneuver that attempts achieve the symmetric sampling ( b / t ) rate pattern minimizing nonsmooth vector populations ( through strong convexity ). the fast rates be confirmed whenever successive comparisons, amongst which friedman periodically tests previous subgradient descent errors including sampling.", "histories": [["v1", "Mon, 21 May 2012 03:29:17 GMT  (1334kb)", "https://arxiv.org/abs/1205.4481v1", "A short version of this paper appears in ICML 2012"], ["v2", "Mon, 2 Jul 2012 14:53:38 GMT  (1199kb)", "http://arxiv.org/abs/1205.4481v2", "This camera-ready version is uploaded for ICML 2012 proceedings"], ["v3", "Wed, 25 Jul 2012 15:15:42 GMT  (1334kb)", "http://arxiv.org/abs/1205.4481v3", "Full length version of ICML'12 with all proofs"], ["v4", "Mon, 1 Oct 2012 16:55:06 GMT  (1335kb)", "http://arxiv.org/abs/1205.4481v4", "Full length version of ICML'12 with all proofs. In this version, a bug in proving Theorem 6 is fixed. We'd like to thank Dr. Francesco Orabona for pointing it out"]], "COMMENTS": "A short version of this paper appears in ICML 2012", "reviews": [], "SUBJECTS": "cs.LG stat.CO stat.ML", "authors": ["hua ouyang", "alexander g gray"], "accepted": true, "id": "1205.4481"}, "pdf": {"name": "1205.4481.pdf", "metadata": {"source": "CRF", "title": "Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure", "authors": ["Hua Ouyang", "Alexander Gray"], "emails": ["agray}@cc.gatech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 5.\n44 81\nv4 [\ncs .L\nG ]\n1 O\nct 2\n01 2"}, {"heading": "1. Introduction", "text": "Nonsmoothness is a central issue in machine learning computation, as many important methods minimize nonsmooth convex functions. For example, using the nonsmooth hinge loss yields sparse support vector machines; regressors can be made robust to outliers by using the nonsmooth absolute loss other than the squared loss; the l1-norm is widely used in sparse reconstructions. In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning).\nSmoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov\u2019s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied. It converges as f(xt)\u2212minx f(x) \u2264 O(1/t) after t iterations. If a nonsmooth function is strongly convex, this rate can be improved to O(1/t2) using the excessive gap technique Nesterov (2005a).\nIn this paper, we extend Nesterov\u2019s smoothing method to the stochastic setting by proposing a stochastic smoothing method for nonsmooth functions. Combining this with a stochastic version of the optimal gradient descent method, we introduce and analyze a new algorithm named Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), for a class of functions that include the popular ML methods of interest.\nTo our knowledge ANSGD is the first stochastic first-order algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions without Polyak\u2019s averaging Polyak and Juditsky (1992). In comparison, the classic SGD converges in O(ln t/t) for\n1. A short version of this paper appears in International Conference of Machine Learning (ICML) 2012.\nnonsmooth strongly convex functions Shalev-Shwartz et al. (2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak\u2019s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD\u2019s convergence rate still can not be faster than O(ln t/t) Shamir (2011). Numerical experiments on real-world datasets also indicate that ANSGD converges much faster in comparing with these state-of-the-art algorithms.\nA perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization Duchi et al. (2011). This work achieves similar iteration complexities as ours, in a parallel computation scenario. In serial settings, ANSGD enjoys better and optimal bounds.\nIn machine learning, many problems can be cast as minimizing a composition of a loss function and a regularization term. Before proceeding to the algorithm, we first describe a different setting of \u201ccomposite minimizations\u201d that we will pursue in this paper, along with our notations and assumptions."}, {"heading": "1.1 A Different \u201cComposite Setting\u201d", "text": "In the classic black-box setting of first-order stochastic algorithms Nemirovski et al. (2009), the structure of the objective function minx{f(x) = E\u03bef(x, \u03be) : \u03be \u223c P} is unknown. In each iteration t, an algorithm can only access the first-order stochastic oracle and obtain a subgradient f \u2032(x, \u03bet). The basic assumption is that f\n\u2032(x) = E\u03bef \u2032(x, \u03be) for any x, where the random vector \u03be is from a fixed distribution P .\nThe composite setting (also known as splitting Lions and Mercier (1979)) is an extension of the black-box model. It was proposed to exploit the structure of objective functions. Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al. (2009); Xiao (2010). A stochastic composite function \u03a6(x) := f(x) + g(x) is the sum of a smooth stochastic convex function f(x) = E\u03bef(x, \u03be) and a nonsmooth (but simple and deterministic) function g(). To minimize \u03a6, previous work construct the following model iteratively:\n\u3008\u2207f(xt, \u03bet),x \u2212 xt\u3009+ 1\n\u03b7t D(x,xt) + g(x), (1)\nwhere \u2207f(xt, \u03bet) is a gradient, D(\u00b7, \u00b7) is a proximal function (typically a Bregman divergence) and \u03b7t is a stepsize.\nA successful application of the composite idea typically relies on the assumption that model (1) is easy to minimize. If g() is very simple, e.g. \u2016x\u20161 or the nuclear norm, it is straightforward to obtain the minimum in analytic forms. However, this assumption does not hold for many other applications in machine learning, where many loss functions (not the regularization term, here the nonsmooth g() becomes the nonsmooth loss function) are nonsmooth, and do not enjoy separability properties Wright et al. (2009). This includes important examples such as hinge loss, absolute loss, and \u01eb-insensitive loss.\nIn this paper, we tackle this problem by studying a new stochastic composite setting: minx\u03a6(x) = f(x) + g(x), where loss function f() is convex and nonsmooth, while g() is\nconvex and Lg-Lipschitz smooth:\ng(x) \u2264 g(y) + \u3008\u2207g(y),x \u2212 y\u3009+ Lg 2 \u2016x\u2212 y\u20162. (2)\nFor clarity, in this paper we focus on unconstrained minimizations. Without loss of generality, we assume that both f() and g() are stochastic: f(x) = E\u03bef(x, \u03be) and g(x) = E\u03beg(x, \u03be), where \u03be has distribution P . If either one is deterministic, its \u03be is then dropped. To make our algorithm and analysis more general, we assume that g() is \u00b5-strongly convex: \u2200x,y,\ng(x) \u2265 g(y) + \u3008\u2207g(y),x \u2212 y\u3009+ \u00b5 2 \u2016x\u2212 y\u20162. (3)\nIf it is not strongly convex, one can simply take \u00b5 = 0.\nThe main idea of our algorithm again stems from exploiting the structures of f() and g(). In Section 2 we propose to form a smooth stochastic approximation of f(), such that the optimal methods Nesterov (2004) can be applied to attain optimal convergence rates. The convergence of our proposed algorithm is analyzed in Section 3, and a batch-to-online conversion is also proposed. Two popular machine learning problems are chosen as our examples in Section 4, and numerical evaluations are presented in Section 5. All proofs in this paper are provided in the appendix."}, {"heading": "2. Approach", "text": ""}, {"heading": "2.1 Stochastic Smoothing Method", "text": "An important breakthrough in nonsmooth minimization was made by Nesterov in a series of works Nesterov (2005b,a, 2007b). By exploiting function structures, Nesterov shows that in many applications, minimizing a well-structured nonsmooth function f(x) can be formulated as an equivalent saddle-point form\nmin x\u2208X f(x) = min x\u2208X max u\u2208U\n[ \u3008Ax,u\u3009 \u2212Q(u) ] , (4)\nwhere u \u2208 Rm, U \u2286 Rm is a convex set, A is a linear operator mapping RD \u2192 Rm and Q(u) is a continuous convex function. Inserting a non-negative \u03b6-strongly convex function \u03c9(u) in (4) one obtains a smooth approximation of the original nonsmooth function\nf\u0302(x, \u03b3) := max u\u2208U\n[ \u3008Ax,u\u3009 \u2212Q(u)\u2212 \u03b3\u03c9(u) ] , (5)\nwhere \u03b3 > 0 is a fixed smoothness parameter which is crucial in the convergence analysis. The key property of this approximation is:\nLemma 1 Nesterov (2005b)(Theorem 1) Function f\u0302(x, \u03b3) is convex and continuously differentiable, and its gradient is Lipschitz continuous with constant Lf\u0302 := \u2016A\u20162 \u03b3\u03b6 , where\n\u2016A\u2016 := max x,u {\u3008Ax,u\u3009 : \u2016x\u2016 = 1, \u2016u\u2016 = 1}. (6)\nNesterov\u2019s smoothing method was originally proposed for deterministic optimization. A major drawback of this method is that the number of iterations N must be known beforehand, such that the algorithm can set a proper smoothness parameter \u03b3 = O\n(2\u2016A\u2016 N+1 )\nto ensure convergence. This makes it unsuitable for algorithms that runs forever, or whose number of iterations is not known. Following his work we propose to extend this smoothing method to stochastic optimization. Our stochastic smoothing differs from the deterministic one in the operator A and smoothness parameter \u03b3, where both will be time-varying.\nWe assume that the nonsmooth part f(x, \u03be) of the stochastic composite function \u03a6() is well structured, i.e. for a specific realization \u03bet, it has an equivalent form like the max function in (4):\nf(x, \u03bet) = max u\u2208U\n[ \u3008A\u03betx,u\u3009 \u2212Q(u) ] , (7)\nwhere A\u03bet is a stochastic linear operator associated with \u03bet. We construct a smooth approximation of this function as:\nf\u0302(x, \u03bet, \u03b3t) := max u\u2208U\n[ \u3008A\u03betx,u\u3009 \u2212Q(u)\u2212 \u03b3t\u03c9(u) ] , (8)\nwhere \u03b3t is a time-varying smoothness parameter only associated with iteration index t, and is independent of \u03bet. Function \u03c9() is non-negative and \u03b6-strongly convex. Due to Lemma 1, f\u0302(x, \u03bet, \u03b3t) is \u2016A\u03bet\u20162 \u03b3t\u03b6 -Lipschitz smooth. It follows that\nLemma 2 \u2200x,y, t, E\u03bef\u0302(x, \u03be, \u03b3t) \u2264 E\u03bef\u0302(y, \u03be, \u03b3t)+E\u03be\u3008\u2207f\u0302(y, \u03be, \u03b3t),x\u2212y\u3009+ E\u03be\u2016A\u03be\u2016 2\n\u03b3t\u03b6 \u2016x\u2212y\u20162.\nWe have the following observation about our composite objective \u03a6(), which relates the reduction of the original and approximated function values.\nLemma 3 For any x,xt, t,\n\u03a6(xt)\u2212 \u03a6(x) \u2264 E\u03be [ f\u0302(xt, \u03be, \u03b3t) + g(xt, \u03be) ] \u2212 E\u03be [ f\u0302(x, \u03be, \u03b3t) + g(x, \u03be) ] + \u03b3tDU , (9)\nwhere DU := maxu\u2208U \u03c9(u).\n2.2 Accelerated Nonsmooth SGD (ANSGD)\nWe are now ready to present our algorithm ANSGD (Algorithm 1). This stochastic algorithm is obtained by applying Nesterov\u2019s optimal method to our smooth surrogate function, and thus has a similar form to that of his original deterministic method Nesterov (2004)(p.78). However, our convergence analysis is more straightforward, and does not rely on the concept of estimate sequences. Hence it is easier to identify proper series \u03b3t, \u03b7t, \u03b1t and \u03b8t that are crucial in achieving fast rates of convergence. These series will be determined in our main results (Thm.6 and 7)."}, {"heading": "3. Convergence Analysis", "text": "To clarify our presentation, we use Table 1 to list some notations that will be used throughout the paper.\nAlgorithm 1 Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD)\nINPUT: series \u03b3t, \u03b7t, \u03b8t \u2265 0 and 0 \u2264 \u03b1t \u2264 1; OUTPUT: xt+1; [0.] Initialize x0 and v0; for t = 0, 1, 2, . . . do\n[1.] yt \u2190 (1\u2212\u03b1t)(\u00b5+\u03b8t)xt+\u03b1t\u03b8tvt\u00b5(1\u2212\u03b1t)+\u03b8t [2.] f\u0302t+1(x) \u2190 max\nu\u2208U\n[ \u3008A\u03bet+1x,u\u3009 \u2212Q(u)\u2212 \u03b3t+1\u03c9(u) ]\n[3.] xt+1 \u2190 yt \u2212 \u03b7t [ \u2207f\u0302t+1(yt) +\u2207gt+1(yt) ]\n[4.] vt+1 \u2190 \u03b8tvt+\u00b5yt\u2212[\u2207f\u0302t+1(yt)+\u2207gt+1(yt)]\n\u00b5+\u03b8t end for\nOur convergence rates are based on the following main lemma, which bounds the progressive reduction \u2206t of the smoothed function value. Actually Line 1, 3, and 4 of Alg.1 are also derived from the proof of this lemma.\nLemma 4 Let \u03b3t be monotonically decreasing. Applying algorithm ANSGD to nonsmooth composite function \u03a6(), we have \u2200x and \u2200t \u2265 0,\n\u2206t+1 \u2264 (1\u2212 \u03b1t)\u2206t + (1\u2212 \u03b1t)(\u03b3t \u2212 \u03b3t+1)DU+\n\u0393t+1 + \u03b1t 2\n[ \u03b8t\u2016x\u2212 vt\u20162 \u2212 (\u00b5+ \u03b8t)\u2016x\u2212 vt+1\u20162 ] +\n\u03b7tpq +\n[\n\u03b1t 2(\u00b5+ \u03b8t) + Lt+1 2 \u03b72t \u2212 \u03b7t ] q2\n(10)\nwhere p := \u2016\u03c3t+1(yt)\u2016 and q := \u2016\u2207f\u0302t+1(yt) +\u2207gt+1(yt)\u2016."}, {"heading": "3.1 How to Choose Stepsizes \u03b7t", "text": "In the RHS of (10), nonnegative scalars p, q \u2265 0 are data-dependent, and could be arbitrarily large. Hence we need to set proper stepsizes \u03b7t such that the last two terms in (10) are\nnon-positive. One might conjecture that: there exist a series ct \u2265 0 such that\n\u03b7tpq +\n[\n\u03b1t 2(\u00b5 + \u03b8t) + Lt+1 2 \u03b72t \u2212 \u03b7t ] q2 \u2264 ctp2. (11)\nIt is easy to verify that if we take \u03b7t = \u03b1t \u00b5+\u03b8t and any series ct \u2265 \u03b1t2(\u00b5+\u03b8t\u2212\u03b1tLt+1) \u2265 0, then (11) is satisfied. To retain a tight bound, we take\nct = \u03b1t\n2(\u00b5 + \u03b8t \u2212 \u03b1tLt+1) . (12)\nTaking expectation on both sides of (10) and noticing that E\u03bet+1|\u03be[t]\u0393t+1 = 0, E\u03bet+1ct \u2264 \u03b1t 2(\u00b5+\u03b8t\u2212\u03b1tE\u03bet+1Lt+1) due to Jensen\u2019s inequality, we have Lemma 5 \u2200x and \u2200t \u2265 0, E\u2206t+1 \u2264 (1\u2212 \u03b1t)E\u2206t + \u03b1t\u03b8tD2t \u2212 \u03b1t(\u00b5+ \u03b8t)D2t+1 +\n\u03b1t 2(\u00b5 + \u03b8t \u2212 \u03b1tELt+1) \u03c32 + (1\u2212 \u03b1t)(\u03b3t \u2212 \u03b3t+1)DU , (13)\nThe optimal convergence rates of our algorithm differs according to the fact of \u00b5 (positive or not). They are presented separately in the following two subsections, where the choices of \u03b3t, \u03b8t, \u03b1t will also be determined."}, {"heading": "3.2 Optimal Rates for Composite Minimizations when \u00b5 = 0", "text": "When \u00b5 = 0, g() is only convex and Lg-Lipschitz smooth, but not assumed to be strongly convex.\nTheorem 6 Take \u03b1t = 2 t+2 , \u03b3t+1 = \u03b1t, \u03b8t = Lg\u03b1t + \u2126\u221a \u03b1t\n+ E\u2016A\u03be\u20162\n\u03b6 and \u03b7t = \u03b1t \u03b8t in Alg.1,\nwhere \u2126 is a constant. We have \u2200x and \u2200t \u2265 0,\nE [\u03a6(xt+1)\u2212 \u03a6(x)] \u2264 4LgD\n2\n(t+ 2)2 + 2E\u2016A\u03be\u20162D2/\u03b6 + 4DU t+ 2 +\n\u221a 2(\u2126D2 + \u03c32/\u2126)\u221a\nt+ 2 , (14)\nwhere D2 := maxiD 2 i .\nIn this result, the variance bound is optimal up to a constant factor Agarwal et al. (2012). The dominating factor is still due to the stochasticity, but not affected by the nonsmoothness of f(). Taking the parameter \u2126 = \u03c3/D, this last term becomes 2 \u221a 2D\u03c3\u221a t+2 . This bound is better than that of stochastic gradient descent or stochastic dual averaging Dekel et al. (2010) for minimizing L-Lipschitz smooth functions, whose rate is O (\nLD20 t + D20+\u03c3 2 \u221a t\n)\n; without the\nsmooth function g(), our bound is of the same order as it, keeping in mind that our rate is for nonsmooth minimizations. This fact underscores the potential of using stochastic optimal methods for nonsmooth functions.\nThe diminishing smoothness parameter \u03b3t = 2\nt+2 indicates that initially a smoother approximation is preferred, such that the solution does not change wildly due to the nonsmoothness and stochasticity. Eventually the approximated function should be closer and closer to the original nonsmooth function, such that the optimality can be reached. Some concrete examples are given in Fig.1.\nThe E\u2016A\u03be\u20162 in our bound is a theoretical constant. In Sec.4 we demonstrate a sampling method, and it turns out to work quite well in estimating E\u2016A\u03be\u20162."}, {"heading": "3.3 Nearly Optimal Rates for Strongly Convex Minimizations", "text": "When \u00b5 > 0, g() is strongly convex, and the convergence rate of ANSGD can be improved to O(1/t).\nTheorem 7 Take \u03b1t = 2 t+1 , \u03b3t+1 = \u03b1t, \u03b8t = Lg\u03b1t + \u00b5 2\u03b1t\n+ E\u2016A\u03be\u20162\n\u03b6 \u2212 \u00b5 and \u03b7t = \u03b1t\u00b5+\u03b8t in Alg.1. Denote\nC := max\n{\n4E\u2016A\u03be\u20162 \u03b6\u00b5 , 2 ( Lg \u00b5\n)1/3 }\n. (15)\nWe have \u2200x and \u2200t \u2265 0,\nE [\u03a6(xt+1)\u2212 \u03a6(x)] \u2264 6.58LgD\u0303\n2\nt(t+ 1) + B + 4DU t+ 1 +\n\u03c32\n\u00b5(t+ 1) , (16)\nwhere\nB :=\n\n\n\n2E\u2016A\u03be\u20162D\u03032/\u03b6 t+1 if 0 \u2264 t < C, 2(C\u22122)E\u2016A\u03be\u20162D\u03032/\u03b6 t(t+1) if t \u2265 C,\n(17)\nand D\u03032 := max0\u2264i\u2264min{t,C} D 2 i .\nNote that C is the smallest iteration index for which one can retain 1/t2 rates for the E\u2016A\u03be\u20162 part (B). Without any knowledge about Lg, \u00b5 and E\u2016A\u03be\u20162, one can set a parameter \u2126 and take \u03b8t = Lg\u03b1t+\n\u00b5 2\u03b1t\n+ E\u2016A\u03be\u20162\n\u2126\u03b6 \u2212\u00b5 in the algorithm. In our experiments, we observe that one can take \u2126 fairly large (of O(E\u2016A\u03be\u20162)), meaning that C can be very small (O(1)), and B is O( 1t2 ) for all t. In this sense, strongly convex ANSGD is almost parameter-free. Without the O(1/t) rate of DU , all terms in our bound are optimal. This is why our rate is called \u201cnearly\u201d optimal. In practice, DU is usually small, and it will be dominated by the last term \u03c3 2\n\u00b5(t+1) ."}, {"heading": "3.4 Batch-to-Online Conversion", "text": "The performance of an online learning (online convex minimization) algorithm is typically measured by regret, which can be expressed as\nR(t) :=\nt\u22121 \u2211\ni=0\n[ \u03a6(xi, \u03bei+1)\u2212 \u03a6(x\u2217t , \u03bei+1) ] , (18)\nwhere x\u2217t := argminx \u2211t\u22121 i=0 [ \u03a6(x, \u03bei+1) ]\n. In the learning theory literature, many approaches are proposed which use online learning algorithms for batch learning (stochastic optimization), called \u201conline-to-batch\u201d (O-to-B) conversions. For convex functions, many of these approaches employ an \u201caveraged\u201d solution as the final solution.\nOn the contrary, we show that stochastic optimization algorithms can also be used directly for online learning. This \u201cbatch-to-online\u201d (B-to-O) conversion is almost free of any additional effort: under i.i.d. assumptions of data, one can use any stochastic optimization algorithm for online learning.\nProposition 8 For any t \u2265 0, E\u03be[t]R(t) \u2264\nt\u22121 \u2211\ni=0\nE\u03be[i] [\u03a6(xi)\u2212 \u03a6(x\u2217)] + E\u03be[t]\nt\u22121 \u2211\ni=0\n[ \u03a6(x\u2217t )\u2212 \u03a6(x\u2217t , \u03bei+1) ]\n(19)\nwhere x\u2217 := argminx\u03a6(x) and x\u2217t := argminx \u2211t\u22121 i=0 [ \u03a6(x, \u03bei+1) ] .\nWhen \u03a6() is convex, the second term in (19) can be bounded by applying standard results in uniform convergence (e.g. Boucheron et al. (2005)):\n\u2211t\u22121 i=1 \u03a6(x \u2217 t )\u2212\u03a6(x\u2217t , \u03bei+1) = O(\n\u221a t).\nTogether with summing up the RHS of (14), we can obtain an O( \u221a t) regret bound. When \u03a6() is strongly convex, the second term in (19) can be bounded using Shalev-Shwartz et al. (2009):\n\u2211t\u22121 i=1 \u03a6(x \u2217 t ) \u2212 \u03a6(x\u2217t , \u03bei+1) = O(ln t). Together with summing up the RHS of (16),\nan O(ln t) regret bound is achieved. The O( \u221a t) and O(ln t) regret bounds are known\nUsing our proposed ANSGD for online learning by B-to-O achieves the same (optimal) regret bounds as state-of-the-art algorithms designated for online learning. However, using O-to-B, one can only retain an O(ln t/t) rate of convergence for stochastic strongly convex optimization. From this perspective, O-to-B is inferior to B-to-O. The sub-optimality of O-to-B is also discussed in Hazan and Kale (2011)."}, {"heading": "4. Examples", "text": "In this section, two nonsmooth functions are given as examples. We will show how these functions can be stochastically approximated, and how to calculate parameters used in our algorithm."}, {"heading": "4.1 Hinge Loss SVM Classification", "text": "Hinge loss is a convex surrogate of the 0 \u2212 1 loss. Denote a sample-label pair as \u03be := {s, l} \u223c P , where s \u2208 RD and l \u2208 R. Hinge loss can be expressed as fhinge(x) := max{0, 1\u2212 lsTx}. It has been widely used for SVM classifiers where the objective is min\u03a6(x) = minE\u03befhinge(x) + \u03bb 2\u2016x\u20162. Note that the regularization term g(x) = \u03bb2 \u2016x\u20162 is \u03bb-strongly convex, hence according to Thm.7, ANSGD enjoys O(1/(\u03bbt)) rates. Taking \u03c9(u) = 12\u2016u\u20162 in (8), it is easy to check that the smooth stochastic approximation of hinge loss is\nf\u0302hinge(x, \u03bet, \u03b3t) = max 0\u2264u\u22641\n{\nu ( 1\u2212 ltsTt x ) \u2212 \u03b3t u2\n2\n}\n. (20)\nThis maximization is simple enough such that we can obtain an equivalent smooth representation:\nf\u0302hinge(x, \u03bet, \u03b3t) =\n\n \n \n0 if lts T t x \u2265 1, (1\u2212ltsTt x)2 2\u03b3t\nif 1\u2212 \u03b3t \u2264 ltsTt x < 1, 1\u2212 ltsTt x\u2212 \u03b3t2 if ltsTt x < 1\u2212 \u03b3t.\n(21)\nSeveral examples of f\u0302hinge with varying \u03b3t are plotted in Fig.1(left) in comparing with the hinge loss.\nHere u is a scalar, hence it is straightforward to calculate E\u2016A\u03be\u20162\n\u03b6 , which will be used to generate sequences \u03b8t. In binary classification, suppose l \u2208 {1,\u22121}. Using definition (6),\none only needs to calculate E(max\u2016x\u2016=1 s T t x) 2. Practically one can take a small subset of k random samples si (e.g. k = 100), and calculate the sample average of the squared norms 1 k \u2211k i=1 \u2016si\u20162. This yields 1k \u2211k i=1(max\u2016x\u2016=1 s T i x) 2, an estimate of E\u2016A\u03be\u20162."}, {"heading": "4.2 Absolute Loss Robust Regression", "text": "Absolute loss is an alternative to the popular squared loss for robust regressions Hastie et al. (2009). Using same notations as Sec.4.1 it can be expressed as fabs(x) := |l\u2212 sTx|. Taking \u03c9(u) = 12\u2016u\u20162 in (8), its smooth stochastic approximation can be expressed as\nf\u0302abs(x, \u03bet, \u03b3t) = max\u22121\u2264u\u22641\n{\nu(lt \u2212 sTt x)\u2212 \u03b3t u2\n2\n}\n. (22)\nSolving this maximization wrt u we obtain an equivalent form:\nf\u0302abs(x, \u03bet, \u03b3t) =\n\n \n \nlt \u2212 sTt x\u2212 \u03b3t2 if lt \u2212 sTt x \u2265 \u03b3t, (lt\u2212sTt x)2\n2\u03b3t if \u2212 \u03b3t \u2264 lt \u2212 sTt x < \u03b3t, \u2212(lt \u2212 sTt x)\u2212 \u03b3t2 if lt \u2212 sTt x < \u2212\u03b3t. (23)\nThis approximation looks similar to the well-studied Huber loss Huber (1964), though they are different. Actually they share the same form only when \u03b3t = 0.5 (green curve in Fig.1 Right).\nThe parameter E\u2016A\u03be\u20162 can be estimated in a similar way as discussed in Sec.4.1."}, {"heading": "5. Experimental Results", "text": "In this section, five publicly available datasets from various application domains will be used to evaluate the efficiency of ANSGD. Datasets \u201csvmguide1\u201d, \u201creal-sim\u201d, \u201crcv1\u201d and \u201calpha\u201d are for binary classifications, and \u201cabalone\u201d is for robust regressions.1\nFollowing our examples in Sec.4, we will evaluate our algorithm using approximated hinge loss for classifications, and approximated absolute loss for regressions. Exact hinge and absolute losses will be used for subgradient descent algorithms that we will compare with, as described in the following section. All losses are squared-l2-norm-regularized. The regularization parameter \u03bb is shown on each figure. When assuming strong-convexity, we take \u00b5 = \u03bb."}, {"heading": "5.1 Algorithms for Comparison and Parameters", "text": "We compare ANSGD with three state-of-the-art algorithms. Each algorithm has a datadependent tuning parameter, denoted by \u2126 (although they have different physical meanings). The best values of \u2126 are found based on a tuning subset of samples. Note that when assuming strong-convexity, our ANSGD is almost parameter-free. As discussed after Thm.7, our experiments indicate that the optimal \u2126 is taken such that E\u2016A\u03be\u20162\n\u2126\u03b6 \u2248 1, meaning that one can simply take \u03b8t = Lg\u03b1t +\n\u00b5 2\u03b1t + 1\u2212 \u00b5. SGD. The classic stochastic approximation Robbins and Monro (1951) is adopted: xt+1 \u2190\nxt \u2212 \u03b7tf \u2032(xt), where f \u2032(xt) is the subgradient. When only assuming convexity (\u00b5 = 0), we use stepsize \u03b7t =\n\u2126\u221a t . When assuming strong-convexity, we follow the stepsize used in SGD2\nBottou: \u03b7t = 1\n\u00b5(t+\u2126) .\nAveraged SGD. This is algorithmically the same as SGD, except that the averaged result x\u0304 := 1t \u2211t i=1 xi is used for testing. We follow the stepsizes suggested by the recent work on the non-asymptotic analysis of SGD Bach and Moulines (2011); Xu (2011), where it is argued that Polyak\u2019s averaging combining with proper stepsizes yield optimal rates. When only assuming convexity, we use stepsizes \u03b7t =\n\u2126\u221a t Bach and Moulines (2011). When\nassuming strong convexity, the stepsize is taken as \u03b7t = 1\n\u2126(1+\u00b5t/\u2126)3/4 Xu (2011).\nAC-SA. This approach Lan (2010); Lan and Ghadimi (2011) is interesting to compare because like ANSGD, it is another way of obtaining a stochastic algorithm based on Nesterov\u2019s optimal method, begging the question of whether it has similar behavior. Theoretically, according to Prop.8 and 9 in Lan and Ghadimi (2011), the bound for the nonsmooth part is of O(1/ \u221a t) for \u00b5 = 0 and O(1/t) for \u00b5 > 0. In comparison, our nonsmooth part converges in O(1/t) for \u00b5 = 0 and O(1/t2) for \u00b5 > 0. Numerically we observe that directly applying AC-SA to nonsmooth functions results in inferior performances.\n1. Dataset \u201calpha\u201d is obtained from ftp://largescale.ml.tu-berlin.de/largescale/, and the other four datasets can be accessed via http://www.csie.ntu.edu.tw/~cjlin/libsvmtools. Dataset \u201crcv1\u201d comes with 20, 242 training samples and 677, 399 testing samples. For \u201csvmguide1\u201d and \u201creal-sim\u201d, we randomly take 60% of the samples for training and 40% for testing. For \u201calpha\u201d and \u201cabalone\u201d, 80% are used for training, and the rest 20% are used for testing."}, {"heading": "5.2 Results", "text": "Due to the stochasticity of all the algorithms, for each setting of the experiments, we run the program for 10 times, and plot the mean and standard deviation of the results using error bars.\nIn the first set of experiments, we compare ANSGD with two subgradient-based algorithms SGD and Averaged SGD. Classification results are shown in Fig.2, 3, 4 and 5, and regression results are shown in Fig.6. In each figure, the left column is for algorithms without strongly convex assumptions, while in the right column the algorithms assume strong-convexity and take \u00b5 = \u03bb. For classification results, we plot function values over the testing set in the first row, and plot testing accuracies in the second row.\nIt is clear that in all these experiments, ANSGD\u2019s function values converges consistently faster than the other two SGD algorithms. In non-strongly convex experiments, it converges significantly faster than SGD and its averaged version. In strongly convex experiments, it still out performs, and is more robust than strongly convex SGD. Averaged SGD performs well in strongly convex settings, in terms of prediction accuracies, although its errors are still higher than ANSGD in the first three datasets. The only exception is in \u201calpha\u201d (Fig.5), where Averaged SGD retains higher function values than ANSGD, but its accuracies are contradictorily higher in early stages. The reason might be that the inexact solution serves as an additional regularization factor, which cannot be predicted by the analysis of convergence rates.\nIn the second set of experiments, we compare ANSGD with AC-SA and its strongly convex version. Results are in Fig.7, 8, 9 and 10. In all experiments our ANSGD significantly outperforms AC-SA, and is much more stable. These experiments confirm the theoretically better rates discussed in Sec.5.1."}, {"heading": "6. Conclusions and Future Work", "text": "We introduce a different composite setting for nonsmooth functions. Under this setting we propose a stochastic smoothing method and a novel stochastic algorithm ANSGD. Convergence analysis show that it achieves (nearly) optimal rates under both convex and strongly\nconvex assumptions. We also propose a \u201cBatch-to-Online\u201d conversion for online learning, and show that optimal regrets can be obtained.\nWe will extend our method to constrained minimizations, as well as cases when the approximated function f\u0302() is not easily obtained by maximizing u. Nesterov\u2019s excessive gap technique has the \u201ctrue\u201d optimal 1/t2 bound, and we will investigate the possibility of integrating it in our algorithm. Exploiting links with statistical learning theories may also be promising."}, {"heading": "Appendix A. Proof of Lemma 3", "text": "Proof\n\u03a6(xt)\u2212 \u03a6(x) = [f(xt)\u2212 f(x)] + [g(xt)\u2212 g(x)] = E\u03be [f(xt, \u03be)] + E\u03be [\u2212f(x, \u03be) + g(xt, \u03be)\u2212 g(x, \u03be)]\n= E\u03be max u\u2208U\n{\n[ \u3008A\u03bext,u\u3009 \u2212Q(u)\u2212 \u03b3t\u03c9(u) ] + \u03b3t\u03c9(u)\n}\n+ E\u03be [\u2212f(x, \u03be) + g(xt, \u03be)\u2212 g(x, \u03be)]\n\u2264 E\u03be max u\u2208U\n[ \u3008A\u03bext,u\u3009 \u2212Q(u)\u2212 \u03b3t\u03c9(u) ]\n+max u\u2208U\n[ \u03b3t\u03c9(u) ] + E\u03be [\u2212f(x, \u03be) + g(xt, \u03be)\u2212 g(x, \u03be)]\n= E\u03be\n[ f\u0302(xt, \u03be, \u03b3t) ] + \u03b3tDU + E\u03be [\u2212f(x, \u03be) + g(xt, \u03be)\u2212 g(x, \u03be)]\n\u2264 E\u03be [ f\u0302(xt, \u03be, \u03b3t)\u2212 f\u0302(x, \u03be, \u03b3t) ]\n+ E\u03be [g(xt, \u03be)\u2212 g(x, \u03be)] + \u03b3tDU . (24)\nThe last inequality is due to the non-negativity of \u03c9() and definitions of f (7) and f\u0302 (8)."}, {"heading": "Appendix B. Proof of Lemma 4", "text": "Before proceeding to the proof of this lemma, we present two auxiliary results. For clarity, in the following lemmas and proofs we use the following notations to denote the smoothly approximated composite function and its expectation:\nFt(x, \u03b3t) := f\u0302t(x) + gt(x) = f\u0302(x, \u03bet, \u03b3t) + g(x, \u03bet) (25)\nand F (x, \u03b3t) := E\u03betFt(x, \u03b3t). (26)\nThe first lemma is on the smoothly approximated function and the smoothness parameter \u03b3t.\nLemma 9 If \u03b3t is monotonically decreasing with t, for any x and t \u2265 0,\nF (x, \u03b3t) \u2264 F (x, \u03b3t+1) \u2264 F (x, \u03b3t) + (\u03b3t \u2212 \u03b3t+1)DU , (27)\nwhere DU := maxu\u2208U \u03c9(u).\nProof The left inequality is obvious, since \u03b3t \u2265 \u03b3t+1 and \u03c9(u) is nonnegative. For the right inequality,\nF (x, \u03b3t+1)\u2212 F (x, \u03b3t) = E\u03bef\u0302(x, \u03be, \u03b3t+1)\u2212 E\u03bef\u0302(x, \u03be, \u03b3t) = max\nu\u2208U [\u3008E\u03beA\u03bex,u\u3009 \u2212Q(u)\u2212 \u03b3t+1\u03c9(u)]\u2212max u\u2208U [\u3008E\u03beA\u03bex,u\u3009 \u2212Q(u)\u2212 \u03b3t\u03c9(u)]\n\u2264 max u\u2208U\n{\n[ \u3008E\u03beA\u03bex,u\u3009 \u2212Q(u)\u2212 \u03b3t+1\u03c9(u) ] \u2212 [ \u3008E\u03beA\u03bex,u\u3009 \u2212Q(u)\u2212 \u03b3t\u03c9(u) ]\n}\n= max u\u2208U [(\u03b3t \u2212 \u03b3t+1)\u03c9(u)] . (28)\nThe second lemma is about proximal methods using Bregman divergence as proxfunctions, which is a direct result of optimality conditions. It appeared in Lan and Ghadimi (2011)(Lemma 2), and is an extension of the \u201c3-point identity\u201d Chen and Teboulle (1993)(Lemma 3.1).\nLemma 10 Lan and Ghadimi (2011) Let l(x) be a convex function. Let scalars s1, s2 \u2265 0. For any vectors u and v, denote their Bregman divergence as D(u,v). If \u2200x,u,v\nx\u2217 = argmin x l(x) + s1D(u,x) + s2D(v,x), (29)\nthen\nl(x) + s1D(u,x) + s2D(v,x) \u2265 l(x\u2217) + s1D(u,x\u2217) + s2D(v,x\u2217) + (s1 + s2)D(x\u2217,x). (30)\nWe are now ready to prove Lemma 4. Proof [Proof of Lemma 4] Due to Lemma 2 and Lipschitz-smoothness of g(x), F (x, \u03b3t+1) has a Lipschitz smooth constant LFt+1 := E\u03be\u2016A\u03be\u20162 \u03b3t+1\u03b6 + Lg. It follows that\nF (xt+1, \u03b3t+1)\n\u2264 F (yt, \u03b3t+1) + \u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt\u3009+ LFt+1 2 \u2016xt+1 \u2212 yt\u20162 = (1\u2212 \u03b1t)F (yt, \u03b3t+1) + \u03b1tF (yt, \u03b3t+1) + \u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt\u3009+ LFt+1 2 \u2016xt+1 \u2212 yt\u20162 = (1\u2212 \u03b1t)F (yt, \u03b3t+1) + \u3008\u2207F (yt, \u03b3t+1), (1 \u2212 \u03b1t)(xt \u2212 yt)\u3009+\n\u03b1tF (yt, \u03b3t+1) + \u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt \u2212 (1\u2212 \u03b1t)(xt \u2212 yt)\u3009+ LFt+1 2 \u2016xt+1 \u2212 yt\u20162\n\u2264 (1\u2212 \u03b1t)F (xt, \u03b3t+1) + \u03b1tF (yt, \u03b3t+1) + \u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt \u2212 (1\u2212 \u03b1t)(xt \u2212 yt)\u3009+ LFt+1 2\n\u2016xt+1 \u2212 yt\u20162, (31)\nwhere the last inequality is due to the convexity of F (). Subtracting F (x, \u03b3t+1) from both sides of the above inequality we have:\nF (xt+1, \u03b3t+1)\u2212 F (x, \u03b3t+1) \u2264 (1\u2212 \u03b1t)F (xt, \u03b3t+1)\u2212 F (x, \u03b3t+1)\n+ \u03b1tF (yt, \u03b3t+1) + \u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt \u2212 (1\u2212 \u03b1t)(xt \u2212 yt)\u3009+ LFt+1 2 \u2016xt+1 \u2212 yt\u20162 \u2264 (1\u2212 \u03b1t) [ F (xt, \u03b3t) + (\u03b3t \u2212 \u03b3t+1)DU ] \u2212 F (x, \u03b3t+1) + \u03b1tF (yt, \u03b3t+1) + \u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt \u2212 (1\u2212 \u03b1t)(xt \u2212 yt)\u3009+ LFt+1 2 \u2016xt+1 \u2212 yt\u20162 \u2264 (1\u2212 \u03b1t) [ F (xt, \u03b3t)\u2212 F (x, \u03b3t) ] \u2212 \u03b1tF (x, \u03b3t+1) + (1\u2212 \u03b1t)(\u03b3t \u2212 \u03b3t+1)DU + \u03b1tF (yt, \u03b3t+1) + \u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt \u2212 (1\u2212 \u03b1t)(xt \u2212 yt)\u3009+ LFt+1 2\n\u2016xt+1 \u2212 yt\u20162, (32)\nwhere the last two inequalities are due to Lemma 9. Denoting \u2206t := F (xt, \u03b3t)\u2212F (x, \u03b3t) and \u03c3t(x) := \u2207Ft(x, \u03b3t)\u2212\u2207F (x, \u03b3t) we can rewrite (32) as:\n\u2206t+1 \u2212 (1\u2212 \u03b1t)\u2206t \u2212 (1\u2212 \u03b1t)(\u03b3t \u2212 \u03b3t+1)DU\n\u2264 \u03b1tF (yt, \u03b3t+1)\u2212 \u03b1tF (x, \u03b3t+1) + \u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt \u2212 (1\u2212 \u03b1t)(xt \u2212 yt)\u3009+ LFt+1 2 \u2016xt+1 \u2212 yt\u20162 (3) \u2264 \u03b1tF (yt, \u03b3t+1)\u2212 \u03b1t [ F (yt, \u03b3t+1) + \u3008\u2207F (yt, \u03b3t+1),x\u2212 yt\u3009+ \u00b5\n2 \u2016x\u2212 yt\u20162\n]\n+\n\u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt \u2212 (1\u2212 \u03b1t)(xt \u2212 yt)\u3009+ LFt+1 2 \u2016xt+1 \u2212 yt\u20162\n= \u2212\u03b1t [ \u3008\u2207Ft+1(yt, \u03b3t+1)\u2212 \u03c3t+1(yt),x\u2212 yt\u3009+ \u00b5\n2 \u2016x\u2212 yt\u20162\n]\n+\n\u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt \u2212 (1\u2212 \u03b1t)(xt \u2212 yt)\u3009+ LFt+1 2 \u2016xt+1 \u2212 yt\u20162\n= \u2212\u03b1t [ \u3008\u2207Ft+1(yt, \u03b3t+1),x\u2212 yt\u3009+ \u00b5\n2 \u2016x\u2212 yt\u20162 + \u03b8t 2 \u2016x\u2212 vt\u20162\n]\n+ \u03b1t\u03b8t 2 \u2016x\u2212 vt\u20162+\n\u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt \u2212 (1\u2212 \u03b1t)(xt \u2212 yt)\u3009+ LFt+1 2 \u2016xt+1 \u2212 yt\u20162 + \u3008\u03c3t+1(yt), \u03b1t(x\u2212 yt)\u3009\n\u2264 \u2212\u03b1t [ \u3008\u2207Ft+1(yt, \u03b3t+1),vt+1 \u2212 yt\u3009+ \u00b5\n2 \u2016vt+1 \u2212 yt\u20162 + \u03b8t 2 \u2016vt+1 \u2212 vt\u20162 + \u00b5+ \u03b8t 2 \u2016x\u2212 vt+1\u20162 ] +\n\u03b1t\u03b8t 2 \u2016x\u2212 vt\u20162 + \u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt \u2212 (1\u2212 \u03b1t)(xt \u2212 yt)\u3009+ LFt+1 2 \u2016xt+1 \u2212 yt\u20162+ \u3008\u03c3t+1(yt), \u03b1t(x\u2212 yt)\u3009, (33)\nwhere the last inequality is due to Lemma 10 (takingD(u,v) = 12\u2016u\u2212v\u20162) and the definition of vt+1:\nvt+1 := argmin x\n\u3008\u2207Ft+1(yt, \u03b3t+1),x\u2212 yt\u3009+ \u00b5\n2 \u2016x\u2212 yt\u20162 + \u03b8t 2 \u2016x\u2212 vt\u20162. (34)\nMinimizing the above directly leads to Line 4 of Alg.1:\nvt+1 = \u03b8tvt + \u00b5yt \u2212\u2207Ft+1(yt, \u03b3t+1)\n\u00b5+ \u03b8t . (35)\nBase on this updating rule, it is easy to verify the following inequality:\n\u2212 \u03b1t [ \u00b5\n2 \u2016vt+1 \u2212 yt\u20162 + \u03b8t 2 \u2016vt+1 \u2212 vt\u20162\n]\n\u2264 \u2212\u03b1t 2\n[\n\u00b5\u03b8t \u00b5+ \u03b8t \u2016vt \u2212 yt\u20162 + 1 \u00b5+ \u03b8t \u2016\u2207Ft+1(yt, \u03b3t+1)\u20162\n]\n\u2264 \u2212\u03b1t 2 (\u00b5+ \u03b8t) \u2016\u2207Ft+1(yt, \u03b3t+1)\u20162.\n(36)\nTo set xt+1 (Line 3 of Alg.1), we follow the classic stochastic gradient descent, such that \u2016xt+1\u2212yt\u20162 can be bounded in terms of \u2016\u2207Ft+1(yt, \u03b3t+1)\u20162: xt+1 = yt\u2212\u03b7t\u2207Ft+1(yt, \u03b3t+1).\nHence\n\u2016xt+1 \u2212 yt\u20162 = \u03b72t \u2016\u2207Ft+1(yt, \u03b3t+1)\u20162, (37)\nand\n\u3008\u2207F (yt, \u03b3t+1),xt+1 \u2212 yt\u3009 = \u3008\u2207Ft+1(yt, \u03b3t+1)\u2212 \u03c3t+1(yt),xt+1 \u2212 yt\u3009 \u2264 \u2212\u03b7t\u2016\u2207Ft+1(yt, \u03b3t+1)\u20162 + \u03b7t\u2016\u03c3t+1(yt)\u2016 \u00b7 \u2016\u2207Ft+1(yt, \u03b3t+1)\u2016.\n(38)\nInserting (35,36,37 and 38) into (33) we have\n\u2206t+1 \u2264 (1 \u2212 \u03b1t)\u2206t + (1\u2212 \u03b1t)(\u03b3t \u2212 \u03b3t+1)DU+ \u03b1t 2 [ \u03b8t\u2016x\u2212 vt\u20162 \u2212 (\u00b5 + \u03b8t)\u2016x\u2212 vt+1\u20162 ] + \u3008\u03c3t+1(yt), \u03b1t(x\u2212 yt) + (1\u2212 \u03b1t)(xt \u2212 yt)\u3009+ \u03b7t\u2016\u03c3t+1(yt)\u2016 \u00b7 \u2016\u2207Ft+1(yt, \u03b3t+1)\u2016+ [\n\u03b1t 2(\u00b5 + \u03b8t) + Lt+1 2 \u03b72t \u2212 \u03b7t ]\n\u2016\u2207Ft+1(yt, \u03b3t+1)\u20162+ \u2329\n\u2207Ft+1(yt, \u03b3t+1), \u2212\u03b1t\u03b8t(vt \u2212 yt)\n\u00b5+ \u03b8t \u2212 (1\u2212 \u03b1t)(xt \u2212 yt)\n\u232a\n.\n(39)\nTaking the last term \u2212\u03b1t\u03b8t(vt\u2212yt)\u00b5+\u03b8t \u2212 (1 \u2212 \u03b1t)(xt \u2212 yt) = 0 recovers the updating rule of yt (Line 1 of Alg.1). Hence our result follows."}, {"heading": "Appendix C. Proof of Theorem 6", "text": "Proof It is easy to verify that by taking \u03b1t = 2 t+2 , \u03b3t+1 = \u03b1t and \u03b8t = Lg\u03b1t+ E\u2016A\u03be\u20162 \u03b6 + \u2126\u221a \u03b1t , we have \u2200t > 1: (1\u2212 \u03b1t\u22121)(\u03b3t\u22121 \u2212 \u03b3t) \u2264 \u03b3t \u2212 \u03b3t+1, (40)\nand\n(1\u2212 \u03b1t) \u03b1t\u22121 2(\u03b8t\u22121 \u2212 \u03b1t\u22121ELt) \u2264 \u03b1t 2(\u03b8t \u2212 \u03b1tELt+1) . (41)\nNext we define and bound weighted sums of D2t that will be used later.\n\u03a8(t) := [\u03b1t\u03b8t \u2212 (1\u2212 \u03b1t)\u03b1t\u22121\u03b8t\u22121]D2t + (1\u2212 \u03b1t) [\u03b1t\u22121\u03b8t\u22121 \u2212 (1\u2212 \u03b1t\u22121)\u03b1t\u22122\u03b8t\u22122]D2t\u22121+ (1\u2212 \u03b1t)(1\u2212 \u03b1t\u22121) [\u03b1t\u22122\u03b8t\u22122 \u2212 (1\u2212 \u03b1t\u22122)\u03b1t\u22123\u03b8t\u22123]D2t\u22122 + \u00b7 \u00b7 \u00b7 ,\n(42)\nwhere replacing \u03b1t and \u03b8t by their definitions we have \u2200t:\n\u03b1t\u03b8t\u2212 (1\u2212\u03b1t)\u03b1t\u22121\u03b8t\u22121 = 4Lg\n(t+ 1)2(t+ 2)2 + 2E\u2016A\u03be\u20162/\u03b6 (t+ 1)(t+ 2) +\n\u221a 2 [ (t+ 1) \u221a t+ 2\u2212 t \u221a t+ 1 ] \u2126\n(t+ 1)(t+ 2) (43)\nSubstituting (43) into (42) and using invoking the definition of D2 we have \u2200t:\n\u03a8(t) \u2264 4LgD2 [\n1\n(t+ 1)2(t+ 2)2 +\nt(t+ 1)\n(t+ 1)(t+ 2)\n1\nt2(t+ 1)2 + (t\u2212 1)t (t+ 1)(t+ 2)\n1 (t\u2212 1)2t2 + \u00b7 \u00b7 \u00b7 ]\n+ 2E\u2016A\u03be\u20162D2\n\u03b6\n[\n1\n(t+ 1)(t+ 2) +\nt(t+ 1)\n(t+ 1)(t+ 2)\n1\nt(t+ 1) + (t\u2212 1)t (t+ 1)(t+ 2)\n1 (t\u2212 1)t + \u00b7 \u00b7 \u00b7 ]\n+ \u221a 2\u2126D2 [\n(t+ 1) \u221a t+ 2\u2212 t \u221a t+ 1\n(t+ 1)(t+ 2) +\nt(t+ 1)\n(t+ 1)(t+ 2)\nt \u221a t+ 1\u2212 (t\u2212 1) \u221a t\nt(t+ 1) +\n(t\u2212 1)t (t+ 1)(t + 2)\n(t\u2212 1) \u221a t\u2212 (t\u2212 2) \u221a t\u2212 1\n(t\u2212 1)t + \u00b7 \u00b7 \u00b7 ]\n= 4LgD\n2\n(t+ 1)(t + 2)\n[(\n1 t+ 1 \u2212 1 t+ 2\n)\n+\n(\n1 t \u2212 1 t+ 1\n)\n+\n(\n1 t\u2212 1 \u2212 1 t\n) + \u00b7 \u00b7 \u00b7 ]\n+ 2E\u2016A\u03be\u20162D2\n\u03b6\n[\n1\n(t+ 1)(t+ 2) +\n1\n(t+ 1)(t+ 2) +\n1\n(t+ 1)(t + 2) + \u00b7 \u00b7 \u00b7\n]\n+\n\u221a 2\u2126D2\n(t+ 1)(t + 2)\n[ (t+ 1) \u221a t+ 2\u2212 t \u221a t+ 1 + t \u221a t+ 1\u2212 (t\u2212 1) \u221a t+ (t\u2212 1) \u221a t\u2212 (t\u2212 2) \u221a t\u2212 1 + \u00b7 \u00b7 \u00b7 ]\n\u2264 \u03b1t\u03b8tD2. (44)\nSince \u00b5 = 0, by recursively applying (13) and 1\u2212 \u03b10 = 0 we have\nE\u2206t+1 \u2264 (1\u2212 \u03b1t)E\u2206t + \u03b1t\u03b8t ( D2t \u2212D2t+1 )\n+ \u03b1t\n2(\u03b8t \u2212 \u03b1tELt+1) \u03c32 + (1\u2212 \u03b1t)(\u03b3t \u2212 \u03b3t+1)DU\n\u2264 (1\u2212 \u03b1t)(1\u2212 \u03b1t\u22121)E\u2206t\u22121 + \u03b1t\u03b8t ( D2t \u2212D2t+1 ) + (1\u2212 \u03b1t)\u03b1t\u22121\u03b8t\u22121 ( D2t\u22121 \u2212D2t ) +\n2\u03b1t 2(\u03b8t \u2212 \u03b1tELt+1) \u03c32 + 2(1 \u2212 \u03b1t)(\u03b3t \u2212 \u03b3t+1)DU\n\u2264 \u00b7 \u00b7 \u00b7 (42) \u2264 t \u220f\ni=0\n(1\u2212 \u03b1i)\u22060 +\u03a8(t) + (t+ 1)\u03b1t\n2(\u03b8t \u2212 \u03b1tELt+1) \u03c32 + (t+ 1)(1 \u2212 \u03b1t)(\u03b3t \u2212 \u03b3t+1)DU\n(44) \u2264 \u03b1t\u03b8tD2 +\n\u03c32\n\u03b8t \u2212 \u03b1tELt+1 + 2DU t+ 2\n= [ \u03b12tELt+1 +\u2126 \u221a \u03b1t ] D2 +\n\u221a \u03b1t\u03c3 2\n\u2126 + 2DU t+ 2 .\n(45)\nCombining with Lemma 3 we have \u2200x\nE [\u03a6(xt+1)\u2212 \u03a6(x)] \u2264 [ \u03b12tELt+1 +\u2126 \u221a \u03b1t ] D2 +\n\u221a \u03b1t\u03c3 2\n\u2126 + 2DU t+ 2 + \u03b3t+1DU\n\u2264 \u03b12tLgD2 + ( \u03b3t+1 + 2\nt+ 2\n)\nDU + \u03b1 2 t E\u2016A\u03be\u20162 \u03b3t+1\u03b6 D2 + \u221a \u03b1t ( \u2126D2 + \u03c32 \u2126 ) .\n(46)\nTaking \u03b3t+1 = \u03b1t = 2\nt+2 our result follows."}, {"heading": "Appendix D. Proof of Theorem 7", "text": "Proof It is easy to verify that by taking \u03b1t = 2 t+1 , we have \u2200t \u2265 1\n(1\u2212 \u03b1t\u22121)(\u03b3t\u22121 \u2212 \u03b3t) \u2264 \u03b3t \u2212 \u03b3t+1. (47)\nand\n(1\u2212 \u03b1t)\u03b12t\u22121 \u2264 \u03b12t (48)\nDenote\nSt := \u03b1t\u03b8t \u2212 (1\u2212 \u03b1t)(\u03b1t\u22121\u03b8t\u22121 + \u00b5\u03b1t\u22121). (49)\nTaking \u03b8t = Lg\u03b1t + \u00b5 2\u03b1t\n+ E\u2016A\u03be\u20162\n\u03b6 \u2212 \u00b5 it is easy to verify that \u2200t \u2265 1:\nSt = 4Lg 1\n(t+ 1)2t2 + 2E\u2016A\u03be\u20162 \u03b6 [ 1 t \u2212 1 t+ 1 ] \u2212 \u00b5 t+ 1 . (50)\nWe want to find the smallest iteration index C such that: when t \u2265 C, St \u2264 0. Without any knowledge about Lg and E\u2016A\u03be\u20162, minimizing St w.r.t t does not yield an analytic form of C. Hence we simply let\n4Lg 1 (t+ 1)2t2 \u2264 \u00b5 2(t+ 1) , (51)\nand\n2E\u2016A\u03be\u20162 \u03b6 [ 1 t \u2212 1 t+ 1 ] \u2264 \u00b5 2(t+ 1) . (52)\nInequality (51) is satisfied when\nt \u2265 2 ( Lg \u00b5 )1/3 , (53)\nand (52) is satisfied when\nt \u2265 4E\u2016A\u03be\u2016 2\n\u03b6\u00b5 . (54)\nCombining these two we reach the definition of C in (15). Next we proceed to prove the bound.\nAs defined in the theorem, we denote D\u03032 = max0\u2264i\u2264min(t,C)D 2 i . By recursively applying\n(13) for 0 \u2264 i \u2264 t and noticing that St \u2264 0 \u2200t \u2265 C, 1\u2212 \u03b11 = 0 we have\nE\u2206t+1 (47) \u2264\nt \u220f\ni=0 (1\u2212 \u03b1i)\u22060 + (t+ 1)(1 \u2212 \u03b1t)(\u03b3t \u2212 \u03b3t+1)DU+ [\n(\u03b1t\u03b8t)D 2 t \u2212 (\u03b1t\u03b8t + \u00b5\u03b1t)D2t+1\n]\n+\n(1\u2212 \u03b1t) [ (\u03b1t\u22121\u03b8t\u22121)D 2 t\u22121 \u2212 (\u03b1t\u22121\u03b8t\u22121 + \u00b5\u03b1t\u22121)D2t ] + (1\u2212 \u03b1t)(1 \u2212 \u03b1t\u22121) [ (\u03b1t\u22122\u03b8t\u22122)D 2 t\u22122 \u2212 (\u03b1t\u22122\u03b8t\u22122 + \u00b5\u03b1t\u22122)D2t\u22121 ] + \u00b7 \u00b7 \u00b7+ t \u220f\ni=1\n(1\u2212 \u03b1i) [ (\u03b10\u03b80)D 2 0 \u2212 (\u03b10\u03b80 + \u00b5\u03b10)D21 ] +\n\u03c32\n\u00b5\n[\n\u03b12t + (1\u2212 \u03b1t)\u03b12t\u22121 + \u00b7 \u00b7 \u00b7+ t \u220f\ni=1\n(1\u2212 \u03b1i)\u03b120\n]\n(48) \u2264 2DU\nt+ 1 + D\u03032\nt \u220f\ni=C\u22121 (1\u2212 \u03b1i) [\u03b1C\u22122\u03b8C\u22122 \u2212 (1\u2212 \u03b1C\u22122)(\u03b1C\u22123\u03b8C\u22123 + \u00b5\u03b1C\u22123)] +\nD\u03032 t \u220f\ni=C\u22122 (1\u2212 \u03b1i) [\u03b1C\u22123\u03b8C\u22123 \u2212 (1\u2212 \u03b1C\u22123)(\u03b1C\u22124\u03b8C\u22124 + \u00b5\u03b1C\u22124)] +\n\u00b7 \u00b7 \u00b7 + D\u03032 t \u220f\ni=2\n(1\u2212 \u03b1i) [\u03b11\u03b81 \u2212 (1\u2212 \u03b11)(\u03b10\u03b80 + \u00b5\u03b10)] + t\u03b12t\u03c3 2\n\u00b5\n(55)\nApplying (50) by ignoring the \u2212 \u00b5t+1 term to the above inequality we can bound the coefficients of Lg and E\u2016A\u03be\u20162 \u03b6 parts separately as follows.\nWhen t \u2265 C, for the Lg part: \u220ft\ni=C\u22121(1\u2212 \u03b1i) (C \u2212 1)2(C \u2212 2)2 + \u220ft i=C\u22122(1\u2212 \u03b1i) (C \u2212 2)2(C \u2212 3)2 + \u220ft i=C\u22123(1\u2212 \u03b1i) (C \u2212 3)2(C \u2212 4)2 + \u00b7 \u00b7 \u00b7+ \u220ft i=2(1\u2212 \u03b1i) 22 \u00b7 12\n= 1\n(t+ 1)t\n[\n1\n(C + 2)(C + 1) +\n1\n(C + 1)C +\n1 C(C \u2212 1)) + \u00b7 \u00b7 \u00b7+ 1 2 \u00b7 1\n]\n\u2264 1 (t+ 1)t\nC+1 \u2211\ni=1\n1 i2 \u2264 \u03c0\n2\n6t(t+ 1)\n(56)\nFor the E\u2016A\u03be\u20162\n\u03b6 part:\n\u03a0ti=C\u22121(1\u2212 \u03b1i) ( 1 C \u2212 2 \u2212 1 C \u2212 1 ) +\u03a0ti=C\u22122(1\u2212 \u03b1i) ( 1 C \u2212 3 \u2212 1 C \u2212 2 ) + \u00b7 \u00b7 \u00b7 + t \u220f\ni=2\n(1\u2212 \u03b1i) ( 1\u2212 1 2 )\n= C \u2212 1 (t+ 1)t \u2212 C \u2212 2 (t+ 1)t + C \u2212 2 (t+ 1)t \u2212 C \u2212 3 (t+ 1)t + \u00b7 \u00b7 \u00b7+ 2 (t+ 1)t \u2212 1 (t+ 1)t = C \u2212 1 (t+ 1)t \u2212 1 (t+ 1)t = C \u2212 2 t(t+ 1) .\n(57)\nCombining with Lemma 3 and taking \u03b3t+1 = \u03b1t = 2 t+1 we have \u2200x:\nE [\u03a6(xt+1)\u2212 \u03a6(x)] \u2264 2DU t+ 1 + 2\u03c02LgD\u0303 2 3t(t+ 1) + 2(C \u2212 2)E\u2016A\u03be\u20162D\u03032/\u03b6 t(t+ 1) + \u03c32 \u00b5(t+ 1) + \u03b3t+1DU\n= 2\u03c02LgD\u0303 2\n3t(t+ 1) + 2(C \u2212 2)E\u2016A\u03be\u20162D\u03032/\u03b6 t(t+ 1) + 4DU t+ 1 + \u03c32 \u00b5(t+ 1) .\n(58)\nWhen 0 \u2264 t \u2264 C, one can simply put C = t in the above, and this completes our proof."}, {"heading": "Appendix E. Proof of Proposition 8", "text": "Proof\nE\u03be[t] R(t) = E\u03be[t]\nt\u22121 \u2211\ni=0\n[ \u03a6(xi, \u03bei+1)\u2212 \u03a6(x\u2217t , \u03bei+1) ]\n= E\u03be[t]\nt\u22121 \u2211\ni=0\n{\n[ \u03a6(xi, \u03bei+1)\u2212 \u03a6(x\u2217) ] + [ \u03a6(x\u2217)\u2212 \u03a6(x\u2217t , \u03bei+1) ]\n}\n=\nt\u22121 \u2211\ni=0\nE\u03be[i+1]\n[ \u03a6(xi, \u03bei+1)\u2212 \u03a6(x\u2217) ] + E\u03be[t]\nt\u22121 \u2211\ni=0\n[\u03a6(x\u2217)\u2212 \u03a6(x\u2217t )] + E\u03be[t] t\u22121 \u2211\ni=0\n[ \u03a6(x\u2217t )\u2212 \u03a6(x\u2217t , \u03bei+1) ]\n\u2264 t\u22121 \u2211\ni=0\nE\u03be[i+1]\n[ \u03a6(xi, \u03bei+1)\u2212 \u03a6(x\u2217) ] + E\u03be[t]\nt\u22121 \u2211\ni=0\n[ \u03a6(x\u2217t )\u2212 \u03a6(x\u2217t , \u03bei+1) ]\n= t\u22121 \u2211\ni=0\nE\u03be[i] [\u03a6(xi)\u2212 \u03a6(x\u2217)] + E\u03be[t]\nt\u22121 \u2211\ni=0\n[ \u03a6(x\u2217t )\u2212 \u03a6(x\u2217t , \u03bei+1) ] ."}], "references": [{"title": "Theory of classification: A survey of some recent advances", "author": ["St\u00e9phane Boucheron", "Olivier Bousquet", "G\u00e1bor Lugosi"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "Boucheron et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2005}, {"title": "Convergence analysis of a proximal-like minimization algorithm using bregman functions", "author": ["Gong Chen", "Marc Teboulle"], "venue": "SIAM J. on Optimization,", "citeRegEx": "Chen and Teboulle.,? \\Q1993\\E", "shortCiteRegEx": "Chen and Teboulle.", "year": 1993}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Daubechies et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Daubechies et al\\.", "year": 2004}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": null, "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["John Duchi", "Yoram Singer"], "venue": "JMLR, (10):2899\u20132934,", "citeRegEx": "Duchi and Singer.,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer.", "year": 2009}, {"title": "Randomized smoothing for stochastic optimization", "author": ["John Duchi", "Peter L. Bartlett", "Martin J. Wainwright"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In COLT,", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Accelerated gradient methods for stochastic optimization and online learning", "author": ["Chonghai Hu", "James T. Kwok", "Weike Pan"], "venue": "In NIPS 22,", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Robust estimation of a location parameter", "author": ["Peter J. Huber"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Huber.,? \\Q1964\\E", "shortCiteRegEx": "Huber.", "year": 1964}, {"title": "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, i: a generic algorithmic framework", "author": ["G. Lan", "S. Ghadimi"], "venue": "SIAM J. on Optimization,", "citeRegEx": "Lan and Ghadimi.,? \\Q2011\\E", "shortCiteRegEx": "Lan and Ghadimi.", "year": 2011}, {"title": "An optimal method for stochastic composite optimization", "author": ["Guanghui Lan"], "venue": "Mathematical Programming,", "citeRegEx": "Lan.,? \\Q2010\\E", "shortCiteRegEx": "Lan.", "year": 2010}, {"title": "Splitting algorithms for the sum of two nonlinear operators", "author": ["P.L. Lions", "B. Mercier"], "venue": "SIAM J. on Numerical Analysis,", "citeRegEx": "Lions and Mercier.,? \\Q1979\\E", "shortCiteRegEx": "Lions and Mercier.", "year": 1979}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A. Nemirovski", "D. Yudin"], "venue": null, "citeRegEx": "Nemirovski and Yudin.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovski and Yudin.", "year": 1983}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. on Optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Introductory Lectures on Convex Optimization, A Basic Course", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}], "referenceMentions": [{"referenceID": 13, "context": "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning).", "startOffset": 126, "endOffset": 154}, {"referenceID": 13, "context": "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov\u2019s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied.", "startOffset": 126, "endOffset": 506}, {"referenceID": 13, "context": "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov\u2019s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied.", "startOffset": 126, "endOffset": 642}, {"referenceID": 13, "context": "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov\u2019s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied. It converges as f(xt)\u2212minx f(x) \u2264 O(1/t) after t iterations. If a nonsmooth function is strongly convex, this rate can be improved to O(1/t2) using the excessive gap technique Nesterov (2005a). In this paper, we extend Nesterov\u2019s smoothing method to the stochastic setting by proposing a stochastic smoothing method for nonsmooth functions.", "startOffset": 126, "endOffset": 851}, {"referenceID": 13, "context": "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov\u2019s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied. It converges as f(xt)\u2212minx f(x) \u2264 O(1/t) after t iterations. If a nonsmooth function is strongly convex, this rate can be improved to O(1/t2) using the excessive gap technique Nesterov (2005a). In this paper, we extend Nesterov\u2019s smoothing method to the stochastic setting by proposing a stochastic smoothing method for nonsmooth functions. Combining this with a stochastic version of the optimal gradient descent method, we introduce and analyze a new algorithm named Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), for a class of functions that include the popular ML methods of interest. To our knowledge ANSGD is the first stochastic first-order algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions without Polyak\u2019s averaging Polyak and Juditsky (1992). In comparison, the classic SGD converges in O(ln t/t) for 1.", "startOffset": 126, "endOffset": 1464}, {"referenceID": 7, "context": "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak\u2019s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD\u2019s convergence rate still can not be faster than O(ln t/t) Shamir (2011).", "startOffset": 34, "endOffset": 59}, {"referenceID": 7, "context": "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak\u2019s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD\u2019s convergence rate still can not be faster than O(ln t/t) Shamir (2011).", "startOffset": 34, "endOffset": 114}, {"referenceID": 7, "context": "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak\u2019s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD\u2019s convergence rate still can not be faster than O(ln t/t) Shamir (2011).", "startOffset": 34, "endOffset": 125}, {"referenceID": 7, "context": "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak\u2019s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD\u2019s convergence rate still can not be faster than O(ln t/t) Shamir (2011). Numerical experiments on real-world datasets also indicate that ANSGD converges much faster in comparing with these state-of-the-art algorithms.", "startOffset": 34, "endOffset": 224}, {"referenceID": 3, "context": "A perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization Duchi et al. (2011). This work achieves similar iteration complexities as ours, in a parallel computation scenario.", "startOffset": 97, "endOffset": 117}, {"referenceID": 3, "context": "A perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization Duchi et al. (2011). This work achieves similar iteration complexities as ours, in a parallel computation scenario. In serial settings, ANSGD enjoys better and optimal bounds. In machine learning, many problems can be cast as minimizing a composition of a loss function and a regularization term. Before proceeding to the algorithm, we first describe a different setting of \u201ccomposite minimizations\u201d that we will pursue in this paper, along with our notations and assumptions. 1.1 A Different \u201cComposite Setting\u201d In the classic black-box setting of first-order stochastic algorithms Nemirovski et al. (2009), the structure of the objective function minx{f(x) = E\u03bef(x, \u03be) : \u03be \u223c P} is unknown.", "startOffset": 97, "endOffset": 705}, {"referenceID": 3, "context": "A perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization Duchi et al. (2011). This work achieves similar iteration complexities as ours, in a parallel computation scenario. In serial settings, ANSGD enjoys better and optimal bounds. In machine learning, many problems can be cast as minimizing a composition of a loss function and a regularization term. Before proceeding to the algorithm, we first describe a different setting of \u201ccomposite minimizations\u201d that we will pursue in this paper, along with our notations and assumptions. 1.1 A Different \u201cComposite Setting\u201d In the classic black-box setting of first-order stochastic algorithms Nemirovski et al. (2009), the structure of the objective function minx{f(x) = E\u03bef(x, \u03be) : \u03be \u223c P} is unknown. In each iteration t, an algorithm can only access the first-order stochastic oracle and obtain a subgradient f \u2032(x, \u03bet). The basic assumption is that f \u2032(x) = E\u03bef \u2032(x, \u03be) for any x, where the random vector \u03be is from a fixed distribution P . The composite setting (also known as splitting Lions and Mercier (1979)) is an extension of the black-box model.", "startOffset": 97, "endOffset": 1102}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a).", "startOffset": 118, "endOffset": 143}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a).", "startOffset": 118, "endOffset": 169}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.", "startOffset": 118, "endOffset": 187}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.", "startOffset": 118, "endOffset": 252}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.", "startOffset": 118, "endOffset": 276}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.", "startOffset": 118, "endOffset": 301}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al. (2009); Xiao (2010).", "startOffset": 118, "endOffset": 319}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al. (2009); Xiao (2010). A stochastic composite function \u03a6(x) := f(x) + g(x) is the sum of a smooth stochastic convex function f(x) = E\u03bef(x, \u03be) and a nonsmooth (but simple and deterministic) function g().", "startOffset": 118, "endOffset": 332}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al. (2009); Xiao (2010). A stochastic composite function \u03a6(x) := f(x) + g(x) is the sum of a smooth stochastic convex function f(x) = E\u03bef(x, \u03be) and a nonsmooth (but simple and deterministic) function g(). To minimize \u03a6, previous work construct the following model iteratively: \u3008\u2207f(xt, \u03bet),x \u2212 xt\u3009+ 1 \u03b7t D(x,xt) + g(x), (1) where \u2207f(xt, \u03bet) is a gradient, D(\u00b7, \u00b7) is a proximal function (typically a Bregman divergence) and \u03b7t is a stepsize. A successful application of the composite idea typically relies on the assumption that model (1) is easy to minimize. If g() is very simple, e.g. \u2016x\u20161 or the nuclear norm, it is straightforward to obtain the minimum in analytic forms. However, this assumption does not hold for many other applications in machine learning, where many loss functions (not the regularization term, here the nonsmooth g() becomes the nonsmooth loss function) are nonsmooth, and do not enjoy separability properties Wright et al. (2009). This includes important examples such as hinge loss, absolute loss, and \u01eb-insensitive loss.", "startOffset": 118, "endOffset": 1265}, {"referenceID": 15, "context": "In Section 2 we propose to form a smooth stochastic approximation of f(), such that the optimal methods Nesterov (2004) can be applied to attain optimal convergence rates.", "startOffset": 104, "endOffset": 120}, {"referenceID": 15, "context": "The key property of this approximation is: Lemma 1 Nesterov (2005b)(Theorem 1) Function f\u0302(x, \u03b3) is convex and continuously differentiable, and its gradient is Lipschitz continuous with constant Lf\u0302 := \u2016A\u20162 \u03b3\u03b6 , where \u2016A\u2016 := max x,u {\u3008Ax,u\u3009 : \u2016x\u2016 = 1, \u2016u\u2016 = 1}.", "startOffset": 51, "endOffset": 68}, {"referenceID": 15, "context": "This stochastic algorithm is obtained by applying Nesterov\u2019s optimal method to our smooth surrogate function, and thus has a similar form to that of his original deterministic method Nesterov (2004)(p.", "startOffset": 50, "endOffset": 199}, {"referenceID": 3, "context": "This bound is better than that of stochastic gradient descent or stochastic dual averaging Dekel et al. (2010) for minimizing L-Lipschitz smooth functions, whose rate is O ( LD2 0 t + D2 0+\u03c3 2 \u221a t )", "startOffset": 91, "endOffset": 111}, {"referenceID": 0, "context": "Boucheron et al. (2005)): t\u22121 i=1 \u03a6(x \u2217 t )\u2212\u03a6(xt , \u03bei+1) = O( \u221a t).", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Boucheron et al. (2005)): t\u22121 i=1 \u03a6(x \u2217 t )\u2212\u03a6(xt , \u03bei+1) = O( \u221a t). Together with summing up the RHS of (14), we can obtain an O( \u221a t) regret bound. When \u03a6() is strongly convex, the second term in (19) can be bounded using Shalev-Shwartz et al. (2009): t\u22121 i=1 \u03a6(x \u2217 t ) \u2212 \u03a6(xt , \u03bei+1) = O(ln t).", "startOffset": 0, "endOffset": 252}, {"referenceID": 0, "context": "Boucheron et al. (2005)): t\u22121 i=1 \u03a6(x \u2217 t )\u2212\u03a6(xt , \u03bei+1) = O( \u221a t). Together with summing up the RHS of (14), we can obtain an O( \u221a t) regret bound. When \u03a6() is strongly convex, the second term in (19) can be bounded using Shalev-Shwartz et al. (2009): t\u22121 i=1 \u03a6(x \u2217 t ) \u2212 \u03a6(xt , \u03bei+1) = O(ln t). Together with summing up the RHS of (16), an O(ln t) regret bound is achieved. The O( \u221a t) and O(ln t) regret bounds are known Using our proposed ANSGD for online learning by B-to-O achieves the same (optimal) regret bounds as state-of-the-art algorithms designated for online learning. However, using O-to-B, one can only retain an O(ln t/t) rate of convergence for stochastic strongly convex optimization. From this perspective, O-to-B is inferior to B-to-O. The sub-optimality of O-to-B is also discussed in Hazan and Kale (2011). 4.", "startOffset": 0, "endOffset": 830}, {"referenceID": 6, "context": "2 Absolute Loss Robust Regression Absolute loss is an alternative to the popular squared loss for robust regressions Hastie et al. (2009). Using same notations as Sec.", "startOffset": 117, "endOffset": 138}, {"referenceID": 9, "context": "This approximation looks similar to the well-studied Huber loss Huber (1964), though they are different.", "startOffset": 53, "endOffset": 77}, {"referenceID": 10, "context": "This approach Lan (2010); Lan and Ghadimi (2011) is interesting to compare because like ANSGD, it is another way of obtaining a stochastic algorithm based on Nesterov\u2019s optimal method, begging the question of whether it has similar behavior.", "startOffset": 14, "endOffset": 25}, {"referenceID": 10, "context": "This approach Lan (2010); Lan and Ghadimi (2011) is interesting to compare because like ANSGD, it is another way of obtaining a stochastic algorithm based on Nesterov\u2019s optimal method, begging the question of whether it has similar behavior.", "startOffset": 26, "endOffset": 49}, {"referenceID": 10, "context": "This approach Lan (2010); Lan and Ghadimi (2011) is interesting to compare because like ANSGD, it is another way of obtaining a stochastic algorithm based on Nesterov\u2019s optimal method, begging the question of whether it has similar behavior. Theoretically, according to Prop.8 and 9 in Lan and Ghadimi (2011), the bound for the nonsmooth part is of O(1/ \u221a t) for \u03bc = 0 and O(1/t) for \u03bc > 0.", "startOffset": 26, "endOffset": 309}, {"referenceID": 9, "context": "It appeared in Lan and Ghadimi (2011)(Lemma 2), and is an extension of the \u201c3-point identity\u201d Chen and Teboulle (1993)(Lemma 3.", "startOffset": 15, "endOffset": 38}, {"referenceID": 1, "context": "It appeared in Lan and Ghadimi (2011)(Lemma 2), and is an extension of the \u201c3-point identity\u201d Chen and Teboulle (1993)(Lemma 3.", "startOffset": 94, "endOffset": 119}, {"referenceID": 1, "context": "It appeared in Lan and Ghadimi (2011)(Lemma 2), and is an extension of the \u201c3-point identity\u201d Chen and Teboulle (1993)(Lemma 3.1). Lemma 10 Lan and Ghadimi (2011) Let l(x) be a convex function.", "startOffset": 94, "endOffset": 163}], "year": 2012, "abstractText": "In this work we consider the stochastic minimization of nonsmooth convex loss functions, a central problem in machine learning. We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs. It is the first stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions (with strong convexity). The fast rates are confirmed by empirical comparisons, in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD.", "creator": "LaTeX with hyperref package"}}}