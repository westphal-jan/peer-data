{"id": "1205.4810", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2012", "title": "Safe Exploration in Markov Decision Processes", "abstract": "uncertain environments under simulated dynamics exploration contestants observed to learn optimal to execute well. existing reinforcement learning algorithms specify autonomous exploration scenarios, assuming they tend to rely for an accurate assumption. the essence of ergodicity is that uncertainty state information eventually reachable amongst thy other agent by following a suitable regression. this assumption ensures for exploration and whereby operate by hypothesis favoring biological events have rarely subsequently visited before. for most physical systems elimination strategy is impractical as the systems would break before any environmental exploration becomes settled place, i. e., complex cognitive approaches know't enter the ergodicity situation. in this paper we address reducing need on safe decisions implemented in markov decision preparation. you too propose a general characterization combining safety - ergodicity. outcomes show efficient imposing safety by costly optimization to whichever resulting set of guaranteed environmental policies costs np - hard. iteration both present an efficient algorithm for completing safe, but potentially suboptimal, exploration. these further core is an optimization formulation in whereby the dynamics restrict attention including determining subset of the secure safe policies : the procedure favors protective policies. our framework is compatible onto some majority british previously proposed sampling efforts, which rely on standard exponential process. our constraints, which include a martian terrain exploration trajectory, suggested that our method is able to aim better than classical exploration methods.", "histories": [["v1", "Tue, 22 May 2012 06:02:09 GMT  (724kb,D)", "https://arxiv.org/abs/1205.4810v1", null], ["v2", "Sat, 30 Jun 2012 09:17:38 GMT  (722kb,D)", "http://arxiv.org/abs/1205.4810v2", null], ["v3", "Fri, 6 Jul 2012 20:56:23 GMT  (725kb,D)", "http://arxiv.org/abs/1205.4810v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["teodor mihai moldovan", "pieter abbeel"], "accepted": true, "id": "1205.4810"}, "pdf": {"name": "1205.4810.pdf", "metadata": {"source": "META", "title": "Safe Exploration in Markov Decision Processes", "authors": ["Teodor Mihai Moldovan", "Pieter Abbeel"], "emails": ["moldovan@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"], "sections": [{"heading": null, "text": "In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don\u2019t satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s)."}, {"heading": "1. Introduction", "text": "When humans learn to control a system, they naturally account for what we think of as safety. For example, when a novice pilot learns how to fly an RC helicopter, they will slowly spin up the blades until the helicopter barely lifts off, then quickly put it back down. They will repeat this a few times, slowly starting to bring the helicopter a little bit off the ground. When doing so they would try out the cyclic (roll and pitch) and rudder (yaw) control, while\u2014until they have become more skilled\u2014at all times staying low enough that simply shutting it down would still have it land safely. When a driver wants to become skilled at driving on snow, they might first slowly drive the car to a wide open space where they could start pushing their limits. When we are skiing downhill, we are careful about not going down a slope into a valley where there is no lift to take us back up.\nOne would hope that exploration algorithms for physical systems would be able to account for safety and have similar behavior naturally emerge. Unfortunately most existing exploration algorithms completely ignore safety issues. More precisely phrased, most existing algorithms have strong exploration guarantees, but to achieve these guarantees they assume ergodicity of the Markov decision process (MDP) in which the exploration takes place. An MDP is ergodic if any state is reachable from any other state by following a suitable policy. This assumption does not hold true in the exploration examples presented above as each of these systems could break during (non-safe) exploration.\nOur first important contribution is a definition of safety, which, at its core, requires restricting attention to policies that preserve ergodicity with some well controlled probability. Imposing safety is, unfortunately, NP-hard in general. Our second important contribution is an approximation scheme leading to guaranteed safe, but potentially sub-optimal, exploration.1 A third contribution is the consideration of\n1Note that existing (unsafe) exploration algorithms are\nar X\niv :1\n20 5.\n48 10\nv3 [\ncs .L\nG ]\n6 J\nul 2\n01 2\nuncertainty in the dynamics model that is correlated over states. While usually the assumption is that uncertainty in different parameters is independent\u2014as this makes problem more tractable computationally\u2014 being able to learn about state-action pairs before visiting them is critical for safety.\nOur experiments illustrate that our method indeed achieves safe exploration, in contrast to plain exploration methods. They also show that our algorithm is almost as computationally efficient as planning in a known MDP\u2014but then, as every step leads to an update in knowledge about the MDP, this computation is to be repeated after every step. Our approach is able to safely explore grid worlds of size up to 50 100. Our method can make safe any type of exploration that relies on exploration bonuses, which is the case for most existing exploration algorithms, including, for example, the methods proposed in (Brafman & Tennenholtz, 2001; Kolter & Ng, 2009). In this article we do not focus on the exploration objective and use existing ones.\nSafe exploration has been the focus of a large number of articles. (Gillula & Tomlin, 2011; Aswani & Bouffard, 2012) propose safe exploration methods for linear systems with bounded disturbances based on model predictive control and reachability analysis. They define safety in terms of safe regions of the state space, which, we will show, is not always appropriate in the context of MDPs. The safe exploration for MDP methods proposed by (Geramifard et al., 2011; Hans et al., 2008) gauge safety based on the best best estimate of the transition measure but they ignore the level of uncertainty in this estimate. As we will show, this is not sufficient to provably guarantee safety.\nProvably efficient exploration is a recurring theme in reinforcement learning (Strehl & Littman, 2005; Li et al., 2008; Brafman & Tennenholtz, 2001; Kearns & Singh, 2002; Kolter & Ng, 2009). Most methods, however, tend to rely on the assumption of ergodicity which rarely holds in interesting practical examples; consequently, these methods are rarely applicable for physical systems. The issue of provably guaranteed safety, or risk aversion, under uncertainty in the MDP parameters has also been studied in the reinforcement literature. In (Nilim & El Ghaoui, 2005) they propose a robust MDP control method assuming the transition frequencies are drawn from an orthogonal convex set by an adversary. Unfortunately, it seems impossible to use their method to constrain some safety objective while optimizing a different exploration objective.\nalso sub-optimal, in that they are not guaranteed to complete exploration in the minimal number of time steps.\nIn (Delage & Mannor, 2007) they present a safe exploration algorithm for the special case of Gaussian distributed ambiguity in the reward and state-actionstate transition probabilities, but their safety guarantees are only accurate if the ambiguity in the transition model is small."}, {"heading": "2. Notation and Assumptions", "text": "Due to space constraints, we will not give a general introduction to Markov decision processes (MDPs). For an introduction to MDPs we refer the readers to (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996).\nWe use capital letters to denote random variables; for example, the total reward is: V := \u2211\u221e t=0RSt,At . We represent the policies and the initial state distributions by probability measures. Usually the measure \u03c0 will correspond to a policy and the measure s := \u03b4(s), which puts measure only in state s, will correspond to starting in state s. With this notation, the usual value recursion, assuming a known transition measure, p, reads:\nEps,\u03c0[V ] = \u2211 a,s\u2032 \u03c0s,a ( E[R]s,a + ps,a,s\u2032E p s\u2032,\u03c0[V ] ) .\nWe specify the transition measure as a superscript of the expectation operator rather than a subscript for typographical convenience; in this case, and in general, the positioning of indexes as subscripts or superscripts adds no extra significance. We will let the transition measure p sometimes sum to less than one, that is \u2211 s\u2032 ps,a,s\u2032 \u2264 1. The missing mass is implicitly assigned to transitioning to an absorbing \u201cend\u201d state, which, for example, allows us to model \u03b3 discounting by simply using \u03b3p as a transition measure.\nWe model ambiguous dynamics in a Bayesian way, allowing the transition measure to also be a random variable. When this is the case, we will use P to denote the, now random, transition measure. The belief, which we will denote by \u03b2, is our Bayesian probability measure over possible dynamics, governing P and R. Therefore, the expected return under the belief and policy \u03c0, starting from state s, is E\u03b2E P s,\u03c0[V ]. We allow beliefs under which transition measures and rewards are arbitrarily correlated. In fact, such correlations are usually necessary to allow for safe exploration. For compactness we will often use lower case letters to denote the expectation of their upper case counterparts. Specifically, we will use the notations p := E\u03b2 [P ] and r := E\u03b2 [R] throughout."}, {"heading": "3. Problem formulation", "text": ""}, {"heading": "3.1. Exploration Objective", "text": "Exploration methods, as those proposed in (Brafman & Tennenholtz, 2001; Kolter & Ng, 2009), operate by finding optimal policies in constructed MDPs with exploration bonuses. The R-max algorithm, for example, constructs an MDP based on the discounted expected transition measure and rewards under the belief, and adds a deterministic exploration bonus equal to the maximum possible reward in the MDP, \u03be\u03b2s,a = rmax, to any transitions that are not sufficiently well known. Our method allows adding safety constraints to any such exploration methods. Henceforth, we will restrict attention to such exploration methods, which can be formalized as optimization problems of the form:\nmaximize \u03c0o E \u03b3p s0,\u03c0o \u221e\u2211 t=0 ( rSt,At + \u03be \u03b2 St,At ) . (1)"}, {"heading": "3.2. Safety Constraint", "text": "The issue of safety is closely related to ergodicity. Almost all proposed exploration techniques presume ergodicity; authors present it as a harmless technical assumption but it rarely holds in interesting practical problems. Whenever this happens, their efficient exploration guarantees cease to hold, often leading to very inefficient policies. Informally, an environment is ergodic if any mistake can be forgiven eventually. More specifically, a belief over MDPs is ergodic if and only if any state is reachable from any other state via some policy or, equivalently, if and only if:\n\u2200s, s\u2032,\u2203 \u03c0r such that E\u03b2EPs,\u03c0r [Bs\u2032 ] = 1, (2)\nwhere Bs\u2032 is an indicator random variable of the event that the system reaches state s\u2032 at least once: Bs\u2032 = 1{\u2203t <\u221e such that St = s\u2032} = min (1, \u2211 t 1St=s\u2032).\nUnfortunately, many environments are not ergodic. For example, our robot helicopter learning to fly cannot recover on its own after crashing. Ensuring almost sure ergodicity is too restrictive for most environments as, typically, there always is a very small, but nonzero, chance of encountering that particularly unlucky sequence of events that breaks the system. Our idea is to restrict the space of eligible policies to those that preserve ergodicity with some user-specified probability, \u03b4, called the safety level. We name these policies \u03b4-safe. Safe exploration now amounts to choosing the best exploration policy from this set of safe policies.\nInformally, if we stopped a \u03b4-safe policy \u03c0o at any time T , we would be able to return from that point to the\nhome state s0 with probability \u03b4 by deploying a return policy \u03c0r. Executing only \u03b4-safe policies in the case of a robot helicopter learning to fly will guarantee that the helicopter is able to land safely with probability \u03b4 whenever we decide to end the experiment. In this example, T is the time when the helicopter is recalled (perhaps because fuel is running low), so we will call T the recall time. Formally, an outbound policy \u03c0o is \u03b4-safe with respect to a home state s0 and a stopping time T if and only if:\n\u2203\u03c0r such that E\u03b2EPs0,\u03c0o [ EPST ,\u03c0r [Bs0 ] ] \u2265 \u03b4. (3)\nNote that, based on Equation (2), any policy is \u03b4-safe for any \u03b4 if the MDP is ergodic with probability one under the belief. For convenience we will assume that the recall time, T , is exponentially distributed with parameter 1 \u2212 \u03b3, but our method also applies when the recall time equals some deterministic horizon. Unfortunately, expressing the set of \u03b4-safe policies is NPhard in general, as implied by the following theorem proven in the appendix.\nTheorem 1. In general, it is NP-hard to decide whether there exist \u03b4-safe policies with respect to a home state, s0, and a stopping time, T , for some belief, \u03b2."}, {"heading": "3.3. Safety Counter-Examples", "text": "We conclude this section with counter-examples to three other, perhaps at first sight more intuitive, definitions of safety. First, we could have tried to define safety in terms of sets of safe states or state-actions. That is, we might think that making the non-safe states and actions unavailable to the planner (or simply inaccessible) is enough to guarantee safety. Figure 1 shows an MDP where the same state-action is used both by a safe and by an unsafe policy. The idea behind this counter-example is that safety depends not only on the states visited, but also on the number of visits, thus, on the policy. This shows that safety should be defined in terms of safe policies, not in terms of safe states or state-actions.\nSecond, we might think that it is perhaps enough to ensure that there exists a return policy for each potential sample MDP from the belief, but not impose that it be the same for all samples. That is, we might think that condition 3 is too strong and, instead, it would be enough to have:\nE\u03b21{\u2203\u03c0r : EPs0,\u03c0oE P ST ,\u03c0r [Bs0 ] = 1} \u2265 \u03b4.\nFigure 2 shows an MDP where this condition holds, yet all policies are naturally unsafe.\nThird, we might think that it is sufficient to simply use the expected transition measure when defining safety, as in the equation below. Figure 3 shows that this is not the case; the expected transition measure is not a sufficient statistic for safety.\n\u2203\u03c0r such that Eps0,\u03c0o [ EpST ,\u03c0r [Bs0 ] ] \u2265 \u03b4."}, {"heading": "4. Guaranteed Safe, Potentially Sub-optimal Exploration", "text": "Although imposing the safety constraint in Equation (3) is NP-hard, as shown in Theorem 1, we can efficiently constrain a lower bound on the safety objective, so the safety condition is still provably satisfied. Doing so could lead to sub-optimal exploration since the set of policies we are optimizing over has shrunk. However, we should keep in mind that the exploration objectives represent approximate solutions to other NP-hard problems, so optimality has already been forfeited in existing (non-safe) approaches to start out\nAlgorithm 1 Safe exploration algorithm\nRequire: prior belief \u03b2, discount \u03b3, safety level \u03b4. Require: function \u03be : belief \u2192 exploration bonus M,N \u2190 new MDP objects repeat s0, \u03d5\u2190 current state and observations update belief \u03b2 with information \u03d5 \u03be\u03b2s,a \u2190 \u03be(\u03b2) (exploration bonus based on \u03b2) \u03c3\u03b2s,a \u2190 \u2211 s\u2032 E\u03b2 [min(0, Ps,a,s\u2032 \u2212 E\u03b2 [Ps,a,s\u2032 ])]\nM.transition measure\u2190 E\u03b2 [P ](1\u2212 1s=s0) M.reward function\u2190 1s=s0 + (1\u2212 1s=s0)\u03c3\u03b2s,a \u03c0r, v \u2190M.solve() N.transition measure\u2190 \u03b3E\u03b2 [P ] N.reward function\u2190 E\u03b2 [Rs,a] + \u03be\u03b2s,a N.constraint reward func.\u2190 (1\u2212 \u03b3)vs + \u03b3\u03c3\u03b2s,a N.constraint lower bound\u2190 \u03b4 \u03c0o, v\u03be, v\u03c3 \u2190 N.solve under constraint() q\u03c3s,a \u2190 (1\u2212 \u03b3)vs + \u03b3\u03c3\u03b2s,a + \u2211 s\u2032 ps,a,s\u2032v \u03c3 s\u2032 a\u2190 argmax{\u03c0os0,a>0} q \u03c3 s0,a (de-randomize policy)\ntake action a in environment until \u03be\u03b2 = 0, so there is nothing left to explore\nwith. Algorithm 1 summarizes the procedure and the experiments presented in the next section show that, in practice, when the ergodicity assumptions are violated, safe exploration is much more efficient than plain exploration.\nPutting together the exploration objective defined in Equation (1) and the safety objective defined in Equation (3) allows us to formulate safe exploration at level \u03b4 as a constrained optimization problem:\nmaximize \u03c0o,\u03c0r E \u03b3p s0,\u03c0o \u2211 t ( rSt,At + \u03be \u03b2 St,At ) such that: E\u03b2E P s0,\u03c0o [ EPST ,\u03c0r [Bs0 ] ] \u2265 \u03b4.\nThe exploration objective is already conveniently formulated as the expected reward in an MDP with transition measure \u03b3p, so we will not modify it. On the other hand, the safety constraint is difficult to deal with as is. Ideally, we would like the safety constraint to also equal some expected reward in an MDP. We will see that, in fact, it takes two MDPs to express the safety constraint.\nFirst, we express the inner term, EPST ,\u03c0r [Bs0 ], as the expected reward in an MDP. We can replicate the behaviour of Bs0 , that is counting only the first time state s0 is reached, by using a new transition measure, P \u00b7 (1 \u2212 1s=s0) under which, once s0 is reached, any further actions lead immediately to the implicit \u201cend\u201d\nstate. Formally, we express this by the identity:\nEPST ,\u03c0r [Bs0 ] = E P \u00b7(1\u22121s=s0 ) ST ,\u03c0r \u221e\u2211 t=0 1St=s0 .\nWe now focus on the outer term, EPs0,\u03c0o [ EPST ,\u03c0r [Bs0 ] ] . Since the recall time, T , is exponentially distributed with parameter 1\u2212\u03b3, we can view ST as the final state in a \u03b3-discounted MDP starting at state s0, following policy \u03c0o. In this MDP, the inner term plays the role of a terminal reward. To put the problem in a standard form, we convert this terminal reward to a step-wise reward by multiplying it by 1\u2212 \u03b3. EPs0,\u03c0o [ EPST ,\u03c0r [Bs0 ] ] = E\u03b3Ps0,\u03c0o\n\u221e\u2211 t=0 (1\u2212 \u03b3) [ EPSt,\u03c0r [Bs0 ] ] .\nAt this point, we have expressed the safety constraint in the MDP formalism, but the transition measures of these MDPs, P (1 \u2212 1s=s0) and \u03b3P , are still random. If we could replace these random transition measures with their expectations under the belief \u03b2 that would significantly simplify the safety constraint. It turns out we can do this, at the expense of making the constraint more stringent. Our tool for doing so is Theorem 2 presented below, but proven in the appendix. It shows that we can replace a belief over MDPs by a single MDP with the expected transition measure, featuring an appropriate reward correction such that, under any policy, the value of this MDP is a lower bound on the expected value under the belief.\nTheorem 2. Let \u03b2 be a belief such that for any policy, \u03c0, and any starting state, s, the total expected reward in any MDP drawn from the belief is between 0 and 1; i.e. 0 \u2264 EPs,\u03c0[V ] \u2264 1, \u03b2-almost surely. Then the following bound holds for any policy, \u03c0, and any starting state, s:\nE\u03b2E P s,\u03c0 \u221e\u2211 t=0 RSt,At \u2265 Eps,\u03c0 \u221e\u2211 t=0 ( E\u03b2 [RSt,At ] + \u03c3 \u03b2 St,At ) where \u03c3\u03b2s,a :=\n\u2211 s\u2032 E\u03b2 [min(0, Ps,a,s\u2032 \u2212 E\u03b2 [Ps,a,s\u2032 ])] .\nWe first apply Theorem 2 to the outer term, yielding the following bound: EPs0,\u03c0o [ EPST ,\u03c0r [Bs0 ] ] = E\u03b3Ps0,\u03c0o\n\u221e\u2211 t=0 (1\u2212 \u03b3) [ EPSt,\u03c0r [Bs0 ] ] \u2265 E\u03b3ps0,\u03c0o\n\u221e\u2211 t=0 ( (1\u2212 \u03b3)E\u03b2EPSt,\u03c0r [Bs0 ] + \u03b3\u03c3 \u03b2 St,At ) .\nWe, then, apply it again to the inner term:\nE\u03b2E P s,\u03c0r [Bs0 ] = E P \u00b7(1\u22121s=s0 ) s,\u03c0r \u221e\u2211 t=0 1St=s0 \u2265 (4)\n\u2265 Ep\u00b7(1\u22121s=s0 )s,\u03c0r \u221e\u2211 t=0 ( 1St=s0 + (1\u2212 1St=s0)\u03c3 \u03b2 St,At ) .\nCombining the last two results allows us to replace the NP-hard safety constraint with a stricter, but now tractable, constraint. The resulting optimization problem corresponds to the guaranteed safe, but potentially sub-optimal exploration problem:\nmaximize \u03c0o,\u03c0r E \u03b3p s0,\u03c0o \u2211 t ( rSt,At + \u03be \u03b2 St,At ) (5)\ns.t.: E\u03b3ps0,\u03c0o \u221e\u2211 t=0 ( (1\u2212 \u03b3)vSt + \u03b3\u03c3 \u03b2 St,At ) \u2265 \u03b4 and\nvs = E p\u00b7(1\u22121s=s0 ) s,\u03c0r \u221e\u2211 t=0 ( 1St=s0 + (1\u2212 1St=s0)\u03c3 \u03b2 St,At ) .\nThe term vs represents our lower bound for the inner term per Equation (4), and is simply the value function of the MDP corresponding to the inner term; i.e. the MDP with transition measure p(1\u22121s=s0) and reward function 1s=s0 +(1\u22121s=s0)\u03c3\u03b2s,a, under policy \u03c0r. Since the return policy, \u03c0r, does not appear anywhere else, we can split the safe exploration problem we obtained in Equation (5) into two steps:\nStep one: find the optimal return policy \u03c0\u2217r , and corresponding value function v\u2217s , by solving the standard MDP problem below:\nE p\u00b7(1\u22121s=s0 ) s,\u03c0r \u221e\u2211 t=0 ( 1St=s0 + (1\u2212 1St=s0)\u03c3 \u03b2 St,At ) .\nStep two: find the optimal exploration policy \u03c0\u2217o under the strict safety constraint, by solving the constrained MDP problem below:\nmaximize \u03c0o E \u03b3p s0,\u03c0o \u2211 t ( rSt,At + \u03be \u03b2 St,At ) s.t.: E\u03b3ps0,\u03c0o\n\u221e\u2211 t=0 ( (1\u2212 \u03b3)v\u2217St + \u03b3\u03c3 \u03b2 St,At ) \u2265 \u03b4.\nThe first step amounts to solving a standard MDP problem while the second step amounts to solving a constrained MDP problem. As shown by (Altman, 1999), both can be solved efficiently either by linear programming, or by value-iteration. In our experiments we used the LP formulation with the stateaction occupation measure as optimization variable.\nSolutions to the constrained MDP problem will usually be stochastic policies, and, in our experiments, we found that following them sometimes leads to random walks which explore inefficiently. We addressed the issue by de-randomizing the exploration policies in favor of safety. That is, whenever the stochastic policy proposes multiple actions with non-zero measure, we choose the one among them that optimizes the safety objective."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Grid World", "text": "Our first experiment models a terrain exploration problem where the agent has limited sensing capabilities. We consider a simple rectangular grid world, where every state has a height Hs. From our Bayesian standpoint these heights are independent, uniformly distributed categorical random variables on the set\n{1, 2, 3, 4, 5}. At any time the agent can attempt to move to any immediately neighboring state. Such move will succeed with probability one if the height of the destination state is no more than one level above the current state; otherwise, the agent remains in the current state with probability one. In other words, the agent can always go down cliffs, but is unable to climb up if they are too steep. Whenever the agent enters a new state it can see the exact heights of all immediately surrounding states. We present this grid world experiment to build intuition and to provide an easily reproducible result. Figure 4 shows a number of examples where our exploration method results in intuitively safe behavior, while plain exploration methods lead to clearly unsafe, suboptimal behavior.\nOur exploration scheme, which we call adapted R-max, is a modified version of R-max exploration (Brafman & Tennenholtz, 2001), where the exploration bonus of moving between two states is now proportional to the\nnumber of neighboring unknown states that would be uncovered as a result of the move, to account for the remote observation model. The safety costs for this exploration setup, as prescribed by Theorem 2 are:\n\u03c3\u03b2s,a = \u22122E\u03b2 [Ps,a](1\u2212 E\u03b2 [Ps,a]) = \u22122Var\u03b2 [Ps,a]\nwhere Ps,a := 1Hs+a\u2264Hs+1 is the probability that attempted move a succeeds in state s and the belief \u03b2 describes the distribution of the heights of unseen states. In practice we found that this correction is a factor of two larger than would be sufficient to give a tight safety bound.\nA somewhat counter intuitive result is that adding safety constraints to the exploration objective will, in fact, improve the fraction of squares explored in randomly generated grid worlds. The reason why plain exploration performs so poorly is that the ergodicity assumptions are violated, so efficiency guarantees no longer hold. Figure 6 in the appendix summarizes our exploration performance results."}, {"heading": "5.2. Martian Terrain", "text": "For our second experiment, we model the problem of autonomously exploring the surface of Mars by a rover such as the Mars Science Laboratory (MSL) (Lockwood, 2006). The MSL is designed to be remote controlled from Earth but communication suffers a latency of 16.6 minutes. At top speed, it could traverse about 20m before receiving new instructions, so it needs to be able to navigate autonomously. In the future, when such rovers become faster and cheaper to deploy, the ability to plan their paths autonomously will become even more important. The MSL is designed to a static stability of 45 degrees, but would only be able to climb slopes up to 5 degrees without slipping (MSL, 2007). Digital terrain models for parts of the surface of Mars are available from the High Resolution Imaging\nScience Experiment (HiRISE) at a scale of 1.00 meter/pixel and accurate to about a quarter of a meter. The MSL would be able to obtain much more accurate terrain models locally by stereo vision.\nThe state-action space of our model MDP is the same as in the previous experiment, with each state corresponding to a square area of 20 by 20 meters on the surface. We allow only transitions at slopes between -45 and 5 degrees. The heights, Hs, are now assumed to be independent Gaussian random variables. Under the prior belief, informed by the HiRISE data, the expected heights and their variances are:\nE\u03b2 [H] = D 20[g \u25e6 h] and\nVar\u03b2 [H] = D 20[g \u25e6 (h\u2212 g \u25e6 h)2] + v0\nwhere h are the HiRISE measurements, g is a Gaussian filter with \u03c3 = 5 meters, \u201c\u25e6\u201d represents image convolution, D20 is the sub-sampling operator and v0 = 2\n\u22124m2 is our estimate of the variance of HiRISE measurements. We model remote sensing by assuming that the MSL can obtain Gaussian noisy measurements of the height at a distance d away with variance v(d) = 10\u22126(d+ 1m)2.\nTo account for this remote sensing model we use a first order approximation of the entropy of H as an exploration bonus:\n\u03be\u03b2s,a = \u2211 s\u2032 Var\u03b2 [Hs\u2032 ]/v(ds,s\u2032).\nFigure 5 shows our simulated exploration results for a 2km by 1km area at \u221230.6 degrees latitude and 202.2 degrees longitude (PSP, 2008). Safe exploration at level 1.0 is no longer possible, but, even at a conservative safety level of .98, our method covers more ground than the regular (unsafe) exploration method which promptly get stuck in a crater. Imposing the safety constraint naively, with respect to the expected\ntransition measure, as argued against at the end of Section 3.3, performs as poorly as unsafe exploration even if the constraint is set at .98."}, {"heading": "5.3. Computation Time", "text": "We implemented our algorithm in Python 2.7.2.7, using Numpy 1.5.1 for dense array manipulation, SciPy 0.9.0 for sparse matrix manipulation and Mosek 6.0.0.119 for linear programming. The discount factor was set to .99 for the grid world experiment and to .999 for Mars exploration. In the latter experiment we also restricted precision to 10\u22126 to avoid numerical instabilities in the LP solver. Table 1 summarizes planning times for our Mars exploration experiments."}, {"heading": "6. Discussion", "text": "In addition to the safety formulation we discussed in Section 3.2, out framework also supports a number of other safety criteria that we did not discuss due to space constraints:\n\u2022 Stricter ergodicity ensuring that return is possible within some horizon, H, not just eventually, with probability \u03b4. \u2022 Ensuring that the probability of leaving some predefined safe set of state-actions is lower than 1\u2212\u03b4. \u2022 Ensuring that the expected total reward under the belief is higher than \u03b4.\nAdditionally, any number and combination of these constraints at different \u03b4-levels can be imposed simultaneously."}, {"heading": "Acknowledgements", "text": "This material is based upon work supported in part by NSF under award IIS-0931463, by ARO under the MAST program, by a Sloan Fellowship, by a gift from Intel, by the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-11-1-0391."}], "references": [{"title": "Constrained Markov Decision Processes", "author": ["Altman", "Eitan"], "venue": null, "citeRegEx": "Altman and Eitan.,? \\Q1999\\E", "shortCiteRegEx": "Altman and Eitan.", "year": 1999}, {"title": "Extensions of Learning-Based Model Predictive Control for Real-Time Application to a Quadrotor Helicopter", "author": ["Aswani", "Anil", "Bouffard", "Patrick"], "venue": "In Proc. American Control Conference (ACC) (to appear),", "citeRegEx": "Aswani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Aswani et al\\.", "year": 2012}, {"title": "A survey of computational complexity results in systems and control", "author": ["Blondel", "Vincent D", "Tsitsiklis", "John N"], "venue": null, "citeRegEx": "Blondel et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2000}, {"title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning", "author": ["Brafman", "Ronen I", "Tennenholtz", "Moshe"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Brafman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Brafman et al\\.", "year": 2001}, {"title": "Percentile optimization in uncertain Markov decision processes with application to efficient exploration", "author": ["Delage", "Erick", "Mannor", "Shie"], "venue": "ICML; Vol. 227,", "citeRegEx": "Delage et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Delage et al\\.", "year": 2007}, {"title": "UAV Cooperative Control with Stochastic Risk Models", "author": ["A Geramifard", "J Redding", "N Roy", "How", "J P"], "venue": "In Proceedings of the American Control Conference (ACC),", "citeRegEx": "Geramifard et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Geramifard et al\\.", "year": 2011}, {"title": "Guaranteed safe online learning of a bounded system", "author": ["Gillula", "Jeremy H", "Tomlin", "Claire J"], "venue": "In 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Gillula et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gillula et al\\.", "year": 2011}, {"title": "Safe exploration for reinforcement learning", "author": ["A Hans", "D Schneega\u00df", "AM Sch\u00e4fer", "S. Udluft"], "venue": "ESANN", "citeRegEx": "Hans et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hans et al\\.", "year": 2008}, {"title": "Near-Optimal Reinforcement Learning in Polynomial Time", "author": ["Kearns", "Michael", "Singh", "Satinder"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["Kolter", "J. Zico", "Ng", "Andrew Y"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning - ICML", "citeRegEx": "Kolter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2009}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["Li", "Lihong", "Littman", "Michael L", "Walsh", "Thomas J"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Introduction: Mars Science Laboratory: The Next Generation of Mars Landers", "author": ["Lockwood", "Mary Kae"], "venue": "Journal of Spacecraft and Rockets,", "citeRegEx": "Lockwood and Kae.,? \\Q2006\\E", "shortCiteRegEx": "Lockwood and Kae.", "year": 2006}, {"title": "Robust Control of Markov Decision Processes with Uncertain Transition Matrices", "author": ["Nilim", "Arnab", "El Ghaoui", "Laurent"], "venue": "Operations Research,", "citeRegEx": "Nilim et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Nilim et al\\.", "year": 2005}, {"title": "A theoretical analysis of Model-Based Interval Estimation", "author": ["Strehl", "Alexander L", "Littman", "Michael L"], "venue": "In Proceedings of the 22nd international conference on Machine learning - ICML", "citeRegEx": "Strehl et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2005}, {"title": "Reinforcement learning: an introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 5, "context": "The safe exploration for MDP methods proposed by (Geramifard et al., 2011; Hans et al., 2008) gauge safety based on the best best estimate of the transition measure but they ignore the level of uncertainty in this estimate.", "startOffset": 49, "endOffset": 93}, {"referenceID": 7, "context": "The safe exploration for MDP methods proposed by (Geramifard et al., 2011; Hans et al., 2008) gauge safety based on the best best estimate of the transition measure but they ignore the level of uncertainty in this estimate.", "startOffset": 49, "endOffset": 93}, {"referenceID": 10, "context": "Provably efficient exploration is a recurring theme in reinforcement learning (Strehl & Littman, 2005; Li et al., 2008; Brafman & Tennenholtz, 2001; Kearns & Singh, 2002; Kolter & Ng, 2009).", "startOffset": 78, "endOffset": 189}], "year": 2012, "abstractText": "In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don\u2019t satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).", "creator": "LaTeX with hyperref package"}}}