{"id": "1206.1208", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2012", "title": "Cumulative Step-size Adaptation on Linear Functions: Technical Report", "abstract": "the esa - es allows an integer strategy satisfying integer loop size parameter,, the step size is adapted measuring the results approaching a so - been feedback matrix. the posterior path is a combination including the intermediate steps realized once the estimation, and chronological importance of global gradient decreases with time. this article studies the csa - es on composites like curves sized with objective linear scales through the investigation of models underlying markov chains. rigorous inference on the average and inverse variation of observed integration size are derived with and without cumulation. the step - value diverges geometrically fast over most cases. furthermore, the relevance of these cumulation above is studied.", "histories": [["v1", "Wed, 6 Jun 2012 13:03:31 GMT  (171kb,D)", "http://arxiv.org/abs/1206.1208v1", null], ["v2", "Fri, 29 Jun 2012 18:56:20 GMT  (157kb,D)", "http://arxiv.org/abs/1206.1208v2", "Parallel Problem Solving From Nature (2012)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexandre adrien chotard", "anne auger", "nikolaus hansen"], "accepted": false, "id": "1206.1208"}, "pdf": {"name": "1206.1208.pdf", "metadata": {"source": "CRF", "title": "Cumulative Step-size Adaptation on Linear Functions", "authors": ["Alexandre Chotard", "Anne Auger", "Nikolaus Hansen"], "emails": ["firstname.lastname@lri.fr"], "sections": [{"heading": null, "text": "Keywords: CSA, cumulative path, evolution path, evolution strategies, step-size adaptation"}, {"heading": "1 Introduction", "text": "Evolution strategies (ESs) are continuous stochastic optimization algorithms searching for the optimum of a real valued function f : Rn \u2192 R. In the (1, \u03bb)-ES, in each iteration, \u03bb new children are generated from a single parent pointX \u2208 Rn by adding a random Gaussian vector to the parent,\nX \u2208 Rn 7\u2192X + \u03c3N (0,C) .\nHere, \u03c3 \u2208 R\u2217+ is called step-size and C is a covariance matrix. The best of the \u03bb children, according to f , becomes the parent of the next iteration. To achieve reasonably fast convergence, step size and covariance matrix have to be adapted throughout the iterations of the algorithm. In this paper, C is the identity and we investigate the so-called Cumulative Step-size Adaptation (CSA) [11,9]. In CSA, a cumulative path is introduced, which is a combination of all steps the algorithm has made, where the importance of a step decreases exponentially with time. Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].\nIn this paper, we study the behaviour of the (1, \u03bb)-CSA-ES on composites of strictly increasing with affine linear functions, e.g. f : x 7\u2192 exp(x2 \u2212 2). Because the CSAES is invariant under translation, under change of an orthonormal basis (rotation and reflection), and under strictly increasing transformations of the f -value, we investigate, w.l.o.g., f : x 7\u2192 x1. Linear functions model the situation when the current parent is far (here infinitely far) from the optimum of a smooth function. To be far from the\nar X\niv :1\n20 6.\n12 08\nv1 [\ncs .L\nG ]\n6 J\nun 2\n01 2\noptimum means that the distance to the optimum is large, relative to the step-size \u03c3. This situation is undesirable and threatens premature convergence. The situation should be handled well, by increasing step widths, by any search algorithm. Solving linear functions is also useful to prove convergence independently of the initial state on more general function classes.\nIn Section 2 we introduce the (1, \u03bb)-CSA-ES, and some of its characteristics on linear functions. In Sections 3 and 4 we study ln(\u03c3t) without and with cumulation, respectively. Section 5 presents an analysis of the variance of the logarithm of the stepsize and in Section 6 we summarize our results.\nNotations In this paper, we denote t the iteration or time index, n the search space dimension,N (0, 1) a standard normal distribution, i.e. a normal distribution with mean zero and standard deviation 1. The multivariate normal distribution with mean vector zero and covariance matrix identity will be denoted N (0, In), the ith order statistic of \u03bb standard normal distributionsNi:\u03bb, and \u03a8i:\u03bb its distribution. If x = (x1, \u00b7 \u00b7 \u00b7 , xn) \u2208 Rn is a vector, then [x]i will be its value on the i\nth dimension, that is [x]i = xi. A random variableX distributed according to a law L will be denotedX \u223c L."}, {"heading": "2 The CSA-ES", "text": "We denote withXt the parent at the tth iteration. From the parent pointXt, \u03bb children are generated: Y t,i =Xt+\u03c3t\u03bet,i with i \u2208 [[1, \u03bb]], and \u03bet,i \u223cN (0, In), (\u03bet,i)i\u2208[[1,\u03bb]] i.i.d. Due to the (1, \u03bb) selection scheme, from these children, the one minimizing the function f is selected: Xt+1 = argmin{f(Y ),Y \u2208 {Y t,1, ...,Y t,\u03bb}}. This latter equation implicitly defines the random variable \u03be?t as\nXt+1 =Xt + \u03c3t\u03be ? t . (1)\nIn order to adapt the step-size, the cumulative path is defined as pt+1 = (1\u2212 c)pt + \u221a c(2\u2212 c) \u03be?t (2)\nwith 0 < c \u2264 1. The constant 1/c represents the life span of the information contained in pt, as after 1/c generations pt is multiplied by a factor that approaches 1/e \u2248 0.37 for c\u2192 0 from below (indeed (1\u2212c)1/c \u2264 exp(\u22121)). The typical value for c is between 1/ \u221a n and 1/n. We will consider that p0 \u223c N (0, In) as it makes the algorithm easier to analyze. The normalization constant \u221a c(2\u2212 c) in front of \u03be?t in Eq. (2) is chosen so that under random selection and if pt is distributed according to N (0, In) then also pt+1 follows N (0, In). Hence the length of the path can be compared to the expected length of \u2016N (0, In)\u2016 representing the expected length under random selection.\nThe step-size update rule increases the step-size if the length of the path is larger than the length under random selection and decreases it if the length is shorter than under random selection:\n\u03c3t+1 = \u03c3t exp\n( c\nd\u03c3\n( \u2016pt+1\u2016 E(\u2016N (0, In)\u2016) \u2212 1 ))\nwhere the damping parameter d\u03c3 determines how much the step-size can change and is set to d\u03c3 = 1. A simplification of the update considers the squared length of the path [4]:\n\u03c3t+1 = \u03c3t exp\n( c\n2d\u03c3\n( \u2016pt+1\u20162 n \u2212 1 )) . (3)\nThis rule is easier to analyse and we will use it throughout the paper.\nPreliminary results on linear functions. Selection on the linear function, f(x) = [x]1, is determined by [Xt]1 + \u03c3t [\u03be ? t ]1 \u2264 [Xt]1 + \u03c3t [ \u03bet,i ] 1\nfor all i which is equivalent to [\u03be?t ]1 \u2264 [ \u03bet,i ] 1 for all i where by definition [ \u03bet,i ] 1\nis distributed according to N (0, 1). Therefore the first coordinate of the selected step is distributed according to N1:\u03bb and all others coordinates are distributed according to N (0, 1), i.e. selection does not bias the distribution along the coordinates 2, . . . , n. Overall we have the following result.\nLemma 1. On the linear function f(x) = x1, the selected steps (\u03be?t )t\u2208N of the (1, \u03bb)ES are i.i.d. and distributed according to the vector \u03be := (N1:\u03bb,N2, . . . ,Nn) where Ni \u223c N (0, 1) for i \u2265 2.\nBecause the selected steps \u03be?t are i.i.d. the path defined in Eq. 2 is an autonomous Markov chain, that we will denote P = (pt)t\u2208N. Note that if the distribution of the selected step would depend on (Xt, \u03c3t) as it is generally the case on non-linear functions, then the path alone would not be a Markov Chain, however (Xt, \u03c3t,pt) would be an autonomous Markov Chain. In order to study whether the (1, \u03bb)-CSA-ES diverges geometrically, we investigate the log of the step-size change, whose formula can be immediately deduced from Eq. 3:\nln ( \u03c3t+1 \u03c3t ) = c 2d\u03c3 ( \u2016pt+1\u20162 n \u2212 1 )\n(4)\nBy summing up this equation from 0 to t\u2212 1 we obtain\n1 t ln ( \u03c3t \u03c30 ) = c 2d\u03c3 ( 1 t t\u2211 k=1 \u2016pk\u20162 n \u2212 1 ) . (5)\nWe are interested to know whether 1t ln(\u03c3t/\u03c30) converges to a constant. In case this constant is positive this will prove that the (1, \u03bb)-CSA-ES diverges geometrically. We recognize thanks to (5) that this quantity is equal to the sum of t terms divided by t that suggests the use of the law of large numbers to prove convergence of (5). We will start by investigating the case without cumulation c = 1 (Section 3) and then the case with cumulation (Section 4)."}, {"heading": "3 Divergence rate of CSA-ES without cumulation", "text": "In this section we study the (1, \u03bb)-CSA-ES without cumulation, i.e. c = 1. In this case, the path always equals to the selected step, i.e. for all t, we have pt+1 = \u03be ? t . We have proven in Lemma 1 that \u03be?t are i.i.d. according to \u03be. This allow us to use the standard law of large numbers to find the limit of 1t ln(\u03c3t/\u03c30) as well as compute the expected log-step-size change.\nProposition 1. Let \u2206\u03c3 := 12d\u03c3n ( E ( N 21:\u03bb ) \u2212 1 ) . On linear functions, the (1, \u03bb)-CSA-"}, {"heading": "ES without cumulation satisfies (i) almost surely limt\u2192\u221e 1t ln (\u03c3t/\u03c30) = \u2206\u03c3 , and (ii)", "text": "for all t \u2208 N, E(ln(\u03c3t+1/\u03c3t)) = \u2206\u03c3 .\nProof. We have identified in Lemma 1 that the first coordinate of \u03be?t is distributed according to N1:\u03bb and the other coordinates according to N (0, 1), hence E ( \u2016\u03be?t \u20162 ) =\nE ( [\u03be?t ]1 2 ) + \u2211n i=2 E ( [\u03be?t ] 2 i ) = E ( N 21:\u03bb ) + n \u2212 1. Therefore E ( \u2016\u03be?t \u20162 ) /n \u2212 1 =\n(E ( N 21:\u03bb ) \u2212 1)/n. By applying this to Eq. (4), we deduce that E(ln(\u03c3t+1/\u03c3t) = 1/(2d\u03c3n)(E(N 21:\u03bb) \u2212 1). Furthermore, as E(N 21:\u03bb) \u2264 E((\u03bbN (0, 1))2) = \u03bb2 < \u221e, we have E(\u2016\u03be?t \u20162) < \u221e. The sequence (\u2016\u03be ? t \u20162)t\u2208N being i.i.d according to Lemma 1, and being integrable as we just showed, we can apply the strong law of large numbers on Eq. (5). We obtain\n1 t ln ( \u03c3t \u03c30 ) = 1 2d\u03c3 ( 1 t t\u22121\u2211 k=0 \u2016\u03be?k\u20162 n \u2212 1 ) a.s.\u2212\u2192 t\u2192\u221e 1\n2d\u03c3\n( E ( \u2016\u03be?\u00b7 \u20162 ) n \u2212 1 ) = 1\n2d\u03c3n\n( E ( N 21:\u03bb ) \u2212 1 )\nut The proposition reveals that the sign of ( E ( N 21:\u03bb ) \u2212 1 )\ndetermines whether the stepsize diverges to infinity. In the following, we show that E ( N 21:\u03bb ) increases in \u03bb for \u03bb \u2265 2 and that the (1, \u03bb)-ES diverges for \u03bb \u2265 3. For \u03bb = 1 and \u03bb = 2, the step-size follows a random walk on the log-scale.\nLemma 2. Let (Ni)i\u2208[[1,\u03bb]] be independent random variables, distributed according to N (0, 1), and Ni:\u03bb the ith order statistic of (Ni)i\u2208[[1,\u03bb]]. Then E ( N 21:1 ) = E ( N 21:2 ) =\n1. In addition, for all \u03bb \u2265 2, E ( N 21:\u03bb+1 ) > E ( N 21:\u03bb ) .\nProof. (see [7] for the full proof) The idea of the proof is to use the symmetry of the normal distribution to show that for two random variables U \u223c \u03a81:\u03bb+1 and V \u223c \u03a81:\u03bb, for every event E1 where U2 < V 2, there exists another event E2 counterbalancing the effect of E1, i.e \u222b E2 (u2 \u2212 v2)P ((U, V ) \u2208 duv) = \u222b E1\n(v2 \u2212 u2)P ((U, V ) \u2208 duv). We then have E ( N 21:\u03bb+1 ) \u2265 E ( N 21:\u03bb ) . As there is a non-negligible set of events E3, distinct of E1 and E2, where U2 > V 2, we have E(N 21:\u03bb+1) > E(N 21:\u03bb). For \u03bb = 1,N1:1 \u223c N (0, 1) so E(N 21:1) = 1. For \u03bb = 2 we have E(N 21:2 +N 22:2) = 2E(N (0, 1)2) = 2, and since the normal distribution is symmetric E(N 21:2) = E(N 22:2), hence E(N 21:2) = 1. ut\nWe can now link Proposition 1 and Lemma 2 into the following theorem:\nTheorem 1. On linear functions, for \u03bb \u2265 3, the step-size of the (1, \u03bb)-CSA-ES without cumulation (c = 1) diverges geometrically almost surely and in expectation at the rate 1/(2d\u03c3n)(E(N 21:\u03bb)\u2212 1), i.e.\n1 t ln ( \u03c3t \u03c30 ) a.s.\u2212\u2192 t\u2192\u221e E ( ln ( \u03c3t+1 \u03c3t )) = 1 2d\u03c3n ( E ( N 21:\u03bb ) \u2212 1 ) . (6)\nFor \u03bb = 1 and \u03bb = 2, without cumulation, the logarithm of the step-size does an additive unbiased random walk i.e. ln\u03c3t+1 = ln\u03c3t + Wt where E[Wt] = 0. More preciselyWt \u223c 1/(2d\u03c3)(\u03c72n/n\u22121) for \u03bb = 1, andWt \u223c 1/(2d\u03c3)((N 21:2+\u03c72n\u22121)/n\u2212 1) for \u03bb = 2, where \u03c72k stands for the chi-squared distribution with k degree of freedom.\nProof. For \u03bb > 2, from Lemma 2 we know that E(N 21:\u03bb) > E(N 21:2) = 1. Therefore E(N 21:\u03bb) \u2212 1 > 0, hence Eq. (6) is strictly positive, and with Proposition 1 we get that the step-size diverges geometrically almost surely at the rate 1/(2d\u03c3)(E(N 21:\u03bb)\u2212 1).\nWith Eq. 4 we have ln(\u03c3t+1) = ln(\u03c3t) +Wt, with Wt = 1/(2d\u03c3)(\u2016\u03be?t \u20162/n \u2212 1). For \u03bb = 1 and \u03bb = 2, according to Lemma 2, E(Wt) = 0. Hence ln(\u03c3t) does an additive unbiased random walk. Furthermore \u2016\u03be\u20162 = N 21:\u03bb+\u03c72n\u22121, so for \u03bb = 1, since N1:1 = N (0, 1), \u2016\u03be\u20162 = \u03c72n. ut\nIn [7] we extend this result on the step-size to |[Xt]1|, which diverges geometrically almost surely at the same rate, given E(exp(\u2212(\u2016xi?\u20162/n\u22121)/(2d\u03c3))) < 1 with xi? \u223c \u03be."}, {"heading": "4 Divergence rate of CSA-ES with cumulation", "text": "We are now investigating the (1, \u03bb)-CSA-ES with cumulation, i.e. 0 < c < 1. The path P is then a Markov chain and contrary to the case where c = 1 we cannot apply a LLN for independent variables to Eq. (5) in order to prove the almost sure geometric divergence. However LLN for Markov chains exist as well, provided the Markov chain satisfies some stability properties: in particular, if the Markov chain P is \u03d5-irreducible, that is, there exists a measure \u03d5 such that every Borel set A of Rn with \u03d5(A) > 0 has a positive probability to be reached in a finite number of steps by P starting from any p0 \u2208 Rn. In addition, the chain P needs to be (i) positive, that is the chain admits an invariant probability measure \u03c0, i.e., for any borelian A, \u03c0(A) = \u222b Rn P (x,A)\u03c0(A) with P (x,A) being the probability to transition in one time step from x into A, and (ii) Harris recurrent which means for any borelianA such that \u03d5(A) > 0, the chain P visits A an infinite number of times with probability one. Under those conditions, P satisfies a LLN, more precisely:\nLemma 3. [10, 17.0.1] Suppose that P is a positive Harris chain with invariant probability measure \u03c0, and let g be a \u03c0-integrable function such that \u03c0(|g|) = \u222b Rn |g(x)|\u03c0(dx) <\n\u221e. Then 1/t \u2211t k=1 g(pk) a.s\u2212\u2192 t\u2192\u221e \u03c0(g).\nThe path P satisfies the conditions of Lemma 3 and exhibits an invariant measure [7]. We now obtain geometric divergence of the step-size and get an explicit estimate of the expression of the divergence rate.\nTheorem 2. The step-size of the (1, \u03bb)-CSA-ES with \u03bb \u2265 2 diverges geometrically fast if c < 1 or \u03bb \u2265 3. Almost surely and in expectation we have for 0 < c \u2264 1,\n1 t ln ( \u03c3t \u03c30 ) \u2212\u2192 t\u2192\u221e 1 2d\u03c3n ( 2(1\u2212 c)E (N1:\u03bb)2 + c ( E ( N 21:\u03bb ) \u2212 1 ))\n\ufe38 \ufe37\ufe37 \ufe38 >0 for \u03bb\u22653 and for \u03bb=2 and c<1 . (7)\nProof. For proving almost sure convergence of ln(\u03c3t/\u03c30)/twe need to use the LLN for Markov chain. We refer to [7] for the proof that P satisfies the right assumptions. We now focus on the convergence in expectation. From Eq. (4) we have E(ln(\u03c3t+1/\u03c3t)) = c/(2d\u03c3)(E(\u2016pt+1\u20162)/n\u2212 1), so E(\u2016pt+1\u20162) = E( \u2211n i=1 [ pt+1 ]2 i ) is the term we have to analyse. For i \u2265 2, there is no selection pressure for [\u03be?t ]i, so we are in these dimensions under random selection. Hence, as [p0]i \u223c N (0, 1), [p1]i \u223c N (0, 1) also. By recurrence, we deduce that [pt]i \u223c N (0, 1). Therefore E( \u2211n i=1 [ pt+1 ]2 i ) =\nE( [ pt+1 ]2 1 ) + (n \u2212 1). By recurrence we show that [ pt+1 ] 1 = (1 \u2212 c)t+1[p0]1 +\u221a\nc(2\u2212 c) \u2211t i=0(1\u2212 c)i [ \u03be?t\u2212i ] 1 . When t goes to infinity, the influence of [p0]1 in this\nequation goes to 0 with (1\u2212 c)t+1, so we can remove it when taking the limit:\nlim t\u2192\u221e\nE ([ pt+1 ]2 1 ) = lim t\u2192\u221e E ((\u221a c(2\u2212 c) t\u2211 i=0 (1\u2212 c)i [ \u03be?t\u2212i ] 1 )2) (8)\nWe will now develop the sum with the square, such that we have either a product[ \u03be?t\u2212i ] 1 [ \u03be?t\u2212j ] 1 with i 6= j, or [ \u03be?t\u2212j ]2 1 . This way, we can separate the variables by using Lemma 1 with the independence of \u03be?i over time. To do so, we use the development formula ( \u2211n i=1 an) 2 = 2 \u2211n i=1 \u2211n j=i+1 aiaj + \u2211n i=1 a 2 i . We take the limit of\nE( [ pt+1 ]2 1 ) and find that it is equal to\nlim t\u2192\u221e\nc(2\u2212c) 2 t\u2211 i=0 t\u2211 j=i+1 (1\u2212c)i+j E ([ \u03be?t\u2212i ] 1 [ \u03be?t\u2212j ] 1 ) \ufe38 \ufe37\ufe37 \ufe38\n=E[\u03be?t\u2212i]1E[\u03be ? t\u2212j]1=E[N1:\u03bb] 2\n+ t\u2211 i=0 (1\u2212c)2i E ([ \u03be?t\u2212i ]2 1 ) \ufe38 \ufe37\ufe37 \ufe38\n=E[N 21:\u03bb]  (9)\nNow the expected value does not depend on i or j, so what is left is to calculate\u2211t i=0 \u2211t j=i+1(1 \u2212 c)i+j and \u2211t i=0(1 \u2212 c)2i. We have \u2211t i=0 \u2211t j=i+1(1 \u2212 c)i+j =\u2211t\ni=0(1\u2212c)2i+1 1\u2212(1\u2212c)t\u2212i 1\u2212(1\u2212c) and when we separates this sum in two, the right hand side goes to 0 for t \u2192 \u221e. Therefore, the left hand side converges to limt\u2192\u221e \u2211t i=0(1 \u2212\nc)2i+1/c, which is equal to limt\u2192\u221e(1 \u2212 c)/c \u2211t i=0(1 \u2212 c)2i. And \u2211t i=0(1 \u2212 c)2i is equal to (1\u2212 (1\u2212 c)2t+2)/(1\u2212 (1\u2212 c)2), which converges to 1/(c(2\u2212 c)). So, by inserting this in Eq. (9) we get that E ([ pt+1 ]2 1 ) \u2212\u2192 t\u2192\u221e 2 1\u2212cc E (N1:\u03bb) 2 +E ( N 21:\u03bb ) , which gives us the right hand side of Eq. (7). By summing E(ln(\u03c3i+1/\u03c3i)) for i = 0, . . . , t \u2212 1 and dividing by t we have the Cesaro mean 1/tE(ln(\u03c3t/\u03c30)) that converges to the same value that E(ln(\u03c3t+1/\u03c3t)) converges to when t goes to infinity. Therefore we have in expectation Eq. (7).\nAccording to Lemma 2, for \u03bb = 2, E(N 21:2) = 1, so the RHS of Eq. (7) is equal to (1\u2212c)/(d\u03c3n)E(N1:2)2. The expected value ofN1:2 is strictly negative, so the previous expression is strictly positive. Furthermore, according to Lemma 2, E(N 21:\u03bb) increases with \u03bb, as does E(N1:2)2. Therefore we have geometric divergence for \u03bb \u2265 2. ut\nFrom Eq. (1) we see that the behavior of the step-size and of (Xt)t\u2208N are directly related. Geometric divergence of the step-size, as shown in Theorem 2, means that\nalso the movements in search space and the improvements on affine linear functions f increase geometrically fast. Analyzing (Xt)t\u2208N with cumulation would require to study a double Markov chain, which is left to possible future research."}, {"heading": "5 Study of the variations of ln (\u03c3t+1/\u03c3t)", "text": "The proof of Theorem 2 shows that the step size increase converges to the right hand side of Eq. (7), for t \u2192 \u221e. When the dimension increases this increment goes to zero, which also suggests that it becomes more likely that \u03c3t+1 is smaller than \u03c3t. To analyze this behavior, we study the variance of ln (\u03c3t+1/\u03c3t) as a function of c and the dimension.\nTheorem 3. The variance of ln (\u03c3t+1/\u03c3t) equals to\nVar ( ln ( \u03c3t+1 \u03c3t )) = c2 4d\u03c32n2 ( E ([ pt+1 ]4 1 ) \u2212 E ([ pt+1 ]2 1 )2 + 2(n\u2212 1) ) .\n(10) Furthermore, E ([ pt+1 ]2 1 ) \u2212\u2192 t\u2192\u221e E ( N 21:\u03bb ) + 2\u22122cc E (N1:\u03bb) 2 and with a = 1\u2212 c\nlim t\u2192\u221e\nE ([ pt+1 ]4 1 ) = (1\u2212 a2)2\n1\u2212 a4 (k4 + k31 + k22 + k211 + k1111) , (11)\nwhere k4 = E ( N 41:\u03bb ) , k31 = 4 a(1+a+2a2) 1\u2212a3 E ( N 31:\u03bb ) E (N1:\u03bb), k22 = 6 a 2 1\u2212a2E ( N 21:\u03bb )2 , k211 = 12 a3(1+2a+3a2) (1\u2212a2)(1\u2212a3) E ( N 21:\u03bb ) E (N1:\u03bb)2 and k1111 = 24 a 6 (1\u2212a)(1\u2212a2)(1\u2212a3)E (N1:\u03bb) 4.\nProof.\nV ar ( ln ( \u03c3t+1 \u03c3t )) = Var ( c 2d\u03c3 ( \u2016pt+1\u20162 n \u2212 1 )) = c2 4d2\u03c3n 2 ( Var ( \u2016pt+1\u20162 ))\ufe38 \ufe37\ufe37 \ufe38 E(\u2016pt+1\u20164)\u2212E(\u2016pt+1\u20162) 2 (12) The first part of Var(\u2016pt+1\u20162), E(\u2016pt+1\u20164), is equal to E(( \u2211n i=1 [ pt+1 ]2 i )2). We develop it along the dimensions such that we can use the independence of [pt+1]i with [pt+1]j for i 6= j, to get E(2 \u2211n i=1 \u2211n j=i+1 [ pt+1 ]2 i [ pt+1 ]2 j + \u2211n i=1 [ pt+1 ]4 i ). For i 6=\n1 [ pt+1 ] i is distributed according to a standard normal distribution, so E ([ pt+1 ]2 i ) =\n1 and E ([ pt+1 ]4 i ) = 3. E ( \u2016pt+1\u20164 ) = 2\nn\u2211 i=1 n\u2211 j=i+1 E ([ pt+1 ]2 i ) E ([ pt+1 ]2 j ) + n\u2211 i=1 E ([ pt+1 ]4 i )\n= 2 n\u2211 i=2 n\u2211 j=i+1 1 + 2 n\u2211 j=2 E ([ pt+1 ]2 1 ) + ( n\u2211 i=2 3 ) + E ([ pt+1 ]4 1 )\n= ( 2\nn\u2211 i=2 (n\u2212 i)\n) + 2(n\u2212 1)E ([ pt+1 ]2 1 ) + 3(n\u2212 1) + E ([ pt+1 ]4 1 ) = E ([ pt+1 ]4 1 ) + 2(n\u2212 1)E ([ pt+1 ]2 1 ) + (n\u2212 1)(n+ 1)\nThe other part left is E(\u2016pt+1\u20162)2, which we develop along the dimensions to get E( \u2211n i=1 [ pt+1 ]2 i )2 = (E( [ pt+1 ]2 1 )+ (n\u2212 1))2, which equals to E( [ pt+1 ]2 1 )2 +2(n\u2212\n1)E( [ pt+1 ]2 1 )+(n\u22121)2. So by subtracting both parts we get E(\u2016pt+1\u20164)\u2212E(\u2016pt+1\u20162)2 =\nE( [ pt+1 ]4 1 )\u2212 E( [ pt+1 ]2 1 )2 + 2(n\u2212 1), which we insert into Eq. (12) to get Eq. (10).\nThe development of E( [ pt+1 ]2 1 ) is the same than the one done in the proof of The-\norem 2. We refer to [7] for the development of E( [ pt+1 ]4 1 ), since limits of space in the paper prevents us to present it here. ut\nThe variable [ pt+1 ] 1\nis independent of the dimension (it is the sum of random variables with distribution N1:\u03bb, which is independent of the dimension). Therefore, from Eq. (10) we deduce that the variance of ln(\u03c3t+1/\u03c3t) behaves, for constant c, roughly like 1/n and its standard deviation like 1/ \u221a n.\nThe standard deviation of ln (\u03c3t+1/\u03c3t) divided by its expected value computes to \u221a E [ pt+1 ]4 1 \u2212 E ( [ pt+1 ]2 1 )2 + 2(n\u2212 1) /( E [ pt+1 ]2 1 \u2212 1 ) and increases in \u221a n with the dimension, given c is constant. Figure 1 shows the time evolution of ln(\u03c3t/\u03c30) for 5001 runs and c = 1 (left) and c = 1/ \u221a n (right). By comparing Figure 1a and Figure 1b we observe smaller variations\nof ln(\u03c3t/\u03c30) with the smaller value of c. Figure 2 shows the relative standard deviation of ln (\u03c3t+1/\u03c3t) (i.e. the standard deviation divided by its expected value). Lowering c, as shown in the left, decreases the relative standard deviation. To get a value below one, c must be smaller for larger dimension. In agreement with Theorem 3, in Figure 2, right, the relative standard deviation increases like \u221a n with the dimension for constant c (three increasing curves). For the choice of c \u2264 1/(1 + n1/3), the relative standard deviation appears to converge to 0.5 for n\u2192\u221e. Larger values like c \u2265 1/(1 + n1/4) seem not appropriate."}, {"heading": "6 Summary", "text": "We investigate throughout this paper the (1, \u03bb)-CSA-ES on affine linear functions composed with strictly increasing transformations. We find, in Theorem 2, the limit distribution for ln(\u03c3t/\u03c30)/t and rigorously prove the desired behaviour of \u03c3 with \u03bb \u2265 2: the\nstep-size diverges geometrically fast. In contrast, without cumulation (c = 1) and with \u03bb = 2, a random walk on ln(\u03c3) occurs, like for the (1, 2)-\u03c3SA-ES [8] (and also for the same symmetry reason). We derive an expression for the variance of the step-size increment. On linear functions when c is kept constant and for n \u2192 \u221e, the standard deviation is about \u221a n times larger than the step-size increment. However with c \u2264 1/n1/3, the standard deviation remains below the actual increment, for n\u2192\u221e."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the ANR-2010-COSI-002 grant (SIMINOLE) of the French National Research Agency and the ANR COSINUS project ANR-08-COSI007-12."}], "references": [{"title": "Performance analysis of evolutionary optimization with cumulative step length adaptation", "author": ["D.V. Arnold", "H.-G. Beyer"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "On the behaviour of evolution strategies optimising cigar functions", "author": ["D.V. Arnold", "H.-G. Beyer"], "venue": "Evolutionary Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Cumulative step length adaptation on ridge functions", "author": ["D.V. Arnold"], "venue": "In Parallel Problem Solving from Nature PPSN IX,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Random dynamics optimum tracking with evolution strategies", "author": ["D.V. Arnold", "H.G. Beyer"], "venue": "In Parallel Problem Solving from Nature PPSN VII,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Optimum tracking with evolution strategies", "author": ["D.V. Arnold", "H.G. Beyer"], "venue": "Evolutionary Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Evolution strategies with cumulative step length adaptation on the noisy parabolic ridge", "author": ["D.V. Arnold", "H.G. Beyer"], "venue": "Natural Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Cumulative step-size adaptation on linear functions: Technical report", "author": ["A. Chotard", "A. Auger", "N. Hansen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "An analysis of mutative \u03c3-self-adaptation on linear fitness functions", "author": ["N. Hansen"], "venue": "Evolutionary Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation", "author": ["N. Hansen", "A. Ostermeier"], "venue": "In International Conference on Evolutionary Computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Markov chains and stochastic stability", "author": ["S.P. Meyn", "R.L. Tweedie"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Step-size adaptation based on non-local use of selection information", "author": ["A. Ostermeier", "A. Gawelczyk", "N. Hansen"], "venue": "In Proceedings of Parallel Problem Solving from Nature \u2014 PPSN III,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}], "referenceMentions": [{"referenceID": 10, "context": "In this paper, C is the identity and we investigate the so-called Cumulative Step-size Adaptation (CSA) [11,9].", "startOffset": 104, "endOffset": 110}, {"referenceID": 8, "context": "In this paper, C is the identity and we investigate the so-called Cumulative Step-size Adaptation (CSA) [11,9].", "startOffset": 104, "endOffset": 110}, {"referenceID": 0, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 1, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 2, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 5, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 3, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 180, "endOffset": 183}, {"referenceID": 3, "context": "A simplification of the update considers the squared length of the path [4]:", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "(see [7] for the full proof) The idea of the proof is to use the symmetry of the normal distribution to show that for two random variables U \u223c \u03a81:\u03bb+1 and V \u223c \u03a81:\u03bb, for every event E1 where U < V , there exists another event E2 counterbalancing the effect of E1, i.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "In [7] we extend this result on the step-size to |[Xt]1|, which diverges geometrically almost surely at the same rate, given E(exp(\u2212(\u2016xi\u2016/n\u22121)/(2d\u03c3))) < 1 with xi \u223c \u03be.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "The path P satisfies the conditions of Lemma 3 and exhibits an invariant measure [7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "We refer to [7] for the proof that P satisfies the right assumptions.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "We refer to [7] for the development of E( [ pt+1 ]4 1 ), since limits of space in the paper prevents us to present it here.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "In contrast, without cumulation (c = 1) and with \u03bb = 2, a random walk on ln(\u03c3) occurs, like for the (1, 2)-\u03c3SA-ES [8] (and also for the same symmetry reason).", "startOffset": 114, "endOffset": 117}], "year": 2017, "abstractText": "The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation, where the step size is adapted measuring the length of a so-called cumulative path. The cumulative path is a combination of the previous steps realized by the algorithm, where the importance of each step decreases with time. This article studies the CSA-ES on composites of strictly increasing with affine linear functions through the investigation of its underlying Markov chains. Rigorous results on the change and the variation of the step size are derived with and without cumulation. The step-size diverges geometrically fast in most cases. Furthermore, the influence of the cumulation parameter is studied.", "creator": "LaTeX with hyperref package"}}}