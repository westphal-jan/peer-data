{"id": "1206.3234", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Adaptive Inference on General Graphical Models", "abstract": "discussed topics and applications involve repeatedly searching variations of the same inference problem ; rather illustrate we constantly intend to introduce optimal evidence to the model examining schedule updates showing conditional dependencies. the goal of improving inference is ultimately generate advantage of what events preserved in the computation thus perform algorithm independently properly satisfying random assumptions. in this paper, we define techniques for filtering inference on general graphs that support marginal statistics and updates to the numerical probabilities and dependencies in logarithmic time. us propose experimental materials for an implementation of our example, now demonstrate its potential performance benefit in the study methods dynamic structure.", "histories": [["v1", "Wed, 13 Jun 2012 14:16:36 GMT  (154kb)", "http://arxiv.org/abs/1206.3234v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.DS cs.AI", "authors": ["umut a acar", "alexander t ihler", "ramgopal mettu", "ozgur sumer"], "accepted": false, "id": "1206.3234"}, "pdf": {"name": "1206.3234.pdf", "metadata": {"source": "CRF", "title": "Adaptive Inference on General Graphical Models", "authors": ["Umut A. Acar", "Alexander T. Ihler", "U.C. Irvine", "Ramgopal R. Mettu"], "emails": ["umut@tti-c.org", "ihler@ics.uci.edu", "mettu@ecs.umass.edu", "osumer@cs.uchicago.edu"], "sections": [{"heading": null, "text": "Many algorithms and applications involve repeatedly solving variations of the same inference problem; for example we may want to introduce new evidence to the model or perform updates to conditional dependencies. The goal of adaptive inference is to take advantage of what is preserved in the model and perform inference more rapidly than from scratch. In this paper, we describe techniques for adaptive inference on general graphs that support marginal computation and updates to the conditional probabilities and dependencies in logarithmic time. We give experimental results for an implementation of our algorithm, and demonstrate its potential performance benefit in the study of protein structure."}, {"heading": "1 Introduction", "text": "It is common in many applications to repeatedly perform inference on a variations of essentially the same graphical model. For example, in a number of learning problems we may use observed data to modify a portion of the model (e.g., fitting an observed marginal distribution), and then recompute various moments of the new model before updating the model further [8]. Another example is in the study of protein structures, where a graphical model can be used to represent the conformation space of a protein structure [15, 9]. The maximum-likelihood configuration in this model then corresponds to the minimum-energy conformation for the corresponding protein. An application of interest in this setting is to perform amino acid mutations in the protein to determine the effect of these mutations to the structure and the function of the protein.\nThe changes described in the examples above can, of course, be handled by incorporating them into the model\n\u2217U. A. Acar is supported by a gift from Intel. \u2020 R. R. Mettu is supported by a National Science Foundation\nCAREER Award (IIS-0643768).\nand then performing inference from scratch. However, in general we may wish to assess thousands of potential changes to the model; for example, the number of possible mutations in a protein structure grows exponentially with the number of considered sites. Adaptive inference refers to the problem of handling changes to the model (e.g. to conditional dependencies and even graph structure) more efficiently than performing inference from scratch. Delcher et al. [6] studied this problem under a set of fairly restrictive conditions, requiring that the graph be tree-structured and supporting only changes to the observed evidence in the model. They show that updates to observed evidence may be performed in expected O(log n) time, where n is the size of the graph. More recently, Acar et al. [2] gave a method of supporting more general changes to the model so long as the model remains tree-structured.\nUnfortunately, many graphical models of interest are not trees, but are \u201cloopy\u201d. In principle, we can perform adaptive inference on loopy graphs by construcing their junction tree [13] and applying existing frameworks to the junction tree itself [6, 2]. This approach, however, can be very slow since even a small change to the graph can cause the junction tree to change dramatically, e.g., creating a cycle by inserting a new edge can require a linear number of changes to the junction tree.\nIn this paper, we present techniques for supporting adaptive inference on general graphical models efficiently. Given a factor graph G with n nodes, maximum degree k, and domain size d (variables can take d different values), we require the user to specify a spanning tree T of G. We then construct a (hierarchical) clustering of G with respect to the spanning tree T (Sec. 3). The hierarchical clustering is a tree of clusters where each cluster represents a subgraph of G. A key property of the clustering is that it has expected O(log n) depth, where the expectations are taken over internal randomization. For each cluster we compute a cluster function, a partial marginalization of factors in the cluster. We show that the cluster functions can be computed in O(\u03b1m) where \u03b1 = dk+1 and m is the size of the boundary of the cluster.\nGiven such a hierarchical clustering, we show how to compute the marginal at any variable by performing a traversal from the top level cluster to the variable. Since maximum path length in the clustering is expected O(log n), we show that marginals can be computed in expected O(\u03b1\u03b2 log n) time where \u03b2 is an upper bound on the boundary size of all clusters (for a tree-structured factor graph \u03b2 = 2). The novel contribution of our approach is that our clustering also allows efficient updates to factors and edge insertions/deletions in the input graph. We show that after any of these updates is applied, it is possible to update the clustering O(\u03b1\u03b2 log n) time and that marginals computed thereafter correctly reflect the updates.\nOur results generalize the previous techniques for adaptive inference with tree-structured factor graph to loopy graphs. The main insight is to partition the loopy graph into a spanning tree and a set of non-tree edges and cluster the graph based on the spanning tree only. This enables updating the hierarchical clustering in expected logarithmic time when an edge is inserted or deleted using RC-Trees. When computing marginals, contributions of the nodes of the graph are computed in the order specified by the clustering on the spanning-tree edges. Compared to previous work on factor trees [2], we also simplify marginal computations.\nWe note that our bounds depend exponentially on the boundary size of the clusters. While this exponential cost can be large in general, for many interesting classes of graphs it can be kept small. Moreover, since our expected running times are logarithmic in n, our approach can still be significantly faster than computing from scratch. This exponential factor is not surprising, since exact inference on general graphs is NP-hard; conventional, algorithms for exact inference also have an exponential dependence on some property of the graph such as the tree width.\nTo evaluate the effectiveness of the proposed techniques, we implemented our algorithm and compared its performance against an implementation of sum-product that performs inference on a junction-tree of the given factor graph. Our experiments on a synthetic benchmark for factor graphs show that our approach can be orders of magnitude faster than sum-product. We also investigate the applicability of our algorithm to study protein structure, and show that our algorithm is considerable faster than sumproduct for modeling several moderately-sized proteins."}, {"heading": "2 Background", "text": "Graphical models provide a convenient formalism for describing structure within a function g(X) defined over a set of variables X = [x1, . . . , xn] (most commonly a joint probability distribution or energy function over the xi). Graphical models use this structure to organize computations involving g(\u00b7) and construct efficient algorithms for many inference tasks, including optimization to find a max-\nimum a posteriori (MAP) configuration, marginalizing, or computing the likelihood of observed data. For the purposes of this paper, we assume that each variable xi takes on values from some finite set and focus primarily on the problem of marginalization."}, {"heading": "2.1 Factor Graphs", "text": "Factor graphs [10] describe the factorization structure of the function g(X) using a bipartite graph consisting of factor nodes and variable nodes. Specifically, suppose such a graph G consists of factor nodes F = {f1, . . . , fm} and variable nodes X = {x1, . . . , xn}, and let Xj \u2286 X denote the neighbors of factor node fj . Then, G is said to be consistent with a function g(\u00b7) if and only if\ng(x1, . . . , xn) = \u220f\nj\nfj(Xj).\nIn a common abuse of notation, we have used the same symbols to indicate both each variable node and its associated variable xi, and similarly for each factor node and its associated function fj .\nIt will often be convenient to refer to vertices without specifying whether they are variable or factor nodes. To this end, we define a set of artificial \u201cfactors\u201d to be associated with both factors and variable nodes; for a generic vertex v we define \u03c8v(Xv) \u2261 1 for v = xi, and \u03c8v(Xv) = fj(Xj) for v = fj ."}, {"heading": "2.2 Marginalization", "text": "A classic inference problem is that of marginalizing the function g(X). Specifically, for some or all of the xi, we are interested in computing the marginal function\ngi(xi) = \u2211\nX\\xi\ng(X).\nWhen the factor graph representation of g(X) is singlyconnected (tree-structured), marginalization can be performed efficiently using sum-product [10]. In treestructured graphs, sum-product is typically formulated as a two-pass sequence: rooting the tree at some node v, messages are sent upward (leaves to root), then back downward, after which one may compute the marginal for any node in the graph. In more general graphs (graphs with cycles), exact inference is less straightforward. One solution is to use a junction tree [11]; this first constructs a tree-structured hypergraph of G, then runs essentially the same inference process to compute marginals. The computational complexity of this process depends on the selected hypergraph and is exponential in the size of the cliques, or nodes of the hypergraph.\nAn alternate but essentially equivalent view of exact inference is given by the bucket elimination algorithm [5].\nBucket elimination chooses a sequence in which to marginalize the variables xi, first multiplying together each of the factors which include xi, then summing over xi to create a new factor and returning it to the pool. In treestructured graphs, a marginal function gi(xi) can be found in a manner similar to the upward pass of sum-product: rooting the tree at the node xi of interest, the summation operations are carried out first on the leaf nodes, followed by their parents, and so on until only the root xi remains. However, bucket elimination does not impose any particular elimination order, and we shall see in the sequel that alternative orders may come with other benefits.\nBucket elimination is closely related to junction tree based inference, and an equivalent junction tree may be defined implicitly by its specified elimination ordering [5]."}, {"heading": "2.3 RC-Trees for Adaptive Inference", "text": "In [2], an algorithm for adaptive inference in factor trees is described using \u201crake and compress\u201d trees (RC-trees). The RC-tree data structure automatically selects an elimination ordering for the variables in the factor tree using a randommate selection procedure, and stores functions at each node in the RC-tree representing sufficient statistics for its subtree. It was shown that construction of the RC-tree data structure requires time and space linear in the number of vertices n of the factor graph, and produces a balanced tree with expected height O(log n).\nThe sufficient statistics stored in the RC-tree can be used to \u201cquery\u201d, or compute marginal distributions in the factor tree by passing information downward, taking at most expected O(kdk+2 log n) time, where k is the maximal degree of the factor tree, and d is the maximal dimension of each variable. Moreover, changes to the tree can also be incorporated in expected O(kdk+2 log n) time, including changes to the tree structure. The nature of the randommate elimination ordering ensures that such changes affect only logarithmically many of the sufficient statistics.\nUnfortunately, this formulation is restricted to treestructured factor graphs, which limits its applicability in practice. In the following sections, we describe a generalization of the RC-tree structure which can cope with cycles in the factor graph while maintaining the desireable properties of the automatically chosen elimination ordering."}, {"heading": "3 Hierarchical Clustering and Inference", "text": "We begin by describing a notion of hierarchical clustering in factor graphs which is compatible with but more general than that induced by RC-trees. We then describe how this clustering can be used to compute the marginal distribution at any vertex of the factor graph.\nX\\Xc g \u00b7 h =\n\u2211\nx g(u, v, x) \u00b7 h(x, y)."}, {"heading": "3.1 Hierarchical Clustering", "text": "For a factor graph G = (X + F,E), a cluster C is simply a set of vertices of G. We define the boundary of a cluster, written \u2202C, as a set of edges with exactly one endpoint in C, and the boundary variables XC of C to be the set of variables (variable nodes) incident to the boundary edges. For each cluster, we also define a cluster function \u03d5C as the partial marginalization of all the factors in that cluster over all variables except the boundary variables:\n\u03d5C(XC) = \u2211\nX\\XC\n\u220f\nfj\u2208C\nfj(Xj).\nFig. 1 shows an example cluster, its boundary and boundary variables.\nWe can then define a hierarchical clustering of G to be a set of clusters C = C1, . . . , Cn such that the following conditions are satisfied:\n1. Every vertex is covered by at least one cluster.\n2. Clusters are nested: given two clusters either one is a subset of the other or they do not intersect. Moreover, if two clusters share a boundary edge, one is a subset of the other.\n3. Each cluster C has a unique identifier vertex v : for any C \u2208 C there is a unique v \u2208 C such that no other cluster contained by C contains v. We write v\u0304 to denote the cluster of identified with vertex v, i.e., v\u0304 = C.\n4. For each maximal subcluster C \u2032 of C = v\u0304, i.e., C \u2032\ncontained in no smaller cluster than C, there is an edge connecting v and some u \u2208 C \u2032.\nFig. 2 shows a factor graph and a valid hierarchical clustering of the graph. Note that, by condition 3, the finest scale of the clustering are individual nodes.\nA hierarchical clustering can be constructed bottom-up, by combining groups of sub-clusters which are adjacent to the same vertex. Since clusters are nested, we can represent a hierarchical clustering as a cluster tree, so that if a cluster C \u2032 is a subset of C, then C is an ancestor of C \u2032 in the tree;\nthe maximal subclusters of C are the children of C. A cluster tree representation of the clustering in Fig. 2 is shown in Fig. 3. In the cluster-tree, each cluster is labeled based on its identifier vertex, e.g., the cluster u\u0304 has identifier u. Also shown for each cluster are the boundary edges.\nThe cluster boundaries and their cluster functions can be computed in the cluster tree recursively, based on those of their immediate children. Let Su\u0304 = {v\u03041, . . . , v\u0304k} be the set of children of u\u0304 in the cluster tree, and let E(u) denote the edges containing u as an endpoint. Then, the boundary of u\u0304 is the set of edges that are in exactly one of E(u), \u2202v\u03041, . . . , \u2202v\u0304k, i.e.\n\u2202u\u0304 = E(u)\u25b3\u2202v\u03041\u25b3 . . .\u25b3\u2202v\u0304k\nwhere \u25b3 is the symmetric set difference operator.\nThe cluster function for u\u0304 can be computed as\n\u03d5u\u0304(Xu\u0304) = \u2211\nX\\Xu\u0304\n\u03c8u(Xu) \u220f\nv\u0304\u2208Su\u0304\n\u03d5v\u0304(Xv\u0304).\n(Recall that the \u03c8u simply refer to factors of g(\u00b7).) Any such hierarchical clustering can be used to define a (partial) elimination ordering, with a variable being eliminated in the first (bottom-most) cluster which contains both the variable and all its neighboring factors. In the bucket elimination algorithm following this partial ordering, each cluster function \u03d5C(XC) then corresponds to the \u201cnew factor\u201d created by marginalizing the factors in a given bucket.\nFinally, we will find it useful to partition the edges of G into two sets. In a hierarchical clustering C, at each cluster C = v\u0304 there exists at least one edge from v\u0304 to each of its maximal subclusters C \u2032 (if there is more than one, we can break ties arbitrarily). The collection of these edges form a subtree (or forest) of the original factor graph. We call these edges the \u201ctree\u201d edges ET \u2286 E of the hierarchical clustering; the remaining edges EN = E \\ ET we call the \u201cnon-tree\u201d edges. In Fig. 2, the non-tree edges EN are shown as dashed."}, {"heading": "3.2 Computing Marginal Distributions", "text": "As with bucket elimination, the root of the cluster tree provides the marginal function for whatever variable is removed last. Moreover, it is also straightforward to compute the marginal at any other vertex by propagating information downward through the cluster tree. We compute the marginal distribution of a node v as follows.\nLet \u2202T u\u0304 be the set of tree edges on the boundary of u\u0304, i.e. \u2202T u\u0304 = \u2202u\u0304\u2229ET , and let v1, . . . , vn be the sequence from v\u0304 to the root (v1 = v\u0304, vn the root). We compute a downward pass of marginalization functions from vn to v2 as\nMv\u0304i(\u00b7) = \u2211\nX\\Xv\u0304i\u22121\n\u03c8vi(\u00b7) \u220f\nu\u0304\u2208Av\u0304i\n\u03d5u\u0304(\u00b7) \u220f\na\u0304\u2208Bv\u0304i\nMa\u0304(\u00b7)\nwhere Av\u0304i = Sv\u0304i\\{v\u0304i\u22121} is the set of children of v\u0304i which are not on the path from v\u0304 to the root, and Bv\u0304i defined in terms of the tree edges as follows. If \u2202T v\u0304i\\\u2202T v\u0304i\u22121 = {(a1, a \u2032 1), . . . , (at, a \u2032 t)} with a \u2032 1, . . . , a \u2032 t \u2208 v\u0304i, then Bv\u0304i = {a\u03041, . . . , a\u0304t}. We know by the properties of the hierarchical clustering that each a\u0304i \u2208 Bv\u0304i is an ancestor of v\u0304i in the cluster tree.\nEach of these \u201cmessages\u201d from parent v\u0304i to child v\u0304i\u22121 is computed using only information on (messages into) the path above v\u0304i. The marginal at node v is computed as\ngv(Xv) = \u2211\nX\\Xv\n\u03c8v(\u00b7) \u220f\nu\u0304\u2208Sv\u0304\n\u03d5u\u0304(\u00b7) \u220f\na\u0304\u2208Bv\u0304\nMa\u0304(\u00b7),\ncombining the information above and below v\u0304.\nIn the previous work [2], the combination of G being treestructured and the selection criteria for creating clusters via rake or compress operations ensured that the computational complexity of each of these calculations was limited. For graphs with cycles, we shall see that these computations may grow more complex (due to the additional \u201cnon-tree\u201d edges), but are still bounded and can be controlled sufficiently well to yield practically useful algorithms."}, {"heading": "4 A Cluster Tree Data Structure", "text": "In this section, we describe a data structure for computing marginal distributions and performing various changes to the structure of the graphical model efficiently.\nThe idea behind our data structure is to maintain a balanced clustering of a factor graph. To do this, we require the user provide a factor graph along with a spanning tree (or forest) for that graph. We then build a hierarchical clustering of the factor graph, in which the specified spanning tree defines the tree edges ET of the clustering. Using this representation, we can perform marginal queries in time proportional to the depth of the cluster tree and to the size of the cluster functions stored at each node.\nTo compute and maintain a balanced clustering, we use the RC-Tree (Rake-and-Compress) tree data structure [1, 3]. This data structure constructs a hierarchical clustering of a tree by performing rake and compress operations and guarantees that the clustering has an expected depth of O(log n) in the size of the tree. The RC-Tree itself mimics the structure of the clustering: each node is a cluster and there is an edge from a cluster/node to its immediate subclusters. Thus, it enables traversing the clustering like an ordinary tree. In addition to these operations, RC-Trees enable inserting and deleting tree edges and updating the hierarchical clustering so that it remains balanced under any change to the underlying tree.\nSince we work with general factor graphs, however, the RC-Tree representation itself does not suffice (RC-Trees are sufficient only for tree-structured factor graphs). To extend the representation, we follow the techniques described in Sec. 3 for computing the boundaries and cluster functions. More specifically, after building the clustering and its RC-Tree, we annotate each cluster with its set of boundary edges, including both tree and non-tree edges, and compute its cluster function as a partial marginalization of its factors over all variables except those on the boundary.\nWith an RC-tree annotated with boundaries and cluster functions, we can query the data structure to compute marginal functions in the manner described in Sec. 3.2.\nTo support changes to the underlying structure efficiently, we explicitly distinguish between tree edges and non-tree edges and we require that the spanning tree is kept consistent under changes. This requires, for example, that the user does not delete a spanning tree edge unless the graph becomes disconnected (i.e., there cannot be non-tree edges crossing the cut defined by that tree edge). In other words, the user is responsible for ensuring that the connectivity of the tree-edges matches the connectivity of the factor graph as a whole. This approach makes our interface somewhat crowded, but there is a reason: we wish to provide complete control to the user about the particular spanning tree being maintained, since this is crucial to performance (as\nwe describe in Sec. 4.1). We note that distinguishing between tree and non-tree edges places no restrictions as to what changes can be performed, and the user can still insert and delete any edge. We simply require that if a tree edge is to be removed, it be replaced by another tree edge (perhaps by promoting a non-tree edge) unless its two endpoints are not connected via any other path. We handle changes to the structure of the factor graph as follows.\nReplacing a factor: To replace a factor f , we first change it in the input factor graph. We then find the cluster f\u0304 that identifies f in the RC-Tree and update all cluster functions on the path from f\u0304 to the root. Since each cluster function depends only on its subclusters, this sequence of updates suffices.\nInsert/delete non-tree edges: Let (u, f) be the non-tree edge being inserted or deleted. We first insert/delete (u, f) into/from the input factor graph. We then find the clusters u\u0304 and f\u0304 in the RC-Tree and visit their ancestors in a bottom-up traversal. When visiting a cluster, we update its boundary edges, which may now need to be changed to exclude (u, f) and recompute its cluster function based on its changed boundary. Since only ancestors of u\u0304 and v\u0304 may have (u, v) as a boundary edge, updating only the ancestors suffices.\nInsert/Delete tree edges: Let (u, f) be the tree edge being inserted or deleted. We first insert/delete (u, f) into/from the factor graph as requested. We then insert/delete (u, f) from the spanning tree and use the change-propagation method supplied by the RC-Tree to update the clustering [1]. Change-propagation will update the RC-Tree by deleting some of the existing clusters and inserting some new clusters. We compute the boundaries and the cluster functions for newly created clusters by starting at the root(s) of the RCTree(s) involved in the operation and performing a top-down traversal until we visit all new clusters. It is a property of the RC-Tree data structure that all new clusters can be found in this way.\nWe note that it is for simplicity of presentation that we assume operations consisting of only single changes\u2014 multiple changes can be performed simultaneously."}, {"heading": "4.1 Interface and Efficiency", "text": "We briefly describe the concrete interface to our data structure and analyze the running time for these operations.\nThe interface supports the following operations: cluster(G,T), query(v), replaceFactor(old, new), insertTreeEdge(e), deleteTreeEdge(e), insertTreeEdge(e), deleteNonTreeEdge(e). The cluster operation takes a factor graph G and a spanning tree T of G and constructs hierarchical clustering. The query operation takes a vertex\nof the factor graph and returns the marginal of the vertex. The replaceFactor operation replaces a factor with another factor. The rest of the operations insert or delete edges in the input factor graph.\nTo analyze the efficiency of our data structure, we define a notion of the measure of a factor graph and its spanning tree. Let G be a factor graph and T a spanning tree; we first define the measure of an edge e \u2208 T , written \u00b5T (e), as one plus the maximum size of the number of non-tree edges that cross a cut defined by e. More precisely, for an edge e from T , let Te and T \u2032e be the components of T separated by deletion of e. Let Ge and G\u2032e be the subgraphs of G induced by the vertices of Te and T \u2032e respectively. Then \u00b5T (e) is the size of the cut between Ge and G\u2032e. The measure of G with respect to T , written \u00b5T (G), is the maximum-sized cut over all edges in T .\nThe importance of this measure is that it helps bound the size of the boundary for a cluster: if the number of tree edges that belong to the boundary of a cluster is b, then the boundary size is at most b \u00b7 \u00b5T (G). Since we use tree contraction to construct the cluster tree, our clusters have at most two tree edges in their boundary. Thus, the boundary of any of our clusters is at most 2\u00b5T (G).\nFig. 4 shows a pairwise graphical model (top), with factors omitted (one for each edge), and two different spanning trees for it (middle and bottom) with spanning tree edges are highlighted. The factor graph has measure 3 with respect to the first spanning tree because removing any tree edge results in a cut of size at most 3. For example, for the edge (4, d) the cut size is 3\u2014it separates d from the graph, which has two incident non-tree edges. Other vertical tree edges behave equivalently, and for the horizontal tree edges, the cut size is two. Thus for the first spanning tree the measure of the graph is small. For the second spanning tree, however, the measure is large. In particular, re-\nmoving the edge (8, h) separates the graph into two components consisting of the vertices at the top and those at the bottom with 8 cross edges. This example can be generalized to n nodes such that the measure with respect to this kind of a spanning tree is n/2.\nBy allowing the user to choose the particular spanning tree being used, our data structure allows the measure of the graph to be kept small. This is important because as we prove in the next section, the measure the complexity of our data structure depends exponentially on \u03b2. In essence, these differences correspond to a good or poor choice of triangulation in the junction tree algorithm, or elimination orderings in bucket elimination. For these algorithms, good heuristics have been found by researchers over time, and are generally applied in an application-dependent manner.\nFor a factor graph G and a spanning tree T , let d be the domain of its variables and let k the maximum degree of its nodes. We define the constant characteristic of G, denoted \u03b1, as the constant \u03b1 = dk+1. Note that representing an (input) factor itself may require this much space.\nFor the analysis consider some graphical model G with spanning tree T , measure \u03b2 = \u00b5T (G) and characteristic \u03b1. Our bounds are in terms of the the characteristic and measure of G. For the bounds we assume that degree of the input graph k and domain size of the variables d are positive constants.\nOur key lemma, stated below, bounds the time for computing the boundary and cluster function of a cluster.\nLemma 4.1 (Cluster Cost) The boundary and cluster function of any cluster can be computed in O(\u03b1\u03b2) time.\nProof: We first note that since each cluster has at most two tree edges, it has a boundary of at most 2\u03b2 edges.\nConsider computing the boundary for some cluster. We will first bound the number of edges participating in the boundary computation. These edges consists of the boundary edges of the subclusters, the edges between the subclusters and the identifier vertex, and the boundary edges of the cluster itself. For counting purposes, suppose we place a pebble at each end point. The number of pebbles contributed by the k subclusters is 2k\u03b2. The number of pebbles contributed by the edges between the identifier and the subclusters is k, because the other endpoints of these edges are inside the clusters and already counted. Finally the pebbles contributed by the boundary edges of the cluster itself is 2\u03b2 because one end point of the boundary edges is inside subclusters. The total number of edges is half the size of the pebbles, i.e., 2k\u03b2+2\u03b2+k2 = (k + 1)\u03b2 + k 2 . By maintaining sorted boundaries and performing a (k + 1)- way merge technique, we can compute the boundary for the cluster in O (\n((k + 1)\u03b2 + k2 ) log k )\ntime. This running time is negligible compared to that of computing the cluster\nfunction, described next.\nFor computing the cluster function note that there can be at most (k + 1)\u03b2 + k2 boundary variables, because each edge is incident on one variable. The combined domain of these variables then has size at most d(k+1)\u03b2+ k 2 . We can compute the cluster functions by considering each member of the combined domain and performing k additions or multiplications, giving total time O(k \u00b7d(k+1)\u03b2+ k 2 ). \u00a5\nTheorem 1 (Hierarchical Clustering) Consider a factor graph G with n nodes and with spanning tree T . Let \u03b1 be the characteristic of G and let \u03b2 be the measure of G with respect to T . We can compute the cluster tree of G in O(\u03b1\u03b2 \u00b7 n) time The resulting cluster tree has n clusters and expected O(log n) depth where the expectation is taken over internal randomization.\nProof: It is known that the cluster tree can be computed in expected O(n) time, independent of the cluster functions and boundaries [1, 3], and that the depth of the cluster tree is O(log n) in expectation. Since computing the boundary and the cluster function for each cluster takes O(\u03b1\u03b2) time, the bound follows. \u00a5\nWe now state the theorem for queries and dynamic changes. Due to space restrictions, we omit the proofs here. Both theorems follow from the fact that changes and queries require traversing a path from the root to an update or a query node while perhaps updating cluster functions and boundaries or computing marginalization functions, which can be performed in O(\u03b1\u03b2) time.\nTheorem 2 (Marginal Queries and Dynamic Changes) Consider a factor graph G with n nodes and with spanning tree T . Let \u03b1 be the characteristic of G and let \u03b2 be the measure of G with respect to T . We can compute the the marginal of a variable in O(\u03b1\u03b2 log n) expected time. Similarly each dynamic change can be processed in expected O(\u03b1\u03b2 log n) time."}, {"heading": "5 Experimental Results", "text": "We compare the performance of a Matlab implementation of our algorithm to a standard implementation of a junction tree-based sum-product algorithm provided by the Bayes\u2019 Net Toolbox (BNT) [12]. We examine the speed-up provide by adaptive inference in two scenarios: synthetic data, which provides some control over the graph size and treewidth of the problems, and graphical models constructed from known protein backbone structures."}, {"heading": "5.1 Synthetic Data", "text": "For our synthetic data set, we randomly generated factor graphs with n variables and m factors, where 50 \u2264 n \u2264 1000, and m = n \u2212 1. We initialize each input graph to be a simple Markov chain, where each factor fi depends on variables xi and xi+1, where 1 \u2264 i < n. This chain comprises the set of tree edges in our algorithm. Then, for given parameters k and \u2113, we add cycles by adding non-tree edges as follows: if i is a multiple of k, we add variable xi to factor fi+\u2113\u22121 to create a cycle of length \u2113. This creates a fairly structured yet loopy graph with limited tree-width.\nThe results for these synthetic experiments are shown in Fig. 5. We initially compute the (wall clock) time required to construct the cluster tree of the graph (using k = 2 and \u2113 = 2). To preserve the predictable tree-width of the problem, updates to the graph structure are performed in pairs by selecting a non-tree edge at random, removing it, updating the cluster tree, adding the edge back in and updating the cluster tree again. We also measure the time to query the marginal at a particular variable as well as the time to update factor definitions (i.e., the values of the factor and not the number of variables it depends on).\nWe find that our build time is slightly faster than direct inference using the BNT, possibly due to differences in elimination ordering, implicit (cluster tree) vs. explicit (junction tree) maintenance of the tree-decomposition, or simply differences in Matlab programming choices. Most importantly, we see that all of our update operations exhibit average running times (over 500 trials) that are logarithmic in n, and are between one to three orders of magnitude faster than performing inference from scratch."}, {"heading": "5.2 Application to Protein Structure", "text": "Graphical models constructed from protein structures have been used to successfully predict structural properties [15]\nas well as free energy [9]. These models are typically constructed by taking each node as an amino acid whose states represent rotamers [7], and basing conditional probabilities on a physical energy function (e.g., [14, 4]). A typical goal of using these models is to efficiently compute a maximum\u2013likelihood (i.e. low\u2013energy) conformation of the protein in its native environment. Updating factors allows us to study, for example, the effects of amino acid mutations, and the addition and removal of edges corresponds directly to allowing backbone motion in the protein. Furthermore, the effect of these updates on the model can then be incorporated in logarithmic time, which was not possible in previous approaches.\nTo test the feasibility of our algorithm for these applications, we constructed factor graphs from five moderatelysized proteins drawn from the SCWRL benchmark [4]. For each protein, we constructed the a factor graph by taking each amino acid as a variable, adding interactions between sequential amino acids as tree edges and steric interactions as non-tree edges. We performed the same updates as for our synthetic test set above. The table in Fig. 6 shows the results of our experiments. We see that for queries and updates, our approach gives a speedup of 3\u201318 times over inference from scratch. These results are consistent with our synthetic experiment above (i.e., graphs with just under a hundred variables), and show that an adaptive approach to inference can be useful in modeling protein structure. We note however, that for larger proteins, our choice of spanning tree (simply the protein backbone) produced graphs whose treewidth was too large for either our algorithm or sum-product. We are currently exploring other methods for choosing the spanning tree in a protein factor graph (e.g., based on rigid secondary structure elements)."}, {"heading": "6 Conclusion", "text": "We describe an efficient algorithm for adaptive inference in general graphical models. Our algorithm constructs a balanced representation of a spanning tree of the input graphical model, and represents cycles in the model by annotating this data structure. We can support all updates\nand marginal computations in expected O(\u03b1\u03b2 log n) time, where \u03b1 is a constant and \u03b2 is the size of a particular graph cut. Our experiments show that approach provides significant speedups on both synthetic and real protein data."}], "references": [{"title": "Dynamizing static algorithms with applications to dynamic trees and history independence", "author": ["U. Acar", "G. Blelloch", "R. Harper", "J. Vittes", "M. Woo"], "venue": "In ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Adaptive Bayesian inference", "author": ["U. Acar", "A.T. Ihler", "R.R. Mettu", "\u00d6 S\u00fcmer"], "venue": "In Proc. NIPS. MIT Press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "An experimental analysis of change propagation in dynamic trees", "author": ["U.A. Acar", "G. Blelloch", "J. Vittes"], "venue": "In Proc. 7th ACM-SIAM W. on Algorithm Eng. and Exp\u2019ts,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "A graph-theory algorithm for rapid protein side-chain prediction", "author": ["A.A. Canutescu", "A.A. Shelenkov", "R.L. Dunbrack Jr."], "venue": "Protein Sci,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["R. Dechter"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Logarithmic-time updates and queries in probabilistic networks", "author": ["A.L. Delcher", "A.J. Grove", "S. Kasif", "J. Pearl"], "venue": "J. Artificial Intelligence Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Rotamer libraries in the 21st century", "author": ["R.L. Dunbrack Jr."], "venue": "Curr Opin Struct Biol,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "An iterative procedure for estimation in contingency tables", "author": ["S.E. Fienberg"], "venue": "Ann. Math. Stat.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1970}, {"title": "Free energy estimates of all-atom protein structures using generalized belief propagation", "author": ["H. Kamisetty", "E. P Xing", "C.J. Langmead"], "venue": "In Proc. 11th Ann. Int\u2019l Conf. Research in Computational Molecular Biology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F. Kschischang", "B. Frey", "H.-A. Loeliger"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Local computations with probabilities on graphical structures and their applications to expert systems", "author": ["S. Lauritzen", "D. Spiegelhalter"], "venue": "J. Royal Stat. Society, Ser. B,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1988}, {"title": "The Bayes net toolbox for Matlab", "author": ["K. Murphy"], "venue": "Computing Science and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1988}, {"title": "A new force field for the molecular mechanical simulation of nucleic acids and proteins", "author": ["S.J. Weiner", "P.A. Kollman", "D.A. Case", "U.C. Singh", "G. Alagona", "S. Profeta Jr.", "P. Weiner"], "venue": "J. Am. Chem. Soc.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1984}, {"title": "Approximate inference and protein folding", "author": ["C. Yanover", "Y. Weiss"], "venue": "In Proc. NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}], "referenceMentions": [{"referenceID": 7, "context": ", fitting an observed marginal distribution), and then recompute various moments of the new model before updating the model further [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 14, "context": "Another example is in the study of protein structures, where a graphical model can be used to represent the conformation space of a protein structure [15, 9].", "startOffset": 150, "endOffset": 157}, {"referenceID": 8, "context": "Another example is in the study of protein structures, where a graphical model can be used to represent the conformation space of a protein structure [15, 9].", "startOffset": 150, "endOffset": 157}, {"referenceID": 5, "context": "[6] studied this problem under a set of fairly restrictive conditions, requiring that the graph be tree-structured and supporting only changes to the observed evidence in the model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] gave a method of supporting more general changes to the model so long as the model remains tree-structured.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "In principle, we can perform adaptive inference on loopy graphs by construcing their junction tree [13] and applying existing frameworks to the junction tree itself [6, 2].", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "In principle, we can perform adaptive inference on loopy graphs by construcing their junction tree [13] and applying existing frameworks to the junction tree itself [6, 2].", "startOffset": 165, "endOffset": 171}, {"referenceID": 1, "context": "In principle, we can perform adaptive inference on loopy graphs by construcing their junction tree [13] and applying existing frameworks to the junction tree itself [6, 2].", "startOffset": 165, "endOffset": 171}, {"referenceID": 1, "context": "Compared to previous work on factor trees [2], we also simplify marginal computations.", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "Factor graphs [10] describe the factorization structure of the function g(X) using a bipartite graph consisting of factor nodes and variable nodes.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "When the factor graph representation of g(X) is singlyconnected (tree-structured), marginalization can be performed efficiently using sum-product [10].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "One solution is to use a junction tree [11]; this first constructs a tree-structured hypergraph of G, then runs essentially the same inference process to compute marginals.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "An alternate but essentially equivalent view of exact inference is given by the bucket elimination algorithm [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "Bucket elimination is closely related to junction tree based inference, and an equivalent junction tree may be defined implicitly by its specified elimination ordering [5].", "startOffset": 168, "endOffset": 171}, {"referenceID": 1, "context": "In [2], an algorithm for adaptive inference in factor trees is described using \u201crake and compress\u201d trees (RC-trees).", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In the previous work [2], the combination of G being treestructured and the selection criteria for creating clusters via rake or compress operations ensured that the computational complexity of each of these calculations was limited.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "To compute and maintain a balanced clustering, we use the RC-Tree (Rake-and-Compress) tree data structure [1, 3].", "startOffset": 106, "endOffset": 112}, {"referenceID": 2, "context": "To compute and maintain a balanced clustering, we use the RC-Tree (Rake-and-Compress) tree data structure [1, 3].", "startOffset": 106, "endOffset": 112}, {"referenceID": 0, "context": "We then insert/delete (u, f) from the spanning tree and use the change-propagation method supplied by the RC-Tree to update the clustering [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "Proof: It is known that the cluster tree can be computed in expected O(n) time, independent of the cluster functions and boundaries [1, 3], and that the depth of the cluster tree is O(log n) in expectation.", "startOffset": 132, "endOffset": 138}, {"referenceID": 2, "context": "Proof: It is known that the cluster tree can be computed in expected O(n) time, independent of the cluster functions and boundaries [1, 3], and that the depth of the cluster tree is O(log n) in expectation.", "startOffset": 132, "endOffset": 138}, {"referenceID": 11, "context": "We compare the performance of a Matlab implementation of our algorithm to a standard implementation of a junction tree-based sum-product algorithm provided by the Bayes\u2019 Net Toolbox (BNT) [12].", "startOffset": 188, "endOffset": 192}, {"referenceID": 14, "context": "Graphical models constructed from protein structures have been used to successfully predict structural properties [15]", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "as well as free energy [9].", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "These models are typically constructed by taking each node as an amino acid whose states represent rotamers [7], and basing conditional probabilities on a physical energy function (e.", "startOffset": 108, "endOffset": 111}, {"referenceID": 13, "context": ", [14, 4]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 3, "context": ", [14, 4]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 3, "context": "To test the feasibility of our algorithm for these applications, we constructed factor graphs from five moderatelysized proteins drawn from the SCWRL benchmark [4].", "startOffset": 160, "endOffset": 163}], "year": 2008, "abstractText": "Many algorithms and applications involve repeatedly solving variations of the same inference problem; for example we may want to introduce new evidence to the model or perform updates to conditional dependencies. The goal of adaptive inference is to take advantage of what is preserved in the model and perform inference more rapidly than from scratch. In this paper, we describe techniques for adaptive inference on general graphs that support marginal computation and updates to the conditional probabilities and dependencies in logarithmic time. We give experimental results for an implementation of our algorithm, and demonstrate its potential performance benefit in the study of protein structure.", "creator": null}}}