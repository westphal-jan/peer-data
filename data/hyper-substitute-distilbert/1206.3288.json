{"id": "1206.3288", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Tightening LP Relaxations for MAP using Message Passing", "abstract": "linear programming ( lp ) relaxations provided effective mathematical tools for observing nearly most probable ( unwanted ) links in graphical models. sv relaxations thus find deployed fastest using message - line algorithms such as belief propagation and, when the routing is tight, provably find their map configuration. the standard network operation goes not tight enough yet many real - scale markets, mobility, and stability has enabled by the acceptance of higher weight cluster - based lp relaxations. theoretical flexibility cost increases exponentially with the size of the clusters and requires the number and type correlation cycles we can use. each propose quickly solve my cluster selection problem monotonically achieving four dual conditions, iteratively shaping varieties with guaranteed orientation, and quickly re - tuning with the added clusters by reusing one existing solution. our dual message - passing algorithm enables one map configuration successively determining sidechain placement, pathogen activation, computer stereo problems, triggering simulations where the ultimate lp relaxation requires.", "histories": [["v1", "Wed, 13 Jun 2012 15:46:00 GMT  (221kb)", "http://arxiv.org/abs/1206.3288v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.CE", "authors": ["david sontag", "talya meltzer", "amir globerson", "tommi s jaakkola", "yair weiss"], "accepted": false, "id": "1206.3288"}, "pdf": {"name": "1206.3288.pdf", "metadata": {"source": "CRF", "title": "Tightening LP Relaxations for MAP using Message Passing", "authors": ["David Sontag", "Talya Meltzer"], "emails": [], "sections": [{"heading": null, "text": "Linear Programming (LP) relaxations have become powerful tools for finding the most probable (MAP) configuration in graphical models. These relaxations can be solved efficiently using message-passing algorithms such as belief propagation and, when the relaxation is tight, provably find the MAP configuration. The standard LP relaxation is not tight enough in many real-world problems, however, and this has lead to the use of higher order cluster-based LP relaxations. The computational cost increases exponentially with the size of the clusters and limits the number and type of clusters we can use. We propose to solve the cluster selection problem monotonically in the dual LP, iteratively selecting clusters with guaranteed improvement, and quickly re-solving with the added clusters by reusing the existing solution. Our dual message-passing algorithm finds the MAP configuration in protein sidechain placement, protein design, and stereo problems, in cases where the standard LP relaxation fails."}, {"heading": "1 Introduction", "text": "The task of finding the maximum aposteriori assignment (or MAP) in a graphical model comes up in a wide range of applications. For an arbitrary graph, this problem is known to be NP hard [11] and various approximation algorithms have been proposed.\nLinear Programming (LP) relaxations are commonly used to solve combinatorial optimization problems in computer science, and have a long history of being used to approximate the MAP problem in general graphical models (e.g., see [9]). LP relaxations have an advantage over other approximate inference schemes in\nthat they come with an optimality guarantee \u2013 if the solution to the linear program is integral, then it is guaranteed to give the global optimum of the MAP problem.\nAn additional attractive quality of LP relaxations is that they can be solved efficiently using messagepassing algorithms such as belief propagation and its generalizations [3, 13, 15]. In particular, by using message-passing algorithms, we can now use LP relaxations for large-scale problems where standard, offthe-shelf LP solvers could not be used [18].\nDespite the success of LP relaxations, there are many real-world problems for which the basic LP relaxation is of limited utility in solving the MAP problem. For example, in a database of 97 protein design problems studied in [18], the standard LP relaxation allowed finding the MAP in only 2 cases.\nOne way to obtain tighter relaxations is to use clusterbased LP relaxations, where local consistency is enforced between cluster marginals. As the size of the clusters grow, this leads to tighter and tighter relaxations. Furthermore, message-passing algorithms can still be used to solve these cluster-based relaxations, with messages now being sent between clusters and not individual nodes. Unfortunately, the computational cost increases exponentially with the size of the clusters, and for many real-world problems this severely limits the number of large clusters that can be feasibly incorporated into the approximation. For example, in the protein design database studied in [18], each node has around 100 states, so even a cluster of only 3 variables would have 106 states. Clearly we cannot use too many such clusters in our approximation.\nIn this paper we propose a cluster-pursuit method where clusters are incrementally added to the relaxation, and where we only add clusters that are guaranteed to improve the approximation. Similar to the work of [16] who worked on region-pursuit for sumproduct generalized belief propagation [19], we show\nhow to use the messages from a given cluster-based approximation to decide which cluster to add next. In addition, by working with a message-passing algorithm based on dual coordinate descent, we monotonically decrease an upper bound on the MAP value."}, {"heading": "2 MAP and its LP Relaxation", "text": "We consider functions over n discrete variables x = {x1, . . . , xn} defined as follows. Given a graph G = (V,E) with n vertices, and potentials \u03b8ij(xi, xj) for all edges ij \u2208 E, define the function\nf(x;\u03b8) = \u2211 ij\u2208E \u03b8ij(xi, xj) + \u2211 i\u2208V \u03b8i(xi) . (1)\nOur goal is to find the MAP assignment, xM , that maximizes the function f(x;\u03b8).\nThe MAP problem can be formulated as a linear program as follows. Let \u00b5 be a vector of marginal probabilities that includes {\u00b5ij(xi, xj)}ij\u2208E over variables corresponding to edges and {\u00b5i(xi)}i\u2208V associated with the nodes. The set of \u00b5 that arise from some joint distribution is known as the marginal polytope [14],\nM(G) = { \u00b5 | \u2203p(x) s.t. p(xi, xj) = \u00b5ij(xi, xj)\np(xi) = \u00b5i(xi)\n} .\nThe MAP problem can then be shown to be equivalent to the following LP,\nmax x f(x,\u03b8) = max \u00b5\u2208M(G) \u00b5 \u00b7 \u03b8 , (2)\nwhere \u00b5 \u00b7 \u03b8 = \u2211 ij\u2208E \u2211 xi,xj \u03b8ij(xi, xj)\u00b5ij(xi, xj) +\u2211\ni \u2211 xi \u00b5i(xi)\u03b8i(xi). There always exists a maximizing \u00b5 that is integral \u2013 a vertex of the marginal polytope \u2013 and which corresponds to xM . Although the number of variables in this LP is only O(|E|+|V |), the difficulty comes from an exponential number of linear inequalities typically required to describe the marginal polytopeM(G).\nThe idea in LP relaxations is to relax the difficult global constraint that the marginals in \u00b5 arise from some common joint distribution. Instead, we enforce this only over some subsets of variables that we refer to as clusters. More precisely, we introduce auxiliary distributions over clusters of variables and constrain the edge distributions \u00b5ij(xi, xj) associated with each cluster to arise as marginals from the cluster distribution.1 Let C be a set of clusters such that each c \u2208 C is a subset of {1, . . . , n}, and let \u03c4c(xc) be any distribution over the variables in c. We also use \u03c4c(xi, xj) to\n1Each edge may participate in multiple clusters.\nrefer to the marginal of \u03c4c(xc) for the edge (i, j), i.e. \u03c4c(xi, xj) = \u2211 xc\\i,j\n\u03c4c(xc). DefineMC(G) as \u2203\u03c4 \u2265 0\u00b5 \u2265 0 \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 xj \u00b5ij(xi, xj) = \u00b5i(xi)\n\u03c4c(xi, xj) = \u00b5ij(xi, xj) \u2200c, (i, j) \u2286 c\u2211 xc \u03c4c(xc) = 1  It is easy to see that MC(G) is an outer bound on M(G), namely MC(G) \u2287 M(G). As we add more clusters to C the relaxation of the marginal polytope becomes tighter. Note that similar constraints should be imposed on the cluster marginals, i.e., they themselves should arise as marginals from some joint distribution. To exactly represent the marginal polytope, such a hierarchy of auxiliary clusters would require clusters of size equal to the treewidth of the graph. For the purposes of this paper, we will not generate such a hierarchy but instead use the clusters to constrain only the associated edge marginals."}, {"heading": "2.1 Choosing Clusters in the LP Relaxation", "text": "Adding a cluster to the relaxation MC(G) requires computations that scale with the number of possible cluster states. The choice of clusters should therefore be guided by both how much we are able to constrain the marginal polytope, as well as the computational cost of handling larger clusters. We will consider a specific scenario where the clusters are selected from a pre-defined set of possible clusters C0 such as triplet clusters. However, we will ideally not want to use all of the clusters in C0, but instead add them gradually based on some ranking criterion.\nThe best ranking of clusters is problem dependent. In other words, we would like to choose the subset of clusters which will give us the best possible approximation to a particular MAP problem. We seek to iteratively improve the approximation, using our current beliefs to guide which clusters to add. The advantage of iteratively selecting the clusters is that we add them only up to the point that the relaxed LP has an integral solution.\nRecently, Sontag and Jaakkola [12] suggested an approach for incrementally adding constraints to the marginal polytope using a cutting-plane algorithm. A similar approach may in principle be applied to adding clusters to the primal problem. One shortcoming of this approach is that it requires solving the primal LP after every cluster added, and even solving the primal LP once is infeasible for large problems involving hundreds of variables and large state spaces.\nIn the next section we present a method that incrementally adds clusters, but which works exclusively within the dual LP. The key idea is that the dual LP\nprovides an upper bound on the MAP value, and we seek to choose clusters to most effectively minimize this bound. Note that an analogous bound minimization strategy is problematic in the primal where we would have to assess how much less the maximum is due to including additional constraints. In other words, obtaining a certificate for improvement is difficult in the primal. Moreover, unlike the dual, the primal algorithm might not give an upper bound on the MAP prior to convergence.\nFinally, we can \u201cwarm start\u201d our optimization scheme after each cluster addition in order to avoid re-solving the dual LP. We do this by reusing the dual variables calculated in the previous iterations which did not have the new clusters."}, {"heading": "3 Dual LP Relaxation", "text": "The obstacles to working in the primal LP lead us to consider the dual of the LP relaxation. Different formulations of the primal LP have lead to different dual LPs, each with efficient message-passing algorithms for solving them [3, 6, 13, 15]. In this paper we focus on a particular dual formulation by Globerson and Jaakkola [3] which has the advantage that the message-passing algorithm corresponds to performing coordinate-descent in the dual LP. Our dual algorithm will address many of the problems that were inherent in the primal approaches, giving us:\n1. Monotonically decreasing upper bound on MAP.\n2. Choosing clusters which give a guaranteed bound improvement.\n3. Simple \u201cwarm start\u201d of tighter relaxation.\n4. An efficient algorithm that scales to very large problems."}, {"heading": "3.1 The Generalized MPLP Algorithm", "text": "The generalized Max-Product LP (MPLP) messagepassing algorithm, introduced in [3], decreases the dual objective of the cluster-based LP relaxation at every iteration. This monotone property makes it ideal for adding clusters since we can initialize the new messages such that the dual value is monotonically decreased.\nAnother key advantage of working in the dual is that the dual objective gives us a certificate of optimality. Namely, if we find an assignment x such that f(x;\u03b8) is equal to the dual objective, we are guaranteed that x is the MAP assignment (since the dual objective upper bounds the MAP value). Indeed, using this property\nwe show in our experiments that MAP assignments can be found for nearly all of the problems we consider.\nWe next describe the generalized MPLP algorithm for the special case of clusters comprised of three nodes. Although the algorithm applies to general clusters, we focus on triplets for simplicity, and because these are the clusters used in the current paper.\nMPLP passes the following types of messages:\n\u2022 Edge to Node: For every edge e \u2208 E (e denotes two indices in V ) and every node i \u2208 e, we have a message \u03bbe\u2192i(xi).\n\u2022 Edge to Edge: For every edge e \u2208 E, we have a message \u03bbe\u2192e(xe) (where xe is shorthand for xi, xj , and i and j are the nodes in the edge).\n\u2022 Triplet to Edge: For every triplet cluster c \u2208 C, and every edge e \u2208 c, we have a message \u03bbc\u2192e(xe).\nThe updates for these messages are given in Figure 1. To guarantee that the dual objective decreases, all messages from a given edge must be sent simultaneously, as well as all messages from a triplet to its three edges.\nThe dual objective that is decreased in every iteration is given by\ng(\u03bb) = \u2211 i\u2208V max xi \u03b8i(xi) + \u2211 k\u2208N(i) \u03bbki\u2192i(xi)  +\n\u2211 e\u2208E max xe\n[ \u03bbe\u2192e(xe) +\n\u2211 c:e\u2208c \u03bbc\u2192e(xe) ] It should be noted, however, that not all \u03bb are dual feasible. Rather, \u03bb needs to result from a reparameterization of the underlying potentials (see [3]). However, it turns out that after updating all the MPLP messages once, all subsequent \u03bb will be dual feasible, regardless of how \u03bb is initialized.2\nBy LP duality, there exists a value of \u03bb such that g(\u03bb) is equal to the optimum of the corresponding primal LP. Although the MPLP updates decrease the objective at every iteration, they may converge to a \u03bb that is not dual optimal, as discussed in [3]. However, as we will show in the experiments, our procedure often finds the exact MAP solution, and therefore also achieves the primal optimum in these cases."}, {"heading": "3.2 Choosing Clusters in the Dual LP Relaxation", "text": "In this section we provide a very simple procedure that allows adding clusters to MPLP, while satisfying the\n2In our experiments, we initialize all messages to zero.\nalgorithmic properties in the beginning of Section 3.\nAssume we have a set of triplet clusters C and now wish to add a new triplet. Denote the messages before adding the new triplet by \u03bbt. Two questions naturally arise. The first is: assuming we decide to add a given triplet, how do we set \u03bbt+1 such that the dual objective retains its previous value g(\u03bbt). The second question is how to choose the new triplet to add.\nThe initialization problem is straightforward. Simply set \u03bbt+1 to equal \u03bbt for all messages from triplets and edges in the previous run, and set \u03bbt+1 for the messages from the new triplet to its edges to zero.3 This clearly results in g(\u03bbt+1) = g(\u03bbt).\nIn order to choose a good triplet, one strategy would be to add different triplets and run MPLP until convergence to find the one that decreases the objective the most. However, this may be computationally costly and, as we show in the experiments, is not necessary. Instead, the criterion we use is to consider the decrease in value that results from just sending messages from the triplet c to its edges (while keeping all other messages fixed).\nThe decrease in g(\u03bb) resulting from such an update has a simple form, as we show next. Assume we are considering adding a triplet c. For every edge e \u2208 c, define be(xe) to be\nbe(xe) = \u03bbe\u2192e(xe) + \u2211\nc\u2032:e\u2208c\u2032 \u03bbc\u2032\u2192e(xe) , (3)\n3It is straightforward to show that \u03bbt+1 is dual feasible.\nwhere the summation over clusters c\u2032 does not include c (those messages are initially zero). The decrease in g(\u03bb) corresponding to updating only messages from c to the edges e \u2208 c can be shown to be\nd(c) = \u2211 e\u2208c max xe be(xe)\u2212max xc [\u2211 e\u2208c be(xe) ] . (4)\nThe above corresponds to the difference between independently maximizing each edge and jointly maximizing over the three edges. Thus d(c) is a lower bound on the improvement in the dual objective if we were to add triplet c. Our algorithm will therefore add the triplet c that maximizes d(c)."}, {"heading": "3.3 The Dual Algorithm", "text": "We now present the complete algorithm for adding clusters and optimizing over them. Let C0 be the predefined set of triplet clusters that we will consider adding to our relaxation, and let CL be the initial relaxation consisting of only edge clusters (pairwise local consistency).\n1. Run MPLP until convergence using the CL clusters.\n2. Find an integral solution x by locally maximizing the single node beliefs bi(xi), where bi(xi) = \u03b8i(xi) +\u2211\nk\u2208N(i) \u03bbki\u2192i(xi). Ties are broken arbitrarily.\n3. If the dual objective g(\u03bbt) is sufficiently close to the primal objective f(x; \u03b8), terminate (since x is approximately the MAP).\n4. Add the cluster c \u2208 C0 with the largest guaranteed bound improvement, d(c), to the relaxation.\n5. Construct \u201cwarm start\u201d messages \u03bbt+1 from \u03bbt.\n6. Run MPLP for N iterations, and return to 2.\nNote that we obtain (at least) the promised bound improvement d(c) within the first iteration of step 6. By allowing MPLP to run for N iterations, the effect of adding the cluster will be propagated throughout the model, obtaining an additional decrease in the bound. Since the MPLP updates correspond to coordinatedescent in the dual LP, every step of the algorithm decreases the upper bound on the MAP. The monotonicity property holds even if MPLP does not converge in step 6, giving us the flexibility to choose the number of iterations N . In Section 5 we show results corresponding to two different choices of N .\nIn the case where we run MPLP to convergence before choosing the next cluster, we can show that the greedy bound minimization corresponds to a cutting-plane algorithm, as stated below.\nTheorem 1. Given a dual optimal solution, if we find a cluster for which we can guarantee a bound decrease, all primal optimal solutions were inconsistent with respect to this cluster.\nProof. By duality both the dual optimum and the primal optimum will decrease. Suppose for contradiction that in the previous iteration there was a primal feasible point that was cluster consistent and achieved the LP optimum. Since we are maximizing the LP, after adding the cluster consistency constraint, this point is still feasible and the optimal value of the primal LP will not change, giving our contradiction.\nThis theorem does not tell us how much the given cluster consistency constraint was violated, and the distinction remains that a typical cutting-plane algorithm would attempt to find the constraint which is most violated."}, {"heading": "4 Related Work", "text": "Since MPLP is closely related to the max-product generalized belief propagation (GBP) algorithm, our work can be thought of as a region-pursuit method for GBP. This is closely related to the work of Welling [16] who suggested a region-pursuit method for sum-product GBP. Similar to our work, he suggested greedily adding from a candidate set of possible clusters. At each iteration, the cluster that results in the largest change in the GBP free energy is added. He showed excellent results for 2D grids, but on fully connected graphs the performance actually started deteriorating\nwith additional clusters. In [17], a heuristic related to maxent normality [19] was used as a stopping criterion for region-pursuit to avoid this behavior. In our work, in contrast, since we are working with the dual function of the LP, we can guarantee monotonic improvement throughout the running of the algorithm.\nOur work is also similar to Welling\u2019s in that we focus on criteria for determining the utility of adding a cluster, not on finding these clusters efficiently. We found in our experiments that a simple enumeration over small clusters proved extremely effective. For problems where triplet clusters alone would not suffice to find the MAP, we could triangulate the graph and consider larger clusters. This approach is reminiscent of the bounded join-graphs described in [1].\nThere is a large body of recent work describing the relationship between message-passing algorithms such as belief propagation, and LP relaxations [7, 15, 18]. Although we have focused here on using one particular message-passing algorithm, MPLP, we emphasize that similar region-pursuit algorithms can be derived for other message-passing algorithms as well. In particular, for all the convex max-product BP algorithms described in [15], it is easy to design region-pursuit methods. The main advantage of using MPLP is its guaranteed decrease of the dual value at each iteration, a guarantee that does not exist for general convex BP algorithms.\nRegion-pursuit algorithms are also conceptually related to the question of message scheduling in BP, as in the work of Elidan et al. [2]. One way to think of region-pursuit is to consider a graph where all the clusters are present all the time, but send and receive non-informative messages. The question of which cluster to add to an approximation, is thus analogous to the question of which message to update next."}, {"heading": "5 Experiments", "text": "Due to the scalable nature of our message-passing algorithm, we can apply it to cases where standard LP solvers cannot be applied to the primal LP (see also [18]). Here we report applications to problems in computational biology and machine vision.4\nWe use the algorithm from Section 3.3 for all of our experiments. We first run MPLP with edge clusters until convergence or for at most 1000 iterations, whichever comes first. All of our experiments, except those intended to show the difference between schedules, use N = 20 for the number of MPLP iterations run after adding a cluster. While running MPLP we use the messages to decode an integral solution, and compare\n4Graphical models for these are given in [18].\nthe dual objective to the value of the integral solution. If these are equal, we have found the MAP solution.5 Otherwise, we keep adding triplets.\nOur results will show that we often find the MAP solution to these hard problems by using only a small number of triplet clusters. This indicates both that triplets are sufficient for characterizing M(G) near the MAP solution of these problems, and that our algorithm can efficiently find the informative triplets."}, {"heading": "5.1 Side-Chain Prediction", "text": "The side-chain prediction problem involves finding the three-dimensional configuration of rotamers given the backbone structure of a protein [18]. This problem can be posed as finding the MAP configuration of a pairwise model, and in [18] the TRBP algorithm [13] was used to find the MAP solution for most of the models studied. However, for 30 of the models, TRBP could not find the MAP solution.\nIn earlier work [12] we used a cutting-plane algorithm to solve these side-chain problems and found the MAP solution for all 30 models. Here, we applied our dual algorithm to the same 30 models and found that it also results in the MAP solution for all of them (up to a 10\u22124 integrality gap). This required adding between 1 and 27 triplets per model. The running time was between 1 minute and 1 hour to solve each problem, with over half solved in under 9 minutes. On average we added only 7 triplets (median was 4.5), another indication of the relative ease with which these techniques can solve the side-chain prediction problem.\n5In practice, we terminate when the dual objective is within 10\u22124 of the decoded assignment, so these are approximate MAP solutions. Note that the objective values are significantly larger than this threshold.\nWe also used these models to study different update schedules. One schedule (which gave the results in the previous paragraph) was to first run a pairwise model for 1000 iterations, and then alternate between adding triplets and running MPLP for 20 more iterations. In the second schedule, we run MPLP to convergence after adding each triplet. Figure 2 shows the two schedules for the side-chain protein \u20181gsk\u2019, one of the side-chain proteins which took us the longest to solve (30 minutes). Running MPLP to convergence results in a much larger number of overall MPLP iterations compared to using only 20 iterations. This highlights one of the advantages of our method: adding a new cluster does not require solving the earlier problem to convergence."}, {"heading": "5.2 Protein Design", "text": "The protein design problem is the inverse of the protein folding problem. Given a particular 3D shape, we wish to find a sequence of amino-acids that will be as stable as possible in that 3D shape. Typically this is done by finding a set of amino-acids and rotamer configurations that minimizes an approximate energy.\nWhile the problem is quite different from side-chain prediction, it can be solved using the same graph structure, as shown in [18]. The only difference is that now the nodes do not just denote rotamers, but also the identity of the amino-acid at that location. Thus, the state-space here is significantly larger than in the sidechain prediction problem (up to 180 states per variable for most variables).\nIn contrast to the side-chain prediction problems, which are often easily solved by general purpose integer linear programming packages such as CPLEX\u2019s branch-and-cut algorithm [5], the sheer size of the protein design problems immediately limits the techniques by which we can attempt to solve them. Algorithms such as our earlier cutting-plane algorithm [12] or CPLEX\u2019s branch-and-cut algorithm require solving the primal LP relaxation at least once, but solving the primal LP on all but the smallest of the design problems is intractable [18]. Branch and bound schemes have been recently used in conjunction with a message passing algorithm [4] and applied to similar protein design problems, although not the ones we solve here.\nWe applied our method to the 97 protein design problems described in [18], adding 5 triplets at a time to the relaxation. The key striking result of these experiments is that our method found the exact MAP configuration for all but one of the proteins6 (up to a precision of 10\u22124 in the integrality gap). This is es-\n6We could not solve \u20181fpo\u2019, the largest protein.\npecially impressive since, as reported in [18], only 2 of these problems were solvable using TRBP, and the primal problem was too big for commercial LP solvers such as CPLEX. For the problem where we did not find the MAP, we did not reach a point where all the triplets in the graph were included, since we ran out of memory beforehand.\nAmong the problems that were solved exactly, the mean running time was 9.7 hours with a maximum of 11 days and a minimum of a few minutes. We note again that most of these problems could not be solved using LP solvers, and when LP solvers could be used, they were typically at least 10 times slower than message-passing algorithms similar to ours (see [18] for detailed timing comparisons).\nNote that the main computational burden in the algorithm is processing triplet messages. Since each variable has roughly 100 states, passing a triplet message requires 106 operations. Thus the number of triplets added is the key algorithmic complexity issue. For the models that were solved exactly, the median number of triplets added was 145 (min: 5, max: 735). As mentioned earlier, for the unsolved model this number grew until the machine\u2019s memory was exhausted. We believe however, that by optimizing our code for speed and memory we will be able to accommodate a larger number of triplets, and possibly solve the remaining model as well. Our current code is written mostly in Matlab, so significant optimization may be possible."}, {"heading": "5.3 Stereo Vision", "text": "Given a stereo pair of images, the stereo problem is to find the disparity of each pixel in a reference image. This disparity can be straightforwardly translated into depth from the camera. The best algorithms currently known for the stereo problem are those that minimize a global energy function [10], which is equivalent to finding a MAP configuration in a pairwise model.\nFor our experiments we use the pairwise model described in [18], and apply our procedure to the \u201cTsukuba\u201d sequence from the standard Middlebury stereo benchmark set [10], reduced in size to contain 116x154 pixels.\nSince there are no connected triplets in the grid graph, we use our method with square clusters. We calculate the bound decrease using square clusters, but rather than add them directly, we triangulate the cycle and add two triplet clusters. This results in an equivalent relaxation, but has the consequence that we may have to wait until MPLP convergence to achieve the guaranteed bound improvement.\nIn the first experiment, we varied the parameters of the\nenergy function to create several different instances. We tried to find the MAP using TRBP, resolving ties using the methods proposed in [8]. In 4 out of 10 cases those methods failed. Using our algorithm, we managed to find the MAP for all 4 cases.7\nFigure 3 shows the dual objective and the decoded integer solution after each MPLP iteration, for one set of parameters.\nIn the results above, we added 20 squares at a time to the relaxation. We next contrasted it with two strategies: one where we pick 20 random squares (not using our bound improvement criterion) and one where we pick the single best square according to the bound criterion. Figure 4 shows the resulting bound per iteration for one of the models. It can be seen that the random method is much slower than the bound criterion based one, and that adding 20 squares at a time is better than just one. We ended up adding 1060 squares when adding 20 at a time, and 83 squares when adding just one. Overall, adding 20 squares at a time turned out to be faster.\nWe also tried running MPLP with all of the square clusters. Although fewer MPLP iterations were needed, the cost of using all squares resulted in an overall running time of about four times longer.\n7For one of these models, a few single node beliefs at convergence were tied, and we used the junction tree algorithm to decode the tied nodes (see [8])."}, {"heading": "6 Conclusion", "text": "In order to apply LP relaxations to real-world problems, one needs to find an efficient way of adding clusters to the basic relaxation such that the problem remains tractable but yields a better approximation of the MAP value.\nIn this paper we presented a greedy boundminimization algorithm on the dual LP to solve this problem, and showed that it has all the necessary ingredients: an efficient message-passing algorithm, \u201cwarm start\u201d of the next iteration using current beliefs, and a monotonically decreasing bound on the MAP.\nWe showed that the algorithm works well in practice, finding the MAP configurations for many real-world problems that were previously thought to be too difficult for known methods to solve. While in this paper we focused primarily on adding triplet clusters, our approach is general and can be used to add larger clusters as well, as long as as the messages in the dual algorithm can be efficiently computed.\nFinally, while here we focused on the MAP problem, similar ideas may be applied to approximating the marginals in graphical models."}, {"heading": "Acknowledgements", "text": "This work was supported in part by the DARPA Transfer Learning program and by the Israeli Science Foundation. D.S. was also supported by a NSF Graduate Research Fellowship."}], "references": [{"title": "Iterative joingraph propagation", "author": ["R. Dechter", "K. Kask", "R. Mateescu"], "venue": "UAI", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Residual belief propagation: informed scheduling for asynchronous message passing", "author": ["G. Elidan", "I. Mcgraw", "D. Koller"], "venue": "UAI", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Fixing max-product: Convergent message passing algorithms for MAP LPrelaxations", "author": ["A. Globerson", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems 21. MIT Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Protein side-chain placement through MAP estimation and problem-size reduction", "author": ["E.J. Hong", "T. Lozano-P\u00e9rez"], "venue": "WABI", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Solving and analyzing side-chain positioning problems using linear and integer programming", "author": ["C.L. Kingsford", "B. Chazelle", "M. Singh"], "venue": "Bioinformatics, 21(7):1028\u20131039", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 28(10):1568\u20131583", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "On the optimality of tree-reweighted max-product message-passing", "author": ["V. Kolmogorov", "M.Wainwright"], "venue": "In UAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Globally optimal solutions for energy minimization in stereo vision using reweighted belief propagation", "author": ["T. Meltzer", "C. Yanover", "Y. Weiss"], "venue": "ICCV", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "On the generation of alternative explanations with implications for belief revision", "author": ["E.G. Santos"], "venue": "UAI", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1991}, {"title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms", "author": ["D. Scharstein", "R. Szeliski"], "venue": "IJCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Finding the MAPs for belief networks is NP-hard", "author": ["Y. Shimony"], "venue": "Aritifical Intelligence, 68(2):399\u2013410", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1994}, {"title": "New outer bounds on the marginal polytope", "author": ["D. Sontag", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems 21. MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "MAP estimation via agreement on trees: message-passing and linear programming", "author": ["M. Wainwright", "T. Jaakkola", "A. Willsky"], "venue": "IEEE Trans. on Information Theory, 51(11):3697\u20133717", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Graphical models", "author": ["M. Wainwright", "M.I. Jordan"], "venue": "exponential families and variational inference. Technical report, UC Berkeley, Dept. of Statistics", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "MAP estimation", "author": ["Y. Weiss", "C. Yanover", "T. Meltzer"], "venue": "linear programming and belief propagation with convex free energies. In UAI", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "On the choice of regions for generalized belief propagation", "author": ["M. Welling"], "venue": "UAI", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Structured region graphs: Morphing EP into GBP", "author": ["M. Welling", "T. Minka", "Y.W. Teh"], "venue": "UAI", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Linear programming relaxations and belief propagation \u2013 an empirical study", "author": ["C. Yanover", "T. Meltzer", "Y. Weiss"], "venue": "JMLR, 7:1887\u20131907", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "IEEE Trans. on Information Theory, 51(7):2282\u2013 2312", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 10, "context": "For an arbitrary graph, this problem is known to be NP hard [11] and various approximation algorithms have been proposed.", "startOffset": 60, "endOffset": 64}, {"referenceID": 8, "context": ", see [9]).", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "An additional attractive quality of LP relaxations is that they can be solved efficiently using messagepassing algorithms such as belief propagation and its generalizations [3, 13, 15].", "startOffset": 173, "endOffset": 184}, {"referenceID": 12, "context": "An additional attractive quality of LP relaxations is that they can be solved efficiently using messagepassing algorithms such as belief propagation and its generalizations [3, 13, 15].", "startOffset": 173, "endOffset": 184}, {"referenceID": 14, "context": "An additional attractive quality of LP relaxations is that they can be solved efficiently using messagepassing algorithms such as belief propagation and its generalizations [3, 13, 15].", "startOffset": 173, "endOffset": 184}, {"referenceID": 17, "context": "In particular, by using message-passing algorithms, we can now use LP relaxations for large-scale problems where standard, offthe-shelf LP solvers could not be used [18].", "startOffset": 165, "endOffset": 169}, {"referenceID": 17, "context": "For example, in a database of 97 protein design problems studied in [18], the standard LP relaxation allowed finding the MAP in only 2 cases.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "For example, in the protein design database studied in [18], each node has around 100 states, so even a cluster of only 3 variables would have 10 states.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "Similar to the work of [16] who worked on region-pursuit for sumproduct generalized belief propagation [19], we show", "startOffset": 23, "endOffset": 27}, {"referenceID": 18, "context": "Similar to the work of [16] who worked on region-pursuit for sumproduct generalized belief propagation [19], we show", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "The set of \u03bc that arise from some joint distribution is known as the marginal polytope [14],", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "Recently, Sontag and Jaakkola [12] suggested an approach for incrementally adding constraints to the marginal polytope using a cutting-plane algorithm.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "Different formulations of the primal LP have lead to different dual LPs, each with efficient message-passing algorithms for solving them [3, 6, 13, 15].", "startOffset": 137, "endOffset": 151}, {"referenceID": 5, "context": "Different formulations of the primal LP have lead to different dual LPs, each with efficient message-passing algorithms for solving them [3, 6, 13, 15].", "startOffset": 137, "endOffset": 151}, {"referenceID": 12, "context": "Different formulations of the primal LP have lead to different dual LPs, each with efficient message-passing algorithms for solving them [3, 6, 13, 15].", "startOffset": 137, "endOffset": 151}, {"referenceID": 14, "context": "Different formulations of the primal LP have lead to different dual LPs, each with efficient message-passing algorithms for solving them [3, 6, 13, 15].", "startOffset": 137, "endOffset": 151}, {"referenceID": 2, "context": "In this paper we focus on a particular dual formulation by Globerson and Jaakkola [3] which has the advantage that the message-passing algorithm corresponds to performing coordinate-descent in the dual LP.", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "The generalized Max-Product LP (MPLP) messagepassing algorithm, introduced in [3], decreases the dual objective of the cluster-based LP relaxation at every iteration.", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "Rather, \u03bb needs to result from a reparameterization of the underlying potentials (see [3]).", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "Although the MPLP updates decrease the objective at every iteration, they may converge to a \u03bb that is not dual optimal, as discussed in [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 15, "context": "This is closely related to the work of Welling [16] who suggested a region-pursuit method for sum-product GBP.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "In [17], a heuristic related to maxent normality [19] was used as a stopping criterion for region-pursuit to avoid this behavior.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In [17], a heuristic related to maxent normality [19] was used as a stopping criterion for region-pursuit to avoid this behavior.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "This approach is reminiscent of the bounded join-graphs described in [1].", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "There is a large body of recent work describing the relationship between message-passing algorithms such as belief propagation, and LP relaxations [7, 15, 18].", "startOffset": 147, "endOffset": 158}, {"referenceID": 14, "context": "There is a large body of recent work describing the relationship between message-passing algorithms such as belief propagation, and LP relaxations [7, 15, 18].", "startOffset": 147, "endOffset": 158}, {"referenceID": 17, "context": "There is a large body of recent work describing the relationship between message-passing algorithms such as belief propagation, and LP relaxations [7, 15, 18].", "startOffset": 147, "endOffset": 158}, {"referenceID": 14, "context": "In particular, for all the convex max-product BP algorithms described in [15], it is easy to design region-pursuit methods.", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Due to the scalable nature of our message-passing algorithm, we can apply it to cases where standard LP solvers cannot be applied to the primal LP (see also [18]).", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "Graphical models for these are given in [18].", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "The side-chain prediction problem involves finding the three-dimensional configuration of rotamers given the backbone structure of a protein [18].", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "This problem can be posed as finding the MAP configuration of a pairwise model, and in [18] the TRBP algorithm [13] was used to find the MAP solution for most of the models studied.", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "This problem can be posed as finding the MAP configuration of a pairwise model, and in [18] the TRBP algorithm [13] was used to find the MAP solution for most of the models studied.", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "In earlier work [12] we used a cutting-plane algorithm to solve these side-chain problems and found the MAP solution for all 30 models.", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "While the problem is quite different from side-chain prediction, it can be solved using the same graph structure, as shown in [18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 4, "context": "In contrast to the side-chain prediction problems, which are often easily solved by general purpose integer linear programming packages such as CPLEX\u2019s branch-and-cut algorithm [5], the sheer size of the protein design problems immediately limits the techniques by which we can attempt to solve them.", "startOffset": 177, "endOffset": 180}, {"referenceID": 11, "context": "Algorithms such as our earlier cutting-plane algorithm [12] or CPLEX\u2019s branch-and-cut algorithm require solving the primal LP relaxation at least once, but solving the primal LP on all but the smallest of the design problems is intractable [18].", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "Algorithms such as our earlier cutting-plane algorithm [12] or CPLEX\u2019s branch-and-cut algorithm require solving the primal LP relaxation at least once, but solving the primal LP on all but the smallest of the design problems is intractable [18].", "startOffset": 240, "endOffset": 244}, {"referenceID": 3, "context": "Branch and bound schemes have been recently used in conjunction with a message passing algorithm [4] and applied to similar protein design problems, although not the ones we solve here.", "startOffset": 97, "endOffset": 100}, {"referenceID": 17, "context": "We applied our method to the 97 protein design problems described in [18], adding 5 triplets at a time to the relaxation.", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "pecially impressive since, as reported in [18], only 2 of these problems were solvable using TRBP, and the primal problem was too big for commercial LP solvers such as CPLEX.", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "We note again that most of these problems could not be solved using LP solvers, and when LP solvers could be used, they were typically at least 10 times slower than message-passing algorithms similar to ours (see [18] for detailed timing comparisons).", "startOffset": 213, "endOffset": 217}, {"referenceID": 9, "context": "The best algorithms currently known for the stereo problem are those that minimize a global energy function [10], which is equivalent to finding a MAP configuration in a pairwise model.", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "For our experiments we use the pairwise model described in [18], and apply our procedure to the \u201cTsukuba\u201d sequence from the standard Middlebury stereo benchmark set [10], reduced in size to contain 116x154 pixels.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "For our experiments we use the pairwise model described in [18], and apply our procedure to the \u201cTsukuba\u201d sequence from the standard Middlebury stereo benchmark set [10], reduced in size to contain 116x154 pixels.", "startOffset": 165, "endOffset": 169}, {"referenceID": 7, "context": "We tried to find the MAP using TRBP, resolving ties using the methods proposed in [8].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "For one of these models, a few single node beliefs at convergence were tied, and we used the junction tree algorithm to decode the tied nodes (see [8]).", "startOffset": 147, "endOffset": 150}], "year": 2008, "abstractText": "Linear Programming (LP) relaxations have become powerful tools for finding the most probable (MAP) configuration in graphical models. These relaxations can be solved efficiently using message-passing algorithms such as belief propagation and, when the relaxation is tight, provably find the MAP configuration. The standard LP relaxation is not tight enough in many real-world problems, however, and this has lead to the use of higher order cluster-based LP relaxations. The computational cost increases exponentially with the size of the clusters and limits the number and type of clusters we can use. We propose to solve the cluster selection problem monotonically in the dual LP, iteratively selecting clusters with guaranteed improvement, and quickly re-solving with the added clusters by reusing the existing solution. Our dual message-passing algorithm finds the MAP configuration in protein sidechain placement, protein design, and stereo problems, in cases where the standard LP relaxation fails.", "creator": "TeX"}}}