{"id": "1206.4599", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "A Unified Robust Classification Model", "abstract": "distributed wide variety scala machine algebra programs such names support vector machine ( lap ), minimax efficient machine ( eps ), and fisher cost analysis ( pea ), exist in binary classification. the purpose of this simulation methods can provide a unified classification version that mixes the above models through a robust optimization approach. without resulting model exhibits significant faults. one is that neither logic supporting other intended criterion codes become added to mpm optimization fda, 2 lake versa. its benefit attempts to provide analogous results to above evaluation methods achieve completion by dealing with the unified model. we give a statistical interpretation of the unified classification model and propose further non - convex optimization algorithm that can are applied regarding objective - convex realization of existing criterion rules.", "histories": [["v1", "Mon, 18 Jun 2012 14:39:39 GMT  (752kb)", "http://arxiv.org/abs/1206.4599v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akiko takeda", "hiroyuki mitsugi", "takafumi kanamori"], "accepted": true, "id": "1206.4599"}, "pdf": {"name": "1206.4599.pdf", "metadata": {"source": "META", "title": "A Unified Robust Classification Model", "authors": ["Akiko Takeda", "Hiroyuki Mitsugi", "Takafumi Kanamori"], "emails": ["takeda@ae.keio.ac.jp,", "kiyurohi7@z2.keio.jp", "kanamori@is.nagoya-u.ac.jp"], "sections": [{"heading": "1. Introduction", "text": "There are a wide variety of machine learning algorithms for binary classification. Support vector machine (SVM) is one of the most successful classification algorithms in modern machine learning (Scho\u0308lkopf & Smola, 2002). The minimax probability machine (MPM) (Lanckriet et al., 2002) and Fisher discriminant analysis (FDA) (Fukunaga, 1990) also address the binary classification problem. Their problem settings assume that only the mean and covariance matrix of each class are known. The optimal hyperplane of MPM is determined by minimizing the worst-case (maximum) probability of misclassification of unseen test samples over all possible class-conditional distributions. FDA is to find a direction which maximizes\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nthe projected class means while minimizing the class variance in this direction.\nThe purpose of this paper is to provide a unified framework for learning algorithms, including SVM, MPM, and FDA, from the viewpoint of robust optimization (Ben-Tal et al., 2009). Robust optimization is an approach that handles optimization problems defined by uncertain inputs. A simple example of robust optimization is\nmax w\u2208W min x\u2208U\nx > w, (1)\nwhere w is the parameter to be optimized under the constraint w \u2208 W and x is an uncertain input in the problem. The uncertainty set U represents the uncertainty of the input. (1) determines the decision making parameter w which maximizes the benefit x>w for the worst-case setup among x \u2208 U . For binary classification, we regard the means x+ and x\u2212 of the data points of each class as uncertain inputs and prepare uncertainty sets U+ and U\u2212 of those uncertain inputs. We assume that x of (1) exists in the Minkowski difference U of U+ and U\u2212, i.e.,\nU = U+ U\u2212 := {x+ \u2212 x\u2212 \u2223\u2223 x+ \u2208 U+, x\u2212 \u2208 U\u2212},\nand define W by {w \u2223\u2223 \u2016w\u20162 = 1}, where \u2016 \u00b7 \u2016 is the Euclidean norm. Then we transform (1) into\nmax w:\u2016w\u20162=1 min x+\u2208U+x\u2212\u2208U\u2212\n(x+ \u2212 x\u2212)>w. (2)\nWe call it robust classification model (RCM)1. This problem always seems to be non-convex because ofW. However, it reduces to a convex problem that includes a constraint \u2016w\u20162 \u2264 1 instead of \u2016w\u20162 = 1 when U+ and U\u2212 do not intersect.\n1 Here we used the terminology of \u201crobust\u201d for the model (2) from the notion of \u201crobust optimization\u201d, not from the notion of \u201crobust statistics\u201d. The aim of the RCM is in providing a unified framework to existing learning methods, not in providing a learning method with better tolerance to outliers.\nIn this paper, we show that RCM (2) reduces to the learning methods mentioned above, depending on a prescribed uncertainty set U . For example, we show that MPM is a special case of (2) with an ellipsoidal uncertainty set U . When U+ and U\u2212 are defined as reduced convex hulls (Bennett & Bredensteiner, 2000), (2) reduces to \u03bd-SVM (Scho\u0308lkopf et al., 2000) if U+ \u2229 U\u2212 = \u2205 and reduces to E\u03bd-SVM (Perez-Cruz et al., 2003), otherwise. The difference between these learning methods turns out only to be in the definition of U of (2). The first contribution of handling the unified model (2) is to obtain new learning methods. For example, we can obtain non-convex variants of MPM and FDA by mimicking Perez-Cruz et al.\u2019s extension (Perez-Cruz et al., 2003) from convex \u03bd-SVM to nonconvex E\u03bd-SVM.\nThe second contribution is to provide theoretical results to above learning methods at once by dealing with the unified model (2). Indeed, we provide statistical interpretation for (2) on the basis of the conventional statistical learning theory. We show that (2) with some corresponding uncertainty set is a good approximation for the worst-case minimization of expected loss functions under uncertain probabilities.\nWe also provide a generalized local optimum search algorithm, that is applicable to non-convex variants of learning models. We prove theoretical results on the local optimum search algorithm.\nThe paper is organized as follows. In Section 2, we elucidate the unified model, RCM (2), for classification problems. In Section 3, we show RCM\u2019s connection with existing learning algorithms and obtain nonconvex variants for MPM and FDA in the same way as non-convex E\u03bd-SVM. In Section 4, we give a statistical interpretation of RCM in terms of minimizing the upper and lower bounds of the worst-case expected loss. In Section 5, we describe a local optimum search algorithm for non-convex RCM. We summarize our contributions and future work in Section 6."}, {"heading": "2. Unified Robust Classification Model", "text": ""}, {"heading": "2.1. Problem Settings", "text": "We shall start by introducing the problem setting and the notations. The observed training samples are denoted as (xi, yi) \u2208 Rd\u00d7{+1,\u22121}, i \u2208M := {1, ...,m}. Let M+ be the set of indices of training samples with the label +1; likewise for M\u2212. Let |M+| = m+ and |M\u2212| = m\u2212, where | \u00b7 | shows the size of the set. The goal of the classification task is to obtain a classi-\nfier that minimizes the prediction error rate for unseen test samples. For the sake of simplicity, we shall focus on linear classifiers, i.e., x>w + b where w (\u2208 Rd) is a vector and b (\u2208 R) is a bias parameter. Most of the discussions in this paper can be directly applied to kernel classifiers (Scho\u0308lkopf & Smola, 2002). Concretely, the change from x \u2208 X to the kernel function k(\u00b7,x) makes statements of Sections 2-4 hold for kernel classifiers, while the algorithm in Section 5 needs small modification.\nWe shall assume that the training samples are not reliable because of noise or measurement errors. To make a classification model less sensitive to noise in the training samples, we shall focus on representative points of each class, denoted by x+ and x\u2212. These points are not necessarily individual samples, but may be means of the data points of each class. Since the training samples are not reliable, it is reasonable to assume that x+ and x\u2212 will involve some uncertainty. The largest possible sets of x+ and x\u2212 are denoted by U+ and U\u2212, respectively, and these sets are defined on the basis of training samples. Throughout this paper, we will assume that both U+ and U\u2212 are convex and compact and that they have interior points. Then, their Minkowski difference U is convex and has a nonempty interior.\nThe way of constructing the uncertainty set U\u00b1 is a very important issue in practice. If we set U too large in (2), the optimal decision is very robust to uncertain data x but too conservative. Moreover, if we define U with complicated functions, we cannot easily solve (2). Many robust optimization studies have used polyhedral sets and ellipsoidal sets as U for the sake of computational tractability. We show examples of U+ and U\u2212 in Section 3. We might possibly deal with more complicated problem setting beyond convex U+ and U\u2212 by using kernelization techniques."}, {"heading": "2.2. Properties of RCM", "text": "To geometrically interpret RCM (2), Figure 1 shows the ellipsoidal uncertainty sets U+, U\u2212 and their Minkowski difference. We can separate the problem (2) into two cases, i.e., whether U+ and U\u2212 have an intersection or not, which is equivalent to whether U includes 0 or not. As shown in Theorem 2.2, there is a large difference in computational effort between the two cases. Before giving an intuitive geometric interpretation of RCM in Theorem 2.2, we introduce Lemma 2.1 that further separates the case 0 \u2208 U into two cases: U includes 0 in its interior, int(U), or on its boundary, bd(U). In the geometric sense, 0 6\u2208 U holds when U+ and U\u2212 are disjoint. 0 \u2208 U implies that U+\nand U\u2212 are joint. In particular, 0 \u2208 bd(U) implies that U+ and U\u2212 touch externally. Lemma 2.1. The optimal value of RCM (2) is positive if and only if 0 6\u2208 U . It is zero if and only if 0 \u2208 bd(U), and it is negative if and only if 0 \u2208 int(U).\nWe can prove \u201cif\u201d parts by using the supporting hyperplane theorem to three cases (0 6\u2208 U , 0 \u2208 bd(U) and 0 \u2208 int(U)). By taking the contrapositive of all the \u201cif\u201d parts, we also can prove \u201conly if\u201d parts.\nLet U\u03b7 be a parametrized uncertainty set for RCM (2) such that U\u03b71 \u2282 U\u03b72 holds for \u03b71 \u2264 \u03b72. Then the following inequality holds:\nmax w:\u2016w\u20162=1 min x\u2208U\u03b71\nx > w \u2265 max w:\u2016w\u20162=1 min x\u2208U\u03b72 x > w.\nThis indicates that the optimal value of (2) is nonincreasing with respect to the inclusion relation of uncertainty sets. Figure 1 (right) plots the non-increasing optimal value of RCM (2) with respect to \u03b7. An uncertainty set U\u03b72 might exist such that the optimal value of (2) becomes zero.\nThe following theorem shows that when 0 6\u2208 U , the equality constraint \u2016w\u20162 = 1 in (2) can be replaced by \u2016w\u20162 \u2264 1 without changing the solution. Moreover, \u2016w\u20162 = 1 can be replaced by \u2016w\u20162 \u2265 1 when 0 \u2208 U . Figure 1 (left and middle) illustrates Theorem 2.2. Theorem 2.2. For an uncertainty set such that 0 6\u2208 U , RCM (2) is equivalent to\nmax w:\u2016w\u20162\u22641 min x\u2208U\nx > w. (3)\nMoreover, the problem is equivalent to\nmin x\u00b1\u2208U\u00b1 \u2016x+ \u2212 x\u2212\u2016, or equivalently, min x\u2208U \u2016x\u2016. (4)\nAn optimal w of (3) can be obtained from x\u2217/\u2016x\u2217\u2016 by using the optimal x\u2217 \u2208 U of (4). For an uncertainty set such that 0 \u2208 int(U), RCM (2) is equivalent to\nmax w:\u2016w\u20162\u22651 min x\u2208U\nx > w. (5)\nMoreover, the problem is equivalent to minx\u2208Uc \u2016x\u2016, where Uc is the closure of the complement of the convex set U . An optimal w of (5) can be obtained from \u2212x\u2217/\u2016x\u2217\u2016 by using the optimal x\u2217 \u2208 Uc.\nProof. Assume 0 6\u2208 U . By applying the discussion on the minimum norm duality (Luenberger, 1969) to (3), we can confirm the equivalence of (3) and minx\u2208U \u2016x\u2016, and the optimal solution w\u2217 = x\u2217/\u2016x\u2217\u2016. On the other hand, in the case of 0 \u2208 int(U), the equivalence of (5) and minx\u2208Uc \u2016x\u2016 is proved from Proposition 3.1 of (Briec, 1997) under the assumption that a convex U has a nonempty interior. Hence, it is enough to show that there exists an optimal solution w\u2217 of (3) (or (5)) such that \u2016w\u2217\u2016 = 1, because the difference between (2) and (3) (or (5)) is only the norm constraint of w.\nLemma 2.1 ensures that the optimal value of (3) is positive, because\nmax w:\u2016w\u20162\u22641 min x\u2208U\nx > w \u2265 max w:\u2016w\u20162=1 min x\u2208U x > w > 0.\nSince the optimal solution w\u2217 of (3) satisfies 0 < \u2016w\u2217\u2016 \u2264 1, the following inequalities hold:\n0 < min x\u2208U\nx > w \u2217 \u2264 min x\u2208U x > w \u2217/\u2016w\u2217\u2016 \u2264 min x\u2208U x > w \u2217.\nThe last inequality comes from the optimality of w\u2217. These inequalities imply that w\u2217/\u2016w\u2217\u2016 is also an optimal solution of (3) and that \u2016w\u2217\u2016 = 1. For the case of 0 \u2208 int(U), we can similarly show that the optimal\nvalue of (5) is negative and that an optimal solution w \u2217 of (5) exists such that \u2016w\u2217\u2016 = 1.\nFor 0 \u2208 int(U), RCM (2) is essentially a non-convex problem, and we need to use non-convex optimization methods to solve it. Section 5 describes an optimization algorithm for non-convex problems of (2)."}, {"heading": "3. Equivalence to Existing Classifiers", "text": "We will show that RCM can be reduced to support vector machine (SVM), minimax probability machine (MPM), or Fisher discriminant analysis (FDA) depending on the prescribed uncertainty set U . In Table 1, \u201c\u00d7\u201d means that the corresponding cases never happen. \u201c \u221a \u201d means that there are no corresponding existing models as far as we know. The models indicated by \u221a are the target in this paper.\nWe denote an optimal solution of (2) as w\u2217 and define the bias term b such that the decision boundary passes through the mid-point of x\u2217+ and x \u2217 \u2212, i.e., b = \u2212(x\u2217+ + x \u2217 \u2212) > w\n\u2217/2. Here, x\u2217+ \u2208 U+ and x\u2217\u2212 \u2208 U\u2212 stand for the optimal solutions of the inner-minimization in (2) for w = w\u2217."}, {"heading": "3.1. Hard-Margin SVM, \u03bd-SVM and E\u03bd-SVM", "text": "Whenever a data set is linearly separable, there are many hyperplanes that correctly classify all training samples. Vapnik-Chervonenkis theory indicates that a large margin classifier has a small generalization error. The problem can be transformed into a quadratic programming problem and the classification method is called hard-margin support vector classification machine (HM-SVM). Here, we define the uncertainty set (convex hull, CH) as follows:\nU\u00b1 = conv{xi \u2223\u2223 i \u2208M\u00b1}, (6)\nwhere conv means convex hull. By using the Wolfe duality, the equivalence of HM-SVM and RCM (4) is obvious for U+ \u2229 U\u2212 = \u2205. HM-SVM has been extended to cope with nonseparable data. C-SVM (Cortes & Vapnik, 1995) and \u03bd-SVM (Scho\u0308lkopf et al., 2000) are typical examples\nof \u201csoft-margin\u201d SVMs. There is a correspondence between C-SVM and \u03bd-SVM. That is, the classifier estimated by C-SVM with C \u2208 (0,\u221e) can be obtained from \u03bd-SVM with a parameter \u03bd \u2208 (\u03bdmin, \u03bdmax] \u2282 [0, 1], and vice versa. Crisp and Burges (2000) showed \u03bdmax = 2min{m+,m\u2212}/m and gave a geometric interpretation for \u03bdmin. For \u03bd \u2208 (\u03bdmax, 1], the optimization problem of \u03bd-SVM is unbounded, and for \u03bd \u2208 [0, \u03bdmin), \u03bd-SVM provides a trivial solution (w = 0 and b = 0). Perez-Cruz et al. (2003) devised extended \u03bd-SVM (E\u03bdSVM) as a way of avoiding such a trivial solution:\nmin w,b,\u03be,\u03c1 \u2212 \u03bd\u03c1 + 1 m\nm\u2211\ni=1\n\u03bei (7)\ns.t. yi(x > i w + b) \u2265 \u03c1\u2212 \u03bei, \u03bei \u2265 0, i \u2208M, \u2016w\u20162 = 1.\nBy forcing the norm of w to be unity, a non-trivial and meaningful solution is obtained for any \u03bd \u2208 [0, \u03bdmin), but this comes at the expense of convexity. It furthermore provides the same solution as \u03bd-SVM for other values of \u03bd. In that sense, E\u03bd-SVM can be regarded as an extension of \u03bd-SVM. It was experimentally found in (Perez-Cruz et al., 2003) that E\u03bd-SVM often has better generalization performance than \u03bd-SVM.\nIn order to connect (E)\u03bd-SVM with RCM, we define U\u03bd\u00b1 as\n8\n<\n:\nX\ni\u2208M\u00b1\n\u03bbixi \u02db \u02db\nX\ni\u2208M\u00b1\n\u03bbi = 1, 0 \u2264 \u03bbi \u2264 2\n\u03bdm , i \u2208 M\u00b1\n9\n=\n;\n. (8)\nThe set (8) is essentially equal to a reduced convex hull (RCH) (Bennett & Bredensteiner, 2000) or soft convex hull (Crisp & Burges, 2000). For linearly nonseparable data set, U\u03bd+ and U\u03bd\u2212 intersect with small \u03bd. Crisp and Burges (2000) showed that \u03bdmin is the largest \u03bd such that two RCHs, U\u03bd+ and U\u03bd\u2212, intersect. The model that finds \u03bdmin corresponds to the case of 0 \u2208 bd(U\u03bd) in the \u201cRCH\u201d of Table 1. Barbero et al. (2012) transformed \u03bd-SVM and E\u03bd-SVM (7) into RCM (2) with U\u03bd\u00b1 in order to give them a geometric interpretation. Using the results, we can relate \u03bd-SVM, E\u03bd-SVM, and RCM (2) as shown in Table 1."}, {"heading": "3.2. Minimax Probability Machine and Its Extension", "text": "The minimax probability machine (MPM) only uses the mean and covariance matrix of each class for classification tasks (Lanckriet et al., 2002). Suppose that x+ (or x\u2212) is a d-dimensional random vector with mean x\u0304+ (or x\u0304\u2212) and covariance \u03a3+ (or \u03a3\u2212). We assume that x\u0304+ 6= x\u0304\u2212 and that \u03a3\u00b1 are positive definite. The MPM minimizes the misclassification probabili-\nties under the worst-case setting as follows:\nmax \u03b1,w,b \u03b1 s.t. inf x\u00b1\u223c(x\u0304\u00b1,\u03a3\u00b1)\nPr{x>\u00b1w + b \u2265 0} \u2265 \u03b1, (9)\nwhere x+ \u223c (x\u0304+,\u03a3+) refers to the class of distributions that have mean x\u0304+ and covariance \u03a3+, but are otherwise arbitrary; likewise for x\u2212. In practice, the mean vectors and covariance matrices of each class are estimated from the training samples.\nLanckriet et al. (2002) represented problem (9) as a convex optimization problem known as a second-order cone program (SOCP) and show the dual form:\nmin \u03ba\n\u03ba s.t. 0 \u2208 U\u03ba := U\u03ba+ U\u03ba\u2212, (10)\nwhere U\u03ba\u00b1 = {x\u0304\u00b1 + \u03a31/2\u00b1 u \u2223\u2223 \u2016u\u2016 \u2264 \u03ba}. (11)\n\u03b1 of (9) corresponds to \u03ba of (10) as \u03ba = \u221a\n\u03b1/(1\u2212 \u03b1). Therefore, MPM (9) is the problem to find the smallest positive \u03ba (denoted by \u03bamax) such that the two ellipsoids intersect, i.e., 0 \u2208 bd(U\u03bamax). The idea of MPM is combined with the idea of the margin maximization in (Nath & Bhattacharyya, 2007). Given acceptable false positive and negative rates, \u03b7+ and \u03b7\u2212, the linear classifier can be estimated by\nmin w,b\n1 2 \u2016w\u20162 s.t. sup\nx\u00b1\u223c(x\u0304\u00b1,\u03a3\u00b1)\nPr{x>\u00b1w + b < 0} \u2264 \u03b7\u00b1. (12)\nIn this paper, we call this model the \u201cmargin maximized MPM\u201d (MM-MPM). In the same way as in MPM, (12) can be transformed into an SOCP.\nRobust optimization techniques for ellipsoidal uncertainty (Ben-Tal et al., 2009) transform RCM (2) with U\u00b1 = U\u03ba\u00b1\u00b1 into\nmin \u2016w\u20162=1\n\u03ba+\u2016\u03a3 1/2 + w\u2016 + \u03ba\u2212\u2016\u03a3 1/2 \u2212 w\u2016 \u2212 (x\u0304+ \u2212 x\u0304\u2212) > w. (13)\nWe define \u03bamax+ and \u03ba max \u2212 as constants such that U \u03ba+ + and U\u03ba\u2212\u2212 touch. For \u03ba\u00b1 \u2208 [0, \u03bamax\u00b1 ), U \u03ba+ + \u2229 U \u03ba\u2212 \u2212 = \u2205 holds, and RCM (13) is equivalent to MM-MPM (12) with \u03ba\u00b1 = \u221a (1\u2212 \u03b7\u00b1)/\u03b7\u00b1. We can confirm this by comparing the dual form of MM-MPM and the dual of (13), that is equivalent to (4). Furthermore, (13) with \u03ba\u00b1 = \u03bamax coincides with MPM (9) (see Table 1)."}, {"heading": "3.3. Fisher Discriminant Analysis and Its Extension", "text": "In Fisher discriminant analysis (FDA) as in MPM (9), a discriminant hyperplane is computed from the means and covariances of random vectors x+ and x\u2212. The hyperplane is determined from the optimal solution w \u2217 to the following problem (Fukunaga, 1990):\nmax w (x\u0304+ \u2212 x\u0304\u2212)>w \u2016(\u03a3+ + \u03a3\u2212)1/2w\u2016 . (14)\nThe problem finds a direction which maximizes the projected class means while minimizing the class variance in this direction.\nLikewise for MPM, FDA has a probabilistic interpretation under the worst-case scenario. Using the ellipsoidal uncertainty set defined by\nU\u03b6 = {x = (x\u0304+ \u2212 x\u0304\u2212) + (\u03a3+ + \u03a3\u2212)1/2u \u2223\u2223 \u2016u\u2016 \u2264 \u03b6},\n(15) FDA (14) can be represented as\nmin \u03b6\n\u03b6 s.t. 0 \u2208 U\u03b6 . (16)\nFDA can be extended to RCM (2) with the uncertainty set U\u03b6 for a prescribed parameter \u03b6 > 0. Let \u03b6max be the optimal value of (16). Then, along the same lines as the MPM in Section 3.2, we find that RCM (2) with U = U\u03b6max is equivalent to FDA. Indeed, RCM (2) with U\u03b6 is transformed into\nmin \u2016w\u20162=1\n\u03b6\u2016(\u03a3+ + \u03a3\u2212)1/2w\u2016 \u2212 (x\u0304+ \u2212 x\u0304\u2212)>w.\nEspecially for \u03b6 \u2208 [0, \u03b6max), the norm constraint is replaced with the convex constraint \u2016w\u20162 \u2264 1 without changing the optimal solution. Here, MM-FDA refers to this estimator. In replacing the Euclidean norm \u2016w\u2016 with the L1-norm \u2016w\u20161, MM-FDA is equivalent to a sparse feature selection model based on FDA (FSFD) (Bhattacharyya, 2004)."}, {"heading": "4. Statistical Interpretation for RCM", "text": "We can give a statistical interpretation for RCM on the basis of statistical learning theory. Let us start by introducing a loss function ` : R\u2192 R that defines the loss of the decision function x>w + b regarding the sample (x, y) as `(y(x>w + b)).\nA goal of the classification task is to obtain an accurate classifier. For this purpose, it is reasonable to minimize the expected loss, E[`(y(x>w + b))], with respect to w and b. Let us define p(x|y) as the conditional probability density of x, given the binary label y, and \u03c0+ and \u03c0\u2212 as the marginal probabilities of the positive and negative labels, respectively. E[`(y(x>w + b))] is computed by\n\u03c0+ \u222b\n`(x>w + b)p(x|+ 1)dx +\u03c0\u2212 \u222b `(\u2212(x>w + b))p(x| \u2212 1)dx.\nSince the true probability distribution is unknown, we cannot minimize the expected loss directly.\nNow let us consider the ambiguity of the probability distribution p(x|y). Let P+ and P\u2212 be sets of probability densities. Each set of probabilities expresses the uncertainty of the conditional probabilities p(x| + 1) and p(x| \u2212 1), respectively. We can use the min-max decision rule for the uncertainty of p(x|y) as follows:\nmin w:\u2016w\u20162=1 max p(x|\u00b11)\u2208P\u00b1 min b\u2208R\nE[`(y(x>w + b))]. (17)\nThe worst-case minimization problem is difficult to solve. Therefore, we propose to solve RCM (2), since we can prove that RCM (2) is a good approximation for minimizing the worst-case expected loss.\nTo relate (17) and RCM, we firstly give an equivalent formulation for RCM. Here, we define x+ and x\u2212 as the mean of the input vector x under the conditional probabilities p(x|+ 1) and p(x| \u2212 1), respectively, i.e., x\u00b1 = \u222b xp(x| \u00b1 1)dx. Here, we assume that all probability distributions in P\u00b1 have the mean vector. Let U+ and U\u2212 be\nU\u00b1 = { \u222b xp(x| \u00b1 1)dx \u2223\u2223\u2223\u2223 p(x| \u00b1 1) \u2208 P\u00b1 } . (18)\nSuppose that the uncertainty sets of probability densities, P\u00b1, are both convex; i.e., a mixture of two probability densities also lies in the uncertainty set. Then U+ and U\u2212 are convex sets. Theorem 4.1. Suppose that `(z) is a non-increasing function. An optimal solution of the RCM with the uncertainty sets U+ and U\u2212 in (18) is also optimal to\nmin w:\u2016w\u20162=1 max x\u00b1\u2208U\u00b1 min b\u2208R J`(w, b;x+,x\u2212), (19)\nwhere\nJ`(w, b;x+,x\u2212) = \u03c0+`(x > +w + b) + \u03c0\u2212`(\u2212x>\u2212w \u2212 b).\nProof. For a fixed w and x\u00b1 \u2208 U\u00b1, minimizing J`(w, b;x+,x\u2212) respect to b is equivalent to\nmin b\u2032\n\u03c0+`((x+ \u2212 x\u2212)>w \u2212 b\u2032) + \u03c0\u2212`(b\u2032).\nSince the objective function above is non-increasing in (x+ \u2212 x\u2212)>w, there exists a non-increasing function \u03c6(z) such that\n\u03c6((x+ \u2212 x\u2212)>w) = min b J`(w, b;x+,x\u2212).\nHence, one has\nmin w:\u2016w\u20162=1 max x\u00b1\u2208U\u00b1 min b J`(w, b;x+,x\u2212)\n= \u03c6( max w:\u2016w\u20162=1 min x\u00b1\u2208U\u00b1\n(x+ \u2212 x\u2212)>w).\nAs a result, the optimal solution of the RCM is also optimal for problem (19).\nTheorem 4.2. We assume that i) `(z) is convex, decreasing, and second-order differentiable, and that ii) 0 \u2264 `\u2032\u2032(z) \u2264 L \u2208 R holds for all z. Suppose that x\u00b1 \u2208 U\u00b1 is in the ball with the radius c, i.e., \u2016x\u00b1\u2016 \u2264 c. Then, for the optimal value J\u2217 of (19), one has\nmin w:\n\u2016w\u20162=1\nmax p(x|\u00b11)\u2208P\u00b1 min b\u2208R\nE[`(y(x>w+b))] \u2208 [J\u2217, J\u2217+Lc 2\n2 ].\nProof. The convexity of `(z) leads to a lower bound, J`(w, b;x+,x\u2212), and the Taylor expansions of `(z) around z = x>+w + b and z = x > \u2212w + b yield an upper bound, J`(w, b;x+,x\u2212) + Lc2 2 , of E[`(y(x >\nw + b))]. Even when the min-max operation is applied, J\u2217 and J\u2217 + Lc 2\n2 remain bounds for the worst-case expected loss (17).\nThe theorem implies that problem (19) minimizes the bounds of (17). Noticing that the optimal solution of problem (19) is available by solving RCM as shown in Theorem 4.1, Theorem 4.2 implies that RCM minimizes the upper and lower bounds of the worst-case expected loss (17) at the same time.\nThere are various ways to estimate the bias term b for RCM. The simplest way is to use b\u2217 = \u2212(x\u2217+ + x \u2217 \u2212) > w\n\u2217/2. Another promising method is to construct an appropriate statistical model for the projected samples (x>i w\n\u2217, yi), i \u2208M . The projected samples, x>i w\u2217, i \u2208 M , are scattered in one-dimensional space, from which we can estimate b on the basis of the statistical model."}, {"heading": "5. Solution Method for RCM", "text": "The RCM has a significantly larger range of parameter \u03ba or \u03b6 than an existing convex model such as MPM, MM-MPM, FDA or FS-FD (see Table 1). Therefore, the RCM enhances a possibility of improving these existing classification models. Indeed, Perez-Cruz et al. (2003) experimentally showed that the generalization performance of E\u03bd-SVM is often better than that of original \u03bd-SVM. In this section, we propose a solution method that is generalized from the local algorithms of (Perez-Cruz et al., 2003; Takeda & Sugiyama, 2008)."}, {"heading": "5.1. Two-stage Optimization Strategy", "text": "Suppose that we solve RCM (2) with the uncertainty set U\u03b7 with one parameter \u03b7 and that U\u03b71 \u2282 int(U\u03b72) holds for \u03b71 < \u03b72. Let us define \u03b7max such that the optimal value of (2) with U = U\u03b7max is zero. First, we need to compute \u03b7max in order to confirm that the given problem (2) is essentially convex or not.\nAlgorithm 5.1.\nThe parameter \u03b7max is obtained as the optimal solution of the convex problem:\nmin \u03b7\n\u03b7 s.t. 0 \u2208 U\u03b7. (20)\nWhen U\u03b7\u00b1 are ellipsoidal sets of (11) (or (15)), the problem reduces to MPM (10) (or FDA (16)). When U\u03b7\u00b1 are RCHs, the problem reduces to a linear programming problem and gives us \u03bdmin.\nIf the input parameter \u03b7 is equal to \u03b7max, we have already obtained an optimal solution from (20). If \u03b7 < \u03b7max, we next solve the convex problem (4) by using a standard optimization software."}, {"heading": "5.2. Local Optimization Algorithm for Non-convex RCM", "text": "For \u03b7 > \u03b7max, RCM (2) is essentially equivalent to (5) that includes a non-convex constraint, \u2016w\u20162 \u2265 1. We next need to solve (2) as a non-convex problem.\nIn the area of global optimization, non-convex RCM (5) (precisely, a problem constructed by taking dual for the inner-minimization in (5)) is known as a reverse convex program (RCP), or canonical d.c. programming. This differs from a conventional convex program only by the presence of a reverse convex constraint (\u2016w\u20162 \u2265 1 in the current case). When all functions are linear except for the reverse convex constraint, the RCP problem is especially called linear reverse convex program (LRCP). E\u03bd-SVM is an LRCP, for which Perez-Cruz et al. (2003) proposed a local optimum search algorithm and Takeda and Sugiyama (2008) proposed a global optimum search algorithm.\nHere, we show a local optimum search algorithm (Algorithm 5.1) that is generalized from the local algorithms (Perez-Cruz et al., 2003; Takeda & Sugiyama,\n2008) of E\u03bd-SVM for non-convex RCM. It is essentially the same as the local algorithm, Algorithm 7, in (Takeda & Sugiyama, 2008) when U of g(w) is an RCH (8) and = 0.\nRCM (2) requires maximizing g(w) = minx\u2208U x > w subject to a non-convex constraint, w>w = 1. Instead of solving the non-convex problem directly, we can iteratively solve the relaxation problems (21) (in Algorithm 5.1). Since g(w) is concave, (21) can be solved by using convex minimization techniques.\nThe non-convex constraint of (2) is linearized at w\u0303t in the algorithm, and the linear constraint w\u0303>t w = 1 is updated every iteration. Note that the negativity of the optimal value of (21) is guaranteed because of 0 \u2208 int(U). As the algorithm proceeds, the solutions w\u0303t improve, i.e.,\ng(w\u0303t) \u2264 g(w\u0302\u2217t ) < g(w\u0302\u2217t /\u2016w\u0302\u2217t \u2016) = g(w\u0303t+1) < 0, (22)\nbecause w\u0303>t w\u0302 \u2217 t = 1 together with w\u0303 > t w\u0303t = 1 implies \u2016w\u0302\u2217t \u2016 > 1. Note that w\u0303t is a feasible solution for (21). Hence, if (21) has no better solutions than w\u0303t, w\u0303t is returned as an optimal solution w\u0302 \u2217 t of (21). The algorithm terminates after that.\nThe computation of g(w) may be difficult for general uncertainty sets. However, we do not need an explicit formula for g(w) in (21). If U is a convex set, we can obtain a dual formulation (max-problem) for g(w) and replace the max-min problem (21) with a simple max-problem, that is, a one-level convex problem. Indeed, when the uncertainty set is an RCH (8) of data points, we can take the dual for g(w) = minx\u2208U x > w and change (21) into a linearized E\u03bd-SVM (7) whose constraint is w\u0303>t w = 1 instead of \u2016w\u20162 = 1. When the algorithm is applied to the RCM having ellipsoidal uncertainty, we analytically obtain the optimal value g(w) for any w. Indeed, for ellipsoidal uncertainty (11), g(w) is equal to the one derived by multiplying the objective function of (13) by -1.\nTheorem 5.2. For any > 0, Algorithm 5.1 terminates in a finite number of iterations.\nProof. Let the negative value gopt be the optimal value of RCM (2). Suppose \u2016w\u0302\u2217t \u2016 > 1 for all t = 1, 2, . . .. Otherwise, the algorithm terminates. By evaluating g(w\u0303t+1)\u2212 g(w\u0303t), we have\n\u221e\u2211\nt=0\n(g(w\u0303t+1)\u2212 g(w\u0303t)) \u2265 \u221e\u2211\nt=0\n( 1\n\u2016w\u0302\u2217t \u2016 \u2212 1\n) gopt > 0.\nThe above inequality and the boundedness of g(w\u0303t) lead to limt\u2192\u221e\n1 \u2016 bw\u2217t \u2016 \u2212 1 = 0. Therefore, \u03b3t exists such that \u2016w\u0302\u2217t \u2016 = 1 + \u03b3t, 0 < \u03b3t = o(1), that leads\nto \u2016w\u0303t \u2212 w\u0302\u2217t \u2016 = \u221a\n2\u03b3t + \u03b32t . Since \u03b3t \u2192 0 holds, the stopping rule \u2016w\u0303t\u2212w\u0302\u2217t \u2016 \u2264 with positive is satisfied in a finite number of iterations.\nWe can show that Algorithm 5.1 with = 0 terminates within a finite number of iterations when the uncertainty set of RCM (2) is represented by a convex polyhedron by mimicking the proof of Theorem 8 for E\u03bd-SVM in (Takeda & Sugiyama, 2008).\nHere, suppose that g(w\u0303\u2217) is differentiable, i.e., g(w) has a unique subgradient at w\u0303\u2217 as \u2202g(w\u0303\u2217) = arg minx\u2208U x > w\u0303\n\u2217 = {\u2207g(w\u0303\u2217)}. For example, g(w) is differentiable under ellipsoidal uncertainty (11) (or (15)). Then Theorem 5.3 shows a sufficient condition for the local optimality of the solution w\u0303\u2217 if it is obtained by Algorithm 5.1 with = 0.\nTheorem 5.3. Suppose that g(w\u0303\u2217) is differentiable. Algorithm 5.1 that terminates with = 0 provides a local solution w\u0303\u2217 to RCM (2) when the maximum eigenvalue of \u22072g(w\u0303\u2217) is less than g(w\u0303\u2217).\nProof. Note that w\u0303\u2217 is the optimal solution of (21) at the final iteration. Therefore, w\u0303\u2217 satisfies the firstand second-order necessary conditions:\n\u2207g(w\u0303\u2217) + \u03b7w\u0303\u2217 = 0, (23) d >\u22072g(w\u0303\u2217)d \u2264 0, \u2200d such that w\u0303\u2217>d = 0, (24)\nwhere \u03b7 is a Lagrange multiplier. We can show \u03b7 = \u2212g(w\u0303\u2217) > 0 by noticing that \u2207g(w\u0303\u2217) is a minimizer to minx\u2208U x > w\u0303\n\u2217 = g(w\u0303\u2217) and using (23). When the maximum eigenvalue of \u22072g(w\u0303\u2217) is less than \u2212\u03b7, w\u0303\u2217 satisfying (23) and (24) also satisfies the second-order sufficient conditions for the local optimality of (2):\n\u2207g(w) + 2\u03b6w = 0, d>(\u22072g(w) + 2\u03b6I)d < 0, \u2200d 6= 0 such that w>d = 0,\nwhere I is the identity matrix and \u03b6(\u2265 0) is a multiplier. This shows that w\u0303\u2217 is a local solution of (2).\nTheorem 5.3 may be extendable to the nondifferentiable case of g(w\u0303\u2217), though more assumptions are necessary (see Theorems 3.2.16, 3.2.20 and 3.2.21 in (Polak, 1997))."}, {"heading": "6. Conclusions", "text": "We developed the robust classification model (RCM), a model which includes SVM, MPM and FDA for specific uncertainty sets. The choice of uncertainty set is significant in this model. This model enables extensions and improvements to SVM to be applied to MPM and FDA, and vice versa.\nThe unified model will be of help in clarifying relationships among existing models and in finding new classifiers and new algorithms. That is, we might be able to devise a new classifier by finding a reasonable uncertainty set for RCM. It will be important to see how the learning algorithm, uncertainty set, and prediction accuracy relate to each other."}], "references": [{"title": "Geometric intuition and algorithms for E\u03bd-svm", "author": ["\u00c1. Barbero", "A. Takeda", "J. L\u00f3pez"], "venue": null, "citeRegEx": "Barbero et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Barbero et al\\.", "year": 2012}, {"title": "Robust Optimization", "author": ["A. Ben-Tal", "L. El-Ghaoui", "A. Nemirovski"], "venue": null, "citeRegEx": "Ben.Tal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.Tal et al\\.", "year": 2009}, {"title": "Duality and geometry in SVM classifiers", "author": ["K.P. Bennett", "E.J. Bredensteiner"], "venue": "In ICML, pp", "citeRegEx": "Bennett and Bredensteiner,? \\Q2000\\E", "shortCiteRegEx": "Bennett and Bredensteiner", "year": 2000}, {"title": "Second order cone programming formulations for feature selection", "author": ["C. Bhattacharyya"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bhattacharyya,? \\Q2004\\E", "shortCiteRegEx": "Bhattacharyya", "year": 2004}, {"title": "Minimum distance to the complement of a convex set: Duality result", "author": ["W. Briec"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Briec,? \\Q1997\\E", "shortCiteRegEx": "Briec", "year": 1997}, {"title": "A geometric interpretation of \u03bd-SVM classifiers", "author": ["D.J. Crisp", "C.J.C. Burges"], "venue": "In NIPS, pp", "citeRegEx": "Crisp and Burges,? \\Q2000\\E", "shortCiteRegEx": "Crisp and Burges", "year": 2000}, {"title": "Introduction to statistical pattern recognition", "author": ["K. Fukunaga"], "venue": null, "citeRegEx": "Fukunaga,? \\Q1990\\E", "shortCiteRegEx": "Fukunaga", "year": 1990}, {"title": "A robust minimax approach to classification", "author": ["G.R.G. Lanckriet", "Ghaoui", "L. El", "C. Bhattacharyya", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2002}, {"title": "Optimization by Vector Space Methods", "author": ["D.G. Luenberger"], "venue": null, "citeRegEx": "Luenberger,? \\Q1969\\E", "shortCiteRegEx": "Luenberger", "year": 1969}, {"title": "Maximum margin classifiers with specified false positive and false negative error rates", "author": ["J.S. Nath", "C. Bhattacharyya"], "venue": "In Proceedings of the seventh SIAM International Conference on Data mining,", "citeRegEx": "Nath and Bhattacharyya,? \\Q2007\\E", "shortCiteRegEx": "Nath and Bhattacharyya", "year": 2007}, {"title": "Extension of the \u03bd-SVM range for classification", "author": ["F. Perez-Cruz", "J. Weston", "D.J.L. Hermann", "B. Sch\u00f6lkopf"], "venue": "In Advances in Learning Theory: Methods, Models and Applications", "citeRegEx": "Perez.Cruz et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Perez.Cruz et al\\.", "year": 2003}, {"title": "Optimization: Algorithms and Consistent Approximations", "author": ["E. Polak"], "venue": null, "citeRegEx": "Polak,? \\Q1997\\E", "shortCiteRegEx": "Polak", "year": 1997}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "New support vector algorithms", "author": ["B. Sch\u00f6lkopf", "A. Smola", "R. Williamson", "P. Bartlett"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2000}, {"title": "\u03bd-support vector machine as conditional value-at-risk minimization", "author": ["A. Takeda", "M. Sugiyama"], "venue": "In ICML, pp", "citeRegEx": "Takeda and Sugiyama,? \\Q2008\\E", "shortCiteRegEx": "Takeda and Sugiyama", "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "The minimax probability machine (MPM) (Lanckriet et al., 2002) and Fisher discriminant analysis (FDA) (Fukunaga, 1990) also address the binary classification problem.", "startOffset": 38, "endOffset": 62}, {"referenceID": 6, "context": ", 2002) and Fisher discriminant analysis (FDA) (Fukunaga, 1990) also address the binary classification problem.", "startOffset": 47, "endOffset": 63}, {"referenceID": 1, "context": "The purpose of this paper is to provide a unified framework for learning algorithms, including SVM, MPM, and FDA, from the viewpoint of robust optimization (Ben-Tal et al., 2009).", "startOffset": 156, "endOffset": 178}, {"referenceID": 13, "context": "When U+ and U\u2212 are defined as reduced convex hulls (Bennett & Bredensteiner, 2000), (2) reduces to \u03bd-SVM (Sch\u00f6lkopf et al., 2000) if U+ \u2229 U\u2212 = \u2205 and reduces to E\u03bd-SVM (Perez-Cruz et al.", "startOffset": 105, "endOffset": 129}, {"referenceID": 10, "context": ", 2000) if U+ \u2229 U\u2212 = \u2205 and reduces to E\u03bd-SVM (Perez-Cruz et al., 2003), otherwise.", "startOffset": 45, "endOffset": 70}, {"referenceID": 10, "context": "\u2019s extension (Perez-Cruz et al., 2003) from convex \u03bd-SVM to nonconvex E\u03bd-SVM.", "startOffset": 13, "endOffset": 38}, {"referenceID": 8, "context": "By applying the discussion on the minimum norm duality (Luenberger, 1969) to (3), we can confirm the equivalence of (3) and minx\u2208U \u2016x\u2016, and the optimal solution w = x\u2217/\u2016x\u2217\u2016.", "startOffset": 55, "endOffset": 73}, {"referenceID": 4, "context": "1 of (Briec, 1997) under the assumption that a convex U has a nonempty interior.", "startOffset": 5, "endOffset": 18}, {"referenceID": 13, "context": "C-SVM (Cortes & Vapnik, 1995) and \u03bd-SVM (Sch\u00f6lkopf et al., 2000) are typical examples of \u201csoft-margin\u201d SVMs.", "startOffset": 40, "endOffset": 64}, {"referenceID": 5, "context": "Crisp and Burges (2000) showed \u03bdmax = 2min{m+,m\u2212}/m and gave a geometric interpretation for \u03bdmin.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "Crisp and Burges (2000) showed \u03bdmax = 2min{m+,m\u2212}/m and gave a geometric interpretation for \u03bdmin. For \u03bd \u2208 (\u03bdmax, 1], the optimization problem of \u03bd-SVM is unbounded, and for \u03bd \u2208 [0, \u03bdmin), \u03bd-SVM provides a trivial solution (w = 0 and b = 0). Perez-Cruz et al. (2003) devised extended \u03bd-SVM (E\u03bdSVM) as a way of avoiding such a trivial solution:", "startOffset": 0, "endOffset": 266}, {"referenceID": 10, "context": "It was experimentally found in (Perez-Cruz et al., 2003) that E\u03bd-SVM often has better generalization performance than \u03bd-SVM.", "startOffset": 31, "endOffset": 56}, {"referenceID": 4, "context": "Crisp and Burges (2000) showed that \u03bdmin is the largest \u03bd such that two RCHs, U\u03bd + and U\u03bd \u2212, intersect.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Barbero et al. (2012) transformed \u03bd-SVM and E\u03bd-SVM (7) into RCM (2) with U\u03bd \u00b1 in order to give them a geometric interpretation.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": "The minimax probability machine (MPM) only uses the mean and covariance matrix of each class for classification tasks (Lanckriet et al., 2002).", "startOffset": 118, "endOffset": 142}, {"referenceID": 1, "context": "Robust optimization techniques for ellipsoidal uncertainty (Ben-Tal et al., 2009) transform RCM (2) with U\u00b1 = U\u00b1 \u00b1 into", "startOffset": 59, "endOffset": 81}, {"referenceID": 6, "context": "w \u2217 to the following problem (Fukunaga, 1990): max w (x\u0304+ \u2212 x\u0304\u2212)w \u2016(\u03a3+ + \u03a3\u2212)1/2w\u2016 .", "startOffset": 29, "endOffset": 45}, {"referenceID": 3, "context": "In replacing the Euclidean norm \u2016w\u2016 with the L1-norm \u2016w\u20161, MM-FDA is equivalent to a sparse feature selection model based on FDA (FSFD) (Bhattacharyya, 2004).", "startOffset": 136, "endOffset": 157}, {"referenceID": 10, "context": "In this section, we propose a solution method that is generalized from the local algorithms of (Perez-Cruz et al., 2003; Takeda & Sugiyama, 2008).", "startOffset": 95, "endOffset": 145}, {"referenceID": 10, "context": "Indeed, Perez-Cruz et al. (2003) experimentally showed that the generalization performance of E\u03bd-SVM is often better than that of original \u03bd-SVM.", "startOffset": 8, "endOffset": 33}, {"referenceID": 10, "context": "E\u03bd-SVM is an LRCP, for which Perez-Cruz et al. (2003) proposed a local optimum search algorithm and Takeda and Sugiyama (2008) proposed a global optimum search algorithm.", "startOffset": 29, "endOffset": 54}, {"referenceID": 10, "context": "E\u03bd-SVM is an LRCP, for which Perez-Cruz et al. (2003) proposed a local optimum search algorithm and Takeda and Sugiyama (2008) proposed a global optimum search algorithm.", "startOffset": 29, "endOffset": 127}, {"referenceID": 10, "context": "1) that is generalized from the local algorithms (Perez-Cruz et al., 2003; Takeda & Sugiyama, 2008) of E\u03bd-SVM for non-convex RCM.", "startOffset": 49, "endOffset": 99}, {"referenceID": 11, "context": "21 in (Polak, 1997)).", "startOffset": 6, "endOffset": 19}], "year": 0, "abstractText": "A wide variety of machine learning algorithms such as support vector machine (SVM), minimax probability machine (MPM), and Fisher discriminant analysis (FDA), exist for binary classification. The purpose of this paper is to provide a unified classification model that includes the above models through a robust optimization approach. This unified model has several benefits. One is that the extensions and improvements intended for SVM become applicable to MPM and FDA, and vice versa. Another benefit is to provide theoretical results to above learning methods at once by dealing with the unified model. We give a statistical interpretation of the unified classification model and propose a non-convex optimization algorithm that can be applied to nonconvex variants of existing learning methods.", "creator": "LaTeX with hyperref package"}}}