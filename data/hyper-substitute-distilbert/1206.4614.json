{"id": "1206.4614", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Information-theoretic Semi-supervised Metric Learning via Entropy Regularization", "abstract": "we propose a general information - theoretic approach applying seraph ( semi - generalized metric convergence paradigm. less - information ) for numerical structure models if not hold upon the observation parameters. given the probability parameterized about a mahalanobis chart, we maximize the entropy of that probability mean real data and calculate precedence on unlabeled data called proper regularization, which allows the supervised locally unsupervised component to manage integrated in a coordinated and chaotic way. furthermore, provides rules for efficiently distinguishing a k - rank projection coefficient from another metric. the result of seraph typically solved mechanically while implemented by an em - rated scheme poorly defined analytical e - step and convex m - step. experiments demonstrate that seraph is using relatively many well - utilized local weighted local metric learning methods.", "histories": [["v1", "Mon, 18 Jun 2012 15:01:43 GMT  (492kb)", "http://arxiv.org/abs/1206.4614v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["gang niu", "bo dai", "makoto yamada", "masashi sugiyama"], "accepted": true, "id": "1206.4614"}, "pdf": {"name": "1206.4614.pdf", "metadata": {"source": "META", "title": "Information-theoretic Semi-supervised Metric Learningvia Entropy Regularization", "authors": ["Gang Niu", "Bo Dai", "Makoto Yamada", "Masashi Sugiyama"], "emails": ["GANG@SG.CS.TITECH.AC.JP", "BOHR.DAI@GMAIL.COM", "YAMADA@SG.CS.TITECH.AC.JP", "SUGI@CS.TITECH.AC.JP"], "sections": [{"heading": "1. Introduction", "text": "A good metric for input data is a key factor for many machine learning algorithms. Classical metric learning methods fall into three types: (a) Supervised type requiring class labels (e.g., Sugiyama, 2007); (b) Supervised type requiring weak labels, i.e., {\u00b11}-valued labels that indicate the similarity/dissimilarity of data pairs (e.g., Weinberger et al., 2005; Davis et al., 2007); (c) Unsupervised type requiring no label information (e.g., Belkin & Niyogi, 2001). Types (a) and (b) have a strict limitation for real-world applica-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ntions since they need lots of labels. Based on the belief that preserving the geometric structure in an unsupervised manner can be better than relying on the limited labels, semisupervised metric learning has emerged. To the best of our knowledge, all semi-supervised extensions employ off-theshelf techniques in type (c) such as principal component analysis (Yang et al., 2006; Sugiyama et al., 2010) or manifold embedding (Hoi et al., 2008; Baghshah & Shouraki, 2009; Liu et al., 2010). They can be regarded as propagating labels along an assistant metric by some unsupervised techniques and learning a target metric implicitly in a supervised manner.\nHowever, the target and assistant metrics assume different forms, one Mahalanobis distance defined over a Euclidean space and one geodesic distance over a curved space or a Riemannian manifold. The two metrics also share slightly different goals: the target metric tries to learn a metric so that data in the same class are close and data from different classes are far apart (e.g., Fisher discriminant analysis1 (Fisher, 1936)), and the assistant one tries to identify and preserve the intrinsic geometric structure (e.g., Laplacian eigenmaps (Belkin & Niyogi, 2001)). Simply putting them together works in practice, but the paradigm is conceptually neither natural nor unified.\nIn this paper, we propose a semi-supervised metric learning approach SERAPH (SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) as an information-theoretic alternative to the manifold-based methods. Our idea is to optimize a metric by optimizing a conditional probability parameterized by that metric. Following entropy regularization (Grandvalet & Bengio, 2004), we maximize the entropy of that probability on labeled data, and minimize it\n1Note that learning a metric is equivalent to learning a projection in the scenario of dimensionality reduction.\non unlabeled data, which can achieve the sparsity of the posterior distribution (Grac\u0327a et al., 2009), i.e., the low uncertainty/entropy of unobserved weak labels. Furthermore, we employ mixed-norm regularization (Ying et al., 2009) to encourage the sparsity of the projection matrix, i.e., the low rank of the projection matrix induced from the metric. Unifying the posterior sparsity and the projection sparsity brings us to the hyper-sparsity. Thanks to this property, the metric learned by SERAPH possesses high discriminability even under a noisy environment.\nOur contributions can be summarized as follows. First, we formulate the supervised metric learning problem as an instance of the generalized maximum entropy distribution estimation (Dud\u0131\u0301k & Schapire, 2006). Second, we propose a semi-supervised extension of the above estimation following entropy regularization (Grandvalet & Bengio, 2004). Notice that our extension is compatible with the manifoldbased extension, which means that SERAPH could adopt an additional manifold regularization term."}, {"heading": "2. Proposed Approach", "text": "In this section, we first formulate the model of SERAPH and then develop the EM-like algorithm to solve the model."}, {"heading": "2.1. Notations", "text": "Suppose we have a training set X = {xi | xi \u2208 Rm}ni=1 that contains n points each with m features. Let the sets of similar and dissimilar data pairs be\nS = {(xi, xj) | xi and xj are similar}, D = {(xi, xj) | xi and xj are dissimilar}.\nWith some abuse of terminology, we refer to S \u222a D as the labeled data and\nU = {(xi, xj) | i 6= j, (xi, xj) 6\u2208 S \u222a D}\nas the unlabeled data. A weak label yi,j = 1 is assigned to (xi, xj) \u2208 S, or yi,j = \u22121 to (xi, xj) \u2208 D. We abbreviate \u2211 (xi,xj)\u2208S\u222aD, \u2211 (xi,xj)\u2208U and \u2211 y\u2208{1,\u22121} as \u2211 S\u222aD,\u2211\nU and \u2211 y . Consider learning a Mahalanobis distance\nmetric for x, x\u2032 \u2208 Rm of the form\nd(x, x\u2032) = \u2016x\u2212 x\u2032\u2016A = \u221a (x\u2212 x\u2032)>A(x\u2212 x\u2032),\nwhere> is the transpose operator and A \u2208 Rm\u00d7m is a symmetric and positive semi-definite matrix to be learned2. The probability pA(y | x, x\u2032) of labeling (x, x\u2032) \u2208 Rm \u00d7 Rm with y = \u00b11 is parameterized by the matrix A. When applying pA(y | x, x\u2032) to (xi, xj), it is abbreviated as pAi,j(y).\n2In this paper, A is always assumed symmetric positive semidefinite and will not be explicitly written for brevity."}, {"heading": "2.2. Basic model", "text": "To begin with, we derive a probabilistic model to investigate the conditional probability of y = \u00b11 given (x, x\u2032) \u2208 Rm\u00d7Rm. We resort to a parametric form of pA(y | x, x\u2032), and will focus on it for the out-of-sample ability.\nThe maximum entropy principle (Jaynes, 1957) suggests that we should choose the probability distribution with the maximum entropy out of all distributions that match the data moments. Let3\nH(pAi,j) = \u2212 \u2211\ny pAi,j(y) ln p A i,j(y)\nbe the entropy of the conditional probability pAi,j(y), and\nf(x, x\u2032, y;A) : Rm \u00d7 Rm \u00d7 {+1,\u22121} 7\u2192 R\nbe a feature function that is convex with respect to A. The constrained optimization problem is\nmax A,pAi,j ,\u03be\n\u2211 S\u222aD H(pAi,j)\u2212 1 2\u03b3 \u03be2\ns.t. \u2223\u2223\u2223\u2211\nS\u222aD EpAi,j [f(xi, xj , y;A)] \u2212 \u2211 S\u222aD f(xi, xj , yi,j ;A) \u2223\u2223\u2223 \u2264 \u03be,\n(1)\nwhere \u03be is a slack variable and \u03b3 > 0 is a regularization parameter. The penalty presumes the Gaussian prior of the expected data moments from the empirical data moments, which is essentially consistent in spirit with the generalized maximum entropy principle (Dud\u0131\u0301k & Schapire, 2006) (see Appendix B.1).\nTheorem 1. The primal solution p\u2217A is given in terms of the dual solution (A\u2217, \u03ba\u2217) by\np\u2217A(y | x, x\u2032) = exp(\u03ba \u2217f(x, x\u2032, y;A\u2217))\nZ(x, x\u2032;A\u2217, \u03ba\u2217) , (2)\nwhere Z(x, x\u2032;A, \u03ba) = \u2211 y\u2032 exp(\u03baf(x, x\n\u2032, y\u2032;A)), and (A\u2217, \u03ba\u2217) can be obtained by solving the dual problem\nmin A,\u03ba \u2211 S\u222aD lnZ(xi, xj ;A, \u03ba)\n\u2212 \u2211 S\u222aD \u03baf(xi, xj , yi,j ;A) + \u03b3 2 \u03ba2.\n(3)\nDefine the regularized log-likelihood function on labeled data (i.e., on observed weak labels) as\nL1(A, \u03ba) = \u2211 S\u222aD ln pAi,j(yi,j)\u2212 \u03b3 2 \u03ba2.\nThen, for supervised metric learning, the regularized maximum log-likelihood estimation and the generalized maximum entropy estimation are equivalent.4\n3Throughout this paper, we adopt that 0 ln 0 = 0. 4The proofs of all theorems are in Appendix A.\nWhen considering f(x, x\u2032, y;A) that should take moments about the metric information into account, we propose\nf(x, x\u2032, y;A, \u03b7) = y\n2 (\u2016x\u2212 x\u2032\u20162A \u2212 \u03b7), (4)\nwhere \u03b7 > 0 is a hyperparameter used as the threshold to separate the sets S and D under the target metric d(x, x\u2032). Now the probabilistic model (2) becomes\npA(y | x, x\u2032) = 1 1 + exp(\u2212\u03bay(\u2016x\u2212 x\u2032\u20162A \u2212 \u03b7)) . (5)\nFor the optimal solution (p\u2217A, A\u2217, \u03ba\u2217), we hope for\np\u2217A(yi,j | xi, xj) > 1/2, yi,j(\u2016xi \u2212 xj\u20162A\u2217 \u2212 \u03b7) < 0,\nso there must be \u03ba\u2217 < 0.\nAlthough we use Eq.(4) as our feature function, other options are available. Please see Appendix C.1 for details."}, {"heading": "2.3. Regularization", "text": "In this subsection, we extend L1(A, \u03ba) by entropy regularization to semi-supervised learning. Moreover, we regularize our objective by trace-norm regularization.\nOur unsupervised part does not rely upon the manifold assumption and is not in the paradigm of smoothing the projected training data. In order to be integrated with the supervised part more naturally in philosophy, we follow the minimum entropy principle (Grandvalet & Bengio, 2004), and hence pAi,j should have low entropy or uncertainty for (xi, xj) \u2208 U . Roughly speaking, the resultant discriminative models prefer peaked distributions on unlabeled data, which carries out a probabilistic low-density separation. Subsequently, according to Grandvalet & Bengio (2004), our optimization becomes\nmax A,\u03ba L2(A, \u03ba) = \u2211 S\u222aD ln pAi,j(yi,j)\u2212 \u03b3 2 \u03ba2\n+ \u00b5 \u2211 U \u2211 y pAi,j(y) ln p A i,j(y),\nwhere \u00b5 \u2265 0 is a regularization parameter.\nIn addition, we hope for the dimensionality reduction ability by encouraging a low-rank projection induced from A. This is helpful in dealing with corrupted data or data distributed intrinsically in a low-dimensional subspace. It is known that the trace is a convex relaxation of the rank for a matrix, so we revise our optimization problem into\nmax A,\u03ba L(A, \u03ba) = \u2211 S\u222aD ln pAi,j(yi,j)\u2212 \u03b3 2 \u03ba2\n+ \u00b5 \u2211 U \u2211 y pAi,j(y) ln p A i,j(y)\u2212 \u03bb tr(A), (6)\nwhere tr(A) is the trace of A, and \u03bb \u2265 0 is a regularization parameter.\nOptimization (6) is the final model of SERAPH, and we say that it is equipped with the hyper-sparsity when both \u00b5 and \u03bb are positive. SERAPH possesses standard kernel and manifold extensions. For more information, please refer to Appendix C.2 and C.3."}, {"heading": "2.4. Algorithm", "text": "From now on we will simplify the model (6) and derive a practical algorithm. First, we eliminate \u03ba from (6), thanks to the fact that we use a simple feature function (4) in (1).\nTheorem 2. Define the simplified optimization problem as5\nmax A L\u0302(A) = \u2211 S\u222aD ln p\u0302Ai,j(yi,j)\n+ \u00b5 \u2211 U \u2211 y p\u0302Ai,j(y) ln p\u0302 A i,j(y)\u2212 \u03bb\u0302 tr(A), (7)\nwhere the simplified probabilistic model is\np\u0302A(y | x, x\u2032) = 1 1 + exp(y(\u2016x\u2212 x\u2032\u20162A \u2212 \u03b7\u0302)) . (8)\nLet A\u0302 and (A\u2217, \u03ba\u2217) be the optimal solutions to (7) and (6), respectively. Then, there exist well-defined hyperparameters \u03b7\u0302 and \u03bb\u0302, such that A\u0302 is equivalent to A\u2217 with respect to d(x, x\u2032), and the resulting p\u0302A(y | x, x\u2032) parameterized by A\u0302 and \u03b7\u0302 is identical to the original pA(y | x, x\u2032) parameterized by A\u2217, \u03ba\u2217 and \u03b7.\nRemark 1. After the simplification, \u03b3 is dropped, \u03b7 and \u03bb are modified, but the regularization parameter \u00b5 remains the same, which means that the tradeoff between the supervised and unsupervised parts has not been affected.\nOptimization (7) could be directly solved by the gradient projection method (Polyak, 1967), even though it is nonconvex. Nevertheless, we would like to pose it as an EMlike iterative scheme to access the derandomization by the initial solution, the stability for the gradient update, and the insensitivity to the step size, just to name a few of the gained algorithmic properties.\nThe EM-like algorithm runs as follows. In the beginning, we initialize a nonparametric probability q(y | xi, xj), and then the M-Step and the E-Step get executed repeatedly until the stopping conditions are satisfied.\nAt the t-th E-Step, similarly to Grac\u0327a et al. (2009), we have for each pair (xi, xj) \u2208 U that\nmin q\nKL(q || pAi,j) + \u00b5Eq[\u2212 ln pAi,j(y)], (9)\nwhere KL is the Kullback-Leibler divergence, and pAi,j is parameterized by the metric A(t) found at the last M-Step. Optimization (9) can be solved analytically.\n5The new functions and parameters are denoted by \u00b7\u0302 within this theorem for the sake of clarity.\nTheorem 3. The solution to (9) is given by\nq(y | xi, xj) = pAi,j(y) exp(\u00b5 ln p A i,j(y))\u2211\ny\u2032 p A i,j(y \u2032) exp(\u00b5 ln pAi,j(y \u2032)) . (10)\nOn the other hand, at the t-th M-Step, we find new metric A(t) through the probability q(y | xi, xj) which is generated in the last E-Step and only defined for (xi, xj) \u2208 U :\nmax A F(A) = \u2211 S\u222aD ln pAi,j(yi,j)\n+ \u00b5 \u2211 U \u2211 y q(y | xi, xj) ln pAi,j(y)\u2212 \u03bb tr(A). (11)\nIt could be solved by the gradient projection method without worry about local maxima using the calculation of\u2207F given by\n\u2207F(A) = \u2212 \u2211 S\u222aD yi,j ( 1\u2212 pAi,j(yi,j) ) xi,j\n\u2212 \u00b5 \u2211 U \u2211 y yq(y | xi, xj) ( 1\u2212 pAi,j(y) ) xi,j \u2212 \u03bbIm,\nwhere xi,j = (xi \u2212 xj)(xi \u2212 xj)>, since the convexity of the feature function f(x, x\u2032, y;A) with respect toA implies the convexity of the objective F(A).\nA remarkable property of F(A) is that its gradient is uniformly bounded, regardless of the scale of A, i.e., the magnitude of tr(A).\nTheorem 4. The objective F(A) is Lipschitz continuous, and the best Lipschitz constant Lip\u2016\u00b7\u2016F (F) with respect to the Frobenius norm \u2016 \u00b7 \u2016F satisfies\nLip\u2016\u00b7\u2016F (F) \u2264 (#S +#D + \u00b5#U)(diam(X )) 2 + \u03bbm,\n(12) where diam(X ) = maxxi,xj\u2208X \u2016xi\u2212xj\u20162 is the diameter of X , and # measures the cardinality of a set.\nIn our current implementation, the initial solution is q(\u22121 | xi, xj) = 1, which means that we treat all unlabeled pairs as dissimilar pairs. The overall asymptotic time complexity is O(n2m + m3) in which the stopping criteria of the M-Step and the whole EM-like iteration are ignored. Discussions about the computational complexity and the fast implementation can be found in Appendix D."}, {"heading": "3. Discussions", "text": "In this section, we discuss the sparsity issues, namely, we can obtain the posterior sparsity (Grac\u0327a et al., 2009) by entropy regularization and the projection sparsity (Ying et al., 2009) by trace-norm regularization.\nBy a \u2018sparse\u2019 posterior distribution, we mean that the uncertainty (i.e., the entropy or variance) is low. See Figure 1 as an example. Recall that supervised metric learning aims\nat a metric under which data in the same class are close and data from different classes are far apart. This results in the metric which ignores the horizontal feature and focuses on the vertical feature. However, the vertical feature is important, and taking care of the posterior sparsity would lead to a better metric as illustrated in (e) and (f). Therefore, we prefer taking the posterior sparsity into account in addition to the aforementioned goal, and then the risk of overfitting weakly labeled data can be significantly reduced.\nWe can rewrite L2(A, \u03ba) as a soft posterior regularization (PR) objective (Grac\u0327a et al., 2009). Let the auxiliary feature function be g(x, x\u2032, y) = \u2212 ln pA(y | x, x\u2032), then maximizing L2(A, \u03ba) is equivalent to\nmax A,\u03ba L1(A, \u03ba)\u2212 \u00b5 \u2211 U EpAi,j [g(xi, xj , y)]. (13)\nOn the other hand, according to optimization (7) of Grac\u0327a et al. (2009), the soft PR objective should take a form as\nmax A,\u03ba L1(A, \u03ba)\u2212min q\n( KL(q || pA) + \u00b5 \u2211 U \u03bei,j ) s.t. Eq[g(xi, xj , y)] \u2264 \u03bei,j ,\u2200(xi, xj) \u2208 U , (14)\nwhere \u03bei,j are slack variables. Since q is unconstrained, we can optimize it with respect to fixed A and \u03ba. It is easy to see that q should be pA (restricted on U), so the KL term is zero and the expectation term is the entropy, which implies the equivalence of optimizations (13) and (14).\nBesides the above posterior sparsity, we also hope for the projection sparsity, which may guide the learned metric to better generalization performance. See Figure 2 as an example of its effectiveness, where the horizontal feature is informative and the vertical feature is useless.\nThe underlying technique is the mixed-norm regularization (Argyriou et al., 2006). Denote the `(2,1)-norm of a symmetric matrix M as \u2016M\u2016(2,1) = \u2211m k=1( \u2211m k\u2032=1M 2 k,k\u2032)\n1/2. Similarly to Ying et al. (2009), let P \u2208 Rm\u00d7m be a projection, and W = P>P be the metric induced from P . Let the i-th column of P and W be Pi and Wi. If Pi is identically zero, the i-th component of x has no contribution to z = Px. Since the column-wise sparsity of W and P are equivalent, we can penalize \u2016W\u2016(2,1) to reach the columnwise sparsity of P .\nNevertheless, this is feature selection rather than dimensionality reduction. Recall that the goal is to select a few most representative directions of input data which are not\nrestricted to the coordinate axes. The solution is to pick an extra transformation V \u2208 Om to rotate x before the projection where Om is the set of orthonormal matrices of size m, and add V to the optimization variables. Consequently, we penalize \u2016W\u2016(2,1), project x to z = PV x, and since A = (PV )>(PV ) = V>WV , we arrive at\nmax A,\u03ba,W,V\nL2(A, \u03ba)\u2212 \u03bb\u2016W\u2016(2,1) (15)\ns.t. A = V>WV,W =W>,W 0, V \u2208 Om.\nThe equivalence of optimizations (6) and (15) is guaranteed by Lemma 1 of Ying et al. (2009).\nMoreover, there is another justification based on the information maximization principle (Gomes et al., 2010). Please see Appendix B.2 for details."}, {"heading": "4. Related Works", "text": "Xing et al. (2002) initiated metric learning based on pairwise similarity/dissimilarity constraints by global distance metric learning (GDM). Several excellent metric learning methods have been developed in the last decade, including neighborhood component analysis (NCA; Goldberger et al., 2004), large margin nearest neighbor classification (LMNN; Weinberger et al., 2005), and information-theoretic metric learning (ITML; Davis et al., 2007).\nBoth ITML and SERAPH are information-theoretic, but the ideas and models are quite different. ITML defines a generative model pA(x) = exp(\u2212 12\u2016x \u2212 \u00b5\u2016 2 A)/Z, where \u00b5 is unknown mean value and Z is a normalizing constant. Compared with GDM, ITML regularizes the KL-divergence between pA0(x) and pA(x), and transforms this term to a Log-Det regularization. By specifying A0 = 1nIm, it becomes the maximum entropy estimation of pA(x). Thus, it prefers the metric close to the Euclidean distance. SERAPH also follows the maximum entropy principle, but the probabilistic model pA(y | x, x\u2032) is discriminative.\nA probabilistic GDM was designed intuitively as a baseline in the experimental part of Yang et al. (2006). It is a special case of our supervised part. In fact, SERAPH is much more general. Please refer to Section 2.2 for details.\nSubsequently, local distance metric learning (LDM; Yang et al., 2006) is the pioneer of semi-supervised metric learning, which assumes that the eigenvectors of A are the principal components of training data. Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning. Liu et al. (2010) brings the element-wise sparsity to Hoi et al. (2008).\nThe manifold extension described in Appendix C.3 can be attached to all metric learning methods, whereas our unsu-\npervised part applies to probabilistic methods only. However, any probabilistic method with an explicit expression of the posterior distribution adopts two semi-supervised extensions, while deterministic methods such as LMNN cannot benefit from entropy regularization.\nDue to limited space, we leave out sparse metric learning and robust metric learning. Instead, we recommend Huang et al. (2009) and Huang et al. (2010) for the latest reviews of sparse and robust metric learning respectively."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Setup", "text": "We compared SERAPH with the Euclidean distance, four famous supervised and two representative semi-supervised metric learning methods6: global distance metric learning (GDM; Xing et al., 2002), neighborhood component analysis (NCA; Goldberger et al., 2004), large margin nearest neighbor classification (LMNN; Weinberger et al., 2005), information-theoretic metric learning (ITML; Davis et al., 2007), local distance metric learning (LDM; Yang et al., 2006), and manifold Fisher discriminant analysis (MFDA; Baghshah & Shouraki, 2009).\nTable 1 describes the specification of the data sets used in our experiments. The top six data sets (i.e., iris, wine, ionosphere, balance, breast cancer, and diabetes) come from the UCI machine learning repository7, while the USPS and MNIST come from the homepage of the late Sam Roweis8. Gray-scale images of handwritten digits are downsampled to 8 \u00d7 8 and 14 \u00d7 14 pixel resolution resulting in 64- and 196-dimensional vectors for USPS and MNIST. The sym-\n6We downloaded the codes of all baseline methods from the official websites provided by the original authors except MFDA.\n7http://archive.ics.uci.edu/ml/. 8http://cs.nyu.edu/\u02dcroweis/data.html.\nbol USPS1\u22125,20 means 20 training data from each of the first 5 classes, USPS1\u221210,40 means 40 training data from each of all 10 classes, MNIST1,7 means digits 1 versus 7, and so forth. Note that in the last two tasks, the dimensionality of data is greater than the size of all training data.\nIn our experiments, all methods were repeatedly run on 50 random samplings. For each random sampling, class labels of the first few data were revealed, and the sets S and D were constructed according to these revealed class labels. The sizes of S, D and U were fixed for all samplings of USPS and MNIST but random for the samplings of UCI data sets. We measured the performance of the one-nearestneighbor classifiers based on the learned metrics as well as the computation time for learning the metrics.\nFour settings of SERAPH were included in our experiments (except on two artificial data sets): SERAPHnone stands for \u00b5 = \u03bb = 0, SERAPHpost for \u00b5 = #(S\u222aD) #U and \u03bb = 0, SERAPHproj for \u00b5 = 0 and \u03bb = 1, and SERAPHhyper for \u00b5 = #(S\u222aD)#U and \u03bb = 1. We fixed \u03b7 = 1 for simplicity.\nThere was no cross-validation for each random sampling, otherwise the learned metrics would be highly dependent upon the final classifier, and also because of the large variance of the classification performance given the limited supervised information. The hyperparameters of other methods, e.g., the number of reduced dimensions, the number of nearest neighbors, and the percentage of principal components, were selected as the best value based on another 10 random samplings if default values or heuristics were not provided by the original authors."}, {"heading": "5.2. Results", "text": "Figures 1 and 2 had previously displayed the visually comprehensive results of the sparsity regularization on two artificial data sets respectively. Subfigures (c) and (d) in both\nfigures were obtained by GDM, while (e) and (f) were generated by SERAPH with \u00b5 = 10\u00b7#(S\u222aD)#U , \u03bb = 0 in Figure 1 and \u00b5 = 0, \u03bb = 300 in Figure 2. We can see from Figures 1 and 2 that SERAPH improved supervised global metric learning dramatically by the sparsity regularization.\nThe experimental results of the one-nearest-neighbor classification are reported in Table 2 (GDM was sometimes very slow and excluded from the comparison). SERAPH is fairly promising, especially with the hyper-sparsity (\u00b5 = #(S\u222aD)#U and \u03bb = 1). It was best or tie over all tasks, and often statistically significantly better than others on UCI data sets except ITML. It was better than all other methods statistically significantly on USPS, and SERAPHhyper outperformed both SERAPHpost and SERAPHproj . Moreover, it improved the accuracy even on the ill-posed MNIST tasks, though the improvement was insignificant on MNIST3,5,8. In a word, SERAPH can reduce the risk of overfitting weakly labeled data with the help of unlabeled data, and hence our sparsity regularization would be reasonable and practical.\nIn vivid contrast with SERAPH that exhibited nice generalization capability, supervised methods might learn a metric even worse than the Euclidean distance due to overfitting\nproblems, especially NCA that optimized the leave-one-out performance based on such limited label information. The powerful LMNN did not behave satisfyingly, since it was hardly fulfilled to find a lot of neighbors belonging to the same class within labeled data. ITML was the second best method though it can only access weakly labeled data, but it became less useful for difficult tasks. On the other hand, we observed that LDM might fail when the principal components of training data were not close to the eigenvectors of the target matrix, and MFDA might fail if the amount of training data cannot recover the underlying manifold well.\nAn observation is that the global metric learning often outperformed the local one, if the supervised information was insufficient. This phenomenon indicates that the local metric learning tends to fit the local neighborhood information exceedingly and then suffers from overfitting problems.\nFinally, we report in Figure 3 the computation time of each algorithm on each task (excluding GDM). Generally speaking, SERAPH was the second fastest method, and the fastest MFDA involves only some matrix multiplication and a single eigen-decomposition. Improvements may be expected if we program in Matlab with C/C++."}, {"heading": "6. Conclusions", "text": "In this paper, we proposed an information-theoretic semisupervised metric learning approach SERAPH as an alternative to the manifold-based methods. The generalized maximum entropy estimation for supervised metric learning was our foundation. Then a semi-supervised extension that can achieve the posterior sparsity was obtained via entropy regularization. Moreover, we enforced a trace-norm regularization that can reach the projection sparsity. The resulting optimization was solved by an EM-like scheme with several nice algorithmic properties, and the learned metric had high discriminability even under a noisy environment.\nExperiments on benchmark data sets showed that SERAPH often outperformed state-of-the-art fully-/semi-supervised metric learning methods given only limited supervised information. A final note is that in our experiments the posterior and projection sparsity were demonstrated to be very helpful for high-dimensional data if and only if they were combined with each other, i.e., integrated into the hypersparsity. An in-depth study of this interaction is left as our future work."}, {"heading": "Acknowledgments", "text": "The authors would like to thank anonymous reviewers for helpful comments. GN is supported by the MEXT scholarship No.103250, MY is supported by the JST PRESTO program, and MS is supported by the FIRST program."}], "references": [{"title": "Semi-supervised metric learning", "author": ["M. Baghshah", "S. Shouraki"], "venue": null, "citeRegEx": "Baghshah and Shouraki,? \\Q2006\\E", "shortCiteRegEx": "Baghshah and Shouraki", "year": 2006}, {"title": "Laplacian eigenmaps and spectral tech", "author": ["M. Belkin", "P. Niyogi"], "venue": null, "citeRegEx": "Belkin and Niyogi,? \\Q2009\\E", "shortCiteRegEx": "Belkin and Niyogi", "year": 2009}, {"title": "Informationtheoretic metric learning", "author": ["J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon"], "venue": "In ICML,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Maximum entropy distribution estimation with generalized regularization", "author": ["M. Dud\u0131\u0301k", "R.E. Schapire"], "venue": "In COLT,", "citeRegEx": "Dud\u0131\u0301k and Schapire,? \\Q2006\\E", "shortCiteRegEx": "Dud\u0131\u0301k and Schapire", "year": 2006}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R.A. Fisher"], "venue": "Annals of Eugenics,", "citeRegEx": "Fisher,? \\Q1936\\E", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Discriminative clustering by regularized information maximization", "author": ["R. Gomes", "A. Krause", "P. Perona"], "venue": "In NIPS,", "citeRegEx": "Gomes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 2010}, {"title": "Posterior vs. parameter sparsity in latent variable models", "author": ["J. Gra\u00e7a", "K. Ganchev", "B. Taskar", "F. Pereira"], "venue": "In NIPS,", "citeRegEx": "Gra\u00e7a et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gra\u00e7a et al\\.", "year": 2009}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Grandvalet and Bengio,? \\Q2004\\E", "shortCiteRegEx": "Grandvalet and Bengio", "year": 2004}, {"title": "Semi-supervised distance metric learning for collaborative image retrieval", "author": ["S. Hoi", "W. Liu", "Chang", "S.-F"], "venue": "In CVPR,", "citeRegEx": "Hoi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hoi et al\\.", "year": 2008}, {"title": "GSML: A unified framework for sparse metric learning", "author": ["K. Huang", "Y. Ying", "C. Campbell"], "venue": "In ICDM,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Robust metric learning by smooth optimization", "author": ["K. Huang", "R. Jin", "Z. Xu", "C. Liu"], "venue": "In UAI,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Information theory and statistical mechanics", "author": ["E.T. Jaynes"], "venue": "Physical Review,", "citeRegEx": "Jaynes,? \\Q1957\\E", "shortCiteRegEx": "Jaynes", "year": 1957}, {"title": "Semi-supervised sparse metric learning using alternating linearization optimization", "author": ["W. Liu", "S. Ma", "D. Tao", "J. Liu", "P. Liu"], "venue": "In KDD,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "A general method for solving extremal problems (in Russian)", "author": ["B.T. Polyak"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Polyak,? \\Q1967\\E", "shortCiteRegEx": "Polyak", "year": 1967}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": "Science, 290:2323\u20132326,", "citeRegEx": "Roweis and Saul,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis", "author": ["M. Sugiyama"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sugiyama,? \\Q2007\\E", "shortCiteRegEx": "Sugiyama", "year": 2007}, {"title": "Semi-supervised local Fisher discriminant analysis for dimensionality reduction", "author": ["M. Sugiyama", "T. Id\u00e9", "S. Nakajima", "J. Sese"], "venue": "Machine Learning,", "citeRegEx": "Sugiyama et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "J. Blitzer", "L. Saul"], "venue": "In NIPS,", "citeRegEx": "Weinberger et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2005}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E. Xing", "A. Ng", "M.I. Jordan", "S. Russell"], "venue": "In NIPS,", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "An efficient algorithm for local distance metric learning", "author": ["L. Yang", "R. Jin", "R. Sukthankar", "Y. Liu"], "venue": "In AAAI,", "citeRegEx": "Yang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2006}, {"title": "Sparse metric learning via smooth optimization", "author": ["Y. Ying", "K. Huang", "C. Campbell"], "venue": "In NIPS,", "citeRegEx": "Ying et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ying et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": ", {\u00b11}-valued labels that indicate the similarity/dissimilarity of data pairs (e.g., Weinberger et al., 2005; Davis et al., 2007); (c) Unsupervised type requiring no label information (e.", "startOffset": 78, "endOffset": 129}, {"referenceID": 20, "context": "To the best of our knowledge, all semi-supervised extensions employ off-theshelf techniques in type (c) such as principal component analysis (Yang et al., 2006; Sugiyama et al., 2010) or manifold embedding (Hoi et al.", "startOffset": 141, "endOffset": 183}, {"referenceID": 17, "context": "To the best of our knowledge, all semi-supervised extensions employ off-theshelf techniques in type (c) such as principal component analysis (Yang et al., 2006; Sugiyama et al., 2010) or manifold embedding (Hoi et al.", "startOffset": 141, "endOffset": 183}, {"referenceID": 9, "context": ", 2010) or manifold embedding (Hoi et al., 2008; Baghshah & Shouraki, 2009; Liu et al., 2010).", "startOffset": 30, "endOffset": 93}, {"referenceID": 13, "context": ", 2010) or manifold embedding (Hoi et al., 2008; Baghshah & Shouraki, 2009; Liu et al., 2010).", "startOffset": 30, "endOffset": 93}, {"referenceID": 4, "context": ", Fisher discriminant analysis1 (Fisher, 1936)), and the assistant one tries to identify and preserve the intrinsic geometric structure (e.", "startOffset": 32, "endOffset": 46}, {"referenceID": 7, "context": "on unlabeled data, which can achieve the sparsity of the posterior distribution (Gra\u00e7a et al., 2009), i.", "startOffset": 80, "endOffset": 100}, {"referenceID": 21, "context": "Furthermore, we employ mixed-norm regularization (Ying et al., 2009) to encourage the sparsity of the projection matrix, i.", "startOffset": 49, "endOffset": 68}, {"referenceID": 12, "context": "The maximum entropy principle (Jaynes, 1957) suggests that we should choose the probability distribution with the maximum entropy out of all distributions that match the data moments.", "startOffset": 30, "endOffset": 44}, {"referenceID": 14, "context": "Optimization (7) could be directly solved by the gradient projection method (Polyak, 1967), even though it is nonconvex.", "startOffset": 76, "endOffset": 90}, {"referenceID": 7, "context": "At the t-th E-Step, similarly to Gra\u00e7a et al. (2009), we have for each pair (xi, xj) \u2208 U that", "startOffset": 33, "endOffset": 53}, {"referenceID": 7, "context": "In this section, we discuss the sparsity issues, namely, we can obtain the posterior sparsity (Gra\u00e7a et al., 2009) by entropy regularization and the projection sparsity (Ying et al.", "startOffset": 94, "endOffset": 114}, {"referenceID": 21, "context": ", 2009) by entropy regularization and the projection sparsity (Ying et al., 2009) by trace-norm regularization.", "startOffset": 62, "endOffset": 81}, {"referenceID": 7, "context": "We can rewrite L2(A, \u03ba) as a soft posterior regularization (PR) objective (Gra\u00e7a et al., 2009).", "startOffset": 74, "endOffset": 94}, {"referenceID": 7, "context": "On the other hand, according to optimization (7) of Gra\u00e7a et al. (2009), the soft PR objective should take a form as", "startOffset": 52, "endOffset": 72}, {"referenceID": 21, "context": "Similarly to Ying et al. (2009), let P \u2208 Rm\u00d7m be a projection, and W = P>P be the metric induced from P .", "startOffset": 13, "endOffset": 32}, {"referenceID": 21, "context": "The equivalence of optimizations (6) and (15) is guaranteed by Lemma 1 of Ying et al. (2009).", "startOffset": 74, "endOffset": 93}, {"referenceID": 6, "context": "Moreover, there is another justification based on the information maximization principle (Gomes et al., 2010).", "startOffset": 89, "endOffset": 109}, {"referenceID": 5, "context": "Several excellent metric learning methods have been developed in the last decade, including neighborhood component analysis (NCA; Goldberger et al., 2004), large margin nearest neighbor classification (LMNN; Weinberger et al.", "startOffset": 124, "endOffset": 154}, {"referenceID": 18, "context": ", 2004), large margin nearest neighbor classification (LMNN; Weinberger et al., 2005), and information-theoretic metric learning (ITML; Davis et al.", "startOffset": 54, "endOffset": 85}, {"referenceID": 2, "context": ", 2005), and information-theoretic metric learning (ITML; Davis et al., 2007).", "startOffset": 51, "endOffset": 77}, {"referenceID": 20, "context": "A probabilistic GDM was designed intuitively as a baseline in the experimental part of Yang et al. (2006). It is a special case of our supervised part.", "startOffset": 87, "endOffset": 106}, {"referenceID": 20, "context": "Subsequently, local distance metric learning (LDM; Yang et al., 2006) is the pioneer of semi-supervised metric learning, which assumes that the eigenvectors of A are the principal components of training data.", "startOffset": 45, "endOffset": 69}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning.", "startOffset": 0, "endOffset": 115}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning.", "startOffset": 0, "endOffset": 147}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning.", "startOffset": 0, "endOffset": 179}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning. Liu et al. (2010) brings the element-wise sparsity to Hoi et al.", "startOffset": 0, "endOffset": 249}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning. Liu et al. (2010) brings the element-wise sparsity to Hoi et al. (2008).", "startOffset": 0, "endOffset": 303}, {"referenceID": 10, "context": "Instead, we recommend Huang et al. (2009) and Huang et al.", "startOffset": 22, "endOffset": 42}, {"referenceID": 10, "context": "Instead, we recommend Huang et al. (2009) and Huang et al. (2010) for the latest reviews of sparse and robust metric learning respectively.", "startOffset": 22, "endOffset": 66}, {"referenceID": 19, "context": "We compared SERAPH with the Euclidean distance, four famous supervised and two representative semi-supervised metric learning methods6: global distance metric learning (GDM; Xing et al., 2002), neighborhood component analysis (NCA; Goldberger et al.", "startOffset": 168, "endOffset": 192}, {"referenceID": 5, "context": ", 2002), neighborhood component analysis (NCA; Goldberger et al., 2004), large margin nearest neighbor classification (LMNN; Weinberger et al.", "startOffset": 41, "endOffset": 71}, {"referenceID": 18, "context": ", 2004), large margin nearest neighbor classification (LMNN; Weinberger et al., 2005), information-theoretic metric learning (ITML; Davis et al.", "startOffset": 54, "endOffset": 85}, {"referenceID": 2, "context": ", 2005), information-theoretic metric learning (ITML; Davis et al., 2007), local distance metric learning (LDM; Yang et al.", "startOffset": 47, "endOffset": 73}, {"referenceID": 20, "context": ", 2007), local distance metric learning (LDM; Yang et al., 2006), and manifold Fisher discriminant analysis (MFDA; Baghshah & Shouraki, 2009).", "startOffset": 40, "endOffset": 64}], "year": 2012, "abstractText": "We propose a general information-theoretic approach called SERAPH (SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric learning that does not rely upon the manifold assumption. Given the probability parameterized by a Mahalanobis distance, we maximize the entropy of that probability on labeled data and minimize it on unlabeled data following entropy regularization, which allows the supervised and unsupervised parts to be integrated in a natural and meaningful way. Furthermore, SERAPH is regularized by encouraging a low-rank projection induced from the metric. The optimization of SERAPH is solved efficiently and stably by an EMlike scheme with the analytical E-Step and convex M-Step. Experiments demonstrate that SERAPH compares favorably with many well-known global and local metric learning methods.", "creator": "LaTeX with hyperref package"}}}