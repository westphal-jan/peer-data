{"id": "1206.4620", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Improved Information Gain Estimates for Decision Tree Induction", "abstract": "modifications of classification and descriptive trees remain popular within learning packages because they define flexible non - parametric models significantly predict well and are computationally unstable even during training learning employment. given induction to classifications processes one aims to find predicates that are maximally informative about the prediction target. to select acceptable predicates chaos approaches estimate an information - theoretic rule interval, the sample gain, estimates for induction and regression prediction. we measure out on the common aggregation procedures are intuitive and show performance by ignoring expectations in improved estimators improving information discrete and the differential entropy we effectively obtain better classification trees. in effect our samples yield improved predictive ratios may are simple modifications enforce evolutionary tree phylogenetic tree code.", "histories": [["v1", "Mon, 18 Jun 2012 15:04:54 GMT  (400kb)", "http://arxiv.org/abs/1206.4620v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["sebastian nowozin"], "accepted": true, "id": "1206.4620"}, "pdf": {"name": "1206.4620.pdf", "metadata": {"source": "META", "title": "Improved Information Gain Estimates for Decision Tree Induction", "authors": ["Sebastian Nowozin"], "emails": ["Sebastian.Nowozin@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "Decision trees are a classic method of inductive inference that is still very popular (Breiman et al., 1984; Hunt et al., 1966). They are not only easy to implement and use for classification and regression tasks, but also offer good predictive performance, computational efficiency, and flexibility. While often heuristically motivated, they can also deal with mixed discrete/continuous features and missing values. As early as in the late 1970\u2019s ID3 introduced the use of information theory for the induction of decision trees from data, described in detail in (Quinlan, 1986). Yet, optimal induction of decision trees according to some global objective is fundamentally hard (Hyafil & Rivest, 1976), and to this day most implementations use randomized greedy algorithms for growing\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\na decision tree (Criminisi et al., 2012). A recent advance has been the use of ensembles of randomized decision trees (Amit & Geman, 1994), such as random forests (Breiman, 2001), improving the predictive performance by averaging predictions of multiple trees.\nReflecting on this rich history, we ask an almost old-fashioned question: Can we improve the quality of learned decision trees by better estimation of information-theoretic quantities? To answer this, we take a look at how decision trees are typically grown and how information theory is used in the process."}, {"heading": "1.1. Decision Tree Induction", "text": "Decision trees are most often induced greedily, in the following manner. Given a data set {(xi, yi)}Ni=1, we start with an empty tree with just a root node. We then sample a number of split function candidates from a fixed distribution. Each split function partitions the training set into a left and right subset by some test on each xi. These subsets are scored so that a high score is assigned to splits that aid in predicting the output y well, i.e. those that reduce the average uncertainty about the predictive target as estimated by the training set. The highest scoring split is selected and the training set is partitioned accordingly into the two child nodes, growing the tree by making the node the parent of the two newly created child nodes. This procedure is applied recursively until some stopping conditions such as a maximum tree depth or minimum sample size are reached, see e.g. (Breiman et al., 1984).\nDifferent split scoring functions exist and have been used in the past (Breiman et al., 1984). A popular split scoring function, the so called information gain is derived from information-theoretic considerations. The information gain is just the mutual information between the local node decision (left or right) and the predictive output.\nTo motivate the information gain score, let b \u2208 B be a random variable denoting the decision whether a sample is assigned to the left (L) or right (R) branch by the split function. Hence B = {L,R}, and there is a joint distribution p(y, b). Once we fix a sample x\nAlgorithm 1 Information gain split selection\n1: Input: Sample set Z = {(xi, yi)}n`i=1, split proposal distribution ps, number of tests T \u2265 1, entropy estimator H\u0302 2: Output: Split function s\u2217 : X \u2192 {L,R} 3: Algorithm: 4: g\u2217 \u2190 0 {Initialize best infogain} 5: for t = 1, . . . , T do 6: Sample binary split s \u223c ps 7: Partition Z into (ZL, ZR) using s 8: g \u2190 \u2212 |ZL|n` H\u0302(YL)\u2212 |ZR| n`\nH\u0302(YR) {Estimate} 9: if g > g\u2217 then\n10: (g\u2217, s\u2217)\u2190 (g, s) {New best infogain} 11: end if 12: end for\nthe variable b is deterministic. We now measure the mutual information (Cover & Thomas, 2006) between the variable b and y. From the definition of mutual information and by some basic manipulation we have\nI(y, b) = DKL(p(y, b) \u2016 p(y)p(b)) (1)\n= \u222b B \u222b Y p(y|b)p(b) log p(y|b) dy db\n\u2212 \u222b B \u222b Y p(y, b) log p(y) dy db (2)\n= Hy \u2212 \u2211\nb\u2208{L,R}\np(b)Hy|b. (3)\nBecause the entropies Hy, Hy|b, and the split marginal p(b) are unknown, they are typically estimated using the training sample. Depending on Y, the measure of integration dy is either discrete (classification) or the Lebesgue measure (regression). Therefore the entropies are either discrete entropies or differential entropies (Cover & Thomas, 2006). The information gain (3) is a popular criterion used to determine the quality of a split (Criminisi et al., 2012), and has been used for classification, regression, and density estimation. It is generally accepted as the criterion of choice except for very unbalanced classification tasks, or when the noise on the prediction targets y is large.\nWhen we use the information gain criterion for recursive tree growing, we execute Algorithm 1 at each node in the decision tree, testing T candidate splits sequentially and keeping the one that achieves the highest estimated information gain. In the algorithm YL is the subset of training set predictions that are sorted into the left branch, i.e. YL = {yi : s(xi) = L}. Likewise YR is the remaining set of predictions. After the best split is selected the left and right child nodes are again subjected to the algorithm.\nIn this work we assume that the information gain (3) is a good criterion by which to select splits. Given this assumption, we address the problem of how to estimate the information gain from a finite sample. As we will see, the currently used estimators can be improved, yielding more accurate estimates of the information gain and in turn better decision trees."}, {"heading": "1.2. Related work", "text": "The importance of how a split proposal is scored has been analyzed in a series of papers. (Mingers, 1989) provides an initial analysis and state that the \u201cchoice of measure affects the size of a tree but not its accuracy\u201d. At that time this was considered a controversial claim and indeed (Buntine & Niblett, 1992) and (Liu & White, 1994) demonstrated that the statistical analysis of Mingers was flawed and the different methods of scoring a split do have a substantial influence on the generalization error of the learned decision tree.\nFor binary classification tasks the choice of split scoring functions has attracted special attention, as in this case the split scoring function can be related to statistical proper scoring rules (Buja et al., 2005). In addition, specialized criteria aiding the interpretability of the resulting tree have been developed (Buja & Lee, 2001). In our work we consider general multiclass classification and multivariate regression tasks where the goal is good predictive performance."}, {"heading": "2. Multiclass Classification", "text": "For multiclass classification, we predict into a finite set of classes Y = {1, 2, . . . ,K}. The most commonly used entropy estimator is derived from empirical class probabilities. For each class k, we count the number of occurences of that class as hk = \u2211 y\u2208Y I(y = k). The\nsum of all counts is n = |Y | = \u2211\nk hk."}, {"heading": "2.1. Naive Entropy Estimate", "text": "To estimate the entropy, we use the empirical class probabilities p\u0302k(Y ) = hk/n, and the estimate\nH\u0302N (Y ) = \u2212 K\u2211\nk=1\np\u0302k(Y ) log p\u0302k(Y ) (4)\n= log n\u2212 1 n K\u2211 k=1 hk log hk, (5)\nwhere in case a term is 0 log 0 it is taken to be zero.\nThe entropy estimator H\u0302N is an instance of a plug-in estimator, where a function is evaluated on an estimated probability distribution. In the case of the dis-\ncrete entropy this is consistent, that is, in the large sample limit n \u2192 \u221e it converges to the true entropy (Antos & Kontoyiannis, 2001). Although (4) is popularly used (see e.g. (Criminisi et al., 2012) and references therein), it is a bad estimator when used in (3). In particular (4) is biased and universally underestimates the true entropy. In fact, the bias is known (Schu\u0308rmann, 2004), and we have for n = |Y | samples that the bias depends on the true but unknown per-class probabilities pk as\nH(Y )\u2212E[H\u0302N (Y )]= K \u2212 1 2n \u2212 1 12n2\n( 1\u2212\nK\u2211 k=1 1 pk\n) +O(n\u22123).\nUnfortunately, the second and higher-order terms of this bias depend on the true unknown probabilities. The first term, K\u221212n , however, can be evaluated and is known as the Miller correction to (4), see (Miller, 1955). The correction is useful for entropy estimation, but has no effect in (3). To see this, let us define the Miller-adjusted estimate, H\u0302M (Y ) = H\u0302N (Y ) + K\u22121 2|Y | , and use it to evaluate the sum in (3), as\n\u2212|ZL| n` H\u0302M (YL)\u2212 |ZR| n` H\u0302M (YR)\n= \u2212 ( |ZL| n` H\u0302N (YL) + |ZR| n` H\u0302N (YR) ) + C,\nwhere the constant C = (K \u2212 1)/n` is independent of the split and thus it influences all split scores in the same way in (3). Therefore, although the entropy estimates are improved in absolute accuracy, the Miller correction has no effect on which split is selected.\nCan we at all hope to achieve a better split from using improved entropy estimators? Estimating the discrete entropy is challenging (Paninski, 2003; Antos & Kontoyiannis, 2001), but superior corrections than the one of Miller have been proposed recently. We cannot give an overview of the many proposed estimators here, but recommend the work (Schu\u0308rmann, 2004) for an overview and quantitative comparison. For our purposes, we select the Grassberger entropy estimate."}, {"heading": "2.2. Grassberger Entropy Estimate", "text": "In two papers (Grassberger, 1988; 2003), Peter Grassberger derived a family of discrete entropy estimators. We use the refined estimator from 2003, given as\nH\u0302G(h) = log n\u2212 1\nn K\u2211 k=1 hkG(hk), (6)\nwhere n = |Y |, and the summation is over all hk > 0 in the histogram. The function G(h) is given as1\nG(hk) = \u03c8(hk) + 1\n2 (\u22121)hk\n( \u03c8( hk + 1\n2 )\u2212 \u03c8(hk 2 )\n) .\nIn the above formula, \u03c8 is the digamma function. For large hk the function G(hk) behaves like a logarithm, and hence in the limit n\u2192\u221e we see that (6) becomes identical to (5). For small sample sizes G(hk) differs from log hk and the resulting estimate H\u0302G is more accurate (Schu\u0308rmann, 2004).\nTo train classification trees using H\u0302G we simply use it instead of H\u0302N in Algorithm 1. The improvement in information gain estimation is dramatic, as shown in Figure 1. It is now fair to ask whether this improvement in information gain estimation also leads to better generalization performance in the final classification tree ensemble. We will address this experimentally in the next section, but generally we expect the improvement to be largest when many classes are used (say, K \u2265 10), because in this case the empirical class frequencies cannot be estimated reliably."}, {"heading": "2.3. Experiments", "text": "For the experimental protocol we follow the recommendations in (Dems\u030car, 2006). We intentionally do not compare against other classification methods, both because the competitive performance of randomized\n1This is the closed-form solution to equation (30) in (Grassberger, 2003), where G(hk) was given only as\nG(hk) = \u03c8(hk) + (\u22121)hk \u222b 1 0 xhk\u22121 x+1 dx.\ndecision trees has been established before and because we want to assess only the contribution of improved information gain estimates. Overall, we use a standard setup for training our classification trees; no pruning is used and there are no missing features.\nAs features we use simple single-dimension thresholding tests of the form s(x, a, b) = (xa \u2264 b), where xa is the a\u2019th element in vector x, and b is a threshold. The proposal distribution at a node is to select a uniformly at random, but b is sampled uniformly from the samples reaching that node. At the leaf nodes we store a single class label, the majority class over all remaining samples at that node. Ties are broken at random. The ensemble of trees is trained using simple replication of the training set. To make predictions at test time, we simply take the majority decision over the individual predictions made by all trees in the ensemble.\nSetup. We use 30 data sets from a variety of sources. We use all multiclass (K \u2265 3) data sets from UCI, as well as four text classification data sets (reuters, tdt2, news20, rcv1) and web data sets.2 For data sets that come with a fixed train/val/test split we have used this split. For data sets that only come with a train/test split we further split the original train set 50/50 into a new train/val set. For data sets that do not provide any split we partition the data into the three subsets according to the proportions 25/25/50. We perform model selection entirely on the val set, selecting the minimum number of samples required to continue splitting in {1, 5, 10}, but fixing the number of trees to eight and the number of feature tests to 256. The best parameter on the validation set is taken and one model is trained on train+val, then evaluated on test. We repeat the procedure five times and report averaged multiclass accuracies on the test set.\nResults. Table 1 shows the average multiclass accuracies achieved on the test set. Generally the performance appears to be on the same level, with some notable exceptions (tdt2, news20, rcv1, and sector) where the Grassberger entropy estimator obtains higher accuracies. A Wilcoxon signed-rank test rejects the null hypothesis of equal performance at a p-value of 0.0267. Overall, the Grassberger estimate yields higher accuracies on 18 data sets, compared to eight data sets for the naive estimate, with four ties. The runtime of the two methods showed no difference and the the entire experiment ran on a single eight core PC in less than 10 hours.\n2From http://www.zjucadcg.cn/dengcai/Data/ TextData.html and http://www.cs.umd.edu/~sen/ lbc-proj/LBC.html\nDiscussion. The difference between the two estimators is small but statistically significant and we can conclude that improved entropy estimation yields improved classification trees. All other things have remained unchanged and therefore the difference is directly attributable to the entropy estimation. The change is most pronounced on data sets with a large number of classes, confirming the superiority of the Grassberger estimate in these situations. Implementation-wise this improvement comes for free: all that is required is to replace the naive entropy estimate with the Grassberger estimate."}, {"heading": "3. Multivariate Regression", "text": "For multivariate regression problems and density estimation tasks we have Y = Rd. Moreover, we require that for the true generating distribution q(x, y) we have that q(y|x) is a density with respect to the Lebesgue measure, that is, the CDF of q(y|x) is absolutely continuous. Our goal is to learn a prediction model p(y|x) by evaluating a decision tree and storing a simple density model p`(y) at the leaf `.\nFor any such distribution we consider its differential entropy (Cover & Thomas, 2006) as a measure of uncertainty, H(q) = \u2212 \u222b y q(y|x) log q(y|x)dy. In general, estimating the differential entropy of an unknown continuous distribution is a more difficult problem than in the discrete case. The next subsection discusses the common approach for training regression trees before we propose a better estimator."}, {"heading": "3.1. Normal approximation", "text": "One strategy is to assume the sample is a realization of a distribution within a known parametric family of distributions. By identifying a member in this family using a point estimate we can compute the entropy in closed form analytically. The most popular multivariate distribution that is amenable to such analytic treatment is the multivariate Normal distribution. For example, (5.2) in (Criminisi et al., 2012) uses the entropy of a multivariate Normal in d dimensions,\nH\u0302MVN-PLUGIN(C\u0302(Y )) = d 2 \u2212 d 2 log(2\u03c0) + 1 2 log |C\u0302(Y )| (7) where C\u0302(Y ) is the sample covariance matrix. This is a consistent plugin-estimator of the entropy because C\u0302(Y ) is a consistent estimator of the covariance parameter of the Normal distribution. A special case is the diagonal approximation H\u0302DIAG(C) = H\u0302MVN-PLUGIN(C I) where only the variances are used and is the elementwise matrix product, such that all covariances are set to zero. However, as for the discrete entropy, this estimator is biased: if the true distribution is Normal, then (7) underestimates its entropy.\nIf we are to accept the Normal approximation, we should use a better estimator of its entropy. Such an estimator exists, as (Ahmed & Gokhale, 1989) show: they derive among all unbiased estimators one that uniformly has the smallest variance. It is\nH\u0302MVN-UMVUE = d\n2 log(e\u03c0) +\n1 2 log | \u2211 y\u2208Y yyT | (8)\n\u22121 2 d\u2211 j=1 \u03c8 ( n+ 1\u2212 j 2 ) ,\nwhere \u03c8 is again the digamma function. While (8) improves over (7), a fundamental problem with the Normal approximation is misspecification: we assume that the samples come from a Normal distribution, but in fact they could come from any other distribution."}, {"heading": "3.2. Non-parametric Entropy Estimation", "text": "Instead of making the assumption that the distribution belongs to a parametric family of distributions, nonparametric entropy estimates aim to work for all distributions satisfying some general assumptions. Typically, the assumptions we make here, bounded support and absolute continuity, are sufficient conditions for these estimates to exist.\nThe 1-nearest neighbor estimator of the differential entropy was proposed by (Kozachenko & Leonenko, 1987) (a detailed description in English is given in (Beirlant et al., 2001)). For each sample we define the one nearest neighbor (1NN) distance \u03c1i = minj\u2208{1,...,n}\\{i} \u2016yj \u2212 yi\u2016. The estimate is\nH\u03021NN = d\nn n\u2211 i=1 log \u03c1i + log(n\u2212 1) + \u03b3 + log Vd, (9)\nwhere \u03b3 \u2248 0.5772 is the Euler-Mascheroni constant, and Vd = \u03c0\nd/2/\u0393(1 + d2 ) is the volume of the ddimensional hypersphere. We can efficiently compute (9) for small number of output dimensions (say, d \u2264 10) by the use of k-d trees (Friedman et al., 1977). Then, computing \u03c1i is O(log n) so that for a fixed dimension we can evaluate H\u03021NN in O(n log n) time. To speed up evaluation, we subsample the set Y used in H1NN to keep only 256 samples, without replacement.\nOther non-parametric estimates of the differential entropy have been derived from kernel density estimates (Beirlant et al., 2001), length of minimum spanning trees (Hero & Michel, 1999; Costa & Hero, 2004), and k-nearest neighbor distances (Goria et al., 2005). The latter two could potentially improve on the 1-NN estimate, but we did not examine them further. For an introduction to the field of non-parametric estimation of information-theoretic functions, see (Wang et al., 2009), and the earlier survey (Beirlant et al., 2001)."}, {"heading": "3.3. Experiments", "text": "For the experiments we use the same features and method as in the classification task, but compare four different entropy estimators. We again follow the recommendations of (Dems\u030car, 2006).\nKernel density leaf model. For each leaf ` in the tree we use the kernel density estimate as leaf density,\n(Ha\u0308rdle et al., 2004)[Section 3.6],\np`(y) = 1\nn` n\u2211\u0300 i=1 kB(y \u2212 y(`)i ),\nwhere kB(y) = k(B \u22121y)/ det(B) is the kernel and B \u2208 Rd\u00d7d is the bandwidth matrix. We estimate the bandwidth matrix using \u201cScott\u2019s rule\u201d as B = n \u22121/(d+4) ` \u03a3\u0302 1/2, where \u03a3\u0302 = 1n`\u22121 \u2211n` i=1 yiy T i + \u03bbI is the regularized sample covariance of all samples reaching this leaf node, and \u03bb \u2265 0 is a regularization parameter determined by model selection. As basis kernel k : Rd \u2192 R+ we use a multivariate standard Normal kernel. Hence p` is a properly normalized density.\nSetup. We use 18 univariate and multivariate regression data sets from the UCI, StatLib, AMSTAT repositories, and one own data set, see Table 2.\nThe absolute continuity assumption is satisfied for most of the data sets we considered, with one notable exception: for the UCI forestfire data set the predictive target is one dimensional (the area of forest burned in a particular district), but is a mixed variable, being exactly zero for around half of the instances, and continuous real-valued for the other half. As such, it does not have an absolutely continuous CDF and the information gain criterion is not justified. If we would continue anyway we can have \u03c1i = 0, leading to invalid expression of log 0 in (9).\nFor other data sets, however, another problem occurs: discretization of an originally continuous variable. For example, in the classic Boston housing benchmark data set from the UCI repository, the predictive target (median value in 1000 USD) is discretized at a granularity of 0.1, whereas the original quantity is clearly continuous. To satisfy the absolute continuity assumption we opted to add\u2014prior to any experiments\u2014a sample from the uniform U(\u2212h/2, h/2) to each predictive target, where h is the discretization bin width. This corresponds to a piecewise constant density in each bin, a reasonable assumption for the small bin sizes used. We do this for all data sets that have two or more identical predictive target vectors, and estimate the bin size h for each dimension individually by taking the smallest positive difference between sorted values in that dimension.\nAs a last step, because the differential entropy is not invariant to affine transformations we standardize each output dimension of the training set individually, but undo this scaling on the target predictions before measuring the root mean squared error.\nWe perform model selection by first splitting the en-\ntire data set into trainval and test in proportions 60/40, and then replicate ten times the following procedure: split the trainval data set into train and val data sets at proportions 40/20, train a model from the train set and evaluate its performance on val. The best model parameters for each estimator is used in the final training replicates using the full trainval data set for training. The following parameters were fixed in all experiments: number of trees 8, minimum samples per leaf 16, number of feature tests 256, nonparametric subsampling size 256. The parameter selected was the kernel density estimation regularization \u03bb \u2208 {10\u22124, 10\u22123, 10\u22122, 0.1, 1}. All models are trained on exactly the same data sets using the same parameters, except for the entropy estimator.\nWe estimate the expected log-likelihood \u3008LL\u3009 for a single sample from the true distribution as average and one unit standard deviations over the ten runs, trained on trainval and tested on test. The log-likelihood is the uniform average of each individual tree model.\nResults. The non-parametric entropy estimator yields the highest ranking holdout likelihood (average rank 1.83) compared to the other estimators. A Friedman test (Dems\u030car, 2006) rejects the null-hypothesis of equal performance (p-value 0.0272, Iman-Davenport statistic 3.31 at N = 18, k = 4). In contrast, for the RMSE performance the null-hypothesis of equal performance is not rejected (p-value 0.350, ImanDavenport statistic 1.12), suggesting that a better loglikelihood does not yield lower RMSE. The runtime of the 1NN estimator is a about 6-10 times larger per data set than for the Normal estimator; the entire experiments completed in 24 hours on an eight core PC.\nDiscussion. MVN-UMVUE and MVN-PLUGIN seem to perform equally well and this indicates that the misspecification error dominates the Normal entropy estimation error. Training with the 1-NN estimator yields the best log-likelihood in 10 out of 18 cases, showing that the conditional density p(y|x) is captured more accurately. This does not translate into a statistically significant improvement in RMSE, where the 1-NN estimator wins 8 out of 18 cases. The lack of improvement in terms of RMSE makes sense from the point of view of empirical risk minimization."}, {"heading": "4. Conclusion", "text": "We have proposed the use of recently developed entropy estimators for decision tree training. Our approach applies only to classification and regression trees using information gain as a split scoring function. While this appears limiting, this variant of decision\ntrees is a popular one, implemented in many software packages and used in hundreds of publications. We therefore suggest that the improvement in predictive performance derived from improved information gain estimates, while small, is useful to many. Furthermore, the changes are well-motivated and require only minor modifications to existing implementations.\nAcknowledgements. The author thanks Jamie Shotton, Antonio Criminisi, and the anonymous reviewers for their helpful feedback."}], "references": [{"title": "Randomized inquiries about shape; an application to handwritten digit recognition", "author": ["Amit", "Yali", "Geman", "Donald"], "venue": "Tech. Rep. 401, Dept. of Statistics,", "citeRegEx": "Amit et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Amit et al\\.", "year": 1994}, {"title": "Convergence properties of functional estimates for discrete distributions", "author": ["Antos", "Andr\u00e1s", "Kontoyiannis", "Ioannis"], "venue": "RSA: Random Structures & Algorithms,", "citeRegEx": "Antos et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2001}, {"title": "Nonparametric entropy estimation", "author": ["Beirlant", "Jan", "Dudewicz", "Edward J", "Gy\u00f6rfi", "L\u00e1szl\u00f3", "van der Meulen", "Edward C"], "venue": "An overview,", "citeRegEx": "Beirlant et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Beirlant et al\\.", "year": 2001}, {"title": "Classification and Regression Trees", "author": ["Breiman", "Leo", "Friedman", "Jerome H", "Olshen", "Richard A", "Stone", "Charles J"], "venue": null, "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Data mining criteria for tree-based regression and classification", "author": ["Buja", "Andreas", "Lee", "Yung-Seop"], "venue": "In KDD,", "citeRegEx": "Buja et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Buja et al\\.", "year": 2001}, {"title": "Loss functions for binary class probability estimation and classification: Structure and applications", "author": ["Buja", "Andreas", "Stuetzle", "Werner", "Shen", "Yi"], "venue": "Technical report,", "citeRegEx": "Buja et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Buja et al\\.", "year": 2005}, {"title": "A further comparison of splitting rules for decision-tree induction", "author": ["Buntine", "Wray", "Niblett", "Tim"], "venue": "Machine Learning,", "citeRegEx": "Buntine et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Buntine et al\\.", "year": 1992}, {"title": "Geodesic entropic graphs for dimension and entropy estimation in manifold learning", "author": ["Costa", "Jose A", "Hero", "Alfred O"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Costa et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Costa et al\\.", "year": 2004}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Dem\u0161ar", "Janez"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dem\u0161ar and Janez.,? \\Q2006\\E", "shortCiteRegEx": "Dem\u0161ar and Janez.", "year": 2006}, {"title": "An algorithm for finding best matches in logarithmic expected time", "author": ["Friedman", "Jerome H", "Bentley", "Jon Louis", "Finkel", "Raphael Ari"], "venue": "ACM Trans. on Mathematical Software,", "citeRegEx": "Friedman et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1977}, {"title": "Finite sample corrections to entropy and dimension estimates", "author": ["Grassberger", "Peter"], "venue": "Physics Letters A,", "citeRegEx": "Grassberger and Peter.,? \\Q1988\\E", "shortCiteRegEx": "Grassberger and Peter.", "year": 1988}, {"title": "Entropy estimates from insufficient samplings", "author": ["Grassberger", "Peter"], "venue": null, "citeRegEx": "Grassberger and Peter.,? \\Q2003\\E", "shortCiteRegEx": "Grassberger and Peter.", "year": 2003}, {"title": "Nonparametric and Semiparametric Models", "author": ["H\u00e4rdle", "Wolfgang", "M\u00fcller", "Marlene", "Sperlich", "Stefan", "Werwatz", "Axel"], "venue": null, "citeRegEx": "H\u00e4rdle et al\\.,? \\Q2004\\E", "shortCiteRegEx": "H\u00e4rdle et al\\.", "year": 2004}, {"title": "Asymptotic theory of greedy approximations to minimal k-point random graphs", "author": ["Hero", "Alfred O", "Michel", "Olivier"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Hero et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hero et al\\.", "year": 1999}, {"title": "Experiments in Induction", "author": ["E.B. Hunt", "J. Marin", "P.T. Stone"], "venue": null, "citeRegEx": "Hunt et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Hunt et al\\.", "year": 1966}, {"title": "Constructing optimal binary decision trees is NP-complete", "author": ["Hyafil", "Laurent", "Rivest", "Ronald L"], "venue": "Information Processing Letters,", "citeRegEx": "Hyafil et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Hyafil et al\\.", "year": 1976}, {"title": "Sample estimate of the entropy of a random vector", "author": ["L.F. Kozachenko", "N.N. Leonenko"], "venue": "Probl. Peredachi Inf.,", "citeRegEx": "Kozachenko and Leonenko,? \\Q1987\\E", "shortCiteRegEx": "Kozachenko and Leonenko", "year": 1987}, {"title": "The importance of attribute selection measures in decision tree induction", "author": ["Liu", "Wei Zhong", "White", "Allan P"], "venue": "Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1994}, {"title": "Note on the bias of information estimates", "author": ["G. Miller"], "venue": "Information theory in Psychology,", "citeRegEx": "Miller,? \\Q1955\\E", "shortCiteRegEx": "Miller", "year": 1955}, {"title": "An empirical comparison of selection measures for decision-tree induction", "author": ["Mingers", "John"], "venue": "Machine Learning,", "citeRegEx": "Mingers and John.,? \\Q1989\\E", "shortCiteRegEx": "Mingers and John.", "year": 1989}, {"title": "Estimation of entropy and mutual information", "author": ["Paninski", "Liam"], "venue": "Neural Comp.,", "citeRegEx": "Paninski and Liam.,? \\Q2003\\E", "shortCiteRegEx": "Paninski and Liam.", "year": 2003}, {"title": "Induction of decision trees", "author": ["Quinlan", "J. Ross"], "venue": "Machine Learning,", "citeRegEx": "Quinlan and Ross.,? \\Q1986\\E", "shortCiteRegEx": "Quinlan and Ross.", "year": 1986}, {"title": "Bias analysis in entropy estimation", "author": ["Sch\u00fcrmann", "Thomas"], "venue": "Journal of Physics A,", "citeRegEx": "Sch\u00fcrmann and Thomas.,? \\Q2004\\E", "shortCiteRegEx": "Sch\u00fcrmann and Thomas.", "year": 2004}, {"title": "Universal estimation of information measures for analog sources", "author": ["Wang", "Qing", "Kulkarni", "Sanjeev R", "Verd\u00fa", "Sergio"], "venue": "Found. Trends Commun. Inf. Theory,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "Decision trees are a classic method of inductive inference that is still very popular (Breiman et al., 1984; Hunt et al., 1966).", "startOffset": 86, "endOffset": 127}, {"referenceID": 14, "context": "Decision trees are a classic method of inductive inference that is still very popular (Breiman et al., 1984; Hunt et al., 1966).", "startOffset": 86, "endOffset": 127}, {"referenceID": 3, "context": "(Breiman et al., 1984).", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "Different split scoring functions exist and have been used in the past (Breiman et al., 1984).", "startOffset": 71, "endOffset": 93}, {"referenceID": 5, "context": "For binary classification tasks the choice of split scoring functions has attracted special attention, as in this case the split scoring function can be related to statistical proper scoring rules (Buja et al., 2005).", "startOffset": 197, "endOffset": 216}, {"referenceID": 18, "context": "The first term, K\u22121 2n , however, can be evaluated and is known as the Miller correction to (4), see (Miller, 1955).", "startOffset": 101, "endOffset": 115}, {"referenceID": 2, "context": "The 1-nearest neighbor estimator of the differential entropy was proposed by (Kozachenko & Leonenko, 1987) (a detailed description in English is given in (Beirlant et al., 2001)).", "startOffset": 154, "endOffset": 177}, {"referenceID": 9, "context": "We can efficiently compute (9) for small number of output dimensions (say, d \u2264 10) by the use of k-d trees (Friedman et al., 1977).", "startOffset": 107, "endOffset": 130}, {"referenceID": 2, "context": "Other non-parametric estimates of the differential entropy have been derived from kernel density estimates (Beirlant et al., 2001), length of minimum spanning trees (Hero & Michel, 1999; Costa & Hero, 2004), and k-nearest neighbor distances (Goria et al.", "startOffset": 107, "endOffset": 130}, {"referenceID": 23, "context": "For an introduction to the field of non-parametric estimation of information-theoretic functions, see (Wang et al., 2009), and the earlier survey (Beirlant et al.", "startOffset": 102, "endOffset": 121}, {"referenceID": 2, "context": ", 2009), and the earlier survey (Beirlant et al., 2001).", "startOffset": 32, "endOffset": 55}, {"referenceID": 12, "context": "(H\u00e4rdle et al., 2004)[Section 3.", "startOffset": 0, "endOffset": 21}], "year": 2012, "abstractText": "Ensembles of classification and regression trees remain popular machine learning methods because they define flexible nonparametric models that predict well and are computationally efficient both during training and testing. During induction of decision trees one aims to find predicates that are maximally informative about the prediction target. To select good predicates most approaches estimate an informationtheoretic scoring function, the information gain, both for classification and regression problems. We point out that the common estimation procedures are biased and show that by replacing them with improved estimators of the discrete and the differential entropy we can obtain better decision trees. In effect our modifications yield improved predictive performance and are simple to implement in any decision tree code.", "creator": "LaTeX with hyperref package"}}}