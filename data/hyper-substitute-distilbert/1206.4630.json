{"id": "1206.4630", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Efficient Decomposed Learning for Structured Prediction", "abstract": "structured prediction replaces the work of functional machine operated applications. unfortunately, in symbolic prediction settings with expressive inter - variable representations, some behaviour - modifying statistical algorithms, e. g. structural svm, prove often intractable. we demonstrate at new way, decomposed models ( decl ), fundamentally performs optimal learning by restricting the inference step to potentially limited circle adjoining the known spaces. we describe training based on the algorithms, target parameters, and output labels, under which characterization is equivalent to semantic learning. we best show that in improved numerical examples, where conventional theoretical assumptions we sometimes completely replace, decl - enhanced rules exhibit systematically more adaptive and as accurate alternative artificial learning.", "histories": [["v1", "Mon, 18 Jun 2012 15:08:38 GMT  (423kb)", "http://arxiv.org/abs/1206.4630v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rajhans samdani", "dan roth"], "accepted": true, "id": "1206.4630"}, "pdf": {"name": "1206.4630.pdf", "metadata": {"source": "META", "title": "Efficient Decomposed Learning for Structured Prediction", "authors": ["Rajhans Samdani", "Dan Roth"], "emails": ["rsamdan2@illinois.edu", "danr@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "Structured output spaces occur in many machine learning applications which aim to label certain sets of interdependent variables where the dependencies between variables dictate what assignments are possible. Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004). Typical discriminative structural learning algorithms (e.g. Collins (2002); Tsochantaridis et al. (2004)) perform a global MAP inference over the entire (hence \u2018global\u2019) output space as an intermediate step. We refer to such learning techniques as global learning (GL). Global inference, and hence GL, can be slow for models with high-order, expressive relations between the output variables.\nGL algorithms perform exact MAP inference as a black box which may be an overkill for several problems, making learning slow. To alleviate this, we propose a novel algorithm which restricts the MAP in-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nference to a smaller part of the output space by using additional information about a) the actual gold labels, b) the constraints on the output space, and c) the underlying parameters, which we want to learn. Consequently, our algorithm is much more efficient than GL. We call our approach Decomposed Learning (DecL) as we decompose the inference into smaller inference procedures over more tractable output spaces. We prove that in some settings, DecL is guaranteed to be equivalent to GL. We present experiments in real-world settings (where our theoretical assumptions may not hold) and show that DecL, with small-sized and problem-specific decompositions, perform as well as GL, while being significantly faster.\nSeveral existing works perform approximate inference during supervised structured learning to the same end. Such approaches can broadly be divided into those that relax the expressive interactions between output variables (Roth & Yih, 2005; Punyakanok et al., 2005; Sutton & Mccallum, 2009) during learning and those that relax the integrality constraints on assignments (Kulesza & Pereira, 2008; Finley & Joachims, 2008; Martins et al., 2009; Meshi et al., 2010). Some of the MCMC-based contrastive techniques (Hinton, 2002; Wick et al., 2011) are conceptually similar to DecL in that they use approximate gradient steps for learning. Our work is also related in spirit to Meshi et al. (2010) who consider a Linear Programming relaxation of the entire inference and perform parameter updates after small message-passing inference steps. However, unlike these techniques, we don\u2019t replace exact inference by approximate inference; instead, we perform exact inference on a smaller output space. The closest works to DecL are Pseudolikelihood-based techniques (Besag, 1974; Sontag et al., 2010) to learning; however, while Pseudolikelihood is consistent asymptotically, DecL aims to achieve equivalence to GL with a finite amount of data.\nThe outline of this paper is as follows. Sec. 2 introduces the problem and notation and Sec. 3 assays two extreme styles of structured prediction. We introduce our approach in Sec. 4 and provide theoretical results in Sec. 5. We finally present empirical results in Sec. 6."}, {"heading": "2. Problem Setting", "text": "Consider a structured prediction setting where a ddimensional input x is drawn from a space X and the output variable y is, w.l.o.g., a vector of binary labels {y1, . . . , yn} drawn from Y \u2208 {0, 1}n. The space Y may be specified by a set of declarative constraints which can be viewed as a form of specifying some domain knowledge over y.\nInference: The labels in y are correlated and so it is advantageous to predict them simultaneously. As is typical, we express the prediction over all variables in y using a scoring function f(x,y; w) = w \u00b7 \u03c6(x,y) as\narg max y\u2208Y f(x,y; w) = arg max y\u2208Y\nw \u00b7 \u03c6(x,y), (1)\nwhere \u03c6(x,y) \u2208 Rd are feature expressed over both x and y, and w \u2208 Rd are weight parameters. We refer to the arg max inference above as MAP inference1.\nStructural Learning and evaluation: The focus of this work is on learning the weight parameter, w, from a given collection of labeled training instances D = (x1,y1), . . . , (xm,ym). As is standard, the quality of a learned hypothesis is measured using a loss function \u2206 : {0, 1}n \u00d7 {0, 1}n \u2192 R\u22650, satisfying \u2206(y,y) = 0, \u2200y \u2208 {0, 1}n.\nWe focus on two popular classes of scoring functions f(x,y; w):\n\u2022 Singleton with constraints: f(x,y; w) is a sum of linear classifiers, fi(x), for individual yi: f(x,y; w) = \u2211n i=1 yifi(x) = \u2211n i=1 yiwi \u00b7 x. The\nvariables contained in y interact solely via mutual constraints. The region of allowed outputs, Y, is specified by these constraints. This model has been used in numerous applications, especially in Natural Language Processing (NLP) where sometimes the constraints are inherent to the problem e.g. tree constraints in dependency parsing (Koo et al., 2010) and sometimes they are added declaratively (Roth & Yih, 2005; Clarke & Lapata, 2006; Barzilay & Lapata, 2006; Roth & Yih, 2007; Clarke & Lapata, 2008; Choi & Cardie, 2009; Ganchev et al., 2010). Exact MAP inference with expressive constraints is often formulated using expensive Integer Linear Programming (ILP) techniques.\n\u2022 Pairwise Markov Networks: For a Pairwise Markov Network (PMN), f is defined over a graph with n nodes and a set of edges given by E. In particular, f is a sum of individual\n1While MAP is used to refer to probabilistic inference, we abuse the terminology here to convey similar import.\nand pairwise potential functions, \u03c6, corresponding to nodes and edges of the graph: f(x,y; w) =\u2211n i=1 \u03c6i(yi,x; w) + \u2211\ni,k\u2208E \u03c6i,k(yi, yk,x; w). f is linear in w. While PMNs are typical to probabilistic graphical model settings (e.g. HMM and CRF (Lafferty et al., 2001)), in this paper, we consider PMNs in a max-margin setting a la Taskar et al. (2004). PMNs are used extensively in many structured prediction applications in computer vision (Boykov et al., 1998), computational biology (Meshi et al., 2010), NLP, and information extraction (Lafferty et al., 2001; Sarawagi & Cohen, 2004). We also consider the case when higher order declarative constraints are added on top of a PMN scoring function (Roth & Yih, 2005)."}, {"heading": "3. Structured Prediction: Learning", "text": "This section discusses two styles of learning the parameter w from the training data D: global learning and local learning, with their shortcomings.\nGlobal Learning Given the inference procedure in (1) and training data D, a popular discriminative learning approach (Tsochantaridis et al., 2004; Taskar et al., 2004) is to minimize an SVM-style convex upper bound on the loss2 over the training data:\nl(w) = m\u2211 j=1 max y\u2208Y ( f(xj,y;w)\u2212 f(xj,yj;w) + \u2206(yj,y) ) (2)\nThe inference step in (2), involving max, is performed globally over all the labels of y and hence we call this style, Global Learning (GL). GL tends to be slow which hinders applications with a large output space or a large number of training examples.\nLocal Learning For faster learning, several approximations to GL have been used which ignore certain structural interactions so that the rest of the structure is easier to learn. We call this general paradigm of learning by relaxing to a more local or easy-to-learn structure, Local Learning (LL). For instance, when highly expressive constraints are used over the structure, then dropping such constraints makes the structure more \u201clocal\u201d and faster to learn: for singleton functions (Punyakanok et al., 2005; Barzilay & Lapata, 2006), ignoring constraints reduces the problem to learning n independent binary classifiers wi; in case of sequential or tree-structured problems, the task reduces to learning with dynamic programming inference (Koo et al., 2010; Roth & Yih, 2005). In case of multi-label classification, ignoring interactions between labels reduces the problem to learning a binary\n2Throughout this paper, we omit the usual l2 regularization term for the sake of brevity.\nclassifier for each label. In most LL scenarios, the ignored constraints, if any, are injected back during inference. Refer to Punyakanok et al. (2005) for a detailed analysis and comparison of GL and LL for singleton scoring functions with constraints.\nLL schemes are much faster than GL; in general, however, LL fails to take advantage of the structure of Y which is where Decomposed Learning comes in."}, {"heading": "4. Decomposed Learning (DecL)", "text": "For a training instance (xj,yj) \u2208 D, let nbr(yj) \u2286 Y be a subset of the output space defining a \u201cneighborhood\u201d around yj, which is referred to as the ground truth or the gold output. The key idea behind decomposed learning (DecL) is to learn w by discriminating the supervised label yj from only all y\u2032 \u2208 nbr(yj) instead of all y\u2032 \u2208 Y. nbr(yj) can use additional information about the structure (Y) and parameters which we intend to learn (w\u2217) such that it captures the structure of Y while being much smaller. Fig. 1 shows the general schema for both GL and DecL, showing the similarities and the differences.\nLet N = {nbr(yj)|j = 1, . . . ,m} be the collection of neighborhoods for all training instances. To pursue the general idea behind our approach, we use a maxmargin formulation (Taskar et al., 2004) for learning over given data D = {(xj,yj)}mj=1. Specifically we minimize a loss function DecL(w;D,N) given by\nm\u2211 j=1 max y\u2208nbr(yj) ( f(xj,y;w)\u2212f(xj,yj;w)+\u2206(y,yj) ) . (3)\nThe idea of looking at a smaller output space is natural; the key question is how do we create these neighborhoods so that the resulting learning algorithm is correct or at least gives a good approximation to GL. To motivate our technique for doing so, we use a simple example of multi-class classification with la-\nbels 1, . . . , r. This problem can be expressed as a structured prediction problem over r binary variables y1, . . . , yr such that an instance with label q is represented as a binary vector y[q] obeying the constraint\u2211\ni y[q]i = 1 and with y[q]i = 1. For a training instance (x,y[q]), GL aims to learn a scoring function which gives a score less than y[q] to all other possible outputs y[i], \u2200i 6= q. Since the outputs are constrained such that any two outputs y[q] and y[i] differ on just the bits yq and yi, this is achieved by merely comparing assignments to pairs of bits yq and yi, \u2200i 6= q. This is exactly what techniques like multiclass SVM (Crammer & Singer, 2002) and constrained classification (Har-Peled et al., 2003) do3. Overall, while multi-class classification is indeed a simple case as the space Y contains just n outputs, we generalize the idea of creating neighborhoods over a large number of variables via smaller and more local comparisons.\nWe generate nbr(yj), by fixing a subset of the output variables to their gold labels in yj, while allowing the rest of them to vary, and repeating the same for different subsets of output variables. We formalize this idea through what we define as decompositions (hence, decomposed learning.) We give theoretically desirable properties of these neighborhoods in Sec. 5.\nDefinition 1. Given a set of n binary output variables indexed by {1, . . . , n}, a decomposition S is a set containing distinct and non-inclusive (possibly overlapping) index sets which are subsets of {1, . . . , n}: S = {s1, . . . , sl| \u2200i, si \u2286 {1, . . . , n};\u2200i, k, si 6\u2282 sk}.\nBefore explaining learning with decompositions, we give some notation. Given two output instances y,y\u2032 \u2208 Y, let s(y,y\u2032) be the set indexing the differences between y and y\u2032 i.e. s(y,y\u2032) = {i : yi 6= y\u2032i}. Given a set s \u2286 {1, . . . , n}, denote \u2212s = {1, . . . , n}\\ s. Let ys \u2208 {0, 1}|s| denote an assignment to the variables indexed by set s. Let (ys,y j \u2212s) be the output formed by replacing variables in yj indexed by s by corresponding variables in ys.\nWe associate one decomposition Sj with each training instance (xj,yj) and do inference during learning as follows. Given a gold output variable yj pick a set s \u2208 Sj , fix variables in yj\u2212s and look at all assignments to ys such that (ys,y j \u2212s) is feasible (i.e. \u2208 Y); select the highest scoring assignment over all feasible selections of ys and over all s \u2208 Sj and return the structure. Given a decomposition Sj for yj, let the corresponding neighborhood be nbr(yj,Sj) given by nbr(yj,Sj) =\n3Interestingly, the one-vs-all technique ignores the given constraint (one-vs-all is a kind of LL technique) and may not be able to obtain linear separation even if the labels are pairwise linearly separable (Har-Peled et al., 2003).\nAlgorithm 1 Subgradient-descent Alg. for DecL\n1: Given: training data: D = (xj,yj)mj=1; step sizes \u03b7t decompositions: S = (S1, . . . ,Sm). 2: w\u2190 0 3: for t = 0 to T do 4: for j = 1 to m do 5: y\u2032 \u2190 arg max\ns\u2208Sj ,ys\u2208{0,1}|s|:(ys,yj\u2212s)\u2208Y\n(f(xj, (ys,y j \u2212s);w) + \u2206(ys,y j \u2212s)) 6: w\u2190 w + \u03b7t ( \u03c6(xj ,yj)\u2212 \u03c6(xj ,y\u2032) ) 7: end for 8: end for\n{y \u2208 Y|\u2203s \u2208 Sj , s(yj,y) \u2286 s}. Using the above style of inference results in minimizing the following convex function for learning\nDecL(w;D) = m\u2211 j=1\nmax s\u2208Sj ,ys\u2208{0,1}|s|:(ys,yj\u2212s)\u2208Y(\nf(xj, (ys,y j \u2212s);w) \u2212 f(x j,yj;w) + \u2206(ys,y j \u2212s) ) .(4)\nTo minimize Eq. 4, we use a subgradient descent scheme shown in Alg. 14.\nLet DecL-k be the special case where all subsets of {1, . . . , n} of size k (k \u2265 1) are considered in the decomposition. For multi-class classification, DecL-2 with \u2206 as the Hamming loss is the same as multiclass SVM (Crammer & Singer, 2002) and Alg. 1 with DecL-2 and \u2206 = 0 (perceptron loss) yields constrained classification (Har-Peled et al., 2003) thus closing our loop on multi-class classification. Note that, in Step 5 of Alg. 1, going over all sets in Sj to find arg max can be slow if the number of sets inside each decomposition is large (e.g. in DecL-k for large k.) To get around this, we compute max over a few sets selected uniformly at random from the decomposition. One can also use more complicated convex optimization techniques which require evaluating the max over just one set at a time (Gaudioso et al., 2006).\nIn practice, decompositions can be guided by domain knowledge \u2014 highly coupled output variables should be put in the same set while somewhat unrelated variables should be kept separate. The complexity of learning is small if the sizes of the sets considered in the decomposition are small. Sec. 5 provides theoretical results on decompositions for certain cases."}, {"heading": "5. Theoretical Analysis", "text": "Our theoretical anaylsis carries a different flavor than standard generalization bounds. We present theoretical results to show some conditions under which DecL\n4Instead of subgradient-descent, DecL can also be used in a cutting-plane method (Tsochantaridis et al., 2004).\nis equivalent to GL. We start with the trivial observation that when each neighborhood is equal to Y, then DecL is the same as GL. Due to the lack of space, we have moved all the proofs to the supplement.\nWe assume that the data is separable. Our interest is in all parameters w\u2217 which satisfy the following margin-separation condition \u2200(xj,yj) \u2208 D:\nf(xj,yj; w\u2217) \u2265 f(xj,y; w\u2217) + \u2206(yj,y), \u2200y \u2208 Y (5)\nthe set of which can be written (omitting regularization and using (2)) as W \u2217 = {w|l(w) = 0} \u2286 Rd. Let W dec = {w|DecL(w;D,N) = 0} \u2286 Rd be the set of weights obtained by DecL5 (we leave the neighborhoods selected for DecL implicit here.) Throughout this section, we assume that there exists at least one separating weight vector in W \u2217.\nAssumption 1: W \u2217 is non-empty.\nWe use the following property to express our results.\nExactness: DecL is said to be exact if W dec = W \u2217 for the given data D.\nOur goal is to find small neighborhoods for DecL for which exactness holds. Note that the Pseudolikelihood-based approaches (Besag, 1974; Dillon & Lebanon, 2010; Sontag et al., 2010) to structured prediction are asymptotically consistent; that is, they are equivalent to GL only in the limit of infinite data. In practice, one uses a finite amount of data to obtain a weight vector by minimizing a convex regularizer on w (e.g. min \u2016w\u2016p for p \u2265 1) while requiring separation (Cond. (5).) In this case, exactness, i.e. W dec = W \u2217, implies that DecL and GL minimize the same regularization function over two equal sets \u2014 if the regularizer is strictly convex, they will output the same weight. Thus exactness is clearly a stronger and more useful property than asymptotic consistency. Our goal is to determine families of decompositions that will result in the exactness of DecL.\nTo analyze exactness of DecL, we use the following property to characterize the loss function \u2206.\nSubadditivity: \u2206(y,y\u2032) is subadditive if \u2200y,y\u2032,y1,y2 \u2208 Y, with s(y,y1) \u222a s(y,y2) = s(y,y\u2032), we have \u2206(y,y\u2032) \u2264 \u2206(y,y1) + \u2206(y,y2).\nSeveral common loss functions like Perceptron loss i.e. no margin requirement, Hamming loss, and zero-one loss are subadditive. We now make the following simple observations. Observation 1 (Closed and Convex). W \u2217 is an\n5W \u2217 (and W dec) is clearly not a singleton set as l(w\u2217) = 0\u21d2 l(\u03bbw\u2217) = 0 \u2200\u03bb \u2265 1.\nintersection of closed half spaces \u2014 one for each separation constraint given by (5). Thus W \u2217 is closed and convex. Similarly, W dec is closed and convex.\nObservation 2 (Outer bound). For all decompositions, the set of separating weights for DecL give an outer-bound on the set of separating weights for GL, i.e. W \u2217 \u2286 W dec as DecL seeks to separate the gold output from only a subset of the output space.\nDue to observation 2, to show that DecL is exact for some decompositions, we need only show that for any w\u2032 /\u2208 W \u2217, we also have w\u2032 /\u2208 W dec \u2014 since both W \u2217 and W dec are closed and convex, we need to show this only for w\u2032 immediately outside the boundary of W \u2217 (see the proof in the supplement.) To this end, we define B(w, ) = {w\u2032| \u2016w\u2032 \u2212w\u2016 \u2264 } as a closed ball of radius centered around w. Theorem 1. DecL is exact if \u2200w \u2208 W \u2217,\u2203 > 0, such that \u2200w\u2032 \u2208 B(w, ), \u2200(xj,yj) \u2208 D the following condition holds for nbr(yj): if \u2203y \u2208 Y with f(xj,y; w\u2032) + \u2206(yj,y) > f(xj,yj; w\u2032) then \u2203y\u2032 \u2208 nbr(yj) with f(xj,y\u2032; w\u2032) + \u2206(yj,y\u2032) > f(xj,yj; w\u2032).\nThis theorem essentially requires that a w\u2032 which does not globally separate examples in D, also does not separate the decomposed learning examples. We note that this theorem is very general and applies to any structured prediction problem (and any \u2206.) We use this theorem to prove exactness for certain decompositions based on some easy to determine characterizations of a) the structure (Y), b) the correct parameters (W \u2217), and c) the data D. The following corollary is an immediate consequence of Theorem 1. Roughly, this corollary requires that the difference between the score of the gold output and that of any other output is bounded by the sum of score differences between the gold output and that of outputs in the neighborhood.\nCorollary 1. DecL is exact if \u2206 is subadditive and \u2200w \u2208W \u2217,\u2203 > 0 such that \u2200w\u2032 \u2208 B(w, ), \u2200(xj,yj) \u2208 D, \u2200y \u2208 Y, s(y,yj) can be partitioned into sets s1, . . . , sl such that \u2200k \u2208 {1, . . . , l}, (ysk ,y j \u2212sk) \u2208 nbr(yj,Sj) and f(xj,y; w\u2032)\u2212 f(xj,yj; w\u2032) \u2264\u2211l\nk=1 ( f(xj, (ysk ,y j \u2212sk)); w \u2032)\u2212 f(xj,yj; w\u2032) ) .(6)\nUsing these general results, we now examine two different classes of scoring functions mentioned in Sec. 2."}, {"heading": "5.1. Exactness of DecL for Singleton Scoring Functions with Constraints", "text": "In this section, we present exactness results for DecL with singleton scoring function f(x,y; w) =\n\u2211n i=1 yifi(x) = \u2211n i=1 yiwi \u00b7 x where the space Y is specified by constraints. For instance, Y can be specified by a collection of l logical constraints: Y = {y \u2208 {0, 1}n | Ck(y) = 1, k = 1, . . . , l} where Ck is a logical function (e.g. OR) over binary variables in y. Y can also be specified by linear constraints over y as Y = {y \u2208 {0, 1}n|Ay \u2264 b}.\nIn several practical applications, the constraint structure has some symmetry to it and we can invoke Cor. 1 to provide exactness guarantees for decompositions with set sizes independent of the number of variables. The following corollaries apply to two such cases with set sizes only dependent on the number of constraints.\nCorollary 2. If Y is specified by k OR constraints, then Decl-(k + 1) is exact for subadditive \u2206.\nCorollary 3. If Y is specified by k (k \u2265 1) linear constraints: Ay \u2264 b (or \u2018\u2265\u2019, \u2018=\u2019), where A is a binary matrix such that any two variables in y participate in at most one constraint, Decl-3k is exact for subadditive \u2206.\nAs a consequence of Cor. 2, if the space Y is specified by k horn clauses (Srikumar & Roth, 2011), then DecL-(k+ 1) is exact regardless of the number of variables in each clause.\nWe also note that the results in this section are based on worst-case analyses. In practice, much smallersized decompositions work well in most cases."}, {"heading": "5.2. Exactness for Pairwise Markov Networks", "text": "While in the last section, we presented exactness results solely based on constraints, in this section, we present decompositions for binary PMNs using some knowledge about the true parameters W \u2217. Recall that, for PMNs, f(x,y; w) =\u2211n i=1 \u03c6i(yi,x; w) + \u2211 i,k\u2208E \u03c6i,k(yi, yk,x; w) where E is the set of edges for the underlying graph. Inference over such functions is NP hard in general.\nA pairwise potential function, \u03c6i,k is called submodular if (\u03c6i,k(1, 1) + \u03c6i,k(0, 0)) \u2212 (\u03c6i,k(1, 0) + \u03c6i,k(0, 1)) > 0 i.e. it prefers similar labels; it is called supermodular if (\u03c6i,k(1, 1) + \u03c6i,k(0, 0))\u2212 (\u03c6i,k(1, 0) + \u03c6i,k(0, 1)) < 0.\nAssumption 2: Assume that \u2200(i, k) \u2208 E,\u2200xj \u2208 D,\u2200w\u2217 \u2208 W \u2217, \u03c6ik(\u00b7, \u00b7,xj; w\u2217) is either submodular or supermodular; also, we know if any given \u03c6ik is submodular or supermodular.\nSuch knowledge about pairwise potential functions is often available in practice, especially for submodular potentials. For instance, in several computer vision tasks, neighboring pixels are more likely to carry the same label (Besag, 1986; Boykov et al., 1998); in infor-\nmation extraction tasks, consecutive words are likely to belong to the same field. We can also approximately determine this information by computing mutual information over labeled data. With Assumption 2, we present a decomposition, which leads to exactness.\nFor each instance (x,yj = {yj1, . . . , yjn}), define Ej = {(u, v) \u2208 E|(\u03c6uv is submodular and yju = yjv) or (\u03c6uv is supermodular and y j u 6= yjv)} i.e. Ej removes those edges from E where the labels on nodes \u201cdisagree\u201d with the corresponding pairwise potentials. With (xj,yj), we associate a decomposition Spair(yj) = {c1, . . . , cl} where c1, . . . , cl are indices of the maximal connected components in Ej .\nTheorem 2. For PMNs where Assumption 2 is satisfied, DecL with Spair is exact with subadditive \u2206.\nAs a simple illustration, consider a sequential HMM where it is known that the same-state transition probabilities are higher than those for different states i.e. all \u03c6ik are submodular. Then for y\nj = 1110011, the corresponding decomposition is Spair = {{1, 2, 3}, {4, 5}, {6, 7}} as it contains the maximal connected components with the same label.\nNotably, graph cuts can be used for efficient learning over binary PMNs with submodular potentials (Szummer et al., 2008). We note that with submodular potentials, we can augment decomposed inference in DecL with graph cuts in a similar fashion to make it even more efficient.\nFinally, DecL can be used when certain additional global constraints are added to PMNs (Roth & Yih, 2005). The exactness guarantees hold for Spair(yj) if \u2200y \u2208 Y, \u2200s \u2208 Spair(yj), (ys,yj\u2212s) \u2208 Y. Exact global inference can replace DecL inference for those training examples where this condition does not hold. In practice, we find (see Sec. 6.3) that Spair works very well for non-binary PMNs in the presence of constraints, where some of our assumptions do not hold."}, {"heading": "6. Experiments", "text": "This section presents experimental results on non-ideal real world settings showing that DecL is effective and robust. We show results on synthetic data as well as two real-world tasks of multi-label classification and information extraction. We perform exact inference using ILP, wherever needed. We show that DecL performs favorably relative to GL on these tasks while greatly reducing the training time. We use appropriate LL approaches as competing baselines. In settings with constraints, we consider another baseline, LL+C, that uses constraints \u2014 which were ignored during learning \u2014 for inference during testing."}, {"heading": "6.1. Experiments on Synthetic Data", "text": "We first analyze DecL with simple decompositions \u2014 DecL-1,2,3 \u2014 with singleton scoring functions in a controlled synthetic setting by measuring the performance and efficiency. We generate data with 10 binary output variables, which are constrained by randomly generated linear constraints. We ensure that the resulting Y contains at least 50 outputs. The features x are sampled randomly from a 20-dimensional space. We randomly generate singleton scoring functions and determine gold labelings for each instance as per Eq. 1 (thus we know that the data is separable.)\nFor learning, we use SVM-Struct (Tsochantaridis et al., 2004) to implement our algorithms. Our LL baseline ignores the constraints during learning thereby reducing it to learning 10 independent binary classifiers. We test on 200 instances and tune C, the regularization penalty parameter, on 100 validation instances; we average over 10 random trials. Fig. 2 plots the loss for each algorithm against the size of the training data and Tab. 1 shows the training time on 320 examples. Note that the training time for DecL could be much lower with preprocessing of data.\nWe observe that although LL and LL+C exhibit relatively low error even with a small amount of data, they fail to converge to a near-perfect classifier like GL, with a large amount of data. On the other hand,\nDecL-2, 3 exhibit performance close to global learning while taking much less time to train."}, {"heading": "6.2. Multi Label Document Classification", "text": "We test various algorithms on a multi-label document classification task over the Reuters dataset (Lewis et al., 2004). We use one section of the data with 6,000 instances and reduce it to the 30 most frequent labels. We keep 3600 instances for training, 1200 for testing, and 1200 for validation.\nWe model the scoring function as a PMN over a complete graph over all the labels to capture interactions between all pairs of labels. We compare DecL-1,2,3 with GL and a local learning (LL) approach which drops the pairwise components reducing the problem to 30 independent binary classifiers. We again use SVM-Struct for learning the parameters for GL and DecL. We measure the performance using a perinstance F1 measure given by F1 = 2ct+p , where t is the number of gold labels for this instance, p is the number of predicted labels, and c is the number of correct predictions; we report averages over all test instances. Tab. 2 presents results with 10-fold cross validation6. DecL-2,3 perform as well as GL and much better than LL. Notably, DecL-2 is 6 times faster than GL. As in the synthetic data experiments, DecL-1, a.k.a. Pseudomax (Sontag et al., 2010), performs badly."}, {"heading": "6.3. Information Extraction: Sequence Tagging with Submodular Potentials", "text": "We test the efficacy of our approach on two information extraction tasks inspired by our analysis of PMNs in Sec. 5.2. Our task is to identify the functional fields (e.g. \u2018author\u2019, \u2018title\u2019, \u2018facilties\u2019, \u2018roommates\u2019) from citations (McCallum et al., 2000) and advertisements (Grenager et al., 2005) datasets. We model this setting as an HMM (a special case of PMN) with different functional fields as hidden states and words as emissions. We add certain global constraints borrowed from Chang et al. (2007) to the HMM, which necessitate ILP-based inference.\n6We observe similar results for averaged per-label F1.\nFor the given tasks, words corresponding to a field, e.g. \u2018title\u2019, occur in long contiguous blocks; thus we assume that the correct HMM transition matrix has a high same-state transition probability which is a generalization of the submodular potentials we assumed in Sec. 5.2 and hence a natural testing ground for our theory. We use the decomposition Spair presented in Sec. 5.2 to perform DecL; intuitively, these decompositions enable DecL to capture the \u201cdiagonal-heavy\u201d nature of the HMM transition matrix while allowing it to learn the transitions between different fields.\nWe perform discriminative learning using averaged structured perceptron (Collins, 2002). We use HMM without constraints as an LL baseline. We obtain an LL+C baseline by adding constraints during test.\nTable 3 presents the results for the two domains. LLbased approaches perform very well for small data sizes because with a less expressive model, they need less data to generalize. However, with large amounts of training data, GL and DecL easily outperform HMM and LL+C. DecL does slightly, although not significantly, better than GL while being 2-8 times faster. Our results compare favorably with the stateof-the-art supervised results reported on these datasets by Chang et al. (2007) (CRR07.) Overall, we gather that by utilizing very simple knowledge of the task at hand (submodular potentials), we can perform nearglobal learning while being very efficient."}, {"heading": "7. Conclusion", "text": "We presented Decomposed Learning (DecL) \u2014 a technique for efficient structural learning. DecL learns efficiently by performing inference over a small part of the output space. We provided theoretical results, which use characterizations of the structure, target parameters, and ground truth labels to decompose the output space such that the resulting DecL is efficient and equivalent to exact learning. While the common approximation practice in structural learning tasks\nis to use approximate MAP inference without guarantees, our approach may provide a way to achieve significant improvements in these cases and can be augmented with existing approximation techniques like LP-relaxation. Indeed, our experimental results indicate that our algorithms are robust and perform very well on real world data.\nAcknowledgments: This research is sponsored by the Army Research Laboratory (ARL) under agreement W911NF-09-2-0053, Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-018, and an ONR Award on Guiding Learning and Decision Making in the Presence of Multiple Forms of Information. Any opinions, findings, conclusions or recommendations are those of the authors and do not necessarily reflect the views of the funding agencies."}], "references": [{"title": "Aggregation via Set Partitioning for Natural Language Generation", "author": ["R. Barzilay", "M. Lapata"], "venue": "In Proc. of HLT/NAACL,", "citeRegEx": "Barzilay and Lapata,? \\Q2006\\E", "shortCiteRegEx": "Barzilay and Lapata", "year": 2006}, {"title": "Spatial interaction and the statistical analysis of lattice systems", "author": ["J. Besag"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Besag,? \\Q1974\\E", "shortCiteRegEx": "Besag", "year": 1974}, {"title": "On the statistical analysis of dirty pictures", "author": ["J. Besag"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Besag,? \\Q1986\\E", "shortCiteRegEx": "Besag", "year": 1986}, {"title": "Markov random fields with efficient approximations", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "In CVPR,", "citeRegEx": "Boykov et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 1998}, {"title": "Guiding semisupervision with constraint-driven learning", "author": ["M. Chang", "L. Ratinov", "D. Roth"], "venue": "In ACL,", "citeRegEx": "Chang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2007}, {"title": "Adapting a polarity lexicon using integer linear programming for domainspecific sentiment classification", "author": ["Y. Choi", "C. Cardie"], "venue": "In EMNLP,", "citeRegEx": "Choi and Cardie,? \\Q2009\\E", "shortCiteRegEx": "Choi and Cardie", "year": 2009}, {"title": "Global inference for sentence compression: An integer linear programming approach", "author": ["J. Clarke", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Clarke and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Clarke and Lapata", "year": 2008}, {"title": "Constraint-based sentence compression: An integer programming approach", "author": ["Clarke", "James", "Lapata", "Mirella"], "venue": "In ACL,", "citeRegEx": "Clarke et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2006}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In EMNLP,", "citeRegEx": "Collins,? \\Q2002\\E", "shortCiteRegEx": "Collins", "year": 2002}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": null, "citeRegEx": "Crammer and Singer,? \\Q2002\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2002}, {"title": "Stochastic composite likelihood", "author": ["J.V. Dillon", "G. Lebanon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dillon and Lebanon,? \\Q2010\\E", "shortCiteRegEx": "Dillon and Lebanon", "year": 2010}, {"title": "Training structural svms when exact inference is intractable", "author": ["T. Finley", "T. Joachims"], "venue": "In ICML,", "citeRegEx": "Finley and Joachims,? \\Q2008\\E", "shortCiteRegEx": "Finley and Joachims", "year": 2008}, {"title": "Posterior regularization for structured latent variable models", "author": ["K. Ganchev", "J. Gra\u00e7a", "J. Gillenwater", "B. Taskar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ganchev et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2010}, {"title": "Unsupervised learning of field segmentation models for information extraction", "author": ["T. Grenager", "D. Klein", "C. Manning"], "venue": "In ACL,", "citeRegEx": "Grenager et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Grenager et al\\.", "year": 2005}, {"title": "Constraint classification for multiclass classification and ranking", "author": ["S. Har-Peled", "D. Roth", "D. Zimak"], "venue": "In NIPS,", "citeRegEx": "Har.Peled et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Har.Peled et al\\.", "year": 2003}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Comput.,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Dual decomposition for parsing with nonprojective head automata", "author": ["T. Koo", "A.M. Rush", "M. Collins", "T. Jaakkola", "D. Sontag"], "venue": "In EMNLP,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Structured learning with approximate inference", "author": ["A. Kulesza", "F. Pereira"], "venue": "In NIPS", "citeRegEx": "Kulesza and Pereira,? \\Q2008\\E", "shortCiteRegEx": "Kulesza and Pereira", "year": 2008}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In ICML,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": null, "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Polyhedral outer approximations with application to natural language parsing", "author": ["A.F.T. Martins", "N.A. Smith", "E.P. Xing"], "venue": "In ICML,", "citeRegEx": "Martins et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "Maximum entropy markov models for information extraction and segmentation", "author": ["A. McCallum", "D. Freitag", "F. Pereira"], "venue": "In ICML,", "citeRegEx": "McCallum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Learning efficiently with approximate inference via dual losses", "author": ["O. Meshi", "D. Sontag", "T. Jaakkola", "A. Globerson"], "venue": "In ICML,", "citeRegEx": "Meshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Meshi et al\\.", "year": 2010}, {"title": "Learning and inference over constrained output", "author": ["V. Punyakanok", "D. Roth", "W. Yih", "D. Zimak"], "venue": "In IJCAI,", "citeRegEx": "Punyakanok et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2005}, {"title": "Integer linear programming inference for conditional random fields", "author": ["D. Roth", "W. Yih"], "venue": "In ICML,", "citeRegEx": "Roth and Yih,? \\Q2005\\E", "shortCiteRegEx": "Roth and Yih", "year": 2005}, {"title": "Global inference for entity and relation identification via a linear programming formulation", "author": ["D. Roth", "W. Yih"], "venue": "Introduction to Statistical Relational Learning,", "citeRegEx": "Roth and Yih,? \\Q2007\\E", "shortCiteRegEx": "Roth and Yih", "year": 2007}, {"title": "Semi-markov conditional random fields for information extraction", "author": ["Sarawagi", "Sunita", "Cohen", "William W"], "venue": "In NIPS,", "citeRegEx": "Sarawagi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sarawagi et al\\.", "year": 2004}, {"title": "More data means less inference: A pseudo-max approach to structured learning", "author": ["D. Sontag", "O. Meshi", "T. Jaakkola", "A. Globerson"], "venue": null, "citeRegEx": "Sontag et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2010}, {"title": "A joint model for extended semantic role labeling", "author": ["V. Srikumar", "D. Roth"], "venue": "In EMNLP,", "citeRegEx": "Srikumar and Roth,? \\Q2011\\E", "shortCiteRegEx": "Srikumar and Roth", "year": 2011}, {"title": "Piecewise training for structured prediction", "author": ["C. Sutton", "A. Mccallum"], "venue": "Machine Learning,", "citeRegEx": "Sutton and Mccallum,? \\Q2009\\E", "shortCiteRegEx": "Sutton and Mccallum", "year": 2009}, {"title": "Learning crfs using graph cuts", "author": ["M. Szummer", "P. Kohli", "D. Hoiem"], "venue": "In ECCV,", "citeRegEx": "Szummer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Szummer et al\\.", "year": 2008}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In NIPS,", "citeRegEx": "Taskar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "In ICML,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "Samplerank: training factor graphs with atomic gradients", "author": ["M. Wick", "K. Rohanimanesh", "K. Bellare", "A. Culotta", "A. McCallum"], "venue": "In ICML,", "citeRegEx": "Wick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wick et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 8, "context": "Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004).", "startOffset": 76, "endOffset": 141}, {"referenceID": 32, "context": "Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004).", "startOffset": 76, "endOffset": 141}, {"referenceID": 31, "context": "Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004).", "startOffset": 76, "endOffset": 141}, {"referenceID": 8, "context": "Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004). Typical discriminative structural learning algorithms (e.g. Collins (2002); Tsochantaridis et al.", "startOffset": 77, "endOffset": 218}, {"referenceID": 8, "context": "Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004). Typical discriminative structural learning algorithms (e.g. Collins (2002); Tsochantaridis et al. (2004)) perform a global MAP inference over the entire (hence \u2018global\u2019) output space as an intermediate step.", "startOffset": 77, "endOffset": 248}, {"referenceID": 23, "context": "Such approaches can broadly be divided into those that relax the expressive interactions between output variables (Roth & Yih, 2005; Punyakanok et al., 2005; Sutton & Mccallum, 2009) during learning and those that relax the integrality constraints on assignments (Kulesza & Pereira, 2008; Finley & Joachims, 2008; Martins et al.", "startOffset": 114, "endOffset": 182}, {"referenceID": 20, "context": ", 2005; Sutton & Mccallum, 2009) during learning and those that relax the integrality constraints on assignments (Kulesza & Pereira, 2008; Finley & Joachims, 2008; Martins et al., 2009; Meshi et al., 2010).", "startOffset": 113, "endOffset": 205}, {"referenceID": 22, "context": ", 2005; Sutton & Mccallum, 2009) during learning and those that relax the integrality constraints on assignments (Kulesza & Pereira, 2008; Finley & Joachims, 2008; Martins et al., 2009; Meshi et al., 2010).", "startOffset": 113, "endOffset": 205}, {"referenceID": 15, "context": "Some of the MCMC-based contrastive techniques (Hinton, 2002; Wick et al., 2011) are conceptually similar to DecL in that they use approximate gradient steps for learning.", "startOffset": 46, "endOffset": 79}, {"referenceID": 33, "context": "Some of the MCMC-based contrastive techniques (Hinton, 2002; Wick et al., 2011) are conceptually similar to DecL in that they use approximate gradient steps for learning.", "startOffset": 46, "endOffset": 79}, {"referenceID": 1, "context": "The closest works to DecL are Pseudolikelihood-based techniques (Besag, 1974; Sontag et al., 2010) to learning; however, while Pseudolikelihood is consistent asymptotically, DecL aims to achieve equivalence to GL with a finite amount of data.", "startOffset": 64, "endOffset": 98}, {"referenceID": 27, "context": "The closest works to DecL are Pseudolikelihood-based techniques (Besag, 1974; Sontag et al., 2010) to learning; however, while Pseudolikelihood is consistent asymptotically, DecL aims to achieve equivalence to GL with a finite amount of data.", "startOffset": 64, "endOffset": 98}, {"referenceID": 13, "context": "Some of the MCMC-based contrastive techniques (Hinton, 2002; Wick et al., 2011) are conceptually similar to DecL in that they use approximate gradient steps for learning. Our work is also related in spirit to Meshi et al. (2010) who consider a Linear Programming relaxation of the entire inference and perform parameter updates after small message-passing inference steps.", "startOffset": 47, "endOffset": 229}, {"referenceID": 16, "context": "tree constraints in dependency parsing (Koo et al., 2010) and sometimes they are added declaratively (Roth & Yih, 2005; Clarke & Lapata, 2006; Barzilay & Lapata, 2006; Roth & Yih, 2007; Clarke & Lapata, 2008; Choi & Cardie, 2009; Ganchev et al.", "startOffset": 39, "endOffset": 57}, {"referenceID": 12, "context": ", 2010) and sometimes they are added declaratively (Roth & Yih, 2005; Clarke & Lapata, 2006; Barzilay & Lapata, 2006; Roth & Yih, 2007; Clarke & Lapata, 2008; Choi & Cardie, 2009; Ganchev et al., 2010).", "startOffset": 51, "endOffset": 201}, {"referenceID": 18, "context": "HMM and CRF (Lafferty et al., 2001)), in this paper, we consider PMNs in a max-margin setting a la Taskar et al.", "startOffset": 12, "endOffset": 35}, {"referenceID": 3, "context": "PMNs are used extensively in many structured prediction applications in computer vision (Boykov et al., 1998), computational biology (Meshi et al.", "startOffset": 88, "endOffset": 109}, {"referenceID": 22, "context": ", 1998), computational biology (Meshi et al., 2010), NLP, and information extraction (Lafferty et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 18, "context": ", 2010), NLP, and information extraction (Lafferty et al., 2001; Sarawagi & Cohen, 2004).", "startOffset": 41, "endOffset": 88}, {"referenceID": 17, "context": "HMM and CRF (Lafferty et al., 2001)), in this paper, we consider PMNs in a max-margin setting a la Taskar et al. (2004). PMNs are used extensively in many structured prediction applications in computer vision (Boykov et al.", "startOffset": 13, "endOffset": 120}, {"referenceID": 32, "context": "Global Learning Given the inference procedure in (1) and training data D, a popular discriminative learning approach (Tsochantaridis et al., 2004; Taskar et al., 2004) is to minimize an SVM-style convex upper bound on the loss over the training data:", "startOffset": 117, "endOffset": 167}, {"referenceID": 31, "context": "Global Learning Given the inference procedure in (1) and training data D, a popular discriminative learning approach (Tsochantaridis et al., 2004; Taskar et al., 2004) is to minimize an SVM-style convex upper bound on the loss over the training data:", "startOffset": 117, "endOffset": 167}, {"referenceID": 23, "context": "For instance, when highly expressive constraints are used over the structure, then dropping such constraints makes the structure more \u201clocal\u201d and faster to learn: for singleton functions (Punyakanok et al., 2005; Barzilay & Lapata, 2006), ignoring constraints reduces the problem to learning n independent binary classifiers wi; in case of sequential or tree-structured problems, the task reduces to learning with dynamic programming inference (Koo et al.", "startOffset": 187, "endOffset": 237}, {"referenceID": 16, "context": ", 2005; Barzilay & Lapata, 2006), ignoring constraints reduces the problem to learning n independent binary classifiers wi; in case of sequential or tree-structured problems, the task reduces to learning with dynamic programming inference (Koo et al., 2010; Roth & Yih, 2005).", "startOffset": 239, "endOffset": 275}, {"referenceID": 23, "context": "Refer to Punyakanok et al. (2005) for a detailed analysis and comparison of GL and LL for singleton scoring functions with constraints.", "startOffset": 9, "endOffset": 34}, {"referenceID": 31, "context": "To pursue the general idea behind our approach, we use a maxmargin formulation (Taskar et al., 2004) for learning over given data D = {(x,y)}j=1.", "startOffset": 79, "endOffset": 100}, {"referenceID": 14, "context": "This is exactly what techniques like multiclass SVM (Crammer & Singer, 2002) and constrained classification (Har-Peled et al., 2003) do.", "startOffset": 108, "endOffset": 132}, {"referenceID": 14, "context": "Interestingly, the one-vs-all technique ignores the given constraint (one-vs-all is a kind of LL technique) and may not be able to obtain linear separation even if the labels are pairwise linearly separable (Har-Peled et al., 2003).", "startOffset": 207, "endOffset": 231}, {"referenceID": 14, "context": "1 with DecL-2 and \u2206 = 0 (perceptron loss) yields constrained classification (Har-Peled et al., 2003) thus closing our loop on multi-class classification.", "startOffset": 76, "endOffset": 100}, {"referenceID": 32, "context": "Instead of subgradient-descent, DecL can also be used in a cutting-plane method (Tsochantaridis et al., 2004).", "startOffset": 80, "endOffset": 109}, {"referenceID": 1, "context": "Note that the Pseudolikelihood-based approaches (Besag, 1974; Dillon & Lebanon, 2010; Sontag et al., 2010) to structured prediction are asymptotically consistent; that is, they are equivalent to GL only in the limit of infinite data.", "startOffset": 48, "endOffset": 106}, {"referenceID": 27, "context": "Note that the Pseudolikelihood-based approaches (Besag, 1974; Dillon & Lebanon, 2010; Sontag et al., 2010) to structured prediction are asymptotically consistent; that is, they are equivalent to GL only in the limit of infinite data.", "startOffset": 48, "endOffset": 106}, {"referenceID": 2, "context": "For instance, in several computer vision tasks, neighboring pixels are more likely to carry the same label (Besag, 1986; Boykov et al., 1998); in infor-", "startOffset": 107, "endOffset": 141}, {"referenceID": 3, "context": "For instance, in several computer vision tasks, neighboring pixels are more likely to carry the same label (Besag, 1986; Boykov et al., 1998); in infor-", "startOffset": 107, "endOffset": 141}, {"referenceID": 30, "context": "Notably, graph cuts can be used for efficient learning over binary PMNs with submodular potentials (Szummer et al., 2008).", "startOffset": 99, "endOffset": 121}, {"referenceID": 32, "context": "For learning, we use SVM-Struct (Tsochantaridis et al., 2004) to implement our algorithms.", "startOffset": 32, "endOffset": 61}, {"referenceID": 19, "context": "We test various algorithms on a multi-label document classification task over the Reuters dataset (Lewis et al., 2004).", "startOffset": 98, "endOffset": 118}, {"referenceID": 27, "context": "Pseudomax (Sontag et al., 2010), performs badly.", "startOffset": 10, "endOffset": 31}, {"referenceID": 21, "context": "\u2018author\u2019, \u2018title\u2019, \u2018facilties\u2019, \u2018roommates\u2019) from citations (McCallum et al., 2000) and advertisements (Grenager et al.", "startOffset": 60, "endOffset": 83}, {"referenceID": 13, "context": ", 2000) and advertisements (Grenager et al., 2005) datasets.", "startOffset": 27, "endOffset": 50}, {"referenceID": 4, "context": "We add certain global constraints borrowed from Chang et al. (2007) to the HMM, which necessitate ILP-based inference.", "startOffset": 48, "endOffset": 68}, {"referenceID": 4, "context": "datasets by Chang et al. (2007). We also show average", "startOffset": 12, "endOffset": 32}, {"referenceID": 8, "context": "We perform discriminative learning using averaged structured perceptron (Collins, 2002).", "startOffset": 72, "endOffset": 87}, {"referenceID": 4, "context": "Our results compare favorably with the stateof-the-art supervised results reported on these datasets by Chang et al. (2007) (CRR07.", "startOffset": 104, "endOffset": 124}], "year": 2012, "abstractText": "Structured prediction is the cornerstone of several machine learning applications. Unfortunately, in structured prediction settings with expressive inter-variable interactions, exact inference-based learning algorithms, e.g. Structural SVM, are often intractable. We present a new way, Decomposed Learning (DecL), which performs efficient learning by restricting the inference step to a limited part of the structured spaces. We provide characterizations based on the structure, target parameters, and gold labels, under which DecL is equivalent to exact learning. We then show that in real world settings, where our theoretical assumptions may not completely hold, DecL-based algorithms are significantly more efficient and as accurate as exact learning.", "creator": "LaTeX with hyperref package"}}}