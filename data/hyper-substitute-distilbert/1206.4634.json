{"id": "1206.4634", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Artist Agent: A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting", "abstract": "fan ink work, called r - e, shows one of ten other appealing painting approaches that has garnered traffic around active internet. alternative possibilities in computer - based ying - e technologies are manipulating abstract complex scene information remotely draw smooth and fluid brush strokes. to confidently find optimal strokes, we propose to model chinese brush as a semantic learning tool, and learn virtual perception - trajectories by maximizing the seriousness of losses in we policy implementation framework. we also provide typical outlines of actions, states, and rewards predicted for a b - e agent. full urgency of our proposed simulations is demonstrated through simulated mis - et experiments.", "histories": [["v1", "Mon, 18 Jun 2012 15:14:24 GMT  (4113kb)", "http://arxiv.org/abs/1206.4634v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.GR stat.ML", "authors": ["ning xie 0003", "hirotaka hachiya", "masashi sugiyama"], "accepted": true, "id": "1206.4634"}, "pdf": {"name": "1206.4634.pdf", "metadata": {"source": "META", "title": "Artist Agent: A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting", "authors": ["Ning Xie", "Hirotaka Hachiya", "Masashi Sugiyama"], "emails": ["XIE@SG.CS.TITECH.AC.JP", "HACHIYA@SG.CS.TITECH.AC.JP", "SUGI@CS.TITECH.AC.JP"], "sections": [{"heading": "1. Introduction", "text": "Among various techniques of non-photorealistic rendering, stroke-based painterly rendering simulates common practices of human painters who create paintings with brush strokes. In this paper, we focus on oriental ink painting.\nUnlike western styles, such as water-color, pastel, and oil painting, which place overlapped strokes into multiple layers (Hertzmann, 1998; Shiraishi & Yamaguchi, 2000), oriental ink painting uses few strokes to convey significant information about the scene. An artist can draw expressive strokes in various styles by soft brush tufts. The appearance of the stroke is therefore determined by the shape of an object to paint, the path and posture of the brush, and the distribution of pigments in the brush.\nDrawing smooth and natural strokes in arbitrary shapes is challenging since an optimal brush trajectory and the\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nposture of a brush footprint1 are different for each shape. Xie et al. (2011) formulated the problem of drawing brush strokes as minimization of an accumulated energy of moving the brush and used the Dynamic Programming (DP) approach to obtain optimal brush strokes. It was demonstrated that smooth and natural brush strokes could be obtained by minimizing the accumulated energy. However, the stroke optimized by DP for a specific shape cannot be applied to other shapes even when the difference is small. Thus, it is not efficient if the target object is composed of many basic shapes, e.g., a Chinese character, since the optimal brush stroke for each shape has to be obtained. Furthermore, ordinary DP cannot directly handle continuous actions and states. Thus, smoothness of resulted brush strokes is highly dependent on the discretization of spaces.\nIn this paper, we introduce a reinforcement learning (RL) approach to solving this problem. We model a soft-tuft brush as an RL agent that makes a sequential decision on which direction to move, and train the agent to draw graceful strokes in arbitrary shapes (see Figure 1). Our idea is to first learn a desired drawing policy by maximizing the sum of rewards from a number of typical training shapes. Then, the trained policy is applied to draw strokes in various new shapes.\nMore specifically, the proposed approach contains two technical challenges: how to design the brush agent and how to train the agent\u2019s policy. We first propose to design the state space of the brush agent to be relative to its surrounding shape, e.g., boundaries and the medial axis, to learn a general drawing policy which is independent of a specific entire shape. Secondly, we propose to formulate stroke drawing by a Markov decision process (MDP) (Sutton & Barto, 1998) and apply a policy gradient method (Williams, 1992) to learn a (local) optimal drawing policy. An advantage of the policy gradient method is that it can naturally handle continuous states and actions which are\n1We use a footprint to denote the region of a canvas which a brush stamps on.\nimportant for obtaining smooth and natural brush strokes. Furthermore, since a policy is a function for selecting actions given a state, a learned policy can be naturally applied to new shapes."}, {"heading": "2. Formulation of Automatic Stroke Generation", "text": "In this section, we formulate the problem of automatic stroke generation as a reinforcement learning (RL) problem."}, {"heading": "2.1. Markov Decision Process", "text": "Let us formulate the procedure of drawing a stroke as a Markov Decision Process (MDP) consisting of a tuple (S,A, pI, pT, R), where S is a set of continuous states, A is a set of continuous actions, pI is the probability-density of the initial state, pT(s\u2032|s, a) is the transition probabilitydensity from the current state s \u2208 S to next state s\u2032 \u2208 S when taking action a \u2208 A, R(s, a, s\u2032) is an immediate reward function for the transition from s to s\u2032 by taking action a.\nLet \u03c0(a|s;\u03b8) be a stochastic policy with parameter \u03b8, which represents the conditional probability density of taking action a given state s. Let h = (s1, a1, . . . , sT , aT , sT+1) be a trajectory of length T . Then the return (i.e., the discounted sum of future rewards) along h is expressed as\nR(h) = T\u2211 t=1 \u03b3t\u22121R(st, at, st+1),\nwhere \u03b3 \u2208 [0, 1) is the discount factor for the future reward.\nThe expected return for parameter \u03b8 is defined by\nJ(\u03b8) = \u222b p(h|\u03b8)R(h)dh,\nwhere\np(h|\u03b8) = p(s1) T\u220f t=1 p(st+1|st, at)\u03c0(at|st,\u03b8).\nThe goal of RL is to find the optimal policy parameter \u03b8\u2217 that maximizes the expected return J(\u03b8):\n\u03b8\u2217 \u2261 argmax J(\u03b8)."}, {"heading": "2.2. Policy Gradient Method", "text": "We use a policy gradient algorithm (Williams, 1992) to solve the above RL problem. That is, the policy parameter \u03b8 is updated via gradient ascent as\n\u03b8 \u2190\u2212 \u03b8 + \u03b5\u2207\u03b8J(\u03b8),\nwhere \u03b5 is a learning rate. The gradient \u2207\u03b8J(\u03b8) is given by\n\u2207\u03b8J(\u03b8) = \u222b \u2207\u03b8p(h|\u03b8)R(h)dh\n= \u222b p(h|\u03b8)\u2207\u03b8log p(h|\u03b8)R(h)dh\n= \u222b p(h|\u03b8) T\u2211 t=1 \u2207\u03b8log \u03c0(at|st,\u03b8)R(h)dh,\nwhere we used the so-called log trick: \u2207\u03b8p(h|\u03b8) = p(h|\u03b8)\u2207\u03b8log p(h|\u03b8). Since p(h|\u03b8) is unknown, the expectation is approximated by the empirical average:\n\u2207\u03b8J\u0302(\u03b8) = 1\nN N\u2211 n=1 T\u2211 t=1 \u2207\u03b8log \u03c0(a(n)t |s (n) t ,\u03b8)R(h (n)),\nwhere {h(n)}Nn=1 are N episodic samples with T steps and h(n) = (s\n(n) 1 , a (n) 1 , . . . , s (n) T , a (n) T , s (n) T+1).\nLet us employ the Gaussian policy function with parameter \u03b8 = (\u00b5>, \u03c3)>, where \u00b5 is the mean vector and \u03c3 is the standard deviation:\n\u03c0(a|s;\u03b8) = 1 \u03c3 \u221a 2\u03c0 exp\n( \u2212 (a\u2212 \u00b5 >s)2\n2\u03c32\n) .\nThen the derivatives of the expected return J(\u03b8) with respect to the parameter \u03b8 are given as\n\u2207\u00b5log\u03c0(a|s;\u03b8) = a\u2212 \u00b5>s\n\u03c32 s,\n\u2207\u03c3log\u03c0(a|s;\u03b8) = (a\u2212 \u00b5>s)2 \u2212 \u03c32\n\u03c33 .\nConsequently, the policy gradients \u2207\u03b8J\u0302(\u03b8) are expressed as\n\u2207\u00b5J(\u03b8) = 1\nN N\u2211 n=1 (R(h(n))\u2212 b) T\u2211 t=1 (a (n) t \u2212 \u00b5>s (n) t )s (n) t \u03c32 ,\n\u2207\u03c3J(\u03b8) = 1\nN N\u2211 n=1 (R(h(n))\u2212b) T\u2211 t=1\n( a (n) t \u2212\u00b5>s (n) t )2 \u2212 \u03c32\n\u03c33 ,\nwhere b is a baseline for reducing the variance of gradient estimates. The optimal baseline that minimizes the variance of the gradient estimate is given as follows (Peters & Schaal, 2006):\nb\u2217 = argmin b Var[\u2207\u03b8J\u0302(\u03b8)]\n' 1 N\n\u2211N n=1R(h (n)) \u2225\u2225\u2225\u2211Tt=1\u2207\u03b8 log \u03c0(a(n)t |s(n)t ;\u03b8)\u2225\u2225\u22252\n1 N \u2211N n=1 \u2225\u2225\u2225\u2211Tt=1\u2207\u03b8 log \u03c0(a(n)t |s(n)t ;\u03b8)\u2225\u2225\u22252 . Finally, the policy parameter \u03b8 = (\u00b5>, \u03c3)> is updated as\n\u00b5\u2190 \u00b5+ \u03b5\u2207\u00b5J(\u03b8), \u03c3 \u2190 \u03c3 + \u03b5\u2207\u03c3J(\u03b8)."}, {"heading": "3. Design of MDP for Brush Agent", "text": "In this section, we give a specific design of state space S, action space A, and immediate reward function R(s, a, s\u2032) for our brush agent to learn smooth and natural strokes. For this purpose, we first extract the boundary of a given object and then calculate the medial axis M , as illustrated in Figure 2."}, {"heading": "3.1. Design of Actions", "text": "To generate elegant brush strokes, the brush agent should move inside the given boundaries properly. To this end, we\nconsider four basic actions of the brush agent: movement of the brush, scaling up/down of the footprint, and rotation of the heading direction of the brush (see Figure 1(a)).\nSince properly covering the whole desired region is the most important issue, we treat the movement of the brush as the primary action (Action 1). The action a specifies the angle of the velocity vector of the agent relative to the medial axis. The action is determined by the Gaussian policy function. In practical applications, the agent should be able to deal with arbitrary strokes in various scales. To achieve stable performance in different scales, we adaptively change the velocity of the brush movement relative to the scale of the current footprint. The other actions (Actions 2, 3, and 4) are automatically optimized to satisfy the assumption that the tip of the agent should touch one side of the boundary; meanwhile, the bottom of the agent should tangent with the other side of the boundary. Otherwise, a new footprint will remain the same posture as the previous one, but just transit to a new position by Action 1."}, {"heading": "3.2. Design of States", "text": "We use the global measurement (the pose configuration of a footprint under the global Cartesian coordinate) and the relative state (the brush agent\u2019s pose and the locomotion information relative to the local surrounding environment). The relative state is calculated based on the global measurement values. Thus, both the global measurement and the relative state should be regarded as a state in terms of an MDP. However, for the calculating return and a policy, we use only the relative state, which allows the agent to learn a drawing policy that can be generalized to new shapes.\nOur relative state space design consists of two parts: Current surrounding shape and upcoming shape. More specifically, our state space is expressed by six features s = (\u03c9, \u03c6, d, \u03ba1, \u03ba2, l) > (see Figures 2 and 3), where\n\u2022 \u03c9 \u2208 (\u2212\u03c0, \u03c0]: The angle of the velocity vector of the brush agent relative to the medial axis.\n\u2022 \u03c6 \u2208 (\u2212\u03c0, \u03c0]: The heading direction of the brush agent relative to the medial axis.\n\u2022 d \u2208 [\u22122, 2]: The ratio of offset distance \u03b4 (see Figure 3) from the centerC of the brush agent to the nearest point P on the medial axis M over the radius r of the brush agent (|d| = \u03b4/r). d takes positive/negative values when the center of the brush agent is on the left-/right-hand side of the medial axis. On the other hand, d takes the value 0 when the center of the brush agent is on the medial axis. Furthermore, when d takes a value in [\u22121, 1], the brush agent is inside the boundaries (for example, dt\u22121 in Figure 3), and when the value of d is in [\u22122,\u22121) or in (1, 2], the brush agent goes over the boundary of one side (for example, dt in Figure 3). In our system, the center of the agent is restricted within the shape. Therefore, the extreme value of d is \u00b12, which means that the center of the agent is on the boundary.\n\u2022 \u03bai(i = 1, 2) \u2208 [0, 1): \u03ba1 provides the current surrounding information on the point Pt, whereas \u03ba2 provides the upcoming shape information on point Pt+1, as illustrated in Figure 3. The values are calculated as\n|\u03bai| = 2\n\u03c0 arctan ( \u03b1\u221a r\u2032i ) ,\nwhere \u03b1 is the parameter to control the sensitivity to the curvature and r\u2032i is the radius of the curve. More specifically, the value takes 0/negative/positive when the shape is straight/left-curved/right-curved, and the larger the value is, the tighter the curve is. Throughout this paper, we use a fixed value \u03b1 = 0.05.\n\u2022 l \u2208 {0, 1}: A binary label that indicates whether the agent moves to a region covered by the previous footprints or not. l = 0 means that the agent moves to a region covered by the previous footprint. Otherwise, l = 1 means that it moves to an uncovered region."}, {"heading": "3.3. Design of Immediate Rewards", "text": "We design the reward function so that the smoother the brush stroke is, the higher the reward is. For this purpose, we define the reward function as\nR(st, at, st+1) = 0 if ft = ft+1 or l = 0,\n1 + (|\u03ba1(t)|+ |\u03ba2(t)|)/2 \u03bb1E (t) location + \u03bb2E (t) posture otherwise.\nThat is, the immediate reward is zero when the brush is blocked by a boundary as ft = ft+1 or the brush is going backward to a region that has already been covered by previous footprints fi (i < t+ 1). |\u03ba1(t)|+ |\u03ba2(t)| adaptively increases immediate rewards depending on the difficulty of the current shape measured by the curvature \u03bai(t) of the medial axis.\nE (t) location measures the quality of the location of the brush agent with respect to the medial axis, defined by\nE (t) location = { \u03c41 |\u03c9t|+\u03c42(|dt|+W ) d \u2208 [\u22122,\u22121)\u222a(1, 2], \u03c41 |\u03c9t| d \u2208 [\u22121, 1],\nwhere \u03c41 and \u03c42 are weight parameters and W is the penalty. Since d contains information whether the agent goes over the boundary or not, as illustrated in Figure 3, the penalty W is added to Elocation when the agent goes over the boundary of the shape. When the brush agent is inside of the boundary, i.e., d \u2208 [\u22121, 1], Elocation depends only on the angle \u03c9t of the velocity vector.\nE (t) posture measures the quality of the posture of the brush agent based on neighboring footprints, defined by\nE (t) posture = \u03b61\u2206\u03c9t + \u03b62\u2206\u03c6t + \u03b63\u2206dt,\nwhere \u2206\u03c9t, \u2206\u03c6t, and \u2206dt are changes in angles \u03c9 of the velocity vector, heading directions \u03c6, and ratios d of the offset distance, respectively. The notation \u2206xt denotes the normalized squared changes between xt\u22121 and xt defined by\n\u2206xt = 1 if xt = xt\u22121 = 0,(xt \u2212 xt\u22121)2 (|xt|+ |xt\u22121|)2 otherwise.\n\u03b61, \u03b62, and \u03b63 are weight parameters. We set the parameters at \u03bb1 = \u03bb2 = 0.5, \u03c41 = \u03c42 = 0.5, and \u03b61 = \u03b62 = \u03b63 = 1/3."}, {"heading": "3.4. Design of Training Sessions for Brush Agent", "text": "Given an appropriately designed MDP, the final step is to design training sessions, which is also highly important to make the brush agent useful in practice.\nFirst of all, we propose to train the agent based on partial shapes, not the entire shapes. An advantage of this local training strategy is that various local shapes can be generated from a single entire shape, which significantly increases the number and variation of training samples. Another merit is that the generalization ability to new shapes can be enhanced, because even when the entire profile of a new shape is quite different from that of training data, the new shape may contain similar local shapes as illustrated in Figure 4(a).\nTo provide a wide variety of local shapes to the agent as training data, we prepared an in-house stroke library. This library contains 80 digitized real single brush strokes that are commonly used in Oriental ink painting. See Figure 4(c) for some examples. We extracted boundaries as the shape information and arranged them in a queue as training samples (see Figure 4(b)).\nIn the training scheme, the initial position of the first episode is chosen to be the start point S of the medial axis (Chin et al., 1995), and the direction to move is chosen to be the goal point G as illustrated in Figure 4(b). We estimate the length of an episode, T from the selected shapes. In the first episode, the initial footprint is set around the start point of the shape. In the following episodes, the initial footprint is set as either the last footprint in the previous episode or the footprint around the\nstart point. It depends on whether the agent moves well or is blocked by the boundary. For each policy, we repeat N episodes to collect data H = [h(1), h(2), . . . , h(N)], where h(n) = [s\n(n) 1 , a (n) 1 , . . . , s (n) T , a (n) T , s (n) T+1]. We then use the\ndata H to calculate the gradient of the return,\u2207\u03b8J(\u03b8), and update the policy parameter M times to optimize the policy.\nTo ensure the continuity along the episodes, we design the initial location of the agent as shown in Figure 4(b): In the first episode, the initial location of the agent is set on the medial axis, with its tipQ pointing to the end corner. In the next episode, the initial location is set to the location of the last footprint in the previous episode.\nThere are two exceptional situations where the new episode\u2019s initial location goes back to the initial location of the previous episode: The first situation is that the length of the up-coming track is much less than the length of the trajectory in T steps. The other situation is that the agent is blocked by the footprints generated by the previous episode."}, {"heading": "4. Experiments", "text": "In this section, we report experimental results."}, {"heading": "4.1. Setup", "text": "We train the policy of the brush agent on the shape shown in Figure 4(c) through our training strategy introduced in Section 3.4. The parameter of the initial policy is set as \u03b8 = (\u00b5>, \u03c3)> = (0, 0, 0, 0, 0, 0, 2)> according to the previous domain knowledge. The agent collects N = 300 episodic samples with trajectory length T = 32. The discount factor is set to \u03b3 = 0.99. The learning rate \u03b5 is set as 0.1/\u2016\u2207\u03b8J\u03b8\u2016. We investigate the average return over 10 trials as functions of policy-update iterations. The return at\neach trial is computed over 300 training episode samples."}, {"heading": "4.2. Results", "text": "The average return along the policy iteration is shown in Figure 5. The graph shows that the average return sharpy increases in an early stage and then it keeps stable after 35 iterations. Figure 6 shows examples of rendering and brush trajectory results in the policy training process.\nTable 1 shows the performance of policies learned by our RL method and the DP method (Xie et al., 2011) on an Intel Core i7 2.70 GHz. According to the discussion in Xie et al. (2011), the performance of the DP method depends on the setup of the parameter in the energy function and the\nnumber of candidates in the DP search space. However, it is hard to manually find the optimal parameters in practice. In Table 1, we list results obtained by the DP method with changing the number of candidates in each step of the DP search space. The results of the expected return and the execution time are significantly different depending on the number of candidates. In the DP method, the best value of the return is 26.27 when the number of candidates is set to 180, but this is computationally very expensive (2.08\u00d7103 seconds). Our RL method outperforms the best DP setup, with much less computation time.\nWe further apply our trained policy to more realistic shapes shown in Figure 7, which were not included in the train-\ning samples. We can observe that the well-trained policy can produce smooth and natural brush strokes in various shapes. We therefore conclude that our RL method is useful in a practical environment.\nFinally, we apply our brush agent to automatic photo conversion into an oriental ink style. We manually drew contours on a photo and let the agent automatically fill the shapes with strokes. The results are shown in Figure 8 and Figure 9, which we think are of good quality."}, {"heading": "5. Related Works", "text": "In this section, we briefly review current state-of-the-art in generating brush drawing, which have two approaches: Physics-based painting and stroke-based rendering."}, {"heading": "5.1. Physics-Based Painting", "text": "This approach aims at reproducing a real painting process and giving users intuitive and natural feeling when holding a mouse or a pen-like device. Several previous works have dealt with modeling the brush shapes, its dynamics, and its interaction with the paper, and simulating the ink dispersion and absorption by the paper.\nAmong the first stream, early representative works include the hairy brushes (Strassmann, 1986) and the physics-based models (Saito & Nakajima, 1999; Chu & Tai, 2004; Chu et al., 2010). For interactive use, these virtual brushes\nare convenient to draw various styles of strokes. Despite the extensive research literature, controlling automatically a virtual brush with six degrees of freedom\u2014three for the Cartesian coordinates and three for their angular orientation (pitch, roll, and yaw)\u2014in addition to the dynamics of the tufts is complex and existing physics-based models are in fact simplifications of the real process.\nOn the other hand, while the digital painting tools provide expert users a professional environment with a canvas, brushes, mixing palettes, and a multitude of color options, non-expert users often prefer simplified environments where paintings can be generated with minimum interaction and painting expertise.\nAnother major problem is that the computational cost is usually very high for satisfactory visual effects to human eyes. Some of them rely on GPU acceleration for satisfactory speed performance (Chu et al., 2010). Also, due to over-simplification, none of these methods has been able to simulate certain special brush strokes such as those impasto ones created with paint knives."}, {"heading": "5.2. Stroke-Based Rendering", "text": "In many situations, it is desirable to automatically convert real images into ink paintings, especially when the user has no painting expertise and is interested only in the painting results rather than in the painting process.\nThe skeleton stroke method (Hsu & Lee, 1994) generates brush strokes from the 2D paths given by either user interaction or automatic extraction from real images. However, the main difficulty is how to specify and vary the width of the strokes along the path as well as the texture of the strokes. One of the solutions is to specify the stroke backbone (Guo & Kunii, 2003) manually by a user. A limitation of such methods is that setting the values on each control point is time-consuming.\nContour-driven methods (Xie et al., 2011) can successfully generate strokes in desired shapes. However, there are several strict constraints: (I) When cutting the boundary region into slices at each step, the cross-sections should not intersect together. (II) A limited number of footprint candidates are only available for making the decision of moving to the next step. Although the second assumption ensures the same stride length of the agent at each step as well as speeding up the algorithm\u2019s execution time, states are typically modeled as discrete variables. This causes the resulting brush path not to be optimized well.\nOur proposed approach belongs to the category of strokebased rendering, with highly automatic and flexible stroke generation ability."}, {"heading": "6. Conclusions", "text": "In this paper, we applied reinforcement learning to oriental ink painting, allowing automatic generation of smooth and natural strokes in arbitrary shapes. Our contributions include careful designs of actions, states, immediate rewards, and training sessions. One of the key ideas was to design the state space of the brush agent to be relative to its surrounding shape, which allows us to learn a general drawing policy independent of a specific entire shape. Another important idea was to train the brush agent locally in the given shape. This contributed highly to enhancing the generalization ability to new shapes, because even when a new shape is quite different from training data as a whole, it contains similar local shapes.\nThe experimental results demonstrated that our RL method gives better performance than the existing DP methods with much less computation time, and our RL agent can successfully draw new complex shapes with smooth and natural brush strokes. Also, applications to automatic photo conversion into an oriental style was demonstrated to be promising."}, {"heading": "Acknowledgments", "text": "The authors would like to thank to the anonymous reviewers for their helpful comments. Ning Xie was supported by MEXT Scholarship, Hirotaka Hachiya was supported by the FIRST program, and Masashi Sugiyama was supported by MEXT KAKENHI 23120004."}], "references": [{"title": "Finding the medial axis of a simple polygon in linear time", "author": ["F. Chin", "J. Snoeyink", "A.W. Cao"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "Chin et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Chin et al\\.", "year": 1995}, {"title": "Real-time painting with an expressive virtual Chinese brush", "author": ["N. Chu", "Tai", "C.-L"], "venue": "IEEE Computer Graphics and Applications,", "citeRegEx": "Chu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2004}, {"title": "Detailpreserving paint modeling for 3D brushes", "author": ["N. Chu", "W. Baxter", "Wei", "L.-Y", "N. Govindaraju"], "venue": "In Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering,", "citeRegEx": "Chu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2010}, {"title": "Nijimi\u201d rendering algorithm for creating quality black ink paintings", "author": ["Q. Guo", "T.L. Kunii"], "venue": "In Proceedings of Computer Graphics International,", "citeRegEx": "Guo and Kunii,? \\Q2003\\E", "shortCiteRegEx": "Guo and Kunii", "year": 2003}, {"title": "Painterly rendering with curved brush strokes of multiple sizes", "author": ["A. Hertzmann"], "venue": "In Proceedings of SIGGRAPH,", "citeRegEx": "Hertzmann,? \\Q1998\\E", "shortCiteRegEx": "Hertzmann", "year": 1998}, {"title": "Drawing and animation using skeletal strokes", "author": ["S.C. Hsu", "H.H. Lee"], "venue": "In Proceedings of SIGGRAPH,", "citeRegEx": "Hsu and Lee,? \\Q1994\\E", "shortCiteRegEx": "Hsu and Lee", "year": 1994}, {"title": "Policy gradient methods for robotics", "author": ["J. Peters", "S. Schaal"], "venue": "In International Conference on Intelligent Robots and Systems,", "citeRegEx": "Peters and Schaal,? \\Q2006\\E", "shortCiteRegEx": "Peters and Schaal", "year": 2006}, {"title": "3D physics-based brush model for painting", "author": ["S. Saito", "M. Nakajima"], "venue": "In Proceedings of SIGGRAPH, Conference Abstracts and Applications,", "citeRegEx": "Saito and Nakajima,? \\Q1999\\E", "shortCiteRegEx": "Saito and Nakajima", "year": 1999}, {"title": "An algorithm for automatic painterly rendering based on local source image approximation", "author": ["M. Shiraishi", "Y. Yamaguchi"], "venue": "In Proceedings of the 1st International Symposium on Non-Photorealistic Animation and Rendering,", "citeRegEx": "Shiraishi and Yamaguchi,? \\Q2000\\E", "shortCiteRegEx": "Shiraishi and Yamaguchi", "year": 2000}, {"title": "Hairy brushes", "author": ["S. Strassmann"], "venue": "In Proceedings of SIGGRAPH, pp", "citeRegEx": "Strassmann,? \\Q1986\\E", "shortCiteRegEx": "Strassmann", "year": 1986}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "Contour-driven Sumi-e rendering of real photos", "author": ["N. Xie", "H. Laga", "S. Saito", "M. Nakajima"], "venue": "Computers & Graphics,", "citeRegEx": "Xie et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "Unlike western styles, such as water-color, pastel, and oil painting, which place overlapped strokes into multiple layers (Hertzmann, 1998; Shiraishi & Yamaguchi, 2000), oriental ink painting uses few strokes to convey significant information about the scene.", "startOffset": 122, "endOffset": 168}, {"referenceID": 12, "context": "Xie et al. (2011) formulated the problem of drawing brush strokes as minimization of an accumulated energy of moving the brush and used the Dynamic Programming (DP) approach to obtain optimal brush strokes.", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "Secondly, we propose to formulate stroke drawing by a Markov decision process (MDP) (Sutton & Barto, 1998) and apply a policy gradient method (Williams, 1992) to learn a (local) optimal drawing policy.", "startOffset": 142, "endOffset": 158}, {"referenceID": 11, "context": "We use a policy gradient algorithm (Williams, 1992) to solve the above RL problem.", "startOffset": 35, "endOffset": 51}, {"referenceID": 0, "context": "In the training scheme, the initial position of the first episode is chosen to be the start point S of the medial axis (Chin et al., 1995), and the direction to move is chosen to be the goal point G as illustrated in Figure 4(b).", "startOffset": 119, "endOffset": 138}, {"referenceID": 12, "context": "Table 1 shows the performance of policies learned by our RL method and the DP method (Xie et al., 2011) on an Intel Core i7 2.", "startOffset": 85, "endOffset": 103}, {"referenceID": 12, "context": "Table 1 shows the performance of policies learned by our RL method and the DP method (Xie et al., 2011) on an Intel Core i7 2.70 GHz. According to the discussion in Xie et al. (2011), the performance of the DP method depends on the setup of the parameter in the energy function and the Table 1.", "startOffset": 86, "endOffset": 183}, {"referenceID": 9, "context": "Among the first stream, early representative works include the hairy brushes (Strassmann, 1986) and the physics-based models (Saito & Nakajima, 1999; Chu & Tai, 2004; Chu et al.", "startOffset": 77, "endOffset": 95}, {"referenceID": 2, "context": "Among the first stream, early representative works include the hairy brushes (Strassmann, 1986) and the physics-based models (Saito & Nakajima, 1999; Chu & Tai, 2004; Chu et al., 2010).", "startOffset": 125, "endOffset": 184}, {"referenceID": 2, "context": "Some of them rely on GPU acceleration for satisfactory speed performance (Chu et al., 2010).", "startOffset": 73, "endOffset": 91}, {"referenceID": 12, "context": "Contour-driven methods (Xie et al., 2011) can successfully generate strokes in desired shapes.", "startOffset": 23, "endOffset": 41}], "year": 2012, "abstractText": "Oriental ink painting, called Sumi-e, is one of the most appealing painting styles that has attracted artists around the world. Major challenges in computer-based Sumi-e simulation are to abstract complex scene information and draw smooth and natural brush strokes. To automatically generate such strokes, we propose to model a brush as a reinforcement learning agent, and learn desired brush-trajectories by maximizing the sum of rewards in the policy search framework. We also elaborate on the design of actions, states, and rewards tailored for a Sumi-e agent. The effectiveness of our proposed approach is demonstrated through simulated Sumi-e experiments.", "creator": "LaTeX with hyperref package"}}}