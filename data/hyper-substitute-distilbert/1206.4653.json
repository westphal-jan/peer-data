{"id": "1206.4653", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Dimensionality Reduction by Local Discriminative Gaussians", "abstract": "we present integer and gaussian ( beta ) dimensionality reduction, especially positive dimensionality matching rule for integration. the ldg objective theorem involves always indication of the leave - one - out input error of a local quadratic field analysis experiment, and thus compute back to finite training point in order to find a mapping, candidate outputs can be scaled from dissimilar data. while complementary, - of - concern - nature optimal dimensionality reduction methods propose gradient elimination or explicit solution approaches, ldg depends accomplished during a 2d eigen - decomposition. thus, it scales better for datasets with a large number of accessibility requirements exceeding training examples. we efficiently adapt ldg to the faster learning implementation, and ensure unless it causes good performance beyond the test bench source differs from comparable desired simpler control data.", "histories": [["v1", "Mon, 18 Jun 2012 15:24:49 GMT  (662kb)", "http://arxiv.org/abs/1206.4653v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["nathan parrish", "maya r gupta"], "accepted": true, "id": "1206.4653"}, "pdf": {"name": "1206.4653.pdf", "metadata": {"source": "META", "title": "Dimensionality Reduction by Local Discriminative Gaussians", "authors": ["Nathan Parrish", "Maya R. Gupta"], "emails": ["nparrish@u.washington.edu", "gupta@ee.washington.edu"], "sections": [{"heading": "1. Introduction", "text": "Dimensionality reduction is the mapping of highdimensional data into a lower-dimensional space while retaining as much of the information content of the data as possible. As a preprocessing step for supervised classification algorithms, dimensionality reduction achieves several important goals. It reduces the storage requirements and algorithm complexity by reducing the input space of the data. It can improve performance of learning algorithms by rejecting spurious or noisy features prior to training and testing. Dimensionality reduction can also protect against overfitting by reducing the number of parameters learned by the\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nclassifier.\nWe present a method for supervised dimensionality reduction that is based on a local discriminative Gaussian (LDG) criterion. The discriminative Gaussian criterion is a smooth approximation to the leave-one-out cross-validation error of a quadratic discriminant analysis (QDA) classifier, so it seeks a mapping where a quadratic boundary separates the classes. Because this goal of separation by class may be difficult to achieve globally, our criterion instead operates locally to each training point.\nThe considered objective function is non-convex with no analytical solution; however, we present an approximation that is solved via a maximal eigenvalue decomposition. The simplicity of the solution is an advantage over other state-of-the-art dimensionality reduction techniques that require iterative solution methods or more complex generalized eigenvalue decompositions.\nWe perform experiments for supervised dimensionality reduction, and for dimensionality reduction for transfer learning. We show that on datasets with a large number of feature dimensions, other state-of-the-art algorithms are either intractably slow or exhibit numerical instability, whereas LDG is able to extract a useful mapping even when the number of features in the original data is in the thousands. We also show that LDG can be easily extended to the transfer learning setting, where the training data is drawn from a different distribution than the test data. Experiments show that LDG is effective in this setting as well."}, {"heading": "2. Problem Formulation", "text": "We take as given a set of labeled training data {(xi, yi)}ni=1, with xi \u2208 Rd and yi \u2208 {1, 2, ...,m} being the ith feature vector and class label respectively.\nWe wish to find a matrix B \u2208 Rd\u00d7l, l < d such that the\nreduced-dimensionality feature vectors {BTxi} can be separated according to class. We measure this separability by the performance of a generative classifier. Let p(xi|yi) be the likelihood of xi given class yi, estimated from the other n\u2212 1 training sample pairs. Then the leave-one-out cross-validation error of a maximum aposteriori (MAP) classifier acting on the mapped features measures the separation achieved by B:\nn\u2211 i=1 I ( p(BTxi|yi)p(yi) < max j p(BTxi|j)p(j) ) , (1)\nwhere the indicator function I(\u00b7) is one if its argument is true and zero otherwise.\nThe discontinuity of the indicator function in (1) makes it difficult to minimize. In order to arrive at a smooth, differentiable objective function that approximates (1), we substitute a log for the indicator and a sum for the max:\nf(B) = n\u2211 i=1 log\n(\u2211m j=1 p(B\nTxi|j)p(j) p(BTxi|yi)p(yi)\n) . (2)\nIn related work, p(xi|j) was assumed to be a Gaussian mixture model (GMM), and the objective was to learn a parameter vector, \u0398, of GMM weights, means, and variances that minimized f(\u0398) with p(xi|j,\u0398) replacing p(BTxi|j) in (2) (Ma & Chang, 2003). The learned parameters were shown to improve the GMM classification performance over the parameters learned by maximimum likelihood estimation. In that work, (2) is motivated as maximizing the mutual information between the class labels and the feature vectors.\nWe assume p(xi|j) is Gaussian, N (xi;\u00b5i,j ,\u03a3i,j). However, to reduce the model bias of assuming one Gaussian per class, we model p(xi|j) as locally Gaussian (Garcia et al., 2010). That is, we estimate the parameters of the Gaussian for point xi and class j by finding the k nearest class j neighbors, in Euclidean distance, to training point xi and using these points to estimate the Gaussian\u2019s maximum likelihood mean and covariance. To reduce estimation variance, we model each covariance matrix as a scaled identity \u03a3i,j = \u03c3 2 i,jI, where I is the properly sized identity matrix. Therefore, p(BTxi|j) = N (BTxi;BT\u00b5i,j , BTB\u03c32i,j).\nObjective (2) is non-convex with no analytical solution. Gradient-descent or global optimization can be used, but become computationally expensive if the number of classes, training samples, or dimensionality are large. Therefore, we propose a tractable approximation that has an analytical solution. The B that minimizes our approximation can be used directly (as\nwe do in our experiments), or as a starting point for a gradient descent approach to minimizing (2).\nWe rewrite (2) as\nf(B) = n\u2211 i=1 ( log ( m\u2211 j=1 p(BTxi|j)p(j) )\n(3)\n\u2212 log ( p(BTxi|yi)p(yi) )) ,\nand bound (2) from below with Jensen\u2019s inequality by replacing the first log term in (3) with\u2211m j=1 p(j) log(p(B\nTxi|j)). Also, we impose the constraint thatBTB = I. This constraint simplifies (2) by making the covariance of the Gaussians in the mapped space independent of B. Furthermore, it makes for a unique solution. After taking the log of the Gaussians, we arrive at the LDG objective:\nB\u2217 = arg min B\u2208Rd\u00d7l n\u2211 i=1\n( 1\u2212 p(yi)\n2\u03c32i,yi \u2206Ti,yiBB T\u2206i,yi (4)\n\u2212 m\u2211\nj=1,j 6=yi\n( p(j)\n2\u03c32i,j \u2206Ti,jBB T\u2206i,j )) s.t. BTB = I.\nwhere \u2206i,j = \u00b5i,j \u2212 xi.\nDespite the approximations, (4) retains an intuitive meaning. The B that minimizes the first term in (4) is the maximum likelihood solution for the correct-class local Gaussians. The second term is composed of m\u22121 different terms, each of which, if minimized individually, will give the maximum likelihood solution for an incorrect-class Gaussian, i.e. a Gaussian distribution trained by the local neighbors of xi coming from a different class. Therefore, (4) can be viewed as a regularized maximum likelihood estimate, where the regularization term attempts to minimize the likelihood of incorrect classes."}, {"heading": "2.1. LDG Solution", "text": "The B that optimizes (4) can be found with one eigendecomposition. Define\nV = n\u2211 i=1 1 \u03c32i,yi \u2206i,yi\u2206 T i,yi\nA = n\u2211 i=1 m\u2211 j=1 p(j) \u03c32i,j \u2206i,j\u2206 T i,j .\nThen (4) can be written as\nB\u2217 = arg min B\u2208Rd\u00d7l\n1 2 Tr ( BT (V \u2212A)B ) (5)\ns.t. BTB = I.\nSince both V and A are real symmetric matrices, it is straightforward to show that the solution to (5) is to set B\u2217\u2019s columns to be the l smallest eigenvectors of matrix (V \u2212A).\nAdditionally, we add a cross-validated regularization parameter, \u03b3, to (4), which we have found in practice can produce a mapping that better separates the data:\nB\u2217\u03b3 = arg min B\u2208Rd\u00d7l n\u2211 i=1\n( 1\n2\u03c32i,yi \u2206Ti,yiBB T\u2206i,yi (6)\n\u2212 \u03b3 m\u2211 j=1\n( p(j)\n2\u03c32i,j \u2206Ti,jBB T\u2206i,j )) s.t. BTB = I.\nThe solution to (6) is to set B\u2217\u03b3 \u2019s columns to be the l smallest eigenvectors of matrix (V \u2212 \u03b3A)."}, {"heading": "3. Related Methods for Linear Dimensionality Reduction", "text": "Fisher discriminant analysis (FDA) (Fisher, 1936) is a supervised technique that chooses B to maximize the ratio of the between-class covariance S(b) to the within-class covariance S(w). The solution is to choose the top eigenvectors of the generalized eigendecomposition S(b)\u03bb = \u03bdS(w)\u03bb. FDA has two drawbacks. First, FDA can perform poorly on multi-modal data where no single linear boundary separates the data by class. Second, the between-class covariance matrix is at most rank m\u2212 1, so FDA can provide at most m\u2212 1 dimensions.\nLocal Fisher discriminant analysis (LFDA) (Sugiyama, 2007) alleviates the drawbacks of FDA. LFDA generalizes FDA by adding a weight based on pairwise sample distances to the between-class and within-class covariance matrices. Thus, LFDA is able to separate multi-modal data. This change also results in LFDA being able to provide greater than m \u2212 1 dimensions. LFDA is solved using the same generalized eigen-decomposition as FDA.\nNeighbourhood components analysis (NCA) (Globerson et al., 2005) is a dimensionality reduction technique that is based on a smooth approximation to the leave-one-out k-NN error. The dimensionality reduction found by NCA was shown to provide good classification accuracy; however, it suffers from two key\ndrawbacks. First, the optimization requires gradient descent, and can be slow for datasets with a large number of features or training examples. Second, the NCA optimization must be re-run for any desired number of final dimensions. This is in contrast to principal components analysis (PCA), FDA, LFDA, and LDG, where B can be found once for the largest number of final dimensions, and then the top submatrices of B are the optimal solution for fewer dimensions.\nFinally, there is a large body of work in distance metric learning and feature selection that is related to linear dimensionality reduction. Distance metric learning addresses the problem of how best to determine the distance between feature vectors in Rd. Linear distance metric learning is primarily concerned with finding a positive semi-definite Mahalanobis metric M that gives the distance between xi and x` as\u221a\n(xi \u2212 x`)TM(xi \u2212 x`). Linear dimensionality reduction can be thought of as finding a low rank Mahalanobis metric M , such that M = BBT . The approaches given in (Globerson & Roweis, 2006; Davis et al., 2007; Weinberger & Saul, 2009) propose convex optimization problems for finding M . These methods suffer from the drawback that rank constraints are non-convex, and thus the M that they find is typically not low rank. However, we can perform dimensionality reduction by rewriting the Mahalanobis metric as M = L\u039bLT and using a feature selection method on the resulting zi = \u039b\n1/2LTxi as proposed in (Globerson & Roweis, 2006; Davis et al., 2007)."}, {"heading": "4. Dimensionality Reduction Experiments", "text": "We perform experiments to compare LDG to several different dimensionality reduction methods: PCA, FDA, LFDA, NCA, and information theoretic metric learning (ITML) (Davis et al., 2007) with feature selection using the maximum-relevance, minimum redundancy criterion (MRMR) (Peng et al., 2005). For the NCA, LFDA, ITML, and MRMR feature selection, we use code provided by the authors. We evaluate the performance of the dimensionality reduction methods via k-NN classification accuracy with k = 3, as was done in (Weinberger & Saul, 2009).\nAs a preprocessing step, we standard normalize the training data so that each feature has a mean of zero and standard deviation of one. We choose the number of neighbors used to estimate the local Gaussians for the LDG dimensionality reduction by fivefold cross-validation using a local QDA classifier (Garcia et al., 2010) on the original data. For LDG, \u03b3 \u2208 {.2, .4, .6, .8, 1}, and we choose whichever \u03b3 min-\nimizes the k-NN leave-one-out cross-validation error at dimensionality equal to the number of classes plus five. We have found that, in general, a few more dimensions than the number of classes present in the data is a good dimensionality at which to choose \u03b3. In the case of ties, we select the largest value of \u03b3. MRMR requires that we discretize the ITML features for feature selection, and we do so by thresholding at the mean, as recommended in the authors\u2019 code.\nWe perform experiments on fifteen datasets, and for each we average the accuracy over ten random 70/30 splits of the training and test data (up to a maximum of 3000 training samples). The datasets that we use can be found either at the UCI Machine Learning Repository or the Machine Learning Dataset Repository. The P53-Mutants dataset contained a large degree of class asymmetry. Therefore, we randomly sampled 143 of the inactive class samples and discarded the rest in order to make a 50/50 split between inactive and active class data (as opposed to the 1% vs 99% split in the original dataset).\nFigure 1 and Table 1 show that for small datasets, LDG is comparable to other state-of-the-art methods. However, LDG provides a clear advantage on the datasets with the largest feature dimensionality.\nNCA and ITML failed to converge in under three hours per training/test split on a standard 2.8 GHz PC for the datasets marked with \u201c-\u201d in Table 1, and results for these datasets are not plotted in Figure 1. Figure 1\nalso shows that ITML has difficulty with the Ringnorm dataset which has some features that are only noise.\nLDG also outperforms FDA and LFDA on some of the datasets. FDA can provide dimensionality only up to one fewer than the number of classes, which limits its performance on the Ionoshpere and Ringnorm datasets. Furthermore, FDA and LFDA exhibit numerical instability in some of the datasets with large feature dimensionality due to the fact that the withinclass covariance matrix is underdetermined. Thus, the generalized eigenvalue decomposition that these algorithms solve fails to find discriminative dimensions. LFDA returns complex eigenvalues for the Arcene and Dexter datasets, and FDA does the same on the Dexter dataset; thus, the LFDA and FDA results are not computable for these datasets.\nIn Table 1, we show the average classification accuracy when the dimensionality is chosen by leave-one-out cross-validation. We do this by increasing the dimensionality until the cross-validation accuracy decreases by adding another dimension. The run-time numbers measure the mean time it takes, in seconds, for the method to produce the dimensions shown in Figure 1 and to select the best dimensionality."}, {"heading": "5. LDG for Transfer Learning", "text": "In this section, we apply LDG dimensionality reduction to transfer learning. In transfer learning, we wish\nto classify test data drawn from some unknown target domain distribution of feature vectors and class labels where we have very few training examples. However, we assume that we have plenty of training examples from a source domain that differs from the target domain, but is thought to be useful for learning. For example, in our experiments we treat resized MNIST handwritten digits as the source and USPS handwritten digits as the target (see Figure 4).\nLet T = {(xi, yi)}nti=1 be the target domain training data drawn iid from some unknown joint distribution pT (x, y). Let S = {(x`, y`)}ns+nt`=nt+1 be the source domain training data drawn iid from unknown joint distribution pS(x, y) 6= pT (x, y), with ns >> nt.\nThe goal in transfer learning is to achieve high classification accuracy in the target domain by training a classifier using both sets of training data. Therefore, one of the goals for dimensionality reduction is to find a B matrix such that the target domain data are separated according to class. However, we now have the added goal that we wish to find a mapping where the source and target domain distributions are similar, i.e. pT (B\nTx, y) \u2248 pS(BTx, y). In Figure 2 we show examples of two different one-dimensional spaces that have been mapped from some higher-dimensional space (which is not shown). The left plot is a mapping in which the target domain data are separated according to class, but the source and target domain distributions are not similar. In the right plot, the target domain data are separated, and additionally, the source domain data distribution is similar to that of the target domain data. Therefore, both the source and target domain data can be used to train a classifier for the test data using the right-side mapping.\nFor transfer learning, we weight objective (2) for the target and source domain training data using parameter \u03b1,\nf(B) = (1\u2212 \u03b1) nt\u2211 i=1 log\n(\u2211m j=1 p(B\nTxi|j)p(j) p(BTxi|yi)p(yi)\n) (7)\n+ \u03b1 ns+nt\u2211 `=nt+1 log\n(\u2211m j=1 p(B\nTx`|j)p(j) p(BTx`|y`)p(y`)\n) .\nWe estimate the parameters of the Gaussian distribution for target domain point xi using the k nearest source domain training examples. The first term in (7) is the primary transfer term. The denominator in this term finds a B that maximizes the likelihood of the target domain data for a Gaussian distribution trained using the local source domain data, thus finding a B that brings the same-class source and target domain data close together. Conversely, the numer-\nator in the first term seeks a B that minimizes the likelihood of the different-class source Gaussians, thus ensuring that the mapping is still discriminative.\nThe second term in (7) is the normal LDG objective function for the source domain data only. Thus, if \u03b1 = 0.5, (7) is very similar to standard LDG dimensionality reduction acting on the pooled source and target domain data. We include this term because if the source and target domain distributions are similar, then we can set \u03b1 = 0.5 to train B using as much data as possible. We choose \u03b1 by cross-validating over the target domain training data. In case of ties, we choose the largest value of \u03b1, thereby defaulting to using as much data as possible. We make the same approximations as in Section 2 to find an analogous approximation to (6) for (7)."}, {"heading": "6. Related Methods for Transfer Learning", "text": "Of the dimensionality reduction techniques described in Section 3, only ITML has been adapted to the transfer learning scenario (Saenko et al., 2010). Let dM (xi, x`) = (xi \u2212 x`)TM(xi \u2212 x`), the squared Mahalanobis distance. The original ITML objective is:\nM\u2217 = arg min M 0 Tr(M)\u2212 log det(M)\ns.t. dM (xi, x`) \u2264 u, if yi = y` (8) dM (xi, x`) \u2265 v, if yi 6= y`.\nFor transfer metric learning, the authors propose to use objective function (8), but generate constraints only between examples from different domains, i.e. xi \u2208 T \u2200i and x` \u2208 S \u2200`. In this way, they find an M that makes distances between examples across the two domains small for same-class data and large for different-class data. We again use MRMR feature selection to find a dimensionality reduction matrix from\nM as described in Section 3."}, {"heading": "7. Transfer Learning Experiments", "text": "We conduct transfer learning experiments for two different classification problems. The first is to classify images according to the category of the object found in the images, a thirty class problem, with datasets from three domains: Amazon product images, images taken with a high-resolution DSLR camera, and images taken with a low-resolution webcam. Examples of the back packs category for these three domains is shown in Figure 3. This dataset was first used by Saenko, et al., and we use the same preprocessing techniques as described in (Saenko et al., 2010) to featurize the images, which results in 800 features per image.\nIn the second problem, the two different domains consist of the grayscale digit images in the MNIST and USPS datasets. The image features are the raw pixel values, and the only preprocessing we use is to resize the MNIST images to 16 x 16 pixels to match the USPS images. We show examples of images from each domain in Figure 4.\nWe compare four dimensionality reduction techniques by their performance using k = 3 nearest-neighbor classification. The first is transfer LDG where we choose \u03b1 from [0, .1, .3, .5] by k-NN cross-validation at dimensionality equal to the number of classes plus five.\nWe compare to pooled PCA and pooled FDA dimensionality reduction. These approaches ignore the difference between the domains by pooling the training data from each domain and then performing standard PCA or FDA. Finally, we compare to linear ITML for transfer learning as described in (Saenko et al., 2010) with MRMR feature selection. We also show results for ITML with no dimensionality reduction.\nThe source domain training data consists of all the available data in that particular domain, and we standard normalize so that it has mean zero and standard deviation one. The target domain training data consists of exactly two examples per class and is standard normalized independently of the source domain training data. We remove any features that exhibit zero variance in the source or target domain.\nFigure 5 plots the accuracy averaged over ten random splits of the target domain test and training data. The results show that LDG is statistically the best or tied for the best at many dimensions in all experiments. Pooled PCA performs well in the datasets where Amazon images act as the source, but fails to perform as well as LDG on the other datasets. Pooled FDA performs poorly on all of the datasets.\nWe do not show results for the best dimensionality chosen by cross-validation, similar to those in Table 1, due to space constraints. We do note that for the dimensions we have plotted, ITML achieves its highest accuracy with no dimensionality reduction, but still does not match the best performance of LDG."}, {"heading": "8. Conclusions", "text": "We have presented LDG dimensionality reduction, a technique that maps the data to a space where classes are separated locally to each training point. LDG is solved via a simple maximal eigenvalue decomposition, and thus scales better than iterative methods and LFDA for large datasets. Furthermore, we have shown that LDG dimensionality reduction can be applied to transfer learning problems with good results."}], "references": [{"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In Proc. International Conference on Machine Learning,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R.A. Fisher"], "venue": "Annals of Human Genetics,", "citeRegEx": "Fisher,? \\Q1936\\E", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "Completely lazy learning", "author": ["E.K. Garcia", "S. Feldman", "M.R. Gupta", "S. Srivastava"], "venue": "IEEE Trans. Knowledge and Data Engineering,", "citeRegEx": "Garcia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Garcia et al\\.", "year": 2010}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S. Roweis"], "venue": "In Proc. Advances in Neural Information Processing Systems", "citeRegEx": "Globerson and Roweis,? \\Q2006\\E", "shortCiteRegEx": "Globerson and Roweis", "year": 2006}, {"title": "Neighbourhood components analysis", "author": ["A. Globerson", "S.T. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "In Proc. Advances in Neural Information Processing Systems", "citeRegEx": "Globerson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2005}, {"title": "Comparison of discriminative training methods for speaker verification", "author": ["C. Ma", "E. Chang"], "venue": "In Proc. IEEE ICASSP 2003,", "citeRegEx": "Ma and Chang,? \\Q2003\\E", "shortCiteRegEx": "Ma and Chang", "year": 2003}, {"title": "Feature selection based on mutual information criteria of maxdependency, max-relevance, and min-redundancy", "author": ["H. Peng", "F Long", "C. Ding"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In Proc. Computer Vision ECCV 2010,", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis", "author": ["M. Sugiyama"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Sugiyama,? \\Q2007\\E", "shortCiteRegEx": "Sugiyama", "year": 2007}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "However, to reduce the model bias of assuming one Gaussian per class, we model p(xi|j) as locally Gaussian (Garcia et al., 2010).", "startOffset": 107, "endOffset": 128}, {"referenceID": 1, "context": "Fisher discriminant analysis (FDA) (Fisher, 1936) is a supervised technique that chooses B to maximize the ratio of the between-class covariance S to the within-class covariance S.", "startOffset": 35, "endOffset": 49}, {"referenceID": 8, "context": "Local Fisher discriminant analysis (LFDA) (Sugiyama, 2007) alleviates the drawbacks of FDA.", "startOffset": 42, "endOffset": 58}, {"referenceID": 4, "context": "Neighbourhood components analysis (NCA) (Globerson et al., 2005) is a dimensionality reduction technique that is based on a smooth approximation to the leave-one-out k-NN error.", "startOffset": 40, "endOffset": 64}, {"referenceID": 0, "context": "The approaches given in (Globerson & Roweis, 2006; Davis et al., 2007; Weinberger & Saul, 2009) propose convex optimization problems for finding M .", "startOffset": 24, "endOffset": 95}, {"referenceID": 0, "context": "However, we can perform dimensionality reduction by rewriting the Mahalanobis metric as M = L\u039bL and using a feature selection method on the resulting zi = \u039b Lxi as proposed in (Globerson & Roweis, 2006; Davis et al., 2007).", "startOffset": 176, "endOffset": 222}, {"referenceID": 0, "context": "We perform experiments to compare LDG to several different dimensionality reduction methods: PCA, FDA, LFDA, NCA, and information theoretic metric learning (ITML) (Davis et al., 2007) with feature selection using the maximum-relevance, minimum redundancy criterion (MRMR) (Peng et al.", "startOffset": 163, "endOffset": 183}, {"referenceID": 6, "context": ", 2007) with feature selection using the maximum-relevance, minimum redundancy criterion (MRMR) (Peng et al., 2005).", "startOffset": 96, "endOffset": 115}, {"referenceID": 2, "context": "We choose the number of neighbors used to estimate the local Gaussians for the LDG dimensionality reduction by fivefold cross-validation using a local QDA classifier (Garcia et al., 2010) on the original data.", "startOffset": 166, "endOffset": 187}, {"referenceID": 7, "context": "Of the dimensionality reduction techniques described in Section 3, only ITML has been adapted to the transfer learning scenario (Saenko et al., 2010).", "startOffset": 128, "endOffset": 149}, {"referenceID": 7, "context": ", and we use the same preprocessing techniques as described in (Saenko et al., 2010) to featurize the images, which results in 800 features per image.", "startOffset": 63, "endOffset": 84}, {"referenceID": 7, "context": "Finally, we compare to linear ITML for transfer learning as described in (Saenko et al., 2010) with MRMR feature selection.", "startOffset": 73, "endOffset": 94}], "year": 2012, "abstractText": "We present local discriminative Gaussian (LDG) dimensionality reduction, a supervised dimensionality reduction technique for classification. The LDG objective function is an approximation to the leave-one-out training error of a local quadratic discriminant analysis classifier, and thus acts locally to each training point in order to find a mapping where similar data can be discriminated from dissimilar data. While other state-ofthe-art linear dimensionality reduction methods require gradient descent or iterative solution approaches, LDG is solved with a single eigen-decomposition. Thus, it scales better for datasets with a large number of feature dimensions or training examples. We also adapt LDG to the transfer learning setting, and show that it achieves good performance when the test data distribution differs from that of the training data.", "creator": "LaTeX with hyperref package"}}}