{"id": "1206.4663", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "The Convexity and Design of Composite Multiclass Losses", "abstract": "secondly consider composite loss likelihood between multiclass inputs comprising probability proper ( i. z., prior - uncertainty ) loss over probability distributions supporting any inverted link algorithm. we observe conditions for their ( strong ) convexity and eliminate both implications. we also evaluate how constant separation of shocks induced by using this same representation increases for adaptive design of families of losses with the same bayes risk.", "histories": [["v1", "Mon, 18 Jun 2012 15:30:52 GMT  (742kb)", "http://arxiv.org/abs/1206.4663v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["mark d reid", "robert c williamson", "peng sun"], "accepted": true, "id": "1206.4663"}, "pdf": {"name": "1206.4663.pdf", "metadata": {"source": "META", "title": "The Convexity and Design of Composite Multiclass Losses", "authors": ["Mark D. Reid", "Robert C. Williamson", "Peng Sun"], "emails": ["MARK.REID@ANU.EDU.AU", "BOB.WILLIAMSON@ANU.EDU.AU", "SUNP08@MAILS.TSINGHUA.EDU.CN"], "sections": [{"heading": "1. Introduction", "text": "We study multiclass proper composite losses which are the composition of a proper loss and and invertible link (both defined formally below). This representation makes the understanding of multiclass losses easier because crucially it seperates two distinct concerns: the statistical and the numerical (Vernet et al., 2011). The statistical properties are controlled by the proper loss. The link function is essentially just a parametrisation. Choice of a suitable link can help \u2014 for example, a nonconvex proper loss can be made convex (and thus more amenable to numerical optimisation) by choice of the appropriate link. In this paper we show how this is possible, when a composite loss is convex, and how to convexify an arbitrary proper multiclass loss. The results extend the results on binary composite losses due to Reid & Williamson (2010)."}, {"heading": "1.1. Previous Work", "text": "Proper losses are the natural losses to use for probability estimation. The key property of a proper loss (see \u00a72.1 below) is that its expected value is always minimised by the distribution defining the expectation. They have been stud-\nAppearing in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nied in detail when n = 2 (the \u201cbinary case\u201d) where there is an integral representation (Buja et al., 2005; Gneiting & Raftery, 2007; Reid & Williamson, 2011), and characterization (Reid & Williamson, 2010) when differentiable.\nThe theory of loss functions makes it clear how one ideally chooses a loss \u2014 one takes account of one\u2019s utility concerning various incorrect predictions (Kiefer, 1987), (Berger, 1985, Section 2.4). The practice rarely involves such a step. There is little guidance in the literature concerning how to choose a loss function; typically heuristic arguments are used for the choice \u2014 confer e.g. (Ighodaro et al., 1982; Nayak & Naik, 1989). Early approaches to multiclass losses used a simple reduction to binary (Dietterich & Bakiri, 1995). More recently, other approaches to the design of losses for multiclass prediction have received attention (Zhang, 2004; Hill & Doucet, 2007; Tewari & Bartlett, 2007; Liu, 2007; Zou et al., 2008; Zhang et al., 2009), although none of these papers developed the connection to proper losses, and most restrict consideration to margin losses (which imply certain symmetry conditions). Zou et al. (2005) proposed a multiclass generalisation of \u201cadmissible losses\u201d (their name for classification calibration) for multiclass margin classification. Liu (2007) considered several multiclass generalisations of hinge loss (suitable for multiclass SVMs) and showed some of them were and others not Fisher consistent. When they were not it was shown how the training algorithm could be modified to make the losses behave consistently. Multiclass losses have also been considered in the development of multiclass boosting (see e.g. Zhu et al., 2009; Mukherjee & Schapire, 2011; Wu & Lange, 2010)."}, {"heading": "1.2. Key Contribution and Significance", "text": "The key point of the paper is as follows. Multiclass losses are necessary in many problems. To date they have typically been constructed as margin losses via a convex function f applied to a generalised notion of \u201cmargin\u201d. This is unsatisfactory from a design perspective because it confounds two distinct issues: the decision theoretic notion\nof a loss (that captures what it is that is important to the end user (Berger, 1985, confer)) and issues associated with the ease of numerical optimisation. Furthermore, margin losses are not particularly well suited to the non-symmetric treatment of different classes, as is necessary in many applications. Fortunately there is a way of neatly separating these two concerns through the use of a composite loss (Vernet et al., 2011) where the statistical properties are controlled by the choice of proper loss, and the optimisation properties via the link. This leads to the natural question: suppose one fixes a proper loss (and hence the statistical properties), how should one choose a link to ensure convexity of the overall loss? The answer is not obvious and not trivial, and is the key technical contribution of the paper (Theorem 5). The result opens up the possibility of a more systematic approach to the design of multiclass losses which previously has been approached in a rather ad hoc manner."}, {"heading": "2. Formal Setup", "text": "Suppose X is some set and Y = {1, . . . ,n} = [n] is a set of labels. We suppose we are given data S = *xi,yi+i2[m] such that yi 2 Y is the label corresponding to xi 2 X . These data follow a joint distribution PX ,Y on X \u21e5 [n]. We denote by EX ,Y and EY |X respectively, the expectation and the conditional expectation with respect to PX ,Y . Given a new observation x we want to predict the probability pi :=P(Y = i|X = x) of x belonging to class i, for i2 [n]. Multiclass classification requires the learner to predict the most likely class of x; that is to find y\u0302 = argmaxi2[n] pi."}, {"heading": "2.1. Losses", "text": "A loss measures the quality of prediction. Let Dn := {(p1, . . . , pn) : \u00c2i2[n] pi = 1,and 0  pi  1, 8i 2 [n]} denote the n-simplex. For multiclass probability estimation, ` : Dn ! Rn+. For classification, the loss ` : [n] ! Rn+. The partial losses `i are the components of `(q) = (`1(q), . . . ,`n(q))0 and `i(q) is the loss incurred by predicting q 2 Dn when y = i. Throughout the paper, A0 denotes transpose of a matrix A, except when applied to a realvalued function where it denotes derivative. We denote the matrix multiplication of compatible matrices A and B by A \u00b7B, so the inner product of two vectors x,y 2 Rn is x0 \u00b7 y.\nThe conditional risk L : Dn\u21e5Dn ! R+ associated with a loss ` is the function\nL(p,q) = EY\u21e0p`Y(q) = p0 \u00b7 `(q) = \u00c2 i2[n] pi`i(q),\nwhere Y \u21e0 p means Y is drawn according to a multinomial distribution with parameter p 2 Dn. In a typical learning problem one will make an estimate q : X ! Dn. The full risk is L(q) = EX EY |X `Y(q(X)).\nMinimizing L(q) over q : X ! Dn is equivalent to minimizing L(p(x),q(x)) over q(x) 2 Dn for all x 2 X where p(x) = (p1(x), . . . , pn(x))0, and pi(x) = P(Y = i|X = x). Thus when there is no restriction on the hypothesis class, it suffices to only consider the conditional risk; confer (Reid & Williamson, 2011).\nIf one is interested in estimating probabilities (` : Dn!Rn+) it is natural to require the associated conditional risk is minimized when estimating the true underlying probability. Such a loss is called proper (formally: if L(p, p) L(p,q), 8p,q 2 Dn). It is strictly proper if the inequality is strict when p 6= q (so it is uniquely minimised by predicting the correct probability). The conditional Bayes risk\nL : Dn 3 p 7! inf q2Dn L(p,q).\nThis function is always concave (Gneiting & Raftery, 2007). If ` is proper, then L(p) = L(p, p) = p0 \u00b7 `(p). Strictly proper losses induce Fisher consistent estimators of probabilities: if ` is strictly proper, p = argminq L(p,q).\nThe losses above are defined on the simplex Dn since the argument (an estimator) represents a probability vector. However it is sometimes desirable to use another set V of predictions. One can consider losses ` : V !Rn+. Suppose there exists an invertible function y : Dn ! V . Then ` can be written as a composition of a loss l defined on the simplex with y 1. That is, `(v) = ly(v) := l (y 1(v)). Such a function ly is a composite loss. If l is proper, we say ` is a proper composite loss, with associated proper loss l and link y . Binary proper composite losses have been studied by (Reid & Williamson, 2010)."}, {"heading": "2.2. Matrix Differential Calculus", "text": "In order to differentiate the losses we project the n-simplex into a subset of Rn 1. Let n\u0303 := n 1. Let\nD\u0303n := {(p1, . . . , pn\u0303)0 : pi 0, 8i 2 [n\u0303], n\u0303\n\u00c2 i=1 pi  1}\ndenote the \u201cbottom\u201d of the n-simplex. We denote by\nPD : Dn 3 p = (p1, . . . , pn)0 7! p\u0303 = (p1, . . . , pn\u0303)0 2 D\u0303n,\nthe projection of the Dn, and\nP 1D : ( p\u03031, . . . , p\u0303n\u0303) 0 7! p = ( p\u03031, . . . , p\u0303n\u0303,1 n\u0303\n\u00c2 i=1 p\u0303i)0\nits inverse.\nWe use the following notation. The kth unit vector ek is the n vector with all components zero except the kth which is 1. The n-vector n := (1, . . . ,1)0. The derivative of a function f is denoted D f and its Hessian H f . The (relative) interior\nof the simplex is D\u030an := {(p1, . . . , pn) : \u00c2i2[n] pi = 1,and 0< pi < 1, 8i 2 [n]} and the boundary is \u2202Dn := Dn \\ D\u030an.\nIf A= [ai j] is an n\u21e5m matrix, vecA is the vector of columns of A stacked on top of each other. The Kronecker product of an m\u21e5n matrix A with a p\u21e5q matrix B is the mp\u21e5nq matrix\nA\u2326B :=\n0 B@ A1,1B \u00b7 \u00b7 \u00b7 A1,nB ... . . .\n... Am,1B \u00b7 \u00b7 \u00b7 Am,nB\n1\nCA .\nWe use the following properties of Kronecker products (see Chapter 2 of Magnus & Neudecker (1999)): (A\u2326B)(C\u2326D) = (AC\u2326BD) for all appropriately sized A,B,C,D, and I1\u2326A = A.\nIf f : Rn !Rm is differentiable at c then the partial derivative of fi with respect to the jth coordinate at c is denoted D j fi(c) The m\u21e5 n matrix of partial derivatives of f is the Jacobian of f and denoted\n(D f (c))i, j := D j fi(c) for i 2 [m], j 2 [n].\nIf F is a matrix valued function DF(X) := D f (vecX) where f (X) = vecF(X).\nWe will require the product rule for matrix valued functions (Vetter, 1970; Fackler, 2005): Suppose f : Rn ! Rm\u21e5p, g : Rn ! Rp\u21e5q so that ( f \u21e5g) : Rn ! Rm\u21e5q. Then\nD( f \u21e5g)(x) = (g(x)0\u2326Im) \u00b7D f (x)+(Iq\u2326 f (x)) \u00b7Dg(x). (1)\nThe Hessian at x 2 X \u2713 Rn of a real-valued function f : X ! R is the n\u21e5 n real, symmetric matrix of second derivatives at x\n(H f (x)) j,k := Dk, j f (x) = \u2202 2 f\n\u2202xk\u2202x j .\nNote that the derivative Dk, j is in row j, column k. It is easy to establish that the Jacobian of the transpose of the Jacobian of f is the Hessian of f . That is,\nH f (x) = D (D f (x))0\n(2)\n(Magnus & Neudecker, 1999). If f : X !Rm for X \u2713Rn is a vector valued function then the Hessian of f at x 2X is the mn\u21e5 n matrix that consists of the Hessians of the functions fi stacked vertically:\nH f (x) :=\n0 B@ H f1(x)\n... H fm(x)\n1\nCA .\nIf A and B are square matrices, A < B ifA B is positive semidefinite."}, {"heading": "3. Derivatives of Composite Losses", "text": "In order to establish the convexity and other properties of composite losses we start by proving some identities for their first and second derivatives.\nSuppose ` = l y 1 is composed of the proper loss l : Dn ! Rn+ and the inverse of the link y : Dn ! V . In order to simplify matters, derivatives for the function ` : V !Rn+ we will assume the set V is a flat, (n 1)-dimensional, convex subset of Rn+. We do so since if V were some arbitrary manifold the extra definitions required to make sense of convexity (e.g., in terms of geodesics) and derivatives on manifolds would obscure the thrust of the results below. Furthermore, little is lost either practically or theoretically by assuming a simple V . In practice, predictions are usually vectors in Rn+, and in theory one could always choose a parametrisation of V in terms of some simpler space U and redefine the link via composition with that parametrisation. Alternatively, since links must be invertible, a composite loss could be defined by a choice of loss and choice of inverse link y 1 : V ! Dn for a V assumed to be flat, etc.\nLet v 2 V fixed but arbitrary with corresponding p\u0303 = y\u0303 1(v) where y\u0303(p\u0303) := y( p\u03031, . . . , p\u0303n\u0303, pn) with pn := \u00c2n\u0303i=1 p\u0303i is the induced function from D\u0303n to V . By the chain rule and the inverse function theorem the derivatives for each of the partial losses `i satisfy\nD`i(v) = D \u21e5 li(y\u0303 1(v)) \u21e4 = Dli(p\u0303) \u00b7 [Dy\u0303( p\u0303)] 1 . (3)\nLet us write eni as the ith n-dimensional unit vector, e n i = (0, . . . ,0,1,0, . . . ,0)0 when i 2 [n], and define eni = 0n when i > n. We can now write Dli(p\u0303) in terms of the n\u21e5 n\u0303 matrix Dl ( p\u0303) using Dli(p\u0303) = (eni )0 \u00b7Dl (p\u0303). Now Dl ( p\u0303) = (Dl\u0303 ( p\u0303)0,Dln( p\u0303)0)0, where l\u0303 ( p\u0303) = (l1( p\u0303), . . . ,ln\u0303(p\u0303))0, and so\nDli(p\u0303) = (eni )0 \u00b7Dl (p\u0303) = (eni )0 \u00b7 \u2713\nDl\u0303 ( p\u0303) Dln( p\u0303)\n\u25c6 . (4)\nFurthermore, since l is proper, Lemma 5 by van Erven et al. (2011) means we can use the relationship between a proper loss and its projected Bayes risk L\u0303 := L P 1D to write\nDl\u0303 ( p\u0303) =W ( p\u0303) \u00b7HL\u0303( p\u0303) (5) Dln( p\u0303) = y( p\u0303)0 \u00b7Dl\u0303 ( p\u0303) (6)\nwhere W ( p\u0303) := In\u0303 n\u0303 \u00b7 p\u03030 and where y( p\u0303) := p\u0303/pn( p\u0303) and pn(p\u0303) := 1 \u00c2i2[n\u0303] pi.\nThus, combining (4\u20136) we have for all i 2 [n\u0303]\nDli(p\u0303) = (en\u0303i )0 \u00b7W (p\u0303) \u00b7HL\u0303( p\u0303), = ((en\u0303i ) 0 (en\u0303i ) 0 \u00b7 n\u0303 \u00b7 p\u03030) \u00b7HL\u0303( p\u0303)\n= (en\u0303i p\u0303) 0 \u00b7HL\u0303(p\u0303), (7)\nand\nDln(p\u0303) = y( p\u0303)0 \u00b7W (p\u0303) \u00b7HL\u0303(p\u0303)\n= 1\npn(p\u0303) p\u03030 \u00b7 (In\u0303 n\u0303 \u00b7 p\u03030) \u00b7HL\u0303( p\u0303)\n= 1\npn(p\u0303) (p\u03030 (1 pn( p\u0303)) p\u03030) \u00b7HL\u0303(p\u0303)\n= p\u03030 \u00b7HL\u0303( p\u0303). (8)\nFinally, noting that by definition en\u0303n = 0, (8) and (7) can be merged and combined with (3) to obtain the following proposition.\nProposition 1 For all i 2 [n], p\u0303 2 \u02da\u0303Dn, and v = y\u0303( p\u0303),\nD`i(v) = en\u0303i p\u0303 0 \u00b7k(p\u0303) (9)\nwhere k( p\u0303) := HL\u0303( p\u0303) [Dy\u0303( p\u0303)] 1.\nUsing the definition of the Hessian H`i = D[D`0i] and the product rule (1) gives\nD \u21e5 D`i(v)0 \u21e4 =Dv[ f (p\u0303) z }| {\u21e5 Dy\u0303( p\u0303)0 \u21e4 1 \u00b7HL\u0303(p\u0303)0 \u00b7 g(p\u0303) z }| { en\u0303i p\u0303 ]\n= \u21e3\nen\u0303i p\u0303 0 \u2326In\u0303 \u2318 \u00b7Dv[ f ( p\u0303)0]\n+ (I1\u2326 f (p\u0303)) \u00b7D en\u0303i y\u0303 1(v)\n= \u21e3\nen\u0303i p\u0303 0 \u2326In\u0303 \u2318 \u00b7Dv h HL\u0303( p\u0303) \u00b7 [Dy\u0303( p\u0303)] 1 i \u21e3\u21e5 Dy\u0303(p\u0303)0 \u21e4 1 HL\u0303( p\u0303)0 \u2318 \u00b7 [Dy\u0303(p\u0303)] 1\nwhere Dv is used to indicate that the derivative is with respect to v even when the terms inside the derivative are expressed using p\u0303. We have now established the following proposition.\nProposition 2 For all i 2 [n], p\u0303 2 \u02da\u0303Dn, and v = y\u0303( p\u0303),\nH`i(v) = \u21e3 en\u0303i p\u0303 0 \u2326In\u0303 \u2318 \u00b7D \u21e5 k y\u0303 1(v) \u21e4\n+ k( p\u0303)0 \u00b7 [Dy\u0303(p\u0303)] 1 .\nwhere k( p\u0303) := HL\u0303( p\u0303) \u00b7 [Dy\u0303( p\u0303)] 1.\nThe product k(p\u0303) := HL\u0303( p\u0303) [Dy\u0303( p\u0303)] 1 that appears in both propositions above can be interpreted as the curvature of the Bayes risk function L\u0303 relative to the rate of change of the link function y\u0303 . When the link function is the identity function y\u0303(p\u0303) = p\u0303 (i.e. when we are in the non-composite proper loss case) the expressions for the derivative and Hessian of each `i simplify to\nD`i(p\u0303) = (en\u0303i p\u0303) 0 \u00b7HL\u0303(p\u0303) (10) H`i(p\u0303) = \u21e3 en\u0303i p\u0303 0 \u2326In\u0303 \u2318 \u00b7D \u21e5 HL\u0303( p\u0303) \u21e4 HL\u0303(p\u0303)0 (11)\nThe form of k as the product of HL\u0303 and Dy\u0303 suggests another simplification. The canonical link function for a loss l with Bayes risk L is defined by the relationship\ny\u0303l (p\u0303) = DL\u0303(p\u0303)0\nfor all p\u0303. (We will show in section 5.1 that this is guaranteed to be a legitimate link.) We see the term k simplifies to k(p\u0303) = In\u0303 since Dy\u0303( p\u0303) = D(DL\u0303( p\u0303)0) = HL\u0303(p\u0303). For this choice of link function, the first and second derivatives become considerably simpler.\nProposition 3 If l : Dn ! Rn+ is a proper loss and y\u0303l is its associated canonical link then, for all i 2 [n], p\u0303 2 \u02da\u0303Dn, and v = y\u0303l (p\u0303), the composite loss `= l y\u0303 satisfies\nD`i(v) = (en\u0303i p\u0303) (12) H`i(v) = \u21e5 HL\u0303( p\u0303) \u21e4 1 . (13)\nThe simplified form of the Hessian above is established by noting that since k( p\u0303) = In\u0303 we have D[k(y\u0303 1(v))] = 0 for all v 2 V in Proposition 2.\nWhile the above propositions hold for any number of classes n, it is instructive (both here and later in the paper) to examine the binary case where n = 2. In this case, Proposition 1 and Proposition 2 reduce to\n`01(v) = (1 p)k( p\u0303) ; `02(v) = pk( p\u0303) (14) `001(v) = (1 p)k 0(p)+k(p)\ny\u0303 0(p) (15)\n`002(v) = pk 0(p)+k(p)\ny\u0303 0(p) (16)\nwhere k(p\u0303) = L\u0303 00(p) y\u0303 0(p\u0303) 0 and so d dvk(y\u0303 1(v)) = k 0(p) y\u0303 0(p) ."}, {"heading": "4. Convexity", "text": "Convexity of a loss is desirable for the ease of numerical optimisation of an empirical risk. We will now consider when multiclass proper losses are convex, and give a characterisation in terms of the corresponding Bayes risk which as we have seen is the natural way to parametrise a loss. The results in this section are the multiclass generalisation of the characterisation of convexity of binary proper losses (Reid & Williamson, 2010). In fact we obtain more general results even in the binary case because here we consider strongly convex losses. We will also show how any nonconvex proper loss can be made convex by suitable choice of a link function, specifically: the canonical link.\nWe define a loss ` : Dn !Rn+ to be convex if for all p 2 Dn, the map Dn 3 q 7! p0 \u00b7 `(q) is convex for all q. That is, a loss is convex if, under and distribution p over outcomes\ni 2 [n], the expected loss Ei\u21e0p[`i(q)] is convex. It is easy to see that ` is convex if and only if `i : Dn ! R+ is convex for all i 2 [n]. (The \u201cif\u201d part follows since a sum of convex functions is convex; the \u201conly if\u201d follows by considering p = ei, for i 2 [n].)\nDefinition 4 Suppose C \u21e2 Rn is convex. A function f : C ! R is strongly convex on C with modulus c 0 if for all x,x0 2C, 8a 2 (0,1),\nf (ax+(1 a)x0) a f (x)+(1 a) f (x0)\n1 2 ca(1 a)kx x0k2.\nWhen c = 0 in the above definition, f is convex. The function f is strongly convex on C with modulus c if and only if x 7! f (x) c2kxk\n2 is convex on C (Hiriart-Urruty & Lemare\u0301chal, 2001, page 73). Therefore, the maps v 7! `i(v) are c-strongly convex if and only if H`i(v)< cIn\u0303. By applying Proposition 2 we obtain the following characterisation of the c-strong convexity of the loss `.\nTheorem 5 A proper composite loss ` = l y 1 is strongly convex with modulus c 2 [0,1] if and only if for all p\u0303 2 \u02da\u0303Dn and for all i 2 [n]\nen\u0303i p\u0303 \u2326In\u0303 \u00b7D k y\u0303 1(v) 4 k( p\u0303)0 \u00b7 [Dy\u0303(p\u0303)] 1 cIn\u0303.\n(17)\nWe now consider the implications of Theorem 5 in two special cases: in the multiclass case with canonical link, and in the binary case with the identity link."}, {"heading": "4.1. Implications for Canonical Links", "text": "Recall that the canonical link y\u0303` is chosen so that y\u0303( p\u0303) = DL\u0303( p\u0303)0. This simplifies k( p\u0303) to the identity matrix In\u0303 so Dk(p\u0303) = 0. In this case, equation (17) reduces to the following corollary.\nCorollary 6 If ` = l y 1 is defined so that y\u0303 = DL\u03030 then each map v 7! `i(v) is c-strongly convex if and only if\u21e5 HL\u0303( p\u0303) \u21e4 1 < cIn\u0303, or equivalently HL\u0303(p\u0303)4 1c In\u0303.\nAn immediate consequence of this result is obtained by observing the definiteness constraint is always met when c = 0 since L\u0303 is always a concave function. Thus, using a canonical link guarantees a composite loss is convex.\nThere is a analogous upper definiteness condition to strong convexity that has implications for optimisation rates. In (Boyd & Vandenberghe, 2004, \u00a79.1.2) it is shown that if a twice differentiable function f : X ! R satisfies\nMI < H f (x)< mI\nfor all x 2 X \u21e2 Rn then the value Mm is an upper bound on the condition number of H f , that is, the ratio of maximum to minimum eigenvalue of H f . This value measures the eccentricity of the sublevel sets of f and controls the rate at which optima of f are approached.\nApplying this result to the Hessian of a composite loss ` with a canonical link shows that the condition number bound is controlled by the Hessian of the Bayes risk of `. Specifically, if the condition number is to be no more than M/m then 1M < HL\u0303( p\u0303) < 1m for all p\u0303. In the case that M = m and the condition number is 1, the only Hessian that suffices is HL\u0303( p\u0303) = In\u0303 which is easily shown to be the Bayes risk surface for square loss. Thus, square loss is the only canonical composite loss for which a condition number of 1 is possible."}, {"heading": "4.2. Implications for Binary Losses", "text": "In the binary case, when n = 2, (15) and (16) and the positivity of y\u0303 0 simplify (17) to two conditions:\n(1 p)k 0(p)  k(p) cy\u0303 0(p) pk 0(p)  k(p) cy\u0303 0(p) , 8p 2 (0,1).\nFurther assuming that y\u0303 is the identity link (y\u0303(v) = v)and letting w(p) := L\u0303(p) gives\nw0(p)  11 p (w(p) c)) w0(p) 1p (w(p) c)\n) , 8p 2 (0,1)\n, 1 p  w0(p) w(p) c  1 1 p , 8p 2 (0,1). (18)\nThe last equivalence is achieved by dividing through by w(p) c which must necessarily be positive since if it were not the final pair of inequalities would imply 1p 1 1 p , a contradiction given that p 2 [0,1]. Note that (18) reduces to (Reid & Williamson, 2010, Corollary 26) for c = 0.\nObserve that if g(p) := log(w(p) c) then g0(p) = w 0(p) w(p) c is the middle term in (18). This allows a simplification of the inequality. Specifically, if we assume w( 12 ) = 1 then\n1 p  g0(p)  1 1 p , 8p 2 (0,1)\n) Z q 1 2 1 p d p Q Z q 1 2 g0(p)d p Q Z q 1 2 1 1 p d p, 8q 2 (0,1) , log(q) log(2) Q g(q) log(1 c) (19) Q log(2) log(1 q), 8q 2 (0,1)\n, 1 2q Q eg(q) log(1 c) Q 1 2(1 q) , 8q 2 (0,1)\nwhich gives the following proposition purely in terms of w(p), rather than w(p) and its derivative.\nProposition 7 Let w(p) = HL\u0303(p) = L\u030300(p) and assume w(1/2) = 1. A proper binary loss ` : D2 ! R2+ is strongly convex with modulus c 2 [0,1] only if\n1 2p Q w(p) c 1 c Q 1 2(1 p) , 8p 2 (0,1), (20)\nwhere Q denotes  for p 12 and denotes for p 12 .\nWhen c= 0 (corresponding to ` being convex) this is equivalent to an expression by Reid & Williamson (2010, Equation 31). Equation 20 is illustrated in Figure 1.\nThe above proposition only gives a necessary condition for strong convexity. (In addition to w belonging to the specified region, w0(p) also needs to be suitably controlled). A sufficient condition is useful for designing strongly convex proper losses. Observe that if\nw(p) = exp \u2713Z p\n1/2 u(t)dt +K\n\u25c6 +C\nwhere u : [0,1]!R and K,c 2R, then \u2202\u2202 p log(w(p) c) = u(p). We require w(1/2) = 1 thus exp \u21e3R 1/2 1/2 u(t)dt +K \u2318 + c = 1, so eK = 1 c and\nw(p) = (1 c)exp \u2713Z p\n1/2 u(t)dt\n\u25c6 + c (21)\nsatisfies (18) if\n1 p  u(p) 1 1 p , 8p 2 (0,1), (22)\nand hence the loss with weight function w is strongly convex with modulus c. Thus by choosing u and constructing\nw via (21) one can design strongly convex proper binary losses.\nOne can ask whether equation (17) can be simplified in the n > 2 case by using a matrix version of the logarithmic derivative trick. Such a result does exist (Horn & Johnson, 1991, Section 6.6.19) but it requires that (HL\u0303(p\u0303)) 1 and D(HL\u0303(p\u0303)) commute for all p\u0303 2 D\u0303n, which is not generally the case."}, {"heading": "5. Designing Losses", "text": "The theory developed above suggests that each choice of proper loss l and link function y results in an overall loss function with properties (e.g., convexity) that depend entirely on their relationship to each other. Given these two \u201cknobs\u201d for parameterising a loss function, we can begin to ask what kind of practical trade-offs are involved when selecting a composite loss as a surrogate loss for a particular problem.\nWe now propose a simple scheme for constructing families of losses with the same Bayes risk. This is achieved by fixing a choice of proper loss l and creating a parameterised family (described below) of link functions ya for parameters a 2 A. Since the Bayes risk is entirely determined by l any composite loss l y 1a for a 2 A will have Bayes risk L(p) = p0l (p). Thus, we are able to examine the effect different choices of composite loss can have on a problem without changing the essential underlying problem.1 Through some simple experiments we validate that, at least in the context of boosting, the choice of link can have a significant affect on the convergence and robustness of learning."}, {"heading": "5.1. Parameterised Links", "text": "In order to construct a parametric family of links we first choose some set of inverse link functions B = {y 11 , . . . ,y 1 B } with a common domain, that is, y 1 b : V ! Dn for a common n and V . This collection will be called the basis set of link functions. We then take the convex hull of B to form a set of inverse link functions Y 1 = conv(B). Each y 1 2 Y 1 is then identified with the unique a 2 A = DB such that \u00c2Bb=1aby 1b = y\n1. For this construction to be valid, it it necessary to show that every such y 1 2Y 1 is indeed an inverse link function, that is, it is invertible.\nThe following proposition shows that it suffices to assume that all of the basis functions are strictly monotonic. A\n1Of course, this argument only holds in a point-wise analysis. That is, where choices for estimates p(x) can be made independently. Once a restricted hypothesis class for the functions p is introduced the choice of link can affect the minimal achievable risk. Understanding this interaction is left as future work.\nfunction f : V ! Rn is monotone if for all distinct u,v 2V ( f (u) f (v))0(u v) 0. Strict monotonicity holds when the inequality is strict.\nProposition 8 Every function y 1 in the set Y 1 = conv(B) is invertible whenever each basis function in B is strictly monotone.\nThis result is a consequence of: 1) strict monotonicity being preserved under convex combination; and 2) strict monotonicity implies invertibility. The first claim is established by considering strictly monotonic f and g and some a 2 [0,1] and noting that if h = a f + (1 a)g then (h(u) h(v))0(u v) = a( f (u) f (v))0(u v)+(1 a)(g(u) g(v))0(u v) > 0. A strictly monotone function f that is not invertible is impossible since if we have ( f (u) f (v))0(u v) > 0 for all u,v then a u 6= v s.t. f (u) = f (v) would lead to a contradiction.\nStrictly monotone basis functions are easily obtained via canonical links for strictly proper losses. By definition, a canonical link satisfies y\u0303 = DL\u0303 for some Bayes risk function. Strict properness guarantees L\u0303 is strictly concave (Vernet et al., 2011) and Kachurovskii\u2019s theorem (Showalter, 1997) states that the derivative of a function is (strictly) monotonic if and only if the function is (strictly) convex. Since ( f ( f 1(u)) f ( f 1(v)))0( f 1(u) f 1(v)) = (u v)0( f 1(u) f 1(v)) we see that strictly monotone functions have strictly monotone inverses and we have established the following proposition.\nProposition 9 If l is a strictly proper loss then its canonical link y\u0303l = DL\u0303 has a strictly monotone inverse.\nThis result means that a set of basis links can be defined via a choice of strictly concave Bayes risk functions. As an example, the class of Fisher-consistent margin losses proposed by Zou et al. (2008) provides a flexible starting point for designing sets of link functions as described above. They give explicit formulae for the inverse link for a composite loss defined by a choice of convex function f : R! R. Specifically, if the loss for predicting v 2 V = {v 2 Rn : \u00c2i vi = 0} is given by `(v) = f(v j) then its inverse link is y 1f (v) = 1 Zf (v) [f 0(vi)] 1 n i=1 where Zf (v) normalises the vector to lie in Dn. Each choice of strictly convex f gives a valid inverse link which can be used as a basis function."}, {"heading": "5.2. Experiments", "text": "In order to test the impact the choice of link has on the convergence rate we ran a simple experiment using a basic multiclass boosting algorithm, much like the LKTreeBoost method (Friedman, 2001) for trees with two terminal nodes. In this experiment l was fixed to be the log\nloss (i.e., li(p) = log pi), and two basis links y 1exp and y 1sq correspond to choosing in the preceeding subsection the convex functions f(t)= e t and f(t)= (1 t)2, respectively. Inverse link functions y 1a = ay 1exp +(1 a)y 1sq were chosen for a 2 {0,0.25,0.5,0.75,1} to construct composite losses `a = l y 1a . For each loss, boosting was performed on data generated by i.i.d. sampling from three 2-dimensional Gaussians at (0,0), (2,2), and (-2,2) with identity covariance. 4,800 training and 1,200 test samples were used with equal class proportions in both sets. The results shown in Figure 2 clearly indicate the importance of careful link selection."}, {"heading": "6. Conclusion", "text": "Composite multiclass losses are a natural family of losses for multiclass probability estimation and classification which provide a seperation of concerns between the statisitical performance (l ) and the parametrisation (y). We have shown that the requirement that the loss decompose into a proper loss and an inverse link gives enough structure to obtain simple expressions for the gradient and derivative of these losses, especially in the binary case or under the additional assumption that the link be canonical for the loss. We used these results to provide sufficient conditions for the convexity and condition numbers for composite losses and a general scheme for designing families of multiclass losses with the same Bayes risk. Preliminary experiments show that there are trade-offs inherent when designing multiclass losses that require further investigation."}, {"heading": "Acknowledgements", "text": "This work was supported by the Australian Research Council (ARC). NICTA is funded by the Australian Government\nas represented by the Department of Broadband, Communications and the Digital Economy and the ARC through the ICT Centre of Excellence program. Peng Sun was a visitor at ANU and NICTA while working on this paper."}], "references": [{"title": "Statistical Decision Theory and Bayesian Analysis", "author": ["Berger", "James O"], "venue": null, "citeRegEx": "Berger and O.,? \\Q1985\\E", "shortCiteRegEx": "Berger and O.", "year": 1985}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe", "year": 2004}, {"title": "Loss functions for binary class probability estimation and classification: Structure and applications", "author": ["Buja", "Andreas", "Stuetzle", "Werner", "Shen", "Yi"], "venue": "Technical report,", "citeRegEx": "Buja et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Buja et al\\.", "year": 2005}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["Dietterich", "Thomas G", "Bakiri", "Ghulum"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1995}, {"title": "Notes on matrix calculus", "author": ["Fackler", "Paul K"], "venue": "North Carolina State University,", "citeRegEx": "Fackler and K.,? \\Q2005\\E", "shortCiteRegEx": "Fackler and K.", "year": 2005}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics, pp. 1189\u20131232,", "citeRegEx": "Friedman,? \\Q2001\\E", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["Gneiting", "Tilmann", "Raftery", "Adrian E"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gneiting et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gneiting et al\\.", "year": 2007}, {"title": "A framework for kernel-based multi-category classification", "author": ["Hill", "Simon I", "Doucet", "Arnaud"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hill et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2007}, {"title": "Fundamentals of Convex Analysis", "author": ["Hiriart-Urruty", "Jean-Baptiste", "Lemar\u00e9chal", "Claude"], "venue": null, "citeRegEx": "Hiriart.Urruty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hiriart.Urruty et al\\.", "year": 2001}, {"title": "Topics in Matrix Analysis", "author": ["Horn", "Roger A", "Johnson", "Charles A"], "venue": null, "citeRegEx": "Horn et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Horn et al\\.", "year": 1991}, {"title": "Admissibility and complete class results for the multinomial estimation problem with entropy and squared error loss", "author": ["Ighodaro", "Ayodele", "Santner", "Thomas", "Brown", "Lawrence"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Ighodaro et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Ighodaro et al\\.", "year": 1982}, {"title": "Introduction to Statistical Inference", "author": ["Kiefer", "Jack Carl"], "venue": "SpringerVerlag, New York,", "citeRegEx": "Kiefer and Carl.,? \\Q1987\\E", "shortCiteRegEx": "Kiefer and Carl.", "year": 1987}, {"title": "Fisher consistency of multicategory support vector machines", "author": ["Liu", "Yufeng"], "venue": "In Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Liu and Yufeng.,? \\Q2007\\E", "shortCiteRegEx": "Liu and Yufeng.", "year": 2007}, {"title": "Matrix Differential Calculus with Applications in Statistics and Econometrics (revised edition)", "author": ["Magnus", "Jan R", "Neudecker", "Heinz"], "venue": null, "citeRegEx": "Magnus et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Magnus et al\\.", "year": 1999}, {"title": "Estimating multinomial cell probabilities under quadratic loss", "author": ["Nayak", "Tapan K", "Naik", "Dayanand N"], "venue": "Journal of the Royal Statistical Society. Series D (The Statistician),", "citeRegEx": "Nayak et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Nayak et al\\.", "year": 1989}, {"title": "Composite binary losses", "author": ["Reid", "Mark D", "Williamson", "Robert C"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reid et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reid et al\\.", "year": 2010}, {"title": "Information, divergence and risk for binary experiments", "author": ["Reid", "Mark D", "Williamson", "Robert C"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reid et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reid et al\\.", "year": 2011}, {"title": "Monotone operators in Banach space and nonlinear partial differential equations", "author": ["Showalter", "Ralph Edwin"], "venue": "American Mathematical Society,", "citeRegEx": "Showalter and Edwin.,? \\Q1997\\E", "shortCiteRegEx": "Showalter and Edwin.", "year": 1997}, {"title": "On the consistency of multiclass classification methods", "author": ["Tewari", "Ambuj", "Bartlett", "Peter L"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tewari et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tewari et al\\.", "year": 2007}, {"title": "Mixability is Bayes risk curvature relative to log loss", "author": ["van Erven", "Tim", "Reid", "Mark D", "Williamson", "Robert C"], "venue": "In Proceedings of the 24th Annual Conference on Learning Theory,", "citeRegEx": "Erven et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2011}, {"title": "Composite multiclass losses", "author": ["Vernet", "Elodie", "Williamson", "Robert C", "Reid", "Mark D"], "venue": "In NIPS2011,", "citeRegEx": "Vernet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vernet et al\\.", "year": 2011}, {"title": "Derivative operations on matrices", "author": ["Vetter", "William J"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Vetter and J.,? \\Q1970\\E", "shortCiteRegEx": "Vetter and J.", "year": 1970}, {"title": "Multicategory vertex discriminant analysis for high-dimensional data", "author": ["Wu", "Tong Tong", "Lange", "Kenneth"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Statistical analysis of some multi-category large margin classification methods", "author": ["Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang and Tong.,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Tong.", "year": 2004}, {"title": "Coherence functions for multicategory margin-based classification methods", "author": ["Zhang", "Zhihua", "Jordan", "Michael I", "Li", "Wu-Jun", "Yeung", "DitYan"], "venue": "In Proceedings of the Twelfth Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Zhang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}, {"title": "The margin vector, admissible loss and multi-class margin-based classifiers", "author": ["Zou", "Hui", "Zhu", "Ji", "Hastie", "Trevor"], "venue": null, "citeRegEx": "Zou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2005}, {"title": "New multicategory boosting algorithms based on multicategory Fisher-consistent losses", "author": ["hastie/ Papers/margin.pdf. Zou", "Hui", "Zhu", "Ji", "Hastie", "Trevor"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Zou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 20, "context": "This representation makes the understanding of multiclass losses easier because crucially it seperates two distinct concerns: the statistical and the numerical (Vernet et al., 2011).", "startOffset": 160, "endOffset": 181}, {"referenceID": 20, "context": "This representation makes the understanding of multiclass losses easier because crucially it seperates two distinct concerns: the statistical and the numerical (Vernet et al., 2011). The statistical properties are controlled by the proper loss. The link function is essentially just a parametrisation. Choice of a suitable link can help \u2014 for example, a nonconvex proper loss can be made convex (and thus more amenable to numerical optimisation) by choice of the appropriate link. In this paper we show how this is possible, when a composite loss is convex, and how to convexify an arbitrary proper multiclass loss. The results extend the results on binary composite losses due to Reid & Williamson (2010).", "startOffset": 161, "endOffset": 706}, {"referenceID": 2, "context": "ied in detail when n = 2 (the \u201cbinary case\u201d) where there is an integral representation (Buja et al., 2005; Gneiting & Raftery, 2007; Reid & Williamson, 2011), and characterization (Reid & Williamson, 2010) when differentiable.", "startOffset": 87, "endOffset": 157}, {"referenceID": 10, "context": "(Ighodaro et al., 1982; Nayak & Naik, 1989).", "startOffset": 0, "endOffset": 43}, {"referenceID": 26, "context": "More recently, other approaches to the design of losses for multiclass prediction have received attention (Zhang, 2004; Hill & Doucet, 2007; Tewari & Bartlett, 2007; Liu, 2007; Zou et al., 2008; Zhang et al., 2009), although none of these papers developed the connection to proper losses, and most restrict consideration to margin losses (which imply certain symmetry conditions).", "startOffset": 106, "endOffset": 214}, {"referenceID": 24, "context": "More recently, other approaches to the design of losses for multiclass prediction have received attention (Zhang, 2004; Hill & Doucet, 2007; Tewari & Bartlett, 2007; Liu, 2007; Zou et al., 2008; Zhang et al., 2009), although none of these papers developed the connection to proper losses, and most restrict consideration to margin losses (which imply certain symmetry conditions).", "startOffset": 106, "endOffset": 214}, {"referenceID": 10, "context": "(Ighodaro et al., 1982; Nayak & Naik, 1989). Early approaches to multiclass losses used a simple reduction to binary (Dietterich & Bakiri, 1995). More recently, other approaches to the design of losses for multiclass prediction have received attention (Zhang, 2004; Hill & Doucet, 2007; Tewari & Bartlett, 2007; Liu, 2007; Zou et al., 2008; Zhang et al., 2009), although none of these papers developed the connection to proper losses, and most restrict consideration to margin losses (which imply certain symmetry conditions). Zou et al. (2005) proposed a multiclass generalisation of \u201cadmissible losses\u201d (their name for classification calibration) for multiclass margin classification.", "startOffset": 1, "endOffset": 545}, {"referenceID": 10, "context": "(Ighodaro et al., 1982; Nayak & Naik, 1989). Early approaches to multiclass losses used a simple reduction to binary (Dietterich & Bakiri, 1995). More recently, other approaches to the design of losses for multiclass prediction have received attention (Zhang, 2004; Hill & Doucet, 2007; Tewari & Bartlett, 2007; Liu, 2007; Zou et al., 2008; Zhang et al., 2009), although none of these papers developed the connection to proper losses, and most restrict consideration to margin losses (which imply certain symmetry conditions). Zou et al. (2005) proposed a multiclass generalisation of \u201cadmissible losses\u201d (their name for classification calibration) for multiclass margin classification. Liu (2007) considered several multiclass generalisations of hinge loss (suitable for multiclass SVMs) and showed some of them were and others not Fisher consistent.", "startOffset": 1, "endOffset": 698}, {"referenceID": 20, "context": "Fortunately there is a way of neatly separating these two concerns through the use of a composite loss (Vernet et al., 2011) where the statistical properties are controlled by the choice of proper loss, and the optimisation properties via the link.", "startOffset": 103, "endOffset": 124}, {"referenceID": 19, "context": "Furthermore, since l is proper, Lemma 5 by van Erven et al. (2011) means we can use the relationship between a proper loss and its projected Bayes risk L\u0303 := L P 1 D to write Dl\u0303 ( p\u0303) =W ( p\u0303) \u00b7HL\u0303( p\u0303) (5) Dln( p\u0303) = y( p\u0303) \u00b7Dl\u0303 ( p\u0303) (6)", "startOffset": 47, "endOffset": 67}, {"referenceID": 20, "context": "Strict properness guarantees L\u0303 is strictly concave (Vernet et al., 2011) and Kachurovskii\u2019s theorem (Showalter, 1997) states that the derivative of a function is (strictly) monotonic if and only if the function is (strictly) convex.", "startOffset": 52, "endOffset": 73}, {"referenceID": 25, "context": "As an example, the class of Fisher-consistent margin losses proposed by Zou et al. (2008) provides a flexible starting point for designing sets of link functions as described above.", "startOffset": 72, "endOffset": 90}, {"referenceID": 5, "context": "In order to test the impact the choice of link has on the convergence rate we ran a simple experiment using a basic multiclass boosting algorithm, much like the LKTreeBoost method (Friedman, 2001) for trees with two terminal nodes.", "startOffset": 180, "endOffset": 196}], "year": 2012, "abstractText": "We consider composite loss functions for multiclass prediction comprising a proper (i.e., Fisherconsistent) loss over probability distributions and an inverse link function. We establish conditions for their (strong) convexity and explore the implications. We also show how the separation of concerns afforded by using this composite representation allows for the design of families of losses with the same Bayes risk.", "creator": "LaTeX with hyperref package"}}}