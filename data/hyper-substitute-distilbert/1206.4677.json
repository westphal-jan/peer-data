{"id": "1206.4677", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Semi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching", "abstract": "in real - world intelligence problems, the class quality - the training dataset does not necessarily reflect that of the test values, which can generate significant estimation bias. if whole class ratio toward candidate test item is conserved, instance pre - weighting threshold resampling allows automated bias management. hopefully, learning comparing class ratio of the test data is helpful when actual labeled token be missing within the test meter. in this paper, we propose also measure expected discrimination ratio in the test bench by matching typical distributions of correlated student test input combinations. we demonstrate relative utility of highly standardized approach through experiments.", "histories": [["v1", "Mon, 18 Jun 2012 15:37:07 GMT  (352kb)", "http://arxiv.org/abs/1206.4677v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["marthinus christoffel du plessis", "masashi sugiyama"], "accepted": true, "id": "1206.4677"}, "pdf": {"name": "1206.4677.pdf", "metadata": {"source": "META", "title": "Semi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching", "authors": ["Marthinus Christoffel du Plessis", "Masashi Sugiyama"], "emails": ["CHRISTO@SG.CS.TITECH.AC.JP", "SUGI@CS.TITECH.AC.JP"], "sections": [{"heading": "1. Introduction", "text": "Most supervised learning algorithms assume that training and test data follow the same probability distribution (Vapnik, 1998; Hastie et al., 2001; Bishop, 2006). However, this de facto standard assumption is often violated in realworld problems, caused by intrinsic sample selection bias or inevitable non-stationarity (Heckman, 1979; Quin\u0303oneroCandela et al., 2009; Sugiyama & Kawanabe, 2012).\nIn classification scenarios, changes in class balance are often observed\u2014for example, the male-female ratio is almost fifty-fifty in the real-world (test set), whereas training samples collected in a research laboratory tends to be dominated by male data. Such a situation is called a class-prior change, and the bias caused by differing class balances can be systematically adjusted by instance re-weighting or resampling if the class balance in the test dataset is known (Elkan, 2001; Lin et al., 2002).\nHowever, the class ratio in the test dataset is often unknown\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nin practice. A possible approach to coping with this problem is to learn a classifier so that the performance for all possible class balances is improved, e.g., through maximization of the area under the ROC curve (Cortes & Mohri, 2004; Cle\u0301menc\u0327on et al., 2009). Another, possibly more direct approach is to estimate the class ratio in the test dataset and use the estimates for instance re-weighting or resampling. In this paper, we focus on the latter scenario under a semi-supervised learning setup (Chapelle et al., 2006), where no labeled data is available from the test domain.\nSaerens et al. (2001) is a seminal paper on this topic, which proposed to estimate the class ratio by the expectationmaximization (EM) algorithm (Dempster et al., 1977)\u2014 alternately updating the test class-prior and class-posterior probabilities from some initial estimates until convergence. This method has been successfully applied to various realworld problems such as word sense disambiguation (Chan & Ng, 2006) and remote sensing (Latinne et al., 2001).\nIn this paper, we first reformulate the above algorithm, and show that this actually corresponds to approximating the test input distribution by a linear combination of class-wise input distributions under the Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951). In this procedure, the class-wise input distributions are approximated via classposterior estimation, for example, by kernel logistic regression (Hastie et al., 2001) or its squared-loss variant (Sugiyama, 2010).\nThis new formulation motivates us to develop a new approach, since indirectly estimating the divergence by estimating the individual class-posterior distributions may not be the best scheme. Recently, KL divergence estimation based on direct density-ratio estimation has been shown to be promising (Nguyen et al., 2010; Sugiyama et al., 2008). Furthermore, a squared-loss variant of the KL divergence called the Pearson (PE) divergence (Pearson, 1900) can also be approximated in the same way, with an analytic solution that can be computed efficiently (Kanamori et al., 2009a). The PE divergence and the KL divergence both belong to the f -divergence class (Ali & Silvey, 1966; Csisza\u0301r,\n1967), which share similar properties. In this paper, with the aid of this density-ratio based PE divergence estimator, we propose a new semi-supervised method for estimating the class ratio in the test dataset. Through experiments, we demonstrate the usefulness of the proposed method."}, {"heading": "2. Problem Formulation and Existing Method", "text": "In this section, we formulate the problem of semisupervised class-prior estimation and review an existing method (Saerens et al., 2001)."}, {"heading": "2.1. Problem Formulation", "text": "Let x \u2208 Rd be the d-dimensional input data, y \u2208 {1, . . . , c} be the class label, and c be the number of classes. We consider class-prior change, i.e., the classprior probability for training data p(y) and that for test data p\u2032(y) are different. However, we assume that the classconditional density for training data p(x|y) and that for test data p\u2032(x|y) are the same:\np(x|y) = p\u2032(x|y). (1)\nNote that training and test joint densities p(x, y) and p\u2032(x, y) as well as training and test input densities p(x) and p\u2032(x) are generally different under this setup.\nThe goal of this paper is to estimate p\u2032(y) from labeled training samples {(xi, yi)}ni=1 drawn independently from p(x, y) and unlabeled test samples {x\u2032i} n\u2032\ni=1 drawn independently from p\u2032(x). Given test labels {y\u2032i} n\u2032 i=1, p \u2032(y) can be naively estimated by n\u2032y/n \u2032, where n\u2032y is the number of test samples in class y. Here, however, we would like to estimate p\u2032(y) without {y\u2032i} n\u2032 i=1."}, {"heading": "2.2. Existing Method", "text": "We give a brief overview of an existing method for semisupervised class-prior estimation (Saerens et al., 2001), which is based on the expectation-maximization (EM) algorithm (Dempster et al., 1977).\nIn the algorithm, test class-prior and class-posterior estimates p\u0302\u2032(y) and p\u0302\u2032(y|x) are iteratively updated as follows:\n1. Obtain an estimate of the training class-posterior probability, p\u0302(y|x), from training data {(xi, yi)}ni=1, for example, by kernel logistic regression (Hastie et al., 2001) or its squared-loss variant (Sugiyama, 2010).\n2. Obtain an estimate of the training class-prior probability, p\u0302(y), from the labeled training data {(xi, yi)}ni=1 as p\u0302(y) = ny/n, where ny is the number of training samples in class y. Set the initial estimate of the test class-posterior probability equal to it: p\u0302\u20320(y) = p\u0302(y).\n3. Repeat until convergence: t = 1, 2, . . .\n(a) Compute a new test class-posterior estimate p\u0302\u2032t(y|x) based on the current test class-prior estimate p\u0302\u2032t\u22121(y) as\np\u0302\u2032t(y|x) = p\u0302\u2032t\u22121(y)p\u0302(y|x)/p\u0302(y)\u2211c\ny\u2032=1 p\u0302 \u2032 t\u22121(y\n\u2032)p\u0302(y\u2032|x)/p\u0302(y\u2032) . (2)\n(b) Compute a new test class-prior estimate p\u0302\u2032t(y) based on the current test class-prior estimate p\u0302\u2032t(y|x) as\np\u0302\u2032t(y) = 1\nn\u2032 n\u2032\u2211 i=1 p\u0302\u2032t(y|x\u2032i). (3)\nThis procedure was shown to converge to a local optimal solution.\nNote that Eq.(2) comes from the Bayes formulae,\np(x|y) = p(y|x)p(x) p(y) and p\u2032(x|y) = p \u2032(y|x)p\u2032(x) p\u2032(y) ,\ncombined with Eq.(1):\np\u2032(y|x) \u221d p \u2032(y)\np(y) p(y|x).\nEq.(3) comes from empirical marginalization of\np\u2032(y) = \u222b p\u2032(y|x)p\u2032(x)dx."}, {"heading": "3. Reformulation of the EM Algorithm as Distribution Matching", "text": "In this section, we show that the above EM algorithm can be interpreted as matching the test input density to a linear combination of class-wise input distributions under the Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951).\nBased on the assumption that the class-conditional densities for training and test data are unchanged (see Eq.(1)), let us model the test input density p\u2032(x) by\nq\u2032(x) = c\u2211 y=1 \u03b8yp(x|y), (4)\nwhere \u03b8y is a coefficient corresponding to p\u2032(y):\nc\u2211 y=1 \u03b8y = 1. (5)\nWe match the model q\u2032(x) with the test input density p\u2032(x) under the KL divergence:\nKL(p\u2032\u2016q\u2032) := \u222b p\u2032(x) log p\u2032(x)\nq\u2032(x) dx\n= \u222b p\u2032(x) log p\u2032(x)dx\n\u2212 \u222b p\u2032(x) log ( c\u2211\ny=1\n\u03b8yp(x|y) ) dx. (6)\nIgnoring the first term (which is a constant) and approximating the expectation in the second term with its empirical average give the following optimization problem:\nmax {\u03b8y}cy=1\n1 n\u2032 n\u2032\u2211 i=1 log\n( c\u2211\ny=1\n\u03b8yp(x \u2032 i|y)\n) , (7)\nsubject to Eq.(5).\nSince the above maximization is a convex optimization problem, the Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient for optimality (Boyd & Vandenberghe, 2004). The KKT conditions for the above problem is given by Eq.(5) and\n1 n\u2032 n\u2032\u2211 i=1 p(x\u2032i|y)\u2211c y\u2032=1 \u03b8y\u2032p(x \u2032 i|y\u2032) = \u03bd, \u2200y = 1, . . . , c,\nwhere \u03bd is a Lagrange multiplier. From these equations, we can determine \u03bd as\n\u03bd = 1 \u00b7 \u03bd =\n( c\u2211\ny=1\n\u03b8y\n) \u00b7  1 n\u2032 n\u2032\u2211 i=1 p(x\u2032i|y)\u2211c y\u2032=1 \u03b8y\u2032p(x \u2032 i|y\u2032)  = 1\nn\u2032 n\u2032\u2211 i=1\n\u2211c y=1 \u03b8yp(x\n\u2032 i|y)\u2211c\ny\u2032=1 \u03b8y\u2032p(x \u2032 i|y\u2032)\n= 1.\nThen the solution {\u03b8y}cy=1 can be calculated by fixed-point iteration as follows (McLachlan & Krishnan, 1997):\n\u03b8y \u2190\u2212 \u03b8y  1 n\u2032 n\u2032\u2211 i=1 p(x\u2032i|y)\u2211c y=1 \u03b8yp(x \u2032 i|y)  . (8) Making the substitution p(x\u2032i|y) = p(y|x\u2032i)p(x\u2032i)/p(y), canceling p(x\u2032i) in the numerator and denominator, and replacing p(y|x) with p\u0302(y|x), we can show that the above updating formula is reduced to\n\u03b8y \u2190\u2212 1\nn\u2032 n\u2032\u2211 i=1 \u03b8yp\u0302(y|x\u2032i)/p\u0302(y)\u2211c y\u2032=1 \u03b8y\u2032 p\u0302(y \u2032|x\u2032i)/p\u0302(y\u2032) ,\nwhich is the same as Eq.(3) with Eq.(2) substituted.\nTherefore, the EM method is essentially equivalent to matching the training and test input distributions under the KL divergence, which uses the class-conditional density p(x|y) as a building block (see Eq.(8)). However, this fact is not apparent in the EM expression because of the cancellation of p(x\u2032i) in the numerator and denominator.\nThe convexity of Eq.(7) implies that there are no local minima. However, this was not recognized in Saerens et al. (2001) since the algorithm was derived via the incomplete data EM method."}, {"heading": "4. Class-Prior Estimation by Direct Divergence Minimization", "text": "The analysis in the previous section motivates us to explore a more direct way to learn coefficients {\u03b8y}cy=1. That is, given an estimator of a divergence from p\u2032 to q\u2032, coefficients {\u03b8y}cy=1 are learned so that the divergence estimator is minimized.\nIn this section, we first review a general framework of approximating the f -divergences (Ali & Silvey, 1966; Csisza\u0301r, 1967) via Legendre-Fenchel convex duality (Keziou, 2003; Nguyen et al., 2010). Then we review two specific methods of divergence estimation for the KL divergence and the Pearson (PE) divergence (Pearson, 1900). Finally, we propose to use the PE divergence estimator for determining the coefficients {\u03b8y}cy=1."}, {"heading": "4.1. Framework of f -Divergence Approximation", "text": "An f -divergence (Ali & Silvey, 1966; Csisza\u0301r, 1967) from p\u2032 to q\u2032 is a general divergence measure defined by a convex function f such that f(1) = 0 as\nDf (p \u2032\u2016q\u2032) := \u222b p\u2032(x)f ( q\u2032(x)\np\u2032(x)\n) dx.\nIt was shown that the f -divergence can be lower-bounded via Legendre-Fenchel convex duality (Rockafellar, 1970) as follows (Keziou, 2003; Nguyen et al., 2010):\nDf (p \u2032\u2016q\u2032) = max\nr\n[\u222b q\u2032(x)r(x)dx\n\u2212 \u222b p\u2032(x)f\u2217(r(x))dx ] , (9)\nwhere f\u2217 is the convex conjugate of f . The maximum is achieved if and only if r(x) = q\u2032(x)/p\u2032(x). Eq.(9) is a useful expression because the right-hand side only contains expectations of r and f\u2217(r(x)), which can be simply approximated by sample averages.\nBelow, we show specific methods of divergence approximation for the KL and PE divergences under model (4)\nand the following parametric expression of the density ratio r(x):\nr(x) = b\u2211 `=0 \u03b1`\u03d5`(x), (10)\nwhere {\u03b1`}b`=0 are parameters and {\u03d5`(x)}b`=0 are basis functions. In practice, we use a constant basis and Gaussian kernels centered at the training data points, i.e., for b = n and ` = 1, 2, . . . , n,\n\u03d50(x) = 1 and \u03d5`(x) = exp ( \u2212\u2016x\u2212 x`\u2016 2\n2\u03c32\n) .\nThis provides a non-parametric divergence estimator (Nguyen et al., 2010; Sugiyama et al., 2008; Kanamori et al., 2012)."}, {"heading": "4.2. KL-Divergence Approximation", "text": "With f(u) = \u2212 log u for u > 0 and +\u221e for u \u2264 0, the f -divergence is reduced to the KL divergence. For this f , the convex conjugate is given by f\u2217(v) = \u22121 \u2212 log(\u2212v) for v < 0 and +\u221e for v \u2265 0. Then, if \u2212\u03b1` is regarded as \u03b1`, an empirical approximation of Eq.(9) under (4) and (10) is given as follows (Nguyen et al., 2010):\nKL(p\u2032\u2016q\u2032) \u2248 max {\u03b1`}b`=0\n[ \u2212\nc\u2211 y=1 \u03b8y ny \u2211 i:yi=y b\u2211 `=0 \u03b1`\u03d5`(xi)\n+ 1\nn\u2032 n\u2032\u2211 i=1 log ( b\u2211 `=0 \u03b1`\u03d5`(x \u2032 i) ) + 1 ] ,\nsubject to \u03b10, \u03b11, . . . , \u03b1b \u2265 0. A similar approach, which directly estimates the inverted ratio p\u2032(x)/q\u2032(x) with the same model (10), is also known (Sugiyama et al., 2008):\nKL(p\u2032\u2016q\u2032) \u2248 max {\u03b1`}b`=0\n[ 1\nn\u2032 n\u2032\u2211 i=1 log ( b\u2211 `=0 \u03b1`\u03d5`(x \u2032 i) )] ,\nsubject to \u03b10, \u03b11, . . . , \u03b1b \u2265 0 and c\u2211\ny=1\n\u03b8y ny \u2211 i:yi=y b\u2211 `=0 \u03b1`\u03d5`(xi) = 1.\nThese are convex optimization problems, and thus global optimal solutions can be obtained by naive optimization. Tuning parameters possibly included in the basis function such as the kernel width can be systematically optimized by cross-validation (Sugiyama et al., 2008). The KL-divergence estimator obtained above was proved to possess superior convergence properties both in parametric and non-parametric setups (Sugiyama et al., 2008; Nguyen et al., 2010).\nHowever, computing the KL-divergence estimator is rather time-consuming because optimization of {\u03b1`}b`=0 needs to be carried out for each {\u03b8y}cy=1."}, {"heading": "4.3. PE-Divergence Approximation", "text": "As an alternative to the KL-divergence, let us consider the PE divergence defined by\nPE(p\u2032\u2016q\u2032) := 1 2\n\u222b ( q\u2032(x) p\u2032(x) \u2212 1 )2 p\u2032(x)dx, (11)\nwhich is a squared-loss variant of the KL divergence and is a f -divergence with f(u) = (t\u2212 1)2/2.\nFor this f , the convex conjugate is given by f\u2217(v) = v2/2+v. Then, an empirical approximation of Eq.(9) under (4) and (10) is given as follows (Kanamori et al., 2009a):\nPE(p\u2032\u2016q\u2032) \u2248 max \u03b1\n[ \u2212 1\n2 \u03b1>G\u0302\u03b1+\u03b1>H\u0302\u03b8 \u2212 1 2\n] ,\nwhere\n\u03b1 = [\u03b10 \u03b11 \u00b7 \u00b7 \u00b7 \u03b1b]> , G\u0302 = 1\nn\u2032 n\u2032\u2211 i=1 \u03d5(x\u2032i)\u03d5(x \u2032 i) >,\n\u03d5(x) = [\u03d50(x) \u03d51(x) \u00b7 \u00b7 \u00b7 \u03d5b(x)] , H\u0302 = [ h\u03021 \u00b7 \u00b7 \u00b7 h\u0302c ] ,\nh\u0302y = 1\nny \u2211 i:yi=y \u03d5(xi), \u03b8 = [\u03b81 \u03b82 \u00b7 \u00b7 \u00b7 \u03b8c]> .\nA regularized solution to the above maximization problem can be obtained analytically as\n\u03b1\u0302 = ( G\u0302+ \u03bbR )\u22121 H\u0302\u03b8, (12)\nwhere \u03bb is a positive constant andR is defined as\nR =\n[ 0 01\u00d7b\n0b\u00d71 Ib\u00d7b\n] .\nThe PE divergence estimator obtained above was proved to have superior convergence properties both in parametric and non-parametric setups (Kanamori et al., 2009a; 2012). Tuning parameters possibly included in the basis function such as the kernel width or the regularization parameter can be systematically optimized by cross-validation (Kanamori et al., 2009a; 2012)."}, {"heading": "4.4. Learning Class Ratios by PE Divergence Matching", "text": "As shown above, the KL and PE divergences can be systematically estimated without density estimation via Legendre-Fenchel convex duality. Among them, the PE divergence estimator, explicitly expressed as\nP\u0302E(\u03b8) := \u22121 2 \u03b8>H\u0302>\n( G\u0302+ \u03bbR )\u22121 G\u0302 ( G\u0302+ \u03bbR )\u22121 H\u0302\u03b8\n+ \u03b8>H\u0302> ( G\u0302+ \u03bbR )\u22121 H\u0302\u03b8 \u2212 1\n2 ,\nis more useful for our purpose of learning class ratios, because of the following reasons: The PE-divergence was shown to be more robust against outliers than the KLdivergence, based on power divergence analysis (Basu et al., 1998; Sugiyama et al., 2012). This is a useful property in practical data analysis suffering high noise and outliers. Furthermore, the above PE-divergence estimator was shown to possess the minimum condition number among a general class of estimators, meaning that it is the most stable estimator (Kanamori et al., 2009b).\nAnother, and practically more important advantage of the above PE divergence estimator is that it can be computed efficiently and analytically. This advantage is even more crucial in our case because we minimize the above PE divergence estimator with respect to \u03b8:\nmin \u03b8 P\u0302E(\u03b8)\nsubject to c\u2211\ny=1\n\u03b8y = 1 and \u03b81, . . . , \u03b8c \u2265 0.\nBecause P\u0302E(\u03b8) is given analytically as a function of \u03b8, we can easily obtain the minimizer \u03b8\u0302 by simple optimization strategies such as alternate gradient descent and projection or just a grid search, without re-computing the PE divergence estimator."}, {"heading": "5. Experiments", "text": "In this section, we report experimental results."}, {"heading": "5.1. Setup", "text": "The following five methods are compared:\n\u2022 EM-KLR: The method of Saerens et al. (2001) (see Section 2.2). The class-posterior probability of the training dataset is estimated using `2-penalized kernel logistic regression with Gaussian kernels. The L-BFGS quasi-Newton implementation included in the \u2018minFunc\u2019 package is used for logistic regression training (Schmidt, 2005).\n\u2022 KL-KDE: The KL divergence estimator based on kernel density estimation (KDE). The class-wise input densities are estimated by KDE with Gaussian kernels. The kernel widths are estimated using likelihood cross-validation (Silverman, 1986).\n\u2022 PE-KDE: The PE divergence estimator based on KDE. The class-wise input densities are estimated by KDE with Gaussian kernels. The kernel widths are estimated using least-squares cross-validation (Silverman, 1986).\nBelow, we compare accuracy of class-prior estimation and classification."}, {"heading": "5.2. Benchmark Datasets", "text": "Here, we use binary-classification benchmark datasets listed in Table 1. We select 10 samples from each of the two classes for the training dataset and 50 samples for the test dataset. The samples in the test set are selected with probability \u03b8\u2217 from the first class and (1\u2212 \u03b8\u2217) from the second class, where \u03b8\u2217 = 0.1, 0.2, 0.3, 0.4, 0.5.\nThe average squared error of the estimated class ratios are given in Figure 1. This shows that methods based on the KL and PE divergences overall outperform EM-KLR, implying that our reformulation of the EM algorithm as distribution matching (see Section 3) contributes to obtaining accurate class-ratio estimates. Among the KL-based methods, KL-KDE tends to perform better than KL-DR. This is because, in KL-KDE, we did not estimate the first term in Eq.(6), which is the negative entropy and is a constant. On the other hand, the negative entropy is also implicitly estimated in KL-DR, possibly incurring additional estimation error. Among the PE-based methods, PE-DR outperforms PE-KDE, showing that directly estimating density ratios without density estimation is more promising as a PE divergence estimator. Overall, PE-DR is shown to be the most accurate.\nNext, we compare classification accuracy when the learned class-prior probabilities are used as instance weights. Figure 2 shows misclassification rates for a regularized leastsquares classifier (Rifkin et al., 2003) with instance weighting. The results show that, as expected, a more accurate estimate of the class ratio tends to give a lower misclassification rate."}, {"heading": "5.3. Real-World Application", "text": "Finally, we demonstrate the usefulness of the proposed approach in a real-world problem of military vehicle classification from geophone recordings (Duarte & Hu, 2004). This is a three class problem: Two vehicle classes and a class of recorded noise. The features are 50-dimensional. In this vehicle classification task, class-prior change is in-\nevitable because the type of vehicles passing through differs depending on time (e.g., day and night).\nn samples are drawn from each of the labeled classes for the training set with the uniform class prior, whereas 100 samples are drawn with probabilities p = [0.6 0.1 0.3] from each of the classes for the test set. Due to the prohibitive computational cost, KL-DR was not included in this exper-\niment.\nIn Figure 3, we plot the `2-distance between the true and estimated class priors and the misclassification rate based on instance-weighted kernel logistic regression (Hastie et al., 2001) averaged over 1000 runs as functions of the number of training samples. As can be seen from the graphs, the performance of all methods improves as the number of training samples increases. Among the compared methods, PE-DR provides the most accurate estimates of the class prior and thus yields the lowest classification error."}, {"heading": "6. Conclusion", "text": "Class-prior change is a problem that is conceivable in many real-world datasets, and it can be systematically corrected for if the class-prior of the test dataset is known. In this paper, we discussed the problem of estimating the test class ratios under the semi-supervised learning setup.\nWe first showed that the EM-based estimator introduced in Saerens et al. (2001) can be regarded as indirectly matching the test input distribution by a linear combination of classwise input distributions. Based on this view, we proposed to use an explicit and possibly more accurate divergence estimator based on density-ratio estimation (Kanamori et al., 2009a) for learning test class-priors. The proposed method was shown to have various nice properties such as high robustness to noise and outliers, superior numerical stability, and excellent computational efficiency. Through experiments, we showed that the class ratios estimated by the proposed method are more accurate than competing methods, which can be translated into better classification accuracy."}, {"heading": "Acknowledgments", "text": "The authors thank the anonymous reviewers for their helpful comments. MCdP was supported by the MEXT scholarship, and MS was supported by AOARD and JST PRESTO."}], "references": [{"title": "A general class of coefficients of divergence of one distribution from another", "author": ["S.M. Ali", "S.D. Silvey"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Ali and Silvey,? \\Q1966\\E", "shortCiteRegEx": "Ali and Silvey", "year": 1966}, {"title": "Robust and efficient estimation by minimising a density power divergence", "author": ["A. Basu", "I.R. Harris", "N.L. Hjort", "M.C. Jones"], "venue": null, "citeRegEx": "Basu et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Basu et al\\.", "year": 1998}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe", "year": 2004}, {"title": "Estimating class priors in domain adaptation for word sense disambiguation", "author": ["Y.S. Chan", "H.T. Ng"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics,", "citeRegEx": "Chan and Ng,? \\Q2006\\E", "shortCiteRegEx": "Chan and Ng", "year": 2006}, {"title": "SemiSupervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "Zien", "A. (eds"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "AUC optimization and the two-sample problem", "author": ["S. Cl\u00e9men\u00e7on", "N. Vayatis", "M. Depecker"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Cl\u00e9men\u00e7on et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cl\u00e9men\u00e7on et al\\.", "year": 2009}, {"title": "AUC optimization vs. error rate minimization", "author": ["C. Cortes", "M. Mohri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Cortes and Mohri,? \\Q2004\\E", "shortCiteRegEx": "Cortes and Mohri", "year": 2004}, {"title": "Information-type measures of difference of probability distributions and indirect observation", "author": ["I. Csisz\u00e1r"], "venue": "Studia Scientiarum Mathematicarum Hungarica,", "citeRegEx": "Csisz\u00e1r,? \\Q1967\\E", "shortCiteRegEx": "Csisz\u00e1r", "year": 1967}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Vehicle classification in distributed sensor networks", "author": ["M.F. Duarte", "Y.H. Hu"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "Duarte and Hu,? \\Q2004\\E", "shortCiteRegEx": "Duarte and Hu", "year": 2004}, {"title": "The foundations of cost-sensitive learning", "author": ["C. Elkan"], "venue": "In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Elkan,? \\Q2001\\E", "shortCiteRegEx": "Elkan", "year": 2001}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Sample selection bias as a specification", "author": ["J.J. Heckman"], "venue": "error. Econometrica,", "citeRegEx": "Heckman,? \\Q1979\\E", "shortCiteRegEx": "Heckman", "year": 1979}, {"title": "A least-squares approach to direct importance estimation", "author": ["T. Kanamori", "S. Hido", "M. Sugiyama"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kanamori et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2009}, {"title": "Condition number analysis of kernel-based density ratio estimation", "author": ["T. Kanamori", "T. Suzuki", "M. Sugiyama"], "venue": "Technical report,", "citeRegEx": "Kanamori et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2009}, {"title": "Statistical analysis of kernel-based least-squares density-ratio estimation", "author": ["T. Kanamori", "T. Suzuki", "M. Sugiyama"], "venue": "Machine Learning,", "citeRegEx": "Kanamori et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2012}, {"title": "Dual representation of \u03c6-divergences and applications", "author": ["A. Keziou"], "venue": "Comptes Rendus Mathe\u0301matique,", "citeRegEx": "Keziou,? \\Q2003\\E", "shortCiteRegEx": "Keziou", "year": 2003}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Kullback and Leibler,? \\Q1951\\E", "shortCiteRegEx": "Kullback and Leibler", "year": 1951}, {"title": "Support vector machines for classification in nonstandard situations", "author": ["Y. Lin", "Y. Lee", "G. Wahba"], "venue": "Machine Learning,", "citeRegEx": "Lin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2002}, {"title": "The EM algorithm and extensions. Wiley series in probability and statistics: Applied probability and statistics", "author": ["G.J. McLachlan", "T. Krishnan"], "venue": null, "citeRegEx": "McLachlan and Krishnan,? \\Q1997\\E", "shortCiteRegEx": "McLachlan and Krishnan", "year": 1997}, {"title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "author": ["X. Nguyen", "M.J. Wainwright", "M.I. Jordan"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling", "author": ["K. Pearson"], "venue": null, "citeRegEx": "Pearson,? \\Q1900\\E", "shortCiteRegEx": "Pearson", "year": 1900}, {"title": "Dataset Shift in Machine Learning", "author": ["J. Qui\u00f1onero-Candela", "M. Sugiyama", "A. Schwaighofer", "Lawrence", "N. (eds"], "venue": null, "citeRegEx": "Qui\u00f1onero.Candela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Qui\u00f1onero.Candela et al\\.", "year": 2009}, {"title": "Regularized leastsquares classification. Advances in Learning Theory: Methods, Model and Applications", "author": ["R. Rifkin", "G. Yeo", "T. Poggio"], "venue": "NATO Science Series III: Computer and Systems Sciences,", "citeRegEx": "Rifkin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rifkin et al\\.", "year": 2003}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "Rockafellar,? \\Q1970\\E", "shortCiteRegEx": "Rockafellar", "year": 1970}, {"title": "Adjusting the outputs of a classifier to new a priori probabilities: A simple procedure", "author": ["M. Saerens", "M. Patrice", "C. Decaestecker"], "venue": "Neural Computation,", "citeRegEx": "Saerens et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Saerens et al\\.", "year": 2001}, {"title": "minFunc\u2014Unconstrained differentiable multivariate optimization in MATLAB", "author": ["M. Schmidt"], "venue": null, "citeRegEx": "Schmidt,? \\Q2005\\E", "shortCiteRegEx": "Schmidt", "year": 2005}, {"title": "Density Estimation: For Statistics and Data Analysis", "author": ["B.W. Silverman"], "venue": null, "citeRegEx": "Silverman,? \\Q1986\\E", "shortCiteRegEx": "Silverman", "year": 1986}, {"title": "Superfast-trainable multi-class probabilistic classifier by least-squares posterior fitting", "author": ["M. Sugiyama"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "Sugiyama,? \\Q2010\\E", "shortCiteRegEx": "Sugiyama", "year": 2010}, {"title": "Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adaptation", "author": ["M. Sugiyama", "M. Kawanabe"], "venue": null, "citeRegEx": "Sugiyama and Kawanabe,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama and Kawanabe", "year": 2012}, {"title": "Direct importance estimation for covariate shift adaptation", "author": ["M. Sugiyama", "T. Suzuki", "S. Nakajima", "H. Kashima", "P. von B\u00fcnau", "M. Kawanabe"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Sugiyama et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2008}, {"title": "Density ratio matching under the Bregman divergence: A unified framework of density ratio estimation", "author": ["M. Sugiyama", "T. Suzuki", "T. Kanamori"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Sugiyama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2012}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1998\\E", "shortCiteRegEx": "Vapnik", "year": 1998}], "referenceMentions": [{"referenceID": 33, "context": "Introduction Most supervised learning algorithms assume that training and test data follow the same probability distribution (Vapnik, 1998; Hastie et al., 2001; Bishop, 2006).", "startOffset": 125, "endOffset": 174}, {"referenceID": 12, "context": "Introduction Most supervised learning algorithms assume that training and test data follow the same probability distribution (Vapnik, 1998; Hastie et al., 2001; Bishop, 2006).", "startOffset": 125, "endOffset": 174}, {"referenceID": 2, "context": "Introduction Most supervised learning algorithms assume that training and test data follow the same probability distribution (Vapnik, 1998; Hastie et al., 2001; Bishop, 2006).", "startOffset": 125, "endOffset": 174}, {"referenceID": 13, "context": "However, this de facto standard assumption is often violated in realworld problems, caused by intrinsic sample selection bias or inevitable non-stationarity (Heckman, 1979; Qui\u00f1oneroCandela et al., 2009; Sugiyama & Kawanabe, 2012).", "startOffset": 157, "endOffset": 230}, {"referenceID": 11, "context": "Such a situation is called a class-prior change, and the bias caused by differing class balances can be systematically adjusted by instance re-weighting or resampling if the class balance in the test dataset is known (Elkan, 2001; Lin et al., 2002).", "startOffset": 217, "endOffset": 248}, {"referenceID": 19, "context": "Such a situation is called a class-prior change, and the bias caused by differing class balances can be systematically adjusted by instance re-weighting or resampling if the class balance in the test dataset is known (Elkan, 2001; Lin et al., 2002).", "startOffset": 217, "endOffset": 248}, {"referenceID": 6, "context": ", through maximization of the area under the ROC curve (Cortes & Mohri, 2004; Cl\u00e9men\u00e7on et al., 2009).", "startOffset": 55, "endOffset": 101}, {"referenceID": 5, "context": "In this paper, we focus on the latter scenario under a semi-supervised learning setup (Chapelle et al., 2006), where no labeled data is available from the test domain.", "startOffset": 86, "endOffset": 109}, {"referenceID": 9, "context": "(2001) is a seminal paper on this topic, which proposed to estimate the class ratio by the expectationmaximization (EM) algorithm (Dempster et al., 1977)\u2014 alternately updating the test class-prior and class-posterior probabilities from some initial estimates until convergence.", "startOffset": 130, "endOffset": 153}, {"referenceID": 12, "context": "In this procedure, the class-wise input distributions are approximated via classposterior estimation, for example, by kernel logistic regression (Hastie et al., 2001) or its squared-loss variant (Sugiyama, 2010).", "startOffset": 145, "endOffset": 166}, {"referenceID": 29, "context": ", 2001) or its squared-loss variant (Sugiyama, 2010).", "startOffset": 36, "endOffset": 52}, {"referenceID": 21, "context": "Recently, KL divergence estimation based on direct density-ratio estimation has been shown to be promising (Nguyen et al., 2010; Sugiyama et al., 2008).", "startOffset": 107, "endOffset": 151}, {"referenceID": 31, "context": "Recently, KL divergence estimation based on direct density-ratio estimation has been shown to be promising (Nguyen et al., 2010; Sugiyama et al., 2008).", "startOffset": 107, "endOffset": 151}, {"referenceID": 22, "context": "Furthermore, a squared-loss variant of the KL divergence called the Pearson (PE) divergence (Pearson, 1900) can also be approximated in the same way, with an analytic solution that can be computed efficiently (Kanamori et al.", "startOffset": 92, "endOffset": 107}, {"referenceID": 2, "context": ", 2001; Bishop, 2006). However, this de facto standard assumption is often violated in realworld problems, caused by intrinsic sample selection bias or inevitable non-stationarity (Heckman, 1979; Qui\u00f1oneroCandela et al., 2009; Sugiyama & Kawanabe, 2012). In classification scenarios, changes in class balance are often observed\u2014for example, the male-female ratio is almost fifty-fifty in the real-world (test set), whereas training samples collected in a research laboratory tends to be dominated by male data. Such a situation is called a class-prior change, and the bias caused by differing class balances can be systematically adjusted by instance re-weighting or resampling if the class balance in the test dataset is known (Elkan, 2001; Lin et al., 2002). However, the class ratio in the test dataset is often unknown Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s). in practice. A possible approach to coping with this problem is to learn a classifier so that the performance for all possible class balances is improved, e.g., through maximization of the area under the ROC curve (Cortes & Mohri, 2004; Cl\u00e9men\u00e7on et al., 2009). Another, possibly more direct approach is to estimate the class ratio in the test dataset and use the estimates for instance re-weighting or resampling. In this paper, we focus on the latter scenario under a semi-supervised learning setup (Chapelle et al., 2006), where no labeled data is available from the test domain. Saerens et al. (2001) is a seminal paper on this topic, which proposed to estimate the class ratio by the expectationmaximization (EM) algorithm (Dempster et al.", "startOffset": 8, "endOffset": 1585}, {"referenceID": 26, "context": "Problem Formulation and Existing Method In this section, we formulate the problem of semisupervised class-prior estimation and review an existing method (Saerens et al., 2001).", "startOffset": 153, "endOffset": 175}, {"referenceID": 26, "context": "Existing Method We give a brief overview of an existing method for semisupervised class-prior estimation (Saerens et al., 2001), which is based on the expectation-maximization (EM) algorithm (Dempster et al.", "startOffset": 105, "endOffset": 127}, {"referenceID": 9, "context": ", 2001), which is based on the expectation-maximization (EM) algorithm (Dempster et al., 1977).", "startOffset": 71, "endOffset": 94}, {"referenceID": 12, "context": "Obtain an estimate of the training class-posterior probability, p\u0302(y|x), from training data {(xi, yi)}ni=1, for example, by kernel logistic regression (Hastie et al., 2001) or its squared-loss variant (Sugiyama, 2010).", "startOffset": 151, "endOffset": 172}, {"referenceID": 29, "context": ", 2001) or its squared-loss variant (Sugiyama, 2010).", "startOffset": 36, "endOffset": 52}, {"referenceID": 26, "context": "However, this was not recognized in Saerens et al. (2001) since the algorithm was derived via the incomplete data EM method.", "startOffset": 36, "endOffset": 58}, {"referenceID": 8, "context": "In this section, we first review a general framework of approximating the f -divergences (Ali & Silvey, 1966; Csisz\u00e1r, 1967) via Legendre-Fenchel convex duality (Keziou, 2003; Nguyen et al.", "startOffset": 89, "endOffset": 124}, {"referenceID": 17, "context": "In this section, we first review a general framework of approximating the f -divergences (Ali & Silvey, 1966; Csisz\u00e1r, 1967) via Legendre-Fenchel convex duality (Keziou, 2003; Nguyen et al., 2010).", "startOffset": 161, "endOffset": 196}, {"referenceID": 21, "context": "In this section, we first review a general framework of approximating the f -divergences (Ali & Silvey, 1966; Csisz\u00e1r, 1967) via Legendre-Fenchel convex duality (Keziou, 2003; Nguyen et al., 2010).", "startOffset": 161, "endOffset": 196}, {"referenceID": 22, "context": "Then we review two specific methods of divergence estimation for the KL divergence and the Pearson (PE) divergence (Pearson, 1900).", "startOffset": 115, "endOffset": 130}, {"referenceID": 8, "context": "Framework of f -Divergence Approximation An f -divergence (Ali & Silvey, 1966; Csisz\u00e1r, 1967) from p\u2032 to q\u2032 is a general divergence measure defined by a convex function f such that f(1) = 0 as", "startOffset": 58, "endOffset": 93}, {"referenceID": 25, "context": "It was shown that the f -divergence can be lower-bounded via Legendre-Fenchel convex duality (Rockafellar, 1970) as follows (Keziou, 2003; Nguyen et al.", "startOffset": 93, "endOffset": 112}, {"referenceID": 17, "context": "It was shown that the f -divergence can be lower-bounded via Legendre-Fenchel convex duality (Rockafellar, 1970) as follows (Keziou, 2003; Nguyen et al., 2010):", "startOffset": 124, "endOffset": 159}, {"referenceID": 21, "context": "It was shown that the f -divergence can be lower-bounded via Legendre-Fenchel convex duality (Rockafellar, 1970) as follows (Keziou, 2003; Nguyen et al., 2010):", "startOffset": 124, "endOffset": 159}, {"referenceID": 21, "context": "This provides a non-parametric divergence estimator (Nguyen et al., 2010; Sugiyama et al., 2008; Kanamori et al., 2012).", "startOffset": 52, "endOffset": 119}, {"referenceID": 31, "context": "This provides a non-parametric divergence estimator (Nguyen et al., 2010; Sugiyama et al., 2008; Kanamori et al., 2012).", "startOffset": 52, "endOffset": 119}, {"referenceID": 16, "context": "This provides a non-parametric divergence estimator (Nguyen et al., 2010; Sugiyama et al., 2008; Kanamori et al., 2012).", "startOffset": 52, "endOffset": 119}, {"referenceID": 21, "context": "(9) under (4) and (10) is given as follows (Nguyen et al., 2010):", "startOffset": 43, "endOffset": 64}, {"referenceID": 31, "context": "A similar approach, which directly estimates the inverted ratio p\u2032(x)/q\u2032(x) with the same model (10), is also known (Sugiyama et al., 2008):", "startOffset": 116, "endOffset": 139}, {"referenceID": 31, "context": "Tuning parameters possibly included in the basis function such as the kernel width can be systematically optimized by cross-validation (Sugiyama et al., 2008).", "startOffset": 135, "endOffset": 158}, {"referenceID": 31, "context": "The KL-divergence estimator obtained above was proved to possess superior convergence properties both in parametric and non-parametric setups (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 142, "endOffset": 186}, {"referenceID": 21, "context": "The KL-divergence estimator obtained above was proved to possess superior convergence properties both in parametric and non-parametric setups (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 142, "endOffset": 186}, {"referenceID": 1, "context": "is more useful for our purpose of learning class ratios, because of the following reasons: The PE-divergence was shown to be more robust against outliers than the KLdivergence, based on power divergence analysis (Basu et al., 1998; Sugiyama et al., 2012).", "startOffset": 212, "endOffset": 254}, {"referenceID": 32, "context": "is more useful for our purpose of learning class ratios, because of the following reasons: The PE-divergence was shown to be more robust against outliers than the KLdivergence, based on power divergence analysis (Basu et al., 1998; Sugiyama et al., 2012).", "startOffset": 212, "endOffset": 254}, {"referenceID": 27, "context": "The L-BFGS quasi-Newton implementation included in the \u2018minFunc\u2019 package is used for logistic regression training (Schmidt, 2005).", "startOffset": 114, "endOffset": 129}, {"referenceID": 28, "context": "The kernel widths are estimated using likelihood cross-validation (Silverman, 1986).", "startOffset": 66, "endOffset": 83}, {"referenceID": 28, "context": "The kernel widths are estimated using least-squares cross-validation (Silverman, 1986).", "startOffset": 69, "endOffset": 86}, {"referenceID": 26, "context": "\u2022 EM-KLR: The method of Saerens et al. (2001) (see Section 2.", "startOffset": 24, "endOffset": 46}, {"referenceID": 27, "context": "For the optimization, the L-BFGS with projection implementation \u2018minFuncBC\u2019 is used (Schmidt, 2005).", "startOffset": 84, "endOffset": 99}, {"referenceID": 24, "context": "Figure 2 shows misclassification rates for a regularized leastsquares classifier (Rifkin et al., 2003) with instance weighting.", "startOffset": 81, "endOffset": 102}, {"referenceID": 12, "context": "In Figure 3, we plot the `2-distance between the true and estimated class priors and the misclassification rate based on instance-weighted kernel logistic regression (Hastie et al., 2001) averaged over 1000 runs as functions of the number of training samples.", "startOffset": 166, "endOffset": 187}, {"referenceID": 23, "context": "We first showed that the EM-based estimator introduced in Saerens et al. (2001) can be regarded as indirectly matching the test input distribution by a linear combination of classwise input distributions.", "startOffset": 58, "endOffset": 80}], "year": 2012, "abstractText": "In real-world classification problems, the class balance in the training dataset does not necessarily reflect that of the test dataset, which can cause significant estimation bias. If the class ratio of the test dataset is known, instance re-weighting or resampling allows systematical bias correction. However, learning the class ratio of the test dataset is challenging when no labeled data is available from the test domain. In this paper, we propose to estimate the class ratio in the test dataset by matching probability distributions of training and test input data. We demonstrate the utility of the proposed approach through experiments.", "creator": "LaTeX with hyperref package"}}}