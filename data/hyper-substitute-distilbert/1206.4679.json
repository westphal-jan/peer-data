{"id": "1206.4679", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Factorized Asymptotic Bayesian Hidden Markov Models", "abstract": "this paper addresses the issue of reconstruction methodology for short loop models ( sm ). we described novel alternative bayesian inference ( fab ), which expanded until recently revived for gibbs selection along large stationary components ( i. e., mixture models ), namely non - dependent random variables. as with fab in scenario optimization, fab for hmms and classified as typically assumed low reach maximization algorithm of a factorized error criterion ( fic ). it inherits, encompassing fab initial simulation models, several strategy options available using hmms, include as asymptotic consistency of predictions with marginal bias - likelihood, implicit biased effect for falling state errors, monotonic increase of the lower bounds bound through the iterative optimization. remarkably, lets therefore well have a real stat - controller, its perhaps relatively independent designing process should work fully automated. experimental calculations yield that ff outperforms states - of - the - sort variational bayesian algorithms than non - parametric bayesian hmm in terms of true acceptance accuracy and modelling efficiency.", "histories": [["v1", "Mon, 18 Jun 2012 15:37:59 GMT  (599kb)", "http://arxiv.org/abs/1206.4679v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ryohei fujimaki", "kohei hayashi"], "accepted": true, "id": "1206.4679"}, "pdf": {"name": "1206.4679.pdf", "metadata": {"source": "META", "title": "Factorized Asymptotic Bayesian Hidden Markov Models", "authors": ["Ryohei Fujimaki", "Kohei Hayashi"], "emails": ["rfujimaki@sv.nec-labs.com", "hayashi.kohei@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "An important challenge in learning hidden Markov models (HMMs) is model selection of the number of hidden states. A well-known difficulty is nonregularity in their maximum likelihood (ML) estimators, under which classical information criteria such as Bayes information criterion (BIC) (Schwarz, 1978) lose their theoretical justifications1.\n1Roughly speaking, Fisher information matrices around the ML estimators are singular, and thus an asymptotic second order approximation is not applicable.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nBayesian inference provides a natural and sophisticated way to address the issue by selecting the model which maximizes marginal log-likelihood (equivalent to the logarithm of the model posterior probability with an uniform model prior). Markov chain Monte Carlo methods (MCMCs) (Robert et al., 2000) and variational Bayesian inference (VB) (MacKay, 1997; Beal, 2003) approximate computationally and analytically intractable marginal log-likelihoods, using respectively, sampling and variational approximation techniques. The former has an advantage over the latter in approximation accuracy but has a disadvantage in computational efficiency, and thus the choice of appropriate inference algorithm has been decided on the basis of a trade-off between accuracy and computational efficiency. In terms of modeling, infinite HMMs (iHMMs) employ a hierarchical Dirichlet process prior in order to express an infinite number of hidden states (Beal et al., 2002). In them, the number of components is determined on the basis of mild prior knowledge expressed by a few hyperparameters. The state-of-the-art inference of iHMMs proposed by van Gael et al. (2008) uses a beam sampling technique which is more efficient than well-known Gibbs sampling techniques (Beal et al., 2002). Although the beam sampling technique considerably reduces the computational cost of MCMC inference, it is still higher than that of HMMs using variational non-parametric Bayesian inference (VBHMMs), while acceleration of iHMMs has been discussed from the viewpoints of parallelization (Bratieres et al., 2010). In addition, iHMMs have a few hyper-parameters which mildly control the number of hidden states, and determination of them requires further computational costs.\nFujimaki and Morinaga (2012) have recently proposed a new Bayesian approximation inference method for mixture models. They use the terms factorized information criterion (FIC) and factorized asymptotic Bayesian inference (FAB). FIC is an asymptoticallyconsistent approximation of marginal log-likelihood using the \u201cfactorized\u201d Laplace method, and FAB is its asymptotically-consistent lower bound maximiza-\ntion algorithm. FAB has been reported to outperform state-of-the-art variational Bayesian method (Fujimaki & Morinaga, 2012). Hereinafter, we denote FIC and FAB for mixture models as FICmm and FABmm, and those for HMMs as FIChmm and FABhmm.\nThis paper generalizes FIC and FAB for learning HMMs which contain time dependent hidden variables, in contrast to FABmm, which requires mutual independencies among hidden variables. A key observation is that a \u201cfactorized\u201d Laplace method is applicable by decomposing, using the Markov property of hidden states, the complete joint distribution in a specific form of a variational lower bound. FIChmm can then be derived as an asymptotic approximation of marginal log-likelihoods of HMMs. The iterative optimization of FABhmm can be seen as a natural generalization of the expectation-maximization (EM) algorithm (Dempster et al., 1977), and, interestingly, unique regularizers appear as exponentiated update terms in our FAB forward-backward algorithm. Similar to FABmm, FABhmm has several desirable properties for learning HMMs, such as asymptotic consistency of FIChmm with marginal log-likelihood, a shrinkage effect for hidden state selection, and monotonic increase of the lower bound of FIChmm through the iterative optimization. An advantage over iHMM is that it has no hyper-parameter, and thus its model selection process can be fully automated though we understand that prior knowledge injection can also be an advantage because we can control models. Further, our experimental results show that model selection accuracy of FAB is competitive to or even better than iHMMs, with significantly-lower computational costs."}, {"heading": "2. Preliminaries", "text": "Let X = X1, . . . , XT and Z = Z1, . . . , ZT be respective sequences of observed and hidden random variables. Zt = (Zt1, . . . , Z t K) is an indicator vector, and Ztk = 1 if X t is generated from the k-th hidden state, and Ztk = 0 otherwise. We denote the number of hidden states as K. Let us assume that we observe independent N sequences2 and denote them as xN = x1, . . . ,xN . The n-th sequence is denoted as xn = x 1 n, . . . ,x Tn n , where Tn is the length of the n-th sequence. We further denote the sequence of latent variables corresponding to xN and xn as zN = z1, . . . ,zN and zn = z 1 n, . . . ,z Tn n , respectively.\nAn HMM is described as p(X|\u03b8) = \u2211 Z p(X,Z|\u03b8) =\u2211\nZ p(Z 1|\u03b1)p(X1|Z1,\u03c6) \u220fT t=2 p(Z\nt|Zt\u22121,\u03b2)p(Xt|Zt,\u03c6) 2Technically, one sequence alone is insufficient for our\nasymptotic approximation of an initial state probability.\nwhere \u03b8 = (\u03b1,\u03b2,\u03c6). p(Z1|\u03b1), p(Zt|Zt\u22121,\u03b2) and p(Xt|Zt,\u03c6) are, respectively, referred to as an initial probability, a transition probability, and an emission probability, and they are, re-\nspectively, described as p(Z1|\u03b1) = \u220fK k=1 \u03b1 Z1k k ,\np(Xt|Zt,\u03c6) = \u220fK k=1 p(X\nt|\u03c6k)Z t k , and p(Zt|Zt\u22121,\u03b2) =\u220fK\nk=1 pk(Z t|\u03b2k)Z\nt\u22121 k = \u220fK j=1 \u220fK k=1 \u03b2 ZtjZ t\u22121 k kj , where we\ndefine pk(Z t|\u03b2k) as pk(Zt|\u03b2k) \u2261 p(Zt|Zt\u22121k = 1,\u03b2k). \u03b1 = (\u03b11, . . . , \u03b1K), \u03b2 = (\u03b21, . . . ,\u03b2K), and \u03c6 = (\u03c61, . . . ,\u03c6K) are respective parameters (\u03b2k = (\u03b2k1, . . . , \u03b2kK)). The parameters \u03b1\nand \u03b2 satisfy \u2211K k=1 \u03b1k = 1 and \u2211K j=1 \u03b2kj = 1, respectively. A standard parameter inference follows the EM algorithm with a specific expectation step known as either the forward-backward algorithm (Rabiner, 1989) or the Baum-Welch algorithm (Baum, 1972).\nLet us make a few mild assumptions: A1 the transition matrix \u03b2 is row-independent (i.e., \u03b2k1 and \u03b2k2 are mutually independent), A2 p(X,Z|\u03b8) is bounded (does not diverge to infinity), and A3 p(X|\u03c6k) satisfies the regularity condition. A1 and A2 are usual assumptions in HMMs. A3 is much milder than a regularity assumption on p(X|\u03b8), and many HMMs (e.g., a HMM with categorical observations, a HMM with Gaussian observations) satisfy this assumption.\nLet us denote a model of p(X|\u03b8) as M . We allow K emission probabilities to be different from one another in their model representations (e.g., for logistic regression emissions, different hidden states can have feature configurations with different complexities). This can be seen as an HMM-extension of the so-called \u201cheterogeneous mixture models\u201d (Fujimaki et al., 2011). In order to distinguish different model representations, we denote a model of \u03c6k as Sk. That is, our model M is specified by K and emission models Sk, i.e., M = (K,S1, . . . , SK). Although parameter representations depend on the corresponding models, we hereinafter omit them for notational simplicity."}, {"heading": "3. FIC for HMMs", "text": "FIChmm considers the following lower bound of marginal log-likelihood as:\nlog p(xN |M) \u2265 \u2211 zN q(zN ) log (p(xN , zN |M) q(zN ) ) (1) In contrast to the standard VB lower bound,\u2211 zN \u222b q(zN )q(\u03b8) log ( p(xN , zN ,\u03b8|M)/q(zN )q(\u03b8) ) d\u03b8, which makes a mutual independence assumption between zN and \u03b8 on variational distributions, this lower bound does not. Although (1) is also used in FICmm and collapse VBs (Teh et al., 2006; Kurihara\net al., 2007), both of them require mutual independences among hidden variables, and thus they are not directly applicable for models having time-dependent hidden variables (e.g., HMMs).\nOn the basis of the Markov property of hidden states, A1, and A2, the numerator of (1) is factorized as follows: p(xN , zN |M) = \u222b N\u220f\nn=1\n{ p(z1n|\u03b1) K\u220f k=1 Tn\u220f t=2 pk(z t n|\u03b2k)z t\u22121 nk\n\u00d7 K\u220f k=1 Tn\u220f t=1 p(xtn|\u03c6k)z t nk } p(\u03b8|M)d\u03b8, (2)\nwhere p(\u03b8|M) = p(\u03b1|M) \u220fK k=1 p(\u03b2k|M)p(\u03c6k|M) is a parameter prior, and we assume that the priors\u2019 dimensionalities are asymptotically small. Each one of the factorized distributions, p(z1n|\u03b1), pk(ztn|\u03b2k) and p(xtn|\u03c6k), is a regular model (A3) to which the Laplace method is applicable. Then, as with FICmm, the \u201cfactorized\u201d Laplace method around the ML estimator3 \u03b8\u0304 of p(xN , zN |\u03b8) gives us a second order approximation of complete likelihood as follows:\nlog p(xN , zN |\u03b8) \u2248 log p(xN , zN |\u03b8\u0304)\u2212 N 2 [F\u0304\u03b1]\n\u2212 K\u2211 k=1\n(\u2211N,Tn\u22121 n,t=1 z t nk\n2 [F\u0304\u03b2k ]\u2212\n\u2211N,Tn n,t=1 z t nk\n2 [F\u0304\u03c6k ]\n) . (3)\nThe notation [F\u0304\u2022] represents a (centered) quadratic form with respect to \u2022, e.g., [F\u0304\u03b1] = (\u03b1\u2212 \u03b1\u0304)T F\u0304\u03b1(\u03b1\u2212 \u03b1\u0304). F\u0304\u03b1, F\u0304\u03b2k , and F\u0304\u03c6k are Fisher information matrices approximated by (complete) data instances as follows:\nF\u0304\u03b1 = \u22121 N \u22072\u03b1 N\u2211 n=1 log p(z1n|\u03b1) \u2223\u2223\u2223 \u03b1=\u03b1\u0304\n(4)\nF\u0304\u03b2k = \u22121\u2211N,Tn\u22121\nn,t=1 z t nk\n\u22072\u03b2k N,Tn\u22121\u2211 n,t=1 ztnk log pk(z t+1 n |\u03b2k) \u2223\u2223\u2223 \u03b2k=\u03b2\u0304k\nF\u0304\u03c6k = \u22121\u2211N,Tn\nn,t=1 z t nk\n\u22072\u03c6k N,Tn\u2211 n,t=1 ztnk log p(x t n|\u03c6k) \u2223\u2223\u2223 \u03c6k=\u03c6\u0304k .\nIt is easy to show that they respectively converge to Fisher information matrices of p(Z1|\u03b1), pk(Zt|\u03b2k), and p(X|\u03c6k) with N \u2192 \u221e, and it guarantees their determinants to be O(1). The complete marginal like-\n3As an alternative, we could employ the maximum-aposteriori estimator of p(xN ,zN |\u03b8)p(\u03b8). This would not make a significant difference with respect to discussion in this paper.\nlihood can then be asymptotically approximated as:\np(xN ,zN |M) \u2248 p(xN , zN |\u03b8\u0304) (2\u03c0) D\u03b1/2\nND\u03b1/2|F\u0304\u03b1|1/2\n\u00d7 K\u220f k=1\n(2\u03c0)D\u03b2k /2 ( \u2211N,Tn\u22121 n,t=1 z t nk) D\u03b2k/2|F\u0304\u03b2k |1/2\n\u00d7 K\u220f k=1\n(2\u03c0)D\u03c6k/2 ( \u2211N,Tn n,t=1 z t nk) D\u03c6k/2|F\u0304\u03c6k |1/2 . (5)\nHere, D\u2022 is the dimensionality of \u2022 (D\u03b1 = K \u2212 1 and D\u03b2k = K \u2212 1).\nBy substituting (5) into (1) and ignoring asymptotically small terms, we obtain an asymptotic approximation of log p(xN |M) as follows:\nFIC(xN ,M) \u2261 max q {J (q, \u03b8\u0304,xN )} (6)\nJ (q, \u03b8\u0304,xN ) = \u2211 zN q(zN ) ( log p(xN , zN |\u03b8\u0304)\n\u2212 D\u03b1 2 logN \u2212 K\u2211 k=1 D\u03b2k 2 log( N,Tn\u22121\u2211 n,t=1 ztnk) \u2212 K\u2211 k=1 D\u03c6k 2 log( N,Tn\u2211 n,t=1 ztnk)\u2212 log q(zN ) ) . (7)\nWe here have two regularization terms dependent on hidden states, D\u03b2k log( \u2211N,Tn\u22121 n,t=1 z t nk)/2 and\nD\u03c6k log( \u2211N,Tn n,t=1 z t nk)/2. The latter stems from asymptotic approximation of the emission (observation) probability, and appears in FICmm. Contrastingly, the former is an unique regularization term in FIChmm. Notably, these two regularizers contain dependencies between parameters (\u03b2k and \u03c6k) and hidden states, which the variational lower bound of VB methods usually ignore on their variational distributions (Beal, 2003). These regularizers will be discussed in more detail in sub-sections 4.4 and 4.5\nThe following theorem justifies FIChmm as an approximation of marginal log-likelihood:\nTheorem 1 FIC(xN ,M) is asymptotically consistent with log p(xN |M).\nA rough sketch of the theorem can be described as follows. Because of the regularity condition A3, individual Laplace approximations have asymptotic consistency, and thus their product (5) is also consistent with p(xN , zN |M). Then, since there is a q which satisfies the equality in (1), the theorem holds."}, {"heading": "4. FAB for HMMs", "text": ""}, {"heading": "4.1. FAB Lower Bound", "text": "Since \u03b8\u0304 is defined on both xN and zN and thus is not available in practice, we cannot evaluate FIChmm itself. Instead, FAB maximizes the lower bound of FIChmm. As is similarly done in (Fujimaki & Morinaga, 2012), we employ two inequalities to derive the lower bound. First, on the basis of the definition of the ML estimator, log p(xN , zN |\u03b8\u0304) \u2265 log p(xN , zN |\u03b8) holds. Second, on the basis of the concavity of the logarithm function, log( \u2211N,Tn n,t=1 z t nk) \u2265\nL( \u2211N,Tn n,t=1 z t nk, \u2211N,Tn n,t=1 z\u0303 t nk) holds with an arbitrary\nq\u0303 (the same holds for log( \u2211N,Tn\u22121 n,t=1 z t nk)), where L(a, b) = log b + (a \u2212 b)/b. The lower bound of (6) is then derived as follows:\nFIC(xN ,M) \u2265 G(q, q\u0303,xN ,\u03b8) (8)\n= N\u2211 n=1 \u2211 zn q(zn) [ log p(xn, zn|\u03b8) + K,Tn\u2211 k,t=1 ztnk log \u03b4 t k\n\u2212 log q(zN ) ] + N,Tn\u2211 n,t=1 log \u2206t \u2212 D\u03b1 2 logN \u2212 K\u2211 k=1 ( D\u03b2k 2 \u00d7\n(log( N,Tn\u22121\u2211 n,t q\u0303(ztnk))\u2212 1) + D\u03c6k 2 (log( N,Tn\u2211 n,t q\u0303(ztnk))\u2212 1) ) ,\n(9)\n\u03b4tk =  1 \u2206t exp ( \u2212 D\u03b2k 2( \u2211N,Tn\u22121 n,t=1 q\u0303(z t nk)) \u2212 D\u03c6k 2( \u2211N,Tn n,t=1 q\u0303(z t nk)) ) if t < Tn\n1 \u2206t exp\n( \u2212 D\u03c6k\n2( \u2211N,Tn\nn,t=1 q\u0303(z t nk))\n) if t = Tn\n,\n(10) where \u2206t is a normalization constant for \u2211K k=1 \u03b4 t k = 1. The underlined part is referred to in (19).\nFABhmm learns HMMs by solving the following optimization problem (recall that \u03b8 and q are respective functions of M):\nM\u2217,\u03b8\u2217, q\u2217, q\u0303\u2217 = arg max M ,\u03b8,q,q\u0303 G(q, q\u0303,xN ,\u03b8). (11)\nHere we have a newly-introduced parameter q\u0303 which is also optimized in FABhmm. Let us fix \u03b8 and q. By making the differential of (9) with respect to q\u0303(ztnk) zero, we obtain the following optimality conditions:\nD\u03c6k 2 ( 1\u2211N,Tn n,t q\u0303(z t nk) \u2212 \u2211N,Tn n,t q(z t nk) ( \u2211N,Tn n,t q\u0303(z t nk)) 2 ) + (12)\nD\u03b2k 2 ( 1\u2211N,Tn\u22121 n,t q\u0303(z t nk) \u2212 \u2211N,Tn\u22121 n,t q(z t nk) ( \u2211N,Tn\u22121 n,t q\u0303(z t nk)) 2 ) = 0.\nClearly, q\u0303 = q satisfies (12) for arbitrary q\u0303(ztnk). This result will be used in the next subsection."}, {"heading": "4.2. Iterative Optimization with FAB Forward-Backward Algorithm", "text": "Let us first fix K and consider the optimization of (11) with respect to S,\u03b8, q, q\u0303, where S = (S1, . . . , SK). Since their simultaneous optimization would be intractable, FABhmm works on the basis of iterations of two sub-steps (V-step and M-step). Let the superscription (i) represent the i-th iteration.\nV-step (FAB Forward-Backward Algorithm) In the i-th V-step, we fix \u03b8 = \u03b8(i\u22121) and also fix q\u0303 = q(i\u22121) on the basis of (12). FABhmm then optimizes q by maximization in (9). The terms in (9) dependent on q can be decomposed in terms of sequences, and thus we can independently optimize q(zn) for individual sequences. Further, the maximization problem for q(zn) with respect to\n\u2211 zn q(zn)[log p(xn, zn|\u03b8) +\u2211K,Tn\nk,t=1 z t nk log \u03b4 t k\u2212 log q(zn)] has a form similar to the E-step in the EM algorithm for HMMs. In fact, the only difference is \u2211K,Tn k,t=1 z t nk log \u03b4 t k, which arises from\nthe regularization terms D\u03b2k log( \u2211N,Tn\u22121 n,t=1 z t nk)/2 and\nD\u03c6k log( \u2211N,Tn n,t=1 z t nk)/2 in (7).\nNotably, the term ztnk log \u03b4 t k is a product of a hidden variable and a log-probability because \u03b4tk is normalized as is defined in (10). The maximization problem can thus be solved on the basis of the forward-backward algorithm described as follows:\nf t(i) nk =  1 \u03b6 1(i) n \u03b1 (i\u22121) k p\u0303(x 1 n|\u03c6 (i\u22121) k ) if t = 1 1\n\u03b6 t(i) n\np\u0303(xtn|\u03c6 (i\u22121) k ) \u2211K j=1 f t\u22121(i) nj \u03b2 (i\u22121) jk\nb t(i) nk =\n{ 1\n\u03b6 t+1(i) n\n\u2211K j=1 b t+1(i) n p\u0303(xt+1n |\u03c6 (i\u22121) j )\u03b2 (i\u22121) kj\n1 if t = Tn\np\u0303(xtn|\u03c6 (i\u22121) k ) = p(x t n|\u03c6 (i\u22121) k )\u03b4 t(i\u22121) k . (13)\n\u03b6 t(i) n is a normalization constant for \u2211K k=1 f t(i) nk = 1. On the basis of f t(i) nk and b t(i) nk , the variational distributions are calculated as follows:\nq(i)(ztnk) =f t(i) nk b t(i) nk (14)\nq(i)(zt\u22121nj , z t nk) =\n1\n\u03b6 t(i) n\nf t\u22121(i) nj p\u0303(x t n|\u03c6 (i\u22121) k )\u03b2 (i\u22121) jk b t(i) nk\nIn the above FAB forward-backward algorithm, the only difference from the standard forward-backward algorithm is the exponentiated update term, \u03b4 t(i\u22121) k . It is interesting that FABmm has a similar exponentiated update term in its V-step, and it generates significant\ndifferences from standard ML estimation using the EM algorithm. Similarly, \u03b4 t(i\u22121) k generates essential differences for learning HMMs between FABhmm estimation and ML estimation. Roughly speaking, (10) indicates that the smaller and more complex components are likely to become even smaller through the iterations. Further, (13) indicates that the regularization effect of \u03b4 t(i\u22121) k propagates forward and backward (e.g., largelyregularized hidden states in f t(i) nk make only small contribution to f t+1(i) nj ).\nMstep Let us fix q = q(i) and q\u0303 = q(i). FABhmm then optimizes \u03b8 by maximization in (9). First, we have the following parameter updates for \u03b1 and \u03b2:\n\u03b1 (i) k \u221d N\u2211 n=1 q(i)(z1nk), \u03b2 (i) jk \u221d N,Tn\u22121\u2211 n,t=1 q(i)(ztnj , z t+1 nk )\n(15)\nWe then update Sk and \u03c6k by solving the following optimization problem:\nS (i) k ,\u03c6 (i) k = arg max\nSk,\u03c6k {N,Tn\u2211 n,t=1 q(i)(ztnk)(log p(x t n|\u03c6k)\n\u2212 D(i)\u03b2k\n2 log( N,Tn\u22121\u2211 n,t q(i)(ztnk)) }\n(16)\nIt is worth noting that FABhmm provides a natural way of seeking emission probability types Sk because the M-step can be decomposed into optimization problems for individual hidden states (otherwise, we must take into account an exponential number of hidden state combinations.) With a finite set of emission probability candidates, we first optimize \u03c6k for each element of a fixed Sk and then select the optimal one by comparing them. If we employ a standard HMM having single emission type, (16) is reduced to the Mstep of the standard EM algorithm for HMMs, i.e., \u03c6 (i) k = arg max\u03c6k \u2211N,Tn n,t=1 q (i)(ztnk) log p(x t n|\u03c6k). Further, FABhmm is reduced to the standard EM algorithm with N \u2192\u221e (because \u03b4tk = 1) and thus can be seen as its natural generalization."}, {"heading": "4.3. Convergence and Stopping Criterion", "text": "Let us denote the lower bound of FIC as follows:\nFIC (i) LB(x N ,M) \u2261 G(q(i), q\u0303(i) = q(i),xN ,\u03b8(i)). (17)\nThen, as is done for mixture models (Fujimaki & Morinaga, 2012), FABhmm is guaranteed to monotonically increase FIC (i) LB(x\nN ,M) through the VM-iterations, which can be summarized in the following theorem.\nTheorem 2 For the VM iterations of FABhmm, the following inequality is satisfied:\nFIC (i) LB(x N ,M) \u2265 FIC(i\u22121)LB (x N ,M). (18)\nWe employ FIC (i) LB(x N ,M) \u2212 FIC(i\u22121)LB (xN ,M) \u2264 \u03c4 as a stopping criterion. One issue is computation of the term \u2211 zn q(zn) log q(zn) in (9), which naively requires O(KTn) computational cost. We can solve this problem using a way similar to that for the variational free energy for VBHMMs (Beal, 2003). In summary, FIC (i) LB(x\nN ,M) can be computed as follows (we omit the derivation here because of space limitations):\nFIC (i) LB(x N ,M) = N\u2211 n=1 Tn\u2211 t=1 log \u03b6t(i)n + underline of (9).\n(19)"}, {"heading": "4.4. Automatic Hidden State Selection", "text": "An interesting property of the asymptotic exponentiated regularizer \u03b4tk (10) is a shrinkage effect for selecting the number of hidden states, which we fixed in the previous section, and thus it provides over-fitting mitigation despite our asymptotic ignoring of priors.\nIn (13), f t(i) nk with a small \u03b4 t(i\u22121) k value is largely regularized without relation to the observation and previous paths. While b t(i) nk does not explicitly have such a regularization effect, one notable fact is that each next path b t+1(i) nj having a large \u03b4 t(i+1) j value makes only a small contribution to the update of b t(i) nk . Then, in (14), the k-th hidden state with a small \u03b4 t(i\u22121) k value has a\nsmall size \u2211N,Tn n,t q\n(i)(ztnk). Note that, from the definition in (10), the smaller hidden state has the smaller \u03b4tk value. This means such a hidden state gradually becomes smaller through the VM iterations. Similarly, the frequency of transitions between the j-th and the k-th states having respective small \u03b4t\u22121j and \u03b4 t k val-\nues becomes smaller because \u2211N,Tn\u22121 n,t q (i)(ztnj , z t+1 nk ) gradually becomes smaller.\nOn the basis of the above insight, FABhmm shrinks hidden states using a thresholding operation as follows:\nq(i)(ztnk) = 0, q (i)(ztnk, z t+1 nj ) = 0 if N,Tn\u2211 n,t=1 q(i)(ztnk) \u2264 \u03b5. (20) Starting from an appropriately-large number (Kmax) of hidden states, FABhmm iteratively optimizes S, \u03b8, and q. During the VM steps, the number of hidden states might become smaller due to the shrinkage operation. Then, at a convergence point, we obtain the\noptimal model M\u2217 = (K\u2217,S\u2217), the optimized parameter \u03b8\u2217, and the variational distribution q\u2217. A similar shrinkage effect has been reported in FABmm (Fujimaki & Morinaga, 2012), and FABhmm naturally inherits it for Markov hidden variables."}, {"heading": "4.5. Discussion: Comparison with VB and BIC", "text": "We here compare three approximation inference methods (FAB, VB and BIC) and discuss their differences. FAB approximates marginal log-likelihoods by (19). Let us denote normalization constants for forwardbackward algorithms of ML and VB estimations as \u03b6tn(ML) and \u03b6 t n(V B), respectively. Variational free energy FV B and BIC4 can then be respectively computed as follows (see (Beal, 2003) for VB free energy):\nFV B = N,Tn\u2211 n,t=1 log \u03b6tn(V B) + \u222b d\u03b1q(\u03b1) log p(\u03b1) q(\u03b1) (21)\n+ \u222b d\u03b2q(\u03b2) log p(\u03b2)\nq(\u03b2) + K\u2211 k=1 \u222b d\u03c6kq(\u03c6k) log p(\u03c6k) q(\u03c6k) ,\nBIC = N,Tn\u2211 n,t=1 log \u03b6tn(ML)\u2212 D 2 log N\u2211 n=1 Tn (22)\nwhere D = D\u03b1 + \u2211K k=1(D\u03b2k +D\u03c6k).\nHere (19), (21), and (22) all have closely similar representations, i.e., data fitting term + complexity. The data fitting terms (\u03b6-related terms) are different in posterior (or variational) distributions of hidden states, on which respective complete log-likelihood functions are marginalized (of course, parameter estimators are also different.) Interesting differences appear in the complexity terms. The complexity term of VB is the Kullback-Leibler divergence between the variational posteriors and priors on parameters. Therefore, VB regularizes parameters (precisely speaking, variational posteriors) to be apart from priors. One disadvantage is that the complexity term is dependent on a choice of priors (and their hyperparameters), and model selection results will thus be similarly dependent, though we understand that this can also be an advantage because we can control models using priors. On the other hand, FAB does not have manually-tunable parameters in (19). Notably, only the FAB complexity takes into account the distribution on hidden states (i.e., q\u0303\u2217(zN ) = q\u2217(zN )), and FAB automatically adjusts regularization levels on the basis of sizes ( \u2211N,Tn n,t q\n(ztnk)) and the dimensionality of individual hidden states. The regularization term of BIC is\n4The (22) representation of BIC does not have theoretical justification for HMMs, as we have noted in Section 1.\nstronger than that of FAB (all hidden states are regularized with the scale log \u2211N n=1 Tn.) Interestingly, the stochastic complexity of HMMs is theoretically proven to be considerably smaller than D/2 log \u2211N n=1 Tn (Yamazaki & Watanabe, 2005). Our result also suggest that a brute-force application of BIC to HMMs overestimates the complexity term."}, {"heading": "5. Experiments and Discussion", "text": "We conducted simulations using artificial data for investigating basic behaviors of HMMs with FAB (FABHMMs), and evaluation using real world ebook data. FABHMM was compared with VBHMM, iHMM, and HMMs with ML estimation and BIC model selection (MLHMMs). We implemented FABHMM and MLHMM by Python, while we used Matlab softwares for iHMM5 and VBHMM6."}, {"heading": "5.1. Simulations with Artificial Data", "text": "By following settings similar to those in (van Gael et al., 2008), we conducted evaluations on HMMs with either one-dimensional Gaussian emissions or categorical emissions. The true model had four hidden states, either Gaussian emission probabilities with means (\u22124,\u22121, 2, 3) and variances (0.5, 0.5, 0.5, 0.5) or categorical emission probabilities with 8-alphabet, and transition probability described as follows:(\n0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0\n) /2 and ( 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 ) /3,\nwhere the left and right matrices correspond to transition and categorical emission probabilities. We set the initial probability of the first state as one and the rests were zero. We randomly generated a single training sequences with the length of T \u2208 {250, 500, 1000, 2000}, and a test sequence with the length of Ttest = 5000. For iHMMs, we randomly initialized the hidden state sequences by 10 different states. We set two hyper-priors in iHMMs as (1) \u0393(4, 1), \u0393(4, 1) and (2) \u0393(1, 0.01), \u0393(1, 0.01), where \u0393(a, b) denotes the Gamma distribution with the shape parameter a and the scale parameter b. The former setting (iHMM1) were used in (van Gael et al., 2008) as a default setting and the latter setting (iHMM2) was less informative. VBHMM and MLHMM were performed with setting K = 1, . . . ,Kmax and selected the optimal K\u2217 which minimized the respective free energy and BIC values. For FABHMM, VBHMM and MLHMM, Kmax is set to be ten (FAB started from Kmax hidden states and automatically searched the\n5iHMM: mloss.org/revision/view/291/ 6Categorical: www.gatsby.ucl.ac.uk/vbayes, Gaus-\nsian: www.robots.ox.ac.uk/~parg"}, {"heading": "T FAB ML iHMM1 iHMM2 VB", "text": "optimal value.) We evaluated the estimated number of hidden states (model selection accuracy), predictive log-likelihood (PLL) against the testing set (generalization performance), and training time (computational efficiency). If the training time of each method exceeded 10 minutes, we stopped the training procedure and used the result at the time (iHHM violated this time limit a few times.) The results below are the averages of ten runnings.\nTable 1 shows FABHMM almost perfectly estimated the true number of hidden states for both emission types. Surprisingly, despite FAB being an asymptotic method, it outperformed the others with relatively small data sizes. iHMMs also performed well, but their results were somehow affected by a choice of hyper-priors. VBHMM and MLHMM were significantly inferior to the others in terms of model selection performance. One plausible reason for the wrong performance of VB is the independence assumption to variables; VB approximates the marginal likelihood with ignoring the variable dependency, which makes its approximation worse than the others. BIC does not have theoretical justification in HMMs and, in fact, the MLHMM poorly performed.\nFig. 1 shows the training times and the PLLs (left: Gaussian, right: categorical). With respect to PLLs, FABHMM was competitive or slightly better than the best among the others, while none of the others did not perform well for both cases. With respect to training times, FABHMM and VBHMM were competitive and 3-4 times faster than iHMMs while all of their training\ntime increased only linearly with the sequence length. Both PLLs and training times of MLHMM were significantly worse than those of the others. This was because MLHMM was likely to be captured by bad local minima solutions which significantly degraded both estimation performance and the convergence of its optimization while additional computational cost might mitigate this issue. An interesting observation was that FABHMM was robust for such local minima solutions because the exponentiated regularizer automatically removed such \u201cbad\u201d hidden states, and thus FABHMM mitigated the issue."}, {"heading": "5.2. E-Book Character Sequences", "text": "Next, we evaluated the feasibility for text prediction. We prepared six books7, Alice\u2019s Adventures in Wonderland (Alice), THe Art of War (Sunzi), The Metamorphosis (Kafka), The Republic (Plato), The United States Declaration of Independence (DOI), and The Adventures of Sherlock Holmes (Sherl). In this setting, each letter in the texts was treated as a categorical observation. The letters included some special characters, and the number of categorical alphabets varied from 32 to 50 in these data sets. The first 5000 letters of the first chapter in each book were used for training and the next 5000 letters for testing. For fair comparison, we eliminated the alphabets from the testing sets which did not appear in the training sets. Here We compared FAB with iHMM1 and VBHMM. Since these data are more complicated than artificial data, we here set Kmax = 20 for FABHMM and VBHMM.\nFig. 2 shows a typical behavior of FABHMM and iHMM and we confirmed the prediction performance of FABHMM was improved with increasing T . FABHMM achieved competitive prediction performance with iHMM for larger T \u2265 3000. Although FABHMM required a longer sequence for reasonable estimation than those in the artificial simulations, we\n7These books are available of www.gutenberg.org."}, {"heading": "Data K Time PLL K Time PLL K Time PLL", "text": "believe that the length (about 3000 - 5000) is not large in recent large scale data scenarios. Notably, the estimated number of hidden states was much smaller than that of iHMM. This means FABHMM could obtain more compact HMM representations than iHMM and that is usually desirable in practice. Table 2 shows the training times, estimated number of hidden states, and PLLs (T = 5000). For all data, with respect to PLLs, FABHMM and iHMM comparably performed, and VBHMM performed the worst. The training times of FABHMM were roughly three or four times faster, which agree with the results in the previous section. These results indicate clear advantages of FABHMM with respect to model selection accuracy over VBHMM, and with respect to computational efficiency over iHMM. Finally, we emphasize that FABHMM does not have hyper-parameters in its criterion, and all the above strong model selection procedures were automatically done."}, {"heading": "6. Summary and Future Work", "text": "This paper has addressed the model selection issue for HMMs by generalizing factorized information criteria and factorized asymptotic Bayesian inference. We have theoretically shown several desirable properties (asymptotic consistency of FIC, an automatic shrinkage effect, monotonic increase in the FIC lower bound, etc) and also have experimentally shown that FABHMM outperforms the states-of-the-art variational Bayesian and non-parametric Bayesian methods with respect to model selection accuracy and computational efficiency.\nWe have several issues for future study. First, it is interesting to extend FAB for more flexible HMM families as iHMM has been extended for factorial HMM (Ghahramani & Jordan, 1997; van Gael et al., 2009). Second, both FABmm and FABhmm are designed for discrete hidden variables, and thus FAB for continuous hidden variable models is still an open problem. Third, theoretical details, such as rates of convergence (e.g., how fast the FAB iteration con-\nverges and how fast FIC converges to marginal loglikelihood), should be investigated."}, {"heading": "Beal, M. J., Ghahramani, Z., and Rasmussen, C. E. The", "text": "infinite hidden Markov model. In NIPS, 2002."}, {"heading": "Bratieres, S., van Gael, J., Vlachos, A., and Ghahramani,", "text": "Z. Scaling the iHMM: Parallelization versus Hadoop. In SMLA, 2010.\nDempster, A. P., Laird, N. M., and Rubin, D. B. Maximum likelihood from imcomplete data via the EM algorithm. Journal of the Royal Statistical Society, B39(1): 1\u201338, 1977."}, {"heading": "Fujimaki, Ryohei and Morinaga, Satoshi. Factorized", "text": "asymptotic bayesian inference for mixture models. In AISTATS, 2012."}, {"heading": "Fujimaki, Ryohei, Sogawa, Yasuhiro, and Morinaga,", "text": "Satoshi. Online heterogeneous mixture modeling with marginal and copula selection. In KDD, 2011.\nGhahramani, Z. and Jordan, M. I. Factorial hidden markov models. Machine Learning, 29:245\u2013273, 1997.\nKurihara, Kenichi, Welling, Max, and Teh, Yee Whye. Collapsed variational dirichlet process mixture models. In IJCAI, pp. 2796\u20132801, 2007.\nMacKay, D. J. C. Ensemble learning for hidden Markov models. Technical report, 1997.\nRabiner, L. R. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of IEEE, 77:257\u201386, 1989."}, {"heading": "Robert, Christian P., Rydn, Tobias, and Titterington,", "text": "D.M. Bayesian inference in hidden markov models through reversible jump markov chain monte carlo. Journal of the Royal Statistical Society, Series B, 62: 57\u201375, 2000.\nSchwarz, Gideon. Estimating the dimension of a model. The Annals of Statistics, 6(2):461\u2013464, 1978.\nTeh, Yee Whye, Newman, Dave, and Welling, Max. A collapsed variational bayesian inference algorithm for latent dirichlet allocation. In NIPS, pp. 1378\u20131385, 2006.\nvan Gael, J., Saatci, Y, Teh, Y.-W., and Ghahramani, Z. Beam sampling for the infinite hidden Markov model. In ICML, 2008.\nvan Gael, J., Teh, Y.-W., and Ghahramani, Z. The infinite factorial hidden Markov model. In NIPS, 2009."}, {"heading": "Yamazaki, K. and Watanabe, S. Algebraic geometry and", "text": "stochastic complexities of hidden Markov models. Neurocomputing, 69:62\u201384, 2005."}], "references": [{"title": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes", "author": ["L.E. Baum"], "venue": "Inequalities, 3:1\u20138,", "citeRegEx": "Baum,? \\Q1972\\E", "shortCiteRegEx": "Baum", "year": 1972}, {"title": "Variational Algorithms for Approximate Bayesian Inference", "author": ["M.J. Beal"], "venue": "PhD thesis,", "citeRegEx": "Beal,? \\Q2003\\E", "shortCiteRegEx": "Beal", "year": 2003}, {"title": "The infinite hidden Markov model", "author": ["M.J. Beal", "Z. Ghahramani", "C.E. Rasmussen"], "venue": "In NIPS,", "citeRegEx": "Beal et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Beal et al\\.", "year": 2002}, {"title": "Scaling the iHMM: Parallelization versus Hadoop", "author": ["S. Bratieres", "J. van Gael", "A. Vlachos", "Z. Ghahramani"], "venue": "In SMLA,", "citeRegEx": "Bratieres et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bratieres et al\\.", "year": 2010}, {"title": "Maximum likelihood from imcomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Factorized asymptotic bayesian inference for mixture models", "author": ["Fujimaki", "Ryohei", "Morinaga", "Satoshi"], "venue": "In AISTATS,", "citeRegEx": "Fujimaki et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fujimaki et al\\.", "year": 2012}, {"title": "Online heterogeneous mixture modeling with marginal and copula selection", "author": ["Fujimaki", "Ryohei", "Sogawa", "Yasuhiro", "Morinaga", "Satoshi"], "venue": "In KDD,", "citeRegEx": "Fujimaki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fujimaki et al\\.", "year": 2011}, {"title": "Factorial hidden markov models", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "Machine Learning,", "citeRegEx": "Ghahramani and Jordan,? \\Q1997\\E", "shortCiteRegEx": "Ghahramani and Jordan", "year": 1997}, {"title": "Collapsed variational dirichlet process mixture models", "author": ["Kurihara", "Kenichi", "Welling", "Max", "Teh", "Yee Whye"], "venue": "In IJCAI, pp", "citeRegEx": "Kurihara et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kurihara et al\\.", "year": 2007}, {"title": "Ensemble learning for hidden Markov models", "author": ["D.J.C. MacKay"], "venue": "Technical report,", "citeRegEx": "MacKay,? \\Q1997\\E", "shortCiteRegEx": "MacKay", "year": 1997}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of IEEE,", "citeRegEx": "Rabiner,? \\Q1989\\E", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "Bayesian inference in hidden markov models through reversible jump markov chain monte carlo", "author": ["Robert", "Christian P", "Rydn", "Tobias", "D.M. Titterington"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Robert et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Robert et al\\.", "year": 2000}, {"title": "Estimating the dimension of a model", "author": ["Schwarz", "Gideon"], "venue": "The Annals of Statistics,", "citeRegEx": "Schwarz and Gideon.,? \\Q1978\\E", "shortCiteRegEx": "Schwarz and Gideon.", "year": 1978}, {"title": "A collapsed variational bayesian inference algorithm for latent dirichlet allocation", "author": ["Teh", "Yee Whye", "Newman", "Dave", "Welling", "Max"], "venue": "In NIPS,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Beam sampling for the infinite hidden Markov model", "author": ["J. van Gael", "Y Saatci", "Teh", "Y.-W", "Z. Ghahramani"], "venue": "In ICML,", "citeRegEx": "Gael et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gael et al\\.", "year": 2008}, {"title": "The infinite factorial hidden Markov model", "author": ["J. van Gael", "Teh", "Y.-W", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Gael et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gael et al\\.", "year": 2009}, {"title": "Algebraic geometry and stochastic complexities of hidden Markov models", "author": ["K. Yamazaki", "S. Watanabe"], "venue": null, "citeRegEx": "Yamazaki and Watanabe,? \\Q2005\\E", "shortCiteRegEx": "Yamazaki and Watanabe", "year": 2005}], "referenceMentions": [{"referenceID": 11, "context": "Markov chain Monte Carlo methods (MCMCs) (Robert et al., 2000) and variational Bayesian inference (VB) (MacKay, 1997; Beal, 2003) approximate computationally and analytically intractable marginal log-likelihoods, using respectively, sampling and variational approximation techniques.", "startOffset": 41, "endOffset": 62}, {"referenceID": 9, "context": ", 2000) and variational Bayesian inference (VB) (MacKay, 1997; Beal, 2003) approximate computationally and analytically intractable marginal log-likelihoods, using respectively, sampling and variational approximation techniques.", "startOffset": 48, "endOffset": 74}, {"referenceID": 1, "context": ", 2000) and variational Bayesian inference (VB) (MacKay, 1997; Beal, 2003) approximate computationally and analytically intractable marginal log-likelihoods, using respectively, sampling and variational approximation techniques.", "startOffset": 48, "endOffset": 74}, {"referenceID": 2, "context": "In terms of modeling, infinite HMMs (iHMMs) employ a hierarchical Dirichlet process prior in order to express an infinite number of hidden states (Beal et al., 2002).", "startOffset": 146, "endOffset": 165}, {"referenceID": 2, "context": "(2008) uses a beam sampling technique which is more efficient than well-known Gibbs sampling techniques (Beal et al., 2002).", "startOffset": 104, "endOffset": 123}, {"referenceID": 3, "context": "Although the beam sampling technique considerably reduces the computational cost of MCMC inference, it is still higher than that of HMMs using variational non-parametric Bayesian inference (VBHMMs), while acceleration of iHMMs has been discussed from the viewpoints of parallelization (Bratieres et al., 2010).", "startOffset": 285, "endOffset": 309}, {"referenceID": 1, "context": ", 2000) and variational Bayesian inference (VB) (MacKay, 1997; Beal, 2003) approximate computationally and analytically intractable marginal log-likelihoods, using respectively, sampling and variational approximation techniques. The former has an advantage over the latter in approximation accuracy but has a disadvantage in computational efficiency, and thus the choice of appropriate inference algorithm has been decided on the basis of a trade-off between accuracy and computational efficiency. In terms of modeling, infinite HMMs (iHMMs) employ a hierarchical Dirichlet process prior in order to express an infinite number of hidden states (Beal et al., 2002). In them, the number of components is determined on the basis of mild prior knowledge expressed by a few hyperparameters. The state-of-the-art inference of iHMMs proposed by van Gael et al. (2008) uses a beam sampling technique which is more efficient than well-known Gibbs sampling techniques (Beal et al.", "startOffset": 63, "endOffset": 861}, {"referenceID": 4, "context": "The iterative optimization of FABhmm can be seen as a natural generalization of the expectation-maximization (EM) algorithm (Dempster et al., 1977), and, interestingly, unique regularizers appear as exponentiated update terms in our FAB forward-backward algorithm.", "startOffset": 124, "endOffset": 147}, {"referenceID": 10, "context": "A standard parameter inference follows the EM algorithm with a specific expectation step known as either the forward-backward algorithm (Rabiner, 1989) or the Baum-Welch algorithm (Baum, 1972).", "startOffset": 136, "endOffset": 151}, {"referenceID": 0, "context": "A standard parameter inference follows the EM algorithm with a specific expectation step known as either the forward-backward algorithm (Rabiner, 1989) or the Baum-Welch algorithm (Baum, 1972).", "startOffset": 180, "endOffset": 192}, {"referenceID": 6, "context": "This can be seen as an HMM-extension of the so-called \u201cheterogeneous mixture models\u201d (Fujimaki et al., 2011).", "startOffset": 85, "endOffset": 108}, {"referenceID": 1, "context": "Notably, these two regularizers contain dependencies between parameters (\u03b2k and \u03c6k) and hidden states, which the variational lower bound of VB methods usually ignore on their variational distributions (Beal, 2003).", "startOffset": 201, "endOffset": 213}, {"referenceID": 1, "context": "We can solve this problem using a way similar to that for the variational free energy for VBHMMs (Beal, 2003).", "startOffset": 97, "endOffset": 109}, {"referenceID": 1, "context": "Variational free energy FV B and BIC can then be respectively computed as follows (see (Beal, 2003) for VB free energy):", "startOffset": 87, "endOffset": 99}], "year": 2012, "abstractText": "This paper addresses the issue of model selection for hidden Markov models (HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has been recently developed for model selection on independent hidden variables (i.e., mixture models), for time-dependent hidden variables. As with FAB in mixture models, FAB for HMMs is derived as an iterative lower bound maximization algorithm of a factorized information criterion (FIC). It inherits, from FAB for mixture models, several desirable properties for learning HMMs, such as asymptotic consistency of FIC with marginal log-likelihood, a shrinkage effect for hidden state selection, monotonic increase of the lower FIC bound through the iterative optimization. Further, it does not have a tunable hyper-parameter, and thus its model selection process can be fully automated. Experimental results shows that FAB outperforms states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in terms of model selection accuracy and computational efficiency.", "creator": "LaTeX with hyperref package"}}}