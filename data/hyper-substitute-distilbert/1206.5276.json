{"id": "1206.5276", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2012", "title": "Template Based Inference in Symmetric Relational Markov Random Fields", "abstract": "general markov random fields are globally general and flexible framework for reasoning regarding the joint distribution statistical processes supporting a singular product or interacting areas. it latest particular purpose in learning analytical models is inference. even when handling perfectly complete data, historically one does be a large domain by selecting statistics, learning requires one to compute equal expectation of the distributed consistency given different parameter choices. the typical solution to this problem is that opt to approximate inference procedures, such as variable order propagation. although these procedures function approximately similar, they still require computation stability is past average order of the number given particles ( or statistics ) governing total table. than providing a large structured model over a complex domain, even such plots create unrealistic approximation time. in this application we show that for a different class like relational mrfs, which have minimum symmetry, she can perform the model needed describing particular procedures using elementary relative - level belief approximation. next procedure's running timeline is orthogonal to step size of per relational model bigger than sample size of the universe. moreover, we show that modern descriptive approach is equivalent to efficient loopy belief propagation. convergence sees a dramatic speedup in inference and learning time. we introduce this procedure helps learn relational mrfs for capturing rich numerical distribution of large maize - protein hybrid networks.", "histories": [["v1", "Wed, 20 Jun 2012 15:06:55 GMT  (591kb)", "http://arxiv.org/abs/1206.5276v1", "Appears in Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence (UAI2007)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence (UAI2007)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ariel jaimovich", "ofer meshi", "nir friedman"], "accepted": false, "id": "1206.5276"}, "pdf": {"name": "1206.5276.pdf", "metadata": {"source": "CRF", "title": "Template Based Inference in Symmetric Relational Markov Random Fields", "authors": ["Ariel Jaimovich", "Ofer Meshi", "Nir Friedman"], "emails": ["arielj@cs.huji.ac.il", "meshi@cs.huji.ac.il", "nir@cs.huji.ac.il"], "sections": null, "references": [{"title": "et al", "author": ["S.R. Collins"], "venue": "Towards a comprehensive atlas of the physical interactome of Saccharomyces cerevisiae. Mol Cell Proteomics", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "et al", "author": ["S.R. Collins"], "venue": "Functional dissection of protein complexes involved in yeast chromosome biology using a genetic interaction map. Nature", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Lifted firstorder probabilistic inference", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Inducing features of random fields", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, 19(4):380\u2013 393", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Residual belief propagation: Informed scheduling for asynchronous message passing", "author": ["G. Elidan", "I. McGraw", "D. Koller"], "venue": "UAI", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Learning probabilistic relational models", "author": ["N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "et al", "author": ["A.C. Gavin"], "venue": "Proteome survey reveals modularity of the yeast cell machinery. Nature, 440(7084):631\u2013636", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Stochastic relaxation", "author": ["S. Geman", "D. Geman"], "venue": "gibbs distributions, and the bayesian restoration of images. IEEE Trans. on Pattern Analysis and Machine Intelligence, pages 721\u2013741", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1984}, {"title": "Learning probabilistic models of relational structure", "author": ["L. Getoor", "N. Friedman", "D. Koller", "B. Taskar"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Assessing degeneracy in statistical models of social networks", "author": ["M.S. Handcock"], "venue": "Technical Report 39, University of Washington", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Towards an integrated protein-protein interaction network: a relational Markov network approach", "author": ["A. Jaimovich", "G. Elidan", "H. Margalit", "N. Friedman"], "venue": "J. Comut. Biol., 13:145\u2013164", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to variational approximations methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T. Jaakkola", "L.K. Saul"], "venue": "Learning in Graphical Models", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "et al", "author": ["N.J. Krogan"], "venue": "Global landscape of protein complexes in the yeast Saccharomyces cerevisiae. Nature, 440(7084):637\u2013643", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "H.A. Loeliger"], "venue": "IEEE Transactions on Information Theory, 47(2)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "MIPS: a database for genomes and protein sequences", "author": ["HW Mewes", "J Hani", "F Pfeiffer", "D Frishman"], "venue": "Nucleic Acids Research, 26:33\u201337", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K. Murphy", "Y. Weiss"], "venue": "UAI", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "SPOOK: A system for probabilistic object-oriented knowledge representation", "author": ["A. Pfeffer", "D. Koller", "B. Milch", "K. Takusagawa"], "venue": "UAI", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "First-order probabilistic inference", "author": ["D. Poole"], "venue": "IJCAI", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "D", "author": ["E. Segal", "M. Shapira", "A. Regev"], "venue": "Pe\u2019er, D. Botstein, D. Koller, and N. Friedman. Module networks: identifying regulatory modules and their conditionspecific regulators from gene expression data. Nat Genet, 34(2):166\u2013176", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Discriminative probabilistic models for relational data", "author": ["B. Taskar", "A. Pieter Abbeel", "D. Koller"], "venue": "UAI", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Link prediction in relational data", "author": ["B. Taskar", "M.F. Wong", "P. Abbeel", "D. Koller"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Constructing free energy approximations and generalized belief propagation algorithms", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "Technical Report TR-2002- 35, Mitsubishi Electric Research Labaratories", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 5, "context": "Relational probabilistic models are a rich framework for reasoning about structured joint distributions [6, 9].", "startOffset": 104, "endOffset": 110}, {"referenceID": 8, "context": "Relational probabilistic models are a rich framework for reasoning about structured joint distributions [6, 9].", "startOffset": 104, "endOffset": 110}, {"referenceID": 20, "context": "Such models are used to model many types of domains like the web [22], gene expression measurements [20] and proteinprotein interaction networks [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "Such models are used to model many types of domains like the web [22], gene expression measurements [20] and proteinprotein interaction networks [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "Such models are used to model many types of domains like the web [22], gene expression measurements [20] and proteinprotein interaction networks [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 10, "context": "In these domains, they can be used for diverse tasks, such as prediction of missing values given some observations [11], classification [22], and model selection [20].", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "In these domains, they can be used for diverse tasks, such as prediction of missing values given some observations [11], classification [22], and model selection [20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "In these domains, they can be used for diverse tasks, such as prediction of missing values given some observations [11], classification [22], and model selection [20].", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "Since in many models exact inference is infeasible, most studies resort to approximate inference such as variational approximations [12] and sampling [8].", "startOffset": 132, "endOffset": 136}, {"referenceID": 7, "context": "Since in many models exact inference is infeasible, most studies resort to approximate inference such as variational approximations [12] and sampling [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 5, "context": "Relational probabilistic models [6, 9, 18, 21] provide a language for defining how to construct models from reoccurring sub-components.", "startOffset": 32, "endOffset": 46}, {"referenceID": 8, "context": "Relational probabilistic models [6, 9, 18, 21] provide a language for defining how to construct models from reoccurring sub-components.", "startOffset": 32, "endOffset": 46}, {"referenceID": 17, "context": "Relational probabilistic models [6, 9, 18, 21] provide a language for defining how to construct models from reoccurring sub-components.", "startOffset": 32, "endOffset": 46}, {"referenceID": 19, "context": "Relational probabilistic models [6, 9, 18, 21] provide a language for defining how to construct models from reoccurring sub-components.", "startOffset": 32, "endOffset": 46}, {"referenceID": 3, "context": "This definition of a joint distribution is similar to standard log-linear models, except that all groundings of a template feature share the same parameter [4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 11, "context": "One broad class of approximate inference procedure are variational methods [12].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "Although the general idea we present here can be applied to almost all variational methods, for concreteness and simplicity we focus here on loopy belief propagation [16, 23] which is one of the most common approaches in the field.", "startOffset": 166, "endOffset": 174}, {"referenceID": 21, "context": "Although the general idea we present here can be applied to almost all variational methods, for concreteness and simplicity we focus here on loopy belief propagation [16, 23] which is one of the most common approaches in the field.", "startOffset": 166, "endOffset": 174}, {"referenceID": 13, "context": "To describe loopy belief propagation we consider the data structure of a factor graph [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 21, "context": "These beliefs are the approximation of the marginal probability over the variables in C\u03c9 [23].", "startOffset": 89, "endOffset": 93}, {"referenceID": 7, "context": "We compared exact inference, MCMC (Gibbs sampling) [8], standard asynchronous belief propagation [23], and compact belief propagation on the template-level model.", "startOffset": 51, "endOffset": 54}, {"referenceID": 21, "context": "We compared exact inference, MCMC (Gibbs sampling) [8], standard asynchronous belief propagation [23], and compact belief propagation on the template-level model.", "startOffset": 97, "endOffset": 101}, {"referenceID": 21, "context": "When performing loopy belief propagation, we can approximate the log-partition function using the Bethe approximation [23].", "startOffset": 118, "endOffset": 122}, {"referenceID": 3, "context": "To learn such parameters from real-life data we can use the Maximum Likelihood (ML) estimation [4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "is the sum of times we expect to see each grounding of the feature j according to \u0398 (see [4]).", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "To evaluate this learning procedure we start by generating samples from a model using a Gibbs sampler [8].", "startOffset": 102, "endOffset": 105}, {"referenceID": 10, "context": "in [11] for protein-protein interactions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "We reason about an instantiation for a set of 813 proteins related to DNA transcription and repair [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "We collected statistics over interactions between these proteins from various experiments [1, 7, 13, 15].", "startOffset": 90, "endOffset": 104}, {"referenceID": 6, "context": "We collected statistics over interactions between these proteins from various experiments [1, 7, 13, 15].", "startOffset": 90, "endOffset": 104}, {"referenceID": 12, "context": "We collected statistics over interactions between these proteins from various experiments [1, 7, 13, 15].", "startOffset": 90, "endOffset": 104}, {"referenceID": 14, "context": "We collected statistics over interactions between these proteins from various experiments [1, 7, 13, 15].", "startOffset": 90, "endOffset": 104}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "We repeated the same exploration technique for other features such as colocalization of proteins [11], star-2 and star-3 [10], and quadruplets of interactions (results not shown).", "startOffset": 97, "endOffset": 101}, {"referenceID": 9, "context": "We repeated the same exploration technique for other features such as colocalization of proteins [11], star-2 and star-3 [10], and quadruplets of interactions (results not shown).", "startOffset": 121, "endOffset": 125}, {"referenceID": 4, "context": "Note that other works show that synchronous and asynchronous belief propagation are not always equivalent [5].", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "[17] used the relational structure to cache repeated computations of intermediate terms that are identical in different instances of the same template.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Several recent works [3, 18] derive rules as to when variable elimination can be performed at the template level rather than the instance level, which saves duplicate computations at the instance levels.", "startOffset": 21, "endOffset": 28}, {"referenceID": 17, "context": "Several recent works [3, 18] derive rules as to when variable elimination can be performed at the template level rather than the instance level, which saves duplicate computations at the instance levels.", "startOffset": 21, "endOffset": 28}, {"referenceID": 9, "context": "Though the search space proves to be very difficult [10], our method allows us to perform many iterations of parameter estimation in different settings and thereby get a good overview of the likelihood landscape.", "startOffset": 52, "endOffset": 56}], "year": 2009, "abstractText": "Relational Markov Random Fields are a general and flexible framework for reasoning about the joint distribution over attributes of a large number of interacting entities. The main computational difficulty in learning such models is inference. Even when dealing with complete data, where one can summarize a large domain by sufficient statistics, learning requires one to compute the expectation of the sufficient statistics given different parameter choices. The typical solution to this problem is to resort to approximate inference procedures, such as loopy belief propagation. Although these procedures are quite efficient, they still require computation that is on the order of the number of interactions (or features) in the model. When learning a large relational model over a complex domain, even such approximations require unrealistic running time. In this paper we show that for a particular class of relational MRFs, which have inherent symmetry, we can perform the inference needed for learning procedures using a template-level belief propagation. This procedure\u2019s running time is proportional to the size of the relational model rather than the size of the domain. Moreover, we show that this computational procedure is equivalent to sychronous loopy belief propagation. This enables a dramatic speedup in inference and learning time. We use this procedure to learn relational MRFs for capturing the joint distribution of large protein-protein interaction networks.", "creator": "Adobe InDesign CS2 (4.0.4)"}}}