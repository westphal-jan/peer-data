{"id": "1206.6407", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Large-Scale Feature Learning With Spike-and-Slab Sparse Coding", "abstract": "methods implement the problem of structured comparison with a greater number of properties. in order actively integrate multiple low amount data labeled examples requiring considering this setting, microsoft created very new feature learning that reinforcement procedure tools on type factor model often call spike - and - slab sparse statistics ( s3c ). prior consideration on them still not prioritized allowing ability to introduce parallel architectures much unlike s3c eliminate the overall problem sizes needed below object recognition. we present comparatively flexible alignment procedure for appropriate for use with gpus whereby allows us to dramatically advantage both improving underlying set count and the magnitude of latent factors that consider challenges advantage trained implementing. we advise that my approach improves whereby the supervised learning capabilities of learning sparse coding uses the spike - ou - slab classical boltzmann machine ( cm ) on ieee cifar - 10 cluster. we assess my cifar - 100 dataset to tell that our method scales to large numbers larger instructions better than previous methods. finally, one use our method to win an nips 2011 focusing on challenges for learning hierarchical memory? transfer methods challenge.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (877kb)", "http://arxiv.org/abs/1206.6407v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap witharXiv:1201.3382"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap witharXiv:1201.3382", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ian j goodfellow", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1206.6407"}, "pdf": {"name": "1206.6407.pdf", "metadata": {"source": "CRF", "title": "Large-Scale Feature Learning With Spike-and-Slab Sparse Coding", "authors": ["Ian J. Goodfellow", "Aaron Courville", "Yoshua Bengio"], "emails": ["goodfeli.@iro.umontreal.ca", "Aaron.Courville@umontreal.ca", "Yoshua.Bengio@umontreal.ca"], "sections": [{"heading": "1. Introduction", "text": "We consider here the problem of unsupervised feature discovery for supervised learning. In supervised learning, one is given a set of examples V = {v(1), . . . , v(m)} and associated labels {y(1), . . . , y(m)}. The goal is to learn a model p(y | v) so that new labels can be predicted from new unlabeled examples v.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nThe idea behind unsupervised feature discovery is that the final learning problem can become much easier if the problem is represented in the right way. By learning the structure of V we can discover a feature mapping \u03c6(v) that can be used to preprocess the data prior to running a standard supervised learning algorithm, such as an SVM.\nThere has been a great deal of recent interest in investigating different unsupervised learning schemes to train \u03c6 from V . In particular, the goal of deep learning (Bengio, 2009) is to learn a function \u03c6 that consists of many layers of processing, each of which receives the previous layers as input and incrementally disentangles the factors of variation in the data. Deep learning systems are usually created by composing together several shallow unsupervised feature learners. Examples of shallow models applied to feature discovery include sparse coding (Raina et al., 2007), restricted Boltzmann machines (RBMs) (Hinton et al., 2006; Courville et al., 2011b), various autoencoderbased models (Bengio et al., 2007), and hybrids of autoencoders and sparse coding (Kavukcuoglu et al., 2010). In the context of probabilistic generative models, such as the RBM, \u03c6(v) is typically taken to be the conditional expectation of the latent variables, and the process of learning \u03c6 consists simply of fitting the generative model to V .\nSingle-layer convolutional models based on simple feature extractors currently achieve state-of-the-art performance on the CIFAR-10 object recognition dataset (Coates and Ng, 2011; Jia and Huang, 2011). It is known that the best models for the detection layer of the convolutional model do not perform well when fewer labeled examples are available (Coates and Ng, 2011). In particular, sparse coding outperforms a simple thresholded linear feature extractor when the number of labeled examples decreases. Our objective is to further improve performance when the number of labeled examples is low by introducing a new feature extraction procedure based on spike-and-slab sparse\ncoding. We hypothesize that these features have a stronger regularizing effect than sparse coding features. Their superior performance with low numbers of labeled examples allows us to improve performance on datasets with high numbers of classes and low numbers of labeled examples.\nIn this paper we overcome two major scaling challenges. First we scale inference in the spike-andslab coding model to work for the large problem sizes required for object recognition. We then use the enhanced regularization properties of spike-and-slab sparse coding to scale object recognition techniques to work with large numbers of classes and small amounts of labeled data."}, {"heading": "2. The Spike-and-Slab Sparse Coding model", "text": "The Spike-and-Slab Sparse Coding (S3C) model consists of latent binary spike variables h \u2208 {0, 1}N , latent real-valued slab variables s \u2208 RN , and real-valued Ddimensional visible vector v \u2208 RD generated according to this process:\n\u2200i \u2208 {1, . . . , N}, d \u2208 {1, . . . , D},\np(hi = 1) = \u03c3(bi)\np(si | hi) = N (si | hi\u00b5i, \u03b1\u22121ii ) (1) p(vd | s, h) = N (vd |Wd:(h \u25e6 s), \u03b2\u22121dd )\nwhere \u03c3 is the logistic sigmoid function, b is a set of biases on the spike variables, \u00b5 and W govern the linear dependence of s on h and v on s respectively, \u03b1 and \u03b2 are diagonal precision matrices of their respective conditionals, and h \u25e6 s denotes the element-wise product of h and s.\nTo avoid overparameterizing the distribution, we constrain the columns ofW to have unit norm, as in sparse coding. We restrict \u03b1 to be a diagonal matrix and \u03b2 to be a diagonal matrix or a scalar. We refer to the variables hi and si as jointly defining the ith hidden unit, so that there are a total of N rather than 2N hidden units. The state of a hidden unit is best understood as hisi, that is, the spike variables gate the slab variables.\nIn the subsequent sections we motivate our use of S3C as a feature discovery algorithm by describing how this model occupies a middle ground between sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM). The S3C model avoids many disadvantages that the ssRBM and sparse coding have when applied as feature discovery algorithms."}, {"heading": "2.1. Comparison to sparse coding", "text": "Sparse coding has been widely used to discover features for classification (Raina et al., 2007). Recently Coates and Ng (2011) showed that this approach achieves excellent performance on the CIFAR-10 object recognition dataset.\nSparse coding (Olshausen and Field, 1997) describes a class of generative models where the observed data v is normally distributed given a set of continuous latent variables s and a dictionary matrix W : v \u223c N (Ws, \u03c3I). Sparse coding places a factorial prior on s such as a Cauchy or Laplace distribution, chosen to encourage the mode of the posterior p(s | v) to be sparse. One can derive the S3C model from sparse coding by replacing the factorial Cauchy or Laplace prior with a spike-and-slab prior.\nOne drawback of sparse coding is that the latent variables are not merely encouraged to be sparse; they are encouraged to remain close to 0, even when they are active. This kind of regularization is not necessarily undesirable, but in the case of simple but popular priors such as the Laplace prior (corresponding to an L1 penalty on the latent variables s), the degree of regularization on active units is confounded with the degree of sparsity. There is little reason to believe that in realistic settings, these two types of complexity control should be so tightly bound together. The S3C model avoids this issue by controlling the sparsity of units via the b parameter that determines how likely each spike unit is to be active, while separately controlling the magnitude of active units via the \u00b5 and \u03b1 parameters that govern the distribution over s. Sparse coding has no parameter analogous to \u00b5 and cannot control these aspects of the posterior independently.\nAnother drawback of sparse coding is that the factors are not actually sparse in the generative distribution. Indeed, each factor is zero with probability zero. The features extracted by sparse coding are only sparse because they are obtained via MAP inference. In the S3C model, the spike variables ensure that each factor is zero with non-zero probability in the generative distribution. Since this places a greater restriction on the code variables, we hypothesize that S3C features provide more of a regularizing effect when solving classification problems.\nSparse coding is also difficult to integrate into a deep generative model of data such as natural images. While Yu et al. (2011) and Zeiler et al. (2011) have recently shown some success at learning hierarchical sparse coding, our goal for our future work is to integrate the feature extraction scheme into a proven\ngenerative model framework such as the deep Boltzmann machine (Salakhutdinov and Hinton, 2009). Such models with their combination of feed-forward and feed-back connections during inference can learn a much richer description of the data than simple stacked feed-forward models. We expect that being able to extract such complicated structure during unsupervised learning on a large number of unlabeled examples will yield even better performance with high numbers of classes and low numbers of labels per class than a feed-forward architecture. Existing inference schemes known to work well in the DBM-type setting are all either sample-based or are based on variational approximations to the model posteriors, while sparse coding schemes typically employ MAP inference. Our use of variational inference makes the S3C framework well-suited to integrate into the known successful strategies for learning and inference in DBM models. It is not obvious how one can employ a variational inference strategy to standard sparse coding with the goal of achieving sparse feature encoding."}, {"heading": "2.2. Comparison to Restricted Boltzmann Machines", "text": "The S3C model also resembles another class of models commonly used for feature discovery: the RBM. An RBM (Smolensky, 1986) is a model defined through an energy function that describes the interactions between the observed data variables and a set of latent variables. It is possible to interpret the S3C as an energy-based model, by rearranging p(v, s, h) to take the form exp{\u2212E(v, s, h)}/Z, with the following energy function:\nE(v, s, h) = 1\n2\nv \u2212 X i Wisihi\n!T \u03b2 v \u2212\nX i Wisihi\n!\n+ 1\n2 NX i=1 \u03b1i(si \u2212 \u00b5ihi)2 \u2212 NX i=1 bihi, (2)\nThe ssRBM model family is a good starting point for S3C because it has demonstrated both reasonable performance as a feature discovery scheme and remarkable performance as a generative model (Courville et al., 2011a). Within the ssRBM family, S3C\u2019s closest relative is a variant of the \u00b5-ssRBM, defined by the following energy function:\nE(v, s, h) = \u2212 NX i=1 vT\u03b2Wisihi + 1 2 vT\u03b2v\n+ 1\n2 NX i=1 \u03b1i(si \u2212 \u00b5ihi)2 \u2212 NX i=1 bihi, (3)\nwhere the variables and parameters are defined identi-\ncally to the S3C. Comparison of equations 2 and 3 reveals that the simple addition of a latent factor interaction term 12 (h\u25e6s)\nTWT\u03b2W (h\u25e6s) to the ssRBM energy function turns the ssRBM into the S3C model. With the inclusion of this term S3C moves from an undirected ssRBM model to the directed graphical model described in equation (1). One can think of this term as designed to cancel the interactions in the RBM\u2019s marginal p(h, s) that make the RBM\u2019s partition function intractable. This change from undirected modeling to directed modeling has three important effects, that we describe in the following paragraphs:\nThe effect on the partition function: The most immediate consequence of the transition to directed modeling is that the partition function becomes tractable. This changes the nature of learning algorithms that can be applied to the model, since most of the difficulty in training an RBM comes from estimating the gradient of the log partition function. The partition function of S3C is also guaranteed to exist for all possible settings of the model parameters, which is not true of the ssRBM.\nThe effect on the posterior: RBMs have a factorial posterior, but S3C and sparse coding have a complicated posterior due to the \u201cexplaining away\u201d effect. This means that for RBMs, features defined by similar basis functions will have similar activations, while in directed models, similar features will compete so that only the most relevant feature will remain active. As shown by Coates and Ng (2011), the sparse Gaussian RBM is not a very good feature extractor \u2013 the set of basis functions W learned by the RBM actually work better for supervised learning when these parameters are plugged into a sparse coding model than when the RBM itself is used for feature extraction. We think this is due to the factorial posterior. In the vastly overcomplete setting, being able to selectively activate a small set of features that cooperate to explain the input likely provides S3C a major advantage in discriminative capability.\nThe effect on the prior: The addition of the interaction term causes S3C to have a factorial prior. This probably makes it a poor generative model, but this is not a problem for the purpose of feature discovery."}, {"heading": "3. Other Related work", "text": "The notion of a spike-and-slab prior was established in statistics by Mitchell and Beauchamp (1988). Outside the context of unsupervised feature discovery for supervised learning, the basic form of the S3C model (i.e. a spike-and-slab latent factor model) has ap-\npeared a number of times in different domains (Lu\u0308cke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and La\u0301zaro-Gredilla, 2011). In most work, the model varies slightly from S3C. For example, Titsias and La\u0301zaroGredilla (2011) share a single spike activation probability parameter across all spike variables. Lu\u0308cke and Sheikh (2011) use exactly the S3C model, but use intractable exact inference. To this literature, we contribute an approximate inference scheme that scales to the kinds of object classifications tasks that we consider. We outline this inference scheme next."}, {"heading": "4. Variational EM for S3C", "text": "Having explained why S3C is a powerful model for unsupervised feature discovery we turn to the problem of how to perform learning and inference in this model. Because computing the exact posterior distribution is intractable, we derive an efficient and effective inference mechanism and a variational EM learning algorithm.\nWe turn to variational EM (Saul and Jordan, 1996) because this algorithm is well-suited for models with latent variables whose posterior is intractable. It works by maximizing a variational lower bound on the loglikelihood called the energy functional (Neal and Hinton, 1999). More specifically, it is a variant of the EM algorithm with the modification that in the Estep, we compute a variational approximation to the posterior rather than the posterior itself. While our model admits a closed-form solution to the M-step, we found that online learning with small gradient steps on the M-step objective worked better in practice. We therefore focus our presentation on the E-step, given in Algorithm 1.\nThe goal of the variational E-step is to maximize the energy functional with respect to a distribution Q over the unobserved variables. We can do this by selecting the Q that minimizes the Kullback\u2013Leibler divergence:\nDKL(Q(h, s)\u2016P (h, s|v)) (4)\nwhere Q(h, s) is drawn from a restricted family of distributions. This family can be chosen to ensure that Q is tractable.\nOur E-step can be seen as analogous to the encoding step of the sparse coding algorithm. The key difference is that while sparse coding approximates the true posterior with a MAP point estimate of the latent variables, we approximate the true posterior with the distribution Q.\nWe use the family Q(h, s) = \u03a0iQ(hi, si). This is a\nricher approximation than the fully factorized family used in the mean field approximation. It allows us to capture the tight correlation between each spike variable and its corresponding slab variable while still allowing simple and efficient inference in the approximating distribution. It also avoids a pathological condition in the mean field distribution where Q(si) can never be updated if Q(hi) = 0.\nObserving that eq. (4) is an instance of the EulerLagrange equation, we find that the solution must take the form\nQ(hi) = h\u0302i,\nQ(si | hi) = N (si | his\u0302i, (\u03b1i + hiWTi \u03b2Wi)\u22121) (5)\nwhere h\u0302i and s\u0302i must be found by an iterative process. In a typical application of variational inference, the iterative process consists of sequentially applying fixed point equations that give the optimal value of the parameters h\u0302i and s\u0302i for one factor Q(hi, si) given\nthe value all of the other factors\u2019 parameters. This is for example the approach taken by Titsias and La\u0301zaroGredilla (2011) who independently developed a variational inference procedure for the same problem. This process is only guaranteed to decrease the KL divergence if applied to each factor sequentially, i.e. first updating h\u03021 and s\u03021 to optimize Q(h1, s1), then updating h\u03022 and s\u03022 to optimize Q(h2, s2), and so on. In a typical application of variational inference, the optimal values for each update are simply given by the solutions to the Euler-Lagrange equations. For S3C, we make three deviations from this standard approach.\nBecause we apply S3C to very large-scale problems, we need an algorithm that can fully exploit the benefits of parallel hardware such as GPUs. Sequential updates across all N factors require far too much run-time to be competitive in this regime.\nWe have considered two different methods that enable parallel updates to all units. In the first method, we start each iteration by partially minimizing the KL divergence with respect to s\u0302. The terms of the KL divergence that depend on s\u0302 make up a quadratic function so this can be minimized via conjugate gradient descent. We implement conjugate gradient descent efficiently by using the R-operator to perform Hessianvector products rather than computing the entire Hessian explicitly (Schraudolph, 2002). This step is guaranteed to improve the KL divergence on each iteration. We next update h\u0302 in parallel, shrinking the update by a damping coefficient. This approach is not guaranteed to decrease the KL divergence on each iteration but it is a widely applied approach that works well in practice (Koller and Friedman, 2009).\nWith the second method (Algorithm 1), we find in practice that we obtain faster convergence, reaching equally good solutions by replacing the conjugate gradient update to s\u0302 with a more heuristic approach. We use a parallel damped update on s\u0302 much like what we do for h\u0302. In this case we make an additional heuristic modification to the update rule which is made necessary by the unbounded nature of s\u0302. We clip the update to s\u0302 so that if s\u0302new has the opposite sign from s\u0302, its magnitude is at most \u03c1s\u0302. In all of our experiments we used \u03c1 = 0.5 but any value in [0, 1] is sensible. This prevents a case where multiple mutually inhibitory s units inhibit each other so strongly that rather than being driven to 0 they change sign and actually increase in magnitude. This case is a failure mode of the parallel updates that can result in s\u0302 amplifying without bound if clipping is not used.\nFigure 1 shows that our E-step produces a sparse representation. Figure 2 shows that the explaining-away\neffect incrementally makes the representation more sparse.\nAlgorithm 1 Fixed-Point Inference Let K be a user-defined number of inference updates to run (e.g. 20)\nInitialize h\u0302(0) = \u03c3(b) and s\u0302(0) = \u00b5. for k=0:K do\nCompute the individually optimal value s\u0302\u2217i for each i simultaneously:\ns\u0302 \u2217 i =\n\u00b5i\u03b1ii + v T \u03b2Wi \u2212Wi\u03b2 hP j 6=i Wj h\u0302j s\u0302 (k) j i \u03b1ii +W T i \u03b2Wi\nClip reflections by assigning\nci = \u03c1sign(s\u0302 \u2217 i )|s\u0302 (k) i |\nfor all i such that sign(s\u0302\u2217i ) 6= sign(s\u0302 (k) i ) and |s\u0302 \u2217 i | > \u03c1|s\u0302 (k) i |, and assigning ci = s\u0302 \u2217 i for all other i. Damp the updates by assigning\ns\u0302 (k+1) i = \u03b7c + (1\u2212 \u03b7)s\u0302 (k)\nwhere \u03b7 \u2208 (0, 1]. Compute the individually optimal values for h\u0302:\nh\u0302 \u2217 i = \u03c3 0@0@v \u2212X j 6=i Wj s\u0302 (k+1) j h\u0302 (k) j \u2212 1 2 Wis\u0302 (k+1) i 1AT \u03b2Wis\u0302(k+1)i + bi \u2212 1\n2 \u03b1ii(s\u0302\n(k+1) i \u2212 \u00b5i) 2 \u2212 1\n2 log(\u03b1ii +W\nT i \u03b2Wi) +\n1 2 log(\u03b1ii)\n\u00ab\nDamp the update to h\u0302:\nh\u0302 (k+1) = \u03b7h\u0302 \u2217 + (1\u2212 \u03b7)h\u0302(k)\nend for"}, {"heading": "5. Performance results", "text": "Our inference scheme achieves very good computational performance, both in terms of memory consumption and in terms of runtime. The computational bottleneck in our classification pipeline is SVM training, not feature learning or feature extraction.\nComparing the computational cost of our inference scheme to others is a difficult task because it could be confounded by differences in implementation and because it is not clear exactly what sparse coding prob-\nlem is equivalent to an equivalent spike-and-slab sparse coding problem. However, we observed informally during our supervised learning experiments that feature extraction using S3C took roughly the same amount of time as feature extraction using sparse coding.\nIn Fig. 4, we show that our improvements to spikeand-slab inference performance allow us to scale spikeand-slab modeling to the problem sizes needed for object recognition tasks.\nAs a large-scale test of our inference scheme\u2019s ability, we trained over 8,000 densely-connected filters on full 32 \u00d7 32 color images. Some example filters are presented in Fig. 5. This exercise demonstrated that our approach scales well to large (over 3,000 dimensional) inputs, though it is not yet known how to use features for classification as effectively as patch-based features which can be incorporated into a convolutional architecture with pooling. For comparison, to our knowledge the largest image patches used in previous spikeand-slab models with lateral interactions were 16\u00d7 16 (Garrigues and Olshausen, 2008).\nFinally, we provide empirical justification for our heuristic inference method. Timing experiments presented in Fig. 6 show that the heuristic method is consistently faster than the conjugate gradient method."}, {"heading": "6. Classification results", "text": "We conducted experiments to evaluate the usefulness of S3C features for supervised learning on the CIFAR10 and CIFAR-100 (Krizhevsky and Hinton, 2009) datasets. Both datasets consist of color images of objects such as animals and vehicles. Each contains\n50,000 train and 10,000 test examples. CIFAR-10 contains 10 classes while CIFAR-100 contains 100 classes, so there are fewer labeled examples per class in the case of CIFAR-100.\nFor all experiments, we used the same overall procedure as Coates and Ng (2011) except for feature learning. CIFAR-10 consists of 32 \u00d7 32 images. We train our feature extractor on 6\u00d76 contrast-normalized and ZCA-whitened patches from the training set. At test time, we extract features from all 6\u00d7 6 patches on an image, then average-pool them. The average-pooling regions are arranged on a non-overlapping grid. Finally, we train an L2-SVM with a linear kernel on the pooled features.\nCoates and Ng (2011) used 1600 basis vectors in all of their sparse coding experiments. They post-processed the sparse coding feature vectors by splitting them into the positive and negative part for a total of 3200 features per average-pooling region. They average-pool on a 2\u00d7 2 grid for a total of 12,800 features per image (i.e. each element of the 2 \u00d7 2 grid averages over a block with sides d(32\u2212 6 + 1)/2e or b(32\u2212 6 + 1)/2c). We used EQ[h] as our feature vector. This does not\nhave a negative part, so using a 2 \u00d7 2 grid we would have only 6,400 features. In order to compare with similar sizes of feature vectors we used a 3\u00d7 3 pooling grid for a total of 14,400 features (i.e. each element of the 3\u00d7 3 grid averages over 9\u00d7 9 locations)."}, {"heading": "6.1. CIFAR-10", "text": "We use CIFAR-10 to evaluate our hypothesis that S3C resembles a more regularized version of sparse coding.\nOn the full dataset, S3C achieves a test set accuracy of 78.3 \u00b1 0.9 % with 95% confidence. Coates and Ng (2011) do not report test set accuracy for sparse coding with \u201cnatural encoding\u201d (i.e., extracting features in a model whose parameters are all the same as in the model used for training) but sparse coding with different parameters for feature extraction than training achieves an accuracy of 78.8 \u00b1 0.9% (Coates and Ng, 2011). Since we have not enhanced our performance by modifying parameters at feature extraction time these results seem to indicate that S3C is roughly equivalent to sparse coding for this classification task. S3C also outperforms ssRBMs, which require 4,096 basis vectors per patch and a 3\u00d73 pooling grid to achieve 76.7 \u00b1 0.9% accuracy. All of these approaches are close to the best result using the pipeline from Coates and Ng (2011) of 81.5% achieved using thresholding of linear features learned with OMP-1. These results show that S3C is a useful feature extractor that performs comparably to the best approaches when large amounts of labeled data are available.\nWe then tested the regularizing effect of S3C by training the SVM on small subsets of the CIFAR-10 training set, but using features that were learned on patches drawn from the entire CIFAR-10 train set. The results, summarized in Figure 3, show that S3C has the advantage over both thresholding and sparse coding for a wide range of amounts of labeled data. (In the extreme low-data limit, the confidence interval becomes too large to distinguish sparse coding from S3C)."}, {"heading": "6.2. CIFAR-100", "text": "Having verified that S3C features help to regularize a classifier, we proceed to use them to improve performance on the CIFAR-100 dataset, which has ten times\nas many classes and ten times fewer labeled examples per class.\nWe compare S3C to two other feature extraction methods: OMP-1 with thresholding, which Coates and Ng (2011) found to be the best feature extractor on CIFAR-10, and sparse coding, which is known to perform well when less labeled data is available. We evaluated only a single set of hyperparameters for S3C. For sparse coding and OMP-1 we searched over the same set of hyperparameters as Coates and Ng (2011) did: {0.5, 0.75, 1.0, 1.25, 1.25} for the sparse coding penalty and {0.1, 0.25, 0.5, 1.0} for the thresholding value. In order to use a comparable amount of computational resources in all cases, we used 1600 hidden units and a 3 \u00d7 3 pooling grid for S3C, while for the other two methods, which double their number of features via sign splitting, we considered 2 \u00d7 2 pooling with 1600 latent variables and 3\u00d73 pooling with 800 latent variables. These results are summarized in Table 1.\nThe best result to our knowledge on CIFAR-100 is 54.8 \u00b1 1% (Jia and Huang, 2011), achieved using a learned pooling structure on top of \u201ctriangle code\u201d features from a dictionary learned using k-means. This feature extractor is very similar to thresholded OMP1 features and is known to perform slightly worse on CIFAR-10. Table 1 shows that S3C is the best known detector layer on CIFAR-100. If combined with the pooling strategy of Jia and Huang (2011) it has the potential to improve on the state of the art. Using a pooling strategy of concatenating 1 \u00d7 1, 2 \u00d7 2 and 3\u00d7 3 pooled features we achieve a test set accuracy of 53.7\u00b1 1%."}, {"heading": "7. Transfer Learning Challenge", "text": "For the NIPS 2011 Workshop on Challenges in Learning Hierarchical Models (Le et al., 2011), the organizers proposed a transfer learning competition. This competition used a dataset consisting of 32 \u00d7 32 color images, including 100,000 unlabeled examples, 50,000 labeled examples of 100 object classes not present in the test set, and 120 labeled examples of 10 object classes present in the test set. We applied the same approach as on the CIFAR datasets and won the com-\npetition, with a test set accuracy of 48.6 %. This approach disregards the 50,000 labels and treats this transfer learning problem as a semi-supervised learning problem. We experimented with some transfer learning techniques but the transfer-free approach performed best on leave-one-out cross-validation on the 120 example training set, so we chose to enter the transfer-free technique in the challenge."}, {"heading": "8. Conclusion", "text": "We have motivated the use of the S3C model for unsupervised feature discovery. We have described a variational approximation scheme that makes it feasible to perform learning and inference in large-scale S3C models. We have demonstrated that S3C is an effective feature discovery algorithm for both supervised and semi-supervised learning with small amounts of labeled data. This work addresses two scaling problems: the computation problem of scaling spike-andslab sparse coding to the problem sizes used in object recognition, and the problem of scaling object recognition techniques to work with more classes."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In Adv. Neural Inf. Proc. Sys", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "In ICML\u20192011,", "citeRegEx": "Coates and Ng.,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng.", "year": 2011}, {"title": "Unsupervised models of images by spike-and-slab RBMs", "author": ["A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In Proceedings of the Twenty-eight International Conference on Machine Learning", "citeRegEx": "Courville et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Courville et al\\.", "year": 2011}, {"title": "A Spike and Slab Restricted Boltzmann Machine", "author": ["A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In AISTATS\u20192011,", "citeRegEx": "Courville et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Courville et al\\.", "year": 2011}, {"title": "Learning horizontal connections in a sparse coding model of natural images", "author": ["P. Garrigues", "B. Olshausen"], "venue": "In NIPS\u201920", "citeRegEx": "Garrigues and Olshausen.,? \\Q2008\\E", "shortCiteRegEx": "Garrigues and Olshausen.", "year": 2008}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Beyond spatial pyramids: Receptive field learning for pooled image features", "author": ["Y. Jia", "C. Huang"], "venue": null, "citeRegEx": "Jia and Huang.,? \\Q2011\\E", "shortCiteRegEx": "Jia and Huang.", "year": 2011}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.-L. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"], "venue": "In NIPS\u201910,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman.", "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "A closed-form EM algorithm for sparse coding", "author": ["J. L\u00fccke", "A.-S. Sheikh"], "venue": null, "citeRegEx": "L\u00fccke and Sheikh.,? \\Q2011\\E", "shortCiteRegEx": "L\u00fccke and Sheikh.", "year": 2011}, {"title": "Bayesian variable selection in linear regression", "author": ["T.J. Mitchell", "J.J. Beauchamp"], "venue": "J. Amer. Statistical Assoc.,", "citeRegEx": "Mitchell and Beauchamp.,? \\Q1988\\E", "shortCiteRegEx": "Mitchell and Beauchamp.", "year": 1988}, {"title": "Bayesian and l1 approaches to sparse unsupervised learning", "author": ["S. Mohamed", "K. Heller", "Z. Ghahramani"], "venue": null, "citeRegEx": "Mohamed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2011}, {"title": "A view of the em algorithm that justifies incremental, sparse, and other variants", "author": ["R. Neal", "G. Hinton"], "venue": null, "citeRegEx": "Neal and Hinton.,? \\Q1999\\E", "shortCiteRegEx": "Neal and Hinton.", "year": 1999}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by V1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1997}, {"title": "Selftaught learning: transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "In Z. Ghahramani, editor,", "citeRegEx": "Raina et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raina et al\\.", "year": 2007}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proc. AISTATS\u20192009,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Exploiting tractable substructures in intractable networks. In NIPS\u201995", "author": ["L.K. Saul", "M.I. Jordan"], "venue": null, "citeRegEx": "Saul and Jordan.,? \\Q1996\\E", "shortCiteRegEx": "Saul and Jordan.", "year": 1996}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["N.N. Schraudolph"], "venue": "Neural Computation,", "citeRegEx": "Schraudolph.,? \\Q2002\\E", "shortCiteRegEx": "Schraudolph.", "year": 2002}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "Parallel Distributed Processing,", "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}, {"title": "Spike and slab variational inference for multi-task and multiple kernel learning", "author": ["M.K. Titsias", "M. L\u00e1zaro-Gredilla"], "venue": null, "citeRegEx": "Titsias and L\u00e1zaro.Gredilla.,? \\Q2011\\E", "shortCiteRegEx": "Titsias and L\u00e1zaro.Gredilla.", "year": 2011}, {"title": "Learning image representations from the pixel level via hierarchical sparse coding", "author": ["K. Yu", "Y. Lin", "J. Lafferty"], "venue": "In CVPR,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M. Zeiler", "G. Taylor", "R. Fergus"], "venue": "In ICML,", "citeRegEx": "Zeiler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}, {"title": "Non-parametric Bayesian dictionary learning for sparse image representations", "author": ["M. Zhou", "H. Chen", "J.W. Paisley", "L. Ren", "G. Sapiro", "L. Carin"], "venue": "In NIPS\u201909,", "citeRegEx": "Zhou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "In particular, the goal of deep learning (Bengio, 2009) is to learn a function \u03c6 that consists of many layers of processing, each of which receives the previous layers as input and incrementally disentangles the factors of variation in the data.", "startOffset": 41, "endOffset": 55}, {"referenceID": 16, "context": "Examples of shallow models applied to feature discovery include sparse coding (Raina et al., 2007), restricted Boltzmann machines (RBMs) (Hinton et al.", "startOffset": 78, "endOffset": 98}, {"referenceID": 6, "context": ", 2007), restricted Boltzmann machines (RBMs) (Hinton et al., 2006; Courville et al., 2011b), various autoencoderbased models (Bengio et al.", "startOffset": 46, "endOffset": 92}, {"referenceID": 1, "context": ", 2011b), various autoencoderbased models (Bengio et al., 2007), and hybrids of autoencoders and sparse coding (Kavukcuoglu et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 8, "context": ", 2007), and hybrids of autoencoders and sparse coding (Kavukcuoglu et al., 2010).", "startOffset": 55, "endOffset": 81}, {"referenceID": 2, "context": "Single-layer convolutional models based on simple feature extractors currently achieve state-of-the-art performance on the CIFAR-10 object recognition dataset (Coates and Ng, 2011; Jia and Huang, 2011).", "startOffset": 159, "endOffset": 201}, {"referenceID": 7, "context": "Single-layer convolutional models based on simple feature extractors currently achieve state-of-the-art performance on the CIFAR-10 object recognition dataset (Coates and Ng, 2011; Jia and Huang, 2011).", "startOffset": 159, "endOffset": 201}, {"referenceID": 2, "context": "It is known that the best models for the detection layer of the convolutional model do not perform well when fewer labeled examples are available (Coates and Ng, 2011).", "startOffset": 146, "endOffset": 167}, {"referenceID": 16, "context": "Sparse coding has been widely used to discover features for classification (Raina et al., 2007).", "startOffset": 75, "endOffset": 95}, {"referenceID": 2, "context": "Recently Coates and Ng (2011) showed that this approach achieves excellent performance on the CIFAR-10 object recognition dataset.", "startOffset": 9, "endOffset": 30}, {"referenceID": 15, "context": "Sparse coding (Olshausen and Field, 1997) describes a class of generative models where the observed data v is normally distributed given a set of continuous latent variables s and a dictionary matrix W : v \u223c N (Ws, \u03c3I).", "startOffset": 14, "endOffset": 41}, {"referenceID": 22, "context": "While Yu et al. (2011) and Zeiler et al.", "startOffset": 6, "endOffset": 23}, {"referenceID": 22, "context": "While Yu et al. (2011) and Zeiler et al. (2011) have recently shown some success at learning hierarchical sparse coding, our goal for our future work is to integrate the feature extraction scheme into a proven", "startOffset": 6, "endOffset": 48}, {"referenceID": 17, "context": "generative model framework such as the deep Boltzmann machine (Salakhutdinov and Hinton, 2009).", "startOffset": 62, "endOffset": 94}, {"referenceID": 20, "context": "An RBM (Smolensky, 1986) is a model defined through an energy function that describes the interactions between the observed data variables and a set of latent variables.", "startOffset": 7, "endOffset": 24}, {"referenceID": 2, "context": "As shown by Coates and Ng (2011), the sparse Gaussian RBM is not a very good feature extractor \u2013 the set of basis functions W learned by the RBM actually work better for supervised learning when these parameters are plugged into a sparse coding model than when the RBM itself is used for feature extraction.", "startOffset": 12, "endOffset": 33}, {"referenceID": 12, "context": "The notion of a spike-and-slab prior was established in statistics by Mitchell and Beauchamp (1988). Outside the context of unsupervised feature discovery for supervised learning, the basic form of the S3C model (i.", "startOffset": 70, "endOffset": 100}, {"referenceID": 11, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011).", "startOffset": 46, "endOffset": 177}, {"referenceID": 5, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011).", "startOffset": 46, "endOffset": 177}, {"referenceID": 13, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011).", "startOffset": 46, "endOffset": 177}, {"referenceID": 24, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011).", "startOffset": 46, "endOffset": 177}, {"referenceID": 21, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011).", "startOffset": 46, "endOffset": 177}, {"referenceID": 5, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011). In most work, the model varies slightly from S3C. For example, Titsias and L\u00e1zaroGredilla (2011) share a single spike activation probability parameter across all spike variables.", "startOffset": 71, "endOffset": 276}, {"referenceID": 5, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011). In most work, the model varies slightly from S3C. For example, Titsias and L\u00e1zaroGredilla (2011) share a single spike activation probability parameter across all spike variables. L\u00fccke and Sheikh (2011) use exactly the S3C model, but use intractable exact inference.", "startOffset": 71, "endOffset": 382}, {"referenceID": 18, "context": "We turn to variational EM (Saul and Jordan, 1996) because this algorithm is well-suited for models with latent variables whose posterior is intractable.", "startOffset": 26, "endOffset": 49}, {"referenceID": 14, "context": "It works by maximizing a variational lower bound on the loglikelihood called the energy functional (Neal and Hinton, 1999).", "startOffset": 99, "endOffset": 122}, {"referenceID": 19, "context": "We implement conjugate gradient descent efficiently by using the R-operator to perform Hessianvector products rather than computing the entire Hessian explicitly (Schraudolph, 2002).", "startOffset": 162, "endOffset": 181}, {"referenceID": 9, "context": "This approach is not guaranteed to decrease the KL divergence on each iteration but it is a widely applied approach that works well in practice (Koller and Friedman, 2009).", "startOffset": 144, "endOffset": 171}, {"referenceID": 5, "context": "For comparison, to our knowledge the largest image patches used in previous spikeand-slab models with lateral interactions were 16\u00d7 16 (Garrigues and Olshausen, 2008).", "startOffset": 135, "endOffset": 166}, {"referenceID": 2, "context": "Previous object recognition work is from (Coates and Ng, 2011; Courville et al., 2011a).", "startOffset": 41, "endOffset": 87}, {"referenceID": 10, "context": "We conducted experiments to evaluate the usefulness of S3C features for supervised learning on the CIFAR10 and CIFAR-100 (Krizhevsky and Hinton, 2009) datasets.", "startOffset": 121, "endOffset": 150}, {"referenceID": 2, "context": "For all experiments, we used the same overall procedure as Coates and Ng (2011) except for feature learning.", "startOffset": 59, "endOffset": 80}, {"referenceID": 2, "context": "9% (Coates and Ng, 2011).", "startOffset": 3, "endOffset": 24}, {"referenceID": 2, "context": "Coates and Ng (2011) do not report test set accuracy for sparse coding with \u201cnatural encoding\u201d (i.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Coates and Ng (2011) do not report test set accuracy for sparse coding with \u201cnatural encoding\u201d (i.e., extracting features in a model whose parameters are all the same as in the model used for training) but sparse coding with different parameters for feature extraction than training achieves an accuracy of 78.8 \u00b1 0.9% (Coates and Ng, 2011). Since we have not enhanced our performance by modifying parameters at feature extraction time these results seem to indicate that S3C is roughly equivalent to sparse coding for this classification task. S3C also outperforms ssRBMs, which require 4,096 basis vectors per patch and a 3\u00d73 pooling grid to achieve 76.7 \u00b1 0.9% accuracy. All of these approaches are close to the best result using the pipeline from Coates and Ng (2011) of 81.", "startOffset": 0, "endOffset": 772}, {"referenceID": 2, "context": "We compare S3C to two other feature extraction methods: OMP-1 with thresholding, which Coates and Ng (2011) found to be the best feature extractor on CIFAR-10, and sparse coding, which is known to perform well when less labeled data is available.", "startOffset": 87, "endOffset": 108}, {"referenceID": 2, "context": "We compare S3C to two other feature extraction methods: OMP-1 with thresholding, which Coates and Ng (2011) found to be the best feature extractor on CIFAR-10, and sparse coding, which is known to perform well when less labeled data is available. We evaluated only a single set of hyperparameters for S3C. For sparse coding and OMP-1 we searched over the same set of hyperparameters as Coates and Ng (2011) did: {0.", "startOffset": 87, "endOffset": 407}, {"referenceID": 7, "context": "8 \u00b1 1% (Jia and Huang, 2011), achieved using a learned pooling structure on top of \u201ctriangle code\u201d features from a dictionary learned using k-means.", "startOffset": 7, "endOffset": 28}, {"referenceID": 7, "context": "8 \u00b1 1% (Jia and Huang, 2011), achieved using a learned pooling structure on top of \u201ctriangle code\u201d features from a dictionary learned using k-means. This feature extractor is very similar to thresholded OMP1 features and is known to perform slightly worse on CIFAR-10. Table 1 shows that S3C is the best known detector layer on CIFAR-100. If combined with the pooling strategy of Jia and Huang (2011) it has the potential to improve on the state of the art.", "startOffset": 8, "endOffset": 401}], "year": 2012, "abstractText": "We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models\u2019 Transfer Learning Challenge.", "creator": "TeX"}}}