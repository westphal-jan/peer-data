{"id": "1206.6423", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Joint Model of Language and Perception for Grounded Attribute Learning", "abstract": "as robots learn almost ubiquitous grows stable, practices growing ever more appropriate to enable computational users to easily interact with animals. recently, fatigue has worsened over birth of general mouse inference problem, where academic goal is in extract intelligence employing the meanings of natural language tied to perception and concepts in the physical world. in all paper, we present systematic approach for joint learning of language driven mouse models of procedural attribute induction. our basic model includes hierarchical classifiers, for example to detect that color change shape, and the language model explicitly focuses on a proper description structure that enables proper construction following repetitive, deep meaning sentences. every approach is evaluated on structured task of interpreting lists that describe sets of objects in a physical workspace. we perform accurate task performance and approximate latent - variable concept systems or memory recognition scenes.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (5766kb)", "http://arxiv.org/abs/1206.6423v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.RO", "authors": ["cynthia matuszek", "nicholas fitzgerald", "luke s zettlemoyer", "liefeng bo", "dieter fox"], "accepted": true, "id": "1206.6423"}, "pdf": {"name": "1206.6423.pdf", "metadata": {"source": "META", "title": "A Joint Model of Language and Perception  for Grounded Attribute Learning", "authors": ["Cynthia Matuszek", "Nicholas FitzGerald", "Luke Zettlemoyer"], "emails": ["cynthia@cs.washington.edu", "nfitz@cs.washington.edu", "lsz@cs.washington.edu", "lfb@cs.washington.edu", "fox@cs.washington.edu"], "sections": [{"heading": "1. Introduction", "text": "Physically grounded settings provide exciting opportunities for learning. For example, a person might be able to teach a robot about objects in its environment. However, to do this, a robot must jointly reason about the different modalities encountered (for example language and vision), and induce rich associations with as little guidance as possible.\nConsider a simple sentence such as \u201cThese are the yellow blocks,\u201d uttered in a setting where there is a phys-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nical workspace that contains a number of objects that vary in shape and color. We assume that a robot can understand sentences like this if it can solve the associated grounded object selection task. Specifically, it must realize that words such as \u201cyellow\u201d and \u201cblocks\u201d refer to object attributes, and ground the meaning of such words by mapping them to a perceptual system that will enable it to identify the specific physical objects referred to. To do so robustly, even in cases where words or attributes are new, our robot must learn (1) visual classifiers that identify the appropriate object properties, (2) representations of the meaning of individual words that incorporate these classifiers, and (3) a model of compositional semantics used to analyze complete sentences.\nIn this paper, we present an approach for jointly learning these components. Our approach builds on existing work on visual attribute classification (Bo et al., 2011) and probabilistic categorial grammar induction for semantic parsing (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011). Specifically, our system induces new grounded concepts (groups of words along with the parameters of the attribute classifier they are paired with) from a set of scenes containing only sentences, images, and indications of what objects are being referred to. As a result, it can be taught to recognize previously unknown object attributes by someone describing objects while pointing out the relevant objects in a set of training scenes. Learning is online, adding one scene at a time, and EM-like, in that the parameters are updated to maximize the expected marginal likelihood of the latent language and visual components of the model. This integrated approach allows for effective model updates with no explicit labeling of logical meaning representations or attribute classifier outputs.\nWe evaluate this approach on data gathered on Ama-\nzon Mechanical Turk, in which people describe sets of objects on a table. Experiments demonstrate that the joint learning approach can effectively extend the set of grounded concepts in an incomplete model initialized with supervised training on a small dataset. This provides a simple mechanism for learning vocabulary in a physical environment."}, {"heading": "2. Overview of the Approach", "text": "Problem We wish to learn a joint language and perception model for the object selection task. The goal is to automatically map a natural language sentence x and a set of scene objects O to the subset G \u2286 O of objects described by x. The left panel of Fig. 1 shows an example scene. Here, O is the set of objects present in this scene. The individual objects o \u2208 O are extracted from the scene via segmentation (the right panel of Fig. 1 shows example segments). Given the sentence x =\u201cHere are the yellow ones,\u201d the goal is to select the five yellow objects for the named set G.\nModel Components Given a sentence and segmented scene objects, we learn a distribution P (G | x,O) over the selected set. Our approach combines recent models of language and vision, including:\n(1) A semantic parsing model that defines P (z|x), a distribution over logical meaning representations z for each sentence x. In our running example, the desired representation z = \u03bbx.color(x, yellow) is a lambdacalculus expression that defines a set of objects that are yellow. For this task, we build on an existing semantic parsing model (Kwiatkowski et al., 2011).\n(2) A set of visual attribute classifiers C, where each classifier c \u2208 C defines a distribution P (c = true|o) of the classifier returning true for each possible object o \u2208 O in the scene. For example, there would be a unique classifier c \u2208 C for each possible color or shape an object can have. We use logistic regression to train classifiers on color and shape features extracted from\nobject segments recorded using a Kinect depth camera.\nJoint Model We combine these language and vision models in two ways. First, we introduce an explicit model of alignment between the logical constants in the logical form z and classifiers in the set C. This alignment would, for example, enable us to learn that the logical constant yellow should be paired with a classifier c \u2208 C that fires on yellow objects.\nNext, we introduce an execution model that allows us to determine what scene objects in O would be selected by a logical expression z, given the classifiers in C. This allows us to, for example, execute \u03bbx.color(x, green)\u2227shape(x, triangle) by testing all of the objects with the appropriate classifiers (for green and triangle), then selecting objects on which both classifiers return true. This execution model includes uncertainty from the semantic parser P (z|x), classifier confidences P (c = true|o), and a deterministic groundtruth constraint that encodes what objects are actually intended to be selected. Full details are in Sec. 5.\nModel Learning We present an approach that learns the meaning of new words from a dataset D = {(xi, Oi, Gi) | i = 1 . . . n}, where each example i contains a sentence xi, the objects Oi, and the selected set Gi. This setup is an abstraction of the situation where a teacher mentions xi while pointing to the objects Gi \u2286 Oi she describes. As described in detail in Sec. 6, learning proceeds in an online, EMlike fashion by repeatedly estimating expectations over the latent logical forms zi and the outputs of the classifiers c \u2208 C, then using these expectations to update the parameters of the component models for language P (z|x) and visual classification P (c|o). To bootstrap the learning approach, we first train a limited language and perception system in a fully supervised way: in this stage, each example additionally contains labeled logical meaning expressions and classifier outputs, as described in Sec. 6."}, {"heading": "3. Related Work", "text": "To the best of our knowledge, this paper presents the first approach for jointly learning visual classifiers and semantic parsers, to produce rich, compositional models that span directly from sensors to meaning. However, there is significant related work on the model components, and on grounded learning in general.\nVision Current state-of-the-art object recognition systems (Felzenszwalb et al., 2009; Yang et al., 2009) are based on local image descriptors, for example SIFT over images (Lowe, 2004) and Spin Images over 3D point clouds (Johnson & Hebert, 1999). Visual\nattributes provide rich descriptions of objects, and have become a popular topic in the vision community (Farhadi et al., 2009; Parikh & Grauman, 2011); although very successful, we still lack a deep understanding of the design rules underlying them and how they measure similarity. Recent work on kernel descriptors (Bo et al., 2010) shows that these hand-designed features are equivalent to a type of match kernel that performs similarly to sparse coding (Yang et al., 2009; Yu & Zhang, 2010) and deep networks (Lee et al., 2009) on many object recognition benchmarks (Bo et al., 2010). We adapt kernel descriptors as feature extractors for attribute classifiers because of their strong empirical performance.\nSemantic Parsing There has been significant work on supervised learning for inducing semantic parsers (Zelle & Mooney, 1996; He & Young, 2006; Wong & Mooney, 2007). Our research builds on work on supervised learning of CCG parsers (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011); there is also work on performing semantic analysis with alternate forms of supervision. Clarke (2010) and Liang (2011) describe approaches to learning semantic parsers from questions paired with database answers, while Goldwasser (2011) presents work on unsupervised learning. However, none of these approaches include joint models of language and vision.\nGrounding There has been significant work on grounded learning more generally in the robotics and vision communities. A full review is beyond the scope of this paper, so we highlight a few examples. Roy developed a series of techniques for grounding words in visual scenes (Mavridis & Roy, 2006; Reckman et al., 2010; Gorniak & Roy, 2003). In computer vision, the grounding problem often relates to detecting objects and attributes in visual information (e.g., see (Barnard et al., 2003)); however, these approaches primarily focus on isolated word meaning, rather than compositional semantic analyses. Most closely related to our work are approaches that learn probabilistic language models from natural language input (Matuszek et al., 2012; Chen & Mooney, 2011), especially those\nthat include a visual component (Tellex et al., 2011). However, these approaches ground language into predefined language formalisms, rather than extending the model to account for entirely novel input."}, {"heading": "4. Background on Semantic Parsing", "text": "Our grounded language learning incorporates a stateof-the-art model, FUBL, for semantic parsing, as reviewed in this section. FUBL (Kwiatkowski et al., 2011) is an algorithm for learning factored Combinatory Categorial Grammar (CCG) lexicons for semantic parsing. Given a dataset {(xi, zi) | i = 1...n} of natural language sentences xi, which are paired with logical forms zi that represent their meaning, UBL learns a factored lexicon \u039b made up of a set of lexemes L and a set of lexical templates T . Lexemes combine with templates in order to form lexical items, which can be used by a semantic parser to parse natural language sentences into logical forms. For example, given the sentence x =\u201cthis red block is in the shape of a half-pipe\u201d and the logical form zi = \u03bbx.color(x, red)\u2227 shape(x, arch), FUBL learns a parse like the example in figure 2. In this parse, the lexeme (half-pipe, [arch]) has combined with the template \u03bb(\u03c9,~v). [\u03c9 ` NP : ~v1] to yield the lexical item half -pipe ` NP : arch. FUBL also learns a log-linear model which produces the probability of a parse y that yields logical form z given the sentence x:\nP (y, z | x; \u0398L,\u039b) = e \u0398L\u00b7\u03c6(x,y,z)\u2211\n(y\u2032,z\u2032) e \u0398L\u00b7\u03c6(x,y\u2032,z\u2032)\n(1)\nwhere \u03c6(x, y, z) is a feature vector encompassing the lexemes and lexical templates used to generate y, amongst other things.\nIn this work, we initialize our parse model using the standard FUBL approach, followed by automatically inducing lexemes paired with new visual attributes not present in the initial training set, as we will see in the next section."}, {"heading": "5. Joint Language/Perception Model", "text": "As described in Sec. 2, the object selection task is to identify a subset of objects, G, given a scene O and an NL sentence x. We define a possible world w to be a set of classifier outputs, where wo,c \u2208 {T, F} specifies the boolean output of classifier c for object o. Our joint probabilistic model is:\nP (G | x,O) = \u2211 z \u2211 w P (G, z, w | x,O) (2)\nwhere the latent variable z over logical forms models linguistic uncertainty and the latent w over possible worlds models perceptual uncertainty.\nWe further decompose (2) into a product of models for language, vision, and grounded execution. This final model selects the named objects G, motivated in Sec. 2 and described below; the final decomposition is:\nP (G, z, w | x,O) = P (z | x)P (w | O)P (G | z, w) (3)\nHere, the language model P (z|x) and vision model P (w|O) are held in agreement by the conditional probability term P (G|z, w). Let z(w) be the set of objects that are selected, under the assignment in w, when z is applied to them. For example, the expression z = \u03bbx.shape(x, cube)\u2227color(x, red) would return true when applied to the objects in w for which the classifiers for the cube and red logical constants return true. Now, P (G|z, w) forces agreement and models object selection by putting all of its probability mass on the set G that equals z(w).\nIn this formulation, the language and vision distributions are conditionally independent given this agreement. The semantic parsing model P (z|x) builds on previous work, as described in eqn. (1). The perceptual classification P (w|O) is defined as follows: we assume each perceptual classifier is applied independently, decomposing this term into:\nP (w | O) = \u220f o\u2208O \u220f c\u2208C P (wo,c|o) (4)\nwhere the probability of a world is simply the product of the probabilities of the individual classifier assignments for all of the objects.\nEach classifier is a logistic regression model, where the probability of a classifier c on a given object o is:\nP (wo,c = 1|o; \u0398P ) = e\u0398\nP c \u00b7\u03c6(o)\n1 + e\u0398 P c \u00b7\u03c6(o)\n(5)\nwhere \u0398Pc is the parameters in \u0398 P for classifier c. This approach provides a simple, direct way to couple the individual language and vision components to model the object selection task.\nInference There are two key inference problems in a model of this type. During learning, we need to compute the marginal distribution P (z, w|x,O,G) over latent logical forms z and perceptual assignments w (see next section). At test time, we must compute arg maxG P (G|x,O) to find the set of named objects.\nComputing this probability distribution requires summing the total probability of all world/logical form pairs that name G. For each possible world w, determining if z names G is equivalent to a SAT problem, as z can theoretically encode an arbitrary logical expression that will name the appropriate G only when satisfied. Computing the marginal probability is then a weighted model counting problem, which is in #-P. However, the logical expressions allowed by our current grammar\u2014conjunctions of unary attribute descriptors\u2014admit efficient exact computation, described below."}, {"heading": "6. Model Learning", "text": "The physically grounded joint learning problem is to induce a model P (G|x,O), given data of the form D = {(xi, Oi, Gi) | i = 1 . . . n}, where each example i contains a sentence xi, the objects Oi, and the selected set Gi. We consider the case where the learner already has a partial model, including a CCG parser with a small vocabulary and a small set of attribute classifiers. The goal is to automatically extend the model to induce new classifiers that are tied to new words in the semantic parser. We first describe the learning algorithm, then present how we initialize the approach by learning decoupled models from small datasets with more extensive annotations.\nAligning Words to Classifiers One key challenge is to learn to create new attribute classifiers associated with unseen words in the sentences xi in the data D. We take a simple, exhaustive approach by creating a set of k new classifiers, initialized to uniform distributions. Each classifier is additionally paired with a new logical constant in the FUBL lambda-calculus language. Finally, a new lexeme is created by pairing each previously unknown word in a sentence in D with either one of these new classifier constants, or the logical expressions from an existing lexeme in the lexicon. The parsing weights for the indicator features for each of these additions are set to 0. This approach learns, through the probabilistic updates described below, to jointly reestimate the parameters of both the new classifiers and the expanded semantic parsing model.\nParameter Estimation We aim to estimate the language parameters \u0398L and perception parameters\n\u0398P from data D = {(xi, Oi, Gi) | i = 1 . . . n}, as defined above. We want to find parameter settings that maximize the marginal log likelihood of D:\nLL(D; \u0398L,\u0398P ) = \u2211\ni=1...n\nlnP (Gi|xi, Oi; \u0398L,\u0398P ) (6)\nThis objective is non-convex due to the sum over latent assignments for the logical form z and attribute classifier outputs w in the definition of P (Gi|xi, Oi; \u0398L,\u0398P ) from eqn. (2). However, if z and w are labeled, the overall algorithm reduces to simply training the loglinear models for the semantic parser P (z|xi; \u0398L) and attribute classifiers P (w|Oi; \u0398P ), both well-studied problems. In this situation, we can use an EM algorithm to first estimate the marginal P (z, w | xi, Oi, Gi; \u0398\nL,\u0398P ), then maximize the expected likelihood according to the distribution, with a weighted version of our familiar log-linear model parameter updates. We present an online version of this approach, with updates computed one example at a time.\nComputing Expectations For each example i, we must compute the marginal over latent variables given by:\nP (z, w | xi, Oi, Gi; \u0398L,\u0398P ) = P (z | xi; \u0398L)P (w | Oi; \u0398P )P (Gi|z, w)\u2211\nz\u2032 \u2211 w\u2032 P (z \u2032 | xi; \u0398L)P (w\u2032 | Oi; \u0398P )P (Gi|z\u2032, w\u2032) (7)\nSince computing all possible parses z is exponential in the length of the sentence, we use beam search to find the top-N parses. This exact inference could be replaced with an approximate method, such as MCSAT, to accommodate a more permissive grammar.\nConditional Expected Gradient For each example, we update the parameters with the expected gradient, according to the marginal distribution above. For the language parameters \u0398L, the gradient is\n\u2206L = \u2211 z\u2032 \u2211 w\u2032 P (z\u2032, w\u2032 | xi, Oi, Gi; \u0398L,\u0398P )\u2217\n(EP (y|xi,z\u2032;\u0398L)\n[ \u03c6Lj (xi, y, z \u2032) ] \u2212\nEP (y,z|xi;\u0398L)\n[ \u03c6Lj (xi, y, z) ] )\n(8)\nwhere the inner difference of expectations is the familiar gradient of a log-linear model for conditional random fields with hidden variables (Quattoni et al., 2007; Kwiatkowski et al., 2010), and is weighted according to the expectation.\nSimilarly, for the perception parameters \u0398P , the gradient is:\n\u2206Pc = \u2211 z\u2032 \u2211 w\u2032\nP (z\u2032, w\u2032 | xi, Oi, Gi; \u0398L,\u0398P )\u2217\u2211 o\u2208Oi [ w\u2032o,c \u2212 P (w\u2032o,c = 1 | \u03c6(o); \u0398P ) ] \u03c6(o) (9)\nwhere the inner sum ranges over the objects and adds in the familiar gradient for logistic regression binaryclassification models.\nOnline Updates We use a simple, online parameter estimation scheme that loops over the data K = 10 (picked on validation set) times. For each data point i consisting of the tuple (xi, Oi, Gi), we perform an update where we take a step according to the above expected gradient over the latent variables. We use a learning rate of 0.1 with a constant decay of .00001 per update for all experiments.\nDiscussion This complete learning approach provides an efficient online algorithm that closely matches the style of interactive, grounded language learning we are pursuing in this work. Given the decayed learning rate, the algorithm is guaranteed to converge, but little can be said about the optimality of the solution. However, as we see in Sec. 7, the approach works well in practice for the object set selection task we consider.\nBootstrapping To construct the initial limited language and perceptual models, we make use of a small, supervised data set Dsup = {(xi, zi, wi, Oi, Gi) | i = 1 . . .m}, which matches our previous setup but additionally labels the latent logical form zi and classifier outputs wi. As mentioned above, learning in this setting is completely decoupled and we can estimate the semantic parsing distribution P (zi|xi; \u0398L) with the FUBL learning algorithm (Kwiatkowski et al., 2011) and the attribute classifiers P (wi|Oi; \u0398P ) with gradient ascent for logistic regression. As we show experimentally, Dsup can often be quite small, and will in general not contain many of the words and attributes that must be additionally learned in the full approach. Exploring approaches for learning without Dsup, such as replacing it with interactive dialog with a human teacher, is an important area for future work."}, {"heading": "7. Experimental Setup", "text": "Data Set Data was collected using a selection of toys, including wooden blocks, plastic food, and building bricks. For each scene, we collected short RGB-D videos with a Kinect depth camera, showing a person gesturing to a subset of the objects. Natural language annotations were gathered using Mechanical Turk; workers were asked to describe the objects being pointed to in the video (see Fig. 3). The referenced\nobjects were then marked as belonging to G, the positive set of objects for that scene. A total of 142 scenes were shown, eliciting descriptions of 12 attributes, divided evenly into shapes and colors. In total, there were 1003 sentence/annotation pairs.\nPerceptual Features To automatically segment objects from each scene, we performed RANSAC plane fitting on the Kinect depth values to find the table plane, then extracted connected components (segments) of points more than a minimum distance above that plane. After getting segmented objects, features for every object are extracted using kernel descriptors (Bo et al., 2011). We extract two types of features, for depth values and RGB values; these correspond to shape and color attributes, respectively. During training, the system learns logistic regression classifiers using these features. In the initialization phase used to bootstrap the model, the annotation provides information about which language attributes relate to shape or color. However, this information is not provided in the training phase.\nLanguage Features We follow (Kwiatkowski et al., 2011) in including a standard set of binary indicator features to define the log-linear model P (z|x; \u0398L) over logical forms, given sentences. This includes indicators for which lexical entries were used and properties of the logical forms that are constructed. These features allow the joint learning approach to weight lexical selection against evidence provided by the compositional analysis and the visual model components."}, {"heading": "8. Results", "text": "This section presents results and a discussion of our evaluation. We demonstrate effective learning in the full model for the object set selection task. We then briefly describe ablation studies and examples of learned models.."}, {"heading": "8.1. Object Set Selection", "text": "To measure set selection task performance, we divided the data according to attribute. To initialize the model, we used the data for six of the attributes to train supervised classifiers, and provided logical forms for the corresponding sentences to train the initial semantic parsing model, as described at the end of Sec. 6. Data for the remaining six attributes were used for evaluation, with 80% allocated for training and 20% held out for testing. Here, all of the visual scenes are previously unseen, the words in the sentences describing the new attributes are unknown, and the only available labels are the output object set G.\nWe report precision, recall, and F1-score on the set selection task. Results are averaged over 10 different runs with the training data presented in different randomized orders. The system performs well, achieving an average precision of 82%, recall of 71%, and a 76% F1-score. This level of performance is achieved relatively quickly; performance generally converges within five passes over the training data."}, {"heading": "8.2. Ablation Studies", "text": "To examine the need for a joint model, we measure performance of two models in which either the language or the visual component is sharply limited. In each case, performance significantly degrades. These results are summarized in Fig. 4.\nVision In order to measure how a set of classifiers would perform on the set selection task with only a simple language model, we manually created a thesaurus of words used in the dataset to refer to target attributes containing, on average, 5 different ways of referring to each color and shape. To learn the unsupervised concepts for this baseline, we first extracted a list of all words appearing in the training corpus but not in the initialization data; words which appear in the thesaurus are grouped into synonym sets. To train classifiers, we collect objects from scenes in which only terms from the given synonym set appear. Any synonym set which does not occur in at least 2 distinct scenes is discarded. The resulting positive and negative objects are used to train classifiers. To generate a predicted set of objects at test time, we find all synonym sets which occur in the sentence x, and determine whether the classifiers associated with those words successfully identify the object.\nAveraged across our trials, the results are as follows: Precision=0.92; Recall=0.41; F1-score=0.55. These results are, on average, notably worse than the performance of the jointly trained model.\nSemantic Parsing As a baseline for testing how well a pure parsing model will perform when the perception model is ablated, we run the parsing model obtained during initialization directly on the test set, training no new classifiers. Since the parser is capable of generating parses by skipping unknown words, this baseline is equivalent to treating the unknown concept words as if they are semantically empty.\nAveraged across our trials, the results are as follows: Precision=0.52; Recall=0.09; F1-score=0.14. Not surprisingly, a substantial number of parses selected no objects, as the parser has no way of determining the meaning of an unknown word."}, {"heading": "8.3. Discussion and Examples", "text": "This section discusses typical training runs and data requirements. We present examples of learned models, highlighting what is learned and typical errors, and then describe simple experiment investigating the amount of supervised data required for initialization.\nClassifier performance after training effects the system\u2019s ability to perform the set selection task. During a sample trial, average accuracy of color and shape classifiers for newly learned concepts are 97% and 74%, respectively. Although these values are sufficient for reasonable task performance, there are some failures\u2014 for example, the shape attributes \u201ccube\u201d and \u201ccylinder\u201d are sometimes challenging to differentiate.\nAs noted in Sec. 4, the semantic parser contains lexemes that pair words with learned classifiers, and features that indicate lexeme use during parsing. Fig. 5 shows some selected word/classifier pairs, along with the weight for their associated feature (each trial pro-\nduces a large number of such lexemes). The classifiers new0\u2013new2 and new3\u2013new5 are color and shape classifiers, respectively. As can be seen, each of the novel attributes is most strongly associated with a newly-created classifier, while irrelevant words such as \u201cthing\u201d tend to parse to null. The system must identify which of the classifier types to use for novel words.\nWe ran additional tests investigating whether the system is able to learn synonyms. Here, we split the data so that the training set has attributes learned during initialization, but are referred to by new, synonymous words. These runs performed comparably to those reported above; the approach easily learns lexemes that pair these new words with the appropriate classifiers.\nFinally, we briefly discuss the effects of reducing the amount of annotated data used to initialize the language and perception model (see Fig. 6). As can be seen, with fewer than 150 sentences, the learned grammar does not seem to have sufficient coverage to model unknown words in joint learning; however, beyond that, performance is quite stable."}, {"heading": "9. Conclusion", "text": "This paper presents a joint model of language and perception for grounded attribute learning. Our approach learns representations of the meanings of natural language, using visual perception to ground those meanings in the physical world. Learning is performed via optimizing the data log-likelihood using an online, EM-like training algorithm. This system is able to learn accurate language and attribute models for the object set selection task, given data containing only language, raw percepts, and the target objects. By jointly learning language and perception models, the approach can identify which novel words are color attributes, shape attributes, or no attributes at all.\nWe believe our approach has significant potential to scale to general language grounding problems. In particular, our modular framework was designed to easily incorporate future advances in visual classification\nand semantic parsing. We are also working to scale the complexity of the language and physical scenes, with the eventual goal of robust learning in completely unconstrained environments."}, {"heading": "Acknowledgments", "text": "This work was funded in part by the Intel Science and Technology Center for Pervasive Computing, the Robotics Consortium sponsored by the U.S. Army Research Laboratory under the Collaborative Technology Alliance Program (W911NF-10-2-0016), and NSF grant IIS-1115966."}], "references": [{"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. De Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Barnard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barnard et al\\.", "year": 2003}, {"title": "Kernel descriptors for visual recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Bo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bo et al\\.", "year": 2010}, {"title": "Depth kernel descriptors for object recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "In IEEE/RSJ Int\u2019l Conf. on Intelligent Robots and Systems (IROS),", "citeRegEx": "Bo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bo et al\\.", "year": 2011}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "In Proc. of the 25th AAAI Conf. on Artificial Intelligence", "citeRegEx": "Chen and Mooney,? \\Q2011\\E", "shortCiteRegEx": "Chen and Mooney", "year": 2011}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["J. Clarke", "D. Goldwasser", "M. Chang", "D. Roth"], "venue": "In Proc. of the Conf. on Computational Natural Language Learning,", "citeRegEx": "Clarke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2010}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "Farhadi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2009}, {"title": "Object detection with discriminatively trained part based models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2009}, {"title": "Confidence driven unsupervised semantic parsing", "author": ["D. Goldwasser", "R. Reichart", "J. Clarke", "D. Roth"], "venue": "In Proceedings. of the Association of Computational Linguistics,", "citeRegEx": "Goldwasser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldwasser et al\\.", "year": 2011}, {"title": "Understanding complex visually referring utterances", "author": ["P. Gorniak", "D. Roy"], "venue": "In Proc. of the HLT-NAACL 2003 Workshop on Learning Word Meaning from NonLinguistic Data,", "citeRegEx": "Gorniak and Roy,? \\Q2003\\E", "shortCiteRegEx": "Gorniak and Roy", "year": 2003}, {"title": "Spoken language understanding using the hidden vector state model", "author": ["Y. He", "S. Young"], "venue": "Speech Communication,", "citeRegEx": "He and Young,? \\Q2006\\E", "shortCiteRegEx": "He and Young", "year": 2006}, {"title": "Using spin images for efficient object recognition in cluttered 3D scenes", "author": ["A. Johnson", "M. Hebert"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Johnson and Hebert,? \\Q1999\\E", "shortCiteRegEx": "Johnson and Hebert", "year": 1999}, {"title": "Lexical generalization in CCG grammar induction for semantic parsing", "author": ["T. Kwiatkowski", "L.S. Zettlemoyer", "S. Goldwater", "M. Steedman"], "venue": "In Proc. of the Conf. on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2011}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A. Ng"], "venue": "In Proc. of the Int\u2019l Conf. on Machine Learning (ICML),", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Proc. of the Association for Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "Int\u2019l Journal of Computer Vision (IJCV),", "citeRegEx": "Lowe,? \\Q2004\\E", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["C. Matuszek", "E. Herbst", "L. Zettlemoyer", "D. Fox"], "venue": "In Proc. of the 13th Int\u2019l Symposium on Experimental Robotics (ISER),", "citeRegEx": "Matuszek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Grounded situation models for robots: Where words and percepts meet", "author": ["N. Mavridis", "D. Roy"], "venue": "In IEEE/RSJ Int\u2019l Conf. on Intelligent Robots and Systems,", "citeRegEx": "Mavridis and Roy,? \\Q2006\\E", "shortCiteRegEx": "Mavridis and Roy", "year": 2006}, {"title": "Relative attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "In Int\u2019l Conf. on Computer Vision,", "citeRegEx": "Parikh and Grauman,? \\Q2011\\E", "shortCiteRegEx": "Parikh and Grauman", "year": 2011}, {"title": "Hidden-state conditional random fields", "author": ["A. Quattoni", "S. Wang", "L. p Morency", "M. Collins", "T. Darrell", "Csail", "Mit"], "venue": "In IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Quattoni et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2007}, {"title": "Learning meanings of words and constructions, grounded in a virtual game", "author": ["H. Reckman", "J. Orkin", "D. Roy"], "venue": "In Proc. of the 10th Conf. on Natural Language Processing (KONVENS),", "citeRegEx": "Reckman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reckman et al\\.", "year": 2010}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M.R. Walter", "A.G. Banerjee", "S. Teller", "N. Roy"], "venue": "In Proc. of the National Conf. on Artificial Intelligence (AAAI),", "citeRegEx": "Tellex et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Y.W. Wong", "R.J. Mooney"], "venue": "In Proc. of the Ass\u2019n for Computational Linguistics,", "citeRegEx": "Wong and Mooney,? \\Q2007\\E", "shortCiteRegEx": "Wong and Mooney", "year": 2007}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Improved local coordinate coding using local tangents", "author": ["K. Yu", "T. Zhang"], "venue": "In Proc. of the Int\u2019l Conf. on Machine Learning (ICML),", "citeRegEx": "Yu and Zhang,? \\Q2010\\E", "shortCiteRegEx": "Yu and Zhang", "year": 2010}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["J.M. Zelle", "R.J. Mooney"], "venue": "In Proc. of the National Conf. on Artificial Intelligence,", "citeRegEx": "Zelle and Mooney,? \\Q1996\\E", "shortCiteRegEx": "Zelle and Mooney", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "In Proc. of the Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Zettlemoyer and Collins,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer and Collins", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Our approach builds on existing work on visual attribute classification (Bo et al., 2011) and probabilistic categorial grammar induction for semantic parsing (Zettlemoyer & Collins, 2005; Kwiatkowski et al.", "startOffset": 72, "endOffset": 89}, {"referenceID": 11, "context": ", 2011) and probabilistic categorial grammar induction for semantic parsing (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011).", "startOffset": 76, "endOffset": 131}, {"referenceID": 11, "context": "For this task, we build on an existing semantic parsing model (Kwiatkowski et al., 2011).", "startOffset": 62, "endOffset": 88}, {"referenceID": 6, "context": "Vision Current state-of-the-art object recognition systems (Felzenszwalb et al., 2009; Yang et al., 2009) are based on local image descriptors, for example SIFT over images (Lowe, 2004) and Spin Images over 3D point clouds (Johnson & Hebert, 1999).", "startOffset": 59, "endOffset": 105}, {"referenceID": 22, "context": "Vision Current state-of-the-art object recognition systems (Felzenszwalb et al., 2009; Yang et al., 2009) are based on local image descriptors, for example SIFT over images (Lowe, 2004) and Spin Images over 3D point clouds (Johnson & Hebert, 1999).", "startOffset": 59, "endOffset": 105}, {"referenceID": 14, "context": ", 2009) are based on local image descriptors, for example SIFT over images (Lowe, 2004) and Spin Images over 3D point clouds (Johnson & Hebert, 1999).", "startOffset": 75, "endOffset": 87}, {"referenceID": 5, "context": "attributes provide rich descriptions of objects, and have become a popular topic in the vision community (Farhadi et al., 2009; Parikh & Grauman, 2011); although very successful, we still lack a deep understanding of the design rules underlying them and how they measure similarity.", "startOffset": 105, "endOffset": 151}, {"referenceID": 1, "context": "Recent work on kernel descriptors (Bo et al., 2010) shows that these hand-designed features are equivalent to a type of match kernel that performs similarly to sparse coding (Yang et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 22, "context": ", 2010) shows that these hand-designed features are equivalent to a type of match kernel that performs similarly to sparse coding (Yang et al., 2009; Yu & Zhang, 2010) and deep networks (Lee et al.", "startOffset": 130, "endOffset": 167}, {"referenceID": 12, "context": ", 2009; Yu & Zhang, 2010) and deep networks (Lee et al., 2009) on many object recognition benchmarks (Bo et al.", "startOffset": 44, "endOffset": 62}, {"referenceID": 1, "context": ", 2009) on many object recognition benchmarks (Bo et al., 2010).", "startOffset": 46, "endOffset": 63}, {"referenceID": 11, "context": "Our research builds on work on supervised learning of CCG parsers (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011); there is also work on performing semantic analysis with alternate forms of supervision.", "startOffset": 66, "endOffset": 121}, {"referenceID": 11, "context": "Our research builds on work on supervised learning of CCG parsers (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011); there is also work on performing semantic analysis with alternate forms of supervision. Clarke (2010) and Liang (2011) describe approaches to learning semantic parsers from questions paired with database answers, while Goldwasser (2011) presents work on unsupervised learning.", "startOffset": 96, "endOffset": 225}, {"referenceID": 11, "context": "Our research builds on work on supervised learning of CCG parsers (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011); there is also work on performing semantic analysis with alternate forms of supervision. Clarke (2010) and Liang (2011) describe approaches to learning semantic parsers from questions paired with database answers, while Goldwasser (2011) presents work on unsupervised learning.", "startOffset": 96, "endOffset": 242}, {"referenceID": 11, "context": "Our research builds on work on supervised learning of CCG parsers (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011); there is also work on performing semantic analysis with alternate forms of supervision. Clarke (2010) and Liang (2011) describe approaches to learning semantic parsers from questions paired with database answers, while Goldwasser (2011) presents work on unsupervised learning.", "startOffset": 96, "endOffset": 360}, {"referenceID": 19, "context": "Roy developed a series of techniques for grounding words in visual scenes (Mavridis & Roy, 2006; Reckman et al., 2010; Gorniak & Roy, 2003).", "startOffset": 74, "endOffset": 139}, {"referenceID": 0, "context": ", see (Barnard et al., 2003)); however, these approaches primarily focus on isolated word meaning, rather than compositional semantic analyses.", "startOffset": 6, "endOffset": 28}, {"referenceID": 15, "context": "Most closely related to our work are approaches that learn probabilistic language models from natural language input (Matuszek et al., 2012; Chen & Mooney, 2011), especially those that include a visual component (Tellex et al.", "startOffset": 117, "endOffset": 161}, {"referenceID": 20, "context": ", 2012; Chen & Mooney, 2011), especially those that include a visual component (Tellex et al., 2011).", "startOffset": 79, "endOffset": 100}, {"referenceID": 11, "context": "FUBL (Kwiatkowski et al., 2011) is an algorithm for learning factored Combinatory Categorial Grammar (CCG) lexicons for semantic parsing.", "startOffset": 5, "endOffset": 31}, {"referenceID": 18, "context": "where the inner difference of expectations is the familiar gradient of a log-linear model for conditional random fields with hidden variables (Quattoni et al., 2007; Kwiatkowski et al., 2010), and is weighted according to the expectation.", "startOffset": 142, "endOffset": 191}, {"referenceID": 11, "context": "As mentioned above, learning in this setting is completely decoupled and we can estimate the semantic parsing distribution P (zi|xi; \u0398) with the FUBL learning algorithm (Kwiatkowski et al., 2011) and the attribute classifiers P (wi|Oi; \u0398 ) with gradient ascent for logistic regression.", "startOffset": 169, "endOffset": 195}, {"referenceID": 2, "context": "After getting segmented objects, features for every object are extracted using kernel descriptors (Bo et al., 2011).", "startOffset": 98, "endOffset": 115}, {"referenceID": 11, "context": "Language Features We follow (Kwiatkowski et al., 2011) in including a standard set of binary indicator features to define the log-linear model P (z|x; \u0398) over logical forms, given sentences.", "startOffset": 28, "endOffset": 54}], "year": 2012, "abstractText": "As robots become more ubiquitous and capable, it becomes ever more important for untrained users to easily interact with them. Recently, this has led to study of the language grounding problem, where the goal is to extract representations of the meanings of natural language tied to the physical world. We present an approach for joint learning of language and perception models for grounded attribute induction. The perception model includes classifiers for physical characteristics and a language model based on a probabilistic categorial grammar that enables the construction of compositional meaning representations. We evaluate on the task of interpreting sentences that describe sets of objects in a physical workspace, and demonstrate accurate task performance and effective latent-variable concept induction in physical grounded scenes.", "creator": "LaTeX with hyperref package"}}}