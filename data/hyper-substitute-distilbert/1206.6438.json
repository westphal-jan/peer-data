{"id": "1206.6438", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation", "abstract": "we study specific problem developing attribute domain adaptation, which serves to'classifiers'create a labeled reference domain to an unlabeled target type. other experimental classes first observe code - invariant parameters and then evolve classifiers representing them. we propose second novel approach seeking recognizes learn distinct pattern. specifically, token substitution method identifies effective grammar space where data in known item and the target domains are mutually identified, it also learns the phrase space discriminatively, optimizing an inclusion - theoretic metric to an algorithm, the expected misclassification error entering the word set. trials show how this optimization can be effectively sorted out with simple method - based procedures investigating how predictable, be cross - validated within escaping any labeled data from the target population. experiments studies on benchmark tasks of object classification and sentiment analysis confirming our linguistic assumptions and demonstrated wider usage of the method over competing ones as classification statistics.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (422kb)", "http://arxiv.org/abs/1206.6438v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yuan shi", "fei sha"], "accepted": true, "id": "1206.6438"}, "pdf": {"name": "1206.6438.pdf", "metadata": {"source": "META", "title": "Information-Theoretical Learning of  Discriminative Clusters for Unsupervised Domain Adaptation", "authors": ["Yuan Shi", "Fei Sha"], "emails": ["yuanshi@usc.edu", "feisha@usc.edu"], "sections": [{"heading": "1. Introduction", "text": "Supervised learning algorithms often assume that the training and the test data are randomly sampled from the same joint distribution. While the assumption facilitates rigorous theoretical analysis and empirical comparison of different algorithms, its validity is often challenged outside of laboratory settings. In realworld applications, there are many factors causing a\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nmismatch between the training and the test data. For instance, imagine developing a face detection system for Facebook mobile users. A tuned classifier on images captured by webcams could be applied to images from mobile phones. In this case, the imaging conditions vary significantly due to background illumination, motion blurring, pose, etc.\nTechniques for addressing learning problems with mismatched distributions are often referred as domain adaptation, or sometimes transfer learning (Daume\u0301 III & Marcu, 2006; Pan & Yang, 2010; Quin\u0303onero-Candela et al., 2009). The source domain refers to the labeled training data, while the target domain refers to the test data. When there is no labeled data from the target domain to help learning classifiers, the problem setting is termed unsupervised domain adaptation.\nUnsupervised domain adaptation is especially challenging as the target domain does not provide explicitly any information on how to optimize classifiers. Note that the objective of domain adaptation is to derive a classifier for the unlabeled (target) data from the labeled (source) data. This goal sets domain adaptation apart from semi-supervised learning, whose primary goal is to improve the performance on the labeled data with unlabeled data (Chapelle et al., 2006). The difference is subtle yet fundamental. For example, model selection or cross-validation using classification accuracy on the target domain is generally impossible.\nExisting approaches thus rely on making strong assumptions on how the data distribution have shifted between the two domains in order to derive classification rules for the target domain. For instance, in covariate shift (Shimodaira, 2000; Bickel et al., 2007; Huang et al., 2007), the marginal distributions of the features are different across domains while the posterior distribution of the label remains the same. This naturally leads to a two-stage learning paradigm:\nthe labeled instances from the source domain are first weighted so as to compensate the difference in marginal distributions. Then, a classifier is trained using the labels and then applied to the unlabeled data.\nOther works have also followed similar paradigms (Pan et al., 2011; Gopalan et al., 2011). In the structural correspondence learning, the original features are first augmented with features that are more likely to be domain invariant and then a classifier is trained (Blitzer et al., 2006). The augmenting features are linear transformation of the original features. Alternatively, in deep learning architecture for domain adaptation, the augmenting features are highly nonlinear transformation of the original ones (Glorot et al., 2011).\nUnderlying all these methods is the assumption that there exists a domain-invariant feature space such that the marginal distributions of two domains are the same in the new feature space. Thus, classifiers learnt in the new space will perform equally well on both the source and the target. Theoretical analysis have showed that the loss on the target domain for any labeling functions depends on the difference between the marginal distributions, thus justifying the need to identify a feature space such that the two domains look alike to each other (Ben-David et al., 2007; Mansour et al., 2009).\nWe hypothesize that this view and practice of twostage learning are restrictive. One possible fallacy is that maximizing the similarity in marginal distributions bear no direct consequence on (dis)similarities between posterior distributions. Thus, if there are multiple feature spaces where the source and the target domains have similar marginals, there is no reason to believe that a classifier trained on an arbitrarily chosen one would necessarily perform well on the target domain. As an extreme case, projecting features into irrelevant feature dimensions would make the two domains look very much alike!\nHence, the caveat is to retain discriminative information for constructing classifiers while we search for the domain-invariant feature space. This seems relatively straightforward to achieve if all we care is the discriminative information about the labels in the source domain. However, our main goal is to have good classifiers for the target domain. Thus, our challenge is how to be discriminative without labels?\nTo address this challenge, we propose a novel learning algorithm for unsupervised domain adaptation. As opposed to existing two-stage approaches where new feature spaces and classifiers are separately optimized, our approach combines the two in a single stage. Moreover, the new feature space is discriminative with re-\nspect to the target domain. We give a brief account in the following, leaving details to sections 2 and 3.\nMain Idea We assume discriminative clustering, namely, data in both the source and the target domains are tightly clustered and clusters corresponds class boundaries. For the same class, the clusters from the two domains are geometrically close to each other. Leveraging these assumptions, our formulation of learning the optimal feature space balances two forces: maximizing domain similarity that makes the source and the target domains look alike, and (approximately) minimizing the expected classification error on the target domain. We define those two forces with information-theoretical quantities: the domain similarity being the negated mutual information between all data and their binary domain labels (source versus target) and the expected classification error being the negated mutual information between the target data and its clusters (ie class) labels estimated from the source data. These two quantities are directly motivated by the nearest neighbor classifiers we use in the new feature space.\nWe show how simple gradient-based methods can be effectively used for numerical optimization to learn the optimal feature space. We evaluated extensively our approach on two benchmark tasks: visual object recognition and sentiment analysis of product reviews. On both of them, the proposed approach outperforms other state-of-the-arts methods significantly.\nContributions To summarize, we contribute to domain adaptation by advocating discriminative clustering as a possible mechanism for adaptation; cf. section 2. We hypothesize that existing approaches of two-stage learning can be significantly improved by taking those cluster structures into consideration. Thus, we propose an one-stage approach jointly learning a domain-invariant feature space and optimizing information-theoretic metrics directly related to discriminative classification on the target domain; cf. section 3. Our empirical results support strongly our modeling assumptions and hypothesis; cf. section 4."}, {"heading": "2. Discriminative Clustering for Domain Adaptation", "text": "At the core of our approach is the assumption of discriminative clustering. Specifically, we assume that, in a suitable feature space, 1) separation. Data in the source and the target domains are discriminatively clustered, where the cluster ids correspond to class labels; 2) Alignment. The clusters from the two domains that correspond to the same label are geomet-\nrically close. Fig. 1 illustrates these two assumptions and how they can be exploited for adaptation.\nArguably, the assumptions are more \u201crelaxed\u201d than those in existing works for adaptation. Specifically, they do not imply that the marginal distributions are the same across domains and certainly do not imply the same posterior distributions either. In fact, these assumptions are readily satisfiable in applications.\nFor example, many datasets exhibit multi-modal marginal distributions where the modes correspond to class labels, particularly if these data are sampled from a generative process of mixture models.\nAs we will show in the following, these two assumptions allow us to define quantitively what to be optimized \u2013 in our case, we would like to identify a domaininvariant feature space such that the expected misclassification error on the target data is minimized. Despite the paucity in labels from the target domain, we will show how the alignment assumption will allow us to define a proxy to the error so as to be optimized."}, {"heading": "3. Proposed Approach", "text": "In what follows, we are given N labeled instances from the source domain: {(xs, ys)} where xs \u2208 X \u2282 RD and ys takes a value from C class labels: ys \u2208 Y = {1, 2, . . . ,C}. We also have M unlabeled instances from the target domain: {xt} where xt \u2208 X . For simplicity, we assume xt and xs have the same domain X , thus\nthe same dimensionality. Extensions to more general cases are possible, analogous to (Kulis et al., 2011).\nOur objective is to construct a classifier f : x \u2208 X \u2192 y \u2208 Y. We would like the classifier performs well on the target domain DT from which xt is sampled. This is inherently an ill-posed problem as we do not have any labels from the target domain.\nTo overcome this difficulty, we leverage the discriminative clustering assumptions which we have previously described. We assume that there is a latent feature space z \u2208 Rd such that i) data in the source and target domains form well-separated clusters and the clusters correspond to labels; ii) the clusters from the source domain are geometrically close to those from the target domain if they are from the same labels.\nWe show how these assumptions can be used to derive information theoretical quantities which reflect data characteristics in each domain. These quantities are parameterized in terms of the latent feature which is in turn a linear transformation of the original feature x. We then show how to combine these quantities so that the optimal linear transformation can be learnt from data. We begin by describing a few key notions."}, {"heading": "3.1. Conditional models in the feature space", "text": "We consider the latent feature space induced by a linear transformation L \u2208 Rd\u00d7D. In the new feature space, we use k-nearest neighbors (kNN) to classify as we have assumed that data form well-separated clusters. Moreover, we choose k = 1 to avoid crossvalidating this parameter.\nThe squared distance between two points xi and xj in this feature space is thus given by\nd2ij = \u2016Lxi \u2212Lxj\u201622 = (xi \u2212 xj)TM(xi \u2212 xj) (1)\nwhere M = LTL defines a (low-rank) Mahalanobis distance metric in the original space.\nGiven a point xi and a set of data points {xj}, we use the following model\npij = e\u2212d 2 ij\u2211\nj 6=i e \u2212d2ij\n(2)\nto define the conditional probability of having xj as xi\u2019s nearest neighbor.\nThe above conditional model has been used in many contexts, including metric learning (Goldberger et al., 2004), dimensionality reduction (Hinton & Roweis, 2002), etc. Characterizing how close a point xi is to other points, this model gives rise to an estimate of the\nposterior p(yi = c|xi) for labeling xi with the class label c, assuming the class labels of {xj} are known,\np\u0302ic = \u2211 j 6=i pij\u03b4jc (3)\nwhere \u03b4jc is 1 if xj \u2019s label is c, and 0 otherwise. Since pij is a normalized probability, p\u0302ic is normalized too. For example, if the label of xi is known, \u2211 c p\u0302ic\u03b4ic would be the probability of correctly classifying xi."}, {"heading": "3.2. Discriminative clustering in the source", "text": "To derive a classifier that can perform well on the target domain, we would certainly need the classifier to perform well on the source domain because we have assumed that the two domains share similar clustering structures. Thus, our first desideratum is to minimize the expected classification error on the source domain, when we classify it using 1-NN. This error is estimated using the empirical average of the leave-one-out accuracy for any given point xs in the source domain DS :\n\u03b5s = 1\u2212 1\nN \u2211 s \u2211 c p\u0302sc\u03b4sc (4)\nNote that, if we minimize this error only and ignore the target domain, we will arrive at the metric learning technique in (Goldberger et al., 2004)."}, {"heading": "3.3. Discriminative clustering in the target", "text": "Since we do not have labels on the target domain, we cannot define the expected classification error as we did in eq. (4) for the source domain. How to be discriminative without using labels?\nConsider an instance xt from the target domain and all the instances {xs} from the source domain, the conditional model pts of eq. (2) gives rise to the probability of having a particular xs as the nearest neighbor of xt. Using this conditional model as well as the source labels to compute the posterior as in eq. (3) would not be the correct posterior for the target domain. However, if our assumptions about two sets of clusters being geometrically close indeed hold in the dataset, then the estimation p\u0302tc should be close to the true posterior.\nIf p\u0302tc approximates the true posterior well and our assumption that the target data is well clustered, then we can reasonably expect that the C-dimensional probability vector p\u0302t = [p\u0302t1, p\u0302t2, . . . , p\u0302tC] should look like an ideal posterior probability vector [0, 0, . . . , 1, . . . , 0] where the only nonzero element 1 occurs at the position corresponding to the correct label.\nSince we do not know the true label, we cannot measure directly the similarity of p\u0302t to the correct and\nideal posterior vector. Nonetheless, we can express our desideratum as reducing the entropy of p\u0302t such that it contains the least amount of confusing labels.\nLet H[p] denote the entropy of a probability vector p. If we minimize \u2211 tH[p\u0302t] only, we could arrive at a degenerate solution where every point xt is assigned to the same class. To avoid this, we instead maximize the mutual information between the data and the estimated label Y\u0302 using p\u0302,\nIt(X; Y\u0302 ) = H[p\u03020]\u2212 1\nM \u2211 t H[p\u0302t] (5)\nand the prior distribution p\u03020 is given by p\u03020 = 1/M \u2211 t p\u0302t. Note that using the empirical distribution of the labels in the source domain to estimate the prior p\u03020 could still lead to degenerate solutions when the labels are uniformly distributed.\nMinimizing the entropy (or similarly, maximizing the mutual information) has been previously studied in the context of (discriminative) clustering, cf. (Gomes et al., 2010; Dhillon et al., 2003). This criterion will identify a feature representation that classifiers can use to achieve a low lower-bound of misclassification error, due to Fano\u2019s inequality (Fisher III & Principe, 1998)."}, {"heading": "3.4. Discriminability: source versus target", "text": "The previous discussion on discriminative clustering in the target domain hinges on the assumption that clusters for the source and the target domain should not be too far from each other. We quantify this notion more precisely in the following. Conceptually, this notion is similar to the idea in existing works to make marginal distributions similar across domains.\nWhy such notion is desirable? To use the source domain\u2019s labels as an proxy to estimate the posterior probabilities for the target data (as in eq. (3)), we would desire the source and the target domain share some common probability supports in the feature space. In particular, consider the case we classify two instances xt and xt\u2032 from the target domain. They are deemed to have the same label c if there are plenty of labeled source data in class c in their neighborhoods. Then we would expect that with high likelihood, xt and xt\u2032 are in each other\u2019s nearest neighbors too \u2014 otherwise, the cluster corresponding to class c in the target domain would not be very \u201ctight\u201d.\nHaving instances from both domains in xt\u2019s nearest neighborhoods thus entails the following. If we create a binary classification problem and assign qi = 1 if xi is from the source and qi = 0 if xi from the target, then given xi, we cannot determine well above chance\nlevel where this instance comes from.\nInstead of constructing an actual binary classifier, we express our desideratum as minimizing the mutual information between the data instance X and its (binary) domain label Q. Analogous to eq. (5), the mutual information is given by,\nIst(X;Q) = H[q\u03020]\u2212 1\nN + M \u2211 i H[q\u0302i] (6)\nwhere q\u0302i is the two-dimensional posterior probability vector of assigning xi to either the source or the target, given all other data points from the two domains. Concretely, the probability is computed according to eq. (3), except the class label \u03b4jc being replaced by the domain label of xj . The estimated prior distribution q\u03020 is computed as 1/(N + M) \u2211 i q\u0302i.\nOne might wonder why we do not compute and minimize the expected error as in the source domain classification eq. (4). This is because we would like to leave some room for the possibility that a certain portion of data in either domain could be \u201coutliers\u201d to the other domain, and thus indeed distinguishable with respect to their origins. Minimizing domain classification error would have the adverse effect of forcing the two domains to be exactly the same. For instance, a degenerate solution would be to map every point to the origin of the feature space.\nWe mention in passing that it is found that the accuracy of a binary domain classifier reflects similarities between domains (Blitzer et al., 2007), thus approximating the original intractable combinatorial measure of similarities (Ben-David et al., 2007)."}, {"heading": "3.5. Learning and model selection", "text": "We have described three information-theoretical quantities: classification accuracies on the source domain \u03b5S of eq. (4), discriminative clustering on the target It(X; Y\u0302 ) of eq. (5), and discriminability between the source and the target Ist(X;Q) of eq. (6).\nThese quantities have been derived from our assumptions about the source and target domains, specifically, the discriminative clustering structures. They are all parameterized in the linear transformation L.\nWe learn the optimal L by balancing these quantities with the following optimization problem\nminimize \u2212 It(X; Y\u0302 ) + \u03bbIst(X;Q) subject to Trace(LTL) \u2264 d\n(7)\nwhere the constraint is to control the scale of distances computed using L.\nThe regularization coefficient \u03bb needs to be crossvalidated. We choose the optimal \u03bb that attains the minimum of \u03b5S . Intuitively, \u03b5S is defined on the source domain with labeled data and thus, more sensible to be used for model selection (Other ways of combining these quantities were also experimented, though the above performs the best in practice.)\nWe comment briefly on the difference between our formulation and the entropy minimization framework for semi-supervised learning (Grandvalet & Bengio, 2005). Their goal is to reduce uncertainty of labeling the unlabeled data. Thus, they use only the entropy term eq (3). More distinctively, they do not need to make the two domains look alike thus there is no need for them to learn a feature space, nor to include a term to minimize the discriminability between the domains."}, {"heading": "3.6. Numerical Optimization", "text": "Eq. (7) is non-convex optimization. We use gradientbased methods to optimize the objective function. While in theory the methods are susceptible to local optimum, we use heuristics to initialize: either the PCA of the target domain data, or the low-rank factorization of a discriminatively trained metric on the source data, such as the one in large margin nearest neighbor (LMNN) (Weinberger & Saul, 2009). In most cases, these heuristics work well and lead to substantially improved results over initialization points. Details are described in the Supplementary Material."}, {"heading": "3.7. Extensions", "text": "When the target domain has a few labeled instance, the domain adaptation problem is referred as semisupervised adaptation. Our approach can be readily extended to incorporate those labeled target domain instances. Details, including experimental results are described in the Supplementary Material."}, {"heading": "4. Experimental Results", "text": "We evaluate the proposed method on two benchmark tasks: object recognition and sentiment analysis of product reviews. We compare the method to baselines and other recently proposed ones for unsupervised domain adaptation (Gopalan et al., 2011; Blitzer et al., 2006; Pan et al., 2011). In the Supplementary Material, we report results on semi-supervised adaptation, where the target domain has a few labeled instances."}, {"heading": "4.1. Setup", "text": "We start by describing the datasets for the two tasks.\nObject recognition. We use four databases of object images: Caltech-256 (Griffin et al., 2007), Amazon (images from online merchants\u2019s catalogues), Webcam (low-resolution images by web cameras), and DSLR (high-resolution images by digital SLR cameras). The last three datasets were studied in (Gopalan et al., 2011; Saenko et al., 2010). Caltech-256 is added to increase the diversity of the domains.\nWe treat each dataset as a domain. There are 10 common object categories: backpack, coffee-mug, calculator, computer-keyboard, computer-monitor, computermouse, head-phones, laptop-101, touring-bike, and video-projector. There are 2533 images in total, with 8 to 151 images per category per domain.\nFollowing the experimental protocols in previous work (Saenko et al., 2010), we extract SURF features (Bay et al., 2006) and encode each image with a 800- bin histogram (the codebook is trained from a subset of Amazon images). The histograms are first normalized to have zero mean and unit standard deviation in each dimension.\nFor each pair of source and target domains, we conduct experiments in 20 random trials. In each trial, we randomly sample labeled data in the source domain as the training set, and unlabeled data in the target domain as the testing set. For semi-supervised domain adaptation, we also sample a few labeled examples in the target domain to augment the training set, see the Supplementary Material for details.\nSentiment analysis. We use the dataset that consists of Amazon product reviews on four product types: kitchen appliances, DVDs, books and electronics (Blitzer et al., 2007). Each product type is used as a separate domain. Each domain has 1,000 positive and 1,000 negative reviews. To reduce computational cost, we select top 400 words of the largest mutual information with the labels. We then represent each review with a 400-dimensional vector of term counts (ie, bag-ofwords). The vectors are normalized to have zero mean and unit standard deviation in each dimension.\nFor each pair of source and target domains, we conduct experiments in 10 random trials. In each trial, we randomly sample 1,600 labeled data in the source domain as the training set, and all data in the target domain as the testing set.\nClassification We learn the feature transformation L by solving the optimization problem eq. (7). We then transform all the data using the matrix and apply 1-nearest neighbor (1-NN) to classify instances from the target domain. 1-NN is used to avoid tuning the number of nearest neighbors. (In the Supplementary\nMaterial, we also report results of using SVMs.)\nHyperparameter tuning Our method has two hyper-parameters: the dimensionality of the new feature subspace and the regularization coefficient \u03bb in eq. (7). We cross-validate them using the model selection procedure described in section 3.5. The range of search for the dimensionality is {20, 40, 70, 100} and {0, 0.25, 1, 4, 16, 64} for \u03bb.\nFor baselines and other methods we have compared to, if there are hyper-parameters to be tuned, we either follow the procedures in those algorithms or give those methods the benefits of doubts by reporting their best performance by using labels from the target domain."}, {"heading": "4.2. Results on unsupervised adaptation", "text": "We compare extensively to several methods.\n\u2022 Baselines. We compare to PCA, where we project all data into the PCA directions computed on the target domain. We also compare to LMNN (Weinberger & Saul, 2009), where we train a large margin nearest neighbor classifier using only the source labeled data. Neither of these methods is developed for domain adaptation and their performances on target domains are indeed inferior to other methods, and especially ours. \u2022 Transfer Component Analysis (TCA) (Pan et al., 2011). This method finds a low-dimensional linear projection such that the source and the target domains have similar marginal distributions, regularized by preserving variances in all the data. To measure similarities in marginals, the method maps data to a kernel feature space. We use Gaussian RBF kernels. \u2022 Structural Correspondence Learning (SCL) (Blitzer et al., 2006). This method augments original features with linearly transformed features. The linear transformation is computed as the principal directions of parameters in binary classifiers predicting whether pivot features are present or not. In our experiments, we have used all 400 features as pivot features. We then train SVMs with the augmented feature vectors on the source domains and apply the resulting classifiers to the target domains. \u2022 Geodesic Flow Subspaces (GFS) (Gopalan et al., 2011). This method interpolates (on Grassman manifold) between the PCA subspaces computed on the source and the target domains respectively. The interpolated subspaces are then used to transform the original features to form super-vectors. The dimensionality of the super-vectors is then\nreduced before applying 1-NN for classification.\n\u2022 Metric Learning (Metric) (Saenko et al., 2010). This method learns a metric measuring the distance between data points using the correspondence information between the source and the target domains. Specifically, the correspondence is defined as data points with the same labels. Thus, this method uses labels from the target domains. Despite that, our results will show our method still outperforms Metric.\nTable 1 and Table 2 summarize the classification accuracies as well as standard errors of all the above methods, as well as ours (we did not apply SCL to object recognition as it is difficult to define what pivot features are for those types of data). We had chosen a subset of all pairs for saving experiment time. The best performing algorithm(s) (statistical significant up to one standard error) for each pair are in bold font.\nIn Table 1 on object recognition, our method performs the best on 5 out of 6 pairs, outperforming other competing methods with a large margin. On the DSLR-Amazon pair, our method performs worse than LMNN, but still significantly better than others.\nOf particular interest is that LMNN outperforms other methods specifically designed for domain adaptation (excluding ours). This confirms our hypothesis: the two-stage learning schemes adopted by TCA and GFS suffer from the fallacy that maximizing marginal similarity does not necessarily lead to well-performing classifiers on the target domain. In particular, we believe such methods could actually destroy discriminative information by forcing the domains to be similar.\nThe results thus support our argument that onestage learning, namely identifying jointly discriminative clustering and low-dimensional feature spaces, is crucial for domain adaptation.\nThe results on sentiment analysis in Table 2 also strongly support similar conclusions. Note that both SCL and our methods outperform other methods significantly. Our methods perform better on 2 out of 4 pairs, thought slightly worse than SCL on the other two. Exploring strengths and weakness of each of these\ntwo methods is a subject of future research."}, {"heading": "5. Related Work", "text": "Information-theoretical approach has been applied to semi-supervised learning (Grandvalet & Bengio, 2005) where the core idea is to reduce the confusability (among possible labels) on unlabeled data by classifiers trained on the labeled data. However, they have assumed that the data are drawn from the same distribution so there is no need to learn a domain-invariant feature space.\nRastrow et al. described an information-theoretical based criterion for model selection in domain adaptation (Rastrow et al., 2010). Model selection is a challenging problem when cross-validation is not possible due to the lack of labeled data on the target domain. However, their approach is two-stage: they refine model parameters on unlabeled data by minimizing the conditional entropy of the labeling function from the initial model tuned on the labeled source data. Consequently, their formulation does not learn an invariant feature space.\nOur work is also related to the recent study of regularized information maximization for discriminative clustering (Gomes et al., 2010). The authors there used a parametric model to compute the posterior probabilities of assigning a data point to various clusters. The objective is to find clustering assignments of all data points such that the mutual information between the data and the cluster ids are maximized. While their work is generalized to semi-supervised clustering, they do not consider domain adaptation, which has fundamentally different goals and constraints from semi-supervised learning, as pointed out previously. In particular, the above-mentioned work does not learn a new feature space."}, {"heading": "6. Conclusion", "text": "We propose an one-stage approach that jointly learns a domain-invariant feature space and optimizes information-theoretic metrics directly related to dis-\ncriminative classification on the target domain. Our empirical results support the validity of our modeling assumptions that data in both source and target domains are discriminatively clustered. We show that existing approaches where learning feature is decoupled from learning discriminative classifiers, can be significantly improved by taking the clustering structures into consideration. For future work, we plan to study discriminatively learning of nonlinear feature transformation for domain adaptation.\nAcknowlegements\nThis work was partially supported by DARPA D11AP00278, NSF IIS-1065243 and a USC Annenberg Fellowship (Y. Shi)."}], "references": [{"title": "SURF: Speeded up robust features", "author": ["H. Bay", "T. Tuytelaars", "L. Van Gool"], "venue": null, "citeRegEx": "Bay et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bay et al\\.", "year": 2006}, {"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": null, "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "Discriminative learning for differing training and test distributions", "author": ["S. Bickel", "M. Br\u00fcckner", "T. Scheffer"], "venue": "In Prof. of ICML, pp", "citeRegEx": "Bickel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "In Proc. of ACL,", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Semi-supervised learning, volume 2. MIT press", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A Zien"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Domain adaptation for statistical classifiers", "author": ["H. Daum\u00e9 III", "D. Marcu"], "venue": "JAIR, 26:101\u2013126,", "citeRegEx": "III and Marcu,? \\Q2006\\E", "shortCiteRegEx": "III and Marcu", "year": 2006}, {"title": "A methodology for information theoretic feature extraction", "author": ["J.W. Fisher III", "J.C. Principe"], "venue": "In Proc. IEEE World Congress on Comp. Intell.,", "citeRegEx": "III and Principe,? \\Q1998\\E", "shortCiteRegEx": "III and Principe", "year": 1998}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proc. of ICML,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Discriminative clustering by regularized information maximization", "author": ["R. Gomes", "A. Krause", "P. Perona"], "venue": "In NIP,", "citeRegEx": "Gomes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 2010}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "In Proc. of ICCV,", "citeRegEx": "Gopalan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2011}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "NIPS, 17:529\u2013236,", "citeRegEx": "Grandvalet and Bengio,? \\Q2005\\E", "shortCiteRegEx": "Grandvalet and Bengio", "year": 2005}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": "Technical report, California Institute of Technology,", "citeRegEx": "Griffin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Griffin et al\\.", "year": 2007}, {"title": "Stochastic neighbor embedding", "author": ["G. Hinton", "S.T. Roweis"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Hinton and Roweis,? \\Q2002\\E", "shortCiteRegEx": "Hinton and Roweis", "year": 2002}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["J. Huang", "A.J. Smola", "A. Gretton", "K.M. Borgwardt", "B. Scholkopf"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "In Proc. of CVPR,", "citeRegEx": "Kulis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "Proc. of COLT,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. on Knowl. Engi.,", "citeRegEx": "Pan and Yang,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang", "year": 2010}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "IEEE Trans. Neur. Nets.,", "citeRegEx": "Pan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2011}, {"title": "Dataset Shift in Machine Learning", "author": ["J. Qui\u00f1onero-Candela", "M. Sugiyama", "A. Schwaighofer"], "venue": null, "citeRegEx": "Qui\u00f1onero.Candela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Qui\u00f1onero.Candela et al\\.", "year": 2009}, {"title": "Unsupervised model adaptation using informationtheoretic criterion", "author": ["A. Rastrow", "F. Jelinek", "A. Sethy", "B. Ramabhadran"], "venue": "In Proc. HLT-NAACL,", "citeRegEx": "Rastrow et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rastrow et al\\.", "year": 2010}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In Proc. of ECCV,", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "J. of Stat. Plan. and Infer.,", "citeRegEx": "Shimodaira,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira", "year": 2000}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "JMLR, 10:207\u2013244,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}], "referenceMentions": [{"referenceID": 20, "context": "Techniques for addressing learning problems with mismatched distributions are often referred as domain adaptation, or sometimes transfer learning (Daum\u00e9 III & Marcu, 2006; Pan & Yang, 2010; Qui\u00f1onero-Candela et al., 2009).", "startOffset": 146, "endOffset": 221}, {"referenceID": 5, "context": "This goal sets domain adaptation apart from semi-supervised learning, whose primary goal is to improve the performance on the labeled data with unlabeled data (Chapelle et al., 2006).", "startOffset": 159, "endOffset": 182}, {"referenceID": 23, "context": "For instance, in covariate shift (Shimodaira, 2000; Bickel et al., 2007; Huang et al., 2007), the marginal distributions of the features are different across domains while the posterior distribution of the label remains the same.", "startOffset": 33, "endOffset": 92}, {"referenceID": 2, "context": "For instance, in covariate shift (Shimodaira, 2000; Bickel et al., 2007; Huang et al., 2007), the marginal distributions of the features are different across domains while the posterior distribution of the label remains the same.", "startOffset": 33, "endOffset": 92}, {"referenceID": 15, "context": "For instance, in covariate shift (Shimodaira, 2000; Bickel et al., 2007; Huang et al., 2007), the marginal distributions of the features are different across domains while the posterior distribution of the label remains the same.", "startOffset": 33, "endOffset": 92}, {"referenceID": 19, "context": "Other works have also followed similar paradigms (Pan et al., 2011; Gopalan et al., 2011).", "startOffset": 49, "endOffset": 89}, {"referenceID": 11, "context": "Other works have also followed similar paradigms (Pan et al., 2011; Gopalan et al., 2011).", "startOffset": 49, "endOffset": 89}, {"referenceID": 3, "context": "In the structural correspondence learning, the original features are first augmented with features that are more likely to be domain invariant and then a classifier is trained (Blitzer et al., 2006).", "startOffset": 176, "endOffset": 198}, {"referenceID": 8, "context": "Alternatively, in deep learning architecture for domain adaptation, the augmenting features are highly nonlinear transformation of the original ones (Glorot et al., 2011).", "startOffset": 149, "endOffset": 170}, {"referenceID": 1, "context": "Theoretical analysis have showed that the loss on the target domain for any labeling functions depends on the difference between the marginal distributions, thus justifying the need to identify a feature space such that the two domains look alike to each other (Ben-David et al., 2007; Mansour et al., 2009).", "startOffset": 261, "endOffset": 307}, {"referenceID": 17, "context": "Theoretical analysis have showed that the loss on the target domain for any labeling functions depends on the difference between the marginal distributions, thus justifying the need to identify a feature space such that the two domains look alike to each other (Ben-David et al., 2007; Mansour et al., 2009).", "startOffset": 261, "endOffset": 307}, {"referenceID": 16, "context": "Extensions to more general cases are possible, analogous to (Kulis et al., 2011).", "startOffset": 60, "endOffset": 80}, {"referenceID": 9, "context": "The above conditional model has been used in many contexts, including metric learning (Goldberger et al., 2004), dimensionality reduction (Hinton & Roweis, 2002), etc.", "startOffset": 86, "endOffset": 111}, {"referenceID": 9, "context": "Note that, if we minimize this error only and ignore the target domain, we will arrive at the metric learning technique in (Goldberger et al., 2004).", "startOffset": 123, "endOffset": 148}, {"referenceID": 10, "context": "(Gomes et al., 2010; Dhillon et al., 2003).", "startOffset": 0, "endOffset": 42}, {"referenceID": 4, "context": "We mention in passing that it is found that the accuracy of a binary domain classifier reflects similarities between domains (Blitzer et al., 2007), thus approximating the original intractable combinatorial measure of similarities (Ben-David et al.", "startOffset": 125, "endOffset": 147}, {"referenceID": 1, "context": ", 2007), thus approximating the original intractable combinatorial measure of similarities (Ben-David et al., 2007).", "startOffset": 91, "endOffset": 115}, {"referenceID": 11, "context": "We compare the method to baselines and other recently proposed ones for unsupervised domain adaptation (Gopalan et al., 2011; Blitzer et al., 2006; Pan et al., 2011).", "startOffset": 103, "endOffset": 165}, {"referenceID": 3, "context": "We compare the method to baselines and other recently proposed ones for unsupervised domain adaptation (Gopalan et al., 2011; Blitzer et al., 2006; Pan et al., 2011).", "startOffset": 103, "endOffset": 165}, {"referenceID": 19, "context": "We compare the method to baselines and other recently proposed ones for unsupervised domain adaptation (Gopalan et al., 2011; Blitzer et al., 2006; Pan et al., 2011).", "startOffset": 103, "endOffset": 165}, {"referenceID": 13, "context": "We use four databases of object images: Caltech-256 (Griffin et al., 2007), Amazon (images from online merchants\u2019s catalogues), Webcam (low-resolution images by web cameras), and DSLR (high-resolution images by digital SLR cameras).", "startOffset": 52, "endOffset": 74}, {"referenceID": 11, "context": "The last three datasets were studied in (Gopalan et al., 2011; Saenko et al., 2010).", "startOffset": 40, "endOffset": 83}, {"referenceID": 22, "context": "The last three datasets were studied in (Gopalan et al., 2011; Saenko et al., 2010).", "startOffset": 40, "endOffset": 83}, {"referenceID": 22, "context": "Following the experimental protocols in previous work (Saenko et al., 2010), we extract SURF features (Bay et al.", "startOffset": 54, "endOffset": 75}, {"referenceID": 0, "context": ", 2010), we extract SURF features (Bay et al., 2006) and encode each image with a 800bin histogram (the codebook is trained from a subset of Amazon images).", "startOffset": 34, "endOffset": 52}, {"referenceID": 4, "context": "We use the dataset that consists of Amazon product reviews on four product types: kitchen appliances, DVDs, books and electronics (Blitzer et al., 2007).", "startOffset": 130, "endOffset": 152}, {"referenceID": 19, "context": "\u2022 Transfer Component Analysis (TCA) (Pan et al., 2011).", "startOffset": 36, "endOffset": 54}, {"referenceID": 3, "context": "\u2022 Structural Correspondence Learning (SCL) (Blitzer et al., 2006).", "startOffset": 43, "endOffset": 65}, {"referenceID": 11, "context": "\u2022 Geodesic Flow Subspaces (GFS) (Gopalan et al., 2011).", "startOffset": 32, "endOffset": 54}, {"referenceID": 22, "context": "\u2022 Metric Learning (Metric) (Saenko et al., 2010).", "startOffset": 27, "endOffset": 48}, {"referenceID": 21, "context": "described an information-theoretical based criterion for model selection in domain adaptation (Rastrow et al., 2010).", "startOffset": 94, "endOffset": 116}, {"referenceID": 10, "context": "Our work is also related to the recent study of regularized information maximization for discriminative clustering (Gomes et al., 2010).", "startOffset": 115, "endOffset": 135}], "year": 2012, "abstractText": "We study the problem of unsupervised domain adaptation, which aims to adapt classifiers trained on a labeled source domain to an unlabeled target domain. Many existing approaches first learn domain-invariant features and then construct classifiers with them. We propose a novel approach that jointly learn the both. Specifically, while the method identifies a feature space where data in the source and the target domains are similarly distributed, it also learns the feature space discriminatively, optimizing an informationtheoretic metric as an proxy to the expected misclassification error on the target domain. We show how this optimization can be effectively carried out with simple gradient-based methods and how hyperparameters can be cross-validated without demanding any labeled data from the target domain. Empirical studies on benchmark tasks of object recognition and sentiment analysis validated our modeling assumptions and demonstrated significant improvement of our method over competing ones in classification accuracies.", "creator": "LaTeX with hyperref package"}}}