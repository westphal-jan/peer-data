{"id": "1206.6450", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Conditional Sparse Coding and Grouped Multivariate Regression", "abstract": "we study mixed problem of multivariate plots where the data values naturally abundant, and a regression matrix is directly be estimated for each group. by propose an introduction above which numerical product of low weighted method matrices is estimated below groups, whether a sparse linear combination of the dictionary elements is estimated between form a model fragment per region. we refer to this method as conditional abundance coding since it makes a modular procedure processing the response samples loosely conditioned on the subject vectors x. this capability captures the shared outputs across her group while reporting to the differences within each grouped. or exploits the same phenomenon behind sparse exponential approach has been successfully conducted in applied operations like computational neuroscience. we propose an algorithm for conditional sparse populations, analyze local theoretical properties yield terms involving predictive accuracy, and utilize experimental introduction of simulation and brain imaging experiments that compare the analysis technique onto reduced uncertainty regression.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (184kb)", "http://arxiv.org/abs/1206.6450v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["min xu", "john d lafferty"], "accepted": true, "id": "1206.6450"}, "pdf": {"name": "1206.6450.pdf", "metadata": {"source": "CRF", "title": "Conditional Sparse Coding and Grouped Multivariate Regression", "authors": ["Min Xu", "John Lafferty"], "emails": ["minx@cs.cmu.edu", "lafferty@galton.uchicago.edu"], "sections": [{"heading": "1. Introduction", "text": "Sparse coding, also called dictionary learning, is an approach to approximating a collection of signals by sparse linear combinations of a codewords chosen from a shared, learned dictionary. The method was proposed by Olshausen & Field (1996) for encoding natural images, with the motivation of developing a simple computational model of neural coding in the visual cortex. Through the use of sparsity and a large learned\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ndictionary of codewords, sparse coding is able to efficiently capture a rich collection of features that are common to a population of signals. Variants of sparse coding have enjoyed considerable success in computer vision (Elad & Aharon, 2006; Lee et al., 2007; Mairal et al., 2009; Yang et al., 2009; Zhou et al., 2010; Bengio et al., 2009).\nIn this paper we apply the intuition behind sparse coding to design a new procedure for multivariate regression with data that fall into possibly overlapping groups or tasks. In traditional multivariate regression, the data consist of a set of response vectors Y \u2208 Rq, and for each Y , a corresponding covariate vector X \u2208 Rp. In a vector autoregressive time series model, for instance, Y = Zt is a vector at time t, and X = Zt\u22121 is the vector at the previous step. In predicting brain activation patterns in neuroscience, Y might be the neural activations in different regions of the brain with X a vector of external stimuli. Under a linear model, Y = BX + \u01eb, where B \u2208 Rq\u00d7p is a matrix of parameters and \u01eb \u2208 Rq is a random, mean zero error vector.\nIn many applications, the data naturally occur in groups or tasks, and assuming the same model Y = BX + \u01eb for each group may be unjustified. For instance, in a non-stationary time series, the distribution of Y = Zt varies over time. In the neuroscience example, different people may have different neuronal activation patterns. In both cases it may be natural to place the data into possibly overlapping groups. More generally, the groups could be determined by any factor in the data or experimental design.\nIn settings where the input and output dimensions p and q are high, the number of parameters in B may be be too large to estimate accurately from limited data. One approach to estimating reduced complexity models is to perform a least squares regression with a rank constraint on the coefficient matrix B. The nuclear norm serves as a convex surrogate for low rank\nconstraints, and has be recently studied in the context of multivariate regression (Yuan et al., 2007; Negahban & Wainwright, 2011). For grouped data, a different model could be estimated for each group using this approach; however, carrying out separate regressions ignores commonality between the groups, and worsens the problem of limited data.\nOur approach is to estimate the parameter matrices as\nB\u0302(g) = K\u2211\nk=1\n\u03b1 (g) k Dk\nwhere each dictionary entry Dk is a low rank matrix, and \u03b1(g) = (\u03b1 (g) 1 , . . . , \u03b1 (g) K ) is a sparse vector; both {Dk} and {\u03b1(g)} are learned from data. The coefficients \u03b1\n(g) k are estimated for each group g, but the\n\u201ccodewords\u201d or \u201cdictionary elements\u201d Dk are shared across groups. This exploits the same intuition behind sparse coding for image analysis. Sparsity allows the dictionary entries Dk to specialize and capture predictive aspects of the data shared by many groups, while the coefficients \u03b1(g) tailor the model to the specific group g. Allowing the size K of the dictionary to be large enables a rich class of parameter matrices to be modeled, while a low rank condition on the individual codeword matrices Dk allows them to be estimated from limited data.\nWe perform both a \u201cpessimistic\u201d and \u201coptimistic\u201d analysis of our method. In the pessimistic analysis, the model may not be correct; that is, we do not assume any underlying common structure among the groups. In this case the model cannot achieve lower risk than the alternative of separate low rank regressions within each group. However, our analysis shows that the method suffers little excess risk relative to separate regressions. In the optimistic analysis, when the learned dictionary has captured common structure between the groups, the method produces an accurate estimator with much lower sample complexity than required by low rank regression. In both analyses, we measure statistical accuracy through non-asymptotic bounds on the excess risk R(D,\u03b1(g)) \u2212 R(B\u2217). We show that the new procedure is effective and practical with experiments on simulated data and brain imaging data, reported in Section 6."}, {"heading": "2. Related Work", "text": "Mairal et al. (2010) have studied a different way of using dictionary learning for supervised tasks; in this approach one first encodes data X and then uses the encoding to perform classification or regression. Our work is more related to multi-task learning (Caruana, 1997; Evgeniou & Pontil, 2004) and is in particular a\ngeneralization of a model by Argyriou et al. (2006). They require that all \u03b1(g) have the same sparsity pattern, so that all groups use the same small subset of dictionary elements. By allowing different groups to use different subsets of the dictionary, our model is much more flexible, though at the cost of requiring a non-convex optimization. Kang et al. (2011) used mixed integer programming to generalize the model of Argyriou et al. (2006) although our formulation is still more flexible and our optimization simpler. The approach of Liu et al. (2010) could be adapted to our setting, although their notion of task-relatedness is very different from ours.\nExisting approaches to theoretical analysis of multitask learning differ significantly from our analysis by focusing on PAC-learnability with respect to a more abstract notion of task-relatedness (Maurer, 2006; Ben-David & Schuller, 2003). Theoretical analysis of sparse coding is rather limited. Some work studies the generalization error of dictionary learning (Vainsencher et al., 2010; Maurer & Pontil, 2010) and the local correctness of the non-convex objective for dictionary learning (Geng et al., 2011). Jeong & Kim (2009) consider sparse approximability and prove an information theoretic lower bound on sparse approximability of general p-dimensional vectors. They further show, non-constructively, that the lower bound can be achieved via an optimally constructed dictionary. We instead consider sparse approximability of a variety of structured spaces with respect to a dictionary that could plausibly be learned by a practical procedure."}, {"heading": "3. Problem Formulation", "text": "In this work we focus on problems where the data are naturally grouped. Suppose we have G groups, indexed by g = 1, . . . , G. Let X (g) i \u2208 Rp, Y (g) i \u2208 Rq denote the explanatory and response variables for the ith sample in group g. For each group, we let B\u2217(g) = arg minB(g) R(B\n(g)) be the oracle regression matrix where we define\nR(B(g)) = EX(g),Y (g)\u2016Y (g) \u2212B(g)X(g)\u20162F .\nFor convenience, we will assume the sample size n is the same for all groups, noting that more generally it will vary with g. Let X(g) = (X (g) 1 , . . . ,X (g) n ) \u2208 R p\u00d7n and Y (g) = (Y (g)1 , . . . , Y (g) n ) \u2208 Rq\u00d7n, with the n samples of group g arranged as matrix columns.\nOur goal is to estimate B\u2217(g). We consider estimates of the form B\u0302(g) = \u2211K\nk=1 \u03b1\u0302 (g) k Dk where each Dk is\na low rank matrix, and \u03b1\u0302(g) = (\u03b1\u0302 (g) 1 , . . . , \u03b1\u0302 (g) K ) is an\nestimated sparse vector. The codewords, or dictionary entries, Dk are themselves estimated from data using nuclear norm regularization from data pooled across groups, as described in Section 4."}, {"heading": "4. Conditional Sparse Coding", "text": "The basic idea underlying conditional sparse coding is to learn a collection of low rank matrices {D1, ...,DK} (a dictionary) and estimate B\u0302(g) as a sparse linear combination of the dictionary entries. We optimize the overall objective function f(\u03b1,D) defined by\nf(\u03b1,D) =\n1\nG\nG\u2211\ng=1\n{ 1\nn\n\u2225\u2225Y (g) \u2212 ( K\u2211\nk=1\n\u03b1 (g) k Dk\n) X(g) \u2225\u22252 F + \u03bb\u2016\u03b1(g)\u20161 }\nwhere the optimization min\u03b1 minD\u2208CD(\u03c4) f(\u03b1,D) is carried out over the set\nCD(\u03c4) = { D \u2208 Rq\u00d7p : \u2016D\u2016\u2217 \u2264 \u03c4 and \u2016D\u20162 \u2264 1 } .\nThe \u21131 norm penalty induces sparsity on the \u03b1 vectors and the nuclear-norm restriction forces the matrices Dk to be low rank. The spectral norm constraint ensures no particular dictionary entry can be too large, and serves as an identifiability constraint; a similar constraint in sparse coding requires that all dictionary vectors must have norm no larger than one.\nThe objective function is biconvex but not jointly convex in \u03b1 and D. Thus, we follow the standard sparse coding approach and alternately optimize over {\u03b1(g)} with fixed {Dk}, and optimize over {Dk} with fixed {\u03b1(g)}. We refer to the algorithm as conditional sparse coding (CSC) since it is a coding procedure for the response vectors Y conditioned on the covariate vectors X.\nAlgorithm 1 Conditional Sparse Coding (CSC)\nInput: Data {(Y (g),X(g)}g=1,...,G, regularization parameters \u03bb and \u03c4 .\n1. Initialize dictionary {D1, ...,DK} as random rank one matrices.\n2. Alternate between the following steps until convergence of f(\u03b1,D):\na. Encoding step: {\u03b1(g)} \u2190 argmin\u03b1(g) f(\u03b1,D) b. Learning step:\n{Dk} \u2190 argminDk\u2208CD(\u03c4) f(\u03b1,D)\nThe encoding step is equivalent to an independent \u21131constrained least squares fit, or lasso optimization, for\neach group g:\nmin \u03b1(g)\u2208RK\n1\nn\nn\u2211\ni=1\n\u2225\u2225\u2225Y (g)i \u2212 G\u2211\ng=1\n\u03b1 (g) k (DkX (g) i )\n\u2225\u2225\u2225 2\n2 + \u03bb\u2016\u03b1(g)\u20161.\n(4.1) A variety of algorithms are available to solve the lasso efficiently, notably iterative soft thresholding, a form of coordinate descent (Friedman et al., 2007).\nFor optimizing the dictionary entries, we designed both a projected gradient descent algorithm and a fast iterative shrinkage and thresholding algorithm (FISTA) following Beck & Teboulle (2009). A complication is that since the constraint set CD(\u03bb) is an intersection of nuclear norm and spectral norm balls, the projection needs to be done with care. We leave details of the optimization algorithms and the projection procedure to the appendix.\nRemarks on implementation details Although learning the dictionary is computationally intensive, fitting the coefficients to the dictionary is very fast due to efficient lasso optimization algorithms. Thus, an easy way to speed up CSC is to learn the dictionary with a smaller number of groups. The CSC optimization, being non-convex, is sensitive to initialization. We suggest random initialization both because our theoretical guarantees assume random initialization and because it works well in practice.\nIn sparse coding, one never picks a dictionary size K equal to or greater than number of vectors to encode to avoid the trivial solution of letting each vector be a dictionary element itself. In CSC however, one can choose K > G because of the nuclear-norm constraint on the dictionary entries. Based both on theory and experimental results, we recommend that \u03c4 is held to a constant between 1 and 0.5, and that \u03bb is then chosen with cross-validation."}, {"heading": "5. Theoretical Analysis", "text": "To get a more complete understanding of CSC, we perform both a pessimistic analysis and an optimistic analysis. In the pessimistic analysis, we do not assume that our model is correct, and we do not assume any underlying common structure among the the groups. It is obvious that, under the general pessimistic setting, we cannot achieve higher statistical accuracy with CSC than with the alternative of estimating separate low-rank matrices for each group. Our pessimistic analysis provides a simple rule for determining, in the worst case, how much worse CSC is than the alternative.\nIn the optimistic analysis, we focus on a very specific\nsetting where we only have to fit the coefficients to a pre-existing set of learned dictionary entries. We assume that the learned dictionary has thus captured common structure that exists among the groups. We show that in this setting CSC can produce an accurate estimator with fewer samples than the alternative of estimating separate matrices.\nIn all of our analyses, we measure statistical accuracy through non-asymptotic bounds on the excess risk R(D,\u03b1(g)) \u2212 R(B\u2217). For clarity of presentation, we will use same symbols c and C to represent possibly different, generic constants in the theorem statements.\nBefore beginning the analysis, we enumerate and justify the underlying assumptions.\nA1. For all groups g, X(g) and Y (g) are zero mean Gaussian random vectors. Let \u03a3 be the (p + q) \u00d7 (p + q) covariance matrix \u03a3 = E[(X(g), Y (g))(X(g), Y (g))T]. Then the spectral norm \u2016\u03a3\u20162 is a constant independent of n. A2. For all groups g, \u2016B\u2217(g)\u2016\u2217 \u2264 L and B\u2217(g) is of rank at most r.\nA3. The sample size satisfies n \u2265 (p + q).\nWe make assumption A1 only to leverage results on concentration of measure; we do not use any other properties of the Gaussian distribution. Our analysis will thus easily extend to subgaussian random vectors. Assumption A2 is merely notation, allowing us to state our bounds in terms of L and r. Assumption A3 is made so that many of the results in our pessimistic analysis can be stated more compactly; we do not make this assumption in our optimistic analysis.\nIt should be emphasized that since we are carrying out an excess risk analysis, we do not require incoherence conditions on our samples X (g) 1 , . . . ,X (g) n , as are often assumed in high-dimensional statistical analysis of sparsity.\nBecause we will repeatedly compare the excess risk rate of CSC against estimating separate matrices, we first prove an excess risk bound for using nuclear-norm regularization in each group.\nTheorem 5.1. Suppose that assumptions A1, A2, A3 hold. Let\nB\u0302(g) = argmin {B : \u2016B\u2016\u2217\u2264L}\n1\nn\nn\u2211\ni=1\n\u2016Y (g)i \u2212BX (g) i \u201622.\nThen with probability at least 1 \u2212 exp(\u2212cp), we have that\nmax g=1,...,G\nR(B\u0302(g)) \u2212R(B\u2217(g)) \u2264 CL2 \u221a (p + q) log(nG)\nn\nwhere c, C are constants depending only on \u2016\u03a3\u20162 as defined in A1.\nWe provide proof sketches of all theorems in Section 5.3."}, {"heading": "5.1. Pessimistic Analysis", "text": "Let Dlearn, \u03b1 learn(g) \u03bb be the dictionary and coefficients output by Conditional Sparse Coding. The results of this section establish bounds on the excess risk R(Dlearn, \u03b1\nlearn(g) \u03bb ) \u2212 R(B\u2217(g)). We stress that we do\nnot assume Dlearn, \u03b1 learn(g) \u03bb is the global minimizer of the non-convex CSC objective f(\u03b1,D). We use only the fact that the learned dictionary and coefficients achieve a lower objective than the random initial dictionary.\nBefore we state our main theorem, it is instructive to first consider the excess risk bound we would obtain if using only the random initial dictionary entries with oracle coefficients, with no additional dictionary learning.\nProposition 5.1. Suppose that assumptions A1, A2, A3 hold. For a given sparsity level s, define\n\u03b1 init(g) oracle\n= argmin {\u03b1(g):\u2016\u03b1(g)\u20160\u2264s,\u2016\u03b1(g)\u20161\u2264L \u221a s} R(Dinit, \u03b1(g)).\nLet K \u2265 max(n, r(p + q)), and \u03bb \u2264 \u221a\nlog K n . Suppose\ns \u2264 r(p + q). Then with probability at least 1 \u2212 1K ,\nmax g=1,...,G\nR(Dinit, \u03b1 init(g) oracle ) \u2212R(B\u2217(g))\n\u2264 CL2 ( (p + q) log(GK)\nn\n)s/r(p+q)\nwhere C is a constant depending only on \u2016\u03a3\u20162 as defined in A1.\nSetting s = r(p+q)2 , we observe that a large enough dictionary of random rank one matrices with the (non-sparse) oracle coefficients yields an excess risk bound that, up to multiplicative constants, matches the bound in Theorem 5.1\u2014the best we can hope for. But because the oracle coefficients \u03b1 init(g) oracle are not sparse, the learned coefficients \u03b1 init(g) \u03bb will be a poor estimate of the oracle coefficients, and the resulting excess risk may be significantly larger.\nProposition 5.1 and the preceding discussion motivate the need for learning the dictionary\u2014we may improve statistical accuracy if we can customize the dictionary, allowing reconstruction of B\u2217(g) from the dictionary using sparse coefficients. Our main theorem in this subsection formalizes this intuition.\nTheorem 5.2. Suppose assumptions A1, A2, A3 hold. Suppose K \u2265 max(n, r(p + q)), \u03bb \u2264 \u221a\nlog K n ,\nand \u03c4 \u2264 1. Then with probability at least 1 \u2212 1K ,\nmax g=1,...,G\nR(Dlearn, \u03b1learn\u03bb ) \u2212R(B\u2217(g))\n\u2264 C max(L2, \u2016\u03b1learn\u03bb \u201621) \u221a (p + q) log(GK)\nn .\nThis result implies that if the learned coefficients are sparse, that is, if \u2016\u03b1learn(g)\u03bb \u20161 \u2264 L, then the excess risk of conditional sparse coding is, up to a multiplicative constant factor, no greater that the excess risk for estimating separate low-rank matrices within each group. Of course, the excess risk can be worse if \u2016\u03b1learn(g)\u03bb \u20161 increases with (p + q) or n; we cannot rule out this possibility because the dictionary learning optimization is nonconvex and does not admit a direct analysis. We note in our experimental section, however, that \u03b1 learn(g) \u03bb is very sparse in our simulations. We note also that our proof uses critically the fact that our algorithm places a nuclear-norm constraint on the dictionary entries, thus showing that the constraint is necessary to reduce overfitting when learning the dictionary.\nTheorem 5.2 and Proposition 5.1 suggest a rule of thumb in applying conditional sparse coding. If the sparsity levels of the coefficients do not decrease with the iterations of dictionary learning, then the resulting statistical accuracy may be poor."}, {"heading": "5.2. Optimistic Analysis", "text": "For our optimistic analysis, we consider the specific setting where the dictionary is already learned and we analyze the excess risk incurred when we fit the coefficients from data that were not used in the dictionary learning process.\nA4. The learned dictionary {Dlearn1 , ...,DlearnK } is independent of the data X\n(g) i for all groups g and\nitems i = 1, ..., n.\nWith the dictionary fixed, we let\n\u03b1 learn(g) oracle \u2261 argmin {\u03b1(g):\u2016\u03b1(g)\u20161\u2264L} R(Dlearn, \u03b1(g))\nbe the sparse coefficients that minimize the true risk. We can then interpret the oracle excess risk R(Dlearn, \u03b1 learn(g) oracle )\u2212R(B\u2217(g)) as a measure of the extent to which the oracle regression matrices B\u2217(g) share structure, and the learned dictionary has captured this structure.\nTheorem 5.3. Suppose assumptions A1, A2, A4 hold. Suppose \u03bb \u2264 \u221a\nlog K n . Then with probability at\nleast 1 \u2212 1n ,\nmax g=1,...,G\nR(Dlearn, \u03b1 learn(g) \u03bb ) \u2212R(B\u2217(g)) \u2264 C max(L2, \u2016\u03b1learn(g)\u03bb \u201621) \u221a log(npKG)\nn\n+ R(Dlearn, \u03b1 learn(g) oracle ) \u2212R(B\u2217(g))\nwhere C is some constant depending only on \u2016\u03a3\u20162 as defined in A1.\nUnder the optimistic assumption that the excess risk R(Dlearn, \u03b1 learn(g) oracle ) \u2212 R(B\u2217(g)) is small, that is, that the dictionary has effectively learned the common information among the groups, then we require on the order of \u221a p + q times fewer samples here to achieve the same excess risk as in Theorem 5.2. If we further assume that \u2016\u03b1learn(g)\u03bb \u20161 does not increase with p and q, meaning that the oracle coefficients are sparse, then the excess risk in the optimistic setting is also lower than the bound in Theorem 5.1."}, {"heading": "5.3. Proof Sketches", "text": "Proof sketch of Theorem 5.1. The crux of our argument is the following uniform generalization error bound.\nLemma 5.1. With probability at least 1 \u2212 exp(\u2212cp), for all matrices B(g) such that \u2016B(g)\u2016\u2217 \u2264 L, R(B(g))\u2212 R\u0302(B(g)) \u2264 CL2 \u221a (p+q) log(Gn)\nn + Ru, where c, C are\nconstants depending only on \u2016\u03a3\u20162 as defined in A1, and Ru is a term that does not depend on B (g).\nWe prove Lemma 5.1 by combining the technique of Greenshtein & Ritov (2004) with a concentration result from random matrix theory which states that for independent subgaussian random vectors Z1, ..., Zn, \u2016 1n \u2211n i=1 ZiZ T i \u2212 \u03a3Z\u20162 \u2264 C \u221a p n with probability at least 1\u2212exp\u2212cp for some constants c, C. Theorem 5.1 then follows from a standard argument.\nProof sketch of Proposition 5.1. The proof is constructive. It uses a theoretical procedure, similar to orthogonal matching pursuit, but infeasible to implement, to produce a \u03b1(g) with sparsity level s for the random rank 1 dictionary entries so that the reconstruction error \u2016B\u2217(g) \u2212 \u2211Kk=1 Dinit\u03b1 (g) k \u2016F and the associated excess risk would be sufficiently low. Since \u03b1initoracle is the optimal set of s-sparse coefficients, we can upper bound its risk with the risk of our constructed coefficients. We do not prove that our bound is tight, but analysis by Jeong & Kim (2009) suggests that our\nbound cannot be significantly improved. We discuss this point further in the appendix.\nProof sketch of Theorem 5.2. We first rewrite the excess risk as\nR(Dlearn, \u03b1 learn(g) \u03bb ) \u2212R(B\u2217(g))\n= R(Dlearn, \u03b1 learn(g) \u03bb ) \u2212 R\u0302(Dlearn, \u03b1 learn(g) \u03bb ) (5.1)\n+ R\u0302(Dlearn, \u03b1 learn(g) \u03bb ) \u2212 R\u0302(Dinit, \u03b1 init(g) oracle ) (5.2) + R\u0302(Dinit, \u03b1 init(g) oracle ) \u2212R(Dinit, \u03b1 init(g) oracle ) (5.3) + R(Dinit, \u03b1 init(g) oracle ) \u2212R(B\u2217(g)) (5.4)\nwhere \u03b1 init(g) oracle is as defined in Proposition 5.1 with s set to r(p+q)2 .\nWe then bound (5.1) using Lemma 5.1. To control (5.2), we observe that although the dictionary learning procedure is nonconvex, it is guaranteed to improve the objective. Thus, we have immediately that (5.2) is at most \u03bb\u2016\u03b1init(g)oracle \u20161. A bound on (5.4) follows from Proposition 5.1. Term (5.3) requires the following lemma concerning uniform generalization error of learning coefficients for a fixed dictionary.\nLemma 5.2. Let D1, ...,DK be a fixed set of dictionary entries with \u2016Dk\u2016\u2217 \u2264 1. We have that with probability at least 1 \u2212 1n , for all coefficients \u03b1(g), maxg R(D,\u03b1\n(g)) \u2212 R\u0302(D,\u03b1(g)) \u2264 C\u2016\u03b1(g)\u201621 \u221a log(GKpn) n + Ru where C is a constant depending only on \u2016\u03a3\u20162 as defined in A1 and Ru is a term that does not depend on \u03b1(g)\nProof sketch of Theorem 5.3. The proof is straightforward by combining Assumption A4, Theorem 5.3, and Lemma 5.2."}, {"heading": "6. Experiments", "text": "The main purpose of our experiments is to compare conditional sparse coding against reduced-rank regression. The experiments also illustrate that the coefficients estimated by CSC are indeed sparse and that the dictionary entries are low rank."}, {"heading": "6.1. Simulation Data", "text": "We generate data using a linear model Y (g) = B\u2217(g)X(g)+\u01eb(g) where \u01eb(g) \u223c N(0, \u03c32Iq) and each B\u2217(g) is a p \u00d7 p square matrix. We build a random design matrix X(g) by drawing each sample X\n(g) i \u223c N(0, Ip).\nWe consider three different settings:\n1. In the structured case, we construct each B\u2217(g)\nas a random 3-sparse linear combination of a set\nWe measure performance of the algorithms in terms of both estimation error 1G \u2211G g=1 \u2016B\u2217(g) \u2212 B\u0302(g)\u2016F and prediction error R\u0302test(B\u0302 (g)), which is computed from a large test set of (X(g), Y (g)) pairs. We compare CSC against performing separate reduced rank regressions for each group using nuclear norm-regularization.\nIt can be seen from Figure 6.1 that when the parameter matrices {B\u2217(g)} have significant common structure, CSC easily outperforms separate regressions with either different or the same design for each group. CSC performs worse in the unstructured case, as expected, but is still competitive with separate regressions.\nIn Figure 6.1, we show the sparsity of the coefficients together with the ranks of the learned dictionary en-\ntries, as a function of iterations of alternation in the algorithm. It is seen that (1) CSC does not require many iterations to converge, (2) the coefficients become increasingly sparse, and (3) although the ranks of the dictionary entries may increase, the learned dictionary entries are still relatively low rank.\nSparsity of coefficients Rank of dictionary entries\nWe note that in Section 8 of the Appendix, we also perform simulations with overlapping groups."}, {"heading": "6.2. fMRI Data", "text": "The dataset, gathered by Mitchell et al. (2008), comprises the brain activity patterns of 9 human subjects when presented with a single concrete English noun. We down-sample the original neural signal by retaining only one measurement in every 4 \u00d7 3 \u00d7 4 voxel region of the brain. More precisely, we have X as a design matrix of neural signals with dimension (p = 434) \u00d7 (n = 60) and Y as the response matrix with dimension (q = 192) \u00d7 (n = 60) of semantic features of the 60 nouns being shown to the subjects. We let each subject be a group and hence we have that G = 9.\nThe goal is to predict the semantic features of the noun being shown to the subject, based only on the neural signal of the subject\u2019s brain. The predicted semantic features can then be used to guess which word the subject was viewing and thus \u201cread the subject\u2019s mind.\u201d\nFollowing Mitchell et al. (2008), we use hold-two-out cross-validation for evaluation. In each run of the experiment, we hold out two words, using the remaining\n58 words for training, and then compute three evaluation metrics: 2 vs. 2 classification, 1 vs. 2 classification, and squared error. Let y1, y2 be the semantic feature vectors of the heldout words. Let y\u03021, y\u03022 be the predicted semantic feature vectors. We say that we correctly made a 2 vs. 2 classification if d(y1, y\u03021) + d(y2, y\u03022) < d(y1, y\u03022) + d(y2, y\u03021) and we say that we correctly made a 1 vs. 2 classification if both d(y1, y\u03021) < d(y1, y\u03022) and d(y2, y\u03022) < d(y2, y\u03021). If we make random predictions, then the expected 1 vs. 2 classification accuracy is 0.25 and the expected 2 vs. 2 classification accuracy is 0.5. Our parameters are tuned by separate cross-validation trials. We used K = 20 dictionary entries.\nIn Figure 3, we compare the performance of CSC to separate trace-norm-regularized regressions for each subject. CSC often shows significant improvement in both 2 vs. 2 and 1 vs. 2 classification tasks, with very few cases of significant degradation. In terms of squared error, CSC shows improvement for most subjects, although on average, the improvement is statistically insignificant.\nAlthough there is indeed sharing of dictionary entries across the various groups (subjects), it is important to mention that the pattern of sharing is unstable from trial to trial. Figure 6.2 shows two patterns of group-dictionary utilization derived from the \u03b1(g) coefficients. We see that in the first trial, subject 3 shares significantly with subject 7, while subject 1 shares with no other subjects; in the second trial, subject 3 shares with subject 5 and subject 1 shares with subjects 6 and 9. The instability is possibly due to the low sample size. As a result of this instability, we cannot deduce subject-subject similarities from the dictionary utilization patterns."}, {"heading": "Acknowledgements", "text": "Research supported in part by NSF grant IIS-1116730 and AFOSR contract FA9550-09-1-0373."}], "references": [{"title": "Multi-task feature learning", "author": ["Argyriou", "Andreas", "Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "NIPS 19,", "citeRegEx": "Argyriou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2006}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["Beck", "Amir", "Teboulle", "Marc"], "venue": "SIAM J. Imaging Sciences,", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "Exploiting task relatedness for multiple task learning", "author": ["Ben-David", "Shai", "Schuller", "Reba"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Ben.David et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2003}, {"title": "Group sparse coding", "author": ["Bengio", "Samy", "Pereira", "Fernando", "Singer", "Yoram", "Strelow", "Dennis"], "venue": "NIPS 22. MIT Press,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Multitask learning", "author": ["Caruana", "Rich"], "venue": null, "citeRegEx": "Caruana and Rich.,? \\Q1997\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1997}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Elad and Aharon,? \\Q2006\\E", "shortCiteRegEx": "Elad and Aharon", "year": 2006}, {"title": "Regularized multi\u2013task learning", "author": ["Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "Proceedings of the Tenth ACM SIGKDD,", "citeRegEx": "Evgeniou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2004}, {"title": "Pathwise coordinate optimization", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "H\u00f6fling", "Holger", "Tibshirani", "Robert"], "venue": "Ann. Appl. Stat.,", "citeRegEx": "Friedman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2007}, {"title": "On the local correctness of l1-minimization for dictionary", "author": ["Geng", "Quan", "Wang", "Huan", "Wright", "John"], "venue": null, "citeRegEx": "Geng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Geng et al\\.", "year": 2011}, {"title": "Persistency in high dimensional linear predictor-selection and the virtue of overparametrization", "author": ["E. Greenshtein", "Y. Ritov"], "venue": null, "citeRegEx": "Greenshtein and Ritov,? \\Q2004\\E", "shortCiteRegEx": "Greenshtein and Ritov", "year": 2004}, {"title": "Learning with whom to share in multitask feature learning", "author": ["Kang", "Zhuoliang", "Grauman", "Kristen", "Sha", "Fei"], "venue": null, "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Learning temporal causal graphs for relational time-series analysis", "author": ["Liu", "Yan", "Niculescu-Mizil", "Alexandru", "Lozano", "Aurelie", "Lu", "Yong"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Non-local sparse models for image restoration", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Task-driven dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "arXiv: 1009.5358,", "citeRegEx": "Mairal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Bounds for linear multi-task learning", "author": ["Maurer", "Andreas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maurer and Andreas.,? \\Q2006\\E", "shortCiteRegEx": "Maurer and Andreas.", "year": 2006}, {"title": "K-dimensional coding scheme in hilbert spaces", "author": ["Maurer", "Andreas", "Pontil", "Massimiliano"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Maurer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maurer et al\\.", "year": 2010}, {"title": "Predicting human brain activity associated with the meanings of nouns", "author": ["Mitchell", "Tom M", "Shinkareva", "Svetlana V", "Carlson", "Andrew", "Chang", "Kai-Min", "Malave", "Vicente L", "Mason", "Robert A", "Just", "Marcel Adam"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "author": ["S. Negahban", "M.J. Wainwright"], "venue": "Annals of Statistics,", "citeRegEx": "Negahban and Wainwright,? \\Q2011\\E", "shortCiteRegEx": "Negahban and Wainwright", "year": 2011}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["Olshausen", "Bruno A", "Field", "David J"], "venue": "Nature,", "citeRegEx": "Olshausen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Olshausen et al\\.", "year": 1996}, {"title": "The sample complexity of dictionary learning", "author": ["D. Vainsencher", "S. Mannor", "A.M. Bruckstein"], "venue": null, "citeRegEx": "Vainsencher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vainsencher et al\\.", "year": 2010}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In CVPR,", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Dimension reduction and coefficient estimation in multivariate linear regression", "author": ["Yuan", "Ming", "A. Ekici", "Lu", "Zhaosong", "R. Monteiro"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "Yuan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2007}, {"title": "Image classification using super-vector coding of local image descriptors", "author": ["Zhou", "Xi", "Yu", "Kai", "Zhang", "Tong", "Huang", "Thomas S"], "venue": "In ECCV\u201910,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "Variants of sparse coding have enjoyed considerable success in computer vision (Elad & Aharon, 2006; Lee et al., 2007; Mairal et al., 2009; Yang et al., 2009; Zhou et al., 2010; Bengio et al., 2009).", "startOffset": 79, "endOffset": 198}, {"referenceID": 20, "context": "Variants of sparse coding have enjoyed considerable success in computer vision (Elad & Aharon, 2006; Lee et al., 2007; Mairal et al., 2009; Yang et al., 2009; Zhou et al., 2010; Bengio et al., 2009).", "startOffset": 79, "endOffset": 198}, {"referenceID": 22, "context": "Variants of sparse coding have enjoyed considerable success in computer vision (Elad & Aharon, 2006; Lee et al., 2007; Mairal et al., 2009; Yang et al., 2009; Zhou et al., 2010; Bengio et al., 2009).", "startOffset": 79, "endOffset": 198}, {"referenceID": 3, "context": "Variants of sparse coding have enjoyed considerable success in computer vision (Elad & Aharon, 2006; Lee et al., 2007; Mairal et al., 2009; Yang et al., 2009; Zhou et al., 2010; Bengio et al., 2009).", "startOffset": 79, "endOffset": 198}, {"referenceID": 21, "context": "constraints, and has be recently studied in the context of multivariate regression (Yuan et al., 2007; Negahban & Wainwright, 2011).", "startOffset": 83, "endOffset": 131}, {"referenceID": 0, "context": "Our work is more related to multi-task learning (Caruana, 1997; Evgeniou & Pontil, 2004) and is in particular a generalization of a model by Argyriou et al. (2006). They require that all \u03b1 have the same sparsity pattern, so that all groups use the same small subset of dictionary elements.", "startOffset": 141, "endOffset": 164}, {"referenceID": 0, "context": "Our work is more related to multi-task learning (Caruana, 1997; Evgeniou & Pontil, 2004) and is in particular a generalization of a model by Argyriou et al. (2006). They require that all \u03b1 have the same sparsity pattern, so that all groups use the same small subset of dictionary elements. By allowing different groups to use different subsets of the dictionary, our model is much more flexible, though at the cost of requiring a non-convex optimization. Kang et al. (2011) used mixed integer programming to generalize the model of Argyriou et al.", "startOffset": 141, "endOffset": 474}, {"referenceID": 0, "context": "Our work is more related to multi-task learning (Caruana, 1997; Evgeniou & Pontil, 2004) and is in particular a generalization of a model by Argyriou et al. (2006). They require that all \u03b1 have the same sparsity pattern, so that all groups use the same small subset of dictionary elements. By allowing different groups to use different subsets of the dictionary, our model is much more flexible, though at the cost of requiring a non-convex optimization. Kang et al. (2011) used mixed integer programming to generalize the model of Argyriou et al. (2006) although our formulation is still more flexible and our optimization simpler.", "startOffset": 141, "endOffset": 555}, {"referenceID": 0, "context": "Our work is more related to multi-task learning (Caruana, 1997; Evgeniou & Pontil, 2004) and is in particular a generalization of a model by Argyriou et al. (2006). They require that all \u03b1 have the same sparsity pattern, so that all groups use the same small subset of dictionary elements. By allowing different groups to use different subsets of the dictionary, our model is much more flexible, though at the cost of requiring a non-convex optimization. Kang et al. (2011) used mixed integer programming to generalize the model of Argyriou et al. (2006) although our formulation is still more flexible and our optimization simpler. The approach of Liu et al. (2010) could be adapted to our setting, although their notion of task-relatedness is very different from ours.", "startOffset": 141, "endOffset": 667}, {"referenceID": 19, "context": "Some work studies the generalization error of dictionary learning (Vainsencher et al., 2010; Maurer & Pontil, 2010) and the local correctness of the non-convex objective for dictionary learning (Geng et al.", "startOffset": 66, "endOffset": 115}, {"referenceID": 8, "context": ", 2010; Maurer & Pontil, 2010) and the local correctness of the non-convex objective for dictionary learning (Geng et al., 2011).", "startOffset": 109, "endOffset": 128}, {"referenceID": 8, "context": ", 2010; Maurer & Pontil, 2010) and the local correctness of the non-convex objective for dictionary learning (Geng et al., 2011). Jeong & Kim (2009) consider sparse approximability and prove an information theoretic lower bound on sparse approximability of general p-dimensional vectors.", "startOffset": 110, "endOffset": 149}, {"referenceID": 7, "context": "1) A variety of algorithms are available to solve the lasso efficiently, notably iterative soft thresholding, a form of coordinate descent (Friedman et al., 2007).", "startOffset": 139, "endOffset": 162}, {"referenceID": 16, "context": "The dataset, gathered by Mitchell et al. (2008), comprises the brain activity patterns of 9 human subjects when presented with a single concrete English noun.", "startOffset": 25, "endOffset": 48}, {"referenceID": 16, "context": "Following Mitchell et al. (2008), we use hold-two-out cross-validation for evaluation.", "startOffset": 10, "endOffset": 33}], "year": 2012, "abstractText": "We study the problem of multivariate regression where the data are naturally grouped, and a regression matrix is to be estimated for each group. We propose an approach in which a dictionary of low rank parameter matrices is estimated across groups, and a sparse linear combination of the dictionary elements is estimated to form a model within each group. We refer to the method as conditional sparse coding since it is a coding procedure for the response vectors Y conditioned on the covariate vectors X. This approach captures the shared information across the groups while adapting to the structure within each group. It exploits the same intuition behind sparse coding that has been successfully developed in computer vision and computational neuroscience. We propose an algorithm for conditional sparse coding, analyze its theoretical properties in terms of predictive accuracy, and present the results of simulation and brain imaging experiments that compare the new technique to reduced rank regression.", "creator": "dvips(k) 5.96.1 Copyright 2007 Radical Eye Software"}}}