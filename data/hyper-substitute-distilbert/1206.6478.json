{"id": "1206.6478", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Maximum Margin Output Coding", "abstract": "in this result we realize output coding uses multi - label prediction. for a multi - label product mixer to represent improved, it is important requiring codewords for storing compression parameters significantly significantly different beside each machine. typical classical meantime, along using traditional modulation styles, codewords in output length tended to be predicted from the input, so it is also critical to have a predictable template encoding.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (347kb)", "http://arxiv.org/abs/1206.6478v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yi zhang 0010", "jeff g schneider"], "accepted": true, "id": "1206.6478"}, "pdf": {"name": "1206.6478.pdf", "metadata": {"source": "META", "title": "Maximum Margin Output Coding", "authors": ["Yi Zhang", "Jeff Schneider"], "emails": ["yizhang1@cs.cmu.edu", "schneide@cs.cmu.edu"], "sections": [{"heading": null, "text": "To find output codes that are both discriminative and predictable, we first propose a max-margin formulation that naturally captures these two properties. We then convert it to a metric learning formulation, but with an exponentially large number of constraints as commonly encountered in structured prediction problems. Without a label structure for tractable inference, we use overgenerating (i.e., relaxation) techniques combined with the cutting plane method for optimization.\nIn our empirical study, the proposed output coding scheme outperforms a variety of existing multi-label prediction methods for image, text and music classification."}, {"heading": "1. Introduction", "text": "In traditional channel coding (Cover & Thomas, 1991; Costello & Forney, 2007), a message is encoded into an alternative (and usually redundant) representation so that it can be recovered accurately after being transmitted through a noisy channel. Error-correcting output coding (ECOC) applies the idea of channel coding to multi-class classification (Dietterich & Bakiri, 1995;\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nAllwein et al., 2001) and more recently to multi-label prediction (Hsu et al., 2009; Tai & Lin, 2010; Zhang & Schneider, 2011): we encode the output into a codeword, learn models to predict the codeword, and then recover the correct output from noisy predictions.\nIn this paper, we study output coding for multi-label prediction and focus on two important issues. First, the coding needs to be discriminative : encodings for different outputs should be significantly different from each other, so that the codeword for the correct output will not be confused with incorrect ones, even under noisy predictions. This corresponds to the concept of code distance in coding theory and is related to good error-correcting capabilities (Cover & Thomas, 1991).\nSecond, output codes should be predictable . In output coding, codewords need to be predicted from the input (instead of being actually transmitted through a channel), so it is critical that codewords are easy to predict. From the channel coding perspective, having predictable codewords (and thus low prediction errors) corresponds to reducing the channel error. In multi-label prediction, finding predictable codewords provides an opportunity to exploit the dependency structure in the label space (Zhang & Schneider, 2011).\nTo design output codes that are both discriminative and predictable, we propose a max-margin formulation defined on the encoding transform. For each sample, the prediction from the input should be close to the encoding of the correct output, and at the same time, the prediction should also be far away from the encoding of any incorrect output. This is naturally captured by maximizing the margin between the prediction distance to correct and incorrect encodings.\nWe then convert this formulation to a metric learning problem of finding the optimal distance metric in the label space, but with an exponentially large number of constraints as commonly encountered in structured prediction problems. In multi-label prediction, howev-\ner, the output space does not provide a structure for tractable inference, and we use overgenerating (i.e., relaxation) techniques combined with the cutting plane method to optimize the metric learning formulation. The encoding and decoding operations can be derived from the optimal distance metric in the label space.\nWe conduct our experiments on multi-label classification of images, text and music. Empirical results show that the proposed output coding scheme outperforms a variety of recent multi-label prediction methods."}, {"heading": "2. Multi-Label Output Codes: Framework and Existing Methods", "text": "In this section, we introduce the general framework for multi-label output coding. Then we review three recently-proposed output codes (Hsu et al., 2009; Tai & Lin, 2010; Zhang & Schneider, 2011), where the encoding is based on random projections, principal component analysis and canonical correlation analysis, respectively. We also argue that these existing output coding schemes are not designed to optimize both discriminability and predictability of the codewords."}, {"heading": "2.1. Framework", "text": "An output coding scheme usually contains three parts: encoding, prediction and decoding. Consider a set of p input variables x \u2208 X \u2286 Rp and a set of q output variables y \u2208 Y \u2286 Rq. In multi-label classification, y will denote the label vector, and thus y \u2208 Y = {0, 1}q. We have a set of n training examples: D = (X,Y) = {(x(i),y(i))}ni=1, where X and Y are matrices of size n\u00d7 p and n\u00d7 q, respectively.\nEncoding. Following previous work (Hsu et al., 2009; Tai & Lin, 2010; Zhang & Schneider, 2011), we consider linear encoding. In this case, the encoding transform can be defined by a set of d linear projections\nV = (v1,v2 . . . ,vd) (1)\nwhere d is the number of projections, each vk (k = 1, 2, . . . , d) is a q \u00d7 1 vector representing a projection direction in the label space, and V is a q \u00d7 d matrix.\nGiven the projection vectors V, the codeword z for an example (x,y) is defined:\nz = VTy = (vT1 y, . . . ,v T d y) T (2)\nwhere z is a d \u00d7 1 vector. Alternatively, we can also include the q original labels y = (y1, . . . , yq) into the codeword z, and in this case we have:\nz = [Iq,V] Ty = (y1, . . . , yq,v T 1 y, . . . ,v T d y) T (3)\nwhere Iq is a q\u00d7q identity matrix, V is a q\u00d7d matrix, and z is a (q + d)\u00d7 1 vector.\nPrediction. After defining the encoding projections {vk}dk=1, we then learn prediction models from training samples {(x(i),y(i))}ni=1 to predict the codeword. For a label projection vTk y in the codeword, a regression model is usually considered:\nm\u0302k \u2190 learn regression({(x(i),vTk y(i))}ni=1) (4)\nand for an original label yj (j = 1, 2, . . . , q), a classifier can be learned from training samples:\np\u0302j \u2190 learn classifier({(x(i), y(i)j )} n i=1) (5)\nGiven a new sample x, a regression model m\u0302k predicts:\nm\u0302k(x) = E(v T k y|x) (6)\nand a classifier p\u0302j predicts:\np\u0302j(x) = (p\u0302j0(x), p\u0302j1(x)) (7)\nwhere\np\u0302j0(x) = P (yj = 0|x) (8) p\u0302j1(x) = P (yj = 1|x) (9)\nDecoding. Given a new testing sample x, the decoding procedure recovers the unknown label vector y from our prediction for the codeword z. The prediction contains {m\u0302k(x)}dk=1 and optionally {p\u0302j(x)} q j=1:\ny\u0302\u2190 decoding(x, {vk}dk=1, {m\u0302k(x)}dk=1, {p\u0302j(x)} q j=1)\n(10) The decoding is usually achieved by maximizing a probability function or minimizing a loss function defined on possible label vector y. Since y \u2208 Y = {0, 1}q, this optimization is usually combinatorial in nature and intractable. As a result, certain approximation is required to obtain the solution y\u0302, e.g., relaxing y into a continuous domain and then rounding the relaxed solution (Hsu et al., 2009; Tai & Lin, 2010) or using approximate inference (Zhang & Schneider, 2011)."}, {"heading": "2.2. Coding with compressed sensing", "text": "Multi-label compressed sensing (Hsu et al., 2009) is one of the earliest works that formally defines a multilabel output code. For encoding, each projection vector vk \u2208 Rq (k = 1, 2, . . . , d) is randomly generated as in compressed sensing (Donoho, 2006; Candes, 2006), e.g., a vector with i.i.d. Gaussian or Bernoulli entries. Thus, the codeword z = (vT1 y, . . . ,v T d y) T contains random projections of the label vector y.\nDecoding follows the sparse approximation algorithms in compressed sensing. Two popular classes are convex relaxation such as `1 penalized least squares (Tropp, 2006), and iterative greedy algorithms such as CoSaMP (Needell & Tropp, 2008). For example, an `1 penalized convex relaxation solves the following problem:\ny\u0302\u2190 argmin y\u2208Rq\n1\n2 d\u2211 k=1 (vTk y \u2212 m\u0302k(x))2 + \u03bb q\u2211 j=1 |yj | (11)\nwhere {m\u0302k(x)}dk=1 are predictions for the codeword z = (vT1 y, . . . ,v T d y) T , and the `1 penalty \u2211q j=1 |yj | = ||y||1 promotes the sparsity of the solution. Note that this problem is solved in relaxed space y \u2208 Rq.\nUse of random projections is justified in compressed sensing, e.g., by the restricted isometry property, that if the true signal y is sufficiently sparse, one can recover y from only a small number of random projections. However, from the output coding perspective, random projections do not specifically promote either discriminative or predictable codewords, and thus may not be the most effective method of output coding."}, {"heading": "2.3. Coding with principal component analysis", "text": "Given the n training examples, principal label space transformation (Tai & Lin, 2010) uses the top d principal components in the label space as the encoding projections:\n{vk}dk=1 \u2190 top d principal components(Y) (12)\nwhich is solved by performing SVD on the label matrix Y and taking the top d right singular vectors. The codeword z = (vT1 y, . . . ,v T d y)\nT contains the top d coordinates of y in the principal component space.\nGiven predicted codeword z\u0302 = (m\u03021(x), . . . , m\u0302d(x)) T for a test sample x, decoding is performed by projecting z\u0302 back to coordinates in the original label space and then rounding them element-wise to 0s and 1s:\ny\u0302\u2190 round(Vz\u0302) (13)\nNote that coding with principal components can potentially produce discriminative codewords. The top d principal components provide a coordinate system that keeps as much sample variance as possible by any d-dimensional projections. Therefore, generated codewords for training samples tend to be spread out and far away from each other, although this does not exactly maximize the minimum codeword distance.\nHowever, coding with principal components does not promote code predictability. Indeed, finding encoding\nprojections as in eq. (12) is solely based on the label matrix Y and does not involve the input X. As a result, this may generate codewords with large code distance but difficult to predict from the input."}, {"heading": "2.4. Coding with canonical correlation analysis", "text": "Predictability for multi-label output codes is addressed in recent work (Zhang & Schneider, 2011), where output projections are obtained by canonical correlation analysis. CCA tries to find an input projection u \u2208 Rp in the feature space and an output projection v \u2208 Rq in the label space such that the projected variables uTx and vTy are maximally correlated:\nargmax u\u2208Rp,v\u2208Rq\nuTXTYv\u221a (uTXTXu)(vTYTYv)\n(14)\nThis can be solved as a generalized eigenvalue problem, and the top d pairs of eigenvectors {(uk,vk)}dk=1 contain the encoding projections {vk}dk=1.\nThe codeword in this method is defined as z = (y1, . . . , yq,v T 1 y, . . . ,v T d y)\nT . For a new sample x, regression predictions for label projections are {m\u0302k(x)}dk=1 and classification predictions for original labels are {p\u0302j0(x), p\u0302j1(x)}qj=1. Decoding is performed by maximizing a joint probability function (including d Gaussian potentials from regression and q Bernoulli potentials from classifiers), or equivalently minimizing the function (Zhang & Schneider, 2011):\ny\u0302\u2190 argmin y\u2208{0,1}q\n1\n2 d\u2211 k=1 (vTk y \u2212 m\u0302k(x))2 \u03c3\u03022k\n+ \u03bb q\u2211 j=1 yj log( p\u0302j0(x) p\u0302j1(x) ) (15)\nwhere \u03c3\u03022k is the estimated mean squared error for regression model m\u0302k. Since the problem is defined on the label space y \u2208 {0, 1}q, approximate inference such as mean-field approximation is used for optimization.\nCoding with canonical correlation analysis improves the code predictability by choosing the projection directions that are maximally correlated with the input. However, this criterion does not optimize the discriminability of the generated codewords. In other words, codewords of different outputs may be close to each other, leading to inadequate error-correcting capabilities. Consequently, even a small amount of prediction error can significantly affect the decoding result."}, {"heading": "3. Maximum Margin Output Coding", "text": "In this section we propose a max-margin output coding scheme where the encoding transform promotes both\ndiscriminability and predictability of the codewords."}, {"heading": "3.1. A Max-Margin Formulation", "text": "As before, codewords are predicted using regression:\nM\u0302(x) = (m\u03021(x), . . . , m\u0302d(x)) T (16)\nwhere each m\u0302k() (k = 1, . . . , d) is a univariate regression function for predicting vTk y, which is learned as in (4), and M\u0302() is the corresponding multivariate regression function for the entire codeword VTy.\nFor each sample i, the codeword VTy(i) should be both predictable and discriminative. For predictability, we want M\u0302(x(i)) to be close to the correct codeword VTy(i). For discriminability, we want the correct codeword VTy(i) to have a large distance to any incorrect codeword VTy,\u2200y 6= y(i). In the context of prediction with output coding, it is even more strightforward and effective if the prediction M\u0302(x(i)) itself has a large distance to any incorrect codeword VTy,\u2200y 6= y(i).\nBased on these goals, we propose the following maxmargin formulation on output projections V:\nargmin V\u2208Rq\u00d7d,{\u03bei}ni=1\n1 2 ||V||2F + C n n\u2211 i=1 \u03bei (17)\ns.t. ||M\u0302(x(i))\u2212VTy(i)||22 +4(y(i),y)\u2212 \u03bei (18)\n\u2264 ||M\u0302(x(i))\u2212VTy||22, \u2200y \u2208 {0, 1}q,\u2200i \u03bei \u2265 0, \u2200i\nwhere || ||F is the Frobenius norm, || ||2 is the `2 norm, C is a regularization parameter, 4(y(i),y) is the hamming distance between binary vectors, and {\u03bei}ni=1 are slack variables, each for a training sample. With the help from slack variables, constraint (18) requires that for any sample i, the prediction distance to the correct codeword, denoted by ||M\u0302(x(i)) \u2212 VTy(i)||22, must be smaller than the prediction distance to any codeword ||M\u0302(x(i)) \u2212VTy||22 by a margin of at least 4(y(i),y). Note that this constraint encourages both small prediction distance to the correct codeword and large prediction distance to incorrect codewords, and hence promotes predictable and discriminative codes.\nTo simplify this formulation, we assume the regression functions M\u0302(x) = (m\u03021(x), . . . , m\u0302d(x))\nT are linear and estimated by least squares. Then given training samples (X,Y), we define the p\u00d7 q projection matrix P:\nP = (XTX)\u22121XTY (19)\nA small amount of regularization can be added to the diagonal of XTX for numerical stability. Using P, the\nregression functions can be written in closed form:\nm\u0302k(x) = [Pvk] Tx, k = 1, 2, . . . , d (20)\nand M\u0302(x) = [PV]Tx (21)\nPlugging eq. (21) into problem (17), we have the following max-margin formulation that is completely defined on projections V and slack variables {\u03bei}ni=1:\nargmin V\u2208Rq\u00d7d,{\u03bei}ni=1\n1 2 ||V||2F + C n n\u2211 i=1 \u03bei (22)\ns.t. ||VT (PTx(i) \u2212 y(i))||22 +4(y(i),y)\u2212 \u03bei \u2264 ||VT (PTx(i) \u2212 y)||22, \u2200y \u2208 {0, 1}q,\u2200i\n\u03bei \u2265 0, \u2200i"}, {"heading": "3.2. Metric Learning Formulation", "text": "Problem (22) is a quadratic program with quadratic constraints, and we first convert it to a metric learning problem. Define q \u00d7 q matrix Q:\nQ = VVT (23)\nwhich is the Mahalanobis distance metric induced by V. Also, define a set of new feature vectors:\n\u03c6iy = P Tx(i) \u2212 y, \u2200y \u2208 {0, 1}q,\u2200i (24)\nNow we formulate the metric learning problem as:\nargmin Q\u2208S+q ,{\u03bei}ni=1\n1 2 trace(Q) + C n n\u2211 i=1 \u03bei (25)\ns.t. \u03c6Tiy(i)Q\u03c6iy(i) +4(y (i),y)\u2212 \u03bei\n\u2264 \u03c6TiyQ\u03c6iy, \u2200y \u2208 {0, 1}q,\u2200i \u03bei \u2265 0, \u2200i\nwhere Q \u2208 S+q is positive semidefinite. The objective function and constraints are linear in Q and {\u03bei}ni=1.\nWe briefly show the equivalence between problem (22) and (25) as follows. For any feasible solution V to (22), we can define Q = VVT \u2208 S+q . Also, for any feasible solution Q to (25), since Q is positive semidefinite and thus has no negative eigenvalue, we can define V as:\nV = Q 1 2 = UD 1 2 (26)\nwhere the q \u00d7 q matrix U contains (as columns) the q eigenvectors of Q, and D is the diagonal matrix of eigenvalues. Given this one-to-one mapping between V and Q, we have trace(Q) = ||V||2F and \u03c6TiyQ\u03c6iy = ||VT (PTx(i) \u2212 y)||22. Therefore, any feasible (or optimal) solution to (25) gives a feasible (or optimal) solution to (22), and vice versa."}, {"heading": "3.3. Incorporating Original Labels and Their Classifiers", "text": "As shown in eq. (3), the codeword can also include q original labels, i.e., z = (y1, . . . , yq,v T 1 y, . . . ,v T d y)\nT . Classifiers {p\u0302j}qj=1 can be learned to predict original labels as in (5), and the decoding algorithm can make use of both regression and classifier outputs, e.g., as in eq. (15). In this case, the encoding projection should also be aware of the original labels (y1, . . . , yq) in the codeword, so that the projection part (vT1 y, . . . ,v T d y) can provide complementary information.\nTo adapt our max-margin formulation (25) to this new information, we assume that classifiers {p\u0302j}qj=1 have already been learned, and thus for each sample x we know the classifier output p\u0302j0(x) = P (yj = 0|x) and p\u0302j1(x) = P (yj = 1|x). We have the new formulation:\nargmin Q\u2208S+q ,{\u03bei}ni=1\n1 2 trace(Q) + C n n\u2211 i=1 \u03bei (27)\ns.t. \u03c6Tiy(i)Q\u03c6iy(i) \u2212 logP (y (i)|x(i)) +4(y(i),y)\u2212 \u03bei\n\u2264 \u03c6TiyQ\u03c6iy \u2212 logP (y|x(i)), \u2200y \u2208 {0, 1}q,\u2200i \u03bei \u2265 0, \u2200i\nwhere\nP (y|x(i)) = q\u220f j=1 P (yj |x(i)) = q\u220f j=1 p\u0302jyj (x (i)) (28)\nis the joint probability of label vector y = (y1, . . . , yq) on sample x(i) predicted by classifiers {p\u0302j}qj=1.\nIn this new formation (27), \u03c6TiyQ\u03c6iy is extended into \u03c6TiyQ\u03c6iy \u2212 logP (y|x(i)). Recall that \u03c6TiyQ\u03c6iy is equivalent to ||VT (PTx(i) \u2212 y)||22 in (22), which is the distance between the regression prediction on the ith sample and the encoding of the label vector y. We expect that the correct label vector y(i) should lead to lower values on this term than other y. Similarly, logP (y|x(i)) is the log-probability of y predicted by classifiers on sample i, and we expect that y(i) should give higher values on this term than other label vectors y. As a result, we now use the combined term \u03c6TiyQ\u03c6iy\u2212logP (y|x(i)) to measure the margin between correct and incorrect outputs. The main outcome of this new formulation is that distance metric Q will focus on the constraints where\u2212logP (y|x(i)) alone is not strong enough to ensure the margin. In other words, the output coding concentrates on the cases where classifiers {p\u0302j}qj=1 alone tend to misclassify."}, {"heading": "3.4. Cutting Plane Method with Overgenerating", "text": "In this section we consider how to solve problem (27). This problem involves an exponentially large number of constraints due to the combinatorial nature of the label space {0, 1}q. As studied in structured prediction (Tsochantaridis et al., 2004; Taskar et al., 2003), problem (27) could be solved efficiently, e.g., by the cutting-plane method, if a computationally tractable separate oracle exists to determine which of the exponentially many constraints is most violated (Tsochantaridis et al., 2004). However, without a specific structure (e.g., a chain or a tree) in the label space to enable efficient inference, the separate oracle for problem (27) is computationally intractable.\nTo address this issue, we use overgenerating (i.e., relaxation) (Finley & Joachims, 2008) with the cutting plane method. To use overgenerating technique, we need to relax \u2200y \u2208 {0, 1}q in the constraint of (27) to a continuous domain, e.g., \u2200y \u2208 [0, 1]q. However, 4(y(i),y) and logP (y|x(i)) in (27) are only defined on y \u2208 {0, 1}q. To handle this, we redefine 4(y(i),y) as\n4\u0303(y(i),y) = ||y(i) \u2212 y||1 = q\u2211 j=1 |y(i)j \u2212 yj | (29)\nThen noticing logP (y|x(i)) = \u2211q j=1 logP (yj |x(i)), we redefine:\nlogP\u0303 (y|x(i)) = q\u2211 j=1 logP\u0303 (yj |x(i)) (30)\nwhere each logP\u0303 (yj |x(i)) is the linear interpolation of logP (yj = 0|x(i)) and logP (yj = 1|x(i)).\nUsing (29) and (30), the new relaxed problem is:\nargmin Q\u2208S+q ,{\u03bei}ni=1\n1 2 trace(Q) + C n n\u2211 i=1 \u03bei (31)\ns.t. \u03c6Tiy(i)Q\u03c6iy(i) \u2212 logP\u0303 (y (i)|x(i)) + 4\u0303(y(i),y)\u2212 \u03bei\n\u2264 \u03c6TiyQ\u03c6iy \u2212 logP\u0303 (y|x(i)), \u2200y \u2208 [0, 1]q,\u2200i \u03bei \u2265 0, \u2200i\nwhere \u2200y \u2208 {0, 1}q in (27) is relaxed to \u2200y \u2208 [0, 1]q.\nThis new problem can be solved by the cutting plane method, because the separate oracle (i.e., finding the most violated constraint for each sample i) is:\nargmin y\u2208[0,1]q\n\u03c6TiyQ\u03c6iy \u2212 logP\u0303 (y|x(i))\u2212 4\u0303(y(i),y) (32)\nwhere logP\u0303 (y|x(i)) and 4\u0303(y(i),y) are linear in y, and \u03c6TiyQ\u03c6iy is quadratic in y given \u03c6iy defined as (24). So (32) is a simple box-constrained quadratic program."}, {"heading": "3.5. Encoding and Decoding", "text": "After solving Q in (31), encoding projections are obtained as (26), and one can choose d, the number of projections, by keeping only the first d columns of V in (26) for any d \u2264 q. The codeword as in (3) includes original labels, and decoding is performed as (15)."}, {"heading": "4. Empirical Study", "text": "Data. We perform experiments on three real-world data sets1: an image data set (Scene), a text data set (Medical) and a music data set (Emotions). Scene is an image collection for outdoor scene recognition. Each image is represented by 294 dimensional color features and labeled as: beach, sunset, fall foliage, field, mountain and urban. Emotions is a music classification problem. Each song is represented by 72 rhythmic and timbre features, and tagged with six emotions: amazed, happy, relaxed, quiet, sad and angry. Medical is a clinical text collection, where each document is represented by 1449 words and labeled with ICD-9-CM codes. Many labels in Medical are rare, so we select the 10 most common labels to study.\nMethods. We compare the proposed max-margin output coding scheme to several recently proposed multi-label output codes as well as a number of other multi-label classification methods:\n\u2022 Binary relevance (BR). This baseline method learns to classify each label independently. It is also called one-vs-all decomposition. \u2022 Coding with compressed sensing (CodingCS) (Hsu et al., 2009). As reviewed in Section 2.2, this method uses random projections for encoding and sparse approximation for decoding. Specifically, we use CoSaMP (Needell & Tropp, 2008) for decoding. \u2022 Coding with PCA (CodingPCA) (Tai & Lin, 2010). As reviewed in Section 2.3, this method uses principal components for encoding, and PCA reconstruction and rounding for decoding. \u2022 Coding with PCA-Redundant (CodingPCA-R). CodingPCA does not include original labels into the codeword. We also try this option to produce more redundancy as in eq. (3). Decoding follows eq. (15). \u2022 Coding with CCA (CodingCCA) (Zhang & Schneider, 2011). As reviewed in Section 2.4, this method uses CCA for encoding. Decoding follows eq. (15). \u2022 Calibrated label ranking (CLR) (Fu\u0308rnkranz et al., 2008). This method combines both one-vs-one and one-vs-all classifiers for multi-label classification. It can also be considered as an output coding method.\n1http://mulan.sourceforge.net/\n\u2022 Multi-label learning by exploiting label dependency (LEAD) (Zhang & Zhang, 2010). This method learns a Bayes network on labels and use it to capture label dependency in multi-label classification. \u2022 Max-Margin coding (MaxMargin). Our max-margin coding formulation where encoding is obtained by solving (31) and (26). Decoding follows eq. (15).\nEvaluation measures. We consider three evaluation measures for multi-label classification:\n\u2022 Subset accuracy: rates of correctly classifying all the labels. It is difficult to achieve high subset accuracy. \u2022 Macro-averaged F-1 score: calculate the F-1 score for each label and take the average over labels. F-1\nscore is popular since the distribution of positives and negatives for a label is usually imbalanced. \u2022 Micro-averaged F-1 score: aggregate true positives, true negatives, false positives and false negatives over labels, and then calculate an overall F-1 score.\nExperimental settings. On each data set, we perform 30 random runs and report means and standard errors of each evaluation measure. The number of training samples in each random run is set to 300.\nFor CodingCS, the number of projections d is set to 100 to provide highly redundant codewords. For CodingPCA, CodingPCA-R, CodingCCA and MaxMargin, the number of output projections is set to the maximum\npossible number: the number of original labels.\nFor all methods, base regression models are ridge regression and base classifiers are `2-penalized logistic regression, and their regularization parameters are chosen by cross validation. For LEAD, the Bayes net is learned using the score-based searching algorithm in the Bayesian Net Toolbox2. For decoding that follows (15), \u03bb is set to 1, i.e., classifiers and regression models are equally weighted in decoding. The parameter C in (31) is set to 106. Most methods need to round their final predictions into 0/1 assignments (e.g., from a probability forecast or a relaxed solution to the la-\n2http://code.google.com/p/bnt/\nbel assignment), and in these cases we use 0.5 as the threshold without further optimization.\nEmpirical Results. Results for the Scene data set are shown in Table 1 - Table 3; results for Medical are shown in Table 4 - Table 6; results for Emotions are shown in Table 7 - Table 9. Each table contains one evaluation measure. From the results we can see:\n\u2022 BR provides a solid baseline with good performance. \u2022 CodingCS generally underperforms, indicating that\nencoding with random projections is not effective.\n\u2022 CodingPCA-R outperforms CodingPCA because CodingPCA-R uses more redundant codewords. \u2022 LEAD\u2019s performance is not stable across data sets. Structure learning for bayes nets is still challenging. \u2022 CLR performs comparably to BR, despite the fact that it is one of the most redundant methods in terms of the number of base models used. \u2022 CodingPCA-R, CodingCCA and MaxMargin are most successful. Their codewords include both label projections and original labels, and their decodings combine both regression and classification outputs. \u2022 MaxMargin outperforms CodingCCA and CodingPCA, because max-margin encoding promotes both code discriminability and code predictability. \u2022 CodingCCA performs better than CodingPCA-R, showing the importance of predictable codewords."}, {"heading": "5. Related Work", "text": "Our work follows the direction of multi-label output coding (Hsu et al., 2009) and is motivated by the recent success of coding with PCA (Tai & Lin, 2010) and CCA (Zhang & Schneider, 2011) and their connections to code distance and code predictability. Our maxmargin formulation is converted into a metric learning problem, as in (Weinberger et al., 2006), but with a metric defined for the label space and an exponential number of constraints caused by label combinations. The optimization technique developed for structured prediction (Tsochantaridis et al., 2004; Taskar et al., 2003), more specically the cutting plane method with overgenerating (Finley & Joachims, 2008), is used to solve our metric learning problem."}, {"heading": "6. Conclusion", "text": "Discriminability and predictability are both important for output codes. In this paper we propose a max-margin formulation for multi-label output coding, which promotes both discriminative and predictable codes. We convert this formulation into a metric learning problem in the label space, and combine overgener-\nating with the cutting plane method for optimization. Our method outperforms many existing methods on multi-label image, text and music data sets."}, {"heading": "Allwein, Erin L., Schapire, Robert E., and Singer, Yoram.", "text": "Reducing multiclass to binary: a unifying approach for margin classifiers. JMLR, 1:113\u2013141, 2001.\nCandes, E. J. Compressive Sampling. In Proceedings of International Congress of Mathematicians, 2006."}, {"heading": "Costello, Daniel J. and Forney, G. David. Channel coding:", "text": "The road to channel capacity. Proceedings of the IEEE, 95(6):1150\u20131177, 2007.\nCover, Thomas M. and Thomas, Joy A. Elements of information theory. Wiley-Interscience, New York, 1991.\nDietterich, Thomas G. and Bakiri, Ghulum. Solving multiclass learning problems via error-correcting output codes. JAIR, 2:263\u2013286, 1995.\nDonoho, D. L. Compressed Sensing. IEEE Trans. Information Theory, 52(4):1289\u20131306, 2006.\nFinley, Thomas and Joachims, Thorsten. Training structural svms when exact inference is intractable. In ICML, pp. 304\u2013311, 2008."}, {"heading": "Fu\u0308rnkranz, Johannes, Hu\u0308llermeier, Eyke, Loza Menc\u0301\u0131a,", "text": "Eneldo, and Brinker, Klaus. Multilabel classification via calibrated label ranking. Mach. Learn., 73(2):133\u2013153, 2008."}, {"heading": "Hsu, Daniel, Kakade, Sham, Langford, John, and Zhang,", "text": "Tong. Multi-label prediction via compressed sensing. In NIPS, pp. 772\u2013780. 2009.\nNeedell, D. and Tropp, J. A. Cosamp: Iterative signal recovery from incomplete and inaccurate samples. Applied and Computational Harmonic Analysis, 26(3), 2008.\nTai, Farbound and Lin, Hsuan-Tien. Multi-label classification with principal label space transformation. In Second International Workshop on learning from MultiLabel Data. 2010."}, {"heading": "Taskar, Benjamin, Guestrin, Carlos, and Koller, Daphne.", "text": "Max-margin markov networks. In NIPS, 2003.\nTropp, Joel A. Just relax: convex programming methods for identifying sparse signals in noise. IEEE Transactions on Information Theory, 52(3):1030\u20131051, 2006.\nTsochantaridis, Ioannis, Hofmann, Thomas, Joachims, Thorsten, and Altun, Yasemin. Support vector machine learning for interdependent and structured output spaces. In ICML, 2004."}, {"heading": "Weinberger, K.Q., Blitzer, J., and Saul, L. Distance metric", "text": "learning for large margin nearest neighbor classification. In NIPS. 2006."}, {"heading": "Zhang, Min-Ling and Zhang, Kun. Multi-label learning by", "text": "exploiting label dependency. In KDD, 2010.\nZhang, Yi and Schneider, Jeff. Multi-label output codes using canonical correlation analysis. In AISTATS. 2011."}], "references": [{"title": "Reducing multiclass to binary: a unifying approach for margin", "author": ["Allwein", "Erin L", "Schapire", "Robert E", "Singer", "Yoram"], "venue": "classifiers. JMLR,", "citeRegEx": "Allwein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2001}, {"title": "Compressive Sampling", "author": ["E.J. Candes"], "venue": "In Proceedings of International Congress of Mathematicians,", "citeRegEx": "Candes,? \\Q2006\\E", "shortCiteRegEx": "Candes", "year": 2006}, {"title": "Channel coding: The road to channel capacity", "author": ["Costello", "Daniel J", "Forney", "G. David"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Costello et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Costello et al\\.", "year": 2007}, {"title": "Solving multiclass learning problems via error-correcting output", "author": ["Dietterich", "Thomas G", "Bakiri", "Ghulum"], "venue": "codes. JAIR,", "citeRegEx": "Dietterich et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1995}, {"title": "Compressed Sensing", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Donoho,? \\Q2006\\E", "shortCiteRegEx": "Donoho", "year": 2006}, {"title": "Training structural svms when exact inference is intractable", "author": ["Finley", "Thomas", "Joachims", "Thorsten"], "venue": "In ICML, pp", "citeRegEx": "Finley et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finley et al\\.", "year": 2008}, {"title": "Multilabel classification via calibrated label ranking", "author": ["F\u00fcrnkranz", "Johannes", "H\u00fcllermeier", "Eyke", "Loza Men\u0107\u0131a", "Eneldo", "Brinker", "Klaus"], "venue": "Mach. Learn.,", "citeRegEx": "F\u00fcrnkranz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "F\u00fcrnkranz et al\\.", "year": 2008}, {"title": "Multi-label prediction via compressed sensing", "author": ["Hsu", "Daniel", "Kakade", "Sham", "Langford", "John", "Zhang", "Tong"], "venue": "In NIPS, pp", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Cosamp: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Needell and Tropp,? \\Q2008\\E", "shortCiteRegEx": "Needell and Tropp", "year": 2008}, {"title": "Multi-label classification with principal label space transformation", "author": ["Tai", "Farbound", "Lin", "Hsuan-Tien"], "venue": "In Second International Workshop on learning from MultiLabel Data", "citeRegEx": "Tai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2010}, {"title": "Max-margin markov networks", "author": ["Taskar", "Benjamin", "Guestrin", "Carlos", "Koller", "Daphne"], "venue": "In NIPS,", "citeRegEx": "Taskar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}, {"title": "Just relax: convex programming methods for identifying sparse signals in noise", "author": ["Tropp", "Joel A"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Tropp and A.,? \\Q2006\\E", "shortCiteRegEx": "Tropp and A.", "year": 2006}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["Tsochantaridis", "Ioannis", "Hofmann", "Thomas", "Joachims", "Thorsten", "Altun", "Yasemin"], "venue": "In ICML,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L. Saul"], "venue": "In NIPS", "citeRegEx": "Weinberger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2006}, {"title": "Multi-label learning by exploiting label dependency", "author": ["Zhang", "Min-Ling", "Kun"], "venue": "In KDD,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Multi-label output codes using canonical correlation analysis", "author": ["Zhang", "Yi", "Schneider", "Jeff"], "venue": "In AISTATS", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": ", 2001) and more recently to multi-label prediction (Hsu et al., 2009; Tai & Lin, 2010; Zhang & Schneider, 2011): we encode the output into a codeword, learn models to predict the codeword, and then recover the correct output from noisy predictions.", "startOffset": 52, "endOffset": 112}, {"referenceID": 7, "context": "Then we review three recently-proposed output codes (Hsu et al., 2009; Tai & Lin, 2010; Zhang & Schneider, 2011), where the encoding is based on random projections, principal component analysis and canonical correlation analysis, respectively.", "startOffset": 52, "endOffset": 112}, {"referenceID": 7, "context": "Following previous work (Hsu et al., 2009; Tai & Lin, 2010; Zhang & Schneider, 2011), we consider linear encoding.", "startOffset": 24, "endOffset": 84}, {"referenceID": 7, "context": ", relaxing y into a continuous domain and then rounding the relaxed solution (Hsu et al., 2009; Tai & Lin, 2010) or using approximate inference (Zhang & Schneider, 2011).", "startOffset": 77, "endOffset": 112}, {"referenceID": 7, "context": "Multi-label compressed sensing (Hsu et al., 2009) is one of the earliest works that formally defines a multilabel output code.", "startOffset": 31, "endOffset": 49}, {"referenceID": 4, "context": ", d) is randomly generated as in compressed sensing (Donoho, 2006; Candes, 2006), e.", "startOffset": 52, "endOffset": 80}, {"referenceID": 1, "context": ", d) is randomly generated as in compressed sensing (Donoho, 2006; Candes, 2006), e.", "startOffset": 52, "endOffset": 80}, {"referenceID": 12, "context": "As studied in structured prediction (Tsochantaridis et al., 2004; Taskar et al., 2003), problem (27) could be solved efficiently, e.", "startOffset": 36, "endOffset": 86}, {"referenceID": 10, "context": "As studied in structured prediction (Tsochantaridis et al., 2004; Taskar et al., 2003), problem (27) could be solved efficiently, e.", "startOffset": 36, "endOffset": 86}, {"referenceID": 12, "context": ", by the cutting-plane method, if a computationally tractable separate oracle exists to determine which of the exponentially many constraints is most violated (Tsochantaridis et al., 2004).", "startOffset": 159, "endOffset": 188}, {"referenceID": 7, "context": "\u2022 Coding with compressed sensing (CodingCS) (Hsu et al., 2009).", "startOffset": 44, "endOffset": 62}, {"referenceID": 6, "context": "\u2022 Calibrated label ranking (CLR) (F\u00fcrnkranz et al., 2008).", "startOffset": 33, "endOffset": 57}, {"referenceID": 7, "context": "Our work follows the direction of multi-label output coding (Hsu et al., 2009) and is motivated by the recent success of coding with PCA (Tai & Lin, 2010) and CCA (Zhang & Schneider, 2011) and their connections to code distance and code predictability.", "startOffset": 60, "endOffset": 78}, {"referenceID": 13, "context": "Our maxmargin formulation is converted into a metric learning problem, as in (Weinberger et al., 2006), but with a metric defined for the label space and an exponential number of constraints caused by label combinations.", "startOffset": 77, "endOffset": 102}, {"referenceID": 12, "context": "The optimization technique developed for structured prediction (Tsochantaridis et al., 2004; Taskar et al., 2003), more specically the cutting plane method with overgenerating (Finley & Joachims, 2008), is used to solve our metric learning problem.", "startOffset": 63, "endOffset": 113}, {"referenceID": 10, "context": "The optimization technique developed for structured prediction (Tsochantaridis et al., 2004; Taskar et al., 2003), more specically the cutting plane method with overgenerating (Finley & Joachims, 2008), is used to solve our metric learning problem.", "startOffset": 63, "endOffset": 113}], "year": 2012, "abstractText": "In this paper we study output coding for multi-label prediction. For a multi-label output coding to be discriminative, it is important that codewords for different label vectors are significantly different from each other. In the meantime, unlike in traditional coding theory, codewords in output coding are to be predicted from the input, so it is also critical to have a predictable label encoding. To find output codes that are both discriminative and predictable, we first propose a max-margin formulation that naturally captures these two properties. We then convert it to a metric learning formulation, but with an exponentially large number of constraints as commonly encountered in structured prediction problems. Without a label structure for tractable inference, we use overgenerating (i.e., relaxation) techniques combined with the cutting plane method for optimization. In our empirical study, the proposed output coding scheme outperforms a variety of existing multi-label prediction methods for image, text and music classification.", "creator": "LaTeX with hyperref package"}}}