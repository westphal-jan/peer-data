{"id": "1206.6485", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Greedy Algorithms for Sparse Reinforcement Learning", "abstract": "feature adaptive methods preservation starts becoming increasingly prominent particularly in the eyes of the reinforcement learning ( rl ) community showing evaluate tool reach across applicability function distributions. favorite approach to the problem of feature selection is implementing impose a performance - derived form offering regularization on the programming method. recent work on $ l _ 16 $ 50 considerably adapted techniques from actual supervised learning literature for use with groups. another effort that has received renewed attention in this supervised learning community is that of using a reliable means that greedily picks new features. such projects have many of historically correct properties called the $ l _ 1 $ regularization methods, while other being extremely efficient and, above some experiments, allowing theoretical improvement on recovery of the true form presenting a predictable fuzzy behavior without sampled data. this paper considers variants of orthogonal regression pursuit ( das ) applied to sampling sequences. newly resulting algorithms frequently identified and compared repeatedly showing existing $ l _ 8 $ regularized theories. we demonstrate there current possibly most natural scenario in which statistical technique achieve it achieve sparse structure increases ; specifically, independent variants, omp - brm, provides more theoretical statements under certain assumptions indicating minimal associated properties. another variant, omp - td, partially parallels optimization methods since manipulating approximation ratios and efficiency into several benchmark problems.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (555kb)", "http://arxiv.org/abs/1206.6485v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["christopher painter-wakefield", "ronald parr"], "accepted": true, "id": "1206.6485"}, "pdf": {"name": "1206.6485.pdf", "metadata": {"source": "META", "title": "Greedy Algorithms for Sparse Reinforcement Learning", "authors": ["Christopher Painter-Wakefield", "Ronald Parr"], "emails": ["PAINT007@CS.DUKE.EDU", "PARR@CS.DUKE.EDU"], "sections": [{"heading": "1. Introduction", "text": "Feature selection and regularization are becoming increasingly prominent tools in the efforts of the reinforcement learning (RL) community to expand the reach and appli-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ncability of RL (Parr et al., 2007; Mahadevan & Maggioni, 2007; Johns, 2010; Johns et al., 2010; Ghavamzadeh et al., 2011). Very often (though not always (Farahmand et al., 2008)) sparseness is viewed as a desirable goal or side effect of regularization. If the true value function is known to be sparse, then the reasons for desiring a sparse solution are clear. Even when the form of the true value function is not known, sparsity may still be desired because sparsity can act as a regularizer, and because sparse solutions tend to be more understandable to humans and more efficient to use. Favoring sparsity can lead to faster algorithms in some cases (Petrik et al., 2010).\nOne optimization-based approach to the problem of feature selection is to impose a sparsity-inducing form of regularization on the learning method. Recent work on L1 regularization has adapted techniques from the supervised learning literature (Tibshirani, 1996) for use with RL (Kolter & Ng, 2009). Another approach that has received renewed attention in the supervised learning community is that of using a simple algorithm that greedily adds new features (Tropp, 2004; Zhang, 2009). Such algorithms have many of the good properties of the L1 regularization methods, while also being extremely efficient and, in some cases, allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data despite the myopia associated with greediness.\nThe most basic greedy algorithm for feature selection for regression, matching pursuit, uses the correlation between the residual and the candidate features to decide which feature to add next (Mallat & Zhang, 1993). This paper considers a variation called orthogonal matching pursuit (OMP), which recomputes the residual after each new feature is added, as applied to reinforcement learning. It is related to BEBFs (Parr et al., 2007), but it differs in that it selects features from a finite dictionary. OMP for RL was explored by Johns (2010) in the context of PVFs (Mahadevan & Maggioni, 2007) and diffusion wavelets (Mahadevan & Maggioni, 2006), but aside from this initial exploration of the topic, we are not aware of any efforts to bring the theoretical and empirical understanding of OMP for reinforcement learning to parity with the understanding of OMP as\napplied to regression.\nThis paper contributes to the theoretical and practical understanding of OMP for RL. Variants of OMP are analyzed and compared experimentally with existing L1 regularized approaches. We demonstrate that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails; however, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMP-TD, lacks theoretical guarantees but empirically outperforms OMPBRM and prior methods both in approximation accuracy and efficiency on several benchmark problems."}, {"heading": "2. Framework and Notation", "text": "We aim to discover exact or good, approximate value functions for Markov reward processes (MRPs): M = (S, P,R, \u03b3). Given a state s \u2208 S, the probability of a transition to a state s\u2032 \u2208 S is given by P (s\u2032|s), and results in an expected reward of R(s). We do not address the question of optimizing the policy for a Markov Decision Process, though we note that policy evaluation, where P = P\u03c0 , by some policy \u03c0, is an important intermediate step in many algorithms. A discount factor \u03b3 discounts future rewards such that the present value of a trajectory st=0 . . . st=n is\u2211n t=0 \u03b3 tR(st).\nThe true value function V \u2217 over states satisfies the Bellman equation:\nV \u2217 = TV \u2217 = R+ \u03b3PV \u2217,\nwhere T is the Bellman operator and V \u2217 is the fixed point of this operator.\nIn practice, the value function, the transition model, and the reward function are often too large to permit an explicit, exact representation. In such cases, an approximation architecture is used for the value function. A common choice is V\u0302 = \u03a6w, where w is a vector of k scalar weights and \u03a6 stores a set of k features in an n\u00d7k feature matrix. Since n is often intractably large, \u03a6 can be thought of as populated by k linearly independent basis functions, \u03d51 . . . \u03d5k, which define the columns of \u03a6. We will refer to a basis formed by selecting a subset of the features using an index set as a subscript. Thus, \u03a6I contains a subset of features from \u03a6, where I is a set of indices such that basis function \u03d5i is included in \u03a6I if i \u2208 I.\nFor the purposes of estimatingw, it is common to replace \u03a6 with \u03a6\u0302, which samples rows of \u03a6, though for conciseness of presentation we will use \u03a6 for both, since algorithms for estimating w are essentially identical if \u03a6\u0302 is substituted for \u03a6. A number of linear function approximation algorithms such as LSTD (Bradtke & Barto, 1996) solve for the\nAlgorithm 1 OMP Input: X \u2208 Rn\u00d7k, y \u2208 Rn, \u03b2 \u2208 R. Output: Approximation weights w. I \u2190 {} w \u2190 0 repeat c\u2190 |XT (y \u2212Xw)| j \u2190 arg maxi/\u2208I ci if cj > \u03b2 then I \u2190 I \u222a {j}\nend if wI \u2190 X\u2020Iy\nuntil cj \u2264 \u03b2 or I\u0304 = {}\nw which is a fixed point:\n\u03a6w = \u03a0\u03c3(R+ \u03b3\u03a6\u2032w) = \u03a0\u03c3T\u03a6w, (1)\nwhere \u03a0\u03c3 is the \u03c3-weighted L2 projection and where \u03a6\u2032 is P\u03a6 in the explicit case and is composed of sampled next features in the sampled case. We likewise overload T for the sampled case.\nWe make use of the notation \u03a6\u2020 to represent the pseudoinverse of \u03a6 specifically defined as \u03a6\u2020 \u2261 (\u03a6T\u03a6)\u22121\u03a6T . In general we assume that the weighting function \u03c3 is implicit in the sampling of our data, in which case the projection operator above is simply \u03a0 = \u03a6\u03a6\u2020.\nWe also consider algorithms which solve for w minimizing the Bellman error:\nw = arg min w\n\u2016T\u03a6w \u2212 \u03a6w\u2016. (2)\nThis is the Bellman residual minimization (BRM) approach espoused by Baird (1995)."}, {"heading": "3. Prior Art", "text": ""}, {"heading": "3.1. OMP for Regression", "text": "Algorithm 1 is the classic OMP algorithm for regression. It is greedy in that it myopically chooses the feature with the highest correlation with the residual and never discards features. We say that target y is m-sparse in X if there exists an Xopt composed of m columns of X and corresponding wopt such that y = Xoptwopt, and Xopt is minimal in the sense there is no X \u2032 composed of fewer columns of X which can satisfy y = X \u2032w. Of the many results for sparse recovery, Tropp\u2019s (2004) is perhaps the most straightforward:\nTheorem 1 If y is m-sparse in X , and\nmax i 6\u2208opt\n\u2016X\u2020optxi\u20161 < 1,\nthen OMP called with X , y, and \u03b2 = 0 will return wopt in m iterations.\nIn the maximization, the vectors xi correspond to the columns of X which are not needed to reconstruct y exactly. In words, this condition requires that the projection of any suboptimal feature into the span of the optimal features must have small weights. Note that any orthogonal basis trivially satisfies this condition.\nTropp also extended this result to the cases where y is approximately sparse in X , and Zhang (2009) extended the result to the noisy case."}, {"heading": "3.2. L1 Regularization in RL", "text": "In counterpoint to the greedy methods based on OMP, which we will explore in the next section, much of the recent work on feature selection in RL has been based on least-squares methods with L1 regularization. For regression, Tibshirani (1996) introduced the LASSO, which takes matrix X and target vector y and seeks a vector w which minimizes \u2016y \u2212 Xw\u20162 subject to a constraint on \u2016w\u20161. While Tibshirani uses a hard constraint, this is equivalent to minimizing\n\u2016y \u2212Xw\u20162 + \u03b2\u2016w\u20161, (3)\nfor some value of \u03b2 \u2208 R+.\nLoth et al. (2007) apply the LASSO to Bellman residual minimization. Replacing the residual y \u2212Xw in equation 3 with the Bellman residual, we obtain\n\u2016R+ \u03b3\u03a6\u2032w \u2212 \u03a6w\u20162 + \u03b2\u2016w\u20161.\nTrivially, if we let X = \u03a6 \u2212 \u03b3\u03a6\u2032 and y = R, we can substitute directly into equation 3 and solve as a regression problem. The regression algorithm used by Loth et al. is very similar to LARS (Efron et al., 2004).\nA harder problem is applying L1 regularization in a fixed point method akin to LSTD. The L1 regularized linear fixed point is the vector w solving\nw = arg min u\n\u2016R+ \u03b3\u03a6\u2032w \u2212 \u03a6u\u20162 + \u03b2\u2016u\u20161.\nintroduced by Kolter & Ng (2009). Kolter and Ng provide an algorithm, LARS-TD, which closely follows the approach of LARS. Johns et al. (2010) followed LARS-TD with an algorithm, LC-TD, which solves for the L1 regularized linear fixed point as a linear complementarity problem.\nIt is instructive to note that LARS bears some resemblance to OMP in that it selects as new features those with the highest correlation to the residual of its current approximation. However, where OMP is purely greedy and seeks to\nAlgorithm 2 OMP-BRM Input: \u03a6 \u2208 Rn\u00d7k : \u03a6ij = \u03d5j(si), \u03a6\u2032 \u2208 Rn\u00d7k : \u03a6\u2032ij = \u03d5j(s\u2032i), R \u2208 Rn : Ri = ri, \u03b3 \u2208 [0, 1), \u03b2 \u2208 R\nOutput: Approximation weights w.\ncall OMP with X = \u03a6\u2212 \u03b3\u03a6\u2032, y = R, \u03b2 = \u03b2\nuse all active features to the fullest, LARS is more moderate and attempts to use all active features equally, in the sense that all active features maintain an equal correlation with the residual. One aspect of the LARS approach that sets it quite apart from OMP is that LARS will remove features from the active set when necessary to maintain its invariants.\nIntuitively, LARS and algorithms based on LARS such as LARS-TD have an advantage in minimizing the number of active features due to their ability to remove features. LC-TD also adds and removes features. These methods do suffer from some disadvantages related to this ability, however. LARS-TD can be slowed down by the repeated adding and removing of features. Worse, both LARS-TD and LC-TD involve computations which are numerically sensitive and are not guaranteed to find the desired solution in all cases since (unlike in the pure regression case) the task of finding an L1 regularized linear fixed point is not a convex optimization problem."}, {"heading": "4. OMP for RL", "text": "We present two algorithms for policy evaluation: OMPBRM and OMP-TD.1 As the names suggest, the first algorithm is based on Bellman residual minimization (BRM), while the second is based on the linear TD fixed point. Algorithm 2, OMP-BRM, is the simpler algorithm in the sense that it essentially performs OMP with features \u03a6 \u2212 \u03b3\u03a6\u2032 using the reward vector as the target value. OMPBRM is different from the OMP-BR algorithm introduced by Johns (2010), which selected basis functions from \u03a6.\nAlgorithm 3, OMP-TD, applies the basic OMP approach to build a feature set for LSTD.2 OMP-TD is similar in approach to the approximate BEBF algorithm of Parr et al. (2007), in which each new feature is an approximation to the current Bellman residual. In OMP-TD, rather than approximate the Bellman residual, we simply add the feature\n1Note that both algorithms reduce to OMP when \u03b3 = 0. 2Johns (2010) called the same algorithm OMP-FP.\nAlgorithm 3 OMP-TD Input: \u03a6 \u2208 Rn\u00d7k : \u03a6ij = \u03d5j(si), \u03a6\u2032 \u2208 Rn\u00d7k : \u03a6\u2032ij = \u03d5j(s\u2032i), R \u2208 Rn : Ri = ri, \u03b3 \u2208 [0, 1), \u03b2 \u2208 R\nOutput: Approximation weights w.\nI \u2190 {} w \u2190 0 repeat c\u2190 |\u03a6T (R+ \u03b3\u03a6\u2032w \u2212 \u03a6w)|/n j \u2190 arg maxi/\u2208I ci if cj > \u03b2 then I \u2190 I \u222a {j}\nend if wI \u2190 (\u03a6TI\u03a6I \u2212 \u03b3\u03a6TI\u03a6\u2032I)\u22121\u03a6TIR\nuntil cj \u2264 \u03b2 or I\u0304 = {}\nwhich, among features not already in use, currently has the highest correlation with the residual. After adding a feature, the new fixed point is computed using the closed form LSTD fixed point equation, and the new residual is computed. The main results of the BEBF paper apply to OMPTD: mainly, that each new feature improves a bound on the distance between the fixed point and the true value function V \u2217, as long as the correlation between the feature and the residual is sufficiently large."}, {"heading": "4.1. Sparse Recovery in OMP-TD", "text": "Theorem 2 Even if V \u2217 is m-sparse in an orthonormal basis, OMP-TD cannot guarantee exact recovery of V \u2217 in m iterations.\nPROOF (By counterexample) Consider the Markov chain in figure 1. The arcs indicate deterministic transitions. Suppose R(S2) = R(S3) = R(S4) = 1, R(S5) = 0, and R(S1) = \u2212(\u03b3 + \u03b32 + \u03b33), then V \u2217 = [0, 1 + \u03b3 + \u03b32, 1 + \u03b3, 1, 0]. With an orthonormal basis \u03a6 defined by the indicator functions \u03d5i(s) = I(s = si), V \u2217 is 3-sparse in \u03a6, with opt = {2, 3, 4}. Starting from the empty set of features, the residual vector is just R. OMP-TD will pick the vector\nwith the correlation with the residual, which will be \u03d51, a vector that is not in opt. 2\nThe next feature added by OMP-TD could be S4, then S3 and S2. Selecting a single vector not in opt suffices to establish the proof, which means that we could have shortened the example by removing states S2 and S3 and connecting S1 directly to S4. However, the longer chain is useful to illustrate an important point about OMP-TD: It is possible to add a gratuitous basis function at the very first step of the algorithm and the mistake may not be evident until an arbitrary number of additional basis functions are added. This example is easily extended so that an arbitrary number of gratuitous basis functions are added before the first basis function in opt is added by making multiple copies of the S1 state (together with the corresponding indicator function features). Such constructions can defeat modifications to OMP-TD that use a window of features and discard gratuitous ones (Jain et al., 2011) for any fixedsize window.\nAn algorithm that chooses features from \u03a6 based upon the Bellman residual (OMP-BR in the terminology of Johns (2010)), would suffer the same difficulties as OMP-TD in this example. The central problem is that the Bellman error may not be a trustworthy guide for selecting features from \u03a6 even if \u03a6 is orthogonal."}, {"heading": "4.2. Sparse Recovery in OMP-BRM", "text": "Lemma 1 If V \u2217 is m-sparse in \u03a6, then R is at least msparse in (\u03a6\u2212 \u03b3P\u03a6). PROOF Since V \u2217 = (I \u2212 \u03b3P )\u22121R, we have\n(I \u2212 \u03b3P )\u22121R = \u03a6optwopt R = \u03a6optwopt \u2212 \u03b3P\u03a6optwopt\n= [\u03a6\u2212 \u03b3P\u03a6]optwopt. 2\nThe implication is that we can perform OMP on the basis (\u03a6\u2212 \u03b3P\u03a6) and, if there is a sparse representation for R in the basis, we will obtain a sparse representation of V \u2217 as well. This permits a sparse recovery claim for OMP-BRM that is in stark contrast to the negative results for OMP-TD. Theorem 3 If V \u2217 is m-sparse in \u03a6, and\nmax i/\u2208opt\n\u2016X\u2020optxi\u20161 < 1, (4)\nfor X = \u03a6 \u2212 \u03b3P\u03a6, then OMP-BRM called with \u03a6, \u03a6\u2032 = P\u03a6, R, \u03b3, and \u03b2 = 0 will return w such that V \u2217 = \u03a6w in at most m iterations. PROOF (sketch) The proof mirrors a similar proof from Tropp (2004) and is provided in detail in the full version of the paper3. Lemma 1 implies that R is m-sparse in X.\n3Full version available at http://www.cs.duke.edu/ \u02dcparr/icml2012-full.pdf\nSince OMP-BRM does OMP with basis X and target R, the sparse recovery results for OMP for regression apply directly. 2\nTropp\u2019s extension to the approximate recovery case also applies directly to OMP-BRM (see full version of the paper3). For the noisy case, we expect that the results of Zhang (2009) could be generalized to RL, but we defer that extension for future work."}, {"heading": "4.3. Sparse Recovery Behavior", "text": "We set up experiments to validate the theory of exact recovery for OMP-BRM, and to investigate the behavior of OMP-TD in similar circumstances. First, we generated a basis for the 50-state chain problem (see section 5) in which the first three basis functions provide an exact reconstruction of V \u2217, and the remaining 997 features are randomly generated, which satisfies equation (4). (Generating such a basis required first generating a much larger (50 x 3000) matrix, then throwing out features which violated the exact recovery condition, and finally trimming the matrix back down to 1000 features. The first three features were constructed by finding two random features which highly correlate with V \u2217, then adding in a third feature which was the reconstruction residual using the first two features.)\nBy using the resulting basis in OMP-BRM with exact data (i.e., where \u03a6\u2032 = P\u03a6), we found that, for sufficiently small threshold value, OMP-BRM uses exactly the first three features in its approximation, in accordance with theory. We also tried OMP-BRM with noisy data by sampling 200 state transitions from the 50-state chain problem. In this case, we found that OMP-BRM reliably selected the first three features before selecting any other features. Interestingly, the same basis proved to enable exact recovery for OMP-TD as well. Using the same 200 samples, OMP-TD selected the first three features before selecting any other features."}, {"heading": "5. Experiments", "text": "While we have theoretical results for OMP-BRM that suggest we should be able to perform optimal recovery under certain conditions on the feature dictionary, practical problems contain noise, which current theory does not address. In addition, the desired conditions on the feature dictionary may not hold and it can be difficult to verify if they do hold since these conditions are a property of both the features and the transition function for OMP-BRM. For OMPTD, we have negative worst-case results, but Section 4.3 gives hope that things may not be so dismal in practice. To gain some understanding of how these algorithms perform under typical conditions, we performed experiments on a number of benchmark RL problems. Figure 2 shows our main results, the approximation accuracy achieved by each\nof four algorithms on each problem. Table 1 summarizes the benchmark problem properties and some experimental settings. The algorithms studied included:\nOMP-BRM: The OMP-BRM algorithm as described above, but with some additional machinery to improve performance in actual use. It is well known (Sutton & Barto, 1998) that BRM is biased in the presence of noise, i.e., when samples are taken from a stochastic transition function. One solution to this problem is to use double samples for each transition. In each of our experiments, we ran with and without doubled samples, and we report the behavior of the better performing option. Table 1 records which experiments use doubled samples. Figure 3 shows how doubling samples affects performance on the 50-state chain problem. In all cases, the number of samples in the Samples column refers to the number of starting states.4 In the doubled case we also add in a small amount (0.01) of L2 regularization in order to keep the algorithm well behaved (by keeping the matrices to be inverted well conditioned).\nOMP-TD: The OMP-TD algorithm as described above, but with a small amount (0.01) of L2 regularization when computing the final solution at each threshold. This change seemed to be important for some of the harder benchmarks such as puddleworld and two-room, where without regularization, the LSTD solution at various threshold values would occasionally exhibit unstable behavior. We use regularization on all problems, as it seems to cause no issues even for benchmarks which do not need it.\nLARS-BRM: This is our implementation of the algorithm of Loth et al. (2007), which effectively treats BRM as a regression problem to be solved using LARS. Note that there is no provision for sample doubling in this algorithm, which may affect its performance on some problems.\nLARS-TD: This is the LARS-TD algorithm of Kolter & Ng (2009). Again we found it useful sometimes to include a small amount (0.01) of L2 regularization, giving the \u201celastic net\u201d (Zou & Hastie, 2005) solution. The additional regularization does not always benefit; however, we report in figure 2 the better of the two for each benchmark. Table 1 records which experiments use L2 regularization in LARSTD.\nIn figure 2, we have plotted performance of the various algorithms on a variety of benchmark problems. The vertical axis is the root mean square error with respect to the true value function, V \u2217. For the discrete state problems, V \u2217 is computed exactly, while for the continuous state prob-\n4This could be interpreted as giving the double-sample case an unfair advantage because there is no \u201cpenalty\u201d for the additional samples collected. On the other hand, counting next states only could be seen as imposing too harsh of a penalty on OMPBRM since the same number of samples would necessarily have sparser coverage of the state space.\nlems V \u2217 is computed at a large number of sampled states by Monte Carlo rollouts. The number of trials over which the values are averaged is reported in table 1. The horizontal axis gives the threshold/regularization coefficient value \u03b2 for the value function V\u03b2 plotted.\nIn general, the meaning of \u03b2 for the OMP algorithms is dif-\nferent than for the LARS algorithms. However, there is a strong similarity between the two in that, for both, a solution at value \u03b2 implies that there are no further features with a correlation with the current residual of \u03b2 or larger. This is explicit in the OMP algorithms, and implicit in the fixed point conditions for LARS-TD (n.b. Kolter & Ng (2009), equation 9) and LARS-BRM. Surprisingly, given that OMP\nTable 1. Benchmark experiment properties and experimental settings.\nProblem State space Features Samples Trials LARS-TD L2? BRM double samples?\nChain Discrete, 50 states 208 500 1000 \u00d7 \u221a Pendulum Continuous, 2d 268 200 1000 \u221a \u221a Blackjack Discrete, 203 states 219 1600 1000 \u00d7 \u00d7 Mountain Car Continuous, 2d 1366 5000 100 \u221a \u00d7 Puddleworld Continuous, 2d 570 2000 500 \u00d7 \u00d7 Two Room Continuous, 2d 2227 5000 1000 \u00d7 \u00d7\nis greedy and never removes features, we find that the sparsity of the solutions given by the algorithms is similar for the same values of \u03b2; e.g., see figure 4.\nAs figure 2 shows, OMP-TD is generally competitive with, or better than, LARS-BRM and LARS-TD on most of the benchmark problems. In addition, we note that the OMPbased algorithms are considerably faster than the LARSbased algorithms; see figure 5 for a comparison of computation time on the puddleworld problem.\nWhile OMP-TD generally leads in our benchmarks, we should point out some caveats. With very small numbers of samples (even fewer than shown in our experiments) OMPTD was somewhat more prone to unstable behavior than the other algorithms. This could simply mean that OMPTD requires more L2 regularization, but we did not explore that in our experiments. An indication that OMP-TD can require more L2 regularization than the other algorithms is evident in our OMP-TD experiments for Mountain Car, where the high variance for low values of \u03b2 arises from just two (out of 100) batches of samples. In these cases, it appears that OMP-TD is adding a feature which is essentially a delta function on a single sample. Without additional L2 regularization, OMP-TD produces very poor value functions for these batches.\nFor all of the experiments except for blackjack, the features are radial basis functions (RBFs). There are multiple widths of RBFs placed in the state space in grids of various spacing. For blackjack, the basis functions are indicators on groups of states. All features are normalized, although we tried both with and without normalization, and the results were qualitatively similar.\nMore information about each of the experimental domains can be found in the full version of the paper3."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper we have explored the theoretical and practical applications of OMP to RL. We analyzed variants of OMP and compared them experimentally with existing L1 regularized approaches. We showed that perhaps the most\nnatural scenario in which one might hope to achieve sparse recovery fails; however, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMP-TD, empirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems.\nThere are two natural directions for further development of this work. Our theoretical results for OMP-BRM built upon the simplest results for sparse recovery in regression and do not apply directly to more realistic scenarios that involve noise. Stronger results may be possible, building upon the work of Zhang (2009). A more interesting, but also more challenging, future direction would be the theoretical development to explain the extremely strong performance of OMP-TD in practice despite negative theoretical results on sparse recovery. For example, there could be an alternate set of conditions on the features that frequently hold in practice and that can be shown theoretically to have good sparse recovery guarantees."}, {"heading": "Acknowledgments", "text": "This work was supported by NSF IIS-1147641. Opinions, findings, conclusions or recommendations herein are those\nof the authors and not necessarily those of NSF. The authors also wish to thank Susan Murphy and Eric Laber for helpful discussions."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "In ICML,", "citeRegEx": "Baird,? \\Q1995\\E", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["Bradtke", "Steven J", "Barto", "Andrew G"], "venue": "Machine Learning,", "citeRegEx": "Bradtke et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bradtke et al\\.", "year": 1996}, {"title": "Least angle regression", "author": ["Efron", "Bradley", "Hastie", "Trevor", "Johnstone", "Iain", "Tibshirani", "Robert"], "venue": "The Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Regularized policy iteration", "author": ["Farahmand", "Amir Massoud", "Ghavamzadeh", "Mohammad", "Szepesv\u00e1ri", "Csaba", "Mannor", "Shie"], "venue": "In NIPS, pp", "citeRegEx": "Farahmand et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2008}, {"title": "Finite-sample analysis of Lasso-TD", "author": ["Ghavamzadeh", "Mohammad", "Lazaric", "Alessandro", "Munos", "R\u00e9mi", "Auer", "Peter"], "venue": "In ICML,", "citeRegEx": "Ghavamzadeh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ghavamzadeh et al\\.", "year": 2011}, {"title": "Orthogonal matching pursuit with replacement", "author": ["P. Jain", "A. Tewari", "I.S. Dhillon"], "venue": "Technical Report arXiv:1106.2774, Arxiv preprint,", "citeRegEx": "Jain et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2011}, {"title": "Basis Construction and Utilization for Markov Decision Processes using Graphs", "author": ["Johns", "Jeffrey"], "venue": "PhD thesis, University of Massachusetts Amherst,", "citeRegEx": "Johns and Jeffrey.,? \\Q2010\\E", "shortCiteRegEx": "Johns and Jeffrey.", "year": 2010}, {"title": "Linear complementarity for regularized policy evaluation and improvement", "author": ["Johns", "Jeffrey", "Painter-Wakefield", "Christopher", "Parr", "Ronald"], "venue": "In NIPS, pp", "citeRegEx": "Johns et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Johns et al\\.", "year": 2010}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["Kolter", "J. Zico", "Ng", "Andrew Y"], "venue": "In ICML, pp", "citeRegEx": "Kolter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2009}, {"title": "Sparse temporal difference learning using LASSO", "author": ["Loth", "Manuel", "Davy", "Preux", "Philippe"], "venue": "ADPRL", "citeRegEx": "Loth et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loth et al\\.", "year": 2007}, {"title": "Value function approximation with diffusion wavelets and Laplacian eigenfunctions", "author": ["Mahadevan", "Sridhar", "Maggioni", "Mauro"], "venue": null, "citeRegEx": "Mahadevan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mahadevan et al\\.", "year": 2006}, {"title": "Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes", "author": ["Mahadevan", "Sridhar", "Maggioni", "Mauro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mahadevan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mahadevan et al\\.", "year": 2007}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["Mallat", "St\u00e9phane G", "Zhang", "Zhifeng"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Mallat et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Mallat et al\\.", "year": 1993}, {"title": "Analyzing feature generation for valuefunction approximation", "author": ["Parr", "Ronald", "Painter-Wakefield", "Christopher", "Li", "Lihong", "Littman", "Michael L"], "venue": "In ICML, pp", "citeRegEx": "Parr et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Parr et al\\.", "year": 2007}, {"title": "Feature selection using regularization in approximate linear programs for Markov decision processes", "author": ["Petrik", "Marek", "Taylor", "Gavin", "Parr", "Ronald", "Zilberstein", "Shlomo"], "venue": "In ICML, pp", "citeRegEx": "Petrik et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Petrik et al\\.", "year": 2010}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1996}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["Tropp", "Joel A"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Tropp and A.,? \\Q2004\\E", "shortCiteRegEx": "Tropp and A.", "year": 2004}, {"title": "On the consistency of feature selection using greedy least squares regression", "author": ["Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang and Tong.,? \\Q2009\\E", "shortCiteRegEx": "Zhang and Tong.", "year": 2009}, {"title": "Regularization and variable selection via the elastic net", "author": ["Zou", "Hui", "Hastie", "Trevor"], "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),", "citeRegEx": "Zou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 13, "context": "cability of RL (Parr et al., 2007; Mahadevan & Maggioni, 2007; Johns, 2010; Johns et al., 2010; Ghavamzadeh et al., 2011).", "startOffset": 15, "endOffset": 121}, {"referenceID": 7, "context": "cability of RL (Parr et al., 2007; Mahadevan & Maggioni, 2007; Johns, 2010; Johns et al., 2010; Ghavamzadeh et al., 2011).", "startOffset": 15, "endOffset": 121}, {"referenceID": 4, "context": "cability of RL (Parr et al., 2007; Mahadevan & Maggioni, 2007; Johns, 2010; Johns et al., 2010; Ghavamzadeh et al., 2011).", "startOffset": 15, "endOffset": 121}, {"referenceID": 3, "context": "Very often (though not always (Farahmand et al., 2008)) sparseness is viewed as a desirable goal or side effect of regularization.", "startOffset": 30, "endOffset": 54}, {"referenceID": 14, "context": "Favoring sparsity can lead to faster algorithms in some cases (Petrik et al., 2010).", "startOffset": 62, "endOffset": 83}, {"referenceID": 13, "context": "It is related to BEBFs (Parr et al., 2007), but it differs in that it selects features from a finite dictionary.", "startOffset": 23, "endOffset": 42}, {"referenceID": 13, "context": "It is related to BEBFs (Parr et al., 2007), but it differs in that it selects features from a finite dictionary. OMP for RL was explored by Johns (2010) in the context of PVFs (Mahadevan & Maggioni, 2007) and diffusion wavelets (Mahadevan & Maggioni, 2006), but aside from this initial exploration of the topic, we are not aware of any efforts to bring the theoretical and empirical understanding of OMP for reinforcement learning to parity with the understanding of OMP as", "startOffset": 24, "endOffset": 153}, {"referenceID": 0, "context": "This is the Bellman residual minimization (BRM) approach espoused by Baird (1995).", "startOffset": 69, "endOffset": 82}, {"referenceID": 2, "context": "is very similar to LARS (Efron et al., 2004).", "startOffset": 24, "endOffset": 44}, {"referenceID": 7, "context": "Johns et al. (2010) followed LARS-TD with an algorithm, LC-TD, which solves for the L1 regularized linear fixed point as a linear complementarity problem.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "2 OMP-TD is similar in approach to the approximate BEBF algorithm of Parr et al. (2007), in which each new feature is an approximation to the current Bellman residual.", "startOffset": 69, "endOffset": 88}, {"referenceID": 5, "context": "Such constructions can defeat modifications to OMP-TD that use a window of features and discard gratuitous ones (Jain et al., 2011) for any fixedsize window.", "startOffset": 112, "endOffset": 131}, {"referenceID": 9, "context": "LARS-BRM: This is our implementation of the algorithm of Loth et al. (2007), which effectively treats BRM as a regression problem to be solved using LARS.", "startOffset": 57, "endOffset": 76}], "year": 2012, "abstractText": "Feature selection and regularization are becoming increasingly prominent tools in the efforts of the reinforcement learning (RL) community to expand the reach and applicability of RL. One approach to the problem of feature selection is to impose a sparsity-inducing form of regularization on the learning method. Recent work on L1 regularization has adapted techniques from the supervised learning literature for use with RL. Another approach that has received renewed attention in the supervised learning community is that of using a simple algorithm that greedily adds new features. Such algorithms have many of the good properties of the L1 regularization methods, while also being extremely efficient and, in some cases, allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data. This paper considers variants of orthogonal matching pursuit (OMP) applied to reinforcement learning. The resulting algorithms are analyzed and compared experimentally with existing L1 regularized approaches. We demonstrate that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails; however, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMPTD, empirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems.", "creator": "LaTeX with hyperref package"}}}