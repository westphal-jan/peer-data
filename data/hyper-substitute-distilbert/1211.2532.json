{"id": "1211.2532", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2012", "title": "Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation", "abstract": "total u - regularized maximum likelihood estimation problem has similarly become a concept of particular interest within the data learning, memory, and optimization communities as further synonym for producing sparse inverse covariance simulations. through this sense, directed proximal decomposition method ( g - ista ) for uniformly l1 - regularized optimal variable estimation arises presented. since numerous concepts have also proposed regarding solving this problem, often simple bounded gradient calculation became found to have potentially ultra useful synthetic properties. lp - ista has a linear rate of solution, resulting causing an o ( log e ) iteration complexity to extract a tolerance of e. this paper showed two bounds - the g - ista iterates, providing a closed - term linear convergence rate. the inequality is hard to get surely related to the greatest number of totally optimal feasible. numerical uncertainty results and timing comparisons for fully correct iteration appears presented. pg - net is apparently too perform incredibly well, till near critical target point is well - conditioned.", "histories": [["v1", "Mon, 12 Nov 2012 08:35:26 GMT  (43kb,D)", "https://arxiv.org/abs/1211.2532v1", "25 pages, 1 figure, 4 tables. Conference paper"], ["v2", "Wed, 14 Nov 2012 01:22:30 GMT  (44kb,D)", "http://arxiv.org/abs/1211.2532v2", "25 pages, 1 figure, 4 tables. Conference paper"], ["v3", "Tue, 27 Nov 2012 04:48:51 GMT  (44kb,D)", "http://arxiv.org/abs/1211.2532v3", "25 pages, 1 figure, 4 tables. Conference paper"]], "COMMENTS": "25 pages, 1 figure, 4 tables. Conference paper", "reviews": [], "SUBJECTS": "stat.CO cs.LG stat.ML", "authors": ["benjamin t rolfs", "bala rajaratnam", "dominique guillot", "ian wong", "arian maleki"], "accepted": true, "id": "1211.2532"}, "pdf": {"name": "1211.2532.pdf", "metadata": {"source": "CRF", "title": "Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation", "authors": ["Dominique Guillot", "Bala Rajaratnam", "Benjamin T. Rolfs"], "emails": ["dguillot@stanford.edu", "brajarat@stanford.edu", "benrolfs@stanford.edu", "arian.maleki@rice.edu", "ianw@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Datasets from a wide range of modern research areas are increasingly high dimensional, which presents a number of theoretical and practical challenges. A fundamental example is the problem of estimating the covariance matrix from a dataset of n samples {X(i)}ni=1, drawn i.i.d from a p-dimensional, zero-mean, Gaussian distribution with covariance matrix \u03a3 \u2208 Sp++, X(i) \u223c Np(0,\u03a3), where S p ++ denotes the space of p \u00d7 p symmetric, positive definite matrices. When n \u2265 p the maximum likelihood covariance estimator \u03a3\u0302 is the sample covariance matrix S = 1\nn \u2211n i=1X (i)X(i) T . A problem however arises when n < p, due\n\u2217Equal contributors.\nar X\niv :1\n21 1.\n25 32\nv3 [\nst at\n.C O\nto the rank-deficiency in S. In this sample deficient case, common throughout several modern applications such as genomics, finance, and earth sciences, the matrix S is not invertible, and thus cannot be directly used to obtain a well-defined estimator for the inverse covariance matrix \u2126 := \u03a3\u22121.\nA related problem is the inference of a Gaussian graphical model ([33, 18]), that is, a sparsity pattern in the inverse covariance matrix, \u2126. Gaussian graphical models provide a powerful means of dimensionality reduction in high-dimensional data. Moreover, such models allow for discovery of conditional independence relations between random variables since, for multivariate Gaussian data, sparsity in the inverse covariance matrix encodes conditional independences. Specifically, if X = (Xi) p i=1 \u2208 Rp is distributed as X \u223c Np(0,\u03a3), then (\u03a3\u22121)ij = \u2126ij = 0 \u21d0\u21d2 Xi \u22a5\u22a5 Xj|{Xk}k 6=i,j, where the notation A \u22a5\u22a5 B|C denotes the conditional independence of A and B given the set of variables C (see [33, 18]). If a dataset, even one with n p is drawn from a normal distribution with sparse inverse covariance matrix \u2126, the inverse sample covariance matrix S\u22121 will almost surely be a dense matrix, although the estimates for those \u2126ij which are equal to 0 may be very small in magnitude. As sparse estimates of \u2126 are more robust than S\u22121, and since such sparsity may yield easily interpretable models, there exists significant impetus to perform sparse inverse covariance estimation in very high dimensional low sample size settings.\nBanerjee et al. [1] proposed performing such sparse inverse covariance estimation by solving the `1-penalized maximum likelihood estimation problem,\n\u0398\u2217\u03c1 = arg min \u0398\u2208Sp++ \u2212 log det \u0398 + \u3008S,\u0398\u3009+ \u03c1 \u2016\u0398\u20161 , (1)\nwhere \u03c1 > 0 is a penalty parameter, \u3008S,\u0398\u3009 = Tr (S\u0398), and \u2016\u0398\u20161 = \u2211\ni,j |\u0398ij|. For \u03c1 > 0, Problem (1) is strongly convex and hence has a unique solution, which lies in the positive definite cone Sp++ due to the log det term, and is hence invertible. Moreover, the `1 penalty induces sparsity in \u0398\u2217\u03c1, as it is the closest convex relaxation of the 0 \u2212 1 penalty, \u2016\u0398\u20160 =\u2211\ni,j I(\u0398ij 6= 0), where I(\u00b7) is the indicator function [5]. The unique optimal point of problem (1), \u0398\u2217\u03c1, is both invertible (for \u03c1 > 0) and sparse (for sufficiently large \u03c1), and can be used as an inverse covariance matrix estimator.\nIn this paper, a proximal gradient method for solving Problem (1) is proposed. The resulting \u201cgraphical iterative shrinkage thresholding algorithm\u201d, or G-ISTA, is shown to converge at a\nlinear rate to \u0398\u2217\u03c1, that is, its iterates \u0398t are proven to satisfy\u2225\u2225\u0398t+1 \u2212\u0398\u2217\u03c1\u2225\u2225F \u2264 s \u2225\u2225\u0398t \u2212\u0398\u2217\u03c1\u2225\u2225F , (2) for a fixed worst-case contraction constant s \u2208 (0, 1), where \u2016\u00b7\u2016F denotes the Frobenius norm. The convergence rate s is provided explicitly in terms of S and \u03c1, and importantly, is related to the condition number of \u0398\u2217\u03c1.\nWe also note that methods outside the penalized likelihood framework have been proposed in the context of graphical models. In particular graphical model estimation and related problems has also be undertaken either in the Bayesian or testing frameworks. The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.\nThe paper is organized as follows. Section 2 describes prior work related to solution of Problem (1). The G-ISTA algorithm is formulated in Section 3. Section 4 contains the convergence proofs of this algorithm, which constitutes the primary mathematical result of this paper. Numerical results are presented in Section 5, and concluding remarks are made in Section 6."}, {"heading": "2 Prior Work", "text": "While several excellent general convex solvers exist (for example, [12] and [4]), these are not always adept at handling high dimensional problems (i.e., p > 1000). As many modern datasets have several thousands of variables, numerous authors have proposed efficient algorithms designed specifically to solve the `1-penalized sparse maximum likelihood covariance estimation problem (1).\nThese can be broadly categorized as either primal or dual methods. Following the literature, we refer to primal methods as those which directly solve Problem (1), yielding a concentration estimate. Dual methods [1] yield a covariance matrix by solving the constrained problem,\nminimize U\u2208Rp\u00d7p\n\u2212 log det(S + U)\u2212 p\nsubject to \u2016U\u2016\u221e \u2264 \u03c1, (3)\nwhere the primal and dual variables are related by \u0398 = (S + U)\u22121. Both the primal and dual problems can be solved using block methods (also known as \u201crow by row\u201d methods), which sequentially optimize one row/column of the argument at each step until convergence. The primal and dual block problems both reduce to `1-penalized regressions, which can be\nsolved very efficiently."}, {"heading": "2.1 Dual Methods", "text": "A number of dual methods for solving Problem (1) have been proposed in the literature. Banerjee et al. [1] consider a block coordinate descent algorithm to solve the block dual problem, which reduces each optimization step to solving a box-constrained quadratic program. Each of these quadratic programs is equivalent to performing a \u201classo\u201d (`1-regularized) regression. Friedman et al. [11] iteratively solve the lasso regression as described in [1], but do so using coordinate-wise descent. Their widely used solver, known as the graphical lasso (glasso) is implemented on CRAN. Global convergence rates of these block coordinate methods are unknown. D\u2019Aspremont et al. [9] use Nesterov\u2019s smooth approximation scheme, which produces an \u03b5-optimal solution in O(1/\u03b5) iterations. A variant of Nesterov\u2019s smooth method is shown to have a O(1/ \u221a \u03b5) iteration complexity in [20, 21]."}, {"heading": "2.2 Primal Methods", "text": "Interest in primal methods for solving Problem (1) has been growing for many reasons. One important reason stems from the fact that convergence within a certain tolerance for the dual problem does not necessarily imply convergence within the same tolerance for the primal.\nYuan and Lin [36] use interior point methods based on the max-det problem studied in [32]. Yuan [37] use an alternating-direction method, while Scheinberg et al. [30] proposes a similar method and show a sublinear convergence rate. Mazumder and Hastie [23] consider blockcoordinate descent approaches for the primal problem, similar to the dual approach taken in [11]. Mazumder and Agarwal [22] also solve the primal problem with block-coordinate descent, but at each iteration perform a partial as opposed to complete block optimization, resulting in a decreased computational complexity per iteration. Convergence rates of these primal methods have not been considered in the literature and hence theoretical guarantees are not available. Hsieh et al. [16] propose a second-order proximal point algorithm, called QUIC, which converges superlinearly locally around the optimum."}, {"heading": "3 Methodology", "text": "In this section, the graphical iterative shrinkage thresholding algorithm (G-ISTA) for solving the primal problem (1) is presented. A rich body of mathematical and numerical work\nexists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31]. A brief description is provided here."}, {"heading": "3.1 General Iterative Shrinkage Thresholding (ISTA)", "text": "Iterative shrinkage thresholding algorithms (ISTA) are general first-order techniques for solving problems of the form\nminimize x\u2208X F (x) := f(x) + g(x), (4)\nwhere X is a Hilbert space with inner product \u3008\u00b7, \u00b7\u3009 and associated norm \u2016\u00b7\u2016, f : X \u2192 R is a continuously differentiable, convex function, and g : X \u2192 R is a lower semi-continuous, convex function, not necessarily smooth. The function f is also often assumed to have Lipschitz-continuous gradient \u2207f , that is, there exists some constant L > 0 such that\n\u2016\u2207f(x1)\u2212\u2207f(x2)\u2016 \u2264 L \u2016x1 \u2212 x2\u2016 (5)\nfor any x1, x2 \u2208 X .\nFor a given lower semi-continuous convex function g, the proximity operator of g, denoted by proxg : X \u2192 X , is given by\nproxg(x) = arg min y\u2208X\n{ g(y) + 1\n2 \u2016x\u2212 y\u20162\n} , (6)\nIt is well known (for example, [8]) that x\u2217 \u2208 X is an optimal solution of problem (4) if and only if\nx\u2217 = prox\u03b6g(x \u2217 \u2212 \u03b6\u2207f(x\u2217)) (7)\nfor any \u03b6 > 0. The above characterization suggests a method for optimizing problem (4) based on the iteration\nxt+1 = prox\u03b6tg (xt \u2212 \u03b6t\u2207f(xt)) (8)\nfor some choice of step size, \u03b6t. This simple method is referred to as an iterative shrinkage thresholding algorithm (ISTA). For a step size \u03b6t \u2264 1L , the ISTA iterates xt are known to\nsatisfy\nF (xt)\u2212 F (x\u2217) ' O ( 1\nt\n) ,\u2200t, (9)\nwhere x\u2217 is some optimal point, which is to say, they converge to the space of optimal points at a sublinear rate. If no Lipschitz constant L for \u2207f is known, the same convergence result still holds for \u03b6t chosen such that\nf(xt+1) \u2264 Q\u03b6t(xt+1, xt), (10)\nwhere Q\u03b6(\u00b7, \u00b7) : X \u00d7 X \u2192 R is a quadratic approximation to f , defined by\nQ\u03b6(x, y) = f(y) + \u3008x\u2212 y,\u2207f(y)\u3009+ 1\n2\u03b6 \u2016x\u2212 y\u20162 . (11)\nSee [3] for more details.\n3.2 Graphical Iterative Shrinkage Thresholding (G-ISTA)\nThe general method described in Section 3.1 can be adapted to the sparse inverse covariance estimation Problem (1). Using the notation introduced in Problem (4), define f, g : Sp++ \u2192 R by f(X) = \u2212 log det(X)+ \u3008S,X\u3009 and g(X) = \u03c1 \u2016X\u20161. Both are continuous convex functions defined on Sp++. Although the function \u2207f(X) = S \u2212X\u22121 is not Lipschitz continuous over Sp++, it is Lipschitz continuous within any compact subset of S p ++ (See Lemma 2 of the Supplemental section).\nLemma 1 ([1, 20]). The solution of Problem (1), \u0398\u2217\u03c1, satisfies \u03b1I \u0398\u2217\u03c1 \u03b2I, for\n\u03b1 = 1\n\u2016S\u20162 + p\u03c1 , \u03b2= min\n{ p\u2212 \u03b1Tr(S)\n\u03c1 , \u03b3\n} , (12)\nand\n\u03b3 =\n{ min{1T |S\u22121|1, (p\u2212 \u03c1\u221ap\u03b1) \u2016S\u22121\u20162 \u2212 (p\u2212 1)\u03b1} if S \u2208 S p ++\n21T \u2223\u2223(S + \u03c1\n2 I)\u22121 \u2223\u22231\u2212 Tr((S + \u03c1 2 I)\u22121) otherwise,\n(13)\nwhere I denotes the p\u00d7p dimensional identity matrix and 1 denotes the p-dimensional vector of ones.\nNote that f + g as defined is a continuous, strongly convex function on Sp++. Moreover, by Lemma 2 of the supplemental section, f has a Lipschitz continuous gradient when restricted\nto the compact domain aI \u0398 bI. Hence, f and g as defined meet the conditions described in Section 3.1.\nThe proximity operator of \u03c1 \u2016X\u20161 for \u03c1 > 0 is the soft-thresholding operator, \u03b7\u03c1 : Rp\u00d7p \u2192 Rp\u00d7p, defined entrywise by\n[\u03b7\u03c1(X)]i,j = sgn(Xi,j) (|Xi,j| \u2212 \u03c1)+ , (14)\nwhere for some x \u2208 R, (x)+ := max(x, 0) (see [8]). Finally, the quadratic approximation Q\u03b6t of f , as in equation (11), is given by\nQ\u03b6t(\u0398t+1,\u0398t) = \u2212 log det(\u0398t) + \u3008S,\u0398t\u3009+ \u3008\u0398t+1 \u2212\u0398t, S \u2212\u0398\u22121t \u3009+ 1\n2\u03b6t \u2016\u0398t+1 \u2212\u0398t\u20162F . (15)\nThe G-ISTA algorithm for solving Problem (1) is given in Algorithm 1. As in [3], the algorithm uses a backtracking line search for the choice of step size. The procedure terminates when a pre-specified duality gap is attained. The authors found that an initial estimate of \u03980 satisfying [\u03980]ii = (Sii+\u03c1) \u22121 works well in practice. Note also that the positive definite check of \u0398t+1 during Step (1) of Algorithm 1 is accomplished using a Cholesky decomposition, and the inverse of \u0398t+1 is computed using that Cholesky factor.\nAlgorithm 1: G-ISTA for Problem (1) input : Sample covariance matrix S, penalty parameter \u03c1, tolerance \u03b5, backtracking constant c \u2208 (0, 1), initial step size \u03b61,0, initial iterate \u03980. Set \u2206 := 2\u03b5. while \u2206 > \u03b5 do\n(1) Line search: Let \u03b6t be the largest element of {cj\u03b6t,0}j=0,1,... so that for \u0398t+1 = \u03b7\u03b6t\u03c1 ( \u0398t \u2212 \u03b6t(S \u2212\u0398\u22121t ) ) , the following are satisfied:\n\u0398t+1 0 and f(\u0398t+1) \u2264 Q\u03b6t(\u0398t+1,\u0398t),\nfor Q\u03b6t as defined in (15). (2) Update iterate: \u0398t+1 = \u03b7\u03b6t\u03c1 ( \u0398t \u2212 \u03b6t(S \u2212\u0398\u22121t ) ) (3) Set next initial step, \u03b6t+1,0. See Section 3.2.1. (4) Compute duality gap:\n\u2206 = \u2212 log det(S + Ut+1)\u2212 p\u2212 log det \u0398t+1 + \u3008S,\u0398\u3009+ \u03c1 \u2016\u0398t+1\u20161 ,\nwhere (Ut+1)i,j = min{max{([\u0398\u22121t+1]i,j \u2212 Si,j),\u2212\u03c1}, \u03c1}. end output: \u03b5-optimal solution to problem (1), \u0398\u2217\u03c1 = \u0398t+1."}, {"heading": "3.2.1 Choice of initial step size, \u03b60", "text": "Each iteration of Algorithm 1 requires an initial step size, \u03b60. The results of Section 4 guarantee that any \u03b60 \u2264 \u03bbmin(\u0398t)2 will be accepted by the line search criteria of Step 1 in the next iteration. However, in practice this choice of step is overly cautious; a much larger step can often be taken. Our implementation of Algorithm 1 chooses the Barzilai-Borwein step [2]. This step, given by\n\u03b6t+1,0 = Tr ((\u0398t+1 \u2212\u0398t)(\u0398t+1 \u2212\u0398t))\nTr ((\u0398t+1 \u2212\u0398t)(\u0398\u22121t \u2212\u0398\u22121t+1)) , (16)\nis also used in the SpaRSA algorithm [35], and approximates the Hessian around \u0398t+1. If a certain number of maximum backtracks do not result in an accepted step, G-ISTA takes the safe step, \u03bbmin(\u0398t) 2. Such a safe step can be obtained from \u03bbmax(\u0398 \u22121 t ), which in turn can be quickly approximated using power iteration."}, {"heading": "4 Convergence Analysis", "text": "In this section, linear convergence of Algorithm 1 is discussed. Throughout the section, \u0398t (t = 1, 2, . . . ) denote the iterates of Algorithm 1, and \u0398\u2217\u03c1 the optimal solution to Problem (1) for \u03c1 > 0. The minimum and maximum eigenvalues of a symmetric matrix A are denoted by \u03bbmin(A) and \u03bbmax(A), respectively. Theorem 1. Assume that the iterates \u0398t of Algorithm 1 satisfy aI \u0398t bI,\u2200t for some fixed constants 0 < a < b. If \u03b6t \u2264 a2,\u2200t, then\u2225\u2225\u0398t+1 \u2212\u0398\u2217\u03c1\u2225\u2225F \u2264 max{ \u2223\u2223\u2223\u22231\u2212 \u03b6tb2 \u2223\u2223\u2223\u2223 , \u2223\u2223\u2223\u22231\u2212 \u03b6ta2\n\u2223\u2223\u2223\u2223} \u2225\u2225\u0398t \u2212\u0398\u2217\u03c1\u2225\u2225F . (17) Furthermore,\n1. The step size \u03b6t which yields an optimal worst-case contraction bound s(\u03b6t) is \u03b6 = 2\na\u22122+b\u22122 .\n2. The optimal worst-case contraction bound corresponding to \u03b6 = 2 a\u22122+b\u22122 is given by\ns(\u03b6) : = 1\u2212 2 1 + b 2\na2\nProof. A direct proof is given in the appendix. Note that linear convergence of proximal gradient methods for strongly convex objective functions in general has already been proven\n(see Supplemental section).\nIt remains to show that there exist constants a and b which bound the eigenvalues of \u0398t,\u2200t. The existence of such constants follows directly from Theorem 1, as \u0398t lie in the bounded domain {\u0398 \u2208 Sp++ : f(\u0398) + g(\u0398) < f(\u03980) + g(\u03980)}, for all t. However, it is possible to specify the constants a and b to yield an explicit rate; this is done in Theorem 2.\nTheorem 2. Let \u03c1 > 0, define \u03b1 and \u03b2 as in Lemma 1, and assume \u03b6t \u2264 \u03b12,\u2200t. Then the iterates \u0398t of Algorithm 1 satisfy \u03b1I \u0398t b\u2032I,\u2200t, with b\u2032 = \u2225\u2225\u0398\u2217\u03c1\u2225\u22252 + \u2225\u2225\u03980 \u2212\u0398\u2217\u03c1\u2225\u2225F \u2264 \u03b2 + \u221a p(\u03b2 + \u03b1).\nProof. See the Supplementary section.\nImportantly, note that the bounds of Theorem 2 depend explicitly on the bound of \u0398\u2217\u03c1, as given by Lemma 1. These eigenvalue bounds on \u0398t+1, along with Theorem 1, provide a closed form linear convergence rate for Algorithm 1. This rate depends only on properties of the solution.\nTheorem 3. Let \u03b1 and \u03b2 be as in Lemma 1. Then for a constant step size \u03b6t := \u03b6 < \u03b1 2, the iterates of Algorithm 1 converge linearly with a rate of\ns(\u03b6) = 1\u2212 2\u03b1 2\n\u03b12 + (\u03b2 + \u221a p(\u03b2 \u2212 \u03b1))2 < 1 (18)\nProof. By Theorem 2, for \u03b6 < \u03b12, the iterates \u0398t satisfy\n\u03b1I \u0398t (\u2225\u2225\u0398\u2217\u03c1\u2225\u22252 + \u2225\u2225\u03980 \u2212\u0398\u2217\u03c1\u2225\u2225F) I\nfor all t. Moreover, since \u03b1I \u0398\u2217 \u03b2I, if \u03b1I \u03980 \u03b2I (for instance, by taking \u03980 = (S + \u03c1I)\n\u22121 or some multiple of the identity) then this can be bounded as:\u2225\u2225\u0398\u2217\u03c1\u2225\u22252 + \u2225\u2225\u03980 \u2212\u0398\u2217\u03c1\u2225\u2225F \u2264 \u03b2 +\u221ap \u2225\u2225\u03980 \u2212\u0398\u2217\u03c1\u2225\u22252 (19) \u2264 \u03b2 +\u221ap(\u03b2 \u2212 \u03b1). (20)\nTherefore,\n\u03b1I \u0398t (\u03b2 + \u221a p(\u03b2 \u2212 \u03b1)) I, (21)\nand the result follows from Theorem 1.\nRemark 1. Note that the contraction constant (equation 18) of Theorem 3 is closely related to the condition number of \u0398\u2217\u03c1,\n\u03ba(\u0398\u2217\u03c1) = \u03bbmax(\u0398\n\u2217 \u03c1)\n\u03bbmin(\u0398 \u2217 \u03c1) \u2264 \u03b2 \u03b1\nas\n1\u2212 2\u03b1 2\n\u03b12 + (\u03b2 + \u221a p(\u03b2 \u2212 \u03b1))2\n\u2265 1\u2212 2\u03b1 2\n\u03b12 + \u03b22 \u2265 1\u2212 2\u03ba(\u0398\u2217\u03c1)\u22122. (22)\nTherefore, the worst case bound becomes close to 1 as the conditioning number of \u0398\u2217\u03c1 increases.\nIt is important to compare the above convergence results to those that have been recently established. In particular, the useful, recent QUIC method [16] warrants a discussion. As soon as the sign pattern of its iterates match that of the true optimum, the non-smooth problem becomes effectively smooth and the QUIC algorithm reduces to a Newton method. At this point, QUIC converges quadratically; however, this is a very local property, and no overall complexity bounds have been specified for QUIC. This can be contrasted with our results, which take advantage of existing bounds on the optimal solution to yield global convergence (i.e., that we can always specify a starting point which meets our conditions). We also note that convergence rates have not been established for the glasso. However, QUIC and glasso can all be very fast in appropriate settings. Each brings a useful addition to the literature by taking advantage of different structural elements (block structure for glasso, second order approaches for QUIC, and conditioning bounds for G-ISTA). We feel there is no silver bullet; each method outperforms the others in certain settings."}, {"heading": "5 Numerical Results", "text": "In this section, we provide numerical results for the G-ISTA algorithm. In Section 5.2, the theoretical results of Section 4 are demonstrated. Section 5.3 compares running times of the G-ISTA, glasso [11], and QUIC [16] algorithms. All algorithms were implemented in C++, and run on an Intel i7\u2212 2600k 3.40GHz\u00d7 8 core with 16 GB of RAM."}, {"heading": "5.1 Synthetic Datasets", "text": "Synthetic data for this section was generated following the method used by [21, 22]. For a fixed p, a p dimensional inverse covariance matrix \u2126 was generated with off-diagonal entries drawn i.i.d from a uniform(\u22121, 1) distribution. These entries were set to zero with some fixed probability (in this case, either 0.97 or 0.85 to simulate a very sparse and a somewhat sparse model). Finally, a multiple of the identity was added to the resulting matrix so that the smallest eigenvalue was equal to 1. In this way, \u2126 was insured to be sparse, positive definite, and well-conditioned. Datsets of n samples were then generated by drawing i.i.d. samples from a Np(0,\u2126\u22121) distribution. For each value of p and sparsity level of \u2126, n = 1.2p and n = 0.2p were tested, to represent both the n < p and n > p cases."}, {"heading": "5.2 Demonstration of Convergence Rates", "text": "The linear convergence rate derived for G-ISTA in Section 4 was shown to be heavily dependent on the conditioning of the final estimator. To demonstrate these results, G-ISTA was run on a synthetic dataset, as described in Section 5.1, with p = 500 and n = 300. Regularization parameters of \u03c1 = 0.75, 0.1, 0.125, 0.15, and 0.175 were used. Note as \u03c1 increases, \u0398\u2217\u03c1 generally becomes better conditioned. For each value of \u03c1, the numerical optimum was computed to a duality gap of 10\u221210 using G-ISTA. These values of \u03c1 resulted in sparsity levels of 81.80%, 89.67%, 94.97%, 97.82%, and 99.11%, respectively. G-ISTA was then run again, and\nthe Frobenius norm argument errors at each iteration were stored. These errors were plotted on a log scale for each value of \u03c1 to demonstrate the dependence of the convergence rate on condition number. See Figure 1, which clearly demonstrates the effects of conditioning."}, {"heading": "5.3 Timing Comparisons", "text": "The G-ISTA, glasso, and QUIC algorithms were run on synthetic datasets (real datasets are presented in the Supplemental section) of varying p, n and with different levels of regularization, \u03c1. All algorithms were run to ensure a fixed duality gap, here taken to be 10\u22125. This comparison used efficient C++ implementations of each of the three algorithms investigated. The implementation of G-ISTA was adapted from the publicly available C++ implementation of QUIC Hsieh et al. [16]. Running times were recorded and are presented in Table 1. Further comparisons are presented in the Supplementary section.\nRemark 2. The three algorithms variable ability to take advantage of multiple processors is an important detail. The times presented in Table 1 are wall times, not CPU times. The comparisons were run on a multicore processor, and it is important to note that the Cholesky decompositions and inversions required by both G-ISTA and QUIC take advantage of multiple\ncores. On the other hand, the p2 dimensional lasso solve of QUIC and p-dimensional lasso solve of glasso do not. For this reason, and because Cholesky factorizations and inversions make up the bulk of the computation required by G-ISTA, the CPU time of G-ISTA was typically greater than its wall time by a factor of roughly 4. The CPU and wall times of QUIC were more similar; the same applies to glasso."}, {"heading": "6 Conclusion", "text": "In this paper, a proximal gradient method was applied to the sparse inverse covariance problem. Linear convergence was discussed, with a fixed closed-form rate. Numerical results have also been presented, comparing G-ISTA to the widely-used glasso algorithm and the newer, but very fast, QUIC algorithm. These results indicate that G-ISTA is competitive, in particular for values of \u03c1 which yield sparse, well-conditioned estimators. The G-ISTA algorithm was very fast on the synthetic examples of Section 5.3, which were generated from well-conditioned models. For poorly conditioned models, QUIC is very competitive. The Supplemental section gives two real datasets which demonstrate this. For many practical applications however, obtaining an estimator that is well-conditioned is important ([29, 34]). To conclude, although second-order methods for the sparse inverse covariance estimation problem have recently been shown to perform well, simple first-order methods cannot be ruled out, as they can also be very competitive in many cases.\nAcknowledgments: Dominique Guillot was supported in part by the National Science Foundation under Grant No. DMS-1106642. Bala Rajaratnam was supported in part by the National Science Foundation under Grant Nos. DMS-0906392 (ARRA), DMS-CMG-1025465, AGS-1003823, DMS-1106642 and grants NSA H98230-11-1-0194, DARPA-YFA N66001-11-14131, and SUWIEVP10-SUFSC10-SMSCVISG0906. Benjamin Rolfs was supported in part by the Department of Energy Office of Science Graduate Fellowship Program DE-AC0506OR23100 (ARRA) and NSF grant AGS-1003823."}, {"heading": "A Supplementary material", "text": "A.1 Lipschitz Continuity of \u2207f(X)\nLemma 2. For any X, Y \u2208 Sp++,\n1 b2 \u2016X \u2212 Y \u20162 \u2264 \u2225\u2225X\u22121 \u2212 Y \u22121\u2225\u2225 2 \u2264 1 a2 \u2016X \u2212 Y \u20162 ,\nwhere a = min{\u03bbmin(X), \u03bbmin(Y )} and b = max{\u03bbmax(X), \u03bbmax(Y )}.\nProof. To prove the right-hand side inequality, notice that\nX\u22121 \u2212 Y \u22121 = X\u22121(Y \u2212X)Y \u22121.\nThus, \u2225\u2225X\u22121 \u2212 Y \u22121\u2225\u2225 2 = \u2225\u2225X\u22121(Y \u2212X)Y \u22121\u2225\u2225 2\n\u2264 \u2225\u2225X\u22121\u2225\u2225\n2 \u2016X \u2212 Y \u20162 \u2225\u2225Y \u22121\u2225\u2225 2\n= \u03bbmax(X \u22121)\u03bbmax(Y \u22121) \u2016X \u2212 Y \u20162 = 1\n\u03bbmin(X)\n1\n\u03bbmin(Y ) \u2016X \u2212 Y \u20162\n\u2264 1 a2 \u2016X \u2212 Y \u20162 .\nTo prove the left inequality, note first that\nY \u2212X = X(X\u22121 \u2212 Y \u22121)Y.\nTherefore,\n\u2016X \u2212 Y \u20162 = \u2225\u2225X(X\u22121 \u2212 Y \u22121)Y \u2225\u2225\n2 \u2264 \u2016X\u20162 \u2225\u2225X\u22121 \u2212 Y \u22121\u2225\u2225 2 \u2016Y \u20162\n= \u03bbmax(X)\u03bbmax(Y ) \u2225\u2225X\u22121 \u2212 Y \u22121\u2225\u2225\n2 \u2264 b2 \u2225\u2225X\u22121 \u2212 Y \u22121\u2225\u2225\n2 .\nThis shows that \u2225\u2225X\u22121 \u2212 Y \u22121\u2225\u2225 2 \u2265 1 b2 \u2016X \u2212 Y \u20162 and concludes the proof.\nThe function \u2207f(X) = S \u2212X\u22121 is Lipschitz continuous on any compact domain, since for X, Y \u2208 Sp++ such that aI X, Y bI,\n\u2016\u2207f(X)\u2212\u2207f(Y )\u2016F = \u2225\u2225X\u22121 \u2212 Y \u22121\u2225\u2225\nF \u2264 \u221ap \u2225\u2225X\u22121 \u2212 Y \u22121\u2225\u2225\n2\n\u2264 \u221a p\na2 \u2016X \u2212 Y \u20162\n\u2264 \u221a p\na2 \u2016X \u2212 Y \u2016F .\nA.2 Proof of Theorem 1\nWe now provide the proof of Theorem 1. Lemma 3. Let \u0398t be as in Algorithm 1 and let \u0398 \u2217 \u03c1 be the optimal point of problem (1). Also, define\nb := max { \u03bbmax(\u0398t), \u03bbmax(\u0398 \u2217 \u03c1) } , a:= min { \u03bbmin(\u0398t), \u03bbmin(\u0398 \u2217 \u03c1) } .\nThen \u2225\u2225\u0398t+1 \u2212\u0398\u2217\u03c1\u2225\u2225F \u2264 max{ \u2223\u2223\u2223\u22231\u2212 \u03b6tb2 \u2223\u2223\u2223\u2223 , \u2223\u2223\u2223\u22231\u2212 \u03b6ta2 \u2223\u2223\u2223\u2223} \u2225\u2225\u0398t \u2212\u0398\u2217\u03c1\u2225\u2225F .\nProof. By construction in Algorithm 1,\n\u0398t+1 = \u03b7\u03b6t\u03c1 ( (\u0398t \u2212 \u03b6t(S \u2212\u0398\u22121t ) ) Moreover, as \u0398\u2217\u03c1 is a fixed point of the ISTA iteration [8, Prop. 3.1], it satisfies\n\u0398\u2217\u03c1 = \u03b7\u03b6t\u03c1 ( \u0398\u2217\u03c1 \u2212 \u03b6t(S \u2212 (\u0398\u2217\u03c1)\u22121) ) .\nThe soft-thresholding operator \u03b7\u03c1(\u00b7) is a proximity operator corresponding to \u03c1 \u2016\u00b7\u20161. Since prox operators are non-expansive [8, Lemma 2.2], it follows that:\u2225\u2225\u0398t+1 \u2212\u0398\u2217\u03c1\u2225\u2225F = \u2225\u2225\u03b7\u03b6t\u03c1 (\u0398t \u2212 \u03b6t(S \u2212\u0398\u22121t ))\u2212 \u03b7\u03b6t\u03c1 (\u0398\u2217\u03c1 \u2212 \u03b6t(S \u2212 (\u0398\u2217\u03c1)\u22121))\u2225\u2225F\n\u2264 \u2225\u2225\u0398t \u2212 \u03b6t(S \u2212\u0398\u22121t )\u2212 (\u0398\u2217\u03c1 \u2212 \u03b6t(S \u2212 (\u0398\u2217\u03c1)\u22121))\u2225\u2225F\n= \u2225\u2225(\u0398t + \u03b6t\u0398\u22121t )\u2212 (\u0398\u2217\u03c1 + \u03b6t(\u0398\u2217\u03c1)\u22121)\u2225\u2225F\nTo bound the latter expression, recall that if h : U \u2282 Rn \u2192 Rm is a differentiable mapping,\nwith x, y \u2208 U , and cx+ (1\u2212 c)y \u2208 U for all c \u2208 [0, 1], then\n\u2016h(x)\u2212 h(y)\u2016 \u2264 sup c\u2208[0,1] {\u2016Jh (cx+ (1\u2212 c)y)\u2016 \u2016x\u2212 y\u2016}\nwhere Jh(\u00b7) is the Jacobian of h. Define h\u03b3 : Sp++ \u2192 Rp 2 by\nh\u03b3(X) = vec(X) + vec(\u03b3X \u22121),\nwhere vec(\u00b7) : Rp\u00d7p \u2192 Rp2 is the vectorization operator defined by\nvec(A) = (A1,, A2,, . . . , Ap,) T\nwith Ai, the i th row of A. Note that for X \u2208 Sp++,\n\u2202X \u2202X = Ip2 and\n\u2202X\u22121\n\u2202X = \u2212X\u22121 \u2297X\u22121,\nwhere \u2297 is the Kronecker product and Ip2 is the p2\u00d7 p2 identity matrix. Then the Jacobian of h\u03b3 is given by:\nJh\u03b3 (X) = Ip2 \u2212 \u03b3X\u22121 \u2297X\u22121.\nApplication of the mean value theorem to h\u03b6t over Zt,c = vec(c\u0398t + (1 \u2212 c)\u0398\u2217\u03c1), c \u2208 [0, 1] yields \u2225\u2225h\u03b6t(\u0398t)\u2212 h\u03b6t(\u0398\u2217\u03c1)\u2225\u2225F \u2264 sup\nc {\u2225\u2225Ip2 \u2212 \u03b6tZ\u22121t,c \u2297 Z\u22121t,c \u2225\u22252}\u2225\u2225vec(\u0398t)\u2212 vec(\u0398\u2217\u03c1)\u2225\u22252 = sup\nc {\u2225\u2225Ip2 \u2212 \u03b6tZ\u22121t,c \u2297 Z\u22121t,c \u2225\u22252}\u2225\u2225\u0398t \u2212\u0398\u2217\u03c1\u2225\u2225F . Denoting the eigenvalues of Zt,c for given values of t and c as 0 < \u03b31 \u2264 \u03b32 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03b3p, the eigenvalues of Ip2 \u2212 \u03b6tZ\u22121t,c \u2297 Z\u22121t,c are {1\u2212 \u03b6t(\u03b3i\u03b3j)\u22121} p i,j=1. By Weyl\u2019s inequality,\n\u03b3p = \u03bbmax(Zt,c)\u2264 max { \u03bbmax(\u0398t), \u03bbmax(\u0398 \u2217 \u03c1) }\n\u03b31 = \u03bbmin(Zt,c)\u2265 min { \u03bbmin(\u0398t), \u03bbmin(\u0398 \u2217 \u03c1) } ,\nand therefore\n\u03bbmin ( Ip2 \u2212 \u03b6tZ\u22121t,c \u2297 Z\u22121t,c ) = 1\u2212 \u03b6t\n\u03b321 \u2265 1\u2212 \u03b6t a2\n\u03bbmax ( Ip2 \u2212 \u03b6tZ\u22121t,c \u2297 Z\u22121t,c ) = 1\u2212 \u03b6t\n\u03b32p \u2264 1\u2212 \u03b6t b2 .\nHence,\nsup c {\u2225\u2225Ip2 \u2212 \u03b6tZ\u22121t,c \u2297 Z\u22121t,c \u2225\u22252} \u2264 max{ \u2223\u2223\u2223\u22231\u2212 \u03b6tb2 \u2223\u2223\u2223\u2223 , \u2223\u2223\u2223\u22231\u2212 \u03b6ta2 \u2223\u2223\u2223\u2223}\nwhich completes the proof.\nIt follows from Lemma 3 that Algorithm 1 converges linearly if\nst(\u03b6t) := max {\u2223\u2223\u2223\u22231\u2212 \u03b6tb2 \u2223\u2223\u2223\u2223 , \u2223\u2223\u2223\u22231\u2212 \u03b6ta2 \u2223\u2223\u2223\u2223} \u2208 (0, 1),\u2200t. (23) Since the minimum of\ns(\u03b6) = max {\u2223\u2223\u2223\u22231\u2212 \u03b6a2 \u2223\u2223\u2223\u2223 , \u2223\u2223\u2223\u22231\u2212 \u03b6b2 \u2223\u2223\u2223\u2223} is at \u03b6 = 2\na\u22122+b\u22122 , Theorem 1 follows directly from Lemma 3. It now remains to show\nthat the eigenvalues of the G-ISTA iterates remain bounded in eigenvalue. A more general convergence result for strongly convex functions exists in the literature; this result is stated below.\nTheorem 4. Let f be strongly convex with convexity constant \u00b5, and \u2207f be Lipschitz continuous with constant L. Then for constant step size 0 < \u03b6 < 2\nL , the iterates of the ISTA\niteration (equation (8)), {xt}t\u22650 to minimize f + g as in (4), satisfy\n\u2016xt+1 \u2212 x\u2217\u2016F \u2264 max {|1\u2212 \u03b6L| , |1\u2212 \u03b6\u00b5|} \u2016xt \u2212 x \u2217\u2016F ,\nwhich is to say that they converge linearly with rate max {|1\u2212 \u03b6L| , |1\u2212 \u03b6\u00b5|}. Furthermore,\n1. The step size which yields an optimal worst-case contraction bound is \u03b6 = 2 \u00b5+L . 2. The optimal worst-case contraction bound corresponding to \u03b6 = 2 \u00b5+L is given by\ns(\u03b6) : = max {|1\u2212 \u03b6L| , |1\u2212 \u03b6\u00b5|}\n= 1\u2212 2 1 + \u00b5\nL\n.\nProof. See [7, 26] and references therein.\nA.3 Proof of Theorem 2\nIn this section, the eigenvalues of \u0398t,\u2200t are bounded. To begin, the eigenvalues of \u0398t+ 1 2 := \u0398t \u2212 \u03b6t(S \u2212\u0398\u22121t ) are bounded.\nLemma 4. Let 0 < a < b be given positive constants and let \u03b6t > 0. Assume aI \u0398t bI. Then the eigenvalues of \u0398t+ 1 2 := \u0398t \u2212 \u03b6t(S \u2212\u0398\u22121t ) satisfy:\n\u03bbmin(\u0398t+ 1 2 ) \u2265\n{ 2 \u221a \u03b6t \u2212 \u03b6t\u03bbmax(S) if a \u2264 \u221a \u03b6t \u2264 b\nmin ( a+ \u03b6t\na , b+ \u03b6t b ) \u2212 \u03b6t\u03bbmax(S) otherwise\n(24)\nand\n\u03bbmax(\u0398t+ 1 2 ) \u2264 max\n( a+\n\u03b6t a , b+ \u03b6t b\n) \u2212 \u03b6t\u03bbmin(S).\nProof. Denoting the eigenvalue decomposition of \u0398t by \u0398t = U\u0393U T ,\n\u0398t+ 1 2\n= \u0398t \u2212 \u03b6t(S \u2212\u0398\u22121t ) = U\u0393UT \u2212 \u03b6t(S \u2212 U\u0393\u22121UT )\n= U ( \u0393\u2212 \u03b6t(UTSU \u2212 \u0393\u22121) ) UT\nLet \u0393 = diag(\u03b31, . . . , \u03b3p) with \u03b31 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03b3p. By Weyl\u2019s inequality, the eigenvalues of \u0398t+ 1 2 are bounded below by\n\u03bbi ( \u0398t+ 1\n2\n) \u2265 \u03b3i +\n\u03b6t \u03b3i \u2212 \u03b6t\u03bbmax(S),\nand bounded above by\n\u03bbi ( \u0398t+ 1\n2\n) \u2264 \u03b3i +\n\u03b6t \u03b3i \u2212 \u03b6t\u03bbmin(S)\nThe function f(x) = x+ \u03b6t x over a \u2264 x \u2264 b has only one extremum which is a global minimum at x = \u221a \u03b6t. Therefore,\nmin a\u2264x\u2264b x+ \u03b6t x =\n{ 2 \u221a \u03b6t if a \u2264 \u221a \u03b6t \u2264 b\nmin ( a+ \u03b6t\na , b+ \u03b6t b\n) otherwise\n,\nand\nmax a\u2264x\u2264b x+ \u03b6t x = max\n( a+\n\u03b6t b , b+ \u03b6t b\n) .\nSince a \u2264 \u03b31 \u2264 b,\n\u03bbmin(\u0398t+ 1 2 ) \u2265 \u03b31 + \u03b6t \u03b31 \u2212 \u03b6t\u03bbmax(S)\n\u2265 min a\u2264x\u2264b\n( x+\n\u03b6t x\n) \u2212 \u03b6t\u03bbmax(S)\n=\n{ 2 \u221a \u03b6t \u2212 \u03b6t\u03bbmax(S) if a \u2264 \u221a \u03b6t \u2264 b\nmin ( a+ \u03b6t\na , b+ \u03b6t b ) \u2212 \u03b6t\u03bbmax(S) otherwise\nSimilarly,\n\u03bbmax(\u0398t+ 1 2 ) \u2264 \u03b3p + \u03b6t \u03b3p \u2212 \u03b6t\u03bbmin(S)\n\u2264 max a\u2264x\u2264b\n( x+\n\u03b6t x\n) \u2212 \u03b6t\u03bbmin(S)\n= max ( a+\n\u03b6t a , b+ \u03b6t b\n) \u2212 \u03b6t\u03bbmin(S).\nIt remains to demonstrate that the soft-thresholded iterates \u0398t+1 remain bounded in eigenvalue.\nLemma 5. Let 0 < a < b and \u03b6t > 0. Then:\nmin ( a+\n\u03b6t a , b+ \u03b6t b\n) = a+\n\u03b6t a\nif and only if \u03b6t \u2264 ab.\nProof. Under the stated assumptions,\na+ \u03b6t a \u2264 b+ \u03b6t b \u21d4 \u03b6t\n( 1\na \u2212 1 b\n) \u2264 b\u2212 a\n\u21d4 \u03b6t \u2264 b\u2212 a 1 a \u2212 1 b \u21d4 \u03b6t \u2264 ab.\nLemma 6. Let A be a symmetric p \u00d7 p matrix. Then the soft-thresholded matrix \u03b7 (A) satisfies\n\u03bbmin(A)\u2212 p \u2264 \u03bbmin(\u03b7 (A))\nIn particular, A is positive definite if \u03bbmin(A) > p .\nProof. Let\nA := {M \u2208Mp : Mi,j \u2208 {0, 1,\u22121}}.\nFor every > 0, the matrix A can be written as\n\u03b7 (A) = A+ 1A1 + 2A2 + \u00b7 \u00b7 \u00b7+ kAk,\nfor some k \u2264 ( p 2 ) + p where Ai \u2208 A, i > 0 and \u2211k i=1 i = . Now let\ncp := max{|\u03bbmin(M)| : M \u2208 A}.\nThe constant cp is finite since A is a finite set. Since \u2212A \u2208 A for every A \u2208 A, and since |\u03bbmin(\u2212A)| = |\u03bbmax(A)|, it follows that\ncp = max{|\u03bbmax(M)| : M \u2208 A}.\nApplying the Gershgorin circle theorem [see, e.g., 15] gives cp \u2264 p. Since p is an eigenvalue of the matrix B such that Bi,j = 1 for all i, j, it follows that cp = p.\nRecursive application of Weyl\u2019s inequality gives that\n\u03bbmin (\u03b7 (A)) \u2265 \u03bbmin(A)\u2212 |\u03bbmax(A1)| \u2212 \u00b7 \u00b7 \u00b7 \u2212 k|\u03bbmax(Ak)|\n\u2265 \u03bbmin(A)\u2212 cp k\u2211 i=1 i = \u03bbmin(A)\u2212 cp .\nRecall from Lemma 1 that the eigenvalues of the optimal solution to problem (1) are bounded below by 1\u2016S\u20162+p\u03c1 . The following theorem shows that \u03b1 = 1 \u2016S\u20162+p\u03c1 is a valid bound to ensure that \u03b1I \u0398t+1 if \u03b1I \u0398t. Lemma 7. Let \u03c1 > 0 and \u03b1 = 1\u2016S\u20162+p\u03c1 < b \u2032. Assume \u03b1I \u0398t b\u2032 and consider\n\u0398t+1 = \u03b7\u03b6t\u03c1 ( \u0398t \u2212 \u03b6t(S \u2212\u0398\u22121t ) )\nThen for every 0 < \u03b6t \u2264 \u03b12, \u03b1I \u0398t+1.\nProof. The result follows by combining Lemma 4 and Lemma 6. Notice first that the hypothesis \u03b6t \u2264 \u03b12 guarantees that \u221a \u03b6t /\u2208 [\u03b1, b\u2032]. Also, from Lemma 5, we have\nmin ( \u03b1 +\n\u03b6t \u03b1 , b\u2032 + \u03b6t b\u2032\n) = \u03b1 +\n\u03b6t \u03b1\nsince \u03b6t \u2264 \u03b12 \u2264 \u03b1b\u2032. Hence, by Lemma 4,\n\u03bbmin(\u0398t+ 1 2 ) \u2265 min\n( \u03b1 +\n\u03b6t \u03b1 , b\u2032 + \u03b6t b\u2032\n) \u2212 \u03b6t\u03bbmax(S)\n= \u03b1 + \u03b6t \u03b1 \u2212 \u03b6t\u03bbmax(S).\nNow, applying Lemma 6 to \u0398t+1 = \u03b7\u03b6t\u03c1(\u0398t+ 1 2 ), we obtain\n\u03bbmin(\u0398t+1) = \u03bbmin ( \u03b7\u03b6t\u03c1(\u0398t+ 1 2 ) )\n\u2265 \u03bbmin(\u0398t+ 1 2 )\u2212 p\u03c1\u03b6t \u2265 \u03b1 + \u03b6t \u03b1 \u2212 \u03b6t\u03bbmax(S)\u2212 p\u03c1\u03b6t.\nWe therefore have \u03b1I \u0398t+1 whenever\n\u03b1 + \u03b6t \u03b1 \u2212 \u03b6t\u03bbmax(S)\u2212 p\u03c1\u03b6t \u2265 \u03b1.\nThis is equivalent to\n\u03b6t\n( 1\n\u03b1 \u2212 \u03bbmax(S)\u2212 p\u03c1\n) \u2265 0.\nSince \u03b6t > 0, this is equivalent to\n1 \u03b1 \u2212 \u03bbmax(S)\u2212 p\u03c1 \u2265 0.\nReorganizing the terms of the previous equation, we obtain that \u03b1I \u0398t+1 if\n\u03b1 \u2264 1 \u03bbmax(S) + p\u03c1 = 1 \u2016S\u20162 + p\u03c1 .\nIt remains to show that the eigenvalues of the iterates \u0398t remain bounded above, for all t.\nLemma 8. Let \u03b1 = 1\u2016S\u20162+p\u03c1 and let \u03b6t \u2264 \u03b1 2,\u2200t. Then the G-ISTA iterates \u0398t satisfy\n\u0398t b\u2032I,\u2200t, with b\u2032 = \u2225\u2225\u0398\u2217\u03c1\u2225\u22252 + \u2225\u2225\u03980 \u2212\u0398\u2217\u03c1\u2225\u2225F .\nProof. By Lemma 7, \u03b1I \u0398t for every t. As \u03b1I \u0398\u2217 (Lemma 1),\n\u039b\u2212t := min{\u03bbmin(\u0398t), \u03bbmin(\u0398\u2217\u03c1)}2 \u2265 \u03b12.\nfor all t. Also, since \u039b+t \u2265 \u039b\u2212t and \u03b6t \u2264 \u03b12,\nmax {\u2223\u2223\u2223\u22231\u2212 \u03b6tb2 \u2223\u2223\u2223\u2223 , \u2223\u2223\u2223\u22231\u2212 \u03b6ta2 \u2223\u2223\u2223\u2223} \u2264 1. Therefore, by Lemma 3,\n\u2016\u0398t \u2212\u0398\u2217\u03c1\u2016F \u2264 \u2016\u0398t\u22121 \u2212\u0398\u2217\u03c1\u2016F .\nApplying this result recursively gives\n\u2016\u0398t \u2212\u0398\u2217\u03c1|F \u2264 \u2016\u03980 \u2212\u0398\u2217\u03c1\u2016F .\nSince \u2016 \u00b7 \u20162 \u2264 \u2016 \u00b7 \u2016F , we therefore have\n\u2016\u0398t\u20162 \u2212 \u2016\u0398\u2217\u03c1\u20162 \u2264 \u2016\u0398t \u2212\u0398\u2217\u03c1\u20162 \u2264 \u2016\u0398t \u2212\u0398\u2217\u03c1\u2016F \u2264 \u2016\u03980 \u2212\u0398\u2217\u03c1\u2016F ,\nand so,\n\u03bbmax(\u0398t) = \u2016\u0398t\u20162 \u2264 \u2016\u0398\u2217\u03c1\u20162 + \u2016\u03980 \u2212\u0398\u2217\u03c1\u2016F\nwhich completes the proof.\nA.4 Additional timing comparisons\nThis section provides additional synthetic timing comparisions for p = 500 and p = 5000. In addition, two real datasets were investigated. The \u201cestrogen\u201d dataset [27] contains p = 652 dimensional gene expression data from n = 158 breast cancer patients. The \u201ctemp\u201d dataset [6] consists of average annual temperature measurements from p = 1732 locations over n = 157 years (1850-2006)."}], "references": [{"title": "d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivarate gaussian or binary data", "author": ["O. Banerjee", "L. El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Two-Point Step Size Gradient Methods", "author": ["Jonathan Barzilai", "Jonathan M. Borwein"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Templates for convex cone problems with applications to sparse signal recovery", "author": ["S. Becker", "E.J. Candes", "M. Grant"], "venue": "Mathematical Programming Computation, 3:165\u2013218,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Uncertainty estimates in regional and global observed temperature changes: A new data set from 1850", "author": ["P. Brohan", "J.J. Kennedy", "I. Harris", "S.F.B. Tett", "P.D. Jones"], "venue": "Journal of Geophysical Research, 111,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Convergence rates in forward-backward splitting", "author": ["George H.G. Chen", "R.T. Rockafellar"], "venue": "Siam Journal on Optimization,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Signal recovery by proximal forwardbackward splitting", "author": ["Patrick L. Combettes", "Val\u00e9rie R. Wajs"], "venue": "Multiscale Modeling & Simulation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "First-order methods for sparse covariance selection", "author": ["Alexandre D\u2019Aspremont", "Onureena Banerjee", "Laurent El Ghaoui"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Hyper-markov laws in the statistical analysis of decomposable graphical models", "author": ["A.P. Dawid", "S.L. Lauritzen"], "venue": "Annals of Statistics, 21:1272\u20131317,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Biostatistics, 9:432\u2013441,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "CVX: Matlab software for disciplined convex programming, version 1.21", "author": ["M. Grant", "S. Boyd"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Large-scale correlation screening", "author": ["A. Hero", "B. Rajaratnam"], "venue": "Journal of the American Statistical Association, 106(496):1540\u20131552,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Hub discovery in partial correlation graphs", "author": ["A. Hero", "B. Rajaratnam"], "venue": "IEEE Transactions on Information Theory, 58(9):6064\u20136078,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Matrix Analysis", "author": ["Roger A. Horn", "Charles R. Johnson"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Sparse inverse covariance matrix estimation using quadratic approximation", "author": ["Cho-Jui Hsieh", "Matyas A. Sustik", "Inderjit S. Dhillon", "Pradeep K. Ravikumar"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Wishart distributions for decomposable covariance graph models", "author": ["K. Khare", "B. Rajaratnam"], "venue": "Annals of Statistics, 39(1):514\u2013555,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Graphical models", "author": ["S.L. Lauritzen"], "venue": "Oxford Science Publications. Clarendon Press,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Wishart distributions for decomposable graphs", "author": ["G. Letac", "H. Massam"], "venue": "Annals of Statistics, 35(3):1278\u20131323,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Smooth optimization approach for sparse covariance selection", "author": ["Zhaosong Lu"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Adaptive first-order methods for general sparse inverse covariance selection", "author": ["Zhaosong Lu"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "A flexible, scalable and efficient algorithmic framework for the Primal graphical lasso", "author": ["Rahul Mazumder", "Deepak K. Agarwal"], "venue": "Pre-print,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "The graphical lasso: New insights and alternatives", "author": ["Rahul Mazumder", "Trevor Hastie"], "venue": "Pre-print,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k)", "author": ["Yurii Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1983}, {"title": "Introductory Lectures on Convex Optimization", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Yurii Nesterov"], "venue": "CORE discussion papers, Universite\u0301 catholique de Louvain, Center for Operations Research and Econometrics (CORE),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Integrated modeling of clinical and gene expression information for personalized prediction of disease outcomes", "author": ["Jennifer Pittman", "Erich Huang", "Holly Dressman", "Cheng-Fang F. Horng", "Skye H. Cheng", "Mei-Hua H. Tsou", "Chii-Ming M. Chen", "Andrea Bild", "Edwin S. Iversen", "Andrew T. Huang", "Joseph R. Nevins", "Mike West"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Flexible covariance estimation in graphical models", "author": ["B. Rajaratnam", "H. Massam", "C. Carvalho"], "venue": "Annals of Statistics, 36:2818\u20132849,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "A note on the lack of symmetry in the graphical lasso", "author": ["Benjamin T. Rolfs", "Bala Rajaratnam"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Sparse inverse covariance selection via alternating linearization methods", "author": ["Katya Scheinberg", "Shiqian Ma", "Donald Goldfarb"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["Paul Tseng"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Determinant maximization with linear matrix inequality constraints", "author": ["Lieven Vandenberghe", "Stephen Boyd", "Shao-Po Wu"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1996}, {"title": "Graphical Models in Applied Multivariate Statistics", "author": ["J. Whittaker"], "venue": "Wiley,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1990}, {"title": "Condition number regularized covariance estimation", "author": ["J. Won", "J. Lim", "S. Kim", "B. Rajaratnam"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse reconstruction by separable approximation", "author": ["Stephen J. Wright", "Robert D. Nowak", "M\u00e1rio A.T. Figueiredo"], "venue": "IEE Transactions on Signal Processing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Model selection and estimation in the gaussian graphical model", "author": ["Ming Yuan", "Yi Lin"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Alternating direction method of multipliers for covariance selection models", "author": ["X.M. Yuan"], "venue": "Journal of Scientific Computing, pages 1\u201313,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 32, "context": "A related problem is the inference of a Gaussian graphical model ([33, 18]), that is, a sparsity pattern in the inverse covariance matrix, \u03a9.", "startOffset": 66, "endOffset": 74}, {"referenceID": 17, "context": "A related problem is the inference of a Gaussian graphical model ([33, 18]), that is, a sparsity pattern in the inverse covariance matrix, \u03a9.", "startOffset": 66, "endOffset": 74}, {"referenceID": 32, "context": "Specifically, if X = (Xi) p i=1 \u2208 R is distributed as X \u223c Np(0,\u03a3), then (\u03a3)ij = \u03a9ij = 0 \u21d0\u21d2 Xi \u22a5\u22a5 Xj|{Xk}k 6=i,j, where the notation A \u22a5\u22a5 B|C denotes the conditional independence of A and B given the set of variables C (see [33, 18]).", "startOffset": 223, "endOffset": 231}, {"referenceID": 17, "context": "Specifically, if X = (Xi) p i=1 \u2208 R is distributed as X \u223c Np(0,\u03a3), then (\u03a3)ij = \u03a9ij = 0 \u21d0\u21d2 Xi \u22a5\u22a5 Xj|{Xk}k 6=i,j, where the notation A \u22a5\u22a5 B|C denotes the conditional independence of A and B given the set of variables C (see [33, 18]).", "startOffset": 223, "endOffset": 231}, {"referenceID": 0, "context": "[1] proposed performing such sparse inverse covariance estimation by solving the `1-penalized maximum likelihood estimation problem,", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Moreover, the `1 penalty induces sparsity in \u0398\u03c1, as it is the closest convex relaxation of the 0 \u2212 1 penalty, \u2016\u0398\u20160 = \u2211 i,j I(\u0398ij 6= 0), where I(\u00b7) is the indicator function [5].", "startOffset": 173, "endOffset": 176}, {"referenceID": 9, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 12, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 13, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 16, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 18, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 27, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 11, "context": "While several excellent general convex solvers exist (for example, [12] and [4]), these are not always adept at handling high dimensional problems (i.", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "While several excellent general convex solvers exist (for example, [12] and [4]), these are not always adept at handling high dimensional problems (i.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Dual methods [1] yield a covariance matrix by solving the constrained problem,", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "[1] consider a block coordinate descent algorithm to solve the block dual problem, which reduces each optimization step to solving a box-constrained quadratic program.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] iteratively solve the lasso regression as described in [1], but do so using coordinate-wise descent.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[11] iteratively solve the lasso regression as described in [1], but do so using coordinate-wise descent.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "[9] use Nesterov\u2019s smooth approximation scheme, which produces an \u03b5-optimal solution in O(1/\u03b5) iterations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "A variant of Nesterov\u2019s smooth method is shown to have a O(1/ \u221a \u03b5) iteration complexity in [20, 21].", "startOffset": 91, "endOffset": 99}, {"referenceID": 20, "context": "A variant of Nesterov\u2019s smooth method is shown to have a O(1/ \u221a \u03b5) iteration complexity in [20, 21].", "startOffset": 91, "endOffset": 99}, {"referenceID": 35, "context": "Yuan and Lin [36] use interior point methods based on the max-det problem studied in [32].", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "Yuan and Lin [36] use interior point methods based on the max-det problem studied in [32].", "startOffset": 85, "endOffset": 89}, {"referenceID": 36, "context": "Yuan [37] use an alternating-direction method, while Scheinberg et al.", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "[30] proposes a similar method and show a sublinear convergence rate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Mazumder and Hastie [23] consider blockcoordinate descent approaches for the primal problem, similar to the dual approach taken in [11].", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "Mazumder and Hastie [23] consider blockcoordinate descent approaches for the primal problem, similar to the dual approach taken in [11].", "startOffset": 131, "endOffset": 135}, {"referenceID": 21, "context": "Mazumder and Agarwal [22] also solve the primal problem with block-coordinate descent, but at each iteration perform a partial as opposed to complete block optimization, resulting in a decreased computational complexity per iteration.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "[16] propose a second-order proximal point algorithm, called QUIC, which converges superlinearly locally around the optimum.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 7, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 23, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 24, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 25, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 30, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 7, "context": "It is well known (for example, [8]) that x\u2217 \u2208 X is an optimal solution of problem (4) if and only if", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "See [3] for more details.", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "Lemma 1 ([1, 20]).", "startOffset": 9, "endOffset": 16}, {"referenceID": 19, "context": "Lemma 1 ([1, 20]).", "startOffset": 9, "endOffset": 16}, {"referenceID": 7, "context": "where for some x \u2208 R, (x)+ := max(x, 0) (see [8]).", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "As in [3], the algorithm uses a backtracking line search for the choice of step size.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "Our implementation of Algorithm 1 chooses the Barzilai-Borwein step [2].", "startOffset": 68, "endOffset": 71}, {"referenceID": 34, "context": "is also used in the SpaRSA algorithm [35], and approximates the Hessian around \u0398t+1.", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "In particular, the useful, recent QUIC method [16] warrants a discussion.", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "3 compares running times of the G-ISTA, glasso [11], and QUIC [16] algorithms.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "3 compares running times of the G-ISTA, glasso [11], and QUIC [16] algorithms.", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "Synthetic data for this section was generated following the method used by [21, 22].", "startOffset": 75, "endOffset": 83}, {"referenceID": 21, "context": "Synthetic data for this section was generated following the method used by [21, 22].", "startOffset": 75, "endOffset": 83}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "For many practical applications however, obtaining an estimator that is well-conditioned is important ([29, 34]).", "startOffset": 103, "endOffset": 111}, {"referenceID": 33, "context": "For many practical applications however, obtaining an estimator that is well-conditioned is important ([29, 34]).", "startOffset": 103, "endOffset": 111}], "year": 2012, "abstractText": "The `1-regularized maximum likelihood estimation problem has recently become a topic of great interest within the machine learning, statistics, and optimization communities as a method for producing sparse inverse covariance estimators. In this paper, a proximal gradient method (G-ISTA) for performing `1-regularized covariance matrix estimation is presented. Although numerous algorithms have been proposed for solving this problem, this simple proximal gradient method is found to have attractive theoretical and numerical properties. G-ISTA has a linear rate of convergence, resulting in an O(log \u03b5) iteration complexity to reach a tolerance of \u03b5. This paper gives eigenvalue bounds for the G-ISTA iterates, providing a closed-form linear convergence rate. The rate is shown to be closely related to the condition number of the optimal point. Numerical convergence results and timing comparisons for the proposed method are presented. G-ISTA is shown to perform very well, especially when the optimal point is well-conditioned.", "creator": "LaTeX with hyperref package"}}}