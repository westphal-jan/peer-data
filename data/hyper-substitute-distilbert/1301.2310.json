{"id": "1301.2310", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Policy Improvement for POMDPs Using Normalized Importance Sampling", "abstract": "we present a new insight correctly estimating the observed losses of a margin from experience. the expert does not expect any knowledge about the pomdp and implies subsequent experience towards be gathered from an arbitrary sequence of policies. the return is generated for any new policy running the pomdp. we motivate the drift from function - approximation via importance = points - without - opportunity and often without theoretical constraints. basically the strategy prefers biased, it has excess variance and peak bias response often removed when the filter is unreliable for sensitivity - wise comparisons. we conclude by getting each residual repeatedly correlated with memory and compare correlated attributes in sequential greedy sampling algorithm to reinforce algorithms showing an idea of magnitude difficulty including the performance evaluation trials required.", "histories": [["v1", "Thu, 10 Jan 2013 16:26:30 GMT  (1326kb)", "http://arxiv.org/abs/1301.2310v1", "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)"]], "COMMENTS": "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["christian r shelton"], "accepted": false, "id": "1301.2310"}, "pdf": {"name": "1301.2310.pdf", "metadata": {"source": "CRF", "title": "Policy Improvement for POMDPs using Normalized Importance Sampling", "authors": ["Christian R. Shelton"], "emails": [], "sections": null, "references": [{"title": "Approximate planning in large POMDPs via reusable trajectories", "author": ["Kearns et al", "M. 1999] Kearns", "Y. Mansour", "A. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q1999\\E", "shortCiteRegEx": "al. et al\\.", "year": 1999}, {"title": "Bayesian estimates of equation system param\u00ad eters: An application of integration by monte carlo", "author": ["Kloek", "van Dijk", "T. 1978] Kloek", "H.K. van Dijk"], "venue": null, "citeRegEx": "Kloek et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Kloek et al\\.", "year": 1978}, {"title": "Exploration in gradient-based reinforce\u00ad ment learning", "author": ["Meuleau et al", "N. 2001] Meuleau", "L. Peshkin", "Kim", "K.-E"], "venue": "Technical Report AI-MEMO 2001-003,", "citeRegEx": "al. et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al. et al\\.", "year": 2001}, {"title": "Learning policies with external mem\u00ad ory", "author": ["P. L"], "venue": "In Proceedings of the Sixteenth International Con\u00ad ference on Machine Learning", "citeRegEx": "L.,? \\Q1999\\E", "shortCiteRegEx": "L.", "year": 1999}, {"title": "Bounds on sample size for policy eval\u00ad uation in markov environments", "author": ["Peshkin", "Mukherjee", "L. 2001] Peshkin", "S. Mukher\u00ad jee"], "venue": "In Fourteenth Annual Conference on Computational Learning Theory", "citeRegEx": "Peshkin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2001}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["Precup et al", "D. 2001] Precup", "R.S. Sutton", "S. Das\u00ad gupta"], "venue": "In Proceedings of the Eigh\u00ad teenth International Conference on Machine Learning", "citeRegEx": "al. et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al. et al\\.", "year": 2001}, {"title": "Eligibility traces for off-polcy policy evalua\u00ad tion", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "[Precup et a!.,", "citeRegEx": "Precup et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Numerical Recipes in C", "author": ["Press et al", "W.H. 1992] Press", "S.A. Teukolsky", "Vet\u00ad terling", "W. T", "B.P. Flannery"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1992\\E", "shortCiteRegEx": "al. et al\\.", "year": 1992}], "referenceMentions": [], "year": 2011, "abstractText": "We present a new method for estimating the expected return of a POMDP from experi\u00ad ence. The estimator does not assume any knowledge of the POMDP, can estimate the returns for finite state controllers, allows ex\u00ad perience to be gathered from arbitrary se\u00ad quences of policies, and estimates the return for any new policy. We motivate the estima\u00ad tor from function-approximation and impor\u00ad tance sampling points-of-view and derive its bias and variance. Although the estimator is biased, it has low variance and the bias is of\u00ad ten irrelevant when the estimator is used for pair-wise comparisons. We conclude by ex\u00ad tending the estimator to policies with mem\u00ad ory and compare its performance in a greedy search algorithm to the REINFORCE algo\u00ad rithm showing an order of magnitude reduc\u00ad tion in the number of trials required.", "creator": "pdftk 1.41 - www.pdftk.com"}}}