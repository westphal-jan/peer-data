{"id": "1301.3834", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Perfect Tree-Like Markovian Distributions", "abstract": "\\ remember that if a purely positive joint cumulative distribution for a group of binary standard variables factors adds to transformation tree, then vertex separation represents all components will the independence relations vary in parallel distribution. the inclusion hypothesis is shown to hold also for multivariate strictly positive normal distributions. such proof offers a new approach of conditional independence theorem holds before assuming two classes of probability regions.", "histories": [["v1", "Wed, 16 Jan 2013 15:48:48 GMT  (178kb)", "http://arxiv.org/abs/1301.3834v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ann becker", "dan geiger", "christopher meek"], "accepted": false, "id": "1301.3834"}, "pdf": {"name": "1301.3834.pdf", "metadata": {"source": "CRF", "title": "Perfect Tree-like Markovian Distributions", "authors": ["Ann Becker", "Dan Geiger", "Christopher Meek"], "emails": ["ta@cs.", "dang@cs.technion.ac.il", "meek@microsoft"], "sections": [{"heading": null, "text": "1 Introduction\nA useful approach to multivariate statistical model ing is to first define the conditional independence con straints that are likely to hold in a domain, and then to restrict the analysis to probability distributions that satisfy these constraints. An increasingly pop ular way of specifying independence constraints are directed and undirected graphical models where inde pendence constraints are encoded through the topolog ical properties of the corresponding graphs (Lauritzen 1982; Lauritzen and Spiegelhalter, 1988; Pearl, 1988; Whittaker, 1990).\nThe key idea behind these specification schemes is to utilize the correspondence between vertex separation in graphs and conditional independence in probability; each vertex represents a variable and if a set of vertices Z blocks all the paths between two vertices, then the corresponding two variables are asserted to be condi tionally independent given the variables corresponding to Z. The success of graphical models stems in part from the fact that vertex separation and conditional in dependence share key properties which render graphs an effective language for specifying independence con straints.\nIn this paper we show that when graphical models are trees and distributions are from specific classes, then the relationship between vertex separation and conditional independence is much more pronounced. More specifically, we show that if a strictly positive\n*Part of this work was done while the author was on sabbatical at Microsoft research.\njoint probability distribution for a set of binary ran dom variables factors according to a tree, then vertex separation represents all and only the independence relations encoded in the distribution. The same result is shown to hold also for multivariate strictly positive normal distributions.\nThe class of Markov trees has been studied in several contexts. Practical algorithms for learning Markov trees from data have been used for pattern recogni tion (Chow and Liu, 1968). Geometrical properties of families of tree-like distributions have been studied in (Settimi and Smith, 1999). F inally, the property of perfectness, when a graphical model represents all and only the conditional independence facts encoded in a distribution, is a key assumption in learning causal relationships from observational data (Glymour and Cooper, 1999).\n2 Preliminaries\nThroughout this article we use lowercase letters for sin gle random variables (e.g., x, y, z) and boldfaced low ercase letters (e.g., x, y, z) for specific values for these random variables. Set of random variables are denoted by capital letters (e.g., X, Y, Z), and their values are denoted by boldfaced capital letters (e.g., X, Y, Z). For example, if Z = { x, y} then Z stands for { x, y} where x is a value of x and y is a value of y. We use P(X) as a short hand notation for P(X =X). We say that P(X) is strictly positive if VX P(X) > 0. We use X y as a short hand notation for X U {y}.\nLet X, Y and Z be three disjoint sets of ran dom variables having a joint probability distribution P(X, Y, Z). Then, X and Y are conditionally inde pendent given Z, denoted by X l_p Y I Z, if and only if\nVXVYVZ P(X, Y, Z)P(Z) = P(X, Z)P(Y, Z).\nWhen P is strictly positive an equivalent definition is that X l_p Y I Z holds if and only if\nVXVYVZ P(X IZ) = P(XIY, Z).\nWhen P(X, Y, Z) is a strictly positive joint normal distribution, then X and Y are conditionally indepen dent given Z if and only if Pxy.Z = 0 for every x EX\n20 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nand y E Y where Pxy.Z is the partial correlation coef ficient of x and y given Z (Cramer, 1946).\nThe ternary relation X ..lp Y I Z was introduced in (Dawid, 1979) and further studied in (e.g., Spohn 1980; Pearl and Paz 1987; Pearl 1988; Geiger and Pearl 1993; Studeny 1992) . The ternary relation X ..lp Y I Z satisfies the following five properties which are called the graphoid axioms (Pearl and Paz, 1987).\n\u2022 Symmetry:\nX ..lp y I z =} y ..lp X I z (1)\n\u2022 Decomposition: X ..lp YW I z =}X ..lp y I z (2)\n\u2022 Weak Union:\nX ..lp YW I z =} X ..lp y I zw (3)\n\u2022 Contraction: X ..lp y I z 1\\ X ..lp w I ZY =}\nX..lp YWIZ (4) If P is strictly positive, then\n\u2022 Intersection:\nX ..lp y I zw 1\\ X ..lp w I ZY =} X ..lp YW I z (5)\nThe following property holds for joint normal distri butions P(X, Y, Z, c) (Pearl, 1988). It also holds for discrete random variables if Z = 0 and c is a binary random variable.\n\u2022 Weak Transitivity:\nX ..lp Y I Z 1\\ X ..lp Y I Zc => X ..lp c I z v c ..lp y I z (6)\nA Markov network of a probability distribution P(x1, . . . , xn) is an undirected graph G\n= (V, E) where V = { x1, . . . , xn} is a set of vertices, one for each ran dom variable x;, and E is a set of edges each repre sented as (x;,xj) such that (x;,xj) E E if and only if \u2022X; ..lp Xj I {x1, ... , Xn} \\ {x;, Xj}\u00b7 A Markov tree is a Markov network where G is a tree. A key property of Markov networks is the following. Let A ..lc B I C stand for the assertion that every path in G between a vertex in A and a vertex in B passes through a vertex in C, where A, B, and C are mu tually disjoint sets of vertices. Note that whenever A ..lc B I C holds in G, A and B are (vertex) sepa rated by C. The ternary relation A ..lc B I C satisfies all the properties we listed for A ..lp B I C and some additional properties that do not hold for A ..lp B I C (Pearl, 1988).\nTheorem 1 (Pearl and Paz, 1987; Pearl, 88) Let G be a Markov network of P(x1, ... , Xn ) , and sup pose Intersection holds for P. Then\nA ..la B I C implies A ..lp B I C (7) for every disjoint set of vertices A, B, and C of G and their corresponding random variables in {x1, . . . , Xn}\u00b7\nThe main result in this paper is a converse to Eq. 7 under suitable conditions. When the converse holds we say that G is a perfect representation of P. To facilitate our argument we must first introduce a new property for conditional independence.\n\u2022 Decomposable transitivity: aB ..lp De I c 1\\ a ..lp e I B D =>\na ..lp c I B V c ..lp e I D (8)\n3 New property of conditional independence\nWe now prove that decomposable transitivity holds for strictly positive joint probability distributions of bi nary random variables and for strictly positive normal distributions. We then show that decomposable tran sitivity holds also for vertex separation in undirected graphs.\nTheorem 2 Let a, c, e be binary random variables, B and D be {possibly empty) sets of binary random vari ables, and P(a, c, e, B, D) be a strictly-positive joint probability distribution for these random variables. Then\naB ..lp De I c 1\\ a ..lp e I BD => a ..lp c I B V c ..lp e I D\nholds for P.\nProof: We use a to denote a value for a, B to denote a value for a set of variables B, and a0 and a1 to denote the two values of a binary random variable a.\nDue to aB ..lp De I c it follows that P(a, B, c, D, e)\u00b7 P(c) = P(a, B, c)\u00b7 P(c, D, e) (9)\nfor every value a, c, e, B, D of the corresponding ran dom variables. Due to a ..lp e I BD it follows that\nP(a0,B,D,e0) \u00b7P(a1,B,D,e1) = P(al, B, D, e0) \u00b7 P(a0, B, D, e1). (10)\nfor every value B, D of B, D. Since c is a binary vari able\nP(a, B, D, e)= P(a, B, c0, D, e)+ P(a, B, cl, D, e) (11)\nNow, substituting Eq. 9 into Eq. 11, then substitut ing the result into Eq. 10, yields using some divisions, which are allowed because P is strictly positive, that\n1 + a(B),B(D) = a(B) + ,B(D) where\nand\n,B(D) = P(c1,D,e0) \u00b7 P(c0,D,e1) P(c0,D,e0) \u00b7P(c1,D,el)\nConsequently, either a(B) = 1 or ,B(D) = 1. Further more, since B and D are arbitrary values of B and D, respectively, we have \\fB\\fD [a(B) = 1 V ,B(D) = 1] which is equivalent to [\\fB a(B) = 1) V ['v'D ,B(D) = 1] which is equivalent to a ..lp c I B V c ..lp e I D. 0\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 21\nTheorem 3 Let a, c, and e be continuous random variables, B and D be {possibly empty) sets of con tinuous random variables, and let P(a, c, e, B, D) be a strictly positive joint normal probability distribution for these random variables. Then,\naB l_p De I c 1\\ a l_p e I BD => a l_p c I B V c l_p e I D (12)\nholds for P.\nProof: We use a formal logical deduction style to em phasize that the only properties of normal distribu tions being used are the ones encoded in Symmetry, Decomposition, Intersection, Weak union, and Weak transitivity. Recall that weak transitivity holds for every normal distribution and that intersection holds for strictly positive normal distributions. The other properties hold for every probability distribution.\nWe now derive the conclusion of Eq. 12 from its an tecedents.\n1. aB l_p De I c (Given)\n2. a l_p e I BD (Given)\n3. a l_p D I cB (W. union, Decomposition, and Symmetry on (1))\n4. B l_p e I cD (W. union, Decomposition, and Symmetry on ( 1))\n5. a l_p e I BDc (Weak union and Symmetry on (1))\n6. a l_p c I BD V c l_p e I BD (Weak transitivity on (2) and (5))\n7. a l_p cD I B V Be l_p e I D (Intersection and Symmetry on (3), (4) and (6))\n8. a l_p c I B V c l_p e I D (Symmetry and Decomposition on (7))\n0\nTheorem 4 Let a, c, and e be distinct vertices of an undirected graph G, and let B and D be two (possibly empty) disjoint sets of vertices of G that do not include a, c or d. Then,\naB l_a De I c 1\\ a l_a e I BD =>\nholds for G.\na l_a c I B V c l_a e I D (13)\nProof: Assume the conclusion of Eq. 13 does not hold in G but its antecedents hold. Then, there exists a path 11 in G between a and c such that no vertices from B reside on 11, and there exists a path /2 in G between c and e such that no vertices from D reside on 11. If B and D are empty, then the concatenated path /1/2 contradicts a l_a e I BD which is assumed to hold in G. Thus, we can assume either B or D are not empty. The concatenated path /1/2 contains a vertex from B or D (or both) because a l_a e I B D is assumed to hold in G. Assume a vertex d E D resides\non the path 11 between a and c, or that a vertex b E B resides on the path 12 between c and e. In the first case vertices a and d are connected and the path that connects them does not include c, and in the second case vertex b and e are connected and the path that connects them does not include c. Thus, in both cases, aB l_a De I c does not hold in G, contradicting our \ufffdumpt\ufffdn. 0\n4 Perfect Markovian trees\nWe are ready to prove the main result.\nTheorem 5 Let G be a Markov tree for a probabil ity distribution P(x1, ... ,xn). If x1, ... ,xn are bi nary random variables and P is a strictly-positive joint probability distribution, or if x1, ... , Xn are continuous random variables and P is a strictly positive joint nor mal distribution then, in both cases,\nA l_a B I C if and only if A l_p B I C (14)\nfor every disjoint set of vertices A, B, and C of G and their corresponding random variables in { x1, ... , xn}.\nProof: Theorem 1 proves one direction of Eq. 14, and so it remains to prove that\nA l_p B I C implies A l_a B I C (15) To prove Eq. 15 it is sufficient to show that\na l_p b I C implies a l_a b I C\nfor every pair of vertices a E A and b E B because A l_p B I C implies VaVb a l_p b I C and VaVb a l_a b I C is equivalent by definition to A l_a B I C.\nWe proceed by contradiction. Let x and y be a pair of vertices for which there exists a set of vertices Z satisfying x l_p y I Z 1\\ \u2022x l_a y I Z (16) and such that x and y are connected with the shortest path among all pairs x', y' for which there exists a set Z' satisfying x' l_p y' I Z' 1\\ \u2022x' l_a y' I Z'.\nSuppose first that the path between x and y is merely an edge connecting the two vertices. We will now reach a contradiction by showing that G cannot be a Markov network of P. In particular, we show that P satisfies x l_p y I Uxy where Uxy are all vertices except x and y. Let Ux be all the vertices on the x side of the edge (x, y) and Uy be the rest of the vertices. (Namely, Ux are the vertices in the component of x after removing the edge (x, y)). Let B = Ux n Z and D = Uy n Z. We proceed by a formal deduction using properties of conditional independence.\n1. B l_a Dy I x (By definition of B and D in G)\n2. B l_p Dy I X (From ( 1) and since G is a Markov network of P)\n3. B l_p y I xD (Weak union on (2))\n4. X l_p y I BD (Z = BD and x l_p y I Z is assumed)\n22 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\n5. xB l_p y I D (Intersection and Symmetry on (3) and (4))\n6. X l_p y I D (Decomposition on (5))\n7. X l_G D I y (By definition of D in G)\n8. X l_p D I y (From (7) and since G is a Markov network of P)\n9. X l_p yD 1 0 (Intersection on (6) and (8))\n10. X l_p Y 1 0 (Decomposition on (9))\n11. X l_G Uy I y (Definition of Uy)\n12. X l_p Uy I y (From (11) and since G is a Markov network of P)\n13. X l_p yUy 10 (Contraction on (10) and (12))\n14. Ux l_c yUy I x (Definition of Ux and Uy)\n15. Ux l_p yUy I X (From (14) and since G is a Markov network of P)\n16. xUx l_p yUy 10 (Contraction and Symmetry on (13) and (15))\n17. x l_p yiUxUy (Weak union and Symmetry on (16))\nNow suppose the path between x andy has more than one edge and that c is a vertex on this path. We reach a contradiction by showing that the pair x, y is not the closest pair of vertices that satisfy Eq. 16 for some set Z', contrary to our selection of these vertices. Let B, D be a partition of Z such that B are the vertices in Z on the x side of c and D = Z \\ B. The rest of the derivation is a formal deduction using properties of conditional independence.\n1. x B l_c Dy I c (By definition of B and D in G)\n2. xB l_p Dy I c (From (1) and since G is a Markov network of P)\n\ufffd3. x l_p yiBD (Z = BD and x l_p y I Z is assumed)\n4. X l_p c I B v c l_p y I D (Decomposable transitivity on (2) and (3))\n5. -,x l_c c I B 1\\ -,c l_p y I D (By definition of B and D in G)\n6. [x l_p c I B 1\\ -,x l_c c I B] [c l_p y I D 1\\ -,c l_c y I D] ((4) and (5))\nv\nEach disjunct in Step (6) exhibits a pair of vertices that are closer to each other than x and y and yet satisfy Eq. 16 for some set Z'. Note that Step ( 4) uses De composable transitivity which holds if Xt, . . . , Xn are binary random variables and P is a strictly-positive joint probability distribution, or if Xt, . . . , Xn are con tinuous random variables and P is a strictly positive joint normal distribution, as assumed. D\n5 Discussion\nOur proof uses a new property of conditional indepen dence that holds for the two classes of probability dis tributions we have focused on. The approach of using logical properties of conditional independence as a way of reasoning follows the approach taken by (Pearl and Paz, 1987) who analyzed the logical properties shared by vertex separation and conditional independence.\nThe algorithmic consequence of Theorem 5 is that in order to check whether a Markov tree of P represents all the conditional independence statements that hold in P, assuming P satisfies Intersection and Decom posable transitivity, requires one to check whether for each edge (x, y) in G, x l_p y 1 0 holds in P. Note that this test is more reliable and simpler than checking for each edge (x, y) in G, whether x l_p y I Uxy holds, as the definition of a Markov tree requires. An open question remains as to what is the minimal computa tion needed to ensure that a general Markov network represents all the conditional independence statements that hold in P and what properties P needs to satisfy to accommodate these computations.\nA straightforward attempt to extend our results with out changing the tests or the assumptions on P is quite limited because we have counter examples to Theo rem 5 when G is a polytree (a directed graph with no underlying undirected cycles) and when P does not satisfy Intersection or Decomposable transitivity. These counter examples, together with the proof of Theorem 5, show that if G is a Markov tree of a prob ability distribution P, then G is a perfect represen tation of P if and only if P satisfies Intersection and Decomposable transitivity.\nReferences\nChow, C., & Liu, C. (1968). Approximating discrete probability distributions with dependence trees. IEEE Transaction on information theory, 14, 462-467.\nCramer, H. (1946). Mathematical methods of statistics. Princeton university press.\nDawid, A. P. (1979). Conditional independence in sta tistical theory (with discussion). Journal of the royal statistical society, Series B, 41, 1-31.\nGeiger, D., & Pearl, J. (1993). Logical and algorith mic properties of conditional independence and graphical models. Annals of Statistics, 21, 2001- 21.\nGlymour, C., & Cooper, G. (1999). Computation, Causation, Discovery. AAAI press/The MIT press, Menlo Park.\nLauritzen, S. (1989). Lectures on contingency tables {3rd edition). Aalborg University.\nLauritzen, S., & Spiegelhalter, D. (1988). Local com putations with probabilities on graphical struc tures and their application to expert systems (with discussion). Journal Royal Statistical So ciety, B, 50, 157-224.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nPearl, J. (1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference. Mor gan Kaufmann, San Mateo, California.\nPearl, J., & Paz, A. (1987). Graphoids: a graph based logic for reasoning about relevancy relations. In Boulay, B., Hogg, D., & Steel, L. (Eds.) , Ad vances in artificial intelligence-//, pp. 357-363. North-Holland, Amsterdam.\nSettimi, R., & Smith, J. (1999). Geometry, moments and bayesian networks with hidden variables. In Proceedings of the seventh international work shop on Artificial Intelligence and Statitics, pp. 293-298 San Francisco. Morgan Kaufmann.\nSpohn, W. (1980). Stochastic independence, causal independence, and shieldability. Journal philo sophical logic, 9, 73-99.\nStudeny, M. (1992). Conditional independence have no finite complete characterization. In Trans actions of the 11th Prague conference on infor mation theory, statistical decision functions and random processes, pp. 3-16 Prague. Academia.\nWhittaker, J. (1990). Graphical models in applied mul tivariate statistics. John Wiley and Sons, Chich ester.\n23"}], "references": [{"title": "Mathematical methods of statistics", "author": ["H. Cramer"], "venue": null, "citeRegEx": "Cramer,? \\Q1946\\E", "shortCiteRegEx": "Cramer", "year": 1946}, {"title": "Conditional independence in sta\u00ad tistical theory (with discussion)", "author": ["A.P. Dawid"], "venue": "Journal of the royal statistical society, Series B,", "citeRegEx": "Dawid,? \\Q1979\\E", "shortCiteRegEx": "Dawid", "year": 1979}, {"title": "Logical and algorith\u00ad", "author": ["D. Geiger", "J. Pearl"], "venue": null, "citeRegEx": "Geiger and Pearl,? \\Q1993\\E", "shortCiteRegEx": "Geiger and Pearl", "year": 1993}, {"title": "Lectures on contingency tables {3rd edition)", "author": ["S. Lauritzen"], "venue": "Aalborg University", "citeRegEx": "Lauritzen,? \\Q1989\\E", "shortCiteRegEx": "Lauritzen", "year": 1989}, {"title": "Probabilistic reasoning in intelligent systems: Networks of plausible inference. Mor\u00ad", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Graphoids: a graph based", "author": ["J. Pearl", "A. Paz"], "venue": null, "citeRegEx": "Pearl and Paz,? \\Q1987\\E", "shortCiteRegEx": "Pearl and Paz", "year": 1987}, {"title": "Stochastic independence, causal independence, and shieldability", "author": ["W. Spohn"], "venue": "Journal philo\u00ad sophical logic,", "citeRegEx": "Spohn,? \\Q1980\\E", "shortCiteRegEx": "Spohn", "year": 1980}, {"title": "Conditional independence have no finite complete characterization", "author": ["M. Studeny"], "venue": null, "citeRegEx": "Studeny,? \\Q1992\\E", "shortCiteRegEx": "Studeny", "year": 1992}, {"title": "Graphical models in applied mul\u00ad tivariate statistics", "author": ["J. Whittaker"], "venue": null, "citeRegEx": "Whittaker,? \\Q1990\\E", "shortCiteRegEx": "Whittaker", "year": 1990}], "referenceMentions": [{"referenceID": 4, "context": "An increasingly pop\u00ad ular way of specifying independence constraints are directed and undirected graphical models where inde\u00ad pendence constraints are encoded through the topolog\u00ad ical properties of the corresponding graphs (Lauritzen 1982; Lauritzen and Spiegelhalter, 1988; Pearl, 1988; Whittaker, 1990).", "startOffset": 224, "endOffset": 305}, {"referenceID": 8, "context": "An increasingly pop\u00ad ular way of specifying independence constraints are directed and undirected graphical models where inde\u00ad pendence constraints are encoded through the topolog\u00ad ical properties of the corresponding graphs (Lauritzen 1982; Lauritzen and Spiegelhalter, 1988; Pearl, 1988; Whittaker, 1990).", "startOffset": 224, "endOffset": 305}, {"referenceID": 0, "context": "Z is the partial correlation coef\u00ad ficient of x and y given Z (Cramer, 1946).", "startOffset": 62, "endOffset": 76}, {"referenceID": 1, "context": "lp Y I Z was introduced in (Dawid, 1979) and further studied in (e.", "startOffset": 27, "endOffset": 40}, {"referenceID": 5, "context": "lp Y I Z was introduced in (Dawid, 1979) and further studied in (e.g., Spohn 1980; Pearl and Paz 1987; Pearl 1988; Geiger and Pearl 1993; Studeny 1992) .", "startOffset": 64, "endOffset": 151}, {"referenceID": 4, "context": "lp Y I Z was introduced in (Dawid, 1979) and further studied in (e.g., Spohn 1980; Pearl and Paz 1987; Pearl 1988; Geiger and Pearl 1993; Studeny 1992) .", "startOffset": 64, "endOffset": 151}, {"referenceID": 2, "context": "lp Y I Z was introduced in (Dawid, 1979) and further studied in (e.g., Spohn 1980; Pearl and Paz 1987; Pearl 1988; Geiger and Pearl 1993; Studeny 1992) .", "startOffset": 64, "endOffset": 151}, {"referenceID": 7, "context": "lp Y I Z was introduced in (Dawid, 1979) and further studied in (e.g., Spohn 1980; Pearl and Paz 1987; Pearl 1988; Geiger and Pearl 1993; Studeny 1992) .", "startOffset": 64, "endOffset": 151}, {"referenceID": 5, "context": "lp Y I Z satisfies the following five properties which are called the graphoid axioms (Pearl and Paz, 1987).", "startOffset": 86, "endOffset": 107}, {"referenceID": 4, "context": "The following property holds for joint normal distri\u00ad butions P(X, Y, Z, c) (Pearl, 1988).", "startOffset": 76, "endOffset": 89}, {"referenceID": 4, "context": "lp B I C (Pearl, 1988).", "startOffset": 9, "endOffset": 22}, {"referenceID": 5, "context": "The approach of using logical properties of conditional independence as a way of reasoning follows the approach taken by (Pearl and Paz, 1987) who analyzed the logical properties shared by vertex separation and conditional independence.", "startOffset": 121, "endOffset": 142}], "year": 2011, "abstractText": "We show that if a strictly positive joint prob\u00ad ability distribution for a set of binary random variables factors according to a tree, then ver\u00ad tex separation represents all and only the in\u00ad dependence relations encoded in the distribu\u00ad tion. The same result is shown to hold also for multivariate strictly positive normal dis\u00ad tributions. Our proof uses a new property of conditional independence that holds for these two classes of probability distributions.", "creator": "pdftk 1.41 - www.pdftk.com"}}}