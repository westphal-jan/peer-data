{"id": "1301.3836", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "The Complexity of Decentralized Control of Markov Decision Processes", "abstract": "planning across distributed memory is partial state information instead considered from another decision - theoretic case. reviewers describe generalizations of both the mdp | pomdp procedures that implements locally decentralized control. regarding even - small linear state agents, the finite - horizon, corresponding up both geographic dir models are complete for nondeterministic random time. these complexity results illustrate a profound difference showing exact and secure control of markov processes. marking contrast to geometric mdp and pomdp problems, the problems whereas basically provably approximate employ converge polynomial - time modeling and regression theorem where weak exponential time approaches solve in and worst case. others have thus recovered strong evidence - satisfying the intuition that localized planning problems had easily be reduced to centralized approaches and resolved just using established techniques.", "histories": [["v1", "Wed, 16 Jan 2013 15:48:55 GMT  (209kb)", "http://arxiv.org/abs/1301.3836v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniel s bernstein", "shlomo zilberstein", "neil immerman"], "accepted": false, "id": "1301.3836"}, "pdf": {"name": "1301.3836.pdf", "metadata": {"source": "CRF", "title": "The Complexity of Decentralized Control of Markov Decision Processes", "authors": ["Daniel S. Bernstein", "Shlomo Zilberstein", "Neil Immerman"], "emails": ["@cs.umass.edu"], "sections": [{"heading": null, "text": "Planning for distributed agents with partial state information is considered from a decision theoretic perspective. We describe generaliza tions of both the MDP and POMDP models that allow for decentralized control. For even a small number of agents, the finite-horizon prob lems corresponding to both of our models are complete for nondeterministic exponential time. These complexity results illustrate a fundamen tal difference between centralized and decentral ized control of Markov processes. In contrast to the MDP and POMDP problems, the problems we consider provably do not admit polynomial time algorithms and most likely require doubly exponential time to solve in the worst case. We have thus provided mathematical evidence corre sponding to the intuition that decentralized plan ning problems cannot easily be reduced to cen tralized problems and solved exactly using estab lished techniques.\n1 Introduction\nAmong researchers in artificial intelligence, there has been growing interest in problems with multiple distributed agents working to achieve a common goal (Grosz & Kraus, 1996; Lesser, 1998; desJardins et al., 1999; Durfee, 1999; Stone & Veloso, 1999). In many of these problems, intera gent communication is costly or impossible. For instance, consider two robots cooperating to push a box (Mataric, 1998). Communication between the robots may take time that could otherwise be spent performing physical actions. Thus, it may be suboptimal for the robots to communi cate frequently. A planner is faced with the difficult task of deciding what each robot should do in between com munications, when it only has access to its own sensory information. Other problems of planning for distributed agents with limited communication include maximizing the\nthroughput of a multiple access broadcast channel (Ooi & Womell, 1996) and coordinating multiple spacecraft on a mission together (Estlin et al., 1999). We are interested in the question of whether these planning problems are computationally harder to solve than problems that involve planning for a single agent or multiple agents with access to the exact same information.\nWe focus on centralized planning for distributed agents, with the Markov decision process (MDP) framework as the basis for our model of agents interacting with an envi ronment. A partially observable Markov decision process (POMDP) is a generalization of an MDP in which an agent must base its decisions on incomplete information about the state of the environment (White, 1993). We extend the POMDP model to allow for multiple distributed agents to each receive local observations and base their decisions on these observations. The state transitions and expected rewards depend on the actions of all of the agents. We call this a decentralized partially observable Markov de cision process (DEC-POMDP). An interesting special case of a DEC-POMDP satisfies the assumption that at any time step the state is uniquely determined from the current set of observations of the agents. This is denoted a decen tralized Markov decision process (DEC-MDP). The MDP, POMDP, and DEC-MDP can all be viewed as special cases of the DEC-POMDP. The relationships among the models are shown in Figure 1.\nThere has been some related work in AI. Boutilier (1999) studies multi-agent Markov decision processes (MMDPs), but in this model, the agents all have access to the same in formation. In the framework we describe, this assumption is not made. Peshkin et al. (2000) use essentially the DEC POMDP model (although they refer to it as a partially ob servable identical payoff stochastic game (POIPSG)) and discuss algorithms for obtaining approximate solutions to the corresponding optimization problem. The models that we study also exist in the control theory literature (Ooi et al., 1997; Aicardi et al., 1987). However, the compu tational complexity inherent in these models has not been studied. One closely related piece of work is that of Tsit-\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 33\nWe discuss the computational complexity of finding opti mal policies for the finite-horizon versions of these prob lems. It is known that solving an MDP is P-complete and that solving a POMDP is PSPACE-complete (Papadim itriou & Tsitsiklis, 1987). We show that solving a DEC POMDP with a constant number, m ;::: 2, of agents is com plete for the complexity class nondeterministic exponen tial time (NEXP). Furthermore, solving a DEC-MDP with a constant number, m ;::: 3, of agents is NEXP-complete. This has a few consequences. One is that these problems provably do not admit polynomial-time algorithms. This trait is not shared by the MDP problems nor the POMDP problems. Another consequence is that any algorithm for solving either problem will most likely take doubly expo nential time in the worst case. In contrast, the exact al gorithms for finite-horizon POMDPs take \"only\" exponen tial time in the worst case. Thus, our results shed light on the fundamental differences between centralized and de centralized control of Markov decision processes. We now have mathematical evidence corresponding to the intuition that decentralized planning problems are more difficult to solve than their centralized counterparts. These results can steer researchers away from trying to find easy reductions from the decentralized problems to centralized ones and to ward completely different approaches.\nA precise categorization of the two-agent DEC-MDP prob lem presents an interesting mathematical challenge. The extent of our present knowledge is that the problem is PSPACE-hard and is contained in NEXP.\n2 Centralized Models\nA Markov decision process (MDP) models an agent acting in a stochastic environment to maximize its long-term re ward. The type of MDP that we consider contains a finite set S of states, with s0 E S as the start state. For each state\ns E S, As is a finite set of actions available to the agent. P is the table of transition probabilities, where P(s'Js, a) is the probability of a transition to state s' given that the agent performed action a in states. R is the reward func tion, where R( s, a) is the expected reward received by the\nagent given that it chose action a in state s.\nThere are several different ways to define \"long-term re ward\" and thus several different measures of optimality. In this paper, we focus on finite-horizon optimality, for which the aim is to maximize the expected sum of rewards re ceived over T time steps. Formally, the agent should maximize\nwhere r(st, at) is the reward received at time step t. A policy <5 for a finite-horizon MDP is a mapping from each states and timet to an action <5(s, t). This is called a non stationary policy. The decision problem corresponding to a finite-horizon MDP is as follows: Given an MDP M, a positive integer T, and an integer K, is there a policy that yields total reward at least K?\nAn MDP can be generalized so that the agent does not nec essarily observe the exact state of the environment at each time step. This is called a partially observable Markov de cision process (POMDP). A POMDP has a state setS, a start state so E S, a table of transition probabilities P, and a reward function R, just as an MDP does. Additionally, it contains a finite set n of observations, and a table 0 of ob servation probabilities, where O(oJa, s') is the probability that o is observed, given that action a was taken and led to state s'. For each observation o E 11, Ao is a finite set of actions available to the agent. A policy <5 is now a mapping from histories of observations o1, ... , Ot to actions in Ao, . The decision problem for a POMDP is stated in exactly the same way as for an MDP.\n3 Decentralized Models\nA decentralized partially observable Markov decision pro cess (DEC-POMDP) is a generalization of a POMDP to allow for distributed control by m agents that may not be able to observe the exact state. A DEC-POMDP contains a finite set S of states, with so E S as the start state. The transition probabilities P ( s' Is, a 1, ... , am) and expected rewards R(s, a1, ... , am) depend on the ac tions of all agents. ni is a finite set of observations for agent i, and 0 is a table of observation probabilities, where O(o1, ... , omJa1, ... , am, s') is the probability that o1, ... , om are observed by agents 1, . . . , m respectively, given that the action tuple (a1, ... , am) was taken and led to state s'. Each agent i has a set of actions A\ufffd for each observation oi E Oi. Notice that this model reduces to a POMDP in the one-agent case.\nFor each a1, ... , am, s', let w(a1, \u2022 \u2022 . , am, s') denote the set of observation tuples that have a nonzero chance of occurring given that the action tuple (a1, ... , am) was taken and led to state s'. To form a decentralized Markov decision process (DEC-MDP), we add the requirement\n34 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nthat for each a1, .. . , am, s', and each (o1, . . . , om) E w(a1, ... , am, s' ) the state is uniquely determined by ( o1, ... , om). In the one-agent case, this model is essentially the same as an MDP.\nWe define a local policy, oi, to be a mapping from local histories of observations of, ... , o\ufffd to actions ai E A\ufffd,. A joint policy, o = (81, ... , om), is defined to be a tu ple of local policies. We wish to find a joint policy that maximizes the total expected return over the finite hori zon. The decision problem is stated as follows: Given a DEC-POMDP M, a positive integer T, and an integer K, is there a joint policy that yields total reward at least K? Let DEC-POMDP m and DEC-MDP m denote the deci sion problems for them-agent DEC-POMDP and them agent DEC-MDP, respectively.\n4 Complexity Results\nIt is necessary to consider only problems for which T < lSI. If we place no restrictions on T, then the upper bounds do not necessarily hold. Also, we assume that each of the elements of the tables for the transition prob abilities and expected rewards can be represented with a constant number of bits. With these restrictions, it was shown in (Papadimitriou & Tsitsiklis, 1987) that the de cision problem for an MDP is P-complete. In the same paper, the authors showed that the decision problem for a POMDP is PSPACE-complete and thus probably does not admit a polynomial-time algorithm. We prove that for all\nm 2: 2, DEC-POMDP m is NEXP-complete, and for all m 2: 3, DEC-MDP m is NEXP-complete, where NEXP = NTIME (2nc ) (Papadimitriou, 1994). Since P ::J. NEXP, we can be certain that there does not exist a polynomial-time algorithm for either problem. Moreover, there probably is not even an exponential-time algorithm that solves either problem.\nFor our reduction, we use a problem called TILING (Pa padimitriou, 1994), which is described as follows: We are given a set of square tile types T = {to, ... , tk }, to gether with two relations H, V \ufffd T x T (the horizontal and vertical compatibility relations, respectively). We are also given an integer n in binary. A tiling is a function f: {O, . . . , n - 1} x {O, . . . , n - 1 } -t T. A tiling f is consistent if and only if (a) f(O, 0) = to, and (b) for all i,j (f(i,j), f(i+1,j)) E H, and (f(i,j), f(i,j+1)) E V. The decision problem is to tell, given T, H, V, and n , whether a consistent tiling exists. It is known that TILING is NEXP-complete.\nTheorem 1 For all m > 2, DEC-POMDP m is NEXP complete.\nProof. First, we will show that the problem is in NEXP. We can guess a joint policy o and write it down in exponential\ntime. This is because a joint policy consists of m map pings from local histories to actions, and since T < lSI, all histories have length less than lSI. A DEC-POMDP together with a joint policy can be viewed as a POMDP to gether with a policy, where the observations in the POMDP correspond to the observation tuples in the DEC-POMDP. In exponential time, each of the exponentially many possi ble sequences of observations can be converted into belief states. The transition probabilities and expected rewards for the corresponding \"belief MDP\" can be computed in exponential time (Kaelbling et al., 1998). It is possible to use dynamic programming to determine whether the policy yields expected reward at least K in this belief MDP. This takes at most exponential time.\nNow we show that the problem is NEXP-hard. For sim plicity, we consider only the two-agent case. Clearly, the problem with more agents can be no easier. We are given an arbitrary instance of TILING. From it, we construct a DEC-POMDP such that the existence of a joint policy that yields a reward of at least zero is equivalent to the existence of a consistent tiling in the original problem. Furthermore, T < lSI in the DEC-POMDP that is constructed. Intu itively, a local policy in our DEC-POMDP corresponds to a mapping from tile positions to tile types, i.e., a tiling, and thus a joint policy corresponds to a pair of tilings. The pro cess works as follows: In the position choice phase, two tile positions are randomly \"chosen\" by the environment. Then, at the tile choice step, each agent sees a different position and must use its policy to determine a tile to be placed in that position. Based on information about where the two positions are in relation to each other, the environ ment checks whether the tile types placed in the two posi tions could be part of one consistent tiling. Only if the nec essary conditions hold do the agents obtain a nonnegative reward. It turns out that the agents can obtain a nonnega tive expected reward if and only if the conditions hold for all pairs of positions the environment can choose, i.e., there exists a consistent tiling.\nWe now present the construction in detail. During the posi tion choice phase, each agent only has one action available to it, and a reward of zero is obtained at each step. The states and the transition probability matrix comprise the nontrivial aspect of this phase. Recall that this phase intu itively represents the choosing of two tile positions. First, let the two tile positions be denoted ( i1, jt) and ( i2 , jz), where 0 ::; i1, i2, j1, jz ::; n - 1. There are 4log n steps in this phase, and each step is devoted to the choosing of one bit of one of the numbers. (We assume that n is a power of two. It is straightforward to modify the proof to deal with the more general case.) The order in which the bits are chosen is important, and it is as follows: The bits of i1 and i2 are chosen from least significant up to most sig nificant, alternating between the two numbers at each step. Then j1 and jz are chosen in the same way. As the bits of\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 35\nthe numbers are being determined, information about the relationships between the numbers is being recorded in the state. How we express all of this as a Markov process is explained below.\nEach state has six components, and each component rep resents a necessary piece of information about the two tile positions being chosen. We describe how each of the com ponents changes with time. A time step in our process can be viewed as having two parts, which we refer to as the stochastic part and the deterministic part. During the stochastic part, the environment \"flips a coin\" to choose either the number 0 or the number 1, each with equal prob ability. After this choice is made, the change in each com ponent of the state can be described by a deterministic finite automaton that takes as input a string of O's and 1 's (the en vironment's coin flips). The semantics of the components, along with their associated automata, are described below:\n1) Bit Chosen in the Last Step This component of the state says whether 0 or 1 was just chosen by the environment. The corresponding automaton consists of only two states.\n2) Number of Bits Chosen So Far This component simply counts up to 4log n, in order to determine when the position choice phase should end. Its automaton consists of 4log n + 1 states.\n3) Equal Tile Positions After the 4log n steps, this component tells us whether the two tile positions chosen are equal or not. For this automa ton, along with the following three, we need to have a no tion of an accept state. Consider the following regular ex pression:\n(00 + 11)*.\nNote that the DFA corresponding to the above expression, on an input of length 4log n, ends in an accept state if and only if (i1,i1) = (iz,jz).\n4) Upper Left Tile Position This component is used to check whether the first tile posi tion is the upper left comer of the grid. Its regular expres sion is as follows:\n(0(0 + 1))*.\nThe corresponding DFA, on an input of length 4log n, ends in an accept state if and only if ( i1, j1) = (0, 0).\n5) Horizontally Adjacent Tile Positions This component is used to check whether the first tile po sition is directly to the left of the second one. Its regular expression is as follows:\n(10)*(01)(11 + 00)* (11 + 00) ... (11 + 00) .\nlogn\nThe corresponding DFA, on an input of length 4log n, ends in an accept state if and only if ( i1 + 1, j1) = ( iz, jz) .\n6) Vertically Adjacent Tile Positions This component is used to check whether the first tile posi tion is directly above the second one. Its regular expression is as follows:\n(11 + 00) ... (11 + 00)(10)*(01)(11 + 00)*.\nlogn\nThe corresponding DFA, on an input of length 4log n, ends in an accept state if and only if(i1,j1 + 1) = (iz,jz).\nSo far we have described the six automata that determine how each of the six components of the state evolve based on input (0 or 1) from the environment. We can take the cross product of these six automata to get a new automaton that is only polynomially bigger and describes how the entire state evolves based on the sequence of O's and 1 's chosen by the environment. This automaton, along with the en vironment's \"coin flips,\" corresponds to a Markov process. The number of states of the process is polylogarithmic inn, and hence polynomial in the size of the TILING instance. The start state s0 is a tuple of the start states of the six au tomata. The table of transition probabilities for this process can be constructed in time poly logarithmic in n.\nWe have described the states, actions, state transitions, and rewards for the position choice phase, and we now describe the observation function. In this DEC-POMDP, the obser vations are uniquely determined from the state. For the states after which a bit of i1 or i1 has been chosen, agent one observes the first component of the state, while agent two observes a dummy observation. The reverse is true for the states after which a bit of i2 or jz has been chosen. Intu itively, agent one \"sees\" only (i1, jl), and agent two \"sees\" only ( iz,)z) .\nWhen the second component of the state reaches its limit, the tile positions have been chosen, and the last four com ponents of the state contain information about the tile po sitions and how they are related. Of course, the exact tile positions are not recorded in the state, as this would require exponentially many states. This marks the end of the posi tion choice phase. In the next step, which we call the tile choice step, each agent has k + 1 actions available to it, corresponding to each of the tile types, to, ... , tk. We de note agent one's choice t1 and agent two's choice t2. No matter which actions are chosen, the state transitions de terministically to some final state. The reward function for this step is the nontrivial part. After the actions are chosen, the following statements are checked for validity:\n1) If (h,jl) = (i2 , j2 ), then t1 = t2. 2)If(i1,jl) = (0,0), then t1 = t0. 3)If(il + 1,jl) = (iz,jz) , then (t1 , t2 ) E H. 4) If (i1,i1 + 1) = (iz,jz) , then (tl , t2 ) E V.\nIf all of these are true, then a reward of 0 is received. Oth erwise, a reward of -1 is received. This reward function can be computed from the TILING instance in polynomial\n36 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\ntime. To complete the construction, the horizon T is set to 4log n (exactly the number of steps it takes the process to reach the tile choice step, and fewer than the number of states lSI).\nNow we argue that the expected reward is zero if and only if there exists a consistent tiling. First, suppose a consis tent tiling exists. This tiling corresponds to a local policy for an agent. If each of the two agents follows this policy, then no matter which two positions are chosen by the en vironment, the agents choose tile types for those positions so that the conditions checked at the end evaluate to true. Thus, no matter what sequence of O's and 1 's the environ ment chooses, the agents receive a reward of zero. Hence, the expected reward for the agents is zero.\nFor the converse, suppose the expected reward is zero. Then the reward is zero no matter what sequence of O's and 1 's the environment chooses, i.e., no matter which two tile positions are chosen. This implies that the four condi tions mentioned above are satisfied for any two tile posi tions that are chosen. The first condition ensures that for all pairs of tile positions, if the positions are equal, then the tile types chosen are the same. This implies that the two agents' tilings are exactly the same. The last three condi tions ensure that this tiling is consistent. 0\nTheorem 2 For all m 2: 3, DEC-MDP m is NEXP complete.\nProof. (Sketch) Inclusion in NEXP follows from the fact that a DEC-MDP is a special case of a DEC-POMDP. For NEXP-hardness, we can reduce a DEC-POMDP with two agents to a DEC-MDP with three agents. We simply add a third agent to the DEC-POMDP and impose the following requirement: The state is uniquely determined by just the third agent's observation, but the third agent always has just one action and cannot affect the state transitions or rewards received. It is clear that the new problem qualifies as a DEC-MDP and is essentially the same as the original DEC POMDP. 0\nThe reduction described above can also be used to con struct a two-agent DEC-MDP from a POMDP and hence show that DEC-MDP2 is PSPACE-hard. However, this technique is not powerful enough to prove the NEXP hardness of the problem. In fact, the question of whether DEC-MDP2 is NEXP-hard remains open. Note that in the reduction in the proof of Theorem 1, the observa tion function is such that there are some parts of the state that are hidden from both agents. This needs to some how be avoided in order to reduce to a two-agent DEC MDP. A simpler task may actually be to derive a better up per bound for the problem. For example, it may be pos sible that DEC-MDP2 E co-NEXP, where co-NEXP = {LIL E NEXP} . Regardless of the outcome, the problem provides an interesting mathematical challenge.\n5 Discussion\nUsing the tools of worst-case complexity analysis, we analyzed two models of decision-theoretic planning for distributed agents. Specifically, we proved that the finite-horizon m-agent DEC-POMDP problem is NEXP complete for m 2: 2 and the finite-horizon m-agent DEC MDP problem is NEXP-complete form 2: 3.\nThe results have some theoretical implications. First, un like the MDP and POMDP problems, the problems we studied provably do not admit polynomial-time algorithms, since P # NEXP. Second, we have drawn a connection be tween work on Markov decision processes and the body of work in complexity theory that deals with the exponen tial jump in complexity due to decentralization (Peterson & Reif, 1979; Babai et al., 1991). Finally, the two-agent DEC-MDP case yields an interesting open problem. The solution of the problem may imply that the difference be tween planning for two agents and planning for more than two agents is a significant one in the case where the state is collectively observed by the agents.\nThere are also more direct implications for researchers try ing to solve problems of planning for distributed agents. Consider the growing body of work on algorithms for ob taining exact or approximate solutions for POMDPs (e.g., Jaakkola et al., 1995; Cassandra et al., 1997; Hansen, 1998). It would have been beneficial to discover that a DEC-POMDP or DEC-MDP is just a POMDP \"in dis guise,\" in the sense that it can easily be converted to a POMDP and solved using established techniques. We have provided evidence to the contrary, however. The complex ity results do not answer all of the questions surrounding how these problems should be attacked, but they do sug gest that the fundamentally different structure of the de centralized problems may require fundamentally different algorithmic ideas.\nFinally, consider the infinite-horizon versions of the afore mentioned problems. It has recently been shown that the infinite-horizon POMDP problem is undecidable (Madani et al., 1999) under several different optimality criteria. Since a POMDP is a special case of a DEC-POMDP, the corresponding DEC-POMDP problems are also undecid able. In addition, because it is possible to reduce a POMDP to a two-agent DEC-MDP, the DEC-MDP problems are also undecidable.\nAcknowledgments\nThe authors thank Micah Adler, Andy Barto, Dexter Kozen, Victor Lesser, Frank McSherry, Ted Perkins, and Ping Xuan for helpful discussions. This work was sup ported in part by the National Science Foundation under grants IRI-9624992, IRI-9634938, and CCR-9877078 and an NSF Graduate Fellowship to Daniel Bernstein.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 37\nReferences\nAicardi, M., Franco, D. & Minciardi, R. (1987). Decentral ized optimal control of Markov chains with a common past information set. IEEE Transactions on Automatic Control, AC-32(Il), 1028-1031.\nBabai, L., Fortnow, L. & Lund, C. (1991). Non deterministic exponential time has two-prover interactive protocols. Computational Complexity, I, 3-40.\nBoutilier, C. (1999). Multiagent systems: Challenges and opportunities for decision-theoretic planning. AI Magazine, 20(4), 35-43.\nCassandra, A., Littman, M. L. & Zhang, N. L. (1997). In cremental pruning: A simple, fast, exact method for par tially observable Markov decision processes. In Proceed ings of the T hirteenth Annual Conference on Uncertainty in Artificial Intelligence (pp. 54-61 ).\ndesJardins, M. E., Durfee, E. H., Ortiz, C. L. & Wolverton, M. J. (1999). A survey of research in distributed, continual planning. AI Magazine, 20(4), 13-22.\nDurfee, E. H. (1999). Distributed problem solving and planning. In Multiagent Systems (pp. 121-164). Cam bridge, MA: The MIT Press.\nEstlin, T., Gray, A., Mann, T., Rabideau, G., Castano, R., Chien, S. & Mjolsness, E. (1999). An integrated system for mulit-rover scientific exploration. In Proceedings of the Sixteenth National Conference on Artificial Intelligence (pp. 541-548).\nGrosz, B. & Kraus, S. (1996). Collaborative plans for com plex group action. Artificial Intelligence, 86(2), 269-357.\nHansen, E. (1998). Solving POMDPs by searching in pol icy space. In Proceedings of the Fourteenth Annual Con ference on Uncertainty in Artificial Intelligence (pp. 211- 219).\nJaakkola, T., Singh, S. P. & Jordan, M. I. (1995). Reinforce ment learning algorithm for partially observable Markov decision problems. In Proceedings of Advances in Neural Information Processing Systems 7 (pp. 345-352).\nKaelbling, L. P., Littman, M. L. & Cassandra, A. R. (1998). Planning and actiong in partially observable stochastic do mains. Artificial Intelligence, IOI( 1-2), 99-134.\nLesser, V. R. (1998). Reflections on the nature of multi agent coordination and its implications for an agent archi tecture. Autonomous Agents and Multi-Agent Systems, 1, 89-111.\nMadani, 0., Hanks, S. & Condon, A. (1999). On the un decidability of probabilistic planning and infinite-horizon partially observable Markov decision process problems. In\nProceedings of the Sixteenth National Conference on Arti\nficial Intelligence (pp. 541-548).\nMataric, M. J. (1998). Using communication to reduce lo cality in distributed multi-agent learning. Journal of Exper imental and Theoretical Artificial Intelligence, 1 0( 3 ), 357- 369.\nOoi, J. M., Verbout, S. M., Ludwig, J. T. & Wornell, G. W. (1997). A separation theorem for periodic sharing informa tion patterns in decentralized control. IEEE Transactions on Automatic Control, 42( 11 ), 1546-1550.\nOoi, J. M. & Wornell, G. W. (1996). Decentralized con trol of a multiple access broadcast channel: Performance bounds. In Proceedings of the 35th Conference on Deci sion and Control (pp. 293-298).\nPapadimitriou, C. H. (1994). Computational Complexity. Reading, MA: Addison-Wesley.\nPapadimitriou, C. H. & Tsitsiklis, J. N. (1987). The com plexity of Markov decision processes. Mathematics of Op erations Research, 12(3), 441-450.\nPeshkin, L., Kim, K.-E., Meuleau, N. & Kaelbling, L. P. (2000). Learning to cooperate via policy search. In Pro ceedings of the Sixteenth International Conference on Un certainty in Artificial Intelligence.\nPeterson, G. L. & Reif, J. R. (1979). Multiple-person al ternation. In 20th Annual Symposium on Foundations of Computer Science (pp. 348-363).\nStone, P. & Veloso, M. (1999). Task decomposition, dy namic role assignment, and low-bandwidth communication for real-time strategic teamwork. Artificial Intelligence, II0(2), 241-273.\nTsitsiklis, J. N. & Athans, M. (1985). On the complexity of decentralized decision making and detection problems. IEEE Transactions on Automatic Control, AC-30(5), 440- 446.\nWhite, D. J. (1993). Markov Decision Processes. West Sussex, England: John Wiley & Sons."}], "references": [{"title": "Decentral\u00ad ized optimal control of Markov chains with a common past information set", "author": ["M. Aicardi", "D. Franco", "R. Minciardi"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Aicardi et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Aicardi et al\\.", "year": 1987}, {"title": "Non\u00ad deterministic exponential time has two-prover interactive protocols", "author": ["L. Babai", "L. Fortnow", "C. Lund"], "venue": "Computational Complexity,", "citeRegEx": "Babai et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Babai et al\\.", "year": 1991}, {"title": "Multiagent systems: Challenges and opportunities for decision-theoretic planning", "author": ["C. Boutilier"], "venue": "AI Magazine, 20(4), 35-43.", "citeRegEx": "Boutilier,? 1999", "shortCiteRegEx": "Boutilier", "year": 1999}, {"title": "In\u00ad cremental pruning: A simple, fast, exact method for par\u00ad tially observable Markov decision processes", "author": ["A. Cassandra", "M.L. Littman", "N.L. Zhang"], "venue": "In Proceed\u00ad ings of the T hirteenth Annual Conference on Uncertainty in Artificial Intelligence (pp", "citeRegEx": "Cassandra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1997}, {"title": "A survey of research in distributed, continual planning", "author": ["M.E. desJardins", "E.H. Durfee", "C.L. Ortiz", "M.J. Wolverton"], "venue": "AI Magazine,", "citeRegEx": "desJardins et al\\.,? \\Q1999\\E", "shortCiteRegEx": "desJardins et al\\.", "year": 1999}, {"title": "Distributed problem solving and planning", "author": ["E.H. Durfee"], "venue": "Multiagent Systems (pp. 121-164). Cam\u00ad bridge, MA: The MIT Press.", "citeRegEx": "Durfee,? 1999", "shortCiteRegEx": "Durfee", "year": 1999}, {"title": "An integrated system for mulit-rover scientific exploration", "author": ["T. Estlin", "A. Gray", "T. Mann", "G. Rabideau", "R. Castano", "S. Chien", "E. Mjolsness"], "venue": "In Proceedings of the Sixteenth National Conference on Artificial Intelligence (pp. 541-548)", "citeRegEx": "Estlin et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Estlin et al\\.", "year": 1999}, {"title": "Collaborative plans for com\u00ad plex group action", "author": ["B. Grosz", "S. Kraus"], "venue": "Artificial Intelligence,", "citeRegEx": "Grosz and Kraus,? \\Q1996\\E", "shortCiteRegEx": "Grosz and Kraus", "year": 1996}, {"title": "Solving POMDPs by searching in pol\u00ad icy space", "author": ["E. Hansen"], "venue": "Proceedings of the Fourteenth Annual Con\u00ad ference on Uncertainty in Artificial Intelligence (pp. 211219).", "citeRegEx": "Hansen,? 1998", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Reinforce\u00ad ment learning algorithm for partially observable Markov decision problems", "author": ["T. Jaakkola", "S.P. Singh", "M.I. Jordan"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Jaakkola et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1995}, {"title": "Planning and actiong in partially observable stochastic do\u00ad mains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Reflections on the nature of multi\u00ad agent coordination and its implications for an agent archi\u00ad tecture", "author": ["V.R. Lesser"], "venue": "Autonomous Agents and Multi-Agent Systems, 1, 89-111.", "citeRegEx": "Lesser,? 1998", "shortCiteRegEx": "Lesser", "year": 1998}, {"title": "On the un\u00ad decidability of probabilistic planning and infinite-horizon partially observable Markov decision process problems", "author": ["Madani", "S. Hanks", "A. Condon"], "venue": null, "citeRegEx": "Madani et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Madani et al\\.", "year": 1999}, {"title": "Using communication to reduce lo\u00ad", "author": ["M.J. Mataric"], "venue": null, "citeRegEx": "Mataric,? \\Q1998\\E", "shortCiteRegEx": "Mataric", "year": 1998}, {"title": "Decentralized con\u00ad trol of a multiple access broadcast channel: Performance bounds", "author": ["J.M. Ooi", "G.W. Wornell"], "venue": "In Proceedings of the 35th Conference on Deci\u00ad sion and Control (pp. 293-298)", "citeRegEx": "Ooi and Wornell,? \\Q1996\\E", "shortCiteRegEx": "Ooi and Wornell", "year": 1996}, {"title": "Computational Complexity", "author": ["C.H. Papadimitriou"], "venue": "Reading, MA: Addison-Wesley. Papadimitriou, C. H. & Tsitsiklis, J. N. (1987). The com\u00ad plexity of Markov decision processes. Mathematics of Op\u00ad erations Research, 12(3), 441-450.", "citeRegEx": "Papadimitriou,? 1994", "shortCiteRegEx": "Papadimitriou", "year": 1994}, {"title": "Learning to cooperate via policy search", "author": ["L. Peshkin", "Kim", "K.-E", "N. Meuleau", "L.P. Kaelbling"], "venue": "In Pro\u00ad ceedings of the Sixteenth International Conference on Un\u00ad certainty in Artificial Intelligence. Peterson,", "citeRegEx": "Peshkin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2000}, {"title": "Task decomposition, dy\u00ad namic role assignment, and low-bandwidth communication for real-time strategic teamwork", "author": ["ternation"], "venue": "Annual Symposium on Foundations of Computer Science (pp. 348-363)", "citeRegEx": "ternation.,? \\Q1999\\E", "shortCiteRegEx": "ternation.", "year": 1999}, {"title": "On the complexity of decentralized decision making and detection problems", "author": ["J.N. Tsitsiklis", "M. Athans"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis and Athans,? \\Q1985\\E", "shortCiteRegEx": "Tsitsiklis and Athans", "year": 1985}, {"title": "Markov Decision Processes", "author": ["D.J. White"], "venue": "West Sussex, England: John Wiley & Sons.", "citeRegEx": "White,? 1993", "shortCiteRegEx": "White", "year": 1993}], "referenceMentions": [{"referenceID": 11, "context": "growing interest in problems with multiple distributed agents working to achieve a common goal (Grosz & Kraus, 1996; Lesser, 1998; desJardins et al., 1999; Durfee, 1999; Stone & Veloso, 1999).", "startOffset": 95, "endOffset": 191}, {"referenceID": 4, "context": "growing interest in problems with multiple distributed agents working to achieve a common goal (Grosz & Kraus, 1996; Lesser, 1998; desJardins et al., 1999; Durfee, 1999; Stone & Veloso, 1999).", "startOffset": 95, "endOffset": 191}, {"referenceID": 5, "context": "growing interest in problems with multiple distributed agents working to achieve a common goal (Grosz & Kraus, 1996; Lesser, 1998; desJardins et al., 1999; Durfee, 1999; Stone & Veloso, 1999).", "startOffset": 95, "endOffset": 191}, {"referenceID": 13, "context": "For instance, consider two robots cooperating to push a box (Mataric, 1998).", "startOffset": 60, "endOffset": 75}, {"referenceID": 6, "context": "Other problems of planning for distributed agents with limited communication include maximizing the throughput of a multiple access broadcast channel (Ooi & Womell, 1996) and coordinating multiple spacecraft on a mission together (Estlin et al., 1999).", "startOffset": 230, "endOffset": 251}, {"referenceID": 19, "context": "A partially observable Markov decision process (POMDP) is a generalization of an MDP in which an agent must base its decisions on incomplete information about the state of the environment (White, 1993).", "startOffset": 188, "endOffset": 201}, {"referenceID": 2, "context": "Boutilier (1999) studies multi-agent Markov decision processes (MMDPs), but in this model, the agents all have access to the same in\u00ad formation.", "startOffset": 0, "endOffset": 17}, {"referenceID": 2, "context": "Boutilier (1999) studies multi-agent Markov decision processes (MMDPs), but in this model, the agents all have access to the same in\u00ad formation. In the framework we describe, this assumption is not made. Peshkin et al. (2000) use essentially the DEC\u00ad POMDP model (although they refer to it as a partially ob\u00ad servable identical payoff stochastic game (POIPSG)) and discuss algorithms for obtaining approximate solutions to the corresponding optimization problem.", "startOffset": 0, "endOffset": 226}, {"referenceID": 15, "context": "We prove that for all m 2: 2, DEC-POMDP m is NEXP-complete, and for all m 2: 3, DEC-MDP m is NEXP-complete, where NEXP = NTIME (2n ) (Papadimitriou, 1994).", "startOffset": 133, "endOffset": 154}, {"referenceID": 10, "context": "The transition probabilities and expected rewards for the corresponding \"belief MDP\" can be computed in exponential time (Kaelbling et al., 1998).", "startOffset": 121, "endOffset": 145}, {"referenceID": 1, "context": "Second, we have drawn a connection be\u00ad tween work on Markov decision processes and the body of work in complexity theory that deals with the exponen\u00ad tial jump in complexity due to decentralization (Peterson & Reif, 1979; Babai et al., 1991).", "startOffset": 198, "endOffset": 241}, {"referenceID": 3, "context": "Consider the growing body of work on algorithms for ob\u00ad taining exact or approximate solutions for POMDPs (e.g., Jaakkola et al., 1995; Cassandra et al., 1997; Hansen, 1998).", "startOffset": 106, "endOffset": 173}, {"referenceID": 8, "context": "Consider the growing body of work on algorithms for ob\u00ad taining exact or approximate solutions for POMDPs (e.g., Jaakkola et al., 1995; Cassandra et al., 1997; Hansen, 1998).", "startOffset": 106, "endOffset": 173}, {"referenceID": 12, "context": "It has recently been shown that the infinite-horizon POMDP problem is undecidable (Madani et al., 1999) under several different optimality criteria.", "startOffset": 82, "endOffset": 103}], "year": 2011, "abstractText": "Planning for distributed agents with partial state information is considered from a decision\u00ad theoretic perspective. We describe generaliza\u00ad tions of both the MDP and POMDP models that allow for decentralized control. For even a small number of agents, the finite-horizon prob\u00ad lems corresponding to both of our models are complete for nondeterministic exponential time. These complexity results illustrate a fundamen\u00ad tal difference between centralized and decentral\u00ad ized control of Markov processes. In contrast to the MDP and POMDP problems, the problems we consider provably do not admit polynomial\u00ad time algorithms and most likely require doubly exponential time to solve in the worst case. We have thus provided mathematical evidence corre\u00ad sponding to the intuition that decentralized plan\u00ad ning problems cannot easily be reduced to cen\u00ad tralized problems and solved exactly using estab\u00ad lished techniques.", "creator": "pdftk 1.41 - www.pdftk.com"}}}