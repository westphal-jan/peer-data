{"id": "1301.3874", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Risk Agoras: Dialectical Argumentation for Scientific Reasoning", "abstract": "theologians propose a formal framework for intelligent systems managers would care about scientific domains, in concern with the carcinogenicity inherent chemicals, or we study its properties. our framework is formulated in a philosophy between functional approaches using discourse, and identifies a model of meta argumentation. the methodology enables extraction of real uncertainty and conflict resolving decision framework suitable for objective reasoning about policy implications.", "histories": [["v1", "Wed, 16 Jan 2013 15:51:26 GMT  (279kb)", "http://arxiv.org/abs/1301.3874v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peter mcburney", "simon parsons"], "accepted": false, "id": "1301.3874"}, "pdf": {"name": "1301.3874.pdf", "metadata": {"source": "CRF", "title": "Risk Agoras: Dialectical Argumentation for Scientific Reasoning", "authors": ["Peter McBurney", "Simon Parsons"], "emails": [], "sections": [{"heading": null, "text": "We propose a formal framework for intelligent systems which can reason about scientific do mains, in particular about the carcinogenicity of chemicals, and we study its properties. Our framework is grounded in a philosophy of sci entific enquiry and discourse, and uses a model of dialectical argumentation. The formalism en ables representation of scientific uncertainty and conflict in a manner suitable for qualitative rea soning about the domain.\n1 INTRODUCTION\nWe seek to build intelligent systems which can reason au tonomously about the risk of carcinogenicity of chemicals, drawing on whatever theoretical or experimental evidence is available. In earlier work (McBurney & Parsons 1999), reviewing the literature on methods of carcinogen risk as sessment, we listed the different types of evidence adduced to support these claims, which may be in the form of: ex perimental results on tissue cultures, animals or human epi demiological studies; analytical comparisons with known carcinogens; or explication of biomedical causal pathways. Evidence from these different sources may conflict, and carcinogen risk assessment usually involves the compar ison and resolution of multiple evidence (E.P.A. U.S. A 1986; Graham, Green, & Roberts 1988). In representing this domain, it therefore seems appropriate to use some form of argumentation (so that the reasons for claims can be represented in association with the claims themselves), and within a dialectical framework (so that cases for and against a particular claim can be compared). In particular, dialectical argumentation enables the representation of un certainty in the underlying scientific knowledge base. This paper presents such a dialectical formalism for an intelli gent system, which we termed a Risk Agora in our earlier work. We begin by examining the nature of scientific dis course.\n2 SCIENTIFIC DISCOURSE\n2.1 A MODEL OF SCIENTIFIC ENQUIRY\nOur chosen application domain is a scientific one. To repre sent this domain, therefore, we seek to ground our formal ism in a philosophical model of scientific enquiry. Firstly, we require a theory of the nature of modem science. Fol lowing Pera (1994), we view the enterprise of science as a three-person dialogue, involving a scientific investiga tor, Nature and a skeptical scientific community. In Pera's model, the investigator proposes theoretical explanations of scientific phenomena and undertakes scientific experiments to test these. The experiments lead to \"replies\" from Na ture in the form of experimental evidence. However, Na ture's responses are not given directly or in a pure form, but are mediated through the third participant, the scien tific community, which interprets the evidence, undertakes a debate as to its meaning and implications, and eventually decides in favor or against proposed theoretical explana tions. The consequence of this model for our formalism is that we provide Nature with a formal role, but manifest it through those of the other participants.\nBut Pera's model of modem science as a dialogue game could apply to many other human dialogues, most of which do not share science's success in explaining and predict ing natural phenomena. Our model of science therefore requires an explanation of its success. Some philosophers of science believe this is due to the application of univer sal principles of assessment of proposed scientific theo ries, such as the falsificationism of Popper or the confir mationism of Camap. However, we do not share these views, instead believing, with Feyerabend (1993), that the standards of assessment used by any scientific community are domain-, context- and time-dependent. This view, that there are neither universal nor objective standards by which scientific theories can be judged, was called \"epistemolog ical anarchism\" by Lakatos (Lakatos & Feyerabend 1999). Instead of universal principles of assessment of theories, we believe science's success arises in part from applying two normative principles of conduct: firstly, that every the-\n372 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\noretical explanation proposed by a scientific investigator is contestable by anyone; and secondly, that every theoretical explanation adopted by a scientific community is defeasi ble. In other words, all scientific theories, no matter how compelling, are always tentative, being held only until bet ter explanations are found, and anyone may propose these.1\nTo build an intelligent system based on these principles, we therefore require a (normative) model of scientific discourse which enables contestation and defeasibility of claims. Our model has several components. At the high est level, we are attempting to model a discourse between reasonable, consenting scientists, who accept or reject ar guments only on the basis of their relative force. An in fluential model for debates of this type is the philosophy of Discourse Ethics developed by Habermas ( 1991) for de bates in ethical and moral domains. Our formalism there fore draws on Habermas, in particular his rules of discourse first fully articulated by Alexy (1990), and these form the basis of the desired properties of the Agora formalism pre sented later in this section.2\nNext, within this structure, we wish to be able to model dialogues in which different participants variously posit, assert, contest, justify, qualify and retract claims. To rep resent such activity requires a model of an argument, and we use Toulmin's (1958) model, within a dialectical frame work. To embody our belief in epistemological anarchism, we permit participants to contest any component of a sci entific argument: its premises; its rules of inference (Toul min 's \"warrants\"); its degrees of support (his \"modalities\"); and its consequences. We believe this is exactly what real scientists do when confronted with new theoretical expla nations of natural phenomena (Feyerabend 1993). When a scientific claim is thus contested, its proponent may re spond, not only by retracting it, but by qualifying it in some way, perhaps reducing its scope of applicability. Naess ( 1966) called this process \"precizating\", and we seek to enable such responses in the system. We thus ground our formalism for the Agora in a model of scientific discourse as dialectical argumentation. 3"}, {"heading": "2.2 DESIRED AGORA PROPERTIES", "text": "As mentioned, we desire our Agora formalism to satisfy the rules for a reasoned discourse proposed by Alexy (1990), which are listed here. In restating these, we have modi fied and re-ordered them slightly, and have ignored rules which deal specifically with discussion of ethical matters. Also, because our formalism is intended for debate regard ing only one chemical at a time, we have ignored Alexy's\n1These two principles are each necessary to explain science's success, but not sufficient.\n2 Alexy's rules have some similarity with Grice's (1975) Max ims for Conversation.\n3Further details of our philosophy of science are contained in (McBurney & Parsons 2000b).\nrules regarding the relevance of utterances. We have also added a property concerning precization.\nPl Anyone may participate in the Agora, and they may execute dialogue moves at any time, subject only to move-specific conditions (defined below).\nP2 Participation entails acceptance of the semantics for the logical language used, and of the associated modality (degrees of support) dictionaries.\nP3 Any participant may assert any claim or consequence of a claim, but may do so only when they have a grounded argument for the claim (respectively, a con sequential argument from the claim).\nP4 Any participant may question or challenge any claim or any consequence of a claim.\nPS Any participant who asserts a claim (respectively, a consequence of a claim) must provide a valued grounded argument for that claim (respectively, a val ued consequential argument from the claim) if queried or challenged by another participant.\nP6 Any participant may question or challenge the grounds, the rules of inference or the modalities for any claim.\nP7 Whenever a participant asserts a valued grounded argu ment for a claim (or a valued consequential argument from a claim), any other participant may assert a val ued grounded argument (respectively, a valued conse quential argument) for the same claim with different dictionary values.\nPS A participant who has provided a grounded argument for a claim which has been challenged should be able to respond by qualifying (precizating) the original claim or argument.\nP9 Any participant who provides a grounded argument for, or a consequential argument from, a claim is not required to provide further defence if no counter arguments are provided by other participants.\nPlO No participant may contradict him or herself.\n3 THE RISK AGORA FORMALISM\n3.1 PRELIMINARY DEFINITIONS\nWe begin by assuming the system is intended to represent debate regarding the carcinogenicity of a specific chemi cal, and that statements concerning this can be expressed in a propositional language \u00a3, whose well-formed formu lae (wffs) we denote by lower-case Greek letters. Subsets of \u00a3 (i.e. sets of wffs) are denoted by upper-case Greek letters, and \u00a3 is assumed closed under the usual connec tives. We assume multiple modes of inference (warrants)\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 373\nare possible, these being denoted by 1- i. These may include non-deductive modes of reasoning, and we make no pre sumptions regarding their validity in any truth model. We assume a finite set of debate participants, denoted by Pi. who are permitted to introduce new wffs and new modes of inference at any time. We denote Nature, in its role in the debate, by PN.\nDefinition 1: A grounded argument for a claim 0, de noted A(-+ 0), is a 3-tuple (G,R,O), where G = (0o, 01, 81,02, . . . , 0n-2, On-1, 0n-1) is an ordered se quence of wffs ()i and possibly -empty sets of wffs 0i, with n \ufffd 1 and with R = (h, h, . . . , 1-n) an ordered sequence of inference rules such that:\nIn other words, each () k ( k = 1, ... , n - 1) is derived from the preceding wff ()k_1 and set of wffs 0k-1 as a result of the application of the k-th rule of inference, 1-k. The rules of inference in any argument may be non-distinct. We call the set {Ok-1} u 0k_1 the grounds (or premises) for Ok.\nDefinition 2: A consequential argument from a claim (), denoted A(O --+ ), is a 3-tuple (0, R, C), where C = (0o, 01, 81, 02, . . . , 0n-2, On-1, 0n-1, On) is an ordered sequence of wffs ()i and possibly -empty sets of wffs 0i, with n \ufffd 1, and with R = (l-1, h, . . . , 1-n) an ordered se quence of inference rules such that:\nOn-1, 0n-1 1-n On.\nIn other words, the wffs ()k in C are derivations from () arising from the successive application of the rules of in ference in R, and we call each ()k in C a consequence of 0.\nIn order that participants may effectively state and con test degrees of commitment to claims, we require a com mon dictionary of degrees of commitment or support (what Toulrnin called \"modalities\"). Our formalism will support any agreed dictionary, whether quantitative (such as a set of probability values or belief measures) or qualitative (such as non-numeric symbols or linguistic qualifiers), provided there is a partial order on its elements. We define dictionar ies for modalities for claims, grounds, consequences and rules of inference.\nDefinition 3: Four modality dictionaries are defined as fol lows, each being a (possibly infinite) set of elements having a partial order. The claims dictionary is denoted by Vc, the grounds dictionary by Va, the consequences dictionary by VQ, and the inference dictionary by VI.\nBecause claims, grounds and consequences are all elements of the same language .C, two or more of the dictionaries Vc, Va and VQ may be the same. However, a distinct dictionary will generally be required for VI. 4 Because of our belief in epistemological anarchism, we do not specify rules of assignment of dictionary labels by participants in the Agora. In particular, the labels assigned to the conclu sions and consequences of arguments are not constrained by those assigned to premises or rules of inference.\nExample I: The generic argumentation dictionary defined for assessment of risk by (Krause et al. 1998) is an exam ple of a linguistic dictionary for statements about claims, grounds or consequences, comprising the set: {Certain, Confirmed, Probable, Plausible, Supported, Open}. The elements of this dictionary are listed in descending order, with each successive label indicating a weaker belief in the claim.\nExample 2: Two examples of Inference Dictionaries are VI= {Valid, Invalid} and VI= {Acceptable, Sometimes Acceptable, Open, Not Acceptable}.\nDefinition 4: A valued grounded argument for a claim (), denoted A(-+ O,D), is a 4-tuple (G,R,O,D), where ( G, R, 0) is a grounded argument for () and D (do,d1, . . . ,dn-1,do,r1,r2, ... ,rn) is an ordered se quence of labels and vectors of labels, with each di a vector of dictionary labels from Vc (fori = 0, . . . , n - 1), with do E Vc and with ri E VI (fori = 1, . . . , n). Each vector di comprises those values of the Claims Dictionary assigned to grounds { ()i} u 0i, the element do is that value of the Claims Dictionary assigned to () and each element ri is that value of the Inference Dictionary assigned to 1-i. A valued consequential argument from a claim 0, denoted A(() --+,D), is defined similarly .\n3.2 DISCOURSE RULES\nWe next define the rules for discourse participants, building on the definitions above. Moves are denoted by 2-ary or 3-ary functions of the form name(Pi: . ), where the first argument denotes the participant executing the move. If the move responds to an earlier move by another participant, that earlier move is the second argument. Arguments are separated by colons. In Section 4, we will show that these rules give operational effect to the Desired Properties.\n4In (McBurney & Parsons 2000c), we model degrees of ac ceptability of inference rules.\n374 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nRule 1: Query and Assertion Moves\n1.1 Pose Claim: Any participant Pi at any time may move:\npose(Pi :---+ (}?)\nwhich asks the Agora if there is a grounded argument for e.\n1.2 Propose Claim: Any participant Pi at any time may propose a claim with move:\npropose(Pi : ( (}, de))\nwhere(} E \u00a3 and de E Vc, which informs the Agora that Pi has a valued grounded argument for(}, and has assigned it a modality of de.\n1.3 Assert Claim: Any participant Pi at any time may assert a claim with move:\nassert(Pi : ((},de))\nwhere (} is a wff and de E Vc, which informs the Agora that Pi has a valued grounded argument for (}, which she believes is compelling.\n1.4 Query Claim: Whenever a propose or assert move relating to ((},de) has been made by participant Pi. any other participant Pi may move:\nquery(Pj : propose(Pi : (0, de)))\nor query(Pj : assert(Pi: (O,de))).\nThese ask participant Pi to provide her valued grounded argument for 0, which she must provide im mediately with move:\nshow_arg(Pi : A(---+ 0, D)).\n1.5 Show Grounded Argument: Any participant Pi may at any time provide a valued grounded argument for 0 with the move:\nshow_arg(Pi: A(-+ O,D)).\n1.6 Pose Consequence: A participant Pk may at any time move:\npose_cons(Pk : () ---+?)\nwhich asks the Agora if there is a consequential argu ment from 0.\n1. 7 Propose Consequence: Similarly to Propose Claim, a participant may move:\npropose_cons(Pi : (0, \u00a2, d\u00a2))\nwhere \u00a2 is a consequence of 0.\n1.8 Assert Consequence: Similarly to Assert Claim, a participant may move:\nasserLcons(Pi : (0, \u00a2, d\u00a2))\nwhere \u00a2 is a consequence of 0.\n1.9 Query Consequence: Similarly to Query Claim, a participant may move:\nquery_cons(Pj : propose(Pi: (0, \u00a2, d\u00a2))).\n1.10 Show Consequential Argument: Any participant Pi may at any time provide a valued consequential argu ment from 0 with the move:\nshow_cons(Pi : A(O ---+,D)).\n1.11 Propose Mode of Inference: Any participant Pi at any time may move:\nwhere I-t is a mode of inference and r t E V 1. This move informs the community that participant Pi be lieves that I-t is a mode of inference of strength at least Tt.\nNote that the query and assertions rules are not symmet ric between grounded and consequential arguments; partic ipants may only propose or assert claims for which they have grounded arguments, but they need not necessarily have considered the consequences of these claims. Next, we explicitly define the Contest Claim rule, with other con testation rules being defined similarly. For brevity in the following, we sometimes write A for A(---+ (},D).\nRule 2: Contestation Moves\n2.1 Contest Claim: Whenever propose or assert relating to (0, de) has been moved by participant Pi. any other participant Pi may contest this by moving:\ncontest(Pj : propose(Pi : (B, de)))\nor contest(Pj :assert(Pi : (0, de))).\nIf any participant Pk subsequently queries this contes tation with:\nquery(Pk : contest(Pj : propose(Pi : (0, de))))\n(or likewise for assert), participant Pi must respond immediately, either with an assignment of an alterna tive modality d0 for claim 0, thus:\npropose(Pi : (0, d0))\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 375\nor assert(Pj : (B, d\ufffd))\n(where d\ufffd =/: do), or with a stronger assertion of the negation of B, thus:\npropose(Pj : (\u2022B,d\ufffd))\nor assert(Pj : (\u2022B,d\ufffd))\n(where d\ufffd >do).\n2.2 Contest Ground:\ncontesLground(Pj :show...arg(Pi : A : (Bt, do,)).\n2.3 Contest Inference:\ncontesLinf(Pj : show...arg(Pi : A : f-t)).\n2.4 Contest Modality:\ncontest.mod(Pj : show...arg(Pi : A(--+ B, D))).\n2.5 Contest Consequence:\ncontesLcons(Pj : show_cons(Pi : A: (Bt, do,))).\nRule 3: Participant Resolution Moves\n3.1 Accept Proposed Claim: Whenever a claim has been proposed by Pi and its grounds demonstrated by moving:\nshow...arg(Pi : A(--+ B, D)),\nany other participant P1 may declare that they accept the proposed claim, with move:\naccept_prop(Pj : show...arg(Pi : A(--+ B, D))).\nThis move is identical with the sequence:\npropose(Pj : ( (}, do))\nshow..arg(Pj : A(--+ B, D)).\n3.2 Accept Asserted Claim: Similarly to accept .prop:\naccept..assert(Pj : show...arg(Pi : A(--+ B, D))).\n3.3 Change Modalities: Any participant Pi who proposes or asserts a claim for(}, and follows this with a demon stration of a valued grounded argument for(} by moving:\nshow...arg(Pi : A(--+ B, D)),\nmay subsequently revise her assignment of modalities with a later move of:\nshow..arg(Pi : A(--+ B, D' )),\nwhere D' =/: D. Likewise, declarations of modal be liefs expressed in other moves (e.g. in accept..assert) may also be revised by subsequently executing the same move with a different set of dictionary values.\n3.4 Accept Mode of Inference: Similarly to accept .prop:\naccepLinf(Pj : propose_inf(Pi: (f-t,rt))).\n3.5 Accept Consequence: Similarly to accept .prop:\naccepU:ons(Pj : show_cons(Pi : A(B --+,D))).\n3.6 Precizate Claim: Any participant Pi who proposes or asserts a claim for (}, and follows this with a demon stration of a valued grounded argument for (} by:\nshow..arg(Pi : A(--+ B, D))\nmay subsequently qualify her argument with:\nprec(Pi : show...arg(Pi : A(--+ B, D)): A' (--+ B, D'))\nwhere A' (--+ (}, D') is an argument for(} identical with A(--+ B, D) except that: (a) it begins from ground <I> U 80 instead of 00, where <I> is not equal to { B} nor to any ground of(}, and (b) D' may be different to D.\n3.7 Retract Claim: Any participant Pi who asserts:\nassert(Pi: (B,do))\nmay at any time subsequently withdraw the claim by:\nretract(Pi: assert(Pi: (B,do))).\nLikewise, for those claims by others accepted by Pi.\n3.8 No contradiction: Any participant Pi who asserts (or accepts an assertion for) (} may not at any time subse quently assert (or accept an assertion for) -.(}, unless they have in the interim moved:\nretract(Pi : assert(Pi : ( (}, do)))\n(or, respectively, its equivalent for accepted claims).\n3.3 DIALOGUE RULES\nDefinition 5: A Dialogue is a finite sequence of discourse moves by participants in the Agora, in accordance with the rules above.\nAs in (Hamblin 1971; Walton & Krabbe 1995; Amgoud, Maudet, & Parsons 2000), we define sets called Commit ment Stores which contain the proposals and assertions made by participants, both individually and for the Agora as a community, and track these as they change.\nDefinition 6: The commitment store of player Pi, i = 1, 2, . .. , denoted CS(Pi), is a possibly empty set {(B,do) I (} E \u00a3, do E Vc}. Each do is the claim dic tionary value assigned by Pi to B.\nThe values in participants' stores are updated by the fol lowing rule:\n376 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nRule 4: Participant Commitment Store Update: When ever participant Pi executes the moves\npropose(Pi : ((),do)),\naccept_prop(Pi: propose(Pj: ((),do))),\nassert(Pi: (e,do)),\naccept..assert(Pi: assert(Pj :((),do)))\nor their equivalents, then the tuple ((),do) is inserted into CS (Pi). Whenever participant Pi executes a retraction move for ((), do), the tuple ((), do) is removed fromCS (Pi). Similarly, whenever Pi executes a Change Modality move for ((), do), the value of ((), do) in C S (P i) is revised.\nWe next define an analogous concept for Nature, with claims inserted into Nature's Commitment Store on the ba sis of the debate at that point in the Agora. This could be achieved in a number of ways. For example, a skeptical community could define Nature's modality for a claim() to be the minimum claim modality assigned by any of those Participants claiming or supporting (). A credulous com munity could instead assign to Nature the maximum claim modality assigned by any of the participants to e. Varia tions on these approaches could utilize majority opinion or weighted voting schemes.\nBecause we wish to model dialectical discourse, we have instead chosen to assign Nature's modalities on the basis of the existence of arguments for and against the claim. To do this, we draw on the generic argumentation dictio nary for debates about carcinogenicity of chemicals pre sented in (Krause et al. 1998), which is based on Toulmin 's (1958) schema. We begin by defining certain relationships between arguments and then the Claims Dictionary for Na ture.\nDefinition 7: An argument A( -t () ) = ( G, R, () ) is con sistent if G = (eo, ()1, e1, ()2, ... , en-2, ()n-1, en-d is consistent, that is if there do not exist a, (3 E eo U { ()1} U e1 u {()2} u 0 0 0 u en-1 such that \u00b7f3 is a consequence of a.\nDefinition 8: Let A(-t ()) = (G,R,()) and B(-t \u00a2>) = (H, S, \u00a2>) be two arguments, where G (eo,e1, e1, ()2, . . . ,()n-1, en-d\u00b7 We say that B(-t \u00a2>) re buts A( -t ()) if\u00a2> = ...,()_ We say that B( -t \u00a2>) undercuts A(-t () ) if,for somea E eou{el}ue1u{e2}u ... uen-1> a= \u2022\u00a2>.\nDefinition 9: The claims dictionary for Nature is the set Vc,N = {Certain, Confirmed, Probable, Plausible, Sup ported, Open}.\nDefinition 10: The commitment store of Nature, denoted CS(PN), is a non-empty set{(e,do,N) I () E \u00a3, do,N E Vc,N }. Each do,N is the claim modality assigned by the Agora community on Nature's behalf to (), in accordance with the next two rules.\nRule 5: Nature's Modalities: The modality do,N of Na ture for the claim () is assigned as follows:\n\u2022 If () is a wff for which no grounded argument has yet been provided by a participant, then do,N is assigned the value Open.\n\u2022 If() is a wff for which at least one grounded argument has been provided by a participant, then do,N is as signed the value Supported.\n\u2022 If() is a wff for which a grounded and consistent argu ment has been provided by a participant, then do,N is assigned the value Plausible.\n\u2022 If () is a wff for which a grounded and consistent ar gument has been provided by a participant, and for which no rebutting arguments have been provided, then do,N is assigned the value Probable.\n\u2022 If () is a wff for which a grounded and consistent ar gument has been provided by a participant, and for which neither rebutting nor undercutting arguments have been provided by participants, then do,N is as signed the value Confirmed.\n\u2022 If () is a logical tautology, then do ,N is assigned the value Certain.\nRule 6: Nature Commitment Store Update: The entries in CS(PN) are updated after each legal move by Agora participants.\n3.4 ARCHITECTURE AND USER INTERFACE\nWe anticipate the Risk Agora system being used to rep resent a completed or on-going scientific debate, but not in real-time. Once instantiated with a specific knowledge base in this way, the Agora could be used for a number of differ ent purposes, which led us (McBurney & Parsons 1999), to propose a layered architecture for the Agora, corresponding to these different functions. The main purposes to be ful filled are: (a) automated reasoning to find arguments for, and the consequences of, particular claims; (b) compari son of the various arguments for and against a claim; and (c) development of an overall case for a claim, coherently combining all the arguments for and against it.\n4 AGORA PROPERTIES\nThe rules defined in the previous section were intended to operationalize the desired Agora properties of Section 2.2. We now verify that this is indeed the case.\nTheorem I: The Agora sy stem defined in Section 3 has Properties P 1 through P 10. Proof. This is straightforward, from the definitions of the\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 377\npermitted moves. Thus, Properties P l and P2 are fulfilled through the overall system design; Property P3 by Rules 1.1-1.3 and 1.6-1.8; Property P4 by Rules 1.4, 1.9, 2.1 and 2.5; Property P5 by Rules 1.2, 1.3, 1.5, 1.7, 1.8 and 1.10; Property P6 by Rules 2.2-2.4; Property P7 by Rules 3.1- 3.5; Property P8 by Rule 3.6; Property P9 by Rules 1 and 2; and Property P lO by Rule 3.8. 0\nMoreover, we can use the definition of the claim modalities for Nature provided by Rule 5 to construct a valuation func tion on wffs and to define a notion of \"proof\" of claims, as follows.\nDefinition 11: Natural valuation is a function v N defined from the set of wffs of C to the set {0, 1 }, such thatv N(B) = 1 precisely when do,N = Confirmed; otherwise, vN(B) = 0.\nDefinition 12: A provisional proof for a claim B is a grounded and consistent argument for B for which neither rebuttal nor undercutting arguments exist.\nOur belief in the defeasibility of all scientific claims leads us to use the term \"provisional proof\" rather than \"proof.\" Likewise, we can think of a natural valuation equal to 1 as signifying \"Currently Accepted as True\" (or \"Defeasibly True\") and 0 as \"Not Currently Accepted as True.\" Our definition of natural valuation thus says that a claim is de feasibly true iff there are no arguments attacking it. We could readily define additional valuation functions which capture degrees of conviction regarding the truth of claims, mapping, for instance, to Probable or to Plausible. With the definitions above, we can now prove soundness of pro visional proofs in the Agora, with respect to the natural valuation function.\nTheorem 2: With the notion of provisional proof, the Agora is consistent and complete with respect to the Natural Val uation Function v N, provided that all grounded arguments for claims are eventually asserted by some Participant. Proof. Consistency here says that all claims B for which there exists a provisional proof are also assigned a valu ation of 1 by the function VN. Completeness says, con versely, that all claims B which are assigned a valuation of 1 by v N also have a provisional proof. Both of these follow from our definitions of v N and of provisional proof, unless a consistent grounded argument for a claim B exists but is not asserted by any Participant. 0\nThe model of science we have adopted asserts that scien tific claims are regarded as \"defeasibly true\" only when the relevant scientific community agrees to so regard them. (After all, even if a transcendent truth exists, science has no privileged means of accessing it.) Our definition of nat ural valuation is in effect a proxy for the scientific commu nity's opinion on the truth of a claim. Accordingly, The orem 2 says that the provisional proof procedure neither under-generates nor over-generates defeasibly true claims,\nprovided all grounded arguments for claims are eventually asserted.\n5 EXAMPLE\nTo illustrate these ideas we present a simple and hypothet ical example of an Agora debate. In a real debate, partic ipants would be free to introduce supporting evidence and modes of inference at any time. For reasons of space, in this example we first list the statements and modes of inference to be asserted, labeled K l through K4, and R l through R3, respectively, about a chemical X:\nKl: X is produced by the human body naturally (i.e. it is endogenous).\nK2: X is endogenous in rats.\nK3: An endogenous chemical is not carcinogenic.\nK4: Bioassay experiments applying X to rats result in sig nificant carcinogenic effects.\nRl (And Introduction): Given a wff \u00a2 and a wff B, we may infer the wff ( \u00a2 1\\ B).\nR2 (Modus Ponens): Given a wff \u00a2 and the wff (\u00a2-+B), we may infer the wff B.\nR3: If a chemical is found to be carcinogenic in an ani mal species, then we may infer it to be carcinogenic in humans.\nWe now give an example of an Agora dialogue concern ing the statement: X is carcinogenic to humans, which we denote by \u00a2. The moves are numbered Ml , M2, .. . , in sequence, and for simplicity we assume the participants are using the claims dictionary of Example 1, abbreviated to { Cert, Conf, Prob, Plaus, Supp, Open}, and the in ference dictionary V1 = {Val, Inval}. Before any dis course move is made, Nature's modality for this claim is dq,,N = Open, as is its modality for \u2022\u00a2. Ignoring claims about any other chemicals, we thus have at commencement thatCS(PN) = {(\u00a2,0pen),(\u2022\u00a2, 0pen)}. Through the dialogue, we show the contents of Nature's commitment store as it changes, in steps numbered NCSO, NCSl , .. .\nNCSO: CS(PN) ={(\u00a2, Open), (\u2022\u00a2, Open)}.\nMl: assert(P1 : (\u00a2,Con!)).\nM2: query(P2 : assert(P1 : (\u00a2,Con!))).\nM3: show....arg(P1 : (K4, R3, \u00a2, ( Conf, Val, Con!))).\nNCSl: CS(PN) ={(\u00a2, Con!), (\u2022\u00a2, Open)}.\nM4: contest(P2 :assert(P1 : (\u00a2, Con!))).\n378 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nM5: query(P3 :contest(P2 :assert(P1 : (\u00a2;,Con!))))\nM6: propose(P2 : ( \u2022\u00a2;, Plaus) ).\nM7: query(P1 :propose(P2 : ( \u2022\u00a2>, Plaus ))).\nM8: show...nrg(P2 : ((Kl, K3), R2, \u2022\u00a2>, ( Conf, Prob, Val, Plaus))).\nNCS2: CS( PN) = {(\u00a2;,Plaus),(\u2022\u00a2;,Plaus)}.\nM9: contest_ground(P 4 : show...nrg(P2 : ((Kl, K3), R2, \u2022\u00a2;,\n(Conf,Prob, Val,Plaus)) : (K3,Prob))).\nMlO: show...nrg(P4: ((K2, K4), RI, -.K3, ( Conf, Con/, Val, Con!)) )\nNCS3: CS( PN) = { ( \u00a2>, Plaus), ( \u2022\u00a2>, Plaus)}.\nObserve that Participant P4 in Move MlO, by providing an argument for -.K3, undercuts the argument presented for \u00a2> by Participant P2 in Move MS. We can also observe the changes in the Natural Valuation of \u00a2; through the course of this debate. At the start, we have VN(\u00a2>) = 0, which changes to vN(\u00a2) = 1 after Move M3, since then de,N = Con/. However, after Move M8, de,N = Plaus, so once again VN(\u00a2>) = 0.\n6 DISCUSSION\nCharacterization of scientific discourse as dialectical argu mentation is not new. Rescher ( 1977) claims to have been the first to propose a dialectical framework for modeling the progress of scientific inquiry, and Pera's (1994) work is also a dialectical approach to science. Among argumen tation theorists, Freeman (1991) also discusses scientific discourse in his study of argument structure. Both Carlson (1983) and Walton and Krabbe (1995) aim to model generic dialogues, but their focus is (respectively) on question-and answer and persuasion dialogues. In addition, neither for malism explicitly permits degrees of support for commit ments to be expressed, which our formalism does.\nNone of these works appears intended for encoding in in telligent systems. Within AI, intelligent systems for scien tific domains have used argumentation for some time (e.g. Fox, Krause, & Ambler 1992). However, these applications have typically involved monolectical rather than dialectical argumentation. More recently, Haggith (1996) developed a dialectical argumentation formalism and applied the re sulting system to a carcinogenicity debate. However, the primary focus of her work was on knowledge representa tion in generic domains of conflict, and so her formalism is not grounded in an explicit philosophy of science. The work of Amgoud, Maudet, & Parsons (2000) is closest in approach to that presented here (and we have drawn upon their formalism), but it is focused on negotiation dialogues,\nagam m a generic context. Their formalism only permits two participants, although this would be relatively easy to amend. As with Haggith's system, their formalism does not permit debate over the rules of inference used. Recent legal argumentation systems, such as those of Verheij ( 1999), do permit this.\nOur formal definition of the Risk Agora enables contes tation and defeasibility of scientific claims. Our system therefore operationalizes the two normative principles of conduct for scientific discourses presented in Section 2.1. We are currently exploring a number of refinements to the Agora. Firstly, Rehg ( 1997) has demonstrated the ratio nality of incorporation of rhetorical devices (such as epi deictic speech and appeals to emotions) in dialectical argu ment and decision-making, and we seek a means to incor porate such devices in the Agora. This would not be novel: the argumentation system of Reed ( 1998), for example, al lows for the modeling of rhetorical devices, although in a monolectical context. Secondly, using the Agora in a de liberative context would require incorporation of values for the projected consequences and the development of an ap propriate qualitative decision-theory, as in (Fox & Parsons 1998; Parsons & Green 1999).\nWe believe the Risk Agora has a number of potential bene fits. Firstly, by articulating precisely the arguments used to assert carcinogenicity, gaps in knowledge and weaknesses in arguments can be identified more readily. Such iden tification could be used to prioritize bio-medical research efforts for the particular chemical. Secondly, by explor ing the logical consequences of claims, the Risk Agora can serve a social maieutic function, making explicit knowl edge which may only be latent. Thirdly, once instantiated with the details of a particular debate, the system could be used for self-education by others outside the scientific community concerned. Indeed, it could potentially form the basis for the making of regulatory or societal deci sions on the issues in question (e.g. Should the chemical be banned?) , and thereby give practical effect to notions of deliberative democracy (McBurney & Parsons 2000a; 2000 In press). Finally, with argumentation increasingly being used in the design of multi-agent systems (Parsons, Sierra, & Jennings 1998), the formalism presented here could readily be adapted for deliberative dialogues between independent software agents.\nAcknowledgments\nThis work was partly funded by the British EPSRC under grant GRIL84117 and a PhD studentship. We also thank the anonymous reviewers for their comments.\nReferences\nAlexy, R. 1990. A theory of practical discourse. In Benhabib, S., and Dallmayr, F., eds., The Communica-\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 379\ntive Ethics Controversy . Cambridge, MA, USA: MIT Press. 151-190. (Published in German 1978).\nAmgoud, L.; Maudet, N.; and Parsons, S. 2000. Mod elling dialogues using argumentation. In Durfee, E., ed., Proceedings of the 4th International Conference on Multi Agent Sy stems ( ICMAS- 2000 ). Boston, MA, USA: IEEE.\nCarlson, L. 1983. Dialogue Games: An Approach to Dis course Analy sis. Dordrecht, The Netherlands: D. Reidel.\nE.P.A. U.S.A. 1986. Guidelines for carcinogen risk assess ment. U.S. Federal Register 51:33991-34003.\nFeyerabend, P. 1993. Against Method. London, UK: Verso, third edition. First edition published 1971.\nFox, J., and Parsons, S. 1998. Arguing about beliefs and actions. In Hunter, A., and Parsons, S., eds., Applications of Uncertainty Formalisms, LNAI 1455. Berlin, Germany: Springer Verlag. 266-302.\nFox, J.; Krause, P.; and Ambler, S. 1992. Arguments, con tradictions and practical reasoning. Proceedings of ECAI 1992, Vienna, Austria.\nFreeman, J. B. 1991. Dialectics and the Macrostructure of Arguments: A Theory of Argument Structure. Berlin, Germany: Foris.\nGraham, J. D.; Green, L. C.; and Roberts, M. J. 1988. In Search of Safety: Chemicals and Cancer Risk. Cambridge, MA, USA: Harvard University Press.\nGrice, H. P. 1975. Logic and conversation. In Cole, P., and Morgan, J. L., eds., Sy ntax and Semantics Ill: Speech Acts. New York City, NY, USA: Academic Press. 41-58.\nHabermas, J. 1991. Moral Consciousness and Communica tive Action. Cambridge, MA, USA: MIT Press. (Published in German 1983).\nHaggith, M. 1996. A Meta-level Argumentation Frame work for Representing and Reasoning about Disagreement. Ph.D. Dissertation, University of Edinburgh, UK.\nHamblin, C. L. 1971. Mathematical models of dialogue. Theoria 37:130-155.\nKrause, P.; Fox, J.; Judson, P.; and Patel, M. 1998. Qual itative risk assessment fulfils a need. In Hunter, A., and Parsons, S., eds., Applications of Uncertainty Formalisms, LNAI 1455. Berlin, Germany: Springer Verlag. 138-156.\nLakatos, 1., and Feyerabend, P. 1999. For and Against Method. Chicago, IL, USA: University of Chicago Press.\nMcBurney, P., and Parsons, S. 1999. Truth or con sequences: using argumentation to reason about risk. In British Psy chological Society Sy mposium on Practical Reasoning. London, UK: BPS.\nMcBurney, P., and Parsons, S. 2000a. Intelligent systems to support deliberative democracy in environmental regula tion. In Peterson, D., ed., AJSB Sy mposium on AI and Legal Reasoning. Birmingham, UK: AISB.\nMcBurney, P., and Parsons, S. 2000b. Modeling scientific discourse. In Pearce, D., ed., Proceedings of the Work shop on Scientific Reasoning in AI and Philosophy of Sci ence, 14th European Conference on Artificial Intelligence (ECAI- 2000). Berlin, Germany: ECAI.\nMcBurney, P., and Parsons, S. 2000c. Tenacious tortoises: a formalism for argument over rules of inference. Techni cal report, Department of Computer Science, University of Liverpool, UK.\nMcBurney, P., and Parsons, S. 2000 (In press). Risk Ago ras: Using dialectical argumentation to debate risk. Risk Management Journal.\nNaess, A. 1966. Communication and Argument: Elements of Applied Semantics. London, UK: Allen and Unwin. (Published in Norwegian 1947).\nParsons, S., and Green, S. 1999. Argumentation and qual itative decision making. In Hunter, A., and Parsons, S., eds., The 5th European Conference on Sy mbolic and Quan titative Approaches to Reasoning and Uncertainty ( EC SQARU99), LNAI 1638. Berlin, Germany: Springer Ver lag. 328-339.\nParsons, S.; Sierra, C.; and Jennings, N. R. 1998. Agents that reason and negotiate by arguing. Journal of Logic and Computation 8(3):261-292.\nPera, M. 1994. The Discourses of Science. Chicago, IL, USA: University of Chicago Press.\nReed, C. 1998. Generating Arguments in Natural Lan guage. Ph.D. Dissertation, University College, University of London, London, UK.\nRehg, W. 1997. Reason and rhetoric in Habermas's The ory of Argumentation. In Jost, W., and Hyde, M. J., eds., Rhetoric and Hermeneutics in Our Time: A Reader. New Haven, CN, USA: Yale University Press. 358-377.\nRescher, N. 1977. Dialectics: A Controversy -Oriented Approach to the Theory of Knowledge. Albany, NY, USA: State University of New York Press.\nToulmin, S. E. 1958. The Uses of Argument. Cambridge, UK: Cambridge University Press.\nVerheij, B. 1999. Automated argument assistance for lawyers. In Proceedings of the Seventh International Con ference on Artificial Intelligence and Law, Oslo, Norway , 43-52. New York City, NY, USA: ACM.\nWalton, D. N., and Krabbe, E. C. W. 1995. Commitment in Dialogue: Basic Concepts of Interpersonal Reasoning. Albany, NY, USA: State University of New York Press."}], "references": [{"title": "A theory of practical discourse", "author": ["R. Alexy"], "venue": null, "citeRegEx": "Alexy,? \\Q1990\\E", "shortCiteRegEx": "Alexy", "year": 1990}, {"title": "Mod\u00ad elling dialogues using argumentation", "author": ["L. Amgoud", "N. Maudet", "S. Parsons"], "venue": "Proceedings of the 4th International Conference on Multi\u00ad Agent", "citeRegEx": "Amgoud et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Amgoud et al\\.", "year": 2000}, {"title": "Verso, third edition", "author": ["UK London"], "venue": "Applications of Uncertainty Formalisms,", "citeRegEx": "London,? \\Q1971\\E", "shortCiteRegEx": "London", "year": 1971}, {"title": "Arguments, con\u00ad tradictions and practical reasoning", "author": ["J. Fox", "P. Krause", "S. Ambler"], "venue": "Proceedings of ECAI", "citeRegEx": "Fox et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Fox et al\\.", "year": 1992}, {"title": "Dialectics and the Macrostructure of Arguments: A Theory of Argument", "author": ["J.B. Austria. Freeman"], "venue": null, "citeRegEx": "Freeman,? \\Q1991\\E", "shortCiteRegEx": "Freeman", "year": 1991}, {"title": "Mathematical models of dialogue", "author": ["C.L. Hamblin"], "venue": "Theoria", "citeRegEx": "Hamblin,? \\Q1971\\E", "shortCiteRegEx": "Hamblin", "year": 1971}, {"title": "Qual\u00ad itative risk assessment fulfils a need", "author": ["P. Krause", "J. Fox", "P. Judson", "M. Patel"], "venue": null, "citeRegEx": "Krause et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Krause et al\\.", "year": 1998}, {"title": "Intelligent systems to support deliberative democracy in environmental regula\u00ad tion", "author": ["London", "P. UK: BPS. McBurney", "S. Parsons"], "venue": null, "citeRegEx": "London et al\\.,? \\Q2000\\E", "shortCiteRegEx": "London et al\\.", "year": 2000}, {"title": "Tenacious tortoises: a formalism for argument over rules of inference", "author": ["P. McBurney", "S. Parsons"], "venue": "Techni\u00ad cal report,", "citeRegEx": "McBurney and Parsons,? \\Q2000\\E", "shortCiteRegEx": "McBurney and Parsons", "year": 2000}, {"title": "In press). Risk Ago\u00ad ras: Using dialectical argumentation to debate risk", "author": ["P. McBurney", "S. Parsons"], "venue": "Risk Management", "citeRegEx": "McBurney and Parsons,? \\Q2000\\E", "shortCiteRegEx": "McBurney and Parsons", "year": 2000}, {"title": "Argumentation and qual\u00ad itative decision making", "author": ["S. Parsons", "S. Green"], "venue": null, "citeRegEx": "Parsons and Green,? \\Q1999\\E", "shortCiteRegEx": "Parsons and Green", "year": 1999}, {"title": "Agents that reason and negotiate by arguing", "author": ["S. Parsons", "C. Sierra", "N.R. Jennings"], "venue": null, "citeRegEx": "Parsons et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Parsons et al\\.", "year": 1998}, {"title": "Generating Arguments in Natural Lan\u00ad", "author": ["C. Press. Reed"], "venue": null, "citeRegEx": "Reed,? \\Q1998\\E", "shortCiteRegEx": "Reed", "year": 1998}, {"title": "Dialectics: A Controversy -Oriented Approach to the Theory of Knowledge", "author": ["N. Rescher"], "venue": null, "citeRegEx": "Rescher,? \\Q1977\\E", "shortCiteRegEx": "Rescher", "year": 1977}, {"title": "Commitment in Dialogue: Basic Concepts of Interpersonal Reasoning", "author": ["D.N. Walton", "E.C.W. Krabbe"], "venue": null, "citeRegEx": "Walton and Krabbe,? \\Q1995\\E", "shortCiteRegEx": "Walton and Krabbe", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "Our formalism there\u00ad fore draws on Habermas, in particular his rules of discourse first fully articulated by Alexy (1990), and these form the basis of the desired properties of the Agora formalism pre\u00ad sented later in this section.", "startOffset": 109, "endOffset": 122}, {"referenceID": 0, "context": "As mentioned, we desire our Agora formalism to satisfy the rules for a reasoned discourse proposed by Alexy (1990), which are listed here.", "startOffset": 102, "endOffset": 115}, {"referenceID": 0, "context": "As mentioned, we desire our Agora formalism to satisfy the rules for a reasoned discourse proposed by Alexy (1990), which are listed here. In restating these, we have modi\u00ad fied and re-ordered them slightly, and have ignored rules which deal specifically with discussion of ethical matters. Also, because our formalism is intended for debate regard\u00ad ing only one chemical at a time, we have ignored Alexy's 1These two principles are each necessary to explain science's success, but not sufficient. 2 Alexy's rules have some similarity with Grice's (1975) Max\u00ad ims for Conversation.", "startOffset": 102, "endOffset": 555}, {"referenceID": 6, "context": "Example I: The generic argumentation dictionary defined for assessment of risk by (Krause et al. 1998) is an exam\u00ad ple of a linguistic dictionary for statements about claims, grounds or consequences, comprising the set: {Certain, Confirmed, Probable, Plausible, Supported, Open}.", "startOffset": 82, "endOffset": 102}, {"referenceID": 5, "context": "As in (Hamblin 1971; Walton & Krabbe 1995; Amgoud, Maudet, & Parsons 2000), we define sets called Commit\u00ad ment Stores which contain the proposals and assertions made by participants, both individually and for the Agora as a community, and track these as they change.", "startOffset": 6, "endOffset": 74}, {"referenceID": 6, "context": "To do this, we draw on the generic argumentation dictio\u00ad nary for debates about carcinogenicity of chemicals pre\u00ad sented in (Krause et al. 1998), which is based on Toulmin 's (1958) schema.", "startOffset": 124, "endOffset": 144}, {"referenceID": 6, "context": "To do this, we draw on the generic argumentation dictio\u00ad nary for debates about carcinogenicity of chemicals pre\u00ad sented in (Krause et al. 1998), which is based on Toulmin 's (1958) schema.", "startOffset": 125, "endOffset": 182}, {"referenceID": 12, "context": "Rescher ( 1977) claims to have been the first to propose a dialectical framework for modeling the progress of scientific inquiry, and Pera's (1994) work is also a dialectical approach to science.", "startOffset": 0, "endOffset": 148}, {"referenceID": 4, "context": "Among argumen\u00ad tation theorists, Freeman (1991) also discusses scientific discourse in his study of argument structure.", "startOffset": 33, "endOffset": 48}, {"referenceID": 4, "context": "Among argumen\u00ad tation theorists, Freeman (1991) also discusses scientific discourse in his study of argument structure. Both Carlson (1983) and Walton and Krabbe (1995) aim to model generic dialogues, but their focus is (respectively) on question-and\u00ad answer and persuasion dialogues.", "startOffset": 33, "endOffset": 140}, {"referenceID": 4, "context": "Among argumen\u00ad tation theorists, Freeman (1991) also discusses scientific discourse in his study of argument structure. Both Carlson (1983) and Walton and Krabbe (1995) aim to model generic dialogues, but their focus is (respectively) on question-and\u00ad answer and persuasion dialogues.", "startOffset": 33, "endOffset": 169}], "year": 2011, "abstractText": "We propose a formal framework for intelligent systems which can reason about scientific do\u00ad mains, in particular about the carcinogenicity of chemicals, and we study its properties. Our framework is grounded in a philosophy of sci\u00ad entific enquiry and discourse, and uses a model of dialectical argumentation. The formalism en\u00ad ables representation of scientific uncertainty and conflict in a manner suitable for qualitative rea\u00ad soning about the domain.", "creator": "pdftk 1.41 - www.pdftk.com"}}}