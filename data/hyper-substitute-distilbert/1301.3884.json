{"id": "1301.3884", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Probabilistic Models for Query Approximation with Large Sparse Binary Datasets", "abstract": "large sparse aggregation of linked registration data supports sequences of records and thousands of attributes available with various domains : customers purchasing terminals, developers visiting email pages, microsoft documents giving descriptions are just three typical examples. real - directed query selectivity estimation ( the problem of estimating their number of segments from graph representation lacking a query predicate ) is an important practical problem for such users.", "histories": [["v1", "Wed, 16 Jan 2013 15:52:06 GMT  (279kb)", "http://arxiv.org/abs/1301.3884v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI cs.DB", "authors": ["dmitry y pavlov", "heikki mannila", "padhraic smyth"], "accepted": false, "id": "1301.3884"}, "pdf": {"name": "1301.3884.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Models for Query Approximation with Large Sparse Binary Data Sets", "authors": ["Dmitry Pavlov", "Heikki Mannila", "Padhraic Smyth"], "emails": [], "sections": [{"heading": null, "text": "Large sparse sets of binary transaction data with millions of records and thousands of attributes occur in various domains: cus tomers purchasing products, users visiting web pages, and documents containing words are just three typical examples. Real-time query selectivity estimation (the problem of estimating the number of rows in the data satisfying a given predicate) is an important practical problem for such databases.\nWe investigate the application of probabilis tic models to this problem. In particular, we study a Markov random field (MRF) ap proach based on frequent sets and maximum entropy, and compare it to the independence model and the Chow-Liu tree model. We find that the MRF model provides substantially more accurate probability estimates than the other methods but is more expensive from a computational and memory viewpoint. To alleviate the computational requirements we show how one can apply bucket elimination and clique tree approaches to take advantage of structure in the models and in the queries. We provide experimental results on two large real-world transaction datasets.\n1 Introduction\nMassive datasets containing huge numbers of records have recently become an object of increasing interest among both the businesses who routinely collect such data and data miners who try to find regularities in them. One class of such datasets is transaction data which is typically binary in nature. This class is char acterized by high sparseness, i.e., there may be hun dreds and thousands of binary attributes but a par ticular record may only have a few of them set to 1.\nExamples include retail transaction data and Web log data, where each row is a transaction or session and each column represents a product or Web page.\nData-owners typically have a lot of questions about their data. For instance, it may be of interest to know how often the pages wl and w2 but not w3 were re quested together. Such types of questions about the data can be formalized as Boolean queries on arbi trary subsets of attributes. The problem then is to find the frequency of rows in the dataset that satisfy query Q, or, equivalently the probability of the query P(Q) with respect to the empirical probability distri bution defined by the data.\nAny Boolean query can be answered using a single scan through the dataset. While this approach has linear complexity and works well for the small datasets, it be comes infeasible for real time queries on huge datasets which do not reside in main memory. We would thus like to have an approximate algorithm that would al low us to trade accuracy in the estimate of P( Q) with the time taken to calculate it. This paper studies dif ferent probabilistic models for approximating P( Q).\nA simple model-based approach is to calculate the marginal frequencies of all attributes and use an inde pendence model for P ( Q) (often the method of choice in commercial relational database systems). The in dependence model is easy to learn, has low time and space requirements but as we shall see below is fairly inaccurate. [Mannila et al., 1999] introduced the idea of using an MRF model based on frequent itemsets and a maximum entropy ( maxent) approach for query ap proximation. The motivation comes from the fact that there exist many efficient data mining algorithms for extracting frequent itemsets from massive data sets, and maxent principle can be used to combine these itemsets to form a coherent probabilistic MRF model.\nIn this paper we show that the itemsets and the max ent distribution define a Markov random field {MRF} by the fundamental MRF theorem [Hammersley and\n466 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nClifford, 1972]. We also improve the standard iterative scaling (IS) algorithm for learning parameters of the MRF models by showing how one can apply bucket elimination and clique tree approaches to take advan tage of structure in the models and in the queries. We show that depending on the number of itemsets used as an input to the maxent models their prediction ac curacy averages within 0.1-1% of the true count.\nWe also investigate an approach based on tree structured belief networks [Chow and Liu, 1968] that fills in the gap between the computationally inexpen sive (but not very accurate) independence model and the relatively expensive maxent solution. We pro vide experimental results on the performance of all the methods on the real data. We compare models in terms of the accuracy of the approximation, and the time and amount of information that the model reqmres.\nThe rest of this paper is organized as follows. In Sec tion 2 we introduce notation and give a formal state ment of the estimation problem. Section 3 gives precise definitions of the models that we apply to the query se lectivity estimation problem and the methodology we use to compare them. Section 4 presents the empiri cal results and in Section 5 we draw conclusions and present some extensions.\n2 Statement of the Problem and Notation\nLet R = {A1 , . . . , Ak} be a table header with k 0/1 val ued attributes (variables) and r be a table of n rows over header R. We assume that k \u00ab n, and that the data are sparse, as we discussed above. A row of the table r satisfies a conjunctive query Q iff the corresponding attributes in the query and in the row have equal values. We are interested in finding the number of rows in the table r satisfying a given con junctive query Q defined on a subset of its attributes. We can view this query selectivity estimation problem in a probabilistic light and pose the problem as esti mating the true frequency of Q in the table r using an approximate (and presumably much smaller and more efficient) probability model PM.\nThe time involved in using a probability model PM is divided into the offline cost Tp, i.e., the time needed for building the model, and the online cost tp ( Q) needed to give the approximate answer to query Q us ing the model PM. We use Sp to denote the amount of space (memory) needed to store the model PM.\nFor a given class of queries, let 7r( Q) denote the prob ability that the query Q is issued. We assume that this distribution is known, but in principle we could\nlearn 7r(Q) for a population or individuals. By ep(Q) we denote the error in answering the query Q, i.e., the difference between the true count Ct(Q) and the count estimated from the model PM. We are interested in the expectation of the relative error with respect to the underlying query distribution E,.[lep(Q)I/Ct(Q)]. We use the empirical relative error defined as\n' 1 E= - NQ's NQueries 2:: j=l lep(Qj)l Ct(Qi) '\n(1)\nwhere NQ'\u2022 is the number of random query drawings from 7r( Q) and Ct ( Qj) is the true count of the query Qj.\n3 Models\n3.1 Full data and Independence model\nThere are a wide variety of options in choosing the model PM. One extreme is to store the entire dataset so that for each record we will only keep a list of columns that have 1 's in them. We will have 100% ac curate estimates, but for most of the real-life datasets this approach will incur inordinately large memory re quirements, namely Sp = O(c 2:::7=1 Nt's (i)), where c is the prespecified number of bits required to store a number to some fixed precision and Nt's(i) is the num ber of positively initialized attributes for record i.\nThe other extreme is also easy to describe-the in dependence model. Since the data are binary-valued, we only have to store one count per attribute. The probability of a conjunctive query is approximated by the product of the probabilities of the single attribute value combinations occurring in the query. Obviously, Sp is small in this method, namely O(kc) bits. The preprocessing can be done by a single scan through the data, and the online cost consists of nQ multipli cations, where nQ is the number of conjuncts in the query Q. However, as we shall see later, the quality of the approximations produced by the independence method can be relatively poor.\n3.2 Model Based on the Multivariate Tree Distribution\nThis model [Chow and Liu, 1968] assumes that there are only pairwise dependencies between the variables and that the dependency graph on the attributes is a tree. To fit a distribution with a tree it is sufficient to know the pairwise marginals of all the variables. The algorithm consists of three steps, namely, com puting the pairwise marginals of the attributes, com puting the mutual information between the attributes and, finally, applying Kruskal's algorithm to find the\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 467\nminimum spanning tree of the full graph whose nodes are the attributes and the weights on the edges are the mutual informations. The dominating term in the overall offline time complexity will be O(k2n) due to the computation of the marginals. The memory re quirements for the algorithm is O(k2c). Once the tree is learned, we can use a standard belief propagation al gorithm [Pearl, 1988] to get the answer to a particular conjunctive query Q in time linear in nQ.\n3.3 Maximum Entropy MRF Model\nAn itemset associated with the binary table r with the header R is defined to be either a single positively initialized attribute or a conjunction of the mutually exclusive positively initialized attributes from R. We will call an itemset T-frequent if its count in the table r is at least T where T is some predefined non-negative threshold.\nThere exist efficient algorithms to compute all the itemsets from large binary tables ( e.g., [Mannila et al., 1994, Agrawal and Srikant, 1994]). In practice the run ning time of these algorithms is linear in both the size of the table and the number of frequent itemsets pro vided that the data are sparse. Thus, by computing itemsets we won't typically incur a high preprocessing cost.\nThe maximum entropy approach makes use of the T frequent itemsets and the associated frequency counts treating them as constraints on the query distribution. Indeed, each pair of (a) an itemset and (b) its associ ated frequency count can be viewed as a value of the marginal distribution on the query variables when they all are positively initialized.\nConsider an arbitrary conjunctive query Q on vari ables XQ = { Ql, . . . , QnQ} . Forcing the estimate PM to be consistent with the T-frequent itemsets for some T > 0 restricts PM to a constrained set P of proba bility distributions within the general nQ-dimensional simplex containing all possible distributions defined on nQ variables. Information about frequencies of the T frequent itemsets for some T > 0 in general under constrains the target distribution and we will need an additional criterion to pick a unique estimate PM ( Q) from the set P of all plausible ones. The maximum entropy principle provides such a criterion. It essen tially instructs one to select a distribution that is as uninformed as possible, i.e., makes the fewest possible commitments about anything the constraints do not specify. Given maximum entropy as a preference cri terion, we face a constrained optimization problem of finding PM(xQ) = arg maxpEP H(P), where H(P) is the entropy of the distribution P. If the constraints are consistent (which is clearly the case with itemset-based\nconstraints) one can show that the target distribution will exist, be unique [Berger et al., 1996, Pietra et al., 1997] and can be found in an iterative fashion using an algorithm known as iterative scaling (IS) [Darroch and Ratcliff, 1972, Csiszar and Tusnady, 1984]. Note that we are estimating the full joint distribution on variables xq, not only the probability of the specific instantiation of variables in the given query Q.\nIn order to get a probability estimate PM ( XQ) we only retain itemsets whose variables are subsets of XQ, i.e., the joint on XQ is estimated in real-time when a query is posed to the system. Enforcing the j-th constraint Cj can be performed by just summing out from PM(xq) all the variables not participating in the j-th itemset (the variables in the constraint Cj should be kept fixed to 1), and requiring that the result of this sum equals the true count /j of the j-th itemset in the table. Con straint Cj will thus look like:\nL PM(xQ)G(A{ = 1, . . . ,A\ufffdi = 1) = /j (2) XQE{O,l}nQ\nwhere G(.) is the indicator function. Whenever we say that initialized query variables XQ satisfy a given constraint Cj, we shall mean that vari ables XQ agree in their values with all the variables Cj. It can be shown (see, e.g., [Jelinek, 1998]) that the maxent distribution will have a special product form\nN P ( ) rr G(xQ satisfies Cj) M XQ = /10 J1j\nj=l (3)\nOnce the constants /1j are estimated, Equation 3 can be used to evaluate any query on the variables in XQ, and Q in particular. The product form of the tar get distribution will contain exactly one factor corre sponding to each of the constraints. Factor J1o is a normalization constant whose value is found from the condition\n(4)\nThe general problem is thus reduced to the problem of finding a set of numbers /1j from Equations 2 and 4. The IS algorithm is well known in the statistical literature as an iterative technique which converges to the maxent solution for problems of this general form (see, e.g., [Jelinek, 1998]). A high-level outline of the most computationally efficient version of the algorithm [Jelinek, 1998] is as follows.\n1. Choose an initial approximation to PM(XQ) 2. While (Not all Constraints are Satisfied)\nFor (j varying over all constraints) Update J1o;\n468 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nUpdate fJj; End;\nEndWhile; 3. Output the constants fJj\nThe update rules for parameter pj corresponding to the constraint Cj at iteration t are:\nwhere Sj is defined as: stj-\nXQ satisfying Cj\n(5)\n(6)\n(7)\nEquation 7 essentially calculates the constraints as in Equation 2 but using the current estimate pMt which in turn is defined by the estimates of the pj 's via Equa tion 3. Since the distribution PMt may not necessarily meet the Cj-th constraint, equations 5 and 6 update the terms tJo and f.lj to enforce it. The algorithm pro ceeds in a round-robin fashion to the next constraint at each iteration getting closer to satisfying all of them.\nConvergence of the algorithm can be determined by various means. In the case of the query selectivity es timation problem, we are interested only in one cell of the distribution on the query variables corresponding to a particular query Q. We monitor this particular cell and terminate the algorithm when\nE is one of the free parameters of the algorithm, in the experiments we chose E = 10-4. It usually takes 10-15 iterations for the algorithm to meet such a convergence criterion.\nPreprocessing for the maxent model consists of finding the itemsets for the entire dataset given a specified threshold T . Suppose that we have selected N itemsets on the sets of variables Ij = {A{ , ... , A\ufffd 1} and we know their frequencies /j in the table r. Then the memory cost is O(c(I:;\ufffd=l nk + N)). The first term in this estimate corresponds to storing the attributes that are set to 1 in each of the itemsets, and the second term to storing the counts of the itemsets in the table r.\nThe main computation in the IS algorithm occurs in summing out the distribution pMt(xQ) according to Equation 7. The total number of summands in Equa tion 7 is 2nQ-n1. Each summand will have a product form and will contain at most N factors. Thus, the overall time complexity of performing the summation in Equation 7 once for all the factors f.1 is I:;f=1 aj2\"'1,\nwhere CXj = nQ - nj. The last estimate is obviously upper-bounded by O(NnQ2nQ). Note that this is in dependent of the size of the original dataset. Although the exponential time complexity in the size of the query makes the method prohibitive for large query sizes, it is still feasible to use it for queries of length 8 or so in practice. The IS algorithm in its formula tion above has linear memory complexity in nQ since the summation in Equation 7 can be performed using backtracking.\nWe note that the maxent distribution in Equation 3 is an MRF model. Suppose, that for a given query we have selected all itemsets that only mention query variables. These itemsets define an undirected graphi cal model H on the query variables with an edge con necting the two nodes iff the corresponding variables are mentioned in some itemset. Each node v in H will naturally have a neighborhood - the set of all nodes in H incident to v. Finally, from the product form of the maxent distribution in Equation 3, and the fact that each factor corresponds to an itemset, it follows that maxent distribution can be viewed as a product of exponential functions on the cliques of the graph H. Hence, by the fundamental MRF theorem [Hammer sley and Clifford, 1972] maxent distribution defines a MRF with respect to the graph H. In the next subsec tion we discuss how one can speed-up the IS algorithm by trading memory for time based on the structure of the itemsets used to constrain the distribution.\n3.4 Trading Memory for Time in Iterative Scaling\nThe strategies that we propose for reducing the com putational complexity of the IS algorithm employ the structure in the queries and in the itemsets.\nBelow we show that in general the time complexity of IS can be reduced to being exponential in the induced width w\u2022 of the graph H. The notion of induced width is closely related to the size of cliques of the graph H and can be thought of being equal to the size of the largest clique after the graph has been triangulated. Finding the ordering of the variables with width equal to w\u2022 is NP-hard in general. In our experiments we used the heuristic of maximum cardinality ordering.\n3.4.1 Bucket Elimination\nBucket elimination [Dechter, 1 996] is essentially a smart way to do bookkeeping as one goes along the updates of factors in the representation of the maxent distribution. The idea here is to use the distributive law (see, e.g., [Aji and McEliece, 1997]) in Equation 7.\nConsider a simple example. We issue a query on six\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 469\nbinary attributes At, ... , A6\u2022 There are frequent item sets corresponding to each single attribute and the fol lowing frequent itemsets of size 2: {A2 = 1, A3 = 1}, { A3 = 1, A4 = 1}, { A4 = 1, A6 = 1}, { A3 = 1, A5 = 1} and { A5 = 1, A6 = 1}. The graphical model H in Figure 1 shows the interactions between the attributes in this example.\n6 P- II G(A;=l) II G(A; =Aj =l)G(:J d ( . \") . H) J-Lo Jl-; Jl-ij :::\ufffde ge z, J m i=l i,j Suppose, that on the current iteration we are updating Jl-56 corresponding to itemset {A5 = 1, A6 = 1}. Ac cording to our update rule in Equation 7, we need to fix attributes A5 and A6 to 1 and sum out the rest of the attributes in pt (A1, ... , A6). It is easy to see that brute force summation over all the values of A1, ... , A4 will involve computing 16 terms, each having a product form. The bucket elimination algorithm will produce exactly the same result but will do it more efficiently the number of terms to evaluate is reduced by a factor of 2 compared to the brute force method:"}, {"heading": "L P(At, ... ' A4, A5 = 1, A6 = 1) = Jl-DJ1-5Jl-6Jl-W", "text": "All .. ,A4\n(\"\"' G(A,=l) G(A,=A,=l)(\"\"' G(A,=l)))) . L....J 11-2 11-23 L....J 11-t\nThe distributive law thus allows for a more time efficient implementation of the IS procedure.\n3.4.2 Clique Tree Ideas\nAnother way to speed-up the IS algorithm is based on the decomposability of the probability distribution with respect to the graph of the model. A detailed treatment of such ideas can be found in [Pearl, 1988, Jirousek and Preucil, 1995, Malvestuto, 1992].\nWe first create a chordal graph H' from H. To en force chordality we use a graph triangulation algorithm [Pearl, 1988]. For the chordal graph, the joint prob ability distribution on the variables corresponding to its vertices can be decomposed into the product of the probability distributions on the maximal cliques of the graph divided over the product of the probability dis tributions on the clique intersections. The maximal cliques of the graph are placed into a join tree (or, in general, a forest) that shows how cliques interact with one another. The join or clique tree for the example in Figure 1 is given in Figure 2.\nFigure 2: Clique forest corresponding to the problem in Figure 1. Cliques and their intersections are shown.\nThus, the original problem is decomposed into smaller problems corresponding to the cliques of the triangu lated graph H'. Each smaller problem can be solved using IS, while distributions corresponding to the in tersections can be found by summing out the corre sponding distributions on the cliques. As we noted above, the time complexity of the IS algorithm grows exponentially with the size of the query. Thus, an al gorithm that solves a number of smaller problems in stead of solving a single large one may be considerably more efficient.\nNote that although the bucket elimination and the clique tree approaches are similar in flavor, they are different. Bucket elimination uses the knowledge of the structure to perform the main summation more efficiently, and 'the rest of the IS algorithm remains unchanged, including the necessity to cycle over all constraints in each iteration. In the clique tree method we essentially use the fact that the maxent distribu tion defines an MRF and, thus, can equivalently be estimated as a product of functions corresponding to cliques of the graph of the model. We use regular IS algorithm to estimate the functions corresponding to cliques and return the product of clique distributions as a result.\n470 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nTable 1: Characteristics of the Data Sets. k is the number of attributes, n is the number of records, Nl's is the number of 1's in the data, E(N1,,) = Nl's/n, Std(Nl's) is the standard deviation of the number of 1's in the record, Max(N1,,) is the maximum number of 1's in the record.\nk n Nl's E(Nl's) Max(Nl's) MS Web Data Set 294 32711 98654 3 2.5\n3.98 35 44 Retail 52 54887 224580 4.09\n4 Empirical Results\n4.1 Conjunctive Queries\nWe ran experiments on the two datasets: \"The Mi crosoft Anonymous Web\" dataset (publicly available at the UCI KDD archive) and a large proprietary dataset of consumer retail transactions. Both datasets contained binary transaction data. Before learning the models we analyzed the structure of the data and the itemsets that can be derived from it. Parameters of the datasets are given in the Table 1. The retail dataset is much more dense than the Microsoft Web Data. For more dense data sets, the larger itemsets will be more frequent, and the resulting graphs of the model will also be more dense.\nWe empirically evaluated ( 1) the independence model, (2) the Chow Liu tree model, and (3) the maxent model using the brute force, bucket elimination and clique tree methods. All experiments were performed on a Pentium III, 450 MHz machine with 128 Mb of memory. We generated 500 random queries for query sizes of 4, 6, and 8, and evaluated different models with respect to the average memory, online time, and error per Equation 1.\nTo select a query we first fixed the number of its vari ables nQ = 4, 6 or 8. Then we picked nQ attributes according to the probability of the attribute taking a value of \"1\" and generated a value for each selected attribute according to its univariate probability distri bution. Note, that negative values for the attributes are more likely in the sparse data than positive ones. Thus, generated queries typically had at most one pos itively initialized attribute.\nThe plots in Figure 3 show the dependence of the aver age relative error on the memory requirements for the model (or the model complexity, e.g., BIC) for the Mi crosoft Web data. The independence model using the least memory is the most inaccurate one. The Chow Liu tree model exhibits intermediate performance be tween the independence and the maxent models.\nNote that all the maxent models, i.e., brute force, bucket elimination and clique tree, have the same av erage error for a fixed query length since they es-\nQuery Size 4\n*\n0 0\nx lndepeooence Model ,11 * Chow-liu Tree Model o Maximum Entr\ufffd Model\n0 0 -3L_ _ __JL_ _ __t __ __! __ ____r:O,__ \ufffd __ _J 3 M U 5\nQuery Size 6 *\n0 0 0 0 0\n5.5\n0\n-3L_ _ ___l_ __ __L_ __ .J...._ _ __l __ _J_ _ ___j 3 3.5 4.5 5.5\nQuery SizeS\n* 0 0 0 0 0 0\n-3L_ _ ___l_ __ __L_ __ .J...._ _ __l __ _J_ _ ___j 3 3.5 4.5 5.5 Log ol the Memory Size\nFigure 3: Average relative error on the 500 random queries as a function of model complexity for the Mi crosoft Web data. The X -axis reflects the number of parameters in the models.\ntimate the same product form of the distribution. Thus, on this figure we only report results for a sin gle \"maximum entropy model\" (since all 3 produce the same estimates, but using different computational methods) . Circles that show results for the maxent model correspond to various values of the threshold\nT that was used to define itemsets. T took values 15, 30, 50, 60, 100 and 200. The higher the value ofT, the less information is supplied to the model and the less accurate the results are. Thus, the leftmost circle corresponds toT= 200 and the rightmost toT= 15.\nThe maxent model outperforms in terms of accuracy the tree model even when the amount of memory for\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 471\nthe maxent model is lower. This assertion holds true for all the query sizes.\nWe also measured the average online time taken by various models to generate the estimated query count. Figure 4 illustrates how the error depends on the on line time for all the models and query sizes 4, 6, and 8. Among the various models the independence model is the fastest but the least accurate of all. The Chow-Liu model fills in the large gap between the independence model and the cluster of the maxent models. On all three plots the diamond corresponding to the Chow Liu model has nearly the same x-coordinate, showing a very slow increase in processing time as query size grows. The quality of the tree model is not as good as for the maxent models. Recall that the amount of information supplied to the Chow-Liu model is com parable to, and for some of the threshold values is even greater than, that for the maxent models.\nThe brute force, clique tree and bucket elimination maxent models have smaller errors than the other models but it takes them longer to produce the esti mates. The error for all three types of maxent models is the same, and so is the y-coordinate for all points\ncorresponding to the same threshold T; the only dif ference is in the online time. The brute force version is the fastest on the queries of size 4. This is not surpris ing as both the clique tree and the bucket elimination methods have an overhead that dominates for short queries. The clique tree method becomes the fastest for query sizes 6 and 8.\nAs the threshold T decreases (the corresponding points on the plots go from left to right, top to bottom) and the number of itemsets (or memory size) increases, we see that the difference in the online time between the maxent algorithms that employ the graph structure and the brute force maxent decreases. We attribute this to the fact that when the number of itemsets in creases, so does the average density of the graph of the model and the average induced width.\nThe performance of various models relative to one an other on the retail data was qualititatively the same as for the Web data with the maxent model being again the most accurate but also the most compu tationally expensive. However, due to the fact that the retail data are much more dense, the memory re quirements and the online running times were gener ally much higher than for the Web data.\n4.2 Arbitrary Boolean Queries\nIt is straightforward to generalize the maxent approach to handle arbitrary Boolean queries. For a given arbi trary (not necessarily conjunctive) query we first esti mate the maxent distribution on the query variables, then transform the query to disjunctive normal form and evaluate the distribution on the disjuncts. This approach is worst-case exponential in the query size.\nWe have run experiments on arbitrary Boolean queries that we generated according to the algorithm de scribed above for conjunctive queries. The only dif ference is that the connective between two attributes was selected as either a disjunction or a conjunction by flipping a fair coin. Table 2 compares results on arbitrary and purely conjunctive queries (nQ is the query length, tp, Ct and ep are the average online time, query count and error across 200 runs of the al gorithms). The maxent models again enjoy a distinct advantage in accuracy over the independence models.\n5 Conclusions and Extensions\nWe have shown that (a) probabilistic models in gen eral, and (b) the MRF /maxent approach in particu lar, provide a useful general framework for approxi mate query answering on large sparse binary datasets. We have empirically analyzed the relative performance of various probabilistic models for this problem and\n472 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nshowed that given sufficient information about the data in the form of itemsets, the MRF /maxent ap proach is the most accurate of all the models. We also showed how bucket elimination and clique tree ideas can be employed for speeding up the learning of these models.\nThe work described in this paper allows for several possible extensions. For arbitrary Boolean queries one can in principle incorporate query structure directly into the IS algorithm or into bucket elimination. An other interesting problem is the issue of how thresh old T should be selected in practice, since the model complexity depends directly on T. Finally, there are several important open questions involving modeling of the query distribution: how should the query model be chosen? can it be learned from online user data? if it is known a priori, can it be profitably used in gen erating the approximate probability model, e.g., can one spend more resources on modeling parts of the data which have high probability of being queried?\nAcknowledgements\nThe research described in this paper was supported in part by NSF CAREER award IRI-9703120.\nReferences\nR. Agrawal and R. Srikant. Fast algorithms for min ing association rules in large databases. In Proceed ings of the Twentieth International Conference on Very Large Data Bases (VLDB'94}, pages 487 - 499, 1994.\nS. M. Aji and R. J. McEliece. The generalized distribu tive law. In Proc. 4th Int. Symp. Commun. Theory Appl., pages 135-146, 1997.\nA.L. Berger, S.A. Della Pietra, and V.J. Della Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-72, 1996.\nC. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE transactions on Information Theory, IT14(3):462-467, 1968.\nI. Csiszar and G. Tusnady. Information geometry and alternating minimization procedures. Statistics f3 Decisions, Supplement Issue, (1):205-237, 1984.\nJ. N. Darroch and D. Ratcliff. Generalized iterative scaling for log-linear models. Annals of Mathemati cal Statistics, 43:1470-1480, 1972.\nR. Dechter. Bucket elimination: A unifying framework for probabilistic inference. Uncertainty in Artificial Intelligence, pages 211-219, 1996.\nJ.M. Hammersley and P.E. Clifford. Markov fields on finite graphs and lattices. Unpublished manuscript, 1972.\nF. Jelinek. Statistical Methods for Speech Recognition. MIT Press, 1998.\nR. Jirousek and S. Preucil. On the effective implemen tation of the ipf procedure. Computational statistics and data analysis, pages 177-189, 1995.\nF.M. Malvestuto. A unique formal system for bi nary decompositions of database relations, probabil ity distributions and graphs. Information Sciences, 59:21-52, 1992.\nH. Mannila, D. Pavlov, and P. Smyth. Predictions with local patterns using cross-entropy. In Proc. of Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 357- 361, 1999.\nH. Mannila, H. Toivonen, and A. I. Verkamo. Effi cient algorithms for discovering association rules. In Knowledge Discovery in Databases, Papers from the 1994 AAAI Workshop (KDD'94}, pages 181 - 192. AAAI Press, 1994.\nJ. Pearl. Probabilistic Reasoning in Intelligent Sys tems: Networks of Plausible Inference. Morgan Kaufmann Publishers Inc., 1988.\nS. Della Pietra, V. Della Pietra, and J. Lafferty. Induc ing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19( 4): 380-393, 1997."}], "references": [{"title": "Fast algorithms for min\u00ad ing association rules in large databases", "author": ["R. Agrawal", "R. Srikant"], "venue": "In Proceed\u00ad ings of the Twentieth International Conference on Very Large Data Bases", "citeRegEx": "Agrawal and Srikant.,? \\Q1994\\E", "shortCiteRegEx": "Agrawal and Srikant.", "year": 1994}, {"title": "The generalized distribu\u00ad tive law", "author": ["S.M. Aji", "R.J. McEliece"], "venue": "In Proc. 4th Int. Symp. Commun. Theory Appl.,", "citeRegEx": "Aji and McEliece.,? \\Q1997\\E", "shortCiteRegEx": "Aji and McEliece.", "year": 1997}, {"title": "A maximum entropy approach to natural language processing", "author": ["A.L. Berger", "S.A. Della Pietra", "V.J. Della Pietra"], "venue": "Computational Linguistics,", "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C.K. Chow", "C.N. Liu"], "venue": "IEEE transactions on Information Theory,", "citeRegEx": "Chow and Liu.,? \\Q1968\\E", "shortCiteRegEx": "Chow and Liu.", "year": 1968}, {"title": "Information geometry and alternating minimization procedures", "author": ["I. Csiszar", "G. Tusnady"], "venue": "Statistics f3 Decisions, Supplement Issue,", "citeRegEx": "Csiszar and Tusnady.,? \\Q1984\\E", "shortCiteRegEx": "Csiszar and Tusnady.", "year": 1984}, {"title": "Generalized iterative scaling for log-linear models", "author": ["J.N. Darroch", "D. Ratcliff"], "venue": "Annals of Mathemati\u00ad cal Statistics,", "citeRegEx": "Darroch and Ratcliff.,? \\Q1972\\E", "shortCiteRegEx": "Darroch and Ratcliff.", "year": 1972}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["R. Dechter"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "Dechter.,? \\Q1996\\E", "shortCiteRegEx": "Dechter.", "year": 1996}, {"title": "Markov fields on finite graphs and lattices", "author": ["J.M. Hammersley", "P.E. Clifford"], "venue": "Unpublished manuscript,", "citeRegEx": "Hammersley and Clifford.,? \\Q1972\\E", "shortCiteRegEx": "Hammersley and Clifford.", "year": 1972}, {"title": "Statistical Methods for Speech Recognition", "author": ["F. Jelinek"], "venue": null, "citeRegEx": "Jelinek.,? \\Q1998\\E", "shortCiteRegEx": "Jelinek.", "year": 1998}, {"title": "On the effective implemen\u00ad tation of the ipf procedure", "author": ["R. Jirousek", "S. Preucil"], "venue": "Computational statistics and data analysis,", "citeRegEx": "Jirousek and Preucil.,? \\Q1995\\E", "shortCiteRegEx": "Jirousek and Preucil.", "year": 1995}, {"title": "A unique formal system for bi\u00ad nary decompositions of database relations, probabil\u00ad ity distributions and graphs", "author": ["F.M. Malvestuto"], "venue": "Information Sciences,", "citeRegEx": "Malvestuto.,? \\Q1992\\E", "shortCiteRegEx": "Malvestuto.", "year": 1992}, {"title": "Predictions with local patterns using cross-entropy", "author": ["H. Mannila", "D. Pavlov", "P. Smyth"], "venue": "In Proc. of Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Mannila et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mannila et al\\.", "year": 1999}, {"title": "Effi\u00ad cient algorithms for discovering association rules. In Knowledge Discovery in Databases, Papers", "author": ["H. Mannila", "H. Toivonen", "A.I. Verkamo"], "venue": "AAAI Workshop", "citeRegEx": "Mannila et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Mannila et al\\.", "year": 1994}, {"title": "Probabilistic Reasoning in Intelligent Sys\u00ad tems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1988\\E", "shortCiteRegEx": "Pearl.", "year": 1988}], "referenceMentions": [{"referenceID": 11, "context": "[Mannila et al., 1999] introduced the idea of using an MRF model based on frequent itemsets and a maximum entropy ( maxent) approach for query ap\u00ad proximation.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "We also investigate an approach based on tree\u00ad structured belief networks [Chow and Liu, 1968] that fills in the gap between the computationally inexpen\u00ad sive (but not very accurate) independence model and the relatively expensive maxent solution.", "startOffset": 74, "endOffset": 94}, {"referenceID": 3, "context": "This model [Chow and Liu, 1968] assumes that there are only pairwise dependencies between the variables and that the dependency graph on the attributes is a tree.", "startOffset": 11, "endOffset": 31}, {"referenceID": 13, "context": "Once the tree is learned, we can use a standard belief propagation al\u00ad gorithm [Pearl, 1988] to get the answer to a particular conjunctive query Q in time linear in nQ.", "startOffset": 79, "endOffset": 92}, {"referenceID": 8, "context": ", [Jelinek, 1998]) that the maxent distribution will have a special product form", "startOffset": 2, "endOffset": 17}, {"referenceID": 8, "context": ", [Jelinek, 1998]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 8, "context": "A high-level outline of the most computationally efficient version of the algorithm [Jelinek, 1998] is as follows.", "startOffset": 84, "endOffset": 99}, {"referenceID": 1, "context": ", [Aji and McEliece, 1997]) in Equation 7.", "startOffset": 2, "endOffset": 26}, {"referenceID": 13, "context": "To en\u00ad force chordality we use a graph triangulation algorithm [Pearl, 1988].", "startOffset": 63, "endOffset": 76}], "year": 2011, "abstractText": "Large sparse sets of binary transaction data with millions of records and thousands of attributes occur in various domains: cus\u00ad tomers purchasing products, users visiting web pages, and documents containing words are just three typical examples. Real-time query selectivity estimation (the problem of estimating the number of rows in the data satisfying a given predicate) is an important practical problem for such databases. We investigate the application of probabilis\u00ad tic models to this problem. In particular, we study a Markov random field (MRF) ap\u00ad proach based on frequent sets and maximum entropy, and compare it to the independence model and the Chow-Liu tree model. We find that the MRF model provides substantially more accurate probability estimates than the other methods but is more expensive from a computational and memory viewpoint. To alleviate the computational requirements we show how one can apply bucket elimination and clique tree approaches to take advantage of structure in the models and in the queries. We provide experimental results on two large real-world transaction datasets.", "creator": "pdftk 1.41 - www.pdftk.com"}}}