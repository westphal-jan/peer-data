{"id": "1301.6715", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "My Brain is Full: When More Memory Helps", "abstract": "we do continuous procedure of enabling satisfactory operational - horizon policies for pomdps whose minimal expected loss metric. possible policies considered are { em free false - tail policies with objective memory } ; a policy is a path from the space or observation - memory pairs to the horizon of e - memeory pairs ( the policy updates the shortest tract it occupies ), and the number of partial memory states is a parameter of the rule implementing the initial - finding algorithms. the systems considered together maintain preliminary implementations after three search heuristics : simulation search, simulated annealing, and genetic algorithms. some compare maximum outcomes to each agent and to achieve optimal policies for each instance. we compare failure times via better policy and of independent steady trading algorithm for pomdps developed by hansen that typically generate a discrete - processor policy - - - despite previous computation of the target for optimization memory operations. the probability of the best policy can significantly decline although your knowledge consuming iteration increases, up to increased amount efficient for requiring improved finite - memory policy. ~ most surprising finding is that more cache works in another way : keeps longer memory than sums needed among an infinite policy, therefore algorithms are more likely both converge to optimal - visibility policies.", "histories": [["v1", "Wed, 23 Jan 2013 15:59:22 GMT  (272kb)", "http://arxiv.org/abs/1301.6715v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["christopher lusena", "tong li", "shelia sittinger", "chris wells", "judy goldsmith"], "accepted": false, "id": "1301.6715"}, "pdf": {"name": "1301.6715.pdf", "metadata": {"source": "CRF", "title": "My Brain is Full: When More Memory Helps", "authors": ["Christopher Lusena", "Tong Li", "Shelia Sittinger", "Chris Wells", "Judy Goldsmith"], "emails": ["@cs."], "sections": [{"heading": null, "text": "We consider the problem of finding good finite-horizon policies for POMDPs under the expected reward metric. The policies con sidered are free finite-memory policies with limited memory; a policy is a mapping from the space of observation-memory pairs to the space of action-memory pairs (the policy up dates the memory as it goes), and the num ber of possible memory states is a parameter of the input to the policy-finding algorithms. The algorithms considered here are prelimi nary implementations of three search heuris tics: local search, simulated annealing, and genetic algorithms. We compare their out comes to each other and to the optimal poli cies for each instance. We compare run times of each policy and of a dynamic programming algorithm for POMDPs developed by Hansen that iteratively improves a finite-state con troller - the previous state of the art for finite memory policies. The value of the best policy can only improve as the amount of memory increases, up to the amount needed for an optimal finite-memory policy. Our most surprising finding is that more memory helps in another way: given more memory than is needed for an optimal policy, the algo rithms are more likely to converge to optimal valued policies.\n1 INTRODUCTION\nPartially observable Markov decision processes (POMDPs) are mathematical models of controlled systems where the controller is uncertain about both the state of the system and about the effects of actions\n{I usena, tongli,smsittO ,chrisw ,goldsmi t} @cs. uky.edu\non the system. While this sounds like a less than optimal state of affairs, it is unfortunately an accurate model of many real systems, from robotic navigation to economics. (For a discussion of applications of POMDPs, see (Cassandra, 1998).)\nSlightly more formally, a POMDP is a tuple M = (S, A, 0, t, o, r), where S is a finite set of states, A is the finite set of actions, 0 the finite set of possible ob servations, t the probabilistic transition function map ping S x A to S, o the observation function mapping S to 0 (deterministically, for this paper), and r the reward function, mapping S x A to the real numbers. Our algorithms require an initial belief state; for the grid worlds, for instance, it is a uniform distribution over all the states.\nIt is unfortunate that POMDPs model so many real world situations because it is computationally in tractable to find or indeed even to store optimal con trol policies for general POMDPs. It has been shown that the optimal policies are generally those that take into account the full history of the system; because the number of possible histories increases exponen tially with each epoch considered - and is uncount able in the limit - it is infeasible to store policies as tables. Fortunately, some policies can be repre sented finitely, and sometimes compactly, as func tions. However, finding such an optimal policy for a finite horizon is PSPACE-hard (Papadimitriou & Tsitsiklis, 1987); for an infinite horizon, it is un computable (Madani, 1998). Furthermore, there can only be a provably good polynomial-time approxi mation algorithm (a so-called \"\ufffd;-approximation\") if P=PSPACE (Lusena et al., 1998). Stationary policies, those that take into account only the current observation, are computationally sim pler: finding the optimal stationary policy for a POMDP is only NP-hard (Papadimitriou & Tsit siklis, 1987). However, this is still not considered tractable. U nsurprisingly, there can only be a prov ably good polynomial-time approximation algorithm if\nNP=P (Lusena et al., 1998). Thus, these policies are unsatisfactory both because of their non-optimality and the difficulty of finding or approximating them.\nThere is, fortunately, a compromise. In this work, we consider finite memory policies. These policies were introduced in Sondik's thesis (Sondik, 1971), but the most general form of finite memory policies has received little attention until recently. (See, for instance, (Hansen, 1998a; Hansen, 1998b; Meuleau et a!., 1999).)\nFinite memory can be used to record the last k states seen; this restriction, finite-history policies, was exten sively explored in the '70's and '80's (Lovejoy, 1991). Memory could instead be used to record the time the system has run (for finite horizon problems), yielding time-dependent policies. We consider free finite mem ory policies. We allow the policy-finding algorithm to determine the use of the memory without any restric tion except a limit on the amount of memory used. This restriction forces the complexity of the problem from PSPACE-hard to NP-hard.\nFormally, a free finite memory policy with M memory states for POMDP M = (S,A,O,t,o,r) is a function 1r f : 0 x M \ufffd Ax M, mapping each pair (observation, memory state) to a pair (action, memory state), where M is a finite set of states, i.e., a finite \"scratch\" mem ory.\nEric Hansen has done some excellent work on finite memory policies. Hansen's algorithms (Hansen, 1998a; Hansen, 1998b) represent policies as finite-state con trollers: each state of the controller represents an ac tion (not necessarily uniquely), and transitions are made based on the next observation. This is another form of restriction on the use of the finite memory. With finite-state controllers, the transition from one action-state to another depends only on the observa tion; with free finite memory, the transition may de pend on the observation and the memory update.\nHansen's work has a major advantage over the other current work on POMDPs: it is significantly faster. Furthermore, his algorithms, which are probabilis tic, usually find good policies. However, we expect that our algorithms will scale significantly better than Hansen's.\nThe biggest difference between our work and that of others such as Hansen is that we are computing policies given a predetermined bound on the number of possible memory states.\nNote that a k-state free finite memory policy for a POMDP M = (S,A, 0, t,o,r) can be considered a sta tionary policy on a new POMDP, M', where the state space for M' is S x M, where M is the set of k memory\nMy Brain is Full 375\nstates. Thus the complexity results for stationary poli cies hold for free finite memory policies with limited memory as well.\nOur idea of applying search heuristics to the space of free finite memory policies for a POMDP has been ap plied by Littman (Littman, 1994) and others to the problem of finding good stationary policies. However, in many instances (including those we discuss in Sec tion 6.1) finite-memory policies out-perform the op timal stationary policy significantly. Meuleau et al. have applied search heuristics to finding good finite state controllers of a fixed size in a learning theoretic context (Meuleau et a!., 1999). Their work is fairly similar to ours, but our preliminary results seem to be better, either because we assume knowledge of the model or because we are working with a better update heuristic in our local search algorithms.\nA major question that this work begins to explore is, How much finite memory should be used? What we show in Section 6 is that the questions of 'How much memory is optimal for a policy' and 'How much mem ory is optimal for a search algorithm' may have differ ent answers.\nIn particular, it seems that giving a search algorithm extra memory to explore greatly increases its probabil ity of finding an optimal policy. Users are thus faced with a common trade-off, since increasing the num ber of possible memory states greatly increases com putation time. Extra memory states increase the probability of finding a better policy, at a cost of more computation time.\nWe should vehemently note that the increased time we discuss is still significantly less than the time needed to compute the optimal history-dependent policy for any but the smallest examples considered, even us ing the state-of-the-art exact policy finding algorithms such as Incremental Pruning (Zhang & Liu, 1997; Cas sandra et a!., 1997) or Hansen's Policy Iteration algo rithms (Hansen, 1998a; Hansen, 1998b).\nThe work discussed here is based on three search heuristics: local search (Sec. 2), genetic algorithms (Sec. 3), and simulated annealing (Sec. 4). The re sulting policies were compared with the optimal policy for each memory size, as calculated by a branch and bound algorithm. In addition, for each POMDP we calculated a finite-state controller using Hansen's al gorithm. We cannot directly compare the finite-state controller to policies found by our algorithms (the con troller represents an infinite horizon policy, whereas ours are for finite horizons, and Hansen's algorithms assume a discounted reward whereas ours assume a to tal reward). We will be considering discounted rewards in the next paper. In the meantime, we show that even\n376 Lusena, Li, Sittinger, Wells, and Goldsmith\nrunning one of our algorithms multiple times and tak ing the best outcome is often significantly faster than running Hansen's algorithm.\n2 LOCAL SEARCH\nLocal search is a technique well-known at least since Biblical times (see Genesis 26:14-33) for finding an op timal value of a function by incremental improvements. A neighborhood structure is defined on the space of solutions, and the algorithm begins with an initial so lution. While there is an improved solution in the neighborhood of the current solution, the current so lution is updated to some better neighbor. Much of the art of designing good local search algorithms goes into the proper definition of neighborhood and the update heuristic.\n2.1 RELATED WORK\nThere are certain parallels between our work and the work done on history-dependent policies, although we find some of the apparent similarities misleading. For instance, both policy and value iteration algorithms are forms of local search: they find local improve ments to the policy under consideration, until that is no longer possible. The difference between the compu tational needs of those algorithms and ours is that our policies tend to be significantly smaller than theirs, so evaluating neighboring policies is faster.\nMeuleau, et a!., have studied a global branch and bound search algorithm for finding the best determin istic policy graph (similar to our branch and bound al gorithm) and a local search algorithm based on gradi ent descent for finding the best stochastic policy graph. Based on the data in (Meuleau et al., 1999), it ap pears that this is not as efficient as our local search algorithms, for several reasons. It is set in a learning theoretic context and it finds finite-state controllers instead of free finite memory policies. We also specu late that they have chosen a less efficient update rule than we have.\n2.2 OUR ALGORITHM\nThe update rule used in our algorithm is randomized first improvement. For each policy, there is a well defined ordering on its neighbors. A random initial policy and one of its neighbors are chosen at random. That neighbor is defined to be the first in the list of neighbors (which is wrapped around to include all neighbors). Once that list is defined, the first improve ment to the current policy that appears on the list is chosen. This process is repeated until no better neigh bor exists, meaning we have found a local maximum.\n3 GENETIC ALGO R ITHM S\nA genetic algorithm is a randomized search algorithm where there is a \"population\" of solutions at each time step. The population evolves via mutation, re production, and fitness selection. Reproduction in volves a combination of the \"phenotypes\" of two mem bers of the population. This formalism was introduced in (Holland, 1975).\n3.1 RELATED WORK\nLanzi has used genetic algorithms as a randomizing technique in conjunction with a learning algorithm to learn a particular type of finite-memory policy (Lanzi, 1998a). He considers two extensions to Wilson's xes system (Wilson, 1995): XeSM (Lanzi, 1998b) and Xe SMH (Lanzi, 1998a).\nxes is a Q-learning-like technique which learns a memoryless policy for an environment via separate exploration and exploitation phases. XeSM extends xes by adding bits of (unconstrained) memory to the policy. Unfortunately, XeSM does not always con verge well when there are states with the same obser vation that require different actions (and thus different memory), since the memory-action pairs are tightly coupled. Therefore, in XeSMH this coupling is loos ened in the exploration phase, and additional heuris tics are applied.\nThus, XeSMH uses free finite memory; like our al gorithms, it uses genetic algorithms to learn a pol icy. Lanzi's work, however, uses an additional con straint and additional heuristics because he is solv ing a learning-theoretic problem rather than a control theoretic one. Therefore it is difficult to compare the outcomes of these heuristics.\nLin, et a!., use genetic algorithms to construct sets of vectors that approximate the value function for the k step finite horizon discounted history-dependent pol icy (Lin et al., 1998). However, they are solving a very different problem from ours.\n3.2 OUR ALGORITHM\nIn our model, a policy corresponds to a phenotype with one \"chromosome.\" A decision for a given observation memory pair is a single \"gene,\" and a particular deci sion (action-memory pair) is an \"allele\" for that gene. For a particular POMDP, a population size must first be determined. A small population will prevent diver sity and produce poor policies, while a large popula tion will cause the algorithm to take too long. The population size should therefore be at least partially dependent on the size of the policy space. We arbitrar-\nily set 30 as the smallest population size, and make all populations grow with the log of the size of the policy space, since it is this space in which we are searching. Because we expect more memory to deliver better poli cies, however, the size of the policy space is determined under the assumption that no memory will be used. So our population grows linearly with the number of observations in the POMDP, and logarithmically with the actions and with the number of memory states.\nThe population is filled initially at random. During a generation, the fitness of each phenotype (policy) is determined by evaluating the policy over a finite hori zon. The values of eacb policy are used to determine the policies' fitnesses. Any policy below two standard deviations of the population's mean of fitness is dis carded. Any policy above two standard deviations of the mean has its fitness set to two standard deviations above the mean, in order to prevent a very good pol icy from destroying the diversity of the matring pool. We thus eliminate all outliers. Once the fitness of each policy has been determined, policies are selected based on their fitness to fill the new mating pool.\nThe mating pool is altered according to the cross-over rate and the mutation rate. We have not yet studied the effects of different rates on generated policies, so we use a preliminary cross-over rate of .5, and a prelim inary mutation rate of .005. The number of pairs that are crossed over is equal to the population size times the cross-over rate. A random observation and a ran dom memory state are selected. All decisions above the selected observation and memory state, and all decisions below the selected observation and memory state, are swapped between the two policies. No two policies are selected more than once for cross-over per generation. After cross-over, p \u00b7 m policies are selected for mutation, where p is the population size and m is the mutation rate. In a mutation, a random decision in a policy is selected, and has its action and memory state randomly changed to a new action/memory-state pair.\nTermination is determined during selection. If the same best policy occurs more than a set number of times (we use a value of 10), or the best policy occurs half this number of times and the standard deviation of the population drops below a given threshold (we use a threshold of .0001), improvement is not taking place. The genetic algorithm halts under either of these con ditions.\n4 SIMULATED ANNEALING\nSimulated annealing is another heuristic for improv ing local search methods. It was introduced by Kirk patrick, et a!. (Kirkpatrick et al., 1983). The method\nMy Brain is Full 377\nis applied by selecting a starting \"temperature\" - the higher the temperature, the more frequently a spon taneous change of state occurs in the otherwise local search. As the changes become smaller, the tempera ture is reduced, and the search becomes a purely local search. The expectation is that the initial high rate of change will help prevent convergence to a suboptimal local optimum.\n4.1 RELATED WORK\nThere are many techniques for approximating the value of a state under a given history-dependent pol icy. Some of those use randomized techniques either implicitly, in learning algorithms for example, or ex plicitly. (See (Lin et al., 1998) for an example of sim ulated annealing applied to this problem.)\n4.2 OUR ALGORITHM\nIn this experiment, the initial temperature was chosen as 95, giving a 95% probability of change at each ini tial local search test. The temperature was decreased by one degree at each iteration. This process was con tinued until a local maximum was achieved, and the temperature dropped to 0.\n5 FINITE-STATE CONTROLLERS\nA finite-state controller is a finite state automaton representing a policy. The states of the automa ton correspond to actions and the transitions of the automaton correspond to observations. There is a well-understood correspondence between vectors of the value function as constructed in value iteration on history-dependent policies for POMDPs and states of a finite controller (Hansen, 1998a).\nThe difference between a finite-state controller and a free finite memory policy is that for a finite-state con troller, there is a tight link between the states of the controller (the policy's memory) and action, whereas free finite memory policies can decouple this link.\n5.1 RELATED WORK\nHansen has several algorithms for finding finite-state controllers for POMDPs. Although there are some sig nificant differences between his work and ours (infinite horizon vs. finite-horizon, controllers vs. free finite memory), we have implemented his policy iteration algorithm for comparison purposes.\nIn Hansen's policy iteration algorithm (Hansen, 1998a), a policy is represented by a finite-state con troller. Finding an c:-optimal policy for infinite hori-\n378 Lusena, Li, Sittinger, Wells, and Goldsmith\nzon POMDPs is done by iteratively improving a finite state controller, where each machine state corresponds to a vector of the value function and is associated with an action choice. Machine state transitions are labeled by observations. Explicitly representing a policy as a finite-state controller makes the policy evaluation very easy. Policy improvement involves adding, changing, and pruning machine states. Because it improves the value function both by the dynamic programming up date and by policy evaluation, policy iteration outper forms value iteration greatly.\nMeuleau et al. present another approach for solving POMDPs by searching in the policy space (Meuleau et al., 1999). In their work, policies are also repre sented by finite-state controllers (policy graphs is the term they use). Like us, they consider a priori con straints on the amount of memory used; unlike us, their memory is restricted to the form of a finite con troller. Their evaluation algorithm is based on an MDP defined by the cross-product of the POMDP and a policy graph; this is much faster than the evaluation algorithms used in our work. Since they impose ex tra constraints on the policy graph, the policy search space is reduced, so large POMDPs can be solved sig nificantly more quickly than with classical approaches.\nThere are other restrictions on finite memory. For in stance, in (Wiering & Schmidhuber, 1997), Weiring and Schmidhuber introduce a notion of HQ-learning that breaks down goal POMDPs into separate sub problems, each of which is solved by a separate agent. These agents can be thought of as corresponding to different memory states. However, this decomposition does not directly correspond to our finite memory poli cies, since their overall policy is limited to using each agent once, in sequence.\n5.2 COMPARISONS WITH OUR WORK\nWe have implemented Hansen's policy iteration algo rithm, and compared our search techniques with it. In those cases where his algorithm halted in less than two hours, the time was comparable, or in some cases better, than most of our algorithms. However, for all the larger instances of POMDPs (for instance, mccal lum, sutton, aloha10 and aloha30) the process either continued past our two-hour cut-off (in the case of mc callum, we stopped it at approximately 2,407 minutes) or died due to numerical instabilities 1.\n'Hansen ran 4x3.95 in his dissertation. It stopped after 10,681 seconds. It is not clear that our implementation would do so, since he used a superior LP programming package, CPLEX, and some instance-tuning of parameters.\nWe have run our algorithms on ten POMDPs (five from Cassandra's database (Cassandra, 1997-9), four others from the literature, and an additional small test case: 1D, 4x3.95, 4x3.C0.95, aloha.10, aloha.30; Sutton's gridworld and McCallum's maze (Littman, 1994), Maze7 and MazelO (Lanzi, 1998a), and test1), for 1, 2, 3, and 4 memory states. In addition, we con sidered 1-10 memory states for 1D and 1-7 memory states for Maze7 and MazelO. We ran each algorithm 100 times on each input and averaged the results where applicable.\n6.2 TIME\nWhen we consider the run time graphs for all our ex amples, we discover that the curves from the different POMDPs do not match. However, for each instance we got results similar to those shown in Figure 1 for aloha.lO. However, usually the genetic algorithm ran more slowly, on average, than local search or simulated annealing.\nOf the 10 POMDPs considered here, 7 can be char acterized as goal oriented. One measure of computa tional effort is to count the number of policy evalua tions for each instance of local search. In Figure 2, we plot the average number of evaluations of policies ver sus the log of the size of the policy space. The plotted line is a quartic polynomial fitted to the data using GNUPLOT, in order to provide a sense of the rela tionship between the size of the POMDP and the time taken to solve it. (The error bars indicate one stan dard deviation from the mean in all the time graphs.) Notice that, because of the restrictions placed on the amount of memory used, the size of the policy space is bounded by 2\u00b0(nlogn), where n is the size of the input, so the log of the size of the policy space is polynomial\n-;\nNumber of Evaluation vs. Policy Space\n30000\n\ufffd .s 20000 \" \u2022 ,\n\ufffd 10000\nlog2 ( I Policy Space I )\ninn.\nBased on all of our runs, local search appears to run in time polynomial in the size of the input, on average over the examples we have tested.\nAs we see in Figure 3, simulated annealing requires roughly the same number of evaluations as local search. Since simulated annealing performs slightly better than local search in most instances, the slight increase in time seems worth it.\nOn the other hand, with our current genetic algorithm, there is no tightly fitted curve for our data with more than one POMDP.\nU nsurprisingly, local search is faster than either ge netic algorithms or simulated annealing.\nMy Brain is Full 379\n6.3 EXPECTATION OF FINDING THE OPTIM AL POLICY\nThere is no consistent predictor of which search algo rithm is most likely to find the optimal policy. In our current implementations, the genetic algorithm is the least likely for most POMDPs; we expect to improve that implementation substantially in upcoming exper iments. Figure 4 shows our results.\n6.4 MORE MEMORY IS BETTER!\nThe most striking result of our research is that searches run with extra memory, i.e., more memory than is needed to get an optimal policy, are still more likely to actually find an optimal policy than searches that are constrained to policies with no \"extra\" memory. For instance, McCallum's maze has an optimal policy with only two memory states, yet Figure 5 shows that the optimal value is found more often when the algorithms are run with more memory.\nOne explanation of this phenomenon is geometric: if there is a local optimum between the starting pol icy and the global optimum and only one path from the start to the global optimum, a local search will get stuck at the local optimum. However, the extra memory functions as an additional dimension, allow ing multiple paths from the initial policy to the global optimum. (Imagine trying to do hill climbing on the curve x3- x from the point (-1, 0): in two dimen sions, one gets stuck on the hump at x = -1/3, but in three dimensions one might get a surface which does not peak at that x value for all z values, so one might be able to work around that local hump.)\nThe flip side of this discovery is that adding mem ory slows down computation, since it increases the size of the policy space. Therefore, one is faced with the time-quality trade-off: allowing the search algorithm to search a larger space of policies dramatically in creases the probability of finding an optimal policy, but will take longer to converge.\nThis effect was consistent for all the POMDPs consid ered and for all three search algorithms. We demon strate it here in Figure 5.\n7 ONGOING WORK\nSeveral extensions and improvements can and should be made to the preliminary implementations used in this work. In (Wells et al., 1999) the positions of the cells in the policies are made maleable, enabling the use of the inversion genetic operator. Additionally, the values of the genetic parameters (such as the cross over and mutation rates) are studied in relation to the\n380 Lusena, Li, Sittinger, Wells, and Goldsmith\nFigure 5: Policies Found (McCallum's Maze)\ns \"\"ObO s .<: Q) = u-<= ..., \u00b7\ufffd ..., <a-\u00a3 \"'\"<a \ufffd\u00b7\ufffd \u00b7;::: -Q) Q) .... \ufffd u1a ::l = \ufffd::g, .SJS s = a.>Ci'i< 0< memory\n1 state\n2 states\n3 states\n4 states\nColour Policy Value < 7.63 7.63 (Optimal for 1 Memory State) > 7.63 \ufffd 9 > 9 \ufffd 10 > 10 < 11.27 11.27 (Optimal for 2+ Memory States)\nquality of generated policies and the speed with which they are produced. Similarly, a better cooling scheme in simulated annealing could lead to converging more quickly to better policies. We are also experimenting with other search space explorations in local search. In a submitted paper, (Lusena et al., 1999), we describe an extension to this work in which we evaluate policies with infinite horizons and discounted rewards.\nA cknowledgements\nThis research supported in part by NSF grant CCR9610348. The authors would like to thank Eric Hansen for pointers to relevant literature, Rafi Finkel and Beth Goldstein for their guide through biblical searches, and Emily Hendren for her help in editing.\nReferences\nCassandra, A. 1997-9. Anthony Cassandra's POMDP Page. http://www.cs.brown.edu/people/arc /research/pomdp.html.\nCassandra, A. 1998 (September). A Survey of POMDP Applications. Tech. rept. MCC-INSL-111-98. Mi-\ncroelectronics and Computer Technology Corpo ration (MCC), Austin, Texas. Presented at the 1998 AAAI Fall Symposium: Planning with par tially observable Markov decision processes.\nCassandra, A., Littman, M.L., & Zhang, N.L. 1997. Incremental Pruning: A Simple, Fast, Exact Method for Partially Observable Markov Decision Processes. Pages 54-61 of: Geiger, D., & Shenoy, Prakash Pundalik (eds), Proceedings of the 13th Conference on Uncertainty in Artificial Intelli gence (UAI-97). San Francisco: Morgan Kauf mann Publishers.\nHansen, E. 1998a. Finite-Memory Control of Par tially Observable Systems. Ph.D. thesis, Dept. of Computer Science, University of Massachussets at Amherst.\nHansen, E. 1998b. Solving POMDPs by searching in policy space. In: Proceedings of AAAI Conference Uncertainty in AI.\nHolland, J.H. 1975. Adaptation in Natural and Arti ficial Systems. Ann Arbor, MI: Univ. Michigan Press.\nKirkpatrick, S., C.D. Gelatt, Jr., & Vecchi, M.P. 1983. Optimization by Simulated Annealing. Science, 220(4598), 671-680.\nLanzi, Pier Luca. 1998a (October). Adding Memory to Wilson's XCS Classifier System to Learn in Partially Observable Environments. Presentation Notes from AAAI 1998 Fall Symposium Series: Planning with Partially Observable Markov Deci sion Processes.\nLanzi, Pier Luca. 1998b. Adding Memory to XCS. In: IEEE Conference on Evolutionary Computation. IEEE Press.\nLin, Alex Z.-Z., Bean, J. C., & C. C. White, III. 1998. A Hybrid Genetic/Optimization Algorithm for Fi nite Horizon Partially Observed Markov Decision Processes. Tech. rept. Dept. oflndustrial and Op erations Engineering, The University of Michigan.\nLittman, M.L. 1994. Memoryless policies: Theoretical limitations and practical results. In: From Ani mals to Animats 3: Proceedings of the Third In ternational Conference on Simulation of Adaptive BehaviorMIT Press, for Lecture Notes in Com puter Science.\nLovejoy, W.S. 1991. A Survey of Algorithmic Meth ods for Partially Observed Markov Decision Pro cesses. Annals of Operations Research, 28, 47-66.\nMy Brain is Full 381\nLusena, C., Goldsmith, J., & Mundhenk, M. 1998. Nonapproximability Results for Markov Decision Processes. Tech. rept. 274-98. University of Ken tucky Department of Computer Science.\nLusena, Christopher, Wells, Chris, Goldsmit, Judy, Sittinger, Shelia, & Li, Tong. 1999. Finite Memory Policies for POMDPs. (submitted).\nMadani, Omid. 1998. On the Computability of Infinite Horizon Partially Observable Markov Decision Processes. Appeared in AAAI fall symposium on Planning with POMDPs.\nMeuleau, Nicolas, Kim, Kee-Eung, Hauskrecht, Milos, Cassandra, Anthony R., & Kaelbling, Leslie P. 1999. Solving POMDPs by Searching the Space of Finite Policies. submitted.\nPapadimitriou, C.H., & Tsitsiklis, J.N. 1987. The Complexity of Markov Decision Processes. Math ematics of Operations Research, 12(3), 441-450.\nSondik, E. 1971. The Optimal Control of Partially Ob servable Markov Processes. Ph.D. thesis, Stanford University.\nWells, Chris, Goldsmith, Judy, & Lusena, Christopher. 1999. Genetic Algorithms for Approximating So lutions to POMDPs. (In Preparation).\nWiering, Marco, & Schmidhuber, Jiirgen. 1997. HQ Learning. Adaptive Behavior, 6:2.\nWilson, Stewart W. 1995. Classifier Fitness Based on Accuracy. Evolutionary Computation, 3(2), 148- 175.\nZhang, N.L., & Liu, W. 1997. Region-Based Ap proximations for Planning in Stochastic Domains. Pages 472-480 of: Geiger, Dan, & Shenoy, Prakash Pundalik (eds), Proceedings of the 13th Conference on Uncertainty in Artificial Intelli gence (UAI-97). San Francisco: Morgan Kauf mann Publishers."}], "references": [{"title": "Anthony Cassandra's POMDP Page. http://www.cs.brown.edu/people/arc /research/pomdp.html", "author": ["A. Cassandra"], "venue": null, "citeRegEx": "Cassandra,? \\Q1997\\E", "shortCiteRegEx": "Cassandra", "year": 1997}, {"title": "Finite-Memory Control of Par\u00ad", "author": ["E. Hansen"], "venue": null, "citeRegEx": "Hansen,? \\Q1998\\E", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Solving POMDPs by searching", "author": ["E. Hansen"], "venue": null, "citeRegEx": "Hansen,? \\Q1998\\E", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Adaptation in Natural and Arti\u00ad", "author": ["J.H. Holland"], "venue": null, "citeRegEx": "Holland,? \\Q1975\\E", "shortCiteRegEx": "Holland", "year": 1975}, {"title": "1998a (October). Adding Memory", "author": ["Lanzi", "Pier Luca"], "venue": null, "citeRegEx": "Lanzi and Luca.,? \\Q1998\\E", "shortCiteRegEx": "Lanzi and Luca.", "year": 1998}, {"title": "Adding Memory to XCS", "author": ["Lanzi", "Pier Luca."], "venue": "In:", "citeRegEx": "Lanzi and Luca.,? 1998b", "shortCiteRegEx": "Lanzi and Luca.", "year": 1998}, {"title": "Memoryless policies: Theoretical", "author": ["M.L. Littman"], "venue": null, "citeRegEx": "Littman,? \\Q1994\\E", "shortCiteRegEx": "Littman", "year": 1994}, {"title": "A Survey of Algorithmic Meth\u00ad ods for Partially Observed Markov Decision Pro\u00ad", "author": ["W.S. Lovejoy"], "venue": null, "citeRegEx": "Lovejoy,? \\Q1991\\E", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "Nonapproximability Results for Markov Decision Processes", "author": ["C. Lusena", "J. Goldsmith", "M. Mundhenk"], "venue": "Tech. rept. 274-98", "citeRegEx": "Lusena et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lusena et al\\.", "year": 1998}, {"title": "Finite\u00ad Memory Policies for POMDPs", "author": ["Lusena", "Christopher", "Wells", "Chris", "Goldsmit", "Judy", "Sittinger", "Shelia", "Li", "Tong"], "venue": null, "citeRegEx": "Lusena et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lusena et al\\.", "year": 1999}, {"title": "On the Computability of Infinite\u00ad Horizon Partially Observable Markov Decision Processes", "author": ["Madani", "Omid."], "venue": "Appeared in AAAI fall symposium on Planning with POMDPs.", "citeRegEx": "Madani and Omid.,? 1998", "shortCiteRegEx": "Madani and Omid.", "year": 1998}, {"title": "Solving POMDPs by Searching the Space of Finite Policies", "author": ["Meuleau", "Nicolas", "Kim", "Kee-Eung", "Hauskrecht", "Milos", "Cassandra", "Anthony R", "Kaelbling", "Leslie P"], "venue": null, "citeRegEx": "Meuleau et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 1999}, {"title": "The Complexity of Markov Decision Processes", "author": ["C.H. Papadimitriou", "J.N. Tsitsiklis"], "venue": "Math\u00ad ematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "The Optimal Control of Partially Ob\u00ad servable Markov Processes", "author": ["E. Sondik"], "venue": "Ph.D. thesis, Stanford University.", "citeRegEx": "Sondik,? 1971", "shortCiteRegEx": "Sondik", "year": 1971}, {"title": "Genetic Algorithms for Approximating So\u00ad lutions to POMDPs. (In Preparation)", "author": ["Wells", "Chris", "Goldsmith", "Judy", "Lusena", "Christopher"], "venue": null, "citeRegEx": "Wells et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Wells et al\\.", "year": 1999}, {"title": "Classifier Fitness Based on Accuracy", "author": ["Wilson", "Stewart W."], "venue": "Evolutionary Computation, 3(2), 148175.", "citeRegEx": "Wilson and W.,? 1995", "shortCiteRegEx": "Wilson and W.", "year": 1995}, {"title": "Region-Based Ap\u00ad proximations for Planning in Stochastic Domains", "author": ["N.L. Zhang", "W. Liu"], "venue": "Proceedings of the 13th Conference on Uncertainty in Artificial Intelli\u00ad", "citeRegEx": "Zhang and Liu,? \\Q1997\\E", "shortCiteRegEx": "Zhang and Liu", "year": 1997}], "referenceMentions": [{"referenceID": 8, "context": "Furthermore, there can only be a provably good polynomial-time approxi\u00ad mation algorithm (a so-called \"\ufffd;-approximation\") if P=PSPACE (Lusena et al., 1998).", "startOffset": 134, "endOffset": 155}, {"referenceID": 8, "context": "NP=P (Lusena et al., 1998).", "startOffset": 5, "endOffset": 26}, {"referenceID": 13, "context": "These policies were introduced in Sondik's thesis (Sondik, 1971), but the most general form of finite memory policies has received little attention until recently.", "startOffset": 50, "endOffset": 64}, {"referenceID": 7, "context": "Finite memory can be used to record the last k states seen; this restriction, finite-history policies, was exten\u00ad sively explored in the '70's and '80's (Lovejoy, 1991).", "startOffset": 153, "endOffset": 168}, {"referenceID": 6, "context": "Our idea of applying search heuristics to the space of free finite memory policies for a POMDP has been ap\u00ad plied by Littman (Littman, 1994) and others to the problem of finding good stationary policies.", "startOffset": 125, "endOffset": 140}, {"referenceID": 11, "context": "Based on the data in (Meuleau et al., 1999), it ap\u00ad pears that this is not as efficient as our local search algorithms, for several reasons.", "startOffset": 21, "endOffset": 43}, {"referenceID": 3, "context": "This formalism was introduced in (Holland, 1975).", "startOffset": 33, "endOffset": 48}, {"referenceID": 11, "context": "present another approach for solving POMDPs by searching in the policy space (Meuleau et al., 1999).", "startOffset": 77, "endOffset": 99}, {"referenceID": 6, "context": "30; Sutton's gridworld and McCallum's maze (Littman, 1994), Maze7 and MazelO (Lanzi, 1998a), and test1), for 1, 2, 3, and 4 memory states.", "startOffset": 43, "endOffset": 58}, {"referenceID": 14, "context": "In (Wells et al., 1999) the positions of the cells in the policies are made maleable, enabling the use of the inversion genetic operator.", "startOffset": 3, "endOffset": 23}, {"referenceID": 9, "context": "In a submitted paper, (Lusena et al., 1999), we describe an extension to this work in which we evaluate policies with infinite horizons and discounted rewards.", "startOffset": 22, "endOffset": 43}], "year": 2011, "abstractText": "We consider the problem of finding good finite-horizon policies for POMDPs under the expected reward metric. The policies con\u00ad sidered are free finite-memory policies with limited memory; a policy is a mapping from the space of observation-memory pairs to the space of action-memory pairs (the policy up\u00ad dates the memory as it goes), and the num\u00ad ber of possible memory states is a parameter of the input to the policy-finding algorithms. The algorithms considered here are prelimi\u00ad nary implementations of three search heuris\u00ad tics: local search, simulated annealing, and genetic algorithms. We compare their out\u00ad comes to each other and to the optimal poli\u00ad cies for each instance. We compare run times of each policy and of a dynamic programming algorithm for POMDPs developed by Hansen that iteratively improves a finite-state con\u00ad troller the previous state of the art for finite memory policies. The value of the best policy can only improve as the amount of memory increases, up to the amount needed for an optimal finite-memory policy. Our most surprising finding is that more memory helps in another way: given more memory than is needed for an optimal policy, the algo\u00ad rithms are more likely to converge to optimal\u00ad valued policies.", "creator": "pdftk 1.41 - www.pdftk.com"}}}