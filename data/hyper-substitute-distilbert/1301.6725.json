{"id": "1301.6725", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Loopy Belief Propagation for Approximate Inference: An Empirical Study", "abstract": "additionally, researchers have appeared that loopy belief propagation - the use of temporal polytree algorithm in simple quantum context with loops implies ambiguous - correcting codes. the most intriguing instance of this represents essentially near self - supervised performance deco turbo codes implementing whose decoding relationship is equivalent just loopy belief propagation in a chain - related electrical network. with this paper we ask : is there every random about the error - response code context, or which loopy propagation work as of inherently error schemein under more recent design? we compare the marginals computed given independent reasoning to four simplest ones producing common sequential reasoning architectures, including two real - global technologies : alarm and qmr. when report that the information messages may converge? when they integrate, they give a good approximation to the correct marginals. however, on robust qmr networks, ambiguity warning beliefs oscillated and requires no obvious relationship to the correct posteriors. we find some initial investigations discussing changes cause below these oscillations, therefore show that any programming puzzles incorrectly studying them lead to faulty experimental results.", "histories": [["v1", "Wed, 23 Jan 2013 16:00:02 GMT  (344kb)", "http://arxiv.org/abs/1301.6725v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["kevin murphy", "yair weiss", "michael i jordan"], "accepted": false, "id": "1301.6725"}, "pdf": {"name": "1301.6725.pdf", "metadata": {"source": "CRF", "title": "Loopy Belief Propagation for Approximate Inference: An Empirical Study", "authors": ["Kevin P. Murphy"], "emails": ["}@cs.berkeley.edu"], "sections": [{"heading": null, "text": "Recently, researchers have demonstrated that \"loopy belief propagation\" - the use of Pearl's polytree algorithm in a Bayesian network with loops - can perform well in the context of error-correcting codes. The most dramatic instance of this is the near Shannon-limit performance of \"Turbo Codes\" - codes whose decoding algorithm is equivalent to loopy belief propagation in a chain-structured Bayesian network. In this paper we ask: is there something spe cial about the error-correcting code context, or does loopy propagation work as an ap proximate inference scheme in a more gen eral setting? We compare the marginals com puted using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR. We find that the loopy beliefs of ten converge and when they do, they give a good approximation to the correct marginals. However, on the QMR network, the loopy be liefs oscillated and had no obvious relation ship to the correct posteriors. We present some initial investigations into the cause of these oscillations, and show that some sim ple methods of preventing them lead to the wrong results.\n1 Introduction\nThe task of calculating posterior marginals on nodes in an arbitrary Bayesian network is known to be NP hard [5]. This is true even for the seemingly easier task of calculating approximate posteriors [6]. Never theless, due to the obvious practical importance of this task, there has been considerable interest in assessing the quality of different approximation schemes, in an attempt to delimit the types of networks and parame ter regimes for which each scheme works best.\nIn this paper we investigate the approximation per-\nformance of \"loopy belief propagation\" . This refers to using the well-known Pearl polytree algorithm [12] on a Bayesian network with loops (undirected cycles). The algorithm is an exact inference algorithm for singly connected networks - the beliefs converge to the cor rect marginals in a number of iterations equal to the diameter of the graph.1 However, as Pearl noted, the same algorithm will not give the correct beliefs for mul tiply connected networks:\nWhen loops are present, the network is no longer singly connected and local propaga tion schemes will invariably run into trouble . .. If we ignore the existence of loops and permit the nodes to continue communicat ing with each other as if the network were singly connected, messages may circulate in definitely around the loops and the process may not converge to a stable equilibrium ... Such oscillations do not normally occur in probabilistic networks . . . which tend to bring all messages to some stable equilibrium as time goes on. However, this asymptotic equi librium is not coherent, in the sense that it does not represent the posterior probabilities of all nodes of the network [12, p.l95]\nDespite these reservations, Pearl advocated the use of belief propagation in loopy networks as an approxima tion scheme (J. Pearl, personal communication) and exercise 4.7 in [12] investigates the quality of the ap proximation when it is applied to a particular loopy belief network.\nSeveral groups have recently reported excellent exper imental results by using this approximation scheme by running algorithms equivalent to Pearl's algorithm on networks with loops. Perhaps the most dramatic instance of this performance is in an error correcting code scheme known as \"Turbo Codes\" [4]. These codes have been described as \"the most exciting and poten tially important development in coding theory in many\n1 This assumes parallel updating of all nodes. The algo rithm can also be implemented in a centralized fashion in which case it converges in two iterations [13).\n468 Murphy, Weiss, and Jordan\nyears\" [11] and have recently been shown [9, 10] to uti lize an algorithm equivalent to belief propagation in a network with loops. Although there is widespread agreement in the coding community that these codes \"represent a genuine, and perhaps historic, break through\" [11], a theoretical understanding of their per formance has yet to be achieved. Yet McEliece et. a! conjectured that the performance of loopy belief prop agation on the Turbo code structure was a special case of a more general phenomenon:\nWe believe there are general undiscovered theorems about the performance of belief propagation on loopy DAGs. These theo rems, which may have nothing directly to do with coding or decoding will show that in some sense belief propagation \"converges with high probability to a near-optimum value\" of the desired belief on a class of loopy DAGs [10].\nProgress in the analysis of loopy belief propagation has been made for the case of networks with a single loop [18, 19, 2, 1]. For the sum-product (or \"belief update\" ) version it can be shown that:\n\u2022 Unless all the conditional probabilities are deter ministic, belief propagation will converge.\n\u2022 There is an analytic expression relating the cor rect marginals to the loopy marginals. The ap proximation error is related to the convergence rate of the messages - the faster the convergence the more exact the approximation.\n\u2022 If the hidden nodes are binary, then thresholding the loopy beliefs is guaranteed to give the most probable assignment, even though the numerical value of the beliefs may be incorrect. This result only holds for nodes in the loop.\nIn the max-product (or \"belief revision\") version, Weiss [19] showed that ( 1) belief propagation may con verge to a stable value or oscillate in a limit cycle and (2) if it converges then it is guaranteed to give the cor rect assignment of values to the hidden nodes. This result is independent of the arity of the nodes and whether the nodes are inside or outside the loop.\nFor the case of networks with multiple loops, Richard son [14] has analyzed the special case of Turbo codes. He has shown that fixed points of the sum-product ver sion always exist, and has given sufficient conditions under which they will be unique and stable (although verifying these conditions may be difficult for large net works).\nTo summarize, what is currently known about loopy propagation is that ( 1) it works very well in an error correcting code setting and (2) there are conditions for a single-loop network for which it can be guaranteed to work well. In this paper we investigate loopy prop agation empirically under a wider range of conditions.\nIs there something special about the error-correcting code setting, or does loopy propagation work as an approximation scheme for a wider range of networks?\n2 The algorithm\nFor completeness, we briefly summarize Pearl's belief propagation algorithm. Each node X computes a be lief BEL(:x) = P(X = :xiE), where E denotes the ob served evidence, by combining messages from its chil dren ..\\y;(:x) and messages from its parents 1rx(uk). (Following Peot and Shachter [13], we incorporate ev idence by letting a node send a message to itself, ..\\x(:x).)\n(1)\nwhere:\nBEL(:x) = a..\\(:x)1r(x)\n,x(tl(x) = ..\\x(x) IT ..\\\ufffd}(x) j\nand:\n(2)\n7r('l(x) = LP(X = xiU = u) IT 1r\ufffd)(uk) (3) u k\nThe message X passes to its parent U; is given by:\n(4) and the message X sends to its child Yj is given by:\n7r\ufffd;+l)(:x) = a?C('l(x)..\\x(x) IT .>.W(x) (5) k;Cj\nFor noisy-or links between parents and children, there exists an analytic expression for 1r( x) and Ax ( u;) that avoids the exhaustive enumeration over parent config urations [12].\nWe made a slight modification to the update rules in that we normalized both ..\\ and 1r messages at each iteration. As Pearl [12] pointed out, normalizing the messages makes no difference to the final beliefs but avoids numerical underflow.\nNodes were updated in parallel: at each iteration all nodes calculated their outgoing messages based on the incoming messages of their neighbors from the pre vious iteration. The messages were said to converge if none of the beliefs in successive iterations changed by more than a small threshold (10-4). All messages were initialized to a vector of ones; random initializa tion yielded similar results, since the initial conditions rapidly get \"washed out\" .\nFor comparison, we also implemented likelihood weighting [17], which is a simple form of importance sampling. Like any sampling algorithm, the errors can be driven towards zero by running the algorithm for long enough; in this paper, we usually used 200 sam ples, so that the total amount of computation time was roughly comparable (to within an order of magnitude)\nto loopy propagation. We did not implement some of the more sophisticated versions of likelihood weight ing, such as Markov blanket scoring (16], since our goal in this paper was to evaluate loopy propagation rather than exhaustively compare the performance of alter native algorithms. (For a more careful evaluation of likelihood weighted sampling in the case of the QMR network, see (8].)\n3 The networks\nWe used two synthetic networks, PYRAMID and toyQMR, and two real world networks, ALARM and QMR. The synthetic networks are sufficiently small that we can perform exact inference, using the junc tion tree algorithm. This allows us to measure the ac curacy of the approximation scheme. All the networks have many loops of different sizes.\n3.1 The PYRAMID network\nFigure 1 shows the structure of the PYRAMID net work. This is a multilayered hierarchical network with local connections between each layer and observations only at the bottom layer. We chose this structure be cause networks of this type are often used in image analysis- the bottom layer would correspond to pix els (see for example (15]).\nAll nodes were binary and the conditional probabilities were represented by tables- entries in the conditional probability tables (CPTs) were chosen uniformly in the range (0, 1].\n3.2 The toyQMR network\nFigure 2 shows the structure of a \"toyQMR\" network. This network is meant to represent the types of net works that arise in medical diagnosis - hidden dis eases in the top layer and observed symptoms in the bottom layer. Here we randomized over structure and parameters - for each experiment the parents of each node in the bottom layer was a randomly chosen subset of the nodes in the top layer. The parents subset was chosen using a simple procedure - each parent-child link was either present or absent with a probability of 0.5.\nAll nodes were binary and the conditional probabilities of the leaves were represented by a noisy-or:\n?(Child= OIParents) = e-Bo-L; B,Parent; (6) where 110 represents the \"leak\" term. The links !1; were chosen uniformly in the range (0, 1] while 110 was chosen uniformly in the range [0, 0.01] (hence the leaks are inhibited with very high probabil ity). The top layer had prior probabilities represented as CPTs and they were chosen uniformly in the range (0, 1].\nLoopy Belief Propagation 469\n3.3 The ALARM network\nFigure 3 shows the structure of the ALARM network - a Bayesian network for monitoring patients in in tensive care. This network was used by (3] to compare various inference algorithms. The arity of the nodes ranges from two to four and all conditional distribu tions are represented by tables. The structure and the CPTs were downloaded from Nir Friedman's Bayesian network repository at: www. cs. huj i. ac. il/\"nir.\n3.4 The QMR-DT network\nThe QMR-DT is a bipartite network whose structure is the same as that shown in figure 2 but the size is much larger. There are approximately 600 diseases and ap proximately 4000 findin nodes, with a number of ob served findings that varies per case. Due to the form of the noisy-or CPTs the complexity of inference is ex ponential in the number of positive findings (7]. Fol lowing (8], we focused on the four CPC cases for which the number of positive findings is less than 20, so that exact inference is possible (using the QUICKSCORE algorithm (7]).\n4 Results\n4.1 Initial experiments\nThe experimental protocol for the PYRAMID network was as follows. For each experimental run, we first gen erated random CPTs. We then sampled from the joint distribution defined by the network and clamped the observed nodes (all nodes in the bottom layer) to their sampled value. Given a structure and observations, we then ran three inference algorithms -junction tree, loopy belief propagation and sampling.\nWe found that loopy belief propagation always con verged in this case with the average number of iter ations equal to 10.2. Figure 4(a) shows the correla tion plot between the exact marginals (calculated us ing junction tree) and the loopy marginals (BEL(x) in equation 1 at convergence). For comparison, fig ure 4(b) shows the correlation between likelihood weighting and the correct marginals. Note that the sampler has been run for 20 times as many iterations as loopy propagation.\nThe experimental protocol for the toyQMR network was similar to that of the PYRAMID network except that we randomized over structure as well. Again we found that loopy belief propagation always converged, with the average number of iterations equal to 8.65. Figure 5 shows the two correlation plots.\nThe protocol for the ALARM network experiments dif fered from the previous two in that the structure and parameters were fixed - only the observed evidence differed between experimental runs. We assumed that all leaf nodes were observed and calculated the pos-\n470 Murphy, Weiss, and Jordan\nFigure 1: The structure of the PYRAMID network. All nodes are binary and observations appear only on the bottom layer. Such networks occur often in image analysis where the bottom layer would correspond to pixels.\nFigure 2: The structure of a toyQMR network. This is a bipartite structure where the conditional distributions of the leaves are noisy-or's. The network shown represents one sample from randomly generated structures where the parents of each symptom were a random subset of the diseases.\nFigure 3: The structure of the ALARM network - a network constructed by medical experts for monitoring patients in intensive care.\n0.8\n0.7\n\ufffd0.4 ,JJ 0.3 :P\"'o 0.2 ./ \u2022. 0 0\n0.1 / 0 0 0.2 0.4 0.6\nexact marginal\na\n,.,/\n0.8\n0.9\n0.8 \ufffd 0.7 1?().6 \ufffd \"'o.s :a \ufffd0.4 \ufffd\n0.3\n0 0.2\n0\n0.4 0.6 exact marginal\nb\n0.8\nFigure 4: Correlation plots between the correct and approximate beliefs for the PYRAMID network, using (a) loopy propagation and (b) likelihood weighting with 200 samples.\nLoopy Belief Propagation 471\nterior marginals of all other nodes. Again we found that loopy belief propagation always converged with the average number of iterations equal to 14.55. Fig ure 6 shows the correlation plots. With 200 samples, the correlation for likelihood weighting is rather weak, perhaps due to the larger arity of some of the nodes (and hence the larger state space); after 1000 samples, the correlation improves considerably.\nThe results presented up until now show that loopy propagation performs well for a variety of architectures involving multiple loops. We now present results for the QMR-DT network which are not as favorable.\nIn the QMR-DT network there was no randomization. We used the fixed structure and calculated posteriors for the four cases for which posteriors have been cal culated exactly by Heckerman [7]. For none of these four cases did loopy propagation converge. Rather, the loopy marginal oscillated between two quite distinct values for nearly all nodes. Figure 7(a) shows three such marginals. After two iterations the marginal seems to converge to a limit cycle with period two. In Figure 7(b) it seems that the correct posteriors al ways lie inside the interval defined by the limit cycle. However, this is not always the case (except, of course, when the interval is 0 to 1 !) .\n4.2 What causes convergence versus oscill ation?\nWhat our initial experiments show is that loopy prop agation does a good job of approximating the correct posteriors if it converges. Unfortunately, on the most challenging case- the QMR-DT network- the al gorithm did not converge. We wanted to see if this oscillatory behavior in the QMR-DT case was related to the size of the network - does loopy propagation tend to converge less for large networks than small\nnetworks?\nTo investigate this question, we tried to cause oscil lation in the toyQMR network. We first asked what, besides the size, is different between toyQMR and real QMR? An obvious difference is in the parameter val ues - while the CPTs for toyQMR are random, the real QMR parameters are not. In particular, the prior probability of a disease node being on is extremely low in the real QMR (typically of the order of 10-3). Would low priors cause oscillations in the toyQMR case? To answer this question we repeated the ex periments reported in the previous section but rather than having the prior probability of each node be ran domly selected in the range [0, 1] we selected the prior uniformly in the range [0, U] and varied U. Unlike the previous simulations we did not set the observed nodes by sampling from the joint - for low priors all the findings would be negative and inference would be trivial. Rather each finding was independently set to positive or negative. Figure 8 shows the results - for small priors the toyQMR network does not converge and we find the same oscillatory behavior as in the real QMR network case.\nIf indeed small priors are responsible for the oscilla tion, then we would expect the real QMR network to converge if the priors were sampled randomly in the range [0, 1]. To check this, we reran loopy propaga tion on the full QMR network with the four tractable cases but changed the priors to be randomly sampled in the range [0, 1]. All other parameters remained the same as in the real QMR network. Now we found convergence on all four cases and the beliefs gave a very good correlation with the ones calculated using QUICKSCORE.\nSmall priors are not the only thing that causes oscil lation. Small weights can, too. The effect of both\n472 Murphy, Weiss, and Jordan\nLoopy Belief Propagation 473\nis to reduce the probability of positive findings. We conjectured that the reason for the oscillations is that the observed data, which has many positive findings, is very untypical in this parameter regime. This would also explain why we didn't find oscillations in the other examples, where the data was sampled from the joint distribution encoded by the network.\nTo test this hypothesis, we reparameterized the pyra mid network as follows: we set the prior probability of the \"1\" state of the root nodes to 0.9, and we utilized the noisy-OR model for the other nodes with a small (0.1) inhibition probability (apart from the leak term, which we inhibited with probability 0.9). This param eterization has the effect of propagating 1 's from the top layer to the bottom. Thus the true marginal at each leaf is approximately (0.1, 0.9), i.e., the leaf is 1 with high probability. We then generated untypical evidence at the leaves by sampling from the uniform distribution, (0.5, 0.5), or from the skewed distribu tion (0.9, 0. 1). We found that loopy propagation still converged2, and that, as before, the marginals to which it converged were highly correlated with the correct marginals. Thus there must be some other explana tion, besides untypicality of the evidence, for the os cillations observed in QMR.\n4.3 Can we fix oscillations easily?\nWhen loopy propagation oscillates between two steady states it seems reasonable to try to find a way to com bine the two values. The simplest thing to do is to average them. Unfortunately, this gave very poor re sults, since the correct posteriors do not usually lie in the midpoint of the interval ( cf. Figure 7 (b)).\n2More precisely, we found that with a convergence threshold of 10-4, 98 out of 100 cases converged; when we lowered the threshold to 10-3, all 100 cases converged.\nWe also tried to avoid oscillations by using \"momen tum\"; replacing the messages that were sent at time t with a weighted average of the messages at times t and t- 1. That is, we replaced the reference to >.\ufffd) in ' Equation 2 with (1- p).\\y1(x)(t) + p.\\y1(x)(t-1) (7) and similarly for 11\"\ufffd) in Equation 3, where 0 :::; J.l :::; 1 is the momentum term. It is easy to show that if the modified system of equations converges to a fixed point F, then F is also a fixed point of the original system (since if>.\ufffd)= >.\ufffd-1), then Equation 7 yields>.\ufffd)). ' ' ' In the experiments for which loopy propagation con verged (PYRAMID, toyQMR and ALARM), we found that adding the momentum term did not change the results - the beliefs that resulted were the same be liefs found without momentum. In the experiments which did not converge (toyQMR with small priors and real QMR), we found that momentum significantly reduced the chance of oscillation. However, in several cases the beliefs to which the algorithm converged were quite inaccurate- see Figure 9.\n5 Discussion\nThe experimental results presented here suggest that loopy propagation can yield accurate posterior marginals in a more general setting than that of error correcting coding - the PYRAMID, toyQMR and ALARM networks are quite different from the error correcting coding graphs yet the loopy beliefs show high correlation with the correct marginals.\nIn error-correcting codes the posterior is typically highly peaked and one might think that this feature is necessary for the good performance of loopy prop agation. Our results suggest that is not the case -\n474 Murphy, Weiss, and Jordan\ncase 16 momentum=0.1\ncase 32 momentum=0.1\nin none of our simulations were the posteriors highly peaked around a single joint configuration. If the prob ability mass was concentrated at a single point the marginal probabilities should all be near zero or one; this is clearly not the case as can be seen in the figures.\nIt might be expected that loopy propagation would only work well for graphs with large loops. However, our results, and previous results on turbo codes, show that loopy propagation can also work well for graphs with many small loops.\nAt the same time, our experimental results suggest a cautionary note about loopy propagation, showing that the marginals may exhibit oscillations that have very little correlation with the correct marginals. We presented some preliminary results investigating the cause of the oscillations and showed that it is not sim ply a matter of the size of the network or the number of parents. Rather the same structure with different parameter values may oscillate or exhibit stable be havior.\nFor all our simulations, we found that when loopy propagation converges, it gives a surprisingly good ap proximation to the correct marginals. Since the dis tinction between convergence and oscillation is easy to make after a small number of iterations, this may sug gest a way of checking whether loopy propagation is appropriate for a given problem.\nAcknowledgements\nWe thank Tommi Jaakkola, David Heckerman and David MacKay for useful discussions. We also thank Randy Miller and the University of Pittsburgh for the use of the QMR-DT database. Supported by MURI ARO DAAH04-96-1-0341.\nReferences\n[1] J. M. Agosta. The structure of Bayes networks for visual recognition. In UAI, volume 4, pages 397-405, 1990.\n[2] S.M. Aji, G.B. Horn, and R.J. McEliece. On the convergence of iterative decoding on graphs with a single cycle. In Proc. 1998 !SIT, 1998.\n[3] I. Beinlich, G. Suermondt, R. Chavez, and G. Cooper. The alarm monitoring system: A case study with two probabilistic inference tech niques for belief networks. In Proc. 2 'nd European Conf. on AI and Medicine, 1989.\n[4] C. Berrou, A. Glavieux, and P. Thitimajshima. Near Shannon limit error-correcting coding and decoding: Turbo codes. In Proc. IEEE Interna tional Communications Conference '93, 1993.\n[5] G. Cooper. The computational complexity of probabilistic inference using Bayesian belief net works. Artificial Intelligence, 42:393-405, 1990.\nLoopy Belief Propagation 475\n[6] P. Dagum and M. Luby. Aproximate probabilis tic inference in Bayesian networks in NP hard. Artificial Intelligence, 60:141-153, 1993.\n[7] D. Heckerman. A tractable inference algorithm for diagnosing multiple diseases. In Proc. Fifth Conf. on Uncertainty in AI, 1989.\n[8] T.S. Jaakkola and M.l. Jordan. Variational prob abilistic inference and the QMR-DT network. lAIR, 10, 1999.\n[9] F. R. Kschischang and B. J. Frey. Iterative de coding of compound codes by probability prop agation in graphical models. IEEE Journal on Selected Areas in Communication, 16(2) :219-230, 1998.\n[10] R.J. McEliece, D.J.C. MacKay, and J.F. Cheng. Turbo decoding as as an instance of Pearl's 'be lief propagation' algorithm. IEEE Journal on Selected Areas in Communication, 16(2):140-152, 1998.\n[11] R.J. McEliece, E. Rodemich, and J.F. Cheng. The Turbo decision algorithm. In Proc. 33rd Aller ton Conference on Communications, Control and Computing, pages 366-379, Monticello, 11, 1995.\n[12] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Mor gan Kaufmann, 1988.\n[13] M.A. Peot and R.D. Shachter. Fusion and prop agation with multiple observations in belief net works. Artificial Intelligence, 48:299-318, 1991.\n[14] Thomas Richardson. The geometry of turbo decoding dynamics. IEEE Trans. on Info. Theory, 1999. To appear.\n[15] L.K. Saul, T. Jaakkola, and M.l. Jordan. Mean field theory for sigmoid belief networks. lAIR, 4:61-76, 1996.\n[16] R. D. Shachter and M. A. Peat. Simulation ap proaches to general probabilistic inference on be lief networks. In Uncertainty in AI, volume 5, 1990.\n[17] M. Shwe and G. Cooper. An empirical analysis of likelihood-weighting simulation on a large, multi ply connected medical belief network. Computers and Biomedical Research, 24:453-475, 1991.\n[18] Y. Weiss. Belief propagation and revision in net works with loops. Technical Report 1616, MIT AI lab, 1997.\n[19] Y. Weiss. Correctness of local probability prop agation in graphical models with loops. Neural Computation, to appear, 1999.\n476\nLearning Bayesian Networks from Incomplete Data with Stochastic Search Algorithms\nJames W. Myers George Mason University Fairfax, VA 22032-4444 mvers2Waierols.com\nKathryn Blackmond Laskey George Mason University Fairfax, VA 22032-4444 klaskeyCdlgmu.edu\nTod Levitt lET Setauket, NY 11733 tlevitt@iet.com\nAbstract This paper describes stochastic search approaches, including a new stochastic algorithm and an adaptive mutation operator, for learning Bayesian networks from incomplete data. This problem is characterized by a huge solution space with a highly multimodal landscape. State-of-the-art approaches all involve using deterministic approaches such as the e:\ufffd.-pectation-maximization algorithm. These approaches are guaranteed to find local maxima, but do not explore the landscape for other modes. Our approach evolves structure and the missing data. We compare our stochastic algorithms and show they all produce accurate results.\n1 INTRODUCTION Bayesian networks are growing in popularity as the model of choice of many AI researchers for problems involving reasoning under uncertainty. They have been implemented in applications in areas such as medical diagnostics, classification systems, software agents for personal assistants, multisensor fusion, and legal analysis of trials. Until recently, the standard approach to constructing belief networks was a labor intensive process of eliciting knowledge from experts. Methods for capturing available data to construct Bayesian networks or to refine an expert -provided network promise to greatly improve both the efficiency of knowledge engineering and the accuracy of the models. For this reason, learning Bayesian networks from data has become an increasingly active area of research. Most of the research to date has relied\non the assumption that data are complete; that is, the values of all variables are known for all cases in the database. This assumption is not very realistic since most real world situations involve incomplete information.\nLearning a Bayesian network can be decomposed into the problem of learning the graph structure and learning the parameters. The first attempts at treating incomplete data involved learning the parameters of a fixed network structure [Lauritzen 1995]. Very recently, researchers have begun to tackle the problem of learning the structure of the network from incomplete data. A major stumbling block in this research is that when information is missing, closed form expressions do not exist for the scoring metric used to evaluate the network structures. This has led many researchers down the path of estimating the score using parametric approaches such as the expectation maximization (EM) algorithm [Dempster, Laird et al. 1977], [Friedman 1998]. The EM algorithm is a proven approach for dealing with incomplete information when building statistical models [Little and Rubin 1987]. EM and related algorithms show promise. However, it has been noted [Friedman 1998] that the search space is large and multimodal. and deterministic search algorithms are prone to find local optima. Multiple restarts have been suggested as a way to deal with this problem.\nAn obvious choice to combat the problem of \"getting stuck\" on local maxima is to use a stochastic search method. This paper explores the use of evolutionary algorithms (EA) and Markov chain Monte Ccirlo (MCMC) algorithms for learning Bayesian networks from incomplete data. We also introduce an algorithm, the Evolutionary Markov Chain Monte Carlo (EMCMC) algorithm. which combines the advantages of the EA and MCMC. which we believe. advances the state of the art for both EA"}], "references": [{"title": "The structure of Bayes networks for visual recognition", "author": ["J.M. Agosta"], "venue": "UAI, volume 4, pages 397-405,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "On the convergence of iterative decoding on graphs with a single cycle", "author": ["S.M. Aji", "G.B. Horn", "R.J. McEliece"], "venue": "Proc. 1998 !SIT,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "The alarm monitoring system: A case study with two probabilistic inference tech\u00ad niques for belief networks", "author": ["I. Beinlich", "G. Suermondt", "R. Chavez", "G. Cooper"], "venue": "Proc. 2 'nd European Conf. on AI and Medicine,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Near Shannon limit error-correcting coding and decoding: Turbo codes", "author": ["C. Berrou", "A. Glavieux", "P. Thitimajshima"], "venue": "Proc. IEEE Interna\u00ad tional Communications Conference '93,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "The computational complexity of probabilistic inference using Bayesian belief net\u00ad works", "author": ["G. Cooper"], "venue": "Artificial Intelligence, 42:393-405,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Aproximate probabilis\u00ad tic inference in Bayesian networks in NP hard", "author": ["P. Dagum", "M. Luby"], "venue": "Artificial Intelligence, 60:141-153,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1993}, {"title": "A tractable inference algorithm for diagnosing multiple diseases", "author": ["D. Heckerman"], "venue": "Proc. Fifth Conf. on Uncertainty in AI,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1989}, {"title": "Variational prob\u00ad abilistic inference and the QMR-DT", "author": ["T.S. Jaakkola", "M.l. Jordan"], "venue": "network. lAIR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Iterative de\u00ad coding of compound codes by probability prop\u00ad agation in graphical models", "author": ["F.R. Kschischang", "B.J. Frey"], "venue": "IEEE Journal on Selected Areas in Communication, 16(2) :219-230,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Turbo decoding as as an instance of Pearl's 'be\u00ad lief propagation' algorithm", "author": ["R.J. McEliece", "D.J.C. MacKay", "J.F. Cheng"], "venue": "IEEE Journal on Selected Areas in Communication, 16(2):140-152,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "The Turbo decision algorithm", "author": ["R.J. McEliece", "E. Rodemich", "J.F. Cheng"], "venue": "Proc. 33rd Aller\u00ad ton Conference on Communications, Control and Computing, pages 366-379, Monticello, 11,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["Judea Pearl"], "venue": "Mor\u00ad gan Kaufmann,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1988}, {"title": "Fusion and prop\u00ad agation with multiple observations in belief net\u00ad works", "author": ["M.A. Peot", "R.D. Shachter"], "venue": "Artificial Intelligence, 48:299-318,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "The geometry of turbo\u00ad decoding dynamics", "author": ["Thomas Richardson"], "venue": "IEEE Trans. on Info. Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Mean field theory for sigmoid belief networks. lAIR", "author": ["L.K. Saul", "T. Jaakkola", "M.l. Jordan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Simulation ap\u00ad proaches to general probabilistic inference on be\u00ad lief networks", "author": ["R.D. Shachter", "M.A. Peat"], "venue": "Uncertainty in AI, volume 5,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1990}, {"title": "An empirical analysis of likelihood-weighting simulation on a large, multi\u00ad ply connected medical belief network", "author": ["M. Shwe", "G. Cooper"], "venue": "Computers and Biomedical Research, 24:453-475,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Belief propagation and revision in net\u00ad works with loops", "author": ["Y. Weiss"], "venue": "Technical Report 1616, MIT AI lab,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Correctness of local probability prop\u00ad agation in graphical models with loops", "author": ["Y. Weiss"], "venue": "Neural Computation, to appear,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 4, "context": "The task of calculating posterior marginals on nodes in an arbitrary Bayesian network is known to be NP\u00ad hard [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 5, "context": "This is true even for the seemingly easier task of calculating approximate posteriors [6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 11, "context": "This refers to using the well-known Pearl polytree algorithm [12] on a Bayesian network with loops (undirected cycles).", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "7 in [12] investigates the quality of the ap\u00ad proximation when it is applied to a particular loopy belief network.", "startOffset": 5, "endOffset": 9}, {"referenceID": 3, "context": "Perhaps the most dramatic instance of this performance is in an error correcting code scheme known as \"Turbo Codes\" [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 10, "context": "years\" [11] and have recently been shown [9, 10] to uti\u00ad lize an algorithm equivalent to belief propagation in a network with loops.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "years\" [11] and have recently been shown [9, 10] to uti\u00ad lize an algorithm equivalent to belief propagation in a network with loops.", "startOffset": 41, "endOffset": 48}, {"referenceID": 9, "context": "years\" [11] and have recently been shown [9, 10] to uti\u00ad lize an algorithm equivalent to belief propagation in a network with loops.", "startOffset": 41, "endOffset": 48}, {"referenceID": 10, "context": "Although there is widespread agreement in the coding community that these codes \"represent a genuine, and perhaps historic, break\u00ad through\" [11], a theoretical understanding of their per\u00ad formance has yet to be achieved.", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "These theo\u00ad rems, which may have nothing directly to do with coding or decoding will show that in some sense belief propagation \"converges with high probability to a near-optimum value\" of the desired belief on a class of loopy DAGs [10].", "startOffset": 233, "endOffset": 237}, {"referenceID": 17, "context": "Progress in the analysis of loopy belief propagation has been made for the case of networks with a single loop [18, 19, 2, 1].", "startOffset": 111, "endOffset": 125}, {"referenceID": 18, "context": "Progress in the analysis of loopy belief propagation has been made for the case of networks with a single loop [18, 19, 2, 1].", "startOffset": 111, "endOffset": 125}, {"referenceID": 1, "context": "Progress in the analysis of loopy belief propagation has been made for the case of networks with a single loop [18, 19, 2, 1].", "startOffset": 111, "endOffset": 125}, {"referenceID": 0, "context": "Progress in the analysis of loopy belief propagation has been made for the case of networks with a single loop [18, 19, 2, 1].", "startOffset": 111, "endOffset": 125}, {"referenceID": 18, "context": "In the max-product (or \"belief revision\") version, Weiss [19] showed that ( 1) belief propagation may con\u00ad verge to a stable value or oscillate in a limit cycle and (2) if it converges then it is guaranteed to give the cor\u00ad rect assignment of values to the hidden nodes.", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "For the case of networks with multiple loops, Richard\u00ad son [14] has analyzed the special case of Turbo codes.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "(Following Peot and Shachter [13], we incorporate ev\u00ad idence by letting a node send a message to itself, .", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "For noisy-or links between parents and children, there exists an analytic expression for 1r( x) and Ax ( u;) that avoids the exhaustive enumeration over parent config\u00ad urations [12].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "As Pearl [12] pointed out, normalizing the messages makes no difference to the final beliefs but avoids numerical underflow.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "For comparison, we also implemented likelihood weighting [17], which is a simple form of importance sampling.", "startOffset": 57, "endOffset": 61}, {"referenceID": 6, "context": "We used the fixed structure and calculated posteriors for the four cases for which posteriors have been cal\u00ad culated exactly by Heckerman [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 0, "context": "Would low priors cause oscillations in the toyQMR case? To answer this question we repeated the ex\u00ad periments reported in the previous section but rather than having the prior probability of each node be ran\u00ad domly selected in the range [0, 1] we selected the prior uniformly in the range [0, U] and varied U.", "startOffset": 237, "endOffset": 243}, {"referenceID": 0, "context": "If indeed small priors are responsible for the oscilla\u00ad tion, then we would expect the real QMR network to converge if the priors were sampled randomly in the range [0, 1].", "startOffset": 165, "endOffset": 171}, {"referenceID": 0, "context": "To check this, we reran loopy propaga\u00ad tion on the full QMR network with the four tractable cases but changed the priors to be randomly sampled in the range [0, 1].", "startOffset": 157, "endOffset": 163}], "year": 2011, "abstractText": "Recently, researchers have demonstrated that \"loopy belief propagation\" the use of Pearl's polytree algorithm in a Bayesian network with loops can perform well in the context of error-correcting codes. The most dramatic instance of this is the near Shannon-limit performance of \"Turbo Codes\" codes whose decoding algorithm is equivalent to loopy belief propagation in a chain-structured Bayesian network. In this paper we ask: is there something spe\u00ad cial about the error-correcting code context, or does loopy propagation work as an ap\u00ad proximate inference scheme in a more gen\u00ad eral setting? We compare the marginals com\u00ad puted using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR. We find that the loopy beliefs of\u00ad ten converge and when they do, they give a good approximation to the correct marginals. However, on the QMR network, the loopy be\u00ad liefs oscillated and had no obvious relation\u00ad ship to the correct posteriors. We present some initial investigations into the cause of these oscillations, and show that some sim\u00ad ple methods of preventing them lead to the wrong results.", "creator": "pdftk 1.41 - www.pdftk.com"}}}