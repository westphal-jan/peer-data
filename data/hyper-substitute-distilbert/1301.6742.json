{"id": "1301.6742", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Multiplicative Factorization of Noisy-Max", "abstract": "theoretical low - or minimal neural generalization phantom - max lab been utilized to reduce the complexity of knowledge requests. enabling this experiment, measurements present accurate theoretical representation of noisy - max that measures computational efficient synthesis in general protocol networks. empirical performances show that sampling method is productive of computing queries affecting special - suited large gradient nodes, qmr - graphs and cpcs, for which no previous exact inference method having been shown to perform well.", "histories": [["v1", "Wed, 23 Jan 2013 16:01:09 GMT  (273kb)", "http://arxiv.org/abs/1301.6742v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["masami takikawa", "bruce d'ambrosio"], "accepted": false, "id": "1301.6742"}, "pdf": {"name": "1301.6742.pdf", "metadata": {"source": "CRF", "title": "Multiplicative Factorization of Noisy-Max", "authors": ["Masami Takikawa", "Bruce D'Ambrosio"], "emails": ["takikawa@iet.com", "dambrosio@iet.com"], "sections": [{"heading": null, "text": "The noisy-or and its generalization noisy max have been utilized to reduce the com plexity of knowledge acquisition. In this pa per, we present a new representation of noisy max that allows for efficient inference in gen eral Bayesian networks. Empirical studies show that our method is capable of com puting queries in well-known large medical networks, QMR-DT and CPCS, for which no previous exact inference method has been shown to perform well.\n1 Introduction\nA Bayesian network is a powerful tool for representing and reasoning with uncertain knowledge. It is based on the observation that conditional independence re lationships among variables can both greatly reduce the number of conditional probabilities that must be specified and also simplify the computation of query results. This is done by factoring the full joint proba bility distribution into smaller distributions which are easier to create and use.\nIndependence of causal infiuence1 (ICI) [Srinivas, 1993] among local parent-child or cause-effect relation\nships allows for further factoring. ICI has been uti lized to reduce the complexity of knowledge acquisi tion [Hendon, 1987]. For example, two well-known large networks for medical diagnosis, the QMR-DT BN20 network [D'Ambrosio, 1994] and the CPCS net work [Pradhan et al., 1994], have benefited from ICI. The QMR-DT BN20 uses the noisy-or [Good, 1961], the best studied and most widely used model of ICI, to model local parent (disease) child (symptom) rela tionships, and the CPCS network uses the noisy-max\n1 Also known as causal independence and intercausal independence.\n[Hendon, 1987; Dfez, 1993], a multi-value generaliza tion of noisy-or.\nDifferent approaches have been proposed to repre sent ICI and to integrate the corresponding mod els into standard general Bayesian network inference such as clique tree propagation (CTP) [Lauritzen and Spiegelhalter, 1988] and symbolic probabilistic infer ence (SPI) [Shachter et al., 1990].2 Local expression language [D'Ambrosio, 1995] provides a comprehensive approach for integration of many local structure mod els, including ICI, into standard Bayesian networks. An additive factorization of ICI using the local ex pression language is presented in [D'Ambrosio, 1995]. Heterogeneous factorization [Zhang and Poole, 1994], temporal belief networks [Heckerman, 1993], and par ent divorcing [Olesen et al., 1989] are other major ap proaches which are capable of representing many forms of causal independences.\nHowever, the results obtained by these approaches are not satisfactory. One of weaknesses is that they impose constraints on variable elimination ordering, severely limiting their ability to find optimal elimination order ing. Zhang [1995] reports experiments with heteroge neous factorization on the CPCS network, and shows that the algorithm is unable to answer two out of 422 possible zero-observation queries, for example. Zhang and Yan [1997] extend clique tree propagation with heterogeneous factorization and show that the result ing algorithm is significantly more efficient than that of [Zhang and Poole, 1994], but it is unable to deal with the CPCS network because it runs out of mem ory when initializing clique trees.\nIn this paper, we describe a new approach to represent noisy-max. Our approach does not impose any unnec essary constraints on elimination ordering, and should\n2For a special kind of Bayesian networks known as poly trees, there are methods to speed up inference using noisy or [Kim and Pearl, 1983] and noisy-max [Diez, 1993]. To deal with loops, these methods require an additional mech anism such as local conditioning.\nbe integrable into standard general Bayesian network inference.\n2 Approaches to Representation of Noisy-Max\nIn this section, we review existing approaches to repre sentation of ICI, with particular attention to noisy-or and noisy-max models.\n2.1 Noisy-Max\nThe noisy-max [Henrion, 1987; Diez, 1993] often rep resents causal models. In noisy-max models, multi ple causes independently influence the effect, and their combination is specified by the max operator. Figure 1 shows this model graphically.\nFigure 1: Noisy-max interaction.\nThe probability distribution of an effect variable E given its parent causes can be expressed as follows:\nP(EIC1, ... , Cn) = n\nL:: II P(E;jC;). max(El,\u00b7\u00b7\u00b7,En)=E i=l\nUsing this model, the knowledge engineer only needs to specify a set of small conditional probability dis tributions, {P(E;!Ci)}, instead of specifying a huge conditional distribution P(EjC1, ... , Cn) which is of ten too large to fit in memory.\nA network built using noisy-max models is not a Bayesian network per se, due to the presence of the max operator. The inference engine must convert this\nMultiplicative Factorization of Noisy-Max 623\nnetwork into a network it can handle, or the engine must be extended to handle the max operator directly.\nA trivial conversion is to convert the max operator into a conditional distribution that deterministically encodes the n-ary max operator as follows:\nP(EIE ... E ) = { 1 if E = \ufffdax(E1, . . . ,En) I ' ' n 0 otherWISe. However, the size of such a conditional distribution is exponential in the number of causes, that is, mn+I where m is the domain size of E, so such distributions are often too large to be useful.\n2.2 Parent Divorcing and Temporal Transformation\nThe size of a conditional distribution that encodes the max operator can be reduced when the n-ary max op erator is decomposed into a set of binary max oper ators. We consider the following two well known ap proaches to the decomposition: parent divorcing [Ole sen et al., 1989] and t emporal t ransformation [Hecker man, 1993].\nParent divorcing constructs a binary tree in which each node encodes the binary operator. Figure 2 shows the decomposition tree constructed by parent divorcing for four causes.\nFigure 2: An example of parent divorcing.\nThe conditional distributions for hidden variables Yi and the effect variable E all deterministically encode the binary max operator. For example, the conditional distribution for E is defined as follows:\nP(EjY, y,) = { 1 if E = \ufffdax(Y1, Y2) 1' 2 0 otherwise.\n624 Takikawa and D'Ambrosio\nThe size of this distribution is m3 where m is the do main size of E. In the general case of n causes, n - 1 such distributions are necessary. Thus, the total size of the conditional distributions for encoding the max operator using parent divorcing is (n - 1)m3 which is much less than m n+ 1 for the trivial conversion for n > 2.\nTemporal transformation constructs a linear decom position tree. The Bayesian network resulting from temporal transformation for the four-cause noisy-max model is shown in Figure 3.\nFigure 3: An example of temporal transformation.\nThe conditional distribution for Yi encodes the iden tity function. Thus Y1 could be omitted by directly connecting E1 to Y2. The conditional distributions for the other hidden variables, Y2 and Y3, and the ef fect variable, E, all deterministically encode the bi nary max operator. Their size is m3 where m is the domain size of E. In the general case of n causes, n -1 such distributions are necessary. Thus, the total size of conditional distributions for encoding the max op erator using temporal transformation is the same as when using parent divorcing.\nUsing either parent divorcing or temporal transforma tion, a number of different decomposition trees can be constructed from the same original network, depend ing on the ordering of the combinations of causes. The efficiency of inference varies exponentially among these trees. Because these transformations are done off-line without knowledge of the actual query and observa tion patterns, there is no way to construct an optimal decomposition tree.\n2.3 Additive Factorization\nLocal expression language [D'Ambrosio, 1995] pro vides a comprehensive approach for integration of many local structure models, including ICI, into stan dard Bayesian networks. The formal syntax is defined\nrecursively as follows3:\nexp -+ dist ribution! exp x expl exp + expl exp- exp,\nwhere a dist ribut ion is a generalized distribution or potentials defined over some rectangular subspace of the Cartesian product of domains of its conditioned and conditioning variables. A generalized distribution does not have to be normalized and it can contain any numeric values such as negative numbers. In this paper, we use the following notation for generalized distributions:\nG( XJ, ... ,XniYJ, .. . ,Ym, (f(X1, . . . ,Xn, Y1, ... , Ym))),\nwhere X; is a conditioned variable, Yj is a conditioning variable, and f is a density function specifying actual numerical probabilities.\nThe semantics of local expressions are quite simple to specify:\nAn expression is equivalent to the distribu tion obtained by evaluating it using the stan dard rules of algebra for each possible combi nation of antecedent values, where a distribu tion contributes 0 when being evaluated for a parent case over which it is not defined.\nLet J;(E, C;) be the density function given by the knowledge engineer for a noisy-or model, defined by\nj;(E = T, C;) = P(E; =TIC;), j;(E = F, C;) = P(E; =FIG;),\n(1)\nfor each cause C;. Then, we can write the conditional probability distribution P(EIC1, ... , Cn) for the noisy or model in local expression language as follows:\nP(EIC1, ... , Cn) = n\nerr G(E =TIC;, (1) )) i=l\nn\n-<II G(E =TIC;, (J;(F, C;)))) i=l n +(II G(E =FIG;, (fi(F, C;)) )) i=l\n(2)\nThe size of this expression is linear in the number of causes. Because this representation of noisy-or con tains additions, we call it an addit ive fact orizat ion of noisy-or.\n30ur definition is a simplification of the original from [D'Ambrosio, 1995]\nAdditive factorizations can be developed for any ICI model, and they are particularly compact for noisy-or and noisy-max. However, there exists a difficult prob lem: the optimal factoring problem and associated inference algorithms are only defined on products of probability distributions, and are not readily applica ble to additive expressions. In order to handle additive expressions efficiently, the inference algorithm must be extended so as to find the best sequence of application of the distributivity, associativity, and commutativity axioms, interspersed with numeric combination oper ators. This task has been found to be extremely dif ficult. SPI [D'Ambrosio, 1995] extended the inference algorithm so that it can handle local expressions, but it tends to combine expressions too early, rather than to wait for further applications of the distributivity ax iom. As a result, it has to carry an unnecessarily large intermediate distribution, which is often too large to fit in memory.\n2.4 Heterogeneous Factorization\nHeterogeneous factorization (HF) [Zhang and Poole, 1994] provides another way to exploit ICI. In HF, an effect variable is referred to as a convergent variable and a variable which is not convergent variable is re ferred to as a regular variable.\nLet f and g be two factors with common convergent variables E1 , ... , En; let A be the set of regular vari ables that appear in both f and g; let B be the set of variables that appear only in f ; and let C be the set of variables that appear only in g. The combination operator 0 in HF is then defined by\nf0g(EI=eJ, ... , En=en, A, B, C) =\nI: I: eu*let2=el enl*nen2=en\nf (E! = eu, ... , En = en!, A, B) xg(E1 = e12, ... , En = en2, A, C),\nfor each value ei of Ei. Note that the base combination operators, *i, are indexed to indicate that different ICI models can be present simultaneously.\nBy considering the conditional probability distribu tions of an individual contribution given a cause Ci as a factor!;, that is, fi(E = a, Ci) = P(Ei = alGi), the conditional probability distribution of an effect vari able E given the cause variables C1, ... , Cn can be factorized as follows:\nThe factorization using the combination operator 0 is called a heterogeneous factorization in contrast to the homogeneous factorization of a standard Bayesian net work in which all factors that factorize the full joint are\nMultiplicative Factorization of Noisy-Max 625\ncombined uniformly through multiplication. In HF, the full joint can be obtained by combining the fac tors in proper order using both multiplication and the operator 0.\nIn order to ensure the correct result, the contributing factors of an effect variable must be combined with themselves before they can be multiplied with other factors. To deal with this rather restrictive order of combination of factors imposed by HF, the concept of deputation is introduced. To depute a convergent variable E is to make a copy E' of E, replacing E with E' in all contributing factors of E, and to set the conditional probability distribution of E as follows: P(EIE') = { 1 if E = E' 0 otherwise. The size of this additional distribution of E is m2 where m is the domain size of E. Deputation makes it possible to combine heterogeneous factors in any or der.\nThe inference algorithms (variable elimination or junc tion tree propagation) can be extended to deal with deputation networks. However, the variable elimina tion ordering must be restricted so that each deputy variable E' appears before the corresponding new reg ular variable E. This restriction imposes significant constraints on the efficiency of inference.\n3 Multiplicative Factorization\nIn this section, we describe a representation of the noisy-max model which has two desirable properties. First, this representation makes it possible to factorize the noisy-max model completely using multiplication, so that standard general Bayesian network inference algorithms such as SPI and the variable elimination algorithm can perform without modification. Second, this representation does not impose any constraints on elimination ordering, so the inference algorithms can achieve maximum efficiency. Because this representa tion uses only multiplication, we call it a multiplicative factorization.\n3.1 Multiplicative Factorization of Noisy-Or\nBefore giving a description of multiplicative factoriza tion of the general noisy-max, we will present the mul tiplicative factorization of noisy-or and show its cor rectness by deriving from it the additive factorization of noisy-or defined in Equation 2.\nThe key idea is to introduce an intermediate (hidden) random variable to each product in the additive fac torization, and to eliminate additions by achieving the\n626 Takikawa and D'Ambrosio\neffects of additions through the standard marginaliza tion of the intermediate variables.\nFor noisy-or with n causes, we introduce an interme diate variable, EF\u00b7, with domain (V, I) and parents C1, ... , Cn, corresponding to the products of the con tributions from C; to E; = F for 1 :S i :S n. We make EF\u00b7 the only parent of E. The resulting network is shown in Figure 4.\nwhere ff is a density function necessary to represent actual numerical values. It returns one if the state of the intermediate variable is I, and returns the individ ual contribution [; defined in Equation 1 if the state is V:\nff(E',E,C;) = { };(E,C;) ifE'=I if E' = V. (4)\nThe conditional probability distribution for E is de fined as follows:\nP(EIEF\u00b7) = { 1 if E = FA EF\u00b7 = V 1 if E = T A EF\u00b7 = I (5) -1 if E = T A EF\u00b7 = V 0 otherwise.\nThe additive factorization of the noisy-or defined in Equation 2 can be obtained by multiplying the above expressions and marginalizing out the intermediate\nvariable:\nP(EJCI, ... , Cn) = L P(EJEFn) X P(EFn JC1, ... , Cn)\nEpn L P(EJEFn) Epn\nn\nx (II G(EF\u00b7 JC;, (ff(EF\u00b7, F, C;)) )) i=l\n= L( G(E = FJEFn = V, (1)) Epn +G(E = TJEF\" =I, (1))\n-G(E = TJEF\u00b7 = V, (1))) n\nX (II G(EFn = IJC;, (1)) i=I +G(EF\" = VJC;, (f;(F, C;)) ))\nn\n= L (II G(E = T, EFn = IJC;, (1))) EF ... i=l n -(II G(E = T, EF\u00b7 = V JC;, (f;(F, C;))))\ni=l n\n+(II G(E = F, EF\u00b7 = VJC;, (f;(F, C;)) )) i=l n (II G(E = T JC;, (1) )) i=l n -(II G(E = TJC;, (f;(F, Ci)) )) i=l n +(II G(E = FJC;, (/;(F, C;)))) i=l\nIn this representation, the size of the conditional dis tribution table of E is always four, regardless of the number of causes.\n3.2 Multiplicative Factorization of Noisy-Max\nNoisy-max is a generalization of noisy-or. It is used extensively in the CPCS network. This section shows how it can be encoded in the multiplicative form.\nFor the sake of simplicity of presentation, assume that the size of the domain of the effect variable is three and the number of causes is two. (We will generalize them later.) Let the domain be (L, M, H), in which values are ordered as L < M <H. Table 1 shows the definition of the max operator for this model.\nIn multiplicative factorization, each intermediate vari able corresponds to some rectangular subspace, and is combined with other intermediate variables to define the space of each value of E. For this simple noisy max model, we need to introduce two intermediate variables, ELL and E(L+M)(L+M)\u00b7 ELL corresponds to the rectangular subspace containing the single L in Table 1, and E(L+M)(L+M) corresponds to the rectan gular subspace containing the single L and three M's. Using these two intermediate variables, we can obtain subspaces containing each value of E as follows. First, the subspace containing L is obtained from ELL\u00b7 Sec ond, the subspace containing M's is computed as the difference between E(L+M)(L+M) and ELL. Figure 5 illustrates this calculation.\nFigure 5: (L+M)(L+ M)- LL = LM +MM + ML.\nFinally, the subspace containing H's is computed as the difference between the whole space and E(L+M)(L+M)\u00b7 Thus, we need two intermediate vari ables for the noisy-max of three values, defined as fol lows:\nP(ELLIC1, Cz) = fl;=l G(ELLICi, (![(ELL, L, Ci)) ),\nP(E(L+M)(L+M) IC1, Cz) = fl;=l G( E(L+M)(L+M)ICi,\n(ff'(E(L+M)(L+M)\u2022 {L, M}, Ci))),\n(6)\nwhere ff' returns one if the state of the intermediate variable is I, and returns the sum of contributions if it is V:\nJ;'(E',D,C;) = { \ufffd fi(a., Ci) D<ED if E' =I. if E' =V.\nThe conditional distribution forE is defined as follows: P(EIELL, E(L+M)(L+M)) =\n1 if E = L /\\ELL = v /\\ E(L+M)(L+M) = I 1 if E = M /\\ELL = I/\\ E(L+M)(L+M) = v -1 if E = M /\\ELL= V /\\ E(L+M)(L+M) =I 1 if E = H /\\ELL = I/\\ E(L+M)(L+M) = I -1 if E = H /\\ELL = I/\\ E(L+M)(L+M) = v 0 otherwise.\n( 7)\nMultiplicative Factorization of Noisy-Max 627\nThe size of this table equals 3 x 2 x 2 = 12. We now develop a general noisy-max representation. Suppose there are n causes (C1 to Cn) and m values (a1 to Ctm where ai < aj if i < j). We need to in troduce m - 1 intermediate variables, each of which corresponds to a hypercube in Cartesian products of values. The ith variable, E(\"'\u2022+\u00b7\u00b7\u00b7+\"';)\", is defined as follows:\nn\nIT G( (8) j=l The conditional distribution forE is then defined using these m - 1 intermediate variables as follows:\n1 if E = Ctj and E(\"'\u2022+\u00b7\u00b7 +\"';)\" = V and all other parents are I for some 1 :S j :S m- 1; -1 if E = Ctj+l and E(\"'\u2022+\u00b7\u00b7\u00b7+\"';)\" = V and all other parents are I for some 1 :S j :S m - 1; 1 if E = am and all parents are I; 0 otherwise.\n(9)\nWhen m = 2, the above representation instantiates to that of the noisy-or defined by Equation 3 and 5. Also, when n = 2 and m = 3, the representation instantiates to that of the simple noisy-max defined by Equation 6 and 7.\nThis representation of the general noisy-max requires m - 1 intermediate variables. Hence, the size of con ditional distribution table for E is m2m-l.\n3.3 Summary of Representations of Noisy-Max\nA summary of representations of noisy-max is shown in Table 2. The second column shows the total size of tables required by the methods enumerated in the first column, where n is the number of causes, and m is the domain size of the effect variable. This size does not include the size of tables developed by the knowledge engineer. The third column entry is \"yes\" if the method requires an extension to the inference algorithm. The fourth column indicates whether the method imposes constraints on variable elimination or dering.\nTo be effective, each method should exponentially re duce the required size from that of trivial conversion. All methods achieved this requirement, although this\nmay not be clear in the case of multiplicative factoriza tion. Notice, however, that the size required for mul tiplicative factorization is always smaller than that of trivial conversion when n 2: 2 and m 2: 2, which we suppose is always the case. If we consider m as a small constant (in the case of noisy-or, m is 2), the gain is exponential in the number of causes.\n4 Experimental Results\nIn this section, we show experimental results obtained using multiplicative factorization. We use two large networks, CPCS and QMR-DT, as the test cases.\n4.1 Experiments Using CPCS Network\nThe CPCS network is created by Pradhan et al. [1994] based on the Computer-Based Patient Case Simula tion system (CPCS-PM), developed by R. Parker and R. Miller [1987]. The CPCS network is a multi-level, multi-valued network and is one of the largest net works in use to date. It contains 422 nodes and 867 arcs. Most of the distributions are specified in the noisy-max interaction models. Some noisy-max nodes have as many as 17 parents and some nodes contain as many as 5 values.\nWe made a marginal query for each individual variable. In these experiments, we used the JSPI inference al gorithm, a successor of SPI [D'Ambrosio, 1995]. JSPI is similar to the variable elimination algorithm [Zhang and Poole, 1996] coupled with a heuristic similar to the minimum size heuristic and the minimum weight heuristic [Kjrerulff, 1993].\nTable 3 shows the cost distribution of the marginal queries. Each row shows the number of variables whose marginal query requires the cost in the specified range. A cost in this case is the number of numerical multiplications required for answering the query using JSPI.\nTable 4 shows the results of nine of the most difficult marginal queries (those in the bottom row of Table 3) with the total values of the 422 marginal queries. The\nfirst column shows the name of variable. The second and third columns show the number of variables and expressions relevant to the query, respectively. The fourth and fifth columns show the costs of the query in the number of numerical multiplications and in CPU time in seconds. The CPU time covers every computa tion required for the query including gathering relevant expressions, finding variable elimination ordering, and actual numeric computation. It is measured using a Pentium II 300MHz with 128MB of memory.\nAs shown in the tables, any marginal query can be answered using less than a million multiplications or four seconds. We have finally tractably computed all prior marginals in the CPCS network.\n4.2 Experiments Using QMR-DT BN20 Network\nA BN20 network [Henrion and Druzdel, 1990] is a two level network in which parent (disease) interactions at a child (symptom) are modeled using the noisy-or in teraction model. The Quick Medical Reference (QMR)\nDT network [D'Ambrosio, 1994] is a very large net work, with over 600 diseases, 4000 findings, and 40,000 disease-findings links. Some findings have as many as 150 parents, and a case can have as many as 50 pos itive findings. We used a set of Scientific American cases supplied by the Institute for Decision Systems Research.\nadditive rep. multiplicative rep. case find vars exps vars exps\n0 20 599 662 619 3088 1 16 580 1239 595 2511 2 15 609 2932 623 4655 3 9 525 714 534 1845 4 8 602 2658 610 4032 5 16 623 1330 639 3447 6 19 623 1745 642 4981 7 9 589 967 598 2608 8 17 610 1240 626 3162 9 8 588 778 596 2498 10 14 576 777 590 2649 11 10 601 1690 611 3940 12 11 594 1747 604 3287 13 8 566 1103 573 2275 14 26 627 1534 653 5667 15 16 599 1578 609 2656\nTable 5: The characteristics of QMR-DT BN20 test cases.\nThe characteristics of the 16 test cases are shown in Table 5. The first column represents the case number, and the second column shows the number of positive findings (evidences). There are two columns each for additive and multiplicative representations of noisy-or. They represent the number of variables and expres sions (distributions or SPI local expressions) relevant to each query. The difference in the number of vari ables between the additive and multiplicative represen tations results from the introduction of intermediate variables in the multiplicative representation.\nThe QMR-DT network was too large to compute queries using JSPI. To compensate, we developed a variable elimination style inference algorithm that worked with heuristics obtained using machine learn ing techniques. We show experimental results ob tained by that algorithm. See [Takikawa, 1998] for the detailed description of the inference algorithm and the learning techniques used to find the heuristics.\nTable 6 shows the results of the Scientific American cases employing the learned heuristic called Mul-Fea Clus-5-250 [Takikawa, 1998]. The first three columns show the case number, the number of multiplications in thousands, and CPU-time in minutes.\nMultiplicative Factorization of Noisy-Max 629\nTable 6 also shows the results taken from [D'Ambrosio, 1995]. In [D'Ambrosio, 1995], D'Ambrosio compared two methods: Quickscore [Heckerman, 1989] and SPI. Quickscore used the temporal transformation of noisy or, and SPI used the additive representation of noisy or. The fourth and fifth columns in Table 6 show the number of multiplications in thousands required by Quickscore and SPI, respectively. Those entries marked with (>2000) indicate that the computation aborted because the number of multiplications ex ceeded a predefined limit of two million multiplica tions. D'Ambrosio used only the first 10 cases.4 Note that he ignored all negative evidences in his experi ments, but the effects of ignoring these was negligible. Comparison with these results clearly shows the supe riority of our method.\n5 Conclusions\nWe have presented a new multiplicative factorization of noisy-max models, and empirically demonstrated that this representation allows for more efficient in ference in large networks than do existing methods.\nUnfortunately, the multiplicative factorization pre sented here is not effectively applicable to all ICI mod els. Noisy-or, noisy-max, noisy-and, and noisy-min in teraction models are examples of ICI models that can be represented effectively with multiplicative factor ization, while noisy-add is an example of ICI models\n4In [D'Ambrosio, 1995], those cases are named from 1 to 10.\n630 Takikawa and D'Ambrosio\nthat cannot be represented effectively. See [Takikawa, 1998] for an analysis of the limitations of multiplicative factorization.\nAcknowledgements\nThis paper has benefited from comments by Jane Jor gensen and the anonymous referees.\nReferences\n[D'Ambrosio, 1994] B. D'Ambrosio. SPI in large BN20 networks. In Poole and Lopez de Mantaras, editors, Tenth Annual Conference on Uncertainty on AI. Morgan Kaufmann, 1994.\n[D'Ambrosio, 1995] B. D'Ambrosio. Local expression languages for probabilistic dependence. Interna tional Journal of Approximate Reasoning, 13:61-81, 1995.\n[Diez, 1993] F. J. Diez. Parameter adjustment in Bayes networks: the generalized noisy-or gate. In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, pages 99-105, 1993.\n[Good, 1961] I. Good. A causal calculus (i). British Journal of Philosophy of Science, 11:305-318, 1961.\n[Heckerman, 1989] D. Heckerman. A tractable infer ence algorithm for diagnosing multiple diseases. In Proceedings of the Fifth Conference on Uncertainty in AI, pages 174-181, 1989.\n[Heckerman, 1993] D. Heckerman. Causal indepen dence for knowledge acquisition and inference. In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, pages 122-127, 1993.\n[Henrion and Druzdel, 1990] M. Henrion and M. Druzdel. Qualitative propaga tion and scenario-based explanation of probabilistic reasoning. In Proceedings of the Sixth Conference on Uncertainty in AI, pages 10-20, 1990.\n[Henrion, 1987] M. Henrion. Some practical issues in constructing belief networks. In L. Kana!, T. Levitt, and J. Lemmer, editors, Uncertainty in Artificial Intelligence, Vol 3, pages 161-174. North-Holland, 1987.\n[Kim and Pearl, 1983] J. H. Kim and J. Pearl. A com putational model for causal and diagnostic reason ing in infe renee engines. In Proceedings of IJCAI83. Karlsruhe, FRG, 1983.\n[Kjrerulff, 1993] U. Kjrerulff. Aspects of efficiency im provements in Bayesian networks. Thesis, Faculty of Technology and Science, Aalborg University, 1993.\n[Lauritzen and Spiegelhalter, 1988] S. Lauritzen and D. Spiegelhalter. Local computations with proba bilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society, B 50, 1988.\n[Olesen et al., 1989] K. G. Olesen, U. Kjrerulff, F. Jensen, B. Falck, S. Andreassen, and S. K. An dersen. A munin network for the median nerve - a case study on loops. Applied Artificial Intelligence, 3:384-403, 1989.\n[Parker and Miller, 1987] R. C. Parker and R. A. Miller. Using causal knowledge to create simulated patient cases: the CPCS project as an extension of Internist-1. In Proceedings of the Eleventh An nual Symposium on Computer Applications in Med ical Care, pages 473-480. IEEE Comp Soc Press, 1987.\n[Pradhan et al., 1994] M. Pradhan, G. Provan, B. Middleton, and M. Henrion. Knowledge engi neering for large belief networks. In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, pages 484-490, 1994.\n[Shachter et al., 1990] R. Shachter, B. D'Ambrosio, and B. DelFavero. Symbolic probabilistic inference in belief networks. In Proceedings Eighth National Conference on AI, pages 126-131. AAAI, 1990.\n[Srinivas, 1993] S. Srinivas. A generalization of the noisy-or model. In Ninth Annual Conference on Un certainty on AI, pages 208-218, 1993.\n[Takikawa, 1998] M. Takikawa. Representations and Algorithms for Efficient Inference in Bayesian Net works. PhD thesis, Department of Computer Sci ence, Oregon State University, 1998.\n[Zhang and Poole, 1994] N. L. Zhang and D. Poole. Intercausal independence and heterogeneous factor ization. In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, pages 606-\n614, 1994.\n[Zhang and Poole, 1996] N. L. Zhang and D. Poole. Exploiting causal independence in Bayesian net work inference. Journal of Artificial Intelligence Re search, 5:301-328, 1996.\n[Zhang and Yan, 1997] N. L. Zhang and L. Yan. Inde pendence of causal influence and clique tree propa gation. In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, 1997.\n[Zhang, 1995] N. L. Zhang. Inference with causal in dependence in the CPSC network. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, 1995."}], "references": [{"title": "SPI in large BN20 networks", "author": ["B. D'Ambrosio"], "venue": "Poole and Lopez de Mantaras, editors, Tenth Annual Conference on Uncertainty on AI. Morgan Kaufmann", "citeRegEx": "D.Ambrosio. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Local expression languages for probabilistic dependence", "author": ["B. D'Ambrosio"], "venue": "Interna\u00ad tional Journal of Approximate Reasoning, 13:61-81", "citeRegEx": "D.Ambrosio. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Parameter adjustment in Bayes networks: the generalized noisy-or gate", "author": ["F.J. Diez"], "venue": "Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, pages 99-105", "citeRegEx": "Diez. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "A causal calculus (i)", "author": ["I. Good"], "venue": "British Journal of Philosophy of Science, 11:305-318", "citeRegEx": "Good. 1961", "shortCiteRegEx": null, "year": 1961}, {"title": "A tractable infer\u00ad ence algorithm for diagnosing multiple diseases", "author": ["D. Heckerman"], "venue": "Proceedings of the Fifth Conference on Uncertainty in AI, pages 174-181", "citeRegEx": "Heckerman. 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Causal indepen\u00ad dence for knowledge acquisition and inference", "author": ["D. Heckerman"], "venue": "Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, pages 122-127", "citeRegEx": "Heckerman. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Qualitative propaga\u00ad tion and scenario-based explanation of probabilistic reasoning", "author": ["M. Henrion", "M. Druzdel"], "venue": "Proceedings of the Sixth Conference on Uncertainty in AI, pages 10-20", "citeRegEx": "Henrion and Druzdel. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Some practical issues in constructing belief networks", "author": ["M. Henrion"], "venue": "L. Kana!, T. Levitt, and J. Lemmer, editors, Uncertainty in Artificial Intelligence, Vol 3, pages 161-174. North-Holland", "citeRegEx": "Henrion. 1987", "shortCiteRegEx": null, "year": 1987}, {"title": "Aspects of efficiency im\u00ad provements in Bayesian networks", "author": ["U. Kjrerulff"], "venue": "Thesis, Faculty of Technology and Science, Aalborg University", "citeRegEx": "Kjrerulff. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Local computations with proba\u00ad bilities on graphical structures and their application to expert systems", "author": ["S. Lauritzen", "D. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society, B 50", "citeRegEx": "Lauritzen and Spiegelhalter. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "and S", "author": ["K.G. Olesen", "U. Kjrerulff", "F. Jensen", "B. Falck", "S. Andreassen"], "venue": "K. An\u00ad dersen. A munin network for the median nerve - a case study on loops. Applied Artificial Intelligence, 3:384-403", "citeRegEx": "Olesen et al.. 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Using causal knowledge to create simulated patient cases: the CPCS project as an extension of Internist-1", "author": ["Parker", "Miller", "1987] R.C. Parker", "R.A. Miller"], "venue": "In Proceedings of the Eleventh An\u00ad nual Symposium on Computer Applications in Med\u00ad", "citeRegEx": "Parker et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Parker et al\\.", "year": 1987}, {"title": "Knowledge engi\u00ad neering for large belief networks", "author": ["M. Pradhan", "G. Provan", "B. Middleton", "M. Henrion"], "venue": "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, pages 484-490", "citeRegEx": "Pradhan et al.. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Symbolic probabilistic inference in belief networks", "author": ["R. Shachter", "B. D'Ambrosio", "B. DelFavero"], "venue": "Proceedings Eighth National Conference on AI, pages 126-131. AAAI", "citeRegEx": "Shachter et al.. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "A generalization of the noisy-or model", "author": ["S. Srinivas"], "venue": "Ninth Annual Conference on Un\u00ad certainty on AI, pages 208-218", "citeRegEx": "Srinivas. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Representations and Algorithms for Efficient Inference in Bayesian Net\u00ad works", "author": ["M. Takikawa"], "venue": "PhD thesis, Department of Computer Sci\u00ad ence, Oregon State University", "citeRegEx": "Takikawa. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Intercausal independence and heterogeneous factor\u00ad ization", "author": ["N.L. Zhang", "D. Poole"], "venue": "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, pages 606614", "citeRegEx": "Zhang and Poole. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Exploiting causal independence in Bayesian net\u00ad work inference", "author": ["N.L. Zhang", "D. Poole"], "venue": "Journal of Artificial Intelligence Re\u00ad search, 5:301-328", "citeRegEx": "Zhang and Poole. 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "Inde\u00ad pendence of causal influence and clique tree propa\u00ad gation", "author": ["Zhang", "Yan", "1997] N.L. Zhang", "L. Yan"], "venue": "In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 1997}, {"title": "Inference with causal in\u00ad dependence in the CPSC network", "author": ["N.L. Zhang"], "venue": "Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Zhang. 1995", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 14, "context": "Independence of causal infiuence1 (ICI) [Srinivas, 1993] among local parent-child or cause-effect relation\u00ad ships allows for further factoring.", "startOffset": 40, "endOffset": 56}, {"referenceID": 0, "context": "For example, two well-known large networks for medical diagnosis, the QMR-DT BN20 network [D'Ambrosio, 1994] and the CPCS net\u00ad work [Pradhan et al.", "startOffset": 90, "endOffset": 108}, {"referenceID": 12, "context": "For example, two well-known large networks for medical diagnosis, the QMR-DT BN20 network [D'Ambrosio, 1994] and the CPCS net\u00ad work [Pradhan et al., 1994], have benefited from ICI.", "startOffset": 132, "endOffset": 154}, {"referenceID": 3, "context": "The QMR-DT BN20 uses the noisy-or [Good, 1961], the best studied and most widely used model of ICI, to model local parent (disease) child (symptom) rela\u00ad tionships, and the CPCS network uses the noisy-max", "startOffset": 34, "endOffset": 46}, {"referenceID": 9, "context": "Different approaches have been proposed to repre\u00ad sent ICI and to integrate the corresponding mod\u00ad els into standard general Bayesian network inference such as clique tree propagation (CTP) [Lauritzen and Spiegelhalter, 1988] and symbolic probabilistic infer\u00ad ence (SPI) [Shachter et al.", "startOffset": 190, "endOffset": 225}, {"referenceID": 13, "context": "Different approaches have been proposed to repre\u00ad sent ICI and to integrate the corresponding mod\u00ad els into standard general Bayesian network inference such as clique tree propagation (CTP) [Lauritzen and Spiegelhalter, 1988] and symbolic probabilistic infer\u00ad ence (SPI) [Shachter et al., 1990].", "startOffset": 271, "endOffset": 294}, {"referenceID": 1, "context": "2 Local expression language [D'Ambrosio, 1995] provides a comprehensive approach for integration of many local structure mod\u00ad els, including ICI, into standard Bayesian networks.", "startOffset": 28, "endOffset": 46}, {"referenceID": 1, "context": "An additive factorization of ICI using the local ex\u00ad pression language is presented in [D'Ambrosio, 1995].", "startOffset": 87, "endOffset": 105}, {"referenceID": 16, "context": "Heterogeneous factorization [Zhang and Poole, 1994], temporal belief networks [Heckerman, 1993], and par\u00ad ent divorcing [Olesen et al.", "startOffset": 28, "endOffset": 51}, {"referenceID": 5, "context": "Heterogeneous factorization [Zhang and Poole, 1994], temporal belief networks [Heckerman, 1993], and par\u00ad ent divorcing [Olesen et al.", "startOffset": 78, "endOffset": 95}, {"referenceID": 10, "context": "Heterogeneous factorization [Zhang and Poole, 1994], temporal belief networks [Heckerman, 1993], and par\u00ad ent divorcing [Olesen et al., 1989] are other major ap\u00ad proaches which are capable of representing many forms of causal independences.", "startOffset": 120, "endOffset": 141}, {"referenceID": 16, "context": "Zhang and Yan [1997] extend clique tree propagation with heterogeneous factorization and show that the result\u00ad ing algorithm is significantly more efficient than that of [Zhang and Poole, 1994], but it is unable to deal with the CPCS network because it runs out of mem\u00ad ory when initializing clique trees.", "startOffset": 170, "endOffset": 193}, {"referenceID": 2, "context": "2For a special kind of Bayesian networks known as poly\u00ad trees, there are methods to speed up inference using noisy\u00ad or [Kim and Pearl, 1983] and noisy-max [Diez, 1993].", "startOffset": 155, "endOffset": 167}, {"referenceID": 7, "context": "The noisy-max [Henrion, 1987; Diez, 1993] often rep\u00ad resents causal models.", "startOffset": 14, "endOffset": 41}, {"referenceID": 2, "context": "The noisy-max [Henrion, 1987; Diez, 1993] often rep\u00ad resents causal models.", "startOffset": 14, "endOffset": 41}, {"referenceID": 1, "context": "Local expression language [D'Ambrosio, 1995] pro\u00ad vides a comprehensive approach for integration of many local structure models, including ICI, into stan\u00ad dard Bayesian networks.", "startOffset": 26, "endOffset": 44}, {"referenceID": 1, "context": "30ur definition is a simplification of the original from [D'Ambrosio, 1995]", "startOffset": 57, "endOffset": 75}, {"referenceID": 1, "context": "SPI [D'Ambrosio, 1995] extended the inference algorithm so that it can handle local expressions, but it tends to combine expressions too early, rather than to wait for further applications of the distributivity ax\u00ad iom.", "startOffset": 4, "endOffset": 22}, {"referenceID": 16, "context": "Heterogeneous factorization (HF) [Zhang and Poole, 1994] provides another way to exploit ICI.", "startOffset": 33, "endOffset": 56}, {"referenceID": 1, "context": "In these experiments, we used the JSPI inference al\u00ad gorithm, a successor of SPI [D'Ambrosio, 1995].", "startOffset": 81, "endOffset": 99}, {"referenceID": 17, "context": "JSPI is similar to the variable elimination algorithm [Zhang and Poole, 1996] coupled with a heuristic similar to the minimum size heuristic and the minimum weight heuristic [Kjrerulff, 1993].", "startOffset": 54, "endOffset": 77}, {"referenceID": 8, "context": "JSPI is similar to the variable elimination algorithm [Zhang and Poole, 1996] coupled with a heuristic similar to the minimum size heuristic and the minimum weight heuristic [Kjrerulff, 1993].", "startOffset": 174, "endOffset": 191}, {"referenceID": 6, "context": "A BN20 network [Henrion and Druzdel, 1990] is a two\u00ad level network in which parent (disease) interactions at a child (symptom) are modeled using the noisy-or in\u00ad teraction model.", "startOffset": 15, "endOffset": 42}, {"referenceID": 0, "context": "DT network [D'Ambrosio, 1994] is a very large net\u00ad work, with over 600 diseases, 4000 findings, and 40,000 disease-findings links.", "startOffset": 11, "endOffset": 29}, {"referenceID": 15, "context": "See [Takikawa, 1998] for the detailed description of the inference algorithm and the learning techniques used to find the heuristics.", "startOffset": 4, "endOffset": 20}, {"referenceID": 15, "context": "Table 6 shows the results of the Scientific American cases employing the learned heuristic called Mul-Fea\u00ad Clus-5-250 [Takikawa, 1998].", "startOffset": 118, "endOffset": 134}, {"referenceID": 1, "context": "Table 6 also shows the results taken from [D'Ambrosio, 1995].", "startOffset": 42, "endOffset": 60}, {"referenceID": 1, "context": "In [D'Ambrosio, 1995], D'Ambrosio compared two methods: Quickscore [Heckerman, 1989] and SPI.", "startOffset": 3, "endOffset": 21}, {"referenceID": 4, "context": "In [D'Ambrosio, 1995], D'Ambrosio compared two methods: Quickscore [Heckerman, 1989] and SPI.", "startOffset": 67, "endOffset": 84}, {"referenceID": 1, "context": "4In [D'Ambrosio, 1995], those cases are named from 1 to 10.", "startOffset": 4, "endOffset": 22}, {"referenceID": 15, "context": "See [Takikawa, 1998] for an analysis of the limitations of multiplicative factorization.", "startOffset": 4, "endOffset": 20}], "year": 2011, "abstractText": "The noisy-or and its generalization noisy\u00ad max have been utilized to reduce the com\u00ad plexity of knowledge acquisition. In this pa\u00ad per, we present a new representation of noisy\u00ad max that allows for efficient inference in gen\u00ad eral Bayesian networks. Empirical studies show that our method is capable of com\u00ad puting queries in well-known large medical networks, QMR-DT and CPCS, for which no previous exact inference method has been shown to perform well.", "creator": "pdftk 1.41 - www.pdftk.com"}}}