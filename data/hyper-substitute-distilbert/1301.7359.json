{"id": "1301.7359", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Merging Uncertain Knowledge Bases in a Possibilistic Logic Framework", "abstract": "your idea addresses the problem of adapting information information in the framework of possibilistic conditioning. it presents several syntactic exchange rules than offer possibilistic knowledge bases, provided are different tools, giving every broader possibilistic semantic base. these combination solutions are normally described at interaction meta - interface during the language of relational arguments. next, an extension of optimal constraint, via the combination languages facilitate evaluating the constraints, is done. a feedback system via a functional method, as is sound and fuzzy with respect than the possibilistic logic semantics, is given.", "histories": [["v1", "Wed, 30 Jan 2013 15:02:23 GMT  (403kb)", "http://arxiv.org/abs/1301.7359v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["salem benferhat", "claudio sossai"], "accepted": false, "id": "1301.7359"}, "pdf": {"name": "1301.7359.pdf", "metadata": {"source": "CRF", "title": "Merging uncertain knowledge bases in a possibilistic logic framework", "authors": ["Salem Benferhat", "Claudio Sossai"], "emails": ["benferhat@irit.fr", "sossai@ladseb.pd.cnr.it"], "sections": [{"heading": null, "text": "1 Introduction\nMerging uncertain information is a crucial problem in designing knowledge base systems. In many situ ations, relevant information is provided by different sources. This requires to perform some combination operation which uses simultaneously the information provided by the different sources in order to come to a conclusion. The way this problem is tackled depends on the way the information is represented. On the one hand, pieces of information pertaining to numeri cal parameters are usually represent.ed by distribution functions (in the sense of some uncertainty theory). These distributions are directly combined by means of operations and yield a new distribution. On the other hand, information may be also expressed in log ical terms, which may be, however, pervaded with un certainty. In this case, some uncertainty weights are attached to the logical formulae. Although similar is sues are raised in the two frameworks, like the handling of conflicting information, the two lines of research in numerical data fusion (e.g., [Abidi and Gonzalez 1992, Flamm and Luisi 1992]) and in symbolic combination (e.g., [Cholvy1992, Benferhat et al. 1995]) have been investigated independently. This paper studies the parallel combination of un certain knowledge bases. Uncertainty is represented in the possibility theory framework either at the se-\nmantical level via possibility distributions or at the syntactical level using possibilistic formulae. Each set of possibilistic formulae can be represented by a possibility distribution and conversely. The combi nation of possibility distributions has been studied in [Dubois and Prade 1992], and different combination methods have been proposed. In this paper, we apply these combination methods to the possibility distri butions associated with the sets of weighted formulae and we look for the syntactical counterparts of these combinations on the sets of possibilistic formulae. The rest of the paper is organized as follows: Section 2 recall the basic elements of standard possibilistic logic (SPL) needed to read the paper (for a complete ex position on SPL; see [Dubois et al. 1994]). Section 3 recalls the semantical combination rules developed in [Dubois and Prade 1992], and proposes their syntacti cal counterpart applied to the possibilistic knowledge bases. Section 4 introduces an extension of possibilistic logic, that we call LPL, where the language is enriched with two new connectives representing a new negation and a new conjunction. An example, inspired from an application in mobile robotics, is, also given.\n2 Standard Possibilistic Logic SPL\n2.1 Possibility distribution and possibilistic entailment\nIn this section, we only consider a propositional lan guage. Greek letters o:, {3,... represent real numbers belonging to [0,1], and uppercase latin letters (A, B, . . . ) represent propositional formulae. Let W be the set of interpretations, one of them being the actual real world. A possibility distribution is a mapping 1r from W to the interval [0,1]. 1r is said to be normalized if ::Jw E W, such that rr(w) =1. 1r represents some back ground knowledge; rr(w) =0 means that the state w is impossible, and rr(w) =1 means that nothing prevents w from being the real world. When rr(wl ) > rr(w2) , w1 is a preferred candidate to w2 as the real state of the world. A possibility distribution 1r induces a map ping grading the necessity (or certainty) of a formula A which evaluates to what extent A is entailed by the available knowledge. The necessity measure Nee is de fined by: Necrr(A) = 1-max{rr(w) I w f= \u2022A}.\nDef. 1 A weighted formula (B a} is said to be a plau sible conclusion of 1f, denoted by 1r H-( B a), if and only if i) Nec1r(B) > Nec'Tr(-.B), and ii) Nec1r(B) \ufffda.\nIf 1f is normalized, then (ii) implies (i). When (B a) is a conclusion of 1r, we say that 1r forces (B a).\n2.2 From possibilistic knowledge bases to possibility distributions\nA possibilistic knowledge base is made of a finite set of weighted formulae I; == { (A; a;), i == 1, n} where a; is understood as a lower bound on the degree of neces sity Nec(A;). A possibilistic knowledge base I; can be associated with a semantics in terms of possibility dis tributions. 1f is said to be compatible with I; if for each (A; a; ) E I;, we have : Nec'Tr(A;) \ufffd a;. In general, there are several possibility distributions compatible with I;. One way to select one possibility distribution is to use the minimum specificity principle. A possi bility distribution 1r is said to be less specific (or less informative) than 1r1 if Vw, 1r (w) \ufffd 1r1 (w) . The min imum specificity principle allocates to the interpreta tions the greatest possibility degrees in agreement with the constraints Nec'Tr (A;) \ufffd a;. We denote by 1f\ufffd the least specific possibility distribution which is compati ble with I;. 1fE is defined by (e.g., [Dubois et al. 1994])\nVw E W, 7rr; (w) = mini=l ,n{1 - a;,w F -.A;}. If w satisfies all the formulae in I; then 1f\ufffd ( w) = 1. The possibility distribution 1fr; is not necessarily nor mal, and Inc(I;) = 1-maxwEW1f\ufffd (w) is called the degree of inconsistency of the knowledge base I;.\n2.3 Possibilistic inference\nThe possibilistic logic inference can be performed at the syntactical level by means of a weighted version of the resolution principle:\n(A VB a) (-.A v C (3)\n(B V C min( a, (3)) It has been shown that Inc(I;) corresponds to the greatest lower bound that can be obtained for the empty clause by the repeated use of the above resolu tion rule. Proving (A a) from a possibilistic knowledge base I; comes down to deriving the contradiction ( .l (3) from I; U {(-.A 1)} with a weight (3 \ufffd a > I nc(I;). It will be denoted by I; f-pref (A a) . This inference method is sound and complete with respect to the possibilistic semantics. Namely [Dubois et al. 1994]:\n1fr; H-(A a) iff I; f-pref (A a) with a > Inc(I;). The syntactic inference f-pref is efficient and has a complexity similar to that of classical logic.\n3 Merging uncertain information\nIn [Dubois and Prade 1992], several proposals have been made to address the problem of combining n pos sibility distributions 1fl,n into a new possibility distri bution. All the proposed combination modes are de-\nMerging possibilistic knowledge bases 9\nfined at the semantical level. This means that these semantical combinations are impractical for large lan guages. Therefore, we need to look for syntactical combination rules which operate directly on possibilis tic knowledge bases. Let us first recall the seman tical combination methods [Dubois and Prade 1992], and then we investigate their syntactical counterparts.\n3.1 Merging possibility distributions\nThe basic combination modes in the possibilistic set ting are the minimum and the maximum of possibility distributions. Namely define:\nVw, 1fcm (w) == mini=l ,n1f ;(w), Vw, 1fdm (w) = maxi=l,n1f; (w) .\nThe conjunctive aggregation makes sense if all the sources are regarded as equally and fully reliable since all values that are considered as impossible by one source but possible by the others are rejected, while the disjunctive aggregation corresponds to a weaker reliability hypothesis where there is at least one reli able source for sure, but we do not know which one. The previous combination modes based on maximum and minimum operators have no reinforcement effect. Namely, if expert 1 assigns possibility 1r1 ( w) < 1 to interpretation w, and expert 2 assigns possibility 1r2 ( w) < 1 to this interpretation then overall, in the conjunctive mode, 1r(w) = 1r1 (w) if 1r1 (w) < 1r2 (w) , re gardless of the value of 1r2 (w). However since both ex perts consider w as rather impossible, and if these opin ions are independent, it may sound reasonable to con sider w as less possible than what each of the experts claims. This type of combination cannot be modelled by any idempotent operation, but can be obtained us ing a triangular norm operation other than min, and a triangular conorm operation other than max.\nDef. 2 A triangular norm (for short t-norm) 0 is a two place function whose domain is the unit square {0, 1 }X[O, 1} and which satisfies the conditions: 1. 0 0 0 = 0, a@ 1 = 1 @a = a; 2. a 0 (3 :::; o 0 1 whenever a :S o and (3 :S 1 ; 3. a 0 (3 = (3 0 a; 4. a 0 ((3 0 o) =(a 0 (3) 0 0.\nA triangular conorm (for short t-conorm) 0!J 1s a two place function in the unit square [0,1] x [0,1] which satisfies the conditions 2-4 given in the previous definition plus the following conditions: 5. 1 0!J 1=1, a 0!J 0 = 0 0!J a= a. Any t-conorm 0!J can be generated from a t-norm 0 through the duality transformation: a 0!J (3 == 1- (1- a0 1- (3). and conversely. The basic t-norms are the mml mum operator, the product operator and the t-norm max(O, a+ (3- 1) called \"Lukasiewicz t-norm\". The duality relation respectively yields the following t conorms: the maximum operator, the \"probabilistic sum\" a+ (3- a* (3, and the \"bounded sum\" min(1, a+ (3). We shall denote by 1r0 and 1f 0!J the possibil ity distributions resulting from the combination using a t-norm 0 and a t-conorm 0!J respectively.\n10 \u00b7Benferhat and Sossai\n3.2 Syntactical combination modes\nWe are interested in the combination of n possibilis tic knowledge bases ,Ei=l,n provided by n sources. Each knowledge base I:; is associated with a possi bility distribution ?T; which is its semantical counter part. Given a semantical combination rule Csem on the ?T;'s, reviewed in the previous section, we look for a syntactical combination Csyn on the I:; 's such that: ?Tc,.m H--(B, a) iff I.:c,yn f--pref (B, a) with a > Inc(I.:c,yn), and where ?Tc,.m (resp. I.:c,YJ is the possibility distribution (resp. the knowledge base) obtained by merging the ?T;'s (resp. the I.:;'s) using Csem (resp. Csyn)\u00b7 Let us first consider the sim ple case of combining two one-formula know ledge bases I.:1 = {(A a)} and I.:2 = {(B f))}. Let ?T1 and ?T2 be the possibility distributions associated respectively to I.;1 and I.:2. We are looking for a knowledge base I.;0 which associated possibility distribution is equal to ?T0. Then we can check that I.:0 results in three pos sibilistic formulae: I.:0 = I.:1 U I.:2 U (A VB a@) f))}, where @) is the t-conorm dual to \u00ae. The generalisa tion to the case of any knowledge bases and also to any t-norm or t-conorm is given by the following theorem:\nTheorem 1 Let I.:1 = {(A; a;)li E I} and I.:2 = { ( Bj J)j) li E J} be two bases. Let ?T1 and ?T2 be their associated possibility distributions. Then, ?T0 and ?T@) are associated with the following bases: I:0 = I:1 UI:2U{(A;V B3 (a;@) .Bi))li E I and j E J} (where@) is the t-conorm dual to the t-norm \u00ae) I: @) = {(A; v Bj a; \u00ae ,Bj )li E I and j E J} (where\u00ae is the t-norm dual to the t-conorm@) ).\nThe proof can be found in (Benferhat et a!., 1997). I.:0 may be inconsistent, but however I; @) is always consistent (provided that I.:1 or I.:2 is consistent). In order to reduce the size of I.:0 and I: @) , the following definitions and facts are useful:\nDef. 3 : A possibilistic formula (A a) of I; is said to be subsumed by I; iff Cut(,E, a) f-A where Cut(,E, a) = {B;(B J))EI.:-{(A a)}and f32:a}.\nFact 1 Let (A a) E I; be a subsumed formula. Then I; and I;- {(A a)} are equivalent, i.e., they induce the same possibility distribution.\nFact 2 The two knowledge bases I; and I; - { (T a)}, where T denotes tautology, are equivalent.\nOn the basis of the previous facts we get:\nFact 3 Let I.:1 and I.:2 be two knowledge bases. Let I;\ufffd be a knowledge base obtained from ,E1 by removing subsumed beliefs and tautologies. Then combining ,E2 with I.:1 is equivalent to combining I.:2 with I.;\ufffd.\n3.3 Example\nBefore giving a short example, we briefly clarify which operators should be used and in which order the in formation should be merged. When combining several\nbases with a same t-norm (resp. a same t-conorm) then the order has no importance, however when sev eral t-norms or t-cornorms are simultaneously used then associativity is no longer guaranteed. Indeed, for instance min( ?T1, ?T2) * ?T3 =J ?T1 *min( ?T2, ?T3). The choice of the operator is related to the dependencies of the sources. If the sources are independent then it is recommended to use a t-norm different from the minimum in order to get some reinforcement effect. Moreover, if the pieces of information are not conflict ing then it is preferred to use a t-norm, otherwise the maximum is more suitable. In practice, a t-conorm different from the maximum is rarely recommended since it yields a cautious inference, and hence a loss of information. Recall that: a@) j3 2: max( a, f)). In general, the available data are: 1. A set of hard rules ,Eh which expresses some in tegrity constraints. These rules have certainty degrees equal to 1 and are available to all the sources, 2. A set of uncertain knowledge bases I:; which comes from different sources, and 3. A set of facts Fj which focuses on particular sit uations. This set of facts can be common to all the sources or specific to each source (in this case, each source provide a know ledge base and a set of facts). We can imagine different schemas to merge these in formation: 1. Combine all the information, with a same t-norm (resp. t-conorm), or 2. Apply (i.e., add) the facts to each base, then com bine the results with a t-norm (resp. t-conorm), or 3. Combine first the possibilistic knowledge bases then add the facts to the resulting knowledge base, or 4. Proceed locally (and recursively) by first applying the facts to some bases, then combine the results by some t-norm (resp. t-conorm). Next, use the result of this combination as an input to other bases, which will again be merged by other t-norms (resp. t-conorms). Repeat iteratively this operation until the request is reached or all the bases and facts are used.\nClearly, the use of a given schema of combination de pends on the considered application. In the example considered in this paper, the last schema is preferred. The simplified example considered in this section con cerns the problem of position estimation (localization) of a mobile robot in a partially and approximately known indoor environment. The environment is de scribed as a graph of \"relevant places\" Pl,\u00b7 . . , Pn con nected by paths. Each place p; corresponds to a cor ner, a door, a section of a corridor etc., and is char acterized by a \"sensory signature\" that captures its distinctive features. The signature is the result of two sources of sensory information: . a ring of ultrasonic sensors which allow to detect the presence of walls or, more generally, of occlusions . a video camera which tracks a fixed light beacon and makes it possible to associate each place with a different image of the beacon itself. For a complete description, see [Bison et a!. , 1997a,b,1998].\nSuppose we are interested in knowing if the robot is at the North-West corner of a given room. We have two\ndifferent knowledge bases expressing the information provided by the two independent sources (sonar sen sors and camera). The following notations are used: A= sonar( wall, west), B = sonar( occlusion, north), C =sonar( occlusion, west), D =sonar( corner, north, west), E = close(wall, west), F =close( wall, north), G = cameralocation- at(pr ), H =at( corner, north, west). The two bases associated to the sources are: .E.={(DI\\EI\\F-+H 1), (BI\\C-+D .5), (AI\\ B -+ D .8)} where -+ is the material implication. The rules mean: - If a corner is perceived in the North-West direction, and if the robot is close to the walls at North and West, then we can say that the robot is at the corner; - If occlusions are perceived in the two directions North-West, then we can say that a corner is perceived at North-West with certainty degree .5; - If an occlusion is perceived at North, and a wall is perceived at West, then we can say that a corner is perceived with a higher certainty degree .8. . Ec = {( G -+ H 1)} This rule means that if the camera sees a corner in the North-West direction, then we can say that the robot is at the North-West corner.\nWe also assume that we have the two sets of facts: Facts= {(A .4), (B .5), (C .8), (E .7), (F .4)} Factc = {(G .6)} Now, let us see to what degree we can deduce that the robot is at the corner, and let us see the influence of the choice of the combination schema. Note that all the available information is consistent, therefore we use a t-norm rather than a t-conorm for merging. 1. The first case is to use the minimum operator, namely we take the concatenation of all the bases and facts. We denote by E f the final base. To check if the robot is at the corner, we first add ( ...,H 1) to E 1. Then applying the possibilistic resolution between (G-+ H 1) and (-,H 1) leads to have (-,G 1), and from ( -,Q 1) and ( G .6) we get a contradiction to a degree .6. We can check that .6 is the best lower bound, since the set of information in E f having cer tainty degree strictly higher than .6 is consistent with (-,H 1). Hence, using the minimum leads to infer that the robot is at corner to a degree .6. 2. The second case consists in first combining the bases Es and Ec with some t-norm 0, then adding the facts to the resulting base. We can easily check that this leads to the same conclusion as in the first case. The reason is that combining Es and Ec with 0 (even dif ferent from the minimum) leads to a knowledge base equivalent to Es U Ec. This is due to the fact that the certainty degree of the rule in Ec is equal to 1 (hence, all the added beliefs will be subsumed by this rule). 3. The last case concerns a local merging. We first add the set Facts to E \u2022. We denote Efs =Facts UEs the result of this step. We do the same thing with the second base, and we denote E fc = Factc U Ec. Since the two sources are independent, we use a t-norm dif ferent from the minimum. Let Er* be the base re sulting of combining Ejc and Efs. using the product t-norm. We have: Ef* = Ejs U Ejc U {(B 1\\ C -+\nMerging possibilistic knowledge bases 11\nDI\\G . 8), (AI\\B-+DI\\G .92), (AVG .76), (BV G .8), (CVG .92), (EVG .88), (FVG .76)} Now, let us add to E f* the assumption ( ..,H 1). Then applying the resolution to (G -+ H 1) and (..,H 1) leads to have (-,G 1). Now, we can check that applying all the possible resolutions between (..,G 1) and the rules: {(B 1\\C-+ DI\\G .8), (AI\\ B -+ DI\\G .92), (A V G .76), (B V G .8), (C V G .92), (EVG .88), (FVG .76)} leads to get the formulae: {(D .8), (E .88), (F .76)}. Applying again the resolution of these formulae with the rule: (D 1\\ E 1\\ F -+ H 1) leads to infer (H .76), which contradicts the assumption ( -,H 1) to a degree . 76. The degree .76 is the best lower bound. Note that, if we use the \"Lukasiewicz\" t-norm, we get a conclusion where it is completely certain that the robot is at the corner. In this example, the product is a good com promise between the minimum which is cautious, and the Lukasiewicz t-norm which is adventurous.\n4 A formal logical system (LPL) for merging possibilistic information\n4.1 Language\nIn this section we will describe a pure syntactical al gorithm to perform information fusion. First we will present the LPL logical language that extends the one presented for possibilistic logic, powerful enough to express from inside the logic meta-statements of the form: \"combine the knowledge bases E1, E2 using a t-norm or a t-conorm\". To this aim, we need to en rich the language with some connectives: & will rep resent the min operator, EB the sup operator and 0 the t-norm operator. This gives the static part of the logic (i.e. combining). To have a complete logic, we need an entailment -+ and a negation. Moreover, all the expressive power of the preceding language must be preserved, hence, for example, we need formulae which have the following meaning: \"the necessity of A is greater than or equal to a number a\" . To this aim, we add propositional constants a that will be inter preted as the possibilistic distribution with constant value a. Lastly, we extend the possibilistic logic lan guage to the first-order case. Summing up, we need the following language: .C =:: aiPIA&BIA EB BI-,AIA -+ BIA 0 Bl('v'x)A(x)l(3x)A(x). We put -,A =def A-+ 0. We take .C to be the set of formulae; it is convenient to define .C1 as the set of formulae with no occurrences of a constants for any a E (0, 1) -notice that 0 and 1 are in .C1. We use uppercase latin letters (A, B, C, . .. ) for formulae, while reserving L, M, N for .C1-formulae, and uppercase greek letters (f, \ufffd, . . . ) for multisets of formulae. As we will see, .C1 formulae form a sub-logic which has all the properties of classical logic.\n4.2 Semantics\nAs usual, to define the semantics we need a compo sitional function from the language to a suitable set of truth values. To find such a function we will move\n12 Benferhat and Sossai\nfrom the usual (global) idea of truth to a local one. Global truth means that once we have fixed a model, the truth value of a sentence is fixed, while in the local description of truth, a sentence can be true or not, depending on the available information. Hence the starting point is to fix the set of the informational states and then define when a fixed informational state makes true (forces) a formula. As the set of infor mational states we take the set of possibilistic distri butions: PD = {1r : MD --+ [0, 1]} where MD is the set of classical first-order structures for the lan guage .C1 on the domain D and given a sentence L of the language .C1 we will indicate with ModD(L) the set of all structures with domain D where L is true. Moreover o: is the possibilistic distribution defined as: o:(w) = o:, hence o:, depending on the context, can have the following meaning: the number o:, the pos sibilistic distribution with constant value the number o:, or the logical constant o: (as in [Pavelka 1979]). In the following 1r1 V 1r2 (resp. 1r1 1\\ 1r2) corresponds to sup( 1r1, 1r2) (resp. inf( 1r1, 1r2)), and \ufffd represents the specificity relation. The definition of local truth is:\nDef. 4 Let D be a domain, x a t-norm. The forcing relation (It-) between the set of possibilistic distribu tions and the sentences of the language .C, is defined by induction as follows:\n7rlt-R(c1, ... ,cn) iff Necrr(Modn(R(c1, ... ,cn))) = 1\n7rlt-CI' iff 7r\ufffdCl' 11\"1+-A@E iff (37r11t-A)(37r21t-E)(7r \ufffd 11\"1 X 11\"2) 1rlt-AE11E iff (37r11t-A)(37r21t-E)(7r \ufffd 1r1 V1r2)\n11\"1+-A-+ E iff ('v'7r11t-A)(37r21t-E)(7r X 11\"1 \ufffd 11\"2) 1rlt-A&E iff (37r11t-A)(37r21t-E)(7r \ufffd 1r1 J\\ 1r2)\n1rlt-('v'x)A(x) iff ('v'u E D)(1rlt-A(u)) 7rlt-(3x)A(x) iff (3u E D)(1rlt-A(u))\nwhere Nec1f : 2MD -t [0, 1] is the necessity func tion associated with the possibility distribution 1r: Nec1r (X ) = 1- VwflX 1r(w).\nFor a given t-norm x , the operator x : PD x PD --+ PD is defined as: (1r1 x 1r2)(w) = 1r1(w) x 1r2(w). For any t-norm, we introduce its adjoint operation named residuation: a::::? b = V{x E [0, 1] : x x a\ufffd b} which is naturally extended to possibility functions: ( 1r1 ::::? 1r2 )( w) = 1r1 ( w) ::::? 1r2 ( w) . The negation is defined as --,7r = 1r ::::? 0. The following fact is easily verified:\nFact 4 Any continous t-norm x over possibility dis tributions distributes over infinite disjunctions, i.e: 1r x (ViEI 1r;) = V;Ef(7rX7r;) hence Q = (PD, x, 1, \ufffd)is a commutative unital quanta[ [Rosenthal 1990}. More over, any t-norm x also distributes over infinite con junctions: 1r x (A;El1r;) = /\\;Ef(7r x 1r;)\nOnce we have the local concept of truth we can define the truth value of a formula \u00a2 as follows:\nDef. 5 For every sentence\u00a2 of .C its truth value 11\u00a211 E Q is defined as: 11\u00a211 = V{1r : 1rlt-\u00a2 }\nThe following properties hold for the above semantics:\nFact 5 For any domain D and continuous t-norm x, the function II \u00b7 II : .C --+ PD satisfies:\nIIR(c1, ... , Cn)ll\nllall IIA & EII IIA El1 Ell IIA0EII\nll'v'xA(x)ll\nll3xA(x)ll\n=\n=\nAw. { \ufffd if w I= R(c1, ... ,en) otherwise Cl'"}, {"heading": "IIAII A IIEII", "text": "IIAIIV IIEII IIAII X IIEII\n1\\ IIA(u)ll uED\nV IIA(u)ll uED\nIIA-+ Ell IIAII=:>IIEII II\u00b7AII\nIlLII\nlla El1 Lll\n=\n\u00b7IIAII\nAw. { Aw. { 1 if w E Modn(L) 0 otherwise\n1 if w E Modn(L) Cl' otherwise\nThe following property shows that classical logic is contained in LPL.\nFact 6 For any domain D, the set ED = { 1r E PD I for allw E MD,1r(w) E {0, 1}} is a Boolean algebra contained in the structure PD.\nNote that .C1-formulae are mapped into the Boolean algebra, so it is natural to expect that they provide an exact copy of first-order logic embedded inside .C.\n4.3 Encoding SPL in LPL\nThis section shows that SPL (standard possibilistic logic) can be merged inside LPL. First, note that the last equality given in the Fact 5 suggests a natural way of representing standard possibilistic logic inside LPL: consider that the least informative (i.e.,\u00b7 spe cific) possibilistic distribution 1r satisfying the condit. N (L) . , { 1 if w f= L lOll ec7f > 0: IS: 7r = AW. 1 th \u00b7 - - o: o erw1se so the token of information N ec1f ( L) 2: o: can be rep resented in LPL by (1 - o:) EB L. Now, it is easy to see that there is a complete translation of the afore mentioned fusion operations. Assume that the set of atomic propositions is the same for the two lan guages: this means that the set of atomic propositions that appear in the formulae L; belonging to the base \ufffd = {(L;,o:;) : i = 1 , \u00b7 \u00b7 \u00b7 , n} is the same as the one that generates .C. We can give the following transla tion T from the language of SPL to .C1 :\n1. T(P) = P for P atomic proposition 2. T(L 1\\ M) = T(L)&T(M) 3. T(L V M) = T(L) EB T(M)\n4. T(L -t M) = T(L) -t T(M) 5. T(-.L) = -.T(L)\nOnce we have a translation for the classical part we can extend it to the bases and their composition:\n1. T({(L;, a;) : i = 1, . . \u00b7 , n}) = &i<n(1-ai) EB T(L;) -"}, {"heading": "2. T(\ufffd1\u00ae\ufffd2) = T(\ufffd1)\u00aeT(\ufffd2), where \ufffd1\u00ae\ufffd2 means", "text": "the combination with the t-norm of the knowledge bases \ufffd1, \ufffd2, as described in Section 3.2.\n3. T(\ufffd1 + \ufffd2) = T(\ufffdl) EB T(\ufffd2), where +represents the sup t-conorm.\nThe above translation is faithful as it is shown:\nFact 7 For every \ufffd \ufffd ?T is the associated distribution of \ufffd iff ?T = I IT(\ufffd)II\u00b7\nHence the meta statement \"fuse the two knowledge bases \ufffd1 and \ufffd2 using a t-norm or t-conorm operator\" can be represented using a sentence of the language.\n5 Proof system\nThe proof system consists of four parts: structural rules, logical rules, axioms for distributivity, and three further \" numerical\" axioms. The dependence of the calculus on the parameter x is quite circumscribed, and limited to numerical rules, so that the parameters do not affect the logical core of the system. The \u00ae connective is in general not idempotent. The calculus will therefore miss the structural rule of contraction, in the style of substructural logics [Girard 1987]; in fact a weak form of contraction is allowed, which can only be applied to the sublanguage \u00a31. In presenting the calculus, we take the freedom of writing 1- instead of the parametric 1-x, assuming we are dealing with a fixed t-norm.\nStructural rules:\nid) Al-A\nexL) r,B,A,ai-C r,A,B,ai-C\n.C1con) r,A,LI-B a,AI-L\nr,a,AI-B\nLogical rules:\n&) rAI-C rBI-C\nr,A&BI-C r,A&BI-C\n0 ) r,A,BI-C\nr,A0Bf-C\n$) r,AI-C r,BI-C\nr,BE!)AI-C\n-+) ri-A a,BI-C r,a,A-tBI-C\n\\f) r,A t 1-B\nr,\\fxA X 1-B\n3) r,A(x)f-B * r,3xA(x )I-B\n1) ri-A r,11-A O) r,o 1- A\n..,..,) -,-,L 1- L L E \u00a31 * X not free in r' B\ncut) r1-B a,BI-C\na,r1-c\nweL) ri-B\nr,AI-B\nL E \u00a31\nri-A r1-B ri-A&B\nri-A a1-B r,af-A0B\nr1-A ri-B ri-AE!)B ri-AE!)B\nr,AI-B ri-A-tB\nri-A x rl-\\fxA x *\nri-A t rl-3xA(x\nr1-1\nMerging possibilistic knowledge bases 13\nDistributivity: 0-& distr)\nE!)- & distr) (A 0 C) & (B 0 C) 1-(A & B) 0 C (A E!) C) & (BE!) C) 1- (A & B) E!) C\n0-\\f distr) \\fxA(x) 0 C -if- \\fx(A(x) 0 C) if x is not free in C\nCD) 'Vx(A E!) B(x)) 1- A E!) \\fxB(x) if xis not free in A\nNumerical rules:\nS') {3 1- a for any f3 S a 0 def) a @ {3 -jj- / where \"Y = a X {3 -,def) -.a -11- 1 where \"Y = a :::? 0\nValidity and completeness hold, as the following theo rem shows:\nTheorem 2 For any r and B: r 1- B if and only if for every ?T if ?T H- \u00ae r then ?T H-B.\nwhere if f is the multiset {A1, . . \u00b7, An} then \u00aef =df A1 \u00ae A2 \u00ae \u00b7 \u00b7 \u00b7 \u00ae An. All the proofs are in [Boldrin and Sossai 1997]. Let us recall the main problem: we have some amount of information repre sented by some graded formulae, we apply the fusion procedure and obtain a theory \ufffd. Given a formula A what is the degree of necessity of A computed using the available knowledge? If \ufffd 1- a EB A then, due to the validity property of the proof system, for every ?T s.t. ?TH-\ufffd we have that ?TH-aEBA and this means that: N ec'll\" (A) \ufffd 1 - a. On the other hand, completeness gives the following property: if for every ?T H-\ufffd we have that N ec'll\" (A) \ufffd 1 -a then there is a proof of the se quent: \ufffd 1- a EB A. Hence the proof system gives a correct (validity) and powerful enough (completeness) procedure to solve the problem. Note that every proof of r 1- a EB A terminates with two types of leaves:\n1. purely logical, i.e. of the form L 1- L\n2. purely numerical, i.e. f3 1- a, or a \u00ae f3 1- \"'f, ...\nThe purely numerical leaves give the constraints which /3, a, 'Y must satisfy: for example, f3 1- a is provable if and only if f3 :::; a. Hence we can use the sequent calculus to determine the necessity value of a formula A as follows: start searching a proof of x EB A with x undetermined, such proof gives the constraints which determine the value of x.\n5.1 Example\nLet us consider again the robotics example. First the available information, given in the example of Section 3.3., can be codified in our language using the following set r: ((D&E&F)EBG)--+ H, ryEB((A&B)--+ D),B1 EB A, B2 EBB, 84 EB E, (}5 EB F, (}6 EB G. Now let us see how a proof (i.e. a single sequent) of r 1- X EB H allows us to compute the value of x as a function of the degrees of the formulae appearing in r 0\nDue to the lack of space we will split the sequent into some smaller parts, nevertheless the reader can easily reconstruct the complete proof. We will start with three sub-proofs that appear inside the proof of r I X EBH.\n14 Benferhat and Sossai\n&-Comp) Note that the following are provable:\n1 .\n2.\ncq 1-x\na1 1-X L1l-L1,x$ (L1&L2) \u00a31,-,\u00a31 1-X Ell (\u00a31&\u00a32)\na1,-,\u00a31&-,\u00a32 1-X Ell (L1&L2) \u00a31,-,\u00a31&-,\u00a32 1- X Ell (L1&L2) (a1 Ell Ll), -,\u00a31&-,\u00a32 1-x Ell (L1&L2)\nNote that this is a proof iff a1 f- x is provable i.e. iff a1 :::; x\nL2 1-\u00a32 L2, -,\u00a32 1-0 0 1-X E!) (L1&L2)\nL2, -,\u00a32 1-X Ell (L1&L2) a2, -,\u00a32&-,\u00a32 1-X Ell (L1&L2) L2, -,\u00a31&-,\u00a32 1- X Ell (L1&L2)\n(a2 Ell \u00a32). L1&\"\"\"'L2 1- X Ell (L1&L2) (a1 Ell Ll)&(a2 Ell L2),L1&\"\"\"'L2 1-x Ell (L1&L2)\nNote that this is a proof iff a2 f- x is provable i.e. iff a2 :::; x\n3. The case: (a1 ffi Ll)&(a2 ffi L2), ...,L1&L2 f- x ffi (L1&L2) is similar to the above one.\n4. L1&L2 1-L1&L2\nL1&L2 1-X Ell (\u00a31&\u00a32) (a! Ell LJ)&(a2 E!) \u00a32), L1&L2 1-x Ell (L1&L2)\nNo constraints on x\nAll the above sequents are provable iff x 2:: a1 and x 2:: a2, i.e. x 2:: (a1 V a2). If we apply the (f) rule on the left to the four proofs we obtain the se quent: ( a1 ffiLl)&( a2 ffiL2), ( ....,\u00a31 &....,L2) ffi (L1 &....,L2)ffi (....,L1&L2) ffi (L1&L2) f- (a1 ffi a2) ffi (L1&L2). Note that due to the fact that (....,L1&'L2) ffi (L1&'L2) ffi (....,L1&L2) ffi (L1&L2) is a classical tautology we have that f- ( -,\u00a31 &-.L2) ffi (L1 &-.L2) ffi ( ....,\u00a31 &L2) ffi (L1 &L2) is a provable sequent and hence, due to the fact that weakening holds, ( a1 ffi L1 )&( a2 ffi L2) f- ( -,\u00a31 &-.L2) ffi (L1&-.L2) ffi (-.L1&L2) ffi (L1&L2) is a provable se quent. Now using .C1 contraction we get a proof of: (a1 ffi Ll)&(a2 ffi L2) f- x ffi (L1&L2) iff x 2:: a1 V a2 The following gives us a Generalized Modus Ponens\nGM)\nBI-B {31-x al-x Al-A Bl-x$B f3 1- x Ell B a 1- x Ell B A, A-t B 1-x Ell B {3, a Ell (A-t B) 1-x Ell B A, a Ell (A -t B) 1-x Ell B\na Ell (A -t B), f3 Ell A 1-x Ell B\nNote that: a ffi (A -t B), j3 ffi A f- x ffi B iff x 2:: a V j3\n0-Comp)\n6 1- 6 {1 1- {1 G 1-G 6,{1 1-6 181 {1 {2 181 {1 1- x G 1-K Ell G H 1-H\n{2, {1 1-x (K Ell G) -t H, G 1-H {2,{11-xE!JH (K$G)-tH,GI-x$H\n(K$G)-tH,6,{11-x$H (K$G)-tH,6,GI-x$H\n(K Ell G) -t H, {2, {! Ell G 1-x Ell H\nThe above is a proof iff x 2:: \ufffd1 x 6 KI-K\nKI-K$G HI-H (K Ell G) -t H, K 1-H\n(K Ell G) -t H, K 1-X Ell H\nKI-K K,GI-K\nK, G 1-K Ell G H 1- H (K Ell G) -t H, K, G 1-H\n(K Ell G) -t H,K,{1 1-x$ H (K Ell G) -t H,K,G 1-x $H (K Ell G) -t H, K, {1 Ell G 1- x Ell H\nNo constraints on x. Combining the two above proofs with an (f)-rule and then applying a 0 rule we obtain the following: (I< ffi G) -+ H, (6 ffi K) 0 (6 ffi G) f x EB H iff x 2:: \ufffd1 x 6 Due to the lack of space we must assume the following abbreviation:\n1. f1: (04\\BE)&(fhEBF), ((D&E&F)ffiG)-+ H, ryffi ((A&B) -t D), (81 ffi A)&(02 ffi B), 06 EB G.\n2. f2 : ((D&E&F) EB G) -t H, (6 ffi (D&E&F)) 0 (06\\f!G).\n3. fa : (84 ffi E)&(05 ffi F), \"1 EB ((A&B) -+D), (81 EB A)&(02 ffi B), 06 ffi G.\n4. f 4 : ( 84 EB E)&( 85 ffi F), \"1 ffi ( (A&B) -+ D), ( 81 EB A)&( 82 EB B).\nLet us prove r1 f- X ffi H: r4 1-6 Ell (E&F&D) 86 Ell G 1-86 Ell G 181- Comp\nr3 1- (86 Ell G) 0 (6 Ell (E&F&D)) r2 1-x Ell H r1 1-xE!J H\nLet us prove f4 f- 6 EB (E&F&D) & - Comp1 r4 1-(a Ell (E&F))&(f3 Ell D) (a Ell (E&F))&(f3 Ell D) 1-{2 Ell (E&F&D) r4 1- {2 Ell (E&F&D)\nThe proof of f4 f- (a ffi (E&F))&(/3 EB D) is: & - Comp3\n(84 Ell E)&(8s Ell F) 1-a Ell (E&F) r4 1-a$ (E&F)\n& - Comp2 GM (81 Ell A)&(82 Ell B) 1-1 Ell (A&B) 1/ Ell (A&B -t D), 1 Ell (A&B) 1-f3 Ell D\n(81 Ell A)&(82 Ell B), 1/ Ell (A&B -+D) 1- {3 Ell D r4l-f3E!JD\nnow applying an & rule to the two above proven for mulas we obtain: r 4 f- (aEB(E&F))&(/3ffiD) Note that the above proof gives us the following constraints:\n1. from 0-Comp we have that: x 2:: 06 x 6. 2. from & -Comp1 we have that: 6 2:: a V j3 3. from GM we have that: j3 2:: 1 V \"1\u00b7 4. from & -Comp2 we have that: 1 2:: 81 V 82 5. from & -Comp3 we have that: a 2:: 84 V 85\nSolving all the above inequalities we have: x 2:: 06 x ( 81 V 82 V 84 V 85 V \"1) If we want the highest value for the necessity of H compatible with the above constraints we get the following function (which leads to minimize the value of x) : x = 06 x (01 V 02 V 04 V 05 V ry) If we substitute the numbers to the corresponding vari ables, the above function gives us the same number as\nin the third proposed solution to the example of Sec tion 3.3. In fact, N(H) = 1 -x = 1 - 0.4 x (0.6 V 0.5 V 0.3 V 0.6 V 0.2) = 1-0.4 X 0.6 = 1-0.24 = 0.76. This procedure computes N(H) as a function of the available information. It is also interesting to note that f1 : (84 E8 E)&(05 E8 F), ((D&E&F) E8 G) -+ H, TJ E8 ((A&B) -+ D), (81 E8 A)&(Oz E8 B), 06 E8 G gives us an explicit analysis of the implicit depen dency /independency assumptions made in the compu tation of X. In fact a comma inside r 1 is equivalent to a \u00ae-combination of the formulae, hence the formu lae separated by a comma are assumed as indepen dent, while those combined by the & are assumed as not independent: for example (04 E8 E)&(05 E8 F) and ((D&E&F) E8 G) -+ H are assumed as independent, while ( 04 EB E) and ( 05 EB F) are assumed as dependent. Last but not least, let us stress the fact that here logic is used as a mathematical method to study the func tion that computes the degree of the conclusion using the degrees of the premises. Such function is directly implemented on the robot.\n6 Conclusions\nThis paper has proposed several syntactical combina tion modes which can be applied to possibilistic knowl edge bases. The work presented in this paper is closely related to the one proposed in (Boldrin and Saffiotti, 1995) . Several differences can be mentioned: first their work is based on possible world semantics while in this paper the algebraic one is used. Secondly, in their world it is possible to identify the different sources (using one modality per source) which is not possible in our work. Thirdly, negation used in the two logics is not the same. Lastly, in this paper it is possible to have a many-level merging of information using difer ent t-norms which is not possible in their approach. Two methods have been investigated: the first one is achieved at the standard possibilistic logic while the second method is based on an extension of the lan guage of possibilistic logic. The first method requires a preliminary partitioning of the knowledge base into sub-bases, then the combination of the sub-bases to generate new knowledge bases, and eventually the ap plication of the resolution algorithm to calculate the necessity degree of a formula. In the second method this process is locally controlled by the sequent calcu lus, which chooses in the base the formulae required for the proof, and it combines them as appropriate to the development of the demonstration. The main limit of the first method is that it does not treat predicate logic, while the main limit of the second method is that it does not allow the combination of bases with t-conorms different from the maximum. Lastly, the combination techniques proposed in this paper have been successfully applied to the problem of position estimation (localization) of a mobile robot in a par tially and approximately known indoor environment (for more details, see [Bison et al., 1997a,b,1998]) . Acknowledgments This work was partially supported by a CNR-CNRS joint\nMerging possibilistic knowledge bases 15\nproject (CNR Code 132.3.1) and the Fusion Project of the European Community. The second author is indebted to A. Saffiotti of Iridia and to G. Chemello of Ladseb.\nReferences\n[Abidi and Gonzalez 1992] Abidi M.A. and Gonzalez R.C. Data Fusion in Robotics and Machine Intelligence. Academic Press, New York.\n[Benferhat et al. 1995] Benferhat S., Dubois D. and Prade H. How to infer from inconsistent beliefs without re vising?. Proc. of the 14th Inter. Joint Conf. on Artif. Intell. (IJCAI'95), 1449-1455.\n[Benferhat et al. 1997] Benferhat S., Dubois D. and Prade H. From semantic to syntactic approaches to informa tion combination in possibilistic logic\". In Aggrega tion and Fusion of Imperfect Information, Studies in Fuzziness and Soft Computing (B. Bouchon-Meunier, ed.), Physica Verlag, 141-151.\n(Bison et al. 1997a] , P. Bison, G. Chemello, C. Sossai and G. Trainito, Logic-Based Sensor Fusion for Localiza tion, in Proc. of the IEEE International Symposium on Computational Intelligence in Robotics and Au tomation (CIRA '97}, IEEE CS, pp. 254-259.\n[Bison et al. 1997b] , P. Bison, G. Chemello, C. Sossai and G. Trainito, A Possibilistic Approach to Sensor Fusion in Mobile Robotics, in Proceedings of the 2nd Eu romicro Workshop on Advanced Mobile Robots (Eu robot'97}, IEEE Computer Society, pp. 73-79.\n[Bison et al. 1998] , P. Bison, G. Chemello, C. Sossai and G. Trainito, Cooperative Localization using Possi bilistic Sensor Fusion, in Proc. of the Srd IFA C Sym posium on Intell. Autonomous Vehicles (IAV'98}.\n[Boldrin and Saffiotti 1995] L. Boldrin and C. Saffiotti. A modal logic for merging partial belief of multiple rea soners. Technical Report TR/IRIDIA/95-19.\n[Boldrin and Sossai 1995] L. Boldrin and C. Sossai. An algebraic semantics for possibilistic logic. In Proc. of UAI-95, pages 24-37.\n[Boldrin and Sossai 1997] L. Boldrin and C. Sossai. Local Possibilistic Logic, Journal of Applied Non-Classical Logic, 7(3):309-333, 1997.\n[Cholvy1992] A logical approach to multi-sources reason ing. In: Applied Logic Conference: Logic at Work, Amsterdam.\n[Dubois et al. 1994] D. Dubois, J. Lang and H. Prade. Possibilistic logic. In D. M. Gabbay et al., eds, Hand book of Logic in Artificial Intelligence and Logic Pro gramming (Volume 3}, 439-513.\n[Dubois and Prade 1992] Dubois D., Prade H. Combina tion of fuzzy information in the framework of possi bility theory. In: Data Fusion in Robotics and Ma chine Intelligence (M.A. Abidi, R.C. Gonzalez, eds.) Academic Press, New York, 481-505.\n[Flamm and Luisi 1992] Flamm and Luisi. Reliability Data and Analysis. Kluwer Academic Publ.\n(Girard 1987] J .-Y. Girard. Linear logic. Theoretical com puter science, 50:1-101, 1987.\n(Pavelka 1979] J. Pavelka. On fuzzy logic i, ii, iii. Zeitschr. f. math. Logik und Grundlagen d. Math, 25:45-52; 119-131; 447-464, 1979.\n[Rosenthal 1990] K. I. Rosenthal. Quantales and their ap plications. Longman, 1990."}], "references": [{"title": "Data Fusion in Robotics and Machine Intelligence", "author": ["Abidi", "M.A. Gonzalez 1992] Abidi", "R.C. Gonzalez"], "venue": null, "citeRegEx": "Abidi et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Abidi et al\\.", "year": 1992}, {"title": "How to infer from inconsistent beliefs without re\u00ad vising", "author": ["Benferhat"], "venue": "Proc. of the 14th Inter. Joint Conf. on Artif. Intell", "citeRegEx": "Benferhat,? \\Q1995\\E", "shortCiteRegEx": "Benferhat", "year": 1995}, {"title": "From semantic to syntactic approaches to informa\u00ad tion combination in possibilistic logic", "author": ["Benferhat"], "venue": "In Aggrega\u00ad tion and Fusion of Imperfect Information, Studies in Fuzziness and Soft Computing (B", "citeRegEx": "Benferhat,? \\Q1997\\E", "shortCiteRegEx": "Benferhat", "year": 1997}, {"title": "Logic-Based Sensor Fusion for Localiza\u00ad", "author": ["Bison"], "venue": "tion, in Proc. of the IEEE International Symposium on Computational Intelligence in Robotics and Au\u00ad tomation (CIRA '97},", "citeRegEx": "Bison,? \\Q1997\\E", "shortCiteRegEx": "Bison", "year": 1997}, {"title": "A Possibilistic Approach to Sensor Fusion in Mobile Robotics, in Proceedings of the 2nd Eu\u00ad romicro Workshop on Advanced Mobile Robots (Eu\u00ad robot'97", "author": ["Bison"], "venue": "IEEE Computer Society,", "citeRegEx": "Bison,? \\Q1997\\E", "shortCiteRegEx": "Bison", "year": 1997}, {"title": "Cooperative Localization using Possi\u00ad bilistic Sensor", "author": ["Bison"], "venue": "Fusion, in Proc. of the Srd IFA C Sym\u00ad posium on Intell. Autonomous Vehicles (IAV'98}", "citeRegEx": "Bison,? \\Q1998\\E", "shortCiteRegEx": "Bison", "year": 1998}, {"title": "A modal logic for merging partial belief of multiple rea\u00ad soners", "author": ["Boldrin", "Saffiotti 1995] L. Boldrin", "C. Saffiotti"], "venue": "Technical Report TR/IRIDIA/95-19", "citeRegEx": "Boldrin et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Boldrin et al\\.", "year": 1995}, {"title": "An algebraic semantics for possibilistic logic", "author": ["Boldrin", "Sossai 1995] L. Boldrin", "C. Sossai"], "venue": "In Proc. of UAI-95,", "citeRegEx": "Boldrin et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Boldrin et al\\.", "year": 1995}, {"title": "Local Possibilistic Logic", "author": ["L. Boldrin", "C. Sossai"], "venue": "Journal of Applied Non-Classical Logic, 7(3):309-333", "citeRegEx": "Boldrin and Sossai 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Combina\u00ad tion of fuzzy information in the framework of possi\u00ad bility theory. In: Data Fusion in Robotics and Ma\u00ad chine Intelligence (M.A", "author": ["Dubois", "D. Prade 1992] Dubois", "H. Prade"], "venue": null, "citeRegEx": "Dubois et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Dubois et al\\.", "year": 1992}, {"title": "Reliability Data and Analysis", "author": ["Flamm", "Luisi 1992] Flamm", "Luisi"], "venue": null, "citeRegEx": "Flamm et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Flamm et al\\.", "year": 1992}, {"title": "Quantales and their ap\u00ad plications", "author": ["K.I. Rosenthal"], "venue": "Longman", "citeRegEx": "Rosenthal 1990", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 8, "context": "All the proofs are in [Boldrin and Sossai 1997].", "startOffset": 22, "endOffset": 47}], "year": 2011, "abstractText": "This paper addresses the problem of merg\u00ad ing uncertain information in the framework of possibilistic logic. It presents several syn\u00ad tactic combination rules to merge possibilis\u00ad tic knowledge bases, provided by different sources, into a new possibilistic knowledge base. These combination rules are first de\u00ad scribed at the meta-level outside the lan\u00ad guage of possibilistic logic. Next, an exten\u00ad sion of possibilistic logic, where the combi\u00ad nation rules are inside the language, is pro\u00ad posed. A proof system in a sequent form, which is sound and complete with respect to the possibilistic logic semantics, is given.", "creator": "pdftk 1.41 - www.pdftk.com"}}}