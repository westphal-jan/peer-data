{"id": "1301.7363", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Empirical Analysis of Predictive Algorithms for Collaborative Filtering", "abstract": "frequent filtering or recommender model use program portion exploring specific preferences to predict additional topics or products a new traveler might consume. in this function we describe several algorithms formulated alongside this task, incorporate techniques based analytical correlation coefficients, vector - based function calculations, and systematic reference methods. we compare the aggregate accuracy of the various methods regarding a set of known problem domains. strategies guide highly basic classes by evaluation indices. the first models accuracy over a set of individual applications expressing conditions above average absolute efficiency. higher second estimates the utility form a consistent list under generic items. a metric needs linear estimate of the indicator that someone user will compare a recommendation through what ordered segment. experiments were run for datasets dealt with database application areas, 4 experimental protocols, connecting the 2 interaction meters from the various algorithms. results indicate error for, wide range of tools, bayesian functions with confidence trees multiple data page exhibit different methods outperform bayesian - clustering and vector - similarity predictions. between vectors and bayesian networks, the preferred estimation scales the the nature affecting the function, nature of query application ( ranked versus matched - by - one parameter ), and higher availability variable votes surrounding which participants coordinate predictions. practical considerations include sampling size of database, speed of predictions, and learning time.", "histories": [["v1", "Wed, 30 Jan 2013 15:02:44 GMT  (291kb)", "http://arxiv.org/abs/1301.7363v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["john s breese", "david heckerman", "carl kadie"], "accepted": false, "id": "1301.7363"}, "pdf": {"name": "1301.7363.pdf", "metadata": {"source": "CRF", "title": "Empirical Analysis of Predictive Algorithms for Collaborative Filtering", "authors": ["John S. Breese", "David Heckerman", "Carl Kadie"], "emails": ["@microsoft."], "sections": [{"heading": null, "text": "Collaborative filtering or recommender sys tems use a database about user preferences to predict additional topics or products a new user might like. In this paper we describe several algorithms designed for this task, in cluding techniques based on correlation coef ficients, vector-based similarity calculations, and statistical Bayesian methods. We com pare the predictive accuracy of the various methods in a set of representative problem domains. We use two basic classes of evalua tion metrics. The first characterizes accuracy over a set of individual predictions in terms of average absolute deviation. The second esti mates the utility of a ranked list of suggested items. This metric uses an estimate of the probability that a user will see a recommen dation in an ordered list.\nExperiments were run for datasets associ ated with 3 application areas, 4 experimen tal protocols, and the 2 evaluation met rics for the various algorithms. Results indicate that for a wide range of con ditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian-clustering and vector similarity methods. Between correlation and Bayesian networks, the preferred method de pends on the nature of the dataset, nature of the application (ranked versus one-by-one presentation), and the availability of votes with which to make predictions. Other con siderations include the size of database, speed of predictions, and learning time.\n1 Introduction\nTypically, automated search over a corpus of items is based on a query identifying intrinsic features of the items sought. Search for textual documents (e.g. Web pages) uses queries containing words or describ ing concepts that are desired in the returned docu ments. Search for titles of compact discs, for example, requires identification of desired artist, genre, or time period. Most content retrieval methodologies use some type of similarity score to match a query describing the content with the individual titles or items, and then present the user with a ranked list of suggestions.\nA complementary method of identifying potentially in teresting content uses data on the preferences of a set of users. Typically, these systems do not use any infor mation regarding the actual content (e.g. words, au thor, description) of the items, but are rather based on usage or preference patterns of other users. So called collaborative filtering or recommender systems [Resnick and Varian, 1997] are built on the assump tion that a good way to find interesting content is to find other people who have similar interests, and then recommend titles that those similar users like.\nThough there is increasing commercial interest in col laborative filtering technology, there has been little published research on the relative performance of var ious algorithms used in collaborative filtering systems. In this paper we describe various collaborative filtering prediction methodologies, including previously pub lished algorithms based on correlation coefficients, as well as algorithms based on learning Bayesian mod els. We present empirical data regarding the relative predictive performance of the various algorithms and extensions. Although we present some results address ing the computational and scalability issues involved in applying the various algorithms, our primary empha sis is the accuracy and the quality of recommendations of the predictive component.\n2 Collaborative Filtering Algorithms\nThe task in collaborative filtering is to predict the util ity of items to a particular user (the active user) based on a database of user votes from a sample or popula tion of other users (the user database). In this paper we will examine two general classes of collaborative filtering algorithms. Memory-based algorithms oper ate over the entire user database to make predictions. In Model-based collaborative filtering, in contrast, uses the user database to estimate or learn a model, which is then used for predictions.\nCollaborative filtering systems are often distinguished by whether they operate over implicit versus explicit votes. Explicit voting refers to a user consciously ex pressing his or her preference for a title, usually on a discrete numerical scale. For example, GroupLens sys tem of Resnick et al. [1994) uses a scale of one (bad) to five (good) for users to rate Netnews articles, and users explicitly rate each article after reading it. Im plicit voting refers to interpreting user behavior or se lections to impute a vote or preference. Implicit votes can based on browsing data (for example in Web ap plications), purchase history (for example in online or traditional stores), or other types of information access patterns.\nRegardless of the type of vote data available, collab orative filtering algorithms must address the issue of missing data- we typically do not have a complete set of votes across all titles. We cannot assume that items are missing at random. In most applications, users will vote on items they have accessed, and are more likely to access (and vote) on items they like.\nMany of the applications of interest to us involve im plicit voting, and some of the algorithms described in the next section rely on an interpretation that any vote appearing in the database indicates a positive pref erence. We also show that by making different as sumptions about the nature of missing data, the per formance of collaborative filtering algorithms can be improved.\n2.1 Memory-Based Algorithms\nGenerally, the task in collaborative filtering is to pre dict the votes of a particular user (we will refer to this user as the active user) from a database of user votes from a sample or population of other users. The user database therefore consists of a set of votes vi,j corre sponding to the vote for user i on item j. If Ii is the set of items on which user i has voted, then we can define the mean vote for user i as:\n1 'ih = -IJ.I L Vi,j 2 jEI;\nIn memory-based collaborative filtering algorithms, we predict the votes of the active user (indicated with a subscript a) based on some partial information regard ing the active user and a set of weights calculated from the user database. We assume that the predicted vote of the active user for item j, Pa,j, is a weighted sum of the votes of the other users:\nn\nPa,j = Va + \ufffd\ufffd; L w(a, i)(vi,j- 'ih) (1) i=l\nwhere n is the number of users in the collaborative filtering database with nonzero weights. The weights w(i, a) can reflect distance, correlation, or similarity between each user i and the active user. \ufffd\ufffd; is a normal izing factor such that the absolute values of the weights sum to unity. In the following, we distinguish between the various collaborative filtering algorithms in terms of the details of the \"weight\" calculation. There are other possible characterizations for memory-based col laborative filtering, however in this paper we restrict ourselves to the formulation described above.\n2.1.1 Correlation\nThis general formulation of statistical collaborative filtering (as opposed to verbal or qualitative anno tations) first appeared in the published literature in the context of the GroupLens project, where the Pear son correlation coefficient was defined as the basis for the weights [Resnick et al., 1994). The correlation be tween users a and i is:\n( \") \"L-j(va,j- Va)(vi,j- Vi) w a, 2 = 1==== =========\nJ\"L-j ( Va,j - Va)2 \"L-j ( Vi,j - Vi)2 (2)\nwhere the summations over j are over the items for which both users a and i have recorded votes.\n2.1.2 Vector Similarity\nIn the field of information retrieval, the similarity be tween two documents is often measured by treating each document as a vector of word frequencies and computing the cosine of the angle formed by the two frequency vectors [Salton and McGill, 1983). We can adopt this formalism to collaborative filtering, where users take the role of documents, titles take the role of words, and votes take the role of word frequencies. Note that under this algorithm, observed votes indi cate a positive preference, there is no role for negative\nvotes, and unobserved items receive a zero vote. The relevant weights are now\n(3)\nwhere the squared terms in the denominator serve to normalize votes so that users that vote on more ti tles will not a priori be more similar to other users. Other normalization schemes, including absolute sum and number of votes, are possible.\n2.2 Extensions to Memory-Based Algorithms\nWe have investigated a number of modifications to the standard algorithms that can improve performance. We describe these extensions here and the effective ness of each is discussed in Section 4.\n2.2.1 Default Voting\nDefault voting is an extension to the correlation algo rithm described in Section 2.1.1. It arose out of the observation that when there are relatively few votes, for either the active user or the matching user, the cor relation algorithm will not do well because it uses only votes in the intersection of the items both individuals have voted on (Ian Ij)\u00b7 If we assume some default value as a vote for titles for which we do not have explicit votes, then we can form the match over the union of voted items,(Ia U Ij), where the default vote value is inserted into the formula for the appropriate unobserved items.\nIn addition, we can assume the same default vote value d for some number of additional items k that neither user has voted on. This has the effect of assuming there are some additional number of unspecified items that neither user voted on, but they would nonetheless agree on.1 In most cases, the value for d will reflect a neutral or somewhat negative preference for these unobserved items.\nIn applications with implicit voting, an observed vote is typically an indication of a positive preference (e.g. a visit to the Web page is assigned a vote value of 1). In this case the default vote can take on the value associ ated with \"did not visit\" or 0. In this instance, default voting takes on the role of extending the data for each user with the true value for missing data. Note, how ever, we only calculate weights for users who match the active user on at least one item.\n1In our experiments, we have used a value of 10,000 or k.\n2.2.2 Inverse User Frequency\nIn applications of vector similarity in information re trieval, word frequencies are typically modified by the inverse document frequency [Salton and McGill, 1983]. The idea is to reduce weights for commonly occurring words, capturing the intuition that they are not as use ful in identifying the topic of a document, while words that occur less frequently are more indicative of topic. We can apply an analogous transformation to votes in a collaborative filtering database, which we term inverse user frequency. The idea is that universally liked items are not as useful in capturing similarity as less common items. We define the fi as log ;:. where 1 nj is the number of users who have voted for item j and n is the total number of users in the database. Note that if everyone has voted on a item j, then the fi is zero.\nTo apply inverse user frequency while using the vec tor similarity algorithm, we use a transformed vote in Equation 3. The transformed vote is simply the orig inal vote multiplied by the fi factor. In the case of correlation, we modify Equation 2 so that the fi is treated as a frequency and an item with a higher fi is assigned more weight in the correlation calculation. The relevant correlation weight with inverse frequency is:\nw(a,i) =\nwhere\n2.2.3\nLj /j Lj fiva,jVi,j- (Lj fiva,j)(Lj fivi,j)) VfJV\nj j j\nj j j\nCase Amplification\nCase amplification refers to a transform applied to the weights used in the basic collaborative filtering pre diction formula as in Equation 1. We transform the estimated weights as follows\nI { Wp. W _ a,t a,i- -( -wP .) a,t if Wa,i 2:: 0 if Wa,i < 0 The transform emphasizes weights that are closer to one, and punishes low weights. A typical value for p for our experiments is 2. 5.\n2.3 Model-Based Methods\nFrom a probabilistic perspective, the collaborative fil tering task can be viewed as calculating the expected value of a vote, given what we know about the user. For the active user, we wish to predict votes on as yet unobserved items. If we assume that the votes are integer valued with a range for 0 to m we have:\nm\nPa,j = E(va,j) = '2:: Pr (va,j = ilva,k, k E Ia) i (4) i=O\nwhere the probability expression is the probability that the active user will have a particular vote value for item j given the previously observed votes. In this paper we examine two alternative probabilistic models for collaborative filtering, cluster models and Bayesian networks.\n2.3.1 Cluster Models\nOne plausible probabilistic model for collaborative fil tering is a Bayesian classifier where the probability of votes are conditionally independent given membership in an unobserved class variable C taking on some rel atively small number of discrete values. The idea is that there are certain groups or types of users cap turing a common set of preferences and tastes. Given the class, the preferences regarding the various items (expressed as votes) are independent. The probability model relating joint probability of class and votes to a tractable set of conditional and marginal distributions is the standard \"naive\" Bayes formulation:\nn\nPr (C = c,v1, ... , vn) = Pr(C = c) Il Pr (viiC =c) i=l\nThe left-hand side of this expression is the probability of observing an individual of a particular cl_(l.Ss and a complete set of vote values. It is straightforWard to cal culate the needed probability expressions for Equation 4 within this framework. This model is also known as a multinomial mixture model.\nThe parameters of the model, the probabilities of class membership Pr(C = c) , and the conditional prob abilities of votes given class Pr (v;IC =c) are esti mated from a training set of user votes, the user database. Since we never observe the class variables in the database of users, we must employ methods that can learn parameters for models with hidden variables. We use the EM algorithm [Dempster et al., 1977) to learn the parameters for a model structure with a fixed number of classes. We choose the number of classes by selecting the model structure that yields the largest (approximate) marginal likelihood of the data. We\nuse the method of Cheeseman and Stutz (1995) to ap proximate the marginal likelihood (see also Chicker ing and Beckerman, 1997). In our experiments, we assume each model structure (every possible number of classes) is equally likely, and use a uniform prior for model parameters. We initialize the EM algorithm using the marginal-plus-noise technique described in [Thiesson et al., 1 997).\n2.3.2 Bayesian Network Model\nAn alternative model formulation for probabilistic col laborative filtering is a Bayesian network with a node corresponding to each item in the domain. The states of each node correspond to the possible vote values for each item. We also include a state corresponding to \"no vote\" for those domains where there is no natural interpretation for missing data.\nWe then apply an algorithm for learning Bayesian net works to the training data, where missing votes in the training data are indicated by the \"no vote\" value. The learning algorithm searches over various model structures in terms of dependencies for each item. In the resulting network, each item will have a set of par ent items that are the best predictors of its votes. Each conditional probability table is represented by a deci sion tree encoding the conditional probabilities for that node. An example of such a tree, for television view ing data (see Section 3.2) is shown in Figure 1. Details of the learning algorithm are discussed in Chickering et al.(1997). In the remainder of the paper the term Bayesian network will refer to these networks with a decision tree for each title.\nIn the experiments that follow, we use a structure prior that penalizes each additional free parameter with probability 0.1, and derive parameter priors from a prior network as described in Chickering et al., 1997.\nIn particular, we use a prior network that encodes a uniform distribution over all possible outcomes and an equivalent sample size of 10. Experiments on subsets of the training data showed these parameters to pro duce accurate results, although there was little sensi tivity.\n3 Empirical Analysis\nThe purpose of this paper is to evaluate the predictive accuracy of the various algorithms for collaborative fil tering. In this section we will describe the evaluation criteria, the various protocols, and the datasets used in the analysis. We then present and discuss the re sults regarding predictive accuracy, as well as several computational considerations.\n3.1 Evaluation Criteria\nThe effectiveness of a collaborative filtering algorithm depends on manner in which recommendations will be presented to the user. To evaluate these algorithms, we have defined metrics based on the type of collaborative filtering application and interface one is providing.\nThere are two basic classes of collaborative filtering ap plications. In the first class, individual items are pre sented one-at-a-time to the users along with a rating indicating potential interest in the topic. The original GroupLens system was in this category- each article in a GNUs-like Netnews interface has an ASCII bar chart indicating the system's prediction regarding the user's possible interest in that article. Thus, each piece of content has an associated estimated rating, and the user interface displays this estimate along with a link to the content or as a part of the display or presenta tion of the item.\nA second class of collaborative filtering applications present the user with an ordered list of recommended items. Examples of systems that present recommen dation lists include PHOAKS (L.Terveen et al., 1997] and SiteSeer (Rucker and Polanco, 1997]. In the spirit of the Internet search engines, these systems provide a ranked list of items (Web sites, music recordings) where highest ranked items are predicted to be most preferred. In these types of systems, the user presum ably will investigate items in the ordered list starting at the top hoping to find interesting items.\nWe have applied two scoring metrics in our evaluations-one appropriate for individual item-by item recommendations and the other appropriate for ranked lists. In both cases, the basic evaluation se quence proceeds as follows. A dataset of users (and their votes) is divided into a training set and a test set. The data for the training set is used as the col-\nlaborative filtering database or to build a probabilistic model. We then cycle through the users in the test set, treating each user as the active user. We divide the votes for each test user into a set of votes that we treat as observed, Ia, and a set that we will attempt to predict, Pa. We use the votes in Ia to predict the votes in Pa as shown in Equations 1 and 4. For individual scoring, we look at the average absolute deviation of the predicted vote to the actual vote on items the users in the test set have actually voted on. That is, if the number of predicted votes in the test set for the active case is ma, then the average absolute deviation for a user is:\n1 Sa=- L IPa,j- Va,jl ma jEPa\nThese scores are then averaged over all the users in the test set of users. This metric was also used in evaluating the GroupLens project (Miller et al., 1997].\nFor ranked scoring, the story is a bit more complex. In information retrieval research, ranked lists of re turned items are evaluated in terms of recall and pre cision. For a given number of returned items, recall is the percentage of relevant items that were returned and precision is the percentage of returned items that are relevant. In a collaborative filtering framework, if votes were binary (like and dislike) and we had com plete preference judgments for a set of users we could develop a similar metric. However, more generally, we wish to estimate the expected utility of a particular ranked list to a user. The expected utility of a list is simply the probability of viewing a recommended item times its utility. In this analysis, we will equate the utility of an item with the difference between the vote and the default or neutral vote in the domain.\nFurthermore, we make an estimate of how likely it is that the user will visit an item on a ranked list. We posit that each successive item in a list is less likely to be viewed by the user with an exponential decay. Then the expected utility of a ranked list of items (sorted by index j in order of declining Va,j) is:\nR = '\"\"\" max( Va,j - d, 0) a L.; 2(j-1)/(<>-1)\nj (5)\nwhere d is the neutral vote and a: is the viewing halflife. The halflife is the number of the item on the list such that there is a 50-50 chance the user will review that item. For these experiments, we used a halflife of 5 items. 2\n2We ran a set of experiments using a halflife of 10 items and found little sensitivity of results.\n48 Breese, Heckennan, and Kadie\nIn scoring a ranked list generated for a user, we ap ply Equation 5 using observed votes where available. For items that are not available, we apply the neutral vote, d, which effectively removes those items from the scoring. The final score for an experiment over a set of active users in the test set is\nR = 100 L:a Ra\n\ufffd Rmax 6a a\nwhere R;::ax is the maximum achievable utility if all observed items had been at the top of the ranked list, ordered by vote value. This transformation allows us to consider results independent of the size of the test set and number of items predicted in a given experi ment.\n3.2 Datasets\nWe evaluated the algorithm for three separate datasets, as follows:\n\u2022 MS Web: This dataset captures individual visits to various areas ( vroots) of the Microsoft corpo rate web site. This is an example of an implicit voting database and application. Each vroot was characterized as being visited (vote of one) or not (no vote).\n\u2022 Television: This dataset uses Neilsen network television viewing data for individuals for a two week period in the summer of 1996. The data was transformed into binary data indicating whether each show was watched, or not, as above.3\n\u2022 EachMovie: This is an explicit voting example us ing data from the EachMovie collaborative filter ing site deployed by Digital Equipment Research Center from 1995 through 1997. 4 Votes ranged in value from 0 to 5.\nTable 3.2 provides additional information about each dataset.\n3.3 Protocols\nWe did two classes of experiments reflecting differing numbers of votes available to the recommenders. In the first protocol, we withhold a single randomly se lected vote for each user in the test set, and try to predict its value given all the other votes the user has voted on. We term this protocol All but 1. In the sec ond set of experiments, we randomly select 2, 5, or 10\n3This dataset was made available for this study courtesy of Nielsen Media Research.\n4For more information see http:/ jwww. research. digital. com/SRC /EachMoviej.\nvotes from each test user as the observed votes, and then attempt to predict the remaining votes. We call these protocols Given 2, Given 5, and Given 10.\nThe All but 1 experiments measure the algorithms' performance when given as much data as possible from each test user. The various Given experiments look at users with less data available, and examine the perfor mance of the algorithms when there is relatively little known about an active user. In running the tests, if a prospective test did not have adequate votes for a trial it was eliminated from the evaluation. Thus the number of trials evaluated under each protocol vary.\n4 Results\nIn the following sections, we compare algorithms and analyze the effects of individual algorithmic exten sions. We use randomized block design where each algorithm is run on the same test cases and observed votes. We will refer to one of these comparisons as an experiment. Our analyses uses ANOVA with the Bon ferroni procedure for multiple comparisons statistics [McClave and Dietrich, 1988). In the tables that fol low, the value in the last row is labeled RD for Required Difference. The difference between any two scores in a column must be at least as big as the value in the RD row in order to be considered statistically signif icant at the 90% confidence level for the experiment as a whole. As a visual aid, a score in boldface is significantly different from the score directly below it in the table.\n4.1 Overall Performance\nThe following tables show the performance of the vari ous major classes of algorithms on the various datasets and experiments. We compared the best performing variation of each algorithm on each dataset, for the different protocols. We also present the scores that result from presenting the user with the most popular items, regardless of the known votes of the individ-\nual. This results in a baseline performance of a \"zero order\" collaborative filtering system, and is labeled as POP in the tables. The algorithm labeled CR+ refers to use of the correlation technique with inverse user frequency, default voting, and case amplification ex tensions. VSIM refers to using the vector similarity method with the inverse user frequency transforma tion. BN and BC refer to the Bayesian network and clustering models respectively.\nOur results show that Bayesian networks with deci sion trees at each node and correlation methods are the best performing algorithms over the experiments we have run. We ran 16 combinations of dataset, pro tocol, and scoring criteria. The Bayesian network and correlation-based were each either best, or statistically equivalent, in 10 cases. Bayesian clustering was best performing in 2 cases and vector similarity was best in 3 cases.\nWe see that the Bayesian network performs best un der the All but 1 protocol. Generally, all the methods perform less well in the Given 2 and Given 5 protocols as might be expected. However the vector similarity and clustering methods are competitive for some of these limited-data scenarios, since these methods can use partial information effectively.\nGiven 2 protocol. Correlation, with extensions, and vector similarity are fairly close in performance, while Bayesian clustering performs relatively poorly. We see that the Bayesian network drops off in performance quite significantly for the Given 2 protocol, relative to correlation and vector similarity. We will discuss this observation below.\nWe see a somewhat different pattern for EachMovie under ranked scoring, shown in Table 4. Here the cor relation algorithm is the top performer overall, with vector similarity performing well with less data. For this dataset and score, the Bayesian network performs worse than any of the other algorithms on all the Given experiments, but is the top performer and is competi tive with correlation for the All but 1 protocol.\nThe Bayesian networks using decision trees suffer in the Given scenarios because they are provided with relatively little data. If a title that is held out for testing appears near the top of a tree, then it's value is set to \"no vote\" in evaluating the probability of a possibly related title. This may result in a title that is provided being ignored or having little impact, simply due to the ordering of the various predicting titles in the tree. The various All But 1 experiments are able to utilize trees to a fuller extent, and therefore perform well relative to the other methods that can use partial data.\nFor absolute deviation, we examined the EachMovie dataset and results are shown in Table 5. This dataset has a vote range of 0 to 5, making vote prediction a relevant task. We examine the same algorithms as in the previous table, except now we use a correla tion algorithm without applying any of the extensions except for inverse user frequency. The other exten sions are not effective for absolute deviation scoring. This basic correlation algorithm performs best in all but the Given 2 experiments, indicating that this al gorithm performs well when given adequate data re garding the active case. The Bayesian clustered model does slightly better than the Bayesian network, and outperforms correlation in the Given 2 and Given 5 cases.\n4.2 Inverse User Frequency\nIn Section 2.2.2 we describe using inverse user fre quency to modify vote values in applying memory based algorithms. We performed a set of 12 experi ments (3 datasets, 4 protocols) each for vector sim ilarity and correlation judging the effect of applying inverse user frequency under ranked scoring. In all experiments, application of IUF improved the ranked score, and in 23 of 24 cases results were statistically significant. The average improvement was 1.9%, with an improvement of 2.2% for the vector similarity algo rithm, and 1.5% for the correlation algorithm.\nIn 8 experiments run on the EachMovie dataset using absolute deviation scoring, the improvement averaged a more impressive 11%. Results were significant in 6 of the 8 experiments. The average improvement was of 15.5% for vector similarity, and 6.5% for correlation.\n4.3 Case Amplification\nCase amplification (Section 2.2.3) modifies weights used in an memory-based algorithm to emphasize higher weights. We performed a set of 12 experiments (3 datasets, 4 protocols) applying case amplification to correlation. The average improvement in the ranked score was 4.8%, and results were significant in 11 of 12\nexperiments. There is no significant effect of case am plification on absolute deviation scoring. We also ran experiments combining case amplification and inverse user frequency, and found the benefits to be additive.\n4.4 Probabilistic Methods\nWe used a training set to build probabilistic models for each dataset. Each title was encoded with an addi tional explicit vote value of \"no vote\" to complete the dataset for probabilistic learning. When scoring with Bayesian networks and cluster models, the \"no vote\" values were explicitly entered into the network when missing, for both ranked and absolute deviation scor ing. For the trees, the \"no vote\" values were entered in each tree independently in order to generate a prob ability for that title. For absolute deviation scoring, the expected vote was calculated by renormalizing the output probabilities, clamping the \"no vote\" probabil ity to zero.\nThere are roughly 1600 movies in the EachMovie dataset, too many to estimate a full model in a reason able amount of time. Therefore the Bayesian methods were trained from EachMovie for the top 300 movies in terms of overall popularity. For testing, all 1600 movies were used. In the other datasets, all items were used for training and testing.\nFor the Bayesian networks, we applied alternate prior specifications which resulted in trees of varying com plexity. Priors that strongly penalized splits generated Bayesian networks with nodes with approximately 2 to 4 parents and 4 to 6 distributions in the decision tree representation. The model with the larger trees had somewhere between 4 and 6 predecessors and 6 to 8 distributions per variable. In all our experiments the larger trees outperformed the smaller tree so we re strict our results to those models. Additional details are available in Breese et al. (1998).\nApplying clustering to the datasets identified 3 classes for the Neilsen dataset, 7 classes for the MS Web dataset, and 9 classes for the EachMovie dataset. The classes found by clustering for the MS Web dataset are shown below. Each entry is a page area or virtual root that distinguishes this class from the others. The class names on the left were manually generated based on inspecting the resulting classes.\nSupport Support Desktop, Knowledge Base, Win dows95 Support, Search, NT Server Support\nWindows Products, Free Downloads, W indows95, Windows95 Support, Windows Family of Prod ucts\nOffice Products, MS Office Info, Free Downloads, MS\nWord News, Office Free Stuff, MS Office\nDevelopers Search, Training, Games, Developer Network, Job Openings\nInternet Explorer Internet Explorer, Free Down loads, IE support, Net Meeting, International IE Content\nInternet Explorer Technical Search, Free Down loads, Products, Internet Explorer, Internet Site Construction for Dev.\nIE Site Builder Internet Site Construction for Dev., Web Site Builders Gallery, Developer Workshop, Sitebuilder Network Membership, Jakarta, Ac tiveX Technology Dev.\nAmong probabilistic methods, the Bayesian network with a decision tree at each item outperformed the cluster models for ranked scoring. In 12 comparisons, there was an average 41% improvemep.t in ranked scores, all differences being statistically significant. For absolute deviation experiments run with the Each Movie data, we found that the cluster model performed slightly better than the trees.\n5 Additional Issues\nAlthough predictive accuracy is probably the most im portant aspect in gauging the efficacy of a collabora tive filtering algorithm, there are other considerations, including size of model, sampling, and runtime perfor mance.\nIf one considers the size of the overall collaborative fil tering prediction representation, memory-based meth ods require a relatively small algorithm code base, plus a user database consisting of a sample of user votes. The model-based methods require the representation of the Bayesian network model, typically having much smaller memory requirements. For example, the user databases required for the memory-based methods for the EachMovie and MS Web datasets were approxi mately 314 and 318 Kilobytes compressed, while the Bayesian network model sizes were 27 and 55 Kilobytes compressed respectively.\nThe number of items in the usage database used for the memory-based methods was determined by exper imenting with the scoring for various sizes of training set. Figure 2 shows the increase in ranked scoring ac curacy as a function of size of training set. We used training set sizes (number of users) of 1637 for Neilsen, 5000 for EachMovie, and 32711 for MS Web. Identi cal training sets were used as the user database for model-based methods, and as the database for learn ing probabilistic models. Our experiments have found\nthat sample sizes on this order are adequate for pur poses of generating recommendations.\nIn terms of runtime performance, the probabilistic, model-based methods were approximately 4 times as fast as the memory-based methods in generating rec ommendations, with correlation generating 3.2 recom mendations per second and the Bayes net generating 12.9 recommendations per second on 266 MHz Pen tium II processor (Eachmovie dataset). Of course, the probabilistic models must be learned. Learning times for the models used in these experiments ranged from less than an hour for Neilsen and up to 8 hours for EachMovie and MS Web.\n6 Conclusions\nThis paper presents an extensive set of experiments re garding the predictive performance of statistical algo rithms for collaborative filtering or recommender sys tems. Results indicate that for a wide range of con ditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian clustering and vector similarity methods. Between cor relation and Bayesian networks, the preferred method depends on the nature of the dataset, nature of the ap plication (ranked or one-by-one presentation), and the availability of votes with which to make predictions. We see that when there are relatively few votes, corre lation and Bayesian networks have less of an advantage over the other techniques.\nOther considerations include the size of database, speed of predictions, and learning time. Bayesian net works are typically have smaller memory requirements and allow for faster predictions than a memory-based\ntechnique such as correlation. However, the Bayesian methods examined here require a learning phase that can take up to several hours and results in a lag before changed behavior is reflected in recommendations.\nWe plan to make the MS Web data used in this study available to learning community through the Irvine repository. As noted, the EachMovie data is currently available. We hope that the availability of this data coupled with discussion spurred by this paper will re sult in additional examination and improvement of col laborative filtering algorithms.\nAcknowledgements\nDatasets for this paper were generously provided by Digital Equipment Corporation (EachMovie), Neilsen Media Research (Neilsen), and Microsoft Corporation (MS Web). Max Chickering, David Hovel, and Robert Rounthwaite contributed to the programming of the algorithms that were used in this study. We also thank Max Chickering, Eric Horvitz, and Chris Meek for use ful discussions. John Riedl also provided useful com ments.\nReferences\n[Breese et al., 1998] Breese, J., Heckerman, D., and Kadie, C. (May, 1998). An experimental compar ison of collaborative filtering methods. Technical Report MSR-TR-98-12, Microsoft Research, Red mond, WA.\n[Cheeseman and Stutz, 1995] Cheeseman, P. and Stutz, J. (1995). Bayesian classification (Auto Class): Theory and results. In Fayyad, U., Piatesky Shapiro, G., Smyth, P., and Uthurusamy, R., ed itors, Advances in Know ledge Discovery and Data Mining, pages 153-180. AAAI Press, Menlo Park, CA.\n[Chickering and Heckerman, 1997] Chickering, D. and Heckerman, D. (1997). Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables. Machine Learning, 29:181-212.\n[Chickering et al., 1997] Chickering, D., Heckerman, D., and Meek, C. (1997). A Bayesian approach to learning Bayesian networks with local structure. In Proceedings of Thirteenth Conference on Un certainty in Artificial Intelligence, Providence, RI. Morgan Kaufmann.\n[Dempster et al., 1977] Dempster, A., Laird, N., and Rubin, D. (1977). Maximum likelihood from incom plete data via the EM algorithm. Journal of the Royal Statistical Society, B 39:1-38.\n[L.Terveen et al., 1997] L.Terveen, Hill, W., Amenta, B., McDconald, D., and Creter, J. (1997). PHOAKS: A system for sharing recommendations. Communications of the ACM, 40(3):59-62.\n[McClave and Dietrich, 1988] McClave, J. T. and Di etrich, F. H. (1988). Statistics. Dellen Publishing Company, San Francisco, fourth edition.\n[Miller et al., 1997] Miller, B., Riedl, J., and Konstan, J. (1997). Experiences with GroupLens: Making Usenet useful again. In Proceeding of the USENIX 1997 Annual Technical Conference, pages 219-231, Anaheim, CA.\n[Resnick et al., 1994] Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., and Riedl, J. (1994). Grouplens: An open ar chitecture for collaborative filtering of netnews. In Proceedings of the ACM 1994 Conference on Com puter Supported Cooperative Work, pages 175-186, New York. ACM.\n[Resnick and Varian, 1997] Resnick, P. and Varian, H. (1997). Recommender systems. Communications of the ACM, 40(3):56-58.\n[Rucker and Polanco, 1997] Rucker, J. and Polanco, M. J. (1997). Siteseer: Personalized navigation of the web. Communications of the ACM, 40(3):56-58.\n[Salton and McGill, 1983] Salton, G. and McGill, M. (1983). Introduction to Modern Information Re trieval. McGraw-Hill, New York.\n[Thiesson et al., 1997] Thiesson, B., Meek, C., Chick ering, D., and Heckerman, D. (December, 1997). Learning mixtures of DAG models. Technical Report MSR-TR-97-30, Microsoft Research, Red mond, WA."}], "references": [{"title": "An experimental compar\u00ad ison of collaborative filtering methods", "author": ["Breese et al", "J. 1998] Breese", "D. Heckerman", "C. Kadie"], "venue": "(May,", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "Bayesian classification (Auto\u00ad Class): Theory and results", "author": ["J. Stutz"], "venue": null, "citeRegEx": "P. and Stutz,? \\Q1995\\E", "shortCiteRegEx": "P. and Stutz", "year": 1995}, {"title": "Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables", "author": ["Chickering", "Heckerman", "D. 1997] Chickering", "D. Heckerman"], "venue": "Machine Learning,", "citeRegEx": "Chickering et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chickering et al\\.", "year": 1997}, {"title": "A Bayesian approach to learning Bayesian networks with local structure", "author": ["Chickering et al", "D. 1997] Chickering", "D. Heckerman", "C. Meek"], "venue": "In Proceedings of Thirteenth Conference on Un\u00ad certainty in Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q1997\\E", "shortCiteRegEx": "al. et al\\.", "year": 1997}, {"title": "Maximum likelihood from incom\u00ad plete data via the EM algorithm", "author": ["Dempster et al", "A. 1977] Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical", "citeRegEx": "al. et al\\.,? \\Q1977\\E", "shortCiteRegEx": "al. et al\\.", "year": 1977}, {"title": "PHOAKS: A system for sharing recommendations", "author": ["L.Terveen et al", "1997] L.Terveen", "W. Hill", "B. Amenta", "D. McDconald", "J. Creter"], "venue": "Communications of the ACM,", "citeRegEx": "al. et al\\.,? \\Q1997\\E", "shortCiteRegEx": "al. et al\\.", "year": 1997}, {"title": "Experiences with GroupLens: Making Usenet useful again", "author": ["Miller et al", "B. 1997] Miller", "J. Riedl", "J. Konstan"], "venue": "In Proceeding of the USENIX", "citeRegEx": "al. et al\\.,? \\Q1997\\E", "shortCiteRegEx": "al. et al\\.", "year": 1997}, {"title": "Grouplens: An open ar\u00ad chitecture for collaborative filtering of netnews", "author": ["Resnick et al", "P. 1994] Resnick", "N. Iacovou", "M. Suchak", "P. Bergstrom", "J. Riedl"], "venue": "In Proceedings of the ACM", "citeRegEx": "al. et al\\.,? \\Q1994\\E", "shortCiteRegEx": "al. et al\\.", "year": 1994}, {"title": "Siteseer: Personalized navigation of the web", "author": ["Rucker", "Polanco", "J. 1997] Rucker", "M.J. Polanco"], "venue": "Communications of the ACM,", "citeRegEx": "Rucker et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Rucker et al\\.", "year": 1997}, {"title": "Introduction to Modern Information Re\u00ad trieval", "author": ["Salton", "McGill", "G. 1983] Salton", "M. McGill"], "venue": null, "citeRegEx": "Salton et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1983}], "referenceMentions": [{"referenceID": 2, "context": "Details of the learning algorithm are discussed in Chickering et al.(1997). In the remainder of the paper the term Bayesian network will refer to these networks with a decision tree for each title.", "startOffset": 51, "endOffset": 75}], "year": 2011, "abstractText": "Collaborative filtering or recommender sys\u00ad tems use a database about user preferences to predict additional topics or products a new user might like. In this paper we describe several algorithms designed for this task, in\u00ad cluding techniques based on correlation coef\u00ad ficients, vector-based similarity calculations, and statistical Bayesian methods. We com\u00ad pare the predictive accuracy of the various methods in a set of representative problem domains. We use two basic classes of evalua\u00ad tion metrics. The first characterizes accuracy over a set of individual predictions in terms of average absolute deviation. The second esti\u00ad mates the utility of a ranked list of suggested items. This metric uses an estimate of the probability that a user will see a recommen\u00ad dation in an ordered list. Experiments were run for datasets associ\u00ad ated with 3 application areas, 4 experimen\u00ad tal protocols, and the 2 evaluation met\u00ad rics for the various algorithms. Results indicate that for a wide range of con\u00ad ditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian-clustering and vector\u00ad similarity methods. Between correlation and Bayesian networks, the preferred method de\u00ad pends on the nature of the dataset, nature of the application (ranked versus one-by-one presentation), and the availability of votes with which to make predictions. Other con\u00ad siderations include the size of database, speed of predictions, and learning time.", "creator": "pdftk 1.41 - www.pdftk.com"}}}