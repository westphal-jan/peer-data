{"id": "1301.7380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Solving POMDPs by Searching in Policy Space", "abstract": "competing algorithms for doing pomdps iteratively improve a value atlas yet implicitly implements local choice and are in : search in value parameter space. this formulation presents simple interface to solving pomdps itself represents a policy item as a finite - state controller because iteratively improves the controller by exploration in policy space. two difference algorithms illustrate dual approach. the two is a policy iteration methodology that can outperform value iteration if solving infinitehorizon pomdps. it provides theoretical capacity for horizontally balanced heuristic search algorithm itself makes extensive specialization based focusing of effort on regions into persistent storage space that are irrelevant, or meaningful to is relevant, from a specified target.", "histories": [["v1", "Wed, 30 Jan 2013 15:04:11 GMT  (266kb)", "http://arxiv.org/abs/1301.7380v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eric a hansen"], "accepted": false, "id": "1301.7380"}, "pdf": {"name": "1301.7380.pdf", "metadata": {"source": "CRF", "title": "Solving POMDPs by Searching in Policy Space", "authors": ["Eric A. Hansen"], "emails": ["hansen@cs.umass.edu"], "sections": [{"heading": null, "text": "Most algorithms for solving POMDPs itera tively improve a value function that implic itly represents a policy and are said to search in value function space. This paper presents an approach to solving POMDPs that repre sents a policy explicitly as a finite-state con troller and iteratively improves the controller by search in policy space. Two related al gorithms illustrate this approach. The first is a policy iteration algorithm that can out perform value iteration in solving infinite horizon POMDPs. It provides the founda tion for a new heuristic search algorithm that promises further speedup by focusing compu tational effort on regions of the problem space that are reachable, or likely to be reached, from a start state.\n1 Introduction\nA partially observable Markov decision process (POMDP) provides an elegant mathematical model for planning and control problems for which there can be uncertainty about the effects of actions and about the current state. It is well-known that a state prob ability distribution updated by Bayesian reasoning is a sufficient statistic that summarizes all information about the history of the process necessary for optimal action selection. Therefore the standard approach to solving a POMDP is to recast it as a completely ob servable MDP with a state space that consists of all possible state probability distributions. In this form, it is solved using dynamic programming or related tech niques that rely on the Markov assumption.\nAlgorithms for solving POMDPs in this way rely on a value function that maps state probability distribu tions to expected values. A value function defined for\nall possible state probability distributions can be rep resented in different ways; for example, as a set of vec tors and a max operator (Smallwood & Sondik 1973) or as a grid of point values with an interpolation rule (e.g., Hauskrecht 1997). Given some explicit represen tation of the value function, a policy is represented im plicitly by the same value function and one-step looka head. Most algorithms for solving POMDPs represent a policy implicitly in this way and improve the policy by gradually improving the value function, typically by repeated \"backups\" using value iteration or reinforce ment learning. Because the policy is only represented implicitly by the value function, such algorithms are said to search in value function space.\nThis paper presents an approach to solving POMDPs that represents a policy explicitly and relies on search in policy space. In this approach, choice of how to represent a policy is critical in a way that it is not for algorithms that search in value function space. It is possible to represent a policy explicitly as a map ping from state probability distributions to actions by partitioning probability space into a finite set of re gions and mapping each region to some action. Sondik (1978) describes a policy iteration algorithm that rep resents a policy in this way. However this algorithm is very complex and difficult to implement and, as a result, is not used in practice.\nIn this paper, we consider an alternative represen tation of a policy as a finite-state controller and present two related algorithms for solving infinite horizon POMDPs by searching in a policy space of finite-state controllers. The first is a policy iteration algorithm, first described by Hansen (1998a), that sim plifies policy iteration for POMDPs by representing a policy as a finite-state controller. It provides the foun dation for a related heuristic search algorithm, pre sented here for the first time, that can focus compu tational effort on regions of the search space that are reachable, or likely to be reached, from a given start state.\n212 Hansen\n2 Background\nConsider a discrete-time POMDP with a finite set of states S, a finite set of actions A, and a finite set of observations Z. Each time period, the sys tem is in some state s E S, an agent chooses an action a E A for which it receives an immediate re ward with expected value r(s, a) E \ufffd. the system makes a transition to state s1 E S with probability Pr(s1ls,a) E [0, 1], and the agent observes z E Z with probability Pr(zls1, a) E [0, 1). The state of the system cannot be directly observed, but the probability that it is in a given state can be calculated. Let b denote a vector of state probabilities, called a belief state, where b( s) denotes the probability that the system is in state s. If action a is taken and observation z follows, the successor belief state, denoted b\ufffd, is determined by re vising each state probability as follows,\na ( I) - Pr(zls1,a)'LsESPr(s1ls,a)b(s) bz 8 Pr(zlb, a) '\nwhere the denominator is a normalizing factor Pr(zlb, a) = Ls'ES Pr(zls', a) LsES Pr(s'ls, a)b(s).\nA POMDP is solved by finding a rule for selecting actions, called a policy, that optimizes a performance objective (or comes acceptably close to doing so). We assume the objective is to maximize the expected total discounted reward over an infinite horizon (where f3 E (0, 1] is a discount factor). By recasting a POMDP as a completely observable MDP with a continuous, lSI dimensional state space that consists of all possible belief states, the problem can be solved by iteration of a dynamic-programming update that performs the following \"one-step backup\" for each belief state b:\nV'(b) :=max [2:: b(s)r(s, a)+ /3 L Pr(zlb, a)V(b\ufffd)J . aEA sES zEZ (1) In words, this says that the value of belief state b is set equal to the immediate reward for taking the best ac tion for b plus the discounted expected value of the resulting belief state b\ufffd. Iteration of the dynamic programming update, called value iteration, converges to the optimal value function in the limit. However the number of belief states that must be \"backed-up\" each iteration is uncountably infinite and it is not obvious how to do this.\nThe key to computing the dynamic-programming up date is Smallwood and Sondik's (1973) proof that it preserves the piecewise linearity and convexity of the value function. A piecewise linear and con vex value function V can be represented by a fi nite set of lSI-dimensional vectors of real numbers, V = { v0, v1, ... , vk}, such that the value of each belief\nstate is defined as follows:\nV(b) = max '\"' b(s)vi(s). O<i<k L..J -- sES The dynamic-programming update transforms a value function V represented in this way into an improved value function V' represented by another finite set of vectors, V'. Several algorithms for performing the dynamic-programming update have been developed. All rely heavily on linear programming and are com putationally intensive; the algorithm that is presently the fastest is described by Cassandra, Littman and Zhang (1997). We do not describe here how to com pute the dynamic-programming update and instead re fer to this paper, Kaelbling et al. (1996), Cassandra et al. {1994), and references therein.\nAlgorithms that search in value function space, such as value iteration, must be able to extract a policy from the value function they iteratively improve. There are two possible ways to do so that correspond to two pos sible representations of a policy.\nOne possibility is to view a policy as a mapping from belief states to actions. Given some representation of a value function mapping belief states to values, a policy cS is extracted using one-step lookahead,\ncS(b) = argTE<lf [p(b, a)+ /3 L Pr(zlb, a)V(b\ufffd)l , zEZ (2) where p(b,a) = LsEsb(s)r(s,a) is the expected im mediate reward for taking action a in belief state b.\nA second possibility is to represent a policy as a finite state controller. A correspondence between vectors and one-step policy choices plays an important role in this interpretation of a policy. Each vector in V' cor responds to the choice of an action, and for each pos sible observation, choice of a vector in V. Among all possible one-step policy choices, the vectors in V' cor respond to those that optimize the value of some belief state. To describe this correspondence between vectors and one-step policy choices, we introduce the follow ing notation. For each vector vi in V1, let a( i) denote the choice of action and, for each possible observation z, let l ( i, z) denote the index of the successor vector in V. Given this correspondence between vectors and one-step policy choices, Kaelbling et al. (1996) point out that an optimal policy for a finite-horizon POMDP can be represented by an acyclic finite-state controller in which each machine state corresponds to a vector in a nonstationary value function.\nValue iteration can also be used to solve infinite horizon POMDPs. The optimal value function for an infinite-horizon POMDP is not necessarily piece wise linear, although it is convex. However it can\nSolving POMDPs by Searching in Policy Space 213\nV{b)\nzl,z2 0 b{l)\nFigure 1: Example of a simple finite-state controller and corresponding value function for a POMDP with two states, two actions (al,a2) and two observations (zl,z2) . Each machine state is labeled by a unique number and by an action to take in that state; state transitions are la beled by observations. Each vector of the value function is labeled by the number of the machine state to which it corresponds. For a two-state POMDP, the belief state can be represented by a single real number between 0 and 1 that represents the probability of being in one of the states; the horizontal axis in the figure on the right represents the belief state in this way. The vertical axes represent the expected value of a belief state. The value function is the upper surface of the vectors and reflects the rule that the controller is started in the machine state that optimizes the value of the starting belief state.\nbe approximated arbitrarily closely by a piecewise lin ear and convex function. Moreover Sondik (1978) and Cassandra et al. (1994) point out that sometimes, al though not reliably, value iteration converges to an optimal piecewise linear and convex value function that is equivalent to a cyclic finite-state controller. The finite-state controller can be extracted from the value function using the correspondence between vec tors and one-step policy choices noted earlier. However a finite-state controller cannot reliably be extracted from a suboptimal value function and so a policy for an infinite-horizon POMDP is generally viewed as a mapping from belief states to actions represented im plicitly by a value function and extracted using equa tion (2).\nFor algorithms that search in value function space, it is important to be able to extract a policy from a value function. For algorithms that search in policy space, it is equally important to be able to compute the value function of a policy; this is called policy evaluation. We conclude this review by pointing out that for a policy represented as a finite-state controller, policy evaluation is straightforward. A piecewise linear and convex value function can be computed by solving the following system of linear equations, where there is one equation for each pair of machine state i and system states:\nvi(s) = r(s, a(i))+ (3) f3 Es\u2022 ,z Pr( s'ls, a(i) )Pr(z!s', a(i) )vl(i,z) (s').\nThe value function has one linear facet or lS I-vector for each machine state of the finite-state controller. Al-\nthough the policy is a finite-state controller, the value function is defined for belief space and the controller is started in the machine state that corresponds to the vector that optimizes the value of the starting belief state. (See Figure 1.)\n3 Policy Iteration\nThe first algorithm we consider that solves a POMDP by searching in policy space is policy iteration. Be cause it includes a policy evaluation step that com putes the value function of a given policy, it must rep resent the policy explicitly and independently of the value function. Sondik (1978) describes a policy itera tion algorithm for POMDPs that represents a policy as a mapping from a finite number of polyhedral regions of belief space to actions. Each region of belief space is represented by a set of linear inequalities that define its boundaries. Because there is no known method for computing the value function of a policy represented in this way, the policy evaluation step of Sondik's al gorithm converts a policy from this representation to an equivalent, or approximately equivalent, finite-state controller; as we have seen, the value function of a finite-state controller can be computed in a straight forward way. However conversion between these two representations is extremely complicated and difficult to implement. As a result, Sondik's algorithm is not used in practice.\nWe now show that policy iteration for POMDPs can be simplified by representing a policy as a finite-state con troller. The obvious simplification is that this makes policy evaluation, the most difficult step of Sondik's algorithm, straightforward. But for this approach to work, we must show that the dynamic-programming update can be interpreted as the transformation of a finite-state controller 8 into an improved finite-state controller 8'; that is, we must show how to perform policy improvement on finite-state controllers. We do this by showing that a simple comparison of the vec tors in V0 and V' provides the basis for such a transfor mation, where V0 is the set of vectors that represents the value function of the current finite-state controller 8 and V' is the output of the dynamic-programming update given V0 as input.\nFirst recall that every vector vi in V0 is associated with an action, denoted a(i), and for each possible observa tion z, a transition to another vector in V0, with index l(i, z). This follows from the fact that V0 is computed by evaluating a finite-state controller. Similarly every vector vi in V' found by the dynamic-programming update is associated with an action, a(j), and for each possible observation z, a transition to a vector in V0, where l(j, z) denotes the index of the vector.\n214 Hansen\nVectors in V' can be duplicates of vectors in V8, that is, they can have the same action and successor links (in which case their vector values will be pointwise equal). If they are not duplicates, they indicate how the finite state controller can be changed to improve the value function - either by changing a machine state (that is, changing its corresponding action and/or successor links) or by adding a machine state. There may also be some machine states for which there is no correspond ing vector in V' and they can be pruned, but only if they are not reachable from a machine state that corre sponds to a vector in V'. (This last point is important\nbecause it preserves the integrity of the finite-state controller.) Thus a finite-state controller can be it eratively improved using a combination of three trans formations; changing machine states, adding machine states, and pruning machine states.\nFigure 2 outlines a policy iteration algorithm with a policy improvement step that uses these simple trans formations to improve a finite-state controller. Fig ure 3 illustrates the policy improvement step with a simple example. Thansformation of the finite-state controller after performing the dynamic-programming update adds little overhead to the policy improvement step because it simply compares the vectors in V' to the vectors in V8 and modifies the finite-state con troller accordingly. If a machine state is changed, the policy evaluation step is invoked to compute the value function of the transformed finite-state controller. We can prove the following generalization of Howards's policy improvement theorem (Hansen 1998b).\nTheorem 1 If a finite-state controller is not optimal, policy improvement transforms it into a finite-state controller with a value function that is as good or better for every belief state and better for some belief state.\nIf the policy improvement step does not change the finite-state controller, that is, if all the vectors in V' are duplicates of vectors in V0, then the Bellman opti mality equation is satisfied and the finite-state con troller must be optimal. Therefore policy iteration can detect convergence to an optimal finite-state con troller. However not every POMDP has an optimal finite-state controller and policy iteration may simply\nSolving POMDPs by Searching in Policy Space 215\nfind a succession of finite-state controllers that are in creasingly close approximations of an optimal policy. We use the same stopping condition Sondik uses to de tect \u20ac-optimality: a finite-state controller is \u20ac-optimal when the Bellman residual is less than or equal to t:(l- (3)/(3, where (3 is the discount factor, and we can prove the following convergence result (Hansen 1998b).\nTheorem 2 Policy iteration converges to an \u20ac optimal finite-state controller after a finite number of iterations.\nAs with completely observable MDPs, policy itera tion can converge to \u20ac-optimality (or optimality) in fewer iterations than value iteration because inter leaving a policy evaluation step with the dynamic programming update accelerates improvement of the value function. For completely observable MDPs, this is not a clear advantage because the policy evalua tion step is more computationally expensive than the dynamic-programming update. For POMDPs, policy evaluation has low-order polynomial complexity com pared to the worst-case exponential complexity of the dynamic-programming update (Littman et al. 1995). Therefore, policy iteration appears to have a clearer advantage over value iteration for POMDPs.\nTable 1 compares the performance of value iteration and policy iteration on nine test problems from Cas sandra et al. (1997). (For these problems, the average number of states is 9.3, the average number of actions is 3.9, and the average number of observations is 4.7.) Their incremental pruning algorithm was used to per form dynamic-programming updates in both value it eration and policy iteration and experiments were per formed on a AlphaStation 200/4 with a 233Mhz pro cessor and 128M of RAM. The results show that policy iteration consistently outperforms value iteration and the increased rate of convergence is often dramatic.\nA few of these test problems have small optimal finite state controllers (lD maze, 4x3CO, 4x4, Cheese and part painting). For them, policy iteration converges quickly and sometimes reduces the error bound from more than 1.0 to zero in a single iteration. For the other problems, finite-state controllers with between a couple hundred and several hundred machine states are generated without converging to optimality. This illustrates that the difficulty of solving a POMDP is primarily a function of the size of the controller needed to achieve good performance and not simply a function of the number of states, actions and observations.\n4 Heuristic search\nAlthough policy iteration converges more quickly that value iteration, both are limited to solving very small POMDPs. The shared bottleneck is the dynamic programming update. The fastest algorithm for per forming it is still prohibitively slow for problems with more than about ten or fifteen states, actions, or ob servations. Policy iteration is faster that value itera tion because it takes fewer iterations of the dynamic programming update to converge. But when a single iteration is computationally prohibitive, policy itera tion is as impractical as value iteration. In this section, we introduce a new approach to solving POMDPs that is closely related to the policy iteration algorithm de scribed in the previous section but differs in an impor tant respect; it does not use the dynamic-programming update to improve a policy. Instead it uses heuristic search.\nHeuristic search has been used before to solve POMDPs approximately. Satia and Lave (1973) describe a branch-and-bound algorithm for solving infinite-horizon POMDPs, given an initial belief state, and Larsen (1989) and Washington (1996,1997) use the best-first heuristic search algorithm AO* in a similar way. For infinite-horizon problems, it is only possible to search to a finite depth and these algorithms find a solution that takes the form of a tree that grows with the depth of the search. The search tree can be repre-\n216 Hansen\nsented by an AND/OR tree in which the nodes of the tree correspond to belief states and the root of the tree is the initial belief state. An OR node represents the choice of an action and an AND node represent a set of possible observations. The value of an OR node is the value of the best action for the belief state that corre sponds to it. The value of an AND node is the sum of the values of the belief states that follow each observa tion, multiplied by the probability of each observation. Upper and lower bounds are computed for belief states on the fringe of the search tree and backed-up through the tree to the starting belief state at its root. Thus expanding the search tree improves the bounds at the interior nodes of the tree. The error bound (the dif ference between the upper and lower bounds on the value of the starting belief state) can be made arbi trarily small by expanding the search tree far enough and, for discounted POMDPs, an E-optimal policy for the belief state at the root of the tree can be found after a finite search (Satia and Lave 1973).\nSeveral possible upper bound functions for evaluating the fringe nodes of the search tree have been discussed by others (e.g., Hauskrecht 1997, Brafman 1997) and we do not add to that discussion here. For a lower bound function, we use the piecewise linear and convex value function of a finite-state controller and improve the lower bound during search by iteratively improv ing the finite-state controller, much as policy iteration does. This is the principal innovation of our heuristic search algorithm.\nRecall that every node of the search tree corresponds to a belief state. Therefore expanding an OR node (and all its child AND nodes), and backing up its lower bound, is equivalent to performing a one-step backup for the corresponding belief state (as in equation 1). This backup may improve the lower bound of the belief state and, if it does, we know that a machine state can be added to the finite-state controller that improves the value of at least this one belief state. Therefore expanding a search node performs a similar function as the dynamic-programming update, and can be in terpreted in a similar way as the (potential) modifica tion of a finite-state controller; the difference is that a node expansion corresponds to a one-step backup for a single belief state whereas the dynamic-programming update performs a one-step backup for all possible be lief states.\nWhen the lower bound for a belief state in the search tree is improved, it is backed-up through the search tree and possibly improves the lower bound of the starting belief state at the root. When it does so, the search algorithm has found a way to improve the value of the starting belief state by modifying the finite-state controller. The finite-state controller is modified as\n1. Specify an initial finite-state controller, o, and select E for detecting convergence to an Eoptimal policy.\n2. Policy evaluation: Compute the value funtion for o by solving the system of equations given by equation (4).\n3. Policy improvement;\n(a) Perform forward search from the starting belief state and back up lower and upper bounds from the leaves of the search tree. Continue until either the lower bound of the starting belief state is improved or the error bound on the value of the starting belief state is less than or equal to E. (b) If the error bound is less than or equal to E, exit with an \u20ac-optimal policy. Otherwise continue.\n(c) Given that forward search has found a change of policy that improves the lower bound of the starting belief state, consider every reachable node in the search tree for which the lower bound has been improved. (A node is said to be reachable if it can be reached by starting from the root node and always selecting actions that optimize the lower bound). For each of these nodes in order from the leaves to the root: i. If its action and successor links are the\nsame as those of a machine state of o, then keep that machine state unchanged in 01\u2022\nii. Else compute the vector for this node and if it pointwise dominates the veetor for a machine state of o, change the action and successor links of that machine state to those of this node. (If it pointwise dominates the vectors of more than one machine state, they can be combined into a single machine state.)\niii. Else add a machine state to o' that has the same action and successor links as this node.\n(d) Prune any machine state of o' that is not reachable from the machine state that optimizes the value of the starting belief state.\nSolving POMDPs by Searching in Policy Space 217\n---------------t \ufffd,' at ,-\"' z2, .. -... ,-... ,, I 3 \\' I 4 \\zf I S \\ 7J' zf z2 Start--et, at \ufffd\ufffdi-ti, at r-\"\\ a2 \ufffd .. ,__, '--'J,,<\ufffd\ufffd--\ufffd' \ufffd\nzl,z2\nb(l)\nV(b)\ufffd--- ---\n2 >< ....... .. ..... ,, '\u00b7\nb(l) I\nFigure 5: Example of how the finite-state controller of Fig ure 1 can be improved by heuristic search. Nodes 3, 4 and 5 in the left panel are potential new machine states that correspond to a path through the search tree, beginning from the starting belief state, for which the lower bound value of each belief state has been improved. Node 5 be comes a new machine state. The vector corresponding to node 4 pointwise dominates the vector for machine state 2 and therefore the action and observation links for machine state 2 can be changed accordingly. Node 3 is a duplicate of machine state 1 and causes no change. The improved finite-state controller and its value function are shown in the right panel.\nfollows. Beginning at the root of the search tree and selecting the action that optimizes the lower bound of each OR node, each reachable node of the search tree for which the lower bound has been improved is identi fied. For each of these nodes, in backwards order from the fringe of the search tree, a corresponding vector is computed based on the current value function and the finite-state controller is modified using the same trans formations used by the policy iteration algorithm. The algorithm is summarized in Figure 4 and illustrated by a simple example in Figure 5.\nHeuristic search can recognize when to change machine states (by detecting pointwise dominance) as well as when to add them. When a machine state is changed, policy evaluation is invoked to recompute the value function; like policy iteration, this algorithm inter leaves a policy improvement step with a policy eval uation step. Any machine state that is not reachable from the machine state that optimizes the value of the starting belief state can be pruned without affect ing the value of the starting belief state and the error bound is simply the difference between the upper and lower bounds for the starting belief state.\nIn our implementation, we use AO* to perform heuris tic search. It expands the search tree in a best-first order and we use the upper bound function to identify the most promising solution tree. To select one of its fringe nodes for expansion, we use the following heuris tic: select for expansion the node (and corresponding\nbelief state b) for which the value\n(UB(b)- V(b)) * (REACHPROB(b) * f3DEPTH(b)\nis greatest, where U B denotes the upper bound function, the lower bound function is the the value function V of the current finite-state controller, REACHPROB(b) denotes the probability of reach ing belief state b beginning from the starting belief state at the root of the tree, and DEPTH(b) denotes the depth of belief state bin the search tree (measured by the number of actions taken along a path from the root) . This selection heuristic focuses computational effort where it is most likely to improve the bounds of the starting belief state.\nThere are several advantages to using heuristic search instead of the dynamic-programming update to im prove a finite-state controller. First and most im portantly, it adds machine states to the finite-state controller only if they improve the value of the start ing belief state. By contrast, policy iteration finds a finite-state controller that optimizes the value of ev ery possible belief state and this is usually a much larger controller. Among the test problems listed in Table 1, for example, an optimal finite-state controller for the cheese grid problem has five machine states when the starting belief state is a uniform probability distribution; policy iteration converges to a finite-state controller with fourteen machine states that optimizes all possible starting belief states. A related advantage of heuristic search is that it can focus computational effort on regions of belief space that are likely to be reached from the starting belief state; for example, it can focus search on the most probable trajectores through the search tree. It also avoids use of linear pro gramming, the most computationally intensive part of the dynamic-programming update.\nThe theoretical properties of the algorithm are similar to those for policy iteration, but are specialized to a starting belief state (Hansen 1998b) .\nTheorem 3 If a finite-state controller does not op timize the value of the starting belief state, heuristic search transforms it into a finite-state controller with an improved value for the starting belief state.\nTheorem 4 The heuristic search algorithm converges after a finite number of steps to a finite-state controller that is \ufffd:-optimal for the starting belief state.\nFor the test problems of Table 1 and several other small POMDPs, this heuristic search algorithm im proves the value of the finite-state controller for a starting belief state faster than policy iteration, and often considerably faster. Results are mixed for im provement of the error bound. For some problems the\n218 Hansen\nerror bound converges quickly to zero or close to it; for others it converges more slowly and the AO* search al gorithm used in the policy improvement step runs out of memory trying to reduce it further. How quickly the error bound converges depends primarily on how closely the upper bound function estimates the opti mal value function for a particular problem. If the upper bound function is not a good estimate, the er ror bound can only be improved by deep expansion of the search tree. Because the quality of the upper bound also determines how aggressively the search tree can be pruned, a poor upper bound function can cause the size of the search tree to quickly exceed available memory. Sophisticated methods for computing upper bound functions have been developed that we have not yet implemented (e.g., Hauskrecht 1997; Brafman 1997) and we expect these will improve performance of the heuristic search algorithm and accelerate conver gence of the error bound. We also plan to implement a memory-bounded version of AO* that can search more deeply in the tree (Chakrabarti et al. 1990; Washing ton 1997).\nThe most promising aspect of this heuristic search al gorithm is its potential for solving problems for which the dynamic-programming update is computationally prohibitive. Consider a simple maze problem de scribed by Hauskrecht (1997) that has 20 states, 6 ac tions, and 8 observations. Although still a very small problem, it is out of the range of dynamic program ming. We tested policy iteration on this problem with an initial finite-state controller with a single machine state. The first iteration of policy iteration took a fraction of a second and resulted in a improved finite state controller with five machine states . The second iteration took two minutes and resulted in a improved finite-state controller with 172 machine states. In the third iteration, the dynamic-programming update ran for 20 hours without finishing, at which point policy iteration was terminated. Clearly this is a problem for which dynamic programming seems computation ally prohibitive. The finite-state controller found after two iterations (and two minutes) had an error bound of 539.7 and a value of 34.3 for a starting belief state that is a uniform state probability distribution. On the same maze problem, our heuristic search algorithm found a finite-state controller with 96 machine states and a value of 52.2 for the same starting belief state, a significant improvement in performance achieved by a smaller controller. After several minutes of expanding the search tree, the algorithm ran out of memory after reducing the error bound to 25.4.\nOf course, this is only a single example and our re sults are preliminary. But it does at least suggest this heuristic search algorithm may extend the range of\nproblems to which the policy space approach described in this paper can be applied. It may do so for several reasons; it eliminates the need to perform the dynamic programming update, it improves a finite-state con troller in an incremental fashion that allows more fine grained control of problem-solving, and it focuses com putation where it is most likely to improve the value of the starting belief state. Testing on a wider range of examples using an improved implementation of the al gorithm is planned to determine how far this approach may extend the range of POMDPs that can be solved by algorithms that use an exact piecewise linear and convex representation of the value function.\n5 Conclusion\nWe have presented two related algorithms - an im proved policy iteration algorithm and a new heuris tic search algorithm - that solve infinite-horizon POMDPs by searching in a policy space of finite-state controllers.\nRepresentation of a policy as a finite-state controller has a number of advantages. An optimal policy for a POMDP is sometimes equivalent to a finite-state con troller, and when it is not, it can be approximated arbitrarily closely by a finite-state controller. Evalua tion of a finite-state controller is straightforward and its value function is piecewise linear and convex. A finite-state controller can also be easier to understand than a policy represented (either explicitly or implic itly) as a mapping from regions of belief space to ac tions, and it can be executed without maintaining a belief state at run-time.\nThe bottleneck of both value iteration and policy iter ation for POMDPs is the dynamic-programming up date; Littman et al. (1995) prove that its worst-case complexity is exponential in the number of actions, ob servations, and vectors in the current value function. Policy iteration is faster than value iteration because it requires fewer iterations of the dynamic-programming update to converge to \u00a3-optimality. Because heuris tic search can improve a finite-state controller without performing the dynamic-programming update, there is some reason to believe it may outperform policy iter ation. Because the finite-state controller it finds opti mizes the value of a starting belief state, and not the value of every possible belief state, it is usually smaller than the controller found by dynamic programming. Heuristic search also focuses computational effort on regions of the search space, or belief space, where im provement of the finite-state controller is most likely.\nThe heuristic search algorithm described here com bines two areas of research on POMDPs that have de veloped indepedently. On the one hand, it draws from\nSolving POMDPs by Searching in Policy Space 219\nwork on exact algorithms for POMDPs that use dy namic programming and a piecewise linear and convex representation of the value function (e.g., Smallwood & Sondik 1973; Sondik 1978; Cassandra et al. 1994; Cassandra et al. 1997). On the other, it draws from work on approximation algorithms for POMDPs that perform forward search from a starting belief state, including work on computing bounds for the fringe nodes of a search tree (e.g., Satia & Lave 1973; Larsen 1989; Washington 1996, 1997; Hauskrecht 1997). In the past, heuristic search has been used to find a so lution that takes the form of a tree that grows as the depth of the search increases. The contribution of this paper is to show that heuristic search can find a com pact finite-state controller (containing cycles) that de scribes infinite-horizon behavior.\nAcknowledgments.\nThanks to Shlomo Zilberstein and an anonymous re viewer for helpful comments and to Tony Cassandra and Milos Hauskrecht for making available their test problems. Support for this work was provided in part by the National Science Foundation under grants IRI9624992, IRI-9634938 and INT-9612092.\nReferences\nBrafman, R.I. (1997). A heuristic variable grid solu tion method for POMDPs. In Proceedings of the Fif teenth National Conference on Artificial Intelligence, 727-733. AAAI Press/The MIT Press.\nCassandra, A; Kaelbling, L.P.; and Littman, M.L. 1994. Acting Optimally in Partially Observable Stochastic Domains. In Proceedings of the Tweltth National Conference on Artificial Intelligence, 1023- 1028. AAAI Press/The MIT Press.\nCassandra, A.; Littman, M.L.; and Zhang, N.L. 1997. Incremental pruning: A simple, fast, exact algorithm for partially observable Markov decision processes. In Proceedings of the T hirteenth Annual Conference on Uncertainty in Artificial Intelligence, 54-61. Morgan Kaufmann Publishers.\nChakrabarti, P.P; Ghose, S.; Acharya, A.; and de Sarkar, S.C. 1990. Heuristic search in restricted mem ory. Artificial Intelligence 41:197-221.\nHansen, E.A. 1998a. An improved policy iteration al gorithm for partially observable MDPs. In Advances in Neural Information Processing Systems 10. In press.\nHansen, E.A. 1998b. Finite-Memory Control of Par tially Observable Systems. Ph.D. Diss., Department of Computer Science, University of Massachusetts at Amherst.\nHauskrecht, M. 1997. Incremental methods for com puting bounds in partially observable Markov decision processes. In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 734-739. AAAI Press/The MIT Press.\nKaelbling, L.P.; Littman, M.L.; and Cassandra, A.R. 1996. Planning and acting in partially observable stochastic domains. Computer Science Technical Re port CS-96-08, Brown University.\nLarsen, J.B. 1989. A Decision Tree Approach to Main taining a Deteriorating Physical System. PhD thesis, University of Texas at Austin.\nLittman, M.L.; Cassandra, A.R.; and Kaebling, L.P. 1995. Efficient dynamic-programming updates in par tially observable Markov decision processes. Com puter Science Technical Report CS-95-19, Brown Uni versity.\nSatia, J.K. and Lave, R.E. 1973. Markovian Decision Processes with Probabilistic Observation of States. Management Science 20(1):1-13.\nSmallwood, R.D. and Sondik, E.J. 1973. The optimal control of partially observable Markov processes over a finite horizon. Operations Research 21:1071-1088.\nSondik, E.J. 1978. The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. Operations Research 26:282-304.\nWashington, R. 1996. Incremental Markov-model planning. In Proceedings of TAI-96, Eighth IEEE In ternational Conference on Tools with Artificial Intelli gence, 41-47.\nWashington, R. 1997. BI-POMDP: Bounded, incre mental partially-observable Markov-model planning."}, {"heading": "In Proceedings of the Fourth European Conference on", "text": "Planning."}], "references": [{"title": "A heuristic variable grid solu\u00ad tion method for POMDPs", "author": ["R.I. Brafman"], "venue": "In Proceedings of the Fif\u00ad teenth National Conference on Artificial Intelligence,", "citeRegEx": "Brafman,? \\Q1997\\E", "shortCiteRegEx": "Brafman", "year": 1997}, {"title": "Acting Optimally in Partially Observable Stochastic Domains", "author": ["A Cassandra", "L.P. Kaelbling", "M.L. Littman"], "venue": "In Proceedings of the Tweltth National Conference on Artificial Intelligence,", "citeRegEx": "Cassandra et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1994}, {"title": "Incremental pruning: A simple, fast, exact algorithm for partially observable Markov decision processes", "author": ["A. Cassandra", "M.L. Littman", "N.L. Zhang"], "venue": "Proceedings of the T hirteenth Annual Conference on Uncertainty in Artificial Intelligence, 54-61. Morgan", "citeRegEx": "Cassandra et al\\.,? 1997", "shortCiteRegEx": "Cassandra et al\\.", "year": 1997}, {"title": "Heuristic search in restricted mem\u00ad ory", "author": ["P.P Chakrabarti", "S. Ghose", "A. Acharya", "S.C. de\u00ad Sarkar"], "venue": "Artificial Intelligence", "citeRegEx": "Chakrabarti et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Chakrabarti et al\\.", "year": 1990}, {"title": "An improved policy iteration al\u00ad gorithm for partially observable MDPs", "author": ["E.A. Hansen"], "venue": "Advances in Neural Information Processing Systems 10. In press.", "citeRegEx": "Hansen,? 1998a", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Finite-Memory Control of Par\u00ad tially Observable Systems", "author": ["E.A. Hansen"], "venue": "Ph.D. Diss., Department of Computer Science, University of Massachusetts at Amherst.", "citeRegEx": "Hansen,? 1998b", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Incremental methods for com\u00ad puting bounds in partially observable Markov decision processes", "author": ["M. Hauskrecht"], "venue": "Proceedings of the Fifteenth National Conference on Artificial Intelligence, 734-739. AAAI Press/The MIT Press.", "citeRegEx": "Hauskrecht,? 1997", "shortCiteRegEx": "Hauskrecht", "year": 1997}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Computer Science Technical Re\u00ad port CS-96-08, Brown University.", "citeRegEx": "Kaelbling et al\\.,? 1996", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "A Decision Tree Approach to Main\u00ad taining a Deteriorating Physical System", "author": ["J.B. Larsen"], "venue": "PhD thesis, University of Texas at Austin.", "citeRegEx": "Larsen,? 1989", "shortCiteRegEx": "Larsen", "year": 1989}, {"title": "Efficient dynamic-programming updates in par\u00ad tially observable Markov decision processes", "author": ["M.L. Littman", "A.R. Cassandra", "L.P. Kaebling"], "venue": "Com\u00ad puter Science Technical Report CS-95-19, Brown Uni\u00ad versity.", "citeRegEx": "Littman et al\\.,? 1995", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "Markovian Decision Processes with Probabilistic Observation of States", "author": ["J.K. Satia", "R.E. Lave"], "venue": "Management Science 20(1):1-13.", "citeRegEx": "Satia and Lave,? 1973", "shortCiteRegEx": "Satia and Lave", "year": 1973}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research 21:1071-1088.", "citeRegEx": "Smallwood and Sondik,? 1973", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs", "author": ["E.J. Sondik"], "venue": "Operations Research 26:282-304.", "citeRegEx": "Sondik,? 1978", "shortCiteRegEx": "Sondik", "year": 1978}, {"title": "Incremental Markov-model planning", "author": ["R. Washington"], "venue": "Proceedings of TAI-96, Eighth IEEE In\u00ad ternational Conference on Tools with Artificial Intelli\u00ad gence, 41-47.", "citeRegEx": "Washington,? 1996", "shortCiteRegEx": "Washington", "year": 1996}, {"title": "BI-POMDP: Bounded, incre\u00ad mental partially-observable Markov-model planning", "author": ["R. Washington"], "venue": "Proceedings of the Fourth European Conference on Planning.", "citeRegEx": "Washington,? 1997", "shortCiteRegEx": "Washington", "year": 1997}], "referenceMentions": [{"referenceID": 12, "context": "Sondik (1978) describes a policy iteration algorithm that rep\u00ad resents a policy in this way.", "startOffset": 0, "endOffset": 14}, {"referenceID": 4, "context": "The first is a policy iteration algorithm, first described by Hansen (1998a), that sim\u00ad plifies policy iteration for POMDPs by representing a policy as a finite-state controller.", "startOffset": 62, "endOffset": 77}, {"referenceID": 11, "context": "The key to computing the dynamic-programming up\u00ad date is Smallwood and Sondik's (1973) proof that it preserves the piecewise linearity and convexity of the value function.", "startOffset": 57, "endOffset": 87}, {"referenceID": 5, "context": "We do not describe here how to com\u00ad pute the dynamic-programming update and instead re\u00ad fer to this paper, Kaelbling et al. (1996), Cassandra et al.", "startOffset": 107, "endOffset": 131}, {"referenceID": 7, "context": "Given this correspondence between vectors and one-step policy choices, Kaelbling et al. (1996) point out that an optimal policy for a finite-horizon POMDP can be represented by an acyclic finite-state controller in which each machine state corresponds to a vector in a nonstationary value function.", "startOffset": 71, "endOffset": 95}, {"referenceID": 10, "context": "Moreover Sondik (1978) and Cassandra et al.", "startOffset": 9, "endOffset": 23}, {"referenceID": 1, "context": "Moreover Sondik (1978) and Cassandra et al. (1994) point out that sometimes, al\u00ad though not reliably, value iteration converges to an optimal piecewise linear and convex value function that is equivalent to a cyclic finite-state controller.", "startOffset": 27, "endOffset": 51}, {"referenceID": 12, "context": "Sondik (1978) describes a policy itera\u00ad tion algorithm for POMDPs that represents a policy as a mapping from a finite number of polyhedral regions of belief space to actions.", "startOffset": 0, "endOffset": 14}, {"referenceID": 5, "context": "We can prove the following generalization of Howards's policy improvement theorem (Hansen 1998b).", "startOffset": 82, "endOffset": 96}, {"referenceID": 1, "context": "Table 1: Comparison of value iteration and policy itera\u00ad tion on nine test problems from Cassandra et al. (1997).", "startOffset": 89, "endOffset": 113}, {"referenceID": 5, "context": "We use the same stopping condition Sondik uses to de\u00ad tect \u20ac-optimality: a finite-state controller is \u20ac-optimal when the Bellman residual is less than or equal to t:(l- (3)/(3, where (3 is the discount factor, and we can prove the following convergence result (Hansen 1998b).", "startOffset": 260, "endOffset": 274}, {"referenceID": 9, "context": "For POMDPs, policy evaluation has low-order polynomial complexity com\u00ad pared to the worst-case exponential complexity of the dynamic-programming update (Littman et al. 1995).", "startOffset": 152, "endOffset": 173}, {"referenceID": 9, "context": "For POMDPs, policy evaluation has low-order polynomial complexity com\u00ad pared to the worst-case exponential complexity of the dynamic-programming update (Littman et al. 1995). Therefore, policy iteration appears to have a clearer advantage over value iteration for POMDPs. Table 1 compares the performance of value iteration and policy iteration on nine test problems from Cas\u00ad sandra et al. (1997). (For these problems, the average number of states is 9.", "startOffset": 153, "endOffset": 398}, {"referenceID": 9, "context": "Satia and Lave (1973) describe a branch-and-bound algorithm for solving infinite-horizon POMDPs, given an initial belief state, and Larsen (1989) and Washington (1996,1997) use the best-first heuristic search algorithm AO* in a similar way.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "Satia and Lave (1973) describe a branch-and-bound algorithm for solving infinite-horizon POMDPs, given an initial belief state, and Larsen (1989) and Washington (1996,1997) use the best-first heuristic search algorithm AO* in a similar way.", "startOffset": 132, "endOffset": 146}, {"referenceID": 10, "context": "The error bound (the dif\u00ad ference between the upper and lower bounds on the value of the starting belief state) can be made arbi\u00ad trarily small by expanding the search tree far enough and, for discounted POMDPs, an E-optimal policy for the belief state at the root of the tree can be found after a finite search (Satia and Lave 1973).", "startOffset": 312, "endOffset": 333}, {"referenceID": 5, "context": "The theoretical properties of the algorithm are similar to those for policy iteration, but are specialized to a starting belief state (Hansen 1998b) .", "startOffset": 134, "endOffset": 148}, {"referenceID": 0, "context": "Sophisticated methods for computing upper bound functions have been developed that we have not yet implemented (e.g., Hauskrecht 1997; Brafman 1997) and we expect these will improve performance of the heuristic search algorithm and accelerate conver\u00ad gence of the error bound.", "startOffset": 111, "endOffset": 148}, {"referenceID": 3, "context": "We also plan to implement a memory-bounded version of AO* that can search more deeply in the tree (Chakrabarti et al. 1990; Washing\u00ad ton 1997).", "startOffset": 98, "endOffset": 142}, {"referenceID": 6, "context": "Consider a simple maze problem de\u00ad scribed by Hauskrecht (1997) that has 20 states, 6 ac\u00ad tions, and 8 observations.", "startOffset": 46, "endOffset": 64}, {"referenceID": 9, "context": "The bottleneck of both value iteration and policy iter\u00ad ation for POMDPs is the dynamic-programming up\u00ad date; Littman et al. (1995) prove that its worst-case complexity is exponential in the number of actions, ob\u00ad servations, and vectors in the current value function.", "startOffset": 110, "endOffset": 132}, {"referenceID": 12, "context": "work on exact algorithms for POMDPs that use dy\u00ad namic programming and a piecewise linear and convex representation of the value function (e.g., Smallwood & Sondik 1973; Sondik 1978; Cassandra et al. 1994; Cassandra et al. 1997).", "startOffset": 138, "endOffset": 228}, {"referenceID": 1, "context": "work on exact algorithms for POMDPs that use dy\u00ad namic programming and a piecewise linear and convex representation of the value function (e.g., Smallwood & Sondik 1973; Sondik 1978; Cassandra et al. 1994; Cassandra et al. 1997).", "startOffset": 138, "endOffset": 228}, {"referenceID": 2, "context": "work on exact algorithms for POMDPs that use dy\u00ad namic programming and a piecewise linear and convex representation of the value function (e.g., Smallwood & Sondik 1973; Sondik 1978; Cassandra et al. 1994; Cassandra et al. 1997).", "startOffset": 138, "endOffset": 228}, {"referenceID": 8, "context": "On the other, it draws from work on approximation algorithms for POMDPs that perform forward search from a starting belief state, including work on computing bounds for the fringe nodes of a search tree (e.g., Satia & Lave 1973; Larsen 1989; Washington 1996, 1997; Hauskrecht 1997).", "startOffset": 203, "endOffset": 281}, {"referenceID": 6, "context": "On the other, it draws from work on approximation algorithms for POMDPs that perform forward search from a starting belief state, including work on computing bounds for the fringe nodes of a search tree (e.g., Satia & Lave 1973; Larsen 1989; Washington 1996, 1997; Hauskrecht 1997).", "startOffset": 203, "endOffset": 281}], "year": 2011, "abstractText": "Most algorithms for solving POMDPs itera\u00ad tively improve a value function that implic\u00ad itly represents a policy and are said to search in value function space. This paper presents an approach to solving POMDPs that repre\u00ad sents a policy explicitly as a finite-state con\u00ad troller and iteratively improves the controller by search in policy space. Two related al\u00ad gorithms illustrate this approach. The first is a policy iteration algorithm that can out\u00ad perform value iteration in solving infinite\u00ad horizon POMDPs. It provides the founda\u00ad tion for a new heuristic search algorithm that promises further speedup by focusing compu\u00ad tational effort on regions of the problem space that are reachable, or likely to be reached, from a start state.", "creator": "pdftk 1.41 - www.pdftk.com"}}}