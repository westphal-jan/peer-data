{"id": "1302.1612", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2013", "title": "Arabic text summarization based on latent semantic analysis to enhance arabic documents clustering", "abstract": "egyptian documents clustering operates an established method for delivering good results utilizing the global information retrieval ( ir ) engine especially comparing the rapid counting of the number of diverse documents present in various context. paper clustering need to only group all files without one cluster using different similarity / distance standards. this timing is often affected by the documents length, that information on the documents is potentially accompanied by small sudden accompanying electronic noise, generally therefore it almost very mainly add accumulated noise while keeping useful facts to boost the performance of documents clustering. if simple paper, team just carefully analyze inverse mechanism of text detection using modified mixed semantic analysis model on arabic documents clustering one order to demonstrate problems considered above, using five similarity / distance measures : perceived proportion, cosine range, distinguishing coefficient, pearson correlation model and low kullback - leibler sensitivity, for two factors : uniform and dense stemming. our experimental results indicate that good design approach additionally solves the problems of noisy information in documents handling, and thus significantly improve the clustering performance.", "histories": [["v1", "Wed, 6 Feb 2013 23:24:37 GMT  (260kb)", "http://arxiv.org/abs/1302.1612v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["hanane froud", "abdelmonaime lachkar", "said alaoui ouatik"], "accepted": false, "id": "1302.1612"}, "pdf": {"name": "1302.1612.pdf", "metadata": {"source": "CRF", "title": "SEMANTIC ANALYSIS TO ENHANCE ARABIC DOCUMENTS CLUSTERING", "authors": ["Hanane Froud", "Abdelmonaime Lachkar", "Said Alaoui Ouatik", "Sidi Mohamed", "Ben Abdellah (USMBA", "Fez", "Sameh H. Ghwanmeh"], "emails": ["hanane_froud@yahoo.fr,", "abdelmonaime_lachkar@yahoo.fr", "s_ouatik@yahoo.com"], "sections": [{"heading": null, "text": "KEYWORDS Information Retrieval Systems, Arabic Language, Arabic Text Clustering, Arabic Text Summarization, Similarity Measures, Latent Semantic Analysis, Root and Light Stemmers."}, {"heading": "1. INTRODUCTION", "text": "There are several research projects investigating and exploring the techniques in traditional Information Retrieval (IR) systems for the English and European languages such as French, German, and Spanish and in Asian languages such as Chinese and Japanese. However, in Arabic language, there is little ongoing research in Arabic traditional Information Retrieval (IR) systems.\nMoreover, the traditional Information Retrieval (IR) systems (without documents clustering) are becoming more and more insufficient for handling huge volumes of relevant texts documents, because to retrieve the documents of interest, the user must formulate the query using the keywords that appear in the documents. This is a difficult task for ordinary people who are not familiar with the vocabulary of the data corpus. Documents clustering may be useful as a complement to these traditional Information Retrieval (IR) systems, by organizing these documents by topics (clusters) in the documents feature space. It has been proved by Bellot & El-B\u00e8ze in [1] that document clustering increase the precision in Information Retrieval (IR) systems for French language.\nOn the other hand, for the Arabic Language Sameh H. Ghwanmeh in [2] presented a comparison study between the traditional Information Retrieval system and the clustered one. The concept of clustering documents has shown significant results on precision compared with traditional Information Retrieval systems without clustering. These results assure the results obtained by Bellot & El-B\u00e8ze [1] during their test on Amaryllis\u201999 corpora for French language.\nTraditional documents clustering algorithms use the full-text in the documents to generate feature vectors. Such methods often produce unsatisfactory results because there is much noisy information in documents. The varying-length problem of the documents is also a significant negative factor affecting the performance. In this paper, we propose to investigate the use of summarization techniques to tackle these issues when clustering documents [13].\nThe goal of a summary is to produce a short representation of a long document. This problem can be solved by building an abstract representation of the whole document and then generating a shorter text or by selecting a few relevant sentences of the original text. With a large volume of text documents, presenting the user with a summary of each document greatly facilitates the task of finding the desired documents so:\n\u2022 Text Summarization can be used to save time. \u2022 Text Summarization can speed up other information retrieval and text mining\nprocesses.\nIn this paper, we propose to use the Latent Semantic Analysis to produce the Arabic summaries that we utilize to represent the documents in the Vector Space Model (VSM) and cluster them, in order to enhance the Arabic documents clustering [14].\nLatent Semantics Analysis (LSA) has been successfully applied to information retrieval [13] [15][16][17] as well as many other related domains. It is based on Singular Value Decomposition (SVD), a mathematical matrix decomposition technique closely akin to factor analysis that is applicable to text corpora. Recently, LSA has been introduced into generic text summarization by [18].\nThis paper is organized as follows. The next section describes the Arabic summarization based Latent Semantic Analysis Model. Section 3 and 4 discuss respectively the Arabic text preprocessing, document representation used in the experiments, and the similarity measures. Section 5 explains experiment settings, dataset, evaluation approaches, results and analysis. Section6 concludes and discusses future work."}, {"heading": "2. ARABIC TEXT SUMMARIZATION BASED ON LATENT SEMANTIC ANALYSIS MODEL", "text": ""}, {"heading": "2.1. LSA Summarization", "text": "In this work, we propose to apply the Latent Semantic Analysis Model in order to generic Arabic Text Summarization [13] [17][18][19]. The process starts with the creation of terms by sentences matrix A = [A1 A2 ... An] with each column vector Ai representing the weighted term-frequency vector of sentence i in the document under consideration. The weighted termfrequency vector Ai = [a1i a2i ... ani] T of sentence i is defined as:\n( ). ( )ij ij ija L t G t= where :\n1. L(tji) is the local weighting for term j in sentence i: L(tji)=tf(tji) where tf(tji) is the number of times term j occurs in the sentence.\n2. G(tji) is the global weighting for term j in the whole document: ( ) log( / ( ))ij ijG t N n t= where N is the total number of sentences in the document, and n(tij) is the number of sentences that contain term j.\nIf there are a total of m terms and n sentences in the document, then we will have an m x n matrix A for the document.\nGiven an m x n matrix A (such as m\u2265n) the SVD of A is defined as [20]:\nTA U V= \u2211\nwhere U = [uij] is an m \u00d7 n column-orthonormal matrix whose columns are called left singular vectors; \u03a3 = diag(\u03c31, \u03c32, \u2026, \u03c3n) is an n \u00d7 n diagonal matrix, whose diagonal elements are non-negative singular values sorted in descending order, and V = [vij] is an n \u00d7 n orthonormal matrix, whose columns are called right singular vectors. If rank(A) = r, then [21] \u03a3 satisfies:"}, {"heading": "1 2 1... ... 0r r n\u03c3 \u03c3 \u03c3 \u03c3 \u03c3+\u2265 \u2265 = = =\u227b", "text": "The interpretation of applying the SVD to the terms by sentences matrix A can be made from two different viewpoints. From transformation point of view, the SVD derives a mapping between the m-dimensional space spawned by the weighted term-frequency vectors and the rdimensional singular vector space. From semantic point of view, the SVD derives the latent semantic structure from the document represented by matrix A. This operation reflects a breakdown of the original document into r linearly-independent base vectors or concepts. Each term and sentence from the document is jointly indexed by these base vectors/concepts.\nA unique SVD feature is that it is capable of capturing and modeling interrelationships among terms so that it can semantically cluster terms and sentences. Further-more, as demonstrated in [21], if a word combination pattern is salient and recurring in document, this pattern will be captured and represented by one of the singular vectors. The magnitude of the corresponding singular value indicates the importance degree of this pattern within the document. Any sentences containing this word combination pattern will be projected along this singular vector, and the sentence that best represents this pattern will have the largest index value with this vector. As each particular word combination pattern describes a certain topic/concept in the document, the facts described above naturally lead to the hypothesis that each singular vector represents a salient topic/concept of the document, and the magnitude of its corresponding singular value represents the degree of importance of the salient topic/concept.\nBased on the above discussion, authors [18] proposed a summarization method which uses the matrix VT. This matrix describes an importance degree of each topic in each sentence. The summarization process chooses the most informative sentence for each topic. It means that the k\u2019th sentence we choose has the largest index value in k\u2019th right singular vector in matrix VT.\nThe proposed method in [18] is as follows:\n1. Decompose the document D into individual sentences, and use these sentences to form the candidate sentence set S, and set k = 1.\n2. Construct the terms by sentences matrix A for the document D.\n3. Perform the SVD on A to obtain the singular value matrix \u2211, and the right singular vector matrix VT. In the singular vector space, each sentence i is represented by the column vector\n[ ]1 2... Ti i i ir\u03c5 \u03c5 \u03c5\u03a8 = of VT.\n4. Select the k\u2019th right singular vector from matrix VT.\n5. Select the sentence which has the largest index value with the k\u2019th right singular vector, and include it in the summary.\n6. If k reaches the predefined number, terminate the operation; otherwise, increment k by one, and go to Step 4.\nIn Step 5 of the above operation, finding the sentence that has the largest index value with the k\u2019th right singular vector is equivalent to finding the column vector i\u03a8 whose k\u2019th element ik\u03c5 is the largest."}, {"heading": "2.2. Arabic Summarization", "text": "In this paper we propose to use the above method to identify semantically important sentences for Arabic Summary creations (Figure 1) in order to enhance the Arabic Documents Clustering task.\nFigure 1. Arabic Text Summarization based on Latent Semantic Analysis Model After building the test corpus, we decompose each document into individual sentences; this decomposition is a source of ambiguity, because on the one hand punctuation is rarely used in Arabic texts and other punctuation that, when it exists, is not always critical to\nDecomposition Sample :\nInput Data\nDocument\nDecomposition using Table.1 :\nSentences Words\nThe weighted term-frequency vector Ai = [a1i a2i ... ani] T of sentence i\nSentences 1. \u0629\u0621\u0627 2. \u0629 \u0627 \u0627 \u0627 \u0627 3. \u0627 \u0627 4. \u0627 ! \u0627 \u0645 # \u0627 $% &' \u0627 \u0629\u0631 )* \u0629 + & , Words \u0629\u0621\u0627\n\u0627 \u0627 \u0627 \u0629 \u0627 \u0627 \u0627 \u0627\nBuilding the terms by sentences matrix A = [A1 A2 ... An]\nApply LSA Model\nExtracting the Relevant Sentences\nDocument Summary\n\u0627 \u0627 \u0627 \u0629\u0621\u0627 \u0629 \u0627 \u0627 \u0646\u0623 \u0625 \u0627 \u0627\n\u0645 ! \"# $ \u0627 \u0627 \u0645 % \u0627 \u0629\u0631 '( # \u0629 )\nguide the decomposition. In addition, some words can mark the beginning of a new sentence (or proposition).\nFor text decomposition [22] uses:\nA morphological decomposition based on punctuation,\nDecomposition based on the recognition of markers morphosyntactic or functional words such as: -&., /0 , \u0648, \u0648\u0623, or, and, but, when. However, these particles may play a role other than to separate phrases.\nIn our experiments, we use the morphosyntactic markers or functional words cited in [23] to decompose the document into individual sentences, in the following table we present some examples of these markers or functional words:"}, {"heading": "3. ARABIC TEXT PREPROCESSING", "text": ""}, {"heading": "3.1. Arabic Language Structure", "text": "The Arabic language is the language of the Holy Quran. It is one of the six official languages of the United Nations and the mother tongue of approximately 300 million people. It is a Semitic language with 28 alphabet letters. His writing orientation is from right-to-left. It can be classified into three types: Classical Arabic (-?@A \u0627 \u0627), Modern Standard Arabic ( \u0627 B ? \u0627) and Colloquial Arabic dialects ( \u0627 \u0627).\nClassical Arabic is fully vowelized and it is the language of the holy Quran. Modern Standard Arabic is the official language throughout the Arab world. It is used in official documents, newspapers and magazines, in educational fields and for communication between Arabs of different nationalities. Colloquial Arabic dialects, on the other hand, are the languages spoken in the different Arab countries; the spoken forms of Arabic vary widely and each Arab country has its own dialect.\nModern Standard Arabic has a rich morphology, based on consonantal roots, which depends on vowel changes and in some cases consonantal insertions and deletions to create inflections and derivations which make morphological analysis a very complex task [24]. There is no capitalization in Arabic, which makes it hard to identify proper names, acronyms, and abbreviations."}, {"heading": "3.2. Stemming", "text": "Arabic word Stemming is a technique that aim to find the lexical root or stem (Figure 2) for words in natural language, by removing affixes attached to its root, because an Arabic word can have a more complicated form with those affixes. An Arabic word can represent a phrase in English, for example the word 23 -4 5555555556 :\u201dto speak with them\u201d is decomposed as follows (Table 2):\nTable 2. Arabic Word Decomposition\nAntefix Prefix Root Suffix Postfix \u0644 \u064a \u062b . \u0646\u0648 4\u0647\nPreposition meaning \u201cto\u201d\nA letter meaning the tense and the person\nof conjugation speak\nTermination of conjugation\nA pronoun Meaning \u201cthem\u201d\nFigure 2. An Example of Root/Stem Preprocessing."}, {"heading": "3.3. Root-based versus Stem-based approaches", "text": "Arabic stemming algorithms can be classified, according to the desired level of analysis, as root-based approach (Khoja [4]); and stem-based approach (Larkey [5]). In this section, a brief review on the two stemming approaches for stemming Arabic Text is presented.\nRoot-Based approach uses morphological analysis to extract the root of a given Arabic word. Many algorithms have been developed for this approach. Al-Fedaghi and Al-Anzi algorithms try to find the root of the word by matching the word with all possible patterns with all possible affixes attached to it [25]. The algorithms do not remove any prefixes or suffixes. Al-Shalabi morphology system uses different algorithms to find the roots and patterns [26]. This algorithm removes the longest possible prefix, and then extracts the root by checking the first five letters\nof the word. This algorithm is based on an assumption that the root must appear in the first five letters of the word. Khoja has developed an algorithm that removes prefixes and suffixes, all the time checking that it\u2019s not removing part of the root and then matches the remaining word against the patterns of the same length to extract the root [4].\nThe aim of the Stem-Based approach or Light Stemmer approach is not to produce the root of a given Arabic word, rather is to remove the most frequent suffixes and prefixes. Light stemmer is mentioned by some authors [27,28,5,29], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.\nIn our work, we believe that the preprocessing of Arabic Documents is challenge and crucial stage. It may impact positively or negatively on the accuracy of any Text Mining tasks; therefore the choice of the preprocessing approaches will lead by necessity to the improvement of any Text Mining tasks very greatly.\nTo illustrate this, in Figure 2, we show an example using Khoja and Light stemmers. It produces different results: root and stem level related to the original word.\nOn the other hand Khoja stemmer can produce wrong results, for example, the word (\u062a I' ) which means (organizations) is stemmed to (J K) which means (he was thirsty) instead of the correct root (4I ).\nPrior to applying document clustering techniques to an Arabic document, the latter is typically preprocessed: it is parsed, in order to remove stop words, and then words are stemmed using tow famous Stemming algorithms: the Morphological Analyzer from Khoja and Garside [4], and the Light Stemmer developed by Larkey [5]. In addition, at this stage in this work, we computed the term-document using tfidf weighting scheme."}, {"heading": "3.4. Document Representation", "text": "There are several ways to model a text document. For example, it can be represented as a bag of words, where words are assumed to appear independently and the order is immaterial. This model is widely used in information retrieval and text mining [6].\nEach word corresponds to a dimension in the resulting data space and each document then becomes a vector consisting of non-negative values on each dimension. Let }{ , ...,1D d dn= be a set of documents and }{ , ...,1T t tm= the set of distinct terms occurring in D. A document is then represented as an m-dimensional vector td\n. Let ( , )tf d t denote the frequency of term t T\u2208 in document t D\u2208 . Then the vector representation of a document d is:\n( ( , ), ..., ( , ))1t tf d t tf d tmd =\nAlthough more frequent words are assumed to be more important, this is not usually the case in practice (in the Arabic language words like \u0625 that means to and 5555 that means in). In fact, more complicated strategies such as the tfidf weighting scheme as described below is normally used instead. So we choose in this work to produce the tfidf weighting for each term for the document representation.\nIn the practice terms those appear frequently in a small number of documents but rarely in the other documents tend to be more relevant and specific for that particular group of documents, and therefore more useful for finding similar documents. In order to capture these terms and reflect their importance, we transform the basic term frequencies ( , )tf d t into the tfidf (term frequency and inversed document frequency) weighting scheme. Tfidf weights the frequency of a term t in a document d with a factor that discounts its importance with its appearances in the whole document collection, which is defined as:\n( , ) ( , ) log( ) ( )\nD tfidf d t tf d t\ndf t = \u00d7\nHere ( )df t is the number of documents in which term t appears, |D| is the numbers of\ndocuments in the dataset. We use , wt d to denote the weight of term t in document d in the following sections."}, {"heading": "4. SIMILARITY MEASURES", "text": "In this section we discuss the five similarity measures that were tested in [3], and we include these five measures in our work to effect the Arabic text document clustering."}, {"heading": "4.1. Metric", "text": "Not every distance measure is a metric. To qualify as a metric, a measure d must satisfy the following four conditions. Let x and y be any two objects in a set and ( , )d x y be the distance between x and y.\n1. The distance between any two points must be non-negative, that is, ( , ) 0d x y \u2265 .\n2. The distance between two objects must be zero if and only if the two objects are\nidentical, that is, ( , ) 0d x y = if and only if x y= .\n3. Distance must be symmetric, that is, distance from x to y is the same as the distance\nfrom y to x, i.e. ( , ) ( , )d x y d y x= .\n4. The measure must satisfy the triangle inequality, which is ( , ) ( , ) ( , )d x z d x y d y z\u2264 + ."}, {"heading": "4.2. Euclidean Distance", "text": "Euclidean distance is widely used in clustering problems, including clustering text. It satisfies all the above four conditions and therefore is a true metric. It is also the default distance measure used with the K-means algorithm.\nMeasuring distance between text documents, given two documents da and db represented by\ntheir term vectors ta and tb\nrespectively, the Euclidean distance of the two documents is defined as\n2 1 2( , ) ( ) ,, ,1 m D t t w wa t aE b t bt = \u2212\u2211 =\nwhere the term set is }{ , ...,1T t tm= . As mentioned previously, we use the tfidf value as term\nweights, that is ( , ),w tfidf d tat a = ."}, {"heading": "4.3. Cosine Similarity", "text": "Cosine similarity is one of the most popular similarity measure applied to text documents, such as in numerous information retrieval applications [6] and clustering too [7]. Given two\ndocuments ta and tb , their cosine similarity is:\n. ( , ) , t ta bSIM t taC b t ta b = \u00d7\nwhere ta and tb are m-dimensional vectors over the term set }{ , ...,1T t tm= . Each dimension represents a term with its weight in the document, which is non-negative. As a result, the cosine similarity is non-negative and bounded between[ ]0,1 . An important property of the cosine similarity is its independence of document length. For example, combining two identical copies\nof a document d to get a new pseudo document 0 d , the cosine similarity between d and 0 d\nis 1, which means that these two documents are regarded to be identical."}, {"heading": "4.4. Jaccard Coefficient", "text": "The Jaccard coefficient, which is sometimes referred to as the Tanimoto coefficient, measures similarity as the intersection divided by the union of the objects. For text document, the Jaccard coefficient compares the sum weight of shared terms to the sum weight of terms that are present in either of the two documents but are not the shared terms. The formal definition is:\n. ( , ) 2 2\n. t ta bSIM t taJ b t t t ta ab b = + \u2212\nThe Jaccard coefficient is a similarity measure and ranges between 0 and 1. It is 1 when the\nt ta b= and 0 when ta and tb\nare disjoint. The corresponding distance measure is 1D S IMJ J= \u2212 and we will use D J instead in subsequent experiments."}, {"heading": "5.5. Pearson Correlation Coefficient", "text": "Pearson\u2019s correlation coefficient is another measure of the extent to which two vectors are related. There are different forms of the Pearson correlation coefficient formula. Given the term set }{ , ...,1T t tm= , a commonly used form is\n,1 , ( , )\n2 2 2 2 ,1 1 , mm w w TF TFat at t b b SIM t taP b m mm w TF m w TFat at t t b b \u00d7 \u2212 \u00d7\u2211 == \u2212 \u2212\u2211 \u2211= =       \nwhere ,1 mTF wa t at= \u2211 = and 1 , mTF wtb t b= \u2211 =\nThis is also a similarity measure. However, unlike the other measures, it ranges from -1 to +1\nand it is 1 when t ta b= . In subsequent experiments we use the corresponding distance measure,\nwhich is 1D SIMP P= \u2212 when 0SIM P \u2265 and D SIMP P= when 0SIM P \u227a ."}, {"heading": "4.6. Averaged Kullback-Leibler Divergence", "text": "In information theory based clustering, a document is considered as a probability distribution of terms. The similarity of two documents is measured as the distance between the two corresponding probability distributions. The Kullback-Leibler divergence (KL divergence), also called the relative entropy, is a widely applied measure for evaluating the differences between two probability distributions.\nGiven two distributions P and Q, the KL divergence from distribution P to distribution Q is defined as\n( || ) log( ) P\nD P Q PKL Q =\nIn the document scenario, the divergence between two distributions of words is:\n, ( || ) log( ).,1 , wm t a D t t wa t aKL b t wt b = \u00d7\u2211 =\nHowever, unlike the previous measures, the KL divergence is not symmetric, i.e. ( || ) ( || )D P Q D Q PK L K L\u2260 . Therefore it is not a true metric. As a result, we use the averaged KL divergence instead, which is defined as:\n( || ) ( || ) ( || ),1 2D P Q D P M D Q MKL KLAvgKL \u03c0 \u03c0= +\nwhere ,1 2\nP Q\nP Q P Q \u03c0 \u03c0= =\n+ + and 1 2 M P Q\u03c0 \u03c0= +\nFor documents, the averaged KL divergence can be computed with the following formula:\n( || ) ( ( || ) ( || )),,1 2 ,1 m D t t D w w D w wa t a t tAvgKL b t bt \u03c0 \u03c0= \u00d7 + \u00d7\u2211 =\nwhere\n, , , ,1 2\n, ,, ,\nww t a t b\nw w w wt a t at b t b\n\u03c0 \u03c0= = + +\nand ,1 2 , w w wt t a t b\u03c0 \u03c0= \u00d7 + \u00d7\nThe average weighting between two vectors ensures symmetry, that is, the divergence from document i to document j is the same as the divergence from document j to document i. The averaged KL divergence has recently been applied to clustering text documents, such as in the family of the Information Bottleneck clustering algorithms [8], to good effect."}, {"heading": "5. EXPERIMENTS AND RESULTS", "text": "In our experiments (Figure 4), we used the K-means algorithm as document clustering method. It works with distance measures which basically aim to minimize the within-cluster distances. Therefore, similarity measures do not directly fit into the algorithm, because smaller values\nindicate dissimilarity. The Euclidean distance and the averaged KL divergence are distance measures, while the cosine similarity, Jaccard coefficient and Pearson coefficient are similarity measures. [3] applies a simple transformation to convert the similarity measure to distance values. Because both cosine similarity and Jaccard coefficient are bounded in [ ]0 , 1 and monotonic, we take 1D SIM= \u2212 as the corresponding distance value. For Pearson coefficient,\nwhich ranges from \u22121 to +1, we take 1D SIM= \u2212 when 0SIM \u2265 and D SIM= when 0SIM \u227a . For the testing dataset, we experimented with different similarity measures for three times: without stemming, and with stemming using the Morphological Analyzer from Khoja and Garside [4] , and the Light Stemmer [5], in two case: in the first one, we apply the proposed method above to summarize for the all documents in dataset and then cluster them. In the second case, we cluster the original documents without summarization. Moreover, each experiment was run 5 times and the results are the averaged value over 5 runs. Each run has different initial seed sets."}, {"heading": "5.1. Dataset", "text": "The testing dataset [9] (Corpus of Contemporary Arabic (CCA)) is composed of 12 several categories, each latter contains documents from websites and from radio Qatar. A summary of the testing dataset is shown in Table 3.\nAs mentioned previously, the baseline method is the full-text representation, for each document, we removed stop words and stem the remaining words by using Khoja stemmer\u2019s and Larkey stemmer\u2019s. Then, to illustrate the benefits of our proposed approach, we use document summaries to cluster our dataset."}, {"heading": "5.2. Results", "text": "The quality of the clustering result was evaluated using two evaluation measures: purity and entropy, which are widely used to evaluate the performance of unsupervised learning algorithms [10] [11].\nThe purity measure evaluates the coherence of a cluster, that is, the degree to which a cluster contains documents from a single category. Given a particular cluster Ci of size ni, the purity of Ci is formally defined as:\n1 ( ) max( )hi i\nhi\nP C n n =\nwhere max( )hi h n is the number of documents that are from the dominant category in cluster Ci and h in represents the number of documents from cluster Ci assigned to category h. In general, the higher the purity value, the better the quality of the cluster is.\nThe entropy measure evaluates the distribution of categories in a given cluster. The entropy of a cluster Ci with size ni is defined to be\n1\n1 ( ) log( )\nlog\nh hk i i\ni h i i\nn n E C\nc n n= = \u2212 \u2211\nwhere c is the total number of categories in the data set and h in is the number of documents from the hth class that were assigned to cluster Ci.\nThe entropy measure is more comprehensive than purity because rather than just considering the number of objects in and not in the dominant category, it considers the overall distribution of all the categories in a given cluster. Contrary to the purity measure, for an ideal cluster with documents from only a single category, the entropy of the cluster will be 0. In general, the smaller the entropy value, the better the quality of the cluster is. Moreover, the averaged entropy of the overall solution is defined to be the weighted sum of the individual entropy value of each cluster, that is,\n1\n( ) k\ni i\ni\nn Entropy E C\nn= =\u2211\nwhere n is the number of documents in our dataset.\nIn the following, The Table 4 and the Table 5 show the average purity and entropy results for each similarity/distance measure with the Morphological Analyzer from Khoja and Garside [4], the Larkey\u2019s Stemmer [5], and without stemming using the full- text representation.\nOn the other hand, the Table 6 and the Table 7 illustrate the results using document summaries with the same stemmers and similarity/distance measures."}, {"heading": "5.2. 1. Results Using Full-Text Representation", "text": "5.2. 1.a. Results with Stemming\nIn Table 4, with Khoja\u2019s stemmer, the overall purity values for the Euclidean Distance, the Cosine Similarity and the averaged KL Divergence are quite similar and perform bad relatively to the other measures. Meanwhile, the Jaccard measure is the better in generating more coherent clusters with a considerable purity score.\nIn this context, using the Larkey\u2019s stemmer, the purity value of the averaged KL Divergence measure is the best one with only 1% difference relatively to the other four measures.\nThe Table 5, shows the higher purity scores (0.77) than those shown in the Table 4 for the Euclidean Distance, the Cosine Similarity and the Jaccard measures. In the other hand the Pearson Correlation and averaged KL Divergence are quite similar but still better than purity values for these measures in the Table 4.\nThe overall entropy value for each measure is shown in the two Tables. Again, the best results are there in the Table 5 that shows the better and similar entropy values for the Euclidean Distance, the Cosine Similarity and the Jaccard measures. However, the averaged KL Divergence performs worst than the other measures but better than the other one in the other Table (Table 4)."}, {"heading": "5.2. 2. Results Using Document Summaries", "text": "Table 6 presents the average purity and entropy results for each similarity/distance measures using document summaries instead the full-text representation with Khoja\u2019s stemmer and Larkey\u2019s stemmer.\nAs shown in Table 6, for the two stemmers, Euclidean Distance, Cosine Similarity, and Jaccard measures are slightly better in generating more coherent clusters which means the clusters have higher purity and lower entropy scores. On the other hand, Pearson and KLD measures perform worst relatively to the other measures. Comparing these results with those obtained in Table 4, we can conclude that the obtained scores was improved specially the overall entropy values.\nA closer look at Tables 5 and 7 shows that, in this latter, the overall entropy values of Euclidean Distance, Cosine Similarity, Jaccard and Pearson measures are nearly similar and proves their ability to produce coherent clusters.\nOn the one side, in the Table 6 we can remark that the purity scores (0.385 Khoja\u2019s stemmer, 0.339 Larkey\u2019s stemmer) are generally higher than those shown in the Table 7 for the all similarity/distance measures, on the other side, the overall entropy values in this table for the Euclidean Distance, the Cosine Similarity and the Jaccard measures with Khoja\u2019s stemmer performs bad than those in the Table 7. However, with Larkey\u2019s stemmer the overall entropy values for each measure performs contrary to their exiting in Table 7.\nThe above results lead as to conclude that:\nFirst, the Tables 4 and 5 show that the use of stemming affects negatively the clustering, this is mainly due to the ambiguity created when we applied the stemming (for example, we can obtain two roots that made of the same letters but semantically different). Our observation\nbroadly agrees with M.El kourdi, A.Bensaid, and T.Rachidi in [12], and with our works in [14][17].\nSecond, the obtained overall entropy values shown in Tables 6 and 7 proves that the summarizing documents can make their topics salient and improve the clustering performance [13] for two times: with and without stemming. However, the obtained purity values seem not promising to improve the clustering task; this is can be due to the bad choice of the number of sentences in summaries because this latter has great impact on the quality of summaries thus could lead to different clustering results. Too few sentences will result in mach sparse vector representation and are not enough to represent the document fully. Too many sentences may introduce noise and degrade the benefits of the summarization."}, {"heading": "6. CONCLUSION", "text": "In this paper, we have proposed to illustrate the benefits of the summarization using the Latent Semantic Analysis Model, by comparing the clustering results based on summarization with the full-text baseline on the Arabic Documents Clustering for five similarity/distance measures for three times: without stemming, and with stemming using Khoja\u2019s stemmer, and the Larkey\u2019s stemmer.\nWe found that the Euclidean Distance, the Cosine Similarity and the Jaccard measures have comparable effectiveness for the partitional Arabic Documents Clustering task for finding more coherent clusters in case we didn\u2019t use the stemming for the full-text representation. On the other hand the Pearson Correlation and averaged KL Divergence are quite similar in theirs results but there are not better than the other measures in the same case.\nInstead of using full-text as the representation for document clustering, we use LSA model as summarization techniques to eliminate the noise on the documents and select the most salient sentences to represent the original documents. Furthermore, summarization can help overcome the varying length problem of the diverse documents. In our experiments using document summaries, we remark that again the Euclidean Distance, the Cosine Similarity and the Jaccard measures have comparable effectiveness to produce more coherent clusters than the Pearson Correlation and averaged KL Divergence, in the two times: with and without stemming."}], "references": [{"title": "Ghwanmeh, \u201cApplying Clustering of Hierarchical K-means-like Algorithm on Arabic Language", "author": ["H. Sameh"], "venue": "International Journal of Information Technology IJIT Volume 3 Number", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Stemming Arabic Text", "author": ["S. Khoja", "R. Garside"], "venue": "http://www.comp.lancs.ac.uk/computing/users/khoja/stemmer.ps", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Improving Stemming for Arabic Information Retrieval: Light Stemming and Co-occurrence Analysis", "author": ["Larkey", "Leah S", "Ballesteros", "Lisa", "Connell", "Margaret"], "venue": "In Proceedings of the 25th Annual International Conference on Research and Development in Information Retrieval (SIGIR", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Neto.\u201dModern Information Retrieval", "author": ["B.R.R.B. Yates"], "venue": "ADDISON-WESLEY, New York,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Fast and Effective Text Mining using Linear-time Document Clustering", "author": ["B. Larsen", "C. Aone"], "venue": "In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "The Information Bottleneck Method", "author": ["N.Z. Tishby", "F. Pereira", "W. Bialek"], "venue": "In Proceedings of the 37th Allerton Conference on Communication, Control and Computing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Karypis.\u201dEvaluation of Hierarchical Clustering Algorithms for Document Datasets", "author": ["G.Y. Zhao"], "venue": "In Proceedings of the International Conference on Information and Knowledge Management,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Karypis.\u201dEmpirical and Theoretical Comparisons of Selected Criterion Functions for Document Clustering", "author": ["G.Y. Zhao"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Web page clustering enhanced by summarization", "author": ["Xuanhui Wang", "Dou Shen", "Hua-Jun Zeng", "Zheng Chen", "Wei-Ying Ma"], "venue": "Proceedings of the 2004 ACM CIKM International Conference on Information and Knowledge Management,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S. Dumais", "G. Furnas", "T. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science, vol. 41, pp.391-407, 1990.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "Stemming Versus Light Stemming for Measuring the Simitilarity between Arabic Words with Latent Semantic Analysis Model\u201d, 2 International Colloquium in Information Science and Technology (CIST", "author": ["H. Froud", "A. Lachkar", "S. Alaoui Ouatik"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Generic text summarization using relevance measure and latent semantic analysis", "author": ["Y.H. Gong", "X. Liu"], "venue": "Proc. The 24th annual international ACM SIGIR, pp. 19 - 25, 2001.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Using Latent Semantic Analysis in text summarization and summary evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "Proceedings of ISIM", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Numerical Recipes in C: The Art of Scientific Computing", "author": ["W. Press"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1992}, {"title": "Using Linear Algebra for Intelligent Information Retrieval", "author": ["M.W. Berry", "S.T. Dumais", "G. W O\u2019Brien"], "venue": "SIAM Review", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "A major offshoot of the DIINAR-MBC project: AraParse, a morphosyntactic analyzer for unvowelled Arabic texts, ACL/EACL", "author": ["R. Ouersighni"], "venue": "Workshop on Arabic Language Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "A new algorithm to generate Arabic root-pattern forms", "author": ["Al-Fedaghi S", "F. Al-Anzi"], "venue": "In proceedings of the 11th national Computer Conference and Exhibition", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1989}, {"title": "A computational morphology system for Arabic", "author": ["Al-Shalabi R", "M. Evens"], "venue": "In Workshop on Computational Approaches to Semitic Languages,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "On Arabic search: improving the retrieval effectiveness via a light temming approach", "author": ["Aljlayl M", "O. Frieder"], "venue": "In ACM CIKM 2002 International Conference on Information and Knowledge Management,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Arabic information retrieval at UMass in TREC-10", "author": ["Larkey L", "M.E. Connell"], "venue": "Proceedings of TREC", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "Building an Arabic Stemmer for Information Retrieval", "author": ["Chen A", "F. Gey"], "venue": "In Proceedings of the 11th Text Retrieval Conference (TREC", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Ghwanmeh in [2] presented a comparison study between the traditional Information Retrieval system and the clustered one.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "In this paper, we propose to investigate the use of summarization techniques to tackle these issues when clustering documents [13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "Latent Semantics Analysis (LSA) has been successfully applied to information retrieval [13] [15][16][17] as well as many other related domains.", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "Latent Semantics Analysis (LSA) has been successfully applied to information retrieval [13] [15][16][17] as well as many other related domains.", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "Latent Semantics Analysis (LSA) has been successfully applied to information retrieval [13] [15][16][17] as well as many other related domains.", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Recently, LSA has been introduced into generic text summarization by [18].", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "LSA Summarization In this work, we propose to apply the Latent Semantic Analysis Model in order to generic Arabic Text Summarization [13] [17][18][19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "LSA Summarization In this work, we propose to apply the Latent Semantic Analysis Model in order to generic Arabic Text Summarization [13] [17][18][19].", "startOffset": 142, "endOffset": 146}, {"referenceID": 12, "context": "LSA Summarization In this work, we propose to apply the Latent Semantic Analysis Model in order to generic Arabic Text Summarization [13] [17][18][19].", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "Given an m x n matrix A (such as m\u2265n) the SVD of A is defined as [20]:", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "If rank(A) = r, then [21] \u03a3 satisfies:", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Further-more, as demonstrated in [21], if a word combination pattern is salient and recurring in document, this pattern will be captured and represented by one of the singular vectors.", "startOffset": 33, "endOffset": 37}, {"referenceID": 11, "context": "Based on the above discussion, authors [18] proposed a summarization method which uses the matrix V.", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "The proposed method in [18] is as follows:", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "For text decomposition [22] uses:", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "Root-based versus Stem-based approaches Arabic stemming algorithms can be classified, according to the desired level of analysis, as root-based approach (Khoja [4]); and stem-based approach (Larkey [5]).", "startOffset": 160, "endOffset": 163}, {"referenceID": 2, "context": "Root-based versus Stem-based approaches Arabic stemming algorithms can be classified, according to the desired level of analysis, as root-based approach (Khoja [4]); and stem-based approach (Larkey [5]).", "startOffset": 198, "endOffset": 201}, {"referenceID": 16, "context": "Al-Fedaghi and Al-Anzi algorithms try to find the root of the word by matching the word with all possible patterns with all possible affixes attached to it [25].", "startOffset": 156, "endOffset": 160}, {"referenceID": 17, "context": "Al-Shalabi morphology system uses different algorithms to find the roots and patterns [26].", "startOffset": 86, "endOffset": 90}, {"referenceID": 1, "context": "Khoja has developed an algorithm that removes prefixes and suffixes, all the time checking that it\u2019s not removing part of the root and then matches the remaining word against the patterns of the same length to extract the root [4].", "startOffset": 227, "endOffset": 230}, {"referenceID": 18, "context": "Light stemmer is mentioned by some authors [27,28,5,29], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.", "startOffset": 43, "endOffset": 55}, {"referenceID": 19, "context": "Light stemmer is mentioned by some authors [27,28,5,29], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.", "startOffset": 43, "endOffset": 55}, {"referenceID": 2, "context": "Light stemmer is mentioned by some authors [27,28,5,29], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.", "startOffset": 43, "endOffset": 55}, {"referenceID": 20, "context": "Light stemmer is mentioned by some authors [27,28,5,29], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.", "startOffset": 43, "endOffset": 55}, {"referenceID": 1, "context": "Prior to applying document clustering techniques to an Arabic document, the latter is typically preprocessed: it is parsed, in order to remove stop words, and then words are stemmed using tow famous Stemming algorithms: the Morphological Analyzer from Khoja and Garside [4], and the Light Stemmer developed by Larkey [5].", "startOffset": 270, "endOffset": 273}, {"referenceID": 2, "context": "Prior to applying document clustering techniques to an Arabic document, the latter is typically preprocessed: it is parsed, in order to remove stop words, and then words are stemmed using tow famous Stemming algorithms: the Morphological Analyzer from Khoja and Garside [4], and the Light Stemmer developed by Larkey [5].", "startOffset": 317, "endOffset": 320}, {"referenceID": 3, "context": "This model is widely used in information retrieval and text mining [6].", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Cosine Similarity Cosine similarity is one of the most popular similarity measure applied to text documents, such as in numerous information retrieval applications [6] and clustering too [7].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "Cosine Similarity Cosine similarity is one of the most popular similarity measure applied to text documents, such as in numerous information retrieval applications [6] and clustering too [7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 5, "context": "The averaged KL divergence has recently been applied to clustering text documents, such as in the family of the Information Bottleneck clustering algorithms [8], to good effect.", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "For the testing dataset, we experimented with different similarity measures for three times: without stemming, and with stemming using the Morphological Analyzer from Khoja and Garside [4] , and the Light Stemmer [5], in two case: in the first one, we apply the proposed method above to summarize for the all documents in dataset and then cluster them.", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "For the testing dataset, we experimented with different similarity measures for three times: without stemming, and with stemming using the Morphological Analyzer from Khoja and Garside [4] , and the Light Stemmer [5], in two case: in the first one, we apply the proposed method above to summarize for the all documents in dataset and then cluster them.", "startOffset": 213, "endOffset": 216}, {"referenceID": 6, "context": "Results The quality of the clustering result was evaluated using two evaluation measures: purity and entropy, which are widely used to evaluate the performance of unsupervised learning algorithms [10] [11].", "startOffset": 196, "endOffset": 200}, {"referenceID": 7, "context": "Results The quality of the clustering result was evaluated using two evaluation measures: purity and entropy, which are widely used to evaluate the performance of unsupervised learning algorithms [10] [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 1, "context": "In the following, The Table 4 and the Table 5 show the average purity and entropy results for each similarity/distance measure with the Morphological Analyzer from Khoja and Garside [4], the Larkey\u2019s Stemmer [5], and without stemming using the full- text representation.", "startOffset": 182, "endOffset": 185}, {"referenceID": 2, "context": "In the following, The Table 4 and the Table 5 show the average purity and entropy results for each similarity/distance measure with the Morphological Analyzer from Khoja and Garside [4], the Larkey\u2019s Stemmer [5], and without stemming using the full- text representation.", "startOffset": 208, "endOffset": 211}, {"referenceID": 8, "context": "Second, the obtained overall entropy values shown in Tables 6 and 7 proves that the summarizing documents can make their topics salient and improve the clustering performance [13] for two times: with and without stemming.", "startOffset": 175, "endOffset": 179}], "year": 2013, "abstractText": "Arabic Documents Clustering is an important task for obtaining good results with the traditional Information Retrieval (IR) systems especially with the rapid growth of the number of online documents present in Arabic language. Documents clustering aim to automatically group similar documents in one cluster using different similarity/distance measures. This task is often affected by the documents length, useful information on the documents is often accompanied by a large amount of noise, and therefore it is necessary to eliminate this noise while keeping useful information to boost the performance of Documents clustering. In this paper, we propose to evaluate the impact of text summarization using the Latent Semantic Analysis Model on Arabic Documents Clustering in order to solve problems cited above, using five similarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard Coefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler Divergence, for two times: without and with stemming. Our experimental results indicate that our proposed approach effectively solves the problems of noisy information and documents length, and thus significantly improve the clustering performance.", "creator": "PDFCreator Version 1.2.3"}}}