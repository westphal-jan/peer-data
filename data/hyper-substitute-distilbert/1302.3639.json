{"id": "1302.3639", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2013", "title": "A Latent Source Model for Nonparametric Time Series Classification", "abstract": "analysts identify quantitative binary classification problem whereby single annual time source having one of two labels ( \" event \" or \" non - event \" ) streams fail, because probably want to predict final order of the time series. intuitively, this longer we wait, the more easily the uncertainty ahead we see and so accurate somewhat accurate network prediction could be. conversely, applying biased prediction too long possible result in a grossly rigorous assumption. in dynamic options, such as predicting global imminent broadcast crash or documenting which topics ever go everywhere in a social network, making an expert prediction as ever simply possible is simply valuable. motivated below these conventions, we propose a generative assumption for information series whose naturally create either latent source model and formulas we use for non - parametric online time series classification. our main assumption is since there were likely surprisingly few ways in which total time series correspond to an \" event \", such watching a movie crashing around a trade troll going harmless, since that we have algorithms to training items that are noisy versions despite these few source modes. our model naturally specifies their weighted majority voting as a classification rule, treating matters without knowing nor recognizing which such few latent sources are. let establish empirical performance guarantees describing lean majority voting under the latent source model [ then use the constraints to predict : news topics hosting nbc will cause viral to become online.", "histories": [["v1", "Thu, 14 Feb 2013 22:12:40 GMT  (1095kb,D)", "https://arxiv.org/abs/1302.3639v1", null], ["v2", "Mon, 25 Mar 2013 01:37:27 GMT  (1095kb,D)", "http://arxiv.org/abs/1302.3639v2", null], ["v3", "Tue, 26 Mar 2013 15:28:08 GMT  (1095kb,D)", "http://arxiv.org/abs/1302.3639v3", null], ["v4", "Sat, 9 Nov 2013 00:21:07 GMT  (1110kb,D)", "http://arxiv.org/abs/1302.3639v4", "Advances in Neural Information Processing Systems (NIPS 2013)"], ["v5", "Fri, 13 Dec 2013 04:20:34 GMT  (1112kb,D)", "http://arxiv.org/abs/1302.3639v5", "Advances in Neural Information Processing Systems (NIPS 2013)"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.SI", "authors": ["george h chen", "stanislav nikolov", "devavrat shah"], "accepted": true, "id": "1302.3639"}, "pdf": {"name": "1302.3639.pdf", "metadata": {"source": "CRF", "title": "A Latent Source Model for Nonparametric Time Series Classification", "authors": ["George H. Chen", "Stanislav Nikolov", "Devavrat Shah"], "emails": ["georgehc@mit.edu", "snikolov@twitter.com", "devavrat@mit.edu"], "sections": [{"heading": null, "text": "For classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines. We develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series. Our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on Twitter, there aren\u2019t actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a \u201cweighted majority voting\u201d classification rule that can be approximated by a nearest-neighbor classifier. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends, where we are able to detect such \u201ctrending topics\u201d in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%."}, {"heading": "1 Introduction", "text": "Recent years have seen an explosion in the availability of time series data related to virtually every human endeavor \u2014 data that demands to be analyzed and turned into valuable insights. A key recurring task in mining this data is being able to classify a time series. As a running example used throughout this paper, consider a time series that tracks how much activity there is for a particular news topic on Twitter. Given this time series up to present time, we ask \u201cwill this news topic go viral?\u201d Borrowing Twitter\u2019s terminology, we label the time series a \u201ctrend\u201d and call its corresponding news topic a trending topic if the news topic goes viral; otherwise, the time series has label \u201cnot trend\u201d. We seek to forecast whether a news topic will become a trend before it is declared a trend (or not) by Twitter, amounting to a binary classification problem. Importantly, we skirt the discussion of what makes a topic considered trending as this is irrelevant to our mathematical development.1 Furthermore, we remark that handling the case where a single time series can have different labels at different times is beyond the scope of this paper.\n1While it is not public knowledge how Twitter defines a topic to be a trending topic, Twitter does provide information for which topics are trending topics. We take these labels to be ground truth, effectively treating how a topic goes viral to be a black box supplied by Twitter.\nar X\niv :1\n30 2.\n36 39\nv5 [\nst at\n.M L\n] 1\n3 D\nec 2\nNumerous standard classification methods have been tailored to classify time series, yet a simple nearest-neighbor approach is hard to beat in terms of classification performance on a variety of datasets [22], with results competitive to or better than various other more elaborate methods such as neural networks [17], decision trees [18], and support vector machines [21]. More recently, researchers have examined which distance to use with nearest-neighbor classification [3, 8, 20] or how to boost classification performance by applying different transformations to the time series before using nearest-neighbor classification [2]. These existing results are mostly experimental, lacking theoretical justification for both when nearest-neighbor-like time series classifiers should be expected to perform well and how well.\nIf we don\u2019t confine ourselves to classifying time series, then as the amount of data tends to infinity, nearest-neighbor classification has been shown to achieve a probability of error that is at worst twice the Bayes error rate, and when considering the nearest k neighbors with k allowed to grow with the amount of data, then the error rate approaches the Bayes error rate [6]. However, rather than examining the asymptotic case where the amount of data goes to infinity, we instead pursue nonasymptotic performance guarantees in terms of how large of a training dataset we have and how much we observe of the time series to be classified. To arrive at these nonasymptotic guarantees, we impose a low-complexity structure on time series.\nOur contributions. We present a model for which nearest-neighbor-like classification performs well by operationalizing the following hypothesis: In many time series applications, there are only a small number of prototypical time series relative to the number of time series we can collect. For example, posts on Twitter are generated by humans, who are often behaviorally predictable in aggregate. This suggests that topics they post about only become trends on Twitter in a few distinct manners, yet we have at our disposal enormous volumes of Twitter data. In this context, we present a novel latent source model: time series are generated from a small collection of m unknown latent sources, each having one of two labels, say \u201ctrend\u201d or \u201cnot trend\u201d. Our model\u2019s maximum a posteriori (MAP) time series classifier can be approximated by weighted majority voting, which compares the time series to be classified with each of the time series in the labeled training data. Each training time series casts a weighted vote in favor of its ground truth label, with the weight depending on how similar the time series being classified is to the training example. The final classification is \u201ctrend\u201d or \u201cnot trend\u201d depending on which label has the higher overall vote. The voting is nonparametric in that it does not learn parameters for a model and is driven entirely by the training data. The unknown latent sources are never estimated; the training data serve as a proxy for these latent sources. Weighted majority voting itself can be approximated by a nearest-neighbor classifier, which we also analyze.\nUnder our model, we show sufficient conditions so that if we have n = \u0398(m log m\u03b4 ) time series in our training data, then weighted majority voting and nearest-neighbor classification correctly classify a new time series with probability at least 1\u2212 \u03b4 after observing its first \u2126(log m\u03b4 ) time steps. As our analysis accounts for how much of the time series we observe, our results readily apply to the \u201conline\u201d setting in which a time series is to be classified while it streams in (as is the case for forecasting trending topics) as well as the \u201coffline\u201d setting where we have access to the entire time series. Also, while our analysis yields matching error upper bounds for the two classifiers, experimental results on synthetic data suggests that weighted majority voting outperforms nearest-neighbor classification early on when we observe very little of the time series to be classified. Meanwhile, a specific instantiation of our model leads to a spherical Gaussian mixture model, where the latent sources are Gaussian mixture components. We show that existing performance guarantees on learning spherical Gaussian mixture models [7, 11, 19] require more stringent conditions than what our results need, suggesting that learning the latent sources is overkill if the goal is classification.\nLastly, we apply weighted majority voting to forecasting trending topics on Twitter. We emphasize that our goal is precognition of trends: predicting whether a topic is going to be a trend before it is actually declared to be a trend by Twitter or, in theory, any other third party that we can collect ground truth labels from. Existing work that identify trends on Twitter [4, 5, 15] instead, as part of their trend detection, define models for what trends are, which we do not do, nor do we assume we have access to such definitions. (The same could be said of previous work on novel document detection on Twitter [12, 13].) In our experiments, weighted majority voting is able to predict whether a topic will be a trend in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%. We empirically find that the Twitter activity of a news topic that becomes a trend tends to follow one of a finite number of patterns, which could be thought of as latent sources.\nOutline. Weighted majority voting and nearest-neighbor classification for time series are presented in Section 2. We provide our latent source model and theoretical performance guarantees of weighted majority voting and nearest-neighbor classification under this model in Section 3. Experimental results for synthetic data and forecasting trending topics on Twitter are in Section 4."}, {"heading": "2 Weighted Majority Voting and Nearest-Neighbor Classification", "text": "Given a time-series2 s : Z \u2192 R, we want to classify it as having either label +1 (\u201ctrend\u201d) or \u22121 (\u201cnot trend\u201d). To do so, we have access to labeled training data R+ and R\u2212, which denote the sets of all training time series with labels +1 and \u22121 respectively. Weighted majority voting. Each positively-labeled example r \u2208 R+ casts a weighted vote e\u2212\u03b3d\n(T )(r,s) for whether time series s has label +1, where d(T )(r, s) is some measure of similarity between the two time series r and s, superscript (T ) indicates that we are only allowed to look at the first T time steps (i.e., time steps 1, 2, . . . , T ) of s (but we\u2019re allowed to look outside of these time steps for the training time series r), and constant \u03b3 \u2265 0 is a scaling parameter that determines the \u201csphere of influence\u201d of each example. Similarly, each negatively-labeled example in R\u2212 also casts a weighted vote for whether time series s has label \u22121. The similarity measure d(T )(r, s) could, for example, be squared Euclidean distance: d(T )(r, s) =\u2211T t=1(r(t) \u2212 s(t))2 , \u2016r \u2212 s\u20162T . However, this similarity measure only looks at the first T time steps of training time series r. Since time series in our training data are known, we need not restrict our attention to their first T time steps. Thus, we use the following similarity measure:\nd(T )(r, s) = min \u2206\u2208{\u2212\u2206max,...,0,...,\u2206max} T\u2211 t=1 (r(t+\u2206)\u2212s(t))2 = min \u2206\u2208{\u2212\u2206max,...,0,...,\u2206max} \u2016r\u2217\u2206\u2212s\u20162T , (1) where we minimize over integer time shifts with a pre-specified maximum allowed shift \u2206max \u2265 0. Here, we have used q\u2217\u2206 to denote time series q advanced by \u2206 time steps, i.e., (q\u2217\u2206)(t) = q(t+\u2206). Finally, we sum up all of the weighted +1 votes and then all of the weighted \u22121 votes. The label with the majority of overall weighted votes is declared as the label for s:\nL\u0302(T )(s; \u03b3) =\n{ +1 if \u2211 r\u2208R+ e \u2212\u03b3d(T )(r,s) \u2265 \u2211 r\u2208R\u2212 e \u2212\u03b3d(T )(r,s),\n\u22121 otherwise. (2)\nUsing a larger time window size T corresponds to waiting longer before we make a prediction. We need to trade off how long we wait and how accurate we want our prediction. Note that knearest-neighbor classification corresponds to only considering the k nearest neighbors of s among all training time series; all other votes are set to 0. With k = 1, we obtain the following classifier:\nNearest-neighbor classifier. Let r\u0302 = arg minr\u2208R+\u222aR\u2212 d(T )(r, s) be the nearest neighbor of s. Then we declare the label for s to be:\nL\u0302 (T ) NN (s) = { +1 if r\u0302 \u2208 R+, \u22121 if r\u0302 \u2208 R\u2212.\n(3)"}, {"heading": "3 A Latent Source Model and Theoretical Guarantees", "text": "We assume there to be m unknown latent sources (time series) that generate observed time series. Let V denote the set of all such latent sources; each latent source v : Z \u2192 R in V has a true label +1 or \u22121. Let V+ \u2282 V be the set of latent sources with label +1, and V\u2212 \u2282 V be the set of those with label \u22121. The observed time series are generated from latent sources as follows:\n1. Sample latent source V from V uniformly at random.3 Let L \u2208 {\u00b11} be the label of V . 2We index time using Z for notationally convenience but will assume time series to start at time step 1. 3While we keep the sampling uniform for clarity of presentation, our theoretical guarantees can easily be extended to the case where the sampling is not uniform. The only change is that the number of training data needed will be larger by a factor of 1\nm\u03c0min , where \u03c0min is the smallest probability of a particular latent source\noccurring.\n2. Sample integer time shift \u2206 uniformly from {0, 1, . . . ,\u2206max}. 3. Output time series S : Z \u2192 R to be latent source V advanced by \u2206 time steps, followed\nby adding noise signal E : Z \u2192 R, i.e., S(t) = V (t + \u2206) + E(t). The label associated with the generated time series S is the same as that of V , i.e., L. Entries of noise E are i.i.d. zero-mean sub-Gaussian with parameter \u03c3, which means that for any time index t,\nE[exp(\u03bbE(t))] \u2264 exp (1\n2 \u03bb2\u03c32\n) for all \u03bb \u2208 R. (4)\nThe family of sub-Gaussian distributions includes a variety of distributions, such as a zeromean Gaussian with standard deviation \u03c3 and a uniform distribution over [\u2212\u03c3, \u03c3].\nThe above generative process defines our latent source model. Importantly, we make no assumptions about the structure of the latent sources. For instance, the latent sources could be tiled as shown in Figure 1, where they are evenly separated vertically and alternate between the two different classes +1 and \u22121. With a parametric model like a k-component Gaussian mixture model, estimating these latent sources could be problematic. For example, if we take any two adjacent latent sources with label +1 and cluster them, then this cluster could be confused with the latent source having label \u22121 that is sandwiched in between. Noise only complicates estimating the latent sources. In this example, the k-component Gaussian mixture model needed for label +1 would require k to be the exact number of latent sources with label +1, which is unknown. In general, the number of samples we need from a Gaussian mixture mixture model to estimate the mixture component means is exponential in the number of mixture components [16]. As we discuss next, for classification, we sidestep learning the latent sources altogether, instead using training data as a proxy for latent sources. At the end of this section, we compare our sample complexity for classification versus some existing sample complexities for learning Gaussian mixture models.\nClassification. If we knew the latent sources and if noise entries E(t) were i.i.d.N (0, 12\u03b3 ) across t, then the maximum a posteriori (MAP) estimate for label L given an observed time series S = s is\nL\u0302 (T ) MAP(s; \u03b3) = { +1 if \u039b(T )MAP(s; \u03b3) \u2265 1, \u22121 otherwise,\n(5)\nwhere\n\u039b (T ) MAP(s; \u03b3) ,\n\u2211 v+\u2208V+ \u2211 \u2206+\u2208D+ exp ( \u2212 \u03b3\u2016v+ \u2217\u2206+ \u2212 s\u20162T )\u2211 v\u2212\u2208V\u2212 \u2211 \u2206\u2212\u2208D+ exp ( \u2212 \u03b3\u2016v\u2212 \u2217\u2206\u2212 \u2212 s\u20162T\n) , (6) and D+ , {0, . . . ,\u2206max}. However, we do not know the latent sources, nor do we know if the noise is i.i.d. Gaussian. We assume that we have access to training data as given in Section 2. We make a further assumption that the training data were sampled from the latent source model and that we have n different training time series. Denote D , {\u2212\u2206max, . . . , 0, . . . ,\u2206max}. Then we approximate the MAP classifier by using training data as a proxy for the latent sources. Specifically, we take ratio (6), replace the inner sum by a minimum in the exponent, replace V+ and V\u2212 by R+ and R\u2212, and replace D+ by D to obtain the ratio:\n\u039b(T )(s; \u03b3) ,\n\u2211 r+\u2208R+ exp ( \u2212 \u03b3 ( min\u2206+\u2208D \u2016r+ \u2217\u2206+ \u2212 s\u20162T ))\u2211\nr\u2212\u2208R\u2212 exp ( \u2212 \u03b3 ( min\u2206\u2212\u2208D \u2016r\u2212 \u2217\u2206\u2212 \u2212 s\u20162T )) . (7)\nPlugging \u039b(T ) in place of \u039b(T )MAP in classification rule (5) yields the weighted majority voting rule (2). Note that weighted majority voting could be interpreted as a smoothed nearest-neighbor approximation whereby we only consider the time-shifted version of each example time series that is closest to the observed time series s. If we didn\u2019t replace the summations over time shifts with minimums in the exponent, then we have a kernel density estimate in the numerator and in the denominator [10, Chapter 7] (where the kernel is Gaussian) and our main theoretical result for weighted majority voting to follow would still hold using the same proof.4\nLastly, applications may call for trading off true and false positive rates. We can do this by generalizing decision rule (5) to declare the label of s to be +1 if \u039b(T )(s, \u03b3) \u2265 \u03b8 and vary parameter \u03b8 > 0. The resulting decision rule, which we refer to as generalized weighted majority voting, is thus:\nL\u0302 (T ) \u03b8 (s; \u03b3) = { +1 if \u039b(T )(s, \u03b3) \u2265 \u03b8, \u22121 otherwise, (8)\nwhere setting \u03b8 = 1 recovers the usual weighted majority voting (2). This modification to the classifier can be thought of as adjusting the priors on the relative sizes of the two classes. Our theoretical results to follow actually cover this more general case rather than only that of \u03b8 = 1.\nTheoretical guarantees. We now present the main theoretical results of this paper which identify sufficient conditions under which generalized weighted majority voting (8) and nearest-neighbor classification (3) can classify a time series correctly with high probability, accounting for the size of the training dataset and how much we observe of the time series to be classified. First, we define the \u201cgap\u201d betweenR+ andR\u2212 restricted to time length T and with maximum time shift \u2206max as:\nG(T )(R+,R\u2212,\u2206max) , min r+\u2208R+,r\u2212\u2208R\u2212,\n\u2206+,\u2206\u2212\u2208D\n\u2016r+ \u2217\u2206+ \u2212 r\u2212 \u2217\u2206\u2212\u20162T . (9)\nThis quantity measures how far apart the two different classes are if we only look at length-T chunks of each time series and allow all shifts of at most \u2206max time steps in either direction.\nOur first main result is stated below. We defer proofs for this section to Appendices A and B. Theorem 1. (Performance guarantee for generalized weighted majority voting) Let m+ = |V+| be the number of latent sources with label +1, and m\u2212 = |V\u2212| = m \u2212m+ be the number of latent sources with label\u22121. For any \u03b2 > 1, under the latent source model with n > \u03b2m logm time series in the training data, the probability of misclassifying time series S with label L using generalized weighted majority voting L\u0302(T )\u03b8 (\u00b7; \u03b3) satisfies the bound\nP(L\u0302(T )\u03b8 (S; \u03b3) 6= L) \u2264 (\u03b8m+ m + m\u2212 \u03b8m ) (2\u2206max + 1)n exp ( \u2212 (\u03b3 \u2212 4\u03c32\u03b32)G(T )(R+,R\u2212,\u2206max) ) +m\u2212\u03b2+1. (10)\nAn immediate consequence is that given error tolerance \u03b4 \u2208 (0, 1) and with choice \u03b3 \u2208 (0, 14\u03c32 ), then upper bound (10) is at most \u03b4 (by having each of the two terms on the right-hand side be \u2264 \u03b42 ) if n > m log 2m\u03b4 (i.e., \u03b2 = 1 + log 2 \u03b4 / logm), and\nG(T )(R+,R\u2212,\u2206max) \u2265 log( \u03b8m+m + m\u2212 \u03b8m ) + log(2\u2206max + 1) + log n+ log 2 \u03b4\n\u03b3 \u2212 4\u03c32\u03b32 . (11)\nThis means that if we have access to a large enough pool of labeled time series, i.e., the pool has \u2126(m log m\u03b4 ) time series, then we can subsample n = \u0398(m log m \u03b4 ) of them to use as training data. Then with choice \u03b3 = 18\u03c32 , generalized weighted majority voting (8) correctly classifies a new time series S with probability at least 1\u2212 \u03b4 if\nG(T )(R+,R\u2212,\u2206max) = \u2126 ( \u03c32 ( log (\u03b8m+ m + m\u2212 \u03b8m ) + log(2\u2206max + 1) + log m \u03b4 )) . (12)\nThus, the gap between setsR+ andR\u2212 needs to grow logarithmic in the number of latent sourcesm in order for weighted majority voting to classify correctly with high probability. Assuming that the\n4We use a minimum rather a summation over time shifts to make the method more similar to existing time series classification work (e.g., [22]), which minimize over time warpings rather than simple shifts.\noriginal unknown latent sources are separated (otherwise, there is no hope to distinguish between the classes using any classifier) and the gap in the training data grows as G(T )(R+,R\u2212,\u2206max) = \u2126(\u03c32T ) (otherwise, the closest two training time series from opposite classes are within noise of each other), then observing the first T = \u2126(log(\u03b8+ 1\u03b8 ) + log(2\u2206max + 1) + log m \u03b4 ) time steps from the time series is sufficient to classify it correctly with probability at least 1\u2212 \u03b4. A similar result holds for the nearest-neighbor classifier (3). Theorem 2. (Performance guarantee for nearest-neighbor classification) For any \u03b2 > 1, under the latent source model with n > \u03b2m logm time series in the training data, the probability of misclassifying time series S with label L using the nearest-neighbor classifier L\u0302(T )NN (\u00b7) satisfies the bound\nP(L\u0302(T )NN (S) 6= L) \u2264 (2\u2206max + 1)n exp ( \u2212 1\n16\u03c32 G(T )(R+,R\u2212,\u2206max)\n) +m\u2212\u03b2+1. (13)\nOur generalized weighted majority voting bound (10) with \u03b8 = 1 (corresponding to regular weighted majority voting) and \u03b3 = 18\u03c32 matches our nearest-neighbor classification bound, suggesting that the two methods have similar behavior when the gap grows with T . In practice, we find weighted majority voting to outperform nearest-neighbor classification when T is small, and then as T grows large, the two methods exhibit similar performance in agreement with our theoretical analysis. For small T , it could still be fairly likely that the nearest neighbor found has the wrong label, dooming the nearest-neighbor classifier to failure. Weighted majority voting, on the other hand, can recover from this situation as there may be enough correctly labeled training time series close by that contribute to a higher overall vote for the correct class. This robustness of weighted majority voting makes it favorable in the online setting where we want to make a prediction as early as possible.\nSample complexity of learning the latent sources. If we can estimate the latent sources accurately, then we could plug these estimates in place of the true latent sources in the MAP classifier and achieve classification performance close to optimal. If we restrict the noise to be Gaussian and assume \u2206max = 0, then the latent source model corresponds to a spherical Gaussian mixture model. We could learn such a model using Dasgupta and Schulman\u2019s modified EM algorithm [7]. Their theoretical guarantee depends on the true separation between the closest two latent sources, namely G(T )\u2217 , minv,v\u2032\u2208V s.t. v 6=v\u2032 \u2016v \u2212 v\u2032\u201622, which needs to satisfy G(T )\u2217 \u03c32 \u221a T . Then with number of training time series n = \u2126(max{1, \u03c3 2T\nG(T )\u2217 }m log m\u03b4 ), gap G (T )\u2217 = \u2126(\u03c32 log m\u03b5 ), and number of initial time steps observed\nT = \u2126 ( max { 1, \u03c34T 2\n(G(T )\u2217)2\n} log [ m\n\u03b4 max\n{ 1, \u03c34T 2\n(G(T )\u2217)2\n}]) , (14)\ntheir algorithm achieves, with probability at least 1 \u2212 \u03b4, an additive \u03b5\u03c3 \u221a T error (in Euclidean distance) close to optimal in estimating every latent source. In contrast, our result is in terms of gap G(T )(R+,R\u2212,\u2206max) that depends not on the true separation between two latent sources but instead on the minimum observed separation in the training data between two time series of opposite labels. In fact, our gap, in their setting, grows as \u2126(\u03c32T ) even when their gap G(T )\u2217 grows sublinear in T . In particular, while their result cannot handle the regime where O(\u03c32 log m\u03b4 ) \u2264 G (T )\u2217 \u2264 \u03c32 \u221a T , ours can, using n = \u0398(m log m\u03b4 ) training time series and observing the first T = \u2126(log m \u03b4 ) time steps to classify a time series correctly with probability at least 1\u2212 \u03b4; see Appendix D for details. Vempala and Wang [19] have a spectral method for learning Gaussian mixture models that can handle smallerG(T )\u2217 than Dasgupta and Schulman\u2019s approach but requires n = \u2126\u0303(T 3m2) training data, where we\u2019ve hidden the dependence on \u03c32 and other variables of interest for clarity of presentation. Hsu and Kakade [11] have a moment-based estimator that doesn\u2019t have a gap condition but, under a different non-degeneracy condition, requires substantially more samples for our problem setup, i.e., n = \u2126((m14 +Tm11)/\u03b52) to achieve an \u03b5 approximation of the mixture components. These results need substantially more training data than what we\u2019ve shown is sufficient for classification.\nTo fit a Gaussian mixture model to massive training datasets, in practice, using all the training data could be prohibitively expensive. In such scenarios, one could instead non-uniformly subsample O(Tm3/\u03b52) time series from the training data using the procedure given in [9] and then feed the resulting smaller dataset, referred to as an (m, \u03b5)-coreset, to the EM algorithm for learning the latent sources. This procedure still requires more training time series than needed for classification and lacks a guarantee that the estimated latent sources will be close to the true latent sources."}, {"heading": "4 Experimental Results", "text": "Synthetic data. We generate m = 200 latent sources, where each latent source is constructed by first sampling i.i.d. N (0, 100) entries per time step and then applying a 1D Gaussian smoothing filter with scale parameter 30. Half of the latent sources are labeled +1 and the other half \u22121. Then n = \u03b2m logm training time series are sampled as per the latent source model where the noise added is i.i.d. N (0, 1) and \u2206max = 100. We similarly generate 1000 time series to use as test data. We set \u03b3 = 1/8 for weighted majority voting. For \u03b2 = 8, we compare the classification error rates on test data for weighted majority voting, nearest-neighbor classification, and the MAP classifier with oracle access to the true latent sources as shown in Figure 2(a). We see that weighted majority voting outperforms nearest-neighbor classification but as T grows large, the two methods\u2019 performances converge to that of the MAP classifier. Fixing T = 100, we then compare the classification error rates of the three methods using varying amounts of training data, as shown in Figure 2(b); the oracle MAP classifier is also shown but does not actually depend on training data. We see that as \u03b2 increases, both weighted majority voting and nearest-neighbor classification steadily improve in performance.\nForecasting trending topics on twitter. We provide only an overview of our Twitter results here, deferring full details to Appendix E. We sampled 500 examples of trends at random from a list of June 2012 news trends, and 500 examples of non-trends based on phrases appearing in user posts during the same month. As we do not know how Twitter chooses what phrases are considered as candidate phrases for trending topics, it\u2019s unclear what the size of the non-trend category is in\n\u221210 \u22128 \u22126 \u22124 \u22122 0 2 4 6 8\n0\n10000\n20000\n30000\n40000\n50000\n60000\nco u n t\nP(early) =0.79\nP(late) =0.21\u2329 early \u232a =2.90 hrs.\u2329\nlate \u232a =1.08 hrs.\nearly late\n\u221210 \u22128 \u22126 \u22124 \u22122 0 2 4 6 8 0\n10000\n20000\n30000\n40000\n50000\nco u n t\nP(early) =0.40\nP(late) =0.60\u2329 early \u232a =1.29 hrs.\u2329\nlate \u232a =1.65 hrs.\ncenter\n\u221210 \u22128 \u22126 \u22124 \u22122 0 2 4 6 8 hours late\n0\n1000\n2000\n3000\n4000\n5000\n6000\nco u n t\nP(early) =0.13\nP(late) =0.87\u2329 early \u232a =1.71 hrs.\u2329\nlate \u232a =2.91 hrs.\nbottom\n0.8\n1.0\ntop\ncenter\nEarly detection vs. position on ROC curve\ncomparison to the size of the trend category. Thus, for simplicity, we intentionally control for the class sizes by setting them equal. In practice, one could still expressly assemble the training data to have pre-specified class sizes and then tune \u03b8 for generalized weighted majority voting (8). In our experiments, we use the usual weighted majority voting (2) (i.e., \u03b8 = 1) to classify time series, where \u2206max is set to the maximum possible (we consider all shifts).\nPer topic, we created its time series based on a pre-processed version of the raw rate of how often the topic was shared, i.e., its Tweet rate. We empirically found that how news topics become trends tends to follow a finite number of patterns; a few examples of these patterns are shown in Figure 3. We randomly divided the set of trends and non-trends into into two halves, one to use as training data and one to use as test data. We applied weighted majority voting, sweeping over \u03b3, T , and data pre-processing parameters. As shown in Figure 4(a), one choice of parameters allows us to detect trending topics in advance of Twitter 79% of the time, and when we do, we detect them an average of 1.43 hours earlier. Furthermore, we achieve a true positive rate (TPR) of 95% and a false positive rate (FPR) of 4%. Naturally, there are tradeoffs between TPR, FPR, and how early we make a prediction (i.e., how small T is). As shown in Figure 4(c), an \u201caggressive\u201d parameter setting yields early detection and high TPR but high FPR, and a \u201cconservative\u201d parameter setting yields low FPR but late detection and low TPR. An \u201cin-between\u201d setting can strike the right balance.\nAcknowledgements. This work was supported in part by the Army Research Office under MURI Award 58153-MA-MUR. GHC was supported by an NDSEG fellowship."}, {"heading": "A Proof of Theorem 1", "text": "Let S be the time series with an unknown label that we wish to classify using training data. Denote m+ , |V+|, m\u2212 , |V\u2212| = m \u2212m+, n+ , |R+|, n\u2212 , |R\u2212|, and R , R+ \u222a R\u2212. Recall that D+ , {0, 1, . . . ,\u2206max}, and D , {\u2212\u2206max, . . . ,\u22121, 0, 1, . . . ,\u2206max}. As per the model, there exists a latent source V , shift \u2206\u2032 \u2208 D+, and noise signal E\u2032 such that\nS = V \u2217\u2206\u2032 + E\u2032. (15)\nApplying a standard coupon collector\u2019s problem result, with a training set of size n > \u03b2m logm, then with probability at least 1\u2212m\u2212\u03b2+1, for each latent source V \u2208 V , there exists at least one time series R in the set R of all training data that is generated from V . Henceforth, we assume that this event holds. In Appendix C, we elaborate on what happens if the latent sources are not uniformly sampled.\nNote that R is generated from V as\nR = V \u2217\u2206\u2032\u2032 + E\u2032\u2032, (16)\nwhere \u2206\u2032\u2032 \u2208 D+ and E\u2032\u2032 is a noise signal independent of E\u2032. Therefore, we can rewrite S in terms of R as follows:\nS = R \u2217\u2206 + E, (17)\nwhere \u2206 = \u2206\u2032 \u2212\u2206\u2032\u2032 \u2208 D (note the change from D+ to D) and E = E\u2032 \u2212 E\u2032\u2032 \u2217\u2206. Since E\u2032 and E\u2032\u2032 are i.i.d. over time and sub-Gaussian with parameter \u03c3, one can easily verify that E is i.i.d. over time and sub-Gaussian with parameter \u221a 2\u03c3.\nWe now bound the probability of error of classifier L\u0302(T )\u03b8 (\u00b7; \u03b3). The probability of error or misclassification using the first T time steps of S is given by\nP ( misclassify S using its first T time steps ) = P(L\u0302(T )\u03b8 (S; \u03b3) = \u22121|L = +1)P(L = +1)\ufe38 \ufe37\ufe37 \ufe38\nm+/m +P(L\u0302(T )\u03b8 (S; \u03b3) = +1|L = \u22121)P(L = \u22121)\ufe38 \ufe37\ufe37 \ufe38 m\u2212/m . (18)\nIn the remainder of the proof, we primarily show how to bound P(L\u0302(T )\u03b8 (S; \u03b3) = \u22121|L = +1). The bound for P(L\u0302(T )\u03b8 (S; \u03b3) = +1|L = \u22121) is almost identical. By Markov\u2019s inequality,\nP(L\u0302(T )\u03b8 (S; \u03b3) = \u22121|L = +1) = P (\n1 \u039b(T )(S; \u03b3) \u2265 1 \u03b8 \u2223\u2223\u2223L = +1) \u2264 \u03b8E[ 1 \u039b(T )(S; \u03b3) \u2223\u2223\u2223L = +1]. (19)\nNow,\nE [\n1\n\u039b(T )(S; \u03b3) \u2223\u2223\u2223L = +1] \u2264 max r+\u2208R+,\u2206+\u2208D EE [\n1\n\u039b(T )(r+ \u2217\u2206+ + E; \u03b3)\n] . (20)\nWith the above inequality in mind, we next bound 1/\u039b(T )(r\u0303+ \u2217 \u2206\u0303+ + E; \u03b3) for any choice of r\u0303+ \u2208 R+ and \u2206\u0303+ \u2208 D. Note that for any time series s,\n1\n\u039b(T )(s; \u03b3) \u2264\n\u2211 r\u2212\u2208R\u2212, \u2206\u2212\u2208D exp ( \u2212 \u03b3\u2016r\u2212 \u2217\u2206\u2212 \u2212 s\u20162T ) exp ( \u2212 \u03b3\u2016r\u0303+ \u2217 \u2206\u0303+ \u2212 s\u20162T\n) . (21) After evaluating the above for s = r\u0303+ \u2217 \u2206\u0303+ + E, a bit of algebra shows that\n1\n\u039b(T )(r\u0303+ \u2217 \u2206\u0303+ + E; \u03b3) \u2264 \u2211 r\u2212\u2208R\u2212, \u2206\u2212\u2208D { exp ( \u2212 \u03b3\u2016r\u0303+ \u2217 \u2206\u0303+ \u2212 r\u2212 \u2217\u2206\u2212\u20162T ) exp ( \u2212 2\u03b3\u3008r\u0303+ \u2217 \u2206\u0303+ \u2212 r\u2212 \u2217\u2206\u2212, E\u3009T )} , (22)\nwhere \u3008q, q\u2032\u3009T , \u2211T t=1 q(t)q \u2032(t) for time series q and q\u2032.\nTaking the expectation of (22) with respect to noise signal E, we obtain the following bound: EE [ 1\n\u039b(T )(r\u0303+ \u2217 \u2206\u0303+ + E; \u03b3) ] \u2264 EE\n[ \u2211 r\u2212\u2208R\u2212, \u2206\u2212\u2208D { exp ( \u2212 \u03b3\u2016r\u0303+ \u2217 \u2206\u0303+ \u2212 r\u2212 \u2217\u2206\u2212\u20162T ) exp ( \u2212 2\u03b3\u3008r\u0303+ \u2217 \u2206\u0303+ \u2212 r\u2212 \u2217\u2206\u2212, E\u3009T )}]\n(i) = \u2211 r\u2212\u2208R\u2212, \u2206\u2212\u2208D exp ( \u2212 \u03b3\u2016r\u0303+ \u2217 \u2206\u0303+ \u2212 r\u2212 \u2217\u2206\u2212\u20162T ) T\u220f t=1 EE(t)[exp ( \u2212 2\u03b3(r\u0303+(t+ \u2206\u0303+)\u2212 r\u2212(t+ \u2206\u2212))E(t) ) ]\n(ii) \u2264 \u2211\nr\u2212\u2208R\u2212, \u2206\u2212\u2208D\nexp ( \u2212 \u03b3\u2016r\u0303+ \u2217 \u2206\u0303+ \u2212 r\u2212 \u2217\u2206\u2212\u20162T ) T\u220f t=1 exp ( 4\u03c32\u03b32(r\u0303+(t+ \u2206\u0303+)\u2212 r\u2212(t+ \u2206\u2212))2 ) =\n\u2211 r\u2212\u2208R\u2212, \u2206\u2212\u2208D exp ( \u2212 (\u03b3 \u2212 4\u03c32\u03b32)\u2016r+ \u2217\u2206+ \u2212 r\u2212 \u2217\u2206\u2212\u20162T ) \u2264 (2\u2206max + 1)n\u2212 exp ( \u2212 (\u03b3 \u2212 4\u03c32\u03b32)G(T ) ) , (23)\nwhere step (i) uses independence of entries of E, step (ii) uses the fact that E(t) is zero-mean subGaussian with parameter \u221a 2\u03c3, and the last line abbreviates the gap G(T ) \u2261 G(T )(R+,R\u2212,\u2206max).\nStringing together inequalities (19), (20), and (23), we obtain P(L\u0302(T )\u03b8 (S; \u03b3) = \u22121|L = +1) \u2264 \u03b8(2\u2206max + 1)n\u2212 exp ( \u2212 (\u03b3 \u2212 4\u03c32\u03b32)G(T ) ) . (24)\nRepeating a similar argument yields\nP(L\u0302(T )\u03b8 (S; \u03b3) = +1|L = \u22121) \u2264 1\n\u03b8 (2\u2206max + 1)n+ exp\n( \u2212 (\u03b3 \u2212 4\u03c32\u03b32)G(T ) ) . (25)\nFinally, plugging (24) and (25) into (18) gives\nP(L\u0302(T )\u03b8 (S; \u03b3) 6= L) \u2264 \u03b8(2\u2206max + 1) n\u2212m+ m\nexp ( \u2212 (\u03b3 \u2212 4\u03c32\u03b32)G(T ) ) + 1\n\u03b8 (2\u2206max + 1) n+m\u2212 m\n(2\u2206max + 1)n+ exp ( \u2212 (\u03b3 \u2212 4\u03c32\u03b32)G(T ) ) = (\u03b8m+ m + m\u2212 \u03b8m ) (2\u2206max + 1)n exp ( \u2212 (\u03b3 \u2212 4\u03c32\u03b32)G(T ) ) . (26)\nThis completes the proof of Theorem 1."}, {"heading": "B Proof of Theorem 2", "text": "The proof uses similar steps as the weighted majority voting case. As before, we consider the case when our training data sees each latent source at least once (this event happens with probability at least 1\u2212m\u2212\u03b2+1). We decompose the probability of error into terms depending on which latent source V generated S:\nP(L\u0302(T )NN (S) 6= L) = \u2211 v\u2208V P(V = v)P(L\u0302(T )NN (S) 6= L|V = v) = \u2211 v\u2208V 1 m P(L\u0302(T )NN (S) 6= L|V = v).\n(27) Next, we bound each P(L\u0302(T )NN (S) 6= L|V = v) term. Suppose that v \u2208 V+, i.e., v has label L = +1; the case when v \u2208 V\u2212 is similar. Then we make an error and declare L\u0302(T )NN (S) = \u22121 when the nearest neighbor r\u0302 to time series S is in the setR\u2212, where\n(r\u0302, \u2206\u0302) = arg min (r,\u2206)\u2208R\u00d7D\n\u2016r \u2217\u2206\u2212 S\u20162T . (28)\nBy our assumption that every latent source is seen in the training data, there exists r\u2217 \u2208 R+ generated by latent source v, and so S = r\u2217 \u2217\u2206\u2217 + E (29) for some shift \u2206\u2217 \u2208 D and noise signalE consisting of i.i.d. entries that are zero-mean sub-Gaussian with parameter \u221a 2\u03c3.\nBy optimality of (r\u0302, \u2206\u0302) for optimization problem (28), we have\n\u2016r \u2217\u2206\u2212 (r\u2217 \u2217\u2206\u2217 + E)\u20162T \u2265 \u2016r\u0302 \u2217 \u2206\u0302\u2212 (r\u2217 \u2217\u2206\u2217 + E)\u20162T for all r \u2208 R,\u2206 \u2208 D. (30)\nPlugging in r = r\u2217 and \u2206 = \u2206\u2217, we obtain\n\u2016E\u20162T \u2265 \u2016r\u0302 \u2217 \u2206\u0302\u2212 (r\u2217 \u2217\u2206\u2217 + E)\u20162T = \u2016(r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217)\u2212 E\u20162T = \u2016r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217\u20162T \u2212 2\u3008r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217, E\u3009T + \u2016E\u20162T , (31)\nor, equivalently, 2\u3008r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217, E\u3009T \u2265 \u2016r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217\u20162T . (32)\nThus, given V = v \u2208 V+, declaring L\u0302(T )NN (S) = \u22121 implies the existence of r\u0302 \u2208 R\u2212 and \u2206\u0302 \u2208 D such that optimality condition (32) holds. Therefore,\nP(L\u0302(T )NN (S) = \u22121|V = v) \u2264 P ( \u22c3 r\u0302\u2208R\u2212,\u2206\u0302\u2208D {2\u3008r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217, E\u3009T \u2265 \u2016r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217\u20162T } )\n(i) \u2264 (2\u2206max + 1)n\u2212P(2\u3008r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217, E\u3009T \u2265 \u2016r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217\u20162T ) \u2264 (2\u2206max + 1)n\u2212P(exp(2\u03bb\u3008r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217, E\u3009T ) \u2265 exp(\u03bb\u2016r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217\u20162T )) (ii) \u2264 (2\u2206max + 1)n\u2212 exp(\u2212\u03bb\u2016r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217\u20162T )E[exp(2\u03bb\u3008r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217, E\u3009T )] (iii)\n\u2264 (2\u2206max + 1)n\u2212 exp(\u2212\u03bb\u2016r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217\u20162T ) exp(4\u03bb2\u03c32\u2016r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217\u20162T ) = (2\u2206max + 1)n\u2212 exp(\u2212(\u03bb\u2212 4\u03bb2\u03c32)\u2016r\u0302 \u2217 \u2206\u0302\u2212 r\u2217 \u2217\u2206\u2217\u20162T ) \u2264 (2\u2206max + 1)n exp(\u2212(\u03bb\u2212 4\u03bb2\u03c32)G(T ))\n(iv) \u2264 (2\u2206max + 1)n exp ( \u2212 1\n16\u03c32 G(T )\n) , (33)\nwhere step (i) is by a union bound, step (ii) is by Markov\u2019s inequality, step (iii) is by subGaussianity, and step (iv) is by choosing \u03bb = 18\u03c32 .\nAs bound (33) also holds for P(L\u0302(T )NN (S) = +1|V = v) when instead v \u2208 V\u2212, we can now piece together (27) and (33) to yield the final result:\nP(L\u0302(T )NN (S) 6= L) = \u2211 v\u2208V 1 m P(L\u0302(T )NN (S) 6= L|V = v) \u2264 (2\u2206max + 1)n exp ( \u2212 1 16\u03c32 G(T ) ) . (34)"}, {"heading": "C Handling Non-uniformly Sampled Latent Sources", "text": "When each time series generated from the latent source model is sampled uniformly at random, then having n > m log 2m\u03b4 (i.e., \u03b2 = 1 + log 2 \u03b4 / logm) ensures that with probability at least 1 \u2212 \u03b4 2 , our training data sees every latent source at least once. When the latent sources aren\u2019t sampled uniformly at random, we show that we can simply replace the condition n > m log 2m\u03b4 with n \u2265 8 \u03c0min\nlog 2m\u03b4 to achieve a similar (in fact, stronger) guarantee, where \u03c0min is the smallest probability of a particular latent source occurring.\nLemma 3. Suppose that the i-th latent source occurs with probability \u03c0i in the latent source model. Denote \u03c0min , mini\u2208{1,2,...,m} \u03c0i. Let \u03bei be the number of times that the i-th latent source appears in the training data. If n \u2265 8\u03c0min log 2m \u03b4 , then with probability at least 1 \u2212 \u03b4 2 , every latent source appears strictly greater than 12n\u03c0min times in the training data.\nProof. Note that \u03bei \u223c Bin(n, \u03c0i). We have\nP ( \u03bei \u2264 1\n2 n\u03c0min\n) \u2264 P ( \u03bei \u2264 1 2 n\u03c0i )\n(i) \u2264 exp ( \u2212 1\n2 \u00b7\n(n\u03c0i \u2212 12n\u03c0i) 2\nn \u00b7 \u03c0i ) = exp ( \u2212 n\u03c0i\n8 ) \u2264 exp ( \u2212 n\u03c0min\n8\n) . (35)\nwhere step (i) uses a standard binomial distribution lower tail bound. Applying a union bound, P ( \u22c3 i\u2208{1,2,...,m} { \u03bei \u2264 1 2 n\u03c0min }) \u2264 m exp ( \u2212 n\u03c0min 8 ) , (36)\nwhich is at most \u03b42 when n \u2265 8 \u03c0min log 2m\u03b4 ."}, {"heading": "D Sample Complexity for the Gaussian Setting Without Time Shifts", "text": "Existing results on learning mixtures of Gaussians by Dasgupta and Schulman [7] and by Vempala and Wang [19] use a different notion of gap than we do. In our notation, their gap can be written as\nG(T )\u2217 , min v,v\u2032\u2208V s.t.v 6=v\u2032 \u2016v \u2212 v\u2032\u20162T , (37)\nwhich measures the minimum separation between the true latent sources, disregarding their labels.\nWe now translate our main theoretical guarantees to be in terms of gap G(T )\u2217 under the assumption that the noise is Gaussian and that there are no time shifts. Theorem 4. Under the latent source model, suppose that the noise is zero-mean Gaussian with variance \u03c32, that there are no time shifts (i.e., \u2206max = 0), and that we have sampled n > m log 4m\u03b4 training time series. Then if\nG(T )\u2217 \u2265 4\u03c32 log 4n 2\n\u03b4 , (38)\nT \u2265 (12 + 8 \u221a 2) log 4n2\n\u03b4 , (39)\nthen weighted majority voting (with \u03b8 = 1, \u03b3 = 18\u03c32 ) and nearest-neighbor classification each classify a new time series correctly with probability at least 1\u2212 \u03b4.\nIn particular, with access to a pool of \u2126(m log m\u03b4 ) time series, we can subsample n = \u0398(m log m \u03b4 ) of them to use as training data. Then provided that G(T )\u2217 = \u2126(\u03c32 log m\u03b4 ) and T = \u2126(log m \u03b4 ), we correctly classify a new time series with probability at least 1\u2212 \u03b4.\nProof. The basic idea is to show that with high probability, our gap G(T ) \u2261 G(T )(R+,R\u2212,\u2206max) satisfies G(T ) \u2265 G(T )\u2217 + 2\u03c32T \u2212 4\u03c3 \u221a G(T )\u2217 log 4n2\n\u03b4 \u2212 4\u03c32\n\u221a T log 4n2\n\u03b4 . (40)\nThe worst-case scenario occurs when G(T )\u2217 = 4\u03c32 log 4n 2\n\u03b4 , at which point we have\nG(T ) \u2265 2\u03c32T \u2212 4\u03c32 log 4n 2\n\u03b4 \u2212 4\u03c32\n\u221a T log 4n2\n\u03b4 . (41)\nThe right-hand side is at least \u03c32T when\nT \u2265 (12 + 8 \u221a 2) log 4n2\n\u03b4 , (42)\nwhich ensures that, with high probability, G(T ) \u2265 \u03c32T . Theorems 1 and 2 each say that if we further have n > m log 4m\u03b4 , and T \u2265 16 log 4n \u03b4 , then we classify a new time series correctly with high probability, where we note that T \u2265 (12 + 8 \u221a 2) log 4n 2\n\u03b4 \u2265 16 log 4n \u03b4 .\nWe now fill in the details. Let r+ and r\u2212 be two time series in the training data that have labels +1 and \u22121 respectively, where we assume that \u2206max = 0. Let v(r+) \u2208 V and v(r\u2212) \u2208 V be the true latent sources of r+ and r\u2212, respectively. This means that r+ \u223c N (v(r+), \u03c32IT\u00d7T ) and r\u2212 \u223c N (v(r\u2212), \u03c32IT\u00d7T ). Denoting E(r+) \u223c N (0, \u03c32IT\u00d7T ) and E(r\u2212) \u223c N (0, \u03c32IT\u00d7T ) to be noise associated with time series r+ and r\u2212, we have\n\u2016r+ \u2212 r\u2212\u20162T = \u2016(v(r+) + E(r+))\u2212 (v(r\u2212) + E(r\u2212))\u20162T = \u2016v(r+) \u2212 v(r\u2212)\u20162T + 2\u3008v(r+) \u2212 v(r\u2212), E(r+) \u2212 E(r\u2212)\u3009+ \u2016E(r+) \u2212 E(r\u2212)\u20162T .\n(43) We shall show that with high probability, for all r+ \u2208 R+ and for all r\u2212 \u2208 R\u2212:\n\u3008v(r+) \u2212 v(r\u2212), E(r+) \u2212 E(r\u2212)\u3009 \u2265 \u22122\u03c3\u2016v(r+) \u2212 v(r\u2212)\u2016T \u221a log 4n2\n\u03b4 , (44) \u2016E(r+) \u2212 E(r\u2212)\u20162T \u2265 2\u03c32T \u2212 4\u03c32 \u221a T log 4n2\n\u03b4 . (45)\n\u2022 Bound (44): \u3008v(r+) \u2212 v(r\u2212), E(r+) \u2212 E(r\u2212)\u3009 is zero-mean sub-Gaussian with parameter\u221a 2\u03c3\u2016v(r+) \u2212 v(r\u2212)\u2016T , so\nP(\u3008v(r+)\u2212v(r\u2212), E(r+)\u2212E(r\u2212)\u3009 \u2264 \u2212a) \u2264 exp ( \u2212 a 2\n4\u03c32\u2016v(r+) \u2212 v(r\u2212)\u20162T\n) = \u03b4\n4n2 (46) with choice a = 2\u03c3\u2016v(r+)\u2212v(r\u2212)\u2016T \u221a log 4n 2\n\u03b4 . A union bound over all pairs of time series in the training data with opposite labels gives\nP ( \u22c3 r+\u2208R+, r\u2212\u2208R\u2212 { \u3008v(r+)\u2212v(r\u2212), E(r+)\u2212E(r\u2212)\u3009 \u2264 \u22122\u03c3\u2016v(r+)\u2212v(r\u2212)\u2016T \u221a log 4n2 \u03b4 }) \u2264 \u03b4 4 .\n(47)\n\u2022 Bound (45): Due to a result by Laurent and Massart [14, Lemma 1], we have\nP(\u2016E(r+) \u2212 E(r\u2212)\u20162T \u2264 2\u03c32T \u2212 4\u03c32 \u221a Ta) \u2264 e\u2212a = \u03b4\n4n2 (48)\nwith choice a = log 4n 2\n\u03b4 . A union bound gives\nP ( \u22c3 r+\u2208R+, r\u2212\u2208R\u2212 { \u2016E(r+) \u2212 E(r\u2212)\u20162T \u2264 2\u03c32T \u2212 4\u03c32 \u221a T log 4n2 \u03b4 }) \u2264 \u03b4 4 . (49)\nAssuming that bounds (44) and (45) both hold, then for all r+ \u2208 R+, r\u2212 \u2208 R\u2212, we have \u2016r+ \u2212 r\u2212\u20162T = \u2016v(r+) \u2212 v(r\u2212)\u20162T + 2\u3008v(r+) \u2212 v(r\u2212), E(r+) \u2212 E(r\u2212)\u3009+ \u2016E(r+) \u2212 E(r\u2212)\u20162T\n\u2265 \u2016v(r+) \u2212 v(r\u2212)\u20162T \u2212 4\u03c3\u2016v(r+) \u2212 v(r\u2212)\u2016T\n\u221a log 4n2\n\u03b4 + 2\u03c32T \u2212 4\u03c32\n\u221a T log 4n2\n\u03b4\n(i) = ( \u2016v(r+) \u2212 v(r\u2212)\u2016T \u2212 2\u03c3 \u221a log 4n2\n\u03b4\n)2 \u2212 4\u03c32 log 4n 2\n\u03b4 + 2\u03c32T \u2212 4\u03c32\n\u221a T log 4n2\n\u03b4\n(ii) \u2265 (\u221a G(T )\u2217 \u2212 2\u03c3 \u221a log 4n2\n\u03b4\n)2 + 2\u03c32T \u2212 4\u03c32 log 4n 2\n\u03b4 \u2212 4\u03c32\n\u221a T log 4n2\n\u03b4 , (50)\nwhere step (i) follows from completing the square, and step (ii) uses our assumption that G(T )\u2217 \u2265 4\u03c32 log 4n 2\n\u03b4 . Minimizing over r+ \u2208 R+ and r\u2212 \u2208 R\u2212, we get G(T ) \u2265 (\u221a G(T )\u2217 \u2212 2\u03c3 \u221a log 4n2\n\u03b4\n)2 + 2\u03c32T \u2212 4\u03c32 log 4n 2\n\u03b4 \u2212 4\u03c32\n\u221a T log 4n2\n\u03b4 . (51)\nThe worst-case scenario occurs when G(T )\u2217 = 4\u03c32 log 4n 2\n\u03b4 , in which case\nG(T ) \u2265 2\u03c32T \u2212 4\u03c32 log 4n 2\n\u03b4 \u2212 4\u03c32\n\u221a T log 4n2\n\u03b4 . (52)\nTheorems 1 and 2 imply that having G(T ) \u2265 \u03c32T , n > m log 4m\u03b4 , and T \u2265 16 log 4n \u03b4 allows weighted majority voting (with \u03b8 = 1, \u03b3 = 18\u03c32 ) and nearest-neighbor classification to each succeed with high probability. We achieve G(T ) \u2265 \u03c32T by asking that\n2\u03c32T \u2212 4\u03c32 log 4n 2\n\u03b4 \u2212 4\u03c32\n\u221a T log 4n2\n\u03b4 \u2265 \u03c32T, (53)\nwhich happens when\nT \u2265 (12 + 8 \u221a 2) log 4n2\n\u03b4 . (54)\nA union bound over the following four bad events (each controlled to happen with probability at most \u03b44 ) yields the final result:\n\u2022 Not every latent source is seen in the training data.\n\u2022 Bound (44) doesn\u2019t hold.\n\u2022 Bound (45) doesn\u2019t hold.\n\u2022 Assuming that the above three bad events don\u2019t happen, we still misclassify."}, {"heading": "E Forecasting Trending Topics on Twitter", "text": "Twitter is a social network whose users post messages called Tweets, which are then broadcast to a user\u2019s followers. Often, emerging topics of interest are discussed on Twitter in real time. Inevitably, certain topics gain sudden popularity and \u2014 in Twitter speak \u2014 begin to trend. Twitter surfaces such topics as a list of top ten trending topics, or trends.\nData. We sampled 500 examples of trends at random from a list of June 2012 news trends and recorded the earliest time each topic trended within the month. Before sampling, we filtered out trends that never achieved a rank of 3 or better on the Twitter trends list5 as well as trends that lasted for less than 30 minutes as to keep our trend examples reasonably salient. We also sampled 500 examples of non-trends at random from a list of n-grams (of sizes 1, 2, and 3) appearing in Tweets created in June 2012, where we filter out any n-gram containing words that appeared in one of our 500 chosen trend examples. Note that as we do not know how Twitter chooses what phrases are considered as topic phrases (and are candidates for trending topics), it\u2019s unclear what the size of the non-trend category is in comparison to the size of the trend category. Thus, for simplicity, we intentionally control for the class sizes by setting them equal. In practice, one could still expressly assemble the training data to have pre-specified class sizes and then tune \u03b8 for generalized weighted majority voting (8). In our experiments, we just use the usual weighted majority voting (2) (i.e., \u03b8 = 1) to classify time series.\nFrom these examples of trends and non-trends, we then created time series of activity for each topic based on the rate of Tweets about that topic over time. To approximate this rate, we gathered 10% of all Tweets from June 2012, placed them into two-minute buckets according to their timestamps, and counted the number of Tweets in each bucket. We denote the count at the t-th time bucket as \u03c1(t), which we refer to as the raw rate. We then transform the raw rate in a number of ways, summarized in Figure 5, before using the resulting time series for classification.\n5On Twitter, trending topics compete for the top ten spots whereas we are only detecting whether a topic will trend or not.\nWe observed that trending activity is characterized by spikes above some baseline rate, whereas non-trending activity has fewer, if any spikes. For example, a non-trending topic such as \u201ccity\u201d has a very high, but mostly constant rate because it is a common word. In contrast, soon-to-be-trending topics like \u201cMiss USA\u201d will initially have a low rate, but will also have bursts in activity as the news spreads. To emphasize the parts of the rate signal above the baseline and de-emphasize the parts below the baseline, we define a baseline-normalized signal \u03c1b(t) , \u03c1(t)/ \u2211t \u03c4=1 \u03c1(\u03c4).\nA related observation is that the Tweet rate for a trending topic typically contains larger and more sudden spikes than those of non-trending topics. We reward such spikes by emphasizing them, while de-emphasizing smaller spikes. To do so, we define a baseline-and-spike-normalized rate \u03c1b,s(t) , |\u03c1b(t)\u2212 \u03c1b(t\u2212 1)|\u03b1 in terms of the already baseline-normalized rate \u03c1b; parameter \u03b1 \u2265 1 controls how much spikes are rewarded (we used \u03b1 = 1.2). In addition, we convolve the result with a smoothing window to eliminate noise and effectively measure the volume of Tweets in a sliding window of length Tsmooth: \u03c1b,s,c(t) , \u2211t \u03c4=t\u2212Tsmooth+1 \u03c1b,s(\u03c4).\nFinally, the spread of a topic from person to person can be thought of as a branching process in which a population of users \u201caffected\u201d by a topic grows exponentially with time, with the exponent depending on the details of the model [1]. This intuition suggests using a logarithmic scaling for the volume of Tweets: \u03c1b,s,c,l(t) , log \u03c1b,s,c(t).\nThe resulting time series \u03c1b,s,c,l contains data from the entire window in which data was collected. To construct the sets of training time seriesR+ andR\u2212, we keep only a small h-hour slice of representative activity r for each topic. Namely, each of the final time series r used in the training data is truncated to only contain the h hours of activity in the corresponding transformed time series \u03c1b,s,c,l. For time series corresponding to trending topics, these h hours are taken from the time leading up to when the topic was first declared by Twitter to be trending. For time series corresponding to non-trending topics, the h-hour window of activity is sampled at random from all the activity for the topic. We empirically found that how news topics become trends tends to follow a finite number of patterns; a few examples of these patterns are shown in Figure 3.\nExperiment. For a fixed choice of parameters, we randomly divided the set of trends and non-trends into two halves, one for training and one for testing. Weighted majority voting with the training data was used to classify the test data. Per time series in the test data, we looked within a window of 2h hours, centered at the trend onset for trends, and sampled randomly for non-trends. We restrict detection to this time window to avoid detecting earlier times that a topic became trending, if it trended multiple times. We then measured the false positive rate (FPR), true positive rate (TPR), and the time of detection if any. For trends, we computed how early or late the detection was compared to the true trend onset. We explored the following parameters: h, the length in hours of each example time series; T , the number of initial time steps in the observed time series s that we use for classification; \u03b3, the scaling parameter; Tsmooth, the width of the smoothing window. In all cases, constant \u2206max in the decision rule (2) is set to be the maximum possible, i.e., since observed signal s has T samples, we compare s with all T -sized chunks of each time series r in training data.\nFor a variety of parameters, we detect trending topics before they appear on Twitter\u2019s trending topics list. Figure 4 (a) shows that for one such choice of parameters, we detect trending topics before Twitter does 79% of the time, and when we do, we detect them an average of 1.43 hours earlier. Furthermore, we achieve a TPR of 95% and a FPR of 4%. Naturally, there are tradeoffs between the FPR, the TPR, and relative detection time that depend on parameter settings. An aggressive parameter setting will yield early detection and a high TPR, but at the expense of a high FPR. A conservative parameter setting will yield a low FPR, but at the expense of late detection and a low\nTPR. An in-between setting can strike the right balance. We show this tradeoff in two ways. First, by varying a single parameter at a time and fixing the rest, we generated an ROC curve that describes the tradeoff between FPR and TPR. Figure 4 (b) shows the envelope of all ROC curves, which can be interpreted as the best \u201cachievable\u201d ROC curve. Second, we broke the results up by where they fall on the ROC curve \u2014 top (\u201caggressive\u201d), bottom (\u201cconservative\u201d), and center (\u201cin-between\u201d) \u2014 and showed the distribution of early and late relative detection times for each (Figure 4(c)).\nWe discuss some fine details of the experimental setup. Due to restrictions on the Twitter data available, while we could determine whether a trending topic is categorized as news based on usercurated lists of \u201cnews\u201d people on Twitter, we did not have such labels for individual Tweets. Thus, the example time series that we use as training data contain Tweets that are both news and non-news. We also reran our experiments using only non-news Tweets and found similar results except that we do not detect trends as early as before; however, weighted majority voting still detects trends in advance of Twitter 79% of the time."}], "references": [{"title": "Trends in social media: Persistence and decay", "author": ["Sitaram Asur", "Bernardo A. Huberman", "G\u00e1bor Szab\u00f3", "Chunyan Wang"], "venue": "In Proceedings of the Fifth International Conference on Weblogs and Social Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Transformation based ensembles for time series classification", "author": ["Anthony Bagnall", "Luke Davis", "Jon Hills", "Jason Lines"], "venue": "In Proceedings of the 12th SIAM International Conference on Data Mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A complexity-invariant distance measure for time series", "author": ["Gustavo E.A.P.A. Batista", "Xiaoyue Wang", "Eamonn J. Keogh"], "venue": "In Proceedings of the 11th SIAM International Conference on Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Beyond trending topics: Real-world event identification on Twitter", "author": ["Hila Becker", "Mor Naaman", "Luis Gravano"], "venue": "In Proceedings of the Fifth International Conference on Weblogs and Social Media,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Emerging topic detection on twitter based on temporal and social terms evaluation", "author": ["Mario Cataldi", "Luigi Di Caro", "Claudio Schifanella"], "venue": "In Proceedings of the 10th International Workshop on Multimedia Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Nearest neighbor pattern classification", "author": ["Thomas M. Cover", "Peter E. Hart"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1967}, {"title": "A probabilistic analysis of EM for mixtures of separated, spherical gaussians", "author": ["Sanjoy Dasgupta", "Leonard Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Querying and mining of time series data: experimental comparison of representations and distance measures", "author": ["Hui Ding", "Goce Trajcevski", "Peter Scheuermann", "Xiaoyue Wang", "Eamonn Keogh"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Scalable training of mixture models via coresets", "author": ["Dan Feldman", "Matthew Faulkner", "Andreas Krause"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Introduction to statistical pattern recognition (2nd ed.)", "author": ["Keinosuke Fukunaga"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Learning mixtures of spherical gaussians: Moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M. Kakade"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Emerging topic detection using dictionary learning", "author": ["Shiva Prasad Kasiviswanathan", "Prem Melville", "Arindam Banerjee", "Vikas Sindhwani"], "venue": "In Proceedings of the 20th ACM Conference on Information and Knowledge Management,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Online l1dictionary learning with application to novel document detection", "author": ["Shiva Prasad Kasiviswanathan", "Huahua Wang", "Arindam Banerjee", "Prem Melville"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["Beatrice Laurent", "Pascal Massart"], "venue": "Annals of Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Twittermonitor: trend detection over the Twitter stream", "author": ["Michael Mathioudakis", "Nick Koudas"], "venue": "In Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Feature-based classification of time-series data", "author": ["Alex Nanopoulos", "Rob Alcock", "Yannis Manolopoulos"], "venue": "International Journal of Computer Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Interval and dynamic time warping-based decision trees", "author": ["Juan J. Rodr\u0131\u0301guez", "Carlos J. Alonso"], "venue": "In Proceedings of the 2004 ACM Symposium on Applied Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q. Weinberger", "Lawrence K. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Distance-function design and fusion for sequence data", "author": ["Yi Wu", "Edward Y. Chang"], "venue": "In Proceedings of the 2004 ACM International Conference on Information and Knowledge Management,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}], "referenceMentions": [{"referenceID": 16, "context": "Numerous standard classification methods have been tailored to classify time series, yet a simple nearest-neighbor approach is hard to beat in terms of classification performance on a variety of datasets [22], with results competitive to or better than various other more elaborate methods such as neural networks [17], decision trees [18], and support vector machines [21].", "startOffset": 314, "endOffset": 318}, {"referenceID": 17, "context": "Numerous standard classification methods have been tailored to classify time series, yet a simple nearest-neighbor approach is hard to beat in terms of classification performance on a variety of datasets [22], with results competitive to or better than various other more elaborate methods such as neural networks [17], decision trees [18], and support vector machines [21].", "startOffset": 335, "endOffset": 339}, {"referenceID": 20, "context": "Numerous standard classification methods have been tailored to classify time series, yet a simple nearest-neighbor approach is hard to beat in terms of classification performance on a variety of datasets [22], with results competitive to or better than various other more elaborate methods such as neural networks [17], decision trees [18], and support vector machines [21].", "startOffset": 369, "endOffset": 373}, {"referenceID": 2, "context": "More recently, researchers have examined which distance to use with nearest-neighbor classification [3, 8, 20] or how to boost classification performance by applying different transformations to the time series before using nearest-neighbor classification [2].", "startOffset": 100, "endOffset": 110}, {"referenceID": 7, "context": "More recently, researchers have examined which distance to use with nearest-neighbor classification [3, 8, 20] or how to boost classification performance by applying different transformations to the time series before using nearest-neighbor classification [2].", "startOffset": 100, "endOffset": 110}, {"referenceID": 19, "context": "More recently, researchers have examined which distance to use with nearest-neighbor classification [3, 8, 20] or how to boost classification performance by applying different transformations to the time series before using nearest-neighbor classification [2].", "startOffset": 100, "endOffset": 110}, {"referenceID": 1, "context": "More recently, researchers have examined which distance to use with nearest-neighbor classification [3, 8, 20] or how to boost classification performance by applying different transformations to the time series before using nearest-neighbor classification [2].", "startOffset": 256, "endOffset": 259}, {"referenceID": 5, "context": "If we don\u2019t confine ourselves to classifying time series, then as the amount of data tends to infinity, nearest-neighbor classification has been shown to achieve a probability of error that is at worst twice the Bayes error rate, and when considering the nearest k neighbors with k allowed to grow with the amount of data, then the error rate approaches the Bayes error rate [6].", "startOffset": 375, "endOffset": 378}, {"referenceID": 6, "context": "We show that existing performance guarantees on learning spherical Gaussian mixture models [7, 11, 19] require more stringent conditions than what our results need, suggesting that learning the latent sources is overkill if the goal is classification.", "startOffset": 91, "endOffset": 102}, {"referenceID": 10, "context": "We show that existing performance guarantees on learning spherical Gaussian mixture models [7, 11, 19] require more stringent conditions than what our results need, suggesting that learning the latent sources is overkill if the goal is classification.", "startOffset": 91, "endOffset": 102}, {"referenceID": 18, "context": "We show that existing performance guarantees on learning spherical Gaussian mixture models [7, 11, 19] require more stringent conditions than what our results need, suggesting that learning the latent sources is overkill if the goal is classification.", "startOffset": 91, "endOffset": 102}, {"referenceID": 3, "context": "Existing work that identify trends on Twitter [4, 5, 15] instead, as part of their trend detection, define models for what trends are, which we do not do, nor do we assume we have access to such definitions.", "startOffset": 46, "endOffset": 56}, {"referenceID": 4, "context": "Existing work that identify trends on Twitter [4, 5, 15] instead, as part of their trend detection, define models for what trends are, which we do not do, nor do we assume we have access to such definitions.", "startOffset": 46, "endOffset": 56}, {"referenceID": 14, "context": "Existing work that identify trends on Twitter [4, 5, 15] instead, as part of their trend detection, define models for what trends are, which we do not do, nor do we assume we have access to such definitions.", "startOffset": 46, "endOffset": 56}, {"referenceID": 11, "context": "(The same could be said of previous work on novel document detection on Twitter [12, 13].", "startOffset": 80, "endOffset": 88}, {"referenceID": 12, "context": "(The same could be said of previous work on novel document detection on Twitter [12, 13].", "startOffset": 80, "endOffset": 88}, {"referenceID": 15, "context": "In general, the number of samples we need from a Gaussian mixture mixture model to estimate the mixture component means is exponential in the number of mixture components [16].", "startOffset": 171, "endOffset": 175}, {"referenceID": 6, "context": "We could learn such a model using Dasgupta and Schulman\u2019s modified EM algorithm [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 18, "context": "Vempala and Wang [19] have a spectral method for learning Gaussian mixture models that can handle smallerG )\u2217 than Dasgupta and Schulman\u2019s approach but requires n = \u03a9\u0303(T m) training data, where we\u2019ve hidden the dependence on \u03c3 and other variables of interest for clarity of presentation.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "Hsu and Kakade [11] have a moment-based estimator that doesn\u2019t have a gap condition but, under a different non-degeneracy condition, requires substantially more samples for our problem setup, i.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "In such scenarios, one could instead non-uniformly subsample O(Tm/\u03b5) time series from the training data using the procedure given in [9] and then feed the resulting smaller dataset, referred to as an (m, \u03b5)-coreset, to the EM algorithm for learning the latent sources.", "startOffset": 133, "endOffset": 136}], "year": 2013, "abstractText": "For classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines. We develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series. Our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on Twitter, there aren\u2019t actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a \u201cweighted majority voting\u201d classification rule that can be approximated by a nearest-neighbor classifier. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends, where we are able to detect such \u201ctrending topics\u201d in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%.", "creator": "LaTeX with hyperref package"}}}