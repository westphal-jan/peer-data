{"id": "1302.5056", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "Pooling-Invariant Image Feature Learning", "abstract": "feedback table searching removes yet a constant component in state - of - the - art partial vision recognition architectures. unfortunately highly problematic methods where designing patch - sorted child learning, these candidates may learn similarity features using the sequencing stage in a given early vision architecture. in this paper, teams offer a novel link learning scheme to partly strike into account the invariance constraint mapping areas after the reverse pooling stage., algorithm is built on backward clustering, and that enjoys strength and retention. we include the implicit function whose justifies sequential use of clustering algorithms, processing empirically conclusions differently the algorithm finds better ways than patch - colored dictionary providing the same dictionary size.", "histories": [["v1", "Tue, 15 Jan 2013 18:47:11 GMT  (2733kb,D)", "http://arxiv.org/abs/1302.5056v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yangqing jia", "oriol vinyals", "trevor darrell"], "accepted": false, "id": "1302.5056"}, "pdf": {"name": "1302.5056.pdf", "metadata": {"source": "CRF", "title": "Pooling-Invariant Image Feature Learning", "authors": ["Yangqing Jia", "Oriol Vinyals", "Trevor Darrell"], "emails": ["jiayq@eecs.berkeley.edu", "vinyals@eecs.berkeley.edu", "trevor@eecs.berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "In the recent decade local patch-based, spatially pooled feature extraction pipelines have been shown to provide good image features for classification. Methods following such a pipeline usually start from densely extracted local image patches (either normalized raw pixel values or handcrafted descriptors such as SIFT or HOG), and perform dictionary learning to obtain a dictionary of codes (filters). The patches are then encoded into an over-complete representation using various algorithms such as sparse coding [14, 17] and simple inner product with a non-linear post-processing [4, 10]. After encoding, spatial pooling with average or max operations are carried out to form a global image representation [19, 1]. The encoding and pooling pipeline can be stacked to produce a final feature vector, which is then used to predict the labels for the images usually via a linear classifier.\nThere is an abundance of literature on single-layered networks for unsupervised feature encoding. Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or lo-\ncality [17]. A particularly interesting finding in the recent papers [3, 15, 4, 16] is that very simple patch-based dictionary learning algorithms like K-means or random selection, combined with feed-forward encoding methods with a naive nonlinearity, produces state-of-the-art performance on various datasets. Explanation of such phenomenon often focuses on the local image patch statistics, such as the frequency selectivity of random samples [16].\nA potential problem with such patch-based learning methods is that it may learn redundant features when we consider the pooling stage, as two codes that are uncorrelated may become highly correlated after pooling due to the introduction of spatial invariance. While using a larger dictionary almost always alleviates this problem, in practice we often want the dictionary to have a limited number of codes due to various reasons. First, feature computation has become the dominant factor in the state-of-the-art image classification pipelines, even with purely feed-forward methods (e.g., threshold encoding [4]) or speedup algorithms (e.g., LLC [17]). Second, reasonably sized dictionary helps to more easily learn further tasks that depends on the encoded features; this is especially true when we have more than one coding-pooling stage such as stacked deep networks, or when one applies more complex pooling stages such as second-order pooling [2], as a large encoding output would immediately drive up the number of parameters in the next layer.\nThus, it would be beneficial to design a dictionary learning algorithm that takes pooling into consideration and learns a compact dictionary. Prior work on addressing such problem often resorts to convolutional approaches [12, 22]. These methods are usually able to find dictionaries that bear a better level of spatial invariance than patch-based Kmeans, but are often non-trivial to train when the dictionary size is large, since a convolution operation has to be carried out instead of simple inner products.\nIn our paper we present a new method that is analogous to the patch-based K-means method for dictionary learning, but takes into account the redundancy that may be introduced in the pooling stage. We show how a K-centroids\n1\nar X\niv :1\n30 2.\n50 56\nv1 [\ncs .C\nV ]\n1 5\nJa n\n20 13\nclustering method applied on the covariance between candidate codes can efficiently learn pooling invariant representations. We also show how one can view dictionary learning as a matrix approximation problem, which finds the best approximation to an \u201coracle\u201d dictionary (which is often very large). It turns out that under this perspective, the performance of various dictionary learning methods can be explained by the recent findings in Nystro\u0308m subsampling [11, 23].\nWe will first review the feature extraction pipeline and the effect of pooling on the learned dictionary in Section 2, and then describe the proposed two-stage dictionary learning method in Section 3. The effectiveness of this simple yet effective dictionary learning algorithms on the standard CIFAR-10 and STL benchmark datasets, as well as the finegrained classification task, in which we show that feature learning plays an important role."}, {"heading": "2. Background", "text": "We illustrate the feature extraction pipeline that is composed of encoding dense local patches and pooling encoded features in Figure 1. Specifically, starting with an input image I, we formally define the encoding and pooling stages as follows.\n(1) Coding. In the coding step, we extract local image patches1, and encode each patch to K activation values based on a dictionary of size K (learned via a separate dictionary learning step). These activations are typically binary (in the case of vector quantization) or continuous (in the case of e.g. sparse coding), and it is generally believed that having an over-complete (K > the dimension of patches) dictionary while keeping the activations sparse helps classification, especially when linear classifiers are used in the later steps.\nWe will mainly focus on what we call the decoupled\n1Although we use the term \u201cpatches\u201d throughout the paper, the pipeline works with local image descriptors, such as SIFT, as well.\nencoding methods, in which the activation of one code does not rely on other codes, such as threshold encoding [4], which computes the inner product between p and each code, with a fixed threshold parameter \u03b1: fk(x) = max{0,d>k p \u2212 \u03b1}. Such methods have been increasingly popular mainly for their efficiency over coupled encoding methods such as sparse coding, for which a joint optimization needs to be carried out. Their employment in several deep models (e.g. [10]) also suggests that such simple nonlinearity may suffice to learn a good classifier in the later stages.\nLearning the dictionary: Recently, it has been found that relatively simple dictionary learning and encoding approaches lead to surprisingly good performances [3, 16]. For example, to learn a dictionary D = {d1,d2, \u00b7 \u00b7 \u00b7 ,dK} of size K from randomly sampled patches P = {p1,p2, \u00b7 \u00b7 \u00b7 ,pN} each reshaped as a vector of pixel values, one could simply adopt the K-means algorithm, which aims to minimize the squared distance between each patch and its nearest code: minD \u2211N i=1 minj \u2016pi \u2212 dj\u201622. We refer to [3] for a detailed comparison about different dictionary learning and encoding algorithms.\n(2) Pooling. Since the coding result are highly overcomplete and highly redundant, the pooling layer aggregates the activations over a spatial region of the image to obtain a K dimensional vector x. Each dimension of the pooled feature xi is obtained by taking the activations of the corresponding code in the given spatial region (also called receptive field in the literature), and performing a predefined operator (usually average or max) on the set of activations.\nFigure 1 shows an example when average pooling is carried out over the whole image. In practice we may define multiple spatial regions per image (such as a regular grid), and the global representation for the image will then be a vector of size K times the number of spatial regions.\nSince the feature extraction for image classification often involves the spatial pooling stage, the patch-level dic-\ntionary learning may not find good dictionaries that produce most informative pooled outputs. In fact, one would reasonably argue that it doesn\u2019t, one immediate reason being that patch-based dictionary learning algorithms often yield similar Gabor filters with small translations. Such filters, when pooled over a certain spatial region, produce highly correlated responses and lead to redundancy in the feature representation. Figure 2 shows two such examples, where filters produce uncorrelated patch-based responses but highly correlated pooled responses.\nConvolutional approaches [12, 22] are usually able to find dictionaries that are more spatially invariant than patchbased K-means, but learning may not scale as well as simple clustering algorithms, especially with hundreds or thousands of codes. In addition, convolutional approaches may still not solve the problem of inter-code invariance: for example, the response of a colored edge filter might have high correlation with that of a gray scale edge filter, and such correlation could not be modeled by spatial invariance."}, {"heading": "3. Pooling-Invariant Dictionary Learning", "text": "We are interested in designing a simple yet effective dictionary learning algorithm that takes into consideration the pooling stage of the feature extraction pipeline, and that models the general invariances among the pooled features. Observing the effectiveness of clustering methods in dictionary learning, we propose to learn a final dictionary of size K in two stages: first, we adopt the patch-based K-means algorithm to learn a more over-complete starting dictionary of size M (M > K); we then perform encoding and pooling using the dictionary, learn the final, smaller dictionary of size K from the statistics of the M pooled features.\nThe motivation of such idea is that K-means is a highly parallelizable algorithm that could be scaled up by simply sharding the data, allowing us to have an efficient algorithm for dictionary learning. Using a starting dictionary allows us to preserve most information on the patch-level, and the second step prunes away the redundancy due to pooling.\nNote that the large dictionary is only used during the feature learning time - after this, for each input image, we only need to encode local patches with the selected, relatively smaller dictionary, not any more expensive than existing feature extraction methods."}, {"heading": "3.1. Feature Selection with Affinity Propagation", "text": "The first step of our algorithm is identical to the patchbased K-means algorithm with a starting dictionary sizeM . After this, we can sample a set of image super-patches of the same as the pooling regions, and obtain the M dimensional pooled features from them. Randomly sampling a large number of pooled features in this way allows us to analyze the pairwise similarities between the codes in the starting dictionary in a post-pooling fashion. We would then like to find a K-dimensional subspace that best represents the M pooled features. Specifically, the similarity between two pooled dimensions (which correspond to two codes in the starting dictionary) i and code j as\ns(i, j) = 2Cij\u221a CiiCjj \u2212 2 (1)\nwhere C is the covariance matrix computed from the random sample of pooled features. We note that this is equivalent to the negative Euclidean distance between the coded output i and the coded output j when the outputs are normalized to have zero mean and standard deviation 1. We then use affinity propagation [9], which is a version of the K-centroids algorithm, to select centroids from existing features. Intuitively, codes that produce redundant pooled output (such as translated versions of the same code) would have high similarity between them, and only one exemplar would be chosen by the algorithm.\nSpecifically, affinity propagation finds centroids from a set of candidates where pairwise similarity s(i, j) (1 \u2264 i, j \u2264 M ) can be computed. It iteratively updates two terms, the \u201cresponsibility\u201d r(i, j) and the \u201cavailability\u201d a(i, j) via a message passing method following such rules\n[9]:\nr(i, k)\u2190 s(i, k)\u2212max k\u2032 6=k {a(i, k\u2032) + s(i, k\u2032)} (2)\na(i, k)\u2190 min{0, r(k, k) + \u2211\ni\u2032 /\u2208{i,k} max{0, r(i\u2032, k)}}\n(if i 6= k) (3) a(k, k)\u2190 \u2211\ni\u2032 6=k max{0, r(i\u2032, k)} (4)\nUpon convergence, the centroid that represents any candidate i is given by arg maxk(a(i, k)+r(i, k)), and the set of centroids S is obtained by\nS = {k|\u2203i, k s.t. k = arg max k\u2032\n(a(i, k\u2032) + r(i, k\u2032))} (5)\nAnd we refer to [9] for details about the nature of such message passing algorithms."}, {"heading": "3.2. Visualization of Selected Filters", "text": "To visually show what codes are selected by affinity propagation, we applied our approach to the CIFAR-10 dataset by first training an over-complete dictionary of 3200 codes, and then performing affinity propagation on the 3200-dimensional pooled features to obtain 256 centroids, which we visualize in Figure 3. Translational invariance appears to be the most dominant factor, as many clusters contain translated versions of the same Gabor like code, especially for gray scale codes. On the other hand, clusters capture more than translation: clusters such as column 5 focus on finding the contrasting colors more than finding edges of exactly the same angle, and clusters such as the last column finds invariant edges of varied color. We note that the selected codes are not necessarily centered (which is the case for convolutional approaches), as the centroids are selected solely from the pooled response covariance statistics, which does not explicitly favor centered patches."}, {"heading": "4. Why Does Clustering Work?", "text": "We will briefly discuss the reason why simple clustering algorithms work well in finding a good dictionary. Essentially, given a dictionary D of size K and specifications on the encoding and pooling operations, the feature extraction pipeline could be viewed as a projection from the original raw image pixels to the pooled feature space RK . Note that this space does not degenerate since the feature extraction is highly nonlinear, and one could also view this from a kernel perspective as defining a specific kernel between images. Then, one way to evaluate the information contained in this embedded feature space is to use the covariance matrix C of the output features, which plays the dual role of the kernel matrix between the encoded patches.\nConsidering that larger dictionaries almost always increases performance [20], and in the limit one could use all\nthe patches P as the dictionary, leading to a N -dimensional space and an N \u00d7N covariance matrix CP . Since in practice we always assume a budget on the dictionary size, the goal is to find a dictionary D of size K, which yields a Kdimensional space and a covariance matrix CD. The approximation to the \u201coracle\u201d encoded space using D could then be computed as:\nCP \u2248 CPDC+DCDP (6)\nwhere CPD is the covariance matrix between the features extracted by P and the one extracted byD, and C+ denotes the matrix pseudo-inverse.\nNote that such explanation works for both the beforepooling case and the after-pooling case. The dictionary learning algorithm can then be thought as finding a dictionary that approximates CP best. Interestingly, this could be thought as a form of the Nystro\u0308m method that subsamples subsets of the matrix columns for approximation. The Nystro\u0308m method has been used to approximate large matrices for spectral clustering [8], and here enables us to explain the mechanism of dictionary learning. Recent research in the machine learning field, notably [11], supports the recent empirical observations in vision: first, it is known that uniformly sampling the columns of the matrix CP already works well in reconstruction, which explains the good performance of random patches in feature learning [16]; second, theoretical results [11, 23] have shown that clustering algorithms works particularly better than other methods as a data-driven way in finding good subsets to approximate the original matrix, justifying the use of clustering in the dictionary learning works.\nIn the patch-based dictionary learning, clustering could be directly applied on the set of patches (i.e. the largest dictionary in the limit). When we consider pooling, though, using all the patches becomes non-trivial, since we need to compute pooled features using each patch as a code, which is computationally overwhelming. Thus, a reasonable approach to consider is to first find a subset of all the patches using patch-based clustering (although this \u201csubset\u201d is still larger than our final dictionary size), and perform clustering on the pooled outputs of this subset only, which leads to the proposed algorithm. The performance of the algorithm is then supported by the Nystro\u0308m theory above.\nBased on the matrix approximation explanation, we further reshape our selected features to match the distribution in the higher-dimensional space corresponding to the starting dictionary. After selecting K centroids denoted by subset S, we can group the original M \u00d7M covariance matrix as\nC = [ CSS CSS\u0304 CS\u0304S CS\u0304S\u0304 ] , W = [ CSS CS\u0304S ] (7)\nwhere CSS\u0304 denotes the covariance between the subset S and the subset S\u0304 and so on. We then approximate the origi-\nnal high-dimensional covariance matrix as\nC \u2248WC+SSW > (8)\nMore importantly, given the pooled outputs xS using the selected filters, the high-dimensional feature could be approximated by\nx \u2248 AxS , where A = WC\u22121SS (9)\nNotice that the implicit dimensionality of the data is still no larger than the numberK of selected codes, so we can apply SVD on the matrix A as A = U\u039bV where \u039b is a K \u00d7K diagonal matrix and U is a M \u00d7K column-wise orthonormal matrix, and compute the low-dimensional feature (with a little abuse of terminology) as\nx\u0304S = \u039bVxS (10)\nThe K \u00d7 K transform matrix \u039bV could be pre-computed during feature selection, and imposes minimum overhead during the actual feature extraction. This transform does not change the dimensionality or the rank of the data, and only changes the shape of the underlying data distribution. In practice, we found the transformed data yields a slightly better performance than the untransformed data when combined with a linear SVM with L2 regularization.\nIf we simply would like to find a low-dimensional representation from the M -dimensional pooled features, one would naturally choose PCA to find the K most significant projections:\nC \u2248 UK\u039bKU>K (11)\nwhere the M \u00d7K matrix UK contains the eigenvectors and the K \u00d7 K diagonal matrix \u039bK contains the eigenvalues. The low-dimensional features are then computed as xK = U>Kx.\nWe note that while this guarantees the best Kdimensional approximation, it does not help in our task since the number of filters are not reduced, as PCA almost\nalways yields non-zero coefficients for all the dimensions. Linearly combining the codes does not work either, due to the nonlinear nature of the encoding algorithm. However, as we will show in Section 5.3, results with PCA show that a larger starting dictionary almost always help performance even when the feature is then reduced to a lower dimensional space of the same size as a smaller dictionary, which justifies the use of matrix approximation to explain the dictionary learning behavior."}, {"heading": "5. Experiments", "text": "We apply our pooling-invariant dictionary learning (PDL) algorithm on several benchmark tasks, including the CIFAR-10 and STL datasets on which performance can be systematically analyzed, and the fine-grained classification task of classifying bird species, on which we show that feature learning provides a significant performance gain compared to conventional methods."}, {"heading": "5.1. CIFAR-10 and STL", "text": "The CIFAR-102 and the STL3 datasets are extensively used to analyze the behavior of feature extraction pipelines. CIFAR-10 contains a large number of training and testing data, while STL contains a very small amount of training data and a large amount of unlabeled images. As our algorithm works with any encoding and pooling operations, we adopted the standard setting usually carried on the dataset: extracting local 6\u00d76 patches with mean subtracted and contrast normalized, whitening the patches with ZCA, and then train the dictionary with normalized K-means. The features are then encoded using one-sided threshold encoding with \u03b1 = 0.25 and average pooled over the four quadrants (2\u00d72) of the image. For STL, we followed [5] and resized them to 32 \u00d7 32. For the PDL algorithm, instead of learning a different set of codes for each pooling quadrant, we learn an\n2http://www.cs.toronto.edu/\u02dckriz/cifar.html 3http://www.stanford.edu/\u02dcacoates/stl10/, [3]\nidentical set of codes in general for pooling the coded outputs. For all experiments, we carry out 5 independent runs and take the mean accuracy to report here."}, {"heading": "5.2. Statistics for Feature Selection", "text": "We first verify whether the learned codes capture the pooling invariance as we claimed in the previous section. To this end, we start from the selected features in Section 3.2, and randomly sample three types of filter responses: (a) pairwise filter responses before pooling between codes in the same cluster, (b) pairwise filter responses after pooling between codes in the same cluster, and (c) pairwise filter responses after pooling between the selected centroids. The distribution of such responses are plotted in Figure 4. The result verifies our conjecture well: first, codes that produce uncorrelated responses before pooling may become correlated after the pooling stage (comparing 4(a) and 4(b)), which could effectively be identified by the affinity propagation algorithm; second, by explicitly taking into consideration the pooling behavior, we are able to select a subset of the features whose responses are lowly correlated (compare 4(b) and 4(c)), which helps preserve more information with a fixed number of codes.\nFigure 4(d) shows the eigenvalues of the original covariance matrix and those of the approximated covariance matrix, using the same setting as in the previous subsection4. The approximation captures the largest eigenvalues of the original covariance matrix well, while dropping at a higher rate for smaller eigenvalues."}, {"heading": "5.3. Classification Performances", "text": "Figure 5 shows the relative improvement obtained on CIFAR-10, when we use a budgeted dictionary of size 200, but perform feature selection from a larger dictionary as indicated by the X axis. The PCA performance is also included in the figure as a loose upper bound of the feature\n4Note that since we only select 256 features, the number of nonzero eigenvalues is 256 for the approximation.\nselection performance. Learning the dictionary with our feature selection method consistently increases the performance as the size of the original dictionary increases, and is able to get about 70% the performance gain as obtained by PCA (again, notice that PCA still requires all the codes to be used, thus does not save feature extraction time).\nThe detailed performance of our algorithm on the two datasets, using different starting and final dictionary sizes, is visualized in Figure 6. Table 1 summarizes the accuracy values of two particular cases - final dictionary sizes of 200 and 1600 respectively. Note that our goal is not to get the best overall performance - as performance always goes up when we use more codes. Rather, we focus on how much gain the pooling-aware dictionary learning gets, given a fixed dictionary size as the budget. Figure 7 shows the performance of the various settings, using PCA to reduce the dimensionality instead of PDL. As we stated in the previous section, this serves as an upper bound of the feature selection algorithms.\nOverall, considering the pooled feature statistics always help us to find better dictionaries, especially when only\na small dictionary is allowed for classification. Choosing from a larger starting dictionary helps increasing performance, although such effect saturates when the dictionary size is much larger than the target size. For the STL dataset, a large starting dictionary may lessen the performance gain (Figure 6(b)). We infer the reason to be that feature selection is more prone to local optimum, and the small training data of STL may cause the performance to be sensitive to suboptimal codebook choices. However, in general the codebook learned by PDL is consistently better than its patch-based counterpart.\nFinally, we note that due to the heavy-tailed nature of the encoded and pooled features (see the eigendecomposition\nof Figure 4), one can infer that the representations obtained with a budget would have a correspondingly bounded performance when combined with linear SVMs. In this paper we have focused on analyzing unsupervised approaches. Incorporating weakly supervised information to guide feature learning / selection or learning multiple layers of feature extraction would be particularly interesting, and would be a possible future direction."}, {"heading": "5.4. Fine-grained Classification", "text": "To show the performance of the feature learning algorithm in the real-world image classification tasks, we tested the performance of our algorithm on the fine-grained classification task, using the 2011 Caltech-UCSD Birds dataset [18]5. Classification of file-grained categories poses a significant challenge for the contemporary vision algorithms, as such classification tasks usually requires the identification of localized appearances like \u201cbirds with yellow spotted feather on the belly\u201d which is hard to capture using manually designed features. Recent work on fine-grained classification usually focuses on the localization of parts [7, 24, 6], and still uses manually designed features. Yao et al. [21] proposed to use a template-based approach for more powerful features, but the approach may be difficult to scale as the number of templates grow larger6. In addition, due to the lack of training data in fine-grained classification tasks, whether supervised feature learning is useful or not is unclear yet.\nWe performed classification on all the 200 bird species provided. For the image pre-processing we followed the same setting as [24, 21] by cropping the images to be cen-\n5http://www.vision.caltech.edu/visipedia/ CUB-200-2011.html\n6Due to computation complexity, some early work such as [7, 21] do not scale up well, and only reported performance on subsets of the whole data [personal communication].\ntered on the birds using 1.5\u00d7 the size of the provided bounding boxes. We then resized each cropped image to 128\u00d7128 to avoid any artifact that may be introduced by the varying number of local features. The training data are expanded by simply mirroring each training image. Then, we extract 5 \u00d7 5 local whitened patches as we did for CIFAR-10, encoded them using threshold encoding with a dictionary of size 1600, and performed 4 \u00d7 4 max pooling since the bird images are larger than those from CIFAR and STL. The dictionary is learned using our feature extraction pipeline, from an original set of 3200 patch-based clustering centers.\nOur classification results, together with previous stateof-the-art baselines from [24], are reported in Table 2. It is somewhat surprising to observe that feature learning provides a significant performance boost in this case, indicating that in addition to part localization (which has been the focus of fine-grained classification), learning appropriate features / descriptors to represent local appearances may be a major factor in fine-grained classification, possibly due to the subtle appearance changes for such tasks. As we have shown here, even simple and fully unsupervised feature learning algorithms such as K-means and PDL could lead to significant accuracy improvement, and we hope this would inspire further advancement in the fine-grained classification research."}, {"heading": "6. Conclusion", "text": "We have proposed a novel algorithm to efficiently take into account the invariance of learned features after the spatial pooling stage. The algorithm is empirically shown to identify redundancy between codes learned in a patch-based way, and yields dictionaries that produces better classification accuracy than simple patch-based approaches. To explain the performance gain we proposed to take a matrix approximation view of the dictionary learning, and show the close connection between the proposed methods and the Nystro\u0308m method. The proposed method does not introduce overheads during classification time, and could be easily \u201cplugged in\u201d to the existing image classification pipelines."}], "references": [{"title": "Learning mid-level features for recognition", "author": ["Y Boureau", "F Bach", "Y LeCun", "J Ponce"], "venue": "CVPR,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic segmentation with second-order pooling", "author": ["J Carreira", "R Caseiro", "J Batista", "C Sminchisescu"], "venue": "ECCV,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A Coates", "H Lee", "A Ng"], "venue": "AISTATS,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A Coates", "A Ng"], "venue": "ICML,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Selecting receptive fields in deep networks", "author": ["A Coates", "AY Ng"], "venue": "NIPS,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Discovering localized attributes for fine-grained recognition", "author": ["K Duan", "D Parikh", "D Crandall", "K Grauman"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Birdlets: Subordinate categorization using volumetric primitives and pose-normalized appearance", "author": ["R Farrell", "O Oza", "N Zhang", "VI Morariu", "T Darrell", "LS Davis"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral grouping using the nystrom method", "author": ["C Fowlkes", "S Belongie", "F Chung", "J Malik"], "venue": "IEEE TPAMI, 26(2):214\u2013225,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Clustering by passing messages between data points", "author": ["BJ Frey", "D Dueck"], "venue": "Science, 315(5814):972\u2013976,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A Krizhevsky", "I Sutskever", "GE Hinton"], "venue": "NIPS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Sampling methods for the nystr\u00f6m method", "author": ["S Kumar", "M Mohri", "A Talwalkar"], "venue": "JMLR, 13(Apr):981\u20131006,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H Lee", "R Grosse", "R Ranganath", "AY Ng"], "venue": "ICML,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J Mairal", "F Bach", "J Ponce", "G Sapiro"], "venue": "JMLR, 11:19\u201360,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by V1", "author": ["B Olshausen", "DJ Field"], "venue": "Vision research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Are sparse representations really relevant for image classification", "author": ["R Rigamonti", "MA Brown", "V Lepetit"], "venue": "In CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "On random weights and unsupervised feature learning", "author": ["A Saxe", "PW Koh", "Z Chen", "M Bhand", "B Suresh", "A Ng"], "venue": "ICML,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Localityconstrained linear coding for image classification", "author": ["J Wang", "J Yang", "K Yu", "F Lv", "T Huang", "Y Gong"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Caltech-UCSD Birds 200", "author": ["P Welinder", "S Branson", "T Mita", "C Wah", "F Schroff", "S Belongie", "P Perona"], "venue": "Technical Report CNS-TR- 2010-001, Caltech,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J Yang", "K Yu", "Y Gong"], "venue": "CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient highly over-complete sparse coding using a mixture model", "author": ["J Yang", "K Yu", "T Huang"], "venue": "ECCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "A codebook-free and annotationfree approach for fine-grained image categorization", "author": ["B Yao", "G Bradski", "L Fei-Fei"], "venue": "CVPR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Deconvolutional networks", "author": ["MD Zeiler", "D Krishnan", "GW Taylor", "R Fergus"], "venue": "CVPR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Improved nystr\u00f6m low-rank approximation and error analysis", "author": ["K Zhang", "IW Tsang", "JT Kwok"], "venue": "ICML,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Pose pooling kernels for subcategory recognition", "author": ["N Zhang", "R Farrell", "T Darrell"], "venue": "CVPR,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "The patches are then encoded into an over-complete representation using various algorithms such as sparse coding [14, 17] and simple inner product with a non-linear post-processing [4, 10].", "startOffset": 113, "endOffset": 121}, {"referenceID": 16, "context": "The patches are then encoded into an over-complete representation using various algorithms such as sparse coding [14, 17] and simple inner product with a non-linear post-processing [4, 10].", "startOffset": 113, "endOffset": 121}, {"referenceID": 3, "context": "The patches are then encoded into an over-complete representation using various algorithms such as sparse coding [14, 17] and simple inner product with a non-linear post-processing [4, 10].", "startOffset": 181, "endOffset": 188}, {"referenceID": 9, "context": "The patches are then encoded into an over-complete representation using various algorithms such as sparse coding [14, 17] and simple inner product with a non-linear post-processing [4, 10].", "startOffset": 181, "endOffset": 188}, {"referenceID": 18, "context": "After encoding, spatial pooling with average or max operations are carried out to form a global image representation [19, 1].", "startOffset": 117, "endOffset": 124}, {"referenceID": 0, "context": "After encoding, spatial pooling with average or max operations are carried out to form a global image representation [19, 1].", "startOffset": 117, "endOffset": 124}, {"referenceID": 12, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 132, "endOffset": 139}, {"referenceID": 3, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 132, "endOffset": 139}, {"referenceID": 13, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 295, "endOffset": 307}, {"referenceID": 18, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 295, "endOffset": 307}, {"referenceID": 19, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 295, "endOffset": 307}, {"referenceID": 16, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 320, "endOffset": 324}, {"referenceID": 2, "context": "A particularly interesting finding in the recent papers [3, 15, 4, 16] is that very simple patch-based dictionary learning algorithms like K-means or random selection, combined with feed-forward encoding methods with a naive nonlinearity, produces state-of-the-art performance on various datasets.", "startOffset": 56, "endOffset": 70}, {"referenceID": 14, "context": "A particularly interesting finding in the recent papers [3, 15, 4, 16] is that very simple patch-based dictionary learning algorithms like K-means or random selection, combined with feed-forward encoding methods with a naive nonlinearity, produces state-of-the-art performance on various datasets.", "startOffset": 56, "endOffset": 70}, {"referenceID": 3, "context": "A particularly interesting finding in the recent papers [3, 15, 4, 16] is that very simple patch-based dictionary learning algorithms like K-means or random selection, combined with feed-forward encoding methods with a naive nonlinearity, produces state-of-the-art performance on various datasets.", "startOffset": 56, "endOffset": 70}, {"referenceID": 15, "context": "A particularly interesting finding in the recent papers [3, 15, 4, 16] is that very simple patch-based dictionary learning algorithms like K-means or random selection, combined with feed-forward encoding methods with a naive nonlinearity, produces state-of-the-art performance on various datasets.", "startOffset": 56, "endOffset": 70}, {"referenceID": 15, "context": "Explanation of such phenomenon often focuses on the local image patch statistics, such as the frequency selectivity of random samples [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 3, "context": ", threshold encoding [4]) or speedup algorithms (e.", "startOffset": 21, "endOffset": 24}, {"referenceID": 16, "context": ", LLC [17]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 1, "context": "Second, reasonably sized dictionary helps to more easily learn further tasks that depends on the encoded features; this is especially true when we have more than one coding-pooling stage such as stacked deep networks, or when one applies more complex pooling stages such as second-order pooling [2], as a large encoding output would immediately drive up the number of parameters in the next layer.", "startOffset": 295, "endOffset": 298}, {"referenceID": 11, "context": "Prior work on addressing such problem often resorts to convolutional approaches [12, 22].", "startOffset": 80, "endOffset": 88}, {"referenceID": 21, "context": "Prior work on addressing such problem often resorts to convolutional approaches [12, 22].", "startOffset": 80, "endOffset": 88}, {"referenceID": 10, "context": "It turns out that under this perspective, the performance of various dictionary learning methods can be explained by the recent findings in Nystr\u00f6m subsampling [11, 23].", "startOffset": 160, "endOffset": 168}, {"referenceID": 22, "context": "It turns out that under this perspective, the performance of various dictionary learning methods can be explained by the recent findings in Nystr\u00f6m subsampling [11, 23].", "startOffset": 160, "endOffset": 168}, {"referenceID": 3, "context": "encoding methods, in which the activation of one code does not rely on other codes, such as threshold encoding [4], which computes the inner product between p and each code, with a fixed threshold parameter \u03b1: fk(x) = max{0,dk p \u2212 \u03b1}.", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "[10]) also suggests that such simple nonlinearity may suffice to learn a good classifier in the later stages.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Learning the dictionary: Recently, it has been found that relatively simple dictionary learning and encoding approaches lead to surprisingly good performances [3, 16].", "startOffset": 159, "endOffset": 166}, {"referenceID": 15, "context": "Learning the dictionary: Recently, it has been found that relatively simple dictionary learning and encoding approaches lead to surprisingly good performances [3, 16].", "startOffset": 159, "endOffset": 166}, {"referenceID": 2, "context": "We refer to [3] for a detailed comparison about different dictionary learning and encoding algorithms.", "startOffset": 12, "endOffset": 15}, {"referenceID": 11, "context": "Convolutional approaches [12, 22] are usually able to find dictionaries that are more spatially invariant than patchbased K-means, but learning may not scale as well as simple clustering algorithms, especially with hundreds or thousands of codes.", "startOffset": 25, "endOffset": 33}, {"referenceID": 21, "context": "Convolutional approaches [12, 22] are usually able to find dictionaries that are more spatially invariant than patchbased K-means, but learning may not scale as well as simple clustering algorithms, especially with hundreds or thousands of codes.", "startOffset": 25, "endOffset": 33}, {"referenceID": 8, "context": "We then use affinity propagation [9], which is a version of the K-centroids algorithm, to select centroids from existing features.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "[9]:", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "And we refer to [9] for details about the nature of such message passing algorithms.", "startOffset": 16, "endOffset": 19}, {"referenceID": 19, "context": "Considering that larger dictionaries almost always increases performance [20], and in the limit one could use all the patches P as the dictionary, leading to a N -dimensional space and an N \u00d7N covariance matrix CP .", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "The Nystr\u00f6m method has been used to approximate large matrices for spectral clustering [8], and here enables us to explain the mechanism of dictionary learning.", "startOffset": 87, "endOffset": 90}, {"referenceID": 10, "context": "Recent research in the machine learning field, notably [11], supports the recent empirical observations in vision: first, it is known that uniformly sampling the columns of the matrix CP already works well in reconstruction, which explains the good performance of random patches in feature learning [16]; second, theoretical results [11, 23] have shown that clustering algorithms works particularly better than other methods as a data-driven way in finding good subsets to approximate the original matrix, justifying the use of clustering in the dictionary learning works.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "Recent research in the machine learning field, notably [11], supports the recent empirical observations in vision: first, it is known that uniformly sampling the columns of the matrix CP already works well in reconstruction, which explains the good performance of random patches in feature learning [16]; second, theoretical results [11, 23] have shown that clustering algorithms works particularly better than other methods as a data-driven way in finding good subsets to approximate the original matrix, justifying the use of clustering in the dictionary learning works.", "startOffset": 299, "endOffset": 303}, {"referenceID": 10, "context": "Recent research in the machine learning field, notably [11], supports the recent empirical observations in vision: first, it is known that uniformly sampling the columns of the matrix CP already works well in reconstruction, which explains the good performance of random patches in feature learning [16]; second, theoretical results [11, 23] have shown that clustering algorithms works particularly better than other methods as a data-driven way in finding good subsets to approximate the original matrix, justifying the use of clustering in the dictionary learning works.", "startOffset": 333, "endOffset": 341}, {"referenceID": 22, "context": "Recent research in the machine learning field, notably [11], supports the recent empirical observations in vision: first, it is known that uniformly sampling the columns of the matrix CP already works well in reconstruction, which explains the good performance of random patches in feature learning [16]; second, theoretical results [11, 23] have shown that clustering algorithms works particularly better than other methods as a data-driven way in finding good subsets to approximate the original matrix, justifying the use of clustering in the dictionary learning works.", "startOffset": 333, "endOffset": 341}, {"referenceID": 4, "context": "For STL, we followed [5] and resized them to 32 \u00d7 32.", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "edu/ \u0303acoates/stl10/, [3]", "startOffset": 22, "endOffset": 25}, {"referenceID": 17, "context": "To show the performance of the feature learning algorithm in the real-world image classification tasks, we tested the performance of our algorithm on the fine-grained classification task, using the 2011 Caltech-UCSD Birds dataset [18]5.", "startOffset": 230, "endOffset": 234}, {"referenceID": 6, "context": "Recent work on fine-grained classification usually focuses on the localization of parts [7, 24, 6], and still uses manually designed features.", "startOffset": 88, "endOffset": 98}, {"referenceID": 23, "context": "Recent work on fine-grained classification usually focuses on the localization of parts [7, 24, 6], and still uses manually designed features.", "startOffset": 88, "endOffset": 98}, {"referenceID": 5, "context": "Recent work on fine-grained classification usually focuses on the localization of parts [7, 24, 6], and still uses manually designed features.", "startOffset": 88, "endOffset": 98}, {"referenceID": 20, "context": "[21] proposed to use a template-based approach for more powerful features, but the approach may be difficult to scale as the number of templates grow larger6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "For the image pre-processing we followed the same setting as [24, 21] by cropping the images to be cen-", "startOffset": 61, "endOffset": 69}, {"referenceID": 20, "context": "For the image pre-processing we followed the same setting as [24, 21] by cropping the images to be cen-", "startOffset": 61, "endOffset": 69}, {"referenceID": 6, "context": "html 6Due to computation complexity, some early work such as [7, 21] do not scale up well, and only reported performance on subsets of the whole data [personal communication].", "startOffset": 61, "endOffset": 68}, {"referenceID": 20, "context": "html 6Due to computation complexity, some early work such as [7, 21] do not scale up well, and only reported performance on subsets of the whole data [personal communication].", "startOffset": 61, "endOffset": 68}, {"referenceID": 23, "context": "BoW SIFT Baseline [24] 18.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "60 Pose Pooling + linear SVM [24] 24.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "21 Pose Pooling + \u03c7 SVM [24] 28.", "startOffset": 24, "endOffset": 28}, {"referenceID": 4, "context": "18 K-means + linear SVM (as in [5]) 38.", "startOffset": 31, "endOffset": 34}, {"referenceID": 23, "context": "Our classification results, together with previous stateof-the-art baselines from [24], are reported in Table 2.", "startOffset": 82, "endOffset": 86}], "year": 2013, "abstractText": "Unsupervised dictionary learning has been a key component in state-of-the-art computer vision recognition architectures. While highly effective methods exist for patchbased dictionary learning, these methods may learn redundant features after the pooling stage in a given early vision architecture. In this paper, we offer a novel dictionary learning scheme to efficiently take into account the invariance of learned features after the spatial pooling stage. The algorithm is built on simple clustering, and thus enjoys efficiency and scalability. We discuss the underlying mechanism that justifies the use of clustering algorithms, and empirically show that the algorithm finds better dictionaries than patch-based methods with the same dictionary size.", "creator": "LaTeX with hyperref package"}}}