{"id": "1302.6677", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2013", "title": "Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization", "abstract": "noise is affected among considerable curse of dimensionality and quickly becomes intractable like the dimensionality algorithm requires overlapping peaks. we propose a coding algorithm involving, with high probability, gives a zero - length approximation of a smallest discrete integral exponential over an exponentially large set. if algorithm relies on solving about too small group of instances at naive discrete combinatorial dynamics problem subject to parameter generated parity restrictions used are binary hash function. despite an ornamental, engineers demonstrate that with my small number of map queries we can still produce small sparse function of discrete graphical games, something can in turn be used, for instance, for function computation or model construction.", "histories": [["v1", "Wed, 27 Feb 2013 06:45:28 GMT  (257kb,D)", "http://arxiv.org/abs/1302.6677v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["stefano ermon", "carla p gomes", "ashish sabharwal", "bart selman"], "accepted": true, "id": "1302.6677"}, "pdf": {"name": "1302.6677.pdf", "metadata": {"source": "CRF", "title": "Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization", "authors": ["Stefano Ermon", "Carla P. Gomes", "Ashish Sabharwal"], "emails": ["ermonste@cs.cornell.edu", "gomes@cs.cornell.edu", "ashish.sabharwal@us.ibm.com", "selman@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "Computing integrals in very high dimensional spaces is a fundamental and largely unsolved problem of scientific computation [4, 7, 24], with numerous applications ranging from machine learning and statistics to biology and physics. As the volume grows exponentially in the dimensionality, the problem quickly becomes computationally intractable, a phenomenon traditionally known as the curse of dimensionality [2].\nWe revisit the problem of approximately computing discrete integrals, namely weighted sums over (extremely large) sets of items. This problem encompasses several important probabilistic inference tasks, such as computing marginals or normalization constants (partition function) in graphical models, which are in turn the cornerstones for parameter and structure learning [32]. Although we focus on the discrete case, the continuous case can in principle also be addressed, as it can be approximated by numerical integration. There are two common approaches to approximate these large discrete sums: sampling and variational methods. Variational methods [17, 32], often inspired by statistical physics, are very fast but do not provide guarantees on the quality of the results. Since sampling and counting can be reduced to each other [16], approximate techniques\nar X\niv :1\n30 2.\n66 77\nv1 [\ncs .L\nG ]\nbased on sampling are quite popular, but they suffer from similar issues because the number of samples required to obtain a statistically reliable estimate often grows exponentially in the problem size. Among sampling techniques, Markov Chain Monte Carlo (MCMC) methods are asymptotically accurate, but guarantees for practical applications exist only in a limited number of cases (fast mixing chains) [16, 18]. They are therefore often used in an heuristic manner. In practice, their performance crucially depends on the choice of the proposal distributions, which often must be domain-specific and expert-designed [9, 21].\nWe introduce a randomized scheme that computes with high probability (1\u2212 \u03b4 for any desired \u03b4 > 0) an approximately correct estimate (within a factor of 1 + for any desired > 0) for general weighted sums defined over exponentially large sets of items, such as the set of all possible variable assignments in a discrete probabilistic graphical model. From a computational complexity perspective, the counting problem we consider is complete for the #P complexity class [28], a set of problems encapsulating the entire Polynomial Hierarchy and believed to be significantly harder than NP.\nThe key idea is to reduce this #P problem to a small number (polynomial in the dimensionality) of instances of a (NP-hard) combinatorial optimization problem defined on the same space and subject to randomly generated \u201cparity\u201d constraints. The rationale behind this approach is that although combinatorial optimization is intractable in the worst case, it has witnessed great success in the past 50 years in fields such as Mixed Integer Programming (MIP) and propositional Satisfiability Testing (SAT). Problems such as computing a Maximum a Posteriori (MAP) assignment, although NP-hard, can in practice often be approximated [25] or solved exactly fairly efficiently [22, 23]. In fact, modern solvers can exploit structure in real-world problems and prune large portions of the search space, often dramatically reducing the runtime. In contrast, in a #P counting problem such as computing a marginal probability, one needs to consider contributions of an exponentially large number of items.\nOur algorithm, called Weighted-Integrals-And-Sums-By-Hashing (WISH), relies on randomized hashing techniques to \u201cevenly cut\u201d a high dimensional space. Such hashing was introduced by Valiant and Vazirani [29] to study the relationship between the number of solutions and the hardness of a combinatorial search. These techniques were also applied by Gomes et al. [11, 12] to obtain bounds on the number of solutions for the SAT problem. Our work is more general in that it can handle general weighted sums, such as the ones arising in probabilistic inference for graphical models. Our work is also closely related to recent work by Hazan and Jaakkola [14], who obtain a lower bound on the partition function by taking suitable expectations of a combination of MAP queries over randomly perturbed models. We improve upon this in two crucial aspects, namely, our estimate is a constant factor approximation of the true partition function (while their bounds have no tightness guarantee), and we provide a concentration result showing that our bounds hold not just in expectation but with high probability with a polynomial number of MAP queries. Note that this is consistent with known complexity results regarding #P and BPPNP; see Remark 1 below.\nWe demonstrate the practical efficacy of the WISH algorithm in the context of computing the partition function of random Clique-structured Ising models, Grid Ising models with known ground truth, and a challenging combinatorial application (Sudoku puzzle) completely out of reach of techniques such as Mean Field and Belief Propagation. We also consider the Model Selection problem in graphical models, specifically in the context of hand-written digit recognition. We show that our \u201canytime\u201d and highly parallelizable algorithm can handle these problems at a level of accuracy and scale well beyond the current state of the art."}, {"heading": "2 Problem Statement and Assumptions", "text": "Let \u03a3 be a (large) set of items. Let w : \u03a3\u2192 R+ be a non-negative function that assigns a weight to each element of \u03a3. We wish to (approximately) compute the total weight of the set, defined as the following discrete integral or \u201cpartition function\u201d\nW = \u2211 \u03c3\u2208\u03a3 w(\u03c3) (1)\nWe assume w is given as input and that it can be compactly represented, for instance in a factored form as the product of conditional probabilities tables. Note however that our results are more general and do not rely on a factored representation.\nAssumption: We assume to have access to an optimization oracle that can solve the following constrained optimization problem\nmax \u03c3\u2208\u03a3 w(\u03c3)1{C}(\u03c3) (2)\nwhere 1{C} : \u03a3 \u2192 {0, 1} is an indicator function for a compactly represented subset C \u2286 \u03a3, i.e., 1{C}(\u03c3) = 1 iff \u03c3 \u2208 C. For concreteness, we discuss our setup and assumptions in the context probabilistic graphical models, which is our motivating application."}, {"heading": "2.1 Inference in Graphical Models", "text": "We consider a graphical model specified as a factor graph with N = |V | discrete random variables xi, i \u2208 V where xi \u2208 Xi. The global random vector x = {xs, s \u2208 V } takes value in the cartesian product X = X1 \u00d7 X2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 XN . We consider a probability distribution over x \u2208 X (called configurations) p(x) = 1Z \u220f \u03b1\u2208I \u03c8\u03b1({x}\u03b1) that factors into potentials or factors \u03c8\u03b1 : {x}\u03b1 7\u2192 R+, where I is an index set and {x}\u03b1 \u2286 V a subset of variables the factor \u03c8\u03b1 depends on, and Z is a normalization constant known as the partition function.\nGiven a graphical model, we let \u03a3 = X be the set of all possible configurations (variable assignments). Define a weight function w : X \u2192 R+ that assigns to each configuration a score proportional to its probability: w(x) = \u220f \u03b1\u2208I \u03c8\u03b1({x}\u03b1). Z may then be rewritten as\nZ = \u2211 x\u2208X w(x) = \u2211 x\u2208X \u220f \u03b1\u2208I \u03c8\u03b1({x}\u03b1) (3)\nComputing Z is typically intractable because it involves a sum over an exponential number of configurations, and is often the most challenging inference task for many families of graphical models. Computing Z is however needed for many inference and learning tasks, such as evaluating the likelihood of data for a given model, computing marginal probabilities, and parameter estimation [32].\nIn the context of graphical models inference, we assume to have access to an optimization oracle that can answer Maximum a Posteriori (MAP) queries, namely, solve the following constrained optimization problem\narg max x\u2208X\np(x | C)\nthat is, we can find the most likely state (and its weight) given some evidence C. This is a strong assumption because MAP inference is known to be an NP-hard problem in general. Notice however that computing Z is a #P-complete problem, a complexity class believed to be even harder than NP."}, {"heading": "2.2 Quadratures of Integrals", "text": "Suppose we are given a quadrature for a continuous (multidimensional) integral of a function f : Rn \u2192 R+ over a high dimensional set S \u2286 Rn\u222b\nS f(x)dx \u2248 \u2211 x\u2208X w(x) = W\nwhere X is some discretization of S (e.g., grid based), and w(x) approximates the integral of f(x) over the corresponding element of volume. In this case, we require a compact representation for w and access to an oracle able to optimize the discretized function, subject to arbitrary constraints. See, e.g., Figure 1.\nFor simplicity, in the following we will restrict ourselves to the binary case, i.e., \u03a3 = X = {0, 1}n. The general multinomial case where the sum is over X1\u00d7X2\u00d7\u00b7 \u00b7 \u00b7\u00d7XN can be transformed into the former case using a binary representation, requiring dlog2 |Xi|e bits (binary variables) per dimension i."}, {"heading": "3 Preliminaries", "text": "We review some results on the construction and properties of universal hash functions; cf. [10, 27]. A reader already familiar with these results may skip to the next section.\nDefinition 1. A family of functions H = {h : {0, 1}n \u2192 {0, 1}m} is pairwise independent if the following two conditions hold when H \u2190R H is a function chosen uniformly at random from H. 1) \u2200x \u2208 {0, 1}n, the random variable H(x) is uniformly distributed in {0, 1}m. 2) \u2200x1, x2 \u2208 {0, 1}n x1 6= x2, the random variables H(x1) and H(x2) are independent.\nA simple way to construct such a function is to think about the family H of all possible functions {0, 1}n \u2192 {0, 1}m. This is a family of not only pairwise independent but fully independent functions. However, each function requires m2n bits to be represented, and is thus impractical in the typical case where n is large. On the other hand, pairwise independent hash functions can be constructed and represented in a much more compact way as follows; see Appendix for a proof.\nProposition 1. Let A \u2208 {0, 1}m\u00d7n, b \u2208 {0, 1}m. The family H = {hA,b(x) : {0, 1}n \u2192 {0, 1}m} where hA,b(x) = Ax+ b mod 2 is a family of pairwise independent hash functions.\nThe space C = {x : hA,b(x) = p} has a nice geometric interpretation as the translated nullspace of the random matrix A. It is therefore a finite dimensional vector space, with operations defined on the field F(2) (arithmetic modulo 2). We will refer to constraints in the form Ax = b mod 2 as parity constraints, as they can be rewritten in terms of XORs operations as Ai1x1 \u2295Ai2x2 \u2295 \u00b7 \u00b7 \u00b7 \u2295Ainxn = bi.\n4 The WISH Algorithm\nWe start with the intuition behind our algorithm to approximate the value of W called WeightedIntegrals-And-Sums-By-Hashing (WISH).\nComputing W as defined in Equation (1) is challenging because the sum is defined over an exponentially large number of items, i.e., |\u03a3| = 2n when there are n binary variables. Let us define\nthe tail distribution of weights as G(u) , |{\u03c3 | w(\u03c3) \u2265 u}|. Note that G is a non-increasing step function, changing values at no more than 2n points. Then W may be rewritten as \u222b R+ G(u)du, i.e., the total area A under the G(u) vs. u curve. One way to approximate W is to (implicitly) divide this area A into either horizontal or vertical slices (see Figure 4), approximate the area in each slice, and sum up.\nSuppose we had an efficient procedure to estimate G(u) given any u. Then it is not hard to see that one could create enough slices by dividing up the x-axis, estimate G(u) at these points, and estimate the area A using quadrature. However, the natural way of doing this to any degree of accuracy would require a number of slices that grows at least logarithmically with the weight range on the x-axis, which is undesirable.\nAlternatively, one could split the y-axis, i.e., the G(u) value range [0, 2n], at geometrically growing values 1, 2, 4, \u00b7 \u00b7 \u00b7 , 2n, i.e., into bins of sizes 1, 1, 2, 4, \u00b7 \u00b7 \u00b7 , 2n\u22121. Let b0 \u2265 b1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 bn be the weights of the configurations at the split points. In other words, bi is the 2\ni-th quantile of the weight distribution. Unfortunately, despite the monotonicity of G(u), the area in the horizontal slice defined by each bin is difficult to bound, as bi and bi+1 could be arbitrarily far from each other. However, the area in the vertical slice defined by bi and bi+1 must be bounded between 2\ni(bi\u2212bi+1) and 2i+1(bi\u2212bi+1), i.e., within a factor of 2. Thus, summing over the lower bound for all such slices and the left-most slice, the total area A must be within a factor of 2 of \u2211n\u22121 i=0 2\ni(bi\u2212 bi+1) + 2nbn = b0 + \u2211n i=1 2\ni\u22121bi. Of course, we don\u2019t know bi. But if we could approximate each bi within a factor of p, we would get a 2p-approximation to the area A, i.e., to W .\nWISH provides an efficient way to realize this strategy, using a combination of randomized hash\nAlgorithm 1 WISH (w : \u03a3\u2192 R+, n = log2 |\u03a3|, \u03b4, \u03b1) T \u2190 \u2308\nln(1/\u03b4) \u03b1 lnn \u2309 for i = 0, \u00b7 \u00b7 \u00b7 , n do\nfor t = 1, \u00b7 \u00b7 \u00b7 , T do Sample hash function hiA,b : \u03a3\u2192 {0, 1}i, i.e. sample uniformly A \u2208 {0, 1}i\u00d7n, b \u2208 {0, 1}i\nwti \u2190 max\u03c3 w(\u03c3) subject to A\u03c3 = b mod 2 end for Mi \u2190 Median(w1i , \u00b7 \u00b7 \u00b7 , wTi )\nend for Return M0 + \u2211n\u22121 i=0 Mi+12 i\nfunctions and an optimization oracle to approximate the bi values with high probability. Note that this method allows us to compute the partition function W (or the area A) by estimating weights bi at n+ 1 carefully chosen points, which is \u201conly\u201d an optimization problem.\nThe key insight to compute the bi values is as follows. Suppose we apply to configurations in \u03a3 a randomly sampled pairwise independent hash function with 2m buckets and use an optimization oracle to compute the weight wm of a heaviest configuration in a fixed (arbitrary) bucket. If we repeat this process T times and consistently find that wm \u2265 w\u2217, then we can infer by the properties of hashing that at least 2m configurations (globally) are likely to have weight at least w\u2217. By the same token, if there were in fact at least 2m+c configurations of a heavier weight w\u0302 > w\u2217 for some c > 0, there is a good chance that the optimization oracle will find wm \u2265 w\u0302 and we would not underestimate the weight of the 2m-th heaviest configuration. As we will see shortly, this process, using pairwise independent hash functions to keep variance low, allows us to estimate bi accurately with only T = O(lnn) samples.\nThe pseudocode of WISH is shown as Algorithm 1. It is parameterized by the weight function w, the dimensionality n, a correctness parameter \u03b4 > 0, and a constant \u03b1 > 0. Notice that the algorithm requires solving only \u0398(n lnn ln 1/\u03b4) optimization instances (MAP inference) to compute a sum defined over 2n items. In the following section, we formally prove that the output is a constant factor approximation of W with probability at least 1 \u2212 \u03b4 (probability over the choice of hash functions). Figure 1 shows the working of the algorithm. As more and more random parity constraints are added in the outer loop of the algorithm (\u201clevels\u201d increasing from 1 to n), the configuration space is (pairwise-uniformly) thinned out and the optimization oracle selects the heaviest (in red) of the surviving configurations. The final output is a weighted sum over the median of T such modes obtained at each level.\nRemark 1. The parity constraints A\u03c3 = b mod 2 do not change the worst-case complexity of an NP-hard optimization problem. Our result is thus consistent with the fact that #P can be approximated in BPPNP, that is, one can approximately count the number of solutions with a randomized algorithm and a polynomial number of queries to an NP oracle [10].\nRemark 2. Although the parity constraints we impose are simple linear equations over a field, they can make the optimization harder. For instance, finding a configuration with the smallest Hamming weight satisfying a set of parity constraints is known to be NP-hard, i.e. equivalent to computing the minimum distance of a parity code [3, 30]. On the other hand, most low density parity check codes can be solved extremely fast in practice using heuristic methods such as message\npassing.\nRemark 3. Each of the optimization instances can be solved independently, allowing natural massive parallelization. We will also discuss how the algorithm can be used in an anytime fashion, and the implications of obtaining suboptimal solutions."}, {"heading": "5 Analysis", "text": "Since many configurations can have identical weight, it will help for the purposes of the analysis to fix, w.l.o.g., a weight-based ordering of the configurations, and a natural partition of the |\u03a3| = 2n configurations into n+ 1 bins that the ordering induces.\nDefinition 2. Fix an ordering \u03c3i, 1 \u2264 i \u2264 2n, of the configurations in \u03a3 such that for 1 \u2264 j < 2n, w(\u03c3j) \u2265 w(\u03c3j+1). For i \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , n}, define bi , w(\u03c32i). Define a special bin B , {\u03c31} and, for i \u2208 {0, 1, \u00b7 \u00b7 \u00b7 , n\u2212 1}, define bin Bi , {\u03c32i+1, \u03c32i+2, \u00b7 \u00b7 \u00b7 , \u03c32i+1}.\nNote that bin Bi has precisely 2 i configurations. Further, for all \u03c3 \u2208 Bi, it follows from the definition of the ordering that w(\u03c3) \u2208 [bi+1, bi]. This allows us to bound the sum of the weights of configurations in Bi (the \u201chorizontal\u201d slices) between 2 ibi+1 and 2 ibi."}, {"heading": "5.1 Estimating the Total Weight", "text": "Our main theorem is that Algorithm 1 provides a constant factor approximation to the partition function.\nTheorem 1. For any \u03b4 > 0 and positive constant \u03b1 \u2264 0.0042, Algorithm 1 makes \u0398(n lnn ln 1/\u03b4) MAP queries and, with probability at least (1\u2212\u03b4), outputs a 16-approximation of W = \u2211 \u03c3\u2208\u03a3w(\u03c3).\nThe proof relies on two intermediate results whose proofs may be found in the Appendix.\nLemma 1. Let Mi = Median(w 1 i , \u00b7 \u00b7 \u00b7 , wTi ) be defined as in Algorithm 1 and bi as in Definition 2. Then, for all c \u2265 2, there exists an \u03b1\u2217(c) > 0 such that for 0 < \u03b1 \u2264 \u03b1\u2217(c),\nPr [ Mi \u2208 [bmin{i+c,n}, bmax{i\u2212c,0}] ] \u2265 1\u2212 exp(\u2212\u03b1T )\nLemma 2. Let L\u2032 , b0 + \u2211n\u22121 i=0 bmin{i+c+1,n}2 i and U \u2032 , b0 + \u2211n\u22121 i=0 bmax{i+1\u2212c,0}2\ni. Then U \u2032 \u2264 22cL\u2032.\nProof of Theorem 1. It is clear from the pseudocode of Algorithm 1 that it makes \u0398(n lnn ln 1/\u03b4) MAP queries. For accuracy analysis, we can write W as:\nW , 2n\u2211 j=1 w(\u03c3j) = w(\u03c31) + n\u22121\u2211 i=0 \u2211 \u03c3\u2208Bi w(\u03c3)\n\u2208 [ b0 +\nn\u22121\u2211 i=0 bi+12 i, b0 + n\u22121\u2211 i=0 bi2 i\n] , [L,U ]\nNote that U \u2264 2L because 2L = 2b0 + \u2211n\u22121 i=0 bi+12 i+1 = 2b0 + \u2211n `=1 b`2 ` = b0 + \u2211n `=0 b`2 ` \u2265 U . Hence, if we had access to the true values of all bi, we could obtain a 2-approximation to W .\nWe do not know true bi values, but Lemma 1 shows that the Mi values computed by Algorithm 1 are sufficiently close to bi with high probability. Recall that Mi is the median of MAP values computed by adding i random parity constraints and repeating the process T times. Specifically, for c \u2265 2, it follows from Lemma 1 that for 0 < \u03b1 \u2264 \u03b1\u2217(c),\nPr [ n\u22c2 i=0 ( Mi \u2208 [bmin{i+c,n}, bmax{i\u2212c,0}] )] \u2265 1\u2212 n exp(\u2212\u03b1T ) \u2265 (1\u2212 \u03b4)\nfor T = log(1/\u03b4)\u03b1 log n, and M0 = b0. Thus, with probability at least (1\u2212 \u03b4) the output of Algorithm 1, M0 + \u2211n\u22121 i=0 Mi+12\ni, lies in the range:[ b0 +\nn\u22121\u2211 i=0 bmin{i+c+1,n}2 i, b0 + n\u22121\u2211 i=0 bmax{i+1\u2212c,0}2 i\n]\nLet us denote this range [L\u2032, U \u2032]. By monotonicity of bi, L \u2032 \u2264 L \u2264 U \u2264 U \u2032. Hence, W \u2208 [L\u2032, U \u2032].\nApplying Lemma 2, we have U \u2032 \u2264 22cL\u2032, which implies that with probability at least 1\u2212 \u03b4 the output of Algorithm 1 is a 22c approximation of W . For c = 2, observing that \u03b1\u2217(2) \u2265 0.0042 (see proof of Lemma 1), we obtain a 16-approximation for 0 < \u03b1 \u2264 0.0042."}, {"heading": "5.2 Estimating the Tail Distribution", "text": "We can also estimate the entire tail distribution of the weights, defined as G(u) , |{\u03c3 | w(\u03c3) \u2265 u}|.\nTheorem 2. Let Mi be defined as in Algorithm 1, u \u2208 R+, and q(u) be the maximum i such that \u2200j \u2208 {0, \u00b7 \u00b7 \u00b7 , i},Mj \u2265 u. Then, for any \u03b4 > 0, with probability \u2265 (1\u2212\u03b4), 2q(u) is an 8-approximation of G(u) computed using O(n lnn ln 1/\u03b4) MAP queries.\nWhile this is an interesting result in its own right, if the goal is to estimate the total weight W , then the scheme in Section 5.1, requiring a total of only \u0398(n lnn ln 1/\u03b4) MAP queries, is more efficient than first estimating the tail distribution for several values of u."}, {"heading": "5.3 Improving the Approximation Factor", "text": "Given a \u03ba-approximation algorithm such as Algorithm 1 and any > 0, we can design a (1 + )- approximation algorithm with the following construction. Let ` = log1+ \u03ba. Define a new set of configurations \u03a3` = \u03a3 \u00d7 \u03a3 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u03a3, and a new weight function w\u2032 : \u03a3` \u2192 R as w\u2032(\u03c31, \u00b7 \u00b7 \u00b7 , \u03c3`) = w(\u03c31)w(\u03c32) \u00b7 \u00b7 \u00b7w(\u03c3`).\nProposition 2. Let W\u0302 be a \u03ba-approximation of \u2211\n\u03c3\u2032\u2208\u03a3` w \u2032(\u03c3\u2032). Then W\u0302 1/` is a \u03ba1/`-approximation of \u2211\n\u03c3\u2208\u03a3w(\u03c3). To see why this holds, observe that W \u2032 = \u2211\n\u03c3\u2032\u2208\u03a3` w \u2032(\u03c3\u2032) = (\u2211 \u03c3\u2208\u03a3w(\u03c3) )` = W `. Since 1\u03baW\n\u2032 \u2264 W\u0302 \u2264 \u03baW \u2032, we obtain that W\u0302 1/` must be a \u03ba1/` = 1 + approximation of W .\nNote that this construction requires running Algorithm 1 on an enlarged problem with ` times more variables. Although the number of optimization queries grows polynomially with `, increasing the number of variables might significantly increase the runtime."}, {"heading": "5.4 Further Approximations", "text": "When the instances defined in the inner loop are not solved to optimality, Algorithm 1 still provides approximate lower bounds on W with high probability.\nTheorem 3. Let w\u0303ti be suboptimal solutions for the optimization problems in Algorithm 1, i.e., w\u0303ti \u2264 wti. Let W\u0303 be the output of Algorithm 1 with these suboptimal solutions. Then, for any \u03b4 > 0, with probability at least 1\u2212 \u03b4, W\u030316 \u2264W .\nFurther, if w\u0303ti \u2265 1Lw t i for some L > 0, then with probability at least 1 \u2212 \u03b4, W\u0303 is a 16L-\napproximation to W .\nThe output is always an approximate lower bound, even if the optimization is stopped early. The lower bound is monotonically non-decreasing over time, and is guaranteed to eventually reach within a constant factor of W . We thus have an anytime algorithm."}, {"heading": "6 Experimental Evaluation", "text": "We implemented WISH using the open source solver ToulBar2 [1] to solve the MAP inference problem. ToulBar2 is a complete solver (i.e., given enough time, it will find an optimal solution and provide an optimality certificate), and it was one of the winning algorithms in the UAI-2010 inference competition. We augmented ToulBar2 with the IBM ILOG CPLEX CP Optimizer 12.3 based techniques borrowed from Gomes et al. [13] to efficiently handle the random parity constraints. Specifically, the set of equations Ax = b mod 2 are linear equations over the field F(2) and thus allow for efficient propagation and domain filtering using Gaussian Elimination.\nFor our experiments, we run WISH in parallel using a compute cluster with 642 cores. We assign each optimization instance in the inner loop to one core, and finally process the results when all optimization instances have been solved or have reached a timeout.\nFor comparison, we consider Tree Reweighted Belief Propagation [31] which provides an upper bound on Z, Mean Field [32] which provides a lower bound, and Loopy Belief Propagation [20] which provides an estimate with no guarantees. We use the implementations of these algorithms available in the LibDAI library [19]."}, {"heading": "6.1 Provably Accurate Approximations", "text": "For our first experiment, we consider the problem of computing the partition function, Z (cf. Eqn. (3)), of random Clique-structured Ising models on n binary variables xi \u2208 {0, 1} for i \u2208 {1, \u00b7 \u00b7 \u00b7 , n}. The interaction between xi and xj is defined as \u03c8ij(xi, xj) = exp(\u2212wij) when xi 6= xj , and 1 otherwise, where wij is uniformly sampled from [0, w \u221a |i\u2212 j| ] and w is a parameter set to 0.2. We further inject some structure by introducing a closed chain of strong repulsive interactions uniformly sampled from [\u221210w, 0]. We consider models with n ranging from 10 to 60. These models have treewidth n and can be solved exactly (by brute force) only up to about n = 25 variables.\nFigure 4(a) shows the results using various methods for varying problem size. We also computed ground truth for n \u2264 25 by brute force enumeration. While other methods start to diverge from the ground truth at around n = 25, our estimate, as predicted by Theorem 1, remains very accurate, visually overlapping in the plot. The actual estimation error is much smaller than the worst-case factor of 16 guaranteed by Theorem 1, as in practice over- and under-estimation errors tend to\ncancel out. For n > 25 we don\u2019t have ground truth, but other methods fall well outside the provable interval provided by WISH, reported as an error bar that is very small compared to the magnitude of errors made by the other methods.\nAll optimization instances generated by WISH for n \u2264 60 were solved (in parallel) to optimality within a timeout of 8 hours, resulting in high confidence tight approximations of the partition function. We are not aware of any other practical method that can provide such guarantees for counting problems of this size, i.e., a weighted sum defined over 260 items."}, {"heading": "6.2 Anytime Usage with Suboptimal Solutions", "text": "Next, we investigate the quality of our results when not all of the optimization instances can be solved to optimality because of timeouts, so that the strong theoretical guarantees of Theorem 1 do not apply (although Theorem 3 still applies). We consider 10 \u00d7 10 binary Grid Ising models, for which ground truth can be computed using the junction tree method [32]. We use the same experimental setup as Hazan and Jaakkola [14], who also use random MAP queries to derive bounds (without a tightness guarantee) on the partition function. Specifically, we have n = 100 binary variables xi \u2208 {\u22121, 1} with interaction \u03c8ij(xi, xj) = exp(wijxixj). For the attractive case, we draw wij from [0, w]; for the mixed case, from [\u2212w,w]. The \u201clocal field\u201d is \u03c8ij(xi) = exp(fixi) where fi, the strength at site i, is sampled uniformly from [\u2212f, f ], where f is a parameter with value 0.1 or 1.0.\nFigure 3 reports the estimation error for the log-partition function, when using a timeout of 15 minutes. We see that WISH provides accurate estimates for a wide range of weights, often improving over all other methods. The slight performance drop of WISH for coupling strengths w \u2248 1 appears to occur because in that weight range the terms corresponding to i \u2248 n/2 parity constraints are the most significant in the output sum M0 + \u2211n\u22121 i=0 Mi+12\ni. Empirically, optimization instances with roughly n/2 parity constraints are often the hardest to solve, resulting in possibly a significant underestimation of the value of W = Z when a timeout occurs. We do not directly compare with the work of Hazan and Jaakkola [14] as we did not have access to their code. However, a visual look at their plots suggests that WISH would provide an improvement in accuracy, although with longer runtime."}, {"heading": "6.3 Hard Combinatorial Structures", "text": "An interesting and combinatorially challenging graphical model arises from Sudoku, which is a popular number-placement puzzle where the goal is to fill a 9\u00d7 9 grid (see Figure 4(b)) with digits from {1, \u00b7 \u00b7 \u00b7 , 9} so that the entries in each row, column, and 3\u00d73 block composing the grid, are all distinct. The puzzle can be encoded as a graphical model with 81 discrete variables with domain {1, \u00b7 \u00b7 \u00b7 , 9}, with potentials \u03c8\u03b1({x}\u03b1) = 1 if and only if all variables in {x}\u03b1 are different, and \u03b1 \u2208 I where I is an index set containing the subsets of variables in each row, column, and block. This defines a uniform probability distribution over all valid complete Sudoku grids (a non-valid grid has probability zero), and the normalization constant Zs equals the total number of valid grids. It is known that Zs = 6.671 \u00d7 1021. This number was computed exactly with a combination of computer enumeration and clever exploitation of properties of the symmetry group [8]. Here, we attempt to approximately compute this number using the general-purpose scheme WISH.\nFirst, following Felgenhauer and Jarvis [8], we simplify the problem by fixing the first block as in Figure 4(b), obtaining a new problem over 72 variables whose normalization constant is\nZ \u2032 = Zs/9! \u2248 254. Next, since we are dealing with a feasibility rather than optimization problem, we replace ToulBar2 with CryptoMiniSAT [26], a SAT solver designed for unweighted cryptographic problems and which natively supports parity constraints. We observed that WISH can consistently find solutions (60% of the times) after adding 52 random parity constraints, while for 53 constraints the success rate drops below 0.5, at 45%. Therefore Mi = 1 in Algorithm 1 for i \u2264 52 and there should thus be at least 252 \u00b7 9! \u2248 1.634 \u00d7 1021 solutions to the Sudoku puzzle. Although Theorem 1 cannot be applied due to timeouts for larger values of i, this estimate is clearly very close to the known true count. In contrast, the simple \u201clocal reasoning\u201d done by variational methods is not powerful enough to find even a single solution. Mean Field and Belief Propagation report an estimated solution count of exp(\u2212237.921) and exp(\u2212119.307), resp., on a relaxed problem where violating a constraint gives a penalty exp(\u221210)."}, {"heading": "6.4 Model Selection", "text": "Many inference and learning tasks require computing the normalization constant of graphical models. For instance, it is needed to evaluate the likelihood of observed data for a given model. This is necessary for Model Selection, i.e., to rank candidate models, or to trigger early stopping during training when the likelihood of a validation set starts to decrease, in order to avoid overfitting [6].\nWe train Restricted Boltzmann Machines (RBM) [15] using Contrastive Divergence (CD) [5, 33] on MNIST hand-written digits dataset. In an RBM there is a layer of nh hidden binary variables h = h1, \u00b7 \u00b7 \u00b7 , hnh and a layer of nv binary visible units v = v1, \u00b7 \u00b7 \u00b7 , vnv . The joint probability distribution is given by P (h, v) = 1Z exp(b\n\u2032v + c\u2032h + h\u2032Wv). We use nh = 50 hidden units and nv = 196 visible units. We learn the parameters b, c,W using CD-k for k \u2208 {1, 10, 15}, where k denotes the number of Gibbs sampling steps used in the inference phase, with 15 training epochs and minibatches of size 20.\nFigure 4(c) depicts confabulations (samples generated with Gibbs sampling) from the three learned models. To evaluate the loglikelihood of the data and determine which model is the best, one needs to compute Z. We use WISH to estimate this quantity, with a timeout of 10 minutes, and then rank the models according to the average loglikelihood of the data. The scores we obtain are \u221241.70,\u221240.35,\u221240.01 for k = 1, 10, 15, respectively (larger scores means higher likelihood). In this case ToulBar2 was not able to prove optimality for all instances, so only Theorem 3 applies to these results. Although we do not have ground truth, it can be seen that the ranking of the models is consistent with what visually appears closer to a large collection of hand-written digits in Figure 4(c). Note that k = 1 is clearly not a good representative, because of the highly uneven distribution of digit occurrences. The ranking of WISH is also consistent with the fact that using more Gibbs sampling steps in the inference phase should provide better gradient estimates and therefore a better learned model. In contrast, Mean Field results in scores \u221235.47,\u221236.08,\u221236.84, resp., and would thus rank the models in reverse order of what is visually the most representative order."}, {"heading": "7 Conclusion", "text": "We introduced WISH, a randomized algorithm that, with high probability, gives a constant-factor approximation of a general discrete integral defined over an exponentially large set. WISH reduces the intractable counting problem to a small number of instances of a combinatorial optimization problem subject to parity constraints used as a hash function. In the context of graphical models, we showed how to approximately compute the normalization constant, or partition function, using a small number of MAP queries. Using state-of-the-art combinatorial optimization tools, we are thus able to provide discrete integral or partition function estimates with approximation guarantees at a scale that could till now be handled only heuristically. Finally, our method is a massively parallelizable and anytime algorithm which can also be stopped early to obtain empirically accurate estimates that provide lower bounds with a high probability."}, {"heading": "Acknowledgments", "text": "Supported by NSF Expeditions in Computing grant on Computational Sustainability #0832782 and NSF Computing Research Infrastructure grant #1059284."}, {"heading": "A Appendix: Proofs", "text": "Proof of Proposition 1. Immediately follows from Lemma 3.\nLemma 3 (pairwise independent hash functions construction). Let a \u2208 {0, 1}n, b \u2208 {0, 1}. Then the family H = {ha,b(x) : {0, 1}n \u2192 {0, 1}} where ha,b(x) = a \u00b7 x+ b mod 2 is a family of pairwise independent hash functions. The function ha,b(x) can be alternatively rewritten in terms of XORs operations \u2295, i.e. ha,b(x) = a1x1 \u2295 a2x2 \u2295 \u00b7 \u00b7 \u00b7 \u2295 anxn \u2295 b.\nProof. Uniformity is clear because it is the sum of uniform Bernoulli random variables over the field F(2) (arithmetic modulo 2). For pairwise independence, given any two configurations x1, x2 \u2208 {0, 1}n, consider the sets of indexes S1 = {i : x1(i) = 1}, S2 = {i : x2(i) = 1}. Then\nH(x1) = \u2211\ni\u2208S1\u2229S2\nai \u2295 \u2211\ni\u2208S1\\S2\nai \u2295 b\n= R(S1 \u2229 S2)\u2295R(S1 \\ S2)\u2295 b H(x2) = R(S1 \u2229 S2)\u2295R(S2 \\ S1)\u2295 b\nNote that R(S1 \u2229 S2), R(S1 \\ S2), R(S2 \\ S1) and b are independent as they depend on disjoint subsets of independent variables. When x1 6= x2, this implies that (H(x1), H(x2)) takes each value in {0, 1}2 with probability 1/4.\nAs pairwise independent random variables are fundamental tools for derandomization of algorithms, more complicated constructions based larger finite fields generated by a prime power F(qk) where q is a prime number are known [27]. These constructions require a smaller number of random bits as input, and would therefore reduce the variance of our algorithm (which is deterministic except for the randomized hash function use).\nProof of Lemma 1. The cases where i + c > n or i \u2212 c < 0 are obvious. For the other cases, let\u2019s define the set of the 2j heaviest configurations as in Definition 2:\nXj = {\u03c31, \u03c32, \u00b7 \u00b7 \u00b7 , \u03c32j}\nDefine the following random variable\nSj(h i A,b) , \u2211 \u03c3\u2208Xj 1{A\u03c3=b mod 2}\nwhich gives the number of elements of Xj satisfying i random parity constraints. The randomness is over the choice of A and b, which are uniformly sampled in {0, 1}i\u00d7n and {0, 1}i respectively. By Proposition 1, hiA,b : \u03a3\u2192 {0, 1}i is sampled from a family of pairwise independent hash functions. Therefore, from the uniformity property in Definition 1, for any \u03c3 the random variable 1{A\u03c3=b mod 2} is Bernoulli with probability 1/2i. By linearity of expectation,\nE[Sj(h i A,b)] = |Xj | 2i = 2j 2i\nFurther, from the pairwise independence property in Definition 1,\nV ar[Sj(h i A,b)] = \u2211 \u03c3\u2208Xj V ar [ 1{A\u03c3=b mod 2} ] = 2j\n2i\n( 1\u2212 1\n2i ) Applying Chebychev Inequality, we get that for any k > 0,\nPr [\u2223\u2223\u2223\u2223Sj(hiA,b)\u2212 2j2i \u2223\u2223\u2223\u2223 > k \u221a 2j 2i ( 1\u2212 1 2i )] \u2264 1 k2\nRecall the definition of the random variable wi = max\u03c3 w(\u03c3) subject to A\u03c3 = b mod 2 (the randomness is over the choice of A and b). Then\nPr[wi \u2265 bj ] = Pr[wi \u2265 w(\u03c32j )] \u2265 Pr[Sj(hiA,b) \u2265 1]\nwhich is the probability that at least one configuration from Xj \u201csurvives\u201d after adding i parity constraints.\nTo ensure that the probability bound 1/k2 provided by Chebychev Inequality is smaller than a 1/2, we need k > \u221a 2. We use k = 3/2 for the rest of this proof, exploiting the following simple observations which hold for k = 3/2 and any c \u2265 2:\nk \u221a 2c \u2264 2c \u2212 1\nk \u221a 2\u2212c \u2264 1\u2212 2\u2212c\nFor j = i+ c and k and c as above, we have that\nPr[wi \u2265 bi+c] \u2265 Pr[Si+c(hiA,b) \u2265 1] \u2265 Pr [ |Si+c(hi)\u2212 2c| \u2264 2c \u2212 1 ] \u2265\nPr [ |Si+c(hi)\u2212 2c| \u2264 k \u221a 2c ] \u2265\nPr [\u2223\u2223Si+c(hiA,b)\u2212 2c\u2223\u2223 \u2264 k \u221a 2c (\n1\u2212 1 2i\n)] \u2265\n1\u2212 1 k2 = 5/9 > 1/2\nSimilarly, for j = i\u2212 c and k and c as above, we have Pr[wi \u2264 bi\u2212c] \u2265 5/9 > 1/2. Finally, using Chernoff inequality (since w1i , \u00b7 \u00b7 \u00b7 , wTi are i.i.d. realizations of wi)\nPr [Mi \u2264 bi\u2212c] \u2265 1\u2212 exp(\u2212\u03b1\u2032(c)T ) (4) Pr [Mi \u2265 bi+c] \u2265 1\u2212 exp(\u2212\u03b1\u2032(c)T ) (5)\nwhere \u03b1\u2032(2) = 2(5/9\u2212 1/2)2, which gives the desired result\nPr [bi+c \u2264Mi \u2264 bi\u2212c] \u2265 1\u2212 2 exp(\u03b1\u2032(c)T ) = 1\u2212 exp(\u2212\u03b1\u2217(c)T )\nwhere \u03b1\u2217(2) = ln 2\u03b1\u2032(2) = 2(5/9\u2212 1/2)2 ln 2 > 0.0042\nProof of Lemma 2. Observe that we may rewrite L\u2032 as follows:\nL\u2032 = b0 + n\u22121\u2211\ni=n\u2212c\u22121 bn2 i + n\u2212c\u22122\u2211 i=0 bi+c+12 i =\nb0 + n\u22121\u2211\ni=n\u2212c\u22121 bn2 i + n\u22121\u2211 j=c+1 bj2 j\u2212c\u22121\nSimilarly,\nU \u2032 = b0 + c\u22121\u2211 i=0 b02 i + n\u22121\u2211 i=c bi+1\u2212c2 i =\nb0 + c\u22121\u2211 i=0 b02 i + n\u2212c\u2211 j=1 bj2 j+c\u22121 = 2cb0 + 2 c n\u2212c\u2211 j=1 bj2 j\u22121 =\n2cb0 + 2 c  c\u2211 j=1 bj2 j\u22121 + n\u2212c\u2211 j=c+1 bj2 j\u22121  \u2264 2cb0 + 2 c\n c\u2211 j=1 b02 j\u22121 + n\u2212c\u2211 j=c+1 bj2 j\u22121  = 22cb0 + 2\n2c n\u2212c\u2211 j=c+1 bj2 j\u22121\u2212c \u2264\n22c b0 + n\u22121\u2211 i=n\u2212c\u22121 bn2 i + n\u22121\u2211 j=c+1 bj2 j\u2212c\u22121  = 22cL\u2032 This finishes the proof.\nProof of Theorem 2. As in the proof of Lemma 1, define the random variable\nSu(h i A,b) , \u2211 \u03c3\u2208{\u03c3|w(\u03c3)\u2265u} 1{A\u03c3=b mod 2}\nthat gives the number of configurations with weight at least u satisfying i random parity constraints. Then for i \u2264 blogG(u)c\u2212 c \u2264 logG(u)\u2212 c using Chebychev and Chernoff inequalities as in Lemma 1\nPr [Mi \u2265 u] \u2265 1\u2212 exp(\u2212\u03b1\u2032T )\nFor i \u2265 dlogG(u)e+ c \u2265 logG(u) + c, using Chebychev and Chernoff inequalities as in Lemma 1\nPr[Mi < u] \u2265 1\u2212 exp(\u2212\u03b1\u2032T )\nTherefore,\nPr\n[ 1\n2c+1 2q(u) \u2264 G(u) \u2264 2c+12q(u)\n] \u2265\nPr blog2G(u)c\u2212c\u22c2 i=0 (Mi \u2265 u) \u22c2( Mdlog2G(u)e+c < u ) \u2265\n1\u2212 n exp(\u2212\u03b1\u2032T ) \u2265 1\u2212 \u03b4\nThis finishes the proof.\nProof of Theorem 3. If w\u0303ti \u2264 wti , from Theorem 1 with probability at least 1 \u2212 \u03b4 we have W\u0303 \u2264 M0 + \u2211n\u22121 i=0 Mi+12 i \u2264 UB\u2032. Since UB\u2032 22c \u2264 LB\u2032 \u2264W \u2264 UB\u2032, it follows that with probability at least 1\u2212 \u03b4, W\u0303 22c \u2264W .\nIf wti \u2265 w\u0303ti \u2265 1Lw t i , then from Theorem 1 with probability at least 1\u2212 \u03b4 the output is 1LLB \u2032 \u2264 W\u0303 \u2264 UB\u2032, and LB\u2032 \u2264W \u2264 UB\u2032."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>Integration is affected by the curse of dimensionality and quickly becomes intractable as<lb>the dimensionality of the problem grows. We propose a randomized algorithm that, with high<lb>probability, gives a constant-factor approximation of a general discrete integral defined over an<lb>exponentially large set. This algorithm relies on solving only a small number of instances of a<lb>discrete combinatorial optimization problem subject to randomly generated parity constraints<lb>used as a hash function. As an application, we demonstrate that with a small number of MAP<lb>queries we can efficiently approximate the partition function of discrete graphical models, which<lb>can in turn be used, for instance, for marginal computation or model selection.", "creator": "LaTeX with hyperref package"}}}