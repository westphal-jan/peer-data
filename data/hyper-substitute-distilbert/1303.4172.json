{"id": "1303.4172", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2013", "title": "Margins, Shrinkage, and Boosting", "abstract": "sensitivity analysis shows that adaboost and lucas block group can correctly approximate maximum margin parameters entirely by designing step wide limits with a fixed small constant. in different way, whereas the unscaled task size is an optimal choice, these errors accept guarantees for friedman's empirically extended \" shrinkage \" procedure for rounding values ( friedman, 1987 ). moments are also predicted after a derivation indicating other step sizes, raising the importance that naive regularized utility searches reject random depth guarantees. the results hold down the exponential inequality and similar losses, adding notably the logistic loss.", "histories": [["v1", "Mon, 18 Mar 2013 07:33:29 GMT  (383kb,D)", "http://arxiv.org/abs/1303.4172v1", "To appear, ICML 2013"]], "COMMENTS": "To appear, ICML 2013", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["matus telgarsky"], "accepted": true, "id": "1303.4172"}, "pdf": {"name": "1303.4172.pdf", "metadata": {"source": "META", "title": "Margins, Shrinkage, and Boosting", "authors": ["Matus Telgarsky"], "emails": ["mtelgars@cs.ucsd.edu"], "sections": [{"heading": "1. Introduction", "text": "AdaBoost and related boosting algorithms greedily aggregate many simple predictors into a single accurate predictor (Freund & Schapire, 1997). One explanation for the efficacy of boosting is that it not only seeks aggregates with low empirical risk, but moreover that it prefers good margins, which leads to improved generalization (Schapire et al., 1997). Since AdaBoost does not attain maximum margins on general instances, a push was made to develop methods which carry such a guarantee (Ra\u0308tsch & Warmuth, 2005; Shalev-Shwartz & Singer, 2008; Rudin et al., 2007).\nThis work shows that margin maximization may be achieved by scaling back the step size. The intuition for this result is simple (cf. Figure 1): when (equivalently) considered as steps in a coordinate descent procedure, the iterates, depicted as a path, approximate the path of constrained optima (for all possible choices of constraint). By scaling back the step size, the optimal path is more finely approximated.\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nAs there have been many proposed step sizes for these methods, this manuscript will study four separate choices, deriving improved bounds for the more regularized choices. While it has been shown before that regularized step sizes have good generalization and asymptotically good margins (Zhang & Yu, 2005), this manuscript shows that straightforward step choices achieve these margins at rates matching explicitly margin-maximizing boosting methods.\nThis practice of scaling back weights was proposed by Friedman (2000, Section 5), who referred to it as a shrinkage scheme (Copas, 1983). This scheme is effective, and adopted in practice (see for instance Bradski (2000, Class CvGBTrees) and Pedregosa et al. (2011, Class GradientBoostingClassifier)); the purpose of this manuscript is to provide theoretical guarantees."}, {"heading": "1.1. Outline", "text": "After summarizing the main content, this introduction closes with connections to related work; thereafter, Section 2 recalls the core algorithm, defines the class of loss functions, and provides the four step sizes.\nar X\niv :1\n30 3.\n41 72\nv1 [\ncs .L\nG ]\n1 8\nM ar\n2 01\n3\nAs boosting is generally studied under the weak learning assumption (a separability condition), the dominant study in this manuscript is also under the condition of separability, and appears in Section 3. The first step is to show that shrinkage does not drastically change the rate of convergence of the empirical risk under these methods. The more involved study is on the topic of margins, and the final subsection compares these bounds to those of other methods.\nGeneral (potentially nonseparable) instances are discussed in Section 4. Once again, the first step is a convergence rate guarantee, which again matches those without shrinkage. This section also demonstrates that, under a certain decomposition of boosting problems, the algorithm is still achieving margins on a separable sub-component of the problem.\nThe manuscript closes with some discussion in Section 5. All proofs are relegated to appendices (in the supplementary material)."}, {"heading": "1.2. Related Work", "text": "Three close works proposed regularized line searches for boosting. First, Friedman (2000) gave the same scheme as is considered here (albeit with only the optimal line search); follow-up work has been mainly empirical, and the questions of convergence rates and margin guarantees do not appear in the literature. Second, Zhang & Yu (2005) also considered regularized line searches, but with a goal of proving consistency; margin maximization is proved as a byproduct, and the analogous results here hold under fewer conditions, and come with rates for the more stringent step sizes. A third work, due to Ra\u0308tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates.\nAs mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5.\nThe primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of Ra\u0308tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5,\nBibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)).\nAs is standard in the above works, this manuscript is only concerned with convergence of empirical quantities.\nIn order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk presented here are thus trivial, but included since they did not appear explicitly in the literature. It is worth mentioning that these methods produce bad constants when applied to the logistic loss; unfortunately, previous work also suffers in this case (for instance, the work of Collins et al. (2002) provided only convergence of empirical risk, and not rates)."}, {"heading": "2. Algorithms and Notation", "text": "First some basic notation. Let {(xi, yi)}mi=1 \u2286 X \u00d7 {\u22121,+1} denote an m-point sample. Take H0 to denote the collection of weak learners; it is assumed that h \u2208 H0 satisfies h(X ) \u2286 [\u22121,+1], and that H0 has some form of bounded complexity, meaning specifically that the set of vectors {(h(x1), . . . , h(xm)) : h \u2208 H0} is finite; this for instance holds if there is a fixed finite set of outputs from H0, e.g., each h is binary. Consequently, let H = {hj}nj=1 denote the effective finite set of hypothesis, and collect the responses on the sample into a matrix A \u2208 [\u22121,+1]m\u00d7n with Aij = \u2212yihj(xi).\nBoosting finds a weighting \u03bb \u2208 Rn of H, which corresponds to a regressor x 7\u2192 \u2211n j=1 \u03bbjhj(x), and thus a binary classification rule after thresholding. The corresponding (l1 minimum) margin M(A\u03bb) over the sam-\nple with respect to \u03bb is\nM(A\u03bb) := min i\u2208[m] \u2212e>i A\u03bb \u2016\u03bb\u20161 = min i\u2208[m]\nyi \u2211n j=1 \u03bbjhj(xi)\n\u2016\u03bb\u20161 .\nLet \u03b3 denote the best (largest) achievable margin; equivalently (Shalev-Shwartz & Singer, 2008), \u03b3 is the weak learning rate (which justifies the choice of l1 margins):\n\u03b3 := max \u03bb\u2208Rn \u2016\u03bb\u20161=1 M(A\u03bb) = max \u03bb\u2208Rn \u2016\u03bb\u20161=1 min i\u2208[m] \u2212e>i A\u03bb\n= min w\u2208\u2206m max j\u2208[n] \u2223\u2223\u2223\u2223\u2223 m\u2211 i=1 wiyihj(xi) \u2223\u2223\u2223\u2223\u2223 = minw\u2208\u2206m \u2016A>w\u2016\u221e. When \u03b3 > 0, the instance is considered separable; classically, this condition is termed the weak learning assumption (Kearns & Valiant, 1989; Freund & Schapire, 1997)."}, {"heading": "2.1. The Family of Loss Functions", "text": "The class L will effectively be \u201cfunctions similar to the exponential loss\u201d. Some of this is for analytic convenience, but some of this appears to be essential, and thus a bit of motivation is appropriate.\nOptimization problems typically take advantage of curvature (e.g., strong convexity) to establish a convergence rate. The analysis here instead uses a relative form of curvature: it suffices for, say, the Hessian to not be too small relative to the gap between the current primal objective value and the primal optimum. In this sense, the exponential loss is ideal, as it is a fixed point of the differentiation operator.\nDefinition 2.1. Given a loss ` : R \u2192 R++ (where R++ denotes positive reals), let C`(z) \u2265 1 (with potentially C`(z) =\u221e) be the tightest positive constant so that, for every x \u2264 z: C`(z)\u22121 \u2264 exp(x)/`(i)(x) \u2264 C`(z) for i \u2208 {0, 1, 2} (the zeroth, first, and second derivatives). \u2666\nSince C`(z) is defined to be the tightest constant, it follows that y \u2264 z implies C`(y) \u2264 C`(z).\nFrom here, the class of loss functions may be defined.\nDefinition 2.2. Let L contain all functions ` : R \u2192 R+ which are twice continuously differentiable, strictly convex, and have C`(z) < \u221e for all z \u2208 R. Additionally, if limz\u2192\u2212\u221e C`(z) = 1, then ` \u2208 L\u221e. \u2666\nCrucially, the two classes L and L\u221e both contain the exponential and logistic losses.\nProposition 2.3. {x 7\u2192 exp(x), x 7\u2192 ln(1 + exp(x))} \u2286 L\u221e.\nAlgorithm 1 boost. Input: loss `, matrix A \u2208 [\u22121,+1]m\u00d7n. Output: Weighting sequence {\u03bbt}\u221et=0.\nInitialize \u03bb0 := 0. for t = 1, 2, . . . : do\nChoose column (weak learner)\njt := arg max j\n|\u2207L(A\u03bbt\u22121)>Aej |.\nSet descent direction vt \u2208 {\u00b1ejt}, whereby\n\u2207L(A\u03bbt\u22121)>Avt = \u2212\u2016\u2207L(A\u03bbt\u22121)>A\u2016\u221e.\nFind \u03b1t via line search. Update \u03bbt := \u03bbt\u22121 + \u03b1tvt.\nend for\nOne way to interpret this is to say \u201cin the limit, logistic loss is the same as exponential loss\u201d. Unfortunately, this treatment of the logistic loss ends up being quite unfair, in the sense that the bounds are not accurately representative of the behavior of the algorithm (see Section 3.3). It is, however, unclear how to better deal with the logistic loss.\nLastly, the relevant primal objective function may be defined.\nDefinition 2.4. Given ` \u2208 L and vector z \u2208 Rm, define L(z) := m\u22121 \u2211m i=1 `(zi), whereby the primal optimization problem for boosting is to minimize L(A\u03bb) over the domain Rn. For convenience, define L\u0304A := inf\u03bb\u2208Rn L(A\u03bb). \u2666"}, {"heading": "2.2. Algorithm", "text": "The algorithm appears in Algorithm 1. Before defining the various step sizes, two more definitions are in order.\nDefinition 2.5. For every t, define \u03b3t := \u2016A>\u2207L(A\u03bbt\u22121)\u2016\u221e/\u2016\u2207L(A\u03bbt\u22121)\u20161. (Note that 1 \u2265 \u03b3t \u2265 \u03b3.) \u2666\nAdditionally, rather than depending on parameter C`(z) for a carefully chosen z, the following definition suffices.\nDefinition 2.6. For t \u2265 1, define Ct := C`(` \u22121(mL(A\u03bbt\u22121))). \u2666\nThe significance of Ct is as follows. Since the algorithm itself is coordinate descent, and moreover since every line search will be shown to guarantee descent, every candidate \u03bb considered in round t will satisfy L(A\u03bb) \u2264 L(A\u03bbt\u22121); thus, for every i \u2208 [m],\n`(e>i A\u03bb) \u2264 mL(A\u03bb) \u2264 mL(A\u03bbt\u22121), and so e>i A\u03bb \u2264 `\u22121(mL(A\u03bbt\u22121)), where the inverse is well-defined since ` is a bijection between R and R++ by definition of L (otherwise C`(z) =\u221e).\nThe collection of step sizes considered here are as follows, in order of least to most aggressive. Throughout these step sizes, \u03bd \u2208 (0, 1] will denote a shrinkage parameter.\nQuadratic upper bound. Rather than performing an optimal line search, i.e., rather than minimizing \u03b1 7\u2192 L(A(\u03bbt\u22121 + \u03b1vt)), a quadratic upper bound of this univariate function may be minimized, which has a closed form solution (cf. the proof of Lemma 3.2). In particular, define the step\nsize \u03b1Qt (\u03bd) := \u03bd\u03b3t/C 4 t . This choice is pleasant algorithmically only when Ct is easy to compute (for instance, Ct = 1 for the exponential loss). In general, however, it is useful as an analytic aid, since most step sizes here can be lower bounded by it. This step size was introduced by Telgarsky (2012, Appendix D.3).\nWolfe. The Wolfe line search is a standard tool from nonlinear optimization (Nocedal & Wright, 2006, chapter 3), and for convex problems it may be implemented with binary search (Telgarsky, 2012, Appendix D.1). More precisely, this choice is a set of step sizes \u03b1Wt (\u03bd) satisfying two conditions. First, the step is explicitly disallowed from being too large:\nL(A(\u03bbt\u22121 + \u03b1vt)) \u2264 L(A\u03bbt\u22121)\u2212 \u03b1(1\u2212 \u03bd/2)\u2016A>\u2207L(A\u03bbt\u22121)\u2016\u221e.\n(2.7)\nSecond, the step should be approximately optimal (in terms of the line search problem):\n\u2207L(A(\u03bbt\u22121 + \u03b1vt))>Avt \u2265 \u2212(1\u2212 \u03bd/4)\u2016\u2207L(A\u03bbt\u22121)>A\u2016\u221e. (2.8)\n(Requiring the reverse inequality (with the right hand side negated) yields the Strong Wolfe Conditions, which are not necessary here.) In contrast to \u03b1Qt (\u03bd), the Wolfe step does not require knowledge of Ct, but will yield nearly identical bounds; in fact, computation of the Wolfe step requires only function evaluations, gradient evaluations, and knowledge of \u03bd, A,vt, \u03bbt.\nAdaBoost. Following the scheme of AdaBoost, define \u03b1At (\u03bd) := \u03bd 2 ln( 1+\u03b3t 1\u2212\u03b3t ), where convention is\nfollowed and \u03b3t = 1 is ignored. Unfortunately,\neven though \u03b3t is loss-dependent, this step will only yield rates with the exponential loss. However, it will be instrumental in analyzing the fully optimizing step size, presented next. This step size was introduced with the original presentation of AdaBoost (Freund & Schapire, 1997), though the analysis here will rather follow a slightly later treatment (Schapire & Singer, 1999).\nOptimal. Let \u03b1Ot (1) be a minimizer to \u03b1 7\u2192 L(A(\u03bbt\u22121 +\u03b1vt)), which, as in the case of \u03b1At (\u03bd), is assumed to exist. For \u03bd \u2208 (0, 1), set \u03b1Ot (\u03bd) = \u03bd\u03b1Ot (1). When A is binary and ` = exp, \u03b1 O t (\u03bd) =\n\u03b1At (\u03bd), though in general this is not true. This step size (with shrinkage!) was suggested by Friedman (2000) for use with the logistic loss.\nTo close, note that \u03b1Qt (\u03bd) and \u03b1 O t (\u03bd) have a simple relationship. Proposition 2.9. If A \u2208 [\u22121,+1]m\u00d7n and ` \u2208 L, then \u03b1Qt (\u03bd) \u2264 \u03b1Ot (\u03bd)."}, {"heading": "3. The Separable Case", "text": "This section considers the setting of separability, meaning the weak learning assumption is satisfied (\u03b3 > 0). The three subsections respectively provide convergence rates in empirical risk, basic margin guarantees, and close with some discussion."}, {"heading": "3.1. Convergence of Empirical Risk", "text": "The basic guarantee is that all of these line search methods, for any loss in L and with arbitrary shrinkage, exhibit the same basic convergence rate as AdaBoost.\nTheorem 3.1. Let boosting matrix A with corresponding \u03b3 > 0 and shrinkage parameter \u03bd \u2208 (0, 1] be given. Given any ` \u2208 L, any > 0, and iterates {\u03bbt}t\u22650 consistent with \u03b1Qt (\u03bd), \u03b1 W t (\u03bd), \u03b1 O t (\u03bd), or \u03b1 A t (\u03bd) with ` = exp, then O( 1\u03b32 ln( 1 )) iterations suffice to ensure L(A\u03bbt) \u2264 , where the O(\u00b7) suppresses terms depending on C1 and \u03bd.\nThe proof is in the appendix, but a basic discussion will appear here for each step size. The proofs are straightforward, as they should be: convergence analyses typically prove a bound for one step, and then iterate the bound. As such, taking 1/\u03bd steps which are \u03bd-factor as long as the original should do at least as well as the original (which is indeed the exhibited trade-off).\nFirst is the quadratic upper bound, which implicitly gives an upper bound for the optimal step as well. The\nproof follows a standard scheme from convex optimization of lower and upper bounding a potential function based on the gradient; the specifics use the relative curvature properties of L, and follow the analysis of Telgarsky (2012, Section 6.1, Appendix D).\nLemma 3.2. Consider the setting of Theorem 3.1, but with each step size \u03b1t satisfying \u03b1 Q t (\u03bd) \u2264 \u03b1t \u2264 \u03b1Ot (\u03bd). Then for any t > t0 \u2265 0,\nL(A\u03bbt) \u2264 L(A\u03bbt0) exp\n( \u2212\u03bd(2\u2212 \u03bd)\n2C6t0+1 t\u2211 i=t0+1 \u03b32i\n) .\nThe reason for the parameter t0 is to mitigate the horrendous dependence on Ct0 , which is potentially very large. In particular, consider ` \u2208 L\u221e, meaning limz\u2192\u2212\u221e C`(z) = 1. C1 may be quite bad, but convergence still happens. It follows that Ct \u2192 1, and thus, by choosing some large t0, the bound provides that perhaps there is an initially slow convergence phase, but eventually it is very fast. That is to stay, Lemma 3.2 may be applied multiple times to give a more refined picture of the convergence, particularly in the case that ` \u2208 L\u221e, which guarantees the constants are eventually near 1.\nNext, the Wolfe step size has a similar guarantee (and the analysis once again heavily relies on techniques due to Telgarsky (2012, 6.1, Appendix D)).\nLemma 3.3. Consider the setting of Theorem 3.1, but with \u03b1t \u2208 \u03b1Wt (\u03bd). Then for any t > t0 \u2265 0,\nL(A\u03bbt) \u2264 L(A\u03bbt0) exp\n( \u2212\u03bd(2\u2212 \u03bd)\n8C6t0+1 t\u2211 i=t0+1 \u03b32i\n) .\n(The denominator blows up by a factor 4 due to extra halves introduced into the Wolfe conditions, specifically to adjust around the natural Wolfe parameters being within (0, 1) and not (0, 1].)\nLastly, consider \u03b1At (\u03bd). As in the statement of Theorem 3.1, this step size is only shown to work with the exponential loss. This may be an artifact of the analysis, however, which perhaps follows too closely the treatment of Schapire & Singer (1999), which only considers the exponential loss; for instance, a slightly modified step size can be used to show convergence with the logistic loss (Collins et al., 2002).\nLemma 3.4. Consider the setting of Theorem 3.1, but with \u03b1t \u2208 \u03b1At (\u03bd) Then for any t > t0 \u2265 0,\nL(A\u03bbt) \u2264 L(A\u03bbt0) t\u220f\ni=t0+1\nC3i\n( 1\u2212 \u03bd\n2 \u03b32i\n) ."}, {"heading": "3.2. Margin Maximization", "text": "The margin rates here follow a simple pattern: the more regularized the step size, the faster the convergence to a good margin. While no lower bounds are presented, this is an interesting and intuitive correspondence (in particular, consistent with Figure 1). Unfortunately, the unconstrained step sizes only have asymptotic convergence (no rates), so the umbrella theorem for this subsection is also asymptotic.\nTheorem 3.5. Let boosting matrix A with corresponding \u03b3 > 0 and shrinkage parameter \u03bd \u2208 (0, 1] be given. Given any ` \u2208 L\u221e, any > 0, and iterates {\u03bbt}t\u22650 consistent with \u03b1Qt (\u03bd), \u03b1 W t (\u03bd), \u03b1 A t (\u03bd) with ` = exp, or \u03b1Ot (\u03bd) with binary A \u2208 {\u22121,+1}m\u00d7n, then there exists T so that for all M(A\u03bbt) \u2265 \u03b3 \u2212 for all t \u2265 T .\nIn contrast with the convergence rates of empirical risk (e.g., Theorem 3.1), the condition ` \u2208 L\u221e is made, rather than simply ` \u2208 L (with improved constants when ` \u2208 L\u221e). This can be interpreted to say: the analysis depends heavily upon the structure of the exponential loss. While this condition is likely unnecessary, on the other extreme it is important for the loss to be strictly convex; if for instance the hinge loss is used, then minimization can stop at any point achieving zero error, in particular at one with poor margin properties.\nReturning to task, the quadratic upper bound comes first.\nLemma 3.6. Suppose the setting of Theorem 3.5, but with \u03b1t = \u03b1 Q t (\u03bd). Additionally let t > t0 \u2265 0 be given with t \u2265 2C 6 1 ln(m)\n\u03b32\u03bd(2\u2212\u03bd) (whereby all margins are nonnega-\ntive by Lemma 3.2). Then\nM(A\u03bbt) \u2265 \u03b3 (\n2\u2212 \u03bd 2C6t0+1\n) \u2212 ln(c0)\nt\u03bd\u03b3 ,\nwhere\nc0 :=max { 1,mCt0+1L(A\u03bbt0) exp ( \u03bd(2\u2212 \u03bd) 2C6t0+1 t0\u2211 i=1 \u03b32i )} .\nTo interpret this bound, first consider the simplifying case that ` = exp, whereby Ct = 1 for all t. Additionally taking t0 = 0, it follows that c0 = m, and the bound is simply\nM(A\u03bbt) \u2265 \u03b3 (\n1\u2212 \u03bd 2\n) \u2212 ln(m)\nt\u03bd\u03b3 ;\nin particular,M(A\u03bbt)\u2192 \u03b3 as \u03bd \u2192 0 and t\u03bd \u2192\u221e. For some other ` \u2208 L\u221e, the denominator term C6t0+1 also\npresents an obstacle to establishing margin maximization; but note that t0 \u2192\u221e suffices, since it combines with ` \u2208 L\u221e via Theorem 3.1 to grant Ct0 \u2192 1.\nThe proof of Lemma 3.6 does not have to work too hard, as the step size appears prominently in the convergence rate bound (cf. Lemma 3.2). As will be discussed in Section 3.3, the rate is nearly ideal.\nThe Wolfe search exhibits a similar rate.\nLemma 3.7. Suppose the setting of Theorem 3.5, but with \u03b1t = \u03b1 W t (\u03bd). Additionally let t > t0 \u2265 0 be given with t \u2265 8C 6 1 ln(m)\n\u03b32\u03bd(2\u2212\u03bd) (whereby all margins are nonnega-\ntive by Lemma 3.3). Then M(A\u03bbt) \u2265 \u03b3 (\n2\u2212 \u03bd 2C2t0+1\n) \u2212 4C1 ln(c0)\nt\u03bd\u03b3 ,\nwhere\nc0 :=max { 1,mCt0+1L(A\u03bbt0) exp ( (2\u2212 \u03bd)\u03b3 2C2t0+1 t0+1\u2211 i=1 \u03b1i )} .\nThe preceding two step choices, \u03b1Qt (\u03bd) and \u03b1 W t (\u03bd), had explicit regularization: the first stops as soon as the steepest matching quadratic turns upward, and the second refuses to go beyond a boundary (cf. eq. (2.7)).\nOn the other hand, the choices \u03b1At (\u03bd) and \u03b1 O t (\u03bd) are only constrained by the data. Recall that one way to derive \u03b1At (\u03bd) is in the case of binary A \u2208 {\u22121,+1}m\u00d7n and ` = exp, where it is crucial that each weak learner is wrong on at least one example: this prevents steps from being too large. The techniques in the following proof follow those used in the margin bounds for regular AdaBoost (and are asymptotic there as well). It is worth noting that not only is this bound the worst, but the analysis is the trickiest.\nLemma 3.8. Consider the setting of Theorem 3.5, but now ` = exp and \u03b1t = \u03b1 A t (\u03bd). Then for any \u2208 (0, \u03b3], there exists T so that M(A\u03bbt) \u2265 \u03b3 \u2212 for all t \u2265 T .\nSimilarly, \u03b1Ot (\u03bd) is only implicitly regularized. The condition that A \u2208 {\u22121,+1}m\u00d7n prevents the negative, constraining examples from having too little influence.\nLemma 3.9. Consider the setting of Theorem 3.5, but now ` = exp, the matrix A is binary, and \u03b1t = \u03b1 O t (\u03bd). Then for any > 0, there exists T so that M(A\u03bbt) \u2265 \u03b3 \u2212 for all t \u2265 T .\nThe above lemmas together provide the proof of Theorem 3.5. But before closing, note that while the results for the unconstrained step sizes were only asymptotic, it is possible to derive a rate for the more modest goal of margins closer to \u03b3/3.\nProposition 3.10. Consider the setting of Theorem 3.5, but specialized with ` = exp and \u03b1t = \u03b1 A t (\u03bd). Let a target margin value \u03b8 < \u03b3 be given. If \u03b8 < \u03b3/(1 + \u03b3) (e.g., it suffices that \u03b8 < \u03b3/2), then\n1\nm m\u2211 i=1 1 [ \u2212eiA\u03bbt \u2016\u03bbt\u20161 < \u03b8 ] \u2264 exp ( \u2212t\u03bd(\u03b32 \u2212 \u03b8\u03b3(2 + \u03b3)) 2 ) .\nIn particular, if \u03b8 < \u03b3/(2 + \u03b3) (e.g., it suffices that \u03b8 < \u03b3/3) and t > 2 ln(m)/(\u03bd(\u03b32 \u2212 \u03b8\u03b3(2 + \u03b3))), then M(A\u03bbt) \u2265 \u03b8.\nNote, of course, that this bound has the severe analytic artifact of demonstrating no benefit of shrinkage!"}, {"heading": "3.3. Discussion", "text": "To get a sense of these margin bounds, first recall Freund\u2019s lower bound on boosting methods in the separable case, which states that \u2126( 1\u03b32 ln( 1 \u03c4 )) iterations are necessary to achieve classification error \u03c4 > 0 (Freund, 1995, Section 2). Setting \u03c4 = 1/m, it follows that \u2126(ln(m)/\u03b32) iterations are necessary to achieve any nonnegative margin. By comparison, with \u03b1Wt (\u03bd) and ` = exp, just 12 ln(m)/\u03b32 iterations with choice \u03bd = 1/2 suffice to reach margin \u03b3/2 (by Lemma 3.7). More generally, \u03b1Wt (\u03bd) reaches margin \u03b3(1 \u2212 \u03bd) with 8 ln(m)/(\u03bd\u03b3)2 iterations (if step size \u03b1Qt (\u03bd) is used, then 2 ln(m)/(\u03bd\u03b3)2 iterations suffice by Lemma 3.6).\nThe explicit margin-maximizing method of ShalevShwartz & Singer (2008) requires t \u2265 32 ln(m)/ 2 iterations to achieve margin \u03b3 \u2212 , where \u2208 (0, \u03b3). By comparison, converting the above multiplicative bound into an additive bound, step size \u03b1Wt ( /\u03b3) requires 8 ln(m)/ 2 iterations. While this bound is slightly better, the comparison is not fair, since \u03b1Wt ( /\u03b3) requires knowledge of \u03b3 in the choice of shrinkage parameter \u03bd. (Pessimistically taking \u03bd = gives an additive guarantee, but with a poor rate.) Consequently, it can be reasoned that shrinkage methods achieve excellent margins, but are best suited for multiplicative guarantees.\nAnother question is how accurately the bounds presented here depict the methods provided. As a brief sanity check, the methods may be run on a problem instance where AdaBoost demonstrably does not achieve maximum margins. The particular instance tested here is a binary matrix A \u2208 {\u22121,+1}8\u00d78 due to Rudin et al. (2004, Theorem 7); recall that AdaBoost, in the present notation (with A binary), corresponds to ` = exp and step size \u03b1At (1) = \u03b1 O t (1) (no shrinkage). Two plots are provided.\n1. Figure 2 is a sanity check, showing that ` = exp\nand \u03b1At (1) = \u03b1 O t (1) may not achieve maximum margins, but shrinkage overcomes this.\n2. Figure 3 demonstrates that the Wolfe search (with ` = exp) is indeed effective, but demanding higher accuracy comes at a price.\nThese plots will be discussed further in Section 5. Additional tests with this matrix demonstrated that the method of Shalev-Shwartz & Singer (2008) indeed performs a tiny bit worse than the Wolfe search, but of course one example is not terribly indicative. Perhaps most importantly, a test with the logistic loss showed that the bound is loose: the logistic loss performs well, and does not suffer a startup cost as indicated by the bounds."}, {"heading": "4. The General Case", "text": "The last technical contribution of this manuscript is to briefly consider the general case (which is potentially nonseparable). Similarly to the separable case, this section will establish convergence rates for empirical risk, margin guarantees, and briefly discuss the connection to existing margin maximizing methods. But first, it is necessary to discuss the structure of the general case, and in particular to develop what margins mean without separability.\nThis section hinges upon the following decomposition of a boosting instance. This decomposition partitions a boosting instance, specifically its examples {(xi, yi)}mi=1, into a hard subset H(A), and an easy subset H(A)c. The easy subset alone is separable, and thus margins will be measured there. Although the analysis will rely heavily on properties of this decom-\nposition due to Telgarsky (2012), the decomposition itself has appeared, with various guarantees, in numerous places (Goldreich & Levin, 1989; Impagliazzo, 1995; Mukherjee et al., 2011). The notation H(A) reflects the fact that this structure has no relation to the choice of ` \u2208 L. Definition 4.1. (Cf. Telgarsky (2012, Definition 5.1, 5.7).) Given a boosting problem encoded in a matrix A \u2208 Rm\u00d7n, a set of examples (rows) H(A) \u2286 [m] is a hard core for A (and the corresponding boosting problem) if it satisfies the following properties.\n\u2022 There exists a weighting \u03bb\u0302 \u2208 Rn with e>i A\u03bb\u0302 < 0 for i \u2208 H(A)c and e>i A\u03bb\u0302 = 0 for i \u2208 H(A).\n\u2022 Every weighting \u03bb \u2208 Rn with e>i A\u03bb < 0 for some i \u2208 H(A) also has e>k A\u03bb > 0 for some k \u2208 H(A).\nAdditionally, define a row-wise partition of A into matrices A0, A+, where A+ has the examples in H(A), and A0 has the examples in H(A) c. \u2666\nThe second property provides that H(A) is difficult: positive margins on some examples force negative margins on others. On the other hand, the complement H(A)c is easy, and moreover can be solved without affecting H(A).\nProposition 4.2. (Cf. Telgarsky (2012, Proposition 5.8, Theorem 5.9).) For any A \u2208 Rm\u00d7n, a hard core H(A) always exists, and is unique.\nWith the decomposition in place, the aforementioned guarantees may be stated. The first, as in the separable case, is convergence of empirical risk. There is hardly anything to do here; the groundwork from Sec-\ntion 3 can be plugged directly into existing techniques to generate this theorem (Telgarsky, 2012, Section 6).\nTheorem 4.3. Let general boosting matrix A be given (i.e., potentially \u03b3 = 0), along with shrinkage parameter \u03bd \u2208 (0, 1], any ` \u2208 L, and target suboptimality > 0. Suppose step sizes {\u03b1t}t\u22650 are consistent with \u03b1Qt (\u03bd), \u03b1 W t (\u03bd), \u03b1 O t (\u03bd), or \u03b1 A t (\u03bd) with ` = exp and A binary. Then O( 1 ) iterations suffice to reach suboptimality > 0.\nIf the instance is either separable (i.e., \u03b3 > 0 as in Section 3) or attains its minimizer (i.e., |H(A)| = m (Telgarsky, 2012, Theorem 5.5)), then the rate improves to O(ln( 1 )).\nLastly come the margin guarantees. As stated above, H(A)c, considered alone, is separable; note furthermore that the definition of hard core provides the existence of a weighting \u03bb\u0302. which has positive margins over H(A)c, but abstains entirely over H(A). Consequently, an approximate minimizer to L(A\u00b7) can always add in a scaling of \u03bb\u0302 and improve its empirical risk while simultaneously improving margins over H(A)c. Consequently, it is natural to expect the methods here to achieve positive margins over H(A)c. Note that the following result only shows that some positive margins are attained, and neither assert some sense under which they are maximal, nor does it provide rates.\nTheorem 4.4. Let general boosting matrix A be given with 1 \u2264 |H(A)| \u2264 m \u2212 1 (i.e., the problem is neither separable, nor is the minimizer attainable). Let shrinkage parameter \u03bd \u2208 (0, 1] and any ` \u2208 L\u221e be given. Suppose step sizes {\u03b1t}t\u22650 are consistent with \u03b1Qt (\u03bd), \u03b1 W t (\u03bd), \u03b1 O t (\u03bd) with ` = exp and binary A, or \u03b1At (\u03bd) with ` = exp and binary A., Then there exists \u03b3\u0302 > 0 so that every example off the hard core (i.e., i \u2208 H(A)c) has margin at least \u03b3\u0302 for all large t.\nTo close, consider once again the comparison to explicit margin maximizing boosting methods as presented by Shalev-Shwartz & Singer (2008). There is no point in discussing the specific method discussed in Section 3.3, whose optimal objective value is exactly \u03b3, which in this case is zero, and the method may happily quit without iterating. Indeed, a primary contribution of Shalev-Shwartz & Singer (2008) is not only to address this issue, but show how the same general boosting scheme can be instantiated for the aforementioned method, as well as methods with tolerance to nonseparability.\nIndeed, consider the \u201csoft-margin\u201d boosting method (Shalev-Shwartz & Singer, 2008), originally due to Warmuth et al. (2006), which, roughly speaking, has\na parameter controlling how many examples to give up on. This is in contrast to the methods here, which not only have a fixed data-dependant structure they try less hard on (the hard core H(A)), but moreover the particular margins achieved over the hard core are determined by the loss function ` \u2208 L. It is of course worth mentioning that the margin analysis in the nonseparable case here is by comparison very incomplete, providing no rates and not even identifying exactly what positive margins are attained."}, {"heading": "5. Discussion", "text": "This manuscript immediately raises a number of questions. Perhaps foremost is the general question of the impact of margins on the efficacy of boosting. Although margins certainly provide an intuitive theory, it is still unclear how much they directly correlate with good algorithms (Reyzin & Schapire, 2006).\nNext, the bounds for the logistic loss are not tight. As there do not appear to be any more forgiving analyses of the logistic loss, the natural question is whether there are new techniques which provide a better characterization.\nLastly, Figure 2 shows a threshold effect: shrinkage 1 does not lead to the right margin, but 1/2 and smaller suffices to reach the maximum margin. (Indeed, experimentation reveals the threshold to be roughly 0.92.) It should be possible to clarify this behavior from the perspective of dynamical systems: smaller steps dodge bad attractors (Rudin et al., 2004; 2007)."}, {"heading": "Acknowledgements", "text": "The author thanks Daniel Hsu and the ICML reviewers for helpful comments and discussions. The author is also deeply indebted to Robert Schapire for numerous discussions, insight, and for suggesting study of the unconstrained step size (at the time, guarantees were only in place for the other choices!). This work was graciously supported by the NSF under grant IIS0713540."}, {"heading": "A. Deferred Material from Section 2", "text": "Proof of Proposition 2.3. There is nothing to show for exp, so consider `(x) = ln(1 + exp(x)), let z \u2208 R be given, and let x \u2264 z be arbitrary.\nConcavity grants ln(1 + exp(x)) \u2264 exp(x). The lower bound can be checked in two stages. First, if x \u2264 min{\u22121, z}, a Taylor expansion gives\nln(1 + ex) \u2265 ex \u2212 sup \u03be\u2208R\n1\n2(1 + \u03be)2 e2x\n\u2265 ex (\n1\u2212 min{e z, e\u22121} 2\n) .\nOn the other hand, if \u22121 \u2264 x \u2264 z, then ex \u2264 ez ln(1 + e\u22121)/ ln(1 + e\u22121) \u2264 ez ln(1 + ex)/ ln(1 + e\u22121).\nNext, `\u2032(x) = ex/(1 + ex), so `\u2032(x) \u2264 ex \u2264 `\u2032(x)(1 + ez). Similarly, `\u2032\u2032(x) = ex/(1 + ex)2, so `\u2032\u2032(x) \u2264 ex \u2264 `\u2032\u2032(x)(1 + ez)2.\nThe following lemma (and its proof) derive \u03b1Qt (\u03bd), establishes \u03b1Qt (\u03bd) \u2264 \u03b1Ot (\u03bd), and gives the basic improvement due to one step satisfying \u03b1 \u2208 [\u03b1Qt (\u03bd), \u03b1Ot (\u03bd)].\nLemma A.1. Let boosting matrix A, shrinkage parameter \u03bd \u2208 (0, 1], and any ` \u2208 L be given. For any iteration t, it holds that \u03b1Qt+1(\u03bd) \u2264 \u03b1Ot+1(\u03bd). Furthermore, any step \u03b1 \u2208 [\u03b1Qt+1(\u03bd), \u03b1Ot+1(\u03bd)] satisfies\nL(A(\u03bbt + \u03b1vt+1)) \u2264 L(A\u03bbt) exp ( \u2212 \u03bd(2\u2212 \u03bd)\u03b32t+1\n2C6t+1\n) .\nProof. This analysis follows a scheme laid out by Telgarsky (2012, Appendix D.3). Let t denote any fixed iteration, and I denote the (possibly unbounded) interval\nI := {\u03b1 \u2265 0 : L(A(\u03bbt + \u03b1vt+1)) \u2264 L(A\u03bbt)} ;\nby continuity of L and choice of vt+1, I is nonempty, with nonempty interior. By second order Taylor ex-\npansion, every \u03b1 \u2208 I satisfies\nL(A(\u03bbt + \u03b1vt+1)) \u2264 L(A\u03bbt) + \u03b1v>t+1A>\u2207L(A\u03bbt)\n+ sup r\u2208I\n\u03b12\n2 v>t+1A >\u22072L(A(\u03bbt + rvt+1))Avt+1\n\u2264 L(A\u03bbt)\u2212 \u03b1\u2016A>\u2207L(A\u03bbt)\u2016\u221e\n+ \u03b12\n2 sup r\u2208I\n1\nm m\u2211 i=1 `\u2032\u2032(e>i A(\u03bbt + rvt+1))A 2 ijt+1\n\u2264 L(A\u03bbt)\u2212 \u03b1\u2016A>\u2207L(A\u03bbt)\u2016\u221e\n+ C2t+1\u03b1 2\n2 sup r\u2208I\n1\nm m\u2211 i=1 `(e>i A(\u03bbt + rvt+1))A 2 ijt+1\n= L(A\u03bbt)\u2212 \u03b1\u2016A>\u2207L(A\u03bbt)\u2016\u221e + C2t+1\u03b1 2\n2 L(A\u03bbt)\n\u2264 L(A\u03bbt)\u2212 \u03b1\u2016A>\u2207L(A\u03bbt)\u2016\u221e + C4t+1\u03b1 2\n2 \u2016\u2207L(A\u03bbt)\u20161,\nwhich made use of `\u2032\u2032 \u2264 Ct+1 exp \u2264 C2t+1` along I, ` \u2264 Ct+1 exp \u2264 C2t+1`\u2032 along I, |Aij | \u2264 1 (since elements of H are bounded in this way), and the definition of I (specifically r = 0 is the worst choice for r \u2208 I). This final expression is a quadratic, whose minimizer must lie within I (since its second derivative exceeds that of L along this interval). Differentiating and setting to zero, the minimizer is\nI 3 \u2016A >\u2207L(A\u03bbt)\u2016\u221e\nC4t+1\u2016\u2207L(A\u03bbt)\u20161 = \u03b3t+1 C4t+1 = \u03b1Qt+1(1).\nThis provides a derivation of the step \u03b1Qt+1(\u03bd), and also shows \u03b1Qt+1(1) \u2264 \u03b1Ot+1(1). Plugging \u03b1 Q t+1(\u03bd) in for \u03b1 in the above quadratic upper bound,\nL(A(\u03bbt + \u03b1vt+1)) \u2264 L(A\u03bbt)\u2212 \u03bd(2\u2212 \u03bd)\u03b32t+1\u2016\u2207L(A\u03bbt)\u20161\n2C4t+1 \u2264 L(A\u03bbt) ( 1\u2212 \u03bd(2\u2212 \u03bd)\u03b32t+1\n2C6t+1 ) \u2264 L(A\u03bbt) exp ( \u2212 \u03bd(2\u2212 \u03bd)\u03b32t+1\n2C6t+1\n) .\nProof of Proposition 2.9. This is the first part of Lemma A.1."}, {"heading": "B. Deferred Material from Section 3", "text": "B.1. Deferred Material from Section 3.1\nProof of Lemma 3.2. By Lemma A.1, for any t, L(A(\u03bbt + \u03b1vt+1)) \u2264 L(A\u03bbt) exp ( \u2212 \u03bd(2\u2212 \u03bd)\u03b32t+1\n2C6t+1\n) .\nNow let t0 \u2264 t be given as in the desired statement, apply this bound t \u2212 t0 times, and use the fact that Ct+1 \u2264 Ct.\nProof of Lemma 3.3. Let t denote any fixed iteration. Substituting c1 = 1 \u2212 \u03bd/2, c2 = 1 \u2212 \u03bd/4, and \u03b7 = C2t+1 in a nearly identical guarantee for the Wolfe line search (Telgarsky, 2012, Proposition D.6) (where \u03b7 is simply the biggest ratio between ` and `\u2032\u2032 in the current sublevel set) provides\nL(A(\u03bbt + \u03b1vt+1))\n\u2264 L(A\u03bbt)\u2212 (1\u2212 \u03bd/2)(\u03bd/4)\u2016A>\u2207L(A\u03bbt)\u20162\u221e\n2C2t+1L(A\u03bbt) \u2264 L(A\u03bbt) ( 1\u2212 \u03bd(2\u2212 \u03bd)\u03b32t+1\n8C6t+1 ) \u2264 L(A\u03bbt) exp ( \u2212 \u03bd(2\u2212 \u03bd)\u03b32t+1\n8C6t+1\n) .\nGiven t0 \u2264 t, applying this bound t \u2212 t0 times and using Ct+1 \u2264 Ct gives the result.\nNext, instead of directly proving Lemma 3.4, a more general lemma is given first, which will be useful later.\nLemma B.1. Consider the setting of Theorem 3.1, except now each step size \u03b1i satisfies\n\u03b1Ai (\u03bd)\u2212 \u03c4 \u2264 \u03b1i \u2264 \u03b1Ai (\u03bd) + \u03c4\nfor some \u03c4 > 0. Then, given t \u2265 t0,\nL(A\u03bbt+1) \u2264 L(A\u03bbt0)\n\u00b7 t+1\u220f\ni=t0+1\ne\u03c4C4i 2 (1\u2212 \u03b32i )\u03bd/2((1 + \u03b3i)1\u2212\u03bd + (1\u2212 \u03b3i)1\u2212\u03bd)\n\u2264 L(A\u03bbt0) t+1\u220f\ni=t0+1\ne\u03c4C4i 2 (1\u2212 \u03b32i )\u03bd/2.\nProof. Fix an iteration t, and set wi = ` \u2032(e>i A\u03bbt) and\nW = \u2211 i wi \u2264 mC2t+1L(A\u03bbt). By convexity of exp(\u00b7),\nL(A\u03bbt+1)\n\u2264 Ct+1 m m\u2211 i=1 exp(e>i A\u03bbt+1)\n\u2264 C2t+1 m\n( W\nW ) m\u2211 i=1 wi exp ( 1 + e>i Avt+1 2 \u03b1t+1\n+ 1\u2212 e>i Avt+1\n2 (\u2212\u03b1t+1)\n)\n\u2264 C2t+1 m W\n( 1\u2212 \u03b3t+1\n2 exp(\u03b1t+1) + 1 + \u03b3t+1 2\nexp(\u2212\u03b1t+1) )\n\u2264 e\u03c4C4t+1\n2 L(A\u03bbt)(1\u2212 \u03b32t+1)\u03bd/2 \u00b7 ( (1\u2212 \u03b3t+1)1\u2212\u03bd + (1 + \u03b3t+1)1\u2212\u03bd ) .\nTo simplify this expression, note that (\u00b7)1\u2212\u03bd is a concave function, and thus\n(1\u2212 \u03b3t+1)1\u2212\u03bd\n2 +\n(1 + \u03b3t+1) 1\u2212\u03bd\n2 \u2264 (\n1\u2212 \u03b3t+1 2 + 1 + \u03b3t+1 2 )1\u2212\u03bd = 1.\nTo finish, given t \u2265 t0, the result follows by t \u2212 t0 applications of these bounds.\nProof of Lemma 3.4. This follows by taking the second bound in Lemma B.1 with the choice \u03c4 = 0.\nProof of Theorem 3.1. The result follows from Lemma 3.2, Lemma 3.3, and Lemma 3.4 with the choice t0 = 0 and using Ct+1 \u2264 Ct.\nB.2. Deferred Material from Section 3.2\nProof of Lemma 3.6. To start, note that\n\u2016\u03bbt+1\u20161 = \u2225\u2225\u2225\u2225\u2225\u03bd t+1\u2211 i=1 vi\u03b3iC \u22124 i \u2225\u2225\u2225\u2225\u2225 1 \u2264 \u03bdC\u221241 t+1\u2211 i=1 \u03b3i \u2264 \u03bd t+1\u2211 i=1 \u03b3i.\nBy the form of L and the optimization guarantee in Lemma 3.2,\nmax k\u2208[m]\nexp(e>k A\u03bbt+1)\n\u2264 m\u2211 i=1 exp(e>i A\u03bbt+1) \u2264 mCt+1L(A\u03bbt+1)\n\u2264 mCt0+1L(A\u03bbt0) exp\n( \u2212\u03bd(2\u2212 \u03bd)\n2C6t0+1 t+1\u2211 i=t0+1 \u03b32i\n)\n= mCt0+1L(A\u03bbt0) exp ( \u03bd(2\u2212 \u03bd) 2C6t0+1 t0\u2211 i=1 \u03b32i )\n\u00b7 exp ( \u2212\u03bd(2\u2212 \u03bd)\n2C6t0+1 t+1\u2211 i=1 \u03b32i\n)\n\u2264 c0 exp ( \u2212 (2\u2212 \u03bd)\n2C6t0+1 t\u2211 i=1 \u03bd\u03b32i\n) ,\nwhere c0 is as in the statement. Since ln(\u00b7) is increasing, it follows that\nmax k\u2208[m] e>k A\u03bbt+1 \u2264 \u2212 (2\u2212 \u03bd) 2C6t0+1 t\u2211 i=1 \u03bd\u03b32i + ln(c0).\nUsing the above bound on \u2016\u03bbt\u20161, since t\u03b3 \u2264 \u2211t+1 i=1 \u03b3t, and \u2212e>k A\u03bbt+1 is nonnegative by the lower bound on t,\nmin k\u2208[m] \u2212e>k A\u03bbt+1 \u2016\u03bbt+1\u20161 \u2265 min k\u2208[m] \u2212e>k A\u03bbt+1 \u03bd \u2211t+1 i=1 \u03b3i\n\u2265 \u03b3 (\n2\u2212 \u03bd 2C6t0+1 ) \u2212 ln(c0) \u03bd \u2211t+1 i=1 \u03b3i\n\u2265 \u03b3 (\n2\u2212 \u03bd 2C6t0+1\n) \u2212 ln(c0)\n(t+ 1)\u03bd\u03b3 .\nProof of Lemma 3.7. Any step size \u03b1t+1 satisfying the Wolfe conditions will have lower bound\n\u03b1t+1 \u2265 (1\u2212 (1\u2212 \u03bd/4))\u2016A>L(A\u03bbt)\u2016\u221e\nC2t L(A\u03bbt)\n\u2265 (1\u2212 (1\u2212 \u03bd/4))\u2016A >L(A\u03bbt)\u2016\u221e C4t \u2016\u2207L(A\u03bbt)\u20161 = \u03bd\u03b3t+1 4C4t+1 \u2265 \u03bd\u03b3 4C41 ;\nindeed this expression appears in proofs demonstrating the improvement due to a single step of the Wolfe search, see for instance Telgarsky (2012, Proof of Proposition D.6, second to last line).\nAdditionally, note\n\u2016\u03bbt+1\u20161 = \u2225\u2225\u2225\u2225\u2225 t+1\u2211 i=1 \u03b1ivi \u2225\u2225\u2225\u2225\u2225 1 \u2264 t+1\u2211 i=1 \u03b1i.\nDirect from the first Wolfe condition (eq. (2.7)),\nL(A\u03bbt+1) = L(A(\u03bbt + \u03b1t+1vt+1)) \u2264 L(A\u03bbt)\u2212 \u03b1t+1(1\u2212 \u03bd/2)\u2016A>\u2207L(A\u03bbt)\u2016\u221e\n\u2264 L(A\u03bbt) ( 1\u2212 \u03b1t+1(1\u2212 \u03bd/2)\u2016A >\u2207L(A\u03bbt)\u2016\u221e\nL(A\u03bbt) ) \u2264 L(A\u03bbt) ( 1\u2212 \u03b1t+1(2\u2212 \u03bd)\u03b3t+1\n2C2t+1\n) .\nNow let t \u2265 t0 be given as in the statement. Applying the above inequality t\u2212 t0 times,\nmax k\u2208[m]\nexp(e>k A\u03bbt+1)\n\u2264 mCt+1L(A\u03bbt+1)\n\u2264 mCt0+1L(A\u03bbt0) exp\n( \u2212 (2\u2212 \u03bd)\u03b3\n2C2t0+1 t+1\u2211 i=t0+1 \u03b1i\n)\n\u2264 mCt0+1L(A\u03bbt0) exp ( (2\u2212 \u03bd)\u03b3 2C2t0+1 t0\u2211 i=1 \u03b1i )\n\u00b7 exp ( \u2212 (2\u2212 \u03bd)\u03b3\n2C2t0+1 t+1\u2211 i=1 \u03b1i\n)\n\u2264 c0 exp ( \u2212 (2\u2212 \u03bd)\u03b3\n2C2t0+1 t+1\u2211 i=1 \u03b1i\n) ,\nwhere c0 is as in the statement. Since ln(\u00b7) is increasing, it follows that\nmax k\u2208[m] e>k A\u03bbt+1 \u2264 \u2212 (2\u2212 \u03bd)\u03b3 2C2t0+1 t+1\u2211 i=1 \u03b1i + ln(c0).\nUsing the above lower bound on \u03b1i in terms of \u03b3i, and since all margins are nonnegative by the lower bound on t,\nmin k\u2208[m] \u2212e>k A\u03bbt+1 \u2016\u03bbt+1\u20161 \u2265 min k\u2208[m] \u2212e>k A\u03bbt+1\u2211t+1 i=1 \u03b1i\n\u2265 \u03b3 (\n2\u2212 \u03bd 2C2t0+1\n) \u2212 ln(c0)\u2211t+1\ni=1 \u03b1i \u2265 \u03b3 (\n2\u2212 \u03bd 2C2t0+1\n) \u2212 4C 4 1 ln(c0)\n(t+ 1)\u03bd\u03b3 .\nThe remainder of this subsection provides proofs for \u03b1At (\u03bd) and \u03b1 O t (\u03bd), but uses some later material, most specifically the quantity \u03a5\u03bd .\nLemma B.2. Consider the setting of Theorem 3.1, except now each step size \u03b1i satisfies\n\u03b1Ai (\u03bd)\u2212 \u03c4 \u2264 \u03b1i \u2264 \u03b1Ai (\u03bd) + \u03c4\nfor some \u03c4 > 0. Let \u03b8 \u2208 [0, \u03b3) be given. Then, given t \u2265 t0, m\u2211 i=1 1 [ \u2212eiA\u03bbt+1 \u2016\u03bbt+1\u20161 < \u03b8 ]\n\u2264 mCt+1 exp(\u03b8\u2016\u03bbt0\u20161)L(A\u03bbt0) t+1\u220f\ni=t0+1\n( e\u03b8\u03c4 (\n1 + \u03b3i 1\u2212 \u03b3i\n)\u03b8\u03bd/2\n\u00b7 e \u03c4C4i 2 (1\u2212 \u03b32i )\u03bd/2((1 + \u03b3i)1\u2212\u03bd + (1\u2212 \u03b3i)1\u2212\u03bd)\n) .\n\u2264 mCt+1 exp(\u03b8\u2016\u03bbt0\u20161)L(A\u03bbt0) t+1\u220f\ni=t0+1\n( e\u03b8\u03c4 (\n1 + \u03b3i 1\u2212 \u03b3i\n)\u03b8\u03bd/2\n\u00b7 e\u03c4C4i (1\u2212 \u03b32i )\u03bd/2 ) .\nProof. To start,\nm\u2211 i=1 1 [ \u2212eiA\u03bbt+1 \u2016\u03bbt+1\u20161 < \u03b8 ]\n= m\u2211 i=1 1 [\u03b8\u2016\u03bbt+1\u20161 + eiA\u03bbt+1 > 0] \u2264 mCt+1 exp(\u03b8\u2016\u03bbt+1\u20161)L(A\u03bbt+1).\nNext, note\n\u2016\u03bbt+1\u20161 = \u2225\u2225\u2225\u2225\u2225\u03bbt0 + t+1\u2211\ni=t0+1\n\u03b1ivi \u2225\u2225\u2225\u2225\u2225 1\n\u2264 \u2016\u03bbt0\u20161 + t+1\u2211\ni=t0+1\n(\u03c4 + \u03b1Ai (\u03bd)).\nCombining these facts with the convergence bound from Lemma B.1, m\u2211 i=1 1 [ \u2212eiA\u03bbt+1 \u2016\u03bbt+1\u20161 < \u03b8 ]\n\u2264 mCt+1 exp(\u03b8\u2016\u03bbt0\u20161)L(A\u03bbt0) t+1\u220f\ni=t0+1\n( e\u03b8\u03c4 (\n1 + \u03b3i 1\u2212 \u03b3i\n)\u03b8\u03bd/2\n\u00b7 e \u03c4C4i 2 (1\u2212 \u03b32i )\u03bd/2((1 + \u03b3i)1\u2212\u03bd + (1\u2212 \u03b3i)1\u2212\u03bd)\n) .\nAs in the proof of Lemma B.1, (\u00b7)1\u2212\u03bd is concave, so the 1/2 may be pushed inside this last term to give\nthe vaguely simpler bound m\u2211 i=1 1 [ \u2212eiA\u03bbt+1 \u2016\u03bbt+1\u20161 < \u03b8 ]\n\u2264 mCt+1 exp(\u03b8\u2016\u03bbt0\u20161)L(A\u03bbt0) t+1\u220f\ni=t0+1\n( e\u03b8\u03c4 (\n1 + \u03b3i 1\u2212 \u03b3i\n)\u03b8\u03bd/2\n\u00b7 e\u03c4C4i (1\u2212 \u03b32i )\u03bd/2 ) .\nProof of Lemma 3.8. Set \u03b8 := \u03b3 \u2212 , whereby \u03b8 \u2208 [0, \u03b3). Invoking Lemma B.2 and simplifying terms via t0 = 0, Ci = 1, and \u03c4 = 0, then for any t,\nm\u2211 i=1 1 [ \u2212eiA\u03bbt+1 \u2016\u03bbt+1\u20161 < \u03b8 ]\n\u2264 m t+1\u220f i=1 (( 1 + \u03b3i 1\u2212 \u03b3i )\u03b8\u03bd/2 \u00b7 1\n2 (1\u2212 \u03b32i )\u03bd/2((1 + \u03b3i)1\u2212\u03bd + (1\u2212 \u03b3i)1\u2212\u03bd)\n\u2264 m t+1\u220f i=1\n(( 1 + \u03b3\n1\u2212 \u03b3 )\u03b8\u03bd/2 \u00b7 1\n2 (1\u2212 \u03b32)\u03bd/2((1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd)\n) ,\nwhere the replacement of \u03b3i by \u03b3 made use of the first part of Lemma B.6. Now, by the second part of Lemma B.6, this inner term is less than 1 iff \u03b8 < \u03a5\u03bd(\u03b3). By Theorem B.5, since \u03b8 < \u03b3, there exists a \u03bd sufficiently small that \u03a5\u03bd(\u03b3) > \u03b8. Consequently, there exists a T so that this product is less than 1/m whenever t \u2265 T , and the result follows.\nProof of Lemma 3.9. Set \u03b8 := \u03b3 \u2212 , whereby \u03b8 \u2208 [0, \u03b3). Since ` \u2208 L\u221e, choose t0 large enough so that\nC8t0 <\n( 1 + \u03b3\n1\u2212 \u03b3\n) (\u03b3\u2212\u03b8)\u03bd 4\n.\nBy Lemma B.7, it follows that the optimal step size satisfies\n\u03b1At (\u03bd)\u2212 \u03c4 \u2264 \u03b1Ot (\u03bd) \u2264 \u03b1At (\u03bd) + \u03c4\nwith \u03c4 = \u03bd2 ln(C 4 t ). Combining this with the bound on Ct0 above,\ne2\u03c4C4t0 = C 8 t0 <\n( 1 + \u03b3\n1\u2212 \u03b3\n) (\u03b3\u2212\u03b8)\u03bd 4\n.\nPlugging this into the general margin bound in Lemma B.2 and additionally replacing \u03b3i with \u03b3\nthanks to the first part of Lemma B.6, and finally setting \u03b8\u2032 := \u03b8 + (\u03b3 \u2212 \u03b8)/2 = (\u03b8 + \u03b3)/2 < \u03b3, m\u2211 i=1 1 [ \u2212eiA\u03bbt+1 \u2016\u03bbt+1\u20161 < \u03b8\n] \u2264 mCt+1 exp(\u03b8\u2016\u03bbt0\u20161)L(A\u03bbt0)\n\u00b7 t+1\u220f\ni=t0+1\n( e\u03b8\u03c4 (\n1 + \u03b3i 1\u2212 \u03b3i\n)\u03b8\u03bd/2\n\u00b7 e \u03c4C4i 2 (1\u2212 \u03b32i )\u03bd/2((1 + \u03b3i)1\u2212\u03bd + (1\u2212 \u03b3i)1\u2212\u03bd) ) \u2264 mCt+1 exp(\u03b8\u2016\u03bbt0\u20161)L(A\u03bbt0)\n\u00b7 t+1\u220f\ni=t0+1\n( e2\u03c4C4t0+1 ( 1 + \u03b3\n1\u2212 \u03b3\n)\u03b8\u03bd/2\n\u00b7 1 2 (1\u2212 \u03b32)\u03bd/2((1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd) ) \u2264 mCt+1 exp(\u03b8\u2016\u03bbt0\u20161)L(A\u03bbt0)\n\u00b7 t+1\u220f\ni=t0+1\n(( 1 + \u03b3\n1\u2212 \u03b3\n)\u03b8\u2032\u03bd/2\n\u00b7 1 2 (1\u2212 \u03b32)\u03bd/2((1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd)\n) .\nBy the second part of Lemma B.6, the term within the product is less than one, and thus for all large t, this entire bound is less than 1, which gives the result.\nProof of Proposition 3.10. To start, note that, for any t \u2265 0,\n(1\u2212 \u03b3t)1\u2212\u03b8(1 + \u03b3t)1+\u03b8 = (1\u2212 \u03b32t )1\u2212\u03b8(1 + \u03b3t)2\u03b8 \u2264 exp ( \u2212\u03b32t (1\u2212 \u03b8) + \u03b3t(2\u03b8) ) = exp ( \u2212\u03b32t + \u03b8\u03b3t(2 + \u03b3t) ) .\n(B.3)\nNext, since\n\u03b8 \u2264 \u03b3 1 + \u03b3 = 1 1 + 1/\u03b3 \u2264 1 1 + 1/\u03b3t = \u03b3t 1 + \u03b3t ,\nthen \u03b8 \u2264 \u03b3/(1 + \u03b3) implies\nd\nd\u03b3t\n( \u2212\u03b32t + \u03b8\u03b3t(2 + \u03b3t) ) = \u22122\u03b3t + 2\u03b8(1 + \u03b3t)\n\u2264 \u22122\u03b3t + 2\u03b3t = 0.\nIn particular, the expression \u2212\u03b32t + \u03b8\u03b3t(2 + \u03b3t) is decreasing in \u03b3, and thus \u03b3t \u2265 \u03b3 implies\n\u2212\u03b32t + \u03b8\u03b3t(2 + \u03b3t) \u2264 \u2212\u03b32 + \u03b8\u03b3(2 + \u03b3),\nand consequently, combined with the bound in (B.3),\n(1\u2212 \u03b3t)1\u2212\u03b8(1 + \u03b3t)1+\u03b8 \u2264 exp(\u2212\u03b32 + \u03b8\u03b3(2 + \u03b3)).\nPlugging this into the simplified generic bound in Lemma B.2 with the specialization ` = exp, \u03c4 = 0, Ci = 1, and t0 = 0, it follows that\nm\u2211 i=1 1 [ \u2212eiA\u03bbt+1 \u2016\u03bbt+1\u20161 < \u03b8 ]\n\u2264 m t+1\u220f i=1 (( 1 + \u03b3i 1\u2212 \u03b3i )\u03b8\u03bd/2 (1\u2212 \u03b32i )\u03bd/2 )\n\u2264 m exp ( \u2212\u03bd(t+ 1)\n2 (\u2212\u03b32 + \u03b8\u03b3(2 + \u03b3))\n) .\nThe rest of the result follows by noting \u03b8 < \u03b3/(2 + \u03b3) implies \u2212\u03b32 + \u03b8\u03b3(2 + \u03b3) < 0, whereby choices\nt > 2 ln(m)\n\u03bd(\u03b32 \u2212 \u03b8\u03b3(2 + \u03b3))\nexist, and plugging this all in to the above bound grants that M(A\u03bbt) \u2265 \u03b8.\nProof of Theorem 3.5. For \u03b1At (\u03bd) and \u03b1 O t (\u03bd), Lemma 3.8 and Lemma 3.9 already state the results in the desired asymptotic form.\nFor the other two, since ` \u2208 L\u221e, t0 can be chosen sufficiently large so that Ct0 is arbitrarily close to 1, whereby the bounds in Lemma 3.6 and Lemma 3.7 become sufficiently tight by taking \u03bd small and t\u03bd large.\nB.2.1. The Quantity \u03a5\u03bd\nDefinition B.4. Define\n\u03a5\u03bd(\u03b3) := 2 \u03bd ln(2)\u2212 2 \u03bd ln((1 + \u03b3)\n1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd)\u2212 ln(1\u2212 \u03b32) ln(1 + \u03b3)\u2212 ln(1\u2212 \u03b3) ;\nin the case that \u03bd = 1, this quantity has been extensively studied in the context of AdaBoost\u2019s margins (Ra\u0308tsch & Warmuth, 2005; Rudin et al., 2004; Schapire & Freund, 2012) \u2666\nThe basic properties of \u03a5\u03bd are as follows.\nTheorem B.5. Suppose \u03b3 \u2208 (0, 1).\n1. \u03b3/2 \u2264 \u03a5\u03bd(\u03b3) \u2264 \u03b3.\n2. lim\u03bd\u21930 \u03a5\u03bd(\u03b3) = \u03b3.\nThe bounds \u03b3/2 \u2264 \u03a51(\u03b3) \u2264 \u03b3 were known in the case that \u03bd = 1 (cf. Ra\u0308tsch & Warmuth (2005) and Schapire & Freund (2012, Bibliographic Notes, Chapter 5)).\nProof. (Item 1, subcase \u03a5\u03bd(\u03b3) \u2265 \u03b3/2.) To start, note that (\u00b7)1\u2212\u03bd is a concave function, whereby\n\u2212 2 \u03bd\nln ( (1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd ) = \u22122\n\u03bd ln\n( 2 ( 1\n2 (1 + \u03b3)1\u2212\u03bd +\n1 2 (1\u2212 \u03b3)1\u2212\u03bd )) \u2265 \u22122 \u03bd ln ( 2 (1) 1\u2212\u03bd ) = \u22122 \u03bd ln (2) .\nIt follows that\n\u03a5\u03bd(\u03b3) \u2265 \u2212 ln(1\u2212 \u03b32)\nln(1 + \u03b3)\u2212 ln(1\u2212 \u03b3) = \u03a51(\u03b3).\nNext recall the series expansion\nln(1 + z) = \u221e\u2211 n=1 (\u22121)n+1 n zn\n(when |z| < 1). Plugging this in to the simplified form of \u03a51(\u03b3) and paying attention to cancellations in the numerator and denominator (odd and even terms, respectively),\n\u03a51(\u03b3) = \u2212 \u2211\u221e n=1 (\u22121)n+1 n (\u03b3) n \u2212 \u2211\u221e n=1 (\u22121)n+1 n (\u2212\u03b3)\nn\u2211\u221e n=1 (\u22121)n+1 n (\u03b3) n \u2212 \u2211\u221e n=1 (\u22121)n+1 n (\u2212\u03b3)n\n= \u22122 \u2211\u221e n=1 (\u22121)2n+1 2n (\u03b3) 2n\n2 \u2211\u221e n=1 (\u22121)2n\u22121+1 2n\u22121 (\u03b3) 2n\u22121\n= \u03b3 \u2211\u221e n=1 1 2n (\u03b3)\n2n\u2211\u221e n=1 1 2n\u22121 (\u03b3) 2n .\nTo finish, note that n \u2265 1 implies 1/(4n \u2212 2) \u2264 1/(2n) \u2264 1/(2n\u2212 1), and thus\n\u03b3 2 = \u03b3 \u2211\u221e n=1 1 2(2n\u22121) (\u03b3) 2n\u2211\u221e n=1 1 2n\u22121 (\u03b3) 2n\n\u2264 \u03b3 \u2211\u221e n=1 1 2n (\u03b3)\n2n\u2211\u221e n=1 1 2n\u22121 (\u03b3) 2n\n\u2264 \u03b3 \u2211\u221e n=1 1 2n\u22121 (\u03b3)\n2n\u2211\u221e n=1 1 2n\u22121 (\u03b3) 2n\n= \u03b3.\nThat is to say, \u03b3/2 \u2264 \u03a51(\u03b3) \u2264 \u03b3, which combined with the above also gives \u03a5\u03bd(\u03b3) \u2265 \u03a51(\u03b3) \u2265 \u03b3/2.\n(Item 1, subcase \u03a5\u03bd(\u03b3) \u2264 \u03b3.) By the power mean inequality (Steele, 2004, Equation 8.12),(\n1 + \u03b3\n2 (1 + \u03b3)\u2212\u03bd + 1\u2212 \u03b3 2\n(1\u2212 \u03b3)\u2212\u03bd )\u22121/\u03bd\n\u2264 (1 + \u03b3) 1+\u03b3 2 (1\u2212 \u03b3) 1\u2212\u03b3 2 .\nIt follows that\n\u2212 2 \u03bd ln\n( 1 + \u03b3\n2 (1 + \u03b3)\u2212\u03bd + 1\u2212 \u03b3 2\n(1\u2212 \u03b3)\u2212\u03bd )\n\u2264 (1 + \u03b3) ln(1 + \u03b3) + (1\u2212 \u03b3) ln(1\u2212 \u03b3).\nAs such,\n\u03a5\u03bd(\u03b3)\n\u2264 (1 + \u03b3) ln(1 + \u03b3) + (1\u2212 \u03b3) ln(1\u2212 \u03b3) ln(1 + \u03b3)\u2212 ln(1\u2212 \u03b3)\n+ \u2212 ln(1 + \u03b3)\u2212 ln(1\u2212 \u03b3) ln(1 + \u03b3)\u2212 ln(1\u2212 \u03b3)\n= \u03b3 ln(1 + \u03b3)\u2212 \u03b3 ln(1\u2212 \u03b3)\nln(1 + \u03b3)\u2212 ln(1\u2212 \u03b3) = \u03b3.\n(Item 2.) Consider the (halved, negated) first term\nln((1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd)\u2212 ln(2) \u03bd = ln(0.5(1 + \u03b3)1\u2212\u03bd + 0.5(1\u2212 \u03b3)1\u2212\u03bd)\n\u03bd .\nBy l\u2019Ho\u0302pital\u2019s rule,\nlim \u03bd\u21920 ln(0.5(1 + \u03b3)1\u2212\u03bd + 0.5(1\u2212 \u03b3)1\u2212\u03bd) \u03bd\n= lim \u03bd\u21920 \u2212(1 + \u03b3)1\u2212\u03bd ln(1 + \u03b3)\u2212 (1\u2212 \u03b3)1\u2212\u03bd ln(1\u2212 \u03b3) (1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd\n= \u22121 2 ((1 + \u03b3) ln(1 + \u03b3) + (1\u2212 \u03b3) ln(1\u2212 \u03b3)) .\nConsequently (recalling that this term was both halved and negated)\nlim \u03bd\u21920\n\u03a5\u03bd(\u03b3) = (1 + \u03b3) ln(1 + \u03b3) + (1\u2212 \u03b3) ln(1\u2212 \u03b3)\nln(1 + \u03b3)\u2212 ln(1\u2212 \u03b3)\n+ \u2212 ln(1 + \u03b3)\u2212 ln(1\u2212 \u03b3) ln(1 + \u03b3)\u2212 ln(1\u2212 \u03b3)\n= \u03b3.\nThe usefulness of \u03a5\u03bd is captured in the following lemma.\nLemma B.6. Let \u03bd \u2208 (0, 1] and \u03b8 \u2208 [0, 1] be given. The map\n\u03b3 7\u2192 ( 1 + \u03b3\n1\u2212 \u03b3\n)\u03b8\u03bd/2 (1\u2212 \u03b32)\u03bd/2\n\u00b7 ((1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd)\nis nonincreasing over [\u03b8, 1]. Additionally, now taking \u03b3 to be fixed, \u03b8 < \u03a5\u03bd(\u03b3) iff\n1\n2\n( 1 + \u03b3\n1\u2212 \u03b3\n)\u03b8\u03bd/2 (1\u2212 \u03b32)\u03bd/2\n\u00b7 ((1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd) < 1.\nProof. Let f(\u03b3) be the prescribed map. To establish f is nonincreasing, it will be shown that each element of the product f(\u03b3) = g(\u03b3)h(\u03b3) is nonincreasing, where\ng(\u03b3) :=\n( 1 + \u03b3\n1\u2212 \u03b3\n)\u03b8\u03bd/2 (1\u2212 \u03b32)\u03bd/2\nh(\u03b3) := (1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd .\nFirst, set \u03bd\u2032 := \u03bd/2, and note\ng\u2032(\u03b3) = d\nd\u03b3 (1 + \u03b3)\u03bd\n\u2032(1+\u03b8)(1\u2212 \u03b3)\u03bd \u2032(1\u2212\u03b8)\n= \u03bd\u2032(1 + \u03b8)(1 + \u03b3)\u03bd \u2032(1+\u03b8)\u22121(1\u2212 \u03b3)\u03bd \u2032(1\u2212\u03b8)\n\u2212 \u03bd\u2032(1\u2212 \u03b8)(1\u2212 \u03b3)\u03bd \u2032(1\u2212\u03b8)\u22121(1 + \u03b3)\u03bd \u2032(1+\u03b8)\n= \u03bd\u2032(1 + \u03b3)\u03bd \u2032(1+\u03b8)\u22121(1\u2212 \u03b3)\u03bd \u2032(1\u2212\u03b8)\u22121\n\u00b7 ((1 + \u03b8)(1\u2212 \u03b3)\u2212 (1\u2212 \u03b8)(1 + \u03b3))\n= 2\u03bd\u2032(1 + \u03b3)\u03bd \u2032(1+\u03b8)\u22121(1\u2212 \u03b3)\u03bd \u2032(1\u2212\u03b8)\u22121 (\u03b8 \u2212 \u03b3) ,\nwhere this last term is nonpositive since \u03b8 \u2264 \u03b3. Consequently, g(\u03b3) is nonincreasing.\nFor h(\u03b3), note similarly that\nh\u2032(\u03b3) = (1\u2212 \u03bd) ( (1 + \u03b3)\u2212\u03bd \u2212 (1\u2212 \u03b3)\u2212\u03bd ) =\n1\u2212 \u03bd (1 + \u03b3)\u03bd(1\u2212 \u03b3)\u03bd ((1\u2212 \u03b3)\u03bd \u2212 (1 + \u03b3)\u03bd)\n\u2264 0.\nTogether f(\u03b3) = g(\u03b3)h(\u03b3) is nonincreasing in \u03b3.\nFor the second statement, note that\n1 > 1\n2\n( 1 + \u03b3\n1\u2212 \u03b3\n)\u03b8\u03bd/2 (1\u2212 \u03b32)\u03bd/2\n\u00b7 ((1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd)\nis equivalent to\n0 > \u2212 ln(2) + \u03bd 2 \u03b8 ln\n( 1 + \u03b3\n1\u2212 \u03b3\n) + \u03bd\n2 ln(1\u2212 \u03b32)\n+ ln((1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd)\nis equivalent to\n\u03b8 < ln(2)\u2212 \u03bd2 ln(1\u2212 \u03b3 2)\u2212 ln((1 + \u03b3)1\u2212\u03bd + (1\u2212 \u03b3)1\u2212\u03bd) \u03bd 2 ln ( 1+\u03b3 1\u2212\u03b3\n) , where the last expression can be written \u03b8 < \u03a5\u03bd(\u03b3).\nB.2.2. Miscellaneous Technical Material\nLemma B.7. Suppose A \u2208 {\u22121,+1}m\u00d7n is binary and ` \u2208 L. Then 1 2 ln ( 1 + \u03b3t 1\u2212 \u03b3t ) \u2212 1 2 ln(C4t ) \u2264 \u03b1Ot (1)\n\u2264 1 2 ln ( 1 + \u03b3t 1\u2212 \u03b3t ) + 1 2 ln(C4t ).\nMore simply,\u2223\u2223\u03b1Ot (\u03bd)\u2212 \u03b1At (\u03bd)\u2223\u2223 \u2264 \u03bd2 ln (C4t ) . Proof. Choose s \u2208 {\u00b11} so that vt+1 = ejs for some ej . Then, by first order conditions on the optimal step size, and adopting shorthand notation where the summations take j fixed according to the preceding text, but i \u2208 [m] may vary,\n0 = \u2211 Aij<0 sAij` \u2032(e>i A(\u03bbt + s\u03b1t+1ej))\n+ \u2211 Aij>0 sAij` \u2032(e>i A(\u03bbt + s\u03b1t+1ej))\n\u2264 C\u22121t+1 \u2211 Aij<0 sAij exp(e > i A\u03bbt) exp(s\u03b1t+1Aij)\n+ C1t+1 \u2211 Aij>0 sAij exp(e > i A\u03bbt) exp(s\u03b1t+1Aij)\n\u2264 exp(\u2212s\u03b1t+1)C\u22122t+1 \u2211 Aij<0 sAij` \u2032(e>i A\u03bbt)\n+ exp(s\u03b1t+1)C 2 t+1 \u2211 Aij>0 sAij` \u2032(e>i A\u03bbt),\nwhich can be rearranged to yield\ns\u03b1t+1 \u2265 1\n2 ln\n( \u2212sC\u22122t+1 \u2211 Aij<0 Aij` \u2032(e>i A\u03bbt)\nsC2t+1 \u2211 Aij>0 Aij`\u2032(e>i A\u03bbt)\n)\n= 1\n2 ln\n(\u2211 Aij<0\n`\u2032(e>i A\u03bbt)\u2211 Aij>0 `\u2032(e>i A\u03bbt)\n) \u2212 1\n2 ln(C4t+1).\nTo simplify further, note that s\u03b3t+1 = \u2212 \u2211m i=1Aij` \u2032(e>i A\u03bbt)\n\u2016\u2207L(A\u03bbt)\u20161\n=\n\u2211 Aij<0 `\u2032(e>i A\u03bbt)\u2212 \u2211 Aij>0 `\u2032(e>i A\u03bbt)\n\u2016\u2207L(A\u03bbt)\u20161 ,\n1 =\n\u2211 Aij<0 `\u2032(e>i A\u03bbt) + \u2211 Aij>0 `\u2032(e>i A\u03bbt)\n\u2016\u2207L(A\u03bbt)\u20161 ,\nwhich can be added and subtracted to yield\u2211 Aij>0\n`\u2032(e>i A\u03bbt)\u2211 Aij<0 `\u2032(e>i A\u03bbt) = 1\u2212 s\u03b3t+1 1 + s\u03b3t+1 ,\nwhereby\ns\u03b1t+1 \u2265 1\n2 ln ( 1 + s\u03b3t+1 1\u2212 s\u03b3t+1 ) \u2212 1 2 ln(C4t+1).\nRepeating the steps above to prove a lower bound on \u03b1t+1, it also follows that\ns\u03b1t+1 \u2264 1\n2 ln ( 1 + s\u03b3t+1 1\u2212 s\u03b3t+1 ) + 1 2 ln(C4t+1).\nTo finish the first part of the result, it suffices to consider the cases s = +1 and s = \u22121 separately, which both lead to the desired pair of inequalities.\nFor the second guarantee, first note that \u03b1Ot (\u03bd) = \u03bd\u03b1Ot (1) and \u03b1 A t (\u03bd) = \u03bd\u03b1 A t (1), and so recalling the form of \u03b1At (1) and scaling the first guarantee by \u03bd, it follows that\n|\u03b1Ot (\u03bd)\u2212 \u03b1At (\u03bd)| \u2264 \u03bd\n2 ln(C4t )."}, {"heading": "C. Deferred Material from Section 4", "text": "Proof sketch of Theorem 4.3. All the convergence rates developed by Telgarsky (2012, Section 6) stem from an inequality\nL(A\u03bbt+1)\u2212 L\u0304A \u2264 (L(A\u03bbt)\u2212 L\u0304A) ( 1\u2212 \u2016A >\u2207L(A\u03bbt)\u20162\u221e\ncL(A\u03bbt)(L(A\u03bbt)\u2212 L\u0304A)\n) ,\nwhere c > 0 is some constant independent of t (or improving with t, in which case the bound may be worsened by taking the choice for t = 0) (Telgarsky, 2012, Proposition 6.2, Proposition D.6). Exactly such a bound was provided for each line search in the proof of its respective optimization guarantee in the separable case (cf. Lemma 3.2, Lemma 3.3; no need to adjust Lemma 3.4, since ` = exp and A binary causes \u03b1At (\u03bd) = \u03b1 O t (\u03bd), and so Lemma 3.2 covers this case). Replacing c with the particulars for each step size will\nonly impact the final rates in Theorems 6.3, 6.6, and 6.12 by these constants. The only other thing to check is that ` \u2208 G, the class of losses considered by Telgarsky (2012, Section 6); it can be checked directly that L \u2282 G.\nIn order to establish the margin properties, the following lemma is essential.\nLemma C.1. Consider the setting of Theorem 4.4. Then there exists T and \u03b3\u0302 so that, for all t \u2265 T ,\n\u2016A>\u2207L(A\u03bbt)\u2016\u221e L(A\u03bbt)\u2212 L\u0304A \u2265 \u03b3\u0302.\nProof sketch. As discussed in the proof of Theorem 4.3, the results of Telgarsky (2012), which are superficially specialized to the Wolfe line search, carry over for the other line searches here with only a change of constants; consequently, those results carry over wholesale.\nTo start, let S be a compact cube containing all iterates, and let \u03b3(A,S) be the corresponding generalized weak learning rate Telgarsky (2012, Definition 4.3).\nBy (Telgarsky, 2012, Theorem 5.9), L + \u03b9im(A+) (i.e., the function which is L(y) when y = A+\u03bb for some \u03bb \u2208 Rn, and \u221e otherwise) has compact level sets, and thus strict convexity of L grants a modulus of strong convexity c > 0 over S; furthermore, it holds for every t that\nL(A+\u03bbt)\u2212 L\u0304A\n\u2264 1 2c \u2225\u2225\u2225\u2207L(A+\u03bbt)\u2212 P1\u2207L(S)\u2229ker(A>+)(\u2207L(A+\u03bbt))\u2225\u2225\u222521 , where P1\u2207L(S)\u2229ker(A>+) denotes the l1 projection onto \u2207L(S) \u2229 ker(A>+), the latter being the kernel (nullspace) of A>+ (Telgarsky, 2012, Lemma 6.8).\nNow choose T so that, for every t \u2265 T ,\nL(A+\u03bbt)\u2212 L\u0304A \u2264 L(A\u03bbt)\u2212 L\u0304 \u2264 2c,\nwhich is possible by the convergence of {\u03bbt}\u221et=1 (cf. Theorem 4.3 or (Telgarsky, 2012, Theorem 6.12)).\nUsing these facts, the definition of \u03b3(A,S), the choice \u03c6 = exp, and the fact inf\u03bb L(A+\u03bb) = inf\u03bb L(A\u03bb) = L\u0304A\n(Telgarsky, 2012, Theorem 5.9),\n\u2016A>\u2207L(A\u03bbt)\u2016\u221e L(A\u03bbt)\u2212 L\u0304A\n\u2265 \u03b3(A,S)\n( \u2016\u2207L(A\u03bbt)\u2212 P1S\u2229ker(A>)(\u2207L(A\u03bbt))\u20161\nL(A\u03bbt)\u2212 L\u0304A\n)\n= \u03b3(A,S) C2T C2T ( \u2016\u2207L(A0\u03bbt)\u20161 L(A\u03bbt)\u2212 L\u0304A\n+ \u2016\u2207L(A+\u03bbt)\u2212 P1S\u2229ker(A>+)(\u2207L(A+\u03bbt))\u20161\nL(A\u03bbt)\u2212 L\u0304A\n)\n\u2265 \u03b3(A,S) C2T\n( L(A0\u03bbt) + \u221a 2c(L(A+\u03bbt)\u2212 L\u0304A)\nL(A\u03bbt)\u2212 L\u0304A\n)\n\u2265 \u03b3(A,S) C2T\n( L(A0\u03bbt)\nL(A\u03bbt)\u2212 L\u0304A\n+\n\u221a (L(A+\u03bbt)\u2212 L\u0304A)(L(A+\u03bbt)\u2212 L\u0304A)\nL(A\u03bbt)\u2212 L\u0304A\n)\n= \u03b3(A,S)\nC2T .\nTo finish, set \u03b3\u0302 := \u03b3(A,S)/C2T .\nAnother technical lemma is helpful.\nLemma C.2. Consider the setting of Theorem 4.4. For each step size choice and B > 0, there exists TB so that for all t \u2265 TB, \u2016\u03bbt\u20161 \u2265 B.\nProof sketch. This follows from Theorem 4.3 and |H(A)| < m. In particular, choose any example i \u2208 H(A)c; there exists > 0 so that 1m`(eiA\n>\u03bb) < (which is a necessary condition for L(A\u03bb) < ) only when e>i A\u03bb \u2264 \u2212B\u2016e>i A\u2016\u221e, and so the result follows by combining this with Ho\u0308lder\u2019s inequality, namely the inequality \u2212e>i A\u03bb \u2264 \u2016e>i A\u2016\u221e\u2016\u03bb\u20161; the optimality guarantee provides that this holds for all large t.\nIn order to proof the margin results, it is helpful to split into two cases, one being the Wolfe step sizes, the other being a generalization of the quadratic upper bound step sizes.\nLemma C.3. Consider the setting of Theorem 4.4, but with step sizes 0.5\u03b1Qi (\u03bd) \u2264 \u03b1i \u2264 1.5\u03b1 Q i (\u03bd). Then there exists \u03b3\u0302 > 0 and T so that, for all t \u2265 T , all margins (over H(A)c) exceed \u03b3\u0302.\nProof sketch. Consider the quadratic upper bound line search in Lemma 3.2 and its proof. It is unclear whether 0.5\u03b1Qi (\u03bd) or 1.5\u03b1 Q i (\u03bd) give a better step, due to the term \u03bd. However, since \u03b1Qi (1) is the minimizer,\nsymmetry grants that 0.5\u03b1Qt (\u03bd) is guaranteed to be a worse choice than anything in the specified interval. As such, plugging this in to the quadratic upper bound yields\nL(A\u03bbt+1) \u2264 L(A\u03bbt)\u2212 c0\u03b3t+1\u2016A>\u2207L(A\u03bbt)\u2016\u221e\n2 .\nfor some constant c0 > 0 depending on C1 and not on t.\nNow choose T1 according to Lemma C.1; by the above and Lemma C.1, for any t \u2265 T1,\nL(A\u03bbt+1)\u2212 L\u0304A\n\u2264 L(A\u03bbt)\u2212 L\u0304A \u2212 \u03b3t+1c0\u2016A>\u2207L(A\u03bbt)\u2016\u221e\n2 \u2264 (L(A\u03bbt)\u2212 L\u0304A) ( 1\u2212 \u03b3t+1c0\u2016A >\u2207L(A\u03bbt)\u2016\u221e\n2(L(A\u03bbt \u2212 L\u0304A) ) \u2264 (L(A\u03bbt)\u2212 L\u0304A) ( 1\u2212 \u03b3t+1c0\u03b3\u0302\n2\n) ,\nwhich, after recursive application, provides\nL(A\u03bbt+1)\u2212L\u0304A \u2264 (L(A\u03bbT1)\u2212L\u0304A) exp\n( \u2212 \u03b3\u0302c0\n2 t+1\u2211 i=T1+1 \u03b3i\n) .\nSince\n\u2016\u03bbt+1\u20161 = \u2016\u03bbT1 + t+1\u2211 i=T1 \u03b1ivi\u20161\n\u2264 \u2016\u03bbT1\u20161 + t+1\u2211\ni=T1+1\n\u03b1i\n\u2264 \u2016\u03bbT1\u20161 + 1.5\u03bd t+1\u2211\ni=T1+1\n\u03b3i,\nit follows that\nL(A\u03bbt+1)\u2212 L\u0304A \u2264 (L(A\u03bbT1 \u2212 L\u0304A)) exp ( \u2212 \u03b3\u0302c0\n3\u03bd (\u2016\u03bbt+1\u20161 \u2212 \u2016\u03bbT1\u20161)\n) .\nFor any iteration t, let bt \u2208 {ei}m0i=1 index any example in [m] \\H(A) which achieves the worst margin (amongst elements off the hard core) for this iteration. Since the optimal error on this example is 0 (Telgar-\nsky, 2012, Theorem 5.9), for any t > T1,\nexp(b>t A\u03bbt) = exp(b>t A\u03bbt)\u2212 0 \u2264 mCT1(L(A\u03bbt)\u2212 L\u0304A)\n\u2264 mCT1(L(A\u03bbT1 \u2212 L\u0304A)) exp ( \u2212 \u03b3\u0302c0\n3\u03bd (\u2016\u03bbt\u20161 \u2212 \u2016\u03bbT1\u20161) ) = exp (\u2212\u03b3\u0302c0\u2016\u03bbt\u20161/(3\u03bd)) \u00b7 Ct(L(A\u03bbT1 \u2212 L\u0304A)) exp(\u03b3\u0302c0\u2016\u03bbT1\u20161/(3\u03bd))\ufe38 \ufe37\ufe37 \ufe38\n=:exp(r)\n.\nApplying ln and rearranging,\n\u2212b>t A\u03bbt \u2016\u03bbt\u20161 \u2265 \u03b3\u0302c0 3\u03bd \u2212 r \u2016\u03bbt\u20161 .\nTo finish, by Lemma C.2, there exists T2 so that\n\u2016\u03bbi\u20161 \u2265 6r\u03bd\n\u03b3\u0302c0\nfor every i \u2265 T2, and setting T := max{T1, T2} gives the desired result.\nLemma C.4. Consider the setting of Theorem 4.4, but specialized so that \u03b1i \u2208 \u03b1Wi (\u03bd). Then there exists \u03b3\u0302 > 0 and T so that, for all t \u2265 T , all margins exceed \u03b3\u0302.\nProof sketch. Choose T1 according to Lemma C.1, and let t \u2265 T1 be arbitrary. Using Lemma C.1, and using the first Wolfe condition (eq. (2.7)) just as in the proof of Lemma 3.7,\nL(A\u03bbt+1)\u2212 L\u0304A \u2264 L(A\u03bbt)\u2212 L\u0304A \u2212 \u03b1t+1(1\u2212 \u03bd/2)\u2016A>\u2207L(A\u03bbt)\u2016\u221e\n= (L(A\u03bbt)\u2212 L\u0304A) ( 1\u2212 \u03b1t+1(1\u2212 \u03bd/2)\u2016A >\u2207L(A\u03bbt)\u2016\u221e\nL(A\u03bbt)\u2212 L\u0304A ) \u2264 (L(A\u03bbt)\u2212 L\u0304A) (1\u2212 \u03b1t+1(1\u2212 \u03bd/2)\u03b3\u0302) \u2264 (L(A\u03bbt)\u2212 L\u0304A) exp (\u2212\u03b1t+1(1\u2212 \u03bd/2)\u03b3\u0302)\nApplying this inequality recursively,\nL(A\u03bbt+1)\u2212 L\u0304A\n\u2264 (L(A\u03bbT1 \u2212 L\u0304A)) exp\n( \u2212(1\u2212 \u03bd/2)\u03b3\u0302\nt+1\u2211 i=T1 \u03b1i\n) .\nNote next, for any t0, that\n\u2016\u03bbt+1\u20161 \u2264 \u2016\u03bbt0\u20161 + t+1\u2211\ni=t0+1\n\u03b1i,\nwhereby\nL(A\u03bbt+1)\u2212 L\u0304A \u2264 (L(A\u03bbT1 \u2212 L\u0304A)) \u00b7 exp (\u2212(1\u2212 \u03bd/2)\u03b3\u0302(\u2016\u03bbt+1\u20161 \u2212 \u2016\u03bbt0\u20161)) ,\nand the remainder of the proof proceeds just as for the quadratic upper bound (cf. Lemma C.3).\nProof sketch of Theorem 4.4. The case of \u03b1Qt (\u03bd) and \u03b1Wt (\u03bd) are handled by Lemma C.3 and Lemma C.4.\nNow consider the case of \u03b1At (\u03bd). Since \u03b3 = 0, Lemma C.6 grants the existence of a large T so that, for all t \u2265 T , \u03b3t \u2264 0.1. Thus, by Lemma C.5, and considering t sufficiently large that Ct is almost 1, the problem reduces to the consideration of \u03b1Qt (\u03bd); in particular, the conditions to apply Lemma C.3, but now for the step \u03b1At (\u03bd), are satisfied. Note that this also handles the case \u03b1Ot (\u03bd), since, for \u03b1 O t (\u03bd) and \u03b1 A t (\u03bd), it was assumed that A is binary and ` = exp.\nC.1. Miscellaneous Technical Material\nLemma C.5. For any r \u2208 [0, 1),\nr \u2264 1 2 ln\n( 1 + r\n1\u2212 r\n) \u2264 r\n1\u2212 r .\nProof. Set g(r) := 12 ln((1 + r)/(1\u2212 r)). Note that\ng\u2032(r) = (1\u2212 r2)\u22121 and g\u2032\u2032(r) = 2r (1\u2212 r2)2 .\nAs such, g is convex (along [0, 1)) and g\u2032(0) = 1, thus g(r) \u2265 r along [0, 1). The second part follows from concavity of ln(\u00b7):\n1 2 ln\n( 1 + r\n1\u2212 r\n) = 1\n2 ln\n( 1 + 2r\n1\u2212 r\n) \u2264 1\n2\n( 2r\n1\u2212 r\n) .\nLemma C.6. Under the conditions of Theorem 4.4, limt\u2192\u221e \u03b3t = 0.\nProof sketch. As discussed in the proof of Theorem 4.3, every step size provides a guarantee of the type\nL(A\u03bbt+1) \u2264 L(A\u03bbt)\u2212 \u03b32tL(A\u03bbt)\nc\nfor some c > 0 (independent of t). The result follows by rearranging this expression and using L(A\u03bbt) \u2265 L\u0304A > 0 (i.e., nonseparability) and L(A\u03bbt \u2212 L(A\u03bbt+1) \u2192 0 (i.e., the convergence result, Theorem 4.3)."}], "references": [{"title": "Boosting a weak learning algorithm by majority", "author": ["Freund", "Yoav"], "venue": "Information and Computation,", "citeRegEx": "Freund and Yoav.,? \\Q1995\\E", "shortCiteRegEx": "Freund and Yoav.", "year": 1995}, {"title": "A decisiontheoretic generalization of on-line learning and an application to boosting", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["Friedman", "Jerome H"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman and H.,? \\Q2000\\E", "shortCiteRegEx": "Friedman and H.", "year": 2000}, {"title": "A hard-core predicate for all one-way functions", "author": ["Goldreich", "Oded", "Levin", "Leonid"], "venue": "STOC, pp", "citeRegEx": "Goldreich et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Goldreich et al\\.", "year": 1989}, {"title": "Hard-core distributions for somewhat hard problems", "author": ["Impagliazzo", "Russell"], "venue": "In FOCS, pp", "citeRegEx": "Impagliazzo and Russell.,? \\Q1995\\E", "shortCiteRegEx": "Impagliazzo and Russell.", "year": 1995}, {"title": "Cryptographic limitations on learning finite automata and boolean formulae", "author": ["Kearns", "Michael", "Valiant", "Leslie"], "venue": "STOC, pp", "citeRegEx": "Kearns et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1989}, {"title": "The convergence rate of AdaBoost", "author": ["Mukherjee", "Indraneel", "Rudin", "Cynthia", "Schapire", "Robert"], "venue": "In COLT,", "citeRegEx": "Mukherjee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2011}, {"title": "Soft margins for adaboost", "author": ["G. R\u00e4tsch", "T. Onoda", "M\u00fcller", "K.-R"], "venue": "Machine Learning,", "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2001\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2001}, {"title": "Efficient margin maximizing with boosting", "author": ["R\u00e4tsch", "Gunnar", "Warmuth", "Manfred"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2005\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2005}, {"title": "How boosting the margin can also boost classifier complexity", "author": ["Reyzin", "Lev", "Schapire", "Robert E"], "venue": "Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Reyzin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Reyzin et al\\.", "year": 2006}, {"title": "The dynamics of AdaBoost: cyclic behavior and convergence of margins", "author": ["Rudin", "Cynthia", "Daubechies", "Ingrid", "Schapire", "Robert E"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rudin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rudin et al\\.", "year": 2004}, {"title": "Analysis of boosting algorithms using the smooth margin function", "author": ["Rudin", "Cynthia", "Schapire", "Robert E", "Daubechies", "Ingrid"], "venue": "Annals of Statistics,", "citeRegEx": "Rudin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rudin et al\\.", "year": 2007}, {"title": "Boosting: Foundations and Algorithms", "author": ["Schapire", "Robert E", "Freund", "Yoav"], "venue": null, "citeRegEx": "Schapire et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 2012}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["Schapire", "Robert E", "Singer", "Yoram"], "venue": "Machine Learning,", "citeRegEx": "Schapire et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1999}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["Schapire", "Robert E", "Freund", "Yoav", "Barlett", "Peter", "Lee", "Wee Sun"], "venue": "In ICML, pp", "citeRegEx": "Schapire et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1997}, {"title": "On the equivalence of weak learnability and linear separability: New relaxations and efficient boosting algorithms", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram"], "venue": "In COLT, pp", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2008}, {"title": "The Cauchy-Schwarz Master Class", "author": ["Steele", "J. Michael"], "venue": null, "citeRegEx": "Steele and Michael.,? \\Q2004\\E", "shortCiteRegEx": "Steele and Michael.", "year": 2004}, {"title": "A primal-dual convergence analysis of boosting", "author": ["Telgarsky", "Matus"], "venue": null, "citeRegEx": "Telgarsky and Matus.,? \\Q2012\\E", "shortCiteRegEx": "Telgarsky and Matus.", "year": 2012}, {"title": "Totally corrective boosting algorithms that maximize the margin", "author": ["Warmuth", "Manfred K", "Liao", "Jun", "R\u00e4tsch", "Gunnar"], "venue": "In ICML, pp", "citeRegEx": "Warmuth et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Warmuth et al\\.", "year": 2006}, {"title": "Boosting with early stopping: Convergence and consistency", "author": ["Zhang", "Tong", "Yu", "Bin"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2005}, {"title": "The only other thing to check is that ` \u2208 G, the class of losses considered by Telgarsky (2012", "author": [], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Proof sketch. As discussed in the proof of Theorem 4.3, the results of Telgarsky (2012), which are superficially specialized to the Wolfe line search, carry over for the other line searches here with only a change", "author": ["\u2265 \u03b3"], "venue": null, "citeRegEx": "\u03b3\u0302.,? \\Q2012\\E", "shortCiteRegEx": "\u03b3\u0302.", "year": 2012}], "referenceMentions": [{"referenceID": 14, "context": "One explanation for the efficacy of boosting is that it not only seeks aggregates with low empirical risk, but moreover that it prefers good margins, which leads to improved generalization (Schapire et al., 1997).", "startOffset": 189, "endOffset": 212}, {"referenceID": 11, "context": "Since AdaBoost does not attain maximum margins on general instances, a push was made to develop methods which carry such a guarantee (R\u00e4tsch & Warmuth, 2005; Shalev-Shwartz & Singer, 2008; Rudin et al., 2007).", "startOffset": 133, "endOffset": 208}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates.", "startOffset": 21, "endOffset": 42}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.", "startOffset": 21, "endOffset": 424}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al.", "startOffset": 21, "endOffset": 850}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al.", "startOffset": 21, "endOffset": 881}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary).", "startOffset": 21, "endOffset": 906}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme.", "startOffset": 21, "endOffset": 1101}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)).", "startOffset": 21, "endOffset": 1560}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf.", "startOffset": 21, "endOffset": 1773}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk presented here are thus trivial, but included since they did not appear explicitly in the literature.", "startOffset": 21, "endOffset": 2024}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk presented here are thus trivial, but included since they did not appear explicitly in the literature.", "startOffset": 21, "endOffset": 2149}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk presented here are thus trivial, but included since they did not appear explicitly in the literature. It is worth mentioning that these methods produce bad constants when applied to the logistic loss; unfortunately, previous work also suffers in this case (for instance, the work of Collins et al. (2002) provided only convergence of empirical risk, and not rates).", "startOffset": 21, "endOffset": 2508}, {"referenceID": 18, "context": "6). The explicit margin-maximizing method of ShalevShwartz & Singer (2008) requires t \u2265 32 ln(m)/ 2 iterations to achieve margin \u03b3 \u2212 , where \u2208 (0, \u03b3).", "startOffset": 0, "endOffset": 75}, {"referenceID": 6, "context": "position due to Telgarsky (2012), the decomposition itself has appeared, with various guarantees, in numerous places (Goldreich & Levin, 1989; Impagliazzo, 1995; Mukherjee et al., 2011).", "startOffset": 117, "endOffset": 185}, {"referenceID": 18, "context": "Indeed, consider the \u201csoft-margin\u201d boosting method (Shalev-Shwartz & Singer, 2008), originally due to Warmuth et al. (2006), which, roughly speaking, has a parameter controlling how many examples to give up on.", "startOffset": 102, "endOffset": 124}, {"referenceID": 10, "context": ") It should be possible to clarify this behavior from the perspective of dynamical systems: smaller steps dodge bad attractors (Rudin et al., 2004; 2007).", "startOffset": 127, "endOffset": 153}, {"referenceID": 10, "context": "in the case that \u03bd = 1, this quantity has been extensively studied in the context of AdaBoost\u2019s margins (R\u00e4tsch & Warmuth, 2005; Rudin et al., 2004; Schapire & Freund, 2012) \u2666", "startOffset": 104, "endOffset": 173}], "year": 2013, "abstractText": "This manuscript shows that AdaBoost and its immediate variants can produce approximate maximum margin classifiers simply by scaling step size choices with a fixed small constant. In this way, when the unscaled step size is an optimal choice, these results provide guarantees for Friedman\u2019s empirically successful \u201cshrinkage\u201d procedure for gradient boosting (Friedman, 2000). Guarantees are also provided for a variety of other step sizes, affirming the intuition that increasingly regularized line searches provide improved margin guarantees. The results hold for the exponential loss and similar losses, most notably the logistic loss.", "creator": "LaTeX with hyperref package"}}}