{"id": "1303.5720", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "An Approximate Nonmyopic Computation for Value of Information", "abstract": "value - of - information analyses provide a straightforward means for selecting yet reliable next feature to performs, and slowly determining whether experiment is better when gather additional features or better act immediately. determining whether next best test to perform, given a state of uncertainty decision to result, requires a complexity of unpredictable value, making uncertain possible sequences of observations. in return, decision analysts and expert - team designers have avoided the intractability through exact computation interpreting the state of information using relying on a third constraint. classical psychologists were biased on the assumption that typically one additional test please be performed, wherever too there is an explanation ta make a large number satisfactory answers. we design a nonmyopic construct effectively grading against one : bypasses the traditional myopic formula by changing the algebraic properties of material samples.", "histories": [["v1", "Wed, 20 Mar 2013 15:30:51 GMT  (293kb)", "http://arxiv.org/abs/1303.5720v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"], ["v2", "Sat, 16 May 2015 23:55:05 GMT  (388kb)", "http://arxiv.org/abs/1303.5720v2", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["david heckerman", "eric j horvitz", "blackford middleton"], "accepted": false, "id": "1303.5720"}, "pdf": {"name": "1303.5720.pdf", "metadata": {"source": "CRF", "title": "An Approximate N onmyopic Computation for Value of Information", "authors": ["David Heckerman", "Eric Horvitz", "Blackford Middleton"], "emails": [], "sections": [{"heading": null, "text": "1 INTRODUCTION\nA person faced with a decision usually has the op portunity to gather additional information about the state of the world before taking action. Decision theoretic methods for determining the value of gath ering additional information date back to the earli est literature on the principle of maximum expected utility (MEU). These methods form an integral part of many probabilistic expert systems, such as Garry's congestive-heart-failure program (Garry and Barnett, 1968) and Pathfinder (Heckerman et al. , 1989; Hecker man et al., 1990), an expert system that assists pathol ogists with the diagnosis of lymph-node diseases. To decide whether or not to perform a test, an expert system computes the value of information of that test. The system recommends that the test be performed if and only if the value of information exceeds the cost\nof the test.1\nIn most decision contexts, a decision maker has the option to perform several tests, and can decide which test to perform after seeing the results of all previ ous tests. Thus, an expert system should consider the value of all possible sequences of tests. Such an anal ysis is intractable, because the number of sequences grows exponentially with the number of tests. Builders of expert systems have avoided the intractability of complete value-of-information analyses by implement ing myopic or greedy value-of-information analyses. In such analyses, a system determines the next best test by computing value of information based on the as sumption that the decision maker will act immedi ately after seeing the results of the single test (Garry et al., 1973; Heckerman et a!., 1990). In this paper, we present an approximate nonmyopic analysis. The analysis avoids the traditional myopic assumption by making use of the statistical properties of large sam ples.\n2 VALUE-OF-INFORMATION COMPUTATIONS FOR DIAGNOSIS\nWe discuss myopic and nonmyopic value-of-information computations in terms of the sim ple model for diagnosis under uncertainty represented by the influence diagram in Figure 1. In this model, the chance node H represents a mutually exclusive and exhaustive set of possible hypotheses, and the decision node D represents a mutually exclusive and exhaustive set of possible alternatives. The value node U repre sents the utility of the decision maker, which depends on the outcome of H and the decision D. The chance nodes E1, E2, . . . , En are observable pieces of evi dence or tests about the true state of H. This model is identical to that for Pathfinder (Heckerman, 1990).\n1This prescription for action assumes that the delta prop erty holds. See Section 3.\n136 Heckerman, Horvitz, and Middleton\nD\nFigure 1: An influence-diagram representation of the problem of diagnosis under uncertainty. The deci sion-maker's utility (rounded rectangular node, U) de pends on a hypothesis (oval node, H) and a decision (square node, D). The variables E; are pieces of evi dence or tests about the true state of H.\nWe make several simplifying assumptions. First, we assume that H is a binary chance variable and D is a binary decision variable. We use H and -oH to denote the two outcomes of H, and D and -oD to denote the two outcomes of D. For definiteness, we assume that the decision maker chooses D (as opposed to -oD), when H occurs. Second, we assume that each piece of evidence, E1, E2, ... , En, is binary. Finally, we assume that each piece of evidence is conditionally in dependent of all other evidence, given H and --,H. In Section 6, we relax these assumptions.\nUsing the assumption of conditional independence of evidence, we can calculate the posterior probability of the hypothesis by multiplying together all of the likelihood ratios, ; ff,\u2022 \ufffdHJ , with the prior odds, ;(\ufffd\ufffd). p(HIE;, ... , Em)\nP(-oHIE;, ... ,Em) p(E1IH) p(EmiH) p(H)\np(E1I..,H) ... p(Emi..,H) p(--.H) We can write this equation more compactly in odds form as\nm O(HIE;, ... , Em)= O(H) IT>.; (1)\ni=l\nwhere >.; is the likelihood ratio ; ff,\u00b7\ufffd\ufffd .\nBecause D and H are binary, it follows from the MEU principle that there exists a threshold probability p*, such that we should take action D if and only if the probability of H exceeds p*. This threshold is the\nprobability of H at which the decision maker is indif ferent between acting and not acting. That is, p* is the point where acting and not acting have equal utility, or\np\u2022U(H, D) + (1-p\u2022)U( -oH, D)= p\u2022U(H, -oD) + (1- p\u2022)U( -oH, -oD) (2)\nIn Equation 2, U(H, D) is the decision maker's utility for the situation where H occurs and action D is taken, U(H, -oD) is the utility when H occurs and action D is not taken, and so on. Solving Equation 2 for p\u2022, we obtain \u2022 c P = C+B where C is the cost of the decision\nand B is the benefit of the decision\nB = U(H, D)- U(H, -oD)\n(3)\n(5)\nIf the decision maker has observed pieces of evidence E1, \ufffd \u2022. . . ,Em, then the decision maker should choose action D if and only if p(HIE1 ... , Em) > p\u2022. In terms of the odds formulation, he should act if and only if\np\u2022 O(HIE1, ... , Em)\ufffd -1 -. -p (6)\nThe weight of evidence, w;, is defined as the log of the likelihood ratio, In>.;. Mapping likelihood ratios into weights of evidence allows us to update the probabil ity of H through the addition of the weights of evi dence. Referring to Equations 1 and 6, we can rewrite the threshold-probability condition in terms of the log likelihood ratio where w; = In>.;. The decision maker should choose action D if and only if\nm \u2022 W=l:w;\ufffdln1\ufffd. -lnO(H)=W\u00b7 (7)\ni=l p\nIn this expression, w\u2022 is the decision threshold in terms of weights of evidence.\n3 MYOPIC ANALYSIS\nLet us assume that the user of a diagnostic system has instantiated zero or more pieces of evidence in the in fluence diagram shown in Figure 1. We can propagate the effects of these instantiations to the uninstantiated nodes, and remove the instantiated nodes from the in fluence diagram. This removal leaves an influence dia gram of the same form as that shown in Figure l. To simplify our notation, we continue to refer to the re maining pieces of evidence as E1, \ufffd \u2022 . . . , En; also, we use p( H) to refer to the probability of the hypothesis H, given the instantiated evidence. The decision maker now considers whether he should observe another piece of evidence before acting. A\nAn Approximate Nonmyopic Computation for \\alue of Information 137\nmyopic procedure for identifying such evidence com putes, for each piece of evidence, the expected utility of the decision maker under the assumption that the decision maker will act after observing only that piece of evidence. In addition, the procedure computes his expected utility if he does not observe any evidence before making his decision. If, for each piece of evi dence, the expected utility given that evidence is less than the expected utility given no evidence, then the decision maker acts immediately in accordance with Equation 6. Otherwise, the decision maker observes the piece of evidence with the highest expected utility; then, the myopic procedure repeats this computation to identify additional evidence for observation. Because the myopic procedure allows for the gathering of addi tional evidence, the procedure is inconsistent with its own assumptions. We return to this observation in the next section.\nIn the remainder of this section, we examine the com putation of expected utilities and introduce notation. Let EU(E, CE) denote the expected utility of the de cision maker who will observe E at cost CE, and then act. Let CE(E, CE) be the certain equivalent of this situation. That is,\nor\nU(CE(E, CE)) = EU(E, CE) (8)\nCE(E, CE) = u-1(EU(E, CE)) (9) where U(-) is the decision maker's utility function: a monotonic increasing function that maps the value of an outcome (e.g., in dollars) to the decision maker's utility for that outcome. Similarly, let EU(</>, 0) de note the expected utility of the decision maker if he acts immediately, and let CE(</>, 0) denote the certain equivalent of this situation. Thus, in the myopic pro cedure, a decision maker should observe the piece of evidence E for which the quantity\nCE(E, CE)- CE(\u00a2, 0) (10) is maximum, provided it is greater than 0.\nIn this paper, to simplify the discussion, we assume that the delta property holds. 2 The delta property states that an increase in value of all outcomes in a lottery by an amount 6 increases the certain equiva lent of that lottery by 6 (Howard, 1967). Under this assumption, we obtain\nCE(E,CE) = CE(E,O)-CE (11) where CE(E, 0) is the certain equivalent of observing E at no cost. Therefore, we have\nCE(E, CE)- CE(\u00a2, 0) = V I(E) -CE (12) where\nV I(E) = CE(E, 0)- CE(</>, 0) (13)\n2The primary result of this research-that we can use the central-limit theorem to make tractable an approximate nonmyopic analysis-is unaffected by this assumption.\nis the value of information of observing E. 3 The quan tity V I(E) represents the largest amount that the de cision maker would be willing to pay to observe E. When we compare Expression 10 with Equation 12, we see that, in the myopic procedure, a decision maker should observe the piece of evidence E (if any) for which the quantity\nV I(E)- CE =NV I(E) (14) is maximum and positive. We call NVI(E) the net value of information of observing E. The decision maker usually specifies directly the cost of observing evidence. In contrast, we can compute V I(E) from the decision maker's utilities and proba bilities. Specifically, from Equations 9 and 13, we have\nV I(E) = u-1(EU(E, 0))- u-1(EU(</>, 0)) To simplify notation, we use the abbreviations\nEU(E,O) = EU(E) and EU(</>,0) = EU(\u00a2) Thus, we obtain\nVI(E) = u-1(EU(E))-u-1(EU(</>)) (15)\nThe computation of EU(</>) is straightforward. We have\nEU(</>) =\n{ p(H)U(H, \ufffdD)+ p(\ufffdH)U(\ufffdH, \ufffdD), p(H) 5, p\u2022\np(H)U(H, D)+ p(\ufffdH)U(\ufffdH, D),\nby definition of p\u2022.\np(H) > p\u2022 (16)\nTo compute EU(E), let us assume that E is defined such that the observation of E increases the proba bility of H. If p(HIE) > p\u2022 and p(HI\ufffdE) > p\u2022, then VI (E) = 0, because the decision maker will not change his decision if he observes E. Similarly, if p(HIE) < p\u2022 and p(HI-.E) < p+, then VI(E) = 0. Thus, we need only to consider the case where p(HIE) > p\u2022 and p(HI\ufffdE) < p\u2022. Let us consider separately the cases H and -.H. We have\nEU(EIH) = p(EIH)U(H, D)+ p(-.EIH)U(H, \ufffdD)\n(17)\nand\nEU(EI\ufffdH) = p(EI-.H)U(-.H, D)+ p(\ufffdEI-.H)U(\ufffdH, \ufffdD) (18)\nwhere EU(EIH) and EU(EI-.H) are the expected util ities of observing E, given H and \ufffdH, respectively. To obtain the expected utility of observing E, we average these two quantities\nEU(E) = p(H)EU(EIH) + p( -.H)EU(EI\ufffdH) (19) To compute V I(E), we combine Equations 15, 16, and 19.\n30ther names for VI(E) include the value of perfect in formation of E and the value of clairvoyance on E.\n138 Heckerman, Horvitz, and Middleton\n4 NONMYOPIC ANALYSIS\nAs we mentioned in the previous section, the my opic procedure for identifying cost-effective observa tions includes the incorrect assumption that the deci sion maker will act after observing only one piece of evidence. This myopic assumption can affect the di agnostic accuracy of an expert system because infor mation gathering might be halted even though there exists some set of features whose value of information is greater that the cost of its observation. For example, a myopic analysis may indicate that no feature is cost effective for observation, yet the value of information for one or more feature pairs (were they computed) could exceed the cost of their observation.\nThere has been little investigation of the accuracy of myopic analyses. In one analysis, Kalagnanam and Henrion, 1990, showed that a myopic policy is opti mal, when the decision maker's utility function U(\u00b7) is linear, and the relationship between hypotheses and evidence is deterministic. In an empirical study, Garry, 1968, demonstrated that the use of a myopic analysis does not diminish significantly the diagnostic accuracy of an expert system for congenital heart disease.\nIn a correct identification of cost-effective evidence, we should take into account the fact that the deci sion maker may observe more than one piece of evi dence before acting. This computation must consider all possible ordered sequences of evidence observation, and is, therefore, intractable.\nLet us consider, however, the following nonmyopic ap proximation for identifying features that are cost ef fective to observe. Again, we assume that the delta property holds. First, under the myopic assumption, we compute the net value of information for each piece of evidence. If there is at least one piece of evi dence that has a positive net value of information, then we identify for observation the piece of evidence with the highest net value of information. Other wise, we arrange the pieces of evidence in descend ing order of their net values of information. Let us label the pieces of evidence E1, \ufffd, . . . , E,_., such that NVI(E;) > NVI(Ei), if and only if i > j. Next, we compute the net value of information of each subsequence of E1, E2, . . \u2022 , E,_.. That is, form= 1, 2, ... n, we compute the difference between the value of information for observing E1, E2, . . \u2022 , Em, and the cost of observing this sequence of evidence. If any such net value of information is greater than 0, then we identify E1 as a piece of evidence that is cost effec tive to observe. Once the decision maker has observed E1. we repeat the entire computation described in this section.\nThis approach does not consider all possible test se quences, but it does overcome one limitation of the myopic analysis. In particular, the method can iden-\ntify sets of features that are cost effective for observa tion, even when the observation of each feature alone is not cost effective.\n5 VALUE OF INFORMATION FOR A SUBSET OF EVIDENCE\nAs in the myopic analysis, we assume that the decision maker can specify the cost of observing a set of evi dence. In this section, we show how we can compute the value of information for a set of evidence from the decision maker's utilities and probabilities.\nAs in the previous section, let us suppose that the decision maker has the option to observe a particu lar subset of evidence { E1, \ufffd, ... , Em} before acting. There are 2m possible instantiations of the evidence in this set, corresponding to the observation of E; or -.E; for every i. Let E denote an arbitrary instantiation; and let Ev and E\ufffdv denote the set of instantiations E such that p(HIE) > p\u2022 and p(HIE) \ufffd p\u2022, respectively. The computation of the value of information for the observation of the set { E1, \ufffd' . . . ,Em} parallels the myopic computation. In particular, we have\nwhere\nand\nEU(E1 ... Em)= p(H)EU(E1 ... Em I H)+ (20) p(-.H)EU(El ... Emi-.H)\nEU(E1 . . . Em I H)= [Eee\u00a3n p(EIH)] U(H, D)+ (21) (L:eee\ufffdn p(EIH)) U(H, -.D)\nEU(E1 ... Emi-.H) = (I:eeen p(EI-.H)) U(-.H, D)+ (22) (I:eee\ufffdn p(\u00a31-.H)) U(-.H, -.D)\nTo obtain V I(E), we combine Equations 15, 16, and 20.\nWhen m is small, we can compute directly the sums in Equations 21 and 22. When m is large, we can com pute these sums using an approximation that involves the central limit theorem as follows. First we express the sums in terms of weights of evidence. We have\nL p(EIH) = p(W > w\u2022IH) (23) \u00a3E\u00a3n\nL p(EI-.H) = p(W > W*I-.H) (24) \u00a3E\u00a3v\nL p(EIH)) = 1- p(W > W*IH) (25) \u00a3E\u00a3\ufffdv L p(EI-.H)) = 1- p(W > W*I-.H) (26) \u00a3EC\ufffdv\nAn Approximate Nonmyopic Computation for \\alue of Information 139\nwhere W and w\u2022 are defined in Equation 7. The term p(W > W'IH), for example, is the probability that the sum of the weight of evidence from the observation of E1, \ufffd . . .. , Em exceeds w\u2022. That is, p(W > w\u2022IH) is the probability that the decision maker will take action D after observing the evidence, given that H is true. Next, let us consider the weight of evidence for one piece of evidence. We have\nw;\nIn p(E;!H) p(E;J\ufffdH) In P(\ufffdE,jH) p(\ufffdE,J\ufffdH)\np(w;IHl pjw;J_-,Hl\np(E;IH) p(E;I..,H)\np(..,E;IH) p(..,E;I..,H)\nTo simplify notation, we let p(E;IH) = a and p(E;I..,H) = fl. The expectation and variance of w, given H and ..,H, are then\na (I -a) EV(wiH) = a ln \"8 + (1- a) In (1 _ fl) (27)\n2a(1-fJ) Var(wiH) =a( I- a)ln {3(1 _a) (28) a (1 -a) EV(wi..,H) = fJ in p + (1-{3) In (1 _ fl) (29)\nVar(wi..,H) = {3(1 - fJ)ln2 ;\ufffd\ufffd = \ufffd; (30) Now, we take advantage of the additive property of weights of evidence. The central-limit theorem states that the sum of independent random variables ap proaches a normal distribution when the number of variables becomes large. Furthermore, the expecta tion and variance of the sum is just the sum of the expectations and variances of the individual random variables, respectively. Because we have assumed that evidence variables are independent, given H or ..,H, the expected value of the sum of the weights of evi dence for E1, \ufffd \u2022 . . . ,Em is\nm EV(WIH) = LEV(w;IH) (31)\n'i=l\nThe variance of the sum of the weights is m\nVar(WIH) = LVar(w;IH) (32) i=l\nThus, p(WIH), the probability distribution over W, is m m\np(WIH) \ufffd N(LEV(w;IH), LVar(wdH)) (33) i=l i=l\nThe expression for ..,H is similar.\np(W1H)\nw w*\nFigure 2: The probability that the total weight of evi dence will exceed the threshold weight is the area un der the normal curve above the threshold weight w\u2022 (shaded region).\nFinally, given the distributions for H and ..,H, we eval uate Equations 23 through 26 using an estimate or table of the cumulative normal distribution. We have\n1 1\"\" -(\u00b7,-\ufffdl' p(W>W.IH)= = e \u2022 dt ay271' w\u00b7\n(34)\nwhere J.L = EV(WIH) and u = Var(WIH). The prob ability that the weight will exceed w\u2022 corresponds to the shaded area in Figure 2. Again, the expression for ..,H is similar. In this analysis, we assume that no probability (p(E;IH) or p(E;I..,H)) is equal to 0 or 1. Thus, all expected values and variances are finite. We relax this assumption in the next section.\n6 RELAXATION OF THE ASSUMPTIONS\nWe can relax the assumption that evidence is two valued with little effort. In particular, we can extend easily the odds-likelihood inference rule, Equation 1, and its logarithmic transform, to include multiple valued evidential variables. In addition, the computa tion of means and variances for multiple-valued eviden tial variables (see Equations 27 through 30) is straight forward.\nIn addition, we can relax the assumption that no prob ability is equal to 0 or 1. For example, let us suppose that\n0 < p(EiiH) =a< 1 p(Eii..,H) = {3 = 1 o < p(E;IH) < I, i=l,2, ... ,n(i#j)\n0 < p(E;I..,H) < 1, i = 1,2, ... ,n (i #j)\nUsing Equations 27 through 30, we obtain\nEV(wjiH) = +oo\n140 Heckerman, Horvitz, and Middleton\nVar(wiiH) EV(wiJ\ufffdH)\nVar(wii\ufffdH)\n+oo\n< 0\n0\nTherefore, although the computation of p(W > w\u2022J\ufffdH) is straightforward, we cannot compute p(W > w\u2022JH) as described in the previous section. Instead, we compute p(W > w\u2022JH), by considering separately the cases Ei and \ufffdEi. We have\np(W > w\u2022JH) p(EiiH) p(W > w\u2022JH Ei) + p(\ufffdEiiH) p(W > w\u2022JH\ufffdEi)\n(35)\nIf \ufffdEi is observed, W = +oo, and p(W > (b) w\u2022JH\ufffdEi) = 1. Consequently, Equation 35 becomes\np(W > w\u2022JH) = p(EiiH) p(W > w\u2022JHEi) + p(\ufffdEJJH)\nWe compute p(W > w\u2022JHEJ) as described in Equa tions 31 through 34, replacing EV(wiiH) with Wj in the summation of Equation 31, and Var(w1JH) with 0 in the summation of Equation 32. The other terms in the summations remain the same, because we have as sumed that evidence variables are independent, given H or \ufffdH. This approach generalizes easily to multiple valued evidence variables and to cases where more than one probability is equal to 0 or 1.\nWe can extend our analysis to special cases of condi tional dependence among evidence variables. For ex ample, Figure 3 shows a schematic of the belief net work for Pathfinder. In this model, there are groups of dependent evidence, where each group is condition ally independent of all other groups. We can apply our analysis to this model by using a clustering tech nique described by Pearl (Pearl, 1988) (pp. 197-204). As in the previous section, suppose we want to com pute the value of information for the set of evidence S = {E1, E2, ... , Em}. For each group of dependent features Gk, we cluster those variables in the inter section of S and Gk into a single variable. Then, we average out all variables in the belief network that are not in S. What remains is a set of clustered vari ables that are conditionally independent, given H and \ufffdH. We can now apply our analysis-generalized to multiple-valued variables-to this model.\nThere are special classes of dependent distributions for which the central-limit theorem is valid. We can use this fact to extend our analysis to other cases of depen dent evidence. For example, the central-limit theorem applies to distributions that form a Markov chain, pro vided the transition probabilities in the chain are not correlated (Billingsley, 1968). Thus, we can extend our analysis to belief networks of the form shown in Figure 4. We can generalize the value-of-information analysis even further, if we use the Markov extension in combination with the clustering approach described in the previous paragraph.\nFigure 3: A schematic belief network for Pathfinder. (a) The features in Pathfinder can be arranged into groups of evidence variables G1, G2, ... Gi. The vari ables within each group are dependent, but the groups are conditionally independent, given the disease vari able H. (b) A detailed view of the evidence variables E;, E;+l, and Ei+2 within group Gk.\nFigure 4: A conditional Markov chain. The evidence variables form a Markov chain conditioned on the vari able H. We can extend our analysis involving the cen tral-limit theorem to this case.\nAn Approximate Nonmyopic Computation for \\alue of Information 141\nIt is difficult for us to extend the analysis to include multiple-valued hypotheses and decisions. The algebra becomes more complex, because the simple p\u2022 model for action no longer applies. There is, however, the opportunity for applying our technique to more com plex problems. In particular, we can abstract a given decision problem into one involving a binary hypoth esis and decision variable. For example, we can ab stract the problem of determining which of n diseases is present in a patient into one of determining whether the disease is benign or malignant. In doing so, we ignore details of the decision maker's preferences, and we introduce dependencies among evidence variables. Nonetheless, the benefits of a nonmyopic analysis may outweigh these drawbacks in some domains.\n7 SUMMARY AND CONCLUSIONS\nWe presented work on the use of the central-limit the orem to compute the value of information for sets of tests. Our technique provides a nonmyopic, yet tractable alternative to traditional myopic analyses for determining the next best piece of evidence to observe. Our approach is limited to information-acquisition de cisions for problems involving (1) specific classes of de pendencies among evidence variables, and (2) binary hypothesis and action variables. Additional research, however, may help to relax these restrictions. For now, we pose the nonmyopic methodology as a new special case tool for identifying cost-effective observations. We hope to see empirical comparisons of the relative accu racy of the nonmyopic analysis with that of traditional myopic analyses. We expect that the results of such evaluations will be sensitive to the details of the ap plication areas.\nAcknowledgments\nThis work was supported by the National Cancer In stitute under Grant R01CA51729-01A1, and by the Agency for Health Care Policy and Research under Grant T2HS00028.\nReferences\nBillingsley, P. (1968). Dependent variables. In Con vergence of Probability Measures, chapter 4. Wiley and Sons, New York.\nCorry, G. and Barnett, G. (1968). Experience with a model of sequential diagnosis. Computers and Biomedical Research, 1:490-507.\nCorry, G., Kassirer, J., Essig, A., and Schwartz, W. ( 1973). Decision analysis as the basis for computer-aided management of acute renal fail ure. American Journal of Medicine, 55:473-484.\nHeckerman, D. (1990). Probabilistic Similarity Net works. PhD thesis, Program in Medical Informa tion Sciences, Stanford University, Stanford, CA. Report STAN-CS-90-1316.\nHeckerman, D., Horvitz, E., and Nathwani, B. (1989). Update on the Pathfinder project. In Proceedings of the Thirteenth Symposium on Computer Appli cations in Medical Care, Washington, DC, pages 203-207. IEEE Computer Society Press, Silver Spring, MD.\nHeckerman, D., Horvitz, E., and Nathwani, B. (1990). Toward normative expert systems: The Pathfinder project. Technical Report KSL-9008, Medical Computer Science Group, Section on Medical Informatics, Stanford University, Stan ford, CA.\nHoward, R. (1967). Value of information lotteries. IEEE Transactions of Systems Science and Cy bernetics, SSC-3(1):54-60.\nKalagnanam, J. and Henrion, M. (1990). A compar ison of decision analysis and expert rules for se quential diagnosis. In Shachter, R., Kana!, L., Levitt, T., and Lemmer, J., editors, Uncertainty in Artificial Intelligence 4, pages 271-281. North Holland, New York.\nPearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Mor gan Kaufmann, San Mateo, CA."}], "references": [{"title": "Dependent variables. In Con\u00ad vergence of Probability Measures, chapter 4", "author": ["P. Billingsley"], "venue": null, "citeRegEx": "Billingsley,? \\Q1968\\E", "shortCiteRegEx": "Billingsley", "year": 1968}, {"title": "Experience with a model of sequential diagnosis", "author": ["G. Corry", "G. Barnett"], "venue": "Computers and Biomedical Research,", "citeRegEx": "Corry and Barnett,? \\Q1968\\E", "shortCiteRegEx": "Corry and Barnett", "year": 1968}, {"title": "Decision analysis as the basis for computer-aided management of acute renal fail\u00ad ure", "author": ["G. Corry", "J. Kassirer", "A. Essig", "W. Schwartz"], "venue": null, "citeRegEx": "Corry et al\\.,? \\Q1973\\E", "shortCiteRegEx": "Corry et al\\.", "year": 1973}, {"title": "Probabilistic Similarity Net\u00ad works. PhD thesis, Program in Medical Informa\u00ad", "author": ["D. Heckerman"], "venue": null, "citeRegEx": "Heckerman,? \\Q1990\\E", "shortCiteRegEx": "Heckerman", "year": 1990}, {"title": "Update on the Pathfinder project", "author": ["D. Heckerman", "E. Horvitz", "B. Nathwani"], "venue": "In Proceedings of the Thirteenth Symposium on Computer Appli\u00ad cations in Medical Care,", "citeRegEx": "Heckerman et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1989}, {"title": "Toward normative expert systems: The Pathfinder project", "author": ["D. Heckerman", "E. Horvitz", "B. Nathwani"], "venue": "Technical Report KSL-9008,", "citeRegEx": "Heckerman et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1990}, {"title": "Value of information lotteries", "author": ["R. Howard"], "venue": "IEEE Transactions of Systems Science and Cy\u00ad bernetics,", "citeRegEx": "Howard,? \\Q1967\\E", "shortCiteRegEx": "Howard", "year": 1967}, {"title": "A compar\u00ad ison of decision analysis and expert rules for se\u00ad quential diagnosis", "author": ["J. Kalagnanam", "M. Henrion"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "Kalagnanam and Henrion,? \\Q1990\\E", "shortCiteRegEx": "Kalagnanam and Henrion", "year": 1990}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Mor\u00ad", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}], "referenceMentions": [{"referenceID": 3, "context": "This model is identical to that for Pathfinder (Heckerman, 1990).", "startOffset": 47, "endOffset": 64}, {"referenceID": 6, "context": "2 The delta property states that an increase in value of all outcomes in a lottery by an amount 6 increases the certain equiva\u00ad lent of that lottery by 6 (Howard, 1967).", "startOffset": 154, "endOffset": 168}, {"referenceID": 8, "context": "We can apply our analysis to this model by using a clustering tech\u00ad nique described by Pearl (Pearl, 1988) (pp.", "startOffset": 93, "endOffset": 106}, {"referenceID": 0, "context": "For example, the central-limit theorem applies to distributions that form a Markov chain, pro\u00ad vided the transition probabilities in the chain are not correlated (Billingsley, 1968).", "startOffset": 162, "endOffset": 181}], "year": 2011, "abstractText": "Value-of-information analyses provide a straightforward means for selecting the best next observation to make, and for determin\u00ad ing whether it is better to gather additional information or to act immediately. Deter\u00ad mining the next best test to perform, given a state of uncertainty about the world, requires a consideration of the value of making all pos\u00ad sible sequences of observations. In practice, decision analysts and expert-system design\u00ad ers have avoided the intractability of exact computation of the value of information by relying on a myopic approximation. Myopic analyses are based on the assumption that only one additional test will be performed, even when there is an opportunity to make a large number of observations. We present a nonmyopic approximation for value of infor\u00ad mation that bypasses the traditional myopic analyses by exploiting the statistical proper\u00ad ties of large samples.", "creator": "pdftk 1.41 - www.pdftk.com"}}}