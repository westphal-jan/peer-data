{"id": "1303.5984", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2013", "title": "Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems", "abstract": "another study the problem against adaptive behavior of a high dimensional linear quadratic ( lq ) response. previous work describes an asymptotic convergence to have optimal trajectory for global adaptive control scales. from recently, for the exponential allocation analysis problem, path regret \u03b3 - $ { o } ( \\ ric { t } ) $ was measured, apart via path parameters. likewise, that bound lives constantly until $ p $, average bandwidth of the resulting space. in this fashion authors consider the allocation avoiding the constraint where the priorities of quantum lq system are rich and resource dimensions are low. we investigate an adequate control scheme that achieves expected regret coefficient of $ { tc } ( p \\ sqrt { pts } ) $, apart produce logarithmic factors. since such, our case considers an average cost of $ ( \u03bb + \\ eps ) $ times the optimum memory after $ t = \\ n ( p ) o ( 1 / \\ rs ^ 00 ) $. this is joining two similar previous bounds on autonomous ensemble dynamics where the network requires time that flies heavily with algorithms by order algorithms achieve regret of $ \\ eps $ times the overall resource.", "histories": [["v1", "Sun, 24 Mar 2013 19:56:49 GMT  (26kb)", "http://arxiv.org/abs/1303.5984v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["morteza ibrahimi", "adel javanmard", "benjamin van roy"], "accepted": true, "id": "1303.5984"}, "pdf": {"name": "1303.5984.pdf", "metadata": {"source": "CRF", "title": "Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems", "authors": ["Morteza Ibrahimi"], "emails": ["ibrahimi@stanford.edu", "adelj@stanford.edu", "bvr@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 3.\n59 84\nv1 [\nst at\n.M L\n] 2\n4 M\n\u221a T ) was shown, apart form logarithmic\nfactors. However, this bound scales exponentially with p, the dimension of the state space. In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large. We present an adaptive control scheme that achieves a regret bound of O(p \u221a T ), apart from logarithmic factors. In particular, our algorithm has an average cost of (1 + \u01eb) times the optimum cost after T = polylog(p)O(1/\u01eb2). This is in comparison to previous work on the dense dynamics where the algorithm requires time that scales exponentially with dimension in order to achieve regret of \u01eb times the optimal cost. We believe that our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks."}, {"heading": "1 Introduction", "text": "In this paper we address the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Formally, the dynamics of a linear quadratic system are given by\nx(t+ 1) = A0x(t) +B0u(t) + w(t + 1),\nc(t) = x(t)TQx(t) + u(t)TRu(t), (1)\nwhere u(t) \u2208 Rr is the control (action) at time t, x(t) \u2208 Rp is the state at time t, c(t) \u2208 R is the cost at time t, and {w(t + 1)}t\u22650 is a sequence of random vectors in Rp with i.i.d. standard Normal entries. The matrices Q \u2208 Rp\u00d7p and R \u2208 Rr\u00d7r are positive semi-definite (PSD) matrices that determine the cost at each step. The evolution of the system is described through the matrices A0 \u2208 Rp\u00d7p and B0 \u2208 Rp\u00d7r. Finally by high dimensional system we mean the case where p, r \u226b 1. A celebrated fundamental theorem in control theory asserts that the above LQ system can be optimally controlled by a simple linear feedback if the pair (A0, B0) is controllable and the pair (A0, Q1/2) is observable. The optimal controller can be explicitly computed from the matrices describing the dynamics and the cost. Throughout this paper we assume that controllability and observability conditions hold.\nWhen the matrix \u03980 \u2261 [A0, B0] is unknown, the task is that of adaptive control, where the system is to be learned and controlled at the same time. Early works on the adaptive control of LQ systems relied on the certainty equivalence principle [2]. In this scheme at each time t the unknown parameter \u03980 is estimated based on the observations collected so far and the optimal controller for the\nestimated system is applied. Such controllers are shown to converge to an optimal controller in the case of minimum variance cost, however, in general they may converge to a suboptimal controller [11]. Subsequently, it has been shown that introducing random exploration by adding noise to the control signal, e.g., [14], solves the problem of converging to suboptimal estimates.\nAll the aforementioned work have been concerned with the asymptotic convergence of the controller to an optimal controller. In order to achieve regret bounds, cost-biased parameter estimation [12, 8, 1], in particular the optimism in the face of uncertainty (OFU) principle [13] has been shown to be effective. In this method a confidence set S is found such that \u03980 \u2208 S with high probability. The system is then controlled using the most optimistic parameter estimates, i.e., \u0398\u0302 \u2208 S with the smallest optimum cost. The asymptotic convergence of the average cost of OFU for the LQR problem was shown in [6]. This asymptotic result was extended in [1] by providing a bound for the cumulative regret. Assume x(0) = 0 and for a control policy \u03c0 define the average cost\nJ\u03c0 = limsup T\u2192\u221e\n1\nT\nT\u2211\nt=0\nE[ct] . (2)\nFurther, define the cumulative regret as\nR(T ) =\nT\u2211\nt=0\n(c\u03c0(t)\u2212 J\u2217) , (3)\nwhere c\u03c0(t) is the cost of control policy \u03c0 at time t and J\u2217 = J(\u03980) is the optimal average cost. The algorithm proposed in [1] is shown to have cumulative regret O\u0303( \u221a T ) where O\u0303 is hiding the logarithmic factors. While no lower bound was provided for the regret, comparison with the multiarmed bandit problem where a lower bound of O( \u221a T ) was shown for the general case [9], suggests that this scaling with time for the cumulative regret is optimal.\nThe focus of [1] was on scaling of the regret with time horizon T . However, the regret of the proposed algorithm scales poorly with dimension. More specifically, the analysis in [1] proves a regret bound of R(T ) < Cpp+r+2 \u221a T . The current paper focuses on (many) applications where the state and control dimensions are much larger than the time horizon of interest. A powerful reinforcement learning algorithm for these applications should have regret which depends gracefully on dimension. In general, there is little to be achieved when T < p as the number of degrees of freedom (pr + p2) is larger than the number of observations (Tp) and any estimator can be arbitrary inaccurate. However, when there is prior knowledge about the unknown parameters A0, B0, e.g., when A0, B0 are sparse, accurate estimation can be feasible. In particular, [3] proved that under suitable conditions the unknown parameters of a noise driven system (i.e., no control) whose dynamics are modeled by linear stochastic differential equations can be estimated accurately with as few as O(log(p)) samples. However, the result of [3] is not directly applicable here since for a general feedback gain L even if A0 and B0 are sparse, the closed loop gain A0 \u2212 B0L need not be sparse. Furthermore, system dynamics would be correlated with past observations through the estimated gain matrix L. Finally, there is no notion of cost in [3] while here we have to obtain bounds on cost and its scaling with p. In this work we extend the result of [3] by showing that under suitable conditions, unknown parameters of sparse high dimensional LQ systems can be accurately estimated with as few as O(log(p + r)) observations. Equipped with this efficient learning method, we show that sparse high dimensional LQ systems can be adaptively controlled with regret O\u0303(p \u221a T ).\nTo put this result in perspective note that even when x(t) = 0, the expected cost at time t + 1 is \u2126(p) due to the noise. Therefore, the cumulative cost at time T is bounded as \u2126(pT ). Comparing this to our regret bound, we see that for T = polylog(p)O( 1\u01eb2 ), the cumulative cost of our algorithm is bounded by (1 + \u01eb) times the optimum cumulative cost. In other words, our algorithm performs close to optimal after polylog(p) steps. This is in contrast with the result of [1] where the algorithm needs \u2126(p2p) steps in order to achieve regret of \u01eb times the optimal cost.\nSparse high dimensional LQ systems appear in many engineering applications. Here we are particularly motivated by an emerging field of applications in marketing and advertising. The use of dynamical optimal control models in advertising has a history of at least four decades, cf. [17, 10] for a survey. In these models, often a partial differential equation is used to describe how advertising expenditure over time translates into sales. The basic problem is to find the advertising expenditure that maximizes the net profit. The focus of these works is to model the temporal dynamics of\nthe advertising expenditure (the control variable) and the variables of interest (sales, goodwill level, etc.). There also exists a rich literature studying the spatial interdependence of consumers\u2019 and firms\u2019 behavior to devise marketing schemes [7]. In these models space can be generalized beyond geographies to include notions like demographies and psychometry.\nCombination of spatial interdependence and temporal dynamics models for optimal advertising was also considered [16, 15]. A simple temporal dynamics model is extended in [15] by allowing state and control variables to have spatial dependence and introducing a diffusive component in the controlled PDE which describes the spatial dynamics. The controlled PDE is then showed to be equivalent to an abstract linear control system of the form\ndx(t)\ndt = Ax(t) +Bu(t). (4)\nBoth [15] and [7] are concerned with the optimal control and the interactions are either dictated by the model or assumed known. Our work deals with a discrete and noisy version of (4) where the dynamics is to be estimated but is known to be sparse. In the model considered in [15] the state variable x lives in an infinite dimensional space. Spatial models in marketing [7] usually consider state variables which have a large number of dimensions, e.g., number of zip codes in the US (\u223c 50K). High dimensional state space and control is a recurring theme in these applications. In particular, with the modern social networks customers are classified in a highly granular way, potentially with each customer representing his own class. With the number of classes and complexity of their interactions, its unlikely that we could formulate an effective model a priori for how classes interact. Further, the nature of these interactions change over time with the changing landscape of Internet services and information available to customers. This makes it important to efficiently learn from real-time data about the nature of these interactions.\nNotation: We bundle the unknown parameters into one variable \u03980 = [A0, B0] \u2208 Rp\u00d7q where q = p+ r and call it the interaction matrix. For v \u2208 Rn, M \u2208 Rm\u00d7n and p \u2265 1, we denote by \u2016v\u2016p the standard p-norm and by \u2016M\u2016p the corresponding operator norm. For 1 \u2264 i \u2264 m, Mi represents the ith row of matrix M . For S \u2286 [m], J \u2286 [n], MSJ is the submatrix of M formed by the rows in S and columns in J . For a set S denote by |S| its cardinality. For an integer n denote by [n] the set {1, . . . , n}."}, {"heading": "2 Algorithm", "text": "Our algorithm employs the Optimism in the Face of Uncertainty (OFU) principle in an episodic fashion. At the beginning of episode i the algorithm constructs a confidence set \u2126(i) which is guaranteed to include the unknown parameter \u03980 with high probability. The algorithm then chooses \u0398\u0303(i) \u2208 \u2126(i) that has the smallest expected cost as the estimated parameter for episode i and applies the optimal control for the estimated parameter during episode i.\nThe confidence set is constructed using observations from the last episode only but the length of episodes are chosen to increase geometrically allowing for more accurate estimates and shrinkage of the confidence set by a constant factor at each episode. The details of each step and the pseudo code for the algorithm follows.\nConstructing confidence set: Define \u03c4i to be the start of episode i with \u03c40 = 0. Let L(i) be the controller that has been chosen for episode i. For t \u2208 [\u03c4i, \u03c4i+1) the system is controlled by u(t) = \u2212L(i)x(t) and the system dynamics can be written as x(t+1) = (A0\u2212B0L(i))x(t)+w(t+1). At the beginning of episode i+1, first an initial estimate \u0398\u0302 is obtained by solving the following convex optimization problem for each row \u0398u \u2208 Rq separately:\n\u0398\u0302(i+1)u \u2208 argmin L(\u0398u) + \u03bb\u2016\u0398u\u20161, (5) where\nL(\u0398u) = 1\n2\u2206\u03c4i+1\n\u03c4i+1\u22121\u2211\nt=\u03c4i\n{xu(t+ 1)\u2212\u0398uL\u0303(i)x(t)}2, \u2206\u03c4i+1 = \u03c4i+1 \u2212 \u03c4i, (6)\nALGORITHM: Reinforcement learning algorithm for LQ systems.\nInput: Precision \u01eb, failure probability 4\u03b4, initial (\u03c1, Cmin, \u03b1) identifiable controller L(0), \u2113(\u03980, \u01eb) Output: Series of estimates \u0398\u0303(i), confidence sets \u2126(i) and controllers L(i)\n1: Let \u21130 = max(1,maxj\u2208[r] \u2016L(0)j \u20162), and\nn0 = 4 \u00b7 103 k2\u211320\n\u03b1(1\u2212 \u03c1)C2min\n( 1\n\u01eb2 +\nk (1 \u2212 \u03c1)2 ) log( 4kq \u03b4 ),\nn1 = 4 \u00b7 103 k2\u2113(\u03980, \u01eb)2\n(1 \u2212 \u03c1)C2min\n( 1\n\u01eb2 +\nk (1 \u2212 \u03c1)2 ) log( 4kq \u03b4 ).\nLet \u2206\u03c40 = n0, \u2206\u03c4i = 4i(1 + i/ log(q/\u03b4))n1 for i \u2265 1, and \u03c4i = \u2211i\nj=0 \u2206\u03c4j . 2: for i = 0, 1, 2, . . . do 3: Apply the control u(t) = \u2212L(i)x(t) until \u03c4i+1 \u2212 1 and observe the trace {x(t)}\u03c4i\u2264t<\u03c4i+1 . 4: Calculate the estimate \u0398\u0302(i+1) from (5) and construct the confidence set \u2126(i+1). 5: Calculate \u0398\u0303(i+1) from (9) and set L(i+1) \u2190 L(\u0398\u0303(i+1)).\nand L\u0303(i) = [I,\u2212L(i)T]T. The estimator \u0398\u0302u is known as the LASSO estimator. The first term in the cost function is the normalized negative log likelihood which measures the fidelity to the observations while the second term imposes the sparsity constraint on \u0398u. \u03bb is the regularization parameter.\nFor \u0398(1),\u0398(2) \u2208 Rp\u00d7q define the distance d(\u0398(1),\u0398(2)) as d(\u0398(1),\u0398(2)) = max\nu\u2208[p] \u2016\u0398(1)u \u2212\u0398(2)u \u20162, (7)\nwhere \u0398u is the uth row of the matrix \u0398. It is worth noting that for k-sparse matrices with k constant, this distance does not scale with p or q. In particular, if the absolute value of the elements of \u0398(1) and \u0398(2) are bounded by \u0398max then d(\u0398(1),\u0398(2)) \u2264 2 \u221a k\u0398max.\nHaving the estimator \u0398\u0302(i) the algorithm constructs the confidence set for episode i as\n\u2126(i) = {\u0398 \u2208 Rp\u00d7q | d(\u0398, \u0398\u0302(i)) \u2264 2\u2212i\u01eb}, (8) where \u01eb > 0 is an input parameter to the algorithm. For any fixed \u03b4 > 0, by choosing \u03c4i judiciously we ensure that with probability at least 1\u2212 \u03b4, \u03980 \u2208 \u2126(i), for all i \u2265 1. (see Theorem 3.2). Design of the controller: Let J(\u0398) be the minimum expected cost if the interaction matrix is \u0398 = [A,B] and denote by L(\u0398) the optimal controller that achieves the expected cost J(\u0398). The algorithm implements OFU principle by choosing, at the beginning of episode i, an estimate \u0398\u0303(i) \u2208 \u2126(i) such that\n\u0398\u0303(i) \u2208 argmin \u0398\u2208\u2126(i) J(\u0398). (9)\nThe optimal control corresponding to \u0398\u0303(i) is then applied during episode i, i.e., u(t) = \u2212L(\u0398\u0303(i))x(t) for t \u2208 [\u03c4i, \u03c4i+1). Recall that for \u0398 = [A,B], the optimal controller is given through the following relations\nK(\u0398) = Q+ATK(\u0398)A\u2212ATK(\u0398)B(BTK(\u0398)B +R)\u22121BTK(\u0398)A , (Riccati equation) L(\u0398) = (BTK(\u0398)B +R)\u22121BTK(\u0398)A .\nThe pseudo code for the algorithm is summarized in the table."}, {"heading": "3 Main Results", "text": "In this section we present performance guarantees in terms of cumulative regret and learning accuracy for the presented algorithm. In order to state the theorems, we first need to present some assumptions on the system.\nGiven \u0398 \u2208 Rp\u00d7q and L \u2208 Rr\u00d7p, define L\u0303 = [I,\u2212LT]T \u2208 Rq\u00d7p and let \u039b \u2208 Rp\u00d7p be a solution to the following Lyapunov equation \u039b\u2212\u0398L\u0303\u039bL\u0303T\u0398T = I. (10) If the closed loop system (A0\u2212B0L) is stable then the solution to the above equation exists and the state vector x(t) has a Normal stationary distribution with covariance \u039b.\nWe proceed by introducing an identifiable regulator.\nDefinition 3.1. For a k-sparse matrix \u03980 = [A0, B0] \u2208 Rp\u00d7q and L \u2208 Rr\u00d7p, define L\u0303 = [I,\u2212LT]T \u2208 Rq\u00d7p and let H = L\u0303\u039bL\u0303T where \u039b is the solution of Eq. (10) with \u0398 = \u03980. Define L to be (\u03c1, Cmin, \u03b1) identifiable (with respect to \u03980) if it satisfies the following conditions for all S \u2286 [q], |S| \u2264 k.\n(1) \u2016A0 \u2212B0L\u20162 \u2264 \u03c1 < 1, (2) \u03bbmin(HSS) \u2265 Cmin, (3) \u2016HScSH\u22121SS\u2016\u221e \u2264 1\u2212 \u03b1.\nThe first condition simply states that if the system is controlled using the regulator L then the closed loop autonomous system is asymptotically stable. The second and third conditions are similar to what is referred to in the sparse signal recovery literature as the mutual incoherence or irreprepresentable conditions. Various examples and results exist for the matrix families that satisfy these conditions [18]. Let S be the set of indices of the nonzero entries in a specific row of \u03980. The second condition states that the corresponding entries in the extended state variable y = [xT, uT] are sufficiently distinguishable from each other. In other words, if the trajectories corresponding to this group of state variables are observed, non of them can be well approximated as a linear combination of the others. The third condition can be thought of as a quantification of the first vs. higher order dependencies. Consider entry j in the extended state variable. Then, the dynamic of yj is directly influenced by entries yS . However they are also influenced indirectly by other entries of y. The third condition roughly states that the indirect influences are sufficiently weaker than the direct influences. There exists a vast literature on the applicability of these conditions and scenarios in which they are known to hold. These conditions are almost necessary for the successful recovery by \u21131 relaxation. For a discussion on these and other similar conditions imposed for sparse signal recovery we refer the reader to [19] and [20] and the references therein.\nDefine \u0398min = mini\u2208[p],j\u2208[q],\u03980 ij 6=0 |\u03980ij |. Our first result states that the system can be learned efficiently from its trajectory observations when it is controlled by an identifiable regulator.\nTheorem 3.2. Consider the LQ system of Eq. (1) and assume \u03980 = [A0, B0] is k-sparse. Let u(t) = \u2212Lx(t) where L is a (\u03c1, Cmin, \u03b1) identifiable regulator with respect to \u03980 and define \u2113 = max(1,maxj\u2208[r] \u2016Lj\u20162). Let n denote the number of samples of the trajectory that is observed. For any 0 < \u01eb < min(\u0398min, \u21132 , 3 1\u2212\u03c1), there exists \u03bb such that, if\nn \u2265 4 \u00b7 10 3 k2\u21132\n\u03b12(1\u2212 \u03c1)C2min\n( 1\n\u01eb2 +\nk (1\u2212 \u03c1)2 ) log( 4kq \u03b4 ) , (11)\nthen the \u21131-regularized least squares solution \u0398\u0302 of Eq. (5) satisfies d(\u0398\u0302,\u03980) \u2264 \u01eb with probability larger than 1\u2212 \u03b4. In particular, this is achieved by taking \u03bb = 6\u2113 \u221a log(4q/\u03b4)/(n\u03b12(1 \u2212 \u03c1)) .\nOur second result states that equipped with an efficient learning algorithm, the LQ system of Eq. (1) can be controlled with regret O\u0303(p \u221a T log 3 2 (1/\u03b4)) under suitable assumptions.\nDefine an \u01eb-neighborhood of \u03980 as N\u01eb(\u03980) = {\u0398 \u2208 Rp\u00d7q | d(\u03980,\u0398) \u2264 \u01eb}. Our assumption asserts the identifiably of L(\u0398) for \u0398 close to \u03980.\nAssumption: There exist \u01eb, C > 0 such that for all \u0398 \u2208 N\u01eb(\u03980), L(\u0398) is identifiable w.r.t. \u03980 and \u03c3L(\u0398\n0, \u01eb) = sup \u0398\u2208N\u01eb(\u03980) \u2016L(\u0398)\u20162 \u2264 C, \u03c3K(\u03980, \u01eb) = sup \u0398\u2208N\u01eb(\u03980) \u2016K(\u0398)\u20162 \u2264 C.\nAlso define\n\u2113(\u03980, \u01eb) = sup \u0398\u2208N\u01eb(\u03980) max(1,max j\u2208[r] \u2016Lj(\u0398)\u20162) .\nNote that \u2113(\u03980, \u01eb) \u2264 max(C, 1), since maxj\u2208[r] \u2016Lj(\u0398)\u20162 \u2264 \u2016L(\u0398)\u20162.\nTheorem 3.3. Consider the LQ system of Eq. (1). For some constants \u01eb, Cmin and 0 < \u03b1, \u03c1 < 1, assume that an initial (\u03c1, Cmin, \u03b1) identifiable regulator L(0) is given. Further, assume that for any \u0398 \u2208 N\u01eb(\u03980), L(\u0398) is (\u03c1, Cmin, \u03b1) identifiable. Then, with probability at least 1\u2212 \u03b4 the cumulative regret of ALGORITHM (cf. the table) is bounded as\nR(T ) \u2264 O\u0303(p \u221a T log 3 2 (1/\u03b4)) , (12)\nwhere O\u0303 is hiding the logarithmic factors."}, {"heading": "4 Analysis", "text": ""}, {"heading": "4.1 Proof of Theorem 3.2", "text": "To prove theorem 3.2 we first state a set of sufficient conditions for the solution of the \u21131-regularized least squares to be within some distance, as defined by d(\u00b7, \u00b7), of the true parameter. Subsequently, we prove that these conditions hold with high probability.\nDefine X = [x(0), x(1), . . . , x(n \u2212 1)] \u2208 Rp\u00d7n and let W = [w(1), . . . , w(n)] \u2208 Rp\u00d7n be the matrix containing the Gaussian noise realization. Further let the Wu denote the uth row of W .\nDefine the normalized gradient and Hessian of the likelihood function (6) as\nG\u0302 = \u2212\u2207L(\u03980u) = 1\nn L\u0303XWTu , H\u0302 = \u22072L(\u03980u) =\n1 n L\u0303XXTL\u0303T . (13)\nThe following proposition, a proof of which can be found in [20], provides a set of sufficient conditions for the accuracy of the \u21131-regularized least squares solution. Proposition 4.1. Let S be the support of \u03980u with |S| < k, and H be defined per Definition 3.1. Assume there exist 0 < \u03b1 < 1 and Cmin > 0 such that\n\u03bbmin(HS,S) \u2265 Cmin , \u2016HSc,SH\u22121S,S\u2016\u221e \u2264 1\u2212 \u03b1 . (14) For any 0 < \u01eb < \u0398min if the following conditions hold\n\u2016G\u0302\u2016\u221e \u2264 \u03bb\u03b1\n3 , \u2016G\u0302S\u2016\u221e \u2264 \u01ebCmin 4k \u2212 \u03bb, (15)\n\u2016H\u0302SCS \u2212HSCS\u2016\u221e \u2264 \u03b1\n12 Cmin\u221a k , \u2016H\u0302SS \u2212HSS\u2016\u221e \u2264 \u03b1 12 Cmin\u221a k , (16)\nthe \u21131-regularized least square solution (5) satisfies d(\u0398\u0302u,\u03980u) \u2264 \u01eb.\nIn the sequel, we prove that the conditions in Proposition 4.1 hold with high probability given that the assumptions of Theorem 3.2 are satisfied. A few lemmas are in order proofs of which are deferred to the Appendix.\nThe first lemma states that G\u0302 concentrates in infinity norm around its mean of zero.\nLemma 4.2. Assume \u03c1 = \u2016A0 \u2212 B0L\u20162 < 1 and let \u2113 = max(1,maxi\u2208[r] \u2016Li\u20162). Then, for any S \u2286 [q] and 0 < \u01eb < \u21132\nP { \u2016G\u0302S\u2016\u221e > \u01eb } \u2264 2|S| exp ( \u2212n(1\u2212 \u03c1)\u01eb 2\n4\u21132\n) . (17)\nTo prove the conditions in Eq. (16) we first bound in the following lemma the absolute deviations of the elements of H\u0302 from their mean H , i.e., |H\u0302ij \u2212Hij |. Lemma 4.3. Let i, j \u2208 [q], \u03c1 = \u2016A0 \u2212B0L\u20162 < 1, and 0 < \u01eb < 31\u2212\u03c1 < n . Then,\nP(|H\u0302ij \u2212Hij | > \u01eb) \u2264 2 exp ( \u2212n(1\u2212 \u03c1) 3\u01eb2\n24\u21132\n) . (18)\nThe following corollary of Lemma 4.3 bounds \u2016H\u0302JS \u2212HJS\u2016\u221e for J, S \u2286 [q].\nCorollary 4.4. Let J, S \u2286 [q], \u03c1 = \u2016A0 \u2212B0L\u20162 < 1, \u01eb < 3|S|1\u2212\u03c1 , and n > 31\u2212\u03c1 . Then,\nP(\u2016H\u0302JS \u2212HJS\u2016\u221e > \u01eb) \u2264 2|J ||S| exp ( \u2212n(1\u2212 \u03c1) 3\u01eb2 24|S|2\u21132 ) . (19)\nThe proof of Corollary 4.4 is by applying union bound as\nP(\u2016H\u0302JS \u2212HJS\u2016\u221e > \u01eb) \u2264 |J ||S| max i\u2208J,j\u2208S P(|H\u0302ij \u2212Hij | > \u01eb/|S|). (20)\nProof of Theorem 3.2. We show that the conditions given by Proposition 4.1 hold. The conditions in Eq. (14) are true by the assumption of identifiability of L with respect to \u03980. In order to make the first constraint on G\u0302 imply the second constraint on G\u0302, we assume that \u03bb\u03b1/3 \u2264 \u01ebCmin/(4k) \u2212 \u03bb, which is ensured to hold if \u03bb \u2264 \u01ebCmin/(6k). By Lemma 4.2, P(\u2016G\u0302\u2016\u221e > \u03bb\u03b1/3) \u2264 \u03b4/2 if\n\u03bb2 = 36\u21132 n(1\u2212 \u03c1)\u03b12 log( 4q \u03b4 ) . (21)\nRequiring \u03bb \u2264 \u01ebCmin/(6k), we obtain n \u2265 36 2 k2\u21132\n\u01eb2\u03b12C2min(1\u2212 \u03c1) log(\n4q\n\u03b4 ) . (22)\nThe conditions on H\u0302 can also be aggregated as \u2016H\u0302[q],S\u2212H[q],S\u2016\u221e \u2264 \u03b1Cmin/(12 \u221a k) . By Corollary 4.4, P(\u2016H\u0302[q]S \u2212H[q]S\u2016\u221e > \u03b1Cmin/(12 \u221a k)) \u2264 \u03b4/2 if\nn \u2265 3456 k 3\u21132\n\u03b12(1\u2212 \u03c1)3C2min log(\n4kq\n\u03b4 ). (23)\nMerging the conditions in Eq. (22) and (23) we conclude that the conditions in Proposition 4.1 hold with probability at least 1\u2212 \u03b4 if\nn \u2265 4 \u00b7 10 3 k2\u21132\n\u03b12(1\u2212 \u03c1)C2min\n( 1\n\u01eb2 +\nk (1\u2212 \u03c1)2 ) log( 4kq \u03b4 ). (24)\nWhich finishes the proof of Theorem 3.2."}, {"heading": "4.2 Proof of Theorem 3.3", "text": "The high-level idea of the proof is similar to the proof of main Theorem in [1]. First, we give a decomposition for the gap between the cost obtained by the algorithm and the optimal cost. We then upper bound each term of the decomposition separately."}, {"heading": "4.2.1 Cost Decomposition", "text": "Writing the Bellman optimality equations [5, 4] for average cost dynamic programming, we get\nJ(\u0398\u0303t) + x(t) TK(\u0398\u0303t)x(t) = min\nu\n{ x(t)TQx(t) + uTRu+ E [ z(t+ 1)TK(\u0398\u0303t)z(t+ 1)|Ft ]} ,\nwhere \u0398\u0303t = [A\u0303, B\u0303] is the estimate used at time t, z(t + 1) = A\u0303tx(t) + B\u0303tu + w(t + 1), and Ft is the \u03c3-field generated by the variables {(z\u03c4 , x\u03c4 )}t\u03c4=0. Notice that the left-hand side is the average cost occurred with initial state x(t) [5, 4]. Therefore,\nJ(\u0398\u0303t) + x(t) TK(\u0398\u0303t)x(t) = x(t) TQx(t) + u(t)TRu(t)\n+ E [ (A\u0303tx(t) + B\u0303tu(t) + w(t+ 1)) TK(\u0398\u0303t)(A\u0303tx(t) + B\u0303tu(t) + w(t + 1))|Ft ] = x(t)TQx(t) + u(t)TRu(t) + E [ (A\u0303tx(t) + B\u0303tu(t)) TK(\u0398\u0303t)(A\u0303tx(t) + B\u0303tu(t))|Ft ]\n+ E [ w(t+ 1)TK(\u0398\u0303t)w(t+ 1)|Ft]\n= x(t)TQx(t) + u(t)TRu(t) + E [ x(t+ 1)TK(\u0398\u0303t)x(t+ 1)|Ft ]\n+ ( (A\u0303tx(t) + B\u0303tu(t)) TK(\u0398\u0303t)(A\u0303tx(t) + B\u0303tu(t)) \u2212 (A0x(t) +B0u(t))TK(\u0398\u0303t)(A0x(t) +B0u(t)) ) .\nConsequently T\u2211\nt=0\n( x(t)TQx(t) + u(t)TRu(t) ) = T\u2211\nt=0\nJ(\u0398\u0303t) + C1 + C2 + C3, (25)\nwhere\nC1 =\nT\u2211\nt=0\n( x(t)TK(\u0398\u0303t)x(t) \u2212 E [ x(t+ 1)TK(\u0398\u0303t+1)x(t+ 1) \u2223\u2223Ft ]) , (26)\nC2 = \u2212 T\u2211\nt=0\nE [ x(t+ 1)T(K(\u0398\u0303t)\u2212K(\u0398\u0303t+1))x(t + 1) \u2223\u2223Ft ] , (27)\nC3 = \u2212 T\u2211\nt=0\n( (A\u0303tx(t) + B\u0303tu(t)) TK(\u0398\u0303t)(A\u0303tx(t) + B\u0303tu(t))\n\u2212 (A0x(t) +B0u(t))TK(\u0398\u0303t)(A0x(t) +B0u(t)) ) . (28)"}, {"heading": "4.2.2 Good events", "text": "We proceed by defining the following two events in the probability space under which we can bound the terms C1, C2, C3. We then provide a lower bound on the probability of these events.\nE1 = {\u03980 \u2208 \u2126(i), for i \u2265 1}, E2 = {\u2016w(t)\u2016 \u2264 2 \u221a p log(T/\u03b4), for 1 \u2264 t \u2264 T + 1}."}, {"heading": "4.2.3 Technical lemmas", "text": "The following lemmas establish upper bounds on C1, C2, C3. Lemma 4.5. Under the event E1 \u2229 E2, the following holds with probability at least 1\u2212 \u03b4.\nC1 \u2264 \u221a 128C\n(1\u2212 \u03c1)2 \u221a T p log( T \u03b4 )\n\u221a log( 1\n\u03b4 ) . (29)\nLemma 4.6. Under the event E1 \u2229 E2, the following holds.\nC2 \u2264 8C (1\u2212 \u03c1)2 p log( T \u03b4 ) logT . (30)\nLemma 4.7. Under the event E1 \u2229 E2, the following holds with probability at least 1\u2212 \u03b4.\n|C3| \u2264 800 ( C 1\u2212 \u03c1 ) 5 2 k\n\u221a( 1 + k\u01eb2\n(1 \u2212 \u03c1)2 ) \u00b7 1 + C Cmin \u00b7 log(pT \u03b4 )\n\u221a log( 4kq\n\u03b4 ) \u00b7 p logT\n\u221a T . (31)\nLemma 4.8. The following holds true. P(E1) \u2265 1\u2212 \u03b4, P(E2) \u2265 1\u2212 \u03b4. (32) Therefore, P(E1 \u2229 E2) \u2265 1\u2212 2\u03b4.\nWe are now in position to prove Theorem 3.3.\nProof (Theorem 3.3). Using cost decomposition (Eq. (25)), under the event E1 \u2229 E2, we have T\u2211\nt=0\n(x(t)TQx(t) + u(t)TRu(t)) =\nT\u2211\nt=0\nJ(\u0398\u0303t) + C1 + C2 + C3\n\u2264 TJ(\u03980) + C1 + C2 + C3, where the last inequality stems from the choice of \u0398\u0303t by the algorithm (cf. Eq (9)) and the fact that \u03980 \u2208 \u2126t, for all t under the event E1. Hence, R(T ) \u2264 C1 + C2 + C3 . Now using the bounds on C1, C2, C3, we get the desired result."}, {"heading": "Acknowledgments", "text": "The authors thank the anonymous reviewers for their insightful comments. A.J. is supported by a Caroline and Fabian Pease Stanford Graduate Fellowship."}, {"heading": "A Proof of technical lemmas", "text": "A.1 Proof of Lemma 4.2\nAs before let \u03c1 \u2261 \u2016A0 \u2212 B0L\u20162, and \u2113 = max(1,maxi\u2208[r] \u2016Li\u20162). Further, for u \u2208 [p], j \u2208 [q], define \u03c6(\u03c4) \u2208 Rp\u00d7p to have all rows equal to zero except the uth row which is equal to L\u0303j(A0 \u2212B0L)\u03c4 . Define \u03a6\u0303j \u2208 Rnp\u00d7np as,\n\u03a6\u0303j =   0 0 . . . 0 0 \u03c6(0) 0 . . . 0 0 \u03c6(1) \u03c6(0) . . . 0 0\n... ... . . . 0 0 \u03c6(n\u2212 2) \u03c6(n\u2212 3) . . . \u03c6(0) 0\n  , (33)\nand let\n\u03a6j = 1\n2 (\u03a6\u0303j + \u03a6\u0303\nT j ). (34)\nLemma A.1. Let \u03bdi denote the ith eigenvalue of \u03a6j and assume \u03c1 < 1. Then,\nnp\u2211\ni=1\n\u03bdi = 0, (35)\nmax i\n|\u03bdi| \u2264 \u2113\n1\u2212 \u03c1 , (36) np\u2211\ni=1\n\u03bd2i \u2264 \u21132n\n2(1\u2212 \u03c1) . (37)\nWe do not prove this lemma here and refer the reader to Lemma A.3 in [3].\nProof (Lemma 4.2). The proof of this lemma follows closely the proof of Proposition 4.2 in [3] which we provide here for the reader\u2019s convenience. Let w \u2208 Rnp be the vector obtained by stacking all the noise vectors up to time n, i.e.,\nw T = [w(1)T, w(2)T, . . . , w(n)T].\nThen we have that\nG\u0302j = L\u0303j\nn\u22121\u2211\nt=1\nx(t)wu(t+ 1) =\nn\u22121\u2211\nt=1\nwu(t+ 1)\nt\u22121\u2211\n\u03c4=0\nL\u0303j(A 0 \u2212B0L)\u03c4w(t \u2212 \u03c4) = wT\u03a6jw.\nwhere \u03a6j is defined in (34). Since w \u223c N(0, Inp) and \u03a6j is symmetric, we can write\nG\u0302j =\nnp\u2211\ni=1\n\u03bdiz 2 i . (38)\nwhere zi \u223c N(0, 1) are independent and \u03bdi\u2019s are the eigenvalues of the matrix \u03a6j . Now we have for any \u03b2 > 0,\nP\n{ np\u2211\ni=1\n\u03bdiz 2 i > n\u01eb\n} \u2264 exp (\u2212n\u03b2\u01eb) pn\u220f\ni=1\nE { exp ( \u03b2\u03bdiz 2 i ) }\n= exp ( \u2212n ( \u03b2\u01eb + 1\n2n\nnp\u2211\ni=1\nlog(1\u2212 2\u03bdi\u03b2) )) .\nLet \u03b2 = (1\u2212 \u03c1)\u01eb/(2\u21132). Then it follows from Eq. (36) and the assumption \u01eb < \u21132 that |2\u03bdi\u03b2| \u2264 1/2. Furthermore, for |x| < 1/2, log(1\u2212 x) > \u2212x\u2212 x2. Hence,\nP(\nnp\u2211\ni=1\n\u03bdiz 2 i > n\u01eb) \u2264 exp\n( \u2212n(\u03b2\u01eb\u2212 2\u03b22 1\nn\nnp\u2211\ni=1\n\u03bd2i )\n)\n\u2264 exp ( \u2212n(1\u2212 \u03c1)\u01eb 2\n4\u21132\n) ,\nwhere the first inequality follows from the fact that \u2211np\ni=1 \u03bdi = 0 (Eq. (35)) and the second inequality is obtained using the bound in Eq. (37). Finally, by the union bound we obtain the desired result\nP { \u2016G\u0302S\u2016\u221e > \u01eb } \u2264 2|S| max j\u2208S P { zT\u03a6jz > n\u01eb }\n\u2264 2|S| exp ( \u2212n(1\u2212 \u03c1)\u01eb 2\n4\u21132\n) .\nA.2 Proof of Lemma 4.3\nLemma A.2. Let R\u0303j \u2208 R(n\u22121)p\u00d7np be obtained by removing the first p rows of \u03a6\u0303j . For i, j \u2208 [q] define R(i, j) = 1/2(R\u0303Tj R\u0303i+ R\u0303 T\ni R\u0303j) \u2208 Rnp\u00d7np. Assume \u03c1 < 1 and let \u03bdl denote the lth eigenvalue of R(i, j). Then,\n|\u03bdl| \u2264 \u21132\n(1 \u2212 \u03c1)2 , (39)\n1\nn\nnp\u2211\nl=1\n\u03bd2l \u2264 2\u21132 (1 \u2212 \u03c1)3 ( 1 + 3 2n 1 1\u2212 \u03c1 ) . (40)\nProof (Lemma 4.3). Our proof of 4.2 here closely follows the proof of Proposition 4.2 in [3].\nNote that H\u0302ij can be written as,\nH\u0302ij = 1\nn\nn\u22121\u2211\nt=1\nL\u0303i x(t)x(t) TL\u0303Tj\n= 1\nn\nn\u22121\u2211\nt=1\nL\u0303i\n( t\u22121\u2211\n\u03c4=0\n(A0 \u2212B0L)\u03c4w(t\u2212 \u03c4) )( t\u22121\u2211\n\u03c4=0\n(A0 \u2212B0L)\u03c4w(t\u2212 \u03c4) )T\nL\u0303Tj\n= 1\nn\nn\u22121\u2211\nt=1\n( t\u22121\u2211\n\u03c4=0\nL\u0303i(A 0 \u2212B0L)\u03c4 ) w(t \u2212 \u03c4)w(t \u2212 \u03c4)T ( t\u22121\u2211\n\u03c4=0\nL\u0303j(A 0 \u2212B0L)\u03c4\n)T\n= 1\nn\nn\u22121\u2211\nt=1\nw(t \u2212 \u03c4)T ( t\u22121\u2211\n\u03c4=0\nL\u0303i(A 0 \u2212B0L)\u03c4\n)T( t\u22121\u2211\n\u03c4=0\nL\u0303j(A 0 \u2212B0L)\u03c4 ) w(t\u2212 \u03c4)\n= 1\nn w\nTR(i, j)w.\nSince w \u223c N(0, Inp) and R(i, j) is symmetric, we can write\nH\u0302ij = 1\nn\nnp\u2211\nl=1\n\u03bdlz 2 l , (41)\nwhere zl \u223c N(0, 1) are independent and \u03bdl\u2019s are the eigenvalues of the matrix R(i, j). Further,\nH\u0302ij \u2212 E(H\u0302ij) = 1\nn\nnp\u2211\nl=1\n\u03bdl(z 2 l \u2212 1), (42)\nHence, using Chernoff bound we get\nP(H\u0302ij \u2212 E(H\u0302ij) > \u01eb) = P( np\u2211\nl=1\n\u03bdl(z 2 l \u2212 1) > \u01ebn)\n\u2264 exp (\u2212\u03b2\u01ebn) exp ( \u22121 2 np\u2211\nl=1\nlog(1\u2212 2\u03b2\u03bdl) ) .\nBy Lemma A.2, for n > 31\u2212\u03c1 we have\n1\nn\nnp\u2211\nl=1\n\u03bd2l \u2264 3\u21132\n(1 \u2212 \u03c1)3 , (43)\nLet \u03b2 = (1\u2212\u03c1) 3\u01eb\n12\u21132 . By assumption \u01eb < 3 1\u2212\u03c1 , we have |2\u03b2\u03bdl| < 1/2. Using the inequality log(1\u2212x) > \u2212x\u2212 x2 for |x| < 1/2, we obtain\nP(H\u0302ij \u2212 E(H\u0302ij) > \u01eb) \u2264 exp ( \u2212\u03b2\u01ebn+ 2\u03b22 np\u2211\nl=1\n\u03bd2l\n)\n\u2264 exp ( \u2212n(1\u2212 \u03c1) 3\u01eb2\n24\u21132\n) ,\nwhich finishes the proof.\nA.3 Proof of Lemma 4.5\nBefore embarking on the proof, we state and prove the following claim which will be repeatedly used in the proofs of Lemmas 4.5, 4.6, and 4.7.\nProposition A.3. Under the event E1 \u2229 E2, the following holds true.\n\u2016x(t)\u2016 \u2264 2 1\u2212 \u03c1\n\u221a p log( T\n\u03b4 ) , for 1 \u2264 t \u2264 T + 1 .\nProof (Proposition A.3). Conditioning on the event E1, \u03980 \u2208 \u2126(i) for i \u2265 1. Furthermore, for all i \u2265 1, \u2126(i) \u2286 N\u01eb(\u03980). Recall our assumption that for all \u0398 \u2208 N\u01eb(\u03980), L(\u0398) is identifiable with respect to \u03980. Consequently, we have \u2016A0 \u2212 B0Lt\u20162 \u2264 \u03c1, for all t \u2265 1, where Lt denotes the controller (used by ALGORITHM ) at time t. Now, we write for 1 \u2264 t \u2264 T + 1,\n\u2016x(t)\u2016 = \u2016 t\u2211\nt1=1\nt\u220f\nt2=t1+1\n(A0 \u2212B0Lt)w(t1)\u2016 \u2264 t\u2211\nt1=1\n\u03c1t\u2212t1\u2016w(t1)\u2016\n\u2264 2 \u221a p log( T\n\u03b4 )\nt\u2211\nt1=1\n\u03c1t\u2212t1 < 2\n1\u2212 \u03c1\n\u221a p log( T\n\u03b4 ) ,\n(44)\nwhere the second inequality holds since we are conditioning on E2.\nArmed with this proposition, we prove Lemma 4.5.\nDefine z(t) = A0x(t) +B0u(t), and Kt = K(\u0398\u0303t) for all t \u2265 0. Since x(0) = 0, we have\nC1 =\nT\u2211\nt=0\n( x(t)TKtx(t)\u2212 E [ x(t+ 1)TKt+1x(t+ 1) \u2223\u2223Ft ])\n= \u2212E [ x(T + 1)TKT+1x(T + 1) \u2223\u2223FT ] + T\u2211\nt=1\n( x(t)TKtx(t) \u2212 E [ x(t)TKtx(t)|Ft\u22121 ]) .\nBecause KT+1 is PSD, the first term is bounded above by zero. To bound the second term, define\nEt1 = {\u03980 \u2208 \u2126\u03c4 , for 1 \u2264 \u03c4 \u2264 t} , Et2 = {\u2016w(\u03c4)\u2016 \u2264 2 \u221a p log(T/\u03b4), for 1 \u2264 \u03c4 \u2264 t} .\nNote that Et+11 \u2286 Et1 and Et+12 \u2286 Et2. Following the approach of [1], it can be shown that the second term is bounded above by\nT\u2211\nt=1\nI{Et\u221211 \u2229E t\u22121 2 }\n( x(t)TKtx(t)\u2212 E [ x(t)TKtx(t)|Ft\u22121 ]) .\nDefine the martingale\nM\u03c4 =\n\u03c4\u2211\nt=1\nI{Et\u221211 \u2229E t\u22121 2 }\n( x(t)TKtx(t) \u2212 E [ x(t)TKtx(t)|Ft\u22121 ]) , M0 = 0 .\nNote that M\u03c4 is a martingale since Et\u221211 and Et\u221212 are Ft\u22121 measurable. In addition,\n|M\u03c4 \u2212M\u03c4\u22121| \u2264 I{Et\u221211 \u2229Et\u221212 } \u2223\u2223\u2223x(t)TKtx(t) \u2212 E [ x(t)TKtx(t)|Ft\u22121 ]\u2223\u2223\u2223\n\u2264 2C I{Et\u221211 \u2229Et\u221212 } \u2016x(t)\u2016 2 \u2264 8C (1 \u2212 \u03c1)2 p log( T \u03b4 ) I{Et\u221211 \u2229E t\u22121 2 }\n\u2264 8C (1 \u2212 \u03c1)2 p log( T \u03b4 ) ,\nwhere the penultimate inequality follows from Proposition A.3. Applying Azuma\u2019s inequality,\nP(MT \u2212M0 \u2265 \u03b2) \u2264 exp ( \u2212 \u03b2\n2(1\u2212 \u03c1)4 128TC2p2 log2(T\u03b4 )\n) .\nHence, with probability at least 1\u2212 \u03b4,\nC1 \u2264 \u221a 128C\n(1\u2212 \u03c1)2 \u221a T p log( T \u03b4 )\n\u221a log( 1\n\u03b4 ) ."}, {"heading": "B Proof of Lemma 4.6", "text": "If the confidence set is not updated at time t+ 1, i.e., \u2126t = \u2126t+1, then K(\u0398\u0303t) = K(\u0398\u0303t+1) and the t-th term in the summation is zero. The way ALGORITHM chooses the lengths of the episodes, \u2206\u03c4i, the number of updates (number of times ALGORITHM changes the policy) is at most log4 T up to time T . Using the bound \u2016K(\u0398t)\u20162 \u2264 C, for t \u2265 1, we have\nC2 = \u2212 T\u2211\nt=0\nE[x(t+ 1)T(K(\u0398\u0303t)\u2212K(\u0398\u0303t+1))x(t + 1)|Ft]\n\u2264 \u2211\ni:\u03c4i\u2264T\n2C \u2016x\u03c4i\u20162\n\u2264 8C (1\u2212 \u03c1)2 p log( T \u03b4 ) log4 T ,\n(45)\nwhere we used Proposition A.3 in the last step."}, {"heading": "C Proof of Lemma 4.7", "text": "Let yt = [xTt , u T t ] T \u2208 Rq\u00d71. We first establish the following proposition. Proposition C.1. Under the event E1 \u2229 E2, The following holds true with probability at least 1\u2212 \u03b4: T\u2211\nt=0\n\u2016(\u03980 \u2212 \u0398\u0303t)yt\u20162 \u2264 10\n(1\u2212 \u03c1)2 p\u01eb 2 log(\nT \u03b4 )(log T )2n1 , (46)\nwhere n1 is defined in ALGORITHM .\nProof (Proposition C.1). Write\nT\u2211\nt=0\n\u2016(\u03980 \u2212 \u0398\u0303t)yt\u20162 = \u2211\ni:\u03c4i\u2264T\n\u03c4i+1\u22121\u2211\nt=\u03c4i\n\u2016(\u03980 \u2212 \u0398\u0303t)yt\u20162\n= \u2211\ni:\u03c4i\u2264T\n\u03c4i+1\u22121\u2211\nt=\u03c4i\n\u2016(A0 \u2212B0L(i) \u2212 A\u0303t + B\u0303tL(i))xt\u20162\n\u2264 \u2211\ni:\u03c4i\u2264T\n\u03c4i+1\u22121\u2211\nt=\u03c4i\n2\u2016(A0 \u2212B0L(i) \u2212 A\u0303t + B\u0303tL(i))(A0 \u2212B0L(i))t\u2212\u03c4i+1x(\u03c4i \u2212 1)\u20162\n+ \u2211\ni:\u03c4i\u2264T\n\u03c4i+1\u22121\u2211\nt=\u03c4i\n2\u2016(A0 \u2212B0L(i) \u2212 A\u0303t + B\u0303tL(i)) t\u2211\nt1=\u03c4i\n(A0 \u2212B0L(i))t\u2212t1w(t1)\u20162 ,\nwhere we used\nx(t) = (A0 \u2212B0L(i))t\u2212\u03c4i+1x(\u03c4i \u2212 1) + t\u2211\nt1=\u03c4i\n(A0 \u2212B0L(i))t\u2212t1w(t1).\nWe proceed by bounding the first term as follows:\n\u2211\ni:\u03c4i\u2264T\n\u03c4i+1\u22121\u2211\nt=\u03c4i\n2\u2016(A0 \u2212B0L(i) \u2212 A\u0303t + B\u0303tL(i))(A0 \u2212B0L(i))t\u2212\u03c4i+1x(\u03c4i \u2212 1)\u20162\n\u2264 \u2211\ni:\u03c4i\u2264T\n\u03c4i+1\u22121\u2211\nt=\u03c4i\n2d(\u03980, \u0398\u0303t) 2\u03c12(t\u2212\u03c4i+1)\u2016x(\u03c4i \u2212 1)\u20162\n\u2264 8 (1\u2212 \u03c1)2 p log( T \u03b4 )\nT\u2211\nt=0\nd(\u03980, \u0398\u0303t) 2 .\n(47)\nTo bound the second term define the matrix\nDt = (A 0 \u2212B0L(i) \u2212 A\u0303t + B\u0303tL(i))[I, (A0 \u2212B0L)1, (A0 \u2212B0L)2, . . . , (A0 \u2212B0L)t\u2212\u03c4i ]. (48)\nThe second term can be written as \u2211T\nt=0 2\u2016Dtwi\u20162 where wi is the vector obtained by stacking all the noise vectors in episode i, i.e.,\nw T i = [w(t) T, w(t\u2212 1)T, . . . , w(\u03c4i)T]T .\nHence, the rth entry in the vector Dtwi is a normal random variable with variance at most \u2016Dtr\u20162 where Dtr is the rth row of matrix Dt and\n\u2016Dtr\u20162 \u2264 d(\u03980, \u0398\u0303t) 2\n(1 \u2212 \u03c12) . (49)\nUsing standard normal tail bound we get\nP((Dtrwi) 2 \u2265 gt) \u2264 exp ( \u2212 (1\u2212 \u03c1 2)\n2d(\u03980, \u0398\u0303t)2 gt\n) . (50)\nTaking\ngt = 2d(\u03980, \u0398\u0303t) 2 (1\u2212 \u03c12) log( pT \u03b4 ) , (51)\nand applying union bound for r \u2208 [p], and 1 \u2264 t \u2264 T , we obtain P ( (Dtrwi) 2 \u2264 gt, for 1 \u2264 t \u2264 T, r \u2208 [p] ) \u2265 1\u2212 \u03b4 . (52)\nConsequently, with probability at least 1\u2212 \u03b4, the second term is bounded by\n2 T\u2211\nt=0\n\u2016Dtwi\u20162 \u2264 2 T\u2211\nt=0\npgt \u2264 4 (1\u2212 \u03c12)p log( pT \u03b4 )\nT\u2211\nt=0\nd(\u03980, \u0398\u0303t) 2. (53)\nFinally, using Theorem 3.2 and the choice of episodes, we have T\u2211\nt=0\nd(\u03980, \u0398\u0303t) 2 =\n\u2211\ni:\u03c4i\u2264T\n(2\u2212i\u01eb)2\u2206\u03c4i = \u2211\ni:\u03c4i\u2264T\n(2\u2212i\u01eb)24i ( 1+ i\nlog( q\u03b4 )\n) n1 \u2264 \u01eb2 ( logT+ log2 T\n2 log( q\u03b4 )\n) n1 ,\nwhere we have used the fact that the number of episodes up to time T is at most logT . Combining Eqs. (47) and (53), we have\nT\u2211\nt=0\n\u2016(\u03980 \u2212 \u0398\u0303t)yt\u20162 \u2264 { 8 (1\u2212 \u03c1)2 p log( T \u03b4 ) +\n4 (1 \u2212 \u03c12)p log( pT \u03b4 ) } T\u2211\nt=0\nd(\u03980, \u0398\u0303t) 2\n\u2264 12 (1\u2212 \u03c1)2 p log( pT \u03b4 )\u01eb2\n( logT + log2 T\n2 log( q\u03b4 )\n) n1\n\u2264 20 (1\u2212 \u03c1)2 p\u01eb 2 log( pT \u03b4 )(log T )2n1 .\n(54)\nCorollary C.2. Using the value of n1, defined in ALGORITHM , we have with probability at least 1\u2212 \u03b4,\nT\u2211\nt=0\n\u2016(\u03980 \u2212 \u0398\u0303t)yt\u20162 \u2264 8 \u00b7 104C2\n(1 \u2212 \u03c1)3C2min pk2\n( 1 + k\u01eb2 (1\u2212 \u03c1)2 ) log( 4kq \u03b4 ) log( pT \u03b4 ) log2 T . (55)\nNow, we are ready to bound C3.\n|C3| \u2264 T\u2211\nt=0\n\u2223\u2223\u2223\u2223\u2016K(\u0398\u0303)1/2\u0398\u0303Tt yt\u20162 \u2212 \u2016K(\u0398\u0303)1/2\u03980Tyt\u20162 \u2223\u2223\u2223\u2223\n\u2264 ( T\u2211\nt=0\n{ \u2016K(\u0398\u0303t)1/2\u0398\u0303tyt\u2016 \u2212 \u2016K(\u0398\u0303t)1/2\u03980yt\u2016 }2)1/2 \u00d7\n( T\u2211\nt=0\n{ \u2016K(\u0398\u0303t)1/2\u0398\u0303tyt\u2016+ \u2016K(\u0398\u0303t)1/2\u03980yt\u2016 }2)1/2\n\u2264 ( T\u2211\nt=0\n\u2016K(\u0398\u0303t)1/2(\u0398\u0303t \u2212\u03980)yt\u20162 )1/2 \u00d7\n( T\u2211\nt=0\n{ \u2016K(\u0398\u0303t)1/2\u0398\u0303tyt\u2016+ \u2016K(\u0398\u0303t)1/2\u03980yt\u2016 }2)1/2\n\u2264 C1/2 ( T\u2211\nt=0\n\u2016(\u0398\u0303t \u2212\u03980)yt\u20162 )1/2 \u00d7 C ( T\u2211\nt=0\n\u2016yt\u20162 )1/2 .\n(56)\nCorollary C.2 provides an upper bound for the first term on the right hand side. In addition, T\u2211\nt=0\n\u2016yt\u20162 \u2264 T\u2211\nt=0\n(1 + \u03c3(Lt) 2)\u2016xt\u20162\n\u2264 (1 + \u03c3(Lt)2) 4 (1 \u2212 \u03c1)2 p log( T \u03b4 )T\n\u2264 4(1 + C 2) (1\u2212 \u03c1)2 p log( T \u03b4 )T .\n(57)\nHere, the first inequality follows from Proposition A.3. Combining the bounds for the terms on the right hand side of Eq. (56), we obtain\n|C3| \u2264 800 ( C 1\u2212 \u03c1 ) 5 2 k\n\u221a( 1 + k\u01eb2\n(1 \u2212 \u03c1)2 ) \u00b7 1 + C Cmin \u00b7 log(pT \u03b4 )\n\u221a log( 4kq\n\u03b4 ) \u00b7 p logT\n\u221a T . (58)"}, {"heading": "D Proof of Lemma 4.8", "text": "We first show that P(E1) \u2265 1 \u2212 \u03b4. According to Theorem 3.2, the sample complexity scales with (1/\u01eb2) log(q/\u03b4). Due to the choice of episode lengths in the algorithm, namely \u2206\u03c4i = 4i(1 + i/ log(q/\u03b4))n1, with probability at least 1\u2212 \u03b4/2i, we have d(\u03980, \u0398\u0302 (i) ) \u2264 2\u2212i\u01eb and thus \u03980 \u2208 \u2126 (i) .\nNow by applying union bound for i \u2265 1,\nP(E1) \u2265 1\u2212 \u221e\u2211\ni=1\n\u03b4 2i = 1\u2212 \u03b4. (59)\nNext we prove the lower bound for the probability of event E2. Let w(t) \u2208 Rp be the noise vector at time t with i.i.d standard normal entries. For any t \u2265 1 and any \u03bb, we have\nP{\u2016w(t)\u20162 \u2265 \u03bbp} = P{e\u03b8 \u2211p i=1 wi(t) 2 \u2265 e\u03b8\u03bbp}\n\u2264 e\u2212\u03b8\u03bbp p\u220f\ni=1\nE{e\u03b8w2i (t)}\n= exp(\u2212p{\u03bb\u03b8 + 1 2 log(1\u2212 2\u03b8)}) \u2264 exp(\u2212p{\u03bb\u03b8 \u2212 \u03b8 \u2212 2\u03b82}), for 0 < \u03b8 < 1\n(60)\nwhere we used the fact that if |x| < 1, then log(1 \u2212 x) > \u2212x \u2212 x2. Choosing \u03b8 = 1/2, and \u03bb = 4 log(T/\u03b4), we obtain\nP{\u2016w(t)\u20162 \u2265 4p log(T/\u03b4)} \u2264 exp(\u2212p log(T/\u03b4)) = (T \u03b4 )\u2212p.\nFinally, by applying union bound for 1 \u2264 t \u2264 T + 1,\nP(E2) = P{\u2016w(t)\u2016 \u2264 2 \u221a p log(T/\u03b4), for 1 \u2264 t \u2264 T + 1}\n\u2265 1\u2212 (T + 1)(T \u03b4 )\u2212p > 1\u2212 \u03b4 .\n(61)"}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>We study the problem of adaptive control of a high dimensional linear quadratic<lb>(LQ) system. Previous work established the asymptotic convergence to an optimal<lb>controller for various adaptive control schemes. More recently, for the average<lb>cost LQ problem, a regret bound of O(<lb>\u221a<lb>T ) was shown, apart form logarithmic<lb>factors. However, this bound scales exponentially with p, the dimension of the<lb>state space. In this work we consider the case where the matrices describing the<lb>dynamic of the LQ system are sparse and their dimensions are large. We present<lb>an adaptive control scheme that achieves a regret bound of O(p<lb>\u221a<lb>T ), apart from<lb>logarithmic factors. In particular, our algorithm has an average cost of (1 + \u01eb)<lb>times the optimum cost after T = polylog(p)O(1/\u01eb). This is in comparison to<lb>previous work on the dense dynamics where the algorithm requires time that scales<lb>exponentially with dimension in order to achieve regret of \u01eb times the optimal cost.<lb>We believe that our result has prominent applications in the emerging area of<lb>computational advertising, in particular targeted online advertising and advertising<lb>in social networks.", "creator": "LaTeX with hyperref package"}}}