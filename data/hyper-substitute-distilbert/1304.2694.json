{"id": "1304.2694", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Apr-2013", "title": "Symmetry-Aware Marginal Density Estimation", "abstract": "the pseudo - blackwell formulation is utilized would enable and improve automated tools of inference in large probabilistic simulate that exhibit symmetries. a formal marginal density tensor further constructed and analyses both problems wherein empirically not outperform standard estimators towards several orders of precision. most developed theory and protocols apply to within third class of probabilistic simulation including discrete relational models considered not susceptible approaching consistent numerical conclusions.", "histories": [["v1", "Tue, 9 Apr 2013 18:47:47 GMT  (842kb,D)", "http://arxiv.org/abs/1304.2694v1", "To appear in proceedings of AAAI 2013"]], "COMMENTS": "To appear in proceedings of AAAI 2013", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mathias niepert"], "accepted": true, "id": "1304.2694"}, "pdf": {"name": "1304.2694.pdf", "metadata": {"source": "CRF", "title": "Symmetry-Aware Marginal Density Estimation", "authors": ["Mathias Niepert"], "emails": [], "sections": [{"heading": "Introduction", "text": "Many successful applications of artificial intelligence research are based on large probabilistic models. Examples include Markov logic networks (Richardson and Domingos 2006), conditional random fields (Lafferty, McCallum, and Pereira 2001) and, more recently, deep learning architectures (Hinton, Osindero, and Teh 2006; Bengio and LeCun 2007; Poon and Domingos 2011). Especially the models one encounters in the statistical relational learning (SRL) literature often have joint distributions spanning millions of variables and features. Indeed, these models are so large that, at first sight, inference and learning seem daunting. For numerous of these models, however, scalable approximate and, to a lesser extend, exact inference algorithms do exist. Most notably, there has been a strong focus on lifted inference algorithms, that is, algorithms that group indistinguishable variables and features during inference. For an overview we refer the reader to (Kersting 2012). Lifted algorithms facilitate efficient inference in numerous large probabilistic models for which inference is NP-hard in principle.\nWe are concerned with the estimation of marginal probabilities based on a finite number of sample points. We show that the feasibility of inference and learning in large and highly symmetric probabilistic models can be explained with the Rao-Blackwell theorem from the field of statistics. The theory and algorithms do not directly depend on the syntactical nature of the relational models such as arity of predicates and number of variables per formula but only on the given automorphism group of the probabilistic model, and are applicable to classes of probabilistic models much broader than the class of statistical relational models. Copyright c\u00a9 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nConsider an experiment where a coin is flipped n times. While a frequentist would assume the flips to be i.i.d., a Bayesian typically makes the weaker assumption of exchangeability \u2013 that the probability of an outcome sequence only depends on the number of \u201cheads\u201d in the sequence and not on their order. Under the non-i.i.d. assumption, a possible corresponding graphical model is the fully connected graph with n nodes and high treewidth. The actual number of parameters required to specify the distribution, however, is only n+1, one for each sequence with 0 \u2264 k \u2264 n \u201cheads.\u201d Bruno de Finetti was the first to realize that such a sequence of random variables can be (re-)parameterized as a unique mixture of n+1 independent urn processes (de Finetti 1938). It is this notion of a parameterization as a mixture of urn processes that is at the heart of our work. A direct application of de Finetti\u2019s results, however, is often impossible since not all variables are exchangeable in realistic probabilistic models.\nMotivated by the intuition of exchangeability, we show that arbitrary model symmetries allow us to re-paramterize the distribution as a mixture of independent urn processes where each urn consists of isomorphic joint assignments. Most importantly, we develop a novel Rao-Blackwellized estimator that implicitly estimates the fewer parameters of the simpler mixture model and, based on these, computes the marginal densities. We identify situations in which the application of the Rao-Blackwell estimator is tractable. In particular, we show that the Rao-Blackwell estimator is always linear-time computable for single-variable marginal density estimation. By invoking the Rao-Blackwell theorem, we show that the mean squared error of the novel estimator is at least as small as that of the standard estimator and strictly smaller under non-trivial symmetries of the probabilistic model. Moreover, we prove that for estimates based on sample points drawn from a Markov chainM, the bias of the Rao-Blackwell estimator is governed by the mixing time of the quotient Markov chain whose convergence behavior is superior to that ofM.\nWe present empirical results verifying that the RaoBlackwell estimator always outperforms the standard estimator by up to several orders of magnitude, irrespective of the model structure. Indeed, we show that the results of the novel estimator resemble those typically observed in lifted inference papers. For the first time such a performance is shown for an SRL model with a transitivity formula.\nar X\niv :1\n30 4.\n26 94\nv1 [\ncs .A\nI] 9\nA pr\n2 01\n3"}, {"heading": "Background", "text": "We review some concepts from group and estimation theory.\nGroup Theory A group is an algebraic structure (G, \u25e6), where G is a set closed under a binary associative operation \u25e6 with an identity element and a unique inverse for each element. We often write G rather than (G, \u25e6). A permutation group acting on a set \u2126 is a set of bijections g : \u2126\u2192 \u2126 that form a group. Let \u2126 be a finite set and let G be a permutation group acting on \u2126. If \u03b1 \u2208 \u2126 and g \u2208 G we write \u03b1g to denote the image of \u03b1 under g. A cycle (\u03b11 \u03b12 ... \u03b1n) represents the permutation that maps \u03b11 to \u03b12, \u03b12 to \u03b13,..., and \u03b1n to \u03b11. Every permutation can be written as a product of disjoint cycles. A generating set R of a group is a subset of the group\u2019s elements such that every element of the group can be written as a product of finitely many elements of R and their inverses.\nWe define a relation \u223c on \u2126 with \u03b1 \u223c \u03b2 if and only if there is a permutation g \u2208 G such that \u03b1g = \u03b2. The relation partitions \u2126 into equivalence classes which we call orbits. We call this partition of \u2126 the orbit partition induced by G. We use the notation \u03b1G to denote the orbit {\u03b1g | g \u2208 G} containing \u03b1. For a permutation group G acting on \u2126 and a sequence A = \u3008\u03b11, ..., \u03b1k\u3009 \u2208 \u2126k we write Ag to denote the image \u3008\u03b11g, ..., \u03b1kg\u3009 of A under g. Moreover, we write AG to denote the orbit of the sequence A.\nPoint Estimation Let s1, ..., sN be N sample points drawn from some distribution P . An estimator \u03b8\u0302N of a parameter \u03b8 is a function of s1, ..., sN . The bias of an estimator is defined by bias(\u03b8\u0302N ) := E[\u03b8\u0302N \u2212 \u03b8] and the variance by Var(\u03b8\u0302N ) := E[(\u03b8\u0302N \u2212 E(\u03b8\u0302N ))2], where E is the expectation with respect to P , the distribution that generated the data. We say that \u03b8\u0302N is unbiased if bias(\u03b8\u0302N ) = 0. The quality of an estimator is often assessed with the mean squared error (MSE) defined by MSE[\u03b8\u0302N ] := E[(\u03b8\u0302N \u2212 \u03b8)2] = Var(\u03b8\u0302N ) + bias(\u03b8\u0302N ) 2.\nTheorem 1 (Rao-Blackwell). Let \u03b8\u0302 be an estimator with E[\u03b8\u03022] < \u221e and T a sufficient statistic both for \u03b8, and let \u03b8\u0302\u2217 := E[\u03b8\u0302 | T ]. Then, MSE[\u03b8\u0302\u2217] \u2264 MSE[\u03b8\u0302]. Moreover, MSE[\u03b8\u0302\u2217] < MSE[\u03b8\u0302] unless \u03b8\u0302\u2217 is a function of \u03b8\u0302.\nFinite Markov chains A finite Markov chainM defines a random walk on elements of a finite set \u2126. For all x, y \u2208 \u2126, Q(x, y) is the chain\u2019s probability to transition from x to y, and Qt(x, y) = Qtx(y) the probability of being in state y after t steps if the chain starts at x. A Markov chain is irreducible if for all x, y \u2208 \u2126 there exists a t such that Qt(x, y) > 0 and aperiodic if for all x \u2208 \u2126, gcd{t \u2265 1 | Qt(x, x) > 0} = 1. An irreducible and aperiodic chain converges to its unique stationary distribution and is called ergodic.\nThe total variation distance dtv of the Markov chain from its stationary distribution \u03c0 at time t with initial state x is defined by\ndtv(Q t x, \u03c0) =\n1\n2 \u2211 y\u2208\u2126 |Qt(x, y)\u2212 \u03c0(y)|.\nFor \u03b5 > 0, let \u03c4x(\u03b5) denote the least value T such that dtv(Q t x, \u03c0) \u2264 \u03b5 for all t \u2265 T . The mixing time \u03c4(\u03b5) is defined by \u03c4(\u03b5) = max{\u03c4x(\u03b5) | x \u2208 \u2126}."}, {"heading": "Related Work", "text": "There are numerous lifted inference algorithms such as lifted variable elimination (Poole 2003), lifted belief propagation (Singla and Domingos 2008; Kersting, Ahmadi, and Natarajan 2009), first-order knowledge compilation (Van den Broeck 2011), and lifted variational inference (Choi and Amir 2012). Probabilistic theorem proving applied to a clustering of the relational model was used to lift the Gibbs sampler (Venugopal and Gogate 2012). Recent work exploits automorphism groups of probabilistic models for more efficient probabilistic inference (Bui, Huynh, and Riedel 2012; Niepert 2012). Orbital Markov chains (Niepert 2012) are a class of Markov chains that implicitly operate on the orbit partition of the assignment space and do not invoke the Rao-Blackwell theorem.\nRao-Blackwellized (RB) estimators have been used for inference in Bayesian networks (Doucet et al. 2000; Bidyuk and Dechter 2007) and latent Dirichlet allocation (Teh, Newman, and Welling 2006) with application in robotics (Stachniss, Grisetti, and Burgard 2005) and activity recognition (Bui, Venkatesh, and West 2002). The RB theorem and estimator are important concepts in statistics (Gelfand and Smith 1990; Casella and Robert 1996)."}, {"heading": "Symmetry-Aware Point Estimation", "text": "An automorphism group of a probabilistic model is a group whose elements are permutations of the probabilistic model\u2019s random variables X that leave the joint distribution P (X) invariant. There is a growing interest in computing and utilizing automorphism groups of probabilistic models for more efficient inference algorithms (Bui, Huynh, and Riedel 2012; Niepert 2012). The line of research is primarily motivated by the highly symmetric nature of statistical relational models and provides a complementary view on lifted probabilistic inference. Here, we will not be concerned with deriving automorphism groups of probabilistic models but with developing algorithms that utilize these permutation groups for efficient marginal density estimation. Hence, we always assume a given automorphism group G of the probabilistic model under consideration.\nWe begin by deriving a re-parameterization of the joint distribution in the presence of symmetries that generalizes the mixture of independent urn processes parameterization for finitely exchangeable variables (Diaconis and Freedman 1980). All random variables are assumed to be discrete.\nLet X = \u3008X1, ..., Xn\u3009 be a finite sequence of discrete random variables with joint distribution P (X), let G be an automorphism group of X, and letO be an orbit partition of the assignment space induced by G. Please note that for any x,x\u2032 \u2208 O \u2208 O we have P (x) = P (x\u2032). For a subsequence X\u0302 of X and an orbit O \u2208 O we write P (X\u0302 = x\u0302 | O) for the marginal density P (X\u0302 = x\u0302) conditioned on O. Thus,\nP (X\u0302 = x\u0302 | O) = 1|O| \u2211 x\u2208O I{x\u3008X\u0302\u3009=x\u0302},\nwhere I is the indicator function and x\u3008X\u0302\u3009 the assignment within x to the variables in the sequence X\u0302. We can now (re-)parameterize the marginal density as a mixture of independent orbit distributions\nP (X\u0302 = x\u0302) = \u2211 O\u2208O P (X\u0302 = x\u0302 | O)P (O),\nwhere P (O) = \u2211\nx\u2208O P (X = x). For instance, the joint distribution of the Markov logic network in Figure 1(a) can be parameterized as a mixture of the distributions for the 10 orbits depicted in Figure 1(c).\nLet us first recall the standard estimator used in most sampling approaches. After collecting N sample points s1, ..., sN the standard estimator for the marginal density \u03b8 := P (X\u0302 = x\u0302) is defined as\n\u03b8\u0302N := 1\nN N\u2211 i=1 I{si\u3008X\u0302\u3009=x\u0302}. (1)\nNow, the symmetry-aware Rao-Blackwell estimator forN sample points s1, ..., sN is defined as\n\u03b8\u0302rbN := 1\nN N\u2211 i=1 P (X\u0302 = x\u0302 | siG), (2)\nwhere G is the given automorphism group that induces O. Hence, the unbiased Rao-Blackwell estimator integrates out the joint assignments of each orbit. We will prove that the mean squared error of the Rao-Blackwell estimator is less\nthan or equal to that of the standard estimator. First, however, we want to investigate under what conditions we can efficiently compute the conditional density of equation (2). To this end, we establish a connection between the orbit of the subsequence X\u0302 under the automorphism group G and the orbit partition of the assignment space induced by G1. Definition 2. Let X be a finite sequence of random variables with joint distribution P (X), let G be an automorphism group of X, let X\u0302 be a subsequence of X, let Val(X) be the assignment space of X, and let s \u2208 Val(X). The orbit Hamming weight of s with respect to the marginal assignment X\u0302 = x\u0302 is defined by\nHG X\u0302=x\u0302\n(s) := \u2211\nA\u2208X\u0302G I{s\u3008A\u3009=x\u0302}.\nBased on this definition, we state a lemma which allows us to compute the density of equation (2) in closed form, without having to enumerate all of the orbit\u2019s elements. Lemma 3. Let X be a finite sequence of random variables with joint distribution P (X), let G be an automorphism group of X, let X\u0302 be a subsequence of X, and let s \u2208 Val(X). Then,\nP (X\u0302 = x\u0302 | sG) = HG X\u0302=x\u0302 (s)\n|X\u0302G| = E[\u03b8\u0302N | HGX\u0302=x\u0302(s)].\nThe following example demonstrates the application of the lemma to the special case of single-variable marginal density estimation for the MLN in Figure 1. Example 4. Let us assume we want to estimate the marginal density P (smokes(A)=1) of the MLN in Figure 1(a). Since G = {(smokes(A) smokes(B))(cancer(A) cancer(B)), ()} we have that \u3008smokes(A)\u3009G={\u3008smokes(A)\u3009, \u3008smokes(B)\u3009}. Given the sample point s = \u30081, 0, 1, 0\u3009 we have that HG \u3008smokes(A)\u3009=\u30081\u3009(s) = 1 and P (smokes(A)=1 | s G) = 12 .\nThus, given a sample point s, the marginal density conditioned on an orbit of the assignment space is computable in closed form using the orbit Hamming weight of s with respect to the marginal assignment since it is a sufficient statistic for the marginal density. If the probabilistic model exhibits symmetries, then the Rao-Blackwell estimator\u2019s MSE is less than or equal to that of the standard estimator. Theorem 5. Let X be a finite sequence of random variables with joint distribution P (X), let G be an automorphism group of X given by a generating set R, let X\u0302 be a subsequence of X, and let \u03b8 := P (X\u0302 = x\u0302) be the marginal density to be estimated. The Rao-Blackwell estimator \u03b8\u0302rbN has the following properties:\n(a) Its worst-case time complexity is O(R|X\u0302G|+N |X\u0302G|); (b) MSE[\u03b8\u0302rbN ] \u2264 MSE[\u03b8\u0302N ]. The inequality of (b) is strict if there exists a joint assignment s with non-zero density and 0 < HG\nX\u0302=x\u0302 (s) < |X\u0302G| > 1.\n1Please note the two different types of orbit partitions discussed here. One results from G acting on the assignment space the other from G acting on sequences of random variables.\nFor single-variable density estimation the worst-case time complexity of the Rao-Blackwell estimator is O(R|X| + N |X|) and, therefore, linear both in the number of variables and the number of sample points. For most symmetric models, the inequality of Theorem 5(b) is strict and the RaoBlackwell estimator outperforms the standard estimator, a behavior we will verify empirically.\nPlease note that in the special case of single-variable marginal density estimation the RB estimator is identical to the estimator that averages the identically distributed variables located in the same orbit. The advantages of utilizing the Rao-Blackwell theory are (1) it directly provides conditions for which the inequality of Theorem 5(b) is strict; (2) it generalizes the single-variable case to marginals spanning multiple variables; (3) it allows us to investigate the completeness of an estimator with respect to a given automorphism group; and (4) it provides the link to the quotient Markov chain in the MCMC setting and its superior convergence behavior presented in the following section.\nThe Rao-Blackwell estimator is unbiased if the drawn sample points are independent. Since it is often only practical to collect sample points from a Markov chain, the bias for a finite number of N points will depend on the chain\u2019s mixing behavior. We will show that if there are non-trivial model symmetries and if we are using the Rao-Blackwell estimator, we only need to worry about the mixing behavior of the Markov chain whose state space is the orbit partition."}, {"heading": "Symmetry-Aware MCMC", "text": "Whenever we collect sample points from a Markov chain, the efficiency of an estimator is influenced by (a) the mixing behavior of the Markov chain and (b) the variance of the estimator under the assumption that the Markov chain has reached stationarity, that is, the asymptotic variance (Neal 2004). That the Rao-Blackwell estimator\u2019s asymptotic variance is at least as low as that of the standard estimator is a corollary of Theorem 5. We show that the same is true for the bias that is caused by the fact that we collect a finite number of sample points from Markov chains which never exactly reach stationarity.\nA lumping of a Markov chain is a partition of its state space which is possible under certain conditions on the transition probabilities of the original Markov chain (Buchholz 1994; Derisavi, Hermanns, and Sanders 2003). Definition 6. LetM be an ergodic Markov chain with transition matrixQ, stationary distribution \u03c0, and state space \u2126, and let C = {C1, ..., Cn} be a partition of the state space. If for all Ci, Cj \u2208 C and all si\u2032, si\u2032\u2032 \u2208 Ci\nQ\u2032(Ci, Cj) := \u2211\nsj\u2208Cj\nQ(si \u2032, sj) = \u2211 sj\u2208Cj Q(si \u2032\u2032, sj)\nthen we say thatM is ordinary lumpable with respect to C. If, in addition, \u03c0(s\u2032i) = \u03c0(s \u2032\u2032 i ) for all s \u2032 i, s \u2032\u2032 i \u2208 Ci and all Ci \u2208 C thenM is exactly lumpable with respect to C. The Markov chainM\u2032 with state space C and transition matrix Q\u2032 is called the quotient chain ofM with respect to C.\nEvery finite ergodic Markov chain is exactly lumpable with respect to an orbit partition of its state space. The fol-\nlowing theorem states this and the convergence behavior of the quotient Markov chain in relation to the original Markov chain (cf. (Boyd et al. 2005)).\nProposition 7. LetM be an ergodic Markov chain and let O be an orbit partition of its state space. Then, the Markov chain M is exactly lumpable with respect to O. If M is reversible, then the quotient Markov chainM\u2032 with respect to O is also reversible. Moreover, the mixing time ofM\u2032 is smaller than or equal to the mixing time ofM. Example 8. Figure 1(b) depicts the state space of the Gibbs chain for the MLN shown in Figure 1(a). The constants renaming automorphism group {(A B), ()} acting on the sets of constants leads to the automorphism group {(smokes(A) smokes(B))(cancer(A) cancer(B)), ()} on the ground level. This permutation group acting on the state space of the Gibbs chain induces an orbit partition which is the state space of the quotient Markov chain (see Figure 1(c)).\nThe explicit construction of the state space of a quotient Markov chain is intractable. Given an automorphism group G, merely counting the number of equivalence classes of the orbit partition of the assignment space induced by G is known to be a #P-complete problem (Goldberg 2001). Nevertheless, if the Rao-Blackwell estimator is utilized, one can draw the sample points from the original Markov chain while analyzing the convergence behavior of the quotient Markov chain of the original chain.\nTheorem 9. Let X be a finite sequence of random variables with joint distribution P (X), letM be an ergodic Markov chain with stationary distribution P , and let O be an orbit partition ofM\u2019s state space. Let \u03b8\u0302rbN be the Rao-Blackwell estimator for N sample points sT+1, ..., sT+N collected from M, after discarding the first T sample points. Then, |bias(\u03b8\u0302rbN )| \u2264 if T \u2265 \u03c4 \u2032( ), where \u03c4 \u2032( ) is the mixing time of the quotient Markov chain ofM with respect to O.\nHence, if one wants to make sure that the absolute value of the bias of the Rao-Blackwell estimator is smaller than a given > 0, one only needs a burn-in period consisting of \u03c4 \u2032( ) simulation steps, where \u03c4 \u2032( ) is the mixing time of the quotient Markov chain. Existing work on analyzing the influence of symmetries in random walks has shown that it is often more convenient to investigate the mixing behavior of the quotient Markov chain (Boyd et al. 2005). In the context of marginal density estimation, Markov chains implicitly operating on the orbit partition of the assignment space were shown to have better mixing behavior (Niepert 2012).\nIn summary, whenever probabilistic models exhibit nontrivial symmetries we can have the best of both worlds. The bias owed to the fact that we are collecting a finite number of sample points from a Markov chain as well as the asymptotic variance (Neal 2004) of the Rao-Blackwell estimator are at least as small as those of the standard estimator. The more symmetric the probabilistic model the larger the reduction in mean squared error.\nWe now present the experimental results for several large probabilistic models, both relational and non-relational."}, {"heading": "Experiments", "text": "The aim of the empirical investigation is twofold. First, we want to verify the efficiency of the novel Rao-Blackwell estimator when applied as a post-processing step to the output of state-of-the-art sampling algorithms. Second, we want to test the hypothesis that the efficiency gains of the novel estimator on standard SRL models are similar empirically to those of state-of-the-art lifted inference algorithms.\nFor the SRL models we computed the orbit partitions of the variables based on the model\u2019s renaming automorphisms (Bui, Huynh, and Riedel 2012). As discussed earlier, renaming automorphisms are computable in time linear in the domain size. We applied GAP (GAP 2012) to compute the variables\u2019 orbit partition. For all non SRL models we computed the automorphism group and the orbit partitions as in (Niepert 2012) using the graph automorphism algorithm SAUCY (Darga, Sakallah, and Markov 2008) and the GAP system, respectively. Overall, the computation of the orbit partitions of the models\u2019 variables took less than one second for each of the probabilistic models we considered.\nWe conducted experiments with several benchmark Markov logic networks, a statistical relational language general enough to capture numerous types of graphical models (Richardson and Domingos 2006). Here, we used (a) the asthma-smokes-cancer MLN (Venugopal and Gogate 2012)\nwith 10% evidence2; (b) the \u201cFriends & Smokers\u201d MLN exactly as specified in (Singla and Domingos 2008) with 10% evidence; and the \u201cFriends & Smokers\u201d MLN with the transitivity formula on the friends relation having weight 1.0, (c) without and (d) with 10% evidence. Each of the models had between 10 and 100 objects in the domain, leading to log-linear models with 102-104 variables and 102-106 features. We used WFOMC (Van den Broeck 2011), to compute the exact single-variable marginals of the asthma MLN. For all other MLNs, existing exact lifted inference algorithm were unable to compute single-variable densities. In these cases, we performed several very long runs (burn-in 1 day; overall 5 days) of a Gibbs sampler guaranteed to be ergodic and made sure that state-of-the-art MCMC diagnostics indicated convergence (Brooks and Gelman 1998).\nWe executed our implementation of the standard Gibbs sampler and ALCHEMY\u2019s implementation of the MC-SAT algorithm (Poon and Domingos 2006) on the MLNs based on 10 separate runs, without a burn-in period. For each sampling algorithm we computed the marginal densities with the standard estimator and the Rao-Blackwell estimator, respectively, which we implemented in the GAP programming language3. Figure 2 depicts, for each MLN, the av-\n2For a random 10% of all people it is known (a) whether they smoke or not and (b) who 10 of their friends are.\n3https://code.google.com/p/lifted-mcmc/\nerage Kullback-Leibler divergence4 between the estimated and precomputed true single-variable marginals of the nonevidence variables plotted against the absolute running time of the algorithms in seconds.\nThe Rao-Blackwell estimator improves the density estimates by at least an order of magnitude and, in the absence of evidence, even up to four orders of magnitude relative to the standard estimator. The improvement of the empirical results is independent of the relational structure of the MLNs. For the MLN with a transitivity formula on the friends relation, generally considered a problematic and as of now not domain-liftable model, the results are as pronounced as for the MLNs known to be domain-liftable.\nWe also conducted experiments with non-SRL models to investigate the efficiency of the approach on graphical models. We executed the Gibbs sampler with and without using the Rao-Blackwell estimator on a 100\u00d7 100 2-coloring grid model with binary random variables. The symmetries of the model are the reflection and rotation automorphisms of the 2-dimensional square grid. Figure 2(e) depicts the plot of the average KL divergence against the running time in seconds, where each pairwise factor between neighboring variables X,Y was defined as exp(0.2) if X 6= Y , and 1 otherwise. Figure 2(f) depicts the plot of the same grid model except that the pairwise factors were defined as 1 if X 6= Y , and 0 otherwise. The results clearly demonstrate the superior performance of the Rao-Backwell estimator even for probabilistic models with a smaller number of symmetries.\nIn addition, we analyzed the impact of the domain size on the estimator performance for (a) domain-liftable MLNs and (b) MLNs not liftable by any state-of-the-art exact lifted inference algorithm. We used the \u201cFriends & Smokers\u201d MLN without evidence; and with and without the transitivity formula on the friends relation. The MLN without transitivity is a standard benchmark for lifted algorithms whereas MLNs with transitivity are considered difficult and no exact lifted inference algorithm exists for such MLNs as of now. Figures 3(a)&(c) depict the time needed to achieve an average KL divergence of less than 10\u22124 plotted against the domain size of the models without and with transitivity. The increase in runtime is far less pronounced with the Rao-\n4We computed both MSE and average KL divergence but omitted the qualitatively identical MSE results due to space constraints.\nBlackwell estimator. The plots resemble those often shown in lifted inference papers where an algorithm that can lift a model is contrasted with one that cannot. The increase in runtime is slightly higher for the model with transitivity but this is owed to the size increase of each variable\u2019s Markov blanket and, thus, the time needed for each Gibbs sampler step. Figures 3(b)&(d) plots the sample size required to achieve an average KL divergence of less than 10\u22124 against the domain size. Interestingly, the number of sample points is almost identical for the model with and without transitivity, indicating that the advantage of the Rao-Blackwell estimator is independent of the model\u2019s formulas.\nIn Figure 3(a) we plot the results of WFOMC for compiling a first-order circuit and computing (a) one singlevariable marginal and (b) all single-variable marginals. WFOMC has constant runtime for exactly computing one single-variable marginal density. The Rao-Blackwell estimation for all of the model\u2019s variables scales sub-linearly and is more efficient than repeated calls to WFOMC. While we do not need to run WFOMC once per single-variable marginal density if the variables are first partitioned into sets of variables with identical marginal densities (de Salvo Braz, Amir, and Roth 2005), the results demonstrate that the symmetry-aware estimator scales comparably to exact lifted inference algorithms on domain-liftable models and that its runtime is polynomial in the domain size of the MLNs."}, {"heading": "Discussion", "text": "A Rao-Blackwell estimator was developed and shown, both analytically and empirically, to have lower mean squared error under non-trivial model symmetries. The presented theory provides a novel perspective on the notion of lifted inference and the underlying reasons for the feasibility of marginal density estimation in large but highly symmetric probabilistic models. For the first time, the applicability of such an approach does not directly depend on the properties of the relational structure such as the arity of predicates and the type of formulas but only on the given evidence and the corresponding automorphism group of the model. We believe the theoretical and empirical insights to be of great interest to the machine learning community and that the presented work might contribute to a deeper understanding of lifted inference algorithms."}, {"heading": "Acknowledgments", "text": "Many thanks to Guy Van den Broeck who provided feedback on an earlier draft of the paper. This work was partially supported by a Google faculty research award."}, {"heading": "Proof of Lemma 3", "text": "Lemma. Let X be a finite sequence of random variables with joint distribution P (X), let G be an automorphism group of X, let X\u0302 be a subsequence of X, let X\u0302G be the orbit of X\u0302, and let s \u2208 Val(X). Then,\nP (X\u0302 = x\u0302 | sG) = HG X\u0302=x\u0302 (s)\n|X\u0302G| = E[\u03b8\u0302N | HGX\u0302=x\u0302(s)].\nProof. Let Gs := {g \u2208 G | sg = s} be the stabilizer subgroup of s. Then,\nP (X\u0302 = x\u0302 | sG) = |{g \u2208 G | s g\u3008X\u0302\u3009 = x\u0302}| |G|\nsince for each x \u2208 sG we have that |{g \u2208 G | sg = x}| = |Gs| by the orbit stabilizer theorem. For each A \u2208 X\u0302G let GA := {g \u2208 G | Ag = X\u0302}. Again, by the orbit stabilizer theorem, we have that |GA| = |GX\u0302| for each A \u2208 X\u0302G, where GX\u0302 is the stabilizer subgroup of X\u0302. Hence,\n|{g \u2208 G | sg\u3008X\u0302\u3009 = x\u0302}| |G| = HG X\u0302=x\u0302 (s)|GX\u0302| |G| = HG X\u0302=x\u0302 (s) |X\u0302G| .\nHence, HG X\u0302=x\u0302 is a sufficient statistic for the marginal density P (X\u0302 = x\u0302). Moreover, we have that\nP (X\u0302 = x\u0302 | sG) = E[\u03b8\u0302N | HGX\u0302=x\u0302(s)]. This concludes the proof."}, {"heading": "Proof of Theorem 5", "text": "Theorem. Let X be a finite sequence of random variables with joint distribution P (X), let G be an automorphism group of X given by R generators, let X\u0302 be a subsequence of X, let X\u0302G be the orbit of X\u0302, and let \u03b8 := P (X\u0302 = x\u0302) be the marginal density to be estimated. The Rao-Blackwell estimator \u03b8\u0302rbN has the following properties:\n(a) Its worst-case time complexity is O(R|X\u0302G|+N |X\u0302G|); (b) MSE[\u03b8\u0302rbN ] \u2264 MSE[\u03b8\u0302N ].\nThe inequality of (b) is strict if there exists a joint assignment s with non-zero density and 0 < HG\nX\u0302=x\u0302 (s) < |X\u0302G| > 1.\nProof. We first construct the set X\u0302G once, which has a worst-case time complexity of R|X\u0302G| (Holt, Eick, and O\u2019Brien 2005). For each sample point, we have to access an array representing the values of the sample point at most |X\u0302G| times. This allows us, for each sample point s, to compute P (X\u0302 = x\u0302 | sG) in time O(|X\u0302G|) by Lemma 3.\nSince HG X\u0302=x\u0302 is a sufficient statistic for \u03b8 and \u03b8\u0302rbN = E[\u03b8\u0302N | HGX\u0302=x\u0302] by Lemma 3, statement (b) follows from the Rao-Blackwell theorem (Blackwell 1947). If there exists a joint assignment s with non-zero density and 0 < HG\nX\u0302=x\u0302 (s) < |X\u0302G| > 1, then \u03b8\u0302N is not a function of HGX\u0302=x\u0302\nand the inequality is strict (Blackwell 1947)."}, {"heading": "Proof of Theorem 9", "text": "Theorem. Let X be a finite sequence of random variables with joint distribution P (X), letM be an ergodic Markov chain with stationary distribution P , and let O be an orbit partition ofM\u2019s state space. Let \u03b8\u0302rbN be the Rao-Blackwell estimator for N sample points sT+1, ..., sT+N collected from M, after discarding the first T sample points. Then, |bias(\u03b8\u0302rbN )| \u2264 if T \u2265 \u03c4 \u2032( ), where \u03c4 \u2032( ) is the mixing time of the quotient Markov chain ofM with respect to O. Proof. For a subsequence X\u0302 of X, let \u03be := X\u0302 = x\u0302 be the marginal assignment whose density \u03b8 is to be estimated, let Val(X) be the assignment space of X, and let S = {sT+1, ..., sT+N} be the multiset of sample points collected from M, after discarding the first T sample points. Since O is a partition of the assignment space, we have that\n\u03b8\u0302rbN = 1\nN \u2211 s\u2208S P (\u03be | sG) = \u2211 O\u2208O P (\u03be | O) 1 N \u2211 s\u2208S I{s\u2208O}.\nHence, E[\u03b8\u0302rbN ] = \u2211 O\u2208O P (\u03be | O)E[I{s\u2208O}], where E[I{s\u2208O}] is the expectation of some sample point being located in the orbit O. E[I{s\u2208O}] defines a probability distribution over the space O. If the sample points are independent, then E[I{s\u2208O}] = P (O), for all O \u2208 O, and the estimator is unbiased. Since we collect sample points from a Markov chain we will often have that E[I{s\u2208O}] 6= P (O).\nBy the assumptions and Proposition 7, the Markov chain M is exactly lumpable with respect to O and, hence, for all states x ofM, all t \u2208 {1, 2, ...}, and all orbits O \u2208 O, we have that \u2211 o\u2208O Q t(x, o) = Q\u2032 t (xG, O), where Qt(x, o) is the probability of the Markov chainM being in state o after t simulation steps if the chain starts in state x. In addition, we start collecting sample points after T \u2265 \u03c4 \u2032( ) simulation steps and, thus,\n1\n2 \u2211 O\u2208O |E[I{s\u2208O}]\u2212 P (O)| \u2264\nmax x\u2208Val(X)\n{ 1\n2 \u2211 O\u2208O |Q\u2032T (xG, O)\u2212 P (O)|\n} \u2264 .\nFinally, |bias(\u03b8\u0302rbN )| = |E[\u03b8\u0302rbN \u2212 \u03b8]| =\u2223\u2223\u2223\u2223\u2223\u2211 O\u2208O P (\u03be | O)E[I{s\u2208O}]\u2212 \u2211 O\u2208O P (\u03be | O)P (O) \u2223\u2223\u2223\u2223\u2223 =\u2223\u2223\u2223\u2223\u2223\u2211\nO\u2208O P (\u03be | O)(E[I{s\u2208O}]\u2212 P (O)) \u2223\u2223\u2223\u2223\u2223 \u2264\u2211 O\u2208O E[I{s\u2208O}]\u2265P (O) (E[I{s\u2208O}]\u2212 P (O)) =\n1\n2 \u2211 O\u2208O |E[I{s\u2208O}]\u2212 P (O)| \u2264 .\nThe last equality follows from a known identity of the total variation distance (Levin, Peres, and Wilmer 2008)."}], "references": [{"title": "and LeCun", "author": ["Y. Bengio"], "venue": "Y.", "citeRegEx": "Bengio and LeCun 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "and Dechter", "author": ["B. Bidyuk"], "venue": "R.", "citeRegEx": "Bidyuk and Dechter 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "P", "author": ["S.P. Boyd", "P. Diaconis", "Parrilo"], "venue": "A.; and Xiao, L.", "citeRegEx": "Boyd et al. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and Gelman", "author": ["S.P. Brooks"], "venue": "A.", "citeRegEx": "Brooks and Gelman 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "T", "author": ["Bui, H.H.", "Huynh"], "venue": "N.; and Riedel, S.", "citeRegEx": "Bui. Huynh. and Riedel 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "H", "author": ["Bui"], "venue": "H.; Venkatesh, S.; and West, G.", "citeRegEx": "Bui. Venkatesh. and West 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "C", "author": ["G. Casella", "Robert"], "venue": "P.", "citeRegEx": "Casella and Robert 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "and Amir", "author": ["J. Choi"], "venue": "E.", "citeRegEx": "Choi and Amir 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "I", "author": ["P.T. Darga", "K.A. Sakallah", "Markov"], "venue": "L.", "citeRegEx": "Darga. Sakallah. and Markov 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Sur la condition d\u2019\u00e9quivalence partielle", "author": ["B. de Finetti 1938] de Finetti"], "venue": "In Colloque consacre\u0301 a la theorie des probabilite\u0301s,", "citeRegEx": "Finetti,? \\Q1938\\E", "shortCiteRegEx": "Finetti", "year": 1938}, {"title": "Lifted first-order probabilistic inference", "author": ["Amir de Salvo Braz", "R. Roth 2005] de Salvo Braz", "E. Amir", "D. Roth"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,", "citeRegEx": "Braz et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Braz et al\\.", "year": 2005}, {"title": "W", "author": ["S. Derisavi", "H. Hermanns", "Sanders"], "venue": "H.", "citeRegEx": "Derisavi. Hermanns. and Sanders 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Freedman", "author": ["P. Diaconis"], "venue": "D.", "citeRegEx": "Diaconis and Freedman 1980", "shortCiteRegEx": null, "year": 1980}, {"title": "S", "author": ["A. Doucet", "N. d. Freitas", "K.P. Murphy", "Russell"], "venue": "J.", "citeRegEx": "Doucet et al. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "A", "author": ["A.E. Gelfand", "Smith"], "venue": "F. M.", "citeRegEx": "Gelfand and Smith 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "L", "author": ["Goldberg"], "venue": "A.", "citeRegEx": "Goldberg 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Y", "author": ["G.E. Hinton", "S. Osindero", "Teh"], "venue": "W.", "citeRegEx": "Hinton. Osindero. and Teh 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "D", "author": ["Holt"], "venue": "F.; Eick, B.; and O\u2019Brien, E. A.", "citeRegEx": "Holt. Eick. and O.Brien 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Counting belief propagation", "author": ["Ahmadi Kersting", "K. Natarajan 2009] Kersting", "B. Ahmadi", "S. Natarajan"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Kersting et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kersting et al\\.", "year": 2009}, {"title": "F", "author": ["J.D. Lafferty", "A. McCallum", "Pereira"], "venue": "C. N.", "citeRegEx": "Lafferty. McCallum. and Pereira 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "E", "author": ["D.A. Levin", "Y. Peres", "Wilmer"], "venue": "L.", "citeRegEx": "Levin. Peres. and Wilmer 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "R", "author": ["Neal"], "venue": "M.", "citeRegEx": "Neal 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Domingos", "author": ["H. Poon"], "venue": "P.", "citeRegEx": "Poon and Domingos 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Domingos", "author": ["H. Poon"], "venue": "P.", "citeRegEx": "Poon and Domingos 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Domingos", "author": ["M. Richardson"], "venue": "P.", "citeRegEx": "Richardson and Domingos 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Domingos", "author": ["P. Singla"], "venue": "P.", "citeRegEx": "Singla and Domingos 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Y", "author": ["Teh"], "venue": "W.; Newman, D.; and Welling, M.", "citeRegEx": "Teh. Newman. and Welling 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "On the completeness of first-order knowledge compilation for lifted probabilistic inference", "author": [], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Broeck,? \\Q2011\\E", "shortCiteRegEx": "Broeck", "year": 2011}, {"title": "V", "author": ["D. Venugopal", "Gogate"], "venue": "2012. On lifting the gibbs sampling algorithm. In Advances in Neural Information Processing Systems 25. 1664\u2013", "citeRegEx": "Venugopal and Gogate 2012", "shortCiteRegEx": null, "year": 1672}, {"title": "E[\u03b8\u0302N | HGX\u0302=x\u0302] by Lemma 3, statement (b) follows from the Rao-Blackwell theorem (Blackwell 1947). If there exists a joint assignment s with non-zero density and 0 < H", "author": [], "venue": null, "citeRegEx": "N,? \\Q1947\\E", "shortCiteRegEx": "N", "year": 1947}], "referenceMentions": [], "year": 2013, "abstractText": "The Rao-Blackwell theorem is utilized to analyze and improve the scalability of inference in large probabilistic models that exhibit symmetries. A novel marginal density estimator is introduced and shown both analytically and empirically to outperform standard estimators by several orders of magnitude. The developed theory and algorithms apply to a broad class of probabilistic models including statistical relational models considered not susceptible to lifted probabilistic inference. Introduction Many successful applications of artificial intelligence research are based on large probabilistic models. Examples include Markov logic networks (Richardson and Domingos 2006), conditional random fields (Lafferty, McCallum, and Pereira 2001) and, more recently, deep learning architectures (Hinton, Osindero, and Teh 2006; Bengio and LeCun 2007; Poon and Domingos 2011). Especially the models one encounters in the statistical relational learning (SRL) literature often have joint distributions spanning millions of variables and features. Indeed, these models are so large that, at first sight, inference and learning seem daunting. For numerous of these models, however, scalable approximate and, to a lesser extend, exact inference algorithms do exist. Most notably, there has been a strong focus on lifted inference algorithms, that is, algorithms that group indistinguishable variables and features during inference. For an overview we refer the reader to (Kersting 2012). Lifted algorithms facilitate efficient inference in numerous large probabilistic models for which inference is NP-hard in principle. We are concerned with the estimation of marginal probabilities based on a finite number of sample points. We show that the feasibility of inference and learning in large and highly symmetric probabilistic models can be explained with the Rao-Blackwell theorem from the field of statistics. The theory and algorithms do not directly depend on the syntactical nature of the relational models such as arity of predicates and number of variables per formula but only on the given automorphism group of the probabilistic model, and are applicable to classes of probabilistic models much broader than the class of statistical relational models. Copyright c \u00a9 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Consider an experiment where a coin is flipped n times. While a frequentist would assume the flips to be i.i.d., a Bayesian typically makes the weaker assumption of exchangeability \u2013 that the probability of an outcome sequence only depends on the number of \u201cheads\u201d in the sequence and not on their order. Under the non-i.i.d. assumption, a possible corresponding graphical model is the fully connected graph with n nodes and high treewidth. The actual number of parameters required to specify the distribution, however, is only n+1, one for each sequence with 0 \u2264 k \u2264 n \u201cheads.\u201d Bruno de Finetti was the first to realize that such a sequence of random variables can be (re-)parameterized as a unique mixture of n+1 independent urn processes (de Finetti 1938). It is this notion of a parameterization as a mixture of urn processes that is at the heart of our work. A direct application of de Finetti\u2019s results, however, is often impossible since not all variables are exchangeable in realistic probabilistic models. Motivated by the intuition of exchangeability, we show that arbitrary model symmetries allow us to re-paramterize the distribution as a mixture of independent urn processes where each urn consists of isomorphic joint assignments. Most importantly, we develop a novel Rao-Blackwellized estimator that implicitly estimates the fewer parameters of the simpler mixture model and, based on these, computes the marginal densities. We identify situations in which the application of the Rao-Blackwell estimator is tractable. In particular, we show that the Rao-Blackwell estimator is always linear-time computable for single-variable marginal density estimation. By invoking the Rao-Blackwell theorem, we show that the mean squared error of the novel estimator is at least as small as that of the standard estimator and strictly smaller under non-trivial symmetries of the probabilistic model. Moreover, we prove that for estimates based on sample points drawn from a Markov chainM, the bias of the Rao-Blackwell estimator is governed by the mixing time of the quotient Markov chain whose convergence behavior is superior to that ofM. We present empirical results verifying that the RaoBlackwell estimator always outperforms the standard estimator by up to several orders of magnitude, irrespective of the model structure. Indeed, we show that the results of the novel estimator resemble those typically observed in lifted inference papers. For the first time such a performance is shown for an SRL model with a transitivity formula. ar X iv :1 30 4. 26 94 v1 [ cs .A I] 9 A pr 2 01 3", "creator": "LaTeX with hyperref package"}}}