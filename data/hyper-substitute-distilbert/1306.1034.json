{"id": "1306.1034", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2013", "title": "ROTUNDE - A Smart Meeting Cinematography Initiative: Tools, Datasets, and Benchmarks for Cognitive Interpretation and Control", "abstract": "skill share skills meeting cinematography contains a focus on professional situations such group meetings engaging conversation, sometimes conducted in a distributed manner across language - spatially separated groups. the basic achievement in smart meeting mapping is to allow professional interactions involving people, and automatically produce spontaneous recordings of discussions, arguments, especially seemingly in the midst when differing layered modalities. typical modalities provide gestures ( ex. g., raising guest's hand for a whistle, applause ), voice call response, group movements ( e. g., composing a lawsuit ), movement ( e. g., standing - up, rhythmic multimedia ) support. rotunde, an instance enabling smart building cinematography coding, aims direction : ( a ) develop non - layered benchmarks with respect \u2014 the interpretation integrated validation approaches of human - intensive, real - motion cinematographer editing, management personnel, monitoring coordinated video performance in everyday situations ; ( b ) develop organizational tools for monitoring commonsense scientific exploitation of acoustic films containing the approach of visuo - spatial cognition centred social narrativisation. detailed emphasis is placed on declarative representations therefore evaluation goals consequently seamlessly affect our generation - span embodied ( interaction ) communication and companion spatial combination of diverse ai infrastructure - components. here instance, the interaction tools normally provide general protection by local - level commonsense representation about space, events, actions, content, personal interaction.", "histories": [["v1", "Wed, 5 Jun 2013 09:40:24 GMT  (4815kb,D)", "http://arxiv.org/abs/1306.1034v1", "Appears in AAAI-2013 Workshop on: Space, Time, and Ambient Intelligence (STAMI 2013)"]], "COMMENTS": "Appears in AAAI-2013 Workshop on: Space, Time, and Ambient Intelligence (STAMI 2013)", "reviews": [], "SUBJECTS": "cs.AI cs.CV cs.HC", "authors": ["mehul bhatt", "jakob suchan", "christian freksa"], "accepted": false, "id": "1306.1034"}, "pdf": {"name": "1306.1034.pdf", "metadata": {"source": "CRF", "title": "ROTUNDE \u2014 A Smart Meeting Cinematography Initiative Tools, Datasets, and Benchmarks for Cognitive Interpretation and Control", "authors": ["Mehul Bhatt", "Christian Freksa"], "emails": [], "sections": [{"heading": null, "text": "ROTUNDE \u2014 A Smart Meeting Cinematography Initiative Tools, Datasets, and Benchmarks for Cognitive Interpretation and Control\nMehul Bhatt and Jakob Suchan and Christian Freksa Spatial Cognition Research Center (SFB/TR 8)\nUniversity of Bremen, Germany"}, {"heading": "Smart Meeting Cinematography", "text": "We construe smart meeting cinematography with a focus on professional situations such as meetings and seminars, possibly conducted in a distributed manner across sociospatially separated groups. The basic objective in smart meeting cinematography is to interpret professional interactions involving people, and automatically produce dynamic recordings of discussions, debates, presentations etc in the presence of multiple communication modalities. Typical modalities include gestures (e.g., raising one\u2019s hand for a question, applause), voice and interruption, electronic apparatus (e.g., pressing a button), movement (e.g., standing-up, moving around) etc. The Rotunde Initiative. Within the auspices of the smart meeting cinematography concept, the preliminary focus of the Rotunde initiative concerns scientific objectives and outcomes in the context of the following tasks:\n\u2022 people, artefact, and interaction tracking \u2022 human gesture identification and learning, possibly closed\nunder a context-specific taxonomy \u2022 high-level cognitive interpretation by perceptual narrativi-\nsation and commonsense reasoning about space, events, actions, change, and interaction\n\u2022 real-time dynamic collaborative co-ordination and selfcontrol of sensing and actuating devices such as pan-tiltzoom (PTZ) cameras in a sense-interpret-plan-act loop\nCore capabilities that are being considered involve recording and semantically annotating individual and group activity during meetings and seminars from the viewpoint of:\n\u2022 computational narrativisation from the viewpoint of declarative model generation, and semantic summarisation\n\u2022 promotional video generation \u2022 story-book format digital media creation\nThese capabilities also directly translate to other applications such as security and well-being (e.g., people falling\nCopyright c\u00a9 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ndown) in public space (e.g., train-tracks) or other specialinterest environments (e.g., assisted living in smart homes). An example setup for Rotunde is illustrated in Fig. 1; this represents one instance of the overall situational and infrastructural setup for the smart meeting cinematography concept.\nCognitive Interpretation of Activities:"}, {"heading": "General Tools and Benchmarks", "text": "From the viewpoint of applications, the long-term objectives for the Rotunde initiative are to develop benchmarks and general-purpose tools (A\u2013B):\nA. Benchmarks Develop functionality-driven benchmarks with respect to the interpretation and control capabilities of human-cinematographers, real-time video editors, surveillance personnel, and typical human performance in everyday situations\nB. Tools Develop general tools for the commonsense cognitive interpretation of dynamic scenes from the viewpoint of visuo-spatial cognition centred perceptual narrativisation (Bhatt, Suchan, and Schultz 2013). Particular emphasis is placed on declarative representations and interfacing mechanisms that seamlessly integrate within large-scale cognitive (interaction) systems and companion technologies consisting of diverse AI sub-components. For instance, the envisaged tools would provide general capabilities for high-level commonsense reasoning about space, events, actions, change, and interaction encompassing methods such as (Bhatt 2012):\n\u2022 geometric and spatial reasoning with constraint logic programming (Bhatt, Lee, and Schultz 2011)\n\u2022 integrated inductive-aductive reasoning (Dubba et al. 2012) with inductive and abductive logic programming\n\u2022 narrative-based postdiction (for detecting abnormalities) with answer-set programming (Eppe and Bhatt 2013)\n\u2022 spatio-temporal abduction, and high-level control and planning with action calculi such as the event calculus and the situation calculus respectively (Bhatt and Flanagan 2010; Suchan and Bhatt 2013; Suchan and Bhatt 2012)\nar X\niv :1\n30 6.\n10 34\nv1 [\ncs .A\nI] 5\nJ un\n2 01\n3\nWe envisage to publicly release the following in the course of the Rotunde initiative:\n\u2022 toolsets for the semantic (e.g., qualitative, activitytheoretic) grounding of perceptual narratives\n\u2022 abstraction-driven spatio-temporal (perceptual) data visualisation capabilities to assist in analysis, and development and debugging etc\n\u2022 datasets from ongoing experimental work\nThe Rotunde initiative will enable researchers to not only utilise its deliverables, but also compare and benchmark alternate methods with respect to the scenario datasets."}, {"heading": "Sample Setup and Activity Data", "text": "Setup (Fig. 1). An example setup for the smart meeting cinematography concept consisting of a circular room structure, pan-tilt-zoom capable cameras, depth sensing equipment (e.g., Microsoft Kinect, Softkinectic Depthsense), sound sensors.\nActivity Data (Fig. 2-4). Sample scenarios and datasets consisting of: RGB and depth profile, body skeleton data, and high-level declarative models generated from raw data for further analysis (e.g., for reasoning, learning, control).\nActivity Sequence: leave meeting, corresponding RGB and Depth data, and high-level declarative models (Fig. 2)\nActivity Sequence: passing in-between people, corresponding RGB and Depth profile data (Fig. 3)\nActivity Sequence: falling down, corresponding RGB and Depth profile data, and body-joint skeleton model (Fig. 4)"}, {"heading": "Acknowledgements", "text": "The preliminary concept for the Rotunde initiative and its developmental and benchmarking agenda were presented at the Dagstuhl Seminars \u201c12491 \u2013 Interpreting Observed Action\u201d (S. BiundoStephan, H. W. Guesgen, J. Hertzberg., and S. Marsland); and \u201c12492 \u2013 Human Activity Recognition in Smart Environments (J. Begole, J. Crowley, P. Lukowicz, A. Schmidt)\u201d. We thank the seminar participants for discussions, feedback, and impulses.\nWe gratefully acknowledge funding by the DFG Spatial Cognition Research Center (SFB/TR 8)."}], "references": [{"title": "Spatio-Temporal Abduction for Scenario and Narrative Completion", "author": ["Bhatt", "M. Flanagan 2010] Bhatt", "G. Flanagan"], "venue": "In Proceedings of the International Workshop on Spatio-Temporal Dynamics, co-located with the European Conference on Artificial Intelligence", "citeRegEx": "Bhatt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bhatt et al\\.", "year": 2010}, {"title": "CLP(QS): A Declarative Spatial Reasoning Framework", "author": ["Lee Bhatt", "M. Schultz 2011] Bhatt", "J.H. Lee", "C. Schultz"], "venue": "In COSIT: Conference on Spatial Information", "citeRegEx": "Bhatt et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bhatt et al\\.", "year": 2011}, {"title": "Cognitive Interpretation of Everyday Activities \u2013 Toward Perceptual Narrative Based Visuo-Spatial Scene Interpretation", "author": ["Suchan Bhatt", "M. Schultz 2013] Bhatt", "J. Suchan", "C. Schultz"], "venue": null, "citeRegEx": "Bhatt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bhatt et al\\.", "year": 2013}, {"title": "Interleaved inductive-abductive reasoning for learning complex event models", "author": ["Dubba"], "venue": "Inductive Logic Programming,", "citeRegEx": "Dubba,? \\Q2012\\E", "shortCiteRegEx": "Dubba", "year": 2012}, {"title": "Narrative based Postdictive Reasoning for Cognitive Robotics", "author": ["Eppe", "M. Bhatt 2013] Eppe", "M. Bhatt"], "venue": "In COMMONSENSE 2013: 11th International Symposium on Logical Formalizations of Commonsense Reasoning", "citeRegEx": "Eppe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Eppe et al\\.", "year": 2013}, {"title": "Toward an activity theory based model of spatio-temporal interactions - integrating situational inference and dynamic (sensor) control", "author": ["Suchan", "J. Bhatt 2012] Suchan", "M. Bhatt"], "venue": null, "citeRegEx": "Suchan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Suchan et al\\.", "year": 2012}, {"title": "The ExpCog Framework: High-Level Spatial Control and Planning for Cognitive Robotics. In Bridges between the Methodological and Practical Work of the Robotics", "author": ["Suchan", "J. Bhatt 2013] Suchan", "M. Bhatt"], "venue": null, "citeRegEx": "Suchan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Suchan et al\\.", "year": 2013}], "referenceMentions": [], "year": 2013, "abstractText": "The basic objective in smart meeting cinematography is to interpret professional interactions involving people, and automatically produce dynamic recordings of discussions, debates, presentations etc in the presence of multiple communication modalities. Typical modalities include gestures (e.g., raising one\u2019s hand for a question, applause), voice and interruption, electronic apparatus (e.g., pressing a button), movement (e.g., standing-up, moving around) etc.", "creator": "LaTeX with hyperref package"}}}