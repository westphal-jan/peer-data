{"id": "1306.1557", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2013", "title": "Extending Universal Intelligence Models with Formal Notion of Representation", "abstract": "selective linguistics is known to be universal, but secretive. its bases, also, the verbal vocabulary ( auditory context ) construction ( mdl ) principles, are significant in testing in the abstract, but user - empirical form. recent progress since remedy this gap leaded to development, the classical communication semantics that follows from formal decomposition of the construct of induction. in phylogenetic paper, linguistic representation of the internal structure in linguistic context facing artificial synthetic agents is tentative, for which type of representations is shown to be an hypothetical meta - heuristic natural developmental threat toward assuming general intelligence. hierarchical representations and model optimization with substantial use of entity - theoretic interpretation of weighted adaptive resonance are usually discussed.", "histories": [["v1", "Thu, 6 Jun 2013 21:11:19 GMT  (583kb)", "http://arxiv.org/abs/1306.1557v1", "proceedings of AGI 2012, Lecture Notes in Artificial Intelligence, Vol. 7716, pp. 242-251, Springer-Verlag, 2012. The final publication is available at link.springer.com"]], "COMMENTS": "proceedings of AGI 2012, Lecture Notes in Artificial Intelligence, Vol. 7716, pp. 242-251, Springer-Verlag, 2012. The final publication is available at link.springer.com", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["alexey potapov", "sergey rodionov"], "accepted": false, "id": "1306.1557"}, "pdf": {"name": "1306.1557.pdf", "metadata": {"source": "CRF", "title": "Extending Universal Intelligence Models with Formal Notion of Representation", "authors": ["Alexey Potapov", "Sergey Rodionov"], "emails": ["potapov@aideus.com", "rodionov@aideus.com"], "sections": [{"heading": null, "text": "Key words: Universal Agents, Kolmogorov Complexity, Minimum Description Length Principle, Representations"}, {"heading": "1 Introduction", "text": "The idea of universal induction and prediction on the basis of algorithmic information theory was invented a long time ago [1]. In theory, it eliminates the fundamental problem of prior probabilities, incorrect solutions of which result in such negative practical effects as overlearning, overfitting, oversegmentation, and so on. It would be rather natural to try to develop some models of universal intelligence on this basis. However, the corresponding detailed models were published only relatively recently (e.g. [2]). Moreover, the theory of universal induction was not popular even in machine learning. The reason is quite obvious \u2013 it offers incomputable methods, which additionally require training sets of large sizes in order to make good predictions.\nUnsurprisingly, such more practical alternatives as the Minimum Description Length (MDL) or the Minimum Message Length (MML) principles became much more popular. These principles help developers to considerably improve performance of machine learning and perception methods, but still they neither completely solve the problem of prior probabilities nor allow for universal machine learning systems.\nOf course, the universal intelligence models inherit the same drawbacks as the universal prediction. Namely, computational intractability is even more considerable here. Optimality of the models is proven up to some constant slowdown factor that\ncan be very large. This slowdown can be eliminated via self-optimization [3], but its time for unbiased intelligence can also be very large. Consequently, most researchers consider universal models as possibly interesting, but pure abstract tools.\nAt the same time, practical success of the MDL principle and its counterparts implies that there is a way toward a realistic implementation of universal induction. However, there is still a very large gap to be bridged. Indeed, applications of the MDL principle rely on hand-crafted heuristic coding schemes invented by developers for each specific task. These schemes specify algorithmically incomplete model spaces with large inductive bias resulting only in weakly learnable systems.\nIn order to bridge this gap, the notion of representation was recently formalized within the algorithmic information theory, and the Representational MDL (RMDL) principle was introduced [4]. This principle can be used to estimate quality of decomposition of the task of model construction for some large data series into relatively independent subtasks. Residual mutual information between these subtasks can be taken into account by adaptive resonance models, which also have the informationtheoretic formalization [5].\nIn this paper, we consider application of the RMDL principle as an unavoidable meta-heuristic for the model of the universal algorithmic intelligence. Only one heuristic is not enough to achieve efficient universal intelligence, but it makes this goal a little bit closer."}, {"heading": "2 Background", "text": "The model of intelligence as some sort of search for the best chain of actions was the first one adopted in the AI field. It can be applied for solving any problem, but only in the case of known determined settings and unlimited computational resources. Universal Solomonoff induction/prediction affords an opportunity to extend this model on the cases of arbitrary (computable) unknown environments. However, the problem of computational resources remains and becomes more complicated. Moreover, unbiased universal agent may need a lot of time to acquire necessary information about the world to become able to secure own survival even possessing infinite computational resources. Because speeding up the search for chains of actions can also be treated as learning, the induction problem should be considered in the first place.\nSolomonoff induction relies on the notion of algorithmic probability, which is calculated for a binary string \u03b1 as:\n\u20ac\nPU (\u03b1) = 2 \u2212 l( p ) p:U( p )=\u03b1 \u2211 , (1)\nwhere U is some Universal Turing Machine (UTM), and p is its program with length l(p) that produces the string \u03b1 being executed on the UTM U.\nProbabilities PU(\u03b1) are referred to as the universal prior distribution. Why are they universal? The basic answer to this question rests on the fact that any universal machine U can be emulated on another universal machine V by some program u: for any p, V(up)=U(p). Consequently,\n\u20ac\nPU (\u03b1) = 2 \u2212 l( p ) p:U( p )=\u03b1 \u2211 = 2l(u ) 2\u2212 l(up ) p:V (up )=\u03b1 \u2211 \u2264 2l(u )PV (\u03b1), (2)\nand similarly . This implies that difference between the algorithmic probabilities of arbitrary string \u03b1 on any two UTMs is not more than some multiplicative constant independent of \u03b1. Given enough data, likelihood will dominate over the difference in prior probabilities, so the choice of the UTM seems to be not too crucial.\nHowever, the amount of necessary additional data can be extremely large in practice. One can still refer to the algorithmic probabilities as universal priors, because no other distribution can be better in arbitrary unknown environment. Universality of this distribution simply means that it is defined on the algorithmically complete model space (any algorithm has non-zero probability and can be learned), and models are naturally ordered by their complexity (it is impossible to specify such universal machine that reverts this order).\nApparently, the universal agent based on the algorithmic probability (such as AI\u03be [2]) may require executing many actions to make history string long enough to neutralize influence of the arbitrarily selected U. And no unbiased intelligence can perform better.\nHowever, we don\u2019t want our universal agent to be absolutely unbiased. Quite the contrary, we do want it to be universal, but biased towards our world. In this context, dependence of the algorithmic probabilities on the choice of UTM appears to be very useful in order to put any prior information and to reduce necessary amount of training data. This idea was pointed out by different authors [6, 7]. It is also said [8] that the choice of UTM can affect the \u201crelative intelligence of agents\u201d.\nUnfortunately, no universal machine can eliminate necessity for exhaustive search for algorithms that produce the whole agent\u2019s history. At the same time, the pragmatic MDL principle is applied to algorithmically incomplete model spaces specified by hand-crafted coding schemes, which allow for efficient non-exhaustive search procedures. Of course, it is unacceptable to replace UTMs with Turing-incomplete machines as the basis of the universal intelligence. Can this intelligence apply the MDL principle in the same way as we do?"}, {"heading": "3 Representational MDL Principle", "text": "The minimum description length principle states that the best model of the given data source is the one which minimizes the sum of\n\u2013 the length, in bits, of the model description; \u2013 the length, in bits, of data encoded with the use of the model.\nIn theory, this principle is based on the Kolmogorov (algorithmic) complexity KU(\u03b1) that is defined for some string \u03b1 as:\n. (3)\nThe MDL principle is derived from the Kolmogorov complexity if one divides the program p for UTM p=\u00b5\u03b4 into the algorithm itself (the regular component of the model) \u00b5 and its input data (the random component) \u03b4:\n\u20ac\nKU (\u03b1) = minp [ l(p) |U (p) =\u03b1 ] = min\u00b5\u03b4 [ l(\u00b5\u03b4) |U (\u00b5\u03b4) =\u03b1] = min\u00b5 min\u03b4 [ l (\u00b5) +\n+l(\u03b4) |U (\u00b5\u03b4) =\u03b1 ] = min \u00b5 l(\u00b5) +min \u03b4 [ l (\u03b4) |U (\u00b5\u03b4) =\u03b1][ ] = min\u00b5 l(\u00b5) + KU (\u03b1 | \u00b5)[ ] .\n(4)\nHere, is the conditional Kolmogorov complexity\nof \u03b1 given \u00b5. Consequently, the equation\n(5)\ngives the best model via minimization of the model complexity l(\u00b5) and the model \u201cprecision\u201d K(\u03b1 | \u00b5)=l(\u03b4), where \u03b4 describes deviations of the data \u03b1 from the model \u00b5. This equation becomes similar to the Bayesian rule, if one assumes \u2013log2P(\u00b5)=l(\u00b5) and \u2013log2P(\u03b1 | \u00b5)=K(\u03b1 | \u00b5).\nThe MDL principle differs from the algorithmic probability in two aspects. The first one consists in selection of a single model. It can be useful in communications between intelligent agents or for reducing the amount of computations [9], but in general the MDL principle is a rough approximation of the algorithmic probability.\nThe second aspect consists in adopting the two-part coding. In practice, it helps to separate regular models from noise. This separation can be considered as a useful heuristic, but it is somewhat arbitrary within the task of model selection. In any case, Kolmogorov complexity is also incomputable. Thus, we still need to bridge the gap between the theoretical MDL principle and its practical applications. This is done (to some extent) within the Representational MDL principle.\nThe main idea here is that machine perception and machine learning methods are applied in practice to mass problems (sets of separate, individual problems of some classes). For example, any image analysis method is applied to different images independently searching for separate image descriptions in a restricted model space. On the contrary, the universal intelligence agent enumerates algorithms producing the whole history string. Let this history consists of a number of substrings (e.g. images) \u03b11\u03b12\u2026\u03b1n. If the agent tries to compute individual Kolmogorov complexities (or algorithmic probabilities) of these strings, the result in the most cases will be poor:\n\u20ac\nKU (\u03b1 i ) i=1\nn\n\u2211 >> KU (\u03b11\u03b12...\u03b1n ) , (6)\nbecause these substrings normally contain a lot of mutual information. This mutual information (let it be denoted by S) should be removed from descriptions of individual data strings, and should be considered as prior information in corresponding subtasks of analysis of individual substrings. This implies usage of the conditional Kolmogorov complexities K(\u03b1i | S). Indeed, one can expect that\n\u20ac\nKU (\u03b11\u03b12...\u03b1n ) \u2248 minS l (S) + KU (\u03b1 i | S) i=1\nn \u2211 \u239b\n\u239d \u239c\n\u239e \u23a0 \u239f << KU (\u03b1 i )\ni=1\nn\n\u2211 . (7)\nSince S can be interpreted as an algorithm (some program for UTM), which produces any given data string from its description, the algorithm S precisely fits the verbal notion of representation formulated by David Marr [10]. The notion of representation is treated in the same way in the papers on AGI (e.g. \u201cinternal representation interprets input reconstructing it\u201d [11]). Therefore, the following more strict definition can be given [4].\nDefinition. The program S for UTM U is called representation of the collection of data strings (e.g. images) , if\n\u20ac\n\u2200\u03b1 \u2208 \u0391( ) \u2203\u00b5,\u03b4 \u2208 0,1{ }*( )U (S\u00b5\u03b4) =\u03b1 . The string \u00b5\u03b4 is called description of \u03b1 within the representation S. This description consists of the regular \u00b5 and the random \u03b4 components.\nConsequently, the RMDL principle states that 1) the best model of the data string within given representation is the model, for which the sum of the length of the model and the length of this data string described with the use of this model is minimal; 2) the best representation of the collection of the data strings is the representation, for which the sum of the length of the representation and the summed length of the minimal descriptions of these data strings within the representation is minimal.\nWhen we consider any practical image analysis method, it uses some representation of images. This representation specifies an inductive bias similar to that specified by the choice of the UTM in algorithmic complexity or probability. However, the universal agent is based on the single UTM, while representations can differ for different sensor modalities or even for different elements of the same modality, they can be Turing-incomplete, and they can be learned and changed during lifetime.\nIt is interesting to note that for any two UTMs U and V and for any representation S for U there exists the equivalent representation S' for V such that KU(\u03b1 | S) = KV(\u03b1 | S') for any \u03b1. Indeed, it is obvious for S'=uS, where u emulates U on V. Thus, the choice of UTM influences on the representation construction, but not on the model selection within equivalent representations. Thus, we will write KS(\u03b1) instead of KU(\u03b1 | S), and KS(\u03b1 | \u00b5) instead of KU(\u03b1 | S\u00b5).\nIt should be pointed out that the RMDL principle is not just an extension of the two-part coding to a \u201cthree-part\u201d coding. Any three- (or more) part coding of an individual string could be re-structured to the two-part coding scheme [9], but S and \u00b5 in the RMDL principle cannot be united, because S describes the problem class, while \u00b5 describes its instance.\nIt is also interesting to note that the idea of deep learning architectures [12] arose from the fact that complexity of some models is exponentially larger within shallow representations than within deep representations. The RMDL principle allows for much more detailed analysis of the representation efficiency."}, {"heading": "4 Hierarchical Representations and Adaptive Resonance", "text": "Separate descriptions of substrings even within a good representation will still contain some mutual information (large-scale regularities in the initial string). Thus, if one has the string \u03b1 divided into the substrings \u03b11\u03b12\u2026\u03b1n, and the descriptions \u00b5i\u03b4i are independently constructed for each substring, it is natural to try to compress the string \u00b5=\u00b51\u00b52\u2026\u00b5n (deltas can be ignored on the next level of description since they are interpreted as noise within the RMDL principle). This string can still be very long, so one would like to divide \u00b5 into larger substrings (or to group \u00b5i) and to describe these substrings within some higher-level representation. Resulting models (regular parts of descriptions) can be further compressed, and so on.\nSpecific division of the string into substrings can be unknown a priori and can be considered as a part of a model. For example, borders of word and sentence segments in speech signals are not known. Images also should be segmented into some regions, which content can be described almost independently. For now, we can ignore the structure of these models and use only whole strings.\nThat is, at first the model \u00b5(1) is constructed for the string \u03b1 within the representation S(1). Then, the model \u00b5(2) is constructed for the string \u00b5(1) within some higherlevel representation S(2), and so on up to some level of abstraction m:\n,\n. (8)\nThe total description length (an approximation of Kolmogorov complexity) of the string \u03b1 can be calculated as:\n\u20ac\nL S (1) ,...,S (m) (\u03b1) = K S (1) (\u03b1 | \u00b5 (1)) + K S ( i ) (\u00b5 ( i) | \u00b5 ( i+1) )\ni=2\nm\u22121 \u2211 + l(\u00b5 (m)), (9)\nwhere .\nIt can be seen that sequential construction (8) of models of higher levels of abstraction is not the same as minimization of the total description length (9). Indeed, one should search for the models on all levels of abstraction simultaneously in order to get the optimal result (9). However, such the exhaustive search is computationally expensive. The sequential model construction is much more practical, but much less robust, because it is bottom-up and greedy.\nHere, one can adopt Grossberg Adaptive Resonance Theory. Some subsets of models should be considered on each level of abstraction, and models on different levels should support or suppress each other. Such models remain, for which resonance is established. Qualitative expression of support values can be derived from the RMDL principle in the form of equation (9), so it can be used in the informationtheoretic formalization of the Adaptive Resonance Theory [5].\nHierarchical decomposition of a problem into slightly dependent sub-problems, construction of their separate solutions, and adaptive correction of these solutions in accordance with the whole problem can be considered as almost universal metaheuristic."}, {"heading": "5 Adoption of the RMDL Principle in Universal Algorithmic Intelligence", "text": "The opinion that representations should be incorporated into the models of general intelligence has been already stated [13, 14]. However, representations are usually implemented only in the form of prior information expressed in a special design of programming language. Besides insufficiency of strict quantitative analysis of representation quality, the main restriction here is absence of decomposition of the model construction task.\nOn the other hand, necessity of decomposition is also realized. In particular, importance of chunks and possibility to solve tasks only of small Kolmogorov complexity are noted [7, 15, 16]. The RMDL principle can strictly account for both these aspects.\nConsider the universal intelligent agent based on the algorithmic probability. We will use Hutter\u2019s AI\u03be model for convenience in order to skip unnecessary detailed descriptions of less known models. The AI\u03be agent is intended to maximize the total reward choosing its actions [2]:\n, (10)\nwhere y<k is the string of agent\u2019s actions till the time moment k, and x<k is the string of sensory history (including reward signals); p are possible agent\u2019s policies consistent with the history, and q are possible algorithmic models of the environment; is\nthe expected future reward summed in the [k, mk] time interval executing algorithms p and q on the UTM U.\nThe formal notion of representation can be almost straightforwardly applied to the agent\u2019s inputs x<k. Although the RMDL principle can be extended from Kolmogorov complexity to algorithmic probability, we will use its basic version for the sake of simplicity (differences between Kolmogorov complexity and algorithmic probability are discussed in our companion paper). If one uses only one best model qopt, the equation (10) can be rewritten:\n, (11)\nTo apply the RMDL principle, one should decompose qopt into some set of (nearly) independent models qi conditioned by some representation S for the segmented history , where m1=0 and mn=k: (however,\nit should be noted that this form of decomposition/segmentation is not universal).\nIn this case, qi can be sought independently. If l(q)\u2248l(S)+l(q1)+\u2026+l(qn), the complexity of the full task will be , while the complexity of the decomposed\ntask will be that is much smaller. One can also divide qi into the model\n\u00b5i and noise \u03b4i further simplifying the search problem. However, in order to calculate it is necessary to predict future values of the input. This is impossible if\ninduction is aborted after construction of the set of decomposed model {qi}. If qi are really independent, they are unpredictable. However, this is not the case in reality. Thus, one should construct a higher level model, which produces the sequence q1:n, and extrapolates it. A number of intermediate levels of the representation can be introduced, and the hierarchical model can be optimized with help of adaptive resonance as it was described in the previous section.\nAnother difference from the pure RMDL principle here is that the environment model q takes agents actions y<k as input. Should the whole history of actions be taken for each partial model qi? Probably, no. Here, one can think about representations for action history.\nIt is attractive to try to decompose the program p in the same way as it was done for the program q. However, there is a huge difference between these programs. The program q is used as the environment model in predicting the inputs x. However, the agent doesn\u2019t need to predict own actions, since they can be chosen directly:\n. (12)\nThis form of search is even less computationally expensive, because action chains have bounded complexity, while programs p can have arbitrarily large complex-\nity. Thus, there is no sense to enumerate all programs p and to decompose them. However, search in the space of all possible action chains is still too computationally expensive. It is clear that any simplification of this exhaustive search should be done very carefully in order to avoid substantial limitations of the agent\u2019s universality.\nThe notion of representation can still be useful here. One can imagine some generalized actions, which can be introduced as some combinations of elementary actions, or even as small programs pi. These generalized actions will be useful only in the case, when the total number of chains of these actions is not larger than the total number of chains of elementary actions (this condition can be expressed also in probabilistic terms for stochastic search). Thus, variety of generalized actions will be smaller, and their introduction can be formally grounded only on the base of a criterion that takes computational costs of the search strategy into account. Such criterion is now absent, and possibility to mathematically introduce representations for actions can be proposed only as an idea.\nIt is interesting to note that if generalized actions are enumerated, one can consider models of the environment that accept chains of these generalized actions as input:\n, . (13)\nIndeed, humans rarely predict explicit reaction of the environment on their each very elementary action. At the same time, generalized actions pi can also accept generalized input strings qi. Indeed, we say \u201ctake the apple\u201d or \u201copen the door\u201d. That is, representations for sensory data (including generalized rewards) and actions are interconnected. Search in the space of generalized entities can be greatly simplified (but representations should be still constructed using the Turing-complete space). This approach can be used to gradually introduce advanced representations as priors for efficient generally intelligent agents starting from low-level representations for raw data and elementary actions and finishing with knowledge representations."}, {"heading": "6 Conclusions", "text": "The notion of representation is extremely useful for almost all cognitive functions. However, it is rarely defined strictly enough. The necessary formal definition was recently given jointly with the Representational MDL principle, which is derived from decomposition of Kolmogorov complexity. In this paper, we discussed possibility to extend the model of universal algorithmic intelligence (namely AI\u03be). We showed that this principle can be rather naturally incorporated into this model making it somewhat closer to efficient artificial general intelligence. Information-theoretic criteria of quality of representations and models can be used for consequently constructing more optimal methods of machine perception and learning, including multilevel systems with adaptive resonance.\nHowever, the RMDL principle only partially solves the problem of quality of representations in the models of universal algorithmic intelligence. It was initially introduced for such tasks, which decomposition is defined a priori (e.g. a computer vision system should analyze images independently), and representations are needed in order to decrease negative effects of this decomposition. However, there is no given decomposition of the task of prediction in the case of the universal agent. Decomposition is necessary for reducing computational complexity, but it leads to increase of algorithmic (Kolmogorov) complexity of environment models. Thus, representations trade computational complexity for algorithmic complexity. Apparently, the RMDL principle based on Kolmogorov complexity is only a particular case of constant computational complexity. In future, generalized RMDL principle should be developed based on Levin complexity (e.g. defined in [17]). Representations for Levin complexity can help to strictly account for the bias in complexity of models, which are used many times in descriptions of different data segments or executed many time during prediction and sequential decision making. Another open problem consists in formalization of representations not only for sensory input, but also for actions. We believe that such formalization can help to develop a theory of efficient self-optimization."}], "references": [{"title": "A Formal Theory of Inductive Inference, par1 and part 2", "author": ["R. Solomonoff"], "venue": "Information and Control, vol. 7, pp. 1\u201322, 224\u2013254", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1964}, {"title": "Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability", "author": ["M. Hutter"], "venue": "Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "The New AI: General & Sound & Relevant for Physics", "author": ["J. Schmidhuber"], "venue": "Artificial General Intelligence. Cognitive Technologies, B. Goertzel and C. Pennachin (Eds.), pp. 175\u2013198. Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Comparative Analysis of Structural Representations of Images based on the Principle of Representational Minimum Description Length", "author": ["A.S. Potapov"], "venue": "Journal of Optical Technology, vol. 75, iss. 11, pp. 715\u2013720", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Theoretic-Informational Approach to the Introduction of Feedback into Multilevel Machine-Vision Systems", "author": ["A.S. Potapov"], "venue": "Journal of Optical Technology, vol. 74, iss. 10, pp. 694\u2013699", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Algorithmic Probability, Heuristic Programming and AGI", "author": ["R. Solomonoff"], "venue": "E.Baum, M.Hutter, E.Kitzelmann (Eds), Advances in Intelligent Systems Research, vol. 10 (Proc. 3 Conf. on AGI, Lugano, Switzerland, March 5-8, 2010), pp. 151\u2013157", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A Computational Approximation to the AIXI Model", "author": ["S. Pankov"], "venue": "Frontiers in Artificial Intelligence and Applications (Proc. 1 AGI Conference), vol. 171, pp. 256\u2013267", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Bias and No Free Lunch in Formal Measures of Intelligence // Journal of Artificial General Intelligence 1, pp", "author": ["B. Hibbard"], "venue": "54\u201361", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Compression and Intelligence: Social Environments and Communication", "author": ["D. Dowe", "J. Hernandez-Orallo", "P. Das"], "venue": "Proc. Artificial General Intelligence \u2013 4 International Conference, Mountain View, CA, USA, August 3\u20136, 2011. Lecture Notes in Computer Science 6830, pp. 204\u2013211, Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Vision: A Computational Investigation into the Human Representation and Processing of Visual Information", "author": ["D. Marr"], "venue": "MIT Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1982}, {"title": "Hebbian Constraint on the Resolution of the Homunculus Fallacy Leads to a Network that Searches for Hidden Cause-Effect Relationships", "author": ["A. Lorincz"], "venue": "B. Goertzel, P. Hitzler, M. Hutter (Eds), Advances in Intelligent Systems Research, vol. 8 (Proc. 2 Conf. on Artificial General Intelligence, Arlington, Virginia, USA, March 6-9, 2009), pp. 126\u2013131", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, vol. 2, no. 1, pp. 1\u2013127", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Program Representation for General Intelligence", "author": ["M. Looks", "B. Goertzel"], "venue": "B. Goertzel, P. Hitzler, M. Hutter (Eds), Advances in Intelligent Systems Research, vol. 8 (Proc. 2 Conf. on Artificial General Intelligence, Arlington, Virginia, USA, March 6-9, 2009), pp. 114\u2013119", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards Practical Universal Search", "author": ["T. Schaul", "J. Schmidhuber"], "venue": "E.Baum, M.Hutter, E.Kitzelmann (Eds), Advances in Intelligent Systems Research, vol. 10 (Proc. 3 Conf. on AGI, Lugano, Switzerland, March 5-8, 2010), pp. 139\u2013144", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Artificial General Segmentation", "author": ["D. Hewlett", "P. Cohen"], "venue": "E.Baum, M.Hutter, E.Kitzelmann (Eds), Advances in Intelligent Systems Research, vol. 10 (Proc. 3 Conf. on AGI, Lugano, Switzerland, March 5\u20138, 2010), pp. 31\u201336", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "The CHREST Architecture of Cognition", "author": ["F. Gobet", "P.C.R. Lane"], "venue": "The Role of Perception in General Intelligence. In: E.Baum, M.Hutter, E.Kitzelmann (Eds), Advances in Intelligent Systems Research, vol. 10 (Proc. 3 Conf. on AGI, Lugano, Switzerland, March 5\u20138, 2010.), pp. 7\u201312", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Frontier Search", "author": ["Y. Sun", "T. Glasmachers", "T. Schaul", "J. Schmidhuber"], "venue": "E.Baum, M.Hutter, E.Kitzelmann (Eds), Advances in Intelligent Systems Research, vol. 10 (Proc. 3 Conf. on AGI, Lugano, Switzerland, March 5-8, 2010.), pp. 158\u2013163", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The idea of universal induction and prediction on the basis of algorithmic information theory was invented a long time ago [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "[2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "This slowdown can be eliminated via self-optimization [3], but its time for unbiased intelligence can also be very large.", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "In order to bridge this gap, the notion of representation was recently formalized within the algorithmic information theory, and the Representational MDL (RMDL) principle was introduced [4].", "startOffset": 186, "endOffset": 189}, {"referenceID": 4, "context": "Residual mutual information between these subtasks can be taken into account by adaptive resonance models, which also have the informationtheoretic formalization [5].", "startOffset": 162, "endOffset": 165}, {"referenceID": 1, "context": "Apparently, the universal agent based on the algorithmic probability (such as AI\u03be [2]) may require executing many actions to make history string long enough to neutralize influence of the arbitrarily selected U.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "This idea was pointed out by different authors [6, 7].", "startOffset": 47, "endOffset": 53}, {"referenceID": 6, "context": "This idea was pointed out by different authors [6, 7].", "startOffset": 47, "endOffset": 53}, {"referenceID": 7, "context": "It is also said [8] that the choice of UTM can affect the \u201crelative intelligence of agents\u201d.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "It can be useful in communications between intelligent agents or for reducing the amount of computations [9], but in general the MDL principle is a rough approximation of the algorithmic probability.", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "Since S can be interpreted as an algorithm (some program for UTM), which produces any given data string from its description, the algorithm S precisely fits the verbal notion of representation formulated by David Marr [10].", "startOffset": 218, "endOffset": 222}, {"referenceID": 10, "context": "\u201cinternal representation interprets input reconstructing it\u201d [11]).", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "Therefore, the following more strict definition can be given [4].", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "Any three- (or more) part coding of an individual string could be re-structured to the two-part coding scheme [9], but S and \u03bc in the RMDL principle cannot be united, because S describes the problem class, while \u03bc describes its instance.", "startOffset": 110, "endOffset": 113}, {"referenceID": 11, "context": "It is also interesting to note that the idea of deep learning architectures [12] arose from the fact that complexity of some models is exponentially larger within shallow representations than within deep representations.", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "Qualitative expression of support values can be derived from the RMDL principle in the form of equation (9), so it can be used in the informationtheoretic formalization of the Adaptive Resonance Theory [5].", "startOffset": 202, "endOffset": 205}, {"referenceID": 12, "context": "The opinion that representations should be incorporated into the models of general intelligence has been already stated [13, 14].", "startOffset": 120, "endOffset": 128}, {"referenceID": 13, "context": "The opinion that representations should be incorporated into the models of general intelligence has been already stated [13, 14].", "startOffset": 120, "endOffset": 128}, {"referenceID": 6, "context": "In particular, importance of chunks and possibility to solve tasks only of small Kolmogorov complexity are noted [7, 15, 16].", "startOffset": 113, "endOffset": 124}, {"referenceID": 14, "context": "In particular, importance of chunks and possibility to solve tasks only of small Kolmogorov complexity are noted [7, 15, 16].", "startOffset": 113, "endOffset": 124}, {"referenceID": 15, "context": "In particular, importance of chunks and possibility to solve tasks only of small Kolmogorov complexity are noted [7, 15, 16].", "startOffset": 113, "endOffset": 124}, {"referenceID": 1, "context": "The AI\u03be agent is intended to maximize the total reward choosing its actions [2]:", "startOffset": 76, "endOffset": 79}, {"referenceID": 16, "context": "defined in [17]).", "startOffset": 11, "endOffset": 15}], "year": 2012, "abstractText": "Solomonoff induction is known to be universal, but incomputable. Its approximations, namely, the Minimum Description (or Message) Length (MDL) principles, are adopted in practice in the efficient, but non-universal form. Recent attempts to bridge this gap leaded to development of the Representational MDL principle that originates from formal decomposition of the task of induction. In this paper, possible extension of the RMDL principle in the context of universal intelligence agents is considered, for which introduction of representations is shown to be an unavoidable meta-heuristic and a step toward efficient general intelligence. Hierarchical representations and model optimization with the use of information-theoretic interpretation of the adaptive resonance are also discussed.", "creator": "Word"}}}