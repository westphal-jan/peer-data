{"id": "1306.3317", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2013", "title": "Sparse Auto-Regressive: Robust Estimation of AR Parameters", "abstract": "lastly this paper simulations present a coherent approach for regression of time series using their own samples. below is a celebrated solution translated as pattern - reversal. dealing with outlier random missed transitions in single periodic series makes the problem between estimation unlikely, although it plays somewhat robust against them. moreover for formal purposes i thus show that hilbert explicitly desired the residual of the - regression partially omitted. ignoring these aims, i would describe a multivariate residual prior on linear residual and randomly obtain the resolution. two simple simulations have been developed on spectrum estimation and speech assessment.", "histories": [["v1", "Fri, 14 Jun 2013 07:49:44 GMT  (405kb)", "http://arxiv.org/abs/1306.3317v1", "4 pages, 4 figures"], ["v2", "Tue, 18 Aug 2015 16:59:06 GMT  (523kb)", "http://arxiv.org/abs/1306.3317v2", "4 pages, 4 figures"]], "COMMENTS": "4 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mohsen joneidi"], "accepted": false, "id": "1306.3317"}, "pdf": {"name": "1306.3317.pdf", "metadata": {"source": "CRF", "title": "Sparse Auto-Regressive: Robust Estimation of AR Parameters", "authors": [], "emails": [], "sections": [{"heading": null, "text": "Keywords: AR parameter estimation, Robust estimation,\nSpectral estimation and Speech coding\n1. Introduction\nFor more than 3 decades Autoregressive (AR) has been considered as an important model in signal processing. This model is exploited in many applications such as time-series analysis, speech processing and spectral estimation. In this model each sample of a signal is represented in terms of a linear combination of the other\nsamples of the signal. The backward equation is:\n[\n] [\n] [ ] [\n]\n(1)\nwhere, is a signal and is the corresponding autoregressive coefficients. is the residual error. Let denote the above equation in the following matrix form:\n(2)\nThis problem is just a regression; a traditional problem is solved by minimizing the least squares estimation. The MSE minimization problem and its solution are:\n\u2016 \u2016\n(3)\n(4)\nIf one assume Gaussian prior for the residual this solution is also MAP estimation and minimum entropy estimation.\nBut Gaussian prior results in sensitivity to outlier and missing samples. Furthermore, many real signals does not fit with the Gaussian distribution because these signals are distributed with tails decaying slower than Gaussian and have peak taper than Gaussian (like speech signals). Gaussian assumption on the residual implies that the observations must to be Gaussian too, but neither the observations nor the residuals are Gaussian. If a presented sample deviates from the mean of the Gaussian distribution, this observation has an adverse effect on the estimations. To address this problem, [1] suggest imposing the mixture of Gaussians distributions for the residual. Parameters of this model need to be learned, so appropriate learning is an important problem that increases the complexity of the method. Another approach assumes the long tailed distributions, [2] exploited student-t distribution. In this paper I use this approach. Moreover it is desired to make autoregressive model robust against missing data. [3] presented the application of autoregressive analysis in missing data problems. In [4] an algorithm was suggested for estimation of AR parameters in missing data situation. From statistical point of view, missing a data causes an outlier in the residual signal, thus the prior on the residual distribution must adopt outliers. Using long tailed distributions may be suitable to adoption in the outlier scenario. In this paper I propose to use multivariate Gaussian distribution as the prior on the residual. By this assumption the least squares regression problem in (2) converts to the sparse regression. An approximation of the sparse regression is least absolutes regression that [5] exploited this regression for robust AR parameters estimation. Least absolutes regression is a MAP estimator by Laplace distribution prior on the residual that it has longer tail in contrast with Gaussian.\n2. Multivariate Gaussian for the residual\ndistribution\nIn this section the multivariate Gaussian distribution will be assumed as the prior distribution for the residual of the regression:\n|\n(\u221a ) | |\nThe ML estimation problem for W is:\n|\n(6)\nLet me first obtain an optimum W:\n( | ) | |\n\u2016 \u2016\nwhere, is a small positive and is introduced just for avoiding the division by zero. \u2016 \u2016 denotes the zero norm of a vector which is defined as the number of nonezero elements of it. Continuing with this distribution, the ML estimation of autoregressive coefficients is as follows:\n\u2016 \u2016\n(7)\nThe constraint of this problem is an over-determined system of linear equations that all the equations cannot be satisfied. [6] has named this system of equations the \"robust sensing\" problem. In robust sensing, the goal is to find vector which maximizes the number of satisfied equations. is the i'th equation. Where denotes i'th row of X. [6] has also showed that robust sensing is an NP-Hard problem and proved that the solution of robust sensing in a certain conditions equal to:\n\u2016 \u2016\n(8)\nProblem (8) equals to the MAP estimation of by Laplace prior on the residual. Equivalence of (7) and (8) is not a surprise result because Laplace distribution agrees with sparse signals. Problem (7) is robust to outliers and gross errors but this equation is very sensitive to a Gaussian residual because Gaussian assumption on the residual does not allow the residual to be exactly zero. Thus, only p equations can be satisfied simultaneously. To handle this problem, I suggest the following problem:\n\u2211\n(9)\nIf tends to zero, this problem equals to (7). is a constant depends on the Gaussian variance ( ). Since 95% samples of a Gaussian process deviates less than from the mean thus seems good. By this the residuals less than approximately penalized by\nnorm 2, residuals greater than approximately penalized by norm 0. This problem similar to (7) is also none convex. To solve it I exploit Graduated nonconvexity (GNC) technique [7] that will be described in Section 4."}, {"heading": "3. Relation to sparse representation", "text": "To understand how sparse residual is related to the sparse representation in terms of over complete bases, I re-write (8) in the following form:\n\u2016 \u2016 \u2016 \u2016\n(10)\nDefining [ ]\n\u2016 [ ]\u2016\n\u2016 \u2016\nPut =[ ] then penalize just some elements of\ncorresponds to .\n(11)\n\u2016 \u2016 \u2211 | |\n(12)\n(12) is a sparse decomposition problem with group sparse constraint. Making sparse the coefficients that corresponds to the identity matrix is equivalent to making sparse the residual of a regression problem. It's possible to exploit some other matrices instead of identity matrix that better makes the residual sparse. But the approach of this paper is not this idea. If computational burden does not have any care, research from this view will be more successful because learning appropriate dictionary for different components of signals provide suitable representation domain for processing. For example if the residual of our problem is band limited, then we can use DCT or FFT matrix instead of identity matrix to have more sparse residual.\n4. Optimization\nProblem (9) is a kind of M-estimator [8]. A well-known method to solve this type of problems is Iterative Reweighted Least Squares (IRLS) [9]. Figure 1 depicts the algorithm for solving problem (9).\nFor small values, the whole problem is extremely nonconvex. Thus it is very probable that the algorithm may be trapped in a local optimum. To reduce this probability the following criterion for re-weighting has been used:\n| |\n(13)\nWhere, p is an increasing scalar tends to 2 as the iterations tend to infinity which is inspired by the idea of GNC."}, {"heading": "5. Application to time series with missing data", "text": "Problems based on minimization of MSE are very sensitive to missing data, because missed data probably have large errors and their squared errors have enormous effect, while M-estimators like the proposed estimation are able to reduce the adverse effect of gross errors. In this section I just show an intuitive experience on the applicability of the proposed estimation by an example. Figure 2 shows a synthetic time series with 64 samples that 25% of their samples has been lost.\nFigure 2-A shows the original series and the series with missed samples. In figure 2-B the estimated time series and its spectrum using original time series has been depicted. Figure 2-C shows the estimated time series and its spectrum using missing time series have been calculated by solving yule-walker equations. Figure 2-D is similar to 2-C but using the proposed algorithm. As it can be seen, the estimated spectrum resolves the peaks of the spectrum more accurately. Table 1 shows performance of the proposed method in spectral estimation of mixed two sinusoids contaminated by AWGN noise while 25% of samples had been lost.\nestimated spectrum by whole data and missed data.\nTable1: Correlation of spectrums; estimated by whole data and lost data.\nMethod/SNR 10 15 20 25 30 Yulw-Waker 0.67 0.70 0.74 0.78 0.81 Proposed 0.67 0.72 0.80 0.86 0.92\nAccording to (9) if the residual contains strong Gaussian noise both methods minimize same cost function thus performances are equal. But if SNR be high two cost functions are equal just for small residuals. Problem (9) for any large value of residuals just considers a constant amount of penalty."}, {"heading": "6. Application to speech coding", "text": "As described previously, the problem (8) differs from traditional AR in the sense of residual distribution. In the proposed problem the residual has Laplace distribution. Let me compare the entropy of Gaussian and Laplace distributions. In this section assume that and are distributions of two residuals that both of them are corresponding to the regression of a signal.\n\u221a\n(14)\n\u221a\n\u221a | |\n(15)\n[ ( )] \u221a (16)\n[ ( )] (\u221a ) (17)\n\u221a\n(18)\nshows the entropy difference of two Gaussian and Laplace sources. Figure 3 depicts both distributions with the same variance. The peak of Laplace distribution is sharper than Gaussian. In other words, the number of samples of a Laplace source around zero is more than Gaussian. But it is obvious that the variance of residual is minimized if Gaussian prior is assumed. Although many samples of a Laplace source are around zero, but few\nentropies of two Gaussian and Laplace sources are equal, the Gaussian source has less variance. Thus according to (18) coding of the Laplace residual cannot be considered for lossless speech compression because the variance of Laplace residual may become large due to some large residuals. But a framework for a lossy coding scheme can be designed. If the samples of the Laplace residual corresponding to large errors be limited to a certain bound, variance of the residual decreases tremendously. Assume this bound is selected so that to satisfy:\n\u221a\n(19)\nSince clipping a Laplace samples to a bound makes the peaks of the PDF to go on the borders, thus indeed the delta entropy is more than the equation (19). In the following I present an experiment on speech coding. For the simulation, 100 sentences from TIMIT database has been used. In this experiment, at first the signals are converted into some sets of non-overlapping frames and then auto-regression is applied in each frame. Afterward the residual for each frame is quantized to levels. Let me introduce the parameter K for the simulations that is equal to the fraction of maximum values of residuals for a signal. By this parameter the bound for the projection of large residuals will be determined. K is inversely related to L in equation (19). Samples which are larger than are projected on , and those which are less than are projected on . Figure 4-A shows the average SNR versus 1/K. for K=1 there is only quantization noise. But by decreasing K, since some samples are clipped (corresponds to large residuals) SNR decrease rapidly. In this figure SNR refers to the difference of the clipped and quantized samples with the original residual samples. Figure 4-B shows the average entropy versus 1/K. As expected entropy has direct relation to SNR. Figure 4-C shows the percentage of the number of residual samples that are clipped versus 1/K. for example if we put K=0.2 only near 0.5% of samples are clipped but the entropy decrease to near 21%. This property does not exist for the residual obtained by Gaussian prior for example if we clip the Gaussian residuals with K=0.2 near 0.9% of samples must be clipped for saving only %15 in entropy. Another advantage of Laplace distribution over Gaussian is concentration of residual samples around zero. Figure 4-D compares the percentage of residual samples that are quantized to zero level versus SNR of additive noise to the original signal. As expected, by AR coding the percentage of small residuals will increase in comparison with the original signal. It can be seen that AR with sparse constraint increases the number of samples around zero. Note that SNR in the last figure is different from SNR in figure 3- A.\nsamples. D) percentage of zero level 8-bit quantized versus SNR"}, {"heading": "7. Conclusion", "text": "Sparse domains in signal processing make interpretations and designs simpler. As was seen in this paper, sparse residual for Auto-regression parameter estimation brings suitable properties for the residual signal. Only two applications of this approach has been studied but this robust estimation can be applied in many fields in which auto-regressive is an appropriate model."}], "references": [{"title": "Variational Bayes for generalized autoregressive models", "author": ["S.J. Roberts", "W.D. Penny"], "venue": "Signal Processing, IEEE Transactions on , vol.50, no.9, pp. 2245- 2257, Sep 2002", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust Autoregression: Student-t Innovations Using Variational Bayes", "author": ["J. Christmas", "R. Everson"], "venue": "Signal Processing, IEEE Transactions on , vol.59, no.1, pp.48-57, Jan. 2011", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonparametric spectral density estimation with missing observations", "author": ["Lee, T.C.M.", "Zhengyuan Zhu"], "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on , vol., no., pp.3041-3044, 19-24 April 2009", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimation of autoregressive spectra with randomly missing data", "author": ["P.M.T. Broersen", "S. de Waele", "R. Bos"], "venue": "Instrumentation and Measurement Technology Conference, 2003. IMTC '03. Proceedings of the 20th IEEE , vol.2, no., pp. 1154- 1159 vol.2, 20-22 May 2003", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "A Generalized Least Absolute Deviation Method for Parameter Estimation of Autoregressive Signals", "author": ["Youshen Xia", "Kamel, M.S."], "venue": "Neural Networks, IEEE Transactions on , vol.19, no.1, pp.107-118, Jan. 2008", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "From Sparse Signals to Sparse Residuals for Robust Sensing", "author": ["V. Kekatos", "G.B. Giannakis"], "venue": "Signal Processing, IEEE Transactions on , vol.59, no.7, pp.3355-3368, July 2011", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual Reconstruction", "author": ["Andrew Blake", "Andrew Zisserman"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "Robust M-estimates and generalized Mestimates for autoregressive parameter estimation", "author": ["A. Basu", "K.K. Paliwal"], "venue": "TENCON '89. Fourth IEEE Region 10 International Conference , vol., no., pp.355-358, 22-24 Nov 1989", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}], "referenceMentions": [{"referenceID": 0, "context": "To address this problem, [1] suggest imposing the mixture of Gaussians distributions for the residual.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "Another approach assumes the long tailed distributions, [2] exploited student-t distribution.", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "[3] presented the application of autoregressive analysis in missing data problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "In [4] an algorithm was suggested for estimation of AR parameters in missing data situation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "An approximation of the sparse regression is least absolutes regression that [5] exploited this regression for robust AR parameters estimation.", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "[6] has named this system of equations the \"robust sensing\" problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] has also showed that robust sensing is an NP-Hard problem and proved that the solution of robust sensing in a certain conditions equal to:", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "To solve it I exploit Graduated nonconvexity (GNC) technique [7] that will be described in Section 4.", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "A well-known method to solve this type of problems is Iterative Reweighted Least Squares (IRLS) [9].", "startOffset": 96, "endOffset": 99}], "year": 2013, "abstractText": "In this paper I present a new approach for regression of time series using their own samples. This is a celebrated problem known as Auto-Regression. Dealing with outlier or missed samples in a time series makes the problem of estimation difficult, so it should be robust against them. Moreover for coding purposes I will show that it is desired the residual of auto-regression be sparse. To these aims, I first assume a multivariate Gaussian prior on the residual and then obtain the estimation. Two simple simulations have been done on spectrum estimation and speech coding.", "creator": "Microsoft\u00ae Word 2010"}}}