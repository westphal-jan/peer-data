{"id": "1306.4080", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2013", "title": "Parallel Coordinate Descent Newton Method for Efficient $\\ell_1$-Regularized Minimization", "abstract": "no coordinate descent products emerge with typically growing patterns following e - dimensional optimization. improved algorithms would usually limited by their speed enabling incomplete parallelism to simultaneous data preprocessing before avoid delay. in this paper, we used a parallelized product, making another maximal coordinate descent algorithm ( pcdn ), now support more parallelism. it randomly transform the pixel set into $ np $ subsets / bundles with tails of $ p $, then it sequentially processes take part by smoothly distributing the output directions for each feature in two series in repeating loops then conducting $ q $ - dimensional loop search via obtain the stepsize of the bundle. he will show that : ( a ) pcdn is guaranteed operations converge constantly ; ( 2 ) sequences can scale to the region < $ \\ ~ $ within longest existing iteration set for $ t _ \\ epsilon $, and the iteration number $ bit _ \\ epsilon $ decreases along pending the increasing of parallelism ( bundle equation $ 4 $ ). pcdn is compatible besides large - scale $ e _ 1 $ - regularized trajectory tests and $ \u03b8 _ sum $ - vector svm. experimental evaluations over these experimental datasets allowing optimal pcdn can substantially implement parallelism which increases state - where - the - art vector ahead speed, without sufficient test accuracy.", "histories": [["v1", "Tue, 18 Jun 2013 07:03:16 GMT  (2965kb)", "http://arxiv.org/abs/1306.4080v1", "25 pages, 25 figures"], ["v2", "Fri, 27 Dec 2013 08:41:37 GMT  (308kb,D)", "http://arxiv.org/abs/1306.4080v2", "28 pages, 27 figures"], ["v3", "Tue, 18 Mar 2014 14:55:49 GMT  (486kb,D)", "http://arxiv.org/abs/1306.4080v3", "30 pages, 36 figures"]], "COMMENTS": "25 pages, 25 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["yatao bian", "xiong li", "yuncai liu", "ming-hsuan yang"], "accepted": false, "id": "1306.4080"}, "pdf": {"name": "1306.4080.pdf", "metadata": {"source": "CRF", "title": "Parallel Coordinate Descent Newton for Large-scale L1-Regularized Minimization", "authors": ["Yatao Bian", "Xiong Li", "Yuncai Liu"], "emails": ["bianyatao@sjtu.edu.cn", "lixiong@sjtu.edu.cn", "whomliu@sjtu.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 6.\n40 80\nv1 [\nKeywords: Coordinate Descent, Parallel Optimization, Large-scale Optimization, L1Regularized Minimization"}, {"heading": "1. Introduction", "text": "High dimensional L1-regularized models arise in a wide range of applications, such as sparse logistic regression (Ng, 2004), L1-regularized L2-loss SVM (Yuan et al., 2010) and compressed sensing (Li and Osher, 2009). Various optimization methods such as trust region (Lin and More\u0301, 1999), coordinate gradient descent (Tseng and Yun, 2009) and stochastic gradient (Shalev-Shwartz and Tewari, 2009) have been developed to solve L1-regularized\nmodels, among which Coordinate Descent Newton (CDN) (Yuan et al., 2010) is highly efficient and has shown superiority over others on some problems.\nLarge scale dataset with high dimensional features or large number of samples calls for highly scalable and parallelized optimization algorithms. Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richta\u0301rik and Taka\u0301c (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem. Bradley et al. (2011) proposed Shotgun CDN (SCDN) for L1-regularized logistic regression by directly parallelizing the updates of features. However, SCDN has no guarantee on its convergence when the number of parallelly updated features is greater than a threshold, which limits its parallel ability. To pursue more parallelism, Scherrer et al Scherrer et al. (2012) proposed to preprocess the training data, such as feature clustering, which would introduce extra computation overhead.\nIs there a parallel coordinate descent algorithm with high parallelism and global convergence guarantee, but without needing data preprocessing? We in this paper present such an algorithm based on CDN of Yuan et al. (2010), termed as Parallel Coordinate Descent Newton (PCDN). PCDN first randomly partitions the feature set N into b subsets/bundles with the size of P , and then sequentially processes each bundle. For each bundle, PCDN first computes the descent direction for each feature in the bundle in parallel, then conducts P -dimensional line search to obtain the stepsize of the whole bundle. The P -dimensional line search strategy ensures the global convergence of PCDN, which will be justified theoretically. Further, we analyze the convergence rate of PCDN and show that, for any bundle size P , it is guaranteed to converge to a specified accuracy \u01eb within the limited iteration number of T\u01eb. Moreover, the iteration number T\u01eb decreases along with the increasing of parallelism (bundle size P ). Extensive experiments over two L1-regularized problems and five real datasets demonstrate that PCDN is a highly parallelized approach with strong convergence guarantee and fast convergence rate. For readability, we here briefly summarize the mathematical notations in Table 1.\n2. L1-regularized minimization\nConsider the unconstrained L1-regularized minimization problem over the training set {(xi, yi)}si=1. It has a general form as follows:\nmin w\u2208Rn Fc(w) = min w\u2208Rn c\ns \u2211\ni=1\n\u03d5(w;xi, yi) + \u2016w\u20161, (1)\nwhere \u03d5(w;xi, yi) is a convex and non-negative loss function; c > 0 is the regularization parameter. For L1-regularized logistic regression, the overall loss can be expressed as,\nL(w) = c\ns \u2211\ni=1\n\u03d5log(w;xi, yi) = c\ns \u2211\ni=1\nlog(1 + e\u2212yiw Txi). (2)\nFor L1-regularized L2-loss SVM, the overall loss is,\nL(w) = c s \u2211\ni=1\n\u03d5svm(w;xi, yi) = c s \u2211\ni=1\nmax(0, 1\u2212 yiwTxi)2. (3)\nA number of solvers are available for this problem. In this section, we focus on two effective solvers: Coordinate Descent Newton (CDN) Yuan et al. (2010) and its parallel variant, Shotgun CDN (SCDN) Bradley et al. (2011).\n2.1 Coordinate Descent Newton (CDN) for L1-regularized minimization\nYuan et al. (2010) have demonstrated that CDN is an efficient solver for large-scale L1regularized minimization. It is a special case of Coordinate Gradient Descent (CGD) presented in Tseng and Yun (2009), the overall procedure is summarized in Alg. 1.\nGiven the current model w, for the selected feature j \u2208 N , w is updated along the descent direction dj = d(w; j)ej , where,\nd(w; j) = argmin d\n{\u2207jL(w)d+ 1\n2 \u22072jjL(w)d2 + |wj + d|}. (4)\nArmijo rule according to Burke (1985) is adopted to determine the stepsize for the line search procedure. Let q be the line search step index, the stepsize \u03b1 = \u03b1(w,d) is determined as follows,\n\u03b1(w,d) = max q=0,1,2,\u00b7\u00b7\u00b7\n{\u03b2q | Fc(w + \u03b2qd)\u2212 Fc(w) \u2264 \u03b2q\u03c3\u2206}, (5)\nwhere \u03b2 \u2208 (0, 1), \u03c3 \u2208 (0, 1), \u03b2q denotes \u03b2 to the power of q and,\n\u2206 = \u2207L(w)Td+ \u03b3dTHd+ \u2016w + d\u20161 \u2212 \u2016w\u20161, (6)\nwhere H \u2261 diag(\u22072L(w)), \u03b3 \u2208 [0, 1). This rule requires only function evaluations. According to Tseng and Yun (2009), larger stepsize will be accepted if we choose either \u03c3 near 0 or \u03b3 near 1.\nAlgorithm 1 Coordinate Descent Newton (CDN) (Yuan et al., 2010)\n1: initialize w0 = 0n\u00d71. 2: for k = 0, 1, 2, \u00b7 \u00b7 \u00b7 do 3: for all j \u2208 N do 4: compute the descent direction dkj = d(w k,j; j) by solving Eq. (4). 5: find the stepsize \u03b1k,j = \u03b1(wk,j , dkj ej) by solving Eq. (5). //1-dimensional line search 6: wk,j+1 \u2190 wk,j + \u03b1k,jdkj ej . 7: end for 8: end for\n2.2 Shotgun CDN (SCDN) for L1-regularized logistic regression\nFor L1-regularized logistic regression, Bradley et al. (2011) proposed SCDN, which is summarized in Alg. 2. It simply updates P\u0304 features in parallel, where each feature update corresponds to an inner iteration of CDN (see Alg. 1). However, its parallel updates for P\u0304 features might increase the risk of divergence, due to the correlation among features. Bradley et al. (2011) provided a problem-specific measure for SCDN\u2019s potential of parallelization: the spectral radius \u03c1 of XTX. With this measure, a upper bound is given to P\u0304 , i.e., P\u0304 \u2264 n/\u03c1 + 1 to achieve speedups linear in P\u0304 . However, \u03c1 can be very large for most large-scale datasets, e.g. \u03c1 = 20, 228, 800 for dataset gisette with n = 5000, which limits the parallel ability of SCDN. Therefore, algorithms with high parallelism are desired to deal with large-scale problems.\nAlgorithm 2 Shotgun CDN (SCDN) (Bradley et al., 2011)\n1: choose number of parallel updates P\u0304 \u2265 1 2: set w = 0n\u00d71 3: while not converged do 4: In parallel on P\u0304 processors 5: choose j \u2208 N uniformly at radom. 6: obtain dj = d(w; j)ej by solving Eq. (4). 7: find the stepsize \u03b1j = \u03b1(w,dj) by solving Eq. (5). //1-dimensional line search 8: w \u2190 w + \u03b1jdj 9: end while"}, {"heading": "3. Parallel coordinate descent newton (PCDN)", "text": "SCDN places no guarantee on its convergence when the number of features to be updated in parallel is greater than a threshold, i.e., P\u0304 > n/\u03c1+ 1. To exploit higher parallelism, we propose a parallel algorithm based on high dimensional Armijo line search which potentially ensures its convergence. The overall procedure of the proposed algorithm, Parallel Coordinate Descent Newton (PCDN), is summarized in Algorithm 3. Note that, the key difference between PCDN and SCDN is the line search, i.e., PCDN performs P -dimensional\nline search for a bundle of features, while SCDN performs 1-dimensional Armijo line search for each feature.\nIn the k-th outer iteration, PCDN randomly partitions the feature index set N into b subsets in a Gauss-Seidel manner, such that,\nN = disjoint union of {Bkb,Bkb+1, \u00b7 \u00b7 \u00b7 ,B(k+1)b\u22121}, \u2200 k = 0, 1, 2, \u00b7 \u00b7 \u00b7 (7)\nwhere B denotes the subset and is termed as bundle throughout this work; P = |B| is the bundle size; b = \u2308 n\nP \u2309 is the number of bundles partitioned fromN . Then PCDN sequentially\nprocesses each bundle in each inner iteration.\nIn the t-th inner iteration1, PCDN first computes the descent directions dtj (step 8) for P features in Bt in parallel, which constitutes the P -dimensional descent direction dt (dtj = 0,\u2200j 6\u2208 Bt). Then it performs P -dimensional Armijo line search (step 10) to obtain the stepsize \u03b1t of the bundle along dt. At last it updates the features in Bt (step 11).\nAlgorithm 3 Parallel Coordinate Descent Newton (PCDN)\n1: choose the bundle size P \u2208 [1, n]. 2: initialize w0 = 0n\u00d71. 3: for k = 0, 1, 2, \u00b7 \u00b7 \u00b7 do 4: {Bkb,Bkb+1, \u00b7 \u00b7 \u00b7 ,B(k+1)b\u22121} \u2190 random partitions of N according to Eq. (7) 5: for t = kb, kb+ 1, \u00b7 \u00b7 \u00b7 , kb+ b\u2212 1 do 6: dt \u2190 0. 7: for all j \u2208 Bt in parallel do 8: compute the descent direction dtj = d(w\nt; j) by solving Eq. (4). 9: end for\n10: find the stepsize \u03b1t = \u03b1(wt,dt) by solving Eq. (5). //P -dimensional line search 11: wt+1 \u2190 wt + \u03b1tdt. 12: end for 13: end for\nThe P -dimensional line search is the key procedure that guarantees the convergence of our proposed PCDN. With P -dimensional line search, the objective function Fc(w) is ensured to be nonincreasing for any bundle Bt (see Lemma 1(3)). In general, the P -dimensional line search tends to return a large stepsize if the P features in Bt are less correlated, and return a small stepsize if the P features in Bt are highly correlated.\nPCDN can better exploit parallelism than SCDN. In step 8 of Alg. 3, the descent direction for P features can be computed in parallel on P threads. Theorem 3 shows that PCDN has a guarantee of global convergence, for any P \u2208 [1, n]. Therefore, the bundle size P which measures the parallelism can be very large when the number of features n is large. In contrast, for SCDN, P\u0304 which measures the parallelism is no more than n/\u03c1+1 according to Bradley et al. (2011).\nFurthermore, PCDN will take less time for each outer iteration compared to CDN. First, the descent direction computing procedure (step 8 in Alg. 3) can be fully parallelized.\n1. Note that t is the cumulative inner iteration index. When referred to t-th iteration of PCDN, it indicates the inner iteration by default.\nSecond, in each outer iteration, PCDN conducts \u2308 n P \u2309 times of P -dimensional line search (step 10 in Alg. 3) while CDN conducts n times of 1-dimensional line search (step 5 in Alg. 1). However, the time cost of each P -dimensional line search is much less than P times of 1-dimensional line search time cost2. Therefore, the overall time cost of PCDN line search is lower than that of CDN. Thus, PCDN costs less time for each outer iteration compared to CDN, which will be verified by experiments in Section 5.2 and 5.3."}, {"heading": "4. On the convergence of PCDN", "text": "In this section, we will theoretically justify the convergence of the proposed PCDN from three aspects: the convergence of P -dimensional line search, the global convergence, the convergence rate. We enclose all the detailed proofs in Appendix A. Before analyzing the convergence of PCDN, we first give the following lemma.\nLemma 1 Let {wt}, {dt}, {\u03b1t} and {Bt} be sequences generated by Alg. 3; \u03bb\u0304(Bt) be the maximum element of (XTX)jj where j \u2208 Bt; \u03bbk be the k-th minimum element of (XTX)jj where j = 1, \u00b7 \u00b7 \u00b7 , n. Then the following results hold.\n(1) EBt [\u03bb\u0304(Bt)] is monotonically increasing w.r.t P ; EBt[\u03bb\u0304(Bt)] is constant w.r.t P if \u03bbi is constant or \u03bb1 = \u03bb2 = \u00b7 \u00b7 \u00b7 = \u03bbn; EBt [\u03bb\u0304(Bt)]/P is monotonically decreasing w.r.t P .\n(2) For L1-regularized logistic regression in Eq. (2) and L1-regularized L2-loss SVM in Eq. (3), the diagonal elements of the Hessian of the loss function L(w) have positive lower bound h and upper bound h\u0304, and the upper bound only depends on the design matrix X. That is,\n\u22072jjL(w) \u2264 \u03b8c(XTX)jj = \u03b8c s \u2211\ni=1\nx2ij , \u2200 j \u2208 N (8)\n0 < h \u2264 \u22072jjL(w) \u2264 h\u0304 = \u03b8c\u03bb\u0304(N ), \u2200 j \u2208 N (9)\nwhere \u03b8 = 14 for logistic regression and \u03b8 = 2 for L2-loss SVM.\n(3) The objective {Fc(wt)} is nonincreasing and \u2206t (Eq. (6)) in the Armijo line search rule satisfies\n\u2206t \u2264 (\u03b3 \u2212 1)dtTHtdt (10)\nFc(w t + \u03b1tdt)\u2212 Fc(wt) \u2264 \u03c3\u03b1t\u2206t \u2264 0 (11)\n2. First, the time cost of one step of P -dimensional line search (denoted as tls) remains approximately constant with varying bundle size P . That is, tls is about the same as that in 1-dimensional line search in CDN. The reason is that in each line search step, it checks if Fc(w+\u03b1d)\u2212Fc(w) \u2264 \u03b1\u03c3\u2206 is satisfied by objective value evaluation (Eq. (5)), the time cost of which is approximately constant with varying P when implemented properly (see Appendix B for details). Second, the P -dimensional line search in PCDN will terminate within finite steps (from Theorem 2), which is less than P times of line search steps in CDN.\nLemma 1(1) will be used to analyze the iteration number T\u01eb given the expected accuracy \u01eb. Lemma 1(2) will be used to prove Theorem 2, 3 and 4. Lemma 1(3) ensures the descent of the objective theoretically and gives upper bound for \u2206t in the Armijo line search, and will be used to prove Theorem 2 and 4. Note that the upper bound (\u03b3 \u2212 1)dtTHtdt is only related to the second order information.\nTheorem 2 (Convergence of P -dimensional line search) Let {Bt} be sequence generated by Alg. 3; \u03bb\u0304(Bt) = max{(XTX)jj | j \u2208 Bt}. Then the P-dimensional line search will converge in finite steps, and the expected line search step number in each iteration can be bounded as,\nE[qt] \u2264 1 + log\u03b2\u22121 \u03b8c 2h(1\u2212 \u03c3 + \u03c3\u03b3) + 1 2 log\u03b2\u22121 P + log\u03b2\u22121 E[\u03bb\u0304(Bt)] (12)\nwhere the expectation is w.r.t the random choices of Bt; qt is the line search step number in the t-th iteration; \u03b2 \u2208 (0, 1), \u03c3 \u2208 (0, 1) and \u03b3 \u2208 [0, 1) are parameters of the Armijo rule (Eq. (5)); \u03b8 and h (the positive lower bound of \u22072jjL(w)) are in Lemma 1(2).\nBecause E[\u03bb\u0304(Bt)] is monotonically increasing w.r.t P (Lemma 1(1)), Theorem 2 predicts that E[qt] increases along with the bundle size P .\nTheorem 3 (Global convergence) Let {wt} be the sequence genereated by Alg. 3, then every cluster point of {wt} is a stationary point of Fc(w).\nTheorem 3 guarantees that PCDN will converge globally for any bundle size P \u2208 [1, n].\nTheorem 4 (Convergence rate) Let w\u2217 minimizes Eq. (1); {wt},{Bt} and {\u03b1t} be the sequences generated by Alg. 3; \u03bb\u0304(Bt) = max{(XTX)jj | j \u2208 Bt} and wT be the output of Alg. 3 after T + 1 iterations. Then,\nE[Fc(w T )]\u2212 Fc(w\u2217) \u2264 nE[\u03bb\u0304(Bt)] inft \u03b1tP (T + 1) [ \u03b8c 2 \u2016w\u2217\u20162 + \u03b8c supt \u03b1 t 2\u03c3(1 \u2212 \u03b3)hFc(0) ]\nwhere the expectation is w.r.t the random choices of Bt; \u03c3 \u2208 (0, 1) and \u03b3 \u2208 [0, 1) are parameters in the Armijo rule (Eq. (5)); \u03b8 and h (the positive lower bound of \u22072jjL(w)) are in Lemma 1(2); E[\u03bb\u0304(Bt)] is determined by the bundle size P and the design matrix X.\nBased on Theorem 4, we can immediately obtain the upper bound (T up\u01eb ) of the iteration number T\u01eb satisfying a specified accuracy \u01eb.\nT\u01eb \u2264 nE[\u03bb\u0304(Bt)] inft \u03b1tP\u01eb [ \u03b8c 2 \u2016w\u2217\u20162 + \u03b8c supt \u03b1 t 2\u03c3(1 \u2212 \u03b3)hFc(0) ] \u2261 T up\u01eb \u221d E[\u03bb\u0304(Bt)] P\u01eb (13)\nwhich says that PCDN can achieve iteration number speedups linear in the bundle size P compared to CDN if E[\u03bb\u0304(Bt)] keeps constant3. In general, E[\u03bb\u0304(Bt)] increases w.r.t P (Lemma 1(1)), thus making the speedup sublinear. Moreover, since E[\u03bb\u0304(Bt)]/P decreases 3. If perform feature-wise normalization over the training data X to ensure \u03bb1 = \u03bb2 = \u00b7 \u00b7 \u00b7 = \u03bbn, then\nE[\u03bb\u0304(Bt)] keeps constant according to Lemma 1(1).\nw.r.t P from Lemma 1(1), T up\u01eb will decrease w.r.t P , so does T\u01eb. Thus PCDN needs less iterations with larger bundle size P to converge to \u01eb accuracy.\nTo verify the upper bound T up\u01eb (Eq. (13)) of the iteration number T\u01eb for a given accuracy \u01eb, we set \u01eb = 10\u22123 and show the iteration number T\u01eb as a function of the bundle size P in Fig. 1, where two document datasets: a9a and real-sim (see Section 5.1 for details) are tested. Because T up\u01eb \u221d E[\u03bb\u0304(Bt)]/P , we draw E[\u03bb\u0304(Bt)]/P instead of T up\u01eb in Fig. 1. The results match the upper bound in Eq. (13): for given \u01eb, T\u01eb (solid green lines) is positive correlated with E[\u03bb\u0304(Bt)]/P (dotted blue lines). Also, T\u01eb is decreasing w.r.t P . These results verify the conclusion that with larger bundle size P less iterations are needed by PCDN to converge to \u01eb accuracy."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1 Experimental setup", "text": "Datasets Five public real datasets4 are used in our experiments. They are summarized in Table 2. news20, rcv1, a9a and real-sim are document datasets, whose samples are normalized to unit vectors. gisette is a handwriting digit problem from NIPS 2003 feature selection challenge, whose features are linearly scaled to the [-1,1] interval.\nFor each dataset, the optimal bundle size P \u2217 under which PCDN has minimum runtime is set according to the following analysis.\nFor Alg. 3, the expected runtime of each iteration of PCDN can be approximated by,\nE[time(t)] \u2248 (P/#thread) \u00b7 tdc +E[qt] \u00b7 tls (14)\nwhere #thread is the number of threads launched by PCDN, and is set to 23 in our experiments; tdc is the time cost of descent direction computing (step 8 in Alg. 3); tls is the time cost of a step of P -dimensional line search, and is approximately constant with varying P (see Appendix B for details). Because E[qt] will increase w.r.t the bundle size P (Theorem 2), Eq. (14) indicates that E[time(t)] increases w.r.t P . Plus the conclusion that (Section 4) PCDN needs less iterations for larger P to converge to \u01eb accuracy, we need to make a tradeoff between the increasing runtime per iteration E[time(t)] and the decreasing iteration number T\u01eb to select the optimal bundle size P\n\u2217. In practice, we run PCDN with varying P and empirically select the optimal P \u2217 for each dataset, which is summaried in Table 3.\nImplementation We compare PCDN with three state-of-the-art L1-regularized solvers, CDN of Yuan et al. (2010), SCDN of Bradley et al. (2011) and TRON (Trust Region Newton Method) of Yuan et al. (2010). All these solvers are implemented in C/C++ language. For the Armijo line search procedures (Eq. (5)) in PCDN, CDN and SCDN, we set \u03c3 = 0.01, \u03b3 = 0 and \u03b2 = 0.5 to keep fair comparison. OpenMP is used as the parallel programming model. Work in parallel is distributed among a team of threads using OpenMP parallel for construct, and the static scheduling of threads is used because it is proved to be very efficient in our experiments.\nThe stopping condition used in our implementation is similar to the outer stopping condition in Yuan et al. (2011). We run CDN with a strict stopping criteria \u01eb = 10\u22128 to otain the optimal value Fc(w\n\u2217), which is used to compute the relative difference to the optimal function value (relative function value difference),\n[Fc(w)\u2212 Fc(w\u2217)]/Fc(w\u2217) (15)\nSome private implementation details are listed as follows:\n\u2022 CDN: we use the source code included in LIBLINEAR5. The shrinking procedure is modified so that it is consistent with the other parallel algorithms.\n\u2022 SCDN: we set P\u0304 = 8 for SCDN following Bradley et al. (2011). Though Bradley et al released the source code for SCDN, we reimplement it in C language based on CDN implementation in LIBLINEAR for fair comparison.\n5. version 1.7, http://www.csie.ntu.edu.tw/~cjlin/liblinear/oldfiles/\n\u2022 PCDN: we implement PCDN carefully, including the data type and the atomic operation. For atomic operation, we apply a compare-and-swap implementation using inline assembly.\n\u2022 TRON (Trust Region Newton Method (Yuan et al., 2010)): TRON is a L1-regularized L2-loss SVM solver by solving constrained optimization formulation. We set \u03c3 = 0.01 and \u03b2 = 0.1 in the projected line search according to Yuan etal Yuan et al. (2010).\nPlatform All experiments are conducted on a 64-bit machine with Intel(R) Xeon(R) CPU (E5645 @ 2.40GHz) and 64GB main memory. We use GNU C/C++/Fortran compilers (version 4.6.0), and the \u201d-O3\u201d optimization flag is set for each solver. We set #thread = 23 for PCDN on our 24-core machine, which is far less than the optimal bundle size P \u2217 given in Table 3. Consequently, our 24-core machine is unable to fully exhibit its parallelism potential. Note that the descent direction computing (step 8 in Alg. 3) in PCDN can be fully parallelized on several hundreds even to thousand threads. PCDN will run even faster on a machine with more cores or with a GPU implementation.\n5.2 Results for L1-regularized L2-loss SVM\nRuntime comparison results of PCDN, CDN and TRON are shown in Fig. 2. We ran each algorithm on 3 datasets with the best regularization parameter c\u2217 (set according to Yuan et al Yuan et al. (2010)) and varying stopping criteria \u01eb, which is set to be equivalent for the three solvers. As shown in Fig. 2 we can clearly see that PCDN was faster than\nCDN and TRON in most cases. As a feature-based parallel algorithm, the proposed PCDN performs especially well for sparse datasets with more features. This could be verified by the result on rcv1 and news20, which are both very sparse (training data sparsity is 99.85% and 99.97%) datasets with lots of features (47,236 and 1,355,191). In this circumstance, PCDN performed much better than TRON. In the best case on news20, PCDN was 29 times faster than TRON and 18 times faster than CDN. Note that for the dataset a9a, sometimes PCDN was slightly slower than TRON. The reason is that a9a is a relatively dense dataset with fewer features than samples (only 123 features with 26,049 samples).\n5.3 Results for L1-regularized logistic regression\nIn this section, we compare PCDN with SCDN and CDN on 5 datasets (we only put part of results here, see Appendix C for more results). In Fig. 3, we show the timing performance of relative function value difference (see Eq. (15)) and test accuracy in the first row and second row, respectively. To estimate the test accuracy, each dataset is split into one fifth for testing and the rest for training. The results indicate that PCDN is much faster than\nCDN and SCDN, where the best speedup compared to CDN is 17.49, it can be much higher if more threads were used for PCDN. The resutls highlight its high speedup and strong ability to pursue parallelism. Note that, for the dataset gisette shown in Fig. 3(b), SCDN was even slower than CDN. The reason accounting for this result is that SCDN is sensitive to correlation among features which makes the convergence rate lower. Moreover, SCDN does not have convergence guarantee for P\u0304 = 8 on the dataset news20, Fig. 3(c) shows that though SCDN converged faster at the beginning, it could not converge to the final model at limited time with a relatively strict stopping criteria \u01eb = 10\u22127. Meanwhile, in all the cases, PCDN had strong convergence guarantee and fast convergence rate."}, {"heading": "6. Discussion and conclusion", "text": "We in this paper introduced a highly parallelized algorithm, Parallel Coordinate Descent Newton (PCDN), with strong convergence guarantee and fast convergence rate under any possible parallelism, for large-scale L1-regularized minimization. PCDN can be generalized to be a generic parallel framework to solve the problem of minimizing the sum of a convex loss term and a seperable term. In our experiment, the ability of PCDN is still limited by\nthe ordinary multi-core implementation where the number of cores as well as the number of threads are few. To fully utilize the potential of parallelism (i.e., the descent direction computing in step 8 in Algorithm 3), an implementation based on heterogeneous computing frameworks (Gaster et al., 2012) such as GPU and FPGA will be much more powerful."}, {"heading": "Appendix A. Proofs", "text": "In this appendix we prove the following lemmas and theorems from Section 4.\nProof of Lemma 1(1)\nProof\nWe first prove that EBt[\u03bb\u0304(Bt)] is monotonically increasing w.r.t P and EBt[\u03bb\u0304(Bt)] is constant w.r.t P , if \u03bbi is constant or \u03bb1 = \u03bb2 = \u00b7 \u00b7 \u00b7 = \u03bbn.\nLet \u03bbk be the k-th minimum of (X TX)jj, j = 1, \u00b7 \u00b7 \u00b7 , n,\nf(P ) = EBt [\u03bb\u0304(Bt)] = 1\nCPn (\u03bbnC\nP\u22121 n\u22121 +\u03bbn\u22121C P\u22121 n\u22122 + \u00b7 \u00b7 \u00b7+\u03bbkCP\u22121k\u22121 + \u00b7 \u00b7 \u00b7+\u03bbPCP\u22121P\u22121 ), 1 \u2264 P \u2264 n\n(16) where CPn is the combinatorial number. For 1 \u2264 P \u2264 n\u2212 1,\nf(P + 1)\u2212 f(P )\n= \u2212\u03bbP CP\u22121P\u22121 CPn + P+1 \u2211\nk=n\n\u03bbk( CPk\u22121 CP+1n \u2212 CP\u22121k\u22121 CPn )\n= \u2212\u03bbP CP\u22121P\u22121 CPn + P+1 \u2211\nk=n\n\u03bbk (P + 1)k \u2212 P (n+ 1) P (n\u2212 P ) CP\u22121k\u22121 CPn\nset k\u0304 = \u2308 (P+1)k P (n+1)\u2309, then (P +1)k\u2212P (n+1) \u2265 0,\u2200k \u2265 k\u0304 and (P +1)k\u2212P (n+1) \u2264 0,\u2200k < k\u0304. The above equation equals\nf(P + 1)\u2212 f(P )\n=\n\n\nk\u0304 \u2211\nk=n\n\u03bbk (P + 1)k \u2212 P (n+ 1) P (n\u2212 P ) CP\u22121k\u22121 CPn\n\n\u2212\n\n\nP+1 \u2211\nk=k\u0304\n\u03bbk P (n + 1)\u2212 (P + 1)k P (n\u2212 P ) CP\u22121k\u22121 CPn\n \u2212 \u03bbP CP\u22121P\u22121 CPn\n\u2265\n\n\nk\u0304 \u2211\nk=n\n\u03bbk\u0304 (P + 1)k \u2212 P (n+ 1) P (n\u2212 P ) CP\u22121k\u22121 CPn\n\n\u2212\n\n\nP+1 \u2211\nk=k\u0304\n\u03bbk\u0304 P (n + 1)\u2212 (P + 1)k P (n\u2212 P ) CP\u22121k\u22121 CPn\n \u2212 \u03bbk\u0304 CP\u22121P\u22121 CPn\n(17)\n= \u03bbk\u0304\n[\n\u2212 CP\u22121P\u22121 CPn + P+1 \u2211\nk=n\n(P + 1)k \u2212 P (n+ 1) P (n\u2212 P ) CP\u22121k\u22121 CPn\n]\n= \u03bbk\u0304\n[\n\u2212 CP\u22121P\u22121 CPn + P+1 \u2211\nk=n\n( CPk\u22121 CP+1n \u2212 CP\u22121k\u22121 CPn )\n]\n= \u03bbk\u0304\n[\nP+1 \u2211\nk=n\nCPk\u22121 CP+1n \u2212 P \u2211\nk=n\nCP\u22121k\u22121 CPn\n]\n= \u03bbk\u0304[1\u2212 1] = 0\nwhere Eq. (17) comes from \u03bbk \u2265 \u03bbk\u0304,\u2200k \u2265 k\u0304 and \u03bbk \u2264 \u03bbk\u0304,\u2200k < k\u0304. So, f(P + 1) \u2212 f(P ) \u2265 0, for1 \u2264 P \u2264 n\u2212 1, that is, EBt \u03bb\u0304(Bt) is monotonically increasing w.r.t P .\nObviously, from Eq. (16), if \u03bb1 = \u03bb2 = \u00b7 \u00b7 \u00b7 = \u03bbn, EBt [\u03bb\u0304(Bt)] = \u03bb1, which is constant w.r.t P .\nThen we prove that EBt[\u03bb\u0304(Bt)]/P is monotonically decreasing w.r.t P . Let \u03bbk be the k-th minimum of (X TX)jj, j = 1, \u00b7 \u00b7 \u00b7 , n,\ng(P ) = EBt [\u03bb\u0304(Bt)]\nP =\n1\nPCPn (\u03bbnC\nP\u22121 n\u22121 +\u03bbn\u22121C P\u22121 n\u22122 +\u00b7 \u00b7 \u00b7+\u03bbkCP\u22121k\u22121 +\u00b7 \u00b7 \u00b7+\u03bbPCP\u22121P\u22121 ), 1 \u2264 P \u2264 n\nFor 1 \u2264 P \u2264 n\u2212 1,\ng(P + 1)\u2212 g(P )\n= \u2212\u03bbP CP\u22121P\u22121 PCPn + P+1 \u2211\nk=n\n\u03bbk( CPk\u22121\n(P + 1)CP+1n \u2212 CP\u22121k\u22121 PCPn )\n= \u2212\u03bbP CP\u22121P\u22121 PCPn + P+1 \u2211\nk=n\n\u03bbk k \u2212 n n\u2212 P CP\u22121k\u22121 PCPn\n\u2264 \u2212\u03bbP CP\u22121P\u22121 PCPn + P+1 \u2211\nk=n\n\u03bbP k \u2212 n n\u2212 P CP\u22121k\u22121 PCPn\n(18)\n= \u03bbP\n[\n\u2212 CP\u22121P\u22121 PCPn + P+1 \u2211\nk=n\nk \u2212 n n\u2212 P CP\u22121k\u22121 PCPn \u03bbP\n]\n= \u03bbP\n[\nCP\u22121P\u22121 PCPn + P+1 \u2211\nk=n\n( CPk\u22121\n(P + 1)CP+1n \u2212 CP\u22121k\u22121 PCPn )\n]\n= \u03bbP\n[\n1\nP + 1\nP+1 \u2211\nk=n\nCPk\u22121 CP+1n \u2212 1 P P \u2211\nk=n\nCP\u22121k\u22121 CPn\n]\n= \u03bbP\n[\n1 P + 1 \u2212 1 P\n]\n\u2264 0 (19)\nwhere Eq. (18) results from k\u2212n n\u2212P \u2264 0 and \u03bbk \u2265 \u03bbP ,\u2200k = n, \u00b7 \u00b7 \u00b7 , P +1, Eq. (19) comes from \u03bbP \u2265 0 and 1P+1 \u2212 1P < 0. So, g(P + 1)\u2212 g(P ) \u2264 0, for1 \u2264 P \u2264 n\u2212 1, that is, E Bt [\u03bb\u0304(Bt)] P\nis monotonically decreasing w.r.t P .\nProof of Lemma 1(2)\nProof For logistic regression,\n\u22072jjL(w) = c s \u2211\ni=1\n\u03c4(yiw Txi)(1\u2212 \u03c4(yiwTxi))x2ij (20)\nwhere \u03c4(s) \u2261 11+e\u2212s is the derivative of the logistic loss function. Because 0 < \u03c4(s) < 1, one can get that 0 < \u22072jjL(w) \u2264 14c \u2211s i=1 x 2 ij (\u201d=\u201d holds when \u03c4(s) = 1 2), so \u03b8 = 1 4 for logistic regression. Also, because in practice |yiwTxi| < \u221e, so there exist \u03c4\u0304 and \u03c4 such that 0 < \u03c4 \u2264 \u03c4(yiwTxi) \u2264 \u03c4\u0304 < 1, so there exists a h > 0 such that 0 < h \u2264 \u22072jjL(w).\nFor L2-loss SVM,\n\u22072jjL(w) = 2c \u2211 i\u2208I(w) x2ij \u2264 2c\ns \u2211\ni=1\nx2ij (21)\nwhere I(w) = {i | yiwTxi < 1}. So \u03b8 = 2 for L2-loss SVM. To ensure that \u22072jjL(w) > 0, we add a very small positive number \u03bd (\u03bd = 10\u221212 in our program). So, h = \u03bd > 0 for L2-loss SVM.\nProof of Lemma 1(3)\nProof We follow the proof in Tseng and Yun (2009), from Eq. (4) and the convexity of 1-norm, for any \u03b1 \u2208 (0, 1),\n\u2207L(w)Td+ 1 2 dTHd+ \u2016w + d\u20161 \u2264 \u2207L(w)T (\u03b1d) + 1 2 (\u03b1d)TH(\u03b1d) + \u2016w + (\u03b1d)\u20161\n= \u03b1\u2207L(w)Td+ 1 2 \u03b12dTHd+ \u2016\u03b1(w + d) + (1\u2212 \u03b1)w\u20161 \u2264 \u03b1\u2207L(w)Td+ 1 2 \u03b12dTHd+ \u03b1\u2016w + d\u20161 + (1\u2212 \u03b1)\u2016w\u20161\nRearranging terms yields\n(1\u2212 \u03b1)\u2207L(w)Td+ (1\u2212 \u03b1)(\u2016w + d\u20161 \u2212 \u2016w\u20161) \u2264 \u2212 1\n2 (1\u2212 \u03b1)(1 + \u03b1)dTHd\nDividing both sides by 1\u2212 \u03b1 > 0 and taking \u03b1 \u2191 1 yields\n\u2207L(w)Td+ \u2016w + d\u20161 \u2212 \u2016w\u20161 \u2264 \u2212dTHd\nso\n\u2206 = \u2207L(w)Td+ \u03b3dTHd\u2016w + d\u20161 \u2212 \u2016w\u20161 \u2264 (\u03b3 \u2212 1)dTHd\nand from the Armijo rule\nFc(w + \u03b1d)\u2212 Fc(w) \u2264 \u03c3\u03b1\u2206 \u2264 0\nhence {Fc(wt)} is nonincreasing.\nProof of Theorem 2: convergence of P -dimensional line search\nProof\nFirst, we prove that (following Lemma 5(b) in Tseng and Yun (2009)) the descent condition in Eq. (5) Fc(w\nt + \u03b1dt) \u2212 Fc(wt) \u2264 \u03c3\u03b1t\u2206t is satisfied for any \u03c3 \u2208 (0, 1) whenever 0 \u2264 \u03b1t \u2264 min {\n1, 2h(1\u2212\u03c3+\u03c3\u03b3) \u03b8c \u221a P\u03bb\u0304(Bt)\n}\n.\nFor any \u03b1 \u2208 [0, 1],\nFc(w + \u03b1d)\u2212 Fc(w) = L(w + \u03b1d)\u2212 L(w) + \u2016w + \u03b1d\u20161 \u2212 \u2016w\u20161\n= \u03b1\u2207L(w)Td+ \u2016w + \u03b1d\u20161 \u2212 \u2016w\u20161 + \u222b 1\n0 (\u2207L(w + t\u03b1d)\u2212\u2207L(w))T (\u03b1d)dt\n\u2264 \u03b1\u2207L(w)Td+ \u03b1(\u2016w + d\u20161 \u2212 \u2016w\u20161) + \u03b1 \u222b 1\n0 \u2016\u2207L(w + t\u03b1d)\u2212\u2207L(w)\u2016\u2016d\u2016dt (22)\nwhere Eq. (22) is from the convexity of 1-norm and the Cauchy-Schwarz inequality, the 2-norm is only w.r.t j \u2208 Bt because dj = 0,\u2200j 6\u2208 Bt. And\n\u2016\u2207L(w + t\u03b1d)\u2212\u2207L(w)\u2016 \u2264 \u2016\u22072L(w\u0304)\u2016\u2016t\u03b1d\u2016\n= t\u03b1\n\u221a\n\u2211 j\u2208Bt (\u22072jjL(w\u0304))2\u2016d\u2016\n\u2264 t\u03b1 \u221a\nP (\u03b8c\u03bb\u0304(Bt))2\u2016d\u2016 (23) = t\u03b1\u03b8c \u221a P\u03bb\u0304(Bt)\u2016d\u2016\nwhere w\u0304 = t\u2032(w + t\u03b1d) + (1 \u2212 t\u2032)w, 0 \u2264 t\u2032 \u2264 1. Eq. (23) uses Lemma 1(2). Substitute the above inequality into Eq. (22) we get\nFc(w + \u03b1d) \u2212 Fc(w)\n\u2264 \u03b1\u2207L(w)Td+ \u03b1(\u2016w + d\u20161 \u2212 \u2016w\u20161) + \u03b12\u03b8c \u221a P\u03bb\u0304(Bt) \u222b 1\n0 t\u2016d\u20162dt\n= \u03b1(\u2207L(w)Td+ \u2016w + d\u20161 \u2212 \u2016w\u20161) + \u03b12\u03b8c \u221a P\u03bb\u0304(Bt) 2 \u2016d\u20162 = \u03b1(\u2207L(w)Td+ \u03b3dTHd+ \u2016w + d\u20161 \u2212 \u2016w\u20161) + \u03b12\u03b8c \u221a P\u03bb\u0304(Bt) 2 \u2016d\u20162 \u2212 \u03b1\u03b3dTHd (24)\nIf we set \u03b1 \u2264 2h(1\u2212\u03c3+\u03c3\u03b3) \u03b8c \u221a P \u03bb\u0304(Bt) ,then\n\u03b12\u03b8c \u221a P\u03bb\u0304(Bt) 2 \u2016d\u20162 \u2212 \u03b1\u03b3dTHd \u2264 \u03b1(h(1\u2212 \u03c3 + \u03c3\u03b3)\u2016d\u20162 \u2212 \u03b3dTHd) \u2264 \u03b1((1 \u2212 \u03c3 + \u03c3\u03b3)dTHd\u2212 \u03b3dTHd) (25) = \u03b1(1\u2212 \u03c3)(1 \u2212 \u03b3)dTHd \u2264 \u2212\u03b1(1\u2212 \u03c3)\u2206 (26) = \u2212\u03b1(1\u2212 \u03c3)(\u2207L(w)Td+ \u03b3dTHd+ \u2016w + d\u20161 \u2212 \u2016w\u20161)\nwhere Eq. (25) comes from Eq. (9) in Lemma 1(2); Eq. (26) uses Lemma 1(3). The above equation together with Eq. (24) proves that Fc(w+\u03b1d)\u2212Fc(w) \u2264 \u03c3\u03b1\u2206 if \u03b1 \u2264 2h(1\u2212\u03c3+\u03c3\u03b3) \u03b8c \u221a P\u03bb\u0304(Bt) .\nThen, from the Armijo line search procedure, we can see that it tests different values of \u03b1 from larger to smaller, it stops right after finding one value that satisfy Fc(w\nt + \u03b1dt)\u2212 Fc(w t) \u2264 \u03c3\u03b1t\u2206t. So in the t-th iteration, the chosen stepsize \u03b1t will satisfy\n\u03b1t \u2265 2h(1\u2212 \u03c3 + \u03c3\u03b3) \u03b8c \u221a P\u03bb\u0304(Bt)\n(27)\nFrom Eq. (5), \u03b1t = \u03b2q, thus the line search step number of the t-th iteration\nqt = 1 + log\u03b2 \u03b1 t \u2264 1 + log\u03b2\u22121\n\u03b8c \u221a P\u03bb\u0304(Bt)\n2h(1\u2212 \u03c3 + \u03c3\u03b3) (28)\nTake expectation on both sides w.r.t the random choices of Bt\nE[qt] \u2264 1 + log\u03b2\u22121 \u03b8c 2h(1\u2212 \u03c3 + \u03c3\u03b3) + 1 2 log\u03b2\u22121 P +EBt [log\u03b2\u22121 \u03bb\u0304(Bt)]\n\u2264 1 + log\u03b2\u22121 \u03b8c 2h(1\u2212 \u03c3 + \u03c3\u03b3) + 1 2 log\u03b2\u22121 P + log\u03b2\u22121 EBt[\u03bb\u0304(Bt)] (29)\nwhere Eq. (29) comes from Jensen\u2019s inequality for concave function log\u03b2\u22121(\u00b7).\nProof of Theorem 3: global convergence\nProof First, we prove that PCDN is a special case of CGD (see Tseng and Yun (2009)) with H = diag(\u22072L(w)).\nNote that, the selection of bundle Bt in Eq. (7) is consistent with that used in CGD (Eq. (12) in Tseng and Yun (2009)). And for descent direction computing in a bundle in Alg. 3, we have,\ndt = \u2211\nj\u2208Bt d(wt; j)ej\n= \u2211\nj\u2208Bt argmin d {\u2207Lj(wt)Td+\n1 2 \u22072jjL(wt)d2 + |wtj + d|}ej (30)\n= argmin d {\u2207L(wt)Td+ 1 2 dTHd+ \u2016w + d\u20161 | dj = 0,\u2200j 6\u2208 Bt} (31) \u2261 dH(wt;Bt) (32)\nwhere Eq. (30) is derived by considering the definition of d(w; j) in Eq. (4); Eq. (31) is obtained by applying the setting of H \u2261 diag(\u22072L(w)); Eq. (32) is defined by following the descent direction definition of Tseng etal (Eq. (6) in Tseng and Yun (2009)). Therefore the definition of direction computing is a CGD manner. Moreover, since PCDN conducts\nArmijo line search for dt, it is clear that PCDN is a special case of CGD by taking H = diag(\u22072L(w)).\nThen, we can use Theorem 1(e) in Tseng and Yun (2009) to prove the global convergence, which requires that {Bt} is chosen under the Gauss-Seidel rule and supt \u03b1t < \u221e. In Eq. (5), \u03b1t \u2264 1, t = 1, 2, ..., which satisfies supt \u03b1t < \u221e. To ensure global convergence, Tseng etal made the following assumption,\n0 < h \u2264 \u22072jjL(wt) \u2264 h\u0304, \u2200j = 1, \u00b7 \u00b7 \u00b7 , n, t = 0, 1, ...\nwhich is fulfilled by Lemma 1(2). According to Theorem 1(e) in Tseng and Yun (2009), any cluster point of {wt} is a stationary point of Fc(w).\nProof of Theorem 4: convergence rate\nFor analysis of convergence rate, we transform Eq. (1) into an equivalent problem with a twice-differentiable regularizer following Shalev-Shwartz and Tewari (2009). Let w\u0302 \u2208 R2n+ , use duplicated features6 x\u0302i \u2261 [xi;\u2212xi] \u2208 R2n and the problem is\nmin w\u0302\u2208R2n\n+\nFc(w\u0302) \u2261 c s \u2211\ni=1\n\u03d5(w\u0302; x\u0302i, yi) +\n2n \u2211\nj=1\nw\u0302j (33)\nNow, Eq. (4) becomes\nd\u0302(w\u0302; j) \u2261 argmin d\u0302\n{\u2207jL(w\u0302)d\u0302+ 1\n2 \u22072jjL(w\u0302)d\u03022 + w\u0302j + d\u0302} = \u2212(\u2207jL(w\u0302) + 1)/\u22072jjL(w\u0302) (34)\nIn the following proof we omit the \u201d\u2227\u201d above each variables for simplicity. Proof Define the potential function as\n\u03a8(w) = \u03b8c\u03bb\u0304(Bt) 2 \u2016w \u2212w\u2217\u20162 + \u03b8c\u03bb\u0304(B t) supt \u03b1 t 2\u03c3(1 \u2212 \u03b3)h Fc(w) = a\u2016w \u2212w \u2217\u20162 + bFc(w) (35)\n6. Though our analysis uses duplicate features, they are not required for an implementation.\nthen\n\u03a8(w)\u2212\u03a8(w + \u03b1d) = a(\u2016w \u2212w\u2217\u20162 \u2212 \u2016w + \u03b1d\u2212w\u2217\u20162) + b(Fc(w)\u2212 Fc(w + \u03b1d)) = a\u03b1(\u22122wTd+ 2w\u2217Td\u2212 \u03b1dTd) + b(Fc(w)\u2212 Fc(w + \u03b1d)) \u2265 a\u03b1(\u22122wTd+ 2w\u2217Td\u2212 \u03b1dTd) + b\u03c3\u03b1(1 \u2212 \u03b3)dTHd (36) = \u2211\nj\u2208Bt a\u03b1(\u22122wjdj + 2w\u2217jdj \u2212 \u03b1d2j ) + b\u03c3\u03b1(1 \u2212 \u03b3)\u22072jjL(w)d2j\n= \u2211 j\u2208Bt a\u03b1(\u22122wjdj + 2w\u2217jdj) + \u03b1[b\u03c3(1 \u2212 \u03b3)\u22072jjL(w)\u2212 a\u03b1]d2j \u2265 \u2211\nj\u2208Bt a\u03b1(\u22122wj + 2w\u2217j )dj (37)\n= \u2211\nj\u2208Bt\n\u03b8c\u03bb\u0304(Bt)\u03b1 \u22072jjL(w) (wj \u2212 w\u2217j )(\u2207jL(w) + 1) (38)\n\u2265 \u2211\nj\u2208Bt\n\u03bb\u0304(Bt)\u03b1 (XTX)jj (wj \u2212 w\u2217j )(\u2207jL(w) + 1) (39)\n\u2265 \u03b1 \u2211 j\u2208Bt (wj \u2212 w\u2217j )(\u2207jL(w) + 1) (40)\nwhere Eq. (36) uses Lemma 1(3), Eq. (37) uses the fact that b\u03c3(1 \u2212 \u03b3)\u22072jjL(w)\u2212 a\u03b1 \u2265 0. Substitute a = \u03b8c\u03bb\u0304(B\nt) 2 and dj = \u2212(\u2207jL(w) + 1)/\u22072jjL(w) into Eq. (37) we get Eq. (38).\nEq. (39) comes from Lemma 1(2) and Eq. (40) comes from the definition of \u03bb\u0304(Bt). Take the expectation w.r.t the random choices of Bt on both sides\nEBt[\u03a8(w)\u2212\u03a8(w + \u03b1d)] \u2265 inf\nt \u03b1tEBt[\n\u2211 j\u2208Bt (wj \u2212 w\u2217j )(\u2207jL(w) + 1)] (41)\n= inf t \u03b1tPEj\n[ (wj \u2212 w\u2217j )(\u2207jL(w) + 1) ]\n= inf t \u03b1t\nP 2n (w \u2212w\u2217)(\u2207L(w) + 1)\n\u2265 inf t \u03b1t\nP 2n (Fc(w)\u2212 Fc(w\u2217)) (42)\nwhere Eq. (41) results from Eq. (40) and Eq. (42) comes from the convexity of L(w).\nSum over T + 1 iterations, with an expectation over the random choices of Bt\nE[\nT \u2211\nt=0\n\u03a8(wt)\u2212\u03a8(wt+1]\n\u2265 inf t \u03b1t\nP 2n E[\nT \u2211\nt=0\nFc(w t)\u2212 Fc(w\u2217)]\n= inf t \u03b1t\nP 2n [E\nT \u2211\nt=0\n[Fc(w t)]\u2212 (T + 1)Fc(w\u2217)]\n\u2265 inf t \u03b1t\nP (T + 1)\n2n [E[Fc(w\nT )]\u2212 Fc(w\u2217)] (43)\nwhere Eq. (43) comes from Lemma 1(b) that {Fc(wt)} is nonincreasing. Rearranging the above inequality gives\nE[Fc(w T )]\u2212 Fc(w\u2217)\n\u2264 2n inft \u03b1tP (T + 1)\nE[ T \u2211\nt=0\n\u03a8(wt)\u2212\u03a8(wt+1)]\n\u2264 2n inft \u03b1tP (T + 1) E[\u03a8(w0)\u2212\u03a8(wT+1)] \u2264 2n inft \u03b1tP (T + 1) E[\u03a8(w0)] (44) = 2nEBt \u03bb\u0304(Bt)\ninft \u03b1tP (T + 1)\n[\n\u03b8c 2 (\u2016w\u2217\u20162) + \u03b8c supj \u03b1j 2\u03c3(1 \u2212 \u03b3)h (Fc(0) ]\n(45)\nwhere Eq. (44) comes from that \u03a8(wT+1) \u2265 0; Eq. (45) comes from the fact that w0 is set to be 0."}, {"heading": "Appendix B. Details about P -dimensional Armijo line search", "text": "In this section we will show that the time cost of one step of our P -dimensional line search tls remains approximately constant with varying P by presenting the implementation details (in Alg. 4). Take w.l.o.g logistic regression for example, we maintain both dTxi and ew Txi , i = 1, \u00b7 \u00b7 \u00b7 , s and follow the implementation technique of Fan etal (see Appendix G in Fan et al. (2008)). That is, using the following form of sufficient decrease condition:\nf(w+ \u03b2qd)\u2212 f(w)\n= \u2016w+\u03b2qd\u20161\u2212 \u2016w\u20161+ c( s \u2211\ni=1\nlog( e(w+\u03b2 q d)Txi + 1\ne(w+\u03b2 qd)Txi + e\u03b2qdTxi\n) + \u03b2q \u2211\ni:yi=\u22121 dTxi)\n\u2264 \u03c3\u03b2q(\u2207L(w)Td+ \u2016w + d\u20161 \u2212 \u2016w\u20161)\n(46)\nIn each line search step in Alg. 4, it check if Eq. (46) is satisfied, one can see that time cost\nAlgorithm 4 P -Dimensional Armijo Line Search Details of PCDN for Logistic Regression (\u03b3 = 0)\n1: Given \u03b2, \u03c3,\u2207L(w),w,d and ewTxi , i = 1, ..., s. 2: \u2206 \u2190 \u2207L(w)Td+ \u2016w + d\u20161 \u2212 \u2016w\u20161. 3: *Compute dTxi, i = 1, \u00b7 \u00b7 \u00b7 , s. 4: for q = 0, 1, 2, \u00b7 \u00b7 \u00b7 do 5: if Eq. (46) is satisfied then 6: w \u2190 w + \u03b2qd. 7: ew T xi \u2190 ewTxie\u03b2qdTxi , i = 1, \u00b7 \u00b7 \u00b7 , s. 8: break 9: else\n10: \u2206 \u2190 \u03b2\u2206. 11: dTxi \u2190 \u03b2dTxi, i = 1, \u00b7 \u00b7 \u00b7 , s. 12: end if 13: end for\nof which remains constant w.r.t P . The only difference in the whole line search procedure is the computing of dTxi = \u2211P j=1 djxij. However, d\nTxi in PCDN could be computed in parallel with P threads and a reduction-sum operation, so the time cost still remains constant."}, {"heading": "Appendix C. More experimental results for logistic regression", "text": "More experimental results are put in this section. Fig. 4 plots the trace of model nnz (number of nonzero elements in w, the first row) and function value Fc(w) (the second row) w.r.t runtime. Fig. 5 and Fig. 6 is the time performance of logistic regression on dataset a9a and real-sim, respectively."}], "references": [{"title": "Sparse online learning via truncated gradient", "author": ["John Langford", "Lihong Li", "Tong Zhang"], "venue": "OpenCL. Elsevier Science & Technology Books,", "citeRegEx": "Langford et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2012}, {"title": "Coordinate descent optimization for l1 minimization with", "author": ["Yingying Li", "Stanley Osher"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Li and Osher.,? \\Q2009\\E", "shortCiteRegEx": "Li and Osher.", "year": 2009}, {"title": "Hogwild: A lock-free", "author": ["Niu", "Benjamin Recht", "Christopher Re", "Stephen J. Wright"], "venue": null, "citeRegEx": "Niu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2004}, {"title": "A coordinate gradient descent method for nonsmooth", "author": ["Paul Tseng", "Sangwoon Yun"], "venue": null, "citeRegEx": "Tseng and Yun.,? \\Q2009\\E", "shortCiteRegEx": "Tseng and Yun.", "year": 2009}, {"title": "Slow learners are fast", "author": ["Alex Smola", "John Langford"], "venue": "In NIPS,", "citeRegEx": "Zinkevich et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": ", 2010) and compressed sensing (Li and Osher, 2009).", "startOffset": 31, "endOffset": 51}, {"referenceID": 3, "context": "Various optimization methods such as trust region (Lin and Mor\u00e9, 1999), coordinate gradient descent (Tseng and Yun, 2009) and stochastic gradient (Shalev-Shwartz and Tewari, 2009) have been developed to solve L1-regularized 1", "startOffset": 100, "endOffset": 121}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al.", "startOffset": 14, "endOffset": 56}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems.", "startOffset": 14, "endOffset": 81}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richt\u00e1rik and Tak\u00e1c (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem.", "startOffset": 14, "endOffset": 230}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richt\u00e1rik and Tak\u00e1c (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem. Bradley et al. (2011) proposed Shotgun CDN (SCDN) for L1-regularized logistic regression by directly parallelizing the updates of features.", "startOffset": 14, "endOffset": 408}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richt\u00e1rik and Tak\u00e1c (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem. Bradley et al. (2011) proposed Shotgun CDN (SCDN) for L1-regularized logistic regression by directly parallelizing the updates of features. However, SCDN has no guarantee on its convergence when the number of parallelly updated features is greater than a threshold, which limits its parallel ability. To pursue more parallelism, Scherrer et al Scherrer et al. (2012) proposed to preprocess the training data, such as feature clustering, which would introduce extra computation overhead.", "startOffset": 14, "endOffset": 753}, {"referenceID": 0, "context": "Algorithms in Langford et al. (2009); Niu et al. (2011); Zinkevich et al. (2009) performed parallelization over samples, while there are often much more features than samples in L1-regularized problems. Richt\u00e1rik and Tak\u00e1c (2012) show that randomized (block) coordinate descent methods can be accelerated by parallelization, and show significant improvement in solving a Lasso problem. Bradley et al. (2011) proposed Shotgun CDN (SCDN) for L1-regularized logistic regression by directly parallelizing the updates of features. However, SCDN has no guarantee on its convergence when the number of parallelly updated features is greater than a threshold, which limits its parallel ability. To pursue more parallelism, Scherrer et al Scherrer et al. (2012) proposed to preprocess the training data, such as feature clustering, which would introduce extra computation overhead. Is there a parallel coordinate descent algorithm with high parallelism and global convergence guarantee, but without needing data preprocessing? We in this paper present such an algorithm based on CDN of Yuan et al. (2010), termed as Parallel Coordinate Descent Newton (PCDN).", "startOffset": 14, "endOffset": 1096}, {"referenceID": 3, "context": "It is a special case of Coordinate Gradient Descent (CGD) presented in Tseng and Yun (2009), the overall procedure is summarized in Alg.", "startOffset": 71, "endOffset": 92}, {"referenceID": 3, "context": "It is a special case of Coordinate Gradient Descent (CGD) presented in Tseng and Yun (2009), the overall procedure is summarized in Alg. 1. Given the current model w, for the selected feature j \u2208 N , w is updated along the descent direction d = d(w; j)ej , where, d(w; j) = argmin d {\u2207jL(w)d+ 1 2 \u2207jjL(w)d + |wj + d|}. (4) Armijo rule according to Burke (1985) is adopted to determine the stepsize for the line search procedure.", "startOffset": 71, "endOffset": 361}, {"referenceID": 3, "context": "It is a special case of Coordinate Gradient Descent (CGD) presented in Tseng and Yun (2009), the overall procedure is summarized in Alg. 1. Given the current model w, for the selected feature j \u2208 N , w is updated along the descent direction d = d(w; j)ej , where, d(w; j) = argmin d {\u2207jL(w)d+ 1 2 \u2207jjL(w)d + |wj + d|}. (4) Armijo rule according to Burke (1985) is adopted to determine the stepsize for the line search procedure. Let q be the line search step index, the stepsize \u03b1 = \u03b1(w,d) is determined as follows, \u03b1(w,d) = max q=0,1,2,\u00b7\u00b7\u00b7 {\u03b2 | Fc(w + \u03b2d)\u2212 Fc(w) \u2264 \u03b2\u03c3\u2206}, (5) where \u03b2 \u2208 (0, 1), \u03c3 \u2208 (0, 1), \u03b2 denotes \u03b2 to the power of q and, \u2206 = \u2207L(w)d+ \u03b3dHd+ \u2016w + d\u20161 \u2212 \u2016w\u20161, (6) where H \u2261 diag(\u22072L(w)), \u03b3 \u2208 [0, 1). This rule requires only function evaluations. According to Tseng and Yun (2009), larger stepsize will be accepted if we choose either \u03c3 near 0 or \u03b3 near 1.", "startOffset": 71, "endOffset": 796}, {"referenceID": 3, "context": "Proof of Lemma 1(3) Proof We follow the proof in Tseng and Yun (2009), from Eq.", "startOffset": 49, "endOffset": 70}, {"referenceID": 3, "context": "First, we prove that (following Lemma 5(b) in Tseng and Yun (2009)) the descent condition in Eq.", "startOffset": 46, "endOffset": 67}, {"referenceID": 3, "context": "Proof of Theorem 3: global convergence Proof First, we prove that PCDN is a special case of CGD (see Tseng and Yun (2009)) with H = diag(\u22072L(w)).", "startOffset": 101, "endOffset": 122}, {"referenceID": 3, "context": "Proof of Theorem 3: global convergence Proof First, we prove that PCDN is a special case of CGD (see Tseng and Yun (2009)) with H = diag(\u22072L(w)). Note that, the selection of bundle Bt in Eq. (7) is consistent with that used in CGD (Eq. (12) in Tseng and Yun (2009)).", "startOffset": 101, "endOffset": 265}, {"referenceID": 3, "context": "(6) in Tseng and Yun (2009)).", "startOffset": 7, "endOffset": 28}, {"referenceID": 3, "context": "Then, we can use Theorem 1(e) in Tseng and Yun (2009) to prove the global convergence, which requires that {Bt} is chosen under the Gauss-Seidel rule and supt \u03b1 < \u221e.", "startOffset": 33, "endOffset": 54}, {"referenceID": 3, "context": "According to Theorem 1(e) in Tseng and Yun (2009), any cluster point of {wt} is a stationary point of Fc(w).", "startOffset": 29, "endOffset": 50}], "year": 2017, "abstractText": "Parallel coordinate descent algorithms emerge with the growing demand for large-scale optimization. These algorithms are usually limited by their divergence under high parallelism or need data preprocessing to avoid divergence. In this paper, we propose a parallelized algorithm, termed as Parallel Coordinate Descent Newton (PCDN), to pursue more parallelism. It randomly partitions the feature set into b subsets/bundles with size of P , then it sequentially processes each bundle by first computing the descent directions for each feature in the bundle in parallel and then conducting P -dimensional line search to obtain the stepsize of the bundle. We will show that: (1) PCDN is guaranteed to converge globally; (2) PCDN can converge to the specified accuracy \u01eb within the limited iteration number of T\u01eb, and the iteration number T\u01eb decreases along with the increasing of parallelism (bundle size P ). PCDN is applied to large-scale L1-regularized logistic regression and L2-loss SVM. Experimental evaluations over five public datasets indicate that PCDN can better exploit parallelism and outperforms state-of-the-art algorithms in speed, without losing test accuracy.", "creator": "LaTeX with hyperref package"}}}