{"id": "1307.1387", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2013", "title": "Examining the Classification Accuracy of TSVMs with ?Feature Selection in Comparison with the GLAD Algorithm", "abstract": "genetic expression data require actively used properly classify and predict patient diagnostic categories. clinical pharmaceuticals know, it is extremely difficult and expensive who utilize gene product targeting examples. moreover, conventional supervised approaches conduct function properly when labelled comparisons ( training schedules ) are conveyed using support vector machines ( sts ) protocols. therefore, in routine paper, we suggest a support vector processor ( cas ) applied semi - supervised cognitive algorithms, then with both primary samples data within sorted samples to analyze the classification of microarray data. at prune the relevant weights and samples we used discrete feature detector technology called recursive feature elimination ( rfe ), what enables crucial to fit the output protein classification model avoid eliminating diagnostic sensitivity problem. we examined parallel classification initial performance of the sb - cr algorithm as experiments with the lateral variation across diseases ( glad ) problem, before comparison are semi - supervised learning methods. comparing merely two methods, we remarked that parallel tsvm - lc surpassed both established core 1 rfe learning glad.", "histories": [["v1", "Thu, 4 Jul 2013 16:06:25 GMT  (400kb)", "http://arxiv.org/abs/1307.1387v1", "UKCI 2011, the 11th Annual Workshop on Computational Intelligence, Manchester, pp 7-12"]], "COMMENTS": "UKCI 2011, the 11th Annual Workshop on Computational Intelligence, Manchester, pp 7-12", "reviews": [], "SUBJECTS": "cs.LG cs.CE", "authors": ["hala helmi", "jon m garibaldi", "uwe aickelin"], "accepted": false, "id": "1307.1387"}, "pdf": {"name": "1307.1387.pdf", "metadata": {"source": "CRF", "title": "Examining the Classification Accuracy of TSVMs with Feature Selection in Comparison with the GLAD Algorithm", "authors": ["Hala Helmi", "Jonathan M. Garibaldi", "Uwe Aickelin"], "emails": ["@cs.nott.ac.uk"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nData mining techniques have traditionally been used to extract hidden predictive information in many diverse contexts. Usually datasets contain thousands of examples. Recently the growth in biology, medical science, and DNA analysis has led to the accumulation of vast amounts of biomedical data that require in-depth analysis.\nAfter years of research and development, many data mining, machine learning, statistical analysis systems and tools are available to be used in biodata analysis. Consequently, this paper will examine a relatively new technique in data mining. This technique is called Transductive Supervised Support Vector Machines [2], also named Semi-Supervised Support Vector Machines S3VMs, located between supervised learning with fully-labelled training data and unsupervised learning without any labelled training data [1]. In this method, we used both labelled and unlabelled samples for training: a small amount of labelled data and a large amount of unlabelled data.\nHala Helmi (PhD student), Jon Garibaldi (Assoc. Prof.) and Professor Uwe Aickelin are with the Intelligent Modelling and Analysis (IMA) Research Group, School of Computer Science, The University of Nottingham, Jubilee Campus, Wollaton Road, Nottingham, NG8 1BB, U.K. {hqh, jmg, uxa} @cs.nott.ac.uk\nThe purpose of this paper is to observe the performance of Transductive SVMs combined with a feature selection method called Recursive Feature Elimination (RFE), which is used to select molecular descriptors for Transductive Support Vector Machines (TSVMs).\nThe paper is organized as follows: section 2 provides a literature review on Support Vector Machines, Transductive Support Vector Machines and finally Recursive Feature Elimination. In section 3, the TSVM algorithm combined with RFE is detailed, as well as a brief summary of the GLAD algorithm based on a recently published paper [3] which aims to compare the prediction accuracy of these two algorithms. Section 4 is dedicated to comparing and analysing the experimental results of both algorithms (TSVM and GLAD). Finally, a summary of the results and discussion will be presented in section 5.\nII. BACKGROUND"}, {"heading": "A. Support Vector Machines", "text": "Support Vector Machine (SVMs), as a supervised machine learning technique, perform well in several areas of biological research, including evaluating microarray expression data [4], detecting remote protein homologies [5] and recognizing translation initiation sites [6]. SVMs have demonstrated the ability not only to separate the entities correctly into appropriate classes, but also to identify instances where established classification is not supported by the data [7]. SVMs are a technique that makes use of training that utilizes samples to determine beforehand which data should be clustered together [4]."}, {"heading": "B. Tranductive Support Vector Machines", "text": "Transductive learning is a method strongly connected to semi-supervised learning, where semi-supervised learning is intermediate between supervised and unsupervised learning. Vapnik introduced Semi-Supervised Learning for Support Vector Machines in the 1990s. His view was that transduction (TSVM) is preferable to induction (SVM), since induction needs to solve a more general problem (inferring a function) before solving a more detailed one (computing outputs for new cases) [8] [9].\nTransductive Support Vector Machines attempt to maximize the hyperplane classifier between two classes using labelled training data; at the same time this forces the hyperplane to be far away from the unlabelled samples. TSVMs seem to be a perfect semi-supervised learning algorithm because they combine the regularization of Support Vector Machines with a straight forward implementation of the clustering assumption [10]."}, {"heading": "C. Recursive Feature Elimination", "text": "Most prediction model algorithms are less effective when the size of the data set is large. There are several methods for decreasing the amount of the feature set. From among these methods we selected a technique called Recursive Feature Elimination (RFE). The basis for RFE is to begin with all the features, select the least useful, remove this feature, and then repeat until some stopping condition is reached.\nFinding the best subset features is too expensive, so RFE decreases the difficulty of feature selection by being \u2018greedy\u2019 [11].\nIII. METHODS\nThis section of the paper is focused on describing TSVMRFE, the problem motivated by the task of classifying biomedical data. The goal is to examine classifier accuracy and classification errors using the Transductive Support Vector Machines method, in order to determine whether this method is an effective model when combined with Recursive Feature Elimination (RFE) compared with the Genetic Learning Across Datasets (GLAD) algorithm."}, {"heading": "A. Support Vector Machines", "text": "The purpose of SVMs is to locate a classifier with the greatest margin between the samples relating to two different classes, where the training error is minimized. Therefore, to achieve this we used a set of -\u074adimensional training samples  = {\u0754}\u0b40\u0b35  labelled {\u08f3}\u0b40\u0b35  and their mapping { (\u0754)}\u0b40\u0b35\n via kernel function:\n\u0713\u0d6b\u0754,\u0754\u0d6f= (\u0908 )\u08fb \u0bcd \u0d6b\u0754\u0d6f\n\u2032\nSVM has the following primal form:\nMinimize over (\u071f , ,\u073e \u03be \u0b35 , \u2026 , \u03be  )\n\u2016\u071f\u2016  + \u0725  \u03be \n\n\u0b40\u0b35\nSubject to: \u2200\u0b40\u0b35  : \u08f3 (\u071f  (\u0722\u2217 \u0908 )\u08fb\u2032 + \u073e ) \u2265 1 \u2212 \u03be  , \u03be  \u2265 0 (1)\nThe SVM predictor for samples x, as shown below, was settled on by the vector inner product between the \u071f and the mapped vector (x), plus the constant .\u073e\n\u08f3 = \u071b\u0716 (\u071f  ()\u2032 + \u073e) The predictor actually corresponds to a separating hyperplane in the mapped feature space. The prediction for each training samples x\u0b67 is connected with a violation term \u03be \u0b67 . The C is a user-specified constant to manage the penalty for these violation terms.\nThe parameter p in the above (1) points to which kind of norm of \u071f is assessed. It is usually set to 1 or 2, resulting in the 1-norm (l\u0b35-SVM) and 2-norm SVM (l\u0b36-SVM) respectively. The 1-norm and 2-norm TSVMs have been discussed in [12] and [13]."}, {"heading": "B. Transductive Support Vector Machines", "text": "In this paper, we are using the extended SVM technique of transductive SVMs and we methodically adept the 2- norm for the TSVM.\nThe standard setting can be illustrated as:\nMinimize over (\u08f3\u0b35 \u2217, \u2026 . ,\u08f3\u0b69 \u2217, w, b, \u03be \u0b35 , \u2026 , \u03be \u0b6b , \u03be \u0b35 \u2217)\n1 2 \u2016w\u2016\u0b36 \u0b36 + C  \u03be \u0b67\n\u0b6b\n\u0b67\u0b40\u0b35\n+ C\u2217  \u03be \u0b68 \u2217\n\u0b69\n\u0b68\u0b40\u0b35\nsubject to: \u2200\u0b67\u0b40\u0b35 \u0b6b : \u08f3 (w  (z \u2217 \u0908 )\u08fb\u2032 + b) \u2265 1 \u2212 \u03be \u0b67 , \u03be \u0b67 \u2265 0\n\u2200\u0b68\u0b40\u0b35 \u0b69 : \u08f3\u0c60 \u2217 \u1240w \u0d6bz \u2217 x\u0b68 \u2217\u0d6f\n\u2032 + b\u1241 \u2265 1 \u2212 \u03be \u0b68 \u2217, \u03be \u0b68 \u2217 \u2265 0 (2)\nWhere each \u08f3\u0b68 \u2217 is the unknown label for x\u0b68 \u2217 which is one of the k unlabelled samples; compared with SVM (1), the formulation (2) of the TSVMs takes the unlabelled data into consideration by representing the violation terms \u03be\n\u0b68 \u2217 caused\nby forecasting each unlabelled pattern (x\u0b68 \u2217) into \u08f3\u0b68 \u2217. The penalty for these violation terms is controlled by a new constant C\u2217 labelled with unlabelled samples, while C consists of labelled samples only.\nPrecisely solving the transductive problem needs a search of all potential assignments of \u08f3\u0b35 \u2217, \u2026 . ,\u08f3\u0b69 \u2217 and identifying the various terms of \u03be\u2217 which are regularly intractable for large data sets. It is worth mentioning the l\u0b36TSVM implemented in the SVMLight [18] [8]."}, {"heading": "C. Recursive Feature Elimination", "text": "Recursive Feature Elimination (RFE) has the advantage of decreasing the number of redundant and recursive features. RFE decreases the difficulty of feature selection by being greedy.\nTo extend SVM feature selection techniques to transductive feature selection is specifically straightforward, as we can produce TSVM-RFE by iteratively eliminating features with weights calculated from TSVM models. We can explain the TSVM-RFE approach as the following standard process. 1. Pre-process data and calculate filtering scores \u071b\u0305.\nMoreover, optionally further normalize data. This\napproach first filters some features based on scores like\nPearson correlation coefficients.\n2. Adjust \u0722as an all-one input vector. 3. Set \u0722\u2190 \u0722 \u2217 \u071b\u0305. Set part of the small entries of \u0722 zero\naccording to a proportion/threshold, and probably discrete non-zero \u0722 to 1. 4. Obtain a (sub-) optimal TSVM as calculated by crossvalidation accuracy (2). 5. For RFE approaches, estimate feature weights \u071b\u0305from the model in step 4 according to:\nJ\u0b72 \u2032 = \u2212\n\u0b35 \u0b36 \u2211 \u03b1\u0b67\u03b1\u0b68\u08f3 \u0b67\u08f3 \u0b68 (x\u0b67/ \u0b6b \u0b67,\u0b68 x\u0b67\u0b72)  (x\u0b68, x\u0b68\u0b72) \u2032 + \u2211 \u03b1\u0b67 (3) \u0b6c \u0b67\u0b40\u0b35\nWhere (x\u0b67/x\u0b67\u0b72) indicates the input samples \u0745 with feature \u0750 removed. The weight of the \u0750-th feature can be clarified as\ns\u0b72= \u0da5 |\u2206 J\u0b72| = \u0da7\u0e2bJ \u2212 J\u0b72 \u2032\u0e2b\nThe following estimation suggested in [11] is easier to measure.\ns\u0b72 \u0b36 \u2248  \u03b1\u0b67\u03b1\u0b68\u08f3 \u0b67\u08f3 \u0b68 (\n\u0b6b\n\u0b67,\u0b68\u0b40\u0b35\nx\u0b67\u0b72)  (x\u0b68\u0b72) \u2032\nSpecifically, the feature weights are identical to the \u071f if the SVM is built upon a linear kernel.\nReturn to step 3 unless there is an acceptable number of features/iterations. Output the closing predictor and features highlighted by large values of z.\nStep 3 comprises selecting a proportion/number of features according to a threshold cutting the vector z. For filtering scores and the RFE method, the vector z is changed to a binary vector. Then the z \u2217 x has the effect of pruning or deactivating some features.\nThe threshold is usually found to prune a (fixed) number/proportion of features at each iteration. The value of the remaining features is then measured by the optimality of the TSVM model obtained in step 4. We then apply crossvalidation accuracy as the performance measure for the TSVM algorithm. For a subset of features selected by choosing a threshold value, we extend the model search upon the free parameters, such as [C, C\u2217\u03c3 (RBF), d(Poly)] and choose the preferable parameter set which results in the highest cross-validation accuracy."}, {"heading": "D. Genetic Learning Across Datasets (GLAD)", "text": "The GLAD algorithm is different from prior algorithms of semi-supervised learning. The GLAD algorithm has been applied as a wrapper method for feature selection. A Genetic Algorithm (GA) was implemented for generating a population of related feature subsets. The labelled data and the unlabelled data samples were computed separately. Linear Discriminant Analysis (LDA) and K-means (K = 2) for these two data forms of cluster algorithms were used [14]. A distinctive two-term scoring function resulted to independently score the labelled and unlabelled data samples. Generally, the score was calculated as a weighted average of the two terms as shown below.\nscore = w \u00d7 score\u0b6a\u0b5f\u0b60 \u0b6a\u0b63\u0b6a\u0b63 \u0b62 + (1 \u2212 w) \u00d7 score\u0b73\u0b6c\u0b6a\u0b5f\u0b60 \u0b6a\u0b63\u0b6a\u0b63 \u0b62 (4)\nAs the typical leave-one-out-cross-validation accuracy for the labelled training samples, they identified the labelled data samples score. The unlabelled data samples score consists of two terms: a cluster separation term and a steady ratio term.\n\u074f\u073f \u074e\u0741 \u0be8\u0bd7 = \u2211 \u0e2b\u0725\u2212 \u0725\u0e2b\u0bb7\n\u2211 \u0e2b\u0725\u2212 \u0725\u0e2b+ \u2211 1 \u0730 \u2211 \u0e2bx\u2212 \u0725\u0e2b\u0bb7\n\u2212\n\u0da8 1\n\u074a\n (\u07e8 \u2212 \u07e8 \u0b76\u0b63\u0b6e) \u0b36\n\n(5)\nC\u0b67 = centroid of cluster; \u03c0\u0b67 = ratio of data in cluster i ; \u03c0 \u0b76\u0b63\u0b6e= expected ratio in cluster i ; N\u0b47 = number of data samples in cluster i; n\u0b61 = number of clusters.\nIV. EXPERIMENTS AND RESULTS\nThis section discusses the results of the experiments which were carried out in order to assess the effectiveness of the classification model accuracy proposed in the previous section."}, {"heading": "A. Datasets", "text": " Leukaemia (AML-ALL): including 7129 probes, two variants of leukaemia are available: acute myeloblastic leukaemia (AML), 25 samples; and acute lymphoblastic leukaemia (ALL), 47 samples [15].\n Lymphoma (DLBCL): consisting of 7129 genes and 58 DLBCL samples. Diffuse large B-cell lymphoma (DLBCL) and 19 samples of follicular lymphoma (FL) [16].\n Chronic Myeloid Leukaemia (CML): contained 30 samples (18 severe emphysema, 12 mild or no emphysema) with 22,283 human gene probe sets [17]."}, {"heading": "B. TSVM Recursive Feature Elimination (TSVM-RFE) Result", "text": " Leukaemia (AML-ALL): the results for the Leukaemia ALL/AML dataset are summarized in Figure 1 in the diagram on the left. TSVM-RFE gave the smallest minimal error of 3.68%, and compassionately smaller errors compared with SVM-RFE: 3.97% for 30, 40, ..., 70 genes. Interestingly, in our experiments both methods gave the lowest error when 60 genes were used. This provided a reasonable suggestion for the number of relevant genes that should be used for the leukaemia data.  Lymphoma (DLBCL): the results for the Lymphoma (DLBCL) dataset are summarized in Figure 1 in the middle diagram. TSVM-RFE gave the smallest minimal error of 3.89%, and quite firmly smaller errors compared to the SVM-RFE: 4.72% for 30, 40, ..., 70 genes. For TSVM, the methods gave the lowest error for 60 genes, while SVM methods gave the lowest error at 50 genes with 4.72% compared to 4.97% for 60 genes. This could give a sensible suggestion for the number of relevant genes that should be used for the lymphoma (DLBCL) data.\n Leukaemia (CML): lastly, the TSVM-RFE and SVM-RFE results for the Leukaemia (CML) dataset are provided in Figure 1 in the diagram on the right. TSVM-RFE gave the smallest minimal error of 6.52%, and critically smaller errors in contrast to the 7.85% SVM-RFE for 30, 40, ..., 70 genes. Both algorithms showed the lowest error when 50 genes were used. This presented a sensible proposal for the number of related genes that should be used for the Leukaemia (CML) data."}, {"heading": "C. Comparing the TSVM Algorithm result with the GLAD Algorithm", "text": "Implementing Genetic Learning Across Datasets involved conducting three experiments using previous datasets, each addressing a different cancer diagnostic problem: ALL/AML for disparity in diagnosis; in CML a dataset predicting the response of imatinib; and in DLBCL for forecasting outcome.\nIn the AML-ALL dataset, the accuracy range using only labelled samples was 73.46%. Combining unlabelled samples with labelled samples increased the range to 75.14%. Adding unlabelled samples increased the accuracy from 59.34% to 65.57% in the CML experiments. The addition of the unlabelled samples to the unlabelled samples for DLBCL raised the accuracy from 49.67% to 55.79%. This shows that the GLAD algorithm outperformed the SVM-RFE and TSVMRFE in some cases when we made use of the labelled data only without gene selection. In Table 1, for example the AML-ALL dataset, the GLAD algorithm gives 73.46% while SVM-RFE and TSVM-RFE accuracy are 52.8% and 55.6% respectively.\nHowever, in the second dataset DLBCL showed that GLAD algorithm accuracy was 49.67% and SVM-RFE 55.8%. Furthermore, the third dataset of CML, SVM-RFE gave 59.02% without gene selection, while GLAD gave 59.34%. On the other hand, TSVM exceeded GLAD when making use of unlabelled data along with labelled data and selecting genes. The results are shown in Table 1.\nFor instance, for the CML dataset using all the samples without gene selection TSVM gave 72.6% when selecting genes based on REF, TSVM exceeded 93.48%, while GLAD gave 65.57% with gene selection. In the same vein, the accuracy for the DLBCL dataset achieved 96.11% by TSVM with gene selection.\nOn the other hand, the GLAD algorithm gave 55.79% with gene selection. As well as this, the TSVM with the AML-ALL dataset with gene selection gave 96.32% while the GLAD algorithm gave 75.14%. This means that the TSVM performed better than the GLAD algorithm, and the performance with gene selection showed a superior result.\nThis paper has investigated topics focused on semisupervised learning. This was achieved by comparing two different methods for semi-supervised learning using previously classified cancer datasets.\nThe results on average for semi-supervised learning surpassed those for supervised learning. However, this shows that the GLAD algorithm outperformed SVM-RFE when we made use of the labelled data only. On the other hand, TSVMRFE exceeded GLAD when unlabelled data along with labelled data were used; it performed much better with gene selection and performed well even if the labelled dataset was small.\nOn the other hand, TSVM still had some drawbacks when increasing the size of the labelled dataset, as the performance did not significantly improve accordingly. Moreover, when the size of the unlabelled samples was extremely small, the time complexity was correspondingly high.\nAs with almost all semi-supervised learning algorithms, TSVM showed some instability, as some results of different runs were not the same. This occurred because unlabelled samples may have been wrongly labelled during the learning process. If we find a way in future to select and eliminate the unlabelled sample first, we can then limit the number of newlylabelled samples for re-training the classifiers.\nREFERENCES\n[1] Abney, S. \"Semisupervised Learning for Computational Linguistics\" (1st ed.). Chapman & Hall/CRC, 2007.\n[2] Zhu, X., \"Semi-Supervised Learning Tutorial\". Department of Computer Sciences University of Wisconsin, Madison, USA, 2007.\n[3] Harris, C. and Ghaffari, N. \"Biomarker discovery across annotated and unannotated microarray datasets using semi-supervised learning\". BMC Genomics 9 Suppl 2:S7, 2008.\n[4] Brown, M.P., Grundy, W.N., Lin, D., Cristianini, N., Sugnet, C.W., Furey, T.S., Ares, M. Jr., Haussler, D. \"Knowledge-based analysis of microarray gene expression data by using support vector machines\". Proc. Natl. Acad. Sci. USA 97:262\u2013267. 2001.\n[5] Jaakkola, T., Haussler, D. and Diekhans, M. \"Using the Fisher kernel method to detect remote protein homologies\". In Proceedings of ISMB, 1999.\n[6] Zien, A., Ratsch, G., Mika, S., Scholkopf, B., Lemmen, C., Smola, A., Lengauer, T., and Muller, K.-R. \"Engineering support vector machine kernels that recognize translation initiation sites\", Bioinformatics, 16:799- 807, 2000.\n[7] Valafar, F. \"Pattern Recognition Techniques in Microarray Data Analysis\". Annals of the New York Academy of Sciences 980(1): 41-64, 2002.\n[8] Gommerman, A., Vovk, V. and Vapnik, V. \"Learning by Transduction\", In Uncertainty in Artificial Intelligence, pp.148-155,1998.\n[9] Collobert, R., Sinz, F., Weston, J. and Bottou, L. \"Large Scale Transductive SVMs\". J. Mach. Learn. Res. 7 (December 2006), 1687-1712, 2006.\n[10] Zhang, R., Wang, W. et al. \"Least Square Transduction Support Vector Machine.\" Neural Processing Letters 29(2): 133-142, 2009.\n[11] Guyon, I., Weston, J. et al. \"Gene Selection for Cancer Classification using Support Vector Machines Machine Learning\". 46(1): 389-422, 2002.\n[12] Bennett, K. and Demiriz, A. \"Semi-supervised support vector machines\". In NIPS,1998.\n[13] Joachims, T. \"Transductive inference for text classification using support vector machines\". In Proceedings of ICML99, pages 200-209, 1999.\n[14] Han, J. and Kamber, M. \"Data Mining: Concepts and Techniuqes\". Morgan Kaufmann Publishers, San Francisco, CA, 2001.\n[15] Golub, T.R. et al. \u201cMolecular classification of cancer: class discovery and class prediction by gene expression monitoring\u201d. Science, 286:531-537, 1999.\n[16] Shipp, M.A. et al. \"Diffuse large B-cell lymphoma outcome prediction by gene expression profiling and supervised machine learning\". Nature Medicine, 8:68-74. 2002.\n[17] Yong, A.S.M. et al. \"Molecular profiling of CD34+ cells identifies low expression of CD7, along with high expression of proteinase 3 or elastase, as predictors of longer survival in patients with CML\". Blood 2006, 107:205-12, 2006.\n[18] Joachims, T. \"Making large-scale support vector machine learning practical\". In Advances in Kernel Methods: Support Vector Machines, 1999."}], "references": [{"title": "Semisupervised Learning for Computational Linguistics", "author": ["S. Abney"], "venue": "(1st ed.). Chapman & Hall/CRC,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Semi-Supervised Learning Tutorial", "author": ["X. Zhu"], "venue": "Department of Computer Sciences University of Wisconsin, Madison, USA,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Biomarker discovery across annotated and unannotated microarray datasets using semi-supervised learning", "author": ["C. Harris", "N. Ghaffari"], "venue": "BMC Genomics 9 Suppl 2:S7,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Knowledge-based analysis of microarray gene expression data by using support vector machines", "author": ["M.P. Brown", "W.N. Grundy", "D. Lin", "N. Cristianini", "C.W. Sugnet", "T.S. Furey", "Ares", "M. Jr.", "D. Haussler"], "venue": "Proc. Natl. Acad. Sci. USA", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Using the Fisher kernel method to detect remote protein homologies", "author": ["T. Jaakkola", "D. Haussler", "M. Diekhans"], "venue": "In Proceedings of ISMB,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Engineering support vector machine kernels that recognize translation initiation", "author": ["A. Zien", "G. Ratsch", "S. Mika", "B. Scholkopf", "C. Lemmen", "A. Smola", "T. Lengauer", "Muller", "K.-R"], "venue": "sites\", Bioinformatics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Pattern Recognition Techniques in Microarray Data Analysis", "author": ["F. Valafar"], "venue": "Annals of the New York Academy of Sciences", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Learning by Transduction", "author": ["A. Gommerman", "V. Vovk", "V. Vapnik"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Large Scale Transductive SVMs", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "J. Mach. Learn. Res", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Least Square Transduction Support Vector Machine.", "author": ["R. Zhang", "W Wang"], "venue": "Neural Processing Letters", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Gene Selection for Cancer Classification using Support Vector Machines Machine Learning", "author": ["I. Guyon", "J Weston"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Semi-supervised support vector machines", "author": ["K. Bennett", "A. Demiriz"], "venue": "In NIPS,1998", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "In Proceedings of ICML-", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Data Mining: Concepts and Techniuqes", "author": ["J. Han", "M. Kamber"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Molecular classification of cancer: class discovery and class prediction by gene expression monitoring", "author": ["Golub", "T.R"], "venue": "Science, 286:531-537,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Diffuse large B-cell lymphoma outcome prediction by gene expression profiling and supervised machine learning", "author": ["Shipp", "M.A"], "venue": "Nature Medicine,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Molecular profiling of CD34+ cells identifies low expression of CD7, along with high expression of proteinase 3 or elastase, as predictors of longer survival in patients with CML", "author": ["Yong", "A.S.M"], "venue": "Blood 2006,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Making large-scale support vector machine learning practical", "author": ["T. Joachims"], "venue": "In Advances in Kernel Methods: Support Vector Machines,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}], "referenceMentions": [{"referenceID": 1, "context": "This technique is called Transductive Supervised Support Vector Machines [2], also named Semi-Supervised Support Vector Machines SVMs, located between supervised learning with fully-labelled training data and unsupervised learning without any labelled training data [1].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "This technique is called Transductive Supervised Support Vector Machines [2], also named Semi-Supervised Support Vector Machines SVMs, located between supervised learning with fully-labelled training data and unsupervised learning without any labelled training data [1].", "startOffset": 266, "endOffset": 269}, {"referenceID": 2, "context": "In section 3, the TSVM algorithm combined with RFE is detailed, as well as a brief summary of the GLAD algorithm based on a recently published paper [3] which aims to compare the prediction accuracy of these two algorithms.", "startOffset": 149, "endOffset": 152}, {"referenceID": 3, "context": "Support Vector Machine (SVMs), as a supervised machine learning technique, perform well in several areas of biological research, including evaluating microarray expression data [4], detecting remote protein homologies [5] and recognizing translation initiation sites [6].", "startOffset": 177, "endOffset": 180}, {"referenceID": 4, "context": "Support Vector Machine (SVMs), as a supervised machine learning technique, perform well in several areas of biological research, including evaluating microarray expression data [4], detecting remote protein homologies [5] and recognizing translation initiation sites [6].", "startOffset": 218, "endOffset": 221}, {"referenceID": 5, "context": "Support Vector Machine (SVMs), as a supervised machine learning technique, perform well in several areas of biological research, including evaluating microarray expression data [4], detecting remote protein homologies [5] and recognizing translation initiation sites [6].", "startOffset": 267, "endOffset": 270}, {"referenceID": 6, "context": "SVMs have demonstrated the ability not only to separate the entities correctly into appropriate classes, but also to identify instances where established classification is not supported by the data [7].", "startOffset": 198, "endOffset": 201}, {"referenceID": 3, "context": "SVMs are a technique that makes use of training that utilizes samples to determine beforehand which data should be clustered together [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 7, "context": "His view was that transduction (TSVM) is preferable to induction (SVM), since induction needs to solve a more general problem (inferring a function) before solving a more detailed one (computing outputs for new cases) [8] [9].", "startOffset": 218, "endOffset": 221}, {"referenceID": 8, "context": "His view was that transduction (TSVM) is preferable to induction (SVM), since induction needs to solve a more general problem (inferring a function) before solving a more detailed one (computing outputs for new cases) [8] [9].", "startOffset": 222, "endOffset": 225}, {"referenceID": 9, "context": "TSVMs seem to be a perfect semi-supervised learning algorithm because they combine the regularization of Support Vector Machines with a straight forward implementation of the clustering assumption [10].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "Finding the best subset features is too expensive, so RFE decreases the difficulty of feature selection by being \u2018greedy\u2019 [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 11, "context": "The 1-norm and 2-norm TSVMs have been discussed in [12] and [13].", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "The 1-norm and 2-norm TSVMs have been discussed in [12] and [13].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "It is worth mentioning the l\u0b36TSVM implemented in the SVMLight [18] [8].", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "It is worth mentioning the l\u0b36TSVM implemented in the SVMLight [18] [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 10, "context": "The following estimation suggested in [11] is easier to measure.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "Linear Discriminant Analysis (LDA) and K-means (K = 2) for these two data forms of cluster algorithms were used [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 14, "context": "\uf0b7 Leukaemia (AML-ALL): including 7129 probes, two variants of leukaemia are available: acute myeloblastic leukaemia (AML), 25 samples; and acute lymphoblastic leukaemia (ALL), 47 samples [15].", "startOffset": 187, "endOffset": 191}, {"referenceID": 15, "context": "Diffuse large B-cell lymphoma (DLBCL) and 19 samples of follicular lymphoma (FL) [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "\uf0b7 Chronic Myeloid Leukaemia (CML): contained 30 samples (18 severe emphysema, 12 mild or no emphysema) with 22,283 human gene probe sets [17].", "startOffset": 137, "endOffset": 141}], "year": 2013, "abstractText": "Gene expression data sets are used to classify and predict patient diagnostic categories. As we know, it is extremely difficult and expensive to obtain gene expression labelled examples. Moreover, conventional supervised approaches cannot function properly when labelled data (training examples) are insufficient using Support Vector Machines (SVM) algorithms. Therefore, in this paper, we suggest Transductive Support Vector Machines (TSVMs) as semi-supervised learning algorithms, learning with both labelled samples data and unlabelled samples to perform the classification of microarray data. To prune the superfluous genes and samples we used a feature selection method called Recursive Feature Elimination (RFE), which is supposed to enhance the output of classification and avoid the local optimization problem. We examined the classification prediction accuracy of the TSVM-RFE algorithm in comparison with the Genetic Learning Across Datasets (GLAD) algorithm, as both are semi-supervised learning methods. Comparing these two methods, we found that the TSVM-RFE surpassed both a SVM using RFE and GLAD.", "creator": "Microsoft Word - UKCI-Hala Helmi.docx"}}}