{"id": "1307.3687", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jul-2013", "title": "On Analyzing Estimation Errors due to Constrained Connections in Online Review Systems", "abstract": "constrained connection is the phenomenon \" online reviewer can only review a target like products / services due less narrow spaces of interests or limited response costs. per analytical work, groups study how crowd connections can affect estimation performance in online measuring systems ( ors ). we find arbitrary reviewers'constrained calls theoretically receive poor estimation performance, both from the expense of estimation expectations and internal variance rao sampling bound.", "histories": [["v1", "Sun, 14 Jul 2013 01:37:48 GMT  (60kb)", "http://arxiv.org/abs/1307.3687v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["junzhou zhao"], "accepted": false, "id": "1307.3687"}, "pdf": {"name": "1307.3687.pdf", "metadata": {"source": "CRF", "title": "On Analyzing Estimation Errors due to Constrained Connections in Online Review Systems", "authors": ["Junzhou Zhao"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n30 7.\n36 87\nv1 [\ncs .S\nI] 1\n4 Ju\nl 2 01\n3\nI. INTRODUCTION\nOnline reviews are more and more important factors for customers to decide whether to buy a product or service in online markets. Due to this reason, online review systems have become battle fields for companies to compete with each other by hiring \u201cInternet Water Mercenaries\u201d, which are also known as paid spammers, to post favorable reviews about their products/services and negative reviews about their competitors\u2019. These fake reviews disturb customers\u2019 judgments on the quality of products/services and ruin companies\u2019 reputation. Hence, an always important problem in online review systems is how to accurately obtain the truth of both reviewers (e.g., the reviewer is a spammer or non-spammer) and items (e.g., the product/service is good or bad) according to unreliable online reviews.\nIn previous studies[10], [1], most of the works ignore the function of the underlying topology of ORS. The topology of an online review system is a bipartite graph representing which reviewers can review which items. Many works explicitly or implicitly assume that reviewers can review all the other items, such as the example shown in Fig. 1(a). In fact, a reviewer can only review a subset of items in real-world, which results in constrained connections for each reviewer in the topology. The constrained connections may be because of either the reviewer\u2019s narrow range of interests or the reviewer\u2019s limited attention capacity (that he cannot afford to review all other items). The constrained connections can affect the performance of jointly estimating the truth of reviewers and items. For example, let us consider a simplest online review system that consists of three reviewers and one item. If we assume the majority of reviewers are non-spammers (that is true in real-world), then in the case of Fig. 1(b), from this topology and reviews by each reviewer we can infer with high confidence that the item is probably good and the bottom reviewer is likely to be a spammer. However, in the case of (c), we cannot obtain a high confidence conclusion because we do not know the reviews of the top reviewer.\nThe simple example tells us that different topologies of\nORS along with unreliable reviews contain different amounts of information for jointly estimating the truth of reviewers and items. Actually, connections between reviewers and items act as constraints in such systems. They constrain the joint probability distribution of the truth of reviewer-item pairs they connect. For example, a non-spammer usually gives good (bad) items good (bad) reviews with high probability, which indicates that the truth of a reviewer and the truth of an item he reviewed are related. Hence the topology of the ORS yields a set of constraints that the truth of reviewers and items must obey, and these constraints help to reduce the uncertainty of parameters in the system.\nIn order to compare the amounts of information contained in different topologies (and reviews), we calculate the Bayesian Crame\u0301r Rao lower bound (BCRLB) of maximum a posteriori estimator (MAPE) in such systems for different bipartite graph models. We find that BCRLB varies for different topologies. This indicates that for some topologies the truth become much difficult to be estimated by any MAPEs."}, {"heading": "II. BACKGROUND AND BASIC RESULTS", "text": ""}, {"heading": "A. Data Model", "text": "Following the existing works[7], [5], [6], we assume that there are a set of reviewers V and a set of items I in an online review system. Each item i \u2208 I is associated with a binary label zi \u2208 {\u00b11}, which is considered to be a random variable representing the quality of item i, e.g., zi = +1 if item i is good; zi = \u22121 if i is bad. Each reviewer can choose items to review. A review represents the reviewer\u2019s attitude to an item. If we use rui \u2208 {\u00b11} to denote u\u2019s review to i, then rui = +1 (or rui = \u22121) means that reviewer u considers item i to be good (or bad). However, reviewers are not always accurate to review items, and we use \u03b8u \u2208 [0, 1] to represent the probability that the reviewer can give correct reviews, i.e., \u03b8u = P (rui = zi). In practice, it is reasonable to assume that the majority of reviewers have \u03b8u > 0.5. This is achieved by\nputting a prior distribution on \u03b8u. A nature choice of such a prior is the beta distribution, i.e., P (\u03b8u) \u221d \u03b8\u03b1\u22121u (1\u2212 \u03b8u)\u03b2\u22121, where \u03b1 > \u03b2.\nBe different from previous works, in this work we assume that a reviewer can not freely choose which items to review. The reasons may be the reviewer\u2019s narrow range of interests, limited attention capacity, and so on. If reviewer u can review item i, we connect u and i by an edge (u, i). This forms a bipartite graph G(V, I, E), where E is the set of edges. Furthermore, we use Iu to denote the set of items that u can review, and we use Vi to denote the set of reviewers who can review i.\nTo make the aforementioned model more general, we assume that items to be reviewed by reviewers are chosen independently with replacement constrained by graph G1. This forms a collection of n review samples R = {r1, r2, \u00b7 \u00b7 \u00b7 , rn} where rk denotes the k-th sample representing some reviewer u gives some item i a review rui. Since items are chosen with replacement, we may observe that reviewer u reviews item i many times. We use nxui to represent the number of times u gives i a review x in the samples R. Note that nxui satisfies \u2211\nu\u2208V\n\u2211\ni\u2208I\n\u2211\nx\u2208{\u00b11} n x ui = n.\nOur goal is to study how G can affect the estimation accuracy when using R to estimate \u03b8 = {\u03b8u}u\u2208V and z = {zi}i\u2208I ."}, {"heading": "B. Maximum A Posteriori Estimator", "text": "A convenient way to estimate parameters of the previous model is by considering \u03b8 as parameters and z as hidden variables[3]. David and Skene[3] presented an expectation maximization (EM) approach to maximize the likelihood. Here we propose to maximize the posteriori of \u03b8 which can include the priori information of \u03b8. That is,\nmax logP (\u03b8|R) = max log \u2211\nz\nP (\u03b8, z|R). (1)\nE-Step: In the E-Step, we need to calculate the probability of hidden variables given the other variables P (z|R, \u03b8), which can be factorized to \u220f\ni P (zi|R\u00b7i, \u03b8). Here R\u00b7i \u2286 R denotes the reviews in the samples that are related to item i. If we denote each factor by \u00b5i(zi), then we can obtain\n\u00b5i(zi) \u2261P (zi|R\u00b7i, \u03b8) = P (R\u00b7i|zi, \u03b8)P (zi|\u03b8)\nP (R\u00b7i|\u03b8) (2)\n\u221dP (zi) \u220f\nu\u2208Vi\nP (Rui|zi, \u03b8u) (3)\n=P (zi) \u220f\nu\u2208Vi\nP (rui = zi|zi, \u03b8u)n zi ui (4)\n\u00d7 P (rui = \u2212zi|zi, \u03b8u)n \u2212zi ui (5)\n=P (zi) \u220f\nu\u2208Vi\n\u03b8 n zi ui u (1 \u2212 \u03b8ui)n \u2212zi ui . (6)\n1Consider items to be shops, each time a consumer buy a product from a shop, he can review the shop.\nM-Step: In the M-Step, we need to solve\n\u03b8(t+1) = argmax \u03b8 Q(\u03b8, \u03b8(t)) (7)\n= argmax \u03b8\nEz|R,\u03b8(t) [logP (\u03b8, z|R)] (8)\n= argmax \u03b8\nEz|R,\u03b8(t) [logP (R|\u03b8, z) + logP (\u03b8)] , (9)\nwhich gives us the following result\n\u03b8(t+1)u =\n\u2211\ni\u2208Iu\n\u2211\nx\u2208{\u00b11} n x ui\u00b5i(x) + \u03b1\u2212 1\n|Ru\u00b7|+ \u03b1+ \u03b2 \u2212 2 . (10)\nHere, Ru\u00b7 is the set of reviews given by reviewer u. The E-step and M-step of the EM algorithm implicitly defines an estimator of \u03b8, i.e., \u03b8\u0302MAP = EM(R). Since R is related to G, then \u03b8\u0302MAP is also related to G. To understand how G can affect the MAP estimator, we go to study the Mean Squared Errors of \u03b8\u0302MAP = {\u03b8\u0302u}u\u2208V ."}, {"heading": "III. ESTIMATION ERRORS ANALYSIS", "text": ""}, {"heading": "A. Lower Bound on Estimation Errors", "text": "The Mean Squared Error of \u03b8\u0302u is defined as MSE(\u03b8\u0302u) = E[\u03b8\u0302u\u2212 \u03b8u]2, which is lower bounded by the Bayesian Crame\u0301r Rao lower bound (BCRLB) under some conditions[9, Chapter 2]. We rewrite Eq. (217) in Van Trees\u2019 book [9, Page 73] and obtain the following relationship\nMSE(\u03b8\u0302u) \u2265 [J\u22121]uu, (11)\nwhere\nJuv = \u2212E [ \u22022 logP (\u03b8|R) \u2202\u03b8u\u2202\u03b8v ]\n(12)\nis the element (u, v) of Fisher information matrix J . The above relationship requires that \u03b8\u0302MAP is weakly unbiased[9, Chapter 2], which is unknown for the MAP estimator defined by EM algorithm. However, it is known that under general conditions, for large n, the posterior distribution of \u03b8 can be approximated by normal distribution[2], [9], [4], [8]\nP (\u03b8|R) \u2192 N (\u03b8\u0302MAP, I(\u03b8\u0302MAP)\u22121) as n \u2192 \u221e,\nwhere I(\u03b8\u0302MAP) is the observed Fisher information matrix, and each element (u, v) of I(\u03b8\u0302MAP) is defined by\n[I(\u03b8\u0302MAP)]uv = \u2212 \u22022 logP (\u03b8|R)\n\u2202\u03b8u\u2202\u03b8v\n\u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8\u0302MAP\n.\nThe above conclusion tells us that \u03b8\u0302MAP defined by the EM algorithm is a consistent estimator of \u03b8 with covariance matrix determined by I. For different G\u2019s, the estimator \u03b8\u0302MAP will have different covariance matrices. We can compare the estimation errors by evaluating I\u2019s on different bipartite graphs. In the following, we find that I is a diagonal matrix and it can be efficiently computed in combining with the EM procedure."}, {"heading": "B. Obtaining BCRLB in Combining with EM Procedure", "text": "Because P (\u03b8|R)P (z|\u03b8,R) = P (\u03b8, z|R), or equivalently logP (\u03b8|R) = logP (\u03b8, z|R)\u2212 logP (z|\u03b8,R), (13)\nThen\n\u22022 logP (\u03b8|R) \u2202\u03b8u\u2202\u03b8v = \u22022 logP (\u03b8, z|R) \u2202\u03b8u\u2202\u03b8v \u2212 \u2202 2 logP (z|\u03b8,R) \u2202\u03b8u\u2202\u03b8v (14)\n= \u2211\nz\n\u22022 logP (\u03b8, z|R) \u2202\u03b8u\u2202\u03b8v P (z|\u03b8(t), R) (15)\n\u2212 \u2211\nz\n\u22022 logP (z|\u03b8,R) \u2202\u03b8u\u2202\u03b8v P (z|\u03b8(t), R) (16)\n\u2261\u2202 2Q(\u03b8, \u03b8(t)) \u2202\u03b8u\u2202\u03b8v \u2212 \u2202 2H(\u03b8, \u03b8(t)) \u2202\u03b8u\u2202\u03b8v (17)\nThe first item of RHS is\n\u22022Q \u2202\u03b82u = \u2211\ni\u2208Iu\n\u2211\nx\u2208{\u00b11}\n\u00b5i(x)\n[\n\u2212n x ui \u03b82u \u2212 n \u2212x ui (1\u2212 \u03b8u)2 ]\n(18)\n\u2212 \u03b1\u2212 1 \u03b82u \u2212 \u03b2 \u2212 1 (1 \u2212 \u03b8u)2\n(19)\nand \u2202 2Q\n\u2202\u03b8u\u2202\u03b8v = 0 if u 6= v. The second item of RHS is\n\u22022H \u2202\u03b82u = \u2211\ni\u2208Iu\n\u2211\nx\u2208{\u00b11}\n\u00b5i(x)\n[\n\u2212n x ui \u03b82u \u2212 n \u2212x ui (1\u2212 \u03b8u)2 ] , (20)\nand \u2202 2H \u2202\u03b8u\u2202\u03b8v = 0 if u 6= v. Finally, we obtain the observed Fisher information matrix\nIuu = \u03b1\u2212 1 \u03b8\u03022u + \u03b2 \u2212 1 (1\u2212 \u03b8\u0302u)2 , (21)\nand Iuv = 0 if u 6= v. This indicates that I is a diagonal matrix. Note that Eq. (21) is convex, Iuu gets the minimum value at \u03b8\u0302\u2217u = 1\n1+ 4 \u221a (\u03b2\u22121)/(\u03b1\u22121) and Iuu gets the maximum value at 0 or 1. This tells us that \u03b8\u0302u is most uncertain when \u03b8\u0302u = \u03b8\u0302\u2217u and most certain at \u03b8\u0302u = 0 or 1. This is consistent with intuition as \u03b8\u0302u can be considered as the parameter of a Bernouli distribution."}, {"heading": "IV. EMPIRICAL RESULTS", "text": "To study how constrained connections can affect the estimation accuracy of MAPE, we first present several bipartite graph models and then study how these models affect the performance of MAPE measured by the accuracy of classifying items and BCRLBs."}, {"heading": "A. Bipartite Graph Models", "text": "1) Random Graph Model Grnd: Each edge (u, i) in Grnd is formed by uniformly choosing a reviewer u \u2208 V and uniformly choosing an item i \u2208 I .\n2) Item Preferential Attachment Graph Model GiPA: The assumption of this model is that popular items are more easily to receive reviews. Hence, an edge (u, i) in GiPA is formed by uniformly random choosing a reviewer u \u2208 V , and choosing item i \u2208 I with probability proportion to i\u2019s degree in GiPA.\n3) Reviewer and Item Preferential Attachment Graph Model GriPA: We can also assume that a reviewer who is more active is more likely to review items. Hence, an edge (u, i) in GiPA is formed by choosing a reviewer u \u2208 V with probability proportion to u\u2019s degree, and choosing item i \u2208 I with probability proportion to i\u2019s degree in GiPA."}, {"heading": "B. Building Ground Truth Known Datasets", "text": "Given a graph built by one of the above models, we describe the procedure of generating review samples R.\nWe specify a set of |V | reviewers and |I| items. Suppose that each user u\u2019s parameter \u03b8u is chosen from beta prior distribution P (\u03b8u) \u221d \u03b8\u03b1\u22121u (1\u2212 \u03b8u)\u03b2\u22121, i.e., reviewer u gives correct review with prior probability \u03b1/(\u03b1+\u03b2). For each item i, we randomly assign a label zi \u2208 {\u00b11} by flipping a fair coin, i.e., P (zi = +1) = P (zi = \u22121) = 0.5. The procedure of generating R is as follows.\nAlgorithm 1: Generating R.\nInput: G(V, I, E), {zi}i\u2208I , {\u03b8u}u\u2208V , n. Output: Review samples R.\n1 R = []; 2 while |R| < n, do 3 Randomly choose an edge (u, i) \u2208 E; 4 Generate a random number x \u223c U(0, 1); 5 rui = zi if x \u2264 \u03b8u else \u2212zi; 6 Put rui into R; 7 end"}, {"heading": "C. Comparing Items Inference Accuracy Under Different Graphs", "text": "In the first experiment, we compare classification accuracy of items under different graph models. We set an item with label +1 (or \u22121) if \u00b5i(+1) > 0.5 (or \u00b5i(\u22121) > 0.5). The accuracy is defined as\nAccuracy = TP + TN\nP +N ,\nwhere TP and TN are the true positive and true negative respectively. P and N are positive and negative respectively. Accuracy describes the fraction of items that can be corrected inferred.\nThe results are shown in Fig. 2. We first generated graphs with number of nodes |V | = 500 and varying number of edges (|E| = 1000, 2000, 3000, 4000, 5000) using different graph models. In each figure, we generated review samples of different sizes (500 \u2264 |R| \u2264 5000), and show accuracy of inferring items averaged over 100 experiments respectively. We observe that when |R| increases, the accuracy also increases and approaches 1. This confirms that the MAPE estimator is asymptotically unbiased. For different graph models, we observe that the accuracy on Grnd is larger than the other two models. This indicates that constrained connections will make the inference performance poor. However, the accuracy curves\non GiPA and GriPA are approximately the same. This indicates that more constrained may not always decrease accuracy. To distinguish the difference of different constrained connections clearly, we study their difference of BCRLBs."}, {"heading": "D. Comparing Estimation Errors Under Different Graphs", "text": "In the second experiment, we study how different graph modes affect the BCRLBs. The settings are same as in the previous experiment. We compare the average rooted mean squared error (RMSE) (defined as RMSE = \u221a MSE) lower bound over different graph models in Fig. 3. The RMSE decreases approximately with rate 1/n over all the graphs. For different graphs, when n is large (we do not consider BCRLB for small n, because MAPE is biased when n is small), RMSE on GriPA has the largest lower bound, then comes GiPA and RMSE on Grnd has the lowest lower bound. This indicates, when more constraints are added on graphs, the RMSE of any MAPEs will always become worse."}, {"heading": "V. CONCLUSION", "text": "The constrained connections are common in real world. A reviewer cannot review all the items due to various reasons in online review systems. In this study, we find that this constrained connection will always cause poor inference performance, both from the viewpoints of inference accuracy and RMSE lower bound."}], "references": [{"title": "Opinion fraud detection in online reviews by network effects", "author": ["Leman Akoglu", "Rishi Chandy", "Christos Faloutsos"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Mathematical Methods of Statistics", "author": ["Harald Cram\u00e9r"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1946}, {"title": "Maximum likelihood estimation of observeerror-rates using the em algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "JSTOR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1979}, {"title": "Assessing the accuracy of the maximum lielihood estimator: Observed versus expected fisher information", "author": ["Bradley Efron", "David V. Hinkley"], "venue": "JSTOR, 65:457\u2013482,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1978}, {"title": "Iterative learning for reliable crowdsourcing systems", "author": ["David R. Karger", "Sewoong Oh", "Devavrat Shah"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Variational inference for crowdsourcing", "author": ["Qiang Liu", "Jian Peng", "Alexander Ihler"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Learning from crowds", "author": ["Vikas C. Raykar", "Shipeng Yu", "Linda H. Zhao", "Gerardo Hermosillo Valadez", "Charles Florin", "Luca Bogoni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Toos for Statistics Inference: Methods for the Exploration of Posterior Distribution and Likelihood Functions", "author": ["Martin A. Tanner"], "venue": "Springer Series in Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Detection, Estimation, and Modulation Theory", "author": ["H.L. Van Trees"], "venue": "Wiley, 1st edition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1968}, {"title": "Review graph based online store review spammer detection", "author": ["Guan Wang", "Sihong Xie", "Bing Liu", "Philip S. Yu"], "venue": "Grnd GiPA GriPA", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "In previous studies[10], [1], most of the works ignore the function of the underlying topology of ORS.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "In previous studies[10], [1], most of the works ignore the function of the underlying topology of ORS.", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "Data Model Following the existing works[7], [5], [6], we assume that there are a set of reviewers V and a set of items I in an online review system.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "Data Model Following the existing works[7], [5], [6], we assume that there are a set of reviewers V and a set of items I in an online review system.", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "Data Model Following the existing works[7], [5], [6], we assume that there are a set of reviewers V and a set of items I in an online review system.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "However, reviewers are not always accurate to review items, and we use \u03b8u \u2208 [0, 1] to represent the probability that the reviewer can give correct reviews, i.", "startOffset": 76, "endOffset": 82}, {"referenceID": 2, "context": "Maximum A Posteriori Estimator A convenient way to estimate parameters of the previous model is by considering \u03b8 as parameters and z as hidden variables[3].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "David and Skene[3] presented an expectation maximization (EM) approach to maximize the likelihood.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "However, it is known that under general conditions, for large n, the posterior distribution of \u03b8 can be approximated by normal distribution[2], [9], [4], [8] P (\u03b8|R) \u2192 N (\u03b8\u0302MAP, I(\u03b8\u0302MAP)) as n \u2192 \u221e, where I(\u03b8\u0302MAP) is the observed Fisher information matrix, and each element (u, v) of I(\u03b8\u0302MAP) is defined by [I(\u03b8\u0302MAP)]uv = \u2212 \u2202 logP (\u03b8|R) \u2202\u03b8u\u2202\u03b8v \u2223", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "However, it is known that under general conditions, for large n, the posterior distribution of \u03b8 can be approximated by normal distribution[2], [9], [4], [8] P (\u03b8|R) \u2192 N (\u03b8\u0302MAP, I(\u03b8\u0302MAP)) as n \u2192 \u221e, where I(\u03b8\u0302MAP) is the observed Fisher information matrix, and each element (u, v) of I(\u03b8\u0302MAP) is defined by [I(\u03b8\u0302MAP)]uv = \u2212 \u2202 logP (\u03b8|R) \u2202\u03b8u\u2202\u03b8v \u2223", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "However, it is known that under general conditions, for large n, the posterior distribution of \u03b8 can be approximated by normal distribution[2], [9], [4], [8] P (\u03b8|R) \u2192 N (\u03b8\u0302MAP, I(\u03b8\u0302MAP)) as n \u2192 \u221e, where I(\u03b8\u0302MAP) is the observed Fisher information matrix, and each element (u, v) of I(\u03b8\u0302MAP) is defined by [I(\u03b8\u0302MAP)]uv = \u2212 \u2202 logP (\u03b8|R) \u2202\u03b8u\u2202\u03b8v \u2223", "startOffset": 149, "endOffset": 152}, {"referenceID": 7, "context": "However, it is known that under general conditions, for large n, the posterior distribution of \u03b8 can be approximated by normal distribution[2], [9], [4], [8] P (\u03b8|R) \u2192 N (\u03b8\u0302MAP, I(\u03b8\u0302MAP)) as n \u2192 \u221e, where I(\u03b8\u0302MAP) is the observed Fisher information matrix, and each element (u, v) of I(\u03b8\u0302MAP) is defined by [I(\u03b8\u0302MAP)]uv = \u2212 \u2202 logP (\u03b8|R) \u2202\u03b8u\u2202\u03b8v \u2223", "startOffset": 154, "endOffset": 157}], "year": 2013, "abstractText": "In this work, we study how constrained connections can cause estimation errors in online review systems. Constrained connection is the phenomenon that a reviewer can only review a subset of products/services due to reviewer\u2019s narrow range of interests or limited attention capacity. We find that reviewers\u2019 constrained connections will cause poor inference performance, both from the measurements of estimation accuracy and Bayesian Cram\u00e9r Rao lower bound.", "creator": "gnuplot 4.6 patchlevel 0"}}}