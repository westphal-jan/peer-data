{"id": "1307.7351", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jul-2013", "title": "Knowledge Representation for Robots through Human-Robot Interaction", "abstract": "the representation of the knowledge needed underlying a robot to perform complex tasks is restricted citing local limitations of perception. one specific way about modelling this situation and its \" social \" robots came to rely mainly the interaction with the user. both want a sub - modal interaction module whose allows to effectively merge information about the environment about the robot operates. in particular, in this paper we present a rich representation framework that potentially be automatically built out computational metric data annotated throughout the gradient collected by 1 user. such a representation, performs then the translations into ground complex simple expressions for motion functions and how manage topological data plans yet convey the target area.", "histories": [["v1", "Sun, 28 Jul 2013 10:45:57 GMT  (2012kb)", "http://arxiv.org/abs/1307.7351v1", "Knowledge Representation and Reasoning in Robotics Workshop at ICLP 2013"], ["v2", "Thu, 1 Aug 2013 16:54:15 GMT  (2012kb)", "http://arxiv.org/abs/1307.7351v2", "Knowledge Representation and Reasoning in Robotics Workshop at ICLP 2013"]], "COMMENTS": "Knowledge Representation and Reasoning in Robotics Workshop at ICLP 2013", "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["emanuele bastianelli", "domenico bloisi", "roberto capobianco", "guglielmo gemignani", "luca iocchi", "daniele nardi"], "accepted": false, "id": "1307.7351"}, "pdf": {"name": "1307.7351.pdf", "metadata": {"source": "CRF", "title": "Knowledge Representation for Robots through Human-Robot Interaction", "authors": ["E. Bastianelli", "D. Bloisi", "R. Capobianco", "G. Gemignani", "L. Iocchi", "D. Nardi"], "emails": ["<lastname>@dis.uniroma1.it"], "sections": [{"heading": null, "text": "Keywords: Knowledge Representation, Robotics, Human-Robot Interaction"}, {"heading": "1 Introduction", "text": "Robots are expected to become consumer products. However, there is still a gap in terms of user expectations and robot functionality. A key limiting factor is the lack of knowledge and awareness of the robot on the problem to be solved and on the operational environment. The difficulties arise from several sources: the current capabilities of perception systems, the difficulty of communicating with humans, and, arguably, the ability to acquire, maintain and use symbolic knowledge.\nOur long term research goal is to improve the performance of robotic systems by addressing the above three limitations, and, specifically to devise \u201cKnowledgeable Robots\u201d [1]. Many researchers are addressing the above challenges. For example, recently, there has been a significant progress in new sensing devices and improved perception capabilities. At the same time, several researchers have addressed forms of collaboration between humans and robots, and voice interaction in particular, that provides a natural approach to communicate with users. Indeed, speech interfaces have reached a level of performance that makes them deployable on hand held devices for applications that go beyond the basic telephone functionality.\nStill, few works address the use of classical symbolic methods for knowledge representation and reasoning on robots. Moreover, when this is addressed, the representation of symbolic knowledge is typically based on a static representation of general knowledge (i.e. for action planning), which is disconnected from both the perception system and the interaction with the user.\nWe propose to address the challenge of knowledgeable robots by embracing the paradigm of symbiotic autonomy [2], which relies on the interaction with the user in order to overcome the limitations of the robot in perception and action capabilities.\nA key issue in the interaction with robots to establish a proper relationship between the symbols used in the representation and the corresponding elements of the operational environment. This process is usually referred to as symbol grounding and, in the case of robots, requires to \u201cclose the loop\u201d with perception. Consequently, symbol grounding characterizes one of the most critical design issues in the interaction with robots, namely the connection between numeric and symbolic representation.\nSpecifically, our work addresses the problem of building high-level representations of the environment that embody both metric and symbolic knowledge about it. To this aim, a multi-modal interaction (including a speech interface) can be used in the process of building a very rich representation, by acquiring information in a suitable and natural way.\nIn order to obtain a representation that the robotic system can effectively use to perform complex tasks, such as \u201cgo near the fridge\u201d or \u201ccheck whether the TV set is on\u201d, metric and semantic information need to be suitably merged. The resulting, integrated representation should enable the system to perform topological navigation, understanding target locations and the position of objects in the environment.\nThe approach that we present in this paper aims at building a rich representation of the robot\u2019s knowledge about the environment, comprising a suitable integration of metric and semantic information. More specifically, we describe the representation and the acquisition of the robot\u2019s knowledge about a specific environment with the use of a multi-modal human-robot interaction system. We also consider the possibility of extending the approach to an on-line construction and maintenance of the semantic map through a continuous long-standing interaction with the user.\nAs a general difference with previous work in producing human augmented maps, as well as in representing the knowledge of the robot about the environment, we aim at a\nrich and detailed representation of a specific environment, as opposed to trying to embed in the robot the knowledge that allows it to operate in any kind of environment, without any previous interaction with the user and the actual operational environment. This is currently possible only through an adequate use of multi-modal natural user interaction (not requiring specific robotic expertise) and, specifically on the use of voice interaction to create a common reference for the symbols representing the knowledge of the robot.\nThe approach described in this paper builds on previous work for off-line building of a semantic map [3], based on two basic components: 1) a subsystem for Simultaneous Localization And Mapping, that provides a metric map of the environment; 2) a multimodal interface, including speech, that allows the user to point at the elements of the environment and to assign their semantic role (see Figure 1). The novel contribution here is the definition of a richer representation of the environment, including a Cell Map and a Topological Graph, and of the corresponding procedures for knowledge construction, that increase the capability of the robot to perform complex reasoning for executing complex tasks.\nThe paper is organized as follows. Section 2 describes the background and a suitable context for our work with respect to state of the art. The representation of the robot\u2019s knowledge is presented in Section 3, while the method for its construction is described in Section 4. Conclusions are drawn in the last section."}, {"heading": "2 Related Word", "text": "Hertzberg and Saffiotti [4] characterize semantic knowledge in robotics by two properties: (i) the need for an explicit representation of knowledge inside the robot; and (ii) the need for grounding the symbols used in this representation in real physical objects, parameters, and events. Many robotic systems nowadays embody some sort of semantic knowledge, but it is often hard-coded in their implementation. This greatly reduces its reuse, the possibility to reason on it, or to share it with other entities (e.g. robots, hardware devices, or humans). Several works have addressed symbolic representation of knowledge for complex robot tasks. For example, Schiffer et al. [5] describe the implementation of an intelligent service robot whose knowledge about the task to be executed is represented with the logic language READYLOG (a variant of Golog). Aker et al. [6] use Answer Set Programming for representing and reasoning about the actions to be accomplished by a housekeeping robot. De Giacomo et al. [7] present a knowledge representation and reasoning framework based on Description Logics for a service robot. In all these works the knowledge base is assumed to be given to the robot and the methods used to actually acquire this knowledge are not discussed. In this paper we focus on the construction of a semantic representation of knowledge, more precisely on semantic mapping.\n\u201cA semantic map for a mobile robot is a map that contains, in addition to spatial information about the environment, assignments of mapped features to entities of known classes\u201d [8]. Two properties come out from this definition: 1) semantic maps are independent from the adopted representation for the geometric map behind it: 2D maps, 3D maps, topological maps, texture-based maps; 2) semantic maps also include commonsense knowledge about generic properties of the labeled entities.\nThe works on semantic mapping can be grouped into two main categories, by distinguishing automatic processing from approaches involving a human user to help the robot in the semantic mapping process.\nIn the first category, we consider works describing fully automaticmethods in which human interaction is not considered at all. A first set of techniques aim at extracting features of the environment from laser based metric maps to support labeling and extract high-level information. These works include determining attributes of rooms [9], doorways detection [10], etc. Moreover, in Galindo et al. [11], environmental knowledge is represented by augmenting a topological map (extracted with fuzzy morphological operators) with semantic knowledge using anchoring. A second set of techniques make use of classification and clustering for automatic segmentation and labeling of metric maps. For example, in Nu\u0308cther et al. [12], environmental knowledge is extracted by labeling 3D points through the gradient difference between neighboring points; points are then classified into floor-points, object-points, or ceiling-points. While generation of 2D topological maps from metric maps has been described in [13] and [14] (using AdaBoost), in Brunskill et al. [15] (using spectral clustering), in [16] (using Voronoi random fields), etc. Finally, a third set of techniques for object recognition and place categorization use visual features, such as in [17], or a combination of visual and range information, provided from an RGB-D camera, such as in [18]. Although significant progress has been made in fully automated semantic mapping, the approach still suffers from errors and lack of generality.\nIn the second category, we consider approaches for human augmented mapping where the user actively supports the robot to acquire the required knowledge about the environment. In particular, the user role is in grounding symbols to objects that are still autonomously recognized by the robotic platform. In this case, the human-robot interaction is uni-modal, and typically achieved through speech. In Diosi et al. [19] an interactive SLAM procedure and a watershed segmentation are employed to create a contextual topological map. In Zender et al. [20] a system to create conceptual representations of human-made indoor environments is described. A robotic platform owns a priori knowledge about spatial concepts, and through them builds up an internal representation of the environment acquired through low-level sensors. The user role throughout the acquisition process is to support the robot in place labeling. However, once achieved, the conceptual representation is also useful for effective human-robot dialogue. Pronobis and Jensfelt [21] present a multi-layered semantic mapping algorithm that combines information about the existence of objects and semantic properties about the space, such as room size, share and appearance. These properties decouple low-level information from high-level room classification. The user input, whenever provided, is integrated in the system as additional properties about existing objects. Finally, Nieto-Granda et al. [22] adopts human augmented mapping based on a multivariate probabilistic model to associate a spatial region to a semantic label. A user guide supports a robot in this process, by instructing the robot in selecting the labels. Few approaches aim at a more advanced form of human-robot collaboration, where the user actively cooperates with the robot to build a semantic map, not only for place categorization and labeling, but also for object recognition and positioning. Such an interaction is more complex and requires natural paradigms, not to result in a tedious\neffort for a non-expert user. For this reason, multi-modal interaction is preferred, to naturally deal with different types of information. For example, Kruijff et al. [23] introduce a system to improve the mapping process by clarification dialogues between human and robot, using natural language; Randelli et al. [3] propose a rich multi-modal interaction, including speech, gesture, and vision enabling for a semantic labeling of environment landmarks that makes the knowledge about the environment actually usable, but without creating a suitable representation.\nThe approach presented in this paper aims at improving human-robot collaboration by allowing non-expert operators to generate rich semantic information about an environment. To this end, we adopt a multi-modal natural interaction framework to build semantic maps, where the acquired knowledge can effectively support the execution of complex tasks. The main differences with previous work on semantic mapping are: 1) a much richer representation of the environment including semantic descriptions of places, objects and functionalities of the objects; 2) an increased flexibility of the system that can be effectively used in different kinds of environments without requiring a substantial training to build a priori knwowledge."}, {"heading": "3 Representation of the Robot\u2019s Knowledge", "text": "In this section we describe the representation of the robot\u2019s knowledge, while the description of how this knowledge is acquired and how the representation is actually built is provided in the next section.\nThe robot\u2019s knowledge is divided in two layers: 1) the specific knowledge that the system acquires about the environment, denoted as world knowledge; 2) the general knowledge about the domain and its use, that we denote as domain knowledge.\nIt is important to point out that, while the two components may resemble the extensional and intensional components of a classical knowledge base, here they are independent of each other. In fact, the world knowledge may be inconsistent with the domain knowledge (e.g., the robot may have discovered a fridge in the living room, rather than or in addition to the one in the kitchen, while the domain knowledge states that \u201cfridges are in the kitchen\u201d). Generally speaking, domain knowledge (which represents an a priori knowledge about the environment) is used to support the action of the robot, only when specific world knowledge is not available. For example, the robot is able to reach an object (e.g., the fridge in the living room) , because it has been previously discovered; if there is no specific knowledge about the position of an object (e.g., a fridge), the system will refer to the general domain knowledge to find out a possible location (e.g., the kitchen).\nMore specifically, the robot\u2019s knowledge is defined by the following structures:\n\u2013 World Knowledge \u2022 Metric Map \u2022 Instance Signature Data Base \u2022 Cell Map \u2022 Topological Graph \u2013 Domain Knowledge \u2022 Conceptual Knowledge Base"}, {"heading": "3.1 Knowledge about the robot\u2019s world", "text": "As already mentioned, a semantic map is typically characterized as a low-level representation of the 2D map (i.e. a grid), that is labelled with symbols (e.g. [21], [20], [22]). Such symbols denote for example, the area corresponding to a kitchen (or simply to a room) or the presence of structural elements of the environment and objects. Our representation builds on a similar structure, but it supports a much more detailed description, based on the interaction and hints provided by the user.\nIn the following definitions we use the term C oncept to refer to a set of symbols used in the conceptual KB to denote general concepts (i.e., abstraction of objects or locations). For example, Fridge and Kitchen are concept names of the general Domain Knowledge discussed in the next subsection. While the term L abel is used to refer to a set of symbols indicating specific instances of objects or locations. For example, f ridge1 is a label denoting a particular fridge and kitchen1 is a label denoting a particular kitchen. In the notation used in this paper, concept names have the first letter capitalized, while labels are denoted with all lowercase letters, typically followed by a digit. The associations between labels and concepts are denoted as label \ufffd\u2192 Concept. For example, f ridge1 \ufffd\u2192 Fridge denotes that the label f ridge1 is related to the concept Fridge (i.e., f ridge1 is a fridge). Notice that the meaning of the labels is simply that of a pointer to the concept that in the domain knowledge represents the general knowledge about the object or location, and it is not an instance of the concept. In this way, a labeled object can be enriched with general domain information, but it is not required to be consistent with the domain KB.\nThe representation formalism of the World Knowledge contains the following elements.\nMetric Map. The Metric Map is represented as an occupancy grid generated by a SLAM method. This map has usually a fine discretization (e.g., 5 cm) and is used for low-level robot tasks, such as localization and navigation.\nInstance signatures. The instance signatures are represented as a data base of structured data, where each instance has a unique label (l \u2208L abel), an associated concept (C \u2208 C oncept) such that l \ufffd\u2192C, and a set of properties (including for example the position in the environment) expressed as attribute-value pairs.\nCell Map. The Cell Map is represented as a discretization of the environment in cells of variable size. Each cell represents a portion of a physical area and is an abstraction of locations that are not distinguishable from the point of view of robot high-level behaviors. The Cell Map also includes a function f :Cell \u2192 2L abel , that maps each cell to a set of labels associated to concepts in the conceptual KB of the Domain Knowledge, and a connectivity relationConnect \u2286Cell\u00d7Cell, that describes the connectivity between adjacent cells.\nTopological Graph. The Topological Graph is a graph where nodes are locations associated to cells in the Cell Map and edges are connections between these locations. Locations are distinguished in two types: static and dynamic. For the static locations, the corresponding positions (i.e., the correspondences with the metric map) are fixed, while in the dynamic locations, the corresponding positions are variable within a given area of the environment. Since the Topological Graph is used by the robot for navigation\npurposes, the edges also contains the specific navigation behavior that is required for the robot to move from one location to another. In this way the topological map is also used to generate appropriate sequences of behaviors to achieve the robot\u2019s navigation goals.\nIn order to better explain these representations, in the remaining of this section we provide an example of World Knowledge for a home environment, while in the next section we describe how this representation has been built and how the knowledge has been acquired.\nFigure 2 shows a graphical representation of the World Knowledge as described above. The Metric Map with a resolution of 5 cm generated by the SLAM method is shown as background of the image, where black pixels represent occupied cells.\nEvery object and location reported in the figure has an entry in the data base of instance signatures. In particular, each instance is determined by a unique label representing an entity within the mapped environment, (e.g. f ridge1), the corresponding concept according to the conceptual KB (e.g. f1 \ufffd\u2192Fridge), and a set of properties (e.g., position = < x,y,\u03b8 >, color = white, open = f alse, ...).\nThe Cell Map is shown in the same image. Cells are delimited by solid borders; colors and text in the cell specify the labels associated to each cell. The labels in the Cell Map refer to a typical area of a home (kitchen, living room, bathroom, bedroom, etc.) and to typical objects (tables, chairs, heaters, doors, etc.). Each cell can have more than one label, thus representing that the same area can be associated with more concepts. In particular, each color corresponds to a label indicated in the legend to the right and text (where present) corresponds to additional labels for that cell corresponding to the\nobjects reported on the right side of the figure. For example, the green cells labeled with f1 (top-right corner of the kitchen area) are mapped to the labels { f1,kitchen} and these labels are associated with the concepts Fridge and Kitchen, i.e. f1 \ufffd\u2192 Fridge and kitchen \ufffd\u2192 Kitchen, respectively. Thus, the corresponding area is characterized as being occupied by a fridge and as belonging to the kitchen. Connectivity relations between cells are not explicitly shown in the figure, but they can be derived by adjacent cells.\nThe Topological Graph, that establishes the relationship between the metric and the symbolic representation, is shown in the figure as a graph connecting oval nodes. Dark nodes are static locations, while light nodes are dynamic locations, that are associated to the main areas (labels) of the environment. The static locations denote specific positions that are of interest for the robot tasks (e.g., the position to enter in a room), while the dynamic locations are used to denote areas, where instantiation of the position (i.e., the mapping to the metric map) for a navigation behavior is executed at run-time, depending on the current status of the robot and on its goals."}, {"heading": "3.2 General knowledge about the domain", "text": "In previous work, domain knowledge has typically been characterized as a conceptual knowledge base, representing a hierarchy of concepts, including their properties and relations, a priori asserted as representative of any environment. The conceptual knowledge base contains a taxonomy of the concepts involved in the environment tied by an is-a relation, as well as their properties and relations (see [11], [13]). These concepts are used in the World Knowledge to characterize the specific instances of the environment, as explained in the previous section. In a typical representation three top-most classes have been considered: Areas, Structural Elements, and Objects. Areas denote places in the environment (corridors, rooms, etc.), Structural Elements are static entities that form the environment and that topologically connect areas (windows, doors, etc.), while Objects are elements in the environment not related to its structure and located within areas (printers, tables, etc.). A snapshot of the conceptual knowledge base used in the experiments is reported in Figure 3.\nThe knowledge base contains also environmental properties of the defined concepts, like size and functionalities of objects, typical connections among places, etc., that are useful to describe the general knowledge about an environment and to support robot task execution. For example, the general knowledge in a home environment that fridges are in kitchens helps the robot to perform tasks related with a fridge even when the exact location of a fridge is not known. A noticeable difference with respect to previous works is that the purpose of this component is not to classify spaces and objects; rather, we focus on the inheritance of properties to support the map acquisition, as well as different actions of the robot in the environment.\nThe uses of the Domain Knowledge base are manifold. Spatial properties of objects can be used to build metric representations of objects (see next section); functional properties can be used to determine location of object (e.g., for devising a plan to search for an object); physical properties can be used to check preconditions for executing transportation, etc.\nAs a matter of fact, any source of domain knowledge may be accessed and used by the robot, such as for example in querying the web, (e.g. RoboEarth [24] and OpenEval[25]), or even querying other robots operating in similar environments."}, {"heading": "4 Acquisition of the Robot\u2019s Knowledge", "text": "The process of building the representation of the robot\u2019s knowledge is composed of two phases that take place, with the help of the user, after the exploration of the environment: 1) Metric Map and Instance Signatures Construction. A 2D metric map is generated through a SLAM module and the set of instance signatures is created; 2) Cell Map and Topological Graph Generation. Starting from the 2D metric map, a grid-based topological representation (cell map) is obtained. Then, the topological graph needed by the robot to perform high level behaviours is built by processing the instance signatures and the cell map."}, {"heading": "4.1 Metric Map and Instance Signatures", "text": "In the first phase of the knowledge acquisition procedure (see Fig. 4), the robot is used to navigate the environment in order to create the 2Dmap and to register the positions of the different objects of interest. The user can tag a specific object by using a commercial laser pointer (Fig. 5a).\nWhile the object is pointed through the laser, the user has to name the object, so that the label can be assigned to it. The image segmentation module is responsible for extracting the 3D shape of the pointed object by exploiting the RGB-D data coming from the Kinect, sensor [26]. The laser dot is detected in the RGB image (Fig. 5b) by searching for the color of the light emitted (green in our case). Then, all the planes in the scene are extracted from the 3D point cloud and those that do not contain the dot\nare discarded. The remaining points are analysed to segment the shape of the object of interest (Fig. 5c).\nIt is worth noticing that the image segmentation module exploits both the information provided by the data base (height, width, and length) and the laser dot position in the Kinect view (used as seed point of the expansion). If the extracted shape is coherent with the dimensional attributes of the corresponding instance signature stored in the data base (i.e., the one with the same label recognized by the vocal interface), then a correct acquisition message is sent to the user in order to acknowledge the successful association.\nThe pose estimation module outputs the 2D position x,y and the bearing \u03b8 of the tagged object. The pose (x,y,\u03b8) is calculated by taking into account the normal corre-\nsponding to the surface of the segmented object in reference frame of the Kinect and then applying a transformation to the reference frame of the robot.\nWhen the acquisition run is terminated all the recorded data (i.e., laser scans and object poses) are processed. The laser scans are used as the input for generating the 2D metric map. A Graph-based SLAM approach, as presented in [27], is used to estimate the 2D map of the environment and the trajectory of the robot. The nodes in the graph represent the poses of the robot, while the edges express spatial constraints arising from odometry measurements. In order to obtain the object map that integrates the instance signatures into the SLAM generated map, new edges are added to the graph together with the previous ones (given by laser scans and robot odometry). Each registered object pose is considered as a new edge of the graph, embodying the pose and the label of the object. When the new expanded graph is complete, a least-square minimization algorithm [28] is performed to obtain the optimized final 2D metric map. The final poses of the tagged objects (coherent with the optimized map) are stored as instance signatures together with corresponding labels."}, {"heading": "4.2 Cell Map and Topological Graph", "text": "In the second phase of the knowledge acquisition procedure the cell map and the topological graph are generated (see Fig. 6).\nThe cell map contains a high-level description about the regions, structural elements, and objects contained in the environment. It is generated using both the instance signatures and the 2D topological map. The algorithm to generate such a map can be further decomposed into the following steps: 1) rasterizing the metric representation of the map into a grid-based topological representation (grid computation); 2) including in this new map representations of the objects and structural elements tagged in the\nenvironment (object addition); 3) automatically labeling the areas of the environment (labeling areas), using contour closure and region filling techniques.\nGrid computation is accomplished by applying the Hough Transform at different resolution levels to identify the lines passing through the walls. In such a way, it is possible to find all the main walls in the map. The cells of the grid have different sizes with respect to the amount of relevant features in the considered area (for example, the cells in the corridor are wider then the ones in the offices).\nObject addition consists in placing the tagged objects into the grid map. For this we use objects\u2019 pose, shape and size, present in the instance signature data base.\nFinally, labeling areas is applied to segment the environment into different rooms. The algorithm is enhanced by considering the tagged objects and structural elements previously included in the environment. For example, a simple heuristic uses the knowledge about doors to separate different areas.\nIn the final step of our knowledge building process, a topological graph is created by using the knowledge contained both in the cell map and in the instance signatures. Such graph embodies the information needed to the robot for navigating and acting in the environment. The constituting nodes of this graph are locations associated to cells in the cell map, while the edges are connections between these locations. For each room, two kinds of nodes are produced: static and dynamic. Static nodes represent locations that have a fixed position on the map and they are used to deal with critical pathways in the map (i.e. doors), where a specific behavior is required. Dynamic nodes represent, instead, variable positions within a given area of the environment. Thus, for each room mapped in the cell map, a dynamic node is created while, for each doorway that connects such a room to another, a static node is created with its fixed position set in front of the considered door. Finally, each static node is connected to the dynamic node of the room it belongs to and to the static node that represents the location in front of the other side of its related door. The construction of the topological graph highlights a twofold aspect of our system: both the knowledge provided through user interaction and a-priori high-level knowledge contribute to refine the overall mapping process."}, {"heading": "5 Conclusions", "text": "In this paper, we propose an approach to build an effective representation of the knowledge about the environment where the robot operates, by relying onmulti-modal humanrobot interaction, including natural language. The proposed framework makes it possible to bridge the gap between the numerical and the symbolic knowledge needed by the robot, by grounding the symbols onto the objects represented in the metric map. As a result, the proposed setting for representing the knowledge of the robot is significantly richer than previous approaches in the literature; in particular, it supports better performance and greater generality with respect to fully automatic methods. Through the acquired representation the robot can execute complex tasks. For example, the robot can execute complex commands such as \u201ccheck whether in the corridor the third window on the left is open\u201d, retrieve its metric position in the map, plan a path and reach the specified location and execute the task.\nThe proposed approach can be further developed in at least two fundamental directions, that we are currently investigating. First, the generation of the map is currently done off-line. A more general setting should enable the robot to grow and update the representation through a continuous interaction with the user. While the proposed approach to symbol grounding can be naturally extended, the new framework would require to ensure the consistency of the representation and to deal with the changes in the environment and the dynamics of objects. Second, currently the robot can only learn knowledge about the environment, but not knowledge about tasks to be accomplished in response to user commands. This extension would require a representation of the robot\u2019s actions and an extended dialogue to acquire input from the user.\nIn conclusion, in our view, the proposed approach brings about a new perspective and new developments for the representation of the robot\u2019s knowledge."}], "references": [{"title": "Knowing, Reasoning, and Acting", "author": ["L.C. Aiello", "D. Nardi", "G. Randelli", "C. Scalzo"], "venue": "Essays in Honour of Hector J. Levesque", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "An effective personal mobile robot agent through symbiotic human-robot interaction", "author": ["S. Rosenthal", "J. Biswas", "M. Veloso"], "venue": "Proc. of 9th International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Knowledge acquisition through humanrobot multimodal interaction", "author": ["G. Randelli", "T.M. Bonanni", "L. Iocchi", "D. Nardi"], "venue": "Intelligent Service Robotics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Using semantic knowledge in robotics", "author": ["J. Hertzberg", "A. Saffiotti"], "venue": "Robotics and Autonomous Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Caesar: An intelligent domestic service robot", "author": ["S. Schiffer", "A. Ferrein", "G. Lakemeyer"], "venue": "Intelligent Service Robotics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Answer set programming for reasoning with semantic knowledge in collaborative housekeeping robotics", "author": ["E. Aker", "V. Patoglu", "E. Erdem"], "venue": "Proc. of IFAC SYROCO", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "A theory and implementation of cognitive mobile robots", "author": ["G. De Giacomo", "L. Iocchi", "D. Nardi", "R. Rosati"], "venue": "Journal of Logic and Computation", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Towards semantic maps for mobile robots", "author": ["A. N\u00fcchter", "J. Hertzberg"], "venue": "Robot. Auton. Syst", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "A virtual sensor for room detection", "author": ["P. Buschka", "A. Saffiotti"], "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Detecting and modeling doors with mobile robots", "author": ["D. Anguelov", "D. Koller", "E. Parker", "S. Thrun"], "venue": "Proc. of the IEEE International Conference on Robotics and Automation (ICRA)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Multi-hierarchical semantic maps for mobile robotics", "author": ["C. Galindo", "A. Saffiotti", "S. Coradeschi", "P. Buschka", "J. Fern\u00e1ndez-Madrigal", "J. Gonz\u00e1lez"], "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "3D Mapping with Semantic Knowledge", "author": ["A. N\u00fcchter", "O. Wulf", "K. Lingemann", "J. Hertzberg", "B. Wagner", "H. Surmann"], "venue": "RoboCup", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Supervised learning of topological maps using semantic information extracted from range data", "author": ["O. Mart\u0131\u0301nez Mozos", "W. Burgard"], "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Building semantic annotated maps by mobile robots", "author": ["N. Goerke", "S. Braun"], "venue": "Proceedings of the Conference Towards Autonomous Robotic Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Topological mapping using spectral clustering and classification", "author": ["E. Brunskill", "T. Kollar", "N. Roy"], "venue": "Proc. of IEEE/RSJ Conference on Robots and Systems (IROS)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Voronoi random fields: Extracting the topological structure of indoor environments via place labeling", "author": ["S. Friedman", "H. Pasula", "D. Fox"], "venue": "Proc. of 19th International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Visual place categorization: Problem, dataset, and algorithm", "author": ["J. Wu", "H.I. Christenseny", "J.M. Rehg"], "venue": "Proc. of IEEE/RSJ Conference on Robots and Systems (IROS)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Categorization of indoor places using the kinect sensor. Sensors", "author": ["O.M. Mozos", "H. Mizutani", "R. Kurazume", "T. Hasegawa"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Interactive slam using laser and advanced sonar", "author": ["A. Diosi", "G. Taylor", "L. Kleeman"], "venue": "Proceedings of the IEEE International Conference on Robotics and Automation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Conceptual spatial representations for indoor mobile robots. Robotics and Autonomous Systems", "author": ["H. Zender", "O. Mart\u0131\u0301nez Mozos", "P. Jensfelt", "G. Kruijff", "W. Burgard"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Large-scale semantic mapping and reasoning with heterogeneous modalities", "author": ["A. Pronobis", "P. Jensfelt"], "venue": "Proceedings of the 2012 IEEE International Conference on Robotics and Automation (ICRA\u201912),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Semantic map partitioning in indoor environments using regional analysis", "author": ["C. Nieto-Granda", "J.G.R. III", "A.J.B. Trevor", "H.I. Christensen"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, October", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Clarification dialogues in humanaugmented mapping", "author": ["G. Kruijff", "H. Zender", "P. Jensfelt", "H. Christensen"], "venue": "Proceedings of the 1st Annual Conference on Human-Robot Interaction (HRI\u201906),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "RoboEarth, - a world wide web for robots", "author": ["M. Waibel", "M. Beetz", "J. Civera", "R. D\u2019Andrea", "J. Elfring", "D. Galvez-Lopez", "K. Haussermann", "R. Janssen", "J. Montiel", "A. Perzylo", "B. Schiessle", "M. Tenorth", "O. Zweigle", "M. van de Molengraft"], "venue": "Robotics and Automation Magazine", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Using the web to interactively learn to find objects", "author": ["M. Samadi", "T. Kollar", "M. Veloso"], "venue": "Proc. 26th Conference on Artificial Intelligence (AAAI)", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Human-robot collaboration for semantic labeling of the environment", "author": ["T.M. Bonanni", "A. Pennisi", "D.D. Bloisi", "L. Iocchi", "D. Nardi"], "venue": "Proc. of the 3rd Workshop on Semantic Perception, Mapping and Exploration (SPME)", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "A tutorial on graph-based SLAM", "author": ["G. Grisetti", "R. Kuemmerle", "C. Stachniss", "W. Burgard"], "venue": "Intelligent Transportation Systems Magazine,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "g2o: A general framework for graph optimization", "author": ["R. Kuemmerle", "G. Grisetti", "H. Strasdat", "K. Konolige", "W. Burgard"], "venue": "Proc. of the IEEE Int. Conf. on Robotics and Automation", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Our long term research goal is to improve the performance of robotic systems by addressing the above three limitations, and, specifically to devise \u201cKnowledgeable Robots\u201d [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "We propose to address the challenge of knowledgeable robots by embracing the paradigm of symbiotic autonomy [2], which relies on the interaction with the user in order to overcome the limitations of the robot in perception and action capabilities.", "startOffset": 108, "endOffset": 111}, {"referenceID": 2, "context": "The approach described in this paper builds on previous work for off-line building of a semantic map [3], based on two basic components: 1) a subsystem for Simultaneous Localization And Mapping, that provides a metric map of the environment; 2) a multimodal interface, including speech, that allows the user to point at the elements of the environment and to assign their semantic role (see Figure 1).", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "Hertzberg and Saffiotti [4] characterize semantic knowledge in robotics by two properties: (i) the need for an explicit representation of knowledge inside the robot; and (ii) the need for grounding the symbols used in this representation in real physical objects, parameters, and events.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "[5] describe the implementation of an intelligent service robot whose knowledge about the task to be executed is represented with the logic language READYLOG (a variant of Golog).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] use Answer Set Programming for representing and reasoning about the actions to be accomplished by a housekeeping robot.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] present a knowledge representation and reasoning framework based on Description Logics for a service robot.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "\u201cA semantic map for a mobile robot is a map that contains, in addition to spatial information about the environment, assignments of mapped features to entities of known classes\u201d [8].", "startOffset": 178, "endOffset": 181}, {"referenceID": 8, "context": "These works include determining attributes of rooms [9], doorways detection [10], etc.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "These works include determining attributes of rooms [9], doorways detection [10], etc.", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "[11], environmental knowledge is represented by augmenting a topological map (extracted with fuzzy morphological operators) with semantic knowledge using anchoring.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12], environmental knowledge is extracted by labeling 3D points through the gradient difference between neighboring points; points are then classified into floor-points, object-points, or ceiling-points.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "While generation of 2D topological maps from metric maps has been described in [13] and [14] (using AdaBoost), in Brunskill et al.", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "While generation of 2D topological maps from metric maps has been described in [13] and [14] (using AdaBoost), in Brunskill et al.", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "[15] (using spectral clustering), in [16] (using Voronoi random fields), etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[15] (using spectral clustering), in [16] (using Voronoi random fields), etc.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "Finally, a third set of techniques for object recognition and place categorization use visual features, such as in [17], or a combination of visual and range information, provided from an RGB-D camera, such as in [18].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "Finally, a third set of techniques for object recognition and place categorization use visual features, such as in [17], or a combination of visual and range information, provided from an RGB-D camera, such as in [18].", "startOffset": 213, "endOffset": 217}, {"referenceID": 18, "context": "[19] an interactive SLAM procedure and a watershed segmentation are employed to create a contextual topological map.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] a system to create conceptual representations of human-made indoor environments is described.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Pronobis and Jensfelt [21] present a multi-layered semantic mapping algorithm that combines information about the existence of objects and semantic properties about the space, such as room size, share and appearance.", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": "[22] adopts human augmented mapping based on a multivariate probabilistic model to associate a spatial region to a semantic label.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] introduce a system to improve the mapping process by clarification dialogues between human and robot, using natural language; Randelli et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] propose a rich multi-modal interaction, including speech, gesture, and vision enabling for a semantic labeling of environment landmarks that makes the knowledge about the environment actually usable, but without creating a suitable representation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[21], [20], [22]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21], [20], [22]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "[21], [20], [22]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "The conceptual knowledge base contains a taxonomy of the concepts involved in the environment tied by an is-a relation, as well as their properties and relations (see [11], [13]).", "startOffset": 167, "endOffset": 171}, {"referenceID": 12, "context": "The conceptual knowledge base contains a taxonomy of the concepts involved in the environment tied by an is-a relation, as well as their properties and relations (see [11], [13]).", "startOffset": 173, "endOffset": 177}, {"referenceID": 23, "context": "RoboEarth [24] and OpenEval[25]), or even querying other robots operating in similar environments.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "RoboEarth [24] and OpenEval[25]), or even querying other robots operating in similar environments.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "The image segmentation module is responsible for extracting the 3D shape of the pointed object by exploiting the RGB-D data coming from the Kinect, sensor [26].", "startOffset": 155, "endOffset": 159}, {"referenceID": 26, "context": "A Graph-based SLAM approach, as presented in [27], is used to estimate the 2D map of the environment and the trajectory of the robot.", "startOffset": 45, "endOffset": 49}, {"referenceID": 27, "context": "When the new expanded graph is complete, a least-square minimization algorithm [28] is performed to obtain the optimized final 2D metric map.", "startOffset": 79, "endOffset": 83}], "year": 0, "abstractText": "The representation of the knowledge needed by a robot to perform complex tasks is restricted by the limitations of perception. One possible way of overcoming this situation and designing \u201cknowledgeable\u201d robots is to rely on the interaction with the user. We propose a multi-modal interaction framework that allows to effectively acquire knowledge about the environment where the robot operates. In particular, in this paper we present a rich representation framework that can be automatically built from the metric map annotated with the indications provided by the user. Such a representation, allows then the robot to ground complex referential expressions for motion commands and to devise topological navigation plans to achieve the target locations.", "creator": "cairo 1.10.2 (http://cairographics.org)"}}}