{"id": "1307.8084", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jul-2013", "title": "Combining Answer Set Programming and POMDPs for Knowledge Representation and Reasoning on Mobile Robots", "abstract": "for widespread realization in domains, behind partial observability, none - existent incentives and unforeseen functions, buildings need patiently adapt sensing, locating and analyzing with emotions though provide presently demanding equilibrium. because robots operators cannot process compelling spatial arguments or capabilities without substantial sufficient knowledge, it is urgent challenge to facilitate smooth effective reasoning | contractors may not have the time wasted expertise to gather secure and accurate feedback. conceptual architecture described via this paper compares sensory code and intentional reasoning and compound semantic challenges, analyzing robots program : ( a ) perfectly and accurately establish maximal rational knowing, resolving problems and confirm existing knowledge using sensor inputs implementing complete human feedback ; and ( to ) probabilistically model the uncertainty in sensor input processing without navigation. basically, sensor set programming ( sf ), a declarative programming paradigm, is combined with hierarchical partially observable markov decision processes ( pomdps ), using internal complexity to reconcile probabilistic beliefs, and using positive and negative observations following early predictions of tasks so can only longer be trusted. all algorithms are evaluated in simulation facility on mobile smartphone locating target objects in indoor domains.", "histories": [["v1", "Mon, 29 Jul 2013 18:55:00 GMT  (808kb,D)", "http://arxiv.org/abs/1307.8084v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["shiqi zhang", "mohan sridharan"], "accepted": false, "id": "1307.8084"}, "pdf": {"name": "1307.8084.pdf", "metadata": {"source": "CRF", "title": "Combining Answer Set Programming and POMDPs for Knowledge Representation and Reasoning on Mobile Robots", "authors": ["Shiqi Zhang", "Mohan Sridharan"], "emails": ["shiqi.zhang6@gmail.com,", "mohan.sridharan@ttu.edu"], "sections": [{"heading": "1 Introduction", "text": "Mobile robots are increasingly being deployed to collaborate with humans in domains such as search and rescue, reconnaissance, and surveillance. These domains characterized by partial observability, non-determinism and unforeseen changes frequently make it difficult for robots to process all sensor inputs or operate without substantial domain knowledge. At the same time, it is a challenge to provide complete domain knowledge in advance, and humans are unlikely to have the time and expertise to provide elaborate and accurate feedback. Widespread deployment of robots in our homes, offices and other complex real-world domains thus poses formidable knowledge representation and reasoning (KRR) challenges\u2014robots need to: (a) represent, revise and reason with incomplete knowledge; (b) automatically adapt sensing and acting to the task at hand; and (c) learn from unreliable, high-level human feedback.\nAlthough there is a rich body of research on knowledge representation and reasoning, the research community is fragmented. For instance, declarative programming paradigms provide commonsense reasoning capabilities for robotics but do not support\nar X\niv :1\n30 7.\n80 84\nv1 [\ncs .A\nI] 2\n9 Ju\nl 2 01\nprobabilistic modeling of uncertainty, which is essential in robot application domains. In parallel, sophisticated algorithms based on probabilistic graphical models are being designed to model the uncertainty in sensing and navigation on robots, but it is difficult to use these algorithms for commonsense reasoning. Furthermore, algorithms developed to combine logical and probabilistic reasoning do not provide the desired expressiveness for commonsense reasoning capabilities such as non-monotonic reasoning and default reasoning. Our previous work described an architecture that combined the appealing KRR capabilities of Answer Set Programming (ASP), a declarative programming paradigm, with the probabilistic uncertainty modeling capabilities of partially observable Markov decision processes (POMDPs) [1]. This paper significantly extends the architecture by making the following contributions: \u2022 Richer representation of domain knowledge in ASP, incrementally revising the knowl-\nedge base (KB) with information obtained from sensor inputs and human feedback. \u2022 Principled generation of prior beliefs from the KB, which are merged with POMDP\nbeliefs using Bayesian updates to adapt sensing and acting to the tasks at hand. \u2022 Modeling and learning from positive and negative observations, identifying situations\nin which the current task should no longer be pursued. All algorithms are evaluated in simulated and physical robots visually localizing target objects in indoor domains. Section 2 discusses limitations of existing work to motivate algorithms described in Section 3. Section 4 presents experimental results, followed by conclusions in Section 5."}, {"heading": "2 Related Work", "text": "Algorithms based on probabilistic graphical models such as POMDPs have been used to model the uncertainty in real-world sensing and navigation, enabling the use of robots in offices and hospitals [2,3,4]. Since the rapid increase in state space dimensions of such formulations of complex problems make real-time operation difficult, researchers have developed algorithms that decompose complex problems into a hierarchy of simpler problems that are computationally tractable [3,5]. However, it is still challenging to use POMDPs and other graphical models in large, complex state-action spaces. Furthermore, these probabilistic algorithms do not readily support representation of, and reasoning with, commonsense domain knowledge.\nThere is a rich body of research on knowledge representation and reasoning. Declarative programming paradigms such as ASP provide appealing capabilities for nonmonotonic logical reasoning and default reasoning [6,7]. ASP has been used for commonsense representation and reasoning for a robot housekeeper [8], and for representing domain knowledge learned through natural language processing [9]. However, ASP does not support probabilistic modeling of the uncertainty in real-world sensing and navigation. Algorithms developed in recent years to combine logical and probabilistic reasoning do not fully exploit the complementary properties of logic programming and probabilistic models. For instance, the switching planner uses a POMDP or a DTPDDL formulation to control a robot\u2019s behavior [2], but the threshold used to switch between the two formulations may result in available information not being fully utilized. Commonsense knowledge and semantic maps have also been used in motion and task-level planning [10,11], but accurate domain knowledge or extensive human supervision may not be readily available. Principled approaches for combining first-order\nlogic and probabilistic reasoning include Markov logic networks (MLNs), which assign weights to logic formulas [12], and Bayesian Logic (BLOG), which relaxes the unique name constraint of first-order probabilistic languages to provide a compact representation of probability distributions over outcomes with varying sets of objects [13]. However, these algorithms do not provide the desired expressiveness for capabilities such as non-monotonic reasoning and default reasoning, which are important for human-robot collaboration. Our prior work demonstrated the integration of ASP with POMDPs on mobile robots [1], but did not fully support capabilities such as incremental knowledge augmentation and Bayesian belief merging. The architecture described in this paper addresses these limitations."}, {"heading": "3 Proposed Architecture", "text": "Figure 1 shows our control architecture. The ASP knowledge base (KB) represents domain knowledge (Section 3.1), while POMDPs probabilistically model a subset of the state space relevant to the task at hand (Section 3.2). Logical inference in ASP is used to: (a) compute prior beliefs relevant to the task, modeled as a Dirichlet distribution; and (b) identify eventualities not being considered by the POMDP formulation based on Beta distributions. The Dirichlet distribution supports Bayesian merging with POMDP beliefs (Section 3.3), while Beta distributions aids in the use of positive and negative observations for early termination of tasks that cannot be accomplished (Section 3.4).\nThe robot uses a learned policy to map merged POMDP beliefs to action choices. Action execution causes the robot to move and process images to obtain observations. Observations are also obtained from passive sensors (e.g., range finders) and high-level human feedback. Observations made with high certainty are (proportionately) more likely to be added into the KB, while other observations update POMDP beliefs. Human feedback is solicited based on availability and need (computed using the entropy of POMDP beliefs). Although this architecture is illustrated below in the context of robots localizing (i.e., computing the locations of) objects in indoor domains, the integration of knowledge representation, non-monotonic logical reasoning and probabilistic uncertainty modeling is applicable to many domains."}, {"heading": "3.1 Knowledge representation and reasoning with ASP", "text": "Answer Set Programming is a declarative programming paradigm that can represent recursive definitions, causal relations, and other constructs that occur in non-mathematical\ndomains and are difficult to express in classical logic formalisms [6,7]. ASP\u2019s signature is a tuple of sets: \u03a3 = \u3008O,F ,P,V \u3009 that define names of objects, functions, predicates and variables available for use. The signature is augmented by sorts (i.e., types) such as: room/1, object/1, class/1, and step/1 (for temporal reasoning). An ASP program is a collection of statements describing objects and relations between them. An answer set is a set of ground literals that represent beliefs of an agent associated with the program. ASP introduces new connectives: default negation (negation by failure) and epistemic disjunction, e.g., unlike \u201c\u00ac a\u201d (classical negation), which implies that \u201ca is believed to be false\u201d, \u201cnot a\u201d only implies that \u201ca is not believed to be true\u201d; and unlike \u201cp \u2228 \u00acp\u201d in propositional logic, \u201cp or \u00acp\u201d is not a tautology.\nIn the target localization domain, the robot learns a domain map and semantic labels for rooms (e.g., \u201ckitchen\u201d, \u201coffice\u201d). Knowledge about hierarchy of objects is learned incrementally from sensor inputs, online repositories and human feedback\u2014Figure 2 shows such a hierarchy. This hierarchy is revised using observations\u2014specific object instances (i.e., leaf nodes) can be added or removed and classes (leaf nodes\u2019 parents are primary classes) can be merged. The domain is thus not static.\nThe robot models and reasons about aspects of the domain that do not change (statics) and can change (fluents), using predicates defined in terms of sorts of their arguments: (1) is(object, class) describes class membership of an object, e.g., is(printer1, printer); (2) subclass(class, class) denotes class hierarchy; (3) in(object, room) describes the room location of an object; (4) exists(class, room), implies that an instance of a specific class exists in a specific room; and (5) holds(fluent, step) implies a fluent is true at a timestep. Predicates are applied recursively when appropriate. The KB also includes hand-coded reasoning and inertial rules such as (assuming at least two rooms in the domain):\n(1) holds(exists(C,R),I) \u2190 holds(in(O,R),I), is(O,C). (2) holds(exists(C1,R),I) \u2190 holds(exists(C2,R),I), subclass(C2,C1). (3) \u00acholds(in(O,R2),I) \u2190 holds(in(O,R1),I), R1! = R2. (4) holds(in(O,R1),I+1) \u2190 holds(in(O,R1),I), not holds(in(O,R2),I+1), R1! = R2.\nThe first rule implies that if object O of class C is in room R, it is inferred that an object of class C exists in R; the second rule applies the first rule in the object hierarchy. The last rule implies that if object O is in room R1, it will continue to be there unless it is known to be elsewhere. As an example of non-monotonic reasoning, consider the program: step(1..end). is(printer1, printer). holds(in(printer1, lab), 1).\nNow, consider adding a new statement holds(in(printer1, office), 2) about a printer\u2019s location in the second time step. The answer sets obtained by reasoning in ASP with and without this statement are shown below (existing facts not repeated):\nWith Without holds(in(printer1,lab),2). \u00acholds(in(printer1,lab),2).\nholds(exists(printer,lab),2). holds(exists(printer,office),2).\nAddition of the new statement thus revises the outcome of the previous inference step. Next, consider modeling the default: \u201cnormally robots cannot climb stairs\u201d with humanoids encoded elegantly as a weak exception:\n\u00acclmbstair(X)\u2190 robot(X), not ab(dclmbstair(X)). % Default rule robot(X)\u2190 wheeled(X). robot(X)\u2190 humanoid(X). ab(dclmbstair(X))\u2190 humanoid(X). wheeled(peoplebot). humanoid(nao). % Specific data\nwhere ab(d(X)) implies \u201cX is abnormal with respect to d\u201d. The result of inference in this example is: \u00acclmbstair(peoplebot) without making any claims about nao, i.e., it is unknown if humanoid robot nao can climb stairs or not. These capabilities are important for real-world human-robot collaboration. Inconsistencies caused by incorrect information being added to the KB are identified and corrected by processing sensor inputs or by posing queries to humans. The architecture (currently) only uses the inference capabilities of ASP to better explore the merging of qualitative and quantitative beliefs; future work will also consider ASP-based planning."}, {"heading": "3.2 Uncertainty modeling with POMDP", "text": "To localize target objects, the robot has to move and analyze images of different scenes. The uncertainty in sensor input processing and navigation is modeled using hierarchical POMDPs as shown in Figure 3. For a target object, the 3D area (e.g., multiple rooms) is modeled as a discrete 2D occupancy grid, each grid storing the probability of target existence. The visual sensing (VS)-POMDP plans an action sequence that maximizes information gain by analyzing a suitable sequence of scenes. For each scene, the scene processing (SP)-POMDP plans the processing of specific regions of images of the scene using relevant algorithms. This formulation uses our prior work on automatic belief propagation and model creation in hierarchical POMDPs [5]; some key features of the hierarchy are described briefly. Consider the VS-POMDP tuple \u3008S,A,T,Z,O,R\u3009 for locating an object in a grid with N cells: \u2022 S : {si, i \u2208 [1,N]} is the state vector; si is the event that the target is in grid cell i. \u2022 A : {ai, i \u2208 [1,N]} is the set of actions; ai causes the robot to move and analyze cell i. \u2022 T : S\u00d7A\u00d7S\u2032\u2192 [0,1] is the state transition function. \u2022 Z : {present, absent} is the observation set that indicates if the target is detected.\n\u2022 O : S\u00d7A\u00d7Z\u2192 [0,1] is the observation function (see below). \u2022 R : S\u00d7A\u00d7S\u2032\u2192 R is the reward specification (see below). Since the state is not directly observable, the robot maintains a belief state, a probability distribution over the states. In the absence of prior knowledge, the belief state is a uniform distribution. Information gain is maximized by computing a sequence of actions that causes the belief distribution to converge to likely target locations. The reward for executing action at at time t is thus defined as the reduction in entropy between belief state Bt\u22121 and the resultant belief state Bt . The observation function models the probability of each observation for each action and state as a Gaussian distribution whose mean and variance depend on the target location, cell being examined, sensor\u2019s field of view, and distance to the object\u2014robots learn this observation function (Section 4). A POMDP solver then computes a policy \u03c0 : Bt 7\u2192 at+1 in the form of a matrix of \u201cweights\u201d that is used to probabilistically select an action for any given belief state.\nExecuting an action using the VS policy causes the robot to move and analyze a specific scene, automatically creating and solving the corresponding SP-POMDP (with one or more levels). The robot uses the SP policy to apply a sequence of visual processing algorithms on a sequence of regions in images of the scene. The SP policy\u2019s terminal action causes a VS belief update and subsequent action selection. Our hierarchical decomposition uses convolutional policies and learned observation functions to support automatic belief propagation between levels of the hierarchy and real-time model and policy creation in each level. Thus, robots automatically, reliably and efficiently adapt visual processing and navigation to the task at hand [5]."}, {"heading": "3.3 Generating Prior Beliefs from Answer Sets", "text": "This section describes the generation of prior beliefs from the ASP KB. For indoor target localization, the robot computes prior belief of existence of desired objects in rooms using: (a) knowledge of the hierarchy of object classes and specific objects in the domain; and (b) postulates that capture object co-occurrence relationships. We illustrate this approach below for target localization and visual processing; some postulates (and their representation) may need to be revised for other sensors or in other domains. Postulate 1: Existence (non-existence) of objects of a primary class (in a room) provides support for the existence (non-existence) of other objects of this class (in the room). The level of support is proportional to logarithm of the number of objects, inspired by Fechner\u2019s law (psychophysics) that states that subjective sensation is proportional to logarithm of stimulus intensity:\nperception = ln(stimulus)+ const\nThis law is applicable to visual processing, the primary source of information in this paper. For a target object, support for its existence in a room is thus given by:\n\u03c8n = { 0 i f an = 0 ln(an)+\u03be otherwise\n(1)\nwhere an is the number of (known) objects of the primary class (of the target) in the room, and \u03be = 1 corresponds to const above. Certain objects are exclusive, e.g., there is typically one fridge in a kitchen. Such properties can be modeled by rules in the KB and by including other postulates (e.g., see below).\nPostulate 2: As the number of known subclasses of a class increases, the influence exerted by the subclasses on each other decreases. This computation is performed recursively in the object hierarchy from each primary class to the lowest common ancestor (LCA) of the primary class and target object. Equation 1 is thus modified as:\n\u03c8n =  0 i f an = 0 ln(an)+\u03be \u220fHnh=1 Wh otherwise (2)\nwhere Hn is the height of the LCA of the target objects and the primary class under consideration. For a class node on the path from the primary class to the LCA, Wh is the number of siblings at height h; W1 = 1. Postulate 3: Prior knowledge of existence of objects in different primary object classes independently provide support for the existence of a specific object (in a room). This postulate is used to merge evidence from different sources, and works well in practice. Prior belief of existence of a target object in room k is thus the summation of evidence from the N primary classes:\n\u03b1k = N\n\u2211 n=1 \u03c8n,k (3)\nThe prior belief of existence of the target in the set of rooms is modeled as a Dirichlet distribution with parameter set \u03b1 , where \u03b1k is computed (as above) using the cardinality of the set of relevant answer set statements obtained through ASP-based inference. The probability density function (pdf) of this K-dimensional Dirichlet distribution is:\nD(\u00b5|\u03b1) = \u0393 (\u03b10) \u0393 (\u03b11) \u00b7 \u00b7 \u00b7\u0393 (\u03b1K)\nK\n\u220f k=1\n\u00b5\u03b1k\u22121k (4)\nwhere \u0393 is the Gamma function used for normalization; \u00b5k \u2208 [0,1] is a distribution of the target\u2019s existence over the rooms; and \u03b10 = \u2211Kk=1 \u03b1k. The expectation of this Dirichlet distribution, E(\u00b5k), serves as the prior belief distribution that is to be merged with the POMDP belief distributions. If event Ek represents the target\u2019s existence in room k, the probability that the target is in room k based on the Dirichlet prior is:\np(Ek|D) = E(\u00b5k) = \u03b1k/\u03b10 (5)\nAlthough the Dirichlet distribution models the conditional probability of existence of the target in each room given its existence in the domain, it does not address the objective of learning from positive and negative observations, thus reasoning about the target\u2019s non-existence in a room or the domain. Towards this objective, parameters \u03b1 also initialize Beta distributions that model the existence of the target in each room:\nB(\u03c6k|\u03b1k,\u03b2k) = \u0393 (\u03b1k +\u03b2k) \u0393 (\u03b1k)\u0393 (\u03b2k) \u03c6 \u03b1k\u22121(1\u2212\u03c6 \u03b2k\u22121) (6)\nwhere \u0393 function is used for normalization and \u03b2k is the support for the non-existence of target in room k. The Beta distribution\u2019s expectation for room k, E(\u03c6k), is the prior belief that the target exists in room k, and event E represents the target\u2019s existence in the entire domain:\np(Ek|B) = E(\u03c6k) = \u03b1k\n\u03b1k +\u03b2k (7)\nThe probability that the target does not exist in the domain can then be derived, assuming Ei and E j are independent events \u2200i 6= j:\np(\u00acE) = \u220f k p(\u00acEk|B) = \u220f k (1\u2212 p(Ek|B)) (8)\nThe Dirichlet distribution thus directly uses the information in the answer sets to model conditional probabilities, while Beta distributions model specific (marginal) probabilities that can be updated by positive and negative observations (see below)."}, {"heading": "3.4 Using Positive and Negative Observations", "text": "Although it is a significant challenge to perform inference using lack of evidence, negative observations can be used to identify eventualities not modeled by the POMDPs, e.g., supporting early termination of tasks that cannot be accomplished. For instance, not observing the target or other related objects in multiple rooms can be used to reason about the object\u2019s absence in the domain; this is not computed in the standard POMDP model, and introducing a (special) terminal state in the POMDPs invalidates the invariance properties exploited for computational efficiency.\nThis section describes our approach to exploit all observations. Let D be the event that the target is detected; and FoV the event that target is in the robot\u2019s field of view. The objective is to calculate p(\u00acE|D) and p(\u00acE|\u00acD), and thus p(E|D) and p(E|\u00acD), given the POMDP belief state B and action a. Towards this objective, the distribution of target existence and non-existence (in the domain) are updated along with the standard POMDP belief update. The prior beliefs computed in Section 3.3 are (re)interpreted as beliefs conditioned on the existence (or non-existence) of the target in the domain. Negative Observations: If the target is not detected, this observation should not change the probability of target\u2019s existence outside the robot\u2019s field of view, p(\u00acFoV |\u00acD), which is the product of the probability that the target exists in the domain, and the probability that the target exists outside the robot\u2019s view given its existence in the domain\u2014 the latter probability can be computed from the POMDP beliefs. The reduction in the probability of target\u2019s existence in the field of view will be added to the probability of the target\u2019s non-existence in the domain:\np(\u00acE|\u00acD) = p(\u00acE)+ p(E)(p(FoV |E)\u2212 p\u2032(FoV |E)) = p(\u00acE)+ p(E)( \u2211\nsi\u2208\u039b(a) B(si)\u2212 \u2211 si\u2208\u039b(a) B\u2032(si)) (9)\nwhere B(si) and B\u2032(si) are the POMDP beliefs of state si before and after the Bayesian update using this observation, while \u039b , a function of action a, is the set of states that imply that the target is within the robot\u2019s field of view. Positive Observations: A positive observation should increase POMDP beliefs in the field of view, and decrease beliefs outside this region and the probability of target\u2019s nonexistence in the domain. The posterior probability of target\u2019s non-existence is computed based on the probabilities of the target being detected given that it exists (or does not exist) in the domain. The probability of the target being detected when it does not exist is set to be a fixed probability of false-positive observations. The probability of target being detected when it exists depends on whether it is inside or outside the robot\u2019s field of view. If the target is in the field of view, the probability of a positive observation\nis given by the POMDP observation functions; if it is outside the field of view, this probability is the probability of false-positives. We can then compute:\np(\u00acE|D) = p(D|\u00acE)p(\u00acE) p(D|E)p(E)+ p(D|\u00acE)p(\u00acE)\n(10)\nThe conditional probability of detecting the target given that it exists (does not exist) is:\np(D|E) = p(D|E,FoV )p(FoV |E)+ p(D|E,\u00acFoV )p(\u00acFoV |E) (11) = p(D|FoV )p(FoV |E)+ p(D|\u00acFoV )p(\u00acFoV |E) = \u2211\nsi\u2208\u039b(a) p(D|si,a)B(si)+ \u03b5 \u2211 si /\u2208\u039b(a) B(si)\nP(D|\u00acE) = P(D|\u00acE,FoV )P(FoV |\u00acE)+P(D|\u00acE,\u00acFoV )P(\u00acFoV |\u00acE) (12) = P(D|\u00acE,\u00acFoV ) = \u03b5 \u2211\nsi /\u2208\u039b(a) B(si)\nwhere \u03b5 is the probability of false positives, i.e., detecting a target when it does not exist, and p(D|si,a) is obtained from the POMDP observation functions."}, {"heading": "3.5 Belief Merging and Information Acquisition", "text": "The KB contains domain knowledge, including information that may not be directly relevant to the current task. The POMDP belief summarizes all observations directly relevant to the current task, but these observations are obtained with varying levels of uncertainty. Our prior work heuristically generated a belief distribution from answer sets and (weighted) averaged it with POMDP beliefs [1]. In this paper, the use of Dirichlet distribution to extract prior beliefs from answer sets supports Bayesian merging:\np\u2032(Ek) = p(Ek|D) \u00b7 p(Ek)\n\u2211i p(Ei|D) \u00b7 p(Ei) (13)\nwhere p(Ek) is the probability that target is in room k based on POMDP beliefs. Our architecture enables the robot to acquire initial domain knowledge by mining online repositories and from minimal human input. This incomplete (and possibly inconsistent) knowledge is revised using sensor inputs and high-level human feedback. If a human (specific humans are not modeled) is nearby and the belief state entropy is high (e.g., for the object being localized), the robot draws the human\u2019s attention, followed by a query about a room\u2019s accessibility or an object\u2019s existence in a room. If the entropy is low, the human is ignored except for safe navigation. The robot currently does not model the uncertainty in human feedback; information provided by humans is added to the KB, and any associated inconsistencies are identified and resolved. Verbal human-robot interaction (HRI) is based on simplistic templates such as: Robot: Where is the [object]? Human: In [room]./I do not know. Robot: Is [room] accessible? Human: Yes./No./I do not know.\nThe robot may observe unforeseen changes in object configurations and the domain, e.g., a door that was open may be closed. Such observations can be added into the KB. Unlike our prior work, the KB is thus augmented and revised continuously, adding and eliminating areas for subsequent analysis based on the task at hand."}, {"heading": "4 Experimental Results", "text": "The following hypotheses were evaluated experimentally: (H1) combining ASP and POMDP improves target localization accuracy and time in comparison with the individual algorithms; (H2) the entropy-based strategy enables a robot to make best use of human feedback; and (H3) using positive and negative observations helps robots identify tasks suitable for early termination. During the evaluation of hypotheses H1 and H2, the approach for using positive and negative observations (Section 3.4) is not included. Furthermore, our approach for generating and merging ASP and POMDP beliefs is compared with: (a) not using prior beliefs from the ASP KB; (b) using relative trust factors to merge beliefs [1]; and (c) using weights drawn from the Dirichlet distribution\u2014likely to assign higher significance to prior beliefs extracted from the KB\u2014to merge beliefs.\nExperiments included some trials on mobile robots and extensive evaluation in simulation, using object models and observation functions learned on physical robots to realistically simulate motion and perception [14]. For instance, a simulated office domain consists of four rooms connected by a surrounding hallway in a 15\u00d715 grid. Fifty objects in ten primary classes (of office electronics) were simulated, and some objects were randomly selected in each trial as targets whose positions were unknown to the robot. The robot revises the KB, including the basic object hierarchy mined from repositories, during experimental trials. Each data point in the results described below is the average of 5000 simulated trials. In each trial, the robot\u2019s location, target objects and locations of objects are chosen randomly. A trial ends when belief in a grid cell exceeds a threshold (e.g., 0.80). Some trials include a time limit (see below)."}, {"heading": "4.1 Simulation Experiments", "text": "To evaluate hypothesis H1, we measured the target localization accuracy and time when the robot used: (a) only ASP; (b) only POMDPs; and (c) ASP and POMDPs. The robot first used domain knowledge and Equations 1-4 to infer the target objects\u2019 locations\u2014 ASP-based reasoning can state if a specific object exists in a specific room but cannot provide the location of the object in the room. Figure 4(a) shows that when the robot has all the domain knowledge (except the target), i.e., value= 100 along x-axis, it can correctly infer the room containing the object. The accuracy decreases when the domain knowledge decreases, e.g., with 50% of domain knowledge, the robot can correctly identify the target\u2019s (room) location with 0.7 accuracy. However, even with 50% domain knowledge, the correct room location is in the top two choices in 95% of the trials. One key expected outcome of using ASP-based inference (and the associated beliefs) is thus the significant reduction in target localization time.\nFor any target object, once ASP identifies the room(s) likely to contain the object, POMDPs focus the robot\u2019s sensing and navigation to determine the specific location of the object. Figure 4(b) summarizes the results of these experiments as a function of the amount of domain knowledge used to generate prior beliefs. These trials have a time limit of 100 units. Trials corresponding to 0 on the x-axis represent the use of only POMDPs. Combining prior beliefs extracted from answer sets with POMDP beliefs significantly increases the target localization accuracy. Although a small amount of prior knowledge (selected randomly in simulation) can cause the robot to waste time\nexploring irrelevant locations, reducing accuracy when the time limit is exceeded and/or observations are incorrect, the belief update helps the robot recover\u2014accuracy is higher if the time limit is relaxed. As the robot acquires and uses more knowledge, localization accuracy steadily improves\u2014with all relevant domain knowledge (except the target), accuracy is 0.96, and errors correspond to trials with objects at the edge of cells.\nNext, we evaluated our approach for generating and merging beliefs. The KB is initialized with 20% domain knowledge. Periodically, information about a few randomly chosen objects is added to the KB to simulate learning from sensor inputs or human feedback. Inference in ASP produces new answer sets that are used to create prior beliefs to be merged with the POMDP beliefs, thus guiding subsequent action selection. As stated earlier, our approach is compared with three other strategies, and Figure 5(a) summarizes the results. The x-axis represents the localization error in units of grid cells in the simulated domain, while the y-axis represents % of trials with errors below a specific value. For instance, with our approach, more than 80% of the trials report an error of \u2264 5 units. These (statistically significant) results indicate that our belief generation and merging strategy results in much lower localization errors than: not using ASP, our previous approach that used weighted averaging (\u201ctrust factors\u201d) to merge beliefs [1], and the Dirichlet-based weighting scheme that assigns weights to ASP and POMDP beliefs based on the degree of correspondence with the Dirichlet distribution. Similar results were observed with different levels of (initial) knowledge\u2014assigning undue importance to ASP or POMDP beliefs can (in fact) hurt performance even when significant domain knowledge is available.\nTo evaluate H2, i.e., the entropy-based strategy to solicit human feedback, the robot is asked to localize target objects within a time limit, e.g., 100 time units. The robot uses ASP and POMDP beliefs (with Bayesian belief merging), starting with a fixed amount of domain knowledge in each trial. The robot can terminate the search if the POMDP beliefs converge, e.g., one of the cells has a belief larger than 0.8. A human appears with 0.5 probability after every action. Soliciting human feedback is modeled as taking twice as much time as (i.e., twice the cost of) a normal POMDP action. The (simulated) human\u2019s response may be correct, not useful or even incorrect. Figure 5(b) summarizes results of these experiments. When the entropy threshold is too small, the\nrobot will always ask questions (when human is nearby), consuming a considerable amount of time in acquiring information. This hurts the robot\u2019s ability to accurately localize objects within the available time. At the same time, when the entropy threshold is too high, the robot rarely asks for feedback, which also hurts localization accuracy. However, over a wide range of entropy values that represent true need for feedback\u2014 performance not very sensitive to choice of threshold\u2014the robot localizes targets with high accuracy while minimizing localization time.\nExperiments were then conducted to evaluate hypothesis H3, i.e., the use of positive and negative observations. In each trial, the KB is fixed, and the target is randomly selected to be present or absent. A baseline policy (for comparison) is designed to use ASP and POMDP beliefs (similar to experiments described above); this policy claims absence of target if it cannot be found within the time limit. Figure 6 summarizes these results. Figure 6(a) shows that exploiting positive and negative observations enables the robot to complete trials within 75 time units in 90% of the trials; trials are mostly completed much before the time limit especially when the target does not exist in the domain. Figure 6(b) also shows that exploiting positive and negative observations results in much higher target localization accuracy while significantly lowering localization time; if all observations are not exploited, comparable localization accuracy is only obtained by allowing a much larger amount of time for localization."}, {"heading": "4.2 Robot Experiments", "text": "We evaluated the architecture on a wheeled robot deployed in an indoor office domain. The robot learns (and revises) a domain map with offices, corridors, labs and common areas, e.g., Figure 7(a), and localizes the desired target objects\u2014Figure 7(b). All algorithms were implemented in the Robot Operating System (ROS). We compared our architecture with two baseline strategies: (a) only POMDP beliefs; and (b) a heuristic policy that makes greedy action choices.\nThe robot successfully localized target objects in all experimental trials\u2014there was no time limit. Since target localization times vary depending on the initial positions of robot and targets, we report target localization times of the baseline strategies as a factor of the target localization time obtained using our architecture. Over 30 trials with each target object, the localization time with just POMDP beliefs is \u2248 1.6 times (averaged over all targets) that with our architecture, while the factor is \u2248 2.4 for the heuristic policy. As observed in simulation trials, trusting ASP beliefs a lot more than POMDP beliefs reduces localization accuracy\u2014just using ASP beliefs results in trials where the targets are not found even after a long period of time. Furthermore, judicious use of human feedback enables the robot to further reduce target localization time.\nDuring the trials, the robot revises the KB and generates relevant POMDP policies in real-time. Consider a trial where the robot knows that a refrigerator and a microwave exist in the \u201ckitchen\u201d and has to localize a coffee maker. The robot merges the learned (prior) knowledge of object classes (in the answer sets) with POMDP beliefs to obtain high initial belief that the coffee maker is in the kitchen. As the robot moves to the kitchen, it meets a human but does not ask for input because the belief entropy is not high. In the office outside the kitchen, the robot detects an HP printer that had recently been moved from the floor above, and the door to an instructor\u2019s office that was closed recently. These facts, although not relevant to the current task, revise the KB. When the robot reaches the kitchen, it processes images to localize the coffee maker. If the robot now has to enter the instructor\u2019s office or find the HP printer, it uses the existing knowledge to automatically generate suitable initial belief distributions, soliciting human feedback appropriately. Videos of some trials are available online: http://youtu.be/EvY_Jt-5BqM, http://youtu.be/DqsR2qDayGQ"}, {"heading": "5 Conclusions", "text": "This paper described an architecture that integrates the knowledge representation and reasoning capabilities of ASP with the probabilistic uncertainty modeling capabilities\nof hierarchical POMDPs. Experimental results in the context of a mobile robot localizing target objects in indoor (office) domains indicates that the architecture enables robots to: (a) represent, revise and reason with incomplete domain knowledge acquired from sensor inputs and human feedback; (b) generate and merge ASP-based beliefs with POMDP beliefs to tailor sensing and navigation to the task at hand; and (c) fully utilize all observations for early termination of tasks that cannot be accomplished. Future work will further explore the planning capabilities of ASP, consider more complex application domains, and investigate the tighter coupling of declarative programming and probabilistic reasoning towards the long-term objective of reliable and efficient human-robot collaboration in real-world application domains."}], "references": [{"title": "ASP+POMDP: Integrating Non-monotonic Logical Reasoning and Probabilistic Planning on Robots", "author": ["S. Zhang", "M. Sridharan", "F.S. Bao"], "venue": "International Conference on Development and Learning and Epigenetic Robotics.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A Switching Planner for Combined Task and Observation Planning", "author": ["M. G\u00f6belbecker", "C. Gretton", "R. Dearden"], "venue": "National Conference on Artificial Intelligence (AAAI).", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards Robotic Assistants in Nursing Homes: Challenges and Results", "author": ["J. Pineau", "M. Montemerlo", "M. Pollack", "N. Roy", "S. Thrun"], "venue": "RAS Special Issue on Socially Interactive Robots. Volume 42.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning Accuracy and Availability of Humans who Help Mobile Robots", "author": ["S. Rosenthal", "M. Veloso", "A. Dey"], "venue": "National Conference on Artificial Intelligence, San Francisco", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Active Visual Sensing and Collaboration on Mobile Robots using Hierarchical POMDPs", "author": ["S. Zhang", "M. Sridharan"], "venue": "Autonomous Agents and Multiagent Systems (AAMAS).", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Knowledge Representation, Reasoning and Declarative Problem Solving", "author": ["C. Baral"], "venue": "Cambridge University Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Answer Sets", "author": ["M. Gelfond"], "venue": "In Frank van Harmelen and Vladimir Lifschitz and Bruce Porter, ed.: Handbook of Knowledge Representation. Elsevier Science", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Answer set programming for collaborative housekeeping robotics: representation, reasoning, and execution", "author": ["E. Erdem", "E. Aker", "V. Patoglu"], "venue": "Intelligent Service Robotics 5(4)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Toward open knowledge enabling for human-robot interaction", "author": ["X. Chen", "J. Xie", "J. Ji", "Z. Sui"], "venue": "Journal of Human-Robot Interaction 1(2)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploiting Probabilistic Knowledge under Uncertain Sensing for Efficient Robot Behaviour", "author": ["M. Hanheide", "C. Gretton", "R. Dearden", "N. Hawes", "J. Wyatt", "A. Pronobis", "A. Aydemir", "M. Gobelbecker", "H. Zender"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI), Barcelona, Spain", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Domain and Plan Representation for Task and Motion Planning in Uncertain Domains", "author": ["L. Kaelbling", "T. Lozano-Perez"], "venue": "IROS Knowledge Representation for Autonomous Robots Workshop.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Markov Logic Networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning 62(1)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "BLOG: Probabilistic Models with Unknown Objects", "author": ["B. Milch", "B. Marthi", "S. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "Statistical Relational Learning. MIT Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Autonomous Learning of Visual Object Models on a Robot Using Context and Appearance Cues", "author": ["X. Li", "M. Sridharan", "C. Meador"], "venue": "Autonomous Agents and Multiagent Systems (AAMAS).", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Our previous work described an architecture that combined the appealing KRR capabilities of Answer Set Programming (ASP), a declarative programming paradigm, with the probabilistic uncertainty modeling capabilities of partially observable Markov decision processes (POMDPs) [1].", "startOffset": 274, "endOffset": 277}, {"referenceID": 1, "context": "Algorithms based on probabilistic graphical models such as POMDPs have been used to model the uncertainty in real-world sensing and navigation, enabling the use of robots in offices and hospitals [2,3,4].", "startOffset": 196, "endOffset": 203}, {"referenceID": 2, "context": "Algorithms based on probabilistic graphical models such as POMDPs have been used to model the uncertainty in real-world sensing and navigation, enabling the use of robots in offices and hospitals [2,3,4].", "startOffset": 196, "endOffset": 203}, {"referenceID": 3, "context": "Algorithms based on probabilistic graphical models such as POMDPs have been used to model the uncertainty in real-world sensing and navigation, enabling the use of robots in offices and hospitals [2,3,4].", "startOffset": 196, "endOffset": 203}, {"referenceID": 2, "context": "Since the rapid increase in state space dimensions of such formulations of complex problems make real-time operation difficult, researchers have developed algorithms that decompose complex problems into a hierarchy of simpler problems that are computationally tractable [3,5].", "startOffset": 270, "endOffset": 275}, {"referenceID": 4, "context": "Since the rapid increase in state space dimensions of such formulations of complex problems make real-time operation difficult, researchers have developed algorithms that decompose complex problems into a hierarchy of simpler problems that are computationally tractable [3,5].", "startOffset": 270, "endOffset": 275}, {"referenceID": 5, "context": "Declarative programming paradigms such as ASP provide appealing capabilities for nonmonotonic logical reasoning and default reasoning [6,7].", "startOffset": 134, "endOffset": 139}, {"referenceID": 6, "context": "Declarative programming paradigms such as ASP provide appealing capabilities for nonmonotonic logical reasoning and default reasoning [6,7].", "startOffset": 134, "endOffset": 139}, {"referenceID": 7, "context": "ASP has been used for commonsense representation and reasoning for a robot housekeeper [8], and for representing domain knowledge learned through natural language processing [9].", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "ASP has been used for commonsense representation and reasoning for a robot housekeeper [8], and for representing domain knowledge learned through natural language processing [9].", "startOffset": 174, "endOffset": 177}, {"referenceID": 1, "context": "For instance, the switching planner uses a POMDP or a DTPDDL formulation to control a robot\u2019s behavior [2], but the threshold used to switch between the two formulations may result in available information not being fully utilized.", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "Commonsense knowledge and semantic maps have also been used in motion and task-level planning [10,11], but accurate domain knowledge or extensive human supervision may not be readily available.", "startOffset": 94, "endOffset": 101}, {"referenceID": 10, "context": "Commonsense knowledge and semantic maps have also been used in motion and task-level planning [10,11], but accurate domain knowledge or extensive human supervision may not be readily available.", "startOffset": 94, "endOffset": 101}, {"referenceID": 11, "context": "logic and probabilistic reasoning include Markov logic networks (MLNs), which assign weights to logic formulas [12], and Bayesian Logic (BLOG), which relaxes the unique name constraint of first-order probabilistic languages to provide a compact representation of probability distributions over outcomes with varying sets of objects [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "logic and probabilistic reasoning include Markov logic networks (MLNs), which assign weights to logic formulas [12], and Bayesian Logic (BLOG), which relaxes the unique name constraint of first-order probabilistic languages to provide a compact representation of probability distributions over outcomes with varying sets of objects [13].", "startOffset": 332, "endOffset": 336}, {"referenceID": 0, "context": "Our prior work demonstrated the integration of ASP with POMDPs on mobile robots [1], but did not fully support capabilities such as incremental knowledge augmentation and Bayesian belief merging.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "domains and are difficult to express in classical logic formalisms [6,7].", "startOffset": 67, "endOffset": 72}, {"referenceID": 6, "context": "domains and are difficult to express in classical logic formalisms [6,7].", "startOffset": 67, "endOffset": 72}, {"referenceID": 4, "context": "This formulation uses our prior work on automatic belief propagation and model creation in hierarchical POMDPs [5]; some key features of the hierarchy are described briefly.", "startOffset": 111, "endOffset": 114}, {"referenceID": 0, "context": "\u2022 T : S\u00d7A\u00d7S\u2032\u2192 [0,1] is the state transition function.", "startOffset": 14, "endOffset": 19}, {"referenceID": 0, "context": "\u2022 O : S\u00d7A\u00d7Z\u2192 [0,1] is the observation function (see below).", "startOffset": 13, "endOffset": 18}, {"referenceID": 4, "context": "Thus, robots automatically, reliably and efficiently adapt visual processing and navigation to the task at hand [5].", "startOffset": 112, "endOffset": 115}, {"referenceID": 0, "context": "where \u0393 is the Gamma function used for normalization; \u03bck \u2208 [0,1] is a distribution of the target\u2019s existence over the rooms; and \u03b10 = \u2211k=1 \u03b1k.", "startOffset": 59, "endOffset": 64}, {"referenceID": 0, "context": "Our prior work heuristically generated a belief distribution from answer sets and (weighted) averaged it with POMDP beliefs [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 0, "context": "Furthermore, our approach for generating and merging ASP and POMDP beliefs is compared with: (a) not using prior beliefs from the ASP KB; (b) using relative trust factors to merge beliefs [1]; and (c) using weights drawn from the Dirichlet distribution\u2014likely to assign higher significance to prior beliefs extracted from the KB\u2014to merge beliefs.", "startOffset": 188, "endOffset": 191}, {"referenceID": 13, "context": "Experiments included some trials on mobile robots and extensive evaluation in simulation, using object models and observation functions learned on physical robots to realistically simulate motion and perception [14].", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "These (statistically significant) results indicate that our belief generation and merging strategy results in much lower localization errors than: not using ASP, our previous approach that used weighted averaging (\u201ctrust factors\u201d) to merge beliefs [1], and the Dirichlet-based weighting scheme that assigns weights to ASP and POMDP beliefs based on the degree of correspondence with the Dirichlet distribution.", "startOffset": 248, "endOffset": 251}], "year": 2013, "abstractText": "For widespread deployment in domains characterized by partial observability, non-deterministic actions and unforeseen changes, robots need to adapt sensing, processing and interaction with humans to the tasks at hand. While robots typically cannot process all sensor inputs or operate without substantial domain knowledge, it is a challenge to provide accurate domain knowledge and humans may not have the time and expertise to provide elaborate and accurate feedback. The architecture described in this paper combines declarative programming and probabilistic reasoning to address these challenges, enabling robots to: (a) represent and reason with incomplete domain knowledge, resolving ambiguities and revising existing knowledge using sensor inputs and minimal human feedback; and (b) probabilistically model the uncertainty in sensor input processing and navigation. Specifically, Answer Set Programming (ASP), a declarative programming paradigm, is combined with hierarchical partially observable Markov decision processes (POMDPs), using domain knowledge to revise probabilistic beliefs, and using positive and negative observations for early termination of tasks that can no longer be pursued. All algorithms are evaluated in simulation and on mobile robots locating target objects in indoor domains.", "creator": "LaTeX with hyperref package"}}}