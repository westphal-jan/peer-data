{"id": "1310.5007", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2013", "title": "Online Classification Using a Voted RDA Method", "abstract": "problems demonstrate a single dual averaging method compiling convex classification operators with explicit regularization. this method carries the update equation under the regularized dual averaging ( adi ) method, but results on separate labels of the examples where complete classification assessment is made. they test fault derivative on a weight of failures made by this computation on the training set, as well as nonlinear generalization rate relation. here also introduce the concept of relative strength of regularization, and show how it produces the mistake bound without generalization coefficients. we began with the method of $ \\ ell _ 1 $ 100 algorithms dynamic regular - frame natural language processing task, and obtained state - of - the - software classification performance technique matching irregular patterns.", "histories": [["v1", "Thu, 17 Oct 2013 04:01:25 GMT  (68kb)", "http://arxiv.org/abs/1310.5007v1", "23 pages, 5 figures"]], "COMMENTS": "23 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["tianbing xu", "jianfeng gao", "lin xiao", "amelia c regan"], "accepted": true, "id": "1310.5007"}, "pdf": {"name": "1310.5007.pdf", "metadata": {"source": "CRF", "title": "Online Classification Using a Voted RDA Method", "authors": ["Tianbing Xu", "Jianfeng Gao", "Lin Xiao"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n31 0.\n50 07"}, {"heading": "1 Introduction", "text": "Driven by the Internet industry, more and more large scale machine learning problems are emerging and require efficient online solutions. An example is online email spam filtering. Each time an email arrives, we need to decide whether it is an spam or not; after a decision is made, we may receive the\ntrue value feedback information from users, and thus update the hypothesis and continue the classification in an online fashion.\nOnline methods such as stochastic gradient descent, where we update the weights based on each sample, would be a good choice. The low computational cost of online methods is associated with their slow convergence rate, which effectively introduces implicit regularization and is possible to prevent overfitting for very large scale training data [Zha04, BB08]. In practice, the optimal generalization performance of online algorithms often only require a small number of passes (epochs) over the training set.\nTo obtain better generalization performance, or to induce particular structure (such as sparsity) into the solution, it is often desirable to add simple regularization terms to the loss function of a learning problem. In the online setting, Langford et al. [LLZ09] proposed a truncated gradient method to induce sparsity in the online gradient method for minimizing convex loss functions with \u21131-regularization, Duchi and Singer [DS09] applied forwardbackward splitting method to work with more general regularizations, and Xiao [Xia10] extended Nesterov\u2019s work [Nes09] to develop regularized dual averaging (RDA) methods. In the case of \u21131 regularization, RDA often generates significantly more sparse solutions than other online methods, which match the sparsity results of batch optimization methods. Recently, Lee and Wright [LW12] show that under suitable conditions, RDA is able to identify the low-dimensional sparse manifold with high probability.\nThe aforementioned work provide regret analysis or convergence rate in terms of reducing the objective function in a convex optimization framework. For classification problems, such an objective function is a weighted sum of a loss function (such as hinge or logistic loss) and a regularization term (such as \u21132 or \u21131 norm). Since the loss function is a convex surrogate for the 0-1 loss, it is often possible to derive a classification error bound based on their regret bound or convergence rate. However, this connection between regret bound and error rate can be obfuscated by the additional regularization term.\nIn this paper, we propose a voted RDA (vRDA) method for regularized online classification, and derive its error bounds (i.e., number of mistakes made on the training set), as well as its generalization performance. We also introduce the concept of relative strength of regularization, and show how it affects the error bound and generalization performance.\nThe voted RDA method shares a similar structure as the voted perceptron algorithm [FS99], which is the combination of the perceptron algorithm [Ros58] and the leave-one-out online-to-batch conversion method [HW95]. More specifically, in the training phase, we perform the update of the RDA\nmethod only on the subsequence of examples where a prediction mistake is made. In the testing phase, we follow the deterministic leave-one-out approach, which labels an unseen example with the majority voting of all the predictors generated in the training phase. In particular, each predictor is weighted by the number of examples it survived to predict correctly in the training phase.\nThe key difference between the voted RDA method and the original RDA method [Xia10] is that voted RDA only updates its predictor when there is a classification error. In addition to numerous advantages in terms of computational learning theory [FW95], it can significantly reduce the computational cost involved in updating the predictor. Moreover, the scheme of update only on errors allows us to derive a bound on the number of classification errors that does not depend on the total number of examples.\nOur analysis on the number of mistakes made by the algorithm is based on the regret analysis of the RDA method [Xia10]. The result depends on the relative strength of regularization, which is captured by the difference between the size of the regularization term of an (unknown) optimal predictor, and the average size of the online predictors generated by the voted RDA method. In absense of the regularization term, our results matches that of the voted perceptron algorithm (up to a small constant). Moreover, our notion of relative strength of regularization and error bound analysis also applies to the voted versions of other online algorithms, including the forward-backward splitting method [DS09]."}, {"heading": "2 Regularized online classification", "text": "In this paper, we mainly consider binary classification problems. Let {(x1, y1), . . . , (xm, ym)} be a set of training examples, where each example consists of a feature vector xi \u2208 Rn and a label yi \u2208 {+1,\u22121}. Our goal is to learn a classification function f : Rn \u2192 {+1,\u22121} that attains a small number of classification errors. For simplicity, we focus on the linear predictor\nf(w, x) = sign(wTx),\nwhere w \u2208 Rn is a weight vector, or predictor. In a batch learning setting, we find the optimal predictor w that minimizes the following empirical risk\nRemp(w) = 1\nm\nm \u2211\ni=1\n\u2113(w, zi) + \u03a8(w),\nwhere \u2113(w, zi) is a loss function at sample zi = (xi, yi), and \u03a8(w) is a regularization function to prevent overfitting or induce particular structure (e.g., \u21131 norm for sparsity). If we use the 0-1 loss function\n\u2113(w, z) = 1 ( y = f(w, x) ) =\n{\n1 if y = f(w, x) 0 otherwise\nthen the total loss \u2211m\ni=1 \u2113(w, zi) is precisely the total number of classification errors made by the predictor w.\nHowever, the 0-1 loss function is non-convex and thus it is very difficult to optimize. In practice, we often use a surrogate convex function, such as the hinge loss \u2113(w, z) = max{0, (1 \u2212 ywTx)}, the logistic loss \u2113(w, z) = log2(1 + exp(\u2212ywTx)), or the exponential loss \u2113(w, z) = exp(\u2212ywTx). We note that these surrogate functions are upper bounds of the 0-1 loss, therefore the corresponding total loss\n\u2211m i=1 \u2113(w, z) is an upper bound on\nthe total number of classification errors. In a online classification setting, the training examples {z1, z2, . . . , zt, . . .} are given one at a time, and accordingly, we generate a sequence of hypotheses wt one at a time. At each time t, we make a prediction f(wt, xt) based on the previous hypothesis wt, then calculate the loss \u2113(wt, zt) based on the true label yt. The next hypothesis wt+1 is updated according to some rules, e.g., online gradient descent [Zin03], based on the information available up to time t. To simplify notation in the online setting, we use a subscript to indicate the loss function at time t, i.e., we write \u2113t(wt) = \u2113(wt, zt) henceforth.\nThe performance of an online learning algorithm is often measured with the notion of regret, which is the difference between the total loss of the online algorithm \u2211\nt \u2113t(wt), and the total cost \u2211\nt \u2113t(w) for a fixed w (which can only be computed from hindsight). With an additional regularized function \u03a8, the regret with respect to w, after T steps, is defined as\nRT (w) \u2261 T \u2211\nt=1\n(\u2113t(wt) + \u03a8(wt))\u2212 T \u2211\nt=1\n(\u2113t(w) + \u03a8(w))\nWe want the regret to be as small as possible when compared with any fixed w. In the rest of this paper, we assume that all the loss functions \u2113t(w) and regularization functions \u03a8(w) are convex in w.\n3 The voted RDA method\nAlgorithm 1 The voted RDA method (training) input: training set {(x1, y1), . . . , (xm, ym)},\nand number of epochs N initialize: k \u2190 1, w1 \u2190 0, c1 \u2190 0, s0 \u2190 0 repeat\nfor i = 1, . . . , m do compute prediction: y\u0302 \u2190 f(wk, xi) if y\u0302 = yi then\nck \u2190 ck + 1 else\ncompute subgradient gk \u2208 \u2202\u2113i(wk) sk \u2190 sk\u22121 + gk update wk+1 according to Eq. (1) ck+1 \u2190 1 k \u2190 k + 1\nend if end for\nuntil N times output: number of mistakes M , and a list of\npredictors {(w1, c1), . . . , (wM , cM)}\nAlgorithm 2 The voted RDA method (testing) given: weighted predictors {(w1, c1), . . . , (wM , cM)} input: an unlabeled instance x output: a predicted label y\u0302 given by:\ny\u0302 = sign ( \u2211M k=1 ck f(wk, x) )\n(3)\nThe voted RDA method is described in Algorithm 1 and Algorithm 2, for training and testing respectively. The structure of the algorithm description is very similar to the voted perceptron algorithm [FS99]. In the training phase (Algorithm 1), we go through the training set N times, and only update the predictor when it makes a classification error. Each predictor wk is associated with a counter ck, which counts the number of examples it processed correctly. These counters are then used in the testing module (Algorithm 2) as the voting weights to generate a prediction on an unlabeled example.\nThe update rule used in Algorithm 1 takes the same form as the RDA method [Xia10]:\nwk+1 = argmin w\n{\n1 k sTkw +\u03a8(w) + \u03b2k k h(w)\n}\n, (1)\nwhere \u03a8(w) is the convex regularization function, h(w) is an auxiliary strongly convex function, and\n\u03b2k = \u03b7 \u221a k, \u2200 k \u2265 1, (2)\nwhere \u03b7 > 0 is a parameter that controls the learning rate. Note that k is the number of classification mistakes, sk is the summation of subgradients for the k samples with classification mistakes, and ck is the counter of survival times for the predictor wk.\nFor example, with \u21131-regularization, we use\n\u03a8(w) = \u03bb\u2016w\u20161, h(w) = 1\n2 \u2016w\u201622.\nIn this case, the update rule (1) has a closed-form solution that employs the shrinkage (soft-thresholding) operator:\nwk+1 = \u2212 \u221a k\n\u03b7 shrink\n(\n1 k sk, \u03bb\n)\n. (4)\nFor a given vector g and threshold \u03bb, the shrinkage operator is defined coordinate-wise as\n(shrink(g, \u03bb))(i) =\n\n\n g(i) \u2212 \u03bb if g(i) > \u03bb, 0 if |g(i)| \u2264 \u03bb, g(i) + \u03bb if g(i) < \u2212\u03bb,\nfor i = 1, . . . , n. Closed-form solutions for other regularization functions can be found, e.g., in Duchi and Singer [DS09] and Xiao [Xia10].\nFor large scale problems, storing the list of predictors {(w1, c1), . . . , (wM , cM )} and computing the majority vote in (3) can be very costly. For linear predictors (i.e., y\u0302 = sign(wTx)), we can replace the majority vote with a single prediction made by the weighted average predictor w\u0303M = (1/M) \u2211M k=1 ckwk,\ny\u0302 = sign ( w\u0303TMx ) = sign\n(\n1\nM\nM \u2211\nk=1\nck(w T k x)\n)\n.\nIn practice, this weighted average predictor generates very similar robust performance as the majority vote [FS99], and saves lots of memory and computational cost."}, {"heading": "4 Bound on the number of mistakes", "text": "We provide an analysis of the voted RDA method for the case N = 1 (i.e., going through the training set once). The analysis parallels that for the voted perceptron algorithm given in Freund and Schapire [FS99]. In this section, we bound the number of mistakes made by the voted RDA method through its regret analysis. Then in the next section, we give its expected error rate in an online-to-batch conversion setting.\nFirst, we recognize that the voted RDA method is equivalent to running the RDA method [Xia10] on the subsequence of training examples where a classification mistake is made. Let M the number of prediction mistakes made by the algorithm after processing the m training examples, and i(k) denote the index of the example on which the k-th mistake was made (by wk). The regret of the algorithm, with respect to a fixed vector w, is defined only on the examples with prediction errors:\nRM (w) =\nM \u2211\nk=1\n( \u2113i(k)(wk) + \u03a8(wk) ) \u2212 M \u2211\nk=1\n( \u2113i(k)(w) + \u03a8(w) ) . (5)\nAccording to Theorem 1 of Xiao [Xia10], the RDA method (1) has the following regret bound:\nRM (w) \u2264 \u03b2Mh(w) + G2\n2\nM \u2211\nk=1\n1\n\u03b2k ,\nwhere G is an upper bound on the norm of the subgradients, i.e., \u2016gk\u20162 \u2264 G for all k = 1, . . . ,M . For simplicity of presentation, we restrict to the case of h(w) = (1/2)\u2016w\u201622 in this paper. If we choose \u03b2k as in (2), then, by Corollary 2 of Xiao [Xia10],\nRM (w) \u2264 ( \u03b7\n2 \u2016w\u201622 +\nG2\n\u03b7\n)\u221a M.\nThis bound is minimized by setting \u03b7 = \u221a 2G/\u2016w\u20162, which results in\nRM (w) \u2264 \u221a 2G\u2016w\u20162 \u221a M. (6)\nTo bound the number of mistakes M , we use the fact that the loss functions \u2113i(w) are surrogate (upper bounds) for the 0-1 loss. Therefore,\nM \u2264 M \u2211\nk=1\n\u2113i(k)(wk).\nCombining the above inequality with the definition of regret in (5) and the regret bound (6), we have\nM \u2264 M \u2211\nk=1\n\u2113i(k)(w) +M\u03bb\u2206(w) + \u221a 2G\u2016w\u20162 \u221a M. (7)\nwhere \u2206(w) is the relative strength of regularization, defined as\n\u2206(w) = \u03a8(w)\u2212 1 M\nM \u2211\nk=1\n\u03a8(wk). (8)\nWe can also further relax the bound by replacing \u2206(w) with \u2206\u0304(w), defined as \u2206\u0304(w) = \u03a8(w)\u2212\u03a8(w\u0304M ), where w\u0304M = 1M \u2211M k=1wk is the (unweighted) average of the predictors generated by the algorithm. Note that by convexity of \u03a8, we have \u2206(w) \u2264 \u2206\u0304(w)."}, {"heading": "4.1 Analysis for the separable case", "text": "Our analysis for the separable case is based on the hinge loss \u2113i(w) = max{0, 1\u2212 yi(wTxi)}.\nAssumption 1 There exists a vector u such that yi(uTxi) \u2265 1 for all i = 1, . . . ,m.\nThis is the standard separability with large margin assumption. Under this assumption, we have\nM \u2211\nk=1\n\u2113i(k)(u) =\nM \u2211\nk=1\nmax{0, 1 \u2212 yi(k)(uTxi(k))} = 0\nfor any M > 0 and any subsequence {i(k)}Mi=1. The margin of separability is defined as \u03b3 = 1/\u2016u\u20162. For convenience, we also let\nR = max i=1,...,m\n\u2016xi\u20162 .\nThen we can set G = R since for hinge loss, \u2212yixi is the subgradient of \u2113i(w), and we have \u2016\u2212 yixi\u20162 = \u2016xi\u20162 \u2264 R for i = 1, . . . ,m. We have the following results under Assumption 1:\n\u2022 If \u03bb = 0 (the case without regularization), then M \u2264 \u221a 2G\u2016u\u20162 \u221a M ,\nwhich implies\nM \u2264 2G2\u2016u\u201622 = 2 ( R\n\u03b3\n)2\n.\nThis is very similar to the mistake bound for the voted perceptron [FS99], with an extra factor of two. Note that this bound is independent of the dimension n and the number of examples m. It also holds for N > 1 (multiple passes over the data).\n\u2022 If \u03bb > 0, the mistake bound also depends on \u2206(u), which is the difference between \u03a8(u) and the unweighted average of \u03a8(w1), . . . ,\u03a8(wM ). More specifically,\nM \u2264 M\u03bb\u2206(u) + \u221a 2R\u2016u\u20162 \u221a M. (9)\nNote that \u03a8(w1), . . . ,\u03a8(wM ) tend to be small for large values of \u03bb (more regularization), and tend to be large for small values of \u03bb (less regularization). We discuss two scenarios:\nThe under-regularization case: \u2206(u) < 0. This happens if the regularization parameter \u03bb is chosen too small, and the generated vectors\nw1, . . . , wM on average has a larger value of \u03a8 than \u03a8(u). In this case, we have\nM \u2264 2 (\n1\n1 + \u03bb|\u2206(u)|\n)2 (R\n\u03b3\n)2\n.\nSo we have a smaller mistake bound than the case of \u201cperfect\u201d regularization (when \u2206(u) = 0). This effect may be related to over-fitting on the training set.\nThe over-regularization case: \u2206(u) > 0. This happens if the regularization parameter \u03bb is chosen too large, and the generated vectors w1, . . . , wM on average has a smaller \u03a8 value than \u03a8(u). If in addition \u03bb|\u2206(u)| < 1, then we have\nM \u2264 2 (\n1\n1\u2212 \u03bb|\u2206(u)|\n)2 (R\n\u03b3\n)2\n,\nwhich can be much larger than the case of \u201cperfect\u201d regularization (meaning \u2206(u) = 0). If \u03bb\u2206(u) \u2265 1, then the inequality (9) holds trivially and does not give any meaningful mistake bound."}, {"heading": "4.2 Analysis for the inseparable case", "text": "We start with the inequality (7). To simplify notation, let L(u) denote the total loss of an arbitrary vector u over the subsequence i(k), k = 1, . . . ,M , i.e.,\nL(u) =\nM \u2211\nk=1\n\u2113i(k)(u). (10)\nThen we have\nM \u2264 L(u) +M\u03bb\u2206(u) + \u221a 2R\u2016u\u20162 \u221a M. (11)\nOur analysis is similar to the error analysis for the perceptron in [SS11]. which relies on the following simple lemma: Lemma 1 Given a, b, c > 0, the inequality ax\u2212 b\u221ax\u2212 c \u2264 0 implies\nx \u2264 c a +\n(\nb\na\n)2\n+ b\na\n\u221a\nc a \u2264\n( \u221a\nc a + b a\n)2\n.\nHere are the case-by-case analysis:\n\u2022 If \u03bb = 0 (the case without regularization), we have\nM \u2264 L(u) + \u221a 2R\u2016u\u20162 \u221a M,\nwhich results in\nM \u2264 ( \u221a L(u) + \u221a 2R\u2016u\u20162 )2 .\nNote that this bound only makes sense if the total loss L(u) is not too large.\n\u2022 If \u03bb > 0, the mistake bound depends on \u2206(u), the relative strength of regularization.\nThe under-regularization case: \u2206(u) < 0. we have\nM \u2264 ( \u221a\nL(u) 1 + \u03bb|\u2206(u)| + \u221a 2R\u2016u\u20162 1 + \u03bb|\u2206(u)| )2 .\nThe over-regularization case: \u2206(u) > 0. If \u03bb|\u2206(u)| < 1, we have\nM \u2264 ( \u221a\nL(u) 1\u2212 \u03bb|\u2206(u)| + \u221a 2R\u2016u\u20162 1\u2212 \u03bb|\u2206(u)| )2 .\nAgain, if \u03bb\u2206(u) \u2265 1, the inequality (11) holds trivially and does not lead to any meaningful bound.\nTheorem 1 Let {(x1, y1), . . . , (xm, ym)} be a sequence of labeled examples with \u2016xi\u20162 \u2264 R. Suppose the voted RDA method (Algorithm 1) makes M prediction errors on the subsequence i(1), . . . , i(M), and generates a sequence of predictors w1, . . . , wM . For any vector u, let L(u) be the total loss defined in (10), and \u2206(u) be the relative strength of regularization defined in (8). If \u03bb\u2206(u) < 1, then the number of mistakes M is bounded by\nM \u2264 ( \u221a L(u) 1\u2212 \u03bb\u2206(u) + \u221a 2R\u2016u\u20162 1\u2212 \u03bb\u2206(u) )2 .\nIn particular, if the training set satisfies Assumption 1, then we have\nM \u2264 2 (\n1\n1\u2212 \u03bb\u2206(u)\n)2 (R\n\u03b3\n)2\n,\nwhere \u03b3 = 1/\u2016u\u20162 is the separation margin.\nThe above theorem is stated in the context of using the hinge loss. However, the analysis for the inseparable case holds for other convex surrogate functions as well, including the hinge loss, logistic loss and exponential loss. We only need to replace R with a constant G, which satisfies G \u2265 \u2016gk\u20162 for all k = 1, . . . ,M .\nFor a strongly convex regularizer such as \u03a8(w) = (\u03bb/2)\u2016w\u201622, the regret bound is on the order of logM [Xia10]. Thus, for any hypothesis u, the training error bound can be derived from\nM(1\u2212 \u03bb\u2206(u)) \u2264 G\u2016u\u20162 logM + L(u).\nOnline SVM is a special case following the above bound with hinge loss and \u21132 regularizer."}, {"heading": "5 Online-to-batch conversion", "text": "The training part of the voted RDA method (Algorithm 1) is an online algorithm, which makes a small number of mistakes when presented with examples one by one (see the analysis in Section 4). In a batch setting, we can use this algorithm to process the training data one by one (possibly going through the data multiple times), and then generate a hypothesis which will be evaluated on a separate test set.\nFollowing Freund and Schapire [FS99], we use the deterministic leaveone-out method for converting an online learning algorithm into a batch learning algorithm. Here we give a brief description. Suppose we have m training examples and an unlabeled instance, all generated i.i.d. at random. Then, for each r \u2208 {0,m}, we run the online algorithm on a sequence of r+1 examples consisting of the first r examples in the training set and the last one being the unlabeled instance. This produces m + 1 predictions for the unlabeled instance, and we take the majority vote of these predictions.\nIt is straightforward to see that the testing module of the voted RDA method (Algorithm 2) outputs exactly such a majority vote, hence the name \u201cvoted RDA.\u201d Our result is a direct corollary of a theorem from Freund and Schapire [FS99], which is a result of the theory developed in Helmbold and Warmuth [HW95].\nTheorem 2 [FS99] Assume all examples {(xi, yi)}i\u22651 are generated i.i.d. at random. Let E be the expected number of mistakes that an online algorithm makes on a randomly generated sequence of m + 1 examples. Then\ngiven m random training examples, the expected probability that the deterministic leave-one-out conversion of this online algorithm makes a mistake on a randomly generated test instance is at most 2E/(m + 1).\nCorollary 1 Assume all examples are generated i.i.d. at random. Suppose that we run Algorithm 1 on a sequence of examples {(x1, y1), . . . , (xm+1, ym+1)} and M mistakes occur on examples with indices i(1), . . . , i(M). Let \u2206(u) and L(u) be defined as in (8) and (10), respectively.\nNow suppose we run Algorithm 1 on m examples {(x1, y1), . . . , (xm, ym)} for a single epoch. Then the probability that Algorithm 2 does not predict ym+1 on the test instance xm+1 is at most\n2\nm+1 E\n\n inf u: 1\u2212\u03bb\u2206(u)>0\n( \u221a\nL(u) 1\u2212\u03bb\u2206(u) + \u221a 2R\u2016u\u20162 1\u2212\u03bb\u2206(u) )2   .\n(The above expectation E[\u00b7] is over the choice of all m+1 random examples.)"}, {"heading": "6 Experiments on parse reranking", "text": "Parse reranking has been widely used as a test bed when adapting machine learning algorithms to natural language processing (NLP) tasks; see, e.g., Collins [Col00], Charniak and Johnson [CJ05], Gao et al. [GAJT07] and Andrew and Gao [AG07]. Here, we briefly describe parse reranking as an online classification problem, following Collins [Col00]. At each time t, we have a sentence st from collection of sentences S. A NLP procedure is used to generate a set of candidate parses (Ht) for the sentence, and introduce a\nfeature mapping \u03c6(s, h) : S \u00d7 H \u2192 Rn from the sentence and candidate parse to a n-dimensional feature vector. For each st, we rank the different candidate parses based on the linear score with a weight vector w, and select the best parse as the one with the largest score, i.e.,\nh\u0302t = argmax h\u2208Ht\nwT\u03c6(st, h). (12)\nIn the training data, we already know the oracle parse h\u2217t for st. If the best parse selected based on (12) is the same as h\u2217t , we have a correct classification; otherwise, we have a wrong classification and need to update the predictor w.\nTo fit into the binary classification framework, we need to identify the best candidate parse other than h\u2217t , i.e., let\nh\u0303t = argmax h\u2208Ht\\{h\u2217t }\nwT\u03c6(st, h).\nThen we define the feature vector for each sentence as\nxt = \u03c6(st, h \u2217 t )\u2212 \u03c6(st, h\u0303t).\nTherefore, if there is a classification error (h\u0302t 6= h\u2217t ), we have h\u0303t = h\u0302t and wTt xt < 0. Otherwise, if the classification is correct, we have h\u0303t 6= h\u0302t = h\u2217t and wTxt \u2265 0. In summary, the binary classifier is defined as\nf(w, xt) =\n{\n+1 if wTxt \u2265 0, \u22121 if wTxt < 0.\nNote that wTxt > 0 gives the notion of a positive margin when the classification is correct.\nWith the above definitions, all training examples has \u201clabel\u201d yt = +1. Correspondingly, when there is a classification error (i.e., wTxt < 0), the hinge loss is\n\u2113t(w) = max{0, 1\u2212 yt(wTxt)} = max{0, 1\u2212wTxt}\nSimilarly, the log loss is \u2113t(w) = log(1 + exp(\u2212wTxt)). We follow the experimental paradigm of parse reranking outlined in Charniak and Johnson [CJ05]. We used the same generative baseline model for generating candidate parsers, and nearly the same feature set, which includes the log probability of a parse according to the baseline model and 1,219,272 additional features. We trained the predictor on Sections 2-19 of\nthe Penn Treebank [MSM93], used Section 20-21 to optimize training parameters, including the regularization weight \u03bb and the learning rate \u03b7, and then evaluated the predictors on Section 22. The training set contains 36K sentences, while the development set and the test set have 4K and 1.7K, respectively. Performance of parsing reranking is measured with the PARSEVAL metric, i.e., F-Score over labelled brackets. For each epoch, we have the F-Score based on the corresponding weights learned from these samples. We use the weighted average of all the predictors generated by the algorithm as the final predictor for testing.\nComparison with Perceptron and TG Our main results are summarized in Tables 1, the F-Score and NNZ are averaged over the results of 20 epoches of online classification. The baseline results are obtained by the parser in Charniak [Cha00]. The implementation of perceptron follows the averaged perceptron algorithm [Col02]. For voted RDA, we report results of the predictors trained using the parameter settings tuned on the development set. We used \u03b7 = 0.05 and \u03bb = 1e \u2212 5 for hinge loss, and \u03b7 = 1000 and \u03bb = 1e \u2212 4 for log loss. Results show that compared to perceptron, voted RDA achieves similar or better F-Scores with more sparse weight vectors. For example, using log loss we are able to achieve an F-score of 0.9174 with only 14% of features. TG is the truncated gradient method [LLZ09]; our vRDA is a better choice than TG in term of the classificaiton performance and sparsity especially for log loss.\nSparsity and Performance Trade Off Since its ability to learn a sparse structured weight vector is an important advantage of voted RDA, we exam in detail how the number of non-zero weights changes during the course of training in Figure 1. In vRDA, the regularization parameter \u03bb controls the model sparsity. For a stronger \u21131 regularizer with large values of \u03bb, it ends up with a simpler model with fewer number of nonzero (NNZ) feature weights; for a weaker regularizer, we will get a more complex model with many more nonzero features weights. From the Figure 1, we may observe the convergence of the online learning along with the number of samples. With a relatively larger value of \u03bb, the simpler model is easy to converge to stationary states with a small number of nonzero feature weights; while for a smaller \u03bb, we have more nonzero feature weights and it will take many more samples for the model to reach stable states.\nFigure 2 illustrates the trade-off between model sparsity and classification performance when we adjust the regularization parameter \u03bb. For hinge loss, with a larger \u03bb, we get more sparse model at the price of a worse F-Score. For the log loss, as is showed in Figure 2, it is able to prevent\noverfitting to some extent. On average, it achieves the best classification performance with average F-Score 0.9174 with the 173K (out of 1.2M) feature chosen by the sparse predictor.\nTraining Errors In Figure 3, we plot the number of mistakes as a function of the number of training samples from voted RDA and perceptron. The results provide empirical justifications of the theoretical analysis on error bounds described in Section 4. First, we observed that the number of training errors grows sub-linearly with the number of training samples. Secondly, as predicted by our analysis, voted RDA without regularization (\u03bb = 0) leads to less training error than perceptron, but would incur more errors from more regularization (\u03bb > 0).\nSingle vs Average Prediction To investigate where the performance gain comes from, we compare the predictions of vRDA by single weight trained at the last sample of each epoch, and the averaged weights learned from all the training samples. In Figure 4, we plot the mean and variance bars with the corrpesonding predictions based on weights trained on 10 epoches. For both Hinge and Log losses, the average predictions have lower variance and better F-Score compared to their single predictions. The large variance for single predictions of Log loss implies that the predictions are quite inconsistent by different epoches samples; thus average predictions is highly desired here.\nConservative Updates Here, in Figure 5 we compare the performance of RDA and vRDA to illustrate the trade-off by conservative updates with mean and variance bars based on 10 epoches. For Hinge loss, the conservative updates (vRDA) is necessary as the hinge loss is 0 when there is classificaiton mistake; thus vRDA has better F-Score. While for Log loss, RDA is better as even there is a classification mistake, we still has a non-zero loss and need to update the weights accordingly. Another gain by conservative updates is from computational perspective. For vRDA, the frequency ratio of updating weights is amount to the error rate of that of RDA. From our experiments, the training time of vRDA is about 89.7% for Hinge loss and 87.2% for Log loss of RDA. These precentages are not the error rate as there are extra common computaions involved."}, {"heading": "7 Conclusion and Discussions", "text": "In this paper, we propose a voted RDA (vRDA) method to address online classification problems with explicity regularization. This method updates the predictor only on the subsequence of training examples where a classification error is made. In addition to significantly reducing the computational cost involved in updating the predictor, this allows us to derive a mistake bound that does not depend on the total number of examples. We also introduce the concept of relative strength of regularization, and show how it affects the mistake bound and the generalization performance.\nOur analysis on mistake bound is based on the regret analysis of the RDA method [Xia10]. In fact, our notion of relative strength of regularization and error bound analysis also applies to the voted versions of other online algorithms that admit a similar regret analysis, including the forward-backward splitting method in [DS09].\nWe tested the voted RDA method with \u21131-regularization on a large-scale parse reranking task in natural language processing, and obtained state-ofthe-art classification performance with fairly sparse models."}], "references": [{"title": "Scalable training of l1regularized log-linear models", "author": ["Galen Andrew", "Jianfeng Gao"], "venue": "In Proceedings of the 24th International Conference on Machine learning(ICML", "citeRegEx": "Andrew and Gao.,? \\Q2007\\E", "shortCiteRegEx": "Andrew and Gao.", "year": 2007}, {"title": "The tradeoffs of large scale learning", "author": ["L\u00e9on Bottou", "Olivier Bousquet"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bottou and Bousquet.,? \\Q2008\\E", "shortCiteRegEx": "Bottou and Bousquet.", "year": 2008}, {"title": "A maximum-entropy-inspired parser", "author": ["Eugene Charniak"], "venue": "In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference (NAACL", "citeRegEx": "Charniak.,? \\Q2000\\E", "shortCiteRegEx": "Charniak.", "year": 2000}, {"title": "Coarse-to-fine n-best parsing and maxent discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics(ACL", "citeRegEx": "Charniak and Johnson.,? \\Q2005\\E", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "Discriminative re-ranking for natural language parsing", "author": ["Michael Collins"], "venue": "In Proceedings of the 17th International Conference on Machine learning(ICML),", "citeRegEx": "Collins.,? \\Q2000\\E", "shortCiteRegEx": "Collins.", "year": 2000}, {"title": "Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proceedings of the ACL-02 conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["John Duchi", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi and Singer.,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer.", "year": 2009}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Freund and Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1999}, {"title": "Sample compression, learnability, and the vapnik-chervonenkis dimension", "author": ["Sally Floyd", "Manfred K. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Floyd and Warmuth.,? \\Q1995\\E", "shortCiteRegEx": "Floyd and Warmuth.", "year": 1995}, {"title": "On weak learning", "author": ["David P. Helmbold", "Manfred K. Warmuth"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Helmbold and Warmuth.,? \\Q1995\\E", "shortCiteRegEx": "Helmbold and Warmuth.", "year": 1995}, {"title": "Sparse online learning via truncated gradient", "author": ["John Langford", "Lihong Li", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "Manifold identication in dual averaging for regularized stochastic online learning", "author": ["Sangkyun Lee", "Stephen J. Wrigh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lee and Wrigh.,? \\Q2012\\E", "shortCiteRegEx": "Lee and Wrigh.", "year": 2012}, {"title": "Building a large annotated corpus of english: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Yurii Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov.,? \\Q2009\\E", "shortCiteRegEx": "Nesterov.", "year": 2009}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["Frank Rosenblatt"], "venue": "Psychological Review,", "citeRegEx": "Rosenblatt.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt.", "year": 1958}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["Lin Xiao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Xiao.,? \\Q2010\\E", "shortCiteRegEx": "Xiao.", "year": 2010}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In International Conference on Machine learning (ICML", "citeRegEx": "Zhang.,? \\Q2004\\E", "shortCiteRegEx": "Zhang.", "year": 2004}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [], "year": 2013, "abstractText": "We propose a voted dual averaging method for online classification problems with explicit regularization. This method employs the update rule of the regularized dual averaging (RDA) method, but only on the subsequence of training examples where a classification error is made. We derive a bound on the number of mistakes made by this method on the training set, as well as its generalization error rate. We also introduce the concept of relative strength of regularization, and show how it affects the mistake bound and generalization performance. We experimented with the method using l1-regularization on a large-scale natural language processing task, and obtained state-of-the-art classification performance with fairly sparse models.", "creator": "LaTeX with hyperref package"}}}