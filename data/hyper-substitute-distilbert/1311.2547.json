{"id": "1311.2547", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2013", "title": "Learning Mixtures of Linear Classifiers", "abstract": "we consider a comparison ( subjective decision ) problem, whereby all regression function is a convex combination whose $ k $ 3 ones. existing approaches are based on the em rule, yielding optimization ideas, without financial guarantees. we develop a simple method largely on numerical techniques and see ` implicit'strategy, that discovers these subspace spanned by the responses'imaginary squares. under a probabilistic assumption on orthogonal inversion and multiplication, one write that each approach promotes nearly polynomial statistical efficiency.", "histories": [["v1", "Mon, 11 Nov 2013 19:50:51 GMT  (115kb)", "http://arxiv.org/abs/1311.2547v1", null], ["v2", "Fri, 11 Apr 2014 18:52:34 GMT  (127kb)", "http://arxiv.org/abs/1311.2547v2", null], ["v3", "Mon, 14 Jul 2014 18:26:20 GMT  (128kb)", "http://arxiv.org/abs/1311.2547v3", null], ["v4", "Wed, 30 Jul 2014 23:40:04 GMT  (134kb)", "http://arxiv.org/abs/1311.2547v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yuekai sun", "stratis ioannidis", "andrea montanari"], "accepted": true, "id": "1311.2547"}, "pdf": {"name": "1311.2547.pdf", "metadata": {"source": "META", "title": "Learning Mixtures of Linear Classifiers", "authors": ["Yuekai Sun", "Andrea Montanari", "Yi"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n31 1.\n25 47\nv1 [\ncs .L\nG ]\n1 1\nN ov"}, {"heading": "1. Introduction", "text": "Since Pearson\u2019s seminal contribution [17], and most notably after the introduction of the EM algorithm [7], mixture models and latent variable models have played a central role in statistics and machine learning, with numerous applications\u2014see, e.g., McLachlan & Peel [14], Bishop [4], and Bartholomew et al. [3]. Despite their ubiquity, fitting the parameters of a mixture model remains a challenging task. The most popular methods (e.g., the EM algorithm or likelihood maximization by gradient ascent) are plagued by local optima and come with little or no guarantees. Computationally efficient algorithms with provable guarantees are an exception in this area. Even the idealized problem of learning mixtures of Gaussians has motivated a copious theoretical literature [2, 15].\nIn this paper we consider the problem of modeling a regression function as a mixture of k components. Namely, we are given labels Yi \u2208 R and feature vectors Xi \u2208 Rd, i \u2208 [n] \u2261 {1, 2, . . . , n}, and we seek estimates of the parameters of a mixture model\nYi \u2223\u2223 Xi=xi \u223c\u2211k\u2113=1 p\u2113 f(yi|xi, u\u2113) . (1)\nHere k is the number of components, (p\u2113)\u2113\u2208[k] are weights of the components, and u\u2113 is a vector of parameters for the \u2113-th component. Models of this type have been intensely studied in the neural network literature since the early nineties [4, 10]. They have also found numerous applications ranging from object recognition [18] to machine translation [12]. These studies are largely based on learning algorithms without consistency guarantees.\nRecently, Chaganty and Liang [5] considered mixtures of linear regressions, whereby the relation between labels and feature vectors is linear within each\n1\ncomponent; i.e., Yi = \u3008u\u2113, Xi\u3009+noise (here and below \u3008a, b\u3009 denotes the standard scalar product in Rm). Equivalently, f(yi|xi, u\u2113) = f0(yi\u2212\u3008xi, u\u2113\u3009) with f0( \u00b7 ) a density of mean zero. Building on a new approach developed by Hsu et al. [9] and Anandkumar et al. [1], these authors propose an algorithm for fitting mixtures of linear regressions with provable guarantees. The basic idea is to regress Y qi , for q \u2208 {1, 2, 3} against the tensors Xi, Xi\u2297Xi, Xi\u2297Xi\u2297Xi. The coefficients of these regressions are tensors whose decomposition gives rise to the parameters u\u2113, p\u2113.\nWhile the work of Chaganty and Liang [5] is a significant step forward, it leaves several open problems:\nStatistical efficiency. Let us assume a standard scaling of the feature vectors, whereby the components (Xi,j)j\u2208[p] are of order one. Then, the mathematical guarantees of Chaganty & Liang [5] require a sample size n \u226b d6. This is substantially larger than the \u2018information-theoretic\u2019 optimal scaling, and is an unrealistic requirement in high-dimension (large d). As noted in [5], this scaling is an intrinsic drawback of the tensor approach which requires working in a higher-dimensional space (tensor space) than the space in which data naturally live.\nLinear regression versus classification. In virtually all applications of the mixture model (1), the labels Yi are categorical\u2014see, e.g., Jordan & Jacobs [10], Bishop [4], Quattoni et al. [18], Liang et al. [12]. In this case, the very first step of Chaganty & Liang, namely, regressing Y 2i on X \u22972 i and Y 3 i onX \u22973 i , breaks down. Consider\u2014to be definite\u2014the important case of binary labels (e.g., Yi \u2208 {0, 1} or Yi \u2208 {+1,\u22121}). Then powers of the labels do not provide any further information (e.g., if Yi \u2208 {0, 1}, then Yi = Y 2i = Y 3i ). Also, since Yi is non-linearly related to u\u2113, Y 2 i does not depend only on u \u22972 \u2113 .\nComputational complexity. The method of [5] requires solving a regularized linear regression in d3 dimensions and factorizing a tensor of third order in d dimensions. Even under optimistic assumptions (finite convergence of iterative schemes), this requres O(d3n+ d4) operations.\nIn this paper, we develop a spectral approach to learning mixtures of linear classifiers in high dimension. For the sake of simplicity, we shall focus on the case of binary labels Yi \u2208 {+1,\u22121}, but we expect our ideas to be more broadly applicable. We consider regression functions of the form f(yi|xi, u\u2113) = f(yi|\u3008xi, u\u2113\u3009), i.e., each component corresponds to a generalized linear model with parameter vector u\u2113 \u2208 Rd. In a nutshell, our method constructs a symmetric matrix Q\u0302 \u2208 Rd\u00d7d by taking a suitable empirical average of the data. The matrix Q\u0302 has the following property: (d \u2212 k) of its eigenvalues are roughly degenerate. The remaining k eigenvalues correspond to eigenvectors that\u2013approximately\u2013 span the same subspace as u1, . . . , uk. Once this space is accurately estimated, the problem dimensionality is reduced to k; as such, it is easy to come up with effective prediction methods (as a matter of fact, simple K-nearest neighbors works very well).\nThe resulting algorithm is computationally efficient, as its most expensive\nstep is computing the eigenvector decomposition of a d\u00d7 d matrix (which takes O(d3) operations). Assuming Gaussian feature vectors Xi \u2208 Rd, we prove that our method is also statistically efficient, i.e., it only requires n = \u2126(d) samples to accurately reconstruct the subspace spanned by u1, . . . , uk. This is the same amount of data needed to estimate the covariance of the feature vectors Xi or a parameter vector u1 \u2208 Rd in the trivial case of a mixture with a single component, k = 1. It is unlikely that a significantly better efficiency can be achieved without additional structure.\nThe assumption of Gaussian feature vectors Xi\u2019s is admittedly restrictive. On one hand, as for the problem of learning mixtures of Gaussians [2, 15], we believe that useful insights can be gained by studying this simple setting. On the other, and as discussed below, our proof does not really require the distribution of the Xi\u2019s to be Gaussian, and a strictly weaker assumption is sufficient. We expect that future work will succeed in further relaxing this assumption."}, {"heading": "1.1. Technical contribution and related work", "text": "Our approach is related to the principal Hessian directions (pHd) method proposed by Li [11] and further developed by Cook [6] and co-workers. PHd is an approach to dimensionality reduction and data visualization. It generalizes principal component analysis to the regression (discriminative) setting, whereby each data point consists of a feature vector Xi \u2208 Rd and a label Yi \u2208 R. Summarizing, the idea is to form the \u2018Hessian\u2019 matrix H\u0302 = n\u22121 \u2211n i=1 YiXiX T i \u2208 Rd\u00d7d. (We assume here, for ease of exposition, that the Xi\u2019s have zero mean and unit covariance.) The eigenvectors associated to eigenvalues with largest magnitude are used to identify a subspace in Rd onto which to project the feature vectors Xi\u2019s.\nUnfortunately, the pHd approach fails in general for the mixture models of interest here, i.e., if Yi \u2223\u2223 Xi=xi \u223c\u2211k\u2113=1 p\u2113 f(yi|\u3008u\u2113, xi\u3009). For instance, it fails when each component is described by a logistic model f(yi = +1|z) = (1 + e\u2212z)\u22121, when features are centered at E(Xi) = 0 (see Appendix D).\nOur approach overcomes this problem by constructing Q\u0302 = n\u22121 \u2211n\ni=1 ZiXiX T i \u2208\nR d\u00d7d. The Zi\u2019s are pseudo-labels obtained by applying a \u2018mirroring\u2019 transformation to the Yi\u2019s. Unlike with H\u0302 , the eigenvector structure of Q\u0302 enables us to estimate the span of u1, . . . , uk.\nAs an additional technical contribution, we establish non-asymptotic bounds on the estimation error, that allows to characterize the trade-off between the data dimension d, and the sample size n. In contrast, rigorous analysis on pHd is limited to the low-dimensional regime of d fixed as n \u2192 \u221e. It would be interesting to generalize the analysis developed here to characterize the highdimensional properties of pHd as well."}, {"heading": "2. Problem Formulation", "text": ""}, {"heading": "2.1. Model", "text": "Consider a dataset comprising n i.i.d. pairs (Xi, Yi) \u2208 Rd \u00d7 {\u22121,+1}, i \u2208 [n]. We refer to the vectors Xi \u2208 Rd as features and to the binary variables as labels. We assume that the features Xi \u2208 Rd are sampled from a Gaussian distribution with mean \u00b5 \u2208 Rd and a positive definite covariance \u03a3 \u2208 Rd\u00d7d. The labels Yi \u2208 {\u22121,+1} are generated by a mixture of linear classifiers, i.e., labels are distributed as follows:\nPr(Yi = +1 | Xi) = \u2211k\n\u2113=1 p\u2113 f(\u3008u\u2113, Xi\u3009) . (2) Here, k \u2265 2 is the number of components in the mixture; (p\u2113)\u2113\u2208[k] are the weights, satisfying of course p\u2113 > 0, \u2211k \u2113=1 p\u2113 = 1; and (u\u2113)\u2113\u2208[k], u\u2113 \u2208 Rd are the normals to the planes defining the k linear classifiers. We refer to each normal u\u2113 as the parameter profile of the \u2113-th classifier; we assume that the profiles u\u2113, \u2113 \u2208 [k], are linearly independent, and that k < n/2.\nWe assume that the function f : R \u2192 [0, 1], characterizing the classifier response, is non-decreasing, strictly concave in [0,+\u221e), and satisfies:\nlim t\u2192\u221e f(t)=1, lim t\u2192\u2212\u221e\nf(t)=0, 1\u2212f(t)=f(\u2212t). (3)\nAs an example, it is useful to keep in mind the logistic function f(t) = (1 + e\u2212t)\u22121. Fig. 1(a) illustrates a mixture of k = 2 classifiers over d = 3 dimensions."}, {"heading": "2.2. Subspace Estimation, Prediction and Clustering", "text": "Our main focus is the following task:\nSubspace Estimation: After observing (Xi, Yi), i \u2208 [n], estimate the subspace spanned by the profiles of the k classifiers, i.e., U \u2261 span(u1, . . . , uk).\nFor U\u0302 an estimate of U , we characterize performance via the principal angle between the two spaces, namely\ndP (U, U\u0302) = max x\u2208U,y\u2208U\u0302\narccos (\n\u3008x,y\u3009 \u2016x\u2016\u2016y\u2016\n) .\nNotice that projecting the features Xi on U entails no loss of information w.r.t. (2); this can be exploited for dimensionality reduction, with the purpose of performing tasks beyond subspace estimation. We review two such tasks below.\nPrediction: Given a new feature vector Xn+1, predict the corresponding label Yn+1.\nClustering: Given a new feature vector and label pair (Xn+1, Yn+1), identify the classifier that generated the label.\nAs we will see in Section 5, our subspace estimate can be used to significantly improve the performance of both prediction and clustering."}, {"heading": "2.3. Technical Preliminary", "text": "We review here a few definitions that are used in our exposition. The subgaussian norm of a random variable X is:\n\u2016X\u2016\u03c82 = sup p\u22651 1\u221a p (E[|X |p])1/p.\nWe say that X is sub-gaussian if \u2016X\u2016\u03c82 < \u221e. We say that a random vector X \u2208 Rd is sub-gaussian if \u3008y,X\u3009 is sub-Gaussian for any y \u2208 Rd, and let \u2016X\u2016\u03c82 \u2261 sup\u2016y\u20162\u22641 \u2016\u3008y,X\u3009\u2016\u03c82.\nWe use the following variant of Stein\u2019s identity [13, 19]. Let X \u2208 Rd, X \u2032 \u2208 Rd\u2032 be jointly Gaussian random vectors, and consider a function h : Rd\n\u2032 \u2192 R that is almost everywhere (a.e.) differentiable and satisfies E[|\u2202h(X \u2032)/\u2202xi|] < \u221e, i \u2208 [d\u2032]. Then, the following identity holds:\nCov(X,h(X \u2032)) = Cov(X,X \u2032)E[\u2207h(X \u2032)]. (4)"}, {"heading": "3. Subspace Estimation", "text": "In this section, we present our algorithm for subspace estimation, which we refer to as SpectralMirror. Our main technical contribution, stated formally below, is that the output U\u0302 of SpectralMirror is a consistent estimator of the subspace U spanned by the parameter vectors u\u2113, as soon as n \u2265 C d for a sufficiently large constant C.\nAlgorithm 1 SpectralMirror Require: Pairs (Xi, Yi), i \u2208 [n] Ensure: Subspace estimate U\u0302\n1: \u00b5\u0302 \u2190 1 \u230an/2\u230b \u2211\u230an/2\u230b i=1 Xi 2: \u03a3\u0302 \u2190 1 \u2308n/2\u2309 \u2211\u230an/2\u230b i=1 (Xi \u2212 \u00b5\u0302)(Xi \u2212 \u00b5\u0302) T 3: r\u0302 \u2190 1 \u230an/2\u230b \u2211\u230an/2\u230b i=1 Yi\u03a3\u0302 \u22121/2(Xi\u2212\u00b5\u0302) 4: for each i \u2208 {\u230an/2\u230b + 1, . . . , n}: Zi \u2190 Yi sgn\u3008r\u0302, Xi\u3009 5: Q\u0302\u2190 1\n\u2308n/2\u2309\nn\u2211\ni=\u230an/2\u230b+1\nZi\u03a3\u0302 \u22121/2(Xi\u2212\u00b5\u0302)(Xi\u2212\u00b5\u0302) T \u03a3\u0302\u22121/2\n6: Find eigendecomposition \u2211d\n\u2113=1 \u03bb\u2113w\u2113w T \u2113 of Q\u0302\n7: Let \u03bb(1), . . . , \u03bb(k) be the k eigenvalues furthest from the median. 8: U\u0302 \u2190 span ( \u03a3\u0302\u22121/2w(1), . . . , \u03a3\u0302 \u22121/2w(k) )"}, {"heading": "3.1. Spectral Mirror Algorithm", "text": "We begin by presenting our algorithm for estimating the subspace span U . Our algorithm consists of three main steps. First, as pre-processing, we estimate the mean and covariance of the underlying features Xi. Second, using these estimates, we identify a vector r\u0302 that concentrates near the convex cone spanned by the profiles (u\u2113)\u2113\u2208[k]. We use this vector to perform an operation we call mirroring: we \u2018flip\u2019 all labels lying in the negative halfspace determined by r\u0302. Finally, we compute a weighted covariance matrix Q\u0302 over all Xi, where each point\u2019s contribution is weighed by the mirrored labels: the eigenvectors of this matrix, appropriately rotated, yield the span U .\nThese operations are summarized in Algorithm 1. We discuss each of the main steps in more detail below: Pre-processing. (Lines 1\u20132) We split the dataset into two halves. Using the first half (i.e., all Xi with 1 \u2264 i \u2264 \u230an2 \u230b), we construct estimates \u00b5\u0302 \u2208 Rd and \u03a3\u0302 \u2208 Rd\u00d7d of the feature mean and covariance, respectively. Standard Gaussian (i.e., \u2018whitened\u2019) versions of features Xi can be constructed as \u03a3\u0302\n\u22121/2(Xi\u2212\u00b5\u0302). Mirroring. (Lines 3\u20134) We compute the vector:\nr\u0302 = 1 \u230an/2\u230b \u2211\u230an/2\u230b i=1 Yi\u03a3\u0302 \u22121/2(Xi\u2212\u00b5\u0302) \u2208 Rd. (5)\nWe refer to r\u0302 as the mirroring direction. In Section 4, we show that r\u0302 is tightly concentrated around its population (n = \u221e) version:\nr \u2261 E [ \u03a3\u22121(X \u2212 \u00b5) \u00b7 (\u2211k \u2113=1 p\u2113g(\u3008u\u2113, X\u3009) )] , (6)\nwhere g(s) \u2261 2f(s) \u2212 1, s \u2208 R. Crucially, r lies in the interior of the convex cone spanned by the parameter profiles, i.e., r = \u2211k \u2113=1 \u03b1\u2113u\u2113, for some positive \u03b1\u2113 > 0, \u2113 \u2208 [k] (see Lemma 2 and Fig. 1(b)). Using this r\u0302, we \u2018mirror\u2019 the labels in the second part of the dataset:\nZi = Yi sgn\u3008r\u0302, Xi\u3009, for \u230an/2\u230b < i \u2264 n.\nIn words, Zi equals Yi for all i in the positive half-space defined by the mirroring direction; instead, all labels for points i in the negative half-space are flipped (i.e., Zi = \u2212Yi). This is illustrated in Figure 1(c). Spectral Decomposition. (Lines 5\u20138) The mirrored labels are used to compute a weighted covariance matrix over whitened features as follows:\nQ\u0302 = 1\n\u2308n2 \u2309\nn\u2211\ni=\u230an/2\u230b+1 Zi\u03a3\u0302\n\u22121/2(Xi \u2212 \u00b5\u0302)(Xi \u2212 \u00b5\u0302)T \u03a3\u0302\u22121/2\nThe spectrum of Q\u0302 has a specific structure, that reveals the span U . In particular, as we will see in Section 4, Q\u0302 converges to a matrix Q that contains an eigenvalue with multiplicity n \u2212 k; crucially, the eigenvectors corresponding to the remaining k eigenvalues span the subspace U , subject to a rotation. As such, the final steps of the algorithm amount to discovering the eigenvalues that \u2018stand out\u2019 (i.e., are different from the eigenvalue with multiplicity n\u2212 k), and rotating the corresponding eigenvectors to obtain U\u0302 . More specifically, let (\u03bb\u2113, w\u2113)\u2113\u2208[d] be the eigenvalues and eigenvectors of Q\u0302. The algorithm computes the median of all eigenvalues, and identifies the k eigenvalues furthest from this median; these are the \u2018outliers\u2019. The corresponding k eigenvectors, multiplied by \u03a3\u0302\u22121/2, yield the subspace estimate U\u0302 .\nWe note that the algorithm does not require knowledge of the classifier response function f . While we assume knowledge of k, an eigenvalue/eigenvectors statistic (see, e.g., Zelnik-Manor and Perona [21]) can be used to estimate k as well."}, {"heading": "3.2. Main Result", "text": "Our main result states that SpectralMirror is a consistent estimator of the subspace spanned by u1, . . . , u\u2113.\nTheorem 1. Denote by U\u0302 the output of SpectralMirror, and let P\u22a5r \u2261 I \u2212 rrT /\u2016r\u20162 be the projector orthogonal to r, given by (6). Then, if \u00b5 = 0, there exists \u01eb0 > 0 such that, for all \u01eb \u2208 [0, \u01eb0),\nPr(dP (P \u22a5 r U, U\u0302) > \u01eb) \u2264 C1 exp(\u2212C2\nn\u01eb2\nd ).\nHere C1 is an absolute constant, and C2 > 0 depends on the distribution of \u03a3, f and u1, . . . , u\u2113.\nIn other words, U\u0302 provides an accurate estimate of P\u22a5r U as soon as n is significantly larger than d. Note that this does not guarantee that U\u0302 spans the direction r \u2208 U ; nevertheless, as shown below, the latter is accurately estimated by r\u0302 (see Lemma 1) and can be added to the span, if necessary. Moreover, our experiments suggest this is rarely the case in practice, as U\u0302 indeed includes the direction r (see Section 5).\nFor simplicity of exposition, our theorem statement refers to \u00b5 = 0. Nevertheless, our proof implies that, in fact, the theorem holds for \u2018most\u2019 \u00b5 \u2208 Rd. Formally, it holds for generic \u00b5: if we add an arbitrarily small random perturbation to \u00b5, Theorem 1 holds with probability 1 w.r.t. this perturbation. We elaborate on this in Section 4, but leave a formal proof of this statement for an extended version of this paper."}, {"heading": "4. Proof of Theorem 1", "text": "We begin by characterizing the limit point of the mirroring direction r\u0302. Observe that E[Y | X = x] = \u2211k\u2113=1 p\u2113g(\u3008u\u2113, x\u3009), where g(s) = 2f(s) \u2212 1, for s \u2208 R. Recall that we denote by r the population version of r\u0302\u2014see Eq. (6).\nLemma 1. There exist an absolute constant C > 0 and c1, c \u2032 1, c \u2032 2 that depend on \u2016X\u2016\u03c82 such that:\nPr(\u2016r\u0302 \u2212 r\u20162\u2265\u01eb)\u2264C exp ( \u2212min {c2n\u01eb2 d , ( c\u20321 \u221a n\u01eb\u2212 c\u20322 \u221a d )2}) .\nThe proof of Lemma 1 relies on a large deviation inequality for sub-Gaussian vectors, and is provided in Appendix A. Crucially, r lies in the interior of the convex cone spanned by the parameter profiles: Lemma 2. r = \u2211k\n\u2113=1 \u03b1\u2113u\u2113 for some \u03b1\u2113 > 0, \u2113 \u2208 [k]. Proof. Observe that\nr = \u2211k\n\u2113=1 p\u2113\u03a3 \u22121 E[(X \u2212 \u00b5)g(\u3008u\u2113, X\u3009)].\nIt thus suffices to show that \u03a3\u22121E[(X \u2212 \u00b5)g(\u3008u,X\u3009)] = \u03b1u, for some \u03b1 > 0. Note that X \u2032 = \u3008u,X\u3009 is normal with mean \u00b50 = uT\u00b5 and variance \u03c320 = uT\u03a3u > 0. The monotonicity of f implies that it is a.e. differentiable, and so is g; moreover, g\u2032 \u2265 0, wherever defined. This, and the fact that g is nonconstant, implies E[g\u2032(X \u2032)] > 0. On the other hand, from Stein\u2019s identity (4), E[g\u2032(X \u2032)] = 1\n\u03c32 0 E[X \u2032g(X \u2032)] < \u221e, as g is bounded. Hence, again from Stein\u2019s identity (4):\n\u03a3\u22121E[(X \u2212 \u00b5)g(\u3008u,X\u3009)] = \u03a3\u22121Cov(X, \u3008u,X\u3009)E[g\u2032(X \u2032)], where X \u2032 \u223c N (\u00b50, \u03c320) = \u03a3\u22121 \u00b7 E[(X \u2212 \u00b5)XTu] \u00b7 E[g\u2032(X \u2032)] = \u03a3\u22121 \u00b7 \u03a3u \u00b7 E[g\u2032(X \u2032)] = E[g\u2032(X \u2032)] \u00b7 u\nand the lemma follows.\nFor r and (\u03b1\u2113)\u2113\u2208[k] as in Lemma 2, define\nz(x) = E[Y sgn(\u3008r,X\u3009) | X = x] = (\u2211k \u2113=1 p\u2113g(\u3008x, u\u2113\u3009) ) \u00b7 sgn (\u2211k \u2113=1 \u03b1\u2113\u3008x, u\u2113\u3009 ) .\nObserve that z(x) is the expectation of the mirrored label at a point x presuming that the mirroring direction is exactly r. Let Q \u2208 Rd\u00d7d be the matrix:\nQ = E[z(X)\u03a3\u22121/2(X \u2212 \u00b5)(X \u2212 \u00b5)T\u03a3\u22121/2].\nThen Q\u0302 concentrates around Q, as stated below.\nLemma 3. Let \u01eb0 \u2261 min{\u03b11, . . . , \u03b1k}\u03c3min(U), where the \u03b1\u2113 > 0 are defined as per Lemma 2. Then\nPr(\u2016Q\u0302\u2212Q\u20162 > \u01eb) \u2264 C exp{\u2212F (\u01eb)},\nwhere F (\u01eb) \u2261 min { c1n\u01eb 2 d , ( c\u20321 \u221a n\u01eb\u2212 c\u20322 \u221a d )2} . Here C is an absolute constant, and c1, c \u2032 1, c \u2032 2 depend on the distribution of X.\nThe proof of Lemma 3 is also provided in Appendix C. We again rely on large deviation bounds for sub-gaussian random variables; nevertheless, our proof diverges from standard arguments because r\u0302, rather than r, is used as a mirroring direction. Additional care is needed to ensure that (a) when r\u0302 is close enough to r, its projection to U still lies in the interior of the convex cone spanned by the profiles, and (b) although r\u0302 may have a (vanishing) component outside the convex cone, the effect this has on Q\u0302 is negligible, for n large enough.\nAn immediate consequence of Lemma 2 is that r reveals a direction in the span U . The following lemma states that the eigenvectors of Q, subject to a rotation, yield the remaining k \u2212 1 directions: Lemma 4. The rank of Q is at most k + 1. One eigenvalue, termed \u03bb0, has multiplicity d \u2212 k. If \u00b5 = 0, the eigenvectors w1, . . . , wk corresponding to the remaining eigenvalues \u03bb1, . . . , \u03bbk are such that\nP\u22a5r U = span(P \u22a5 r \u03a3 \u22121/2w1, . . . , P \u22a5 r \u03a3 \u22121/2wk),\nwhere P\u22a5r is the projection orthogonal to r.\nProof. Note that\nQ = E[z(X)\u03a3\u2212 1 2X(X \u2212 \u00b5)T\u03a3\u2212 12 ] = E[z(\u03a31/2W + \u00b5)WWT ], where W \u223c N (0, I)\n= E [ k\u2211\n\u2113=1\np\u2113g(\u3008\u03a3 1 2W+\u00b5,u\u2113\u3009) sgn(\u3008\u03a3 1 2W+\u00b5,r\u3009)WWT ]\n= E [ k\u2211\n\u2113=1\np\u2113g(\u3008W + \u00b5\u0303, u\u0303\u2113\u3009) sgn(\u3008W + \u00b5\u0303, r\u0303\u3009)WWT ]\nfor u\u0303\u2113 \u2261 \u03a3 1 2u\u2113, r\u0303 \u2261 \u03a3 1 2 r, and \u00b5\u0303 \u2261 \u03a3\u2212 12\u00b5. Hence Q =\u2211k\u2113=1 p\u2113Q\u2113 where\nQ\u2113 = E[g(\u3008u\u0303\u2113,W + \u00b5\u0303\u3009) sgn(\u3008r\u0303,W + \u00b5\u0303\u3009)WWT ].\nBy a simple rotation invariance argument, Q\u2113 can be written as\nQ\u2113 = a\u2113I + b\u2113(u\u0303\u2113r\u0303 T + r\u0303u\u0303T\u2113 ) + c\u2113u\u0303\u2113u\u0303 T \u2113 + d\u2113r\u0303r\u0303 T (7)\nfor some a\u2113, b\u2113, c\u2113, d\u2113 \u2208 R. Let a = \u2211k \u2113=1 p\u2113a\u2113. Then\nQ\u2212 aI = k\u2211\n\u2113=1\np\u2113d\u2113r\u0303r\u0303 T + r\u0303(\nk\u2211\n\u2113=1\np\u2113b\u2113u\u0303\u2113) T+\n+ (\nk\u2211\n\u2113=1\np\u2113b\u2113u\u0303\u2113)r\u0303 T +\nk\u2211\n\u2113=1\np\u2113c\u2113u\u0303\u2113u\u0303 T \u2113 .\nLet P\u22a5r\u0303 be the projector orthogonal to r\u0303, i.e., P \u22a5 r\u0303 = I \u2212 r\u0303r\u0303 T\n\u2016r\u0303\u20162 2 . Let v\u2113 \u2261 P\u22a5r\u0303 u\u0303\u2113. By Lemma 2, r\u0303 = \u2211k \u2113=1 \u03b1\u2113u\u0303\u2113, where \u03b1\u2113 > 0; this, and the linear independence of u\u0303\u2113, implies that v\u2113 6= 0, for all \u2113 \u2208 [k]. Define\nR \u2261 P\u22a5r\u0303 (Q\u2212 aI)P\u22a5r\u0303 = \u2211k \u2113=1 \u03b3\u2113v\u2113v T \u2113 ,\nwhere \u03b3\u2113 = p\u2113c\u2113, \u2113 \u2208 [k]. We will show below that, if \u00b5 = 0, then \u03b3\u2113 6= 0 for all \u2113 \u2208 [k]. This implies that rank(R) = k \u2212 1. Indeed,\nR = P\u22a5r\u0303 \u2211 \u03b3\u2113u\u0303\u2113u\u0303 T \u2113 P \u22a5 r\u0303 = P \u22a5 r\u0303 R\u0303P \u22a5 r\u0303 ,\nwhere R\u0303 has rank k by the linear independence of profiles. As P\u22a5 is a projector orthogonal to a 1-dimensional space,R has rank at least k\u22121. On the other hand, range(R) \u2286 U\u0303 , for U\u0303 = span(u\u03031, . . . , u\u0303\u2113), and r\u0303TRr\u0303 = 0 where r\u0303 \u2208 U\u0303 \\ {0}), so rank(R) = k\u2212 1. The latter also implies that range(R) = P\u22a5r\u0303 U\u0303 , as range(R)\u22a5r\u0303, range(R) \u2286 U\u0303 , and dim(range(R)) = k \u2212 1.\nThe above imply that Q has one eigenvalue of multiplicity n\u2212 k, namely a. Moreover, the eigenvectors w1, . . . , wk corresponding to the remaining eigenvalues (or, the non-zero eigenvalues of Q\u2212 aI) are such that\nP\u22a5r\u0303 \u03a3 1/2U = P\u22a5r\u0303 span(w1, . . . , wk).\nThe lemma thus follows by multiplying both sides of the above equality with P\u22a5r \u03a3 \u22121/2, and using the fact that P\u22a5r \u03a3 \u22121/2P\u22a5r\u0303 = P \u22a5 r \u03a3\n\u22121/2. It thus remains to show that \u03b3\u2113 6= 0, for all \u2113 \u2208 [k]. Note that,\nc\u2113\u3008u\u0303\u2113, v\u2113\u30092 (7) = \u3008v\u2113, (Q\u2113 \u2212 a\u2113I)v\u2113\u3009 = (8)\nCov(g(\u3008u\u0303\u2113,W + \u00b5\u0303\u3009) sgn(\u3008r\u0303,W + \u00b5\u0303\u3009); \u3008W, v\u2113\u30092) \u2261 c\u0303\u2113\nIt thus suffices to show that c\u0303\u2113 < 0. Lemma 2 implies that u\u0303\u2113 = v\u2113+ cr\u0303 for some c > 0, so, for \u00b5 = 0,\nc\u0303\u2113 = Cov[g(\u3008v\u2113,W \u3009+ c\u3008r\u0303,W \u3009) sgn(r\u0303,W ); \u3008W, v\u2113\u30092].\nLet X = \u3008v\u2113,W \u3009 and Y = \u3008r\u0303,W \u3009; then, X and Y are independent Gaussians with zero mean. Moreover, c\u0303\u2113 can be written as c\u0303\u2113 = Cov[F (X);X\n2] where F (x) = EY [g(x + cY ) sgn(Y )]. The symmetry assumption (3) implies that g is antisymmetric, i.e., g(\u2212x) = \u2212g(x). This implies that F (\u2212x) = EY [g(\u2212x + cY ) sgn(Y )] Y \u2032\u2261\u2212Y = EY \u2032 [g(\u2212x \u2212 cY \u2032) sgn(\u2212Y \u2032)] = F (x), i.e., F is symmetric. Further,\nF \u2032(x) = Ey[g \u2032(x+ cY ) sgn(Y )]\n=\n\u222b \u221e\n0\n(g\u2032(x+ cy)\u2212 g\u2032(x\u2212 cy))e \u2212y2/2\u03c3Y\n\u03c3 \u221a 2\u03c0 dy\nThe strict concavity of g in [0,\u221e) implies that g\u2032 is decreasing in [0,+\u221e), and the antisymmetry of g implies that g\u2032 is symmetric. Take x > 0: if x > cy \u2265 0, g\u2032(x+cy) > g\u2032(x\u2212cy), while if x \u2264 cy, then g\u2032(x\u2212cy) = g\u2032(cy\u2212x) > g\u2032(cy+x), so F \u2032(x) is negative for x > 0. By the symmetry of F , F \u2032(x) is positive for x < 0. As such, F (x) = G(x2) for some strictly decreasing G, and c\u0303\u2113 = Cov(G(Z);Z) for Z = X2; hence, c\u0303\u2113 < 0.\nDenote by \u03bb0 the eigenvalue of multiplicity d \u2212 k in Lemma 4. Let \u2206 = min\u2113\u2208[k] |\u03bb0 \u2212 \u03bb\u2113| be the gap between \u03bb0 and the remaining eigenvalues. Then, Lemmas 3 and 4 imply the following lemma, from which Theorem 1 readily follows:\nLemma 5. Let U\u0302 be our estimate for U . If \u03bb1, . . . , \u03bbk are separated from \u03bb0 by at least \u2206, then for \u01eb \u2264 min(\u01eb0/\u2206, 14 ), we have\nPr(dP (U, U\u0302) > \u01eb) \u2264 C exp ( \u2212 F (\u2206\u01eb) ) ,\nwhere \u01eb0, F are defined as per Lemma 3.\nProof. If we ensure \u2016Q\u0302\u2212Q\u2016 \u2264 \u2206/4, then by Weyl\u2019s theorem [8], d\u2212k eigenvalues of Q\u0302 are contained in [\u03bbk+1 \u2212\u2206/4, \u03bbk+1 +\u2206/4], and the remaining eigenvalues\nare outside this set, and will be detected by SpectralMirror. Moreover, by the Davis-Kahan sin(\u03b8) theorem,\ndp(range(Q), range(Q\u0302)) \u2264 1\n\u2206\u2212 \u2016Q\u0302\u2212Q\u20162 \u2016Q\u0302\u2212Q\u20162\n= 1\n\u2206 \u2016Q\u0302\u2212Q\u20162\n\u2212 1 .\nThus the event dp(U, U\u0302) \u2264 \u01eb is implied by \u2016Q\u0302 \u2212 Q\u20162 \u2264 \u2206\u01eb1+\u01eb \u2264 \u2206\u01eb. Moreover, this implies that sufficient condition for \u2016Q\u0302\u2212Q\u20162 \u2264 \u2206/4 (which is required for SpectralMirror to detect the correct eigenvalues) is that \u01eb \u2264 14 . The lemma thus follows from Lemma 3."}, {"heading": "4.1. Extensions", "text": "Though our proof above is for \u00b5 = 0, this assumption is used only in establishing that the parameters c\u0303\u2113 in (8) are non-zero and, hence, all directions in P \u22a5 r U are present in the range of Q. In general, these parameters can be computed in closed form for any \u00b5 through (8), and will be non-zero for generic \u00b5.\nMoreover, the Gaussianity of X is used to establish that the \u2018whitened\u2019 features W are uncorrelated, which in turn yields Eq. (7). We again believe that the Theorem can be extended to more general distributions, provided that this fact\u2014i.e., that features can be decorrelated under the transform \u03a3\u2212 1\n2\u2014still holds."}, {"heading": "5. Experiments", "text": "We conduct computational experiments to validate the performance of our procedure on subspace estimation, prediction, and clustering. We generate synthetic\ndata as using profiles ui \u223c N (0, I), i = 1, 2 and mixture weights p\u2113 sampled uniformly at random from the k-dimensional simplex. Features are also Gaussian: Xi \u223c N (0, I), i = 1, . . . , n; labels generated by the \u2113-th classifier are given by yi = sign(u T \u2113 Xi), i = 1, . . . , n. We use k = 2 in our experiments. Convergence. We study first how well SpectralMirror estimates the span U . Figure 2(a) shows the convergence of U\u0302 to U in terms of (the sin of) the largest principal angle between the subspaces versus the sample size n. We also plot the convergence versus the effective sample size n/d (Figure 2(a)). The curves for different values of d align in Figure 2, indicating that the upper bound in Thm. 1 correctly predicts the sample complexity as n \u2248 \u0398(d). Though not guaranteed by Theorem 1, in all experiments r was indeed spanned by U\u0302 , so the addition of r\u0302 to U\u0302 was not necessary. Prediction through K-NN. Next, we use the estimated subspace to aid in the prediction of expected labels. Given a new feature vector X , we use the average label of its K nearest neighbors (K-NN) in the training set to predict its expected label. We do this for two settings: once over the raw data (the \u2018ambient\u2019 space), and once over data for which the features X are first projected to U\u0302 , the estimated span. Note that projected features are of dimension 2. For each n, we repeat this procedure 25 times with K = \u221a n and K = logn. We record the average root mean squared error between the predicted and true labels over the 25 runs. Figures 3(a) and 3(b) show that, despite the error in the estimated subspace, using K-NN on the subspace outperforms K-NN on the ambient space. Prediction and Clustering through EM. We use the estimated subspace to predict the label of individual classifiers. We use the Expectation-Maximization (EM) to fit the individual profiles both over the training set, as well as on the dataset projected to the estimated subspace U\u0302 . When give a new feature vector X , we simply set sgn(uTi X) to be the predicted label of classifier i. We conducted two experiments in this setting: (a) initialize EM close to the true profiles ui, i \u2208 [k], and (b) randomly initialize EM and choose the best set of profiles from 30 runs. For each n we run EM 10 times and evaluate fitted profiles by using\nthem to predict the label of new features. We record the average normalized 0-1 loss over 10 runs.\nThe first set of experiments, illustrated in Figure 4(a), measures the statistical efficiency of EM over the estimated subspace versus EM over the ambient space. The second set of experiments, illustrated in Figure 4(b), aims to capture the additional improvement due to the reduction in the number of local minima in the reduced space. In both cases we see that constraining the estimated profiles to lie in the estimated subspace improves the statistical efficiency of EM; in the more realistic random start experiments, enforcing the subspace constraint also improves the performance of EM by reducing the number of local minima, thereby allowing EM to reach better local minima.\nFinally, we use the fitted profiles ui to identify the classifier generating a label given the features and the label. To do this, once the profiles ui have been detected by EM, we use a logistic model margin condition to identify the classifier that generated a label, given the label and its features. Figure 4(c) shows the result for EM initialized at a random point, after choosing the best set of profiles from out of 30 runs. We evaluate the performance of this clustering procedure using the normalized 0-1 loss. Again, constraining the estimated profiles to the estimated subspace significantly improves the performance on this clustering task."}, {"heading": "6. Conclusions", "text": "We have proposed SpectralMirror, a method for discovering the span of a mixture of linear classifiers. Our method relies on a non-linear transform of the labels, which we refer to as \u2018mirroring\u2019. Moreover, we have provided consistency guarantees and non-asymptotic bounds, that also imply the near optimal statistical efficiency of the method. Finally, we have shown that, despite the fact that SpectralMirror discovers the span only approximately, this is sufficient to allow for a significant improvement in both prediction and clustering, when the features are projected to the estimated span.\nWe have already discussed several technical issues that remain open, and that we believe are amenable to further analysis. These include extending Theorem 1 to generic \u00b5, amending the Gaussianity assumption, and applying our bounds to other pHd-inspired methods.\nAn additional research topic is to further improve the computational complexity of the estimation of the eigenvectors of the \u2018mirrored\u2019 matrix Q\u0302. This is of greatest interest in cases where the covariance \u03a3 and mean \u00b5 are a priori known. This would be the case when, e.g., the method is applied repeatedly and, although the features X are sampled from the same distribution each time, labels Y are generated from a different mixture of classifiers. In this case, SpectralMirror lacks the preprocessing step, that requires estimating \u03a3 and is thus computationally intensive; the remaining operations amount to discovering the spectrum of Q\u0302, an operation that can be performed more efficiently. For example, we can use a regularized M-estimator to exploit the fact that\n\u03a3\u22121/2Q\u0302\u03a3\u22121/2 should be the sum of a multiple of the identity and a low rank matrix\u2014see, e.g., [16]."}, {"heading": "Appendix A: A Large-Deviation Lemma", "text": "Lemma 6. Let X \u2208 Rd be a sub-Gaussian random vector, i.e. \u3008a,X\u3009 is subGaussian for any a \u2208 Rd. Then there exist universal constants c1, c2 such that\nPr(\u2016X\u20162 \u2265 t) \u2264 c1 exp ( \u2212min { c2(t\n2 \u2212 d \u2016\u03a3\u20162) 4d \u2016X\u20162\u03c82 , (t2 \u2212 d \u2016\u03a3\u20162)2 64c2 \u2016X\u20164\u03c82\n}) .\nProof. By the (exponential) Markov inequality, we have\nPr(\u2016X\u20162 \u2265 t) = Pr ( exp(\u03bb \u2016X\u201622) \u2265 exp(\u03bbt2) ) \u2264 E[exp(\u03bb \u2016X\u2016 2 2)]\nexp(\u03bbt2) . (9)\nLet Z be uniformly distributed on the unit sphere Sd\u22121. Then \u221a dZ is isotropic so dE[\u3008Z, a\u30092] = \u2016a\u201622 for any a. This implies\nE[exp(\u03bb \u2016X\u201622)] = EX [exp(\u03bbdEZ [\u3008Z,X\u30092])] \u2264 EX [EZ [exp(\u03bbd\u3008Z,X\u30092)]].\nWe interchange the order of expectation to obtain\nE[exp(\u03bb \u2016X\u201622)] \u2264 EZ [EX [exp(\u03bbd\u3008X,Z\u30092)]] \u2264 sup {EX [exp(\u03bbd\u3008X,Z\u30092)] | z \u2208 Sd\u22121}.\n\u3008X, z\u3009 is sub-Gaussian (for a fixed z) so \u3008X, z\u30092 is (noncentered) sub-exponential.\nIf \u03bbd < c/ \u2225\u2225\u3008X, z\u30092 \u2212E[\u3008X, z\u30092] \u2225\u2225 \u03c81 , then\nE[exp(\u03bbd\u3008X, z\u30092)] \u2264 exp(\u03bbdE[\u3008X, z\u30092])E[exp(\u03bbd(\u3008X, z\u30092 \u2212E[\u3008X, z\u30092]))] \u2264 exp(\u03bbdE[\u3008X, z\u30092] + cd2\u03bb2\n\u2225\u2225\u3008X, z\u30092 \u2212E[\u3008X, z\u30092] \u2225\u22252 \u03c81 )\n\u2264 exp(\u03bbdE[\u3008X, z\u30092] + 4cd2\u03bb2 \u2225\u2225\u3008X, z\u30092 \u2225\u22252 \u03c81 ) \u2264 exp(\u03bbdE[\u3008X, z\u30092] + 16cd2\u03bb2 \u2016\u3008X, z\u3009\u20164\u03c82). (10)\nWe substitute this bound into (9) to obtain\nPr(\u2016X\u20162 \u2265 t) \u2264 exp(16cd2\u03bb2 \u2016X\u2016 4 \u03c82 + \u03bb(d \u2016\u03a3\u20162 \u2212 t2)), (11)\nwhere \u03a3 is the covariance matrix of X . We optimize over \u03bb to obtain\nPr(\u2016X\u20162 \u2265 t) \u2264 exp ( \u2212 (t\n2 \u2212 d \u2016\u03a3\u20162)2 64cd2 \u2016X\u20164\u03c82\n) .\nIf the optimum lies outside the region where (10) holds, we can set\n\u03bb = c 4d \u2016X\u20162\u03c82 \u2264 c\nd \u2016\u3008X, z\u30092 \u2212E[\u3008X, z\u30092]\u2016\u03c81 in (11) to obtain:\nPr(\u2016X\u20162 \u2265 t) \u2264 exp ( c3 +\nc(d \u2016\u03a3\u20162 \u2212 t2) 4d \u2016X\u20162\u03c82\n) .\nWe combine these two bounds to obtain\nPr(\u2016X\u20162 \u2265 t) \u2264 c1 exp ( \u2212min { c2(t\n2 \u2212 d \u2016\u03a3\u20162) 4d \u2016X\u20162\u03c82 , (t2 \u2212 d \u2016\u03a3\u20162)2 64c2 \u2016X\u20164\u03c82\n}) .\nNote that the t2 bound always holds. However, for small t, the t4 term yields a tighter bound."}, {"heading": "Appendix B: Proof of Lemma 1 (Weak Convergence of r\u0302)", "text": "Proof. We expand \u2016r\u0302n \u2212 r\u20162 to obtain\n\u2016r\u0302n \u2212 r\u20162 = \u2225\u2225\u2225 1 n n\u2211\ni=1\n\u03a3\u0302\u22121Yi(Xi \u2212 \u00b5\u0302)\u2212 \u03a3\u22121 E[s(X)(X \u2212 \u00b5)] \u2225\u2225\u2225 2\n(12)\n\u2264 \u2225\u2225\u03a3\u0302\u22121 \u2225\u2225 2 \u2225\u2225\u2225 1 n n\u2211\ni=1\nYi(Xi \u2212 \u00b5\u0302)\u2212E[s(X)(X \u2212 \u00b5)] \u2225\u2225\u2225 2\n(13)\n+ \u2016E[s(X)(X \u2212 \u00b5)]\u20162\u2016\u03a3\u0302\u22121 \u2212 \u03a3\u22121\u20162 + (oP (1))2. (14)\nThe higher order terms generically look like\nPr(\u2016X \u2212E[X ]\u20162 \u2016Y \u2212E[Y ]\u20162 > \u01eb). (15)\nWe apply the union bound to deduce\nPr(\u2016X \u2212E[X ]\u20162 \u2016Y \u2212E[Y ]\u20162 > \u01eb) \u2264 Pr(\u2016X \u2212E[X ]\u20162 > \u221a \u01eb) +Pr(\u2016X \u2212E[X ]\u20162 > \u221a \u01eb).\nFor any \u01eb < 1, \u221a \u01eb > \u01eb and we have\nPr(\u2016X \u2212E[X ]\u20162 > \u221a \u01eb) \u2264 Pr(\u2016X \u2212E[X ]\u20162 > \u01eb).\nSince terms of the form Pr(\u2016X \u2212E[X ]\u20162 > \u01eb) appear in our tail bounds, we can handle terms like (15) with a constant factor (say 2). Our bounds involve multiplicative constants anyways, so we omit these terms to simplify our derivation.\nWe expand the first term to obtain\n\u2225\u2225\u2225 1 n n\u2211\ni=1\nYi(Xi \u2212 \u00b5\u0302)\u2212E[Y (X \u2212 \u00b5)] \u2225\u2225\u2225 2\n\u2264 |E[s(X)]| \u2016\u00b5\u0302\u2212 \u00b5\u20162 + \u2016\u00b5\u20162 \u2223\u2223\u2223 1 n n\u2211\ni=1\nYi \u2212E[s(X)] \u2223\u2223\u2223 2\n+ \u2225\u2225\u2225 1 n n\u2211\ni=1\nYi(Xi \u2212 \u00b5)\u2212E[s(X)(X \u2212 \u00b5)] \u2225\u2225\u2225 2\n+ (oP (1)) 2.\n\u00b5\u0302\u2212 \u00b5 is sub-Gaussian with sub-Gaussian norm \u2016X\u2016\u03c82\u221a n , so there exist universal c1 and c2 s.t.\nPr(\u2016\u00b5\u0302\u2212 \u00b5\u20162 > t) \u2264 c1 exp ( \u2212c2n(t\n2 \u2212 d \u2016\u03a3\u20162) 4d \u2016X\u20162\u03c82\n) .\nY is bounded between 1 and -1, so\n1. Chernoff\u2019s inequality yields\nPr (\u2223\u2223\u2223 1 n n\u2211\ni=1\nYi \u2212E[s(X)] \u2223\u2223\u2223 > t ) \u2264 2 exp(\u2212nt2/2).\n2. Yi(Xi \u2212 \u00b5) are also sub-Gaussian. Thus there exist universal c1 and c2 such that\nPr (\u2225\u2225\u2225 1 n n\u2211\ni=1\nYi(Xi \u2212 \u00b5)\u2212E[s(X)(X \u2212 \u00b5)] \u2225\u2225\u2225 2 > t ) \u2264 c1 exp\n( \u2212c2n(t\n2 \u2212 d \u2016\u03a3\u20162) 4d \u2016X\u20162\u03c82\n) .\nWe expand the second term in (14) to obtain\n\u2016\u03a3\u0302\u22121 \u2212 \u03a3\u22121\u2016 = \u2016\u03a3\u22121/2\u2016\u2016\u03a31/2\u03a3\u0302\u22121\u03a31/2 \u2212 I\u2016\u2016\u03a3\u22121/2\u2016.\nWe expand the middle term to obtain\n\u2016\u03a3\u22121/2\u03a3\u0302\u03a3\u22121/2 \u2212 I\u2016\n\u2264 \u2225\u2225\u2225 ( 1 n n\u2211\ni=1\n\u03a3\u22121/2(Xi \u2212 \u00b5)(Xi \u2212 \u00b5)T\u03a3\u22121/2 )\u22121 \u2212 I \u2225\u2225\u2225 2\n+ 2 \u2016\u00b5\u20162 \u2016\u00b5\u0302\u2212 \u00b5\u20162 + (oP (1))2\nWe use Theorem 5.39 in [20] to bound the first term:\nPr (\u2225\u2225\u2225 1 n n\u2211\ni=1\nWiW T i \u2212 I \u2225\u2225\u2225 2 > t ) \u2264 2 exp(\u2212c\u20321( \u221a nt\u2212 c\u20322 \u221a d)2),\nwhere c\u20321, c \u2032 2 depend on the sub-Gaussian norm ofW . We substitute these bounds into our expression for \u2016r\u0302 \u2212 r\u20162 to deduce\nPr(\u2016r\u0302 \u2212 r\u20162 \u2265 \u01eb) \u2264 C exp ( \u2212min { c1n\u01eb 2 d , ( c\u20321 \u221a n\u01eb \u2212 c\u20322 \u221a d )2}) ,\nwhere C is an absolute constant and c1, c \u2032 1, c \u2032 2 depend on the sub-Gaussian norm of X ."}, {"heading": "Appendix C: Proof of Lemma 3 (Weak Convergence of Q\u0302)", "text": "Let r\u0303 denote the projection of r onto span{u, v}. Lemma 7. If \u2016r\u0302\u2212r\u20162 \u2264 \u01eb0 = min {\u03b11, \u03b12} sin(\u03b8), then r\u0303 also lies in the positive quadrant.\nProof. r lies in interior of the conic hull of {u1, . . . , uk}, so we can express r as\u2211k i=1 \u03b1iui, where ri > 0. If r\u0303 also lies in the conic hull, then r\u0303 = \u2211k i=1 \u03b2iui for some \u03b2i > 0.\n\u01eb0 = \u2016r\u0303 \u2212 r\u20162 = \u2225\u2225\u2225\u2225\u2225 k\u2211\ni=1 (\u03b1i \u2212 \u03b2i)ui \u2225\u2225\u2225\u2225\u2225 2 = \u221a (\u03b1\u2212 \u03b2)UUT (\u03b1 \u2212 \u03b2)\n\u2265 \u2016\u03b1\u2212 \u03b2\u20162 \u03c3min(U) \u2265 \u2016\u03b1\u2212 \u03b2\u2016\u221e \u03c3min(U).\nTo ensure \u03b2 is component-wise positive, we must have \u2016\u03b1\u2212 \u03b2\u2016\u221e < min{\u03b11, . . . , \u03b1k}. A sufficient condition is \u01eb0 \u2264 min{\u03b11, . . . , \u03b1k}\u03c3min(U).\nWe are now ready to prove Lemma 3. We expand \u2016Q\u0302n \u2212 Q\u20162 (and neglect\nhigher order terms) to obtain\n\u2016Q\u0302n \u2212Q\u20162 \u2264 \u2225\u2225\u2225 1 n n\u2211\ni=1\nZi\u03a3 \u22121/2(Xi \u2212 \u00b5)(Xi \u2212 \u00b5)T\u03a3\u22121/2 \u2212Q \u2225\u2225\u2225 2\n+ 2\u2016\u03a3\u0302\u22121/2 \u2212 \u03a3\u22121/2\u20162 E [ \u2016(X \u2212 \u00b5)(X \u2212 \u00b5)\u03a3\u22121/2\u20162 ] + 2\u2016\u03a3\u22121/2\u20162\u2016\u00b5\u0302\u2212 \u00b5\u20162 E [ \u2016(X \u2212 \u00b5)\u03a3\u22121/2\u20162 ] + (oP (1)) 2.\nThe second and third terms can be bounded using the same bounds used in the analysis of how fast r\u0302 converges to r. Thus we focus on how fast\nn\u2211\ni=1\nZi\u03a3 \u22121/2(Xi \u2212 \u00b5)(Xi \u2212 \u00b5)T\u03a3\u22121/2\ncoverges to Q. First, we note that\nPr (\u2225\u2225\u2225 1 n n\u2211\ni=1\nZiWiW T i \u2212Q \u2225\u2225\u2225 2 > t )\n\u2264 Pr (\u2225\u2225\u2225 1\nn\nn\u2211\ni=1\nZiWiW T i \u2212Q \u2225\u2225\u2225 2 > t | r\u0302 \u2208 B\u01eb0(r) )\n+Pr(r\u0302 \u2208 B\u01eb0(r)). Let Z\u0303i denote the \u201ccorrected\u201d version of the Zi\u2019s, i.e. the Zi\u2019s we obtain if we use the projection of r\u0302 onto the span{u, v} to flip the labels, and Wi denote \u03a3\u22121/2(Xi \u2212 \u00b5). We have\n\u2225\u2225\u2225 1 n n\u2211\ni=1\nZiWiW T i \u2212Q \u2225\u2225\u2225 2\n(16)\n\u2264 \u2225\u2225\u2225 1 n n\u2211\ni=1\nZ\u0303iWiW T i \u2212Q \u2225\u2225\u2225 2 + \u2225\u2225\u2225 1 n n\u2211\ni=1\n(Zi \u2212 Z\u0303i)WiWTi \u2225\u2225\u2225 2 . (17)\nThe probability the first term is large is bounded by\nPr (\u2225\u2225\u2225 1 n n\u2211\ni=1\nZ\u0303iWiW T i \u2212Q \u2225\u2225\u2225 2 > t | r\u0302 \u2208 B\u01eb0(r) )\n\u2264 sup r\u0302\u2208B\u01eb0 (r) Pr\n(\u2225\u2225\u2225 1 n n\u2211\ni=1\nZ\u0303iWiW T i \u2212Q \u2225\u2225\u2225 2 > t | r\u0302 \u2208 B\u01eb0(r) )\nThe Zi\u2019s are independent of Wi\u2019s because the Zi\u2019s were computed using r\u0302 that was in turn computed independently of the Xi\u2019s. Thus this is a sum of i.i.d. r.v. and we can bound it by\nsup r\u0302\u2208B\u01ebr (r) Pr\n(\u2225\u2225\u2225 1 n n\u2211\ni=1\nZ\u0303iWiW T i \u2212Q \u2225\u2225\u2225 2 > t | r\u0302 \u2208 B\u01eb0(r) )\n\u2264 2 exp(\u2212c1( \u221a nt\u2212 c2 \u221a d)2),\nwhere c1, c2 depend on the sub-Gaussian norm of Z\u0303W . This is a consequence of Remark 5.40 in [20].\nWe now focus on bounding the second term in (17). Let Wi denote the spherically symmetric (whitened) version of X . We restrict ourselves to the 3d subspace spanned by u, v, r\u0302. Let Cr\u0302 (for cone) be the \u201cbad\u201d region, i.e. the region where Z 6= Z\u0303. We have\n\u2225\u2225\u2225 1 n n\u2211\ni=1\n(Zi \u2212 Z\u0303i)WiWTi \u2225\u2225\u2225 2 = \u2225\u2225\u2225 2 n n\u2211\ni=1\n1Cr\u0302(Wi)WiW T i \u2225\u2225\u2225 2 .\n1Cr\u0302 is bounded, hence 1Cr\u0302(Wi)Wi is sub-Gaussian and\nPr (\u2225\u2225\u2225 2 n n\u2211\ni=1\n1Cr\u0302(Wi)WiW T i \u2212 2E[1Cr\u0302(W )WWT ] \u2225\u2225\u2225 2 > t | r\u0302 \u2208 B\u01eb0(r) )\n\u2264 2 exp(\u2212c1( \u221a nt\u2212 c2 \u221a d)2),\nwhere c1, c2 depend on the sub-Gaussian norm of X . It remains to bound\u2225\u2225E[1Cr\u0302(W )WWT ] \u2225\u2225 2 .\nWe use Jensen\u2019s inequality to obtain \u2225\u2225E[1Cr\u0302(W )WWT ] \u2225\u2225 2 \u2264 E[1Cr\u0302(W ) \u2225\u2225WWT \u2225\u2225 2 ]\n\u2264 CPr(W \u2208 Cr\u0302), where the constant C depends on the number of mixture components. Finally, we bound Pr(W \u2208 Cr\u0302).\nThe distribution of Wi is spherically symmetric so the probability that Wi \u2208 Cr\u0302 is proportional to the surface area of the set\nS = {w \u2208 Sk+1 | r\u0302Tw \u2264 0, uT1 w, . . . , uTkw \u2265 0}. This set is contained in the set\nS\u0304 = {w \u2208 Sk+1 | \u2016w \u2212 u1\u20162 \u2264 \u2016r\u0302 \u2212 r\u20162} This set is nothing but the spherical cap of radius \u2016r\u0302 \u2212 r\u20162 in k+1 dimensions. An upper bound is\n\u03b3(S\u0304) \u2264 exp(\u2212(k + 1) cos(2\u2212 r 2\n2ab )2),\nwhere \u03b3 denotes the uniform measure on the unit sphere. We substitute these bounds into our expression for the second term to obtain\nE[1Cr\u0302(W )WW T ] \u2264 C arccos ( 2\u2212 \u01eb2r\n2\n) (1\u2212 cos\u2220(u, v))(1 + cos\u2220(u, v)+\ncos2 \u2220(u, v))\n\u2264 C arccos ( 1\u2212 \u01eb 2 r\n2\n) .\nWe combines these bounds to deduce\nPr(\u2016Q\u0302\u2212Q\u20162 > C arccos ( 1\u2212 \u01eb 2 r\n2\n) + \u01eb | r\u0302 \u2208 B\u01eb0(r))\n\u2264 C exp ( \u2212min { c1n\u01eb 2\nd , ( c\u20321 \u221a n\u01eb \u2212 c\u20322\n\u221a d )2 }) .\nFinally, we have\nPr(\u2016Q\u0302\u2212Q\u20162 > \u01eb | r\u0302 \u2208 B\u01eb0(r)) \u2264 Pr(\u2016Q\u0302\u2212Q\u20162 > \u01eb | \u2016r\u0302 \u2212 r\u20162 \u2264 \u221a 1\u2212 cos(\u01eb/2), r\u0302 \u2208 B\u01eb0(r))\n+Pr(\u2016r\u0302 \u2212 r\u20162 \u2264 \u221a 1\u2212 cos(\u01eb/2)).\nLet\nf(\u01eb) := min\n{ c1n\u01eb 2\nd , ( c\u20321 \u221a n\u01eb\u2212 c\u20322\n\u221a d )2 } .\nThe first term is bounded by C exp (\u2212f(\u01eb/2)) and the second term is bounded by C exp ( \u2212f( \u221a 1\u2212 cos(\u01eb/2)) ) . We deduce\nPr(\u2016Q\u0302\u2212Q\u20162 > \u01eb | \u2016r\u0302 \u2212 r\u20162 \u2264 \u01eb0) \u2264 C(exp(\u2212f( \u221a 1\u2212 cos(\u01eb/2))) + exp(\u2212f(\u01eb/2)))\n\u2264 C exp(\u2212f(min{ \u221a 1\u2212 cos(\u01eb/2), \u01eb/2}))."}, {"heading": "Appendix D: Principal Hessian Directions", "text": "In this section, we apply the principal Hessian directions (pHd) [11] method to our setting, and demontrate its failure to discover the space spanned by parameter profile when \u00b5 = 0. Recall that pHd considers a setting in which features Xi \u2208 Rd are i.i.d. and normally distributed with mean \u00b5 and covariance \u03a3, while labels Yi \u2208 R lie, in expectation, in a k-dimensional manifold. In particular, some smooth h : Rk \u2192 R, k \u226a d:\nE[Y | X = x] = h(\u3008u1, x\u3009, . . . , \u3008uk, x\u3009)\nwhere u\u2113 \u2208 Rd, \u2113 \u2208 [k]. The method effectively creates an estimate\nH\u0302 = n\u22121 n\u2211\ni=1\nYi \u03a3 \u2212 1 2XiX T i \u03a3 \u2212 1 2 \u2208 Rd\u00d7d\nof the Hessian\nH = E[\u22072xh(\u3008u1, X\u3009, . . . , \u3008uk, X\u3009)] = UTE[\u22072vh(\u3008u1, X\u3009, . . . , \u3008uk, X\u3009)]U,\n(18)\nwhere \u22072vh is the Hessian of the mapping v 7\u2192 h(v), for v \u2208 Rk, and U the matrix of profiles. As in our case, the method discovers span(u1, . . . , uk) from the eigenvectors of the eigenvalues that \u201cstand out\u201d, after appropriate rotation in terms of \u03a3.\nUnfortunately, in the case of linear classifiers,\nh(v) =\nk\u2211\n\u2113=1\np\u2113g(v\u2113)\nfor g(s) = 2f(s) \u2212 1 is anti-symmetric. As a result, \u22072vh is a diagonal matrix whose \u2113-th entry in the diagonal is g\u2032\u2032(v\u2113). Since g is anti-symmetric, so is g\u2032\u2032. Hence, if \u00b5 = 0 we have that E[g\u2032\u2032(\u3008u\u2113, X\u3009)] = 0; hence, the Hessian H given by (18) will in fact be zero, and the method will fail to discover any signal pertaining to span(U).\nThis calls for an application of the pHd method to a transform of the labels Y . This ought to be non-linear, as an affine transform would preserve the above property. Moreover, given that these labels are binary, polynomial transforms do not add any additional signal to Y , and are therefore not much help in accomplishing this task. In contrast, the \u2018mirrorring\u2019 approach that we propose provides a means of transforming the labels so that their expectation indeed carries sufficient information to extract the span U , as evidenced by Theorem 1."}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["Anima Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade", "Matus Telgarsky"], "venue": "arXiv preprint arXiv:1210.7559,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Learning mixtures of arbitrary gaussians", "author": ["Sanjeev Arora", "Ravi Kannan"], "venue": "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Latent variable models and factor analysis: A unified approach, volume 899", "author": ["David J Bartholomew", "Martin Knott", "Irini Moustaki"], "venue": "Wiley. com,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Latent variable models. In Learning in graphical models, pages 371\u2013403", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Spectral experts for estimating mixtures of linear regressions", "author": ["Arun Tejasvi Chaganty", "Percy Liang"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Principal hessian directions revisited", "author": ["R Dennis Cook"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["Arthur P Dempster", "Nan M Laird", "Donald B Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["Michael I Jordan", "Robert A Jacobs"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "On principal hessian directions for data visualization and dimension reduction: another application of stein\u2019s lemma", "author": ["Ker-Chau Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "An end-to-end discriminative approach to machine translation", "author": ["Percy Liang", "Alexandre Bouchard-C\u00f4t\u00e9", "Dan Klein", "Ben Taskar"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Siegel\u2019s formula via stein\u2019s identities", "author": ["Jun S Liu"], "venue": "Statistics  Probability Letters,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "Finite mixture models", "author": ["Geoffrey McLachlan", "David Peel"], "venue": "Wiley. com,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu"], "venue": "Statistical Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Conditional random fields for object recognition", "author": ["Ariadna Quattoni", "Michael Collins", "Trevor Darrell"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Estimation of the mean of a multivariate normal distribution", "author": ["Charles M Stein"], "venue": "In Prague Symposium on Asymptotic Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1973}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Since Pearson\u2019s seminal contribution [17], and most notably after the introduction of the EM algorithm [7], mixture models and latent variable models have played a central role in statistics and machine learning, with numerous applications\u2014see, e.", "startOffset": 103, "endOffset": 106}, {"referenceID": 12, "context": ", McLachlan & Peel [14], Bishop [4], and Bartholomew et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": ", McLachlan & Peel [14], Bishop [4], and Bartholomew et al.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Even the idealized problem of learning mixtures of Gaussians has motivated a copious theoretical literature [2, 15].", "startOffset": 108, "endOffset": 115}, {"referenceID": 13, "context": "Even the idealized problem of learning mixtures of Gaussians has motivated a copious theoretical literature [2, 15].", "startOffset": 108, "endOffset": 115}, {"referenceID": 3, "context": "Models of this type have been intensely studied in the neural network literature since the early nineties [4, 10].", "startOffset": 106, "endOffset": 113}, {"referenceID": 8, "context": "Models of this type have been intensely studied in the neural network literature since the early nineties [4, 10].", "startOffset": 106, "endOffset": 113}, {"referenceID": 15, "context": "They have also found numerous applications ranging from object recognition [18] to machine translation [12].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "They have also found numerous applications ranging from object recognition [18] to machine translation [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 4, "context": "Recently, Chaganty and Liang [5] considered mixtures of linear regressions, whereby the relation between labels and feature vectors is linear within each 1", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "[9] and Anandkumar et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], these authors propose an algorithm for fitting mixtures of linear regressions with provable guarantees.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "While the work of Chaganty and Liang [5] is a significant step forward, it leaves several open problems: Statistical efficiency.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "Then, the mathematical guarantees of Chaganty & Liang [5] require a sample size n \u226b d.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "As noted in [5], this scaling is an intrinsic drawback of the tensor approach which requires working in a higher-dimensional space (tensor space) than the space in which data naturally live.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": ", Jordan & Jacobs [10], Bishop [4], Quattoni et al.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": ", Jordan & Jacobs [10], Bishop [4], Quattoni et al.", "startOffset": 31, "endOffset": 34}, {"referenceID": 15, "context": "[18], Liang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The method of [5] requires solving a regularized linear regression in d dimensions and factorizing a tensor of third order in d dimensions.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "On one hand, as for the problem of learning mixtures of Gaussians [2, 15], we believe that useful insights can be gained by studying this simple setting.", "startOffset": 66, "endOffset": 73}, {"referenceID": 13, "context": "On one hand, as for the problem of learning mixtures of Gaussians [2, 15], we believe that useful insights can be gained by studying this simple setting.", "startOffset": 66, "endOffset": 73}, {"referenceID": 9, "context": "Our approach is related to the principal Hessian directions (pHd) method proposed by Li [11] and further developed by Cook [6] and co-workers.", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "Our approach is related to the principal Hessian directions (pHd) method proposed by Li [11] and further developed by Cook [6] and co-workers.", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "We assume that the function f : R \u2192 [0, 1], characterizing the classifier response, is non-decreasing, strictly concave in [0,+\u221e), and satisfies:", "startOffset": 36, "endOffset": 42}, {"referenceID": 11, "context": "We use the following variant of Stein\u2019s identity [13, 19].", "startOffset": 49, "endOffset": 57}, {"referenceID": 16, "context": "We use the following variant of Stein\u2019s identity [13, 19].", "startOffset": 49, "endOffset": 57}, {"referenceID": 14, "context": ", [16].", "startOffset": 2, "endOffset": 6}], "year": 2017, "abstractText": "We consider a regression (discriminative learning) problem, whereby the regression function is a convex combination of k linear classifiers. Existing approaches are based on the EM algorithm, or similar techniques, without provable guarantees. We develop a simple method based on spectral techniques and a \u2018mirroring\u2019 trick, that discovers the subspace spanned by the classifiers\u2019 parameter vectors. Under a probabilistic assumption on the feature vector distribution, we prove that this approach has nearly optimal statistical efficiency.", "creator": "LaTeX with hyperref package"}}}