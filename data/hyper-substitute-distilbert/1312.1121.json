{"id": "1312.1121", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2013", "title": "Interpreting random forest classification models using a feature contribution method", "abstract": "integrated interpretation demonstrates cause of now most artifacts about static model evaluation process. the validation of the relationship between unique advantages & outputs arises relatively easy for statistical models, such as serial regressions, thanks to the availability - multiple parameters and additional historical significance. for \" black box \" models, such as tree forest, this information operates hidden above the model structure. this work serves an implementation for designing variable contributions assuming random forest classification models. aggregation allows for the determination without the locus its component node on any model prediction about an individual instance. by analysing feature contributions for a training dataset, the most significant variables seem digitally determined and their typical response towards modifications made containing single classes, i. e., class - specific feature labelled \" categories \", are discovered. these patterns represent a standard behaviour of continuous model environment ensures considerably extensive additional assessment at the model specifications via your new application. interpretation of feature contributions inside two uci diagnostic packages shows the potential of the complementary components. the robustness measures results is demonstrated through an extensive analysis of database additions calculated utilizing a large region accurately selected native forest tables.", "histories": [["v1", "Wed, 4 Dec 2013 11:57:53 GMT  (218kb,D)", "http://arxiv.org/abs/1312.1121v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anna palczewska", "jan palczewski", "richard marchese robinson", "daniel neagu"], "accepted": false, "id": "1312.1121"}, "pdf": {"name": "1312.1121.pdf", "metadata": {"source": "CRF", "title": "Interpreting random forest classification models using a feature contribution method", "authors": ["Anna Palczewska", "Jan Palczewski", "Richard Marchese Robinson", "Daniel Neagu", "John Moores"], "emails": ["a.m.wojak@bradford.ac.uk", "j.palczewski@leeds.ac.uk", "r.l.marcheserobinson@ljmu.ac.uk", "d.neagu@bradford.ac.uk"], "sections": [{"heading": null, "text": "\u2217a.m.wojak@bradford.ac.uk \u2020j.palczewski@leeds.ac.uk \u2021r.l.marcheserobinson@ljmu.ac.uk \u00a7d.neagu@bradford.ac.uk\nar X\niv :1\n31 2.\n11 21\nv1 [\ncs .L\nG ]"}, {"heading": "1 Introduction", "text": "Models are used to discover interesting patterns in data or to predict a specific outcome, such as drug toxicity, client shopping purchases, or car insurance premium. They are often used to support human decisions in various business strategies. This is why it is important to ensure model quality and to understand its outcomes. Good practice of model development [17] involves: 1) data analysis 2) feature selection, 3) model building and 4) model evaluation. Implementing these steps together with capturing information on how the data was harvested, how the model was built and how the model was validated, allows us to trust that the model gives reliable predictions. But, how to interpret an existing model? How to analyse the relation between predicted values and the training dataset? Or which features contribute the most to classify a specific instance?\nAnswers to these questions are considered particularly valuable in such domains as chemoinformatics, bioinformatics or predictive toxicology [15]. Linear models, which assign instance-independent coefficients to all features, are the most easily interpreted. However, in the recent literature, there has been considerable focus on interpreting predictions made by non-linear models do not render themselves to straightforward methods for the determination of variable/feature influence. In [8], the authors present a method for a local interpretation of Support Vector Machine (SVM) and Random Forest models by retrieving the variable corresponding to the largest component of the decision-function gradient at any point in the model. Interpretation of classification models using local gradients is discussed in [4]. A method for visual interpretation of kernel-based prediction models is described in [11]. Another approach, which is presented in detail later, was proposed in [12] and aims at shedding light at decision-making process of regression random forests.\nOf interest to this paper is a popular \u201cblack-box\u201d model \u2013 the random forest model [5]. Its author suggests two measures of the significance of a particular variable [6]: the variable importance and the Gini importance. The variable importance is derived from the loss of accuracy of model predictions when values of one variable are permuted between instances. Gini importance is calculated from the Gini impurity criterion used in the growing of trees in the random forest. However, in [16], the authors showed that the above measures are biased in favor of continuous variables and variables with many categories. They also demonstrated that the general representation of variable importance is often insufficient for the complete understanding of the relationship between input variables and the predicted value.\nFollowing the above observation, Kuzmin et al. propose in [12] a new technique to calculate a feature contribution, i.e., a contribution of a variable to the prediction, in a random forest model. Their method applies to models generated\nfor data with numerical observed values (the observed value is a real number). Unlike in the variable importance measures [6], feature contributions are computed separately for each instance/record. They provide detailed information about relationships between variables and the predicted value. It is the extent and the kind of influence (positive/negative) of a given variable. This new approach was positively tested in [12] on a Quantitative Structure-Activity (QSAR) model for chemical compounds. The results were not only informative about the structure of the model but also provided valuable information for the design of new compounds.\nThe procedure from [12] for the computation of feature contributions applies to random forest models predicting numerical observed values. This paper aims to extend it to random forest models with categorical predictions, i.e., where the observed value determines one from a finite set of classes. The difficulty of achieving this aim lies in the fact that a discrete set of classes does not have the algebraic structure of real numbers which the approach presented in [12] relies on. Due to the high-dimensionality of the calculated feature contributions, their direct analysis is not easy. We develop three techniques for discovering class-specific feature contribution \u201dpatterns\u201d in the decision-making process of random forest models: the analysis of median feature contributions, of clusters and log-likelihoods. This facilitates interpretation of model predictions as well as allows a more detailed analysis of model reliability for an unseen data.\nThe paper is organised as follows. Section 2 provides a brief description of random forest models. Section 3 presents our approach for calculating feature contributions for binary classifiers, whilst Section 4 describes its extension to multi-class classification problems. Section 5 introduces three techniques for finding patterns in feature contributions and linking them to model predictions. Section 6 contains applications of the proposed methodology to two real world datasets from the UCI Machine Learning repository. Section 7 concludes the work presented in this paper."}, {"heading": "2 Random Forest", "text": "A random forest (RF) model introduced by Breiman [5] is a collection of tree predictors. Each tree is grown according to the following procedure [6]:\n1. the bootstrap phase: select randomly a subset of the training dataset \u2013 a local training set for growing the tree. The remaining samples in the training dataset form a so-called out-of-bag (OOB) set and are used to estimate the RF\u2019s goodness-of-fit.\n2. the growing phase: grow the tree by splitting the local training set at each\nnode according to the value of one variable from a randomly selected subset of variables (the best split) using classification and regression tree (CART) method [7].\n3. each tree is grown to the largest extent possible. There is no pruning.\nThe bootstrap and growing phases require an input of random quantities. It is assumed that these quantities are independent between trees and identically distributed. Consequently, each tree can be viewed as sampled independently from the ensemble of all tree predictors for a given training dataset.\nFor prediction, an instance is run through each tree in a forest down to a terminal node which assigns it a class. Predictions supplied by the trees undergo a voting process: the forest returns ca class with the maximum number of votes. Draws are resolved through a random selection.\nTo present our feature contribution procedure in the following section, we have to develop a probabilistic interpretation of the forest prediction process. Denote by C = {C1, C2, . . . , CK} the set of classes and by \u2206K the set\n\u2206K = { (p1, . . . , pK) : K\u2211 k=1 pk = 1 and pk \u2265 0 } .\nAn element of \u2206K can be interpreted as a probability distribution over C. Let ek be an element of \u2206K with 1 at position k \u2013 a probability distribution concentrated at class Ck. If a tree t predicts that an instance i belongs to a class Ck then we write Y\u0302i,t = ek. This provides a mapping from predictions of a tree to the set \u2206K of probability measures on C. Let\nY\u0302i = 1\nT T\u2211 t=1 Y\u0302i,t, (1)\nwhere T is the overall number of trees in the forest. Then Y\u0302i \u2208 \u2206K and the prediction of the random forest for the instance i coincides with a class Ck for which the k-th coordinate of Y\u0302i is maximal.1"}, {"heading": "3 Feature Contributions for Binary Classifiers", "text": "The set \u2206K simplifies considerably when there are two classes, K = 2. An element p \u2208 \u2206K is uniquely represented by its first coordinate p1 (p2 = 1 \u2212\n1The distribution Y\u0302i is calculated by the function predict in the R package randomForest [13] when the type of prediction is set to prob.\np1). Consequently, the set of probability distributions on C is equivalent to the probability weight assigned to class C1.\nBefore we present our method for computing feature contributions, we have to examine the tree growing process. After selecting a training set, it is positioned in the root node. A splitting variable (feature) and a splitting value are selected and the set of instances is split between the left and the right child of the root node. The procedure is repeated until all instances in a node are in the same class or further splitting does not improve prediction. The class that a tree assigns to a terminal node is determined through majority voting between instances in that node.\nWe will refer to instances of the local training set that pass through a given node as the training instances in this node. The fraction of the training instances in a node n belonging to class C1 will be denoted by Y nmean. This is the probability that a randomly selected element from the training instances in this node is in the first class. In particular, a terminal node is assigned to class C1 if Y nmean > 0.5 or Y nmean = 0.5 and the draw is resolved in favor of class C1.\nThe feature contribution procedure for a given instance involves two steps: 1) the calculation of local increments of feature contributions for each tree and 2) the aggregation of feature contributions over the forest. A local increment corresponding to a feature f between a parent node (p) and a child node (c) in a tree is defined as follows:\nLIcf =  Y cmean \u2212 Y pmean, if the split in the parent is performed over the feature f ,\n0, otherwise.\nA local increment for a feature f represents the change of the probability of being in class C1 between the child node and its parent node provided that f is the splitting feature in the parent node. It is easy to show that the sum of these changes, over all features, along the path followed by an instance from the root node to the terminal node in a tree is equal to the difference between Ymean in the terminal and the root node.\nThe contribution FCfi,t of a feature f in a tree t for an instance i is equal to the sum of LIf over all nodes on the path of instance i from the root node to a terminal node. The contribution of a feature f for an instance i in the forest is then given by\nFCfi = 1\nT T\u2211 t=1 FCfi,t. (2)\nThe feature contributions vector for an instance i consists of contributions FCfi of all features f .\nNotice that if the following condition is satisfied:\n(U) for every tree in the forest, local training instances in each terminal node are of the same class\nthen Y\u0302i representing forest\u2019s prediction (1) can be written as Y\u0302i = ( Y r + \u2211 f FCfi , 1\u2212 Y r \u2212 \u2211 f FCfi ) (3)\nwhere Y r is the coordinate-wise average of Ymean over all root nodes in the forest. If this unanimity condition (U) holds, feature contributions can be used to retrieve predictions of the forest. Otherwise, they only allow for the interpretation of the model."}, {"heading": "3.1 Example", "text": "We will demonstrate the calculation of feature contributions on a toy example using a subset of the UCI Iris Dataset [3]. From the original dataset, ten records were selected \u2013 five for each of two types of the iris plant: versicolor (class 0) and virginica (class 1) (see Table 1). A plant is represented by four attributes: Sepal.Length (f1), Sepal.Width (f2), Petal.Length (f3) and Petal.Width (f4). This dataset was used to generate a random forest model with two trees, see Figure 1. In each tree, the local training set (LD) in the root node collects those records which were chosen by the random forest algorithm to build that tree. The LD sets in the child nodes correspond to the split of the above set according to the value of a selected feature (it is written between branches). This process is repeated until reaching terminal nodes of the tree. Notice that the condition (U) is satisfied \u2013 for both trees, each terminal node contains local training instances of the same class: Ymean is either 0 or 1.\nThe process of calculating feature contributions runs in 2 steps: the determination of local increments for each node in the forest (a preprocessing step) and the calculation of feature contributions for a particular instance. Figure 1 shows Y nmean and the local increment LI c f for a splitting feature f in each node. Having computed these values, we can calculate feature contributions for an instance by running it through both trees and summing local increments of each of the four features. For example, the contribution of a given feature for the instance x1 is calculated by summing local increments for that feature along the path p1 = n0 \u2192 n1 in tree T1 and the path p2 = n0 \u2192 n1 \u2192 n4 \u2192 n5 in tree T2. According to Formula (2) the contribution of feature f2 is calculated as\nFCf2x1 = 1\n2\n( 0 + 1\n4\n) = 0.125\nand the contribution of feature f3 is\nFCf3x1 = 1\n2\n( \u2212 3\n7 \u2212 9 28 \u2212 1 2\n) = \u22120.625.\nThe contributions of features f1 and f4 are equal to 0 because these attributes are not used in any decision made by the forest. The predicted probability Y\u0302x1 that x1 belongs to class 1 (see Formula (3)) is\nY\u0302x1 = 1\n2 (3 7 + 4 7 ) \ufe38 \ufe37\ufe37 \ufe38\nY\u0302 r\n+ ( 0 + 0.125\u2212 0.625 + 0 )\ufe38 \ufe37\ufe37 \ufe38\u2211 f FC f x1 = 0.0\nTable 2 collects feature contributions for all 10 records in the example dataset. These results can be interpreted as follows:\n\u2022 for instances x1, x3, the contribution of f2 is positive, i.e., the value of this feature increases the probability of being in class 1 by 0.125. However, the large negative contribution of the feature f3 implies that the value of this feature for instances x1 and x3 was decisive in assigning the class 0 by the forest.\n\u2022 for instances x6, x7, x9, the decision is based only on the feature f3.\n\u2022 for instances x2, x4, x5, the contribution of both features leads the forest decision towards class 0.\n\u2022 for instances x8, x10, Y\u0302 is 0.5. This corresponds to the case where one of the trees points to class 0 and the other to class 1. In practical applications, such situations are resolved through a random selection of the class. Since Y\u0302 r = 0.5, the lack of decision of the forest has a clear interpretation in terms of feature contributions: the amount of evidence in favour of one class is counterbalanced by the evidence pointing towards the other."}, {"heading": "4 Feature Contributions for General Classifiers", "text": "When K > 2, the set \u2206K cannot be described by a one-dimensional value as above. We, therefore, generalize the quantities introduced in the previous section to a multi-dimensional case. Y nmean in a node n is an element of \u2206K , whose k-th coordinate, k = 1, 2, . . . , K, is defined as\nY nmean,k = |{i \u2208 TS(n) : i \u2208 Ck}|\n|TS(n)| , (4)\nwhere TS(n) is the set of training instances in the node n and | \u00b7 | denotes the number of elements of a set. Hence, if an instance is selected randomly from a local training set in a node n, the probability that this instance is in class Ck is given by the k-th coordinate of the vector Y nmean. Local increment LI c f is analogously generalized to a multidimensional case:\nLIcf =  Y cmean \u2212 Y pmean, if the split in the parent is performed over the feature f ,\n(0, . . . , 0)\ufe38 \ufe37\ufe37 \ufe38 K times , otherwise,\nwhere the difference is computed coordinate-wise. Similarly, FCfi,t and FC f i are extended to vector-valued quantities. Notice that if the condition (U) is satisfied, Equation (3) holds with Y r being a coordinate-wise average of vectors Y nmean over all root nodes n in the forest.\nTake an instance i and let Ck be the class to which the forest assigns this instance. Our aim is to understand which variables/features drove the forest to make that prediction. We argue that the crucial information is the one which explains the value of the k-th coordinate of Y\u0302i. Hence, we want to study the k-th coordinate of FCfi for all features f .\nPseudo-code to calculate feature contributions for a particular instance towards the class predicted by the random forest is presented in Algorithm 1. Its inputs consist of a random forest model RF and an instance i which is represented as a vector of feature values. In line 1, k \u2208 {1, 2, . . . , K} is assigned the index of a class predicted by the random forest RF for the instance i. The following line creates a vector of real numbers indexed by features and initialized to 0.\nThen for each tree in the forest RF the instance i is run down the tree and feature contributions are calculated. The quantity SplitFeature(parent) identifies a feature f on which the split is performed in the node parent. If the value i(f) of that feature f for the instance i is lower or equal to the threshold SplitV alue(parent), the route continues to the left child of the node parent. Otherwise, it goes to the right child (each node in the tree has either two children or is a terminal node). A position corresponding to the feature f in the vector FC is updated according to the change of value of Ymean,k, i.e., the k-th coordinate of Ymean, between the parent and the child.\nAlgorithm 1 FC(RF ,i) 1: k \u2190 forest predict(RF, i) 2: FC \u2190 vector(features) 3: for each tree T in forest F do 4: parent\u2190 root(T ) 5: while parent ! = TERMINAL do 6: f \u2190 SplitFeature(parent) 7: if i[f ] <= SplitV alue(parent) then 8: child\u2190 leftChild(parent) 9: else 10: child\u2190 rightChild(parent) 11: end if 12: FC[f ]\u2190 FC[f ] + Y childmean,k \u2212 Y parent mean,k 13: parent\u2190 child 14: end while 15: end for 16: FC \u2190 FC / nTrees(F ) 17: return FC\nAlgorithm 2 provides a sketch of the preprocessing step to compute Y nmean for all nodes n in the forest. The parameter D denotes the set of instances used for training of the forest RF . In line 2, TS is assigned the set used for growing tree T . This set is further split in nodes according to values of splitting variables. We propose to use DFS (depth first search [9]) to traverse the tree and compute the vector Y nmean once a training set for a node n is determined. There is no need to store a training set for a node n once Y nmean has been calculated.\nAlgorithm 2 Ymean(RF,D) 1: for each tree T in forest F do 2: TS \u2190 training set for tree T 3: use DFS algorithm to compute training sets in all other nodes n of tree T\nand compute the vector Y nmean according to formula (4). 4: end for"}, {"heading": "5 Analysis of Feature Contributions", "text": "Feature contributions provide the means to understand mechanisms that lead the model towards particular predictions. This is important in chemical or biological applications where the additional knowledge of the forest\u2019s decision-making process can inform the development of new chemical compounds or explain their interactions with living organisms. Feature contributions may also be useful for assessing the reliability of model predictions for unseen instances. They provide complementary information to forest\u2019s voting results. This section proposes three techniques for finding patterns in the way a random forest uses available features and linking these patterns with the forest\u2019s predictions."}, {"heading": "5.1 Median", "text": "The median of a sequence of numbers is such a value that the number of elements bigger than it and the number of elements smaller than it is identical. When the number of elements in the sequence is odd, this is the central elements of the sequence. Otherwise, it is common to take the midpoint between the two most central elements. In statistics, the median is an estimator of the expectation which is less affected by outliers than the sample mean. We will use this property of the median to find a \u201cstandard level\u201d of feature contributions for representatives of a particular class. This standard level will facilitate an understanding of which features are instrumental for the classification. It can also be used to judge the reliability of forest\u2019s prediction for an unseen instance.\nFor a given random forest model, we select those instances from the training dataset that are classified correctly. We calculate the medians of contributions of every feature separately for each class. The medians computed for one class are combined into a vector which is interpreted as providing the aforementioned \u201cstandard level\u201d for this class. If most of instances from the training dataset belonging to a particular class are close to the corresponding vector of medians, we may treat this vector justifiably as a standard level. When a prediction is requested for a new instance, we query the random forest model for the fraction of trees voting for each class and calculate feature contributions leading to its final prediction.\nIf a high fraction of trees votes for a given class and the feature contributions are close to the standard level for this class, we may reasonably rely on the prediction. Otherwise we may doubt the random forest model prediction.\nIt may, however, happen that many instances from the training dataset correctly predicted to belong to a particular class are distant from the corresponding vector of medians. This might suggest that there is more than one standard level, i.e., there are multiple mechanisms relating features to correct classes. The next subsection presents more advanced methods capable of finding a number of standard levels \u2013 distinct patterns followed by the random forest model in its prediction process."}, {"heading": "5.2 Cluster Analysis", "text": "Clustering is an approach for grouping elements/objects according to their similarity [10]. This allows us to discover patterns that are characteristic for a particular group. As we discussed above, feature contributions in one class may have more than one \u201dstandard level\u201d. When this is discovered, clustering techniques can be employed to find if there is a small number of distinct standard levels, i.e., feature contributions of the instances in the training dataset group around a few points with only a relatively few instances being far away from them. These few instances are then treated as unusual representatives of a given class. We shall refer to clusters of instances around these standard levels as \u201dcore clusters\u201d.\nThe analysis of core clusters can be of particular importance for applications. For example, in the classification of chemical compounds, the split into clusters may point to groups of compounds with different mechanisms of activity. We should note that the similarity of feature contributions does not imply that particular features are similar. We examined several examples and noticed that clustering based upon the feature values did not yield useful results whereas the same method applied to feature contributions was able to determine a small number of core clusters.\nFigure 2 demonstrates the process of analysis of model reliability for a new instance using cluster analysis. In a preprocessing phase, feature contributions for instances in the training dataset are obtained. The optimal number of clusters for each class can be estimated by using, e.g., the Akaike information criterion (AIC), the Bayesian information criterion (BIC) or the Elbow method [10, 14]. We noticed that these methods should not be rigidly adhered to: their underlying assumption is that the data is clustered and we only have to determine the number of these clusters. As we argued above, we expect feature contributions for various instances to be grouped into a small number of clusters and we accept a reasonable number outliers interpreted as unusual instances for a given class. Clustering algorithms try to push those outliers into clusters, hence increasing their number\nunnecessarily. We recommend, therefore, to treat the calculated optimal number of clusters as the maximum value and consecutively decrease it looking at the structure and performance of the resulting clusters: for each cluster we assess the average fraction of trees voting for the predicted class across the instances belonging to this cluster as well as the average distance from the centre of the cluster. Relatively large clusters with the former value close to 1 and the latter value small form the group of core clusters.\nTo assess the reliability of the model prediction for a new instance, we recommend looking at two measures: the fraction of trees voting for the predicted class as well as the cluster to which the instance is assigned based on its feature contributions. If the cluster is one of the core clusters and the distance from its centre is relatively small, the instance is a typical representative of its predicted class. This together with high decisiveness of the forest suggests that the model\u2019s prediction should be trusted. Otherwise, we should allow for an increased chance of misclassification."}, {"heading": "5.3 Log-likelihood", "text": "Feature contributions for a given instance form a vector in a multi-dimensional Euclidean space. Using a popular k-mean clustering method, for each class we\ndivide vectors corresponding to feature contributions of instances in the training dataset into groups minimizing the Euclidean distance from the centre in each group. Figure 3 shows a box-plot of feature contributions for instances in a core cluster in a hypothetical random forest model. Notice that some features are stable within a cluster \u2013 the height of the box is small. Others (F1 and F4) display higher variability. One would therefore expect that the same divergence of contributions for features F3 and F4 from their mean value should be treated differently. It is more significant for the feature F3 than for the feature F4. This is unfortunately not taken into account when the Euclidean distance is considered. Here, we propose an alternative method for assessing the distance from the cluster centre which takes into account the variation of feature contributions within a cluster. Our method has probabilistic roots and we shall present it first from a statistical point of view and provide other interpretations afterwards.\nWe assume that feature contributions for instances within a cluster share the same base values (\u00b5f ) - the centre of the cluster. We attribute all discrepancies between this base value and the actual feature contributions to a random perturbation. These perturbations are assumed to be normally distributed with the mean 0 and the variance \u03c32f , where f denotes the feature. The variance of the perturbation for each feature is selected separately \u2013 we use the sample variance computed from feature contributions of instances of the training dataset belonging to this cluster. Although it is clear that perturbations for different features exhibit some dependence, it is impossible to assess it given the number of instances in a cluster\nand a large number of features typically in use.2 Therefore, we resort to a common solution: we assume that the dependence between perturbations is small enough to justify treating them as independent. Summarising, our statistical model for the distribution of feature contributions within a cluster is as follows: feature contributions for instances within a cluster are composed of a base value and a random perturbation which is normally distributed and independent between features.\nTake an instance iwith feature contributions FCfi . The log-likelihood of being in a cluster with the centre (\u00b5f ) and variances of perturbations (\u03c32f ) is given by\nLLi = \u2211 f ( \u2212 (FC f i \u2212 \u00b5f )2 2\u03c32f \u2212 1 2 log(2\u03c0\u03c32f ) ) . (5)\nThe higher the log-likelihood the bigger the chance of feature contributions of the instance i to belong to the cluster. Notice that the above sum takes into account the observations we made at the beginning of this subsection. Indeed, as the second term in the sum above is independent of the considered instance, the log-likelihood is equivalent to \u2211\nf\n( \u2212 (FC f i \u2212 \u00b5f )2\n2\u03c32f\n) ,\nwhich is the negative of the squared weighted Euclidean distance between FCfi and \u00b5f . The weights being inversely proportional to the variability of a given feature contribution in the training instances in the cluster. In our toy example of Figure 3, this corresponds to penalizing more for discrepancies for features F2 and F3, and significantly less for discrepancies for features F1 and F4.\nIn the following section, we analyse relations between the log-likelihood and classification for a UCI Breast Cancer Wisconsin Dataset."}, {"heading": "6 Applications", "text": "In this section, we demonstrate how the techniques from the previous section can be applied to improve understanding of a random forest model. We consider one example of a binary classifier using the UCI Breast Cancer Wisconsin Dataset [1] (BCW Dataset) and one example of a general classifier for the UCI Iris Dataset [3]. We complement our studies with a robustness analysis.\n2A covariance matrix of feature contribution has F (F + 1)/2 distinct entries, where F is the number of features. This value is usually larger than the size of a cluster making it impossible to retrieve useful information about the dependence structure of feature contributions. Application of more advanced methods, such as principal component analysis, is left for future research."}, {"heading": "6.1 Breast Cancer Wisconsin Dataset", "text": "The UCI Breast Cancer Wisconsin Dataset contains characteristics of cell nuclei for 569 breast tissue samples; 357 are diagnosed as benign and 212 as malignant. The characteristics were captured from a digitized image of a fine needle aspirate (FNA) of a breast mass. There are 30 features, three (the mean, the standard error and the average of the three largest values) for each of the following 10 characteristics: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry and fractal dimension. For brevity, we numbered these features from F1 to F30 according to their order in the data file.\nTo reduce correlation between features and facilitate model interpretation, the min-max (minimal-redundancy-maximal-relevance) method was applied and the following features were removed from the dataset: 1, 3, 8, 10, 11, 12, 13, 15, 19, 20, 21, 24, 26. A random forest model was generated on 2/3 randomly selected instances using 500 trees. The other 1/3 of instances formed the testing dataset. The validation showed that the model accuracy was 0.9682 (only 6 instances out of 189 were classified incorrectly); similar accuracy was achieved when the model was generated using all the features.\nWe applied our feature contribution algorithm to the above random forest binary classifier. To align notation with the rest of the paper, we denote the class \u201cmalignant\u201d by 1 and the class \u201cbenign\u201d by 0. Aggregate results for the feature contributions for all training instances and both classes are presented in Figure 4. Light-grey bars show medians of contributions for instances of class 0, whereas black bars show medians of contributions for instances of class 1 (malignant). Notice that there are only a few significant features in the graph: F4 \u2013 the mean of the cell area, F7 \u2013 the mean of the cell concavity, F14 \u2013 the standard deviation of the cell area, F23 \u2013 the average of three largest measurements of the cell perimeter and F28 \u2013 the average of three largest measurements of concave points. This selection of significant features is perfectly in agreement with the results of the permutation based variable importance (the left panel of Figure 5) and the Gini importance (the right panel of Figure 5). Interpreting the size of bars as the level of importance of a feature, our results are in line with those provided by the Gini index. However, the main advantage of the approach presented in this paper lies in the fact that one can study the reasons for the forest\u2019s decision for a particular instance.\nComparison of feature contributions for a particular instance with medians of feature contributions for all instances of one class provides valuable information about the forest\u2019s prediction. Take an instance predicted to be in class 1. In a typical case when the large majority of trees votes for class 1 the feature contributions for that instance are very close to the median values (see Figure 6a). This happens for around 80% of all instances from the testing dataset predicted to be\nbreastrfmtest\nin class 1. However, when the decision is less unanimous, the analysis of feature contributions may reveal interesting information. As an example, we have chosen instances 194 and 537 (see Table 3) which were classified correctly as malignant (class 1) by a majority of trees but with a significant number of trees expressing an opposite view. Figure 6b presents feature contributions for these two instances (grey and light grey bars) against the median values for class 1 (black bars). The largest difference can be seen for the contributions of very significant features F23, F4 and F14: it is highly negative for the two instances under consideration compared to a large positive value commonly found in instances of class 1. Recall that a negative value contributes towards the classification in class 0. There are also three new significant attributes (F2, F22 and F27) that contribute towards the correct classification as well as unusual contributions for features F7 and F28. These newly significant features are judged as only moderately important by both of the variable importance methods in Figure 5. It is, therefore, surprising to note that the contribution of these three new features was instrumental in correctly classifying instances 194 and 537 as malignant. This highlights the fact that features which may not generally be important for the model may, nonetheless, be important for classifying specific instances. The approach presented in this paper is able to identify such features, whilst the standard variable importance measures for random forest cannot."}, {"heading": "6.2 Cluster Analysis and Log-likelihood", "text": "The training dataset previously derived for the BCW Dataset was partitioned according to the true class labels. A clustering algorithm implemented in the R package kmeans was run separately for each class. This resulted in the determination of three clusters for class 0 and three clusters for class 1. The structure and size of clusters is presented in Table 4. Each class has one large cluster: cluster 3 for class 0 and cluster 2 for class 1. Both have a bigger concentration of points around the cluster centre (small average distance) than the remaining clusters. This suggests that there is exactly one core cluster corresponding to a class. This explains the success of the analysis based on the median as the vectors of\nmedians are close to the centres of unique core clusters.\nFigure 7 lends support to our interpretation of core clusters. The left panel shows the box-plot of the fraction of trees voting for class 0 among training instances belonging to each of the three clusters. A value close to one represents predictions for which the forest is nearly unanimous. This is the case for cluster 3. Two other clusters comprise around 10% of the training instances for which the random forest model happened to be less decisive. A similar pattern can be observed in the case of class 1, see the right panel of the same figure. The unanimity of the forest is observed for the most numerous cluster 2 with other clusters showing lower decisiveness. The reason for this becomes clear once one looks at the variability of feature contributions within each cluster, see Figure 8. The upper and lower ends of the box corresponds to 25% and 75% quantiles, whereas the whiskers show the full range of the data. Cluster 2 enjoys a minor variability of all the contributions which supports our earlier claims of the similarity of instances (in terms of their feature contributions) in the core class. One can see much higher variability in two remaining clusters showing that the forest used different features as evidence to classify instances in each of these clusters. Although in cluster 2 all contributions were positive, in clusters 1 and 3 there are features with negative contributions. Recall that a negative value of a feature contribution provides evidence against being in the corresponding class, here class 1.\nBased on the observation that clusters correspond to a particular decisionmaking route for the random forest model, we introduced the log-likelihood as a way to assess the distance of a given instance from the cluster centre, or, in a probabilistic interpretation, to compute the likelihood3 that the instance belongs to the given cluster. It should however be clarified that one cannot compare the likelihood for the core cluster in class 0 with the likelihood for the core cluster in class 1. The likelihood can only be used for comparisons within one cluster: having two instances we can say which one is more likely to belong to a given cluster. By comparing it to a typical likelihood for training instances in a given cluster we\n3The likelihood is obtained by applying the exponential function to the log-likelihood.\ncan further draw conclusions about how well an instance fits that cluster. Figure 9 presents the log-likelihoods for the two core clusters (one for each class) for instances from the testing dataset. Shapes are used to mark instances belonging to each class: circles for class 0 and triangles for class 1. Notice that likelihoods provide a very good split between classes: instances belonging to class 0 have a high log-likelihood for the core cluster of class 0 and rather low log-likelihood for the core cluster of class 1. And vice-versa for instances of class 1."}, {"heading": "6.3 Iris Dataset", "text": "In this section we use the UCI Iris Dataset [3] to demonstrate interpretability of feature contributions for multi-classification models. We generated a random forest model on 100 randomly selected instances. The remaining 50 instances were used to assess the accuracy of the model: 47 out of 50 instances were correctly classified. Then we applied our approach for determining the feature contributions for the generated model. Figure 10 presents medians of feature contributions for each of the three classes. In contrast to the binary classification case, the medians are positive for all classes. A positive feature contribution for a given class\nmeans that the value of this feature directs the forest towards assigning this class. A negative value points towards the other classes.\nFeature contributions provide valuable information about the reliability of random forest predictions for a particular instance. It is commonly assumed that the more trees voting for a particular class, the higher the chance that the forest decision is correct. We argue that the analysis of feature contributions offers a more refined picture. As an example, take two instances: 120 and 150. The first one was classified in class Versicolour (88% of trees voted for this class). The second one was assigned class Virginica with 86% of trees voting for this class. We are, therefore, tempted to trust both of these predictions to the same extent. Table 5 collects feature contributions for these instances towards their predicted classes. Recall that the highest contribution to the decision is commonly attributed to features 3 (Petal.Length) and 4 (Petal Width), see Figure 10. These features also make the highest contributions to the predicted class for instance 150. The indecisiveness of the forest may stem from an unusual value for the feature 1 (Sepal.Length) which points towards a different class. In contrast, the instance 120 shows stan-\ndard (low) contributions of the first two features and unusual contributions of the last two features: very low for feature 3 and high for feature 4. Recall that features 3 and 4 tend to contribute most to the forest\u2019s decision (see Figure 10) with values between 0.25 and 0.35. The low value for feature 3 is non-standard for its predicted class, which increases the chance of it being wrongly classified. Indeed, both instances belong to class Virginica while the forest classified the instance 120 wrongly as class Versicolour and the instance 150 correctly as class Virginica.\nThe cluster analysis of feature contributions for the UCI Iris Dataset revealed that it is sufficient to consider only two clusters for each class. Cluster sizes are 5\nand 45 for class Setosa, 4 and 46 for class Versicolour and 5 and 44 for class Virginica. Core clusters were straightforward to determine: for each class, the largest of the two clusters was selected as the core cluster. Figure 11 displays an analysis of log-likelihoods for all instances in the dataset. For every instance, we computed feature contributions towards each class and calculated log-likelihoods of being in the core clusters of the respective classes. On the graph, each point represents one instance. The coordinate LH1 is the log-likelihood for the core cluster of class Setosa, the coordinate LH2 is the log-likelihood for the core cluster of class Versicolour and the coordinate LH3 is the log-likelihood for the core cluster of class Virginica. Shapes of points show the true classification: class Setosa is represented by circles, Versicolour by triangles and Virginica by diamonds. Notice that points corresponding to instances of the same class tend to group together. This can be interpreted as the existence of coherent patterns in the reasoning of the random forest model."}, {"heading": "6.4 Robustness Analysis", "text": "For the validity of the study of feature contributions, it is crucial that the results are not artefacts of one particular realization of a random forest model but that they convey actual information held by the data. We therefore propose a method for robustness analysis of feature contributions. We will use the UCI Breast Cancer Wisconsin Dataset studied in Subsection 6.1 as an example.\nWe removed instance number 3 from the original dataset to allow us to perform tests with an unseen instance. We generated 100 random forest models with 500 trees with each model built using an independent randomly generated training set with 379 \u2248 2/3\u00b7568 instances. The rest of the dataset for each model was used for its validation. The average model accuracy was 0.963. For each generated model, we collected medians of feature contributions separately for training and testing datasets and each class. The variation of these quantities over models for class 1 and the training dataset are presented using a box plot in Figure 12a. The top of the box is the 75% quantile, the bottom is the 25% quantile, while the bold line in the middle is the median (recalling that this is the median of the median feature contributions across multiple models). Whiskers show the extent of minimal and maximal values for each feature contribution. Notice that the variation between simulations is moderate and conclusions drawn for one realization of the random forest model in Subsection 6.1 would hold for each of the generated 100 random forest models.\nA testing dataset contains those instances that do not take part in the model generation. One can, therefore, expect more errors in the classification of the forest, which, in effect, should imply lower stability of the calculated feature contributions. Indeed, the box plot presented in Figure 12b shows a slight tendency\ntowards increased variability of the feature contributions when compared to Figure 12a. However, these results are qualitatively on a par with those obtained on the training datasets. We can, therefore, conclude that feature contributions computed for a new (unseen) instance provide reliable information. We further tested this hypothesis by computing feature contributions for instance number 3 that did not take part in the generation of models. The statistics for feature contributions for this instance over 100 random forest models are shown in Figure 12c. Similar results were obtained for other instances."}, {"heading": "7 Conclusions", "text": "Feature contributions provide a novel approach towards black-box model interpretation. They measure the influence of variables/features on the prediction outcome and provide explanations as to why a model makes a particular decision. In this work, we extended the feature contribution method of [12] to random forest classification models and we proposed three techniques (median, cluster analysis and log-likelihood) for finding patterns in the random forest\u2019s use of available features. Using UCI benchmark datasets we showed the robustness of the proposed methodology. We also demonstrated how feature contributions can be applied to understand the dependence between instance characteristics and their predicted classification and to assess the reliability of the prediction. The relation between feature contributions and standard variable importance measures was also investigated. The software used in the empirical analysis was implemented in R as an add-on for the randomForest package and is currently being prepared for submission to CRAN [2] under the name rfFC.\nAcknowledgements. This work is partially supported by BBSRC and Syngenta Ltd through the Industrial CASE Studentship Grant (No. BB/H530854/1)."}], "references": [{"title": "How to explain individual classification decisions", "author": ["David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "Klaus-Robert Muller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Classification and regression trees", "author": ["Leo Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": "Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1984}, {"title": "Interpretation of nonlinear qsar models applied to ames mutagenicity data", "author": ["Lars Carlsson", "Ernst Ahlberg Helgee", "Scott Boyer"], "venue": "Journal of Chemical Information and Modeling,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Introduction to Algorithms", "author": ["Thomas H. Cormen", "Clifford Stein", "Ronald L. Rivest", "Charles E. Leiserson"], "venue": "McGraw-Hill Higher Education,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Principles of data mining", "author": ["David J. Hand", "Padhraic Smyth", "Heikki Mannila"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Visual interpretation of kernel-based prediction models", "author": ["Katja Hansen", "David Baehrens", "Timon Schroeter", "Matthias Rupp", "Klaus- Robert Muller"], "venue": "Molecular Informatics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Interpretation of qsar models based on random forest methods", "author": ["Victor E. Kuz\u2019min", "Pavel G. Polishchuk", "Anatoly G. Artemenko", "Sergey A. Andronati"], "venue": "Molecular Informatics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Classification and regression by randomforest", "author": ["Andy Liaw", "Matthew Wiener"], "venue": "R News,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Mining of Massive Datasets", "author": ["Anand Rajaraman", "Jeffrey D. Ullman"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Interpreting linear support vector machine models with heat map molecule coloring", "author": ["Lars Rosenbaum", "Georg Hinselmann", "Andreas Jahn", "Andreas Zell"], "venue": "Journal of Cheminformatics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Conditional variable importance for random forests", "author": ["Carolin Strobl", "Anne-Laure Boulesteix", "Thomas Kneib", "Thomas Augustin", "Achim Zeileis"], "venue": "BMC Bioinformatics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Best practices for qsar model development, validation, and exploitation", "author": ["Alexander Tropsha"], "venue": "Molecular Informatics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}], "referenceMentions": [{"referenceID": 11, "context": "Good practice of model development [17] involves: 1) data analysis 2) feature selection, 3) model building and 4) model evaluation.", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "But, how to interpret an existing model? How to analyse the relation between predicted values and the training dataset? Or which features contribute the most to classify a specific instance? Answers to these questions are considered particularly valuable in such domains as chemoinformatics, bioinformatics or predictive toxicology [15].", "startOffset": 332, "endOffset": 336}, {"referenceID": 2, "context": "In [8], the authors present a method for a local interpretation of Support Vector Machine (SVM) and Random Forest models by retrieving the variable corresponding to the largest component of the decision-function gradient at any point in the model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Interpretation of classification models using local gradients is discussed in [4].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "A method for visual interpretation of kernel-based prediction models is described in [11].", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Another approach, which is presented in detail later, was proposed in [12] and aims at shedding light at decision-making process of regression random forests.", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "However, in [16], the authors showed that the above measures are biased in favor of continuous variables and variables with many categories.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "propose in [12] a new technique to calculate a feature contribution, i.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "This new approach was positively tested in [12] on a Quantitative Structure-Activity (QSAR) model for chemical compounds.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "The procedure from [12] for the computation of feature contributions applies to random forest models predicting numerical observed values.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "The difficulty of achieving this aim lies in the fact that a discrete set of classes does not have the algebraic structure of real numbers which the approach presented in [12] relies on.", "startOffset": 171, "endOffset": 175}, {"referenceID": 1, "context": "node according to the value of one variable from a randomly selected subset of variables (the best split) using classification and regression tree (CART) method [7].", "startOffset": 161, "endOffset": 164}, {"referenceID": 7, "context": "An element p \u2208 \u2206K is uniquely represented by its first coordinate p1 (p2 = 1 \u2212 1The distribution \u0176i is calculated by the function predict in the R package randomForest [13] when the type of prediction is set to prob.", "startOffset": 168, "endOffset": 172}, {"referenceID": 3, "context": "We propose to use DFS (depth first search [9]) to traverse the tree and compute the vector Y n mean once a training set for a node n is determined.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "Clustering is an approach for grouping elements/objects according to their similarity [10].", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": ", the Akaike information criterion (AIC), the Bayesian information criterion (BIC) or the Elbow method [10, 14].", "startOffset": 103, "endOffset": 111}, {"referenceID": 8, "context": ", the Akaike information criterion (AIC), the Bayesian information criterion (BIC) or the Elbow method [10, 14].", "startOffset": 103, "endOffset": 111}, {"referenceID": 6, "context": "In this work, we extended the feature contribution method of [12] to random forest classification models and we proposed three techniques (median, cluster analysis and log-likelihood) for finding patterns in the random forest\u2019s use of available features.", "startOffset": 61, "endOffset": 65}], "year": 2013, "abstractText": "Model interpretation is one of the key aspects of the model evaluation process. The explanation of the relationship between model variables and outputs is relatively easy for statistical models, such as linear regressions, thanks to the availability of model parameters and their statistical significance. For \u201cblack box\u201d models, such as random forest, this information is hidden inside the model structure. This work presents an approach for computing feature contributions for random forest classification models. It allows for the determination of the influence of each variable on the model prediction for an individual instance. By analysing feature contributions for a training dataset, the most significant variables can be determined and their typical contribution towards predictions made for individual classes, i.e., class-specific feature contribution \u201dpatterns\u201d, are discovered. These patterns represent a standard behaviour of the model and allow for an additional assessment of the model reliability for a new data. Interpretation of feature contributions for two UCI benchmark datasets shows the potential of the proposed methodology. The robustness of results is demonstrated through an extensive analysis of feature contributions calculated for a large number of generated random forest models. \u2217a.m.wojak@bradford.ac.uk \u2020j.palczewski@leeds.ac.uk \u2021r.l.marcheserobinson@ljmu.ac.uk \u00a7d.neagu@bradford.ac.uk 1 ar X iv :1 31 2. 11 21 v1 [ cs .L G ] 4 D ec 2 01 3", "creator": "TeX"}}}