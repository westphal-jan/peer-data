{"id": "1312.4384", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Rectifying Self Organizing Maps for Automatic Concept Learning from Web Images", "abstract": "we undertake the problem of learning concepts independently from noisy homogeneous image search algorithm. going beyond low item attributes, considering new colour matrix texture, we explore object - labelled datasets for effective learning of common level concepts, such include scene categories. the idea is devised for discovering common artifacts shared bearing similarities to graphs by with a method the is able to organise the spectral stream eliminating irrelevant instances. we derive a symmetric constraint database outlier detection option, directly rectifying coherent organizing matrices ( rsom ). given inconsistent image collection returned for a static test, rsom provides perfectly pruned redundant outliers. whether dimension is used one train a query spanning a different characteristics requiring video viewer. the primary method outperforms the state - since - the - art studies on primary task model learning low - level concepts, and it is done effectively operating these system concepts encoding algorithms. it is targeted to work at large complexity with no supervision through exploiting efficiently available sources.", "histories": [["v1", "Mon, 16 Dec 2013 14:51:00 GMT  (6565kb,D)", "http://arxiv.org/abs/1312.4384v1", "present CVPR2014 submission"]], "COMMENTS": "present CVPR2014 submission", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["eren golge", "pinar duygulu"], "accepted": false, "id": "1312.4384"}, "pdf": {"name": "1312.4384.pdf", "metadata": {"source": "CRF", "title": "Rectifying Self Organizing Maps for Automatic Concept Learning from Web Images", "authors": ["Eren Golge", "Pinar Duygulu"], "emails": ["eren.golge@bilkent.edu.tr", "pinar.duygulu@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "The need for manually labelled data continues to be one of the most important limitations in large scale object/scene recognition. Recently, use of visual attributes have become attractive as being helpful in describing properties shared by multiple categories and resulting in novel category recognition. owever, most of the methods require learning of visual attributes from labelled data, and cannot eliminate human effort. Yet, it may be more difficult to describe an attribute than an object, and localisation may not be trivial.\nAlternatively, images tagged with attribute names are available on the web in large amounts. However, data collected from web inherits all type of challenges due to illumination, reflection, scale, and pose variations as well as camera and compression effects[22]. Most importantly, the collection is very noisy with several irrelevant images as well as variety of images corresponding to different char-\nacteristic properties of the attribute (Figure1). Localisation of attributes inside the images arises as another important issue. The region corresponding to the attribute may cover only a fraction of the image, or the same attribute may be in different forms in different parts of an image.\nFor the data collected through querying to be beneficial for automatic learning of attributes, we propose a novel method to obtain an organised collection with irrelevant images removed. Our intuition is that, given an attribute category defined by the query word, although the list of images returned is likely to include irrelevant ones, there will be some common characteristics shared among subset of images. Our main idea is to obtain visually coherent subsets, that are possibly corresponding to semantic subcategories, through clustering and to build models for each sub-category (see Figure2). The model for each attribute category is then a collection of multiple models, each representing a different property of the attribute.\nWe aim to answer not only \u201dwhich attribute is in the image?\u201d, but also \u201dwhere the attribute is?\u201d. For this purpose, we consider image patches as the basic units for providing localisation. To retain only the relevant patches that describe the attribute category correctly, during clustering we need to remove outliers, i.e. irrelevant ones. The outliers may resemble to each other while not being similar to the correct category patches resulting in a separate outlier cluster. Alternatively, some outlier patches could be mixed with correct category patches inside salient clusters corresponding to relevant ones. These patches, that we refer to\nar X\niv :1\n31 2.\n43 84\nv1 [\ncs .C\nV ]\n1 6\nD ec\n2 01\nas outlier elements, should also be removed for the data to be sufficiently clean for learning.\nWe propose a novel method Rectifying Self Organizing Maps (RSOM) which improves the well-known Self Organizing Maps (SOM) [7] through detection and elimination of outliers. The purpose of RSOM is to \u201drectify\u201d the data by purifying it not only from outlier clusters but also from outlier elements in salient clusters. It is a generic method for capturing category specific characteristics through organising the set of given instances into sub-categories pruned from irrelevant instances.\nGoing beyond low-level attributes, RSOM is capable of learning higher level concept, as we show through learning scene concepts. In this case, we treat each image as a single instance, and aim to find groups of images representing a different property of scene category, at the same time by eliminating the ones that are either irrelevant, or poor to sufficiently describe any characteristics."}, {"heading": "2. Related work", "text": "The use of attributes has been the focus of many recent studies [2, 8, 1]. In [3], Farhadi et al. learn complex attributes (shape, materials, parts) in a fully supervised way focusing on recognition of new types of objects. In [10], for human labelled animal categories, semantic attribute annotations available from studies in cognitive science were used in a binary fashion for zero-shot learning. In this study, we focus on attribute learning independent of object categories. Torresani et al. [21] introduce classemes, attributes that do not have specific semantic meanings, but meanings expected to emerge from intersections of properties, and they obtain training data directly from web image search. Rastegari et al. [18] propose discovering implicit attributes that are not necessarily semantic but preserve categoryspecific traits through learning discriminative hyperplanes with max-margin and locality sensitive hashing criteria. We learn different intrinsic properties of each attribute through discriminative models obtained from separate clusters that are ultimately combined in a single semantic. Learning semantic appearance attributes, such as colour, texture and\nshape, on large scale ImageNet dataset is attacked in [19] relying on image level human labels using Amazon\u2019s Mechanical Turk for supervised learning. We learn attributes from real world images collected from web with no human effort for labelling. Another study on learning colour names from web images is proposed in [22] where a PLSA based model is used for representing the colour names of pixels. Similar to ours, the approach of Ferrari and Zisserman [4] considers attributes as patterns sharing some characteristic properties where basic units are the image segments with uniform appearance. We prefer to work on patch level alternative to pixel level which is not suitable for region level attributes such as texture, image level which is very noisy, or segment level which is difficult to obtain clearly."}, {"heading": "3. Rectifying Self Organizing Maps", "text": "Revisiting Self Organizing Maps (SOM): Intrinsic dynamics of SOM are inspired from developed animal brain where each part is known to be receptive to different sensory inputs and which has a topographically organized structure[7]. This phenomena, which is called as \u201dreceptive field\u201d in visual neural systems [6], is simulated with SOM, where neurons are represented by weights that are calibrated to make neurons sensitive to different type of inputs. Elicitation of this structure is furnished by competitive learning approach.\ninput with M instances X = {x1, x2..., xM}. Let N = {n1, n2, ..., nK} be the locations of neuron units on the SOM map andW = {w1, w2, ..., wK} be the associated weights. The neuron whose weight vector is most similar to the input instance xi is called as the winner and denoted by v\u0302. The weights of the winner and units in the neighbourhood are adjusted towards the input vector at each iteration t with delta learning rule (Eq.1).\nwtj = w t\u22121 j + h(ni, nv\u0302 : t, \u03c3t)[xi \u2212 wt\u22121j ] (1)\nUpdate step is scaled by the window function h(ni, nv\u0302 : t, \u03c3t) for each SOM unit, inversely proportional to the distance to the winner (Eq.2). Learning rate is a gradually decreasing value, resulting in larger updates at the beginning and finer updates as the algorithm evolves. \u03c3t defines the neighbouring effect so with the decreasing \u03c3, neighbour update steps are getting smaller in each epoch. Note that, there are different alternatives for update and windows functions in SOM literature.\nh(ni, nv\u0302 : t, \u03c3t) = t exp\n\u2212||nj \u2212 nv\u0302||2\n2\u03c3t2 (2)\nClustering and outlier detection with RSOM: We introduce excitation scores E = {e1, e2, . . . , eK} where ej , the score for neuron unit j, is updated as in Eq.3.\netj = e t\u22121 j + \u03c1 t(\u03b2j + zj) (3)\nAs in the original SOM, window function is getting smaller with each iteration.\nHere, zj is the activation or win count for the unit j, for one epoch. \u03c1 is learning solidity scalar that represents the decisiveness of learning with dynamically increasing value, assuming that later stages of the algorithm has more impact on the definition of salient SOM units. \u03c1 is equal to the inverse of the learning rate . \u03b2j is the total measure of the activation of jth unit in an epoch, caused by all the winners of the epoch but the neuron itself (Eq.4).\n\u03b2j = u\u2211 v\u0302=1 h(nj , nv\u0302)zv\u0302 (4)\nAt the end of the iterations, normalized ej is a quality value of a unit j. Higher value of ej indicates that total amount of excitation of the unit j in whole learning period is high thus it is responsive to the given class of instances and it captures notable amount of data. Low excitation values indicate the contrary. RSOM is capable of detecting outlier units via a threshold \u03b8 in the range [0, 1].\nLet C = {c1, c2, . . . , cK} be the cluster centres corresponding to each unit. cj is considered to be a salient cluster if ej \u2265 \u03b8, and an outlier cluster otherwise.\nThe excitation scores E are the measure for saliency of neuron units in RSOM. Given the data belonging to a category, we expect that data is composed of sub-categories that share common properties. For instance red images might include darker or lighter tones to be captured by clusters but they are supposed to share a common characteristics of being red. In that sense, for the calculation of the excitation scores we use individual activations of the units as well as the activations as being in a neighbourhood of another unit. Individual activations measure the saliency of being a salient cluster corresponding to a particular subcategory, such as lighter red. Neighbourhood activations count the saliency in terms of the shared regularity between sub-categories. If we don\u2019t count the neighbourhood effect, some unrelated clusters would be called salient since large number of outlier instances could be grouped in a unit, e.g. noisy white background patches in red images.\nOutlier instances of salient clusters, namely the outlier elements should also be detected. After the detection of outlier neurons, statistics of the distances between neuron weight wi and its corresponding instance vectors (assuming weights prototyping instances grouped by the neuron) is used as a measure of instance divergence. If the distance between the instance vector xj and its winner\u2019s weight w\u0302i is more than the distances of other instances having the same winner, xj is raised as an outlier element. We exploit box plot statistics, similar to [15]. If the distance of the instance to its cluster\u2019s weight is more than the upper-quartile value, then it is detected as an outlier. The portion of the data, covered by the upper whisker is decided by \u03c4 .\nRSOM provides good basis of cleansing of poor instances whereas computing cost is relatively smaller since RSOM is capable of discarding items with one shot of learning phase. Therefore, an additional data cleansing iteration after clustering phase is not required. All the necessary information (excitation scores, box plot statistics) for outliers is calculated at runtime of learning. Hence, RSOM is suitable for large scale problems.\nRSOM is also able to estimate number of intrinsic clusters of the data. We use PCA for that purpose, with defined variation value \u03bd to be captured by the principle components. Given data and \u03bd, principle components are found and number of principle components describing the data with variation \u03bd is used as the number of clusters for the further processing of RSOM. If we increase \u03bd, RSOM latches more clusters therefore \u03bd should be carefully chosen.\nDiscussion of other methods on outlier detection with SOM: [13, 14] utilise the habitation of the instances. Frequently observed similar instances excites the network to learn some regularities and divergent instances are observed as outliers. [5] benefits from weights prototyping the instances in a cluster. Thresholded distance of instances to the weight vectors are considered as indicator of being outlier. In [23], aim is to have different mapping of activated neuron for the outlier instances. The algorithm learns the formation of activated neurons on the network for outlier and inlier items with no threshold. It suffers from the generality, with its basic assumption of learning from network mapping. LTD-KN [20] performs Kohonen learning rule inversely. An instance activates only the winning neuron as in the usual SOM, but LTD-KN updates winning neuron and its learning windows decreasingly.\nThese algorithms only eliminate outlier instances ignoring outlier clusters. RSOM finds outlier clusters as well as the outlier instances in the salient clusters. Another difference of RSOM is the computation cost. Most of outlier detection algorithms model the data and iterate over the data again to label outliers. It is not suitable for large scale data. RSOM has the ability to detect outlier clusters and the items all in the learning phase. Thus, there is no need for learning a model of the data first, then detecting outliers, it is all done in a single pass in our method. With the support of GPGPU programming RSOM scales to large amount of data."}, {"heading": "4. Concept learning with RSOM", "text": ""}, {"heading": "4.1. Learning low-level attributes", "text": "Data collection and clustering: We collect web images through querying for colour and texture names. The data is weakly labelled, with the labels given for the entire image, rather than the specific regions. Most importantly, it includes irrelevant images, as well as images with a tiny portion corresponding to the query keyword. Each image\nis densely divided into non-overlapping fixed-size patches to sufficiently capture the required information. We assume that the large volume of the data itself is sufficient to provide instances at various scales and illuminations, and therefore we did not perform any scaling or normalisation. The collection of all patches extracted from all images for a single attribute is then given to RSOM to obtain clusters which are likely to capture different characteristics of the attribute\nTraining attribute models: Each cluster obtained through RSOM is used to train a separate classifier for the attribute Positive examples are selected as the members of the cluster and negative instances are selected among the outliers removed by RSOM for that attribute and also among random elements from other attribute categories. We use linear SVM classifier with L1 norm.\nLearned models are used for two different purposes: (i) detection of the attributes on novel images, and (ii) recognition of scenes with the help of learned attribute classifiers.\nAttribute recognition on novel images: The goal of this task is to label a given image with a single attribute name. For this purpose, first we divide the image into grids in three levels using spatial pyramiding [11]. Nonoverlapping patches are extracted from each grid in all three levels. Recall that, we have separate classifiers for each salient cluster. We run all of the classifiers on each grid for all patches in all levels. Each grid at each level is labelled by the maximum response classifier among all the outputs for the patches. All of those confidence values are then merged with a weighted sum to a label for the entire image.\nDi = 3\u2211 l=1 Nl\u2211 n=1 1 23\u2212l hie \u2212(x\u0302\u2212x)/2\u03c32 (5)\nHere, Nl is the grid number for level l and hi is the confidence value for grid i. We include a Gaussian filter, where x\u0302 is center of the image and x is location of the spatial pyramid grid, to give more priority to the detections around the center of the image for reducing noisy background effect.\nAttribute based scene recognition: We use the learned low-level attributes to describe an image for the task of scene recognition. Similar to the first task, we get the confidence values for each grid in three levels of the spatial pyramid. However, rather than using a single value for the maximum classifier output, we use the entire vector for each grid. That is, we keep the confidence values for all the classifiers for each grid. Then, we concatenate these vectors for all grids in all levels to get a single feature vector of size 3xNxK for the image, which is then used for scene classification. Here N is the number of grids at each level, and K is the number of different concepts. This rich and high dimensional representation poses good classification performance with simple linear models."}, {"heading": "4.2. Learning higher level concepts", "text": "To show that RSOM is capable of being generalised to higher level concepts, we collected images for scene categories from web to learn these concepts. In this case, we use the entire images as instances, and aim to discover group of images each representing a different property of the scene category. These clusters are then used as models similar to the attribute learning. Specifically, we perform experiments for scene classification for 15 scene categories as in [11]. Note that, we do not use any manually labelled training set, but directly the noisy web images which are pruned and organised by RSOM. This task is also different than the use of low-level attributes for scene recognition, in this case we learn the scene concept directly without requiring any other information."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Qualitative evaluation of clusters", "text": "As Figure3 depicts, RSOM captures different characteristics of concepts in separate salient clusters, while eliminating outlier clusters that group irrelevant images which are coherent among themselves, as well as outlier elements wrongly mixed with the elements of salient clusters."}, {"heading": "5.2. Implementation details", "text": "Parameters of RSOM are selected on a small held-out set. Figure4 depicts the effect of parameters \u03b8, \u03c4 and \u03bd. For each parameter the other two are fixed at the optimum value obtained through cross-validation. SVM parameters are also selected with 10-fold cross validation and grid-search. We end the search process when the\ncurrent accuracy is less than the average accuracy of the 5 to 10 step back. Our RSOM implementation is powered by GPGPU programming over CUDA environment, resulting in a large time reduction."}, {"heading": "5.3. Attribute learning", "text": "Datasets: We collected images from Google for 11 distinct colours as in [22] and 13 textures. We included the terms \u201dcolour\u201d and \u201dtexture\u201d in the queries, such as \u201dred colour\u201d, or \u201dwooden texture\u201d, to reduce the chance of semantic mismatching. For each attribute, 500 images are collected and patches are extracted from each image. To test the results on a human labelled dataset, we use Ebay dataset provided by [22] which has labels for the pixels in cropped regions. Unlike [22], we didn\u2019t apply gamma correction.\nTasks : Evaluation of our framework is performed over two different tasks: (i) detection of the learned attributes on novel images and (ii) recognition of scenes using learned attributes. To evaluate the first task, we use three different datasets. The first dataset is Bing Search Images curated by ourselves from the top 35 images returned with some pruning to eliminate some semantic mismatches. Second dataset is Google Colour Images [22] previously used by [22] for learning colour attributes. We used the data only for testing. The last dataset is sample annotated images from ImageNet [19] for 25 attributes. The second task on scene recognition is performed on MIT-indoor [17], and Scene-15 [11] datasets.\nRepresentation: For color concepts we use 10x20x20 bins Lab colour histograms and for texture concepts we also include BoW representation for densely sampled SIFT [REF] features with 4000 words.\nWe keep the feature dimensions high to utilise from the over-complete representations of the instances when we apply L1 norm linear SVM classifier. We divide the training images into 100x100 non-overlapping patches. We did not perform any scaling or normalization and capture different varieties with crowd of the patches. For testing, we divide the images into 21 grids at each of the three levels.\nResults: Figure 5 compares the accuracy of the proposed method (RSOM) with three other methods on the task of attribute learning. As a baseline method (BL), we use all\nthe images returned for the concept query to train a single model. As the results show, the performance is very low suggesting that the data should be organised to capture the intra-class variations. As two other methods for clustering the data, we used k-means (KM) and original SOM algorithm (SOM), and again train different models for each clusters. The low results support the need for pruning of the data through outlier elimination. Results also show that, on novel test sets with images having different characteristics than the images used in training, RSOM can still perform very well on learning of attributes.\nNote that, on ImageNet dataset, we obtained 37.4% accuracy compared to 36.8% of Russakovsky and Fei-Fei[19].\nOur method is also utilised for retrieving images on EBAY dataset as in [22]. We utilise RSOM with patches obtained from the entire images (RSOM) as well as from the masks provided by [22] (RSOM-M). As shown in Table1, even without masks RSOM is comparable to the performance of the PLSA based method of [22], and with the same setting RSOM outperforms the PLSA based method.\nOn the task of scene recognition with learned attributes, we compare our method (RSOM-A) with state-of-the-art methods on MIT-indoor [17] and Scene-15 [11] datasets. Our method performs competitively with[12] while using shorter feature vectors, and outperforms the others."}, {"heading": "5.4. Learning concepts of scene categories", "text": "As an alternative to recognising scenes through the learned low-level attributes, we directly learn higher level concepts for scene categories. We focus on learning 15 scene concepts used in [11], through collecting images from web for these concepts. We have shown that, our method is competitive with the state-of-the-art studies without requiring any supervised training. We made a slight change on our\noriginal RSOM implementation for recognising scene concepts (which we refer to as RSOM-S) by finding the hard negatives at the first classification and using them in another classification (we refer to this new method as RSOM-SHM). As the results in Figure6 show, we achieve better performances than the state-of-the-art studies with this simple addition, still without requiring any supervisory input."}, {"heading": "6. Conclusion", "text": "In this work we propose Rectifying Self Organizing Maps that is akin to SOM with clustering properties and novel with respect to outlier detection dynamics. We use RSOM for weakly supervised learning of visual concepts from large scale noisy web data. Multiple classifiers are built for each attribute from clusters pruned from outliers, to have each classifier sensitive to a different visual variation. Our experiments show that we are able to capture low level concepts on novel images and have a good basis for higher level recognition tasks like scene recognition with inexpensive setting. We also show that we can directly learn higher level concepts. As the future work, this framework will be extended to capture more localized concepts like objects, and will also be applied to learn concepts from videos."}], "references": [{"title": "Adding unlabeled samples to categories by learned attributes", "author": ["J. Choi", "M. Rastegari", "A. Farhadi", "L.S. Davis"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "and D", "author": ["A. Farhadi", "I. Endres"], "venue": "Hoiem. D.: Attribute-centric recognition for crosscategory generalization. CVPR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "CVPR", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning visual attributes", "author": ["V. Ferrari", "A. Zisserman"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "A kohonen som based", "author": ["T. Harris"], "venue": "machine health monitoring system which enables diagnosis of faults not seen in the training set. Neural Networks. IJCNN\u201993-Nagoya.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1993}, {"title": "Receptive fields", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "binocular interaction and functional architecture in the cat\u2019s visual cortex. The Journal of physiology", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1962}, {"title": "Self-organizing maps", "author": ["T. Kohonen"], "venue": "Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Attribute and simile classifiers for face verification", "author": ["N. Kumar", "A.C. Berg", "P.N. Belhumeur", "S.K. Nayar"], "venue": "ICCV", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Scene recognition on the semantic manifold", "author": ["R. Kwitt", "N. Vasconcelos", "N. Rasiwasia"], "venue": "ECCV", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Harvesting mid-level visual concepts from large-scale internet images", "author": ["Q. Li", "J. Wu", "Z. Tu"], "venue": "CVPR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A model of habituation applied to mobile robots", "author": ["S. Marsland", "U. Nehmzow", "J. Shapiro"], "venue": "Proceedings of Towards Intelligent Mobile Robots", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Novelty Detection for Robot Neotaxis", "author": ["S. Marsland", "U. Nehmzow", "J. Shapiro"], "venue": "Proceedings 2nd NC", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Self-organizing maps for outlier detection", "author": ["A. Mu\u00f1oz", "J. Muruz\u00e1bal"], "venue": "Neurocomputing", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Scene recognition and weakly supervised object localization with deformable part-based models", "author": ["M. Pandey", "S. Lazebnik"], "venue": "ICCV", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Recognizing indoor scenes", "author": ["A. Quattoni", "A. Torralba"], "venue": "CVPR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Attribute discovery via predictable discriminative binary codes", "author": ["M. Rastegari", "A. Farhadi", "D. Forsyth"], "venue": "ECCV", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Attribute learning in largescale datasets", "author": ["O. Russakovsky", "L. Fei-Fei"], "venue": "Trends and Topics in Computer Vision", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Novelty detection in a kohonen-like network with a long-term depression learning rule", "author": ["D. Theofilou", "V. Steuber", "E.D. Schutter"], "venue": "Neurocomputing", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient object category recognition using classemes", "author": ["L. Torresani", "M. Szummer", "A. Fitzgibbon"], "venue": "ECCV", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning color names for real-world applications", "author": ["J. Van De Weijer", "C. Schmid", "J. Verbeek", "D. Larlus"], "venue": "Image Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Novelty detection using self-organizing maps", "author": ["A. Ypma", "E. Ypma", "R.P. Duin"], "venue": "Proc. of ICONIP\u201997", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 21, "context": "However, data collected from web inherits all type of challenges due to illumination, reflection, scale, and pose variations as well as camera and compression effects[22].", "startOffset": 166, "endOffset": 170}, {"referenceID": 6, "context": "We propose a novel method Rectifying Self Organizing Maps (RSOM) which improves the well-known Self Organizing Maps (SOM) [7] through detection and elimination of outliers.", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "The use of attributes has been the focus of many recent studies [2, 8, 1].", "startOffset": 64, "endOffset": 73}, {"referenceID": 7, "context": "The use of attributes has been the focus of many recent studies [2, 8, 1].", "startOffset": 64, "endOffset": 73}, {"referenceID": 0, "context": "The use of attributes has been the focus of many recent studies [2, 8, 1].", "startOffset": 64, "endOffset": 73}, {"referenceID": 2, "context": "In [3], Farhadi et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "In [10], for human labelled animal categories, semantic attribute annotations available from studies in cognitive science were used in a binary fashion for zero-shot learning.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "[21] introduce classemes, attributes that do not have specific semantic meanings, but meanings expected to emerge from intersections of properties, and they obtain training data directly from web image search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] propose discovering implicit attributes that are not necessarily semantic but preserve categoryspecific traits through learning discriminative hyperplanes with max-margin and locality sensitive hashing criteria.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Learning semantic appearance attributes, such as colour, texture and shape, on large scale ImageNet dataset is attacked in [19] relying on image level human labels using Amazon\u2019s Mechanical Turk for supervised learning.", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "Another study on learning colour names from web images is proposed in [22] where a PLSA based model is used for representing the colour names of pixels.", "startOffset": 70, "endOffset": 74}, {"referenceID": 3, "context": "Similar to ours, the approach of Ferrari and Zisserman [4] considers attributes as patterns sharing some characteristic properties where basic units are the image segments with uniform appearance.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "Revisiting Self Organizing Maps (SOM): Intrinsic dynamics of SOM are inspired from developed animal brain where each part is known to be receptive to different sensory inputs and which has a topographically organized structure[7].", "startOffset": 226, "endOffset": 229}, {"referenceID": 5, "context": "This phenomena, which is called as \u201dreceptive field\u201d in visual neural systems [6], is simulated with SOM, where neurons are represented by weights that are calibrated to make neurons sensitive to different type of inputs.", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "RSOM is capable of detecting outlier units via a threshold \u03b8 in the range [0, 1].", "startOffset": 74, "endOffset": 80}, {"referenceID": 14, "context": "We exploit box plot statistics, similar to [15].", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "Discussion of other methods on outlier detection with SOM: [13, 14] utilise the habitation of the instances.", "startOffset": 59, "endOffset": 67}, {"referenceID": 13, "context": "Discussion of other methods on outlier detection with SOM: [13, 14] utilise the habitation of the instances.", "startOffset": 59, "endOffset": 67}, {"referenceID": 4, "context": "[5] benefits from weights prototyping the instances in a cluster.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "In [23], aim is to have different mapping of activated neuron for the outlier instances.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "LTD-KN [20] performs Kohonen learning rule inversely.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "For this purpose, first we divide the image into grids in three levels using spatial pyramiding [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "Specifically, we perform experiments for scene classification for 15 scene categories as in [11].", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": "Datasets: We collected images from Google for 11 distinct colours as in [22] and 13 textures.", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "To test the results on a human labelled dataset, we use Ebay dataset provided by [22] which has labels for the pixels in cropped regions.", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "Unlike [22], we didn\u2019t apply gamma correction.", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "Second dataset is Google Colour Images [22] previously used by [22] for learning colour attributes.", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "Second dataset is Google Colour Images [22] previously used by [22] for learning colour attributes.", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "The last dataset is sample annotated images from ImageNet [19] for 25 attributes.", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "The second task on scene recognition is performed on MIT-indoor [17], and Scene-15 [11] datasets.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "The second task on scene recognition is performed on MIT-indoor [17], and Scene-15 [11] datasets.", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "As a baseline method (BL), we use all Bing Google [22] ImageNet [19] EBAY [22] 0.", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "As a baseline method (BL), we use all Bing Google [22] ImageNet [19] EBAY [22] 0.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "As a baseline method (BL), we use all Bing Google [22] ImageNet [19] EBAY [22] 0.", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "Method RSOM-M RSOM PLSA-reg [22].", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "Equal Error Rates on EBAY dataset for image retrieval using the configuration of [22].", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "RSOM does not utilise the image masks used in [22], while RSOM-M does.", "startOffset": 46, "endOffset": 50}, {"referenceID": 18, "context": "8% of Russakovsky and Fei-Fei[19].", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Our method is also utilised for retrieving images on EBAY dataset as in [22].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "We utilise RSOM with patches obtained from the entire images (RSOM) as well as from the masks provided by [22] (RSOM-M).", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "As shown in Table1, even without masks RSOM is comparable to the performance of the PLSA based method of [22], and with the same setting RSOM outperforms the PLSA based method.", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "On the task of scene recognition with learned attributes, we compare our method (RSOM-A) with state-of-the-art methods on MIT-indoor [17] and Scene-15 [11] datasets.", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "On the task of scene recognition with learned attributes, we compare our method (RSOM-A) with state-of-the-art methods on MIT-indoor [17] and Scene-15 [11] datasets.", "startOffset": 151, "endOffset": 155}, {"referenceID": 11, "context": "Our method performs competitively with[12] while using shorter feature vectors, and outperforms the others.", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "We focus on learning 15 scene concepts used in [11], through collecting images from web for these concepts.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "Method MIT-indoor [17] Scene-15 [11]", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Method MIT-indoor [17] Scene-15 [11]", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "[12] VQ 47.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] 43.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] 44% 82.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] 81.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Comparison of our methods on scene recognition in relation to state-of-the-art studies on MIT-Indoor [17] and Scene-15 [11] datasets.", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "Comparison of our methods on scene recognition in relation to state-of-the-art studies on MIT-Indoor [17] and Scene-15 [11] datasets.", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "ac cu ra cy RSOM [11]", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "3% for RSOM-S+HM , versus 81% for [11] .", "startOffset": 34, "endOffset": 38}], "year": 2013, "abstractText": "We attack the problem of learning concepts automatically from noisy web image search results. Going beyond low level attributes, such as colour and texture, we explore weakly-labelled datasets for the learning of higher level concepts, such as scene categories. The idea is based on discovering common characteristics shared among subsets of images by posing a method that is able to organise the data while eliminating irrelevant instances. We propose a novel clustering and outlier detection method, namely Rectifying Self Organizing Maps (RSOM). Given an image collection returned for a concept query, RSOM provides clusters pruned from outliers. Each cluster is used to train a model representing a different characteristics of the concept. The proposed method outperforms the state-of-the-art studies on the task of learning low-level concepts, and it is competitive in learning higher level concepts as well. It is capable to work at large scale with no supervision through exploiting the available sources.", "creator": "LaTeX with hyperref package"}}}