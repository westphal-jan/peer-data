{"id": "1312.4824", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2013", "title": "Generation, Implementation and Appraisal of an N-gram based Stemming Algorithm", "abstract": "quantitative language prediction testing has always been looked lively. single n gram tokenization technique works yet, remarkably, it often generates stems / start creating repetitive characters, rather than sequential ones. we present a linguistic technique that facilitates the concept of formal grams design stage ahead still examines our examples with an established participant in student field association's stemmer. porters stemmer is language dependent, and validity of existing proposed experiment is not isolated from it.", "histories": [["v1", "Tue, 17 Dec 2013 15:32:56 GMT  (401kb)", "http://arxiv.org/abs/1312.4824v1", "9 pages"], ["v2", "Thu, 16 Jan 2014 04:05:36 GMT  (391kb)", "http://arxiv.org/abs/1312.4824v2", "10 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["b p pande", "pawan tamta", "h s dhami"], "accepted": false, "id": "1312.4824"}, "pdf": {"name": "1312.4824.pdf", "metadata": {"source": "CRF", "title": "Generation, Implementation and Appraisal of a Language Independent Stemming Algorithm", "authors": ["B. P. Pande", "H. S. Dhami", "Pawan Tamta", "James Mayfield", "Paul McNamee"], "emails": [], "sections": [{"heading": null, "text": "A language independent Stemmer has always been looked for. Single N-gram tokenization technique works well, however, it often generates stems that start with intermediate characters, rather than initial ones. We present a novel technique that takes the concept of N-grams one step ahead and compare our method with an established algorithm in the field- Porter\u2019s Stemmer. Porter\u2019s Stemmer is language dependent, and performance of our proposed method is not inferior to it.\nKeywords: Information Retrieval, Stemming, Conflation, N gram, Stemmer"}, {"heading": "1. Introduction:", "text": "Stemming, a very useful tool under the domain of Information Retrieval (IR), supported by almost each and every modern indexing and search systems. Sometimes put under a single roof with its sibling process, lemmatization, stemming is nothing but to reduce the morphological variants of a word into a single form- the stem. Before indexing, truncating attached affixes (suffixes or prefixes) from terms, makes thus generated stem to represent a broader concept than the original term, the stemming process eventually increases the number of retrieved documents in an IR system. Although, the effectiveness of stemmers has always been argued, still stemming is kept as an active method in the IR toolbox.\nWe have a rich literature of stemming process- there are linguistic and statistical approaches. The former are rule based techniques [3, 6, 10, 12], in which a priori knowledge of the target language morphology is required. The latter are subject to\n9]. There are also some lexicon based approaches [4].\nIn the urn of the statistical techniques, the most interesting and robust technique that caught our attention is \u201cSingle N-gram Stemming\u201d by James Mayfield and Paul McNamee [8]. The authors hypothesized that exploiting overlapping sequences of N characters, the benefits of stemming can be achieved in a language independent way. They argued that some of the N-grams (actually, low frequent) derived from a word will be common to only those portions of the word that do not exhibit morphological variation i.e. they can serve as an adequate stem substitute. They suggested selecting the word internal N-gram with the highest inverse document frequency (IDF) as a word\u2019s pseudo stem. They gave an excellent reasoning that a particular morphological variation being exhibited by affixes tends to be repeated across many different words, and therefore exhibit low IDF. As an instance, they tabled document frequencies of different N-grams of the word \u2018juggling\u2019 (from the CLEF 2002 collection) and the suggested pseudo-4 and pseudo-5 stems, according to their concept.\nThe key behind our present research resides in the tables being talked above. The pseudo-4 stem for currency and warrens were rren and rens respectively. The pseudo-5 stem for juggler, currency and warrens were ggler, rency and rrens respectively. A general insight says that a stem has to start from the beginning character of the word, not from anywhere else. So, is there any scope of improvement over this already elegant method?\nEndeavouring to answer the above question is the essence of our present paper. We work for the better utilization of N-grams, keeping the idea of counting their frequencies intact. Rather than depending on the simple concept of IDF, we develop a novel algorithm that gives better approximations of stems."}, {"heading": "2. Proposed Method:", "text": "The authors [8] reported in their experimental results that out of the raw words, Snowball stems [13], pseudo stems and 4-grams, the trend was for 4-grams to perform best. 4-grams, the initial 4 characters of any word- best representation of a word stem. This strengthens our insight that for a valid (not from a strict linguistic point of view) and nicely approximated stem, it should start from the\n(i.e. words internal N-grams), we approximate stem from the frequencies of 4- gram, 5-gram, 6-gram and so on? Seems a promising direction.\nOur idea is to take 4-gram as our initial guess for finding the stem. Obviously, this rules out words with length 3 or less. This would be a stunning advantage as many of the stop words are of length 3 or less. Now, for a given word, what we are left with finding frequencies of 4-gram, 5-gram and so on up to the length of the word and selecting the best from them. Before we embark upon our approach, let us look at some of the different N-gram frequencies from COCA [1]. We start with the same word juggling 1 .\nWe infer that an N-gram of a word that serves as an adequate approximation of the stem must has a relative good frequency across the corpus. All the N-grams less than this must be highly frequent and above this tend to exhibit lower frequencies gradually, ending in the total frequency of the given word (there may be the case where two or more consecutive entries are identical.)Well, above table successfully meets our assumption.\nThus, we have an integer variable in hand whose values tend to decrease (or remain constant) with the increase in the length of a partial word (N-gram) by one unit. It is very likely that this fall is very steep at the transition from prefix part of a word (stem) into the derivational or suffix part. But how can we catch a relative maximum fall? One way is to take deviation of each two consecutive entries. We define a metric as under\n(2.1)\nWhere Fi is the frequency of the i th N-gram. Next we calculate the second order deviation\n1 The NEWSPAPER search option was selected and the minimum frequency was set to 10.\nWe define our procedure in two phases: Phase 1:\n1) Initialization:\nwhere M is a very high integer value\n2) Recursion:\nCalculate , 4< i \u2264 |word| If\nElse\n3) Termination:\na.) If N=|word|, stop. Else, calculate . b.) If >0, stop.\nWhere is a threshold value, which defines how much frequency deviation of two N-grams is acceptable. It is kept either zero or to a minimum value. If at the end of Phase 1, , move to Phase2.\nPhase 2: If the frequencies of last three N-grams (Fi s) are identical, delete the last three characters of the input word only if deleting them makes the resulting stem greater than three, i.e.\nIf\nAt the end of the above procedure, contains the value of the N-gram that approximates the required stem. Phase 2 controls over that words that exhibit common suffixes (like \u2013ing), if they aren\u2019t stripped in Phase 1."}, {"heading": "3. Design of Experiments:", "text": "We chose an experimental approach to assess the effectiveness of the proposed method. We took a sample of 100 random English words and applied Porter\u2019s Snowball framework and our procedure. The results are reported in Table 3.1. Identical stems generated by either algorithm are shown in bold. Four PoS, Noun,\ngenerated using a web based tool Random Word Generator (Plus) [14]. The verbs were taken from a different source [2], as base forms of the verbs were being generated by the above mentioned tool. We took past and present participle forms as proxy of verb. Detailed procedure and calculations are tabulated in appendix (Table A.1).\nWhen generating a specific PoS, many options regarding complexity of a word were given by this Random Word Generator (Plus) tool, like common, average, obscure etc. We stuck to very common, common and average only, as below these, the generated words were exhibiting very low frequency distribution in COCA. We selected NEWSPAPER search area in COCA and for words, generated under very common and common complexities, the minimum frequency was set to 100. For average complexity words, it was set to 10. Most of the verbs were relatively common, hence the minimum frequency was set to 100 for them as well. Such words are marked with a hash symbol in Table A.1.\nTo compare the stemming results of Snowball (Porter) stemmer and our N-gram stemmer, we employed direct evaluation method. Like the comparison made by Chris D. Paise [11], using length truncation as a baseline, we employed Levenshtein distance [5] as a base measure. The distance is the number of deletions, insertions, or substitutions required to transform the source string into the target string. The Levenshtein distance therefore, represents by how many units a word has been stripped by a stemmer. For our sample of the 100 words, these distances are shown in Table 3.1, LDSnow and LDN-gram for Snowball stems and our N-gram stems respectively.\n9 Braid Braid Brai 0 1\n10 Fleet Fleet Flee 0 1 11 Midwife Midwif Midwife 1 0 12 Haze Haze Haze 0 0 13 Prophet Prophet Prophe 0 1 14 Vacancy Vacanc Vacanc 1 1 15 Ninety Nineti Ninet 1 1 16 Menace Menac Menac 1 1 17 Laceration Lacer Lacerat 5 3 18 Training Train Train 3 3 19 Librarian Librarian Librar 0 3 20 Ordaining Ordain Ordain 3 3 21 Mainstream Mainstream Mainst 0 4 22 Bloodier Bloodier Blood 0 3 23 Abject Abject Abject 0 0 24 Substitute Substitut Substitut 1 1 25 Overseas Oversea Overseas 1 0 26 Freehand Freehand Freehan 0 1 27 Despotic Despot Despoti 2 1 28 Predicted Predict Predic 2 3 29 Lyric Lyric Lyric 0 0 30 Eleventh Eleventh Eleven 0 2 31 Admonishing Admonish Admoni 3 5 32 Quiet Quiet Quiet 0 0 33 Macabre Macabr Maca 1 3 34 Unloaded Unload Unload 2 2 35 Quizzical Quizzic Quizzi 2 3 36 Alto Alto Alto 0 0 37 Undeveloped Undevelop Undevelo 2 3 38 Casual Casual Casual 0 0 39 Recognized Recogn Recogni 4 3 40 Devastating Devast Devastat 5 3 41 Abused Abus Abuse 2 1 42 Abusing Abus Abusing 3 0 43 Admired Admir Admire 2 1 44 Admiring Admir Admir 3 3 45 Believed Believ Belie 2 3 46 Believing Believ Belie 3 4 47 Borrowed Borrow Borrow 2 2 48 Borrowing Borrow Borrow 3 3 49 Carried Carri Carrie 2 1 50 Carrying Carri Carry 3 3\n52 Consulting Consult Consult 3 3 53 Deceived Deceiv Deceive 2 1 54 Deceiving Deceiv Deceiv 3 3 55 Decorated Decor Decor 4 4 56 Decorating Decor Decor 5 5 57 Employed Employ Employ 2 2 58 Employing Employ Employ 3 3 59 Explained Explain Explain 2 2 60 Explaining Explain Explain 3 3 61 Finished Finish Finish 2 2 62 Finishing Finish Finish 3 3 63 Forbidden Forbidden Forbid 0 3 64 Forbidding Forbid Forbid 4 4 65 Gathered Gather Gather 2 2 66 Gathering Gather Gather 3 3 67 Improved Improv Improv 2 2 68 Improving Improv Improv 3 3 69 Laughed Laugh Laugh 2 2 70 Laughing Laugh Laugh 3 3 71 Listened Listen Listen 2 2 72 Listening Listen Listen 3 3 73 Mended Mend Mende 2 1 74 Mending Mend Mending 3 0 75 Nipped Nip Nippe 3 1 76 Nipping Nip Nipp 4 3 77 Plucked Pluck Pluck 2 2 78 Plucking Pluck Pluck 3 3 79 Preached Preach Preach 2 2 80 Preaching Preach Preach 3 3 81 Enormously Enorm Enorm 5 5 82 Monthly Month Month 2 2 83 Solemnly Solemn Solemn 2 2 84 Abnormally Abnorm Abnormal 4 2 85 Diligently Dilig Diligen 5 3 86 Jubilantly Jubil Jubil 5 5 87 Frightfully Fright Fright 5 5 88 Swiftly Swift Swift 2 2 89 Miserable Miser Miser 4 4 90 Thankfully Thank Thankful 5 2 91 Blissfully Bliss Blissful 5 2 92 Reluctantly Reluct Reluctan 5 3 93 Viciously Vicious Vicious 2 2\n95 Hopelessly Hopeless Hopeless 2 2 96 Briskly Brisk Briskly 2 0 97 Delightfully Delight Delight 5 5 98 Anxiously Anxious Anxious 2 2 99 Obnoxiously Obnoxi Obnoxious 5 2\n100 Inwardly Inward Inward 2 2\nWe hypothesize that if our stemmer is as efficient as that of Porter\u2019s, then the distributions of Levenshtein distances between a word and its stripped stem tend to be same. For a given word, we have two treatments in hand, Porter algorithm and N-gram algorithm. Thus for a given word, we have a pair of LDs. For comparison, we set the null hypothesis H0 that no differences exist between the two stemmers.\nBecause the two sets of measures can be considered as two measures associated to the same sample, we decided to employ a statistical test for paired samples. In particular, a nonparametric statistical test, the Wilcoxon signed-rank test was employed by us, since there was no evidence about the distribution of these distances. The Wilcoxon test is based on two paired series (xi, yi) of N observed values, one series for each out of the observed variables X, Y to be compared. The absolute differences di=abs (xi,-yi) and sign (di) of differences are to be computed next. Then the absolute values are ranked, discarding zeros. If Nr is the reduced\nsample size, the final test statistic is , which is\napproximated by a Normal variable for large N."}, {"heading": "4. Experimental Results:", "text": "On conducting the Wilcoxon test over our sample data, we get the p value of 0.54. As p>0.05, fail to reject the null hypothesis H0. Thus, we have strong evidence that our N-gram Stemmer is not inferior to Porter\u2019s Stemmer."}, {"heading": "5. Conclusions:", "text": "We endeavoured to modify a statistical stemming technique and came up with one whose results are comparable with a well known linguistic stemmer. 58 out of 100 generated stems were identical and 22 stems differ by only one character. As a language neutral approach is always preferable over a technique in which a\nexiting and promising. Further work may lead to test our procedure with languages other than English."}, {"heading": "6. References:", "text": "1. Corpus of Contemporary American English (COCA). Available at\n<http://corpus.byu.edu/coca/>, visited 11 April 2013.\n2. Forms of Verbs | List of Verb Forms in English. Available at\nhttp://targetstudy.com/languages/english/verbs/, visited 4 April, 2013\n3. Harman Donna (1991). How effective is suffixing? Journal of the American\nSociety for Information Science, 42, 7-15.\n4. Krovetz Robert (1993). Viewing morphology as an inference process.\nProceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, 191-202. 5. Levenshtein V. I. (1966). Binary codes capable of correcting deletions,\ninsertions and reversals. Soviet Physics- Doklady, 10(8), 707-710.\n6. Lovins, J. B. (1968). Development of a stemming algorithm. Mechanical\nTranslation and Computational Linguistics, 11, 22-31.\n7. Majumder Prasenjit, et al. (2007). YASS: Yet another suffix stripper. ACM\nTransactions on Information Systems. 25(4), Article No. 18.\n8. Mayfield James and McNamee Paul (2003). Single N-gram stemming.\nProceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, 415-416. 9. Melucci Massimo and Orio Nicola (2007). Design, Implementation, and\nEvaluation of a Methodology for Automatic Stemmer Generation. Journal of the American Society for Information Science and Technology, 58(5), 673\u2013686.\n10. Paice Chris D. (1990). Another stemmer. ACM SIGIR Forum, 24(3). 56-61. 11. Paice Chris D. (1994). An evaluation method for stemming algorithms.\nProceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, 42-50.\n12. Porter M. F. (1980). An algorithm for suffix stripping. Program 14, 130-137 13. Porter, M. F. (2001). Snowball: A language for stemming algorithms. Available\nat <http://snowball.tartarus.org/>,visited 11 April 2013.\n<http://watchout4snakes.com/CreativityTools/RandomWord/RandomWordPlus .aspx>, visited 4 April 2013."}], "references": [{"title": "How effective is suffixing", "author": ["Harman Donna"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "Viewing morphology as an inference process", "author": ["Krovetz Robert"], "venue": "Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1993}, {"title": "Binary codes capable of correcting deletions, insertions and reversals", "author": ["I. Levenshtein V"], "venue": "Soviet Physics- Doklady,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1966}, {"title": "Development of a stemming algorithm", "author": ["J.B. Lovins"], "venue": "Mechanical Translation and Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1968}, {"title": "YASS: Yet another suffix stripper", "author": ["Majumder Prasenjit"], "venue": "ACM Transactions on Information Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Single N-gram stemming", "author": ["Mayfield James", "McNamee Paul"], "venue": "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Design, Implementation, and Evaluation of a Methodology for Automatic Stemmer Generation", "author": ["Melucci Massimo", "Orio Nicola"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "An evaluation method for stemming algorithms", "author": ["D. Paice Chris"], "venue": "Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "An algorithm for suffix stripping", "author": ["F. Porter M"], "venue": "Program", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1980}, {"title": "Snowball: A language for stemming algorithms", "author": ["M.F. Porter"], "venue": "Available at <http://snowball.tartarus.org/>,visited", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "The former are rule based techniques [3, 6, 10, 12], in which a priori knowledge of the target language morphology is required.", "startOffset": 37, "endOffset": 51}, {"referenceID": 3, "context": "The former are rule based techniques [3, 6, 10, 12], in which a priori knowledge of the target language morphology is required.", "startOffset": 37, "endOffset": 51}, {"referenceID": 8, "context": "The former are rule based techniques [3, 6, 10, 12], in which a priori knowledge of the target language morphology is required.", "startOffset": 37, "endOffset": 51}, {"referenceID": 1, "context": "There are also some lexicon based approaches [4].", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "In the urn of the statistical techniques, the most interesting and robust technique that caught our attention is \u201cSingle N-gram Stemming\u201d by James Mayfield and Paul McNamee [8].", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "The authors [8] reported in their experimental results that out of the raw words, Snowball stems [13], pseudo stems and 4-grams, the trend was for 4-grams to perform best.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "The authors [8] reported in their experimental results that out of the raw words, Snowball stems [13], pseudo stems and 4-grams, the trend was for 4-grams to perform best.", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "Paise [11], using length truncation as a baseline, we employed Levenshtein distance [5] as a base measure.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "Paise [11], using length truncation as a baseline, we employed Levenshtein distance [5] as a base measure.", "startOffset": 84, "endOffset": 87}], "year": 2013, "abstractText": "A language independent Stemmer has always been looked for. Single N-gram tokenization technique works well, however, it often generates stems that start with intermediate characters, rather than initial ones. We present a novel technique that takes the concept of N-grams one step ahead and compare our method with an established algorithm in the fieldPorter\u2019s Stemmer. Porter\u2019s Stemmer is language dependent, and performance of our proposed method is not inferior to it.", "creator": "Microsoft\u00ae Office Word 2007"}}}