{"id": "1312.5419", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Large-scale Multi-label Text Classification - Revisiting Neural Networks", "abstract": "large - difference datasets with multi - labels are becoming readily available, yet the demand involving large - scale finite - label estimation algorithm is also increasing. regarding this guise, users routinely successfully utilize a single - layer neural security approach in rapid - scale multi - data page capture tasks with recently proposed learning techniques. we carried four experiments on fragmented textual arrays with varying characteristics and size, and show me a generic neural networks model equipped with flexible experimental techniques considering neural networks components xml - 3d layer, resonance, shape matching often performs neither well as or entirely outperforms the seemingly would - give - the - art approaches on fixed - variety datasets with common languages.", "histories": [["v1", "Thu, 19 Dec 2013 06:53:24 GMT  (161kb,D)", "https://arxiv.org/abs/1312.5419v1", "12 pages, 3 figures, prepared for ICLR 2014"], ["v2", "Mon, 30 Dec 2013 05:41:15 GMT  (158kb,D)", "http://arxiv.org/abs/1312.5419v2", "12 pages, 3 figures, submitted to ICLR 2014"], ["v3", "Thu, 15 May 2014 11:32:03 GMT  (497kb,D)", "http://arxiv.org/abs/1312.5419v3", "16 pages, 4 figures, submitted to ECML 2014"]], "COMMENTS": "12 pages, 3 figures, prepared for ICLR 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jinseok nam", "jungi kim", "eneldo loza menc\\'ia", "iryna gurevych", "johannes f\\\"urnkranz"], "accepted": false, "id": "1312.5419"}, "pdf": {"name": "1312.5419.pdf", "metadata": {"source": "CRF", "title": "Large-scale Multi-label Text Classification \u2014 Revisiting Neural Networks", "authors": ["Jinseok Nam", "Jungi Kim", "Eneldo Loza Menc\u0131\u0301a", "Iryna Gurevych", "Johannes F\u00fcrnkranz"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "As the amount of textual data on the web and in digital libraries is increasing rapidly, the need for augmenting unstructured data with metadata is also increasing. Systematically maintaining a high quality digital library requires extracting a variety types of information from unstructured text, from trivial information such as title and author, to non-trivial information such as descriptive keywords and categories. Time- and costwise, a manual extraction of such information from ever-growing document collections is impractical.\nMulti-label classification is an automatic approach for addressing such problems by learning to assign a suitable subset of categories from an established classification system to a given text. In the literature, one can find a number of multi-label classification approaches for a variety of tasks in different domains such as bioinformatics [1], music [27], and text [8]. In the simplest case, multi-label classification may be viewed as a set of binary classification tasks that decides for each label independently whether it should be assigned to the document or not. However, this so-called binary relevance approach ignores dependencies between the labels, so that current research in multi-label classification concentrates on the question of how such dependencies can be exploited [22, 3]. One such approach is BP-MLL [31], which formulates multi-label classification problems as a neural network with multiple output nodes, one for each label. The output layer is able to model dependencies between the individual labels. ar X\niv :1\n31 2.\n54 19\nv3 [\ncs .L\nG ]\n1 5\nM ay\n2 01\n2 In this work, we directly build upon BP-MLL and show how a simple, single hidden layer NN may achieve a state-of-the-art performance in large-scale multi-label text classification tasks. The key modifications that we suggest are (i) more efficient and more effective training by replacing BP-MLL\u2019s pairwise ranking loss with cross entropy and (ii) the use of recent developments in the area of deep learning such as rectified linear units (ReLUs), Dropout, and AdaGrad.\nEven though we employ techniques that have been developed in the realm of deep learning, we nevertheless stick to single-layer NNs. The motivation behind this is twofold: first, a simple network configuration allows better scalability of the model and is more suitable for large-scale tasks. Second, as it has been shown in the literature [14], popular feature representation schemes for textual data such as variants of tf-idf term weighting already incorporate a certain degree of higher dimensional features, and we speculate that even a single-layer NN model can work well with text data. This paper provides an empirical evidence to support that a simple NN model equipped with recent advanced techniques for training NN performs as well as or even outperforms state-ofthe-art approaches on large-scale datasets with diverse characteristics."}, {"heading": "2 Multi-label Classification", "text": "Formally, multi-label classification may be defined as follows: X \u2282 RD is a set of M instances, each being a D-dimensional feature vector, and L is a set of labels. Each instance x is associated with a subset of the L labels, the so-called relevant labels; all other labels are irrelevant for this example. The task of the learner is to learn a mapping function f : RD \u2192 2L that assigns a subset of labels to a given instance. An alternative view is that we have to predict an L-dimensional target vector y \u2208 {0, 1}L, where yi = 1 indicates that the i-th label is relevant, whereas yi = 0 indicates that it is irrelevant for the given instance.\nMany algorithms have been developed for tackling this type of problem. The most straightforward way is binary relevance (BR) learning; it constructs L binary classifiers, which are trained on the L labels independently. Thus, the prediction of the label set is composed of independent predictions for individual labels. However, labels often occur together, that is, the presence of a specific label may suppress or exhibit the likelihood of other labels.\nTo address this limitation of BR, pairwise decomposition (PW) and label powerset (LP) approaches consider label dependencies during the transformation by either generating pairwise subproblems [8, 19] or the powerset of possible label combinations [28]. Classifier chains [22, 3] are another popular approach that extend BR by including previous predictions into the predictions of subsequent labels.\n[6] present a large-margin classifier, RankSVM, that minimizes a ranking loss by penalizing incorrectly ordered pairs of labels. This setting can be used for multi-label classification by assuming that the ranking algorithm has to rank each relevant label before each irrelevant label. In order to make a prediction, the ranking has to be calibrated [8], i.e., a threshold has to be found that splits the ranking into relevant and irrelevant labels. Similarly, Zhang and Zhou [31] introduced a framework that learns ranking errors in neural networks via backpropagation (BP-MLL)."}, {"heading": "2.1 State-of-the-art multi-label classifiers and limitations", "text": "The most prominent learning method for multi-label text classification is to use a BR approach with strong binary classifiers such as SVMs [23, 29] despite its simplicity. It is well known that characteristics of high-dimensional and sparse data, such as text data, make decision problems linearly separable [14], and this characteristic suits the strengths of SVM classifiers well.\nUnlike benchmark datasets, real-world text collections consist of a large number of training examples represented in a high-dimensional space with a large amount of labels. To handle such datasets, researchers have derived efficient linear SVMs [15, 7] that can handle large-scale problems. The training time of these solvers scales linearly with the number of instances, so that they show good performance on standard benchmarks. However, their performance decreases as the number of labels grows and the label frequency distribution becomes skewed [18, 23]. In such cases, it is also intractable to employ methods that minimize ranking errors among labels [6, 31] or that learn joint probability distributions of labels [10, 3]."}, {"heading": "3 Neural Networks for Multi-label Classification", "text": "In this section, we propose a neural network-based multi-label classification framework that is composed of a single hidden layer and operates with recent developments in neural network and optimization techniques, which allow the model to converge into good regions of the error surface in a few steps of parameter updates. Our approach consists of two modules (Figure 1): a neural network that produces label scores (Sections 3.2\u2013 3.5), and a label predictor that converts label scores into binary using a thresholding technique (Section 3.3).\n4"}, {"heading": "3.1 Rank Loss", "text": "The most intuitive objective for multi-label learning is to minimize the number of misordering between a pair of relevant label and irrelevant label, which is called rank loss:\nL(y, f(x)) = w(y) \u2211 yi<yj I (fi(x) > fj(x)) + 1 2 I (fi = fj) (1)\nwhere w(y) is a normalization factor, I (\u00b7) is the indicator function, and fi (\u00b7) is a prediction score for a label i. Unfortunately, it is hard to minimize due to non-convex property of the loss function. Therefore, convex surrogate losses have been proposed as alternatives to rank loss [25, 6, 31]."}, {"heading": "3.2 Pairwise Ranking Loss Minimization in Neural Networks", "text": "Let us assume that we would like to make a prediction on L labels from D dimensional input features. Consider the neural network model with a single hidden layer in which F hidden units are defined and input units x \u2208 RD\u00d71 are connected to hidden units h \u2208 RF\u00d71 with weights W(1) \u2208 RF\u00d7D and biases b(1) \u2208 RF\u00d71. The hidden units are connected to output units o \u2208 RL\u00d71 through weights W(2) \u2208 RL\u00d7F and biases b(2) \u2208 RL\u00d71. The network, then, can be written in a matrix-vector form, and we can construct a feed-forward network f\u0398 : x\u2192 o as a composite of non-linear functions in the range [0, 1]:\nf\u0398(x) = fo ( W(2)fh ( W(1)x + b(1) ) + b(2) ) (2)\nwhere \u0398 = {W(1),b(1),W(2),b(2)}, and fo and fh are element-wise activation functions in the output layer and the hidden layer, respectively. Specifically, the function f\u0398 (x) can be re-written as follows:\nz(1) = W(1)x + b(1), h = fh ( z(1) ) z(2) = W(2)h + b(2), o = fo ( z(2)\n) where z(1) and z(2) denote the weighted sum of inputs and hidden activations, respec-\ntively. Our aim is to find a parameter vector \u0398 that minimizes a cost function J(\u0398;x,y). The cost function measures discrepancy between predictions of the network and given targets y.\nBP-MLL [31] minimizes errors induced by incorrectly ordered pairs of labels, in order to exploit dependencies among labels. To this end, it introduces a pairwise error function (PWE), which is defined as follows:\nJPWE(\u0398;x,y) = 1 |y||y\u0304| \u2211\n(p,n)\u2208y\u00d7y\u0304\nexp(\u2212(op \u2212 on)) (3)\nwhere p and n are positive and negative label index associated with training example x. y\u0304 represents a set of negative labels and | \u00b7 | stands for the cardinality. The PWE is relaxation of the loss function in Equation 1 that we want to minimize.\n5 As no closed-form solution exists to minimize the cost function, we use a gradientbased optimization method.\n\u0398(\u03c4+1) = \u0398(\u03c4) \u2212 \u03b7\u2207\u0398(\u03c4)J(\u0398 (\u03c4);x,y) (4)\nThe parameter \u0398 is updated by adding a small step of negative gradients of the cost function J(\u0398(\u03c4);x,y) with respect to the parameter\u0398 at step \u03c4 . The parameter \u03b7, called the learning rate, determines the step size of updates."}, {"heading": "3.3 Thresholding", "text": "Once training of the neural network is finished, its output may be interpreted as a probability distribution p (o|x) over the labels for a given document x. The probability distribution can be used to rank labels, but additional measures are needed in order to split the ranking into relevant and irrelevant labels. For transforming the ranked list of labels into a set of binary predictions, we train a multi-label threshold predictor from training data. This sort of thresholding methods are also used in [6, 31]\nFor each document xm, labels are sorted by the probabilities in decreasing order. Ideally, if NNs successfully learn a mapping function f\u0398, all correct (positive) labels will be placed on top of the sorted list and there should be large margin between the set of positive labels and the set of negative labels. Using F1 score as a reference measure, we calculate classification performances at every pair of successive positive labels and choose a threshold value tm that produces the best performance (Figure 1 (b)).\nAfterwards, we can train a multi-label thresholding predictor t\u0302 = T (x; \u03b8) to learn t as target values from input pattern x. We use linear regression with `2-regularization to learn \u03b8\nJ (\u03b8) = 1\n2M M\u2211 m=1 (T (xm; \u03b8)\u2212 ti)2 + \u03bb 2 \u2016\u03b8\u201622 (5)\nwhere T (xm; \u03b8) = \u03b8Txm and \u03bb is a parameter which controls the magnitude of the `2 penalty.\nAt test time, these learned thresholds are used to predict a binary output y\u0302kl for label l of a test document xk given label probabilities okl; y\u0302kl = 1 if okl > T (xk; \u03b8), otherwise 0."}, {"heading": "3.4 Ranking Loss vs. Cross Entropy", "text": "BP-MLL is supposed to perform better in multi-label problems since it takes label correlations into consideration than the standard form of NN that does not. However, we have found that BP-MLL does not perform as expected in our preliminary experiments, particularly, on datasets in textual domain.\nConsistency w.r.t Rank Loss Recently, it has been claimed that none of convex loss functions including BP-MLL\u2019s loss function (Equation 3) is consistent with respect to rank loss which is non-convex and has discontinuity [2, 9]. Furthermore, univariate surrogate loss functions such as log loss are rather consistent with rank loss [4].\nJlog(\u0398;x,y) = w (y) \u2211 l log ( 1 + e\u2212y\u0307lzl )\n6 where w (y) is a weighting function that normalizes loss in terms of y and zl indicates prediction for label l. Please note that the log loss is often used for logistic regression in which y\u0307 \u2208 {\u22121, 1} a target and zl is output of a linear function zl = \u2211 kWlkxk + bl where Wlk is a weight from input xk to output zl and bl is bias for label l. A typical choice is, for instance, w(y) = (|y||y\u0304|)\u22121 as in BP-MLL. In this work, we set w(y) = 1, then the log loss above is equivalent to cross entropy (CE), which is commonly used to train neural networks for classification tasks if we use sigmoid transfer function in the output layer, i.e. fo(z) = 1/ (1 + exp(\u2212z)), or simply fo(z) = \u03c3 (z):\nJCE(\u0398;x,y) = \u2212 \u2211 l (yl log ol) + (1\u2212 yl) log(1\u2212 ol)) (6)\nwhere ol and yl are the prediction and the target for label l, respectively. Let us verify the equivalence between the log loss and the CE. Consider the log loss function for only label l.\nJlog(\u0398;x, yl) = log(1 + e \u2212y\u0307lzl) = \u2212 log\n( 1\n1 + e\u2212y\u0307lzl\n) (7)\nAs noted, y\u0307 in the log loss takes either \u22121 or 1, which allows us to split the above equation as follows:\n\u2212 log (\n1\n1 + e\u2212y\u0307lzl\n) = { \u2212 log (\u03c3 (zl)) if y\u0307 = 1 \u2212 log (\u03c3 (\u2212zl)) if y\u0307 = \u22121 (8)\nThen, we have the corresponding CE by using a property of the sigmoid function \u03c3 (\u2212z) = 1\u2212 \u03c3 (z)\nJCE (\u0398;x, yl) = \u2212 (yl log ol + (1\u2212 yl) log (1\u2212 ol)) (9)\nwhere y \u2208 {0, 1} and ol = \u03c3 (zl).\nComputational Expenses In addition to consistency with rank loss, CE has an advantage in terms of computational efficiency; computational cost for computing gradients of parameters with respect to PWE is getting more expensive as the number of labels grows. The error term \u03b4(2)l for label l which is propagated to the hidden layer is defined as\n\u03b4 (2) l =  \u2212 1|y||y\u0304| \u2211 n\u2208y\u0304 exp(\u2212(ol \u2212 on))f \u2032o(z (2) l ), if l \u2208 y 1 |y||y\u0304| \u2211 p\u2208y exp(\u2212(op \u2212 ol))f \u2032o(z (2) l ), if l \u2208 y\u0304\n(10)\nWhereas the computation of \u03b4(2)l = \u2212yl/ol + (1\u2212 yl)/(1\u2212 ol)f \u2032 o(z (2) l ) for the CE can be\nperformed efficiently, obtaining error terms \u03b4(2)l for the PWE is L times more expensive than one in ordinary NN utilizing the cross entropy error function. This also shows that BP-MLL scales poorly w.r.t. the number of unique labels.\nPlateaus To get an idea of how differently both objective functions behave as a function of parameters to be optimized, let us draw graphs containing cost function values. Note that it has been pointed out that the slope of the cost function as a function of the\nparameters plays an important role in learning parameters of neural networks [26, 11] which we follow.\nConsider two-layer neural networks consisting of W (1) \u2208 R for the first layer, W(2) \u2208 R4\u00d71 for the second, output layer. Since we are interested in function values with respect to two parameters W (1) and W (2)1 out of 5 parameters, W (2)\n{2,3,4} is set to a fixed value c. In this paper we use c = 0.3 Figure 2 (a) shows different shapes of the functions and slope steepness. In figure 2 (a) both curves have similar shapes, but the curve for PWE has plateaus in which gradient descent can be very slow in comparison with the CE. Figure 2 (b) shows that CE with ReLUs, which is explained the next Section, has a very steep slope compared to CE with tanh. Such a slope can accelerate convergence speed in learning parameters using gradient descent. We conjecture that these properties might explain why our set-up converges faster than the other configurations, and BP-MLL performs poorly in most cases in our experiments."}, {"heading": "3.5 Recent Advances in Deep Learning", "text": "In recent neural network and deep learning literature, a number of techniques were proposed to overcome the difficulty of learning neural networks efficiently. In particular, we make use of ReLUs, AdaGrad, and Dropout training, which are briefly discussed in the following.\n3 The shape of the functions is not changed even if we set c to arbitrary value since it is drawn by function values in z-axis with respect to only W (1) and W (2)1 .\n8 Rectified Linear Units Rectified linear units (ReLUs) have been proposed as activation units on the hidden layer and shown to yield better generalization performance [21, 12, 30]. A ReLU disables negative activation (ReLU(x) = max(0, x)) so that the number of parameters to be learned decreases during the training. This sparsity characteristic makes ReLUs advantageous over the traditional activation units such as sigmoid and tanh in terms of the generalization performance.\nLearning Rate Adaptation with AdaGrad Stochastic gradient descent (SGD) is a simple but effective technique for minimizing the objective functions of NNs (Equation 4). When SGD is considered as an optimization tool, one of the problems is the choice of the learning rate. A common approach is to estimate the learning rate which gives lower training errors on subsamples of training examples [16] and then decrease it over time. Furthermore, to accelerate learning speed of SGD, one can utilize momentum [24].\nInstead of a fixed or scheduled learning rate, an adaptive learning rate method, namely AdaGrad, was proposed [5]. The method determines the learning rate at iteration \u03c4 by keeping previous gradients \u22061:\u03c4 to compute the learning rate for each dimension of parameters \u03b7i,\u03c4 = \u03b70/ \u221a\u2211\u03c4 t=1 \u2206 2 i,t where i stands for an index of each dimension of parameters and \u03b70 is the initial learning rate and shared by all parameters. For multi-label learning, it is often the case that a few labels occur frequently, whereas the majority only occurs rarely, so that the rare ones need to be updated with larger steps in the direction of the gradient. If we use AdaGrad, the learning rates for the frequent labels decreases because the gradient of the parameter for the frequent labels will get smaller as the updates proceed. On the other hand, the learning rates for rare labels remain comparatively large.\nRegularization using Dropout Training In principle, as the number of hidden layers and hidden units in a network increases, its expressive power also increases. If one is given a large number of training examples, training a larger networks will result in better performance than using a smaller one. The problem when training such a large network is that the model is more prone to getting stuck in local minima due to the huge number of parameters to learn. Dropout training [13] is a technique for preventing overfitting in a huge parameter space. Its key idea is to decouple hidden units that activate the same output together, by randomly dropping some hidden units\u2019 activations. Essentially, this corresponds to training an ensemble of networks with smaller hidden layers, and combining their predictions. However, the individual predictions of all possible hidden layers need not be computed and combined explicitly, but the output of the ensemble can be approximately reconstructed from the full network. Thus, dropout training has a similar regularization effect as ensemble techniques."}, {"heading": "4 Experimental Setup", "text": "We have shown that why the structure of NNs needs to be reconsidered in the previous Sections. In this Section, we describe evaluation measures to show how effectively NNs perform by combining recent development in learning neural networks based on the\n9 fact that the univariate loss is consistent with respect to rank loss on large-scale textual datasets.\nEvaluation Measures Multi-label classifiers can be evaluated in two groups of measures: bipartition and ranking. Bipartition measures operate on classification results, i.e. a set of labels assigned by classifiers to each document, while ranking measures operate on the ranked list of labels. In order to evaluate the quality of a ranked list, we consider several ranking measures [25]. Given a document x and associated label information y, consider a multi-label learner f\u03b8(x) that is able to produce scores for each label. These scores, then, can be sorted in descending order. Let r(l) be the rank of a label l in the sorted list of labels. We already introduced Rank loss, which is concerned primarily in this work, in Section 3.1. One-Error evaluates whether the top most ranked label with the highest score is is a positive label or not: I ( r\u22121(1)(f\u03b8(x)) /\u2208 y\n) where r\u22121(1) indicates the index of a label positioning on the first place in the sorted list. Coverage measures on average how far one needs to go down the ranked list of labels to achieve recall of 100%: maxli\u2208y r(li) \u2212 1. Average Precision or AP measures the average fraction of labels preceding relevant labels in the ranked list of labels: 1 |y| \u2211 li\u2208y |{lj\u2208y|r(lj)\u2264r(li)}| r(li)\n. For bipartition measures, Precision, Recall, and F1 score are conventional methods to evaluate effectiveness of information retrieval systems. There are two ways of computing such performance measures: Micro-averaged measures and Macro-averaged measures4[20].\nPmicro = \u2211L l=1 tpl\u2211L\nl=1 tpl + fpl , Rmicro =\n\u2211L l=1 tpl\u2211L\nl=1 tpl + fnl , F1\u2212micro =\n\u2211L l=1 2tpl\u2211L\nl=1 2tpl + fpl + fnl\nPmacro = 1\nL L\u2211 l=1 tpl tpl + fpl , Rmacro = 1 L L\u2211 l=1 tpl tpl + fnl , F1\u2212macro = 1 L L\u2211 l=1\n2tpl\n2tpl + fpl + fnl\nDatasets Our main interest is in large-scale text classification, for which we selected six representative domains, whose characteristics are summarized Table 1. For Reuters21578 we used the same training/test split as previous works [29]. Training and test data were switched for RCV1-v2 [17] which originally consists of 23,149 train and 781,265 test documents. The EUR-Lex, Delicious and Bookmarks datasets were taken from the MULAN repository.5 Except for Delicious and Bookmarks, all documents are represented with tf-idf features with cosine normalization such that length of the document vector is 1 in order to account for the different document lengths.\nIn addition to these standard benchmark datasets, we prepared a large-scale dataset from documents of the German Education Index (GEI).6 The GEI is a database of links\n4 Note that scores computed by micro-averaged measures might be much higher than that by macro-averaged measures if there are many rarely-occurring labels for which the classification system does not perform well. This is because macro-averaging weighs each label equally, whereas micro-averaged measures are dominated by the results of frequent labels. 5 http://mulan.sourceforge.net/datasets.html 6 http://www.dipf.de/en/portals/portals-educational-information/german-education-index\n10\nto more than 800,000 scientific articles with metadata, e.g. title, authorship, language of an article and index terms. We consider a subset of the dataset consisting of approximately 300,000 documents which have abstract as well as the metadata. Each document has multiple index terms which are carefully hand-labeled by human experts with respect to the content of articles. We processed plain text by removing stopwords and stemming each token. To avoid the computational bottleneck from a large number of labels, we chose the 1,000 most common labels out of about 50,000. We then randomly split the dataset into 90% for training and 10% for test.\nAlgorithms Our main goal is to compare our NN-based approach to BP-MLL. NNA stands for the single hidden layer neural networks which have ReLUs for its hidden layer and which are trained with SGD where each parameter of the neural networks has their own learning rate using AdaGrad. NNAD additionally employs Dropout based on the same settings as NNA. T and R following BP-MLL indicate tanh and ReLU as a transfer function in the hidden layer. For both NN and BP-MLL, we used 1000 units in the hidden layer over all datasets. 7 As Dropout works well as a regularizer, no additional regularization to prevent overfitting was incorporated. The base learning rate \u03b70 was also determined among [0.001, 0.01, 0.1] using validation data.\nWe also compared the NN-based algorithms to binary relevance (BR) using SVMs (Liblinear) as a base learner, as a representative of the state-of-the-art. The penalty parameter C was optimized in the range of [10\u22123, 10\u22122, . . . , 102, 103] based on either average of micro- and macro-average F1 or rankloss on validation set. BRB refers to linear SVMs where C is optimized with bipartition measures on the validation dataset. BR models whose penalty parameter is optimized on ranking measures are indicated as BRR. In addition, we apply the same thresholding technique which we utilize in our NN approach (Section 3.3) on a ranked list produced by BR models (BRR)."}, {"heading": "5 Results", "text": "We evaluate our proposed models and other baseline systems on datasets with varying statistics and characteristics. We first show experimental results that confirm that the\n7 The optimal number of hidden units of BP-MLL and NN was tested among 20, 50, 100, 500, 1000 and 4000 on validation datasets. Usually, the more units are in the hidden layer, the better performance of networks is. We chose it in terms of computational efficiency.\n11\ntechniques discussed in Section 3.5 actually contribute to an increased performance of NN-based multi-label classification, and then compare all algorithms on the six abovementioned datasets in order to get an overall impression of their performance.\nBetter Local Minima and Acceleration of Convergence Speed First we intend to show the effect of ReLUs and AdaGrad in terms of convergence speed and rank loss. The left part of Figure 3 shows that all three results of AdaGrad (red lines) show a lower rank loss than all three versions of momentum. Moreover, within each group, ReLUs outperform the versions using tanh or sigmoid activation functions. That NNs with ReLUs at the hidden layer converge faster into a better weight space has been previously observed for the speech domain [30].8 This faster convergence is a major advantage of combining recently proposed learning components such as ReLUs and AdaGrad, which facilitates a quicker learning of the parameters of NNs. This is particularly important for the large-scale text classification problems that are the main focus of this work.\nDecorrelating Hidden Units While Output Units Remain Correlated One major goal of multi-label learners is to minimize rank loss by leveraging inherent correlations in a label space. However, we conjecture that these correlations also may cause overfitting because if groups of hidden units specialize in predicting particular label subsets that occur frequently in the training data, it will become harder to predict novel label combinations that only occur in the test set. Dropout effectively fights this by randomly dropping individual hidden units, so that it becomes harder for groups of hidden units to specialize in the prediction of particular output combinations, i.e., they decorrelate the hidden units, whereas the correlation of output units still remains. Particularly, a subset of output activations o and hidden activations h would be correlated through W(2).\n8 However, unlike the results of [30], in our preliminary experiments adding more hidden layers did not further improve generalization performance.\n12\nWe observed overfitting across all datasets except for Reuters-21578 and RCV1-v2 under our experimental settings. The right part of Figure 3 shows how well Dropout prevents NNs from overfitting on the test data of EUR-Lex. In particular, we can see that with increasing numbers of parameter updates, the performance of regular NNs eventually got worse in terms of rank loss. On the other hand, when dropout is employed, convergence is initially slower, but eventually effectively prevents overfitting.\nLimiting Small Learning Rates in BP-MLL The learning rate strongly influences convergence and learning speed [16]. As we have already seen in the Figure 2, the slope of PWE is less steep than CE, which implies that smaller learning rates should be used. Specifically, we observed PWE allows only smaller learning rate 0.01 (blue markers) in contrast with CE that works well a relatively larger learning rate 0.1 (red markers) in Figure 4. In the case of PWE with the larger learning rate (green markers), interestingly, dropout (rectangle markers in green) makes it converge towards much better local minima, yet it is still worse than the other configurations. It seems that the weights of BP-MLL oscillates in the vicinity of local minima and, indeed, converges worse local minima. However, it makes learning procedure of BP-MLL slow compared to NNs with CE making bigger steps for parameter updates.\nWith respect to Dropout, Figure 4 also shows that for the same learning rates, networks without Dropout converge much faster than ones working with Dropout in terms of both rank loss and MAP. Regardless of the cost functions, overfitting arises over the networks without Dropout and it is likely that overfitting is avoided effectively as discussed earlier.\nComparison of Algorithms Table 3 shows detailed results of all experiments with all algorithms on all six datasets, except that we could not obtain results of BP-MLL\n8 A trajectory for PWE \u03b7 = 0.1 is missing in the figure because it got 0.2 on the rankloss measure which is much worse than the other configurations.\n13\non EUR-Lex within a reasonable time frame. In an attempt to summarize the results, Table 2 shows the average rank of each algorithm in these six datasets according to all ranking an bipartition measures discussed in Section 4.\nWe can see that although BP-MLL focuses on minimizing pairwise ranking errors, thereby capturing label dependencies, the single hidden layer NNs with cross-entropy minimization (i.e., NNA and NNAD) work much better not only on rank loss but also on other ranking measures. The binary relevance (BR) approaches show acceptable performance on ranking measures even though label dependency was ignored during the training phase. In addition, NNA and NNAD perform as good as or better than other methods on bipartition measures as well as on ranking measures.\nWe did not observe significant improvements by replacing hidden units of BP-MLL from tanh to ReLU. However, if we change the cost function in the previous setup from PWE to CE, significant improvements were obtained. Because BP-MLLRAD is the same architecture as NNAD except for its cost function,9 we can say that the differences in the effectiveness of NNs and BP-MLL are due to the use of different cost functions. This also implies that the main source of improvements for NNs against BP-MLL is replacement of the cost function. Again, Figure 4 shows the difference between two cost functions more explicitly."}, {"heading": "6 Conclusion", "text": "This paper presents a multi-label classification framework based on a neural network and a simple threshold label predictor. We found that our approach outperforms BPMLL, both in predictive performance as well as in computational complexity and convergence speed. We have explored why BP-MLL as a multi-label text classifier does not perform well. Our experimental results showed the proposed framework is an effective method for the multi-label text classification task. Also, we have conducted extensive analysis to characterize the effectiveness of combining ReLUs with AdaGrad for fast convergence rate, and utilizing Dropout to prevent overfitting which results in better generalization.\n9 For PWE we use tanh in the output layer, but sigmoid is used for CE because predictions o for computing CE with targets y needs to be between 0 and 1.\n14\n15\nAcknowledgments This work has been supported by the Information Center for Education of the German Institute for Educational Research (DIPF) under the Knowledge Discovery in Scientific Literature (KDSL) program."}], "references": [{"title": "Multi-label classification on tree-and dag-structured hierarchies", "author": ["W. Bi", "J.T. Kwok"], "venue": "Proceedings of the 28th International Conference on Machine Learning. pp", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "On the (non-)existence of convex, calibrated surrogate losses for ranking", "author": ["C. Calauz\u00e8nes", "N. Usunier", "P. Gallinari"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "author": ["K. Dembczy\u0144ski", "W. Cheng", "E. H\u00fcllermeier"], "venue": "Proceedings of the 27th International Conference on Machine Learning. pp", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Consistent multilabel ranking through univariate losses", "author": ["K. Dembczy\u0144ski", "W. Kot\u0142owski", "E. H\u00fcllermeier"], "venue": "Proceedings of the 29th International Conference on Machine Learning. pp", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research 12,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A kernel method for multi-labelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Liblinear: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Multilabel classification via calibrated label ranking", "author": ["J. F\u00fcrnkranz", "E. H\u00fcllermeier", "E. Loza Menc\u0131\u0301a", "K. Brinker"], "venue": "Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "On the consistency of multi-label learning", "author": ["W. Gao", "Z.H. Zhou"], "venue": "Artificial Intelligence 199\u2013200,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Collective multi-label classification", "author": ["N. Ghamrawi", "A. McCallum"], "venue": "Proceedings of the 14th ACM International Conference on Information and Knowledge Management. pp", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, JMLR W&CP. pp", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, JMLR W&CP. pp", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "Proceedings of the 10th European Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Efficient backprop. In: Neural Networks: tricks of the trade", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.R. M\u00fcller"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Support vector machines classification with a very large-scale taxonomy", "author": ["T.Y. Liu", "Y. Yang", "H. Wan", "H.J. Zeng", "Z. Chen", "W.Y. Ma"], "venue": "SIGKDD Explorations 7(1),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Efficient voting prediction for pairwise multilabel classification", "author": ["E. Loza Menc\u0131\u0301a", "S.H. Park", "J. F\u00fcrnkranz"], "venue": "Neurocomputing 73(7-9),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning. pp", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Classifier chains for multi-label classification", "author": ["J. Read", "B. Pfahringer", "G. Holmes", "E. Frank"], "venue": "Machine Learning 85(3),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Statistical topic models for multi-label document classification", "author": ["T.N. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers"], "venue": "Machine Learning 88(1-2),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature 323(6088),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1986}, {"title": "Boostexter: A boosting-based system for text categorization", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning 39(2/3),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Accelerated learning in layered neural networks", "author": ["S.A. Solla", "E. Levin", "M. Fleisher"], "venue": "Complex Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1988}, {"title": "Multi-label classification of music into emotions", "author": ["K. Trohidis", "G. Tsoumakas", "G. Kalliris", "I. Vlahavas"], "venue": "Proceedings of the 9th International Conference on Music Information Retrieval. pp", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Random k-labelsets for multilabel classification", "author": ["G. Tsoumakas", "I. Katakis", "I.P. Vlahavas"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Multilabel classification with meta-level features in a learning-to-rank framework", "author": ["Y. Yang", "S. Gopal"], "venue": "Machine Learning 88(1-2),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M.Z. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G.E. Hinton"], "venue": "In: Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Multilabel neural networks with applications to functional genomics and text categorization", "author": ["M.L. Zhang", "Z.H. Zhou"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "In the literature, one can find a number of multi-label classification approaches for a variety of tasks in different domains such as bioinformatics [1], music [27], and text [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 26, "context": "In the literature, one can find a number of multi-label classification approaches for a variety of tasks in different domains such as bioinformatics [1], music [27], and text [8].", "startOffset": 160, "endOffset": 164}, {"referenceID": 7, "context": "In the literature, one can find a number of multi-label classification approaches for a variety of tasks in different domains such as bioinformatics [1], music [27], and text [8].", "startOffset": 175, "endOffset": 178}, {"referenceID": 21, "context": "However, this so-called binary relevance approach ignores dependencies between the labels, so that current research in multi-label classification concentrates on the question of how such dependencies can be exploited [22, 3].", "startOffset": 217, "endOffset": 224}, {"referenceID": 2, "context": "However, this so-called binary relevance approach ignores dependencies between the labels, so that current research in multi-label classification concentrates on the question of how such dependencies can be exploited [22, 3].", "startOffset": 217, "endOffset": 224}, {"referenceID": 30, "context": "One such approach is BP-MLL [31], which formulates multi-label classification problems as a neural network with multiple output nodes, one for each label.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "Second, as it has been shown in the literature [14], popular feature representation schemes for textual data such as variants of tf-idf term weighting already incorporate a certain degree of higher dimensional features, and we speculate that even a single-layer NN model can work well with text data.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "To address this limitation of BR, pairwise decomposition (PW) and label powerset (LP) approaches consider label dependencies during the transformation by either generating pairwise subproblems [8, 19] or the powerset of possible label combinations [28].", "startOffset": 193, "endOffset": 200}, {"referenceID": 18, "context": "To address this limitation of BR, pairwise decomposition (PW) and label powerset (LP) approaches consider label dependencies during the transformation by either generating pairwise subproblems [8, 19] or the powerset of possible label combinations [28].", "startOffset": 193, "endOffset": 200}, {"referenceID": 27, "context": "To address this limitation of BR, pairwise decomposition (PW) and label powerset (LP) approaches consider label dependencies during the transformation by either generating pairwise subproblems [8, 19] or the powerset of possible label combinations [28].", "startOffset": 248, "endOffset": 252}, {"referenceID": 21, "context": "Classifier chains [22, 3] are another popular approach that extend BR by including previous predictions into the predictions of subsequent labels.", "startOffset": 18, "endOffset": 25}, {"referenceID": 2, "context": "Classifier chains [22, 3] are another popular approach that extend BR by including previous predictions into the predictions of subsequent labels.", "startOffset": 18, "endOffset": 25}, {"referenceID": 5, "context": "[6] present a large-margin classifier, RankSVM, that minimizes a ranking loss by penalizing incorrectly ordered pairs of labels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "In order to make a prediction, the ranking has to be calibrated [8], i.", "startOffset": 64, "endOffset": 67}, {"referenceID": 30, "context": "Similarly, Zhang and Zhou [31] introduced a framework that learns ranking errors in neural networks via backpropagation (BP-MLL).", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "The most prominent learning method for multi-label text classification is to use a BR approach with strong binary classifiers such as SVMs [23, 29] despite its simplicity.", "startOffset": 139, "endOffset": 147}, {"referenceID": 28, "context": "The most prominent learning method for multi-label text classification is to use a BR approach with strong binary classifiers such as SVMs [23, 29] despite its simplicity.", "startOffset": 139, "endOffset": 147}, {"referenceID": 13, "context": "It is well known that characteristics of high-dimensional and sparse data, such as text data, make decision problems linearly separable [14], and this characteristic suits the strengths of SVM classifiers well.", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "To handle such datasets, researchers have derived efficient linear SVMs [15, 7] that can handle large-scale problems.", "startOffset": 72, "endOffset": 79}, {"referenceID": 6, "context": "To handle such datasets, researchers have derived efficient linear SVMs [15, 7] that can handle large-scale problems.", "startOffset": 72, "endOffset": 79}, {"referenceID": 17, "context": "However, their performance decreases as the number of labels grows and the label frequency distribution becomes skewed [18, 23].", "startOffset": 119, "endOffset": 127}, {"referenceID": 22, "context": "However, their performance decreases as the number of labels grows and the label frequency distribution becomes skewed [18, 23].", "startOffset": 119, "endOffset": 127}, {"referenceID": 5, "context": "In such cases, it is also intractable to employ methods that minimize ranking errors among labels [6, 31] or that learn joint probability distributions of labels [10, 3].", "startOffset": 98, "endOffset": 105}, {"referenceID": 30, "context": "In such cases, it is also intractable to employ methods that minimize ranking errors among labels [6, 31] or that learn joint probability distributions of labels [10, 3].", "startOffset": 98, "endOffset": 105}, {"referenceID": 9, "context": "In such cases, it is also intractable to employ methods that minimize ranking errors among labels [6, 31] or that learn joint probability distributions of labels [10, 3].", "startOffset": 162, "endOffset": 169}, {"referenceID": 2, "context": "In such cases, it is also intractable to employ methods that minimize ranking errors among labels [6, 31] or that learn joint probability distributions of labels [10, 3].", "startOffset": 162, "endOffset": 169}, {"referenceID": 24, "context": "Therefore, convex surrogate losses have been proposed as alternatives to rank loss [25, 6, 31].", "startOffset": 83, "endOffset": 94}, {"referenceID": 5, "context": "Therefore, convex surrogate losses have been proposed as alternatives to rank loss [25, 6, 31].", "startOffset": 83, "endOffset": 94}, {"referenceID": 30, "context": "Therefore, convex surrogate losses have been proposed as alternatives to rank loss [25, 6, 31].", "startOffset": 83, "endOffset": 94}, {"referenceID": 0, "context": "The network, then, can be written in a matrix-vector form, and we can construct a feed-forward network f\u0398 : x\u2192 o as a composite of non-linear functions in the range [0, 1]:", "startOffset": 165, "endOffset": 171}, {"referenceID": 30, "context": "BP-MLL [31] minimizes errors induced by incorrectly ordered pairs of labels, in order to exploit dependencies among labels.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "This sort of thresholding methods are also used in [6, 31] For each document xm, labels are sorted by the probabilities in decreasing order.", "startOffset": 51, "endOffset": 58}, {"referenceID": 30, "context": "This sort of thresholding methods are also used in [6, 31] For each document xm, labels are sorted by the probabilities in decreasing order.", "startOffset": 51, "endOffset": 58}, {"referenceID": 1, "context": "t Rank Loss Recently, it has been claimed that none of convex loss functions including BP-MLL\u2019s loss function (Equation 3) is consistent with respect to rank loss which is non-convex and has discontinuity [2, 9].", "startOffset": 205, "endOffset": 211}, {"referenceID": 8, "context": "t Rank Loss Recently, it has been claimed that none of convex loss functions including BP-MLL\u2019s loss function (Equation 3) is consistent with respect to rank loss which is non-convex and has discontinuity [2, 9].", "startOffset": 205, "endOffset": 211}, {"referenceID": 3, "context": "Furthermore, univariate surrogate loss functions such as log loss are rather consistent with rank loss [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 25, "context": "parameters plays an important role in learning parameters of neural networks [26, 11] which we follow.", "startOffset": 77, "endOffset": 85}, {"referenceID": 10, "context": "parameters plays an important role in learning parameters of neural networks [26, 11] which we follow.", "startOffset": 77, "endOffset": 85}, {"referenceID": 20, "context": "Rectified Linear Units Rectified linear units (ReLUs) have been proposed as activation units on the hidden layer and shown to yield better generalization performance [21, 12, 30].", "startOffset": 166, "endOffset": 178}, {"referenceID": 11, "context": "Rectified Linear Units Rectified linear units (ReLUs) have been proposed as activation units on the hidden layer and shown to yield better generalization performance [21, 12, 30].", "startOffset": 166, "endOffset": 178}, {"referenceID": 29, "context": "Rectified Linear Units Rectified linear units (ReLUs) have been proposed as activation units on the hidden layer and shown to yield better generalization performance [21, 12, 30].", "startOffset": 166, "endOffset": 178}, {"referenceID": 15, "context": "A common approach is to estimate the learning rate which gives lower training errors on subsamples of training examples [16] and then decrease it over time.", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "Furthermore, to accelerate learning speed of SGD, one can utilize momentum [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 4, "context": "Instead of a fixed or scheduled learning rate, an adaptive learning rate method, namely AdaGrad, was proposed [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 12, "context": "Dropout training [13] is a technique for preventing overfitting in a huge parameter space.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "In order to evaluate the quality of a ranked list, we consider several ranking measures [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "There are two ways of computing such performance measures: Micro-averaged measures and Macro-averaged measures4[20].", "startOffset": 111, "endOffset": 115}, {"referenceID": 28, "context": "For Reuters21578 we used the same training/test split as previous works [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "Training and test data were switched for RCV1-v2 [17] which originally consists of 23,149 train and 781,265 test documents.", "startOffset": 49, "endOffset": 53}, {"referenceID": 29, "context": "That NNs with ReLUs at the hidden layer converge faster into a better weight space has been previously observed for the speech domain [30].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "8 However, unlike the results of [30], in our preliminary experiments adding more hidden layers did not further improve generalization performance.", "startOffset": 33, "endOffset": 37}, {"referenceID": 15, "context": "Limiting Small Learning Rates in BP-MLL The learning rate strongly influences convergence and learning speed [16].", "startOffset": 109, "endOffset": 113}], "year": 2014, "abstractText": "Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL\u2019s ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.", "creator": "LaTeX with hyperref package"}}}