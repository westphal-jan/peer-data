{"id": "1312.5697", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Using Web Co-occurrence Statistics for Improving Image Categorization", "abstract": "site recognition and localization are paramount tasks in computer optimization. the necessity of this work is the incorporation of contextual inference which accounts drastically improve object retrieval and localization. for instance, an emerges natural to acknowledge not strangers see an ocean to appear in isolated small maritime world ocean. programmers consider a database approach to encapsulate such present sense knowledge using co - occurrence statistics from databases documents. in merely counting the number different local nouns ( such into ravens, sharks, fishes, foxes. ) co - cases surrounding common websites, we obtain immediately greater estimate of expected co - occurrences in database media. we then cast different challenges of interpreting numerous co - exist statistics concerning the predictions affecting image - based approaches as numerical optimization package. the resulting optimization algorithm remain as a component for our inference procedure. among the simplicity before the comparative optimization problem, it became appropriate in assessing spatial recognition and localization accuracy. moreover, we observe significant improvements by recognition and localization probability for both imagenet detection 2012 and sun protection algorithms.", "histories": [["v1", "Thu, 19 Dec 2013 18:53:47 GMT  (52345kb,D)", "https://arxiv.org/abs/1312.5697v1", null], ["v2", "Fri, 20 Dec 2013 18:12:16 GMT  (52434kb,D)", "http://arxiv.org/abs/1312.5697v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["samy bengio", "jeff dean", "dumitru erhan", "eugene ie", "quoc le", "rew rabinovich", "jonathon shlens", "yoram singer"], "accepted": false, "id": "1312.5697"}, "pdf": {"name": "1312.5697.pdf", "metadata": {"source": "CRF", "title": "Using Web Co-occurrence Statistics for Improving Image Categorization", "authors": ["Samy Bengio", "Jeff Dean", "Dumitru Erhan", "Eugene Ie", "Quoc Le", "Andrew Rabinovich", "Jonathon Shlens"], "emails": ["bengio@google.com", "jeff@google.com", "dumitru@google.com", "eugeneie@google.com", "qvl@google.com", "amrabino@google.com", "shlens@google.com", "singer@google.com"], "sections": [{"heading": "1 Introduction", "text": "Object recognition from images is a challenging task at the intersection of computer vision and machine learning. A major source of difficulty stems from the fact that the number of object classes is large and it is easy to confuse visually related objects. The bulk of the work on object recognition focused on the task of identifying individual objects from a single or multiple patches of the image. The existence of semantically related objects in a single image is often sidestepped, though as the number of object classes increases the potential of class confusion dramatically increases as well.\nFew approaches were proposed in the computer vision literature to incorporate contextual information, see for instance [4, 11, 12]. Existing object classification methods that incorporate context fall roughly into two categories. The first considers global image features to be the source of context, thus trying to capture class-specific features [12]. The second classifies objects while taking into account the existence of the rest of the objects in the scene [11]. The latter work used Google Sets as a source of information for co-occurrences of objects. Unfortunately this source of information was inferior to using co-occurrence information gathered from image training data. Research in the second setting has further generalized in various ways. The work of Galleguillos et al. [4] considers both semantic context, that is, co-occurrences of objects as well as spatial relations between objects. In [1], the authors further extend the latter approach by proposing an \u201cObject Relation Network\u201d that models behavioral relations between objects in images. The aforementioned approaches typically require laborious localization and labeling of objects in training images. To mitigate this problem,\nar X\niv :1\n31 2.\n56 97\nv2 [\ncs .C\nV ]\n2 0\nthe usage of unlabeled data through grouping of image regions of the same context was proposed in [5].\nIn this paper we present an approach that infuses an external information source that captures the statistical tendency of visual objects to co-occur in images. Rather than resorting to the image content itself for the co-occurrence model, we use a large collection of textual data to count the co-occurrences for any set of objects, as illustrated in Figure 1.\nAs mentioned above, most of prior work, including the cited papers, is built upon the visual training set itself. To our knowledge, there was not a substantial effort to incorporate external information from vastly different information sources, such as web documents that is considered in this paper. In contrast, modern speech recognition approaches have been using language models for many years [10]. In speech recognition systems, the acoustic model is trained to recognize individual or short sequences of phonemes. Its output is used by a \u201cdecoder\u201d that incorporates language statistics, typically in the form of a Markovian model whose parameters are estimated from heterogeneous text sources. We propose to use an analogous (yet simpler) approach for performing multiple object recognition. While images do not exhibit the rich morphological and syntactic structure of natural languages, we show that the co-existence of semantically related objects can still be leveraged. Our approach indeed draws a parallel line to speech recognition systems in which the acoustic model is combined with a language model that is typically constructed from text-based sources. However, due to the lack of temporal structure our decoding procedure diverges from the dynamic-programming and cast the inference task as a static optimization problem.\nThe end result is a general and scalable scheme that can be applied to different sources of images while incorporating the statistical co-occurrence model without the need of specialized training data. We term our approach Laconic as an acronym for label consistency for image categorization.\nThe rest of the paper is structured as follows. Sec. 2 describes the Laconic model and the underlying optimization that is used as an inference tool. Sec. 3 describes the co-occurrence model and how it is estimated from the web. Sec. 4 presents experimental results on two different datasets (Sun12 and ImageNet) for which significant performance improvements are shown when using Laconic. Finally, some conclusions and potential extensions are provided in Sec. 5."}, {"heading": "2 The Laconic Setting", "text": "We start by establishing the notation used throughout the paper. We denote scalars by lower case letters, vectors by boldface lower case letters, e.g. v \u2208 Rp, and matrices by upper case letters. The transpose of a matrixA is denotedAT. Vectors are also viewed as p\u00d71 matrices. Hence, the 2-norm squared of a vector can be denoted as vTv. We denote the number of different object classes by p.\nThe Laconic objective consists of three components: an image-based object score which is obtained from an image classifier (see Sec. 4), a co-occurrence score based on term proximity in text-based web data (see Sec. 3), and a regularization term to prevent overfitting. We denote the vector of object scores by \u00b5 \u2208 Rp where the value of \u00b5j increases with the likelihood that object j appears in the image. We refer to this vector as the external field. We denote the matrix of co-occurrence statistics or object pairwise similarity by S \u2208 Rp\u00d7p+ . This matrix is highly sparse and its entries are non-negative. Its construction is described in Sec. 3. We also optionally add domain constraints on the set of admissible solutions in order to extract semantics from the scores inferred for each label and as an additional mechanism to guard against overfitting.\nAbstractly, our inference amounts to finding a vector \u03b1 \u2208 Rp minimizing the following objective, Q(\u03b1|\u00b5, S) = E(\u03b1|\u00b5) + \u03bbC(\u03b1|S) + R(\u03b1) s.t. \u03b1 \u2208 \u2126 , (1)\nwhere \u03bb and are hyper-parameters to be selected on a separate validation set. For concreteness we describe the inference procedure as a minimization task. The first term E(\u03b1|\u00b5) measures the conformity of the inferred vector \u03b1 to the external field \u00b5. We tested the following terms\n\u2212\u00b5T\u03b1 [linear] (2)\u2211p j=1 \u03b1j log ( \u03b1j \u00b5j ) + \u00b5j \u2212 \u03b1j [relative entropy] . (3)\nThe linear score can be used in all settings, in particular, when the outputs of the object recognizers take general values in Rp. The relative entropy score can be used when the outputs are non-negative, especially in the case where the outputs are normalized to reside in the probability simplex. Such normalization often takes place by applying a softmax function at the top layer of a neural network.\nShifting our focus to the second term C(\u03b1|S), we experimented with the following scores, \u2212\u03b1TS\u03b1 [Ising] (4)\u2211\ni,j Si,j D(\u03b1i \u2212 \u03b1j) [difference] . (5)\nThe Ising model assesses similarity between activation levels of object category pairs. When Si,j is high, the two categories tend to co-occur in natural texts underscoring the potential of similar co-occurrence in natural images. Thus, the value of \u03b1j will be \u201cpulled up\u201d by \u03b1i if the external field of the latter, \u00b5i, is large. The sum of (4) and (2) is essentially the Ising model [7]. In contrast to the physical model, we do not require an integer solution. Alas, for general matrices S, the objective function Q is not convex in \u03b1. Thus, while we are able to use classical optimization techniques, a convergent sequence is likely to end in a local optimum. We revisit this issue in the sequel.\nThe divergence D : R2 \u2192 R+ assesses the difference between the activation levels of two labels. Let \u03b4 denote the difference of an arbitrary pair \u03b1i\u2212\u03b1j . We evaluated and experimented with several options for D, based on `p norms and the Huber loss. We report result for the Huber loss, defined as,\nDH(\u03b4) = { \u03b42/2 |\u03b4| \u2264 1 (|\u03b4| \u2212 1/2) |\u03b4| > 1 . (6)\nAll the difference-based penalties we constructed are convex in\u03b1. However, difference-based penalties inhibit the activation values of objects that tend to co-occur frequently in texts to become vastly different from each other. This property while seemingly useful is in fact a two-edged sword as it may create \u201challucination\u201d artifacts when solely one of two highly-correlated labels appears in an image. Indeed, as our experimental results indicate, by using multiple random starting points, the Ising-like penalty yields better retrieval performance.\nFinally, we need to describe the regularization component and the domain constraints. Throughout our experiments we incorporated a 2-norm regularization, namely R(\u03b1) = \u03b1T\u03b1. In some of our experiments we cast an additional requirement in order to find a small subset of the most relevant labels. Concretely, we use a conjunction of a simplex constraint and an\u221e-norm constraint, yielding,\n\u2126 = \u03b1 s.t. \u2211 j \u03b1j \u2264 N , \u2016\u03b1\u2016\u221e \u2264 1 , \u2200j : \u03b1j \u2265 0  . (7) The above domain constraints relax the combinatorial requirement to have at most N object classes presented in the image. Since fractional solutions are admissible, we often get more thanN non-zero\nindices in \u03b1. In order to find the optimum under the domain constraints of (7) we need to perform gradient projection steps in which each gradient step of (1) is followed by a projection onto \u2126. An efficient projection procedure is provided in the supplementary material."}, {"heading": "3 Label Co-Occurrences", "text": "In order to estimate the prior probability of observing two labels i and j in the same image, we harvested a sample of web documents, totalling a few billion documents. For each document we examined every possible sub-sequence (coined window) of consecutive words of length 20. We then counted the number of times each label was observed along with the number of co-occurrences of label-pairs within each window. We next constructed estimates for the point-wise mutual information:\nsi,j = log\n( p(i, j)\np(i)p(j)\n) , (8)\nwhere p(i, j) and p(i) are the aforementioned counts normalized to the probability simplex.\nWe discarded all pairs whose co-occurrence count was below a threshold. Since web data is relatively noisy and our collection was fairly large, even bizarre co-occurrences can be observed numerous times. We thus set the threshold to 106 appearances. We only kept pairs whose point-wise mutual information was positive, corresponding to label-pairs which tend to appear together. We then transformed the scores using the logit function,\nSi,j =  1 1 + exp(\u2212si,j) , if si,j > 0\n0 otherwise. (9)\nIn Fig. 2 we illustrate a typical matrix as processed by equation (9) for the following 10 classes: chair, bicycle, bookshelf, car, keyboard, monitor, street, window, street sign, mountain. Dark squares correspond to label-pairs of high mutual information, light squares to low mutual information, and crossed squares to negative mutual information, which are not used. Pairs of objects that indeed tend to occur in the same image, such as keyboard and monitor, attain higher scores than object pairs that are not unlikely to be observed in the same image, such as street and keyboard."}, {"heading": "4 Experiments", "text": "In this section we report the results of two sets of experiments with public datasets. The first dataset, Sun12, is small yet the largest of its kind as each image is annotated with multiple labels. The second dataset, ImageNet for detection, is much larger but most images are associated with a single label.\nIn order to evaluate Laconic, we used the following metrics:\n\u2022 Precision@k is the number of correct labels returned in the top k positions, divided by k. \u2022 AveragePrecision is the mean of Precision@ki where ki is the position obtained by target\nlabel i, for all target labels.\n\u2022 Detection@k is the number of correct detections returned in the top k positions, where a detection is valid if both the label is correct and the returned bounding box overlaps at least 50% with the target bounding box.\nFor each dataset, the training set was used to train a deep neural network (DNN) for classification. The validation set was used to select all hyper-parameters for the DNN and Laconic. The results for all the three metrics above are reported for the test sets."}, {"heading": "4.1 Sun12", "text": "Sun12 is a subset of the SUN dataset [13], consisting of fully annotated images from SUN. To our knowledge, Sun12 is the largest publicly available dataset with multiple objects per image that spans thousands of classes. There is a total of 16,856 images containing 165,271 objects from 3,765 classes. We split the dataset in our experiments into disjoint sets of 85%, 10%, and 5% for training, validation and testing respectively. The training set contains about 10 examples per object. The test set consists of 798 images and is comprised of 7,761 annotated objects.\nWe created two types of co-occurrence matrices: one from web pages and one from the Sun12 training set. Since Sun12 contains multiple labels per image, we built a co-occurrence matrix of object classes that appear together in images from the training set. We report Laconic results using the web-induced co-occurrence matrix, a matrix constructed from the Sun12 training set, and a convex combination of both. The mixture coefficient was selected using the validation set.\nFor training the DNN we sampled around 1.5M cropped windows of varying sizes from the training set, each of which contains at least 70% of one target object. These were then used to train a large scale convolutional network similar to [8], which won the last ImageNet challenge. Concretely, the model consisted of several layers, each of which performed a set of convolutions followed by local contrast normalization and max-pooling. These layers were followed by several fully connected linear and sigmoidal layers, finally ending with a softmax layer with 3765 outputs, where each output corresponds to one of the object classes in Sun12. The full model was trained using stochastic gradient descent combined with the AdaGrad algorithm [3] along with the Dropout [6] regularization technique. We used mini-batches of size 128 and an initial learning rate of 0.001.\nAt test time, we extracted the following seven patches at inference time from the test image. The first is the largest possible square patch centered at the middle of the image, which we refer to as \u201cthe original crop box\u201d. The rest of the patches were the original crop box enlarged by 5%, the original crop box shrunk by 5%, and the original crop box translated up, down, left and right by 5%. The prediction scores of these 7 boxes were then averaged.\nWe first ran a set of experiments in order to compare the various Laconic settings. Table 1 compares several combinations of the Laconic objective function described in (1), by varying the conformity term described in (2) and (3), or the co-occurrence term using either (4) or (6). We also tested the incorporation of domain constraints as described by (7). The combination consisting of a linear conformity term within the Ising model, a 2-norm regularization without further domain constraints seems to yield good performance overall. These experiments used the co-occurrence matrix constructed from web pages. Furthermore, since the Ising model for general matrices is non-definite, we used 10 random initializations for \u03b1 and selected the best solution according to (1). Each inference of a single image took a few milliseconds on a standard Linux machine. In comparison, naively using a conditional random field such as the one described in [11], would require the evaluation of\nabout ( 3765 5 ) combinations, which is greater than 1015. Instead, only a few thousands evaluations were required when using Laconic.\nTable 2 provides comparison results of the baseline model (using the DNN only) to the various Laconic settings. In these experiments we evaluated three settings for the co-occurrence matrix: estimation using web data only, estimation using the Sun12 training set only, and a convex combination of both matrices, termed Mixture-Laconic in the table. As can be seen, the Laconic approach with the web co-occurrence matrix gives significantly better performance than both the baseline model and the Laconic approach with the Sun12 co-occurrence matrix. The combination of both co-occurrence matrices gives slightly better results overall.\nExamples of classification results by the baseline and Laconic are shown in Figure 3, where we see that Laconic often surfaced labels that were not found by the baseline, based on the additional cooccurrence information. Figure 4 also compares Laconic to the baseline for precision@k for various values of k. Again, it shows that Laconic\u2019s performance is better than the baseline for all values of k."}, {"heading": "4.2 ImageNet Detection Dataset", "text": "Our second set of experiments is with the ImageNet dataset [2] (fall 2011 release). The entire dataset, typically used as a recognition benchmark, has almost 22,000 categories and 15 million images. A subset of this dataset is provided with bounding boxes which can be used for detection. There are 3623 categories that have bounding boxes and the total number of bounding boxes is 615,513. We divided the dataset into two subsets of equal size: half for training and half for testing.\nWe trained a deep neural network on this dataset. The architecture of the model is based on the model described in [9]. It consists of nine layers, has local receptive fields, employs pooling, local contrast normalization, and a softmax output layer for multiclass prediction.\nTo safeguard from overfitting, we added translated versions of the bounding boxes and negative examples. First, for every instance of the bounding boxes, we cropped 10 images, each translated up to 15 pixels in the horizontal and vertical directions. These transformations yielded about three million positive instances. Next, we sampled negative patches and treated them as a \u201cbackground\u201d category. To this end, we sampled random patches from the training images and made sure that the overlapping area with the correct bounding boxes is less than 30% of the total area. We also sampled examples that are difficult to classify by extracting patches that overlap with the correct bounding boxes at the four corners. The total number of negative (background) images is twelve million. Each of the above image patches were resized to an image consisting of 100x100 pixels. The total number of training images that were provide to learning algorithm for the deep neural network was fifteen million.\nTraining was performed using stochastic gradient descent with mini-batches. The resulting deep neural network was used for detecting objects in the training set using a sliding window. The sliding window procedure was applied at nine different scales where the i\u2019th replica was of size 1.1i of the base window size. For a specific scale, the overlap between one window to the next window is 80% of the size of the window. The windows were then resized to images of 100x100 pixels each using cubic interpolation. The patches were then fed to the deep network to obtain predictions. The above scheme is naturally massively parallelized in the inference step.\nIn order to asses the improvement of Laconic over the deep neural network we performed evaluation of two tasks: detection and classification. Since the co-occurrence matrix is extracted from the web pages, it may not convey any location information, and thus cannot help in localizing objects. It is therefore an interesting experiment on its own to see whether the external text-based information can improve the detection accuracy. Table 3 compares the performance of the baseline network with\nLaconic for two values of k: k = 1 and k = 5. The results show that Laconic substantially improves both classification and detection.\nNote that even when there only a single object class is associated with an image, Laconic can help in \u201csurfacing\u201d the correct class even if it is not the most likely class according to the baseline DNN output. Indeed, most images are typically under-labelled. That is, there are more objects that appear in the images and thus could have been added to the set of labels (object classes) of the image. Since ImageNet was manually labeled where most images were associated with a single label, the existence of additional objects that tend to co-occur in natural text can improve the classification accuracy. In other words, if the patch classifier assigned high values to related object classes albeit not the single target class, Laconic can increase the likelihood of the correct target class, leveraging the co-occurrence information between the target class and the aforementioned object classes.\nFinally, it is worth noting that all results provided for the ImageNet detection dataset, showing that Laconic was better than baseline, are statistically significant with 99% confidence, given the size of the test set. On the other hand, the Sun12 dataset being so small, none of the results are statistically significant, even at the 90% level."}, {"heading": "5 Conclusion", "text": "Image classification becomes a difficult task as the number of object classes increases. We proposed a new approach for incorporating side information extracted from web documents with the output of a deep neural network in order to improve classification and detection accuracy. We empirically evaluated our algorithm on two different datasets, Sun12 and Imagenet, and obtained consistent improvements in classification and detection accuracy on both datasets.\nIn future work we plan to go one step further and incorporate spatial textual information. For instance, we could count the number of times we observe sentences such \u201cthe chair is beside the table\u201d or \u201cthe car is under the bridge\u201d and construct triplets of the form (object1, relation, object2), where the relation expresses a spatial correspondence between the two objects. Such information could potentially lead to a more comprehensive and accurate visual scene analysis.\nOn the inference front, we plan to investigate alternative tractable approaches. For instance, approaches based on belief propagation have been used for related machine vision tasks. However, our preliminary experiments with belief propagation yielded poor performance and are thus not reported in the paper."}], "references": [{"title": "Understanding web images by object relation network", "author": ["N. Chen", "Q.-Y. Zhou", "V.K. Prasanna"], "venue": "In Proceedings of the 21st World Wide Web Conference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In IEEE Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J.C. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Object categorization using co-occurence, location and appearance", "author": ["C. Galleguillos", "A. Rabinovich", "S. Belongie"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Learning spatial context: Using stuff to find things", "author": ["G. Heitz", "D. Koller"], "venue": "In ECCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Beitrag zur theorie des ferromagnetismus", "author": ["E. Ising"], "venue": "Z. Phys.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1925}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M.A. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Fundamentals of Speech Recognition", "author": ["L. Rabiner", "B.-H. Juang"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Objects in context", "author": ["A. Rabinovich", "A. Vedaldi", "C. Galleguillos", "E. Wiewiora", "S. Belongie"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Contextual priming for object detection", "author": ["A. Torralba"], "venue": "International Journal of Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva", "A. Torralba"], "venue": "In 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}], "referenceMentions": [{"referenceID": 3, "context": "Few approaches were proposed in the computer vision literature to incorporate contextual information, see for instance [4, 11, 12].", "startOffset": 119, "endOffset": 130}, {"referenceID": 10, "context": "Few approaches were proposed in the computer vision literature to incorporate contextual information, see for instance [4, 11, 12].", "startOffset": 119, "endOffset": 130}, {"referenceID": 11, "context": "Few approaches were proposed in the computer vision literature to incorporate contextual information, see for instance [4, 11, 12].", "startOffset": 119, "endOffset": 130}, {"referenceID": 11, "context": "The first considers global image features to be the source of context, thus trying to capture class-specific features [12].", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "The second classifies objects while taking into account the existence of the rest of the objects in the scene [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 3, "context": "[4] considers both semantic context, that is, co-occurrences of objects as well as spatial relations between objects.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "In [1], the authors further extend the latter approach by proposing an \u201cObject Relation Network\u201d that models behavioral relations between objects in images.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "the usage of unlabeled data through grouping of image regions of the same context was proposed in [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 9, "context": "In contrast, modern speech recognition approaches have been using language models for many years [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "The sum of (4) and (2) is essentially the Ising model [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 12, "context": "Sun12 is a subset of the SUN dataset [13], consisting of fully annotated images from SUN.", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "These were then used to train a large scale convolutional network similar to [8], which won the last ImageNet challenge.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "The full model was trained using stochastic gradient descent combined with the AdaGrad algorithm [3] along with the Dropout [6] regularization technique.", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "The full model was trained using stochastic gradient descent combined with the AdaGrad algorithm [3] along with the Dropout [6] regularization technique.", "startOffset": 124, "endOffset": 127}, {"referenceID": 10, "context": "In comparison, naively using a conditional random field such as the one described in [11], would require the evaluation of", "startOffset": 85, "endOffset": 89}, {"referenceID": 1, "context": "Our second set of experiments is with the ImageNet dataset [2] (fall 2011 release).", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "The architecture of the model is based on the model described in [9].", "startOffset": 65, "endOffset": 68}], "year": 2013, "abstractText": "Object recognition and localization are important tasks in computer vision. The focus of this work is the incorporation of contextual information in order to improve object recognition and localization. For instance, it is natural to expect not to see an elephant to appear in the middle of an ocean. We consider a simple approach to encapsulate such common sense knowledge using co-occurrence statistics from web documents. By merely counting the number of times nouns (such as elephants, sharks, oceans, etc.) co-occur in web documents, we obtain a good estimate of expected co-occurrences in visual data. We then cast the problem of combining textual co-occurrence statistics with the predictions of image-based classifiers as an optimization problem. The resulting optimization problem serves as a surrogate for our inference procedure. Albeit the simplicity of the resulting optimization problem, it is effective in improving both recognition and localization accuracy. Concretely, we observe significant improvements in recognition and localization rates for both ImageNet Detection 2012 and Sun 2012 datasets.", "creator": "TeX"}}}