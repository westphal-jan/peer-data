{"id": "1312.6098", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "On the number of response regions of deep feed forward networks with piece-wise linear activations", "abstract": "this paper introduced the complexity of deep scale amplifier networks with linear presynaptic couplings and inverted linear images. this is his approach to narrowly term body oriented physics providing its raw power of broad and shallow array architectures. how particular, authors offer a solution for comparing tall and shallow fibers that belong to the family providing sampling - wise linear simulations based on empirical neuroscience. we gather at a deep ( assuming hidden cells ) rectifier multilayer perceptron ( cube ) with visual information only and compare both with a single layer version of the texture. sometimes greedy asymptotic regime as same frequency of units goes to it, if sampled shallow machine has $ 2n $ 2d units puts $ n _ 0 $ 00, then initial number of singleton equations keeps $ o ( n ^ { n _ 0 } ) $. a fuzzy bucket scenario holds $ n $ results. hidden products on closed sphere has $ \\ omega ( p ^ { nz _ 0 } ) $. historians consider sampling as a first step towards understanding the complexity of these models enabling systems using better constructions in particular framework easily produce more accurate results ( especially for highly interesting case of when the number between hidden layers occurs far infinity ).", "histories": [["v1", "Fri, 20 Dec 2013 20:22:31 GMT  (65kb,D)", "http://arxiv.org/abs/1312.6098v1", null], ["v2", "Mon, 6 Jan 2014 19:53:34 GMT  (242kb,D)", "http://arxiv.org/abs/1312.6098v2", null], ["v3", "Mon, 27 Jan 2014 22:13:09 GMT  (244kb,D)", "http://arxiv.org/abs/1312.6098v3", null], ["v4", "Mon, 10 Feb 2014 17:24:12 GMT  (291kb,D)", "http://arxiv.org/abs/1312.6098v4", null], ["v5", "Fri, 14 Feb 2014 17:52:12 GMT  (2248kb,D)", "http://arxiv.org/abs/1312.6098v5", "17 pages, 9 figures"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["razvan pascanu", "guido montufar", "yoshua bengio"], "accepted": true, "id": "1312.6098"}, "pdf": {"name": "1312.6098.pdf", "metadata": {"source": "CRF", "title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "authors": ["Razvan Pascanu", "Guido Mont\u00fafar"], "emails": ["r.pascanu@gmail.com", "montufar@mis.mpg.de", "yoshua.bengio@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "Deep systems are believed to play an important role in information processing of intelligent agents. A common hypothesis underlying this belief is that a deep model can be exponentially more efficient at representing some functions than its shallow counter-part (Bengio, 2009).\nThe argument is usually a compositional one. Higher layers in the model can re-use primitives constructed by the lower layers in order to build gradually more complex functions. For example, on a vision task, one would hope that the first layer learns Gabor filters capable to detect edges of different orientation. These edges are then put together at the second layer to form part-of-object shapes. On higher layers these part-of-object shapes are combined further to obtain detectors for more complex part-of-object shapes or the objects. Such a behaviour is empirically illustrated, for instance, in Zeiler and Fergus (2013); Lee et al. (2009). On the other hand, a shallow model has to construct detectors of target objects based only on the detectors learnt by the first layer.\nIn addition to such inspections a wealth of evidence for the validity of this hypothesis comes from deep models consistently outperforming shallow ones on a variety of tasks and datasets (see, e.g., Goodfellow et al., 2013; Hinton et al., 2012b,a). However theoretical results on the representational power of deep models are limited, usually due to the composition of nonlinear functions in deep models which makes mathematical analysis difficult. Up to now, the theoretical results have\nar X\niv :1\n31 2.\n60 98\nv1 [\ncs .L\nG ]\nbeen with circuit operations (neural net unit computations) that were substantially different from those being used in real state-of-the-art applications of deep learning, such as logic gates (Ha\u030astad, 1986), linear + threshold units with non-negative weights (Ha\u030astad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011). Bengio and Delalleau (2011) shows that deep sum-product networks (Poon and Domingos, 2011) can use exponentially less nodes to express some families of polynomials compared to the shallow ones.\nThis note analyzes the representational power of deep MLP with rectifier units. Rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), and in general piece-wise linear activation function (for example the maxout unit (Goodfellow et al., 2013)), are becoming a popular choice in designing deep models, and most current state-of-the-art results involve using one of such activations (Goodfellow et al., 2013; Hinton et al., 2012b). Glorot et al. (2011) show that rectifier units have several properties, that makes the optimization problem easier than the more traditional case of using smooth and bounded activations, such as tanh or sigmoid.\nIn this work we take advantage of the piece-wise linear nature of the rectifier unit to mathematically analyse the behaviour of deep rectifier MLPs. Given that the model is a composition of piece-wise linear functions, it is itself a piece-wise linear function. We compare the flexibility of a deep model versus a shallow one by counting the number of linear regions they define over the input space for a fixed number of hidden units. We believe this approach to be extensible to any other piecewise linear activation function or architecture, including the maxout activation and convolutional networks."}, {"heading": "2 Preliminaries", "text": "Definition 1. The models that we consider are defined by a feed forward network with the following properties. Each hidden unit receives a number of real inputs x1, . . . , xn from the previous layer, computes the weighted sum s = \u2211 i\u2208[n] wixi + b, and outputs the rectified value rect(s) = max{0, s}. The last layer computes a linear combination of its inputs. We call w the weights and b the bias of the unit.\nGiven a vector of naturals n = (n0, n1, . . . , nL), we denote by Fn the set of all functions Rn0 \u2192 RnL that can be represented by a network with n0 input units and nl units in layer l for l \u2208 [L]. As mentioned in the introduction, the functions in this class are continuous piece-wise linear.\nWe denote byR(n) the maximal number of regions of linearity of a function in F(n).\nAs formalized in Lemma 2, a vector-valued function f with k scalar coordinates fi, each of which is piece-wise linear, has as many regions of linearity as distinct intersections of the regions of linearity of the coordinates fi. Lemma 2. Consider an internal layer of width k. For each i \u2208 [k], let Ri = {Ri1, . . . , RiNi} be the regions of linearity of the function fi : Rn \u2192 R represented by unit i in this layer. Then the regions of linearity of the function f = (fi)i\u2208[k] : Rn \u2192 Rk are R = {Rj1,...,jk = R1j1 \u2229 \u00b7 \u00b7 \u00b7 \u2229 Rkjk}(j1,...,jk)\u2208[N1]\u00d7\u00b7\u00b7\u00b7[Nk].\nProof. A function (fi)i\u2208[k] : Rn \u2192 Rk is linear iff all its coordinates are. Remark 3. A non singular affine transformation has the same regions of linearity as its input. In particular, the number of (linear) output units does not affect the number of input space partition regions if the output weight matrix is full rank.\nA formal proof of this remark is given in Lemma 4. Lemma 4. Let g : Rn \u2192 Rm be the pre-synaptic activation of an intermediate layer of width m of a deep feed forward rectifier model, where g = (gi)i\u2208[m],\ngi(x) = W:,i  f0(x) f1(x)\n... fk(x)\n+ b,\nwhere fi\u2208[k] are the activation of the layer below. If gi are non singular affine transformations, then the distinct linear regions of g are the same as the regions of gi, for any i \u2208 [m].\nProof. Following Lemma 2, the regions of linear behaviour of f = (fi)i\u2208[k] is R. Because gi is a linear function of f , it inherits the same discontinuity boundaries, and hence the regions of linearity Rgi are the same regions R of f for any i \u2208 [m]. Note that we assume gi is not constant but actually varies linearly with the input.\nIf we look at g, it has the linear regions Rg = {Rj1 , . . . Rjm = Rg1j1 \u2229 \u00b7 \u00b7 \u00b7 \u2229 Rgmjm }(j1...jm)\u2208[N1]\u00d7\u00b7\u00b7\u00b7[Nm]. But because the intersection of any pair R g1 j1 \u2229 \u00b7 \u00b7 \u00b7\u2229Rgmjm is either empty or some region of R, a consequence of the fact that each Rgi is the set of non-overlapping regions R, it means Rg is R."}, {"heading": "3 One layer rectifier MLP", "text": "Let us look at how many linear regions we have for a single layer MLP with n0 input units and n1 hidden units.\nWe first reformulate the rectifier unit as follows:\nrect(s) = I(s) \u00b7 s, (1) where I is the indicator function defined as:\nI(s) = {\n1 , s > 0 0 , otherwise (2)\nWe can now write the single layer MLP as the function f : Rn0 \u2192 Rny :\nf(x) = W (out) 1 diag  I(W(1)0: x + b(1)0 )\u00b7 \u00b7 \u00b7 I(W(1)n1:x + b (1) n1 ) W(1)x +W\n(out) 1 diag  I(W(1)0: x + b(1)0 )\u00b7 \u00b7 \u00b7 I(W(1)n1:x + b (1) n1 ) b(1) + b(out) (3)\nFrom this formulation it is clear that each unit i in the first layer has to operational modes. They are devided by a hyperplane Hi that corresponds to W (1) i: x + b (1) i = 0. Below the hyperplane, the activation of the unit is constant equal to zero, and above the hyperplane, it is linear with gradient equal to \u03bdri, where \u03bd is the unit normal of the hyperplane.\nIt follows that the number of regions of a single layer MLP corresponds to the number of regions formed by intersecting the hyperplanes Hi. Problem 1. Into how many regions does an arrangement of N hyperplanes split Euclidean space?\nThe answer to Problem 1 in the general case is given by following Theorem 5, which is one of the central results from the theory of hyperplane arrangements.\nA region of a hyperplane arrangement A is a connected component of the complement of the hyperplanes. A region is relatively bounded if its intersection with the space spanned by the normals of the hyperplanes, is bounded. The rank of an arrangement is the dimension of the space spanned by the normals of the hyperplanes. Let r(A) denote the number of regions and b(A) the number of relatively bounded regions of A. Theorem 5 (Zaslavsky, 1975). Let A be an arrangement in n-dimensional real vector space. Then\nr(A) = (\u22121)n\u03c7A(\u22121) (4) b(A) = (\u22121)rank(A)\u03c7A(1). (5)\nFor arrangements in general position the result can be stated as follows (see Stanley, 2004, Proposition 2.4). Proposition 6. LetA be an n-dimensional arrangement ofm hyperplanes in general position. Then\nr(A) = n\u2211\nk=0\n( m\nk\n) (6)\nb(A) = ( m\u2212 1 n ) . (7)\nThe number of regions of m lines in general position in R2 is equal to the number of intersections plus the number of lines plus one, which is:\nr(A) = ( m\n2\n) +m+ 1 (8)\nNote that an arrangement is in general position if for any three hyperplanes Hi, Hj and Hk of the arrangement we have Hi \u2229Hj 6= Hi \u2229Hk. For the purpose of illustration, we will sketch a proof for the 2-dimensional case using the sweep hyperplane method.\nProof sketch for two dimensional arrangements. We prove equation 8 by induction over the number of lines m.\nBase case m = 0\nIt is obvious that in this case we have a single linear region which corresponds to whole plane. Therefore r(A0) = 1. Induction step\nLet us assume that for m regions we have r(Am) = ( m 2 ) + m + 1 and us add a new line to the arrangement lm+1. Because we assumed the lines are in general position, lm+1 has to intersect each of the existing lines lk in a distinct point. Fig. 3 depicts this situation for m = 2.\nThe m intersection points split the line lm into m + 1 segments. Each of these segments cut an existing region of the arrangement. In Fig. 3 the two intersection points result in three segments that split each of the regions r1, r2, r3 into two. Therefore by adding the line we get m+ 1 new regions.\nr(Am+1) = r(Am) +m = m(m\u22121)2 +m+ 1 +m+ 1 = m\n2\u2212m+2m 2 + (m+ 1) + 1 = m(m+1) 2 + (m+ 1) + 1 = ( m+1 2 ) + (m+ 1) +m\n(9)\nProposition 7. The input space partition of the model with n0 inputs and n1 hidden units is given by an arrangement of n1 hyperplanes in n0-dimensional space. The maximal number of regions of such an arrangement isR(n0, n1, ny) = \u2211n0 j=0 ( n1 j ) .\nProof. This can be seen as a consequence of Lemma 2. The maximal number of regions is produced by a generic arrangement of n1 hyperplanes in Rn0 , which is given in Proposition 6."}, {"heading": "4 Several layers", "text": "Let r(n0, n1), b(n0, n1), and u(n0, n1) denote the number of regions, bounded regions, and unbounded regions of a generic arrangement of n1 hyperplanes in n0-dimensional space.\nProposition 8. The number of regions of linearity of a width-m internal layer of a rectifier MLP with constant sign weights can be made as large as\nr + 1 + (m\u2212 1)u, where r and u are the number of regions and unbounded regions with negative values of the previous layer, respectively. In particular,\nR(n0, n1, n2, ny) \u2265 n0\u2211 k=0 ( n1 k ) + 1 + (n2 \u2212 1) ( n0\u2211 k=0 ( n1 k ) \u2212 ( n1 \u2212 1 n0 )) .\nProof. Given the fixed choice of weights for each unit in the considered layer, if the units were linear they would have the same regions of linearity. Using a large negative bias, different for each unit, each unit thresholds the unbounded negative regions of the same piece-wise linear function at different levels. The special case follows using a generic arrangement in the first layer, and negative weights in the second layer.\nThe proof is reminiscent of the hyperplane sweep method used for the proof of equation 8. Fig. 2 describes the gist of this proposition for the case when we have two input units, n0 = 2, three units in the first hidden layer, and 3 units in the second hidden layer, and a single output unit.\nProposition 9. Consider a single layer rectified MLP with 2n units and n0 inputs. Then the maximal number of regions of linearity of the functions represented by this network is\nR(n0, 2n, 1) = n0\u2211 k=0 ( 2n k ) ,\nwhich implies that R(n0, 2n, 1) = O(nn0), when n0 = O(1).\nConsider a two layer rectified MLP with hidden layers of width n and n0 inputs. Then the maximal number of regions of linearity of the functions represented by this network is at least\nR(n0, n, n, 1) \u2265 n0\u2211 k=0 ( n k ) + 1 + (n\u2212 1) ( n0\u2211 k=0 ( n k ) \u2212 ( n\u2212 1 n0 )) ,\nwhich implies that R(n0, n, n, 1) = \u2126 (nn0) , when n0 = O(1).\nWe use following notation:\n\u2022 f(n) = \u0398(g(n)) means that there are two positive constants c1 and c2 such that c1g(n) \u2264 f(n) \u2264 c2g(n) for all n larger than some N .\n\u2022 f(n) = \u2126(g(n)) means that there is a positive constant c1 such that f(n) \u2265 c1g(n) for all n larger than some N .\n\u2022 f(n) = O(g(n)) means that there is a positive constant c2 such that f(n) \u2264 c2g(n) for all n larger than some N .\nIt is known that n0\u2211 k=0 ( m k ) = \u0398 (( 1\u2212 2n0 m )\u22121( m n0 )) , when n0 \u2264 m 2 \u2212\u221am. (10)\nFurthermore, it is known that\n( m\nk\n) = mk\nk!\n( 1 +O( 1m ) ) , when k = O(1). (11)\nWhen n0 is constant, n0 = O(1), we have that( 2n\nn0\n) = 2n0\nn0! nn0\n( 1 +O ( 1 2n )) (12)\nand so, n0\u2211 k=0 ( 2n k ) = \u0398 (( 1\u2212 2n0 2n )\u22121( 2n n0 )) (13)\n= \u0398 (nn0) (14)\nas well as n0\u2211 k=0 ( n k ) + 1 + (n\u2212 1) ( n0\u2211 k=0 ( n k ) \u2212 ( n\u2212 1 n0 )) = \u0398(nn0)."}, {"heading": "5 Discussion and Conclusions", "text": "In this paper we introduced a novel way of understanding the expressiveness of a model provided that it represents a piecewise linear function. We argue that by counting the number of linear regions this model can represent we can look at, for e.g., how well it can approximate an arbitrary curved surface Computational Geometry provides us the tool to make such statements as for example, Theorem 5 introduced by Zaslavsky in 1975.\nNote that in our exploration we only considered linear output units. This is not a restriction, as the output activation itself is not parametrized. Therefore we argue that if there is a target function ftarg that we want to model with a rectifier MLP with \u03c3 as its output activation function, then there exists a function f \u2032targ such that \u03c3(f \u2032 targ) = ftarg. If \u03c3 has an inverse (as, for e.g., with sigmoid), f \u2032targ = \u03c3 \u22121(ftarg). For activations that do not have an inverse like softmax, there are infinitely many functions f \u2032targ that respect the required relation. We just need to pick one, for e.g., with\nsoftmax we can pick log(ftarg). By analysing how well we can model f \u2032targ with a linear output rectifier MLP we get an indirect measure of how well we can model ftarg with an MLP that has \u03c3 as its output activation function.\nAs future work, we believe that by using a better construction we can get a more meaningful comparison between a two layer model and a single layer model. We argue that for the theory of deep learning analysing the case where the number of layers goes to infinity would be a very interesting question that can be explored in a similar manner. Lastly, we think this particular approach can be extended to other kinds of piece-wise linear models such as convolutional networks with rectifiers or maxout networks."}], "references": [{"title": "On the expressive power of deep architectures", "author": ["Y. Bengio", "O. Delalleau"], "venue": "Algorithmic Learning Theory,", "citeRegEx": "Bengio and Delalleau.,? \\Q2011\\E", "shortCiteRegEx": "Bengio and Delalleau.", "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Almost optimal lower bounds for small depth circuits", "author": ["J. H\u00e5stad"], "venue": "In Proceedings of the 18th annual ACM Symposium on Theory of Computing,", "citeRegEx": "H\u00e5stad.,? \\Q1986\\E", "shortCiteRegEx": "H\u00e5stad.", "year": 1986}, {"title": "On the power of small-depth threshold circuits", "author": ["J. H\u00e5stad", "M. Goldmann"], "venue": "Computational Complexity,", "citeRegEx": "H\u00e5stad and Goldmann.,? \\Q1991\\E", "shortCiteRegEx": "H\u00e5stad and Goldmann.", "year": 1991}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinv"], "venue": "Technical report,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "Montreal (Qc), Canada,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": null, "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Sum-product networks: A new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "Poon and Domingos.,? \\Q2011\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2011}, {"title": "An introduction to hyperplane arrangements", "author": ["R. Stanley"], "venue": "In Lect. notes, IAS/Park City Math. Inst.,", "citeRegEx": "Stanley.,? \\Q2004\\E", "shortCiteRegEx": "Stanley.", "year": 2004}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Technical report,", "citeRegEx": "Zeiler and Fergus.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "Such a behaviour is empirically illustrated, for instance, in Zeiler and Fergus (2013); Lee et al.", "startOffset": 62, "endOffset": 87}, {"referenceID": 4, "context": "Such a behaviour is empirically illustrated, for instance, in Zeiler and Fergus (2013); Lee et al. (2009). On the other hand, a shallow model has to construct detectors of target objects based only on the detectors learnt by the first layer.", "startOffset": 88, "endOffset": 106}, {"referenceID": 2, "context": "been with circuit operations (neural net unit computations) that were substantially different from those being used in real state-of-the-art applications of deep learning, such as logic gates (H\u00e5stad, 1986), linear + threshold units with non-negative weights (H\u00e5stad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011).", "startOffset": 192, "endOffset": 206}, {"referenceID": 3, "context": "been with circuit operations (neural net unit computations) that were substantially different from those being used in real state-of-the-art applications of deep learning, such as logic gates (H\u00e5stad, 1986), linear + threshold units with non-negative weights (H\u00e5stad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011).", "startOffset": 259, "endOffset": 286}, {"referenceID": 0, "context": "been with circuit operations (neural net unit computations) that were substantially different from those being used in real state-of-the-art applications of deep learning, such as logic gates (H\u00e5stad, 1986), linear + threshold units with non-negative weights (H\u00e5stad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011).", "startOffset": 302, "endOffset": 330}, {"referenceID": 8, "context": "Bengio and Delalleau (2011) shows that deep sum-product networks (Poon and Domingos, 2011) can use exponentially less nodes to express some families of polynomials compared to the shallow ones.", "startOffset": 65, "endOffset": 90}, {"referenceID": 1, "context": "Rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), and in general piece-wise linear activation function (for example the maxout unit (Goodfellow et al.", "startOffset": 16, "endOffset": 60}, {"referenceID": 7, "context": "Rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), and in general piece-wise linear activation function (for example the maxout unit (Goodfellow et al.", "startOffset": 16, "endOffset": 60}, {"referenceID": 0, "context": "been with circuit operations (neural net unit computations) that were substantially different from those being used in real state-of-the-art applications of deep learning, such as logic gates (H\u00e5stad, 1986), linear + threshold units with non-negative weights (H\u00e5stad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011). Bengio and Delalleau (2011) shows that deep sum-product networks (Poon and Domingos, 2011) can use exponentially less nodes to express some families of polynomials compared to the shallow ones.", "startOffset": 303, "endOffset": 360}, {"referenceID": 0, "context": "been with circuit operations (neural net unit computations) that were substantially different from those being used in real state-of-the-art applications of deep learning, such as logic gates (H\u00e5stad, 1986), linear + threshold units with non-negative weights (H\u00e5stad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011). Bengio and Delalleau (2011) shows that deep sum-product networks (Poon and Domingos, 2011) can use exponentially less nodes to express some families of polynomials compared to the shallow ones. This note analyzes the representational power of deep MLP with rectifier units. Rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), and in general piece-wise linear activation function (for example the maxout unit (Goodfellow et al., 2013)), are becoming a popular choice in designing deep models, and most current state-of-the-art results involve using one of such activations (Goodfellow et al., 2013; Hinton et al., 2012b). Glorot et al. (2011) show that rectifier units have several properties, that makes the optimization problem easier than the more traditional case of using smooth and bounded activations, such as tanh or sigmoid.", "startOffset": 303, "endOffset": 984}], "year": 2017, "abstractText": "This paper explores the complexity of deep feed forward networks with linear presynaptic couplings and rectified linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piece-wise linear functions based on computational geometry. We look at a deep (two hidden layers) rectifier multilayer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime as the number of units goes to infinity, if the shallow model has 2n hidden units and n0 inputs, then the number of linear regions is O(n0). A two layer model with n number of hidden units on each layer has \u03a9(n0). We consider this as a first step towards understanding the complexity of these models and argue that better constructions in this framework might provide more accurate comparisons (especially for the interesting case of when the number of hidden layers goes to infinity).", "creator": "LaTeX with hyperref package"}}}