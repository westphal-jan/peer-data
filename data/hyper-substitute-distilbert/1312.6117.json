{"id": "1312.6117", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Comparison three methods of clustering: k-means, spectral clustering and hierarchical clustering", "abstract": "comparison of three outcomes of the processes and identify relevant function gain loss advantages and calculate them. averages rate of the clustering methods and how authors calculate the range level then be one or the important factor for scaling the overall effectiveness, so working paper showed multiple way each calculate objective average rate compared selected methods. clustering algorithms range be divided in several categories excluding weighted density coefficients, hierarchical ordering via density based algorithms. generally speaking we critically compare different algorithms by scalability, used to work with unknown attribute, clusters formed by conventional, having minimal knowledge process efficient computer to minimize overlapping input parameters, classes for dealing error sorting and extra deposition that same error pattern for preparing a second data, again, there is weaker effect on the input data, increase rates of high overlap, k - means is weak of the simplest terms using calculation ; clustering is an unsupervised problem.", "histories": [["v1", "Thu, 19 Dec 2013 21:45:10 GMT  (896kb)", "http://arxiv.org/abs/1312.6117v1", "all of figure were create by myself and i want to expand this idea"], ["v2", "Thu, 13 Nov 2014 05:52:05 GMT  (0kb,I)", "http://arxiv.org/abs/1312.6117v2", "This paper has been withdrawn by the author due to improve add more results"]], "COMMENTS": "all of figure were create by myself and i want to expand this idea", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kamran kowsari"], "accepted": false, "id": "1312.6117"}, "pdf": {"name": "1312.6117.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kamran Kowsari"], "emails": [], "sections": [{"heading": null, "text": "Comparison of three kind of the clustering and find\ncost function and loss function and calculate them.\nError rate of the clustering methods and how to\ncalculate the error percentage always be one on the\nimportant factor for evaluating the clustering\nmethods, so this paper introduce one way to calculate\nthe error rate of clustering methods. Clustering\nalgorithms can be divided into several categories\nincluding partitioning clustering algorithms,\nhierarchical algorithms and density based algorithms.\nGenerally speaking we should compare clustering\nalgorithms by:\n1. Scalability\n2. Ability to work with different attribute\n3. Clusters formed by conventional\n4. Having minimal knowledge of the computer\nto recognize the input parameters.\n5. Classes for dealing with noise and extra\ndeposition(same error rate for clustering a\nnew data)\n6. Thus, there is no effect on the input data.\n7. different dimensions of high levels\nK-means is one of the simplest approach to clustering\n(clustering is an unsupervised problem). \u201cThe\ndifference between k-means and k-means++ on the\nreal-world datasets was also substantial.\u201d[3]. In k-\nmeans++ achievement is bigger than 10% accuracy\nimprovement over k-means, and it often performed\nmuch better, on the other hand, k-means++ achieved\npotentials 20 to 1000 times smaller than those\nachieved by standard k-means.\nHierarchical clustering: as we know every time we\nhave to know the value of the k for k-means, but for\nHierarchical clustering we don\u2019t need the value of k\nthat is the number of the data cluster. In this project\nwe calculate Hierarchical clustering with the k-center\ncost function and compare this kind of the clustering\nwith K-means. This kind of clustering is really useful\nfor data that K value is not available and result of\noutput is many cluster in one figure like DNA\nsequences, RNA sequences etc. which the number of\nthe cluster is not available for specialist and case by\ncase is different \u201cFirst, there was apparent gene-\ncentered clustering of RNA-exclusive events, and\nsecond, high proportion of the putative RNA editing\nsites seemed to be predominant, often to monozygotic\nlevel, of the variant (vs. wt.) nucleotide harboring\nreads.\u201d[1]\nSpectral clustering: \u201cthis method is a more powerful and specialized algorithm (compared to K-means), derives its name from spectral analysis of a graph, which is how the data are represented. Each object to be clustered can initially be represented as an ndimensional numeric vector, but the difference with this algorithm is that there must also be some method for performing a comparison between each object and expressing this comparison as a scalar. \u201d [2]\nGenerally speaking, Spectral clustering is more powerful than k-means because it can cluster data point which are not in very close to each other\u2019s. If look at figure 1 it compares this two method. In this project we compare three different algorithms\nby details. Many clustering algorithms on data sets\nwith low volume)\nLess than a few percent of big data may well work,\nbut millions have done. Clustering on a sample big\ndata set may therefore be inaccurate. Finally we talk\nabout these three algorithms."}, {"heading": "K-Means", "text": "K-means clustering is one of the easiest method of clustering which data is given D =\n(\ud835\udc4b1 \u2026 \u2026 \u2026 . \ud835\udc4b\ud835\udc5b) in d-dimensional vectors. The aim\nis to identify groups of data points and assign each\npoint to one of the groups. What is good clustering?\nThere is no universal approach, but we will try K-\nmeans approach. But it has lot of problems that we\ncould not calculate the error rate and error percentage\nalthough the loss function is available.\nOne measure of how good a clustering is, would be the sum of distances to the center.\nTherefore, K-means wants to minimize this \u00a3 (quantity), choosing \u00b5\u2019s and a\u2019s to minimize but it is difficult to do analytically because a\u2019s are binary assignments.\nSo K-means algorithm tries to iteratively solve the minimization (sort of greedy algorithm)."}, {"heading": "Minimize \u00a3 with respect to a\u2019s and \u00b5\u2019s by:", "text": "1- Initialize \u00b51 to \u00b5k arbitrarily.\n2- Choose the optimal assignment \u201ca\u201d for given\ncenters \u00b5. [Fix \u00b5 optimize a].\n3- Choose optimal \u00b5 for fixed \u201ca\u201d [Fix a\noptimize \u00b5].\n4- Repeat (iteratively) 2 & 3 until convergence.\nK-Means is not guaranteed to converge to global minimum of this function. In fact finding global\nminimum is NP hard problem, but it is reasonable to some extent.\n1- Length of vectors is M (point & a centroid) so computing the distance between two\npoints is O (M).\n2- Reassign clusters: k = # of clusters N = # of points = O (KN). So computing distance\n= O(MKN)\n3- Computing centroids: Where each point assigned once to a certain centroid = O\n(NM) in that cluster k.\n4- Assuming reassigning of clusters and computing centroids each done at least once\nfor I iterations = O(IKNM).\nSo the time complexity of K-means is equal to \ud835\udc42(\ud835\udc5b2) if K value which number of cluster and I value which number of the iteration will be available; although, if number of the iteration value is not available, time complexity can be exponential time or \ud835\udc42(2\ud835\udc5b) .\n\u00a3 = \u2211 \ud835\udc8c\ud835\udc8b=\ud835\udfcf \u2211 \ud835\udc99\ud835\udc8a \ud835\udc82\ud835\udc94\ud835\udc94\ud835\udc8a\ud835\udc88\ud835\udc8f\ud835\udc86\ud835\udc85 \ud835\udc95\ud835\udc90 \ud835\udc8b | |\ud835\udc99\ud835\udc8a \u2212 \ud835\udf41\ud835\udc8b || 2 = \u2211 \ud835\udc8c\ud835\udc8b=\ud835\udfcf \u2211 \ud835\udc82\ud835\udc8a\ud835\udc8b | |\ud835\udc99\ud835\udc8a \u2212 \ud835\udf41\ud835\udc8b || 2\n\ud835\udc8f\n\ud835\udc8a=\ud835\udfcf\nWhere:\n\u00a3 = Quantity\n\u2211 \ud835\udc58\ud835\udc57=1 = sum of all clusters\n\u2211 \ud835\udc65\ud835\udc56 \ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b\ud835\udc52\ud835\udc51 \ud835\udc61\ud835\udc5c \ud835\udc57 | |\ud835\udc65\ud835\udc56 \u2212 \ud835\udf07\ud835\udc57 || 2 = sum of all distances in cluster into 2\nWhere: \ud835\udc4e\ud835\udc56\ud835\udc57= {1 if xi assigned to j }\nZero otherwise\nFigure 1: simplest approaches to K-means Clustering after many iteration"}, {"heading": "Spectral Clustering", "text": "Although K-means is useful and several project was done with this method, K-means in reality has a plenty of problems such as this algorithm can cluster very well for all data sets, so a new kind of clustering method was needed to find better cluster with lower error rate \u201cIn this paper we talk about how to find the error rate with K value\u201d. Spectral clustering solve this problem by using affinity matrix which is very big matrix \ud835\udc41 \u00d7 \ud835\udc41 and this matrix find the best relation between each two different data points. Time complexity of Spectral Clustering is\ud835\udc42(\ud835\udc5b3), but in Kmeans time complexity is not fixable which can be \ud835\udc42(\ud835\udc5b2) to \ud835\udc42(2\ud835\udc5b) that is one of the most important problems of K-means; however, sometimes K-means for easier project K-means implementation is easier and also more sensitive. If you look at Figure 1: you\ncan see it has a lot of error rate in the evaluation part we considered on the result of our data sets. \u201cWhen the distortion measure is squared error, the most commonly used algorithm for vector quantization\u2019s k-means, which has both theoretical support and the virtue of simplicity. The k-means algorithm employs an iterative procedure. At each iteration, the algorithm assign each data point to the nearest centroid, and recalculates the cluster centroids. The procedure stops when the total sum of squared error stabilizes.\u201d[12].\n[16]"}, {"heading": "Hierarchical Clustering", "text": "The other problem of K-means and Spectral\nclustering algorithms are that they cannot find how many cluster we have and another things is that this method cannot cluster with different K value. It is mean the result cannot be clustered by two different value of K; for example, in many real project K value is unavailable, so if we need to cluster by more than two K value the result come into two or more figure , so we do not have the result on one specific figure. Time complexity of Hierarchical Clustering for two method is that for agglomerative clustering is \ud835\udc42(\ud835\udc5b3) and for Divisive clustering with an exhaustive search is \ud835\udc42(2\ud835\udc5b) [17]\nWith same data of Figure 2\nThe left Image is original data which is\ncome from The George Washington\nUniversity, school of medicine and\nhealth sciences, MGPC project\n(McCormick Genomic and Proteomic\nCenter) and the right image is after\ncluster the data \u201calso optimized data\npoints and samples\u201d"}, {"heading": "Evaluation", "text": "One of the sophisticated factor for clustering algorithms and evaluated is the average of error rate for each clustering method. But one of the important factor for calculate the error rate is that the result of the labeling of each clustering method maybe will be completely different we our test labels; for example, K value is equal to 2 and clustering method find two different labels, so the error rate of this algorithm can\nbe more than %50. In the result of one data set, the result of data set was 99.98 percent for the K is equal to 2. For clustering into two clusters, the error rate cannot be more than 50 percent, so the real data set of this data set is equal to 0.02 percent because all of the clustering algorithm guess the labels, and the cannot find exact labels, so the real error rate is equal to minimum of the error rate by different labels of K.\nFor K = 2:\nError Rate = min {|L \u2013 TestLabels |, 100 - |L \u2013 TestLabels |}\nFor K > 2\nLoop 1 to 2\ud835\udc58\nError Rate = min {|L \u2013 TestLabels |}\nThis method is only available for k = 2, so we need a general algorithm for find error rate by this two labels for all value of K which can calculate the error rate ;for example if we have 3 different clusters we could not find clusters by one comparison.\nThis method has exponential time complexity, but by the value of K not by N and N>>K which K is very smaller than N data points; for example, for our first data set we do clustering on 14977 data points, but K= 2 and we need to\nIn our Data set 1 with 14977 *2 dimensions by k=2 SP clustering has 17 error so the error rate is %0. 11 but with K-means has 2902, so error rate is %19.37.\nData Set of 2000 Data points (Genes) and\nwith 15 samples (dimensions)\nTwo moon Data Sets 14977 data points with two\ndimensions"}], "references": [{"title": "kmeans++: the advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "Proceedings of the eighteenth annual ACMSIAM symposium on Discrete algorithms,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Spectral clustering with perturbed data", "author": ["L. Huang", "D. Yan", "M.I. Jordan", "N. Taft"], "venue": "In Advances in Neural Information Processing Systems (NIPS), December", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Minkowski metric, feature weighting and anomalous cluster initializing in K-Means clustering", "author": ["R.C. Amorim", "B Mirkin"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Learning spectral clustering", "author": ["F. Bach", "M. Jordan"], "venue": "In Proc. of NIPS-16. MIT Press,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Clustering with Bregman divergence", "author": ["Banerjee", "S. Merugu", "I. Dhillon", "J. Ghosh"], "venue": "Proceeding of SIAM Data Mining conference,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Clustering Large Datasets in Arbitrary Metric Spaces,", "author": ["V. Ganti"], "venue": "Proc. 15th Int\u2019l Conf. Data Eng., IEEE CS Press, Los Alamitos, Calif.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Efficient and Effective Clustering Method for Spatial Data Mining,", "author": ["R. Ng", "J. Han"], "venue": "Proc. 20th Int\u2019l Conf. Very Large Data Bases,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "Fast approximate spectral clustering. InProceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '09)", "author": ["Donghui Yan", "Ling Huang", "Michael I. Jordan"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Spectral grouping using the Nystr \u0308om method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Quantization", "author": ["R.M. Gray", "D.L. Neuhoff"], "venue": "IEEE Transactions of Information Theory, 44(6):2325\u20132383,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "and A", "author": ["S. Gunter", "N.N. Schraudolph"], "venue": "V. N. Vishwanathan.Fast iterative kernel principal component analysis. Journal of Machine Learning Research, 8:1893\u20131918,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "SLINK: an optimally efficient algorithm for the single-link cluster method", "author": ["R. Sibson"], "venue": "The Computer Journal (British Computer Society)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1973}], "referenceMentions": [{"referenceID": 7, "context": "\u201d[12].", "startOffset": 1, "endOffset": 5}, {"referenceID": 11, "context": "Time complexity of Hierarchical Clustering for two method is that for agglomerative clustering is O(n) and for Divisive clustering with an exhaustive search is O(2) [17] Figure 4 : Simplest approaches to Spectral Clustering", "startOffset": 165, "endOffset": 169}], "year": 2013, "abstractText": "Comparison of three kind of the clustering and find cost function and loss function and calculate them. Error rate of the clustering methods and how to calculate the error percentage always be one on the important factor for evaluating the clustering methods, so this paper introduce one way to calculate the error rate of clustering methods. Clustering algorithms can be divided into several categories including partitioning clustering algorithms, hierarchical algorithms and density based algorithms. Generally speaking we should compare clustering algorithms by: 1. Scalability 2. Ability to work with different attribute 3. Clusters formed by conventional 4. Having minimal knowledge of the computer to recognize the input parameters. 5. Classes for dealing with noise and extra deposition(same error rate for clustering a new data) 6. Thus, there is no effect on the input data. 7. different dimensions of high levels K-means is one of the simplest approach to clustering (clustering is an unsupervised problem). \u201cThe difference between k-means and k-means++ on the real-world datasets was also substantial.\u201d[3]. In kmeans++ achievement is bigger than 10% accuracy improvement over k-means, and it often performed much better, on the other hand, k-means++ achieved potentials 20 to 1000 times smaller than those achieved by standard k-means. Hierarchical clustering: as we know every time we have to know the value of the k for k-means, but for Hierarchical clustering we don\u2019t need the value of k that is the number of the data cluster. In this project we calculate Hierarchical clustering with the k-center cost function and compare this kind of the clustering with K-means. This kind of clustering is really useful for data that K value is not available and result of output is many cluster in one figure like DNA sequences, RNA sequences etc. which the number of the cluster is not available for specialist and case by case is different \u201cFirst, there was apparent genecentered clustering of RNA-exclusive events, and second, high proportion of the putative RNA editing sites seemed to be predominant, often to monozygotic level, of the variant (vs. wt.) nucleotide harboring reads.\u201d[1] Spectral clustering: \u201cthis method is a more powerful and specialized algorithm (compared to K-means), derives its name from spectral analysis of a graph, which is how the data are represented. Each object to be clustered can initially be represented as an ndimensional numeric vector, but the difference with this algorithm is that there must also be some method for performing a comparison between each object and expressing this comparison as a scalar. \u201d [2] Generally speaking, Spectral clustering is more powerful than k-means because it can cluster data point which are not in very close to each other\u2019s. If look at figure 1 it compares this two method. In this project we compare three different algorithms by details. Many clustering algorithms on data sets with low volume) Less than a few percent of big data may well work, but millions have done. Clustering on a sample big data set may therefore be inaccurate. Finally we talk about these three algorithms.", "creator": "Microsoft\u00ae Word 2013"}}}