{"id": "1312.6712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2013", "title": "Invariant Factorization Of Time-Series", "abstract": "time - series classification becoming an important domain of machine learning and a plethora of methods have been developed besides the task. directly comparison both scientific fields, this study approaches fundamentally novel method which decomposes a time - band dataset into latent patterns ; membership weights of local orders to overlapping patterns. the definition proceeds formalized as structured segment representation formulation whose a tailored stochastic coordinate stability optimization is needed. her h - series definition summarized alongside a new feature representation consisting thus the sums comprising generalized membership columns, which have frequencies different local patterns. analysts describe various distinct window sizes uniformly encoded in order precisely encapsulate the projections of edges from different sizes. finally, a large - scale experimental deployment against 6 state of the art baselines and 43 binary tree datasets commercially conducted. the proposed convergence comes by 72 narrow - relative modest margins in matter of graphical accuracy.", "histories": [["v1", "Mon, 23 Dec 2013 22:15:59 GMT  (46kb,D)", "http://arxiv.org/abs/1312.6712v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["josif grabocka", "lars schmidt-thieme"], "accepted": false, "id": "1312.6712"}, "pdf": {"name": "1312.6712.pdf", "metadata": {"source": "CRF", "title": "Invariant Factorization of Time Series", "authors": ["Josif Grabocka", "Lars Schmidt-Thieme"], "emails": ["}@ismll.uni-hildesheim.de"], "sections": [{"heading": "1 Introduction", "text": "Time-series classification is a pillar problem of machine learning and its existence spans over decades of research. Series data emerge in a myriad of application domains, from health-care and astronomy up to economics and botanics. In comparison to other types of data, time series exhibit a high degree of intra-class variability, where patterns occur shifted in time, distorted and scaled. Therefore traditionally strong classifiers, such as Support Vector Machines (SVM), fail to excel in terms of prediction accuracy [14].\nA series of attempts have been proposed to address the intra-class variations of time-series patterns. An early pioneer method called Dynamic Time Warping (DTW), (still considered competitive [11, 27]), computes the similarity among series by re-aligning the time indexes. The algorithm explores all the possible relative alignments of time indexes of two series and picks the one yielding the minimum overall distance [18].\nThe research of time-series classification can be approximately categorized into distance metrics, invariant classifiers, feature extraction and bag-of-patterns streams. Distance metrics focus on defining measurements on the similarity of two series instances [18, 7, 8, 4]. Invariant classifiers, on the other hand, aim at embedding similarities into classifiers. For instance, the invariant kernel functions have\nbeen applied to measure instance similarities in the projected space of a non-linear SVM [32, 14]. Another paper proposes to generate all pattern variations as new instances and inflate the training set [13]. The bag-of-patterns approach splits the time-series into local segments and collects statistics over the segments. Those local segments are converted into symbolic words and a histogram of the words\u2019 occurrences is built [22, 23]. Another study constructs a supervised codebook generated from local patterns, which is used to create features for a random forest classifiers [5].\nIn comparison to existing approaches this study proposes a new perspective. We assume that time series are generated by a set of latent (hidden) patterns which occur at different time stamps and different frequencies across instances. In addition those patterns might be convoluted and/or distorted to produce derived local patterns.\nWe would like to introduce the concept through the illustration of Figure 1. A synthetic dataset consists of two classes A (green) and B (black), each having two instances. All the time series are composed of three segments of 20 points, while each segment is a convolutional derivative of two latent patterns depicted in red and blue. In other words, each segment is a weighted sum of a single-peaked and double-peaked pattern. The shown coefficients of the convolution are degrees of membership that each local segment has to one of those two latent patterns.\nBoth Euclidean and DTW based nearest neighbor classifiers have 100% error on a leave-one-out experiment on the dataset of Figure 1. As can be observed, instance A1 is closer to B1 than A2, and the same applies for all other series. In fact the rationale behind this dataset is that A has a higher frequency of the red single-peaked pattern, while B has a higher domination of the blue double-peaked pattern. The method presented in this paper detects the latent patterns, measures the degrees of membership and sums them up into a bag-of-pattern approach. Our approach converts the series of Figure 1 into a new representation F, concretely: FA1 = [1.9, 1.1], FA2 = [1.7, 1.3], FB1 = [1.3, 1.7], FB2 = [1.1, 1.9]. A nearest neighbor classifier over the new representation F yields 0% error.\nIn this paper, we will propose a method which detects a set of latent patterns for a time series dataset together with\nar X\niv :1\n31 2.\n67 12\nv1 [\ncs .L\nG ]\n2 3\nD ec\n2 01\n3\na convolutional degree of membership weights. The product of the membership weights with the patterns approximates the original segments. In contrast to the aforementioned synthetic example, real datasets have segments occurring at arbitrary locations and being of different sizes. Our method employs a sliding window approach to split the series into overlapping local segments and utilizes a factorization model to decompose the segments into latent patterns and weights. We formalize the objective function of the factorization and propose a stochastic coordinate descent technique in order to optimize the objective. The sum of the learned membership degrees is used to project the time series into a new representation. Ultimately, in order to resolve the scale invariance of the patterns, sums of memberships from different sliding window sizes are concatenated.\nA throughout experimental comparison is conducted on 43 datasets of the UCR time-series collection against six state of the art baselines. Our method outperforms all the baselines with a statistically significant margin of improvement. Considering the evidence of our experimental results and our survey of related work, we conclude that our prediction accuracy figures are the best published in the realm of time-series classification, regarding the UCR collection of datasets."}, {"heading": "2 Related Work", "text": "Time-series classification has been elaborated in a vast number of occasions, therefore a complete survey of all the published papers is out of our scope. Instead, we will structure the related work into a set of categories and mention relevant prominent studies.\n2.1 Distance Metrics and Invariant Classifiers: A significant portion of time-series research has centralized around the definition of accurate similarity metrics. The most popular of those approaches is the Dynamic Time\nWarping (DTW) measure [18], which overcomes deficiencies of the L2 norm distance by aligning the time indexes of two series instances. The similarity measure is typically plugged into a nearest neighbor classifier. DTW produces competitive prediction accuracies [11, 31] and has been speed up using lower boundary heuristics [27].\nOther similarity based distance metrics have extended the edit distance of strings into the time-series domain [7, 8]. Furthermore, the longest common subsequence of time series has also been used as an indication of similarity [28]. Moreover, similarities of sequential data have been measured using sparse spatial sample kernels [19]. A state of the art method called complexity-invariant distance metric (CID) introduces the total variation regularization for time-series. CID significantly improves the accuracy of DTW [3, 4].\nEfforts have been dedicated on incorporating time-series variations into popular classifiers. For instance DTW has been used as a SVM kernel [14], even though the resulting kernel is not positive semi definite. Consecutively, another study has proposed a Gaussian elastic kernel [32]. A method which produces a semi-definite kernel is called global alignment kernels and builds an average statistics from all possible warping paths of time indexes [9]. In addition, another study has inflated the training set by adding new instances that represent variations of original training data [13].\n2.2 Feature Extraction and Bag-of-patterns: Other researchers have emphasized the extraction of series features for boosting classification. Dimensionality reduction has been used to project the time series into a low-rank data space [17], while a recent method incorporates class segregation into the projection [12].\nHowever, the most prominent state of the art technique for extracting time-series features is called shapelets mining. Shapelets represent the most discriminative series segment (or set of segments), which yields the maximal prediction\naccuracy [26, 24]. A related study detects a set of shapelets and transforms the series data into a new representation, defined by the distance to those shapelets [15].\nA recent direction of research has drawn attention on the need to segment the time series into local patterns and measure the frequencies of patterns as classification features. For instance frequencies of time-series motifs have been fed into standard classifiers [6]. Another attempt has focused on building histograms of local patterns represented as symbolic words [23]. Those symbolic words are produced by a piecewise constant approximation technique called SAX [21], while the frequencies of the SAX words are used ultimately for classification [22, 23]. One similar bag-of-words approach has also been applied to long biomedical data [30]. Moreover, a bag-of-patterns study proposes to extract series segments of various lengths and positions and generate a supervised codebook of those patterns [5]. A random forest classifier has been trained over the extracted features. That study demonstrates considerable improvements over baselines in terms of prediction accuracy [5].\n2.3 Factorization of Time Series There have been a few attempts in generating invariant time-series features through factorization. A shift-invariant sparse coding of signals has been proposed for reconstructing noisy or missing series segments [20]. In similar domains, sparse coding factorization has been applied for deriving shift and 2D rotation invariant features of hand writing data [2], and also invariant features of audio data [16]. Moreover, a temporal decomposition of multivariate streams has been used to discover patterns in patients\u2019 clinical events [29].\nOur method differs from distance metrics principally. Instead of measuring the similarity of series, we project the data into a new representation, where similar instances are positioned close to each other. Furthermore, the proposed method distances away from existing bag-of-patterns methods because we learn a latent decomposition of patterns, instead of counting the occurrence of segments on the original time-series. Finally, our contributions over the existing factorization methods rely on (i) a novel approach in detecting both shift and scale invariant features for time series, and (ii) building a bag-of-patterns representation of the learned invariant features for a classification scenario."}, {"heading": "3 Definitions and Notations", "text": "1. Time-series: A time-series is an ordered sequence of\npoint values. In a dataset of N series instances, where each series hasQ points, we denote the series dataset as T \u2208 RN\u00d7Q. 2. Sliding Window Segment: A sliding window content of size L \u2208 N, is a series subsequence starting at a position j \u2208 {1, . . . , Q \u2212 L} of a series i of dataset T, and is denoted as Si,j \u2208 RL, Si,j := (Ti,j , Ti,j+1, . . . , Ti,j+L\u22121).\n3. All Dataset Segments: The starting position of each sliding window segment is incremented by an offset \u03b4 = {1, . . . , L}, therefore the maximum number of segments per series is defined as M := Q\u2212L\u03b4 . All the segments of a time-series datasets are denoted as S \u2208 RN\u00d7M\u00d7L. 4. Latent Patterns: Our method mines forK-many latent patterns, each having the same size as one segment, i.e L. So, the latent patterns are denoted as P \u2208 RK\u00d7L. 5. Degrees of Membership: Each instance of a dataset will be approximated via the product of latent patterns and the set of membership degrees to those patterns. Each segment of a series will have one membership weight to each of the K latent patterns. Consequently, the degrees of membership of all time-series are defined as D \u2208 RN\u00d7M\u00d7K ."}, {"heading": "4 Invariant Factorization of Time Series", "text": "4.1 Segmenting the Time-Series The series of the dataset are segmented in a sliding window approach having size L and increment \u03b4. The segmentation of each series is described in Algorithm 1. Once derived, the segments are normalized to mean 0 and deviation 1.\nAlgorithm 1 SegmentSeries\nRequire: T \u2208 RN\u00d7Q, L \u2208 N, \u03b4 \u2208 N Ensure: S \u2208 RN\u00d7M\u00d7L\n1: for i = 1, . . . , N, j = 1, . . . ,M do 2: for l = 1, . . . , L do 3: Si,j,l \u2190 Ti, \u03b4(j\u22121)+l\u22121 4: end for 5: Si,j \u2190 normalize(Si,j) 6: end for 7: return S\n4.2 Invariant Factorization Objective The objective of the invariant factorization relies on approximating every segment of a series segment as a product of the defined latent patterns P and the membership degrees D. The objective function is described in Equation 4.1.\nargmin D,P N\u2211 i=1 M\u2211 j=1 L\u2211 l=1\n( Si,j,l \u2212\nK\u2211 k=1 Di,j,kPk,l\n)2 (4.1)\n+ \u03bbP K\u2211 k=1 L\u2211 l=1 P 2k,l\nSubject To: K\u2211 k=1 Di,j,k = 1, Di,j,k \u2265 0, \u2200i, j, k\nThe objective function is composed of two loss terms and one constraint. Firstly, the latent patterns P and the\nmemberships D should approximate the normalized segments of the series dataset. Therefore, minimizing the L2 norm of the reconstruction error achieves the goal. In addition, a second regularization loss term is added in order to prohibit the patterns P from over-fitting. A hyper-parameter \u03bbP controls the degree of regularization. Finally, we impose equality and positivity constraints on the membership degrees. The membership degrees of every segment Di,j sum-up to one, because each segment needs to have the same impact factor. Otherwise, in a bag-of-patterns representation of series, different segments would have different scales of memberships. The positivity constraint, on the other hand, prohibits non-interpretable negative memberships.\nWe would like to illustrate the invariant factorization objective with a concrete illustration, shown in Figure 2. A learned decomposition, as in Equation 4.1, is depicted for the Gun Point dataset. On the left top, a series instance is presented, while the dataset\u2019s latent patterns and the membership degrees of the instance are found below. The product of the patterns and memberships yield the series approximation shown in the right top chart. The series is split into 8 overlapping segments of size 45, each starting at an offset of 13 points. For instance, the 7-th segment starts at 79 and has a high membership value to the 6-th pattern, which matches the descending structure. However, please note that other patterns also contribute with smaller membership degrees (patterns 4 and 5) in order to fit exactly the original segment content.\n4.3 Learning the Patterns and Memberships In order to learn the latent patterns and the memberships we are going to optimize the objective function of Equation 4.1 via stochastic coordinate descent, which operates by updating each cell of D,P in the direction of the first derivative of the objective.\n4.3.1 Update Rules for Latent Patterns In order to compute the update rules for the patterns, we first define the error in approximating a point l of the segment j, in time-series i, as \u03bei,j,l.\nLet \u03bei,j,l := Si,j,l \u2212 K\u2211 k=1 Di,j,kPk,l\nP \u2217k,l := argmin z \u03bbP z 2 + \u2211 i,j (\u03bei,j,l +Di,j,kPk,l \u2212Di,j,kz)2\n2\u03bbPP \u2217 k,l \u2212 2 \u2211 i,j ( \u03bei,j,l +Di,j,k(Pk,l \u2212 P \u2217k,l) ) Di,j,k = 0\nSubsequently the optimal value of every point l of a latent pattern k is denoted as P \u2217k,l and is found by solving the first derivative in a coordinate descent way. Therefore the optimal value of Pk,l is defined in Equation 4.2. Please note that the error values don\u2019t have to be recomputed for each point over all latent patterns, instead we can incrementally update the error terms. Equation 4.3 refreshes the error terms after the change of the pattern value.\nP \u2217k,l :=\n\u2211 i,j (\u03bei,j,l +Di,j,kPk,l)Di,j,k\n\u03bbP + \u2211 i,j D 2 i,j,k (4.2) \u03bei,j,l \u2190 \u03bei,j,l \u2212 (P \u2217k,l \u2212 Pk,l)Di,j,k(4.3)\n4.3.2 Update Rules for Membership Degrees The update rules for the membership degrees needs to preserve an equality constraint, which enforce the memberships of a segment to sum to one. Therefore any direct update of a membership Di,j,k will violate the constraint. In order to avoid this bottleneck, we propose to update the memberships in pairs, inspired by a similar strategy known as the Sequential Minimal Optimization algorithm [25]. The idea is to draw two random membership weights Di,j,k, Di,j,w and update them such that their sum, denoted Q = Di,j,k + Di,j,w, remains equal before and after the updates. In that way, if we increase one membership, then the other would have to decrease and vice versa, while the aim is to find the combination which yield the smallest approximation error. Therefore, the optimal value of Di,j,k, will be denoted by D\u2217i,j,k and is algebraically derived as the solution of the first derivative of our objective function.\nD\u2217i,j,k = argmin z \u2211 l (\u03bei,j,l +Di,j,kPk,l +Di,j,wPw,l\u2212\nQPw,l + z(Pw,l \u2212 Pk,l))2 D\u2217i,j,k = \u2212 \u2211 l (\u03bei,j,l \u2212Di,j,k (Pw,l \u2212 Pk,l)) (Pw,l \u2212 Pk,l)\u2211\nl (Pw,l \u2212 Pk,l) 2\nOnce the optimal valueD\u2217i,j,k is defined then we have to ensure the constraints. Equation 4.4 crops the optimal value to be nonnegative and not exceed the sum of the membership pairs. The error term is refreshed to include the changes of both memberships of the pair in Equation 4.5. As a last step we can commit the optimal values, by preserving their sum before the updates. As Equation 4.6 shows, the best value of Di,j,w can be deduced from the optimal value of Di,j,k.\nD\u2217i,j,k \u2190 max(0,min(Q,D\u2217i,j,k))(4.4) \u03bei,j,l \u2190 \u03bei,j,l \u2212 (D\u2217i,j,k \u2212Di,j,k)Pk,l, and(4.5) \u03bei,j,l \u2190 \u03bei,j,l \u2212 (Q\u2212D\u2217i,j,k \u2212Di,j,w)Pw,l Di,j,k \u2190 D\u2217i,j,k, Di,j,w \u2190 Q\u2212D\u2217i,j,k(4.6)\n4.4 Efficient Initialization Since the objective function of Equation 4.1 is nonlinear in terms of P and D together, then a coordinate descent optimization is not guaranteed to avoid local optima. Therefore, good initial values of the patterns and the memberships are crucial for the learning process. The intuition leads into assigning some of the segments as initial patterns, however it is not obvious which of them provide the best initialization.\nThe answer is addressed via a technique utilized to\nfind the initial centroids in a clustering setup [1]. The patterns (analogy to centroids) are initialized to segments with a probability proportional to the distance to all the other segments [1]. Therefore, we are assured to pick centroid segments which are evenly distributed across the space of all series segments. The initialization steps are detailed in Algorithm 2. Please note that the first pattern has to be drawn randomly in a uniform distribution, while the other patterns are chosen randomly from the dataset segments based on the probability of their distance to the existing patterns. The function C measures the distance of a segment to the closest existing pattern.\nAlgorithm 2 Initialize\nRequire: S \u2208 RN\u00d7M\u00d7L, L \u2208 N,K \u2208 N Ensure: D \u2208 RN\u00d7M\u00d7K , P \u2208 RK\u00d7L\n1: P1 \u2190 Si\u2032,j\u2032 , drawn i\u2032, j\u2032 \u223c U(N,M) 2: for k = 2, . . . ,K do 3: Pk \u2190 Si\u2032,j\u2032 , with probability weights C(Si\u2032,j\u2032)\n2\u2211 i,j C(Si,j) 2\n4: end for 5: for i = 1, . . . , N ; j = 1, . . . ,M do 6: k\u2032 = argmink\u2208{1,...,K} ||Si,j \u2212 Pk||2\n7: Di,j,k \u2190\n{ 1 k = k\u2032\n0 k 6= k\u2032 , k = 1, . . . ,K\n8: end for 9: return D,P\nThe initialization of the membership degrees is more trivial than patterns. The degree index k\u2032 denotes that pattern Pk\u2032 is the closest to segment Si,j and its membership Di,j,k\u2032 is set to 1, while all the other membership degrees are initialized to zero.\n4.5 Learning Algorithm Algorithm 3 finally combines all the steps of the factorization process. In the beginning, the memberships and the patterns are initialized using Algorithm 2. Next the errors are initialized, then the coordinate descent technique updates all the parameters in a number of iterations, denoted as a hyper-parameter I. Subsequently, the degrees of membership and the patterns are learned by setting the aforementioned optimal values. The membership and pattern indexes are visited in random order to speed up the convergence.\n4.6 A New Invariant Representation The final representation will sum the membership degrees in a bag-of-patterns strategy. It enables a quantification of which local patterns appear in a series and how often. The shift invariance is achieved by segmenting the series in a sliding window approach and the scale invariance is addressed using different sliding window sizes. Algorithm 4 describes the algorithmic steps. The algorithm iterates over \u03a6 many different scales of an initial sliding windows size L and solves an invariant fac-\nAlgorithm 3 InvariantFactorization\nRequire: T \u2208 RN\u00d7Q, L \u2208 N, \u03b4 \u2208 N,K \u2208 N, \u03bbP \u2208 R, I \u2208 N Ensure: D \u2208 RN\u00d7M\u00d7K , P \u2208 RK\u00d7L 1: S \u2190 SegmentSeries(T, L, \u03c3) 2: (D,P )\u2190 Initialize(S,L,K) 3: {Initialize the errors} 4: for \u2200i \u2208 NN1 ,\u2200j \u2208 NM1 ,\u2200l \u2208 NL1 do 5: \u03bei,j,l := Si,j,l \u2212 \u2211K k=1Di,j,kPk,l\n6: end for 7: {Update the patterns&memberships iteratively} 8: for iteration = 1, . . . , I do 9: {Update all degrees of membership}\n10: for \u2200i \u2208 NN1 ,\u2200j \u2208 NM1 randomly do 11: for 1, . . . ,K, {Draw K-many pairs} do 12: k,w \u223c U(K,K), s.t. Di,j,k +Di,j,w 6= 0 13: Q\u2190 Di,j,k +Di,j,w 14: {Solve and crop the optimal memberships} 15: D\u2217i,j,k = \u2212 \u2211 l(\u03bei,j,l\u2212Di,j,k(Pw,l\u2212Pk,l))(Pw,l\u2212Pk,l)\u2211 l(Pw,l\u2212Pk,l) 2\n16: D\u2217i,j,k \u2190 max ( 0,min(Q,D\u2217i,j,k) ) 17: {Update the error terms} 18: for l = 1, . . . , L do 19: \u03bei,j,l \u2190 \u03bei,j,l \u2212 (D\u2217i,j,k \u2212Di,j,k)Pk,l 20: \u03bei,j,l \u2190 \u03bei,j,l \u2212 (Q\u2212D\u2217i,j,k \u2212Di,j,w)Pw,l 21: end for 22: {Commit the values of the pair} 23: Di,j,k \u2190 D\u2217i,j,k 24: Di,j,w \u2190 Q\u2212D\u2217i,j,k 25: end for 26: end for 27: {Update all patterns} 28: for \u2200k \u2208 NK1 ; \u2200l \u2208 NL1 , randomly do 29: P \u2217k,l = \u2211 i,j(\u03bei,j,l+Di,j,kPk,l)Di,j,k \u03bbP + \u2211 i,j D 2 i,j,k 30: {Update the error terms} 31: for i = 1, . . . , N ; j = 1, . . . ,M do 32: \u03bei,j,l \u2190 \u03bei,j,l \u2212 (P \u2217k,l \u2212 Pk,l)Di,j,k 33: end for 34: {Commit the pattern\u2019s point value} 35: Pk,l \u2190 P \u2217k,l 36: end for 37: end for 38: return D,P\ntorization from Algorithm 3 per each size. The frequencies of the learned memberships are summed up for all K patterns and the procedure is repeated for every sliding window size. Finally each time series contains K\u03a6 many features, which denote the frequencies of patterns at different sizes and positions.\nThe new representation will be used for classification,\ninstead of the original time series. We deployed a polynomial kernel Support Vector Machines, because we need to capture the interaction among features, i.e. the interaction among patterns of various sizes.\nAlgorithm 4 InvariantRepresentation\nRequire: T \u2208 RN\u00d7Q, L \u2208 N, \u03b4 \u2208 N,K \u2208 N, \u03bbP \u2208 R, I \u2208 N,\u03a6 \u2208 N Ensure: F \u2208 RN\u00d7(K\u03a6) 1: for s = 1, . . . ,\u03a6 do 2: L\u2032 \u2190 L \u00b7 s 3: D \u2190 InvariantFactorization(T, L\u2032, \u03b4,K, \u03bbP , I) 4: for i = 1, . . . , N ; k = 1, . . . ,K do 5: M \u2190 Q\u2212L \u2032\n\u03b4 6: Fi,k+(s\u22121)K \u2190 \u2211M j=1Di,j,k 7: end for 8: end for 9: return F\n4.7 Algorithmic Complexity The run-time complexity of the method is dominated by the updates of memberships and has an order O(NMKLI). Concretely our method needs 48.4 hours to compute on the StarLightCurves (the largest) dataset, while for instance DTW needs 87 hours. The space complexity of our method depends on the storage of the segments S and the memberships D, which is O(NM max(K,L))."}, {"heading": "5 Experimental Results", "text": "5.1 Baselines We compared the prediction accuracy of our method, denoted Invariant Factorization (INFA), against the following six state of the art baselines: \u2022 TSBF: The bag-of-features framework for time series\n(TSBF) uses a supervised codebook to extract features for a random forest classifier [5]. \u2022 SSSK: Sparse Spatial Similarity Kernel (SSSK) measures sequence similarity through sampling sequence features at different resolutions [19]. \u2022 BOW: The Bag of Words (BOW) method decomposes the series into local SAX words and uses a histogram representation of words as the new feature representation [22, 23]. \u2022 DTW: Dynamic Time Warping (DTW) computes the best alignment of time indexes resulting in the mininal distance [18, 27]. \u2022 CID: The complexity invariant distance (CID) adds a L2-based total variation regularization term into the DTW distance [4]. \u2022 FSH: Fast shapelet (FSH) extracts the most discriminative segment of the series dataset, such that the distance from the dataset instances to the optimal shapelet can be used as a feature for classification [26].\n5.2 Setup and Reproducibility We conducted a largescale experimentation in 43 time-series dataset from the UCR collection1. Our protocol complied to the default train/test split of the data, which is an established benchmark split and is used by the baselines. The metric of comparison is the error rate, i.e. the misclassification rate. Table 1 shows the datasets used for experimentation together with the number of classes, the number of training instances, the number of testing instances and the length of the series.\nOur method has a relatively high number of hyperparameters, however most of them can be analytically adjusted. Since all the segments are normalized, then the latent patterns should also have mean 0 and standard deviation 1. Therefore, \u03bbP = 1 by the definition of the Tikhonov regularization. We searched for four different sliding windows sizes, i.e. \u03a6 = 4 and L = 20% of Q, so L\u2032 \u2208 {20%, 40%, 60%, 80%} of Q. The number of latent patterns needs to be set large enough to avoid underfitting and was set to K = 50% of Q. A fine grained sliding window offset was applied as \u03b4 = 5% of L. However, in order to ensure the scalability for the four largest datasets (Cin ECG, InlineSkate, MALLAT, StarLightCurves) we set their parameters to K = 10% of Q and \u03b4 = 20% of L. The maximum number of iterations was set to I = 15. The applied classifier was a polynomial kernel SVM with a polynomial degree being 3 and the complexity parameter 1, which are competitive SVM settings for the UCR collection [13]. Since the algorithm is based on a probabilistic initialization, it might be possible that it converges to different closeby optima in each execution. However, in our experiments, those optima were very close and the final prediction accuracy results have insignificant differences. The authors are devoted to promote full reproducibility, therefore the source code, the data and instructions are publicly available2.\n5.3 Results The error rate results of the six state of the art baselines and our method INFA are presented in Table 1. The best performing method for each dataset (row) is emphasized in bold. In order to compare multiple classifiers across a large number of datasets we follow the established benchmarks of counting wins and Wilcoxon\u2019s Signed-Rank test for statistical significance [10]. To be fair with the baselines, we retrieved the results from the baselines\u2019 publications [5, 4, 26] over the same data splits as INFA. In addition, we verified the published results of the baselines with our own experimental checkups.\nThree comparative figures are conducted, the first of which counts the absolute number of wins. Each dataset awards a total value of 1, which is split into equal fractions in case methods have equal error rate scores. The \u201dAbsolute\n1www.cs.ucr.edu/\u02dceamonn/time_series_data 2http://fs.ismll.de/publicspace/\nInvariantFactorization/\nwins\u201d row, in the bottom of the table, counts the datasets where a method has the best prediction accuracy. As can be trivially deduced, our method has a clear superiority in terms of absolute wins, scoring 19.37 wins against 9.00 wins of the second best method. In addition INFA outperforms by large margins all the baselines in an one-to-one comparisons of wins. INFA has more wins, yet the predominant analysis is whether or not those wins represent statistically significant differences. Each cell on the bottom row represents the p value of the Wilcoxon Signed-Rank test on the error rate values of INFA against each baseline. Our method has a statistically significant difference over the error results of all baselines with a two-tailed hypothesis and the standard significance level of 95% confidence (p \u2264 0.05).\nBased on our survey of related work, the results presented in this study are the best published prediction accuracy scores in the realm of time-series classification, with respect to the UCR collection of datasets."}, {"heading": "6 Conclusions", "text": "In this study we presented Invariant Factorization, a method that initially decomposes the time series into a set of overlapping segments via a sliding window approach. The segments are approximated by learning a set of latent patterns and degrees of memberships of each segment to each pattern. We formalized the factorization as a constraint objective function and proposed a stochastic coordinate descent method to solve it. The new representation of time series are the sums of the membership weights, which represent frequencies of local patterns. Features from various sliding window sizes were concatenated to encapsulate interaction among patterns of various scales. Finally we conducted a thorough experimental comparison against 6 state of the art baselines in 43 real-life time series datasets. Our method outperforms all the baselines with statistically significant margins and marks the best published results in the realm of time-series classification, regarding the UCR collection of datasets."}], "references": [{"title": "k-means++: the advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Shift and 2d rotation invariant sparse coding for multivariate signals", "author": ["Q. Barthelemy", "A. Larue", "A. Mayoue", "D. Mercier", "J.I. Mars"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A complexity-invariant distance measure for time series", "author": ["Gustavo E.A.P.A. Batista", "Xiaoyue Wang", "Eamonn J. Keogh"], "venue": "In SDM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Cid: an efficient  complexity-invariant distance for time series", "author": ["Gustavo E.A.P.A. Batista", "Eamonn J. Keogh", "Oben Moses Tataw", "Vinicius M.A. Souza"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A bag-of-features framework to classify time series", "author": ["M. Baydogan", "G. Runger", "E. Tuv"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Motif-based classification of time series with bayesian networks and svms", "author": ["Krisztian Buza", "Lars Schmidt-Thieme"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "On the marriage of lp-norms and edit distance", "author": ["Lei Chen", "Raymond Ng"], "venue": "In Proceedings of the Thirtieth international conference on Very large data bases - Volume 30,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Spade: On shape-based pattern detection in streaming time series", "author": ["Yueguo Chen", "M.A. Nascimento", "Beng-Chin Ooi", "A. Tung"], "venue": "In Data Engineering,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Fast global alignment kernels", "author": ["Marco Cuturi"], "venue": "In Getoor et al., editor, Proceedings of the ICML 2011,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Janez Dem\u0161ar"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Querying and mining of time series data: experimental comparison of representations and distance measures", "author": ["Hui Ding", "Goce Trajcevski", "Peter Scheuermann", "Xiaoyue Wang", "Eamonn J. Keogh"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Classification of sparse time series via supervised matrix factorization", "author": ["Josif Grabocka", "Alexandros Nanopoulos", "Lars Schmidt- Thieme"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Invariant time-series classification", "author": ["Josif Grabocka", "Alexandros Nanopoulos", "Lars Schmidt- Thieme"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Support vector machines and dynamic time warping for time series", "author": ["Steinn Gudmundsson", "Thomas Philip Runarsson", "Sven Sigurdsson"], "venue": "In IJCNN,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Classification of time series by shapelet transformation. Data Mining and Knowledge Discovery, accepted subject to minor corrections", "author": ["J. Hills", "J. Lines", "E. Baranauskas", "J. Mapp", "A. Bagnall"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Pooling robust shiftinvariant sparse representations of acoustic signals", "author": ["Po-Sen Huang", "Jianchao Yang", "Mark Hasegawa-Johnson", "Feng Liang", "Thomas S. Huang"], "venue": "In IN- TERSPEECH. ISCA,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Dimensionality reduction for fast similarity search in large time series databases", "author": ["Eamonn Keogh", "Kaushik Chakrabarti", "Michael Pazzani", "Sharad Mehrotra"], "venue": "Knowledge and Information Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Scaling up dynamic time warping for datamining applications", "author": ["Eamonn J. Keogh", "Michael J. Pazzani"], "venue": "In KDD,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Spatial representation for efficient  sequence classification", "author": ["P.P. Kuksa", "V. Pavlovic"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Coding timevarying signals using sparse, shift-invariant representations", "author": ["Michael S. Lewicki", "Terrence J. Sejnowski"], "venue": "In Proceedings of NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Experiencing sax: a novel symbolic representation of time series", "author": ["Jessica Lin", "Eamonn Keogh", "Li Wei", "Stefano Lonardi"], "venue": "Data Min. Knowl. Discov.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Rotation-invariant similarity in time series using bag-of-patterns representation", "author": ["Jessica Lin", "Rohan Khade", "Yuan Li"], "venue": "J. Intell. Inf. Syst.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Finding structural similarity in time series data using bag-of-patterns representation", "author": ["Jessica Lin", "Yuan Li"], "venue": "In Proceedings of the 21st International Conference on Scientific and Statistical Database Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Logical-shapelets: an expressive primitive for time series classification", "author": ["Abdullah Mueen", "Eamonn J. Keogh", "Neal Young"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Advances in kernel methods. chapter Fast training of support vector machines using sequential minimal optimization, pages 185\u2013208", "author": ["John C. Platt"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Fast shapelets: A scalable algorithm for discovering time series shapelets", "author": ["T. Rakthanmanon", "E. Keogh"], "venue": "Proceedings of the 13th SIAM International Conference on Data Mining,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Searching and mining trillions of time series subsequences under dynamic time warping", "author": ["Thanawin Rakthanmanon", "Bilson Campana", "Abdullah Mueen", "Gustavo Batista", "Brandon Westover", "Qiang Zhu", "Jesin Zakaria", "Eamonn Keogh"], "venue": "In Proceedings of the 18th ACM SIGKDD,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Discovering similar multidimensional trajectories", "author": ["M. Vlachos", "G. Kollios", "D. Gunopulos"], "venue": "In Data Engineering,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "Towards heterogeneous temporal clinical event pattern discovery: a convolutional approach", "author": ["Fei Wang", "Noah Lee", "Jianying Hu", "Jimeng Sun", "Shahram Ebadollahi"], "venue": "In Proceedings of ACM SIGKDD,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Bag-of-words representation for biomedical time series classification", "author": ["Jin Wang", "Ping Liu", "Mary F.H. She", "Saeid Nahavandi", "Abbas Kouzani"], "venue": "Biomedical Signal Processing and Control,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Experimental comparison of representation methods and distance measures for time series data", "author": ["Xiaoyue Wang", "Abdullah Mueen", "Hui Ding", "Goce Trajcevski", "Peter Scheuermann", "Eamonn Keogh"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Time series classification using support vector machine with gaussian elastic metric kernel", "author": ["Dongyu Zhang", "Wangmeng Zuo", "David Zhang", "Hongzhi Zhang"], "venue": "In ICPR,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}], "referenceMentions": [{"referenceID": 13, "context": "Therefore traditionally strong classifiers, such as Support Vector Machines (SVM), fail to excel in terms of prediction accuracy [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "An early pioneer method called Dynamic Time Warping (DTW), (still considered competitive [11, 27]), computes the similarity among series by re-aligning the time indexes.", "startOffset": 89, "endOffset": 97}, {"referenceID": 26, "context": "An early pioneer method called Dynamic Time Warping (DTW), (still considered competitive [11, 27]), computes the similarity among series by re-aligning the time indexes.", "startOffset": 89, "endOffset": 97}, {"referenceID": 17, "context": "The algorithm explores all the possible relative alignments of time indexes of two series and picks the one yielding the minimum overall distance [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "Distance metrics focus on defining measurements on the similarity of two series instances [18, 7, 8, 4].", "startOffset": 90, "endOffset": 103}, {"referenceID": 6, "context": "Distance metrics focus on defining measurements on the similarity of two series instances [18, 7, 8, 4].", "startOffset": 90, "endOffset": 103}, {"referenceID": 7, "context": "Distance metrics focus on defining measurements on the similarity of two series instances [18, 7, 8, 4].", "startOffset": 90, "endOffset": 103}, {"referenceID": 3, "context": "Distance metrics focus on defining measurements on the similarity of two series instances [18, 7, 8, 4].", "startOffset": 90, "endOffset": 103}, {"referenceID": 31, "context": "For instance, the invariant kernel functions have been applied to measure instance similarities in the projected space of a non-linear SVM [32, 14].", "startOffset": 139, "endOffset": 147}, {"referenceID": 13, "context": "For instance, the invariant kernel functions have been applied to measure instance similarities in the projected space of a non-linear SVM [32, 14].", "startOffset": 139, "endOffset": 147}, {"referenceID": 12, "context": "Another paper proposes to generate all pattern variations as new instances and inflate the training set [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "Those local segments are converted into symbolic words and a histogram of the words\u2019 occurrences is built [22, 23].", "startOffset": 106, "endOffset": 114}, {"referenceID": 22, "context": "Those local segments are converted into symbolic words and a histogram of the words\u2019 occurrences is built [22, 23].", "startOffset": 106, "endOffset": 114}, {"referenceID": 4, "context": "Another study constructs a supervised codebook generated from local patterns, which is used to create features for a random forest classifiers [5].", "startOffset": 143, "endOffset": 146}, {"referenceID": 17, "context": "The most popular of those approaches is the Dynamic Time Warping (DTW) measure [18], which overcomes deficiencies of the L2 norm distance by aligning the time indexes of two series instances.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "DTW produces competitive prediction accuracies [11, 31] and has been speed up using lower boundary heuristics [27].", "startOffset": 47, "endOffset": 55}, {"referenceID": 30, "context": "DTW produces competitive prediction accuracies [11, 31] and has been speed up using lower boundary heuristics [27].", "startOffset": 47, "endOffset": 55}, {"referenceID": 26, "context": "DTW produces competitive prediction accuracies [11, 31] and has been speed up using lower boundary heuristics [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 6, "context": "Other similarity based distance metrics have extended the edit distance of strings into the time-series domain [7, 8].", "startOffset": 111, "endOffset": 117}, {"referenceID": 7, "context": "Other similarity based distance metrics have extended the edit distance of strings into the time-series domain [7, 8].", "startOffset": 111, "endOffset": 117}, {"referenceID": 27, "context": "Furthermore, the longest common subsequence of time series has also been used as an indication of similarity [28].", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "Moreover, similarities of sequential data have been measured using sparse spatial sample kernels [19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 2, "context": "CID significantly improves the accuracy of DTW [3, 4].", "startOffset": 47, "endOffset": 53}, {"referenceID": 3, "context": "CID significantly improves the accuracy of DTW [3, 4].", "startOffset": 47, "endOffset": 53}, {"referenceID": 13, "context": "For instance DTW has been used as a SVM kernel [14], even though the resulting kernel is not positive semi definite.", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "Consecutively, another study has proposed a Gaussian elastic kernel [32].", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "A method which produces a semi-definite kernel is called global alignment kernels and builds an average statistics from all possible warping paths of time indexes [9].", "startOffset": 163, "endOffset": 166}, {"referenceID": 12, "context": "In addition, another study has inflated the training set by adding new instances that represent variations of original training data [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "Dimensionality reduction has been used to project the time series into a low-rank data space [17], while a recent method incorporates class segregation into the projection [12].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Dimensionality reduction has been used to project the time series into a low-rank data space [17], while a recent method incorporates class segregation into the projection [12].", "startOffset": 172, "endOffset": 176}, {"referenceID": 25, "context": "accuracy [26, 24].", "startOffset": 9, "endOffset": 17}, {"referenceID": 23, "context": "accuracy [26, 24].", "startOffset": 9, "endOffset": 17}, {"referenceID": 14, "context": "A related study detects a set of shapelets and transforms the series data into a new representation, defined by the distance to those shapelets [15].", "startOffset": 144, "endOffset": 148}, {"referenceID": 5, "context": "For instance frequencies of time-series motifs have been fed into standard classifiers [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 22, "context": "Another attempt has focused on building histograms of local patterns represented as symbolic words [23].", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "Those symbolic words are produced by a piecewise constant approximation technique called SAX [21], while the frequencies of the SAX words are used ultimately for classification [22, 23].", "startOffset": 93, "endOffset": 97}, {"referenceID": 21, "context": "Those symbolic words are produced by a piecewise constant approximation technique called SAX [21], while the frequencies of the SAX words are used ultimately for classification [22, 23].", "startOffset": 177, "endOffset": 185}, {"referenceID": 22, "context": "Those symbolic words are produced by a piecewise constant approximation technique called SAX [21], while the frequencies of the SAX words are used ultimately for classification [22, 23].", "startOffset": 177, "endOffset": 185}, {"referenceID": 29, "context": "One similar bag-of-words approach has also been applied to long biomedical data [30].", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": "Moreover, a bag-of-patterns study proposes to extract series segments of various lengths and positions and generate a supervised codebook of those patterns [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "That study demonstrates considerable improvements over baselines in terms of prediction accuracy [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 19, "context": "A shift-invariant sparse coding of signals has been proposed for reconstructing noisy or missing series segments [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "In similar domains, sparse coding factorization has been applied for deriving shift and 2D rotation invariant features of hand writing data [2], and also invariant features of audio data [16].", "startOffset": 140, "endOffset": 143}, {"referenceID": 15, "context": "In similar domains, sparse coding factorization has been applied for deriving shift and 2D rotation invariant features of hand writing data [2], and also invariant features of audio data [16].", "startOffset": 187, "endOffset": 191}, {"referenceID": 28, "context": "Moreover, a temporal decomposition of multivariate streams has been used to discover patterns in patients\u2019 clinical events [29].", "startOffset": 123, "endOffset": 127}, {"referenceID": 24, "context": "In order to avoid this bottleneck, we propose to update the memberships in pairs, inspired by a similar strategy known as the Sequential Minimal Optimization algorithm [25].", "startOffset": 168, "endOffset": 172}, {"referenceID": 0, "context": "The answer is addressed via a technique utilized to find the initial centroids in a clustering setup [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "The patterns (analogy to centroids) are initialized to segments with a probability proportional to the distance to all the other segments [1].", "startOffset": 138, "endOffset": 141}, {"referenceID": 4, "context": "1 Baselines We compared the prediction accuracy of our method, denoted Invariant Factorization (INFA), against the following six state of the art baselines: \u2022 TSBF: The bag-of-features framework for time series (TSBF) uses a supervised codebook to extract features for a random forest classifier [5].", "startOffset": 296, "endOffset": 299}, {"referenceID": 18, "context": "\u2022 SSSK: Sparse Spatial Similarity Kernel (SSSK) measures sequence similarity through sampling sequence features at different resolutions [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "\u2022 BOW: The Bag of Words (BOW) method decomposes the series into local SAX words and uses a histogram representation of words as the new feature representation [22, 23].", "startOffset": 159, "endOffset": 167}, {"referenceID": 22, "context": "\u2022 BOW: The Bag of Words (BOW) method decomposes the series into local SAX words and uses a histogram representation of words as the new feature representation [22, 23].", "startOffset": 159, "endOffset": 167}, {"referenceID": 17, "context": "\u2022 DTW: Dynamic Time Warping (DTW) computes the best alignment of time indexes resulting in the mininal distance [18, 27].", "startOffset": 112, "endOffset": 120}, {"referenceID": 26, "context": "\u2022 DTW: Dynamic Time Warping (DTW) computes the best alignment of time indexes resulting in the mininal distance [18, 27].", "startOffset": 112, "endOffset": 120}, {"referenceID": 3, "context": "\u2022 CID: The complexity invariant distance (CID) adds a L2-based total variation regularization term into the DTW distance [4].", "startOffset": 121, "endOffset": 124}, {"referenceID": 25, "context": "\u2022 FSH: Fast shapelet (FSH) extracts the most discriminative segment of the series dataset, such that the distance from the dataset instances to the optimal shapelet can be used as a feature for classification [26].", "startOffset": 209, "endOffset": 213}, {"referenceID": 12, "context": "The applied classifier was a polynomial kernel SVM with a polynomial degree being 3 and the complexity parameter 1, which are competitive SVM settings for the UCR collection [13].", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "In order to compare multiple classifiers across a large number of datasets we follow the established benchmarks of counting wins and Wilcoxon\u2019s Signed-Rank test for statistical significance [10].", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "To be fair with the baselines, we retrieved the results from the baselines\u2019 publications [5, 4, 26] over the same data splits as INFA.", "startOffset": 89, "endOffset": 99}, {"referenceID": 3, "context": "To be fair with the baselines, we retrieved the results from the baselines\u2019 publications [5, 4, 26] over the same data splits as INFA.", "startOffset": 89, "endOffset": 99}, {"referenceID": 25, "context": "To be fair with the baselines, we retrieved the results from the baselines\u2019 publications [5, 4, 26] over the same data splits as INFA.", "startOffset": 89, "endOffset": 99}], "year": 2013, "abstractText": "Time-series classification is an important domain of machine learning and a plethora of methods have been developed for the task. In comparison to existing approaches, this study presents a novel method which decomposes a time-series dataset into latent patterns and membership weights of local segments to those patterns. The process is formalized as a constrained objective function and a tailored stochastic coordinate descent optimization is applied. The time-series are projected to a new feature representation consisting of the sums of the membership weights, which captures frequencies of local patterns. Features from various sliding window sizes are concatenated in order to encapsulate the interaction of patterns from different sizes. Finally, a large-scale experimental comparison against 6 state of the art baselines and 43 real life datasets is conducted. The proposed method outperforms all the baselines with statistically significant margins in terms of prediction accuracy.", "creator": "LaTeX with hyperref package"}}}