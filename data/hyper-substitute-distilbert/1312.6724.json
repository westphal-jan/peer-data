{"id": "1312.6724", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2013", "title": "Local algorithms for interactive clustering", "abstract": "neurons study actual results of remote foraging algorithms across objectives classified as natural limiting conditions. optimal algorithms start against any initial clustering and help make local changes in each object ; both are virtual artifacts representing many applications. we show that in this constrained setting actors must naturally design provably efficient libraries that produce accurate clusterings. we truly show that our algorithms perform well on real - world data.", "histories": [["v1", "Tue, 24 Dec 2013 00:16:37 GMT  (2189kb,D)", "https://arxiv.org/abs/1312.6724v1", null], ["v2", "Fri, 7 Feb 2014 05:12:20 GMT  (2189kb,D)", "http://arxiv.org/abs/1312.6724v2", null], ["v3", "Thu, 19 Mar 2015 23:45:54 GMT  (2403kb,D)", "http://arxiv.org/abs/1312.6724v3", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["pranjal awasthi", "maria-florina balcan", "konstantin voevodski"], "accepted": true, "id": "1312.6724"}, "pdf": {"name": "1312.6724.pdf", "metadata": {"source": "CRF", "title": "Local algorithms for interactive clustering", "authors": ["Pranjal Awasthi", "Maria Florina Balcan", "Konstantin Voevodski"], "emails": ["PAWASTHI@CS.CMU.EDU", "NINAMF@CS.CMU.EDU", "KVODSKI@GOOGLE.COM"], "sections": [{"heading": null, "text": "assumptions. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable features in many applications. We show that in this constrained setting one can still design provably efficient algorithms that produce accurate clusterings. We also show that our algorithms perform well on real-world data."}, {"heading": "1. Introduction", "text": "Clustering is usually studied in an unsupervised learning scenario where the goal is to partition the data given pairwise similarity information. Designing provably-good clustering algorithms is challenging because given a similarity function there may be multiple plausible clusterings of the data. Traditional approaches resolve this ambiguity by making assumptions on the data-generation process. For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al. (2010); Moitra and Valiant (2010); Belkin and Sinha (2010). Although this helps define the \u201cright\u201d clustering one should be looking for, real-world data rarely comes from such well-behaved probabilistic models. An alternative approach is to use limited user supervision to help the algorithm reach the desired answer. This approach has been facilitated by the availability of cheap crowd-sourcing tools in recent years. In certain applications such as search and document classification, where users are willing to help a clustering algorithm arrive at their own desired answer with a small amount of additional prodding, interactive algorithms are very useful. Hence, the study of interactive clustering algorithms has become an exciting new area of research.\nIn many practical settings we already start with a fairly good clustering computed with semiautomated techniques. For example, consider an online news portal that maintains a large collection of news articles. The news articles are clustered on the \u201cback-end,\u201d and are used to serve several \u201cfront-end\u201d applications such as recommendations and article profiles. For such a system, we do not have the freedom to compute arbitrary clusterings and present them to the user, which has been proposed in prior work. But it is still feasible to get limited feedback and locally edit the clustering.\nar X\niv :1\n31 2.\n67 24\nv3 [\ncs .D\nS] 1\n9 M\nIn particular, we may only want to change the \u201cbad\u201d portion revealed by the feedback without changing the rest of the clustering. Motivated by these observations, in this paper we study the problem of designing local algorithms for interactive clustering.\nWe propose a theoretical interactive model and provide strong experimental evidence supporting the practical applicability our algorithms. In our model we start with an initial clustering of the data. The algorithm then interacts with the user in stages. In each stage the user provides limited feedback on the current clustering in the form of split and merge requests. The algorithm then makes a local edit to the clustering that is consistent with user feedback. Such edits are aimed at improving the problematic part of the clustering pointed out by the user. The goal of the algorithm is to quickly converge (using as few requests as possible) to a clustering that the user is happy with - we call this clustering the target clustering.\nIn our model the user may request a certain cluster to be split if it is overclustered (intersects two or more clusters in the target clustering). The user may also request to merge two given clusters if they are underclustered (both intersect the same target cluster). Note that the user may not tell the algorithm how to perform the split or the merge; such input is infeasible because it requires a manual analysis of all the objects in the corresponding clusters. We also restrict the algorithm to only make local changes at each step, i.e., in response we may change only the cluster assignments of the points in the corresponding clusters. If the user requests to split a cluster Ci, we may change only the cluster assignments of points in Ci, and if the user requests to merge Ci and Cj , we may only reassign the points in Ci and Cj .\nThe split and merge requests described above are a natural form of feedback. It is easy for users to spot over/underclustering issues and request the corresponding splits/merges (without having to provide any additional information about how to perform the edit). For our model to be practically applicable, we also need to account for noise in the user requests. In particular, if the user requests a merge, only a fraction or a constant number of the points in the two clusters may belong to the same target cluster. Our model (See Section 2) allows for such noisy user responses.\nWe study the complexity of algorithms in the above model (the number of edits requests needed to find the target clustering) as a function of the error of the initial clustering. The initial error may be evaluated in terms of underclustering error \u03b4u and overclustering error \u03b4o (See Section 2). Because the initial error may be fairly small,1 we would like to develop algorithms whose complexity depends polynomially on \u03b4u, \u03b4o and only logarithmically on n, the number of data points. We show that this is indeed possible given that the target clustering satisfies a natural stability property (see Section 2). We also develop algorithms for the well-known correlation-clustering objective function Bansal et al. (2004), which considers pairs of points that are clustered inconsistently with respect to the target clustering (See Section 2).\nAs a pre-processing step, our algorithms compute the average-linkage tree of all the points in the data set. Note that if the target clustering C\u2217 satisfies our stability assumption, then the averagelinkage tree must be consistent with C\u2217 (see Section 3). However, in practice this average-linkage tree is much too large to be directly interpreted by the users. Still, given that the edit requests are somewhat consistent with C\u2217, we can use this tree to efficiently compute local edits that are consistent with the target clustering. Our analysis then shows that after a limited number of edit requests we must converge to the target clustering.\n1. Given 2 different k clusterings, \u03b4u and \u03b4o is atmost k2.\nOur Results In Section 3 we study the \u03b7-merge model. Here we assume that the user may request to split a cluster Ci only if Ci contains points from several ground-truth clusters. The user may request to merge Ci and Cj only if an \u03b7-fraction of points in each Ci and Cj are from the same ground-truth cluster.\nFor this model for \u03b7 > 0.5, given an initial clustering with overclustering error \u03b4o and underclustering error \u03b4u, we present an algorithm that requires \u03b4o split requests and 2(\u03b4u + k) log 1\n1\u2212\u03b7 n\nmerge requests to find the target clustering, where n is the number of points in the dataset. For \u03b7 > 2/3, given an initial clustering with correlation-clustering error \u03b4cc, we present an algorithm that requires at most \u03b4cc edit requests to find the target clustering.\nIn Section 4 we relax the condition on the merges and allow the user to request a merge even if Ci and Cj only have a single point from the same target cluster. We call this the unrestricted-merge model. Here the requirement on the accuracy of the user response is much weaker and we need to make further assumptions on the nature of the requests. More specifically, we assume that each merge request is chosen uniformly at random from the set of feasible merges. Under this assumption we present an algorithm that with probability at least 1\u2212 requires \u03b4o split requests and O(log k \u03b4 2 u) merge requests to find the target clustering. We develop several algorithms for performing the split and merge requests under different assumptions. Each algorithm uses the global average-linkage tree Tglob to compute a local clustering edit. Our splitting procedure finds the node in Tglob where the corresponding points are first split in two. It is more challenging to develop a correct merge procedure, given that we allow \u201cimpure\u201d merges, where one or both clusters have points from another target cluster (other than the one that they both intersect). To perform such merges, in the \u03b7-merge model we develop a procedure to extract the \u201cpure\u201d subsets of the two clusters, which must only contain points from the same target cluster. Our procedure searches for the deepest node in Tglob that has enough points from both clusters. In the unrestricted-merge model, we develop another merge procedure that either merges the two clusters or merges them and splits them. This algorithm always makes progress if the proposed merge is \u201cimpure,\u201d and makes progress on average if it is \u201cpure\u201d (both clusters are subset of the same target cluster).\nWhen the data satisfies stronger assumptions, we present more-scalable split and merge algorithms that do not require any global information. These procedures compute the edit by only considering the points in the user request and the similarities between them.\nIn Section 5 we demonstrate the effectiveness of our algorithms on real data. We show that for the purposes of splitting known over-clusters, the splitting procedure proposed here computes the best splits, when compared to other well-known techniques. We also test the entire proposed framework on newsgroup documents data, which is quite challenging for traditional unsupervised clustering methods Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005); Dasgupta and Hsu (2008); Dai et al. (2010); Boulis and Ostendorf (2004); Zhong (2005). Still, we find that our algorithms perform fairly well; for larger settings of \u03b7 we are able find the target clustering after a limited number of edit requests. Related work Interactive models for clustering studied in previous works Balcan and Blum (2008); Awasthi and Zadeh (2010) were inspired by an analogous model for learning under feedback Angluin (1998). In this model, the algorithm can propose a hypothesis to the user (in this case, a clustering of the data) and get some feedback regarding the correctness of the current hypothesis. As in our model, the\nfeedback considered is split and merge queries. The goal is to design efficient algorithms which use very few queries to the user. A critical limitation in prior work is that the algorithm has the freedom to choose any arbitrary clustering as the starting point and can make arbitrary changes at each step. Hence these algorithms may propose a series of \u201cbad\u201d clusterings to the user to quickly prune the search space and reach the target clustering. Our interactive clustering model is in the context of an initial clustering; we are restricted to only making local changes to this clustering to correct the errors pointed out by the user. This model is well-motivated by several applications, including the Google application described in the experimental section.\nBasu et al. Basu et al. (2004) study the problem of minimizing the k-means objective in the presence of limited supervision. This supervision is in the form of pairwise must-link and cannot-link constraints. They propose a variation of the Lloyd\u2019s method for this problem and show promising experimental results. The split/merge requests that we study are a more natural form of interaction because they capture macroscopic properties of a cluster. Getting pairwise constraints among data points involves much more effort on the part of the user and is unrealistic in many scenarios.\nThe stability property that we consider is a natural generalization of the \u201cstable marriage\u201d property (see Definition 2) that has been studied in a variety of previous works Balcan et al. (2008); Bryant and Berry (2001). It is the weakest among the stability properties that have been studied recently such as strict separation and strict threshold separation Balcan et al. (2008); Krishnamurthy et al. (2012). This property is known to hold for real-world data. In particular, Voevodski et al. (2012) observed that this property holds for protein sequence data, where similarities are computed with sequence alignment and ground truth clusters correspond to evolutionary-related proteins."}, {"heading": "2. Notation and Preliminaries", "text": "Given a data set X of n points we define C = {C1, C2, . . . Ck} to be a k-clustering of X where the Ci\u2019s represent the individual clusters. Given two clusterings C and C\u2032, we define the distance between a cluster Ci \u2208 C and the clustering C\u2032 as:\ndist(Ci, C\u2032) = |{C \u2032j \u2208 C\u2032 : C \u2032j \u2229 Ci 6= \u2205}| \u2212 1.\nThis distance is the number of additional clusters in C\u2032 that contain points from Ci; it evaluates to 0 when all points in Ci are contained in a single cluster in C\u2032. Naturally, we can then define the distance between C and C\u2032 as: dist(C, C\u2032) = \u2211 Ci\u2208C dist(Ci, C\n\u2032). Notice that this notion of clustering distance is asymmetric: dist(C, C\u2032) 6= dist(C\u2032, C). Also note that dist(C, C\u2032) = 0 if and only if C refines C\u2032. Observe that if C is the ground-truth clustering, and C\u2032 is a proposed clustering, then dist(C, C\u2032) can be considered an underclustering error, and dist(C\u2032, C) an overclustering error.\nAn underclustering error is an instance of several clusters in a proposed clustering containing points from the same ground-truth cluster; this ground-truth cluster is said to be underclustered. Conversely, an overclustering error is an instance of points from several ground-truth clusters contained in the same cluster in a proposed clustering; this proposed cluster is said to be overclustered. In the following sections we use C\u2217 = {C\u22171 , C\u22172 , . . . C\u2217k} to refer to the ground-truth clustering, and use C to refer to a proposed clustering. We use \u03b4u to refer to the underclustering error of a proposed clustering, and \u03b4o to refer to the overclustering error. In other words, we have \u03b4u = dist(C\u2217, C) and \u03b4o = dist(C, C\u2217). We also use \u03b4 to denote the sum of the two errors: \u03b4 = \u03b4u + \u03b4o. We call \u03b4 the under/overclustering error, and use the \u03b4(C, C\u2217) to refer to the error of C with respect to C\u2217.\nWe also observe that we can define the distance between two clusterings using the correlationclustering objective function. Given a proposed clustering C, and a ground-truth clustering C\u2217, we define the correlation-clustering error \u03b4cc as the number of (ordered) pairs of points that are clustered inconsistently with C\u2217:\n\u03b4cc = |{(u, v) \u2208 X \u00d7X : c(u, v) 6= c\u2217(u, v)}|,\nwhere c(u, v) = 1 if u and v are in the same cluster in C, and 0 otherwise; c\u2217(u, v) = 1 if u and v are in the same cluster in C\u2217, and 0 otherwise.\nNote that we may divide the correlation-clustering error \u03b4cc into overclustering component \u03b4cco and underclustering component \u03b4ccu:\n\u03b4cco = |{(u, v) \u2208 X \u00d7X : c(u, v) = 1 and c\u2217(u, v) = 0}|\n\u03b4ccu = |{(u, v) \u2208 X \u00d7X : c(u, v) = 0 and c\u2217(u, v) = 1}|\nIn our formal analysis we model the user as an oracle that provides edit requests.\nDefinition 1 (Local algorithm) We say that an interactive clustering algorithm is local if in each iteration only the cluster assignments of points involved in the oracle request may be changed. If the oracle requests to split Ci, the algorithm may only reassign the points in Ci. If the oracle requests to merge Ci and Cj , the algorithm may only reassign the points in Ci \u222a Cj .\nWe next formally define the properties of a clustering that we study in this work.\nDefinition 2 (Stability) Given a clustering C = {C1, C2, \u00b7 \u00b7 \u00b7Ck} over a domain X and a similarly function S : X \u00d7 X 7\u2192 <, we say that C satisfies stability with respect to S if for all i 6= j, and for all A \u2282 Ci and A\u2032 \u2286 Cj , S(A,Ci \\ A) > S(A,A\u2032), where for any two sets A,A\u2032, S(A,A\u2032) = Ex\u2208A,y\u2208A\u2032S(x, y).\nIn our analysis, we assume that the ground-truth clustering satisfies stability, and we have access to the corresponding similarity function. In addition, we also study the following stronger properties of a clustering, which were first introduced in Balcan et al. (2008).\nDefinition 3 (Strict separation) Given a clustering C = {C1, C2, \u00b7 \u00b7 \u00b7Ck} over a domain X and a similarly function S : X \u00d7X 7\u2192 <, we say that C satisfies strict separation with respect to S if for all i 6= j, x, y \u2208 Ci and z \u2208 Cj , S(x, y) > S(x, z).\nDefinition 4 (Strict threshold separation) Given a clustering C = {C1, C2, \u00b7 \u00b7 \u00b7Ck} over a domainX and a similarly function S : X\u00d7X 7\u2192 <, we say that C satisfies strict threshold separation with respect to S if there exists a threshold t such that, for all i, x, y \u2208 Ci, S(x, y) > t, and, for all i 6= j, x \u2208 Ci, y \u2208 Cj , S(x, y) \u2264 t.\nClearly, strict separation and strict threshold separation imply stability. In order for our algorithms to make progress, the oracle requests must be somewhat consistent\nwith the target clustering.\nDefinition 5 (\u03b7-merge model) In the \u03b7-merge model the oracle requests have the following properties split(Ci): Ci contains points from two or more target clusters. merge(Ci, Cj): At least an \u03b7-fraction of the points in each Ci and Cj belong to the same target cluster.\nDefinition 6 (Unrestricted-merge model) In the unrestricted-merge model the oracle requests have the following properties split(Ci): Ci contains points from two or more target clusters. merge(Ci, Cj): At least 1 point in each Ci and Cj belongs to the same target cluster.\nNote that the assumptions about the nature of the split requests are the same in both models. In the \u03b7-merge model, the oracle may request to merge two clusters if both have a constant fraction of points from the same target cluster. In the unrestricted-merge model, the oracle may request to merge two clusters if both have some points from the same target cluster."}, {"heading": "2.1 Generalized clustering error", "text": "We observe that the clustering errors defined in the previous section may be generalized by abstracting their common properties. We define the following properties of a natural clustering error, which is any integer-valued error that decreases when we locally improve the proposed clustering.\nDefinition 7 We say that a clustering error is natural if it satisfies the following properties:\n\u2022 If there exists a cluster Ci that contains points from C\u2217j and some other ground-truth cluster(s), then splitting this cluster into two clusters Ci,1 = Ci \u2229C\u2217j (which contains only points from C\u2217j ), and Ci,2 = Ci \u2212 Ci,1 (which contains the other points) must decrease the error.\n\u2022 If there exists two clusters that contain only points from the same target cluster, then merging them into one cluster must decrease the error.\n\u2022 The error is integer-valued.\nWe expect a lot of definitions of clustering error to satisfy the above criteria (especially the first two properties), in addition to other domain-specific criteria. Clearly, the under/overclustering error \u03b4 = \u03b4u + \u03b4o and the correlation-clustering error \u03b4cc are also natural clustering errors (Claim 8). As before, for a natural clustering error \u03b3, a proposed clustering C and the target clustering C\u2217, we will use \u03b3(C, C\u2217) to denote the magnitude of the error of C with respect to C\u2217.\nMoreover, it is easy to see that the under/overclustering error defined in the previous section is the lower-bound on any natural clustering error (Theorem 9).\nClaim 8 The under/overclustering error and the correlation clustering error satisfy Definition 7 and hence are natural clustering errors.\nTheorem 9 For any natural clustering error \u03b3, any proposed clustering C, and any target clustering C\u2217, \u03b3(C, C\u2217) \u2265 \u03b4(C, C\u2217).\nProof Given any proposed clustering C, and any target clustering C\u2217, we may transform C into C\u2217 via the following sequence of edits. First, we split all over-clustering instances using the following iterative procedure: while there exists a cluster Ci that contains points from C\u2217j and some other ground-truth cluster(s), we split it into two clusters Ci,1 = Ci \u2229C\u2217j and Ci,2 = Ci\u2212Ci,1. Note that this iterative split procedure will require exactly \u03b4o split edits, where \u03b4o is the initial overclustering error. Then, when we are left with only \u201cpure\u201d clusters (each intersects exactly one target cluster), we merge all under-clustering instances using the following iterative procedure: while there exist two clusters Ci and Cj that contain only points from the same target cluster, merge Ci and Cj . Note that this iterative merge procedure will require exactly \u03b4u merge edits, where \u03b4u is the initial underclustering error. Let us use \u03b3 to refer to any natural clustering error of C with respect to C\u2217. By the first property of natural clustering error, each split must have decreased \u03b3 by at least one. By the second property, each merge must have decreased \u03b3 by at least one as well. Given that we performed exactly \u03b4 = \u03b4o + \u03b4u edits, it follows that initially \u03b3(C, C\u2217) must have been at least \u03b4.\nFor additional discussion about comparing clusterings see Meila\u0306 (2007). Note that several criteria discussed in Meila\u0306 (2007) satisfy our first two properties (for a similarity measure we may replace \u201dmust decrease the error\u201d with \u201dmust increase the similarity\u201d). In addition, the Rand and Mirkin criteria discussed in Meila\u0306 (2007) are closely related to the correlation clustering error defined here (all three measures are a function of the number of pairs of points that are clustered incorrectly)."}, {"heading": "3. The \u03b7-merge model", "text": "In this section we describe and analyze the algorithms in the \u03b7-merge model. As a pre-processing step for all our algorithms, we first run the hierarchical average-linkage algorithm on all the points in the data set to compute the global average-linkage tree, which we denote by Tglob. The leaf nodes in this tree contain the individual points, and the root node contains all the points. The tree is computed in a bottom-up fashion: starting with the leafs in each iteration the two most similar nodes are merged, where the similarity between two nodes N1 and N2 is the average similarity between points in N1 and points in N2.\nWe assign a label \u201cimpure\u201d to each cluster in the initial clustering; these labels are used by the merge procedure. Given a split or merge request, a local clustering edit is computed from the global tree Tglob as described in Figure 1 and Figure 2.\nTo implement Step 1 in Figure 1, we start at the root of Tglob and \u201cfollow\u201d the points in Ci down one of the branches until we find a node that splits them. In order to implement Step 2 in Figure 2, it suffices to start at the root of Tglob and perform a post-order traversal, only considering nodes that have \u201cenough\u201d points from both clusters, and return the first output node.\nThe split procedure is fairly intuitive: if the average-linkage tree is consistent with the target clustering, it suffices to find the node in the tree where the corresponding points are first split in two. It is more challenging to develop a correct merge procedure: note that Step 2 in Figure 2 is only correct if \u03b7 > 0.5, which ensures that if two nodes in the tree have more than an \u03b7-fraction of the points from Ci and Cj , one must be an ancestor of the other. If the average-linkage tree is consistent with the ground-truth, then clearly the node equivalent to the corresponding target cluster (that Ci and Cj both intersect) will have enough points from Ci and Cj ; therefore the node that we find in Step 2 must be this node or one of its descendants. In addition, because our merge procedure replaces two clusters with three, we require pure/impure labels for the merge requests to terminate:\n\u201cpure\u201d clusters may only have other points added to them, and retain this label throughout the execution of the algorithm.\nWe now state the performance guarantee for these split and merge algorithms.\nTheorem 10 Suppose the target clustering satisfies stability, and the initial clustering has overclustering error \u03b4o and underclustering error \u03b4u. In the \u03b7-merge model, for any \u03b7 > 0.5, the algorithms in Figure 1 and Figure 2 require at most \u03b4o split requests and 2(\u03b4u + k) log 1\n1\u2212\u03b7 n merge requests to\nfind the target clustering.\nIn order to prove the theorem, we must do some preliminary analysis. First, we observe that if the target clustering satisfies stability, then every node of the average-linkage tree must be laminar (consistent) with respect to the ground-truth clustering.\nInformally, each node in a hierarchical clustering tree T is laminar (consistent) with respect to the clustering C if for each clusterCi \u2208 C, the points inCi are first grouped together in T before they are grouped with points from any other clusterCj 6=i. We formally state and prove these observations next.\nDefinition 11 (Laminar) A node N is laminar with respect to a clustering C if for each cluster Ci \u2208 C we have either N \u2229 Ci = \u2205, N \u2286 Ci, or Ci \u2286 N .\nLemma 12 Suppose the ground-truth clustering C\u2217 over a domain X satisfies stability with respect to a similarity function S. Let T be the average-linkage tree for X constructed with S. Then every node in T is laminar w.r.t. C\u2217.\nProof The proof of this statement can be found in Balcan et al. (2008). The intuition is that if there is a node in T that is not laminar w.r.t. C\u2217, then the average-linkage algorithm, at some step, must have merged A \u2282 C\u2217i , with B \u2282 C\u2217j for some i 6= j. However, this will contradict the stability property for the sets A and B.\nIt follows that the split computed by the algorithm in Figure 1 must also be consistent with the target clustering; we call such splits clean.\nDefinition 13 (Clean split) A partition (split) of a cluster Ci into clusters Ci,1 and Ci,2 is said to be clean ifCi,1 andCi,2 are non-empty, and for each ground-truth clusterC\u2217j such thatC \u2217 j \u2229Ci 6= \u2205, either C\u2217j \u2229 Ci = C\u2217j \u2229 Ci,1 or C\u2217j \u2229 Ci = C\u2217j \u2229 Ci,2.\nWe now prove the correctness of the split/merge procedures.\nLemma 14 If the ground-truth clustering satisfies stability and \u03b7 > 0.5 then,\na. The split procedure in Figure 1 always produces a clean split.\nb. The new cluster added in Step 4 in Figure 2 must be \u201cpure\u201d, i.e., it must contain points from a single ground-truth cluster.\nProof a. For purposes of contradiction, suppose the returned split is not clean: Ci,1 andCi,2 contain points from the same ground-truth cluster C\u2217j . It must be the case that Ci contains points from several ground-truth clusters, which implies that w.l.o.g. Ci,1 contains points from some other groundtruth cluster C\u2217l 6=j . This implies that N1 is not laminar w.r.t. C\u2217, which contradicts Lemma 12. b. By our assumption, at least 12 |Ci| points from Ci and 1 2 |Cj | points from Cj are from the same ground-truth clusterC\u2217l . Clearly, the nodeN \u2032 in Tglob that is equivalent toC\u2217l (which contains all the points in C\u2217l and no other points) must contain enough points from Ci and Cj , and only ascendants and descendants of N \u2032 may contain more than an \u03b7 > 1/2 fraction of points from both clusters. Therefore, the node N that we find with a depth-first search must be N \u2032 or one of its descendants, and will only contain points from C\u2217l .\nUsing the above lemma, we can prove the bounds on the split and merge requests stated in Theorem 10. Proof [Proof of Theorem 10]\nWe first give a bound on the number of splits. Observe that each split reduces the overclustering error by exactly 1. To see this, suppose we execute Split(C1), and call the resulting clusters C2 and C3. Call \u03b41 the overclustering error before the split, and \u03b42 the overclustering error after the split. Let\u2019s use k1 to refer to the number of ground-truth clusters that intersect C1, and define k2 and k3 similarly. Due to the clean split property, no ground-truth cluster can intersect both C2 and C3,\ntherefore it must be the case that k2 + k3 = k1. Also, clearly k2, k3 > 0. Therefore we have:\n\u03b42 = \u03b41 \u2212 (k1 \u2212 1) + (k2 \u2212 1) + (k3 \u2212 1) = \u03b41 \u2212 k1 + (k2 + k3)\u2212 1 = \u03b41 \u2212 1.\nMerges cannot increase overclustering error. Therefore the total number of splits may be at most \u03b4o. We next give the arguments about the number of impure and pure merges.\nWe first argue that we cannot have too many \u201cimpure\u201d merges before each cluster inC is marked \u201cpure.\u201d Consider the clustering P = {Ci \u2229C\u2217j | Ci is marked \u201cimpure\u201d and Ci \u2229C\u2217j 6= \u2205}. Clearly, at the start |P | = \u03b4u + k. A merge does not increase the number of clusters in P , and the splits do not change P at all (because of the clean split property). Moreover, each impure merge (a merge of two impure clusters or a merge of a pure and an impure cluster) depletes some Pi \u2208 P by moving \u03b7|Pi| of its points to a pure cluster. Clearly, we can then have at most log1/(1\u2212\u03b7) n merges depleting each Pi. Since each impure merge must deplete some Pi, it must be the case that we can have at most (\u03b4u + k) log1/(1\u2212\u03b7) n impure merges in total.\nNotice that a pure cluster can only be created by an impure merge, and there can be at most one pure cluster created by each impure merge. Clearly, a pure merge removes exactly one pure cluster. Therefore the number of pure merges may be at most the total number of pure clusters that are created, which is at most the total number of impure merges. Therefore the total number of merges must be less than 2(\u03b4u + k) log1/(1\u2212\u03b7) n.\nWe can also restate the run-time bound in Theorem 10 in terms of any natural clustering error \u03b3. The following collorary follows from Theorem 10 and Theorem 9.\nCorollary 15 Suppose the target clustering satisfies stability, and the initial clustering has clustering error \u03b3, where \u03b3 is any natural clustering error as defined in Definition 7. In the \u03b7-merge model, for any \u03b7 > 0.5, the algorithms in Figure 1 and Figure 2 require at most O(\u03b3 + k) log 1\n1\u2212\u03b7 n edit\nrequests to find the target clustering."}, {"heading": "3.1 Algorithms for correlation-clustering error", "text": "To bound the number of edit requests with respect to the correlation clustering objective, we must use a different merge procedure, which is described in Figure 3.\nHere instead of creating a new \u201cpure\u201d cluster, we add these points to the larger of the two clusters in the merge. Notice that the new algorithm is much simpler than the merge algorithm for the under/overclustering error. Using this merge procedure and the split procedure presented earlier gives the following performance guarantee.\nTheorem 16 Suppose the target clustering satisfies stability, and the initial clustering has correlationclustering error of \u03b4cc. In the \u03b7-merge model, for any \u03b7 > 2/3, using the split and merge procedures in Figures 1 and 3 requires at most \u03b4cc edit requests to find the target clustering.\nProof Consider the contributions of individual points to \u03b4cco and \u03b4ccu, which are defined as:\n\u03b4cco(u) = |{v \u2208 X : c(u, v) = 1 and c\u2217(u, v) = 0}|\n\u03b4ccu(u) = |{v \u2208 X : c(u, v) = 0 and c\u2217(u, v) = 1}| We first argue that a split of a cluster Ci must reduce \u03b4cc by at least 1. Given that the split is clean, it is easy to verify that the outcome may not increase \u03b4ccu(u) for any u \u2208 Ci. We can also verify that for each u \u2208 Ci, \u03b4cco(u) must decrease by at least 1. This completes the argument, given that the correlation-clustering error with respect to all other pairs of points must remain the same.\nWe now argue that if \u03b7 > 2/3, each merge of Ci and Cj must reduce \u03b4cc by at least 1. Without loss of generality, suppose that |Ci| \u2265 |Cj |, and let us use P to refer to the \u201cpure\u201d subset of Cj that is moved toCi. We observe that the outcome must remove at least \u03b41 pairwise correlation-clustering errors, where \u03b41 satisfies \u03b41 \u2265 2|P |(\u03b7|Ci|). Similarly, we observe that the outcome may add at most \u03b42 pairwise correlation-clustering errors, where \u03b42 satisfies:\n\u03b42 \u2264 2|P |((1\u2212 \u03b7)|Ci|) + 2|P |((1\u2212 \u03b7)|Cj |) \u2264 4|P |((1\u2212 \u03b7)|Ci|).\nIt follows that for \u03b7 > 2/3, \u03b41 must exceed \u03b42; therefore the sum of the pairwise correlationclustering errors must decrease, giving a lower correlation-clustering error total.\nObserve that the runtime bound in Theorem 16 is tight: in some instances any local algorithm requires at least \u03b4cc edits to find the target clustering. To verify this, suppose the target clustering is composed of n singleton clusters, and the initial clustering contains n/2 clusters of size 2. In this instance, the initial correlation clustering error \u03b4cc = n/2, and the oracle must issue at least n/2 split requests before we reach the target clustering (no matter how the algorithm reassigns the corresponding points)."}, {"heading": "3.2 Algorithms under stronger assumptions", "text": "When the data satisfies stronger stability properties we may simplify the presented algorithms and/or obtain better performance guarantees. In particular, if the data satisfies the strict separation property from Balcan et al. (2008), we may change the split and merge algorithms to use the local averagelinkage tree, which is constructed from only the points in the edit request. In addition, if the data satisfies strict threshold separation, we may remove the restriction on \u03b7 and use a different merge procedure that is correct for any \u03b7 > 0.\nTheorem 17 Suppose the target clustering satisfies strict separation, and the initial clustering has overclustering error \u03b4o and underclustering error \u03b4u. In the \u03b7-merge model, for any \u03b7 > 0.5, the algorithms in Figure 4 and Figure 5 require at most \u03b4o split requests and 2(\u03b4u + k) log 1\n1\u2212\u03b7 n merge\nrequests to find the target clustering.\nProof Let us use L\u2217 to refer to the ground-truth clustering of the points in the split/merge request. If the target clustering satisfies strict separation, it is easy to verify that every node in the local average-linkage tree Tloc must be laminar (consistent) w.r.t. L\u2217. We can then use this observation to prove the equivalent of Lemma 14 for the split procedure in Figure 4 and the merge procedure in Figure 5. The analysis in Theorem 10 remains unchanged.\nTheorem 18 Suppose the target clustering satisfies strict threshold separation, and the initial clustering has overclustering error \u03b4o and underclustering error \u03b4u. In the \u03b7-merge model, for any \u03b7 > 0, the algorithms in Figure 4 and Figure 6 require at most \u03b4o split requests and 2(\u03b4u + k) log 1\n1\u2212\u03b7 n\nmerge requests to find the target clustering.\nProof If the target clustering satisfies strict threshold separation, we can verify that the split procedure in Figure 4 and the merge procedure in Figure 6 are correct for any \u03b7 > 0. The analysis in Theorem 10 remains unchanged.\nTo verify that the split procedure always produces a clean split, again let us use L\u2217 to refer to the ground-truth clustering of the points in the split request. We can again verify that each node in the local average-linkage tree Tloc must be laminar (consistent) w.r.t. L\u2217. It follows that the split procedure always produces a clean split. Note that clearly this argument does not depend on the setting of \u03b7.\nWe now verify that the new cluster added by the merge procedure Figure 6 must be \u201cpure\u201d (must contain points from a single target cluster). To see this, observe that in the graph G in Figure 6, all pairs of points from the same target cluster are connected before any pairs of points from different target clusters. It follows that the first component that contains at least an \u03b7-fraction of points from Ci and Cj must be \u201cpure\u201d. Note that this argument applies for any \u03b7 > 0.\nNote that the merge procedure in Figure 6 is correct for \u03b7 \u2264 0.5 only if the target clustering satisfies strict threshold separation: there is a single threshold t such that for all i, x, y \u2208 C\u2217i , S(x, y) > t, and, for all i 6= j, x \u2208 C\u2217i , y \u2208 C\u2217j , S(x, y) \u2264 t. When only strict separation holds (the threshold for each target cluster may be different), this procedure may first connect points from different target clusters, and for \u03b7 \u2264 0.5 this component may then be large enough to be output.\nAs in Corollary 15, we may also restate the run-time bounds in Theorem 17 and Theorem 18 in terms of any natural clustering error \u03b3. The following corollaries follow from Theorem 17, Theorem 18 and Theorem 9.\nCorollary 19 Suppose the target clustering satisfies strict separation, and the initial clustering has clustering error \u03b3, where \u03b3 is any natural clustering error as defined in Definition 7. In the \u03b7-merge model, for any \u03b7 > 0.5, the algorithms in Figure 4 and Figure 5 require at most O(\u03b3 + k) log 1\n1\u2212\u03b7 n\nedit requests to find the target clustering.\nCorollary 20 Suppose the target clustering satisfies strict threshold separation, and the initial clustering has clustering error \u03b3, where \u03b3 is any natural clustering error as defined in Definition 7. In the \u03b7-merge model, for any \u03b7 > 0, the algorithms in Figure 4 and Figure 6 require at most O(\u03b3 + k) log 1\n1\u2212\u03b7 n edit requests to find the target clustering."}, {"heading": "4. The unrestricted-merge model", "text": "In this section we further relax the assumptions about the nature of the oracle requests. As before, the oracle may request to split a cluster if it contains points from two or more target clusters. For merges, now the oracle may request to merge Ci and Cj if both clusters contain only a single point from the same ground-truth cluster. We note that this is a minimal set of assumptions for a local algorithm to make progress, otherwise the oracle may always propose irrelevant splits or merges that cannot reduce clustering error. For this model we propose the merge algorithm described in Figure 7. The split algorithm remains the same as in Figure 1.\nTo provably find the ground-truth clustering in this setting we require that each merge request must be chosen uniformly at random from the set of feasible merges. This assumption is consistent with the observation in Awasthi and Zadeh (2010) that in the unrestricted-merge model with arbitrary request sequences, even very simple cases (ex. union of intervals on a line) require a prohibitively large number of requests. We do not make additional assumptions about the nature of the\nsplit requests; in each iteration any feasible split may be proposed by the oracle. In this setting our algorithms have the following performance guarantee.\nTheorem 21 Suppose the target clustering satisfies stability, and the initial clustering has overclustering error \u03b4o and underclustering error \u03b4u. In the unrestricted-merge model, with probability at least 1\u2212 , the algorithms in Figure 1 and Figure 7 require \u03b4o split requests and O(log k \u03b4 2 u) merge requests to find the target clustering.\nThe above theorem is proved in a series of lemmas. We first state a lemma regarding the correctness of the Algorithm in Figure 7. We argue that if the algorithm merges Ci and Cj , it must be the case that both Ci and Cj only contain points from the same ground-truth cluster.\nLemma 22 If the algorithm in Figure 7 merges Ci and Cj in Step 3, it must be the case that Ci \u2282 C\u2217l and Cj \u2282 C\u2217l for some ground-truth cluster C\u2217l .\nProof We prove the contrapositive. Suppose Ci and Cj both contain points from C\u2217l , and in addition Ci \u222aCj contains points from some other ground-truth cluster. Let us define S1 = C\u2217l \u2229Ci and S2 = C \u2217 l \u2229 Cj . Because the clusters C \u2032i, C \u2032j result from a clean split, it follows that S1, S2 \u2286 C \u2032i or S1, S2 \u2286 C \u2032j . Without loss of generality, assume S1, S2 \u2286 C \u2032i. Then clearly C \u2032i 6= Ci and C \u2032i 6= Cj , so Ci and Cj are not merged.\nThe \u03b4o bound on the number of split requests follows from the observation that each split reduces the overclustering error by exactly 1 (as before), and the fact that the merge procedure does not increase overclustering error.\nLemma 23 The merge algorithm in Figure 7 does not increase overclustering error.\nProof Suppose Ci and Cj are not both \u201cpure\u201d (one or both contain elements from several groundtruth clusters), and hence we obtain two new clusters C \u2032i, C \u2032 j . Let us call \u03b41 the overclustering error before the merge, and \u03b42 the overclustering error after the merge. Let\u2019s use k1 to refer to the number of ground-truth clusters that intersect Ci, k2 to refer to the number of ground-truth clusters that intersect Cj , and define k\u20321 and k \u2032 2 similarly. The new clusters C \u2032 i and C \u2032 j result from a \u201cclean\u201d split, therefore no ground-truth cluster may intersect both of them. It follows that k\u20321 + k \u2032 2 \u2264 k1 + k2. Therefore we now have:\n\u03b42 = \u03b41 \u2212 (k1 \u2212 1)\u2212 (k2 \u2212 1) + (k\u20321 \u2212 1) + (k\u20322 \u2212 1) = \u03b41 \u2212 (k1 + k2) + (k\u20321 + k\u20322) \u2264 \u03b41.\nIf Ci and Cj are both \u201cpure\u201d (both are subsets of the same ground-truth cluster), then clearly the merge operation has no effect on the overclustering error.\nThe following lemmas bound the number of impure and pure merges. Here we call a proposed merge pure if both clusters are subsets of the same ground-truth cluster, and impure otherwise.\nLemma 24 The merge algorithm in Figure 7 requires at most \u03b4u impure merge requests.\nProof We argue that the result of each impure merge request must reduce the underclustering error by at least 1. Suppose the oracle requests to merge Ci and Cj , and C \u2032i and C \u2032 j are the resulting clusters. Clearly, the local edit has no effect on the underclustering error with respect to target clusters that do not intersect Ci or Cj . In addition, because the new clusters C \u2032i and C \u2032 j result from a clean split, for target clusters that intersect exactly one of Ci, Cj , the underclustering error must stay the same. For target clusters that intersect both Ci and Cj , the underclustering error must decrease by exactly one; the number of such target clusters is at least one.\nLemma 25 The probability that the algorithm in Figure 7 requires more than O(log k \u03b4 2 u) pure merge requests is less than .\nProof We first consider the pure merge requests involving points from some ground-truth cluster C\u2217i , the total number of pure merge requests (involving any ground-truth cluster) can then be bounded with a union-bound.\nTo facilitate our argument, let us assign an identifier to each cluster containing points from C\u2217i in the following manner:\n1. Maintain a CLUSTER-ID variable, which is initialized to 1.\n2. To assign a \u201cnew\u201d identifier to a cluster, set its identifier to CLUSTER-ID, and increment CLUSTER-ID.\n3. In the initial clustering, assign a new identifier to each cluster containing points from C\u2217i .\n4. When we split a cluster containing points from C\u2217i , assign its identifier to the newly-formed cluster containing points from C\u2217i .\n5. When we merge two clusters and one or both of them are impure, if one of the clusters contains points from C\u2217i , assign its identifier to the newly-formed cluster containing points from C\u2217i . If both clusters contain points from C \u2217 i , assign a new identifier to the newly-formed\ncluster containing points from C\u2217i .\n6. When we merge two clustersC1 andC2, and both contain only points fromC\u2217i , if the outcome is one new cluster, assign it a new identifier. If the outcome is two new clusters, assign them the identifiers of C1 and C2.\nClearly, when clusters containing points from C\u2217i are assigned identifiers in this manner, the maximum value of CLUSTER-ID is bounded by O(\u03b4i), where \u03b4i denotes the underclustering error of the initial clustering with respect to C\u2217i : \u03b4i = dist(C \u2217 i , C). To verify this, consider that we assign exactly \u03b4i + 1 new identifiers in Step-3, and each time we assign a new identifier in Steps 5 and 6, the underclustering error of the edited clustering with respect to C\u2217i decreases by one.\nWe say that a pure merge request involving points fromC\u2217i is original if the user has never asked us to merge clusters with the given identifiers, otherwise we say that this merge request is repeated. Given that the maximum value of CLUSTER-ID is bounded by O(\u03b4i), the total number of original merge requests must be O(\u03b42i ). We now argue that if a merge request is not original, we can lower bound the probability that it will result in the merging of the two clusters.\nFor repeated merge request Mi = Merge(C1, C2), let Xi be a random variable defined as follows:\nXi =  1 if neither C1 nor C2 have been involved in a merge request since the last time a merge of clusters with these identifiers was proposed.\n0 otherwise.\nClearly, when Xi = 1 it must be the case that C1 and C2 are merged. We observe that Pr[Xi = 1] > 12\u03b4i+1 . To verify this, observe that in each step the probability that the user requests to merge C1 and C2 is 1m , and the probability that the user requests to merge C1 or C2 with some other cluster is less than 2\u03b4im , where m is the total number of possible merge requests; we can then bound the probability that the former happens before the latter.\nWe can then use a Chernoff bound to argue that after t = O(log k \u03b4 2 i ) repeated merge requests, the probability that \u2211t\ni=1Xi < \u03b4i (which must be true if we need more repeated merge requests) is less than /k. Therefore, the probability that we need more than O(log k \u03b4\n2 i ) repeated merge\nrequests is less than /k. By the union-bound, the probability that we need more than O(log k \u03b4\n2 i ) repeated merge re-\nquests for any ground-truth cluster C\u2217i is less than k \u00b7 /k = . Therefore with probability at least 1 \u2212 for all ground-truth clusters we need \u2211 iO(log k \u03b4 2 i ) = O(log k \u2211 i \u03b4 2 i ) = O(log k \u03b4 2 u) repeated merge requests, where \u03b4u is the underclustering error of the original clustering. Similarly, for all ground-truth clusters we need \u2211 iO(\u03b4 2 i ) = O(\u03b4 2 u) original merge requests. Adding the two terms together, it follows that with probability at least 1 \u2212 we need a total of O(log k \u03b4 2 u) pure merge requests.\nAs in the previous section, we also restate the run-time bound in Theorem 21 in terms of any natural clustering error \u03b3. The following collorary follows from Theorem 21 and Theorem 9.\nCorollary 26 Suppose the target clustering satisfies stability, and the initial clustering has clustering error \u03b3, where \u03b3 is any natural clustering error as defined in Definition 7. In the unrestrictedmerge model, with probability at least 1 \u2212 , the algorithms in Figure 1 and Figure 7 require O(log k \u03b3 2) edit requests to find the target clustering.\nAs in the previous section, if the data satisfies strcit separation, then instead of the split procedure in Figure 1 we can use the procedure in Figure 4, which uses the local average-linkage tree (constructed from only the points in the user request). We can then obtain the same performance guarantee as in Theorem 21 for the algorithms in Figure 4 and Figure 7."}, {"heading": "5. Experimental Results", "text": "We perform two sets of experiments: we first test the proposed split procedure on the clustering of business listings maintained by Google, and also test the proposed framework in its entirety on the much smaller newsgroup documents data set."}, {"heading": "5.1 Clustering business listings", "text": "Google maintains a large collection of data records representing businesses. These records are clustered using a similarity function; each cluster should contain records about the same distinct\nbusiness; each cluster is summarized and served to users online via various front-end applications. Users report bugs such as \u201cyou are displaying the name of one business, but the address of another\u201d (caused by over-clustering), or \u201ca particular business is shown multiple times\u201d (caused by underclustering). These bugs are routed to operators who examine the contents of the corresponding clusters, and request splits/merges accordingly. The clusters involved in these requests may be quite large and usually contain records about several businesses. Therefore automated tools that can perform the requested edits are very helpful.\nIn particular, here we evaluate the effectiveness of our proposed split procedure in computing correct cluster splits. We consider a binary split correct if the two resulting sub-clusters are \u201cclean\u201d using Definition 13, and consider the split incorrect otherwise. Note that a clean split is sufficient and necessary for reducing the under/overclustering error. To compute the splits, we use the algorithm in Figure 4, which we refer to as Clean-Split. This algorithm is easier to implement and run than the algorithm in Figure 1 because we do not need to compute the global average-linkage tree. But it is still provably correct under stronger assumptions on the data (see Theorem 17 and Theorem 18).\nFor comparison purposes, we use two well-known techniques for computing binary splits: the optimal 2-median clustering (2-Median), and a \u201csweep\u201d of the second-smallest eigenvector of the corresponding Laplacian matrix. Let {v1, . . . , vn} be the order of the vertices when sorted by their eigenvector entries, we compute the partition {v1, . . . , vi} and {vi+1, . . . , vn} such that its conductance is smallest (Spectral-Balanced), and a partition such that the similarity between vi and vi+1 is smallest (Spectral-Gap).\nWe compare the split procedures on 20 over-clusters that were discovered during a clusteringquality evaluation2. The results are presented in Table 1. We observe that the Clean-Split algorithm works best, giving a correct split in 19 out of the 20 cases. The well-known Spectral-Balanced technique usually does not give correct splits for this application. The balance constraint usually causes it to put records about the same business on both sides of the partition (especially when all the \u201cclean\u201d splits are not well-balanced), which increases clustering error. As expected, the Spectral-Gap technique improves on this limitation (because it does not have a balance constraint), but the result often still increases clustering error. The 2-Median algorithm performs fairly well, but it may not be the right technique for this problem: the optimal centers may correspond to listings about the same business, and even if they represent distinct businesses, the resulting partition is still sometimes incorrect.\nIn addition to using the clean-split criterion, we also evaluate the computed splits using the correlation-clustering (cc) error. We find that using this criterion Clean-Split and 2-Median compute the best splits, while the other two algorithms perform significantly worse. The results for CleanSplit and 2-Median are presented in Table 2. Note that a clean split is sufficient to reduce the correlation-clustering error, but it is not necessary. Our experiments illustrate these observations: Clean-Split makes progress in reducing the cc-error in 19 out of 20 cases (when the resulting split\n2. the data set is available at voevodski.org/data/businessListingsDatasets/description.html."}, {"heading": "4 -117 -117", "text": "is clean), while 2-Median is able to still reduce the cc-error even when the resulting split is not clean. Overall, in 12 instances the two algorithms give a tie in performance; in 4 instances CleanSplit makes more progress in reducing the correlation-clustering error; and in 4 instances 2-Median makes more progress. Also note that Clean-Split fails to reduce the cc-error only once; while 2- Median fails to reduce the cc-error 4 times."}, {"heading": "5.2 Clustering newsgroup documents", "text": "In order to test our entire framework (the iterative application of our algorithms), we perform computational experiments on newsgroup documents data.3 The objects in these data sets are posts to twenty different online forums (newsgroups). We sample these data to compute 5 data sets of manageable size (containing 276-301 elements), which are labeled A through E in the figures. Each data set contains some documents from every newsgroup.\nEach post/document is represented by a term frequency - inverse document frequency (tf-idf) vector Salton and Buckley (1988). We use cosine similarity to compare these vectors, which gives a similarity measure between 0 and 1 (inclusive). We compute an initial clustering by using the\n3. http://people.csail.mit.edu/jrennie/20Newsgroups/\nfollowing procedure to perturb the ground-truth: for each document we keep its ground-truth cluster assignment with probability 0.5, and otherwise reassign it to one of the other clusters, which is chosen uniformly at random.\nIn each iteration, we compute the set of all feasible splits and merges: a split of a cluster is feasible if it contains points from 2 or more ground-truth clusters, and a merge is feasible if at least an \u03b7- fraction of points in each cluster are from the same ground-truth cluster. Then, we choose one of the feasible edits uniformly at random, and ask the algorithm to compute the corresponding edit. We continue this process until we find the ground-truth clustering or we reach 20000 iterations. Note that for the \u03b7-merge model, our theoretical analysis is applicable to any edit-request sequence, but in our experiments for simplicity we still select a feasible edit uniformly at random.\nOur initial clusterings have over-clustering error of about 100, under-clustering error of about 100; and correlation-clustering error of about 5000.\nWe notice that for newsgroup documents it is difficult to compute average-linkage trees that are very consistent with the ground-truth. This observation was also made in other clustering studies that report that the hierarchical trees constructed from these data have low purity Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005). These observations suggest that these data are quite challenging for clustering algorithms. To test how well our algorithms can perform with better data, we prune the data sets by repeatedly finding the outlier in each target cluster and removing it, where the outlier is the point with minimum sum-similarity to the other points in the target cluster. For each data set, we perform experiments with the original (unpruned) data set, a pruned data set with 2 points removed per target cluster, and a pruned data set with 4 points removed per target cluster, which prunes 40 and 80 points, respectively (given that we have 20 target clusters).\n5.2.1 EXPERIMENTS IN THE \u03b7-MERGE MODEL\nWe first experiment with local clustering algorithms in the \u03b7-restricted merge setting. Here we use the algorithm in Figure 1 to perform the splits, and the algorithm in Figure 2 to perform the merges. We show the results of running our algorithm on data set A in Figure 8. The complete experimental results are in the Apppendix. We find that for larger settings of \u03b7, the number of edit requests (necessary to find the target clustering) is very favorable and is consistent with our theoretical analysis. The results are better for pruned datasets, where we get very good performance\nregardless of the setting of \u03b7. The results for algorithms in Figure 1 and Figure 3 (for the correlationclustering objective) are very favorable as well."}, {"heading": "5.2.2 EXPERIMENTS IN THE UNRESTRICTED-MERGE MODEL", "text": "We also experiment with algorithms in the unrestricted merge model. Here we use the same algorithm to perform the splits, but use the algorithm in Figure 7 to perform the merges. We show the results on dataset A in Figure 9. The complete experimental results are in the Apppendix. We find that for larger settings of \u03b7 our results are better than our theoretic analysis (we only show results for \u03b7 \u2265 0.5), and performance improves further for pruned datasets. Our investigations show that for unpruned datasets and smaller settings of \u03b7, we are still able to quickly get close to the target clustering, but the algorithms are not able to converge to the target due to inconsistencies in the average-linkage tree. We can address some of these inconsistencies by constructing the tree in a more robust way, which indeed gives improved performance for unpruned data sets."}, {"heading": "5.2.3 EXPERIMENTS WITH SMALL INITIAL ERROR", "text": "We also consider a setting where the initial clustering is already very accurate. In order to simulate this scenario, when we compute the initial clustering, for each document we keep its ground-truth cluster assignment with probability 0.95, and otherwise reassign it to one of the other clusters, which is chosen uniformly at random. This procedure usually gives us initial clusterings with overclustering and under-clustering error between 5 and 20, and correlation-clustering error between 500 and 1000. As expected, in this setting our interactive algorithms perform much better, especially on pruned data sets. Figure 10 displays the results; we can see that in these cases it often takes less than one hundred edit requests to find the target clustering in both models."}, {"heading": "5.2.4 IMPROVED PERFORMANCE USING A ROBUST AVERAGE-LINKAGE TREE", "text": "When we investigate the inconsistencies in the average linkage trees, we observe that there are \u201coutlier\u201d points that are attached near the root of the tree, which are incorrectly split off and remerged by the algorithm without making any progress towards finding the target clustering.\nWe can address these outliers by constructing the average-linkage tree in a more robust way: first find groups (\u201cblobs\u201d) of similar points of some minimum size, compute an average-linkage tree for each group, and then merge these trees using average-linkage. The tree constructed in such fashion may then be used by our algorithms.\nWe tried this approach, using Algorithm 2 from Balcan and Gupta (2010) to compute the \u201cblobs\u201d. We find that using the robust average-linkage tree gives better performance for the unpruned data sets, but gives no gains for the pruned data sets. Figure 11 displays the comparison for the five unpruned data sets. For the pruned data sets, it\u2019s likely that the robust tree and the standard tree are very similar, which explains why there is little difference in performance (results not shown)."}, {"heading": "6. Discussion", "text": "In this work we motivated and studied a new framework and algorithms for interactive clustering. Our framework models practical constraints on the algorithms: we start with an initial clustering that we cannot modify arbitrarily, and are only allowed to make local edits consistent with user requests. In this setting, we develop several simple, yet effective algorithms under different assumptions about the nature of the edit requests and the structure of the data. We present theoretical analysis that shows that our algorithms converge to the target clustering after a small number of edit requests. We also present experimental evidence that shows that our algorithms work well in practice.\nSeveral directions come out of this work. It would be interesting to relax the condition on \u03b7 in the \u03b7-merge model, and the assumption about the request sequences in the unrestricted-merge model. It is important to study additional properties of an interactive clustering algorithm. In particular, it is often desirable that the algorithm never increase the error of the current clustering. Our algorithms in Figures 1, 3 and 7 have this property, but the algorithm in Figure 2 does not."}, {"heading": "Appendix A. Complete Experimental Results", "text": "The following figures show the complete experimental results for all the algorithms. Figure 12 and Figure 13 give the results in the \u03b7-merge model. Figure 14 and Figure 15 give the results in the \u03b7-merge model for the algorithms in Figure 1 and Figure 3 (for the correlation-clustering objective). Figure 16 and Figure 17 give the results in the unrestricted-merge model.\nData Set A\nData Set B\nData Set C\nData Set A\nData Set B\nData Set C\nData Set A\nData Set B\nData Set C"}], "references": [{"title": "On spectral learning of mixtures of distributions", "author": ["D. Achlioptas", "F. McSherry"], "venue": "In Proceedings of the 18th Annual Conference on Learning Theory,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2005\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2005}, {"title": "Queries and concept learning", "author": ["D. Angluin"], "venue": "Machine Learning,", "citeRegEx": "Angluin.,? \\Q1998\\E", "shortCiteRegEx": "Angluin.", "year": 1998}, {"title": "Learning mixtures of arbitrary Gaussians", "author": ["S. Arora", "R. Kannan"], "venue": "In Proceedings of the 33rd ACM Symposium on Theory of Computing,", "citeRegEx": "Arora and Kannan.,? \\Q2001\\E", "shortCiteRegEx": "Arora and Kannan.", "year": 2001}, {"title": "Supervised clustering", "author": ["Pranjal Awasthi", "Reza Bosagh Zadeh"], "venue": "In NIPS,", "citeRegEx": "Awasthi and Zadeh.,? \\Q2010\\E", "shortCiteRegEx": "Awasthi and Zadeh.", "year": 2010}, {"title": "Clustering with interactive feedback", "author": ["Maria-Florina Balcan", "Avrim Blum"], "venue": "In ALT,", "citeRegEx": "Balcan and Blum.,? \\Q2008\\E", "shortCiteRegEx": "Balcan and Blum.", "year": 2008}, {"title": "Robust hierarchical clustering", "author": ["Maria-Florina Balcan", "Pramod Gupta"], "venue": "In COLT,", "citeRegEx": "Balcan and Gupta.,? \\Q2010\\E", "shortCiteRegEx": "Balcan and Gupta.", "year": 2010}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "In Proceedings of the 40th annual ACM symposium on Theory of computing,", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Correlation clustering", "author": ["Nikhil Bansal", "Avrim Blum", "Shuchi Chawla"], "venue": "Machine Learning,", "citeRegEx": "Bansal et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2004}, {"title": "Active semisupervision for pairwise constrained clustering", "author": ["Sugato Basu", "A. Banjeree", "ER. Mooney", "Arindam Banerjee", "Raymond J. Mooney"], "venue": "Proceedings of the 2004 SIAM International Conference on Data Mining (SDM-04,", "citeRegEx": "Basu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2004}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In FOCS,", "citeRegEx": "Belkin and Sinha.,? \\Q2010\\E", "shortCiteRegEx": "Belkin and Sinha.", "year": 2010}, {"title": "Combining multiple clustering systems", "author": ["Constantinos Boulis", "Mari Ostendorf"], "venue": "European conference on Principles and Practice of Knowledge Discovery in Databases(PKDD),", "citeRegEx": "Boulis and Ostendorf.,? \\Q2004\\E", "shortCiteRegEx": "Boulis and Ostendorf.", "year": 2004}, {"title": "Isotropic PCA and affine-invariant clustering", "author": ["S. Charles Brubaker", "Santosh Vempala"], "venue": "CoRR, abs/0804.3575,", "citeRegEx": "Brubaker and Vempala.,? \\Q2008\\E", "shortCiteRegEx": "Brubaker and Vempala.", "year": 2008}, {"title": "A structured family of clustering and tree construction methods", "author": ["David Bryant", "Vincent Berry"], "venue": "Adv. Appl. Math.,", "citeRegEx": "Bryant and Berry.,? \\Q2001\\E", "shortCiteRegEx": "Bryant and Berry.", "year": 2001}, {"title": "Bayesian maximum margin clustering", "author": ["Bo Dai", "Baogang Hu", "Gang Niu"], "venue": "In Proceedings of the 2010 IEEE International Conference on Data Mining, ICDM", "citeRegEx": "Dai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2010}, {"title": "Learning mixtures of Gaussians", "author": ["S. Dasgupta"], "venue": "In Proceedings of the 40th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "Hierarchical sampling for active learning", "author": ["Sanjoy Dasgupta", "Daniel Hsu"], "venue": "In ICML,", "citeRegEx": "Dasgupta and Hsu.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Hsu.", "year": 2008}, {"title": "Bayesian hierarchical clustering", "author": ["Katherine A. Heller", "Zoubin Ghahramani"], "venue": "In ICML,", "citeRegEx": "Heller and Ghahramani.,? \\Q2005\\E", "shortCiteRegEx": "Heller and Ghahramani.", "year": 2005}, {"title": "Efficiently learning mixtures of two Gaussians", "author": ["Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant"], "venue": "In STOC,", "citeRegEx": "Kalai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2010}, {"title": "The spectral method for general mixture models", "author": ["R. Kannan", "H. Salmasian", "S. Vempala"], "venue": "In Proceedings of the 18th Annual Conference on Learning Theory,", "citeRegEx": "Kannan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2005}, {"title": "Efficient active algorithms for hierarchical clustering", "author": ["Akshay Krishnamurthy", "Sivaraman Balakrishnan", "Min Xu", "Aarti Singh"], "venue": null, "citeRegEx": "Krishnamurthy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2012}, {"title": "Comparing clusterings - an information based distance", "author": ["Marina Meil\u0103"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Meil\u0103.,? \\Q2007\\E", "shortCiteRegEx": "Meil\u0103.", "year": 2007}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In FOCS,", "citeRegEx": "Moitra and Valiant.,? \\Q2010\\E", "shortCiteRegEx": "Moitra and Valiant.", "year": 2010}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Gerard Salton", "Christopher Buckley"], "venue": "Information processing and management,", "citeRegEx": "Salton and Buckley.,? \\Q1988\\E", "shortCiteRegEx": "Salton and Buckley.", "year": 1988}, {"title": "Active clustering of biological sequences", "author": ["Konstantin Voevodski", "Maria-Florina Balcan", "Heiko R\u00f6glin", "Shang-Hua Teng", "Yu Xia"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Voevodski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Voevodski et al\\.", "year": 2012}, {"title": "Generative model-based document clustering: a comparative study", "author": ["Shi Zhong"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Zhong.,? \\Q2005\\E", "shortCiteRegEx": "Zhong.", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al.", "startOffset": 119, "endOffset": 150}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al.", "startOffset": 119, "endOffset": 172}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al.", "startOffset": 119, "endOffset": 189}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al.", "startOffset": 119, "endOffset": 214}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al.", "startOffset": 119, "endOffset": 243}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al. (2010); Moitra and Valiant (2010); Belkin and Sinha (2010).", "startOffset": 119, "endOffset": 264}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al. (2010); Moitra and Valiant (2010); Belkin and Sinha (2010).", "startOffset": 119, "endOffset": 291}, {"referenceID": 0, "context": "For example, there is a large body of work that focuses on clustering data that is generated by a mixture of Gaussians Achlioptas and McSherry (2005); Kannan et al. (2005); Dasgupta (1999); Arora and Kannan (2001); Brubaker and Vempala (2008); Kalai et al. (2010); Moitra and Valiant (2010); Belkin and Sinha (2010). Although this helps define the \u201cright\u201d clustering one should be looking for, real-world data rarely comes from such well-behaved probabilistic models.", "startOffset": 119, "endOffset": 316}, {"referenceID": 7, "context": "We also develop algorithms for the well-known correlation-clustering objective function Bansal et al. (2004), which considers pairs of points that are clustered inconsistently with respect to the target clustering (See Section 2).", "startOffset": 88, "endOffset": 109}, {"referenceID": 9, "context": "We also test the entire proposed framework on newsgroup documents data, which is quite challenging for traditional unsupervised clustering methods Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005); Dasgupta and Hsu (2008); Dai et al.", "startOffset": 161, "endOffset": 177}, {"referenceID": 9, "context": "We also test the entire proposed framework on newsgroup documents data, which is quite challenging for traditional unsupervised clustering methods Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005); Dasgupta and Hsu (2008); Dai et al.", "startOffset": 161, "endOffset": 207}, {"referenceID": 9, "context": "We also test the entire proposed framework on newsgroup documents data, which is quite challenging for traditional unsupervised clustering methods Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005); Dasgupta and Hsu (2008); Dai et al.", "startOffset": 161, "endOffset": 232}, {"referenceID": 9, "context": "We also test the entire proposed framework on newsgroup documents data, which is quite challenging for traditional unsupervised clustering methods Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005); Dasgupta and Hsu (2008); Dai et al. (2010); Boulis and Ostendorf (2004); Zhong (2005).", "startOffset": 233, "endOffset": 251}, {"referenceID": 7, "context": "(2010); Boulis and Ostendorf (2004); Zhong (2005).", "startOffset": 8, "endOffset": 36}, {"referenceID": 7, "context": "(2010); Boulis and Ostendorf (2004); Zhong (2005). Still, we find that our algorithms perform fairly well; for larger settings of \u03b7 we are able find the target clustering after a limited number of edit requests.", "startOffset": 8, "endOffset": 50}, {"referenceID": 2, "context": "Related work Interactive models for clustering studied in previous works Balcan and Blum (2008); Awasthi and Zadeh (2010) were inspired by an analogous model for learning under feedback Angluin (1998).", "startOffset": 73, "endOffset": 96}, {"referenceID": 2, "context": "Related work Interactive models for clustering studied in previous works Balcan and Blum (2008); Awasthi and Zadeh (2010) were inspired by an analogous model for learning under feedback Angluin (1998).", "startOffset": 97, "endOffset": 122}, {"referenceID": 1, "context": "Related work Interactive models for clustering studied in previous works Balcan and Blum (2008); Awasthi and Zadeh (2010) were inspired by an analogous model for learning under feedback Angluin (1998). In this model, the algorithm can propose a hypothesis to the user (in this case, a clustering of the data) and get some feedback regarding the correctness of the current hypothesis.", "startOffset": 186, "endOffset": 201}, {"referenceID": 7, "context": "Basu et al. Basu et al. (2004) study the problem of minimizing the k-means objective in the presence of limited supervision.", "startOffset": 0, "endOffset": 31}, {"referenceID": 6, "context": "The stability property that we consider is a natural generalization of the \u201cstable marriage\u201d property (see Definition 2) that has been studied in a variety of previous works Balcan et al. (2008); Bryant and Berry (2001).", "startOffset": 174, "endOffset": 195}, {"referenceID": 6, "context": "The stability property that we consider is a natural generalization of the \u201cstable marriage\u201d property (see Definition 2) that has been studied in a variety of previous works Balcan et al. (2008); Bryant and Berry (2001). It is the weakest among the stability properties that have been studied recently such as strict separation and strict threshold separation Balcan et al.", "startOffset": 174, "endOffset": 220}, {"referenceID": 6, "context": "The stability property that we consider is a natural generalization of the \u201cstable marriage\u201d property (see Definition 2) that has been studied in a variety of previous works Balcan et al. (2008); Bryant and Berry (2001). It is the weakest among the stability properties that have been studied recently such as strict separation and strict threshold separation Balcan et al. (2008); Krishnamurthy et al.", "startOffset": 174, "endOffset": 381}, {"referenceID": 6, "context": "The stability property that we consider is a natural generalization of the \u201cstable marriage\u201d property (see Definition 2) that has been studied in a variety of previous works Balcan et al. (2008); Bryant and Berry (2001). It is the weakest among the stability properties that have been studied recently such as strict separation and strict threshold separation Balcan et al. (2008); Krishnamurthy et al. (2012). This property is known to hold for real-world data.", "startOffset": 174, "endOffset": 410}, {"referenceID": 6, "context": "The stability property that we consider is a natural generalization of the \u201cstable marriage\u201d property (see Definition 2) that has been studied in a variety of previous works Balcan et al. (2008); Bryant and Berry (2001). It is the weakest among the stability properties that have been studied recently such as strict separation and strict threshold separation Balcan et al. (2008); Krishnamurthy et al. (2012). This property is known to hold for real-world data. In particular, Voevodski et al. (2012) observed that this property holds for protein sequence data, where similarities are computed with sequence alignment and ground truth clusters correspond to evolutionary-related proteins.", "startOffset": 174, "endOffset": 502}, {"referenceID": 6, "context": "In addition, we also study the following stronger properties of a clustering, which were first introduced in Balcan et al. (2008).", "startOffset": 109, "endOffset": 130}, {"referenceID": 20, "context": "For additional discussion about comparing clusterings see Meil\u0103 (2007). Note that several criteria discussed in Meil\u0103 (2007) satisfy our first two properties (for a similarity measure we may replace \u201dmust decrease the error\u201d with \u201dmust increase the similarity\u201d).", "startOffset": 58, "endOffset": 71}, {"referenceID": 20, "context": "For additional discussion about comparing clusterings see Meil\u0103 (2007). Note that several criteria discussed in Meil\u0103 (2007) satisfy our first two properties (for a similarity measure we may replace \u201dmust decrease the error\u201d with \u201dmust increase the similarity\u201d).", "startOffset": 58, "endOffset": 125}, {"referenceID": 20, "context": "For additional discussion about comparing clusterings see Meil\u0103 (2007). Note that several criteria discussed in Meil\u0103 (2007) satisfy our first two properties (for a similarity measure we may replace \u201dmust decrease the error\u201d with \u201dmust increase the similarity\u201d). In addition, the Rand and Mirkin criteria discussed in Meil\u0103 (2007) are closely related to the correlation clustering error defined here (all three measures are a function of the number of pairs of points that are clustered incorrectly).", "startOffset": 58, "endOffset": 331}, {"referenceID": 6, "context": "Proof The proof of this statement can be found in Balcan et al. (2008). The intuition is that if there is a node in T that is not laminar w.", "startOffset": 50, "endOffset": 71}, {"referenceID": 6, "context": "In particular, if the data satisfies the strict separation property from Balcan et al. (2008), we may change the split and merge algorithms to use the local averagelinkage tree, which is constructed from only the points in the edit request.", "startOffset": 73, "endOffset": 94}, {"referenceID": 3, "context": "This assumption is consistent with the observation in Awasthi and Zadeh (2010) that in the unrestricted-merge model with arbitrary request sequences, even very simple cases (ex.", "startOffset": 54, "endOffset": 79}, {"referenceID": 22, "context": "Each post/document is represented by a term frequency - inverse document frequency (tf-idf) vector Salton and Buckley (1988). We use cosine similarity to compare these vectors, which gives a similarity measure between 0 and 1 (inclusive).", "startOffset": 99, "endOffset": 125}, {"referenceID": 14, "context": "This observation was also made in other clustering studies that report that the hierarchical trees constructed from these data have low purity Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005).", "startOffset": 157, "endOffset": 173}, {"referenceID": 14, "context": "This observation was also made in other clustering studies that report that the hierarchical trees constructed from these data have low purity Telgarsky and Dasgupta (2012); Heller and Ghahramani (2005). These observations suggest that these data are quite challenging for clustering algorithms.", "startOffset": 157, "endOffset": 203}, {"referenceID": 5, "context": "We tried this approach, using Algorithm 2 from Balcan and Gupta (2010) to compute the \u201cblobs\u201d.", "startOffset": 47, "endOffset": 71}], "year": 2015, "abstractText": "We study the design of interactive clustering algorithms for data sets satisfying natural stability assumptions. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable features in many applications. We show that in this constrained setting one can still design provably efficient algorithms that produce accurate clusterings. We also show that our algorithms perform well on real-world data.", "creator": "LaTeX with hyperref package"}}}