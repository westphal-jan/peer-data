{"id": "1312.7606", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2013", "title": "Distributed Policy Evaluation Under Multiple Behavior Strategies", "abstract": "we apply diffusion strategies i develop fully - distributed cooperative synchronized learning solutions, therefore all actors in infinite network communicate only and all external environment versus reject predictions about existing environment. optimization algorithms can also thus relegated to off - fault sampling, meaning whereas the agents still learn good predict item function by a behavior based following the actual policies they are traveling. the proposed incentive strategy feels efficient, with marginal complexity in both computation requirements than memory flows. let provide a mean - square - range performance analysis will establish convergence under constant data - size bounds, which endow the network with continuous learning capabilities. the results show a clear gain behind cooperation : perhaps the best agents can estimate the solution, cooperate considers stability and mutual bias and variance of the prediction bias ; but, second importantly, greedy network agent helping to approach partially optimal solution that when none of pure individual agents could ( e. g., while performing usual behavior policies restrict each agent shall sample a small portion of the state space ).", "histories": [["v1", "Mon, 30 Dec 2013 00:16:34 GMT  (1232kb)", "https://arxiv.org/abs/1312.7606v1", null], ["v2", "Wed, 5 Nov 2014 19:50:03 GMT  (2971kb)", "http://arxiv.org/abs/1312.7606v2", "36 pages, 4 figures, accepted for publication on IEEE Transactions on Automatic Control"]], "reviews": [], "SUBJECTS": "cs.MA cs.AI cs.DC cs.LG", "authors": ["sergio valcarcel macua", "jianshu chen", "santiago zazo", "ali h sayed"], "accepted": false, "id": "1312.7606"}, "pdf": {"name": "1312.7606.pdf", "metadata": {"source": "CRF", "title": "Distributed Policy Evaluation Under Multiple Behavior Strategies", "authors": ["Sergio Valcarcel"], "emails": ["sergio@gaps.ssr.upm.es;", "santiago@gaps.ssr.upm.es).", "cjs09@ucla.edu;", "sayed@ee.ucla.edu)."], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n76 06\nv2 [\ncs .M\nA ]\n5 N\nov 2\nWe apply diffusion strategies to develop a fully-distributed cooperative reinforcement learning algorithm in which agents in a network communicate only with their immediate neighbors to improve predictions about their environment. The algorithm can also be applied to off-policy learning, meaning that the agents can predict the response to a behavior different from the actual policies they are following. The proposed distributed strategy is efficient, with linear complexity in both computation time and memory footprint. We provide a mean-square-error performance analysis and establish convergence under constant step-size updates, which endow the network with continuous learning capabilities. The results show a clear gain from cooperation: when the individual agents can estimate the solution, cooperation increases stability and reduces bias and variance of the prediction error; but, more importantly, the network is able to approach the optimal solution even when none of the individual agents can (e.g., when the individual behavior policies restrict each agent to sample a small portion of the state space).\nIndex Terms\nAdaptive networks, Arrow-Hurwicz algorithm, diffusion strategies, distributed processing, gradient\ntemporal difference, mean-square-error, reinforcement learning, saddle-point problem\nThis work was supported in part by the Spanish Ministry of Science and Innovation in the program CONSOLIDER-INGENIO 2010 under the Grant CSD2008-00010 COMONSENS and by the NSF grants CCF-1011918 and ECCS-1407712. A short preliminary version dealing with a special case of this work appears in the conference publication [1].\nS. V. Macua and S. Zazo are with the Department of Signals, Systems and Radiocommunications, Escuela Te\u0301cnica Superior de Ingenieros de Telecomunicaio\u0301n, Universidad Polite\u0301cnica de Madrid, Madrid 28040, Spain (e-mail: sergio@gaps.ssr.upm.es; santiago@gaps.ssr.upm.es).\nJ. Chen and A. H. Sayed are with the Department of Electrical Engineering, University of California, Los Angeles, CA 90095 USA (e-mail: cjs09@ucla.edu; sayed@ee.ucla.edu).\nNovember 6, 2014 DRAFT\n2 I. INTRODUCTION\nConsider the problem in which a network of autonomous agents collaborate to predict the response of the environment to their actions. The network forms a connected graph, where there is at least one path between every pair of nodes. The agents learn locally from their individual interactions with the environment and share knowledge with their neighbors. Only direct neighborhood communication is allowed. We assume the environment can be modeled as a Markov decision process. The agents do not have access to the actual state of the environment, but just to feature vectors representing it. The feature representation is convenient in problems with very large state dimensions since it is computationally more efficient to work with features of smaller dimension than the size of the original state-space.\nIn the scenario under study in this work, every agent takes actions according to an individual policy, which is possibly different from that of every other agent. The objective of the agents is to assess the response of the environment to a common hypothetical behavior, the target policy, which is the same for every agent but different from the actual behavior policies they are following. This problem of predicting the response to a target policy different from the behavior policy is commonly referred as off-policy learning [2]. Off-policy learning has been claimed to be necessary when the agents need to perform tasks in complex environments because they could perform many different predictions in parallel from a single stream of data [3]\u2013[5].\nThe predictions by the agents are made in the form of value functions [2], [6], [7]. The gradienttemporal-difference (GTD) algorithm is one useful method for computing approximate value functions. It was originally proposed for the single agent scenario in [8], [9], and derived by means of the stochastic optimization of a suitable cost function. The main advantages of this single-agent GTD are its low complexity and its convergence guarantees (for diminishing step-sizes) under the off-policy setting. In Section III of this work we apply diffusion strategies to develop a distributed GTD algorithm that extends the single-agent GTD to multi-agent networks. There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18]. Consensus strategies have been successfully applied to the solution of static optimization problems, where the objective does not drift with time. They have been studied largely under diminishing step-size conditions to ensure agreement among cooperating agents. Diffusion strategies, on the other hand, have been proved to be particularly apt at endowing networks with continuous adaptation and learning abilities to enable tracking of drifting conditions. There are several forms of diffusion; recent overviews appear in [19]\u2013[21]. It has been shown in [22] that the dynamics of diffusion networks leads to enhanced stability and lower mean-\nNovember 6, 2014 DRAFT\n3 square-error (MSE) than consensus networks. In particular, the analysis in [20]\u2013[22] shows that consensus networks combine local data and in-neighborhood information asymmetrically, which can make the state of consensus networks grow unbounded even when all individual agents are mean stable in isolation. This behavior does not happen in diffusion networks, in which local and external information are symmetrically combined by construction, enhancing the stability of the network. For these reasons, we focus in the remainder of this article on the derivation of a diffusion strategy for GTD over multi-agent networks. As a byproduct of this derivation, we show that the GTD algorithm, motivated as a two time-scales stochastic approximation in [9], is indeed a stochastic Arrow-Hurwicz algorithm applied to the dual problem of the original formulation.\nThe convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23]. For a distributed algorithm, the analysis becomes more demanding because the estimation process at each node is influenced by the estimates at the other nodes, so the error propagates across the network. Another difficulty in the distributed case is that the agents may follow different behavior policies and, thus, their individual cost functions could have different minimizers. In Section IV, we will analyze the steady-state and transient behavior of the proposed distributed algorithm, deriving closed-form expressions that characterize the network performance for sufficiently small constant step-sizes. We employ constant, as opposed to decaying step-sizes, because we are interested in distributed solutions that are able to continually adapt and learn. The performance analysis will reveal that when the agents follow the same behavior policy, they will be able to find an unbiased estimator for the centralized solution. On the other hand, when the agents behave differently, they will approach, up to some bias, the solution of a convex combination of their individual problems. This bias is proportional to the step-size, so it becomes negligible when the step-size is sufficiently small. One important benefit that results when the agents behave differently is that, although the agents do not directly share their samples, the in-network experience becomes richer in a manner that the diffusion strategy is able to exploit. In particular, in the reinforcement learning literature, it is customary to assume that the behavior policy must allow the agents to visit every possible state infinitely often. We will relax this assumption and show that the distributed algorithm is able to perform well even when the individual agents only visit small portions of the state-space, as long as there are other agents that explore the remaining regions. Therefore, even though none of the agents can find the optimal estimate of the value function by itself, they can achieve it through cooperation. This is an interesting capability that emerges from the networked solution.\nIn this work, we consider a setting in which the agents can communicate with their neighbors, but they\nNovember 6, 2014 DRAFT\n4 operate without influencing each other. This setup is meaningful in many real applications. Consider, for example, a water purification plant controlled and monitored by a wireless actuator-sensor network, in which each device is attached to a different water-tank. The quality of the water (e.g., the amount of bacteria) in one tank will be influenced by the decisions (e.g., delivering some amount of chlorine) made by the device controlling that tank, independently of what other devices do. Still, since all water tanks behave similarly under similar circumstances, the devices in the network can benefit from sharing their individual knowledge."}, {"heading": "A. Related works", "text": "There are several insightful works in the literature that address issues pertaining to distributed learning albeit under different scenarios and conditions than what is studied in this article. For example, the work in [24] proposes a useful algorithm, named QD-learning, which is a distributed implementation of Q-learning using consensus-based stochastic approximation. The diffusion strategy proposed herein is different in several respects. QD-learning asymptotically solves the optimal control problem, learning the policy that maximizes the long-term reward of the agents. Here, we focus on predicting the long-term reward for a given policy, which is an important part of the control problem. However, QD-learning is developed in [24] under the assumption of perfect-knowledge of the state. Here, we study the case in which the agents only know a feature representation of the state, which is used to build a parametric approximation of the value function, allowing us to tackle large problems, for which Q-learning schemes can diverge [25], [26]. Finally, we enforce constant step-sizes in order to enable continuous adaptation and learning. In comparison, the analysis in [24] employs a diminishing step-size that dies out as time progresses and, therefore, turns off adaptation and is not able to track concept drifts in the data.\nAnother related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26]. In addition, although the algorithm in [27] is distributed, in the sense that there is no fusion center, it requires full connectivity (i.e., every node must be able to exchange information with every other node in the network), which is a restrictive assumption that prevents the algorithm from large-scale deployments. In this article, we focus on fully distributed solutions that only require the network of agents to be connected (but not necessarily fully connected). Other related\u2014but more heuristic\u2014approaches include [28], [29].\nNovember 6, 2014 DRAFT\n5"}, {"heading": "B. Notation", "text": "Lower case letters are used to denote both scalar values and vectors. Matrices are denoted by upper case letters. Boldface notation denotes random variables (e.g., s is a realization for s). The state of the environment and the action taken by an agent are denoted by s and a, respectively. With a slight abuse of notation, s(i) and a(i) denote the state and action variables at time i. Moreover, whenever a variable is specific to some agent k we add a subscript (e.g., sk(i) = s means that the environment seen by agent k is at state s at time i).\nAll vectors are column vectors. Superscript \u00b7\u22a4 denotes transposition. The identity matrix of size S is denoted by IS , the null matrix of size M \u00d7 L is denoted by 0M\u00d7L, and 1M and 0M stand for vectors of ones and zeros of length M , respectively. The Kronecker product operation is denoted by \u2297. The spectrum, m-th eigenvalue and spectral radius of a matrix are denoted by \u03bb(\u00b7), \u03bbm(\u00b7) and \u03c1(\u00b7), respectively. The operator col{\u00b7} stacks vectors (or matrices) into a long vector (or a tall matrix); while vec[\u00b7] stacks the columns of a matrix, one beneath the other, into a long vector. The operator diag{\u00b7} creates a diagonal matrix (a block-diagonal matrix) from a given vector (a set of square matrices). The Euclidean (semi)norm is given by \u2016y\u20162D , y\u22a4Dy, where D is a positive (semi)definite matrix. The expected value operator with respect to probability distribution d is denoted by Ed[\u00b7]; we use multiple sub-indexes (e.g., Ed,\u03c6,P [\u00b7]) when the expectation is taken with regard to multiple distributions."}, {"heading": "II. BELLMAN EQUATION AND VALUE FUNCTIONS", "text": ""}, {"heading": "A. Markov decision processes (MDP)", "text": "We consider Markov decision processes (MDP) [6], [7] that are characterized by a finite set of states S of size S , |S|; a finite set of actions A; the kernel of transition probabilities P(s\u2032|s, a), which gives the probability of going from one state s to another state s\u2032, given an action a; and the reward function r : S \u00d7A\u00d7 S \u2192 R that the agent wants to predict, which is associated with every transition, such that r (s, a, s\u2032) denotes the reward received by a generic agent for the transition from s to s\u2032 after taking action a.\nThe agents want to predict the response of their environment when they follow some stationary policy \u03c0, such that \u03c0(a|s) stands for the probability of an agent choosing action a when the environment is at state s. We assume that the finite-state Markov chain resulting from the MDP is irreducible and aperiodic under any policy of interest. Thus, it has a unique positive stationary probability distribution of visiting each state [6, App. A] [30] denoted by d\u03c0 = [d\u03c0(1), . . . , d\u03c0(S)]\u22a4, such that d\u03c0(s) > 0, for all 1 \u2264 s \u2264 S.\nNovember 6, 2014 DRAFT\n6 The state transition probabilities of the Markov chain, from initial state s to destination s\u2032 are given by\np\u03c0ss\u2032 , P ( s\u2032 | s ) = \u2211 a\u2208A P ( s\u2032|s, a ) \u03c0(a|s) (1)\nWe collect p\u03c0ss\u2032 into an S \u00d7 S matrix P \u03c0 as its (s, s\u2032)-th entry.\nB. Value function\nIn order to make predictions of the reward signal, we use state value functions, v : S \u2192 R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]\u2013[7]. This time window spans from i = 0 to i = \u221e, but it has an effective length controlled by a constant \u03b3 \u2208 (0, 1), which trades short-sighted (\u03b3 \u2192 0) vs. long-term planning (\u03b3 \u2192 1). The value function for target policy \u03c0, starting from some initial state s \u2208 S at time i, is defined as:\nv\u03c0(s) , E\u03c0,P\n[ \u221e\u2211\nt=1\n\u03b3t\u22121r(i+ t) \u2223\u2223\u2223\u2223\u2223 s(i) = s ]\n(2)\nwhere r(i + 1) , r(s(i),a(i), s(i + 1)), and the expectation is taken with regard to all possible statetransitions. Note that a(i) is random because it is drawn from a probability distribution \u03c0, which together with the probabilistic transition dictated by P, leads to a random future state s(i+1). Let s\u2032 denote the destination state after transitioning from s. Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]:\nv\u03c0(s) = E\u03c0,P [r(i+ 1) + \u03b3r(i+ 2) + . . . | s(i) = s]\n= E\u03c0,P [r(i+ 1) | s(i) = s] + \u03b3E\u03c0,P [r(i+ 2) + \u03b3r(i+ 3) + . . . | s(i) = s]\n= r\u03c0(s) + \u03b3E\u03c0,P\n[ \u221e\u2211\nt=1\n\u03b3t\u22121r(i+ 1 + t) | s(i) = s ]\n= r\u03c0(s) + \u03b3E\u03c0,P [ v\u03c0(s\u2032) | s(i+ 1) = s\u2032 ( ght] = r\u03c0(s) + \u03b3 \u2211\ns\u2032\u2208S\n\u2211 a\u2208A P ( s\u2032|s, a ) \u03c0(a|s)v\u03c0(s\u2032)\n= r\u03c0(s) + \u03b3 \u2211\ns\u2032\u2208S p\u03c0ss\u2032v\n\u03c0(s\u2032) (3)\nwhere r\u03c0(s) denotes the expected reward that can be collected over the next transition when the agent is currently at state s:\nr\u03c0(s) , E\u03c0,P [r(s,a, s\u2032)] = \u2211\na\u2208A \u03c0(a|s)\n\u2211 s\u2032\u2208S P(s\u2032|s, a)r(s, a, s\u2032) (4)\nNovember 6, 2014 DRAFT\n7 Let v\u03c0 and r\u03c0 be the vectors of length S that collect the values v\u03c0(s) and r\u03c0(s) for all s \u2208 S, respectively:\nv\u03c0 ,   v\u03c0(1) ...\nv\u03c0(S)\n  \u2208 RS , r\u03c0 ,   r\u03c0(1) ...\nr\u03c0(S)\n  \u2208 RS (5)\nThen, Eq. (3) can be written in vector form as the linear system of equations:\n(IS \u2212 \u03b3P \u03c0)v\u03c0 = r\u03c0 (6)\nWe shall refer to v\u03c0 as the value vector. There are two challenges when we aim to obtain v\u03c0 from (6). The first challenge is that the size of the state-space can be very large (e.g., the chess game has 1047 possible states, making (6) computationally intractable). The second challenge arises when the agents do not know anything about the environment, thus P \u03c0 and r\u03c0 are unavailable. In the following subsections we review how to address these two issues."}, {"heading": "C. Approximate value function as a saddle-point problem", "text": "For the single agent scenario, references [8], [9] introduced efficient algorithms with convergence guarantees under general conditions. The algorithms save on computations by relying on features that span a space of much lower dimensionality than the size of the original state space. More formally, let x : S \u2192 RM be some mapping from states to features, such that xs is the feature vector of length M \u226a S that represents the state s. Now, it would be efficient to approximate the original value function v\u03c0(s) as a parametric function of xs, for some parameter vector w \u2208 RM . When this is done, the problem of making a prediction (i.e., estimating the value vector v\u03c0) becomes equivalent to seeking a parameter vector w\u22c6 that is optimal in a certain sense. Among many parametrizations, a linear approximation of the form\nv\u03c0(s) \u2248 x\u22a4s w (7)\nhas been extensively studied in the literature (see, e.g., [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands. Moreover, it is expected that if one chooses the mapping of features carefully, then the linear approximation model will generally provide good results (see, e.g., [33]\u2013[38] ). Let X be the matrix of size S \u00d7 M formed by stacking the transposed feature\nNovember 6, 2014 DRAFT\n8 vectors, x\u22a4s , on top of each other:\nX ,   x\u22a41 ...\nx\u22a4S\n  \u2208 RS\u00d7M (8)\nThen, the linear approximation (7) can be expressed in vector form as:\nv\u03c0 \u2248 Xw (9)\nBy modeling the value function in the form (9), we solve for w by using the approximation (9) in (6). Doing so leads to the approximate Bellman equation:\nXw = r\u03c0 + \u03b3P \u03c0Xw (10)\nIn this paper, we assume that the features available for the agents constitute a linearly independent set of basis functions, which effectively represent the states. Thus, X is full rank by construction. However, the fixed point equation (10) may not have a solution w in general because the right-hand side need not lie in the range space of X, which we denote by X. To address this issue, one approach is to solve instead the projected Bellman equation [26]:\nXw = \u03a0(r\u03c0 + \u03b3P \u03c0Xw) (11)\nwhere \u03a0 is a projection operator onto X. Since X is a linear space, the projection operator with respect to some metric norm \u2016 \u00b7 \u2016D is defined as:\n\u03a0x , argmin x\u0304\u2208X\n\u2016x\u2212 x\u0304\u20162D (12)\nwhere D is a symmetric positive-definite matrix. The matrix \u03a0 is given by\n\u03a0 = X(X\u22a4DX)\u22121X\u22a4D (13)\nTherefore, for different choices of D, we have different projection operators. However, some choices for D will lead to simpler solutions, as we will reveal in Subsection II-D.\nEquation (11) is now an over-determined consistent linear system of equations. To solve for w, reference\nNovember 6, 2014 DRAFT\n9 [9] considered the weighted least-squares problem:\nminimize w\nJPB(w) , \u2016\u03a0(r\u03c0 + \u03b3P \u03c0Xw)\u2212Xw\u20162D (14)\nwhere the cost function JPB(w) is referred to as the projected Bellman error. Since Xw already lies in X and D is positive definite, it can be verified that\nJPB(w) = \u2016\u03a0r\u03c0 \u2212\u03a0(IS \u2212 \u03b3P \u03c0)Xw\u20162D\n= (r\u03c0 \u2212 (IS \u2212 \u03b3P \u03c0)Xw)\u22a4\u03a0\u22a4D\u03a0(r\u03c0 \u2212 (IS \u2212 \u03b3P \u03c0)Xw) = ( X\u22a4Dr\u03c0 \u2212Bw )\u22a4 ( X\u22a4DX )\u22121 ( X\u22a4Dr\u03c0 \u2212Bw ) (15)\nwhere B , X\u22a4D(IS \u2212 \u03b3P \u03c0)X. Using (15), it can also be verified that the solution w\u22c6 that minimizes JPB(w) satisfies the following normal equations [39]:\nB\u22a4(X\u22a4DX)\u22121Bw\u22c6 = B\u22a4(X\u22a4DX)\u22121X\u22a4Dr\u03c0 (16)\nSince \u2016P \u03c0\u2016\u221e = 1 and \u03b3 < 1, we can bound the spectral radius of \u03b3P \u03c0 by\n\u03c1(\u03b3P \u03c0) \u2264 \u2016\u03b3P \u03c0\u2016\u221e = \u03b3 < 1 (17)\nThus, the inverse (IS \u2212 \u03b3P \u03c0)\u22121 exists. In addition, since the matrices D and X have full-rank by assumption, we conclude that matrix B is invertible, so the minimizer w\u22c6 is given by\nw\u22c6 = ( X\u22a4D (IS \u2212 \u03b3P \u03c0)X )\u22121 X\u22a4Dr\u03c0 (18)\nIf the quantities {P \u03c0, r\u03c0} were known, one would proceed to solve (18) and determine the desired vector w\u22c6 and the sought-after value vector v\u03c0 from (9). However, we want the agents to learn v\u03c0 without any prior knowledge of the environment. In other words, we cannot assume P \u03c0 and r\u03c0 are known. For this reason, we need to develop an alternative solution method. In the process of doing so, first for single-agents, we shall arrive at the same gradient temporal difference method of [9] albeit by using a fundamentally different approach involving a primal-dual argument. The approach will subsequently enable us to generalize to a fully distributed solution.\nSo let us continue with the single-agent case for now. Our first step relies on relating Eq. (15) to the saddle-point conditions of a convex optimization problem. Indeed, minimizing JPB(w) in (14) is\nNovember 6, 2014 DRAFT\n10\nequivalent to the following quadratic programming problem:\nminimize \u03b5,w\n1 2 \u03b5\u22a4(X\u22a4DX)\u22121\u03b5\ns.t. \u03b5 = X\u22a4Dr\u03c0 \u2212Bw (19)\nwhere we have introduced the splitting variable \u03b5. Since problem (19) is convex and satisfies Slater\u2019s condition [40], strong duality holds and the primal and dual optimal values are attained and equal and they form a saddle-point of the Lagrangian. Specifically, the Lagrangian of (19) is\nL(\u03b5, w, \u03b8)= 1 2 \u2016\u03b5\u20162(X\u22a4DX)\u22121 + \u03b8\u22a4\n( X\u22a4Dr\u03c0 \u2212Bw \u2212 \u03b5 ) (20)\nwhere \u03b8 is the Lagrange multiplier. By minimizing L(\u03b5, w, \u03b8) over \u03b5 and w, we obtain that the dual function is g(\u03b8) = \u2212\u221e unless B\u22a4\u03b8 = 0M , in which case we have\ng(\u03b8) = \u22121 2 \u03b8\u22a4X\u22a4DX\u03b8 + \u03b8\u22a4X\u22a4Dr\u03c0 (21)\nTherefore, the dual problem of (19) is given by\nminimize \u03b8\n1 2 \u03b8\u22a4X\u22a4DX\u03b8 \u2212 \u03b8\u22a4X\u22a4Dr\u03c0\ns.t. B\u22a4\u03b8 = 0M\n(22)\nThe main reason to solve (22) instead of the primal problem (19) is that the dual formulation removes the inverse in the weighting matrix, X\u22a4DX. This transformation brings two benefits. First, in Sec. II-D, we will see that it is straightforward to optimize (22) from samples. Second, as it is explained in Sections III and IV-B, problem (22) leads to a distributed algorithm in which the agents are able to combine their individual experience to solve the problem.\nHad we assumed P \u03c0 and r\u03c0 to be known, problem (22) would be trivial, with unique solution \u03b8 = 0M . However, since we do not assume any prior knowledge, we are going to employ instead a primal-dual algorithm that leads to an iterative stochastic-approximation mechanism to learn from samples. First, we derive the Lagrangian of (22) as\nL(\u03b8,w) = 1 2 \u03b8\u22a4X\u22a4DX\u03b8 \u2212 \u03b8\u22a4X\u22a4Dr\u03c0 + w\u22a4B\u22a4\u03b8\n= \u03b8\u22a4X\u22a4D ( 1\n2 X\u03b8 + (IS \u2212 \u03b3P \u03c0)Xw \u2212 r\u03c0\n) (23)\nwhere w denotes the Lagrange multiplier. We use the same notation w to denote the dual variable for (23) because it can be verified that by computing the dual of the dual problem (22) we recover the original\nNovember 6, 2014 DRAFT\n11\nproblem (14), which is equivalent to (19). Thus, the optimal dual variable w\u22c6 of (23) is also the optimal solution to (14). To find a saddle-point {\u03b8\u22c6, w\u22c6} of the Lagrangian (23) we alternate between applying gradient descent to L(\u03b8,w) with respect to \u03b8 and gradient ascent with respect to w:\n\u03b8i+1 = \u03b8i \u2212 \u00b5\u03b8X\u22a4D (X\u03b8i + (IS \u2212 \u03b3P \u03c0)Xwi \u2212 r\u03c0) (24a)\nwi+1 = wi + \u00b5wX \u22a4(IS \u2212 \u03b3P \u03c0)\u22a4DX\u03b8i (24b)\nwhere \u00b5\u03b8 and \u00b5w are positive step-sizes.\nConstruction (24a)\u2013(24b) is the well-known Arrow-Hurwicz algorithm (see, e.g., [41], [42, Ch. 9.3.3]\nand [43, Ch. 10])."}, {"heading": "D. Primal-dual stochastic optimization", "text": "As mentioned before, since the agents do not have prior knowledge of the environment, we need to replace (24a)\u2013(24b) by constructions that do not depend on the quantities {P \u03c0, r\u03c0}. In order to find the solution directly from samples, we need to convert these gradient iterations into stochastic approximations. The selection of an appropriate weighted norm \u2016 \u00b7 \u2016D in (14) now becomes relevant. If we choose a weighting matrix D that represents the probability distribution of visiting each state, then we can express the terms that appear in (24a)\u2013(24b) as expectations that we can substitute with their sample estimates. We proceed to explain the details.\nLet us set the weighting matrix in (14) equal to the state-visitation probability induced by the behavior policy (which we emphasize with the corresponding superscript), i.e., D , D\u03c6 , diag{d\u03c6}. Equations (24a)\u2013(24b) depend on P \u03c0 and r\u03c0, meaning that the agent aims to predict the value vector along the expected trajectory that would have been induced by the target policy \u03c0. However, the state-visitation distribution of this trajectory, d\u03c0 , does not match the distribution of the samples actually gathered by the agent, given by d\u03c6. Importance sampling [44, Ch. 9.7] is a technique for estimating properties of a particular distribution, while only having samples generated from a different distribution. Let us introduce importance weights that measure the dissimilarity between the target (\u03c0) and behavior (\u03c6) policies.\n\u03be(a, s) , \u03c0(a|s) \u03c6(a|s) (25)\nBy using importance sampling, reference [23] showed that we can write the gradient inside (24a) in terms of moment values of the behavior policy as follows:\nX\u22a4D\u03c6 (X\u03b8i + (IS \u2212 \u03b3P \u03c0)Xwi \u2212 r\u03c0)\nNovember 6, 2014 DRAFT\n12\n= \u2211\ns\u2208S d\u03c6(s)xs\n x\u22a4s \u03b8i + ( x\u22a4s \u2212 \u03b3 \u2211\ns\u2032\u2208S p\u03c0ss\u2032x \u22a4 s\u2032\n) wi \u2212 \u2211\na\u2208A\n\u2211 s\u2032\u2208S P(s\u2032|s, a)\u03c0(a|s)r(s, a, s\u2032)\n \n= \u2211\ns\u2208S\n\u2211\na\u2208A\n\u2211 s\u2032\u2208S P(s\u2032|s, a)\u03c0(a|s)d\u03c6(s) \u00b7 xs ( x\u22a4s \u03b8i + (xs \u2212 \u03b3xs\u2032)\u22a4wi \u2212 r(s, a, s\u2032) )\n= \u2211\ns\u2208S\n\u2211\na\u2208A\n\u2211 s\u2032\u2208S P(s\u2032|s, a)\u03c6(a|s)\u03be(a, s)d\u03c6(s) \u00b7 xs ( x\u22a4s \u03b8i + (xs \u2212 \u03b3xs\u2032)\u22a4wi \u2212 r(s, a, s\u2032) )\n= Ed\u03c6,\u03c6,P [ xs ( x\u22a4s \u03b8i + (xs \u2212 \u03b3xs\u2032)\u22a4wi \u2212 r(s,a, s\u2032) ) \u03be(a, s) ] (26)\nSimilarly, we can express the gradient inside (24b) as\nX\u22a4(IS \u2212 \u03b3P \u03c0)\u22a4D\u03c6X\u03b8i = Ed\u03c6,\u03c6,P [ (xs \u2212 \u03b3xs\u2032)x\u22a4s \u03be(a, s) ] \u03b8i (27)\nThe agent does not know these expected values though. Rather, at every time-step, the agent observes transitions of the form {xi, a(i), xi+1, r(i+1)}, where xi , xs(i) denotes the feature vector observed at time i.\nIn addition, the agent knows both its behavior policy \u03c6 and the target policy \u03c0 that it wants to evaluate so it can compute the importance weight. Nevertheless, in an actual implementation, the agent need not know the states but just features, hence, the actual policies must be conditioned on the feature vectors. Slightly abusing notation, we introduce the importance weight that the node computes at every time step:\n\u03be(i) , \u03c0(a(i)|xi) \u03c6(a(i)|xi) \u2248 \u03c0(a(i)|s(i)) \u03c6(a(i)|s(i)) , \u03be(a(i), s(i)) (28)\nSince a sample of a random variable is an unbiased estimator of its expected value, we can build a pair of stochastic approximation recursions from (24a)\u2013(24b) and (26)\u2013(27):\n\u03b8i+1 = \u03b8i \u2212 \u00b5\u03b8xi ( x\u22a4i \u03b8i + \u03b4 \u22a4 i+1wi \u2212 r(i+ 1) ) \u03be(i) (29a)\nwi+1 = wi + \u00b5w\u03b4i+1x \u22a4 i \u03b8i\u03be(i) (29b)\nwhere we introduced \u03b4i+1 , xi\u2212\u03b3xi+1. Recursions (29a)\u2013(29b) coincide with the single-agent gradienttemporal difference (GTD2) algorithm, which was derived in [9] using a different approach. The above derivation from (19) to (29b) shows that GTD2 is a stochastic Arrow-Hurwicz algorithm applied to the dual problem of (14). More importantly, as we will see in the following sections, the primal-dual approach is convenient for a multi-agent formulation, since it leads to a meaningful in-network statevisitation distribution that combines the individual stationary distributions of the agents, thus overcoming\nNovember 6, 2014 DRAFT\n13\nnon-exploratory individual behavior policies."}, {"heading": "III. MULTI-AGENT LEARNING", "text": "We now consider a network of N connected agents that operate in similar but independent MDPs. The state-space S, action-space A, and transition probabilities P are the same for every node, but their actions do not influence each other. Thus, the transition probabilities seen by each agent k are only determined by its own actions, ak(i) \u2208 A, and the previous state of its environment, sk(i) \u2208 S:\nsk(i+ 1) \u223c P(\u00b7|sk(i), ak(i)), k = 1 . . . N (30)\nThis assumption is convenient because it makes the problem stationary without forcing each agent to know the actions and feature vectors of every other agent in the network. The agents aim to predict the response of their environment to a common target policy \u03c0 while they follow different behavior policies, denoted by \u03c6k(a|s) each. Motivated by recent results on network behavior in [21], [45], we note that, through collaboration, each agent may contribute to the network with its own experience. Let D\u03c6k be the diagonal matrix that represents the stationary state-visitation distribution for agent k. We then introduce the following global problem in place of (22) with D substituted by D\u03c6k :\nminimize \u03b8\nN\u2211\nk=1\n\u03c4k\n( 1\n2 \u03b8\u22a4X\u22a4D\u03c6kX\u03b8 \u2212 \u03b8\u22a4X\u22a4D\u03c6kr\u03c0\n)\ns.t.\nN\u2211\nk=1\n\u03c4k ( X\u22a4D\u03c6k(IS \u2212 \u03b3P \u03c0)X )\u22a4 \u03b8 = 0\n(31)\nwhere \u03c4 = [\u03c41, . . . , \u03c4N ]\u22a4 is a vector of non-negative parameters whose purpose is to weight the contribution of each agent\u2019s local problem to the global problem, such that \u03c4\u22a41N = 1. Since the dual problem (22) removes the inverse of the weighting matrices X\u22a4D\u03c6kX, we can introduce the in-network stationary distribution\nD\u03c6 ,\nN\u2211\nk=1\n\u03c4kD \u03c6k (32)\nNote that solving the aggregated problem (31) is effectively solving the single-agent problem (22) with D replaced by D\u03c6. The Lagrangian of (31) is given by\nL(\u03b8,w) =\nN\u2211\nk=1\n\u03c4kLk(\u03b8,w) (33)\nNovember 6, 2014 DRAFT\n14\nwhere the individual Lagrangians are given by\nLk(\u03b8,w) = \u03b8 \u22a4X\u22a4D\u03c6k\n( 1\n2 X\u03b8 + (IS \u2212 \u03b3P \u03c0)Xw \u2212 r\u03c0\n) (34)\nwhich are similar to (23) but with stationary distribution D\u03c6k . In order to find the global saddle-point of the aggregate Lagrangian (33) in a cooperative and stochastic manner, we apply diffusion strategies [19]\u2013[21]. We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45]. The algorithm consists of two-steps: the adaptation step, at which every agent updates its own intermediate estimate independently of the other agents; and the combination step, at which every agent combines its neighbors\u2019 estimates. Similar to the derivation of the single-agent algorithm (29a)\u2013(29b), we can express the gradient of the individual Lagrangians (34) in terms of moment values (i.e., replacing D\u03c6 by D\u03c6k into (26)\u2013(27)). We then follow a primal-dual approach and apply ATC twice: i) for minimizing Lk(\u03b8,w) in (34) over \u03b8 through stochastic gradient descent:\n\u03b8\u0302k,i+1=\u03b8k,i \u2212 \u00b5\u03b8xk,i ( x\u22a4k,i\u03b8k,i + \u03b4 \u22a4 k,i+1wk,i \u2212 rk(i+ 1) ) \u03bek(i)\n(35a)\n\u03b8k,i+1= \u2211\nl\u2208Nk clk \u03b8\u0302l,i+1 (35b)\nand ii) for maximizing Lk(\u03b8,w) in (34) over w through stochastic gradient ascent:\nw\u0302k,i+1 = wk,i + \u00b5w\u03b4k,i+1x \u22a4 k,i\u03b8k,i\u03bek(i) (35c) wk,i+1 = \u2211\nl\u2208Nk clk w\u0302l,i+1 (35d)\nwhere Nk stands for the neighborhood of agent k (i.e., the set of agents that are able to communicate with agent k in a single hop, including k itself), \u03b8\u0302 and w\u0302 correspond to the locally adapted estimates, and \u03b8 and w correspond to the combined estimates for the adapt-then-combine strategy. The combination coefficients {clk} define the weights on the links in the network and can be chosen freely by the designer, as long as they satisfy:\nclk \u2265 0, \u2211\nl\u2208Nk clk = 1, clk = 0 if l /\u2208 Nk (36)\nckk > 0 for at least one agent k (37)\nLet C , [clk] be the combination matrix. Then, condition (36) implies that C is left-stochastic. Condition (37) means that there is at least one agent that trusts its local measurements and is able to perform its own\nNovember 6, 2014 DRAFT\n15\nadaptation step. We also assume that the topology of the network is connected (i.e., there is at least one path between any pair of nodes) and that the combination matrix C remains fixed over time. Therefore, conditions (36)\u2013(37) ensure that C is a primitive matrix (i.e., there exists j > 0 such that all entries of Cj are strictly positive) [19], [46]. It follows from the Perron-Frobenius Theorem [47] that C has a unique eigenvalue at one, while all other eigenvalues are strictly inside the unit circle. We normalize the entries of the eigenvector that is associated with the eigenvalue at one to add up to one and refer to it as the Perron eigenvector of C . All its entries will be strictly positive. We we will show in Sec. IV-G and App. B that the values for {\u03c4k} turn out to be determined by this Perron eigenvector.\nIterations (35a)\u2013(35d) constitute the proposed diffusion off-policy GTD algorithm, which we remark\nis a fully distributed algorithm because the combination step is taken only over Nk.\nAlgorithm 1: Diffusion off-policy GTD algorithm. This procedure runs in parallel at every node k.\nInputs: Target \u03c0 and behavior \u03c6k policies, neighborhood Nk, weights {clk, l = 1, . . . , N}, and step-sizes \u00b5\u03b8, \u00b5w Initialize estimates \u03b8k,0, wk,0 for every time-step i = 1 to T do\nTake action ak(i) \u223c \u03c6k(\u00b7|xk,i) Observe feature vector xk,i+1 and reward rk(i+ 1) Perform local adaptation steps (35a) and (35c) Combine in-neighborhood estimates into \u03b8k,i+1, wk,i+1 using (35b) and (35d)\nend for Return: wk,T+1\nNovember 6, 2014 DRAFT\n16"}, {"heading": "IV. PERFORMANCE ANALYSIS", "text": "In this section we analyze the existence and uniqueness of the optimal solution to the multi-agent learning problem (31). We extend the energy conservation arguments of [16]\u2013[19] to perform a mean-squareerror (MSE) analysis of the diffusion GTD algorithm (35a)\u2013(35d) and provide convergence guarantees under sufficiently small step-sizes. We also obtain closed form expressions of the mean-square-deviation (MSD) and analyze the bias of the algorithm. We will rely on some reasonable conditions on the data, as explained next."}, {"heading": "A. Data model", "text": "To begin with, we model the quantities appearing in (35a)\u2013(35d) as instantaneous realizations of random variables, which we denote by using boldface notation. We aggregate the variables into vectors of length 2M each:\n\u03b1k,i ,   \u03b8k,i\nwk,i\n  , \u03c8k,i ,   \u03b8\u0302k,i\nw\u0302k,i\n  (38)\ngk,i+1 ,   \u2212\u03b7xk,i \u00b7 \u03bek(i) \u00b7 rk(i+ 1)\n0M\n  (39)\nwhere we are now writing \u00b5w , \u00b5 and \u00b5\u03b8 , \u03b7\u00b5w, such that \u03b7 > 0 is the step-size ratio between the two adaptation steps. We further introduce the following 2M \u00d7 2M coefficient matrix:\nGk,i+1 ,   \u03b7xk,ix \u22a4 k,i\u03bek(i) \u03b7xk,i\u03b4 \u22a4 k,i+1\u03bek(i)\n\u2212\u03b4k,i+1x\u22a4k,i\u03bek(i) 0M\u00d7M\n  (40)\nThen, the diffusion algorithm (35a)\u2013(35d) with stochastic variables can be expressed as\n\u03c8k,i+1 = \u03b1k,i \u2212 \u00b5 (Gk,i+1\u03b1k,i + gk,i+1) (41a) \u03b1k,i+1 = \u2211\nl\u2208Nk clk\u03c8l,i+1 (41b)\nWe assume the following conditions for (41a)\u2013(41b):\nAssumption 1. The state transitions {(sk(i), sk(i+1))} visited by each agent k are i.i.d. samples, with initial states {sk(i)} drawn from the stationary distribution d\u03c6k .\nNovember 6, 2014 DRAFT\n17\nAssumption 2. There is some positive probability that every state is visited by at least one agent, thus D\u03c6 in (32) is positive-definite.\nAssumption 3. The feature matrix X and the expected reward signal r\u03c0 are bounded from below and from above.\nGiven the sequence of states visited by each agent {sk(1), sk(2), . . . , sk(i), . . .}, the segments that start and end at the same state are independent of one another. When the Markov chain that defines these state transitions has short mixing time, these segments tend to be short (see, e.g., [30]). Assumption 1 approximates these independent segments with sequences of just one step. This is a customary approximation (see, e.g., [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.i.d. samples, rendering Gk,i+1 and gk,i+1 independent of \u03b1k,i.\nAssumption 2 refers to a property of the network. For a single-agent algorithm, the agent should visit every state with positive probability; otherwise it may not be able to approach the value function. Here, we impose the milder condition that every state must be visited by at least one agent.\nAssumption 3 holds for most practical implementations, and will be used in the stability analysis."}, {"heading": "B. Existence and uniqueness of solution", "text": "Solving the aggregated dual problem (31) is equivalent to finding the saddle-points {wo, \u03b8o} of the global Lagrangian (33). A saddle-point of the Lagrangian must satisfy [40]:\nL(\u03b8o, wo) = min \u03b8 max w L(\u03b8,w) = max w min \u03b8 L(\u03b8,w) (42)\nThese conditions are equivalent to the following system of linear equations:\n\u2207\u03b8L(\u03b8,w)=X\u22a4D\u03c6 (X\u03b8 \u2212 r\u03c0 + (IS \u2212 \u03b3P \u03c0)Xw) = 0M (43)\n\u2207wL(\u03b8,w)=X\u22a4(IS \u2212 \u03b3P \u03c0)\u22a4D\u03c6X\u03b8 = 0M (44)\nTo find the saddle-point {\u03b8o, wo}, we solve for \u03b8 in (44) first. Since Assumption 2 establishes that D\u03c6 has full-rank, we recall from (16)\u2013(18) that X\u22a4D\u03c6(\u03b3P \u03c0\u2212IS)X is invertible and, hence, \u03b8o = 0M . Then, substituting \u03b8o into (43) yields:\nwo = ( X\u22a4D\u03c6 (IS \u2212 \u03b3P \u03c0)X )\u22121 X\u22a4D\u03c6r\u03c0 (45)\nNovember 6, 2014 DRAFT\n18\nEquation (45) therefore illustrates one clear benefit of cooperation. If the behavior policy of some agent prevents him from exploring the entire state-space, then some of the entries of its corresponding d\u03c6k will be zero and the agent may be unable to estimate the value vector on its own. Nevertheless, as long as any other agent in the network can visit these unexplored states, the matrix D\u03c6 will be positive-definite, guaranteeing the existence and uniqueness of a solution wo.\nWe remark that the off-policy solution wo in (45) is in fact an approximation to the on-policy solution\nthat the agents wish to predict, which is given by (18) when D , D\u03c0:\nw\u03c0 = ( X\u22a4D\u03c0 (IS \u2212 \u03b3P \u03c0)X )\u22121 X\u22a4D\u03c0r\u03c0 (46)\nThat is, the obtained solution (45) is still an approximation of (46) because D\u03c6\u0304 is not necessarily the same as D\u03c0. However, it is interesting to realize that, by using diffusion strategies, the agents can estimate the exact on-policy solution if the scalars {\u03c4k} could be set to satisfy N\u2211\nk=1\n\u03c4kd \u03c6k = d\u03c0 \u21d4 wo = w\u03c0 (47)\nIn the next subsections, we analyze the conditions that allow diffusion GTD to converge to (45)."}, {"heading": "C. Error recursion", "text": "We introduce the following error measures, which measure the difference between the estimates\n{\u03b1k,i,\u03c8k,i} at time i and the optimal solution \u03b1o = col{\u03b8o, wo} for each agent k:\n\u03c8\u0303k,i , \u03b1 o \u2212\u03c8k,i (48)\n\u03b1\u0303k,i , \u03b1 o \u2212\u03b1k,i (49)\nThen, subtracting both sides of (41a)\u2013(41b) from \u03b1o, we obtain\n\u03c8\u0303k,i+1=(I2M \u2212 \u00b5Gk,i+1) \u03b1\u0303k,i + \u00b5 (Gk,i+1\u03b1o + gk,i+1) (50)\nUsing the fact that clk = 0 if l /\u2208 Nk, the error recursion for the combination step becomes\n\u03b1\u0303k,i = \u2211\nl\u2208Nk clk\u03c8\u0303l,i =\nN\u2211\nl=1\nclk\u03c8\u0303l,i (51)\nWe collect the error variables from across the network into block vectors of size 2MN :\n\u03c8\u0303i , col{\u03c8\u03031,i, . . . , \u03c8\u0303N,i} (52)\nNovember 6, 2014 DRAFT\n19\n\u03b1\u0303i , col{\u03b1\u03031,i, . . . , \u03b1\u0303N,i} (53)\nLet C and Ri be matrices of size 2MN \u00d7 2MN defined by\nC , C \u2297 I2M (54)\nRi , diag{G1,i, . . . ,GN,i} (55)\nand let Gi be the matrix of size 2MN \u00d7 2M defined by\nGi , col{G1,i, . . . ,GN,i} (56)\nWe also introduce the vectors of length 2MN :\ngi , col{g1,i, . . . ,gN,i} (57)\nni , Gi\u03b1 o + gi (58)\nThen, the individual error recursions in (48)\u2013(49) lead to the following network recursion:\n\u03b1\u0303i+1 = C\u22a4 ( I2MN \u2212 \u00b5Ri+1 ) \u03b1\u0303i + \u00b5C\u22a4ni+1 (59)\nThis recursion shows how the error dynamics evolves over the network over time."}, {"heading": "D. Convergence in the mean", "text": "Introduce the following expected values for each agent:\nGk , EGk,i =  \n\u03b7X\u22a4D\u03c6kX \u03b7X\u22a4D\u03c6k(IS \u2212 \u03b3P \u03c0)X\n\u2212X\u22a4(IS \u2212 \u03b3P \u03c0)\u22a4D\u03c6kX 0M\u00d7M\n  (60)\ngk , Egk,i =   \u2212\u03b7X\u22a4D\u03c6kr\u03c0\n0M\n  (61)\nSince Assumption 1 implies that the variables Ri+1 and \u03b1\u0303i are independent of each other, then by taking expectations of both sides of (59) we obtain\nE\u03b1\u0303i+1 = C\u22a4 ( I2MN \u2212 \u00b5R ) E\u03b1\u0303i + \u00b5C\u22a4(G\u03b1o + g) (62)\nNovember 6, 2014 DRAFT\n20\nwhere\nR , ERi = diag{G1, . . . , GN} (63)\nG = EGi = col{G1, . . . , GN} (64)\ng = Egi = col{g1, . . . , gN} (65)\nTherefore, the convergence of (62) is guaranteed when the matrix C\u22a4(I2MN \u2212 \u00b5R) is stable.\nTheorem 1 (Mean convergence). For the data model of Section IV-A, there exists small enough step-sizes, say 0 < \u00b5 < \u00b5o (for some \u00b5o > 0 given by (122) in Appendix A), such that the matrix C\u22a4(I2MN \u2212\u00b5R) is stable and, therefore, the mean-error recursion (62) is stable for every agent k = 1, . . . , N and converges to the bias value given by\n\u03b1\u0303\u221e , lim i\u2192\u221e\nE\u03b1\u0303i = ( I2MN \u2212 C\u22a4 (I2MN \u2212 \u00b5R) )\u22121 \u00b5C\u22a4 (G\u03b1o + g)\nProof: See Appendix A.\nAs it is explained in Appendix A, the value \u00b5o only depends on the inputs of the algorithm, namely, data-samples (state-features and transition rewards), the weighted-topology matrix C , the cost-weights {\u03c4k}, and the step-size ratio parameter \u03b7."}, {"heading": "E. Mean-square stability", "text": "Although the error vector converges in the mean, we still need to ensure that it has bounded fluctuations around its fixed point value. To do so, we study the evolution and steady-state value of the variance E\u2016\u03b1\u0303i\u20162. By computing the weighted squared Euclidean (semi)norm of both sides of (59)\u2014 using an arbitrary positive (semi)definite weighting matrix \u03a3 that we are free to choose\u2014and applying the expectation operator, we obtain the following variance relation:\nE\u2016\u03b1\u0303i+1\u20162\u03a3 = E \u2016\u03b1\u0303i\u20162\u03a3\u2032 + 2b\u22a4\u03a3 E\u03b1\u0303i +Tr ( \u00b52\u03a3C\u22a4RnC ) (66)\nwhere\n\u03a3\u2032 , (I2MN \u2212 \u00b5R\u22a4)C\u03a3C\u22a4(I2MN \u2212 \u00b5R) + \u00b52E [ (Ri+1 \u2212R)\u22a4C\u03a3C\u22a4(Ri+1 \u2212R) ] (67) b\u03a3 , \u00b5E [( I2MN \u2212 \u00b5R\u22a4i+1 ) C\u03a3C\u22a4ni+1 ] (68)\nRn , E [ nin \u22a4 i ] = E [ (Gi\u03b1 o + gi)(Gi\u03b1 o + gi) \u22a4 ]\n(69)\nNovember 6, 2014 DRAFT\n21\nLet \u03c3 = vec(\u03a3). Using the Kronecker product property vec(Y \u03a3Z) = (Z\u22a4 \u2297 Y )vec(\u03a3) [39], we can vectorize \u03a3\u2032 in (67) and find that its vector form is related to \u03a3 via the following linear relation: \u03c3\u2032 , vec(\u03a3\u2032) = F\u03c3, where the matrix F is given by\nF , (( I2MN \u2212 \u00b5R\u22a4 ) C ) \u2297 (( I2MN \u2212 \u00b5R\u22a4 ) C ) + \u00b52E [( (R\u22a4i+1 \u2212R\u22a4)C ) \u2297 ( (R\u22a4i+1 \u2212R\u22a4)C )]\n(70)\nFurthermore, using the property Tr(\u03a3Y ) = (vec[Y \u22a4])\u22a4\u03c3, we can rewrite (66) as:\nE\u2016\u03b1\u0303i+1\u20162\u03c3 = E\u2016\u03b1\u0303i\u20162F\u03c3 + 2\u03c3\u22a4U \u00b7 E\u03b1\u0303i + h\u22a4\u03c3 (71)\nwhere\nU , \u00b5E [ ( C\u22a4ni+1 ) \u2297 ( C\u22a4 (I2MN \u2212 \u00b5Ri+1) ) ] (72) h , \u00b52vec [ C\u22a4RnC ] (73)\nIn (71) we are using the notation \u2016x\u20162\u03c3 to represent \u2016x\u20162\u03a3. Note that (71) is not a true recursion because the weighting matrices corresponding to \u03c3 and F\u03c3 are different. Moreover, recursion (71) is coupled with the mean-error recursion (62). To study the convergence of (71) we will expand it into a state-space model following [39], [49]. Let L , 2MN and let p(x) denote the characteristic polynomial of the L2 \u00d7 L2 matrix F , given by\np(x) , det(xI \u2212F) = xL2 + pL2\u22121xL 2\u22121 + . . .+ p0 (74)\nBy the Cayley-Hamilton Theorem [39], we know that every matrix satisfies its characteristic equation (i.e., p(F) = 0), so that\nFL2 = \u2212p0IL2 \u2212 p1F \u2212 . . .\u2212 pL2\u22121FL 2\u22121 (75)\nReplacing \u03c3 in (71) by F j\u03c3, j = 0, . . . , L2 \u2212 1, we can derive the following state-space model:   E\u2016\u03b1\u0303i+1\u2016 2 \u03c3 E\u2016\u03b1\u0303i+1\u2016 2 F\u03c3\n... E\u2016\u03b1\u0303i+1\u2016 2\nFL 2\u22121\u03c3\n \n\ufe38 \ufe37\ufe37 \ufe38 Wi+1\n=   0 1 0 \u00b7\u00b7\u00b7 0 0 0 1 \u00b7\u00b7\u00b7 0 ... . .. 0\n0 0 0 \u00b7\u00b7\u00b7 1\n\u2212p0 \u2212p1 \u2212p2 \u00b7\u00b7\u00b7 \u2212pL2\u22121\n \n\ufe38 \ufe37\ufe37 \ufe38 T\n  E\u2016\u03b1\u0303i\u2016 2 \u03c3 E\u2016\u03b1\u0303i\u2016 2 F\u03c3\n... E\u2016\u03b1\u0303i\u2016 2\nFL 2\u22121\u03c3\n \n\ufe38 \ufe37\ufe37 \ufe38 Wi\n+2\n  \u03c3\u22a4U \u03c3\u22a4FU\n... \u03c3\u22a4FL 2\u22121U\n \n\ufe38 \ufe37\ufe37 \ufe38 Q\nE\u03b1\u0303i +\n  h\u22a4\u03c3 h\u22a4F\u03c3\n... h\u22a4FL 2\u22121\u03c3\n \n\ufe38 \ufe37\ufe37 \ufe38 Y\n(76)\nNovember 6, 2014 DRAFT\n22\nWe combine (76) with the mean-recursion (62) and rewrite them more compactly as:   Wi+1\nE\u03b1\u0303i+1\n  =   T 2Q\n0 C\u22a4(I2MN \u2212 \u00b5R)\n    Wi\nE\u03b1\u0303i\n +  \nY\nC\u22a4G\u03b1o + g\n  (77)\nTheorem 2 (Mean-square stability). Assume the step-size parameter \u00b5 is sufficiently small so that terms that depend on higher-order powers of \u00b5 can be ignored. Then, for the data model of Section IV-A, there exists 0 < \u00b5oMS \u2264 \u00b5o (for \u00b5o used in Theorem 1 and given by (122) in Appendix A), such that when 0 < \u00b5 < \u00b5oMS, the variance recursion (77) is mean-square stable.\nProof: Observe that the stability of the joint recursion (77) is equivalent to the stability of the\nmatrices T and C\u22a4(I2MN \u2212\u00b5R), which is further equivalent to the following conditions on their spectral radii:\n\u03c1 ( C\u22a4(I2MN \u2212 \u00b5R) ) < 1, \u03c1 (T ) < 1 (78)\nThe first condition is the same mean-stability condition that was discussed in Theorem 1. For the second condition, we note from (76) that T is in companion form, and it is known that its eigenvalues are the roots of p(x), which are also the eigenvalues of F . Therefore, a necessary and sufficient condition for the stability of T is the stability of the matrix F . When the step-sizes are small enough, the last term in (70) can be ignored since it depends on \u00b52 and we can write\nF \u2248 ( C\u22a4 (I2MN \u2212 \u00b5R) )\u22a4 \u2297 ( C\u22a4 (I2MN \u2212 \u00b5R) )\u22a4 (79)\nwhich is stable if C\u22a4 (I2MN \u2212 \u00b5R) is stable. We remark that 0 < \u00b5oMS \u2264 \u00b5o is chosen to dismiss higher-order powers of \u00b5, and that \u00b5o (see Appendix A) only depends on the inputs of the algorithm (i.e., data-samples, the weighted-topology matrix C , the weights {\u03c4k} and the parameter \u03b7)."}, {"heading": "F. Mean-square performance", "text": "Taking the limit of both sides of (71) we obtain:\nlim i\u2192\u221e E\u2016\u03b1\u0303i+1\u20162\u03c3 = lim i\u2192\u221e E\u2016\u03b1\u0303i\u20162F\u03c3 + 2\u03c3\u22a4U lim i\u2192\u221e E\u03b1\u0303i + h \u22a4\u03c3 (80)\nTheorem 1 guarantees that limi\u2192\u221eE\u03b1\u0303i = \u03b1\u0303\u221e, so the steady-state variance recursion in (80) leads to\nlim i\u2192\u221e\nE\u2016\u03b1\u0303i\u20162\u03c3 = q\u22a4(I \u2212F)\u22121\u03c3 (81)\nNovember 6, 2014 DRAFT\n23\nwhere q , h+ 2U \u03b1\u0303\u221e. Result (81) is useful because it allows us to derive several performance metrics through the proper selection of the free weighting parameter vector \u03c3 (or, equivalently, the parameter matrix \u03a3). For example, the network mean-square-deviation (MSD) is defined as the average of the MSD of all the agents in the network:\nMSD network , lim i\u2192\u221e 1 N\nN\u2211\nk=1\nE\u2016\u03b1\u0303k,i\u20162 = lim i\u2192\u221e E\u2016\u03b1\u0303i\u201621 N I2MN\n(82)\nChoosing the weighting matrix in (81) as \u03a3 = I2MN/N , we get:\nMSD network =\n1\nN q\u22a4(I \u2212F)\u22121vec(I2MN ) (83)\nWe can also obtain the MSD of any particular node k, as\nMSDk , lim i\u2192\u221e\nE\u2016\u03b1\u0303i\u20162Jk (84)\nwhere Jk is a block-diagonal matrix of N blocks of size 2M \u00d7 2M , such that all blocks in the diagonal are zero except for block k which is the identity matrix. Following the same procedure as with the network MSD we obtain\nMSDk = q \u22a4(I \u2212F)\u22121vec(Jk) (85)"}, {"heading": "G. Bias analysis", "text": "We showed in (45) that, under Assumption 2, there exists a unique solution \u03b1o for the global optimization problem (31). On the other hand, the error recursion (62) converges in the mean-square sense to some bias value \u03b1\u0303\u221e. Now, we examine under which conditions \u03b1\u0303\u221e is small when the step-size is small. The analysis of the bias value \u03b1\u0303\u221e in (66) is similar to the examination developed in [18, Theorem 3] for multi-objective optimization. The main difference lies in the fact that we do not assume the matrix R in (63) to be symmetric.\nTheorem 3 (Bias at small step-size). Consider the data model of Section IV-A, where the combination matrix C is primitive left-stochastic (it satisfies (36)\u2013(37)). Suppose the Perron eigenvector that corresponds to the eigenvalue of C at one is equal to the vector of weights {\u03c4k} in the global problem (31) (i.e., \u03c4\u22a4C = \u03c4\u22a4 and \u03c4\u22a41N = 1). Assume further that the step-size \u00b5 is sufficiently small to ensure mean-square stability. Then, it holds that\n\u03b1\u0303\u221e = O(\u00b5) (86)\nNovember 6, 2014 DRAFT\n24\nProof: See Appendix B.\nWe remark that the bias in (86) comes from agents following different behavior policies, which means that they are solving different optimization problems, with different minimizer each. When they use diffusion strategies, the combination step pulls them toward the global solution. Nevertheless, the adaptation step pushes each agent towards the minimizer of its individual cost function. Note, however, that if all agents followed the same behavior policy, their individual optimization problems would be identical, therefore, both the adaptation and the combination steps would pull them toward the global solution and their fixed-point estimates would be unbiased with respect to the solution of the global optimization problem (31), as stated in [1]. More formally, Gk , G\u0304 and gk , g\u0304 would be the same for every agent k, and the saddle-point conditions of the Lagrangian of the global problem (33) would not depend on the combination weights {\u03c4k}: N\u2211\nk=1\n\u03c4k(G\u0304\u03b1 o + g\u0304) = G\u0304\u03b1o + g\u0304 = 02M\nTherefore, if all the agents followed the same behavioral policy, then G\u03b1o + g = 02MN and, from (66), we would conclude that \u03b1\u0303\u221e = 0."}, {"heading": "V. SIMULATIONS", "text": "Consider a group of animals foraging in a 2D-world (see Figure 2). The group forms a network of N = 15 agents with arbitrarily connected topology and neighborhood size |N |k varying between 2 and 9. The weights of the links (i.e., the clk elements of C) are obtained independently by each node following an averaging rule [19], [50], such that equal weight is given to any member of the neighborhood, including itself (i.e., clk = 1/|Nk|, l \u2208 Nk). Note this rule leads to a left (rather than doubly) stochastic combination matrix C that satisfies (36)\u2013(37). We assume that the combination matrix C\u2014and, hence, the network topology\u2014remains fixed.\nThe world is a discrete, bounded square with 20 rows and 20 columns, which amounts to S = 400 states. Each agent self-localizes itself in the grid by sensing a Gaussian radial basis function of its distance to M = 64 fixed markers (i.e., each of these values is a feature). The agents move in four possible directions, namely, A = {north, south, east, west}. At every time step, the agents move and consume some energy (i.e., they receive some negative reward). In the north-east corner of the world, there is food, which the agents understand as positive reward. However, there is a large area below the food with a predator that is harmful to go through, so agents receive large negative reward if they visit these states (see caption of Figure 2 to see the exact numerical values).\nNovember 6, 2014 DRAFT\n25\nSince the agents are getting negative reward at every time step (because of energy consumption) they want to know how to reach the food, while losing as less energy as possible. A natural policy, denoted \u03c01, could be to go straight to the food with high (0.8) probability and low (0.2) probability of going in another direction, but then the agents would face the harmful predator and the total expected reward may be low. Thus, we say that \u03c01 is a myopic policy. Another more insightful policy, denoted \u03c02, could be to take a detour and avoid the predator\u2019s area with very high (0.95) probability. Nevertheless, if the detour takes too long, then the agents would consume too much energy and it may not be worth trying. In order to evaluate which policy is better (myopic \u03c01 or detour \u03c02), the agents have to learn the value vector of each candidate-policy from samples. If the agents were learning on-policy, they would have to follow one candidate policy for long enough so they could apply stochastic optimization over the samples, then they would have to start again but following the other candidate policy. In other words, on-policy learning does not allow to reuse samples while evaluating different policies. The benefit of the off-policy formulation is that the agents can evaluate several target policies in parallel from a single data stream.\nWe consider the case in which the agents are territorial and tend to settle in different regions each. In other words, the behavior policies of the agents {\u03c6k} are all different and constrain exploration to some regions of the state-space. At every time-step, each agent is attracted to the state at the center of its territory with 0.8 probability, and it moves in a different direction with 0.2 probability. Since the agents only have samples of state-transitions in their respective territories, it is difficult for them to predict the value vector for each of the target policies (\u03c01 and \u03c02). However, since they sample complementary regions of the state-space, they can collaborate, applying diffusion strategies, to learn the value vector of the two target policies and evaluate which one is better.\nFigure 3 shows1 the exact value vector for the myopic and detour policies, as well as its cooperative\u2013 using the proposed diffusion GTD algorithm\u2013and non-cooperative approximation for one agent, under the considered constrained-exploration off-policy multi-agent setting. Figure 4 shows the learning curve of the algorithm. Since the agents have only samples from small portions of the state-space, the noncooperative algorithm may diverge. On the other hand, when the agents cooperate (i.e., communicate their estimates to their neighbors), the diffusion algorithm allows them to benefit from the experience from other agents in the network, and they approach the same solution as a centralized architecture (i.e., with a fusion center that gathers all the samples from every node) would achieve but more efficiently,\n1Code available at http://gaps.ssr.upm.es/images/sergio/tsp-coop-pred-20131004.zip\nNovember 6, 2014 DRAFT\n26\nby communicating only within neighborhoods."}, {"heading": "VI. CONCLUSION", "text": "Diffusion GTD maintains the efficiency of the single-agent GTD2 [9], with linear complexity in both computation time and memory footprint. With diffusion GTD, the agents learn directly from samples (without any apriori knowledge of the environment) and cooperate to improve the stability and accuracy of their prediction. We remark that cooperation is fully distributed with communications only within each agent\u2019s neighborhood; neither fusion-center, nor multi-hop communications are required.\nWe provided conditions that guarantee convergence of the proposed diffusion GTD and derived performance bounds for sufficiently small step-sizes. Although our analysis assumes stationarity, constant step-sizes are a desirable feature for an adaptive network, since it allows the network to learn continuously, and to track concept drifts in the data."}, {"heading": "APPENDIX A", "text": "PROOF OF THEOREM 1\nTo study the spectrum of C\u22a4 (I2MN \u2212 \u00b5R), we express the combination coefficient matrix in its Jordan canonical form:\nC\u22a4 = YCJCY \u22121 C (87)\nNovember 6, 2014 DRAFT\n27\nUsing the property (U \u2297 V )(Y \u2297 Z) = (UY )\u2297 (V Z), we obtain\nC\u22a4(I2MN \u2212 \u00b5R) = (YC \u2297 I2M )(JC \u2297 I2M )(Y \u22121C \u2297 I2M ) ( I2MN \u2212 \u00b5R )\n= (YC \u2297 I2M )(JC \u2297 I2M ) ( I2MN \u2212 \u00b5(Y \u22121C \u2297 I2M )R(YC \u2297 I2M ) ) (Y \u22121C \u2297 I2M ) (88)\nso that, by similarity,\n\u03bb ( C\u22a4 (I2MN \u2212 \u00b5R) ) = \u03bb ((JC \u2297 I2M ) (I2MN \u2212 \u00b5E)) (89)\nwhere\nE , (Y \u22121C \u2297 I2M )R(YC \u2297 I2M ) (90)\nNovember 6, 2014 DRAFT\n28\nAs stated in Section III, conditions (36)\u2013(37) ensure that C\u22a4 is a primitive right-stochastic matrix. Hence, from the Perron-Frobenius Theorem [47], the Jordan canonical form of C can be expressed as\nJC = diag { 1, J0C } (91)\nwhere all the eigenvalues of J0C are strictly inside the unit circle. Moreover, since C \u22a4 is right-stochastic it has one right-eigenvector of all ones associated with its unit eigenvalue, and its corresponding left eigenvector, p, has positive entries (i.e., pk > 0, 1 \u2264 k \u2264 N ):\nC\u22a41N = 1N , p\u22a4C\u22a4 = p\u22a4, 1\u22a4Np = 1 (92)\nWe therefore decompose\nY \u22121C = col { p\u22a4, Y lC } , YC = [1N Y r C ] (93)\nand partition E as\nE =   G\u0304 E12\nE21 E22\n  (94)\nNovember 6, 2014 DRAFT\n29\nG\u0304 , ( p\u22a4 \u2297 I2M ) R (1N \u2297 I2M ) = N\u2211\nk=1\npkGk (95)\nE12 , ( p\u22a4 \u2297 I2M ) R (Y rC \u2297 I2M ) (96) E21 , ( Y lC \u2297 I2M ) R (1N \u2297 I2M ) (97) E22 , ( Y lC \u2297 I2M ) R (Y rC \u2297 I2M ) (98)\nIntroduce the following shorthand in (89):\nS , (JC \u2297 I2M ) (I2MN \u2212 \u00b5E) (99)\nThen, expanding (91) and (94)-(98) into (99) we have\nS =  \nI2M \u2212 \u00b5G\u0304 \u2212\u00b5E12\n\u2212\u00b5 ( J0C \u2297 I2M ) E21 ( J0C \u2297 I2M ) (IL \u2212 \u00b5E22)\n  (100)\nwhere L , 2M(N \u2212 1). Using the same technique proposed in [51], [52], we appeal to eigenvalue perturbation analysis to examine the spectral radius of (100). We introduce the N \u00d7N diagonal matrix \u2126\u01ebN , diag{\u01eb, \u01eb2, \u01eb3, . . . , \u01ebN} with parameter \u01eb > 0. Let JG\u0304 = Y \u22121G\u0304 G\u0304YG\u0304 be the Jordan canonical form of G\u0304 and introduce the similarity transformation\n\u03a6 =   YG\u0304\u2126 \u01eb 2M 02M\u00d7L\n0L\u00d72M \u221a \u00b5 \u03c3 \u2126\u03b2N\u22121 \u2297 I2M\n  (101)\nwith parameters \u01eb, \u03b2, and \u03c3. We apply the similarity transformation (101) to S:\n\u03a6\u22121S\u03a6 =   I2M \u2212 \u00b5J \u01ebG\u0304 \u2212 \u00b5 \u221a \u00b5 \u03c3 S12\n\u2212\u03c3\u221a\u00b5 S21 J0\u03b2C \u2297 I2M \u2212 \u00b5S22\n  (102)\nwhere\nS12 , (\u2126\u01eb2M )\u22121Y \u22121G\u0304 E12 ( \u2126\u03b2N\u22121 \u2297 I2M ) (103) S21 , (( (\u2126\u03b2N\u22121) \u22121J0C ) \u2297 I2M ) E21YG\u0304\u2126\u01eb2M (104) S22 , (( (\u2126\u03b2N\u22121) \u22121J0C ) \u2297 I2M ) E22 ( \u2126\u03b2N\u22121 \u2297 I2M ) (105)\nand J \u01eb G\u0304 and J0\u03b2C have the same form as the (upper triangular) Jordan canonical forms JG\u0304 and J 0 C , except that the unit entries are replaced by \u01eb and \u03b2, respectively. By applying Gerschgorin theorem [47] to (102),\nNovember 6, 2014 DRAFT\n30\nwe can identify the regions where the eigenvalues of S should lie: \u2223\u2223\u2223\u03bb(S)\u2212 ( 1\u2212 \u00b5\u03bbm(G\u0304) )\u2223\u2223\u2223 \u2264 \u00b5\u01eb+ \u00b5 \u221a \u00b5\n\u03c3\nL\u2211\nq=1\n\u2223\u2223\u2223[S12]mq \u2223\u2223\u2223 (106)\n\u2223\u2223\u2223\u03bb(S)\u2212 ( \u03bbk+1(C)\u2212 \u00b5 [S22]mm )\u2223\u2223\u2223 \u2264 \u03b2 + \u03c3\u221a\u00b5 2M\u2211\nq=1\n\u2223\u2223\u2223[S21]mq \u2223\u2223\u2223+ \u00b5 L\u2211\nq=1 q 6=m\n\u2223\u2223\u2223[S22]mq \u2223\u2223\u2223 (107)\nwhere [\u00b7]mq stands for the element at row m and column q of a matrix. Although we use the same subscript m in both equations, note that 1 \u2264 m \u2264 2M in (106) while 1 \u2264 m \u2264 L in (107); in addition, recall that C is an N \u00d7N matrix, hence, we use subscript k = ceil{m/(2M)}, where ceil{\u00b7} rounds a real number to its nearest greater or equal integer.\nWe are looking for sufficient conditions that guarantee that the mean recursion (62) converges for small step-size. Recall from (89) that (62) converges when |\u03bb(S)| < 1. Let us solve for |\u03bb(S)| in (106) first. Since |z| \u2212 |y| \u2264 |z \u2212 y|, we obtain\n|\u03bb(S)| \u2212 \u2223\u22231\u2212 \u00b5\u03bbm(G\u0304) \u2223\u2223 \u2264 \u2223\u2223\u03bb(S)\u2212 (1\u2212 \u00b5\u03bbm(G\u0304)) \u2223\u2223 \u2264 \u00b5\u01eb+ \u00b5 \u221a \u00b5\n\u03c3 \u03c7(12) (108)\nwhere \u03c7(12) , \u2211L\nq=1 \u2223\u2223\u2223[S12]mq \u2223\u2223\u2223. Therefore,\n|\u03bb(S)| \u2264 \u00b5\u01eb+ \u00b5 \u221a \u00b5\n\u03c3 \u03c7(12) +\n\u2223\u22231\u2212 \u00b5\u03bbm(G\u0304) \u2223\u2223 (109)\nSince S , G\u0304 and C are not generally guaranteed to be symmetric, their eigenvalues may be complex. Using the fact that 1\u2212 z \u2264 ( 1\u2212 12z )2 for z \u2208 R, we obtain\n\u2223\u22231\u2212 \u00b5\u03bbm(G\u0304) \u2223\u2223 \u2264 1\u2212 \u00b5Re{\u03bbm(G\u0304)}+ \u00b52\n2\n\u2223\u2223\u03bbm(G\u0304) \u2223\u22232 (110)\nwhere Re{\u00b7} denotes the real part of a complex number. Combining (109) and (110), the stability condition implied by (106) requires finding small step-sizes \u00b5 such that\n\u00b5 |\u03bbm(G\u0304)|2\n2 +\n\u221a \u00b5 \u03c7(12)\n\u03c3 + \u01eb\u2212 Re{\u03bbm(G\u0304)} < 0 (111)\nwhich leads to\n0< \u00b5 <\n \u2212 \u03c7(12) \u03c3 + \u221a(\u03c7(12) \u03c3 )2 + 2|\u03bbm(G\u0304)|2(Re{\u03bbm(G\u0304)} \u2212 \u01eb) |\u03bbm(G\u0304)|2   2\n(112)\nwhere, in order to guarantee that the term inside the square root in the right side of (112) is positive, we\nNovember 6, 2014 DRAFT\n31\nchoose\n0 < \u01eb < min 1\u2264m\u22642M\nRe{\u03bbm(G\u0304)} (113)\nWe now show that Re{\u03bbm(G\u0304)} is always positive. If we transform G\u0304 into a similar matrix:\nG\u0304\u221a\u03b7 ,\n  IM 0M\u00d7M\n0M\u00d7M \u221a \u03b7IM\n  G\u0304   IM 0M\u00d7M\n0M\u00d7M 1\u221a \u03b7 IM\n  =  \n\u221a \u03b7X\u22a4D\u03c6X X\u22a4D\u03c6(IS \u2212 \u03b3P\u03c0)X\n\u2212X(IS \u2212 \u03b3P\u03c0)\u22a4D\u03c6X\u22a4 0M\u00d7M\n  (114)\nand use [41, Theorem 3.6] on G\u0304\u221a\u03b7, we can establish that Re{\u03bbm(G\u0304)} > 0. Now, we solve for |\u03bb(S)| from (107). Let us abbreviate the sums in the right side of (107) as\n\u03c7(21) ,\n2M\u2211\nq=1\n|[S21]mq| , \u03c7(22) , L\u2211\nq=1 q 6=m\n|[S22]mq| (115)\nIn a manner similar to (108), we have\n|\u03bb(S)| \u2212 |\u03bbk+1(C)\u2212 \u00b5[S22]mm| \u2264 \u03b2 + \u03c3 \u221a \u00b5\u03c7(21) + \u00b5\u03c7(22) (116)\nUsing (116) and the fact |z \u2212 y| \u2264 |z|+ |y| yields the following condition on \u00b5 for stability:\n\u221a \u00b5 (\u221a \u00b5 ( \u03c7(22) + |[S22]mm| ) +\u03c3\u03c7(21) ) < 1\u2212 |\u03bbk+1(C)| \u2212 \u03b2 (117)\nThe following conditions on the step-size are jointly sufficient to satisfy (117):\n0 < \u221a \u00b5 < 1\u2212 |\u03bbk+1(C)| \u2212 \u03b2 (118)\n0 < \u221a \u00b5 < 1\u2212 \u03c3\u03c7(21) \u03c7(22) + |[S22]mm|\n(119)\nFrom the Perron-Frobenius, we know that \u03bbk+1(C) < 1, for 1 \u2264 k \u2264 N \u2212 1. Moreover, Assumption 3 guarantees that any element of S is bounded from below and above. Therefore, there exist parameters 0 < \u03b2 < 1 \u2212 |\u03bbk+1(C)| and 0 < \u03c3 < 1/\u03c7(21) that make the right side of (118) and (119) positive, respectively. Hence, we can square both inequalities and obtain the following conditions:\n0 < \u00b5 < (1\u2212 |\u03bbk+1(C)| \u2212 \u03b2)2 (120)\n0 < \u00b5 <\n( 1\u2212 \u03c3\u03c7(21)\n\u03c7(22) + |[S22]mm|\n)2 (121)\nNovember 6, 2014 DRAFT\n32\nLet us bound (112), (120) and (121) by\n\u00b5o = min    (1\u2212 |\u03bbk+1(C)| \u2212 \u03b2)2 ,   1\u2212 \u03c3\u03c7(21) \u03c7(22) + |[S22]jj|   2 ,\n \u2212 \u03c7(12) \u03c3 + \u221a(\u03c7(12) \u03c3 )2 + 2|\u03bbm(G\u0304)|2(Re{\u03bbm(G\u0304)} \u2212 \u01eb) |\u03bbm(G\u0304)|2   2   \n(122)\nfor 1 \u2264 m \u2264 2M , 1 \u2264 k \u2264 N \u2212 1 and 1 \u2264 j \u2264 L. We conclude that if the step-size 0 < \u00b5 < \u00b5o, then diffusion GTD is mean-stable.\nAs a final remark, note that \u00b5o depends on the eigenvalues of G\u0304, the eigenvalues of the weightedtopology matrix C , and the constructed matrix S in (100) (note that \u03c3, \u01eb and \u03b2 are similarity parameters, and the terms \u03c7(22) and \u03c7(22) are defined in (115) simply as a short hand of sums of the elements in S). Recall that G\u0304 is given by (95) as the weighted sum of the individual Gk, which only depend on the data samples, the importance weights and the step-size ratio parameter \u03b7. Finally, recall that S depends on the Jordan canonical form of C and R, where the latter is defined in (63) from the individual Gk. Thus, all the terms involved in \u00b5o are input data to the algorithm."}, {"heading": "APPENDIX B", "text": "PROOF OF THEOREM 3\nWe follow an argument similar to [18], [53]. It suffices to show that lim\u00b5\u21920 \u2016\u03b1\u0303\u221e\u2016\n\u00b5 = \u03b5o, where \u03b5o is\na constant independent of \u00b5. Substituting (88), (90) and (100) into (66) yields\n\u03b1\u0303\u221e = \u00b5 (YC \u2297 I2M ) (I2MN \u2212 S)\u22121 (Y \u22121C \u2297 I2M )C\u22a4 (G\u03b1o + g) (123)\nExpanding (54), (87) and (93) into (123) leads to\n\u03b1\u0303\u221e = \u00b5(YC \u2297 I2M )(I2MN \u2212 S)\u22121(JC \u2297 I2M )   ( p\u22a4 \u2297 I2M ) (G\u03b1o + g)\n( Y lC \u2297 I2M ) (G\u03b1o + g)\n  (124)\nFrom now on, assume that the weights used in (31) in defining the global cost are the entries of the Perron eigenvector of C , i.e., p , \u03c4 . Then, the first row of the last term in (124) stands for the saddle-point conditions of the Lagrangian of the global problem (33):\n(\u03c4\u22a4\u2297I2M )(G\u03b1o + g) = N\u2211\nk=1\n\u03c4k (Gk\u03b1 o + gk) =\n[ \u03b7X\u22a4D\u03c6(X\u03b8o + (IS \u2212 \u03b3P\u03c0)Xwo \u2212 r\u03c0)\n\u2212X\u22a4(IS \u2212 \u03b3P\u03c0)\u22a4D\u03c6X\u03b8o\n] = 02M (125)\nNovember 6, 2014 DRAFT\n33\nTherefore, expanding JC into (124) yields\n\u03b1\u0303\u221e = \u00b5(YC \u2297 I2M )(I2MN \u2212 S)\u22121  \n02M\n(J0C \u2297 I2M )(Y lC \u2297 I2M ) (G\u03b1o + g)\n  (126)\nFrom (100), we know that the upper-left block of I2MN \u2212 S is given by \u00b5G\u0304. By using [41, Theorem 3.6], we establish that the similar matrix G\u0304\u221a\u03b7 in (114) is invertible. Therefore, G\u0304 is also invertible so we can use the following relation\n[ Z11 Z12\nZ21 Z22\n]\u22121 = [ Z\u22121 11 + Z\u22121 11 Z12UZ21Z \u22121 11 \u2212Z\u22121 11 Z12U\n\u2212UZ21Z\u2212111 U\n]\nwhere U = (Z22 \u2212 Z21Z\u2212111 Z12)\u22121, to write\n(I2MN \u2212 S)\u22121 =   H11 H12\nH21 H22\n  (127)\nIn this way, relation (126) simplifies to\n\u03b1\u0303\u221e = \u00b5 (YC \u2297 I2M )   H12(J 0 CY l C \u2297 I2M )(G\u03b1o + g)\nH22(J 0 CY l C \u2297 I2M )(G\u03b1o + g)\n  (128)\nThe only terms in (128) that depend on \u00b5 are H12 and H22. In the limit, when \u00b5 \u2192 0 these two terms become independent of \u00b5:\nlim \u00b5\u21920\nH12 = \u2212G\u0304\u22121E12(I2MN \u2212 J0C \u2297 I2M )\u22121 (129)\nlim \u00b5\u21920\nH22 = (I2MN \u2212 J0C \u2297 I2M )\u22121 (130)\nHence, we can conclude that lim\u00b5\u21920 \u2016\u03b1\u0303\u221e\u2016\n\u00b5 = \u03b5o."}], "references": [{"title": "Cooperative off-policy prediction of Markov decision processes in adaptive networks", "author": ["S.V. Macua", "J. Chen", "S. Zazo", "A.H. Sayed"], "venue": "Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP), Vancouver, British Columbia, Canada, May 2013, pp. 4539\u20134543.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "Proc. Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS), vol. 2, Taipei, Taiwan, 2011, pp. 761\u2013768. November 6, 2014  DRAFT  34", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Scaling-up knowledge for a cognizant robot", "author": ["T. Degris", "J. Modayil"], "venue": "Notes AAAI Spring Symposium Series, Palo Alto, CA, USA, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-timescale nexting in a reinforcement learning robot", "author": ["J. Modayil", "A. White", "R.S. Sutton"], "venue": "Adaptive Behavior, vol. 22, no. 2, pp. 146\u2013160, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["L. M"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Dynamic Programming and Optimal Control, 4th ed", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A convergent O(n) temporal-difference algorithm for off-policy learning with linear function approximation", "author": ["R.S. Sutton", "C. Szepesvari", "H.R. Maei"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS) 21, Vancouver, British Columbia, Canada, 2008, pp. 1609\u20131616.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesvari", "E. Wiewiora"], "venue": "Proc. Int. Conf. on Machine Learning (ICML), Montreal, Quebec, Canada, 2009, pp. 993\u20131000.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Distributed asynchronous deterministic and stochastic gradient optimization algorithms", "author": ["J. Tsitsiklis", "D. Bertsekas", "M. Athans"], "venue": "IEEE Transactions on Automatic Control, vol. 31, no. 9, pp. 803\u2013812, 1986.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1986}, {"title": "Distributed subgradient methods for multi-agent optimization", "author": ["A. Nedic", "A. Ozdaglar"], "venue": "IEEE Transactions on Automatic Control, vol. 54, no. 1, pp. 48\u201361, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Convergence rate analysis of distributed gossip (linear parameter) estimation: Fundamental limits and tradeoffs", "author": ["S. Kar", "J. Moura"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 5, no. 4, pp. 674\u2013690, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Decentralized parameter estimation by consensus based stochastic approximation", "author": ["S. Stankovic", "M. Stankovic", "D. Stipanovic"], "venue": "IEEE Transactions on Automatic Control, vol. 56, no. 3, pp. 531\u2013543, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Consensus problems in networks of agents with switching topology and time-delays", "author": ["R. Olfati-Saber", "R. Murray"], "venue": "IEEE Transactions on Automatic Control, vol. 49, pp. 1520\u20131533, Sep 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis", "author": ["C. Lopes", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3122 \u20133136, July 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion LMS Strategies for Distributed Estimation", "author": ["F.S. Cattivelli", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 3, pp. 1035\u20131048, March 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Diffusion adaptation strategies for distributed optimization and learning over networks", "author": ["J. Chen", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 8, pp. 4289\u20134305, Aug. 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed Pareto optimization via diffusion strategies", "author": ["\u2014\u2014"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 7, no. 2, pp. 205\u2013220, Apr. 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Diffusion adaptation over networks", "author": ["A.H. Sayed"], "venue": "Academic Press Library in Signal Processing, R. Chellapa and S. Theodoridis, Eds. Elsevier, 2014, vol. 3, pp. 323\u2013454. Also available as arXiv:1205.4220v1, May 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Diffusion strategies for adaptation and learning over networks", "author": ["A.H. Sayed", "S.-Y. Tu", "J. Chen", "X. Zhao", "Z.J. Towfic"], "venue": "IEEE Signal Processing Magazine, vol. 30, no. 3, pp. 155\u2013171, May 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive networks", "author": ["A.H. Sayed"], "venue": "Proceedings of the IEEE, vol. 102, no. 4, pp. 460\u2013497, April 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Diffusion strategies outperform consensus strategies for distributed estimation over adaptive networks", "author": ["S.-Y. Tu", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 12, pp. 6217\u20136234, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "GQ(\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H.R. Maei", "R.S. Sutton"], "venue": "Proc. Conference on Artificial General Intelligence (AGI), vol. 1, Lugano, Switzerland, 2010, pp. 91\u201396. November 6, 2014  DRAFT  35", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "QD-learning: A collaborative distributed strategy for multi-agent reinforcement learning through consensus + innovations", "author": ["S. Kar", "J.M.F. Moura", "H.V. Poor"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 7, pp. 1848\u20131862, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1848}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "Proc. Int. Conf. on Machine Learning (ICML), Tahoe City, CA, USA, 1995, pp. 30\u201337.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE Transactions on Automatic Control, vol. 42, no. 5, pp. 674\u2013690, 1997.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "The Borkar-Meyn theorem for asynchronous stochastic approximations", "author": ["S. Bhatnagar"], "venue": "Systems and Control Letters, vol. 60, no. 7, pp. 472\u2013478, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed value functions", "author": ["J. Schneider", "W.-K. Wong", "A. Moore", "M. Riedmiller"], "venue": "Proc. Int. Conf. on Machine Learning (ICML), Bled, Slovenia, 1999, pp. 371\u2013378.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Efficient distributed reinforcement learning through agreement", "author": ["P. Varshavskaya", "L. Kaelbling", "D. Rus"], "venue": "Distributed Autonomous Robotic Systems 8, H. Asama, H. Kurokawa, J. Ota, and K. Sekiyama, Eds. Springer Berlin Heidelberg, 2009, pp. 367\u2013378.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Should one compute the temporal difference fix point or minimize the Bellman residual? The unified oblique projection view", "author": ["B. Scherrer"], "venue": "Proc. Int. Conf. on Machine Learning (ICML), Haifa, Israel, 2010, pp. 959\u2013966.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Parametric value function approximation: A unified view", "author": ["M. Geist", "O. Pietquin"], "venue": "IEEE Symp. on Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), Paris, France, 2011, pp. 9\u201316.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Basis function adaptation in temporal difference reinforcement learning", "author": ["I. Menache", "S. Mannor", "N. Shimkin"], "venue": "Annals of Operations Research, vol. 134, pp. 215\u2013238, 2005.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning", "author": ["R. Parr", "L. Li", "G. Taylor", "C. Painter-Wakefield", "M. Littman"], "venue": "Proc. Int. Conf. on Machine Learning (ICML), Helsinki, Finland, 2008, pp. 752\u2013759.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Basis function adaptation methods for cost approximation in MDP", "author": ["H. Yu", "D.P. Bertsekas"], "venue": "Proc. IEEE Symp. on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), Nashville, TN, USA, 2009, pp. 74\u201381.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning representation and control in Markov decision processes: New frontiers", "author": ["S. Mahadevan"], "venue": "Foundations and Trends in Machine Learning, vol. 1, no. 4, pp. 403\u2013565, Apr. 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Predictive state temporal difference learning", "author": ["B. Boots", "G.J. Gordon"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS) 23, 2010, pp. 271\u2013279.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Sketch-based linear value function approximation", "author": ["M.G. Bellemare", "J. Veness", "M. Bowling"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS) 25, 2012, pp. 2222\u20132230.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive Filters", "author": ["A.H. Sayed"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2008}, {"title": "Numerical solution of saddle point problems", "author": ["M. Benzi", "G.H. Golub", "J. Liesen"], "venue": "Acta Numerica, vol. 14, pp. 1\u2013137, 2005.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2005}, {"title": "Introduction to Optimization", "author": ["B.T. Polyak"], "venue": "Optimization Software Inc.,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1987}, {"title": "Studies in Linear and Non-linear Programming", "author": ["K.J. Arrow", "L. Hurwicz", "H. Uzawa"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1958}, {"title": "On the limiting behavior of distributed optimization strategies", "author": ["J. Chen", "A.H. Sayed"], "venue": "Proc. Annual Allerton Conference on Communication, Control, and Computing, Monticello, IL, USA, October 2012, pp. 1535\u20131542.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-negative Matrices and Markov Chains", "author": ["E. Seneta"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "The O.D.E. method for convergence of stochastic approximation and reinforcement learning", "author": ["V.S. Borkar", "S. Meyn"], "venue": "SIAM Journal on Control and Optimization, vol. 38, pp. 447\u2013469, 1999.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1999}, {"title": "Transient analysis of data-normalized adaptive filters", "author": ["T.Y. Al-Naffouri", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 51, no. 3, pp. 639\u2013652, 2003.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2003}, {"title": "Convergence in multiagent coordination, consensus, and flocking", "author": ["V. Blondel", "J. Hendrickx", "A. Olshevsky", "J. Tsitsiklis"], "venue": "Proc. IEEE Conf. on Decision and Control, and European Control Conf. (CDC-ECC), Seville, Spain, 2005, pp. 2996\u2013 3000.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2005}, {"title": "Asynchronous adaptation and learning over networks \u2014 Part II: Performance analysis", "author": ["X. Zhao", "A.H. Sayed"], "venue": "submitted for publication. Also available as arXiv:1312.5438, Dec. 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "The learning behavior of adaptive networks \u2014 Part I: Transient analysis", "author": ["J. Chen", "A.H. Sayed"], "venue": "submitted for publication. Also available as arXiv:1312.7581, Dec. 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Performance limits for distributed estimation over LMS adaptive networks", "author": ["X. Zhao", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 10, pp. 5107\u20135124, 2012. November 6, 2014  DRAFT", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "A short preliminary version dealing with a special case of this work appears in the conference publication [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "This problem of predicting the response to a target policy different from the behavior policy is commonly referred as off-policy learning [2].", "startOffset": 138, "endOffset": 141}, {"referenceID": 2, "context": "Off-policy learning has been claimed to be necessary when the agents need to perform tasks in complex environments because they could perform many different predictions in parallel from a single stream of data [3]\u2013[5].", "startOffset": 210, "endOffset": 213}, {"referenceID": 4, "context": "Off-policy learning has been claimed to be necessary when the agents need to perform tasks in complex environments because they could perform many different predictions in parallel from a single stream of data [3]\u2013[5].", "startOffset": 214, "endOffset": 217}, {"referenceID": 1, "context": "The predictions by the agents are made in the form of value functions [2], [6], [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "The predictions by the agents are made in the form of value functions [2], [6], [7].", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "The predictions by the agents are made in the form of value functions [2], [6], [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "It was originally proposed for the single agent scenario in [8], [9], and derived by means of the stochastic optimization of a suitable cost function.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "It was originally proposed for the single agent scenario in [8], [9], and derived by means of the stochastic optimization of a suitable cost function.", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18].", "startOffset": 99, "endOffset": 103}, {"referenceID": 14, "context": "There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18].", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18].", "startOffset": 134, "endOffset": 138}, {"referenceID": 18, "context": "There are several forms of diffusion; recent overviews appear in [19]\u2013[21].", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "There are several forms of diffusion; recent overviews appear in [19]\u2013[21].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "It has been shown in [22] that the dynamics of diffusion networks leads to enhanced stability and lower mean-", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "In particular, the analysis in [20]\u2013[22] shows that consensus networks combine local data and in-neighborhood information asymmetrically, which can make the state of consensus networks grow unbounded even when all individual agents are mean stable in isolation.", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "In particular, the analysis in [20]\u2013[22] shows that consensus networks combine local data and in-neighborhood information asymmetrically, which can make the state of consensus networks grow unbounded even when all individual agents are mean stable in isolation.", "startOffset": 36, "endOffset": 40}, {"referenceID": 8, "context": "As a byproduct of this derivation, we show that the GTD algorithm, motivated as a two time-scales stochastic approximation in [9], is indeed a stochastic Arrow-Hurwicz algorithm applied to the dual problem of the original formulation.", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23].", "startOffset": 188, "endOffset": 191}, {"referenceID": 8, "context": "The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23].", "startOffset": 193, "endOffset": 196}, {"referenceID": 22, "context": "The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23].", "startOffset": 198, "endOffset": 202}, {"referenceID": 23, "context": "For example, the work in [24] proposes a useful algorithm, named QD-learning, which is a distributed implementation of Q-learning using consensus-based stochastic approximation.", "startOffset": 25, "endOffset": 29}, {"referenceID": 23, "context": "However, QD-learning is developed in [24] under the assumption of perfect-knowledge of the state.", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "Here, we study the case in which the agents only know a feature representation of the state, which is used to build a parametric approximation of the value function, allowing us to tackle large problems, for which Q-learning schemes can diverge [25], [26].", "startOffset": 245, "endOffset": 249}, {"referenceID": 25, "context": "Here, we study the case in which the agents only know a feature representation of the state, which is used to build a parametric approximation of the value function, allowing us to tackle large problems, for which Q-learning schemes can diverge [25], [26].", "startOffset": 251, "endOffset": 255}, {"referenceID": 23, "context": "In comparison, the analysis in [24] employs a diminishing step-size that dies out as time progresses and, therefore, turns off adaptation and is not able to track concept drifts in the data.", "startOffset": 31, "endOffset": 35}, {"referenceID": 26, "context": "Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26].", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26].", "startOffset": 305, "endOffset": 309}, {"referenceID": 25, "context": "Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26].", "startOffset": 311, "endOffset": 315}, {"referenceID": 26, "context": "In addition, although the algorithm in [27] is distributed, in the sense that there is no fusion center, it requires full connectivity (i.", "startOffset": 39, "endOffset": 43}, {"referenceID": 27, "context": "Other related\u2014but more heuristic\u2014approaches include [28], [29].", "startOffset": 52, "endOffset": 56}, {"referenceID": 28, "context": "Other related\u2014but more heuristic\u2014approaches include [28], [29].", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "Markov decision processes (MDP) We consider Markov decision processes (MDP) [6], [7] that are characterized by a finite set of states S of size S , |S|; a finite set of actions A; the kernel of transition probabilities P(s\u2032|s, a), which gives the probability of going from one state s to another state s\u2032, given an action a; and the reward function r : S \u00d7A\u00d7 S \u2192 R that the agent wants to predict, which is associated with every transition, such that r (s, a, s\u2032) denotes the reward received by a generic agent for the transition from s to s\u2032 after taking action a.", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "Markov decision processes (MDP) We consider Markov decision processes (MDP) [6], [7] that are characterized by a finite set of states S of size S , |S|; a finite set of actions A; the kernel of transition probabilities P(s\u2032|s, a), which gives the probability of going from one state s to another state s\u2032, given an action a; and the reward function r : S \u00d7A\u00d7 S \u2192 R that the agent wants to predict, which is associated with every transition, such that r (s, a, s\u2032) denotes the reward received by a generic agent for the transition from s to s\u2032 after taking action a.", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "Value function In order to make predictions of the reward signal, we use state value functions, v : S \u2192 R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]\u2013[7].", "startOffset": 214, "endOffset": 217}, {"referenceID": 4, "context": "Value function In order to make predictions of the reward signal, we use state value functions, v : S \u2192 R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]\u2013[7].", "startOffset": 219, "endOffset": 222}, {"referenceID": 6, "context": "Value function In order to make predictions of the reward signal, we use state value functions, v : S \u2192 R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]\u2013[7].", "startOffset": 223, "endOffset": 226}, {"referenceID": 1, "context": "Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = E\u03c0,P [r(i+ 1) + \u03b3r(i+ 2) + .", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = E\u03c0,P [r(i+ 1) + \u03b3r(i+ 2) + .", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = E\u03c0,P [r(i+ 1) + \u03b3r(i+ 2) + .", "startOffset": 118, "endOffset": 121}, {"referenceID": 7, "context": "Approximate value function as a saddle-point problem For the single agent scenario, references [8], [9] introduced efficient algorithms with convergence guarantees under general conditions.", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "Approximate value function as a saddle-point problem For the single agent scenario, references [8], [9] introduced efficient algorithms with convergence guarantees under general conditions.", "startOffset": 100, "endOffset": 103}, {"referenceID": 25, "context": ", [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands.", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": ", [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": ", [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands.", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": ", [33]\u2013[38] ).", "startOffset": 2, "endOffset": 6}, {"referenceID": 36, "context": ", [33]\u2013[38] ).", "startOffset": 7, "endOffset": 11}, {"referenceID": 25, "context": "To address this issue, one approach is to solve instead the projected Bellman equation [26]:", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "[9] considered the weighted least-squares problem:", "startOffset": 0, "endOffset": 3}, {"referenceID": 37, "context": "Using (15), it can also be verified that the solution w that minimizes JPB(w) satisfies the following normal equations [39]: B\u22a4(X\u22a4DX)\u22121Bw\u22c6 = B\u22a4(X\u22a4DX)\u22121X\u22a4Dr\u03c0 (16) Since \u2016P \u2016\u221e = 1 and \u03b3 < 1, we can bound the spectral radius of \u03b3P \u03c0 by \u03c1(\u03b3P ) \u2264 \u2016\u03b3P \u2016\u221e = \u03b3 < 1 (17) Thus, the inverse (IS \u2212 \u03b3P \u03c0)\u22121 exists.", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "In the process of doing so, first for single-agents, we shall arrive at the same gradient temporal difference method of [9] albeit by using a fundamentally different approach involving a primal-dual argument.", "startOffset": 120, "endOffset": 123}, {"referenceID": 38, "context": ", [41], [42, Ch.", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": "By using importance sampling, reference [23] showed that we can write the gradient inside (24a) in terms of moment values of the behavior policy as follows: X\u22a4D\u03c6 (X\u03b8i + (IS \u2212 \u03b3P )Xwi \u2212 r)", "startOffset": 40, "endOffset": 44}, {"referenceID": 8, "context": "Recursions (29a)\u2013(29b) coincide with the single-agent gradienttemporal difference (GTD2) algorithm, which was derived in [9] using a different approach.", "startOffset": 121, "endOffset": 124}, {"referenceID": 20, "context": "Motivated by recent results on network behavior in [21], [45], we note that, through collaboration, each agent may contribute to the network with its own experience.", "startOffset": 51, "endOffset": 55}, {"referenceID": 41, "context": "Motivated by recent results on network behavior in [21], [45], we note that, through collaboration, each agent may contribute to the network with its own experience.", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "In order to find the global saddle-point of the aggregate Lagrangian (33) in a cooperative and stochastic manner, we apply diffusion strategies [19]\u2013[21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 20, "context": "In order to find the global saddle-point of the aggregate Lagrangian (33) in a cooperative and stochastic manner, we apply diffusion strategies [19]\u2013[21].", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45].", "startOffset": 106, "endOffset": 110}, {"referenceID": 41, "context": "We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45].", "startOffset": 112, "endOffset": 116}, {"referenceID": 18, "context": ", there exists j > 0 such that all entries of C are strictly positive) [19], [46].", "startOffset": 71, "endOffset": 75}, {"referenceID": 42, "context": ", there exists j > 0 such that all entries of C are strictly positive) [19], [46].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "We extend the energy conservation arguments of [16]\u2013[19] to perform a mean-squareerror (MSE) analysis of the diffusion GTD algorithm (35a)\u2013(35d) and provide convergence guarantees under sufficiently small step-sizes.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "We extend the energy conservation arguments of [16]\u2013[19] to perform a mean-squareerror (MSE) analysis of the diffusion GTD algorithm (35a)\u2013(35d) and provide convergence guarantees under sufficiently small step-sizes.", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": ", [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": ", [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.", "startOffset": 7, "endOffset": 10}, {"referenceID": 43, "context": ", [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 37, "context": "Using the Kronecker product property vec(Y \u03a3Z) = (Z\u22a4 \u2297 Y )vec(\u03a3) [39], we can vectorize \u03a3\u2032 in (67) and find that its vector form is related to \u03a3 via the following linear relation: \u03c3\u2032 , vec(\u03a3\u2032) = F\u03c3, where the matrix F is given by F , (( I2MN \u2212 \u03bcR\u22a4 ) C ) \u2297 (( I2MN \u2212 \u03bcR\u22a4 ) C ) + \u03bcE [( (Ri+1 \u2212R\u22a4)C ) \u2297 ( (Ri+1 \u2212R\u22a4)C )]", "startOffset": 65, "endOffset": 69}, {"referenceID": 37, "context": "To study the convergence of (71) we will expand it into a state-space model following [39], [49].", "startOffset": 86, "endOffset": 90}, {"referenceID": 44, "context": "To study the convergence of (71) we will expand it into a state-space model following [39], [49].", "startOffset": 92, "endOffset": 96}, {"referenceID": 37, "context": "By the Cayley-Hamilton Theorem [39], we know that every matrix satisfies its characteristic equation (i.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "Note, however, that if all agents followed the same behavior policy, their individual optimization problems would be identical, therefore, both the adaptation and the combination steps would pull them toward the global solution and their fixed-point estimates would be unbiased with respect to the solution of the global optimization problem (31), as stated in [1].", "startOffset": 361, "endOffset": 364}, {"referenceID": 18, "context": ", the clk elements of C) are obtained independently by each node following an averaging rule [19], [50], such that equal weight is given to any member of the neighborhood, including itself (i.", "startOffset": 93, "endOffset": 97}, {"referenceID": 45, "context": ", the clk elements of C) are obtained independently by each node following an averaging rule [19], [50], such that equal weight is given to any member of the neighborhood, including itself (i.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "CONCLUSION Diffusion GTD maintains the efficiency of the single-agent GTD2 [9], with linear complexity in both computation time and memory footprint.", "startOffset": 75, "endOffset": 78}, {"referenceID": 46, "context": "Using the same technique proposed in [51], [52], we appeal to eigenvalue perturbation analysis to examine the spectral radius of (100).", "startOffset": 37, "endOffset": 41}, {"referenceID": 47, "context": "Using the same technique proposed in [51], [52], we appeal to eigenvalue perturbation analysis to examine the spectral radius of (100).", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "APPENDIX B PROOF OF THEOREM 3 We follow an argument similar to [18], [53].", "startOffset": 63, "endOffset": 67}, {"referenceID": 48, "context": "APPENDIX B PROOF OF THEOREM 3 We follow an argument similar to [18], [53].", "startOffset": 69, "endOffset": 73}], "year": 2014, "abstractText": "We apply diffusion strategies to develop a fully-distributed cooperative reinforcement learning algorithm in which agents in a network communicate only with their immediate neighbors to improve predictions about their environment. The algorithm can also be applied to off-policy learning, meaning that the agents can predict the response to a behavior different from the actual policies they are following. The proposed distributed strategy is efficient, with linear complexity in both computation time and memory footprint. We provide a mean-square-error performance analysis and establish convergence under constant step-size updates, which endow the network with continuous learning capabilities. The results show a clear gain from cooperation: when the individual agents can estimate the solution, cooperation increases stability and reduces bias and variance of the prediction error; but, more importantly, the network is able to approach the optimal solution even when none of the individual agents can (e.g., when the individual behavior policies restrict each agent to sample a small portion of the state space).", "creator": "LaTeX with hyperref package"}}}