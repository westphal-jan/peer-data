{"id": "1401.0247", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jan-2014", "title": "Robust Hierarchical Clustering", "abstract": "much notably, some widely accepted techniques aiding data purification is agglomerative clustering. complex algorithms have been unsuccessfully fielded across many vast worlds ranging via symbolic biology with space sciences to computer vision in part realizing their output can easy to capture. inevitably, evaluation fails well known, however, that many of the classic agglomerative alignment algorithms are not robust under q \\ cite { qcluster2005 }. making this paper we define and take forth new robust algorithm assuming bottom - up comparative inference. calculations show as our algorithm approaches be used concerning cluster sampling in computers where the data satisfies a statement of grammatical properties right though any traditional agglomerative algorithms fail. problems also show how to adapt the algorithm onto simple inductive setting where any empirical data is only their small random sample of the entire data fragment. experimental evaluations / computational based real world data fields show that our algorithm consistently achieves better statistics given other classical algorithms in total presence of noise.", "histories": [["v1", "Wed, 1 Jan 2014 04:16:21 GMT  (896kb,D)", "https://arxiv.org/abs/1401.0247v1", "35 pages"], ["v2", "Sun, 13 Jul 2014 01:51:05 GMT  (896kb,D)", "http://arxiv.org/abs/1401.0247v2", "37 pages"]], "COMMENTS": "35 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["maria-florina balcan", "yingyu liang", "pramod gupta"], "accepted": false, "id": "1401.0247"}, "pdf": {"name": "1401.0247.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Maria-Florina Balcan", "Yingyu Liang", "Pramod Gupta"], "emails": ["ninamf@cs.cmu.edu.", "yliang39@gatech.edu.", "pramodg@google.com."], "sections": [{"heading": "1 Introduction", "text": "Many data mining and machine learning applications ranging from computer vision to biology problems have recently faced an explosion of data. As a consequence it has become increasingly important to develop effective, accurate, robust to noise, fast, and general clustering algorithms, accessible to developers and researchers in a diverse range of areas.\nOne of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006). In hierarchical clustering the goal is not to find a single partitioning of the data, but a hierarchy (generally represented by a tree) of partitions which may reveal interesting structure in the data at multiple levels of granularity. The most widely used hierarchical methods are the agglomerative clustering techniques; most of these techniques start with a separate cluster for each point and then progressively merge the two closest clusters until only a single cluster remains. In all cases, we assume that we have a measure of similarity between pairs of objects, but the different schemes are distinguished by how they convert this into a measure of similarity between two clusters. For example, in single linkage the similarity between two clusters is the maximum similarity between points in these two different clusters. In complete linkage, the similarity between two clusters is the minimum similarity between points in these\n\u2217A preliminary version of this article appeared under the title Robust Hierarchical Clustering in the Proceedings of the TwentyThird Conference on Learning Theory, 2010. \u2020ninamf@cs.cmu.edu. School of Computer Science, Carnegie Mellon University. \u2021yliang39@gatech.edu. College of Computing, Georgia Institute of Technology. \u00a7pramodg@google.com. Google, Inc..\nar X\niv :1\n40 1.\n02 47\nv2 [\ncs .L\nG ]\n1 3\nJu l 2\n01 4\ntwo different clusters. Average linkage defines the similarity between two clusters as the average similarity between points in these two different clusters (Dasgupta and Long, 2005; Jain et al., 1999).\nSuch algorithms have been used in a wide range of application domains ranging from biology to social sciences to computer vision mainly because they are quite fast and the output is quite easy to interpret. It is well known, however, that one of the main limitations of the agglomerative clustering algorithms is that they are not robust to noise (Narasimhan et al., 2006). In this paper we propose and analyze a robust algorithm for bottom-up agglomerative clustering. We show that our algorithm satisfies formal robustness guarantees and with proper parameter values, it will be successful in several natural cases where the traditional agglomerative algorithms fail.\nIn order to formally analyze correctness of our algorithm we use the framework introduced by Balcan et al. (2008). In this framework, we assume there is some target clustering (much like a k-class target function in the multi-class learning setting) and we say that an algorithm correctly clusters data satisfying property P if on any data set having property P , the algorithm produces a tree such that the target is some pruning of the tree. For example if all points are more similar to points in their own target cluster than to points in any other cluster (this is called the strict separation property), then any of the standard single linkage, complete linkage, and average linkage agglomerative algorithms will succeed1. See Figure 1 for an example. However, with just tiny bit of noise, for example if each point has even just one point from a different cluster that it is similar to, then these standard algorithms will all fail (we elaborate on this in Section 2.2). See Figure 2 for an example. This brings up the question: is it possible to design an agglomerative algorithm that is robust to these types of situations and more generally can tolerate a substantial degree of noise? The contribution of our paper is to provide a positive answer to this question; we develop a robust, linkage based algorithm that will succeed in many interesting cases where standard agglomerative algorithms will fail. At a high level, our new algorithm is robust to noise in two different and important ways. First, it uses more global information for determining the similarities between clusters; second, it uses a robust linkage procedure involving a median test for linking the clusters, eliminating the influence of the noisy similarities."}, {"heading": "1.1 Our Results", "text": "In particular, in Section 3 we show that if the data satisfies a natural good neighborhood property, then our algorithm can be used to cluster well in the tree model, that is, to output a hierarchy such that the target clustering is (close to) a pruning of that hierarchy. The good neighborhood property relaxes the strict separation property, and only requires that after a small number of bad points (which could be extremely malicious) have been removed, for the remaining good points in the data set, in the neighborhood of their target cluster\u2019s size, most of their nearest neighbors are from their target cluster. We show that our algorithm produces a hierarchy with a pruning that assigns all good points correctly. In Section 4, we further generalize this to allow for a good fraction of \u201cboundary\u201d points that do not fully satisfy the good neighborhood property. Unlike the good points, these points may have many nearest neighbors outside their target cluster in the neighborhood of their target cluster\u2019s size; but also unlike the bad points, they have additional structure: they fall into a sufficiently large subset of their target cluster, such that all points in this subset have most of their nearest neighbors from this subset. As long as the fraction of boundary points in such subsets is not too large, our algorithm can produce a hierarchy with a pruning that assigns all good and boundary points correctly.\n1We note however that the Ward\u2019s minimum variance method, another classic linkage based procedure, might fail under the strict separation property in the presence of unbalanced clusters. We provide a concrete example in Appendix C.\nWe further show how to adapt our algorithm to the inductive setting with formal correctness guarantees in Section 5. In this setting, the clustering algorithm only uses a small random sample over the data set and generates a hierarchy over this sample, which also implicitly represents a hierarchy over the entire data set. This is especially useful when the amount of data is enormous such as in astrophysics and biology. We prove that our algorithm requires only a small random sample whose size is independent of that of the entire data set and depends only on the noise and the confidence parameters.\nWe then perform experimental evaluations of our algorithm on synthetic data and real-world data sets. In controlled experiments on synthetic data presented in Section 6.1, our algorithm achieves results consistent with our theoretical analysis, outperforming several other hierarchical algorithms. We also show in Section 6.2 that our algorithm performs consistently better than other hierarchical algorithms in experiments on several real-world data. These experimental results suggest that the properties and the algorithm we propose can handle noise in real-world data as well. To obtain good performance, however, our algorithm requires tuning the noise parameters which roughly speaking quantify the extent to which the good neighborhood property is satisfied."}, {"heading": "1.2 Related Work", "text": "In agglomerative hierarchical clustering (Dasgupta and Long, 2005; Duda et al., 2000; Jain and Dubes, 1981; Jain et al., 1999), the goal is not to find a single partitioning of the data, but a hierarchy (generally represented by a tree) of partitionings which may reveal interesting structure in the data at multiple levels of granularity. Traditionally, only clusterings at a certain level are considered, but as we argue in Section 2 it is more desirable to consider all the prunings of the tree, since this way we can then handle much more general situations.\nAs mentioned above, it is well known that standard agglomerative hierarchical clustering techniques are not tolerant to noise (Nagy, 1968; Narasimhan et al., 2006). Several algorithms have been proposed to make the hierarchical clustering techniques more robust to noise, such as Wishart\u2019s method (Wishart, 1969), and CURE (Guha et al., 1998). Ward\u2019s minimum variance method (Ward, 1963) is also more preferable in the presence of noise. However, these algorithms have no theoretical guarantees for their robustness. Also, our empirical study demonstrates that our algorithm has better tolerance to noise.\nOn the theoretical side, Balcan et al. (2008) analyzed the \u03bd-strict separation property, a generalization of the simple strict separation property discussed above, requiring that after a small number of outliers have been removed all points are strictly more similar to points in their own cluster than to points in other clusters. They provided an algorithm for producing a hierarchy such that the target clustering is close to some pruning of the tree, but via a much more computationally expensive (non-agglomerative) algorithm. Our algorithm is simpler and substantially faster. As discussed in Section 2.1, the good neighborhood property is much broader than the \u03bd-strict separation property, so our algorithm is much more generally applicable compared to their algorithm specifically designed for \u03bd-strict separation.\nIn a different statistical model, Chaudhuri and Dasgupta (2010) proposed a generalization of Wishart\u2019s method. They proved that given a sample from a density function, the method constructs a tree that is consistent with the cluster tree of the density. Although not directly targeting at robustness, the analysis shows the method successfully identifies salient clusters separated by low density regions, which suggests the method can be robust to the noise represented by the low density regions.\nFor general clustering beyond hierarchical clustering, there are also works proposing robust algorithms and analyzing robustness of the algorithms; see (Garc\u0131\u0301a-Escudero et al., 2010) for a review. In particular, the trimmed k-means algorithm (Garc\u0131\u0301a-Escudero and Gordaliza, 1999), a variant of the classical k-means algorithm, updates the centers after trimming points that are far away and thus are likely to be noise. (Gal-\nlegos, 2002; Gallegos and Ritter, 2005) introduced an interesting mathematical probabilistic framework for clustering in the presence of outliers, and used maximum likelihood approach to estimate the underlying parameters. An algorithm combining the above two approaches is then proposed in (Garc\u0131\u0301a-Escudero et al., 2008). (Hennig, 2008; Ackerman et al., 2013) studied the robustness of the classical algorithms such as k-means from the perspective of how the clusters are changed after adding some additional points."}, {"heading": "1.3 Structure of the Paper", "text": "The rest of the paper is organized as follows. In Section 2, we formalize our model and define the good neighborhood property. We describe our algorithm and prove it succeeds under the good neighborhood property in Section 3. We then prove that it also succeeds under a generalization of the good neighborhood property in Section 4. In Section 5, we show how to adapt our algorithm to the inductive setting with formal correctness guarantees. We provide the experimental results in Section 6, and conclude the paper in Section 7."}, {"heading": "2 Definitions. A Formal Setup", "text": "We consider a clustering problem (S, `) specified as follows. Assume we have a data set S of n objects. Each x \u2208 S has some (unknown) \u201cground-truth\u201d label `(x) in Y = {1, . . . , k}, where we will think of k as much smaller than n. Let Ci = {x \u2208 S : `(x) = i} denote the set of points of label i (which could be empty), and denote the target clustering as C = {C1, . . . , Ck}. Let C(x) be a shorthand of Cl(x), and nC denote the size of a cluster C.\nGiven another proposed clustering h, h : S \u2192 Y , we define the error of h with respect to the target clustering to be\nerr(h) = min \u03c3\u2208Sk [ Pr x\u2208S [\u03c3(h(x)) 6= `(x)] ]\n(1)\nwhere Sk is the set of all permutations on {1, . . . , k}. Equivalently, the error of a clustering C\u2032 = {C \u20321, . . . , C \u2032k} is min\u03c3\u2208Sk 1 n \u2211 i |Ci\u2212C \u2032\u03c3(i)|. This is popularly known as Classification Error (Meila\u0306 and Heckerman, 2001; Balcan et al., 2013; Voevodski et al., 2012). We will be considering clustering algorithms whose only access to their data is via a pairwise similarity function K(x, x\u2032) that given two examples outputs a number in the range [\u22121, 1]. We will say that K is a symmetric similarity function ifK(x, x\u2032) = K(x\u2032, x) for all x, x\u2032. In this paper we assume that the similarity function K is symmetric.\nOur goal is to produce a hierarchical clustering that contains a pruning that is close to the target clustering. Formally, the goal of the algorithm is to produce a hierarchical clustering: that is, a tree on subsets such that the root is the set S, and the children of any node S\u2032 in the tree form a partition of S\u2032. The requirement is that there must exist a pruning h of the tree (not necessarily using nodes all at the same level) that has error at most . Balcan et al. (2008) have shown that this type of output is necessary in order to be able to analyze non-trivial properties of the similarity function. For example, even if the similarity function satisfies the requirement that all points are more similar to all points in their own cluster than to any point in any other cluster (this is called the strict separation property) and even if we are told the number of clusters, there can still be multiple different clusterings that satisfy the property. In particular, one can show examples of similarity functions and two significantly different clusterings of the data satisfying the strict separation property. See Figure 1 for an example. However, under the strict separation property, there is a single hierarchical\ndecomposition such that any consistent clustering is a pruning of this tree. This motivates clustering in the tree model, which is the model we consider in this work as well.\nGiven a similarity function satisfying the strict separation property (see Figure 1 for an example), we can efficiently construct a tree such that the ground-truth clustering is a pruning of this tree (Balcan et al., 2008). Moreover, the standard linkage single linkage, average linkage, and complete linkage algorithms would work well under this property. However, one can show that if the similarity function slightly deviates from the strict separation condition, then all these standard agglomerative algorithms will fail (we elaborate on this in section 2.2). In this context, the main question we address in this work is: Can we develop other more robust, linkage based algorithms that will succeed under more realistic and yet natural conditions on the similarity function?\nNote The strict separation property does not guarantee that all the cutoffs for different points x are the same, so single linkage would not necessarily have the right clustering if it just stopped once it has k clusters; however the target clustering will provably be a pruning of the final single linkage tree; this is why we define success based on prunings."}, {"heading": "2.1 Properties of the Similarity Function", "text": "We describe here some natural properties of the similarity functions that we analyze in this paper. We start with a noisy version of the simple strict separation property (mentioned above) which was introduced in (Balcan et al., 2008) and we then define an interesting and natural generalization of it.\nProperty 1. The similarity function K satisfies \u03bd-strict separation for the clustering problem (S, `) if for some S\u2032 \u2286 S of size (1 \u2212 \u03bd)n, K satisfies strict separation for (S\u2032, `). That is, for all x, x\u2032, x\u2032\u2032 \u2208 S\u2032 with x\u2032 \u2208 C(x) and x\u2032\u2032 6\u2208 C(x) we have K(x, x\u2032) > K(x, x\u2032\u2032).\nSo, in other words we require that the strict separation is satisfied after a number of bad points have been removed. A somewhat different condition is to allow each point to have some bad immediate neighbors as long as most of its immediate neighbors are good. Formally:\nProperty 2. The similarity function K satisfies \u03b1-good neighborhood property for the clustering problem (S, `) if for all points x we have that all but \u03b1n out of their nC(x) nearest neighbors belong to the cluster C(x).\nNote that the \u03b1-good neighborhood property is different from the \u03bd-strict separation property. For the \u03bd-strict separation property we can have up to \u03bdn bad points that can misbehave; in particular, these \u03bdn bad points can have similarity 1 to all the points in S; however, once we remove these points the remaining points are more similar to points in their own cluster than to points in other cluster. On the other hand, for the \u03b1-good neighborhood property we require that for all points x all but \u03b1n out of their nC(x) nearest neighbors belong to the cluster C(x). (So we cannot have a point that has similarity 1 to all the points in S.) Note however that different points might misbehave on different \u03b1n neighbors. We can also consider a property that generalizes both the \u03bd-strict separation and the \u03b1-good neighborhood property. Specifically:\nProperty 3. The similarity functionK satisfies (\u03b1, \u03bd)-good neighborhood property for the clustering problem (S, `) if for some S\u2032 \u2286 S of size (1\u2212 \u03bd)n, K satisfies \u03b1-good neighborhood for (S\u2032, `). That is, for all points x \u2208 S\u2032 we have that all but \u03b1n out of their nC(x)\u2229S\u2032 nearest neighbors in S\u2032 belong to the cluster C(x).\nClearly, the (\u03b1, \u03bd)-good neighborhood property is a generalization of both the \u03bd-strict separation and \u03b1-good neighborhood property. Formally,\nFact 1. If the similarity function K satisfies the \u03b1-good neighborhood property for the clustering problem (S, `), then K also satisfies the (\u03b1, 0)-good neighborhood property for the clustering problem (S, `).\nFact 2. If the similarity functionK satisfies the \u03bd-strict separation property for the clustering problem (S, `), then K also satisfies the (0, \u03bd)-good neighborhood property for the clustering problem (S, `).\nBalcan et al. (2008) have shown that if K satisfies the \u03bd-strict separation property with respect to the target clustering, then as long as the smallest target cluster has size 5\u03bdn, one can in polynomial time construct a hierarchy such that the ground-truth is \u03bd-close to a pruning of the hierarchy. Unfortunately the algorithm presented in (Balcan et al., 2008) is computationally very expensive: it first generates a large list of \u2126(n2) candidate clusters and repeatedly runs pairwise tests in order to laminarize these clusters; its running time is a large unspecified polynomial. The new robust linkage algorithm we present in Section 3 can be used to get a simpler and much faster algorithm for clustering accurately under the \u03bd-strict separation and the more general (\u03b1, \u03bd)-good neighborhood property.\nGeneralizations Our algorithm succeeds under an even more general property called weak good neighborhood, which allows a good fraction of points to only have nice structure in their small local neighborhoods. The relations between these properties are described in Section 4.1, and the analysis under the weak good neighborhood is presented in Section 4.2."}, {"heading": "2.2 Standard Linkage Based Algorithms Are Not Robust", "text": "As we show below, even if the data satisfies the good neighborhood property, the standard single linkage, average linkage, and complete linkage algorithms might fail. The contribution of our work is to develop\na robust, linkage based algorithm that will succeed under these natural conditions. More specifically, we can show an example where the standard single linkage, average linkage, and complete linkage algorithms would perform very badly, but where our algorithm would work well. In particular, let us slightly modify the example in Figure 1, by adding a little bit of noise, to form links of high similarity between points in different inner blobs2. See Figure 2 for a precise description of the similarity. In this example all the single linkage, average linkage, and complete linkage algorithms would in the first n/2 stages merge the matched pairs of points. From that moment on, no matter how they perform, none of the natural and desired clusterings will even be 1/2 close to any of the prunings of the hierarchy produced. Notice however, that K satisfies the \u03b1-good neighborhood with respect to any of the desired clusterings (for \u03b1 = 1/n), and that our algorithm will be successful on this instance. The \u03bd-strict separation is not satisfied in this example either, for any constant \u03bd."}, {"heading": "3 Robust Median Neighborhood Linkage", "text": "In this section, we propose a new algorithm, Robust Median Neighborhood Linkage, and show that it succeeds for instances satisfying the (\u03b1, \u03bd)-good neighborhood property.\nInformally, the algorithm maintains a threshold t and a list C\u2032t of subsets of points of S; these subsets are called blobs for convenience. We first initialize the threshold to a value t that is not too large and not too small (t = 6(\u03b1+ \u03bd)n+ 1), and initialize C\u2032t\u22121 to contain |S| blobs, one for each point in S. For each t, the algorithm builds C\u2032t from C\u2032t\u22121 by merging two or more blobs as follows. It first builds a graph Ft, whose\n2Since, usually, the similarity function between pairs of objects is constructed based on heuristics, this could happen in practice; for example we could have a similarity measure that puts a lot of weight on features such as date or names, and so we could easily have a document about Learning being more similar to a document about Football than to other documents about Learning. While this example seems a little bit contrived, in Figure 7 in Section 4 we will give a naturally occurring example where the standard single linkage, average linkage, and complete linkage algorithms still fail but our algorithm succeeds because it satisfies a generalization of the good neighborhood property that we will discuss in Section 4.\nAlgorithm 1 Robust Median Neighborhood Linkage Input: Similarity function K on a set of points S, n = |S|, noise parameters \u03b1 > 0, \u03bd > 0. Step 1 Initialize t = 6(\u03b1+ \u03bd)n+ 1.\nInitialize C\u2032t\u22121 to be a list of blobs so that each point is in its own blob. while |C\u2032t\u22121| > 1 do\nStep 2 Build a graph Ft whose vertices are points in S and whose edges are specified as follows.\nLet Nt(x) denote the t nearest neighbors of x. for any x, y \u2208 S that satisfy |Nt(x) \u2229Nt(y)| \u2265 t\u2212 2(\u03b1+ \u03bd)n do Connect x, y in Ft. end for\nStep 3 Build a graph Ht whose vertices are blobs in C\u2032t\u22121 and whose edges are specified as follows.\nLet NF (x) denote the neighbors of x in Ft. for any Cu, Cv \u2208 C\u2032t\u22121 do\nif Cu, Cv are singleton blobs, i.e. Cu = {x}, Cv = {y} then Connect Cu, Cv in Ht, if |NF (x) \u2229NF (y)| > (\u03b1+ \u03bd)n. else Set St(x, y) = |NF (x) \u2229NF (y) \u2229 (Cu \u222a Cv)|, i.e. the number of\npoints in Cu \u222a Cv that are common neighbors of x, y in Ft. Connect Cu, Cv in Ht, if medianx\u2208Cu,y\u2208CvSt(x, y) > |Cu|+|Cv | 4 .\nend if end for\nStep 4 Merge blobs in C\u2032t\u22121 to get C\u2032t Set C\u2032t = C\u2032t\u22121. for any connected component V in Ht with | \u22c3 C\u2208V C| \u2265 4(\u03b1+ \u03bd)n do\nUpdate C\u2032t by merging all blobs in V into one blob. end for\nStep 5 Increase threshold t = t+ 1.\nend while\nOutput: Tree T with single points as leaves and internal nodes corresponding to the merges performed.\nvertices are the data points in S and whose edges are constructed by connecting any two points that share at least t \u2212 2(\u03b1 + \u03bd)n points in common out of their t nearest neighbors. Then it builds a graph Ht whose vertices correspond to blobs in C\u2032t\u22121 and whose edges are specified in the following way. Two singleton blobs Cu = {x} and Cv = {y} are connected in Ht if the points x, y have more than (\u03b1 + \u03bd)n common neighbors in Ft. For blobs Cu and Cv that are not both singleton, the algorithm performs a median test. In this test, for each pair of points x \u2208 Cu, y \u2208 Cv, it computes the number St(x, y) of points z \u2208 Cu\u222aCv that are the common neighbors of x and y in Ft. It then connects Cu and Cv in Ht if medianx\u2208Cu,y\u2208CvSt(x, y) is larger than 1/4 fraction of |Cu|+ |Cv|. Once Ht is built, we analyze its connected components in order to create C\u2032t. For each connected component V ofHt, if V contains sufficiently many points from S in its blobs we merge all its blobs into one blob in C\u2032t. After building C\u2032t, the threshold is increased and the above steps are repeated until only one blob is left. The algorithm finally outputs the tree with single points as leaves\nand internal nodes corresponding to the merges performed. The full details of our algorithm are described in Algorithm 1. Our main result in this section is the following:\nTheorem 1. Let K be a symmetric similarity function satisfying the (\u03b1, \u03bd)-good neighborhood for the clustering problem (S, `). As long as the smallest target cluster has size greater than 6(\u03bd+\u03b1)n, Algorithm 1 outputs a hierarchy such that a pruning of the hierarchy is \u03bd-close to the target clustering in time O(n\u03c9+1), where O(n\u03c9) is the state of the art for matrix multiplication.\nIn the rest of this section, we will first describe the intuition behind the algorithm in Section 3.1 and then prove Theorem 1 in Section 3.2."}, {"heading": "3.1 Intuition of the Algorithm under the Good Neighborhood Property", "text": "We begin with some convenient terminology and a simple fact about the good neighborhood property. In the definition of the (\u03b1, \u03bd)-good neighborhood property (see Property 3), we call the points in S\u2032 good points and the points in B = S \\ S\u2032 bad points. Let Gi = Ci \u2229 S\u2032 be the good set of label i. Let G = \u222aiGi denote the whole set of good points; so G = S\u2032. Clearly |G| \u2265 n\u2212 \u03bdn. Recall that nCi is the number of points in the cluster Ci. Note that the following is a useful consequence of the (\u03b1, \u03bd)-good neighborhood property.\nFact 3. Suppose the similarity functionK satisfies the (\u03b1, \u03bd)-good neighborhood property for the clustering problem (S, `). As long as t is smaller than nCi , for any good point x \u2208 Ci, all but at most (\u03b1+ \u03bd)n out of its t nearest neighbors lie in its good set Gi.\nProof. Let x \u2208 Gi. By definition, out of its |Gi| nearest neighbors in G, there are at least |Gi| \u2212 \u03b1n points from Gi. These points must be among its |Gi| + \u03bdn nearest neighbors in S, since there are at most \u03bdn bad points in S \\ G. This means that at most (\u03b1 + \u03bd)n out of its |Gi| + \u03bdn nearest neighbors are outside Gi. Notice |Gi|+ \u03bdn \u2265 nCi , we have that at most (\u03b1 + \u03bd)n out of its nCi nearest neighbors are outside Gi, as desired.\nIntuition We first assume for simplicity that all the target clusters have the same size nC and that we know nC . In this case it is quite easy to recover the target clusters as follows. We first construct a graph F whose vertices are points in S; we connect two points in F if they share at least nC \u2212 2(\u03bd + \u03b1)n points in common among their nC nearest neighbors. By Fact 3, if the target clusters are not too small (namely nC > 6(\u03bd + \u03b1)n), we are guaranteed that no two good points in different target clusters will be connected in F , and that all good points in the same target cluster will be connected in F . If there are no bad points (\u03bd = 0), then each connected component of F corresponds to a target cluster, and we could simply output them. Alternatively, if there are bad points (\u03bd > 0), we can still cluster well as follows. We construct a new graph H on points in S by connecting points that share more than (\u03b1+ \u03bd)n neighbors in the graph F . The key point is that in F a bad point can be connected to good points from only one single target cluster. This then ensures that no good points from different target clusters are in the same connected component in H . So, if we output the largest k components of H , we will obtain a clustering with error at most \u03bdn. See Figure 3 for an illustration.\nIf we do not know nC , we can still use a pretty simple procedure. Specifically, we start with a threshold t \u2264 nC that is not too small and not too large (say 6(\u03bd + \u03b1)n < t \u2264 nC), and build a graph Ft on S by connecting two points if they share at least t\u2212 2(\u03bd+\u03b1)n points in common out of their t nearest neighbors. We then build another graph Ht on S by connecting points if they share more than (\u03b1 + \u03bd)n neighbors in the graph Ft. The key idea is that when t \u2264 nC , good points from different target clusters share less than t \u2212 2(\u03bd + \u03b1)n neighbors, and thus are not connected in Ft and Ht. If the k largest connected components\nof Ht all have sizes greater than (\u03b1 + \u03bd)n and they cover at least a (1 \u2212 \u03bd) fraction of the whole set of points S, then these components must correspond to the target clusters and we can output them. Otherwise, we increase the critical threshold and repeat. By the time we reach nC , all good points in the same target clusters will get connected in Ft and Ht, so we can identify the k largest components as the target clusters.\nNote that as mentioned above, when t \u2264 nC , each connected component in Ht can contain good points from only one target cluster. An alternative procedure is to reuse this information in later thresholds, so that we do not need to build the graph Ht from scratch as described in the above paragraph. Specifically, we maintain a list C\u2032t of subsets of points; these subsets are called blobs for convenience. We start with a list where each blob contains a single point. At each threshold t, we build Ft on the points in S as before, but build Ht on the blobs in C\u2032t\u22121 (instead of on the points in S). When building Ht, for two singleton blobs, it is safe to connect them if their points share enough neighbors in Ft. For non-singleton blobs, it turns out that we can use a median test to outvote the noise3. In particular, for two blobs Cu and Cv that are not both singleton, we compute for all x \u2208 Cu and y \u2208 Cv the quantity St(x, y), which is the number of points in Cu \u222a Cv that are the common neighbors of x and y in Ft. We then connect the two blobs in Ht if medianx\u2208Cu,y\u2208CvSt(x, y) is sufficiently large. See Figure 4 for an illustration and see Step 3 in Algorithm 1 for the details.\nIn the general case where the sizes of the target clusters are different, similar ideas can be applied. The\n3The median test is quite robust and as we show, it allows some points in these blobs to have weaker properties than the good neighborhood. See Section 4 for examples of such points and a theoretical analysis of the robustness.\nkey point is that when t \u2264 nCi , good points fromCi share less than t\u22122(\u03bd+\u03b1)n neighbors with good points outside, and thus are not connected to them in Ft. Then in Ht, we can make sure that no blobs containing good points in Ci will be connected with blobs containing good points outside Ci. When t = nCi , good points in Ci form a clique in Ft, then all the blobs containing good points in Ci are connected in Ht, and thus are merged. See Figure 5 for an illustration. Full details are presented in Algorithm 1 and the proof of Theorem 1 in the following subsection."}, {"heading": "3.2 Correctness under the Good Neighborhood Property", "text": "In this subsection, we prove Theorem 1 for our algorithm. The correctness follows from Lemma 1 and the running time follows from Lemma 2. Before proving these lemmas, we begin with a useful fact which follows immediately from the design of the algorithm.\nFact 4. In Algorithm 1, for any t, if a blob in Ct contains at least one good point, then at least 3/4 fraction of the points in that blob are good points.\nProof. This is clearly true when the blob is singleton. When it is non-singleton, it must be formed in Step 4 in Algorithm 1, so it contains at least 4(\u03b1 + \u03bd)n points. Then the claim follows since there are at most \u03bdn bad points.\nLemma 1. The following claims are true in Algorithm 1: (1) For any Ci such that t \u2264 |Ci|, any blob in C\u2032t containing good points from Ci will not contain good points outside Ci. (2) For any Ci such that t = |Ci|, all good points in Ci belong to one blob in C\u2032t.\nProof. Before proving the claims, we first show that the graph Ft constructed in Step 2 has the following useful properties. Recall that Ft is constructed on points in S by connecting any two points that share at least t \u2212 2(\u03b1 + \u03bd)n points in common out of their t nearest neighbors. For any Ci such that t \u2264 |Ci|, we have:\n(a) If x is a good point in Ci and y is a good point outside Ci, then x and y are not connected in Ft.\nTo see this, first note that by Fact 3, x has at most (\u03b1+\u03bd)n neighbors outsideCi out of the t nearest\nx y\nCu C v\nB G\nNow we prove Claim (1) in the lemma by induction on t. The claim is clearly true initially. Assume for induction that the claim is true for the threshold t\u2212 1 < |Ci|, that is, for any Ci such that t\u2212 1 < |Ci|, any blob in C\u2032t\u22121 containing good points from Ci will not contain good points outside Ci. We now prove that the graph Ht constructed in Step 3 has the following properties, which can be used to show that the claim is still true for the threshold t.\n\u2022 If Cu \u2208 C\u2032t\u22121 contains good points from Ci and Cv \u2208 C\u2032t\u22121 contains good points outside Ci, then they cannot be connected in Ht.\nIf both Cu and Cv are singleton blobs, say Cu = {x}, Cv = {y}, then by Property (a) of Ft, the common neighbors of x and y can only be bad points, and thus Cu and Cv cannot be connected. If one of the two blobs (say Cu) is a singleton blob and the other is not, then Cu contains only one good point, and by Fact 4, at least 3/4 fraction of the points in Cv are good points. If both Cu and Cv are non-singleton blobs, then by Fact 4, at least 3/4 fraction of the points in Cu and Cv are good points. Therefore, in both cases, the number of pairs (x, y) with good points x \u2208 Cu and y \u2208 Cv is at least 34 |Cu| \u00d7 3 4 |Cv| > |Cu||Cv | 2 . That is, more than half of the pairs (x, y) with x \u2208 Cu and y \u2208 Cv are pairs of good points; see Figure 6 for an illustration. This means there exist good points x\u2217 \u2208 Cu, y\u2217 \u2208 Cv such that St(x\u2217, y\u2217) is no less than medianx\u2208Cu,y\u2208CvSt(x, y). By the induction assumption, x\u2217 is a good point in Ci and y\u2217 is a good point outside Ci. Then by Property (a)(b) of Ft,\nx\u2217 and y\u2217 have no common neighbors in Ft, and thus medianx\u2208Cu,y\u2208CvSt(x, y) = 0. Therefore, Cu and Cv are not connected in Ht.\n\u2022 If Cu \u2208 C\u2032t\u22121 contains good points from Ci, Cv \u2208 C\u2032t\u22121 contains good points outside Ci, and Cw \u2208 C\u2032t\u22121 contains only bad points, then Cw cannot be connected to both Cu and Cv in Ht.\nTo prove this, assume for contradiction that Cw is connected to both Cu and Cv. First, note the following fact about Cw. Since any non-singleton blob must be formed in Step 4 in the algorithm and contain at least 4(\u03b1 + \u03bd)n points and thus cannot contain only bad points, Cw must be a singleton blob, containing only a bad point z. Next, we show that if Cw = {z} is connected to Cu, then z must be connected to some good point in Ci in Ft. If Cu is a singleton blob, say Cu = {x}, then by Step 4 in the algorithm, z and x share more than (\u03b1 + \u03bd)n common neighbors in Ft. By Property (a)(b) of Ft, the common neighbors of x and z in Ft can only be good points in Ci or bad points. Since there are at most \u03bdn bad points, z must be connected to some good point in Ci in Ft. If Cu is not a singleton blob, then by Step 4 in the algorithm, medianx\u2208CuSt(x, z) > (|Cu| + |Cw|)/4. By Fact 4, at least 3/4 fraction of the points in Cu are good points. So there exists a good point x\u2217 \u2208 Cu such that St(x\u2217, z) \u2265 medianx\u2208CuSt(x, z), which leads to St(x\u2217, z) > (|Cu| + |Cw|)/4 > \u03bdn. By the induction assumption, x\u2217 is a good point in Ci. Then by Property (a) of Ft, x\u2217 is only connected to good points from Ci and bad points. Since St(x\n\u2217, z) > \u03bdn, z and x\u2217 must share some common neighbors from Ci. Therefore, z is connected to some good point in Ci in Ft. Similarly, if Cw = {z} is connected to Cv, z must be connected to some good point outside Ci in Ft. But then z is connected to both a good point in Ci and a good point outside Ft, which contradicts Property (b) of Ft.\nBy the properties of Ht, no connected component contains both good points in Ci and good points outside Ci. So Claim (1) is still true for the threshold t. By induction, it is true for all thresholds.\nFinally, we prove Claim (2). First, at the threshold t = |Ci|, all good points in Ci are connected in Ft. This is because any good point in Ci has at most (\u03b1+ \u03bd)n neighbors outside Ci, so when t = |Ci|, any two good points in Ci share at least t\u2212 2(\u03b1+ \u03bd)n common neighbors and thus are connected in Ft.\nSecond, all blobs in C\u2032t\u22121 containing good points in Ci are connected in Ht. There are two cases.\n\u2022 If no good points in Ci have been merged, then all singleton blobs containing good points in Ci will be connected in Ht.\nThis is because all good points in Ci are connected in Ft, and thus they share at least |Gi| \u2265 6(\u03b1+ \u03bd)n\u2212 \u03bdn points as common neighbors in Ft.\n\u2022 If some good points in Ci have already been merged into non-singleton blobs in C\u2032t\u22121, we can show that in Ht these non-singleton blobs will be connected to each other and connected to singleton blobs containing good points from Ci.\nConsider two non-singleton blobs Cu and Cv that contain good points from Ci. By Fact 4, at least 3/4 fraction of the points in Cu and Cv are good points. So there exist good points x\u2217 \u2208 Cu and y\u2217 \u2208 Cv such that St(x\u2217, y\u2217) \u2264 medianx\u2208Cu,y\u2208CvSt(x, y). By Claim (1), x\u2217 and y\u2217 must be good points in Ci. Then they are connected to all good points in Ci in Ft, and thus St(x\u2217, y\u2217) is no less than the number of good points in Cu and Cv, which is at least 3(|Cu| + |Cv|)/4. Now we have medianx\u2208Cu,y\u2208CvSt(x, y) \u2265 St(x\u2217, y\u2217) \u2265 3(|Cu|+ |Cv|)/4 > (|Cu|+ |Cv|)/4, and thus Cu, Cv are connected in Ht.\nConsider a non-singleton blob Cu and a singleton blob Cv that contain good points from Ci. The above argument also holds, so Cu, Cv are connected in Ht.\nTherefore, in both cases, all blobs in C\u2032t\u22121 containing good points in Ci are connected in Ht. Then in Step 4, all good points in Ci are merged into a blob in C\u2032t.\nLemma 2. Algorithm 1 has a running time of O(n\u03c9+1).\nProof. The initializations in Step 1 takeO(n) time. To compute Ft in Step 2, for any x \u2208 S, let It(x, y) = 1 if y is within the t nearest neighbors of x, and let It(x, y) = 0 otherwise. Initializing It takes O(n2) time. Next we compute Nt(x, y), the number of common neighbors between x and y. Notice that Nt(x, y) =\u2211\nz\u2208S It(x, z)It(y, z), so Nt = ItI T t . Then we can compute the adjacency matrix Ft (overloading notation for the graph Ft) from Nt. These steps take O(n\u03c9) time. To compute the graph Ht in Step 3, first define NSt = Ft(Ft)T . Then for two points x and y, NSt(x, y) is the number of their common neighbors in Ft. Further define a matrix FCt as follows: if x and y are connected in Ft and are in the same blob in C\u2032t\u22121, then let FCt(x, y) = 1; otherwise, let FCt(x, y) = 0. As a reminder, for two points x that belongs to Cu \u2208 C\u2032t\u22121 and y that belongs to Cv \u2208 C\u2032t\u22121, St(x, y) is the number of points in Cu \u222a Cv they share as neighbors in common in Ft. FCt is useful for computing St: since for x \u2208 Cu and y \u2208 Cv,\nSt(x, y) = \u2211 z\u2208Cv Ft(x, z)Ft(y, z) + \u2211 z\u2208Cu Ft(x, z)Ft(y, z)\n= \u2211 z\u2208S Ft(x, z)FCt(y, z) + \u2211 z\u2208S FCt(x, z)Ft(y, z),\nwe have St = Ft(FCt)T + FCt(Ft)T . Based on NSt and St, we can then build the graph Ht. All these steps take O(n\u03c9) time.\nWhen we perform merges in Step 4 or increase the threshold in Step 5, we need to recompute the above data structures, which takes O(n\u03c9) time. Since there are O(n) merges and O(n) thresholds, Algorithm 1 takes time O(n\u03c9+1) in total."}, {"heading": "4 A More General Property: Weak Good Neighborhood", "text": "In this section we introduce a weaker notion of good neighborhood property and prove that our algorithm also succeeds for data satisfying this weaker property.\nTo motivate the property, consider a point x with the following neighborhood structure. In the neighborhood of size nC(x), x has a significant amount of its neighbors from other target clusters. However, in a smaller, more local neighborhood, x has most of its nearest neighbors from its target clusters C(x). In practice, points close to the boundaries between different target clusters typically have such neighborhood structure; for this reason, points with such neighborhood are called boundary points.\nWe present an example in Figure 7. A document close to the boundary between the two fields AI and Statistics has the following neighborhood structure: out of its n/4 nearest neighbors, it has all its neighbors from its own field; but out of its n/2 nearest neighbors, it has n/4 neighbors outside its field. With 1/8 fraction of such boundary points, the clustering {AI,Statistics} satisfies the (\u03b1, \u03bd)-good neighbor property only for \u03b1 \u2265 1/4 or \u03bd \u2265 1/8. This is because either we view all the boundary points as bad points in the (\u03b1, \u03bd)-good neighborhood property which leads to \u03bd \u2265 1/8, or we need \u03b1 \u2265 1/4 since a boundary point\nhas n/4 neighbors outside its target cluster. Similarly, {AI,ParameterEstimation,HypothesisTesting} and {Learning,Planning,Statistics} satisfy the property only for \u03b1 \u2265 1/4 or \u03bd \u2265 1/16.\nFor this example, either \u03b1 is too large so that Theorem 1 is not applicable, or \u03bd is too large so that the guarantee in Theorem 1 leads to constant error rate. However, it turns out that our algorithm can still successfully produce a hierarchy as in Figure 7(b) where the desired clusterings ({AI,Statistics}, {Learning,Planning,ParameterEstimation,HypothesisTesting}, {Learning,Planning,Statistics}, and {AI,ParameterEstimation,HypothesisTesting}) are prunings of the hierarchy. As we show, the reason is that each of these prunings satisfies a generalization of the good neighborhood property which takes into account the boundary points, and for which our algorithm still succeeds. Note that the standard linkage algorithms fail on this example 4. In the following, we first formalize this property and discuss how it relates to the properties of the similarity function described in the paper so far. We then prove that our algorithm succeeds under this property, correctly clustering all points that are not adversarially bad.\n4 For any fixed non-boundary point y and fixed boundary point x in the other field, the probability that y has similarity 1.0 only with x is 2\nn (1\u2212 2 n )n/16\u22121 \u2248 2 n e\u22121/8. Since there are n/16 such boundary points x and 7n/8 such non-boundary points y, when\nn is sufficiently large, with high probability n/12 non-boundary points have similarity 1.0 with one single boundary point. Then the standard linkage algorithms (in particular, single linkage, average linkage, and complete linkage) would first merge these pairs of points with similarity 1.0. From then on, no matter how they perform, any pruning of the hierarchy produced will have error higher than 1/12.\nFor clarity, we first relax the \u03b1-good neighborhood to the weak (\u03b1, \u03b2)-good neighborhood defined as follows.\nProperty 4. A similarity function K satisfies weak (\u03b1, \u03b2)-good neighborhood property for the clustering problem (S, `), if for each p \u2208 S, there exists Ap \u2286 C(p) of size greater than 6\u03b1n such that p \u2208 Ap and\n\u2022 any point in Ap has at most \u03b1n neighbors outside Ap out of the |Ap| nearest neighbors;\n\u2022 for any such subset Ap \u2286 C(p), at least \u03b2 fraction of points in Ap have all but at most \u03b1n nearest neighbors from C(p) out of their nC(p) nearest neighbors.\nInformally, the first condition implies that every point falls into a sufficiently large subset of its target cluster, and points in the subset are close to each other in the sense that most of their nearest neighbors are in the subset. This condition is about the local neighborhood structure of the points. It shows that each point has a local neighborhood in which points closely relate to each other. Note that the local neighborhood should be large enough so that the membership of the point is clearly established: it should have size comparable to the number of connections to points outside (\u03b1n). Here we choose a minimum size of greater than 6\u03b1n mainly because it guarantees that our algorithm can still succeed in the worst case. The second condition implies that for points in these large enough subsets, a majority of them have most of their nearest neighbors from their target cluster. This condition is about more global neighborhood structure. It shows how the subsets are closely related to those in the same target cluster in the neighborhood of size equal to the target cluster size. Note that in this more global neighborhood, we do not require all points in these subsets have most nearest neighbors from their target clusters; we allow the presence of (1 \u2212 \u03b2) fraction of points that may have a significant number of nearest neighbors outside their target clusters.\nNaturally, as we can relax the \u03b1-good neighborhood property to the (\u03b1, \u03bd)-good neighborhood property, we can relax the weak (\u03b1, \u03b2)-good neighborhood to the weak (\u03b1, \u03b2, \u03bd)-good neighborhood as follows. Informally, it implies that the target clustering satisfies the weak (\u03b1, \u03b2)-good neighborhood property after removing a few bad points.\nProperty 5. A similarity functionK satisfies weak (\u03b1, \u03b2, \u03bd)-good neighborhood property for the clustering problem (S, `), if there exist a subset of points B of size at most \u03bdn, and for each p \u2208 S \\ B, there exists Ap \u2286 C(p) \\B of size greater than 6(\u03b1+ \u03bd)n such that p \u2208 Ap and\n\u2022 any point in Ap has at most \u03b1n neighbors outside Ap out of the |Ap| nearest neighbors;\n\u2022 for any such subset Ap \u2286 Ci \\ B, at least \u03b2 fraction of points in Ap have all but at most \u03b1n nearest neighbors from Ci \\B out of their |Ci \\B| nearest neighbors in S \\B.\nFor convenience, we call points in B bad points. If a point in Ci \\ B has all but at most \u03b1n nearest neighbors from Ci \\B out of its |Ci \\B| nearest neighbors in S \\B, we call it a good point. Then the second condition in the definition can be simply stated as: any Ap has at least \u03b2 fraction of good points. Note that Ci can contain points that are neither bad nor good. Such points are called boundary points, since in practice such points typically lie close to the boundaries between target clusters.\nAs a concrete example, consider the clustering {AI,Statistics} in Figure 7(c). It satisfies the weak (\u03b1, \u03b2, \u03bd)-good neighborhood property with probability at least 1\u2212\u03b4 when the number of points n = O(ln 1\u03b4 ). To see this, first note that for a fixed point y and a fixed boundary point x in the other field, the probability that K(y, x) = 1 is 2/n. Since there are n/16 boundary point in the other field, by Hoeffding bound, the probability that y has similarity 1 with more than n/32 points is bounded by exp{\u22122 \u00b7 n/16 \u00b7 (1/2)2} = exp{\u2212n/32}. By union bound, with probability at least 1\u2212 n exp{\u2212n/32}, no point has similarity 1 with\nmore than n/32 points. Then by setting Ap as the area that p falls in, we can see that the clustering satisfies the weak (\u03b1, \u03b2, \u03bd)-good neighborhood property for \u03b1 = 1/32, \u03b2 = 7/8 and \u03bd = 0. Note that there may also be some adversarial bad points. Then the weak (\u03b1, \u03b2, \u03bd)-good neighborhood property is satisfied when \u03b1 = 1/32, \u03b2 = 7/8 and \u03bd is the fraction of bad points. See Section 6.1 for simulations of this example and its variants."}, {"heading": "4.1 Relating Different Versions of Good Neighborhood Properties", "text": "The relations between these properties are illustrated in Figure 8. The relations between the weak good neighborhood properties and other properties are discussed below, while the other relations in the figure follow from the facts in Section 2.1.\nBy setting Ap = Ci for p \u2208 Ci in the definition, we can see that the weak (\u03b1, \u03b2)-good neighborhood property is a generalization of the \u03b1-good neighborhood property when each target cluster has size greater than 6\u03b1n. Formally,\nFact 5. If the similarity function K satisfies the \u03b1-good neighborhood property for the clustering problem (S, `) and mini |Ci| > 6\u03b1n, then K also satisfies the weak (\u03b1, \u03b2)-good neighborhood property for the clustering problem (S, `) for any 0 < \u03b2 \u2264 1.\nProof. IfK satisfies the \u03b1-good neighborhood property and mini |Ci| > 6\u03b1n, then we have: for any p \u2208 Ci, there exists a subset Ci \u2286 Ci of size greater than 6\u03b1n, such that out of the nCi nearest neighbors, any point x \u2208 Ci has at most \u03b1n neighbors outside Ci. So K satisfies both conditions of the weak (\u03b1, \u03b2)-good neighborhood property.\nBy setting Ap = Gi for p \u2208 Gi in the definition, we can see that the weak (\u03b1, \u03b2, \u03bd)-good neighborhood property generalizes the (\u03b1, \u03bd)-good neighborhood property when each target cluster has size greater than 7(\u03b1+\u03bd)n. Also, by setting \u03bd = 0, we can see that the weak (\u03b1, \u03b2)-good neighborhood property is equivalent to the weak (\u03b1, \u03b2, 0)-good neighborhood.\nFact 6. If the similarity function K satisfies the (\u03b1, \u03bd)-good neighborhood property for the clustering problem (S, `) and mini |Ci| > 7(\u03b1+ \u03bd)n, then K also satisfies the weak (\u03b1, \u03b2, \u03bd)-good neighborhood property for the clustering problem (S, `) for any 0 < \u03b2 \u2264 1.\nProof. If K satisfies the (\u03b1, \u03bd)-good neighborhood property and mini |Ci| > 7(\u03b1+ \u03bd)n, then we have: for any p \u2208 Gi = Ci \\B, there exists a subset Gi \u2286 Gi of size greater than 6(\u03b1+ \u03bd)n, such that out of the |Gi| nearest neighbors in S \\B, any good point x \u2208 Gi has at most \u03b1n neighbors outside Gi. So K satisfies both conditions of the weak (\u03b1, \u03b2, \u03bd)-good neighborhood property.\nFact 7. If the similarity function K satisfies the weak (\u03b1, \u03b2)-good neighborhood property for the clustering problem (S, `), then K also satisfies the weak (\u03b1, \u03b2, 0)-good neighborhood property for the clustering problem (S, `).\nProof. By setting \u03bd = 0 in the definition of the weak (\u03b1, \u03b2, \u03bd)-good neighborhood property, we can see that it is the same as the weak (\u03b1, \u03b2)-good neighborhood property."}, {"heading": "4.2 Correctness under the Weak Good Neighborhood Property", "text": "Now we prove that our algorithm also succeeds under the weak (\u03b1, \u03b2, \u03bd)-good neighborhood property when \u03b2 \u2265 7/8. Formally,\nTheorem 2. Let K be a symmetric similarity function satisfying the weak (\u03b1, \u03b2, \u03bd)-good neighborhood property for the clustering problem (S, `) with \u03b2 \u2265 7/8. Then Algorithm 1 outputs a hierarchy such that a pruning of the hierarchy is \u03bd-close to the target clustering in time O(n\u03c9+1), where O(n\u03c9) is the state of the art for matrix multiplication.\nTheorem 2 is a generalization of Theorem 1, and the proof follows a similar reasoning. The proof of correctness is from Lemma 5 stated and proved below and the running time follows from Lemma 2. The intuition is as follows. First, by similar arguments as for the good neighborhood property, each point p in S \\B will only be merged with other points inAp at t \u2264 |Ap|, and all points inAp will belong to one blob at t = |Ap| (Lemma 3), since in the local neighborhood of size |Ap|, the point has most of its nearest neighbor fromAp. Then, we need to show that such blobs will be correctly merged. The key point is to show that even in the presence of boundary points, the majority of points in such blobs are good points (Lemma 4). Then the median test can successfully distinguish blobs containing good points from different target clusters, and our algorithm can correctly merge blobs from the same target clusters together.\nTo formally prove the correctness, we begin with Lemma 3. The proof is similar to that for Lemma 1, replacing Ci with Ap.\nLemma 3. The following claims are true in Algorithm 1: (1) For any point p \u2208 S \\ B and t such that t \u2264 |Ap|, any blob in C\u2032t containing points from Ap will not contain points in (S \\Ap) \\B. (2) For any point p \u2208 S \\B and t = |Ap|, all points in Ap belong to one blob in C\u2032t.\nLemma 3 states that for any p \u2208 S \\ B, we will form Ap before merging them with points outside. Then we only need to make sure that these Ap formed will be correctly merged. More precisely, we need to consider the blobs that are \u201cfully formed\u201d in the following sense:\nDefinition 1. A blob Cu \u2208 C\u2032t in Algorithm 1 is said to be fully formed if for any point p \u2208 Cu \\B,Ap \u2286 Cu.\nTo show that fully formed blobs are correctly merged, the key point is to show that the majority of points in such blobs are good points, and thus the median test in the algorithm can successfully distinguish blobs containing good points from different target clusters. This key point is in fact a consequence of Lemma 3:\nLemma 4. For any fully formed blob Cu \u2208 C\u2032t in Algorithm 1, at least \u03b2 fraction of points in Cu \\ B are good points.\nProof. It suffices to show that there exist a set of points P \u2286 Cu \\ B, such that {Ap : p \u2208 P} is a partition of Cu \\ B. Clearly Cu \\ B = \u222ap\u2208Cu\\BAp. So we only need to show that sets in {Ap : p \u2208 Cu \\ B} are\nlaminar, that is, for any p, q \u2208 Cu \\ B, either Ap \u2229 Aq = \u2205 or Ap \u2286 Aq or Aq \u2286 Ap. See Figure 9 for an illustration.\nAssume for contradiction that there existAp andAq such thatAp\\Aq 6= \u2205, Aq\\Ap 6= \u2205 andAp\u2229Aq 6= \u2205. Without loss of generality, suppose |Ap| \u2264 |Aq|. Then by the second claim in Lemma 3, when t = |Ap|, all points in Ap belong to one blob in C\u2032t. In other words, this blob contains Ap \u2229 Aq and Ap \\ Aq. So for t \u2264 |Aq|, the blob contains points in Aq and also points in S \\ B \\ Aq, which contradicts the first claim in Lemma 3.\nWe are now ready to prove the following lemma that implies Theorem 2.\nLemma 5. The following claims are true in Algorithm 1: (1) For any Ci such that t \u2264 |Ci|, any blob in C\u2032t containing points in Ci \\ B will not contain points in (S \\ Ci) \\B. (2) For any Ci such that t = |Ci|, all points in Ci \\B belong to one blob in C\u2032t.\nProof. Before proving the claims, we first show that the graph Ft constructed in Step 2 has the following useful properties by an argument similar to that in Lemma 1. Recall that Ft is constructed on points in S by connecting any two points that share at least t\u22122(\u03b1+\u03bd)n points in common out of their t nearest neighbors. For any Ci such that t \u2264 |Ci|, we have:\n(a) If x is a good point in Ci and y is a good point outside Ci, then x and y are not connected in Ft.\nTo see this, first note that by Fact 3, x has at most (\u03b1 + \u03bd)n neighbors outside Ci out of the t nearest neighbors. Suppose y is a good point from Cj . If nCj \u2265 t, then y has at most (\u03b1 + \u03bd)n neighbors in Ci; if nCj < t, y has at most (\u03b1 + \u03bd)n + t \u2212 nCj neighbors in Ci. In both cases, y has at most (\u03b1+ \u03bd)n+ max(0, t\u2212 nCj ) < t\u2212 5(\u03b1+ \u03bd)n neighbors in Ci, since nCj > 6(\u03b1+ \u03bd)n and t > 6(\u03b1 + \u03bd)n. Then x and y have at most t \u2212 4(\u03b1 + \u03bd)n common neighbors, so they are not connected in Ft.\n(b) If x is a good point inCi, y is a good point outsideCi, and z is a bad point, then z cannot be connected to both x and y in Ft.\nTo prove this, we will show that if z is connected to x, then z cannot be connected to y. First, by\nthe same argument as above, out of the t nearest neighbors, y has less than t\u22125(\u03b1+\u03bd)n neighbors in Ci. Second, by Fact 3, x has at most (\u03b1+ \u03bd)n neighbors outside Ci. If z has less than t\u2212 3(\u03b1+ \u03bd)n neighbors in Ci, then z and x share less than t\u2212 3(\u03b1+ \u03bd)n+ (\u03b1+ \u03bd)n = t\u2212 2(\u03b1+ \u03bd)n neighbors and will not be connected. So z must have at least t \u2212 3(\u03b1 + \u03bd)n neighbors in Ci, and thus cannot have more than 3(\u03b1+\u03bd)n neighbors outside Ci. The two statements show that y and z share less than t\u2212 5(\u03b1+ \u03bd)n neighbors in Ci, and at most 3(\u03b1+ \u03bd)n neighbors outside Ci. So they share less than t\u2212 2(\u03b1+ \u03bd)n+ 3(\u03b1+ \u03bd)n = t\u2212 2(\u03b1+ \u03bd)n neighbors and thus are not connected in Ft.\nNow we prove Claim (1) in the lemma by induction on t. The claim is clearly true initially. Assume for induction that the claim is true for the threshold t\u2212 1, that is, for any Ci such that t\u2212 1 \u2264 |Ci|, any blob in C\u2032t\u22121 containing points in Ci \\ B will not contain points in (S \\ Ci) \\ B. We now prove that the graph Ht constructed in Step 3 has the following properties, which can be used to show that the claim is still true for the threshold t.\n\u2022 If Cu \u2208 C\u2032t\u22121 contains points from Ci \\B and Cv \u2208 C\u2032t\u22121 contains points from (S \\Ci) \\B, then they cannot be connected in Ht.\nSuppose one of them (say Cu) is not fully formed, that is, there is a point p \u2208 Cu \\ B such that Ap 6\u2286 Cu. Then by Lemma 3, the algorithm will not merge Cu with Cv at this threshold. More precisely, since not all points in Ap belong to Cu, we have t \u2212 1 < |Ap| by Claim (2) in Lemma 3. Then by Claim (1) in Lemma 3, since Cv contains points in (S \\ Ap) \\ B, Cu and Cv will not be merged in C\u2032t. So they are not connected in Ht. So we only need to consider the other case when Cu and Cv are fully formed blobs. By Lemma 4, the majority of points in the two blobs are good points. The good points from different target clusters have few common neighbors in Ft, then by the median test in our algorithm, the two blobs will not be connected in Ht. Formally, we can find two good points x\u2217 \u2208 Cu, y\u2217 \u2208 Cv that satisfy the following two statements.\n\u2013 St(x\u2217, y\u2217) \u2265 medianx\u2208Cu,y\u2208CvSt(x, y). By Lemma 4, at least \u03b2 \u2265 7/8 fraction of points in Cu \\B are good points. The fraction of good points in Cu is at least\n\u03b2|Cu \\B| |Cu \\B|+ |B| \u2265 7/8\u00d7 6(\u03b1+ \u03bd)n 6(\u03b1+ \u03bd)n+ \u03bdn \u2265 3 4\nsince |Cu \\ B| \u2265 6(\u03b1 + \u03bd)n and |B| \u2264 \u03bdn. Similarly, at least 34 fraction of points in Cv are good points. Then among all the pairs (x, y) such that x \u2208 Cu, y \u2208 Cv, at least 34 \u00d7 3 4 > 1 2 fraction are pairs of good points. So there exist good points x\u2217 \u2208 Cu, y\u2217 \u2208 Cv such that St(x\n\u2217, y\u2217) \u2265 medianx\u2208Cu,y\u2208CvSt(x, y). \u2013 St(x\u2217, y\u2217) \u2264 (|Cu|+ |Cv|)/4.\nThe fraction of good points in Cu \u222a Cv is at least 34 . Since in Ft, good points in Cu are not connected to good points in Cv, we have St(x\u2217, y\u2217) \u2264 (|Cu|+ |Cv|)/4.\nCombining the two statements, we have medianx\u2208Cu,y\u2208CvSt(x, y) \u2264 (|Cu| + |Cv|)/4 and thus Cu and Cv are not connected in Ht.\n\u2022 If in C\u2032t\u22121, Cu contains points from Ci \\ B, Cv contains points from (S \\ Ci) \\ B, and Cw contains only bad points, then Cw cannot be connected to both Cu and Cv.\nBy the same argument as above, we only need to consider the case when Cu and Cv are fully formed blobs. To prove the claim in this case, assume for contradiction that Cw is connected to both Cu and Cv. First, note the following fact about Cw. Since any non-singleton blob must be formed in Step 4 in the algorithm and contain at least 4(\u03b1+ \u03bd)n points and thus cannot contain only bad points, Cw must be a singleton blob, containing only a bad point z. Next, we show that if Cw = {z} are connected to Cu in Ht, then z must be connected to at least one good point in Cu in Ft. We have medianx\u2208CuSt(x, z) > |Cu|+|Cw| 4 , which means z is connected to more than |Cu|4 points in Cu in Ft. By the same argument as above, at least 3/4 fraction of points in Cu are good points, then z must be connected to at least one good point in Cu. Similarly, if Cw is connected to Cv in Ht, then z must be connected to at least one good point in Cv in Ft. But this contradicts Property (b) of Ft, so Cw cannot be connected to both Cu and Cv in Ht.\nBy the properties ofHt, no connected component contains both points inCi\\B and points in (S\\Ci)\\B. So Claim (1) is still true for the threshold t. By induction, it is true for all thresholds.\nFinally, we prove Claim (2). By Lemma 3, when t = |Ci|, for any point p \u2208 Ci \\ B, Ap belong to the same blob. So all points in Ci \\ B are in sufficiently large blobs. We will show that any two of these blobs Cu, Cv are connected in Ht, and thus will be merged into one blob. By Lemma 4, we know that more than 3/4 fraction of points in Cu (Cv respectively) are good points, and thus there exist good points x\u2217 \u2208 Cu, y\u2217 \u2208 Cv such that St(x\u2217, y\u2217) \u2264 medianx\u2208Cu,y\u2208CvSt(x, y). By Claim (1), all good points in Cu and Cv are from Ci, so they share at least t \u2212 2(\u03b1 + \u03bd)n neighbors when t = |Ci|, and thus are connected in Ft. Then St(x\u2217, y\u2217) is at least the number of good points in Cu \u222a Cv, which is at least 3(|Cu|+ |Cv|)/4. Then medianx\u2208Cu,y\u2208CvSt(x, y) \u2265 St(x\u2217, y\u2217) > (|Cu| + |Cv|)/4. Therefore, all blobs containing points from Ci \\B are connected in Ht and thus merged into a blob."}, {"heading": "5 The Inductive Setting", "text": "Many clustering applications have recently faced an explosion of data, such as in astrophysics and biology. For large data sets, it is often resource and time intensive to run an algorithm over the entire data set. It is thus increasingly important to develop algorithms that can remove the dependence on the actual size of the data and still perform reasonably well.\nIn this section we consider an inductive model that formalizes this problem. In this model, the given data is merely a small random subset of points from a much larger data set. The algorithm outputs a hierarchy over the sample, which also implicitly represents a hierarchy over the data set. In the following we describe the inductive version of our algorithm and prove that when the data satisfies the good neighborhood properties, the algorithm achieves small error on the entire data set, requiring only a small random sample whose size is independent of that of the entire data set."}, {"heading": "5.1 Formal Definition", "text": "First we describe the formal definition of the inductive model. In this setting, the given data S is merely a small random subset of points from a much larger abstract instance space X . For simplicity, we assume that X is finite and that the underlying distribution is uniform over X . Let N = |X| denote the size of the entire instance space, and let n = |S| denote the size of the sample.\nOur goal is to design an algorithm that based on the sample produces a hierarchy of small error with respect to the whole distribution. Formally, we assume that each node u in the hierarchy derived over the\nsample induces a cluster (a subset of X). For convenience, u is also used to denote the blob of sampled points it represents. The cluster u induces over X is implicitly represented as a function fu : X \u2192 {0, 1}, that is, for each x \u2208 X , fu(x) = 1 if x is a point in the cluster and 0 otherwise. We say that the hierarchy has error at most if it has a pruning fu1 , . . . , fuk of error at most ."}, {"heading": "5.2 Inductive Robust Median Neighborhood Linkage", "text": "The inductive version of our algorithm is described in Algorithm 2. To analyze the algorithm, we first present the following lemmas showing that, when the data satisfies the good neighborhood property, a sample of sufficiently large size also satisfies the weak good neighborhood property.\nAlgorithm 2 Inductive Robust Median Neighborhood Linkage Input: similarity function K, n \u2208 Z+, parameters \u03b1 > 0, \u03bd > 0.\nGet a hierarchy on the sample Sample i.i.d. examples S = {x1, . . . , xn} uniformly at random from X . Run Algorithm 1 with parameters (2\u03b1, 2\u03bd) on S and obtain a hierarchy T .\nGet the implicit hierarchy over X for any x \u2208 X do\nLet NS(x) denote the 6(\u03b1+ \u03bd)n nearest neighbors of x in S. Initialize u = root(T ) and fu(x) = 1. while u is not a leaf do\nLet w be the child of u that contains the most points in NS(x). Set u = w and fu(x) = 1.\nend while end for\nOutput: Hierarchy T and {fu, u \u2208 T}.\nLemma 6. Let K be a symmetric similarity function satisfying the (\u03b1, \u03bd)-good neighborhood for the clustering problem (X, `). Consider any fixed x \u2208 X \\B. If the sample size satisfies n = \u0398 ( 1 \u03b1 ln 1 \u03b4 ) , then with probability at least 1\u2212 \u03b4, x has at most 2\u03b1n neighbors outside (C(x) \\B)\u2229 S out of the |(C(x) \\B)\u2229 S| nearest neighbors in S \\B. Proof. Suppose x \u2208 Gi. Let NN(x) denote its |Gi| nearest neighbors in X . By assumption we have that |NN(x) \\Gi| \u2264 \u03b1N and |Gi \\NN(x)| \u2264 \u03b1N . Then by Chernoff bounds, with probability at least 1\u2212 \u03b4 at most 2\u03b1n points in our sample are in NN(x) \\Gi and at most 2\u03b1n points in our sample are in Gi \\NN(x).\nWe now argue that at most 2\u03b1n of the |Gi \u2229 S| nearest neighbors of x in S \\ B can be outside Gi. Let n1 be the number of points in (NN(x) \\Gi) \u2229 S, n2 be the number of points in (Gi \\NN(x)) \u2229 S, and n3 be the number of points in (Gi \u2229 NN(x)) \u2229 S. Then |Gi \u2229 S| = n2 + n3 and we know that n1 \u2264 2\u03b1n, n2 \u2264 2\u03b1n. We consider the following two cases. \u2022 n1 \u2265 n2. Then n1 + n3 \u2265 n2 + n3 = |Gi \u2229 S|. This implies that the |Gi \u2229 S| nearest neighbors of x in the sample all lie inside NN(x), since by definition all points inside NN(x) are closer to x than any point outside NN(x). But we are given that at most n1 \u2264 2\u03b1n of them can be outside Gi. Thus, we get that at most 2\u03b1n of the |Gi \u2229 S| nearest neighbors of x are not from Gi.\n\u2022 n1 < n2. This implies that the |Gi \u2229 S| nearest neighbors of x in the sample include all the points in NN(x) in the sample, and possibly some others too. But this implies in particular that it includes all the n3 points in Gi \u2229 NN(x) in the sample. So, it can include at most |Gi \u2229 S| \u2212 n3 = n2 \u2264 2\u03b1n points not in Gi \u2229 NN(x). Even if all those are not in Gi, the |Gi \u2229 S| nearest neighbors of x still include at most 2\u03b1n points not from Gi.\nIn both cases, at most 2\u03b1n of the |Gi \u2229 S| nearest neighbors of x in S \\B can be outside Gi.\nLemma 7. LetK be a symmetric similarity function satisfying the (\u03b1, \u03bd)-good neighborhood for the clustering problem (X, `). If the sample size satisfies n = \u0398 ( 1\nmin(\u03b1,\u03bd) ln 1 \u03b4min(\u03b1,\u03bd)\n) , then with probability at least"}, {"heading": "1\u2212 \u03b4, K satisfies the (2\u03b1, 2\u03bd)-good neighborhood with respect to the clustering induced over the sample S.", "text": "Proof. First, by Chernoff bounds, when n \u2265 3\u03bd ln 2 \u03b4 , we have that with probability at least 1\u2212 \u03b4/2, at most 2\u03bdn bad points fall into the sample. Next, by Lemma 6 and union bound, when n = \u0398 ( 1 \u03b1 ln n \u03b4 ) we have that with probability at least 1\u2212\u03b4/2, for any Ci, any x \u2208 Gi \u2229 S, x has at most 2\u03b1n points outside Gi \u2229 S out of its |Gi \u2229 S| nearest neighbors in (X \\ B) \u2229 S. Therefore, if n = \u0398 ( 1\nmin(\u03b1,\u03bd) ln n \u03b4\n) , then with probability at least 1 \u2212 \u03b4, the similarity\nfunction satisfies the (2\u03b1, 2\u03bd)-good neighborhood property with respect to the clustering induced over the sample S. It now suffices to show n is large enough so that n = \u0398 (\n1 min(\u03b1,\u03bd) ln n \u03b4\n) . To see this, let \u03b7 = min(\u03b1, \u03bd).\nSince lnn \u2264 tn\u2212 ln t\u2212 1 for any t, n > 0, we have\nc \u03b7 lnn \u2264 c \u03b7\n( \u03b7\n2c n+ ln\n2c \u03b7 \u2212 1 ) = n 2 + c \u03b7 ln 2c e \u00b7 \u03b7\nfor any constant c > 0. Then n = \u0398 ( 1 \u03b7 ln 1 \u03b7 ) implies n = \u0398 ( 1 \u03b7 lnn ) , and n = \u0398 ( 1 \u03b7 ln 1 \u03b4\u00b7\u03b7 ) implies\nn = \u0398 ( 1 \u03b7 ln n \u03b4 ) .\nTheorem 3. Let K be a symmetric similarity function satisfying the (\u03b1, \u03bd)-good neighborhood for the clustering problem (X, `). As long as the smallest target cluster has size greater than 12(\u03bd + \u03b1)N , then Algorithm 2 with parameters n = \u0398 ( 1\nmin(\u03b1,\u03bd) ln 1 \u03b4\u00b7min(\u03b1,\u03bd)\n) produces a hierarchy with a pruning that is\n(\u03bd + \u03b4)-close to the target clustering with probability 1\u2212 \u03b4.\nProof. Note that by Lemma 7, with probability at least 1\u2212 \u03b4/4, we have that K satisfies the (2\u03b1, 2\u03bd)-good neighborhood with respect to the clustering induced over the sample. Moreover, by Chernoff bounds, with probability at least 1 \u2212 \u03b4/4, each Gi has at least 6(\u03bd + \u03b1)n points in the sample. Then by Theorem 1, Algorithm 1 outputs a hierarchy T on the sample S with a pruning that assigns all good points correctly. Denote this pruning as {u1, . . . , uk} such that ui \\B = (Ci \u2229 S) \\B.\nNow we want to show that fu1 , . . . , fuk have error at most \u03bd + \u03b4 with probability at least 1 \u2212 \u03b4/2. For convenience, let u(x) be a shorthand of u`(x). Then it is sufficient to show that with probability at least 1\u2212 \u03b4/2, a (1\u2212 \u03b4) fraction of points x \u2208 X \\B have fu(x)(x) = 1.\nFix Ci and a point x \u2208 Ci \\ B. By Lemma 6, with probability at least 1 \u2212 \u03b42/2, out of the |Gi \u2229 S| nearest neighbors of x in S \\B, at most 2\u03b1n can be outside Gi. Recall that Algorithm 2 checks NS(x), the 6(\u03b1 + \u03bd)n nearest neighbors of x in S. Then out of NS(x), at most 2(\u03b1 + \u03bd)n points are outside Gi \u2229 S. By Lemma 1, ui contains Gi \u2229S, so ui must contain at least 4(\u03b1+ \u03bd)n points in NS(x). Consequently, any\nancestor w of ui, including ui, has more points in NS(x) than any other sibling of w. Then we must have fw(x) = 1 for any ancestor w of ui. In particular, fui(x) = 1. So, for any point x \u2208 X \\B, with probability at least 1\u2212 \u03b42/2 over the draw of the random sample, fu(x)(x) = 1.\nThen by Markov inequality, with probability at least 1 \u2212 \u03b4/2, a (1 \u2212 \u03b4) fraction of points x \u2208 X \\ B have fu(x)(x) = 1. More precisely, let Ux denote the uniform distribution over X \\ B, and let US denote the distribution of the sample S. Let I(x, S) denote the event that fu(x)(x) 6= 1. Then we have\nEx\u223cUx,S\u223cUS [I(x, S)] = ES\u223cUS [ Ex\u223cUx [I(x, S)|S] ] \u2264 \u03b42/2.\nThen by Markov inequality, we have\nPrS\u223cUS [ Ex\u223cUx [I(x, S)|S] \u2265 \u03b4 ] \u2264 \u03b4/2\nwhich means that with probability at least 1\u2212 \u03b4/2 over the draw of the random sample S, a (1\u2212 \u03b4) fraction of points x \u2208 X \\B have fu(x)(x) = 1.\nSimilarly, Algorithm 2 also succeeds for the weak good neighborhood property. By similar arguments as those in Lemma 6 and 7, we can prove that K satisfies the weak good neighborhood property over a sufficiently large sample (Lemma 8), which then leads to the final guarantee Theorem 4. For clarity, the proofs are provided in Appendix B.\nLemma 8. Let K be a symmetric similarity function satisfying the weak (\u03b1, \u03b2, \u03bd)-good neighborhood for the clustering problem (X, `). Furthermore, it satisfies that for any p \u2208 X \\ B, |Ap| > 24(\u03b1 + \u03bd)N . If the sample size satisfies n = \u0398 ( 1\nmin(\u03b1,\u03bd) ln 1 \u03b4min(\u03b1,\u03bd)\n) , then with probability at least 1 \u2212 \u03b4, K satisfies the\n(2\u03b1, 1516\u03b2, 2\u03bd)-good neighborhood with respect to the clustering induced over the sample S.\nTheorem 4. Let K be a symmetric similarity function satisfying the weak (\u03b1, \u03b2, \u03bd)-good neighborhood for the clustering problem (X, `) with \u03b2 \u2265 1415 . Furthermore, it satisfies that for any p \u2208 X \\ B, |Ap| > 24(\u03b1 + \u03bd)N . Then Algorithm 2 with parameters n = \u0398 ( 1\nmin(\u03b1,\u03bd) ln 1 \u03b4\u00b7min(\u03b1,\u03bd)\n) produces a hierarchy with\na pruning that is (\u03bd + \u03b4)-close to the target clustering with probability 1\u2212 \u03b4."}, {"heading": "6 Experiments", "text": "In this section, we compare our algorithm (called RMNL for convenience) with popular hierarchical clustering algorithms, including standard linkage algorithms (Sneath and Sokal, 1973; King, 1967; Everitt et al., 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al., 1998), and EigenCluster (Cheng et al., 2006).\nTo evaluate the performance of the algorithms, we use the model discussed in Section 2. Given a hierarchy output by an algorithm, we generate all possible prunings of size k5, where k is the number of clusters in the target clustering. Then we compute the Classification Error of each pruning with respect to the target clustering, and report the best error. The Classification Error of a computed clustering h with respect to the\n5Note that we generate all prunings of size k for evaluating the performance of various algorithms only. The hierarchical clustering algorithms do not need to generate these prunings when creating the hierarchies.\ntarget clustering ` is the probability that a point chosen at random from the data is labeled incorrectly 6. Formally,\nerr(h) = min \u03c3\u2208Sk [ Pr x\u2208S [\u03c3(h(x)) 6= `(x)] ]\nwhere Sk is the set of all permutations on {1, . . . , k}. For reporting results, we follow the methodology in (Guha et al., 1998): for all algorithms accepting input parameters (including (Generalized) Wisharts\u2019 Method, CURE, and RMNL), the experiments are repeated on the same data over a range of input parameter values, and the best results are considered.\nData sets To emphasize the effect of noise on different algorithms, we perform controlled experiments on a synthetic data set AIStat. This data set contains 512 points. It is an instance of the example discussed in Section 4 and is described in Figure 7. We further consider the following real-world data sets from UCI Repository (Bache and Lichman, 2013): Wine, Iris, BCW (Breast Cancer Wisconsin), BCWD (Breast Cancer Wisconsin Diagnostic), Spambase, and Mushroom. We also consider the MNIST data set (LeCun et al., 1998) and use two subsets of the test set for our experiments: Digits0123 that contains the examples of the digits 0, 1, 2, 3, and Digits4567 that contains the examples of the digits 4, 5, 6, 7.\nWe additionally consider the 10 data sets (PFAM1 to PFAM10) from (Voevodski et al., 2012), which are created by randomly choosing 8 families (of size between 1000 and 10000) from the biology database Pfam (Punta et al., 2012), version 24.0, October 2009. The similarities for the PFAM data sets are generated by biological sequence alignment software BLAST (Altschul et al., 1990). BLAST performs one versus all queries by aligning a queried sequence to sequences in the data set, and produces a score for each alignment. The score is a measure of the alignment quality and thus can be used as similarity. However, BLAST does not consider alignments with some of the sequences, in which case we assign similarities 0 to the corresponding sequences and exclude them from the neighbors of the queried sequence.\nThe smaller data sets are used in the transductive setting: Wine (178 points of dimension 13), Iris (150 \u00d7 4), BCW (699 \u00d7 10), and BCWD (569 \u00d7 32). The larger ones are used in the inductive setting: Spambase (4601 \u00d7 57), Mushroom (8124 \u00d7 22), Digits0123 (4157 \u00d7 784), Digits4567 (3860 \u00d7 784), and PFAM1 to PFAM10 (10000 \u223c 100000 sequences each)."}, {"heading": "6.1 Synthetic Data", "text": "Here we compare the performance of the algorithms on the synthetic data AIStat. Recall that the clustering {AI,Statistics} satisfies the weak (\u03b1, \u03b2, \u03bd)-good neighborhood property for \u03b1 = 1/32, \u03b2 = 7/8, \u03bd = 0 with high probability (See Figure 7 in Section 4). We conduct three sets of experiments, where we vary the values of \u03b1 and \u03bd by modifying the similarities between the points.\n(a) For each point x, we choose \u2206\u03b1n points y from the other field and set the similarities K(x, y) = K(y, x) = 1, so that the value of \u03b1 is increased to 1/32 + \u2206\u03b1. By varying \u2206\u03b1, we control \u03b1 = 1/32 + i/256 for i = 0, . . . , 8 and run the clustering algorithms on the modified data.\n(b) We randomly choose \u03bdn points x, and then set the similarity between x and any other point to be 1 minus the original similarity. This introduces \u03bdn bad points. We thus control \u03bd = i/256 for i = 0, . . . , 8.\n6To compute this error for a computed clustering in polynomial time, we first find its best match to the target clustering using the Hungarian Method (Kuhn, 1955) for min-cost bipartite matching in time O(n3), and then calculate the error as the fraction of points misclassified in matched clusters.\n(c) We perform the above two modifications simultaneously, that is, we control \u03b1 = 1/32 + i/256 and \u03bd = i/256 for i = 0, . . . , 8.\nNote that the instance no longer satisfies the weak good neighborhood property when \u03b1 + \u03bd \u2265 1/24. This is because the weak good neighborhood requires that each point p 6\u2208 B falls into a subset Ap of size greater than 6(\u03b1 + \u03bd)n with desired properties (see Property 5), and the largest such subsets in AIStat have size n/4.\nFigure 10 shows the results of these experiments, averaged over 10 runs. When \u03b1 + \u03bd < 1/24, the instance satisfies the weak good neighborhood property and our algorithm has error at most \u03bd. Moreover, even if the instance does not satisfy the weak good neighborhood property when \u03b1+\u03bd \u2265 1/24, our algorithm still reports lower error. All the other algorithms have higher error than our algorithm and fail rapidly as \u03b1 + \u03bd increases. This demonstrates that in cases modeled by the properties we propose, our algorithm will be successful while the traditional agglomerative algorithms fail."}, {"heading": "6.2 Real-World Data", "text": "In this section, we compare the performance of our algorithm with the other algorithms on real-world data sets and show that our algorithm consistently outperforms the others."}, {"heading": "6.2.1 Transductive Setting", "text": "Here we compare the performance of the algorithms in the transductive setting where the algorithms use all the points in the data set. Figure 11 shows that our algorithm consistently achieves lowest or close to lowest errors on all the data sets. Ward\u2019s Method is the best among the other algorithms, but still shows larger errors than our algorithms. All the other algorithms generally show worse performance, and report\nsignificantly higher errors on some of the data sets. The comparison shows the robustness of our algorithm to the noise in the real world data sets.\nTo further evaluate the robustness of the algorithms, in the following we show their performance when different types of noise are added to the data. Since our algorithm requires additional parameters to characterize noise, we also discuss their robustness to parameter tuning.\nRobustness to Noise Here we present the performance of the algorithms when Gaussian noise or corruption noise is added and the level of noise is increased monotonically. The Gaussian noise model essentially corresponds to additive perturbations to the data entries and it is a very common type of noise studied throughout machine learning. The corruption noise models data corruption or missing values, and is also frequently studied in machine learning and coding theory (Blum et al., 2007; Feldman et al., 2008; Wigderson and Yehudayoff, 2012; Moitra and Saks, 2013). The experiments on different types of noise then evaluate the robustness of the algorithms to noise caused by different reasons in real world scenarios. Note that the instance is not in a metric space after adding noise to the similarities, so in this case, we only evaluate algorithms that can be run on non-metric instances.\nWe consider three types of noise: corruption noise to the attributes, corruption noise to the similarities, and Gaussian noise added to the attributes. The first type of noise is generated as follows: normalize the entries in the data matrix to [0, 1]; randomly pick p fraction of the entries; replace each sampled entry with a random value independently generated from N(0, 1), where p is the parameter indicating the level of noise. The second type of noise is generated using the same approach, but is added to the similarity matrix. The third type of noise is generated as follows: normalize the entries in the data matrix to [0, 1]; add a random value independently generated from N(0, p2) to each entry, where p is the parameter indicating the level of noise.\nFigure 12 shows the results of different algorithms in the presence of noise, averaged over 30 runs. The rows correspond to different types of noise added, and the columns correspond to different data sets. The first row shows the results when corruption noise is added to the attributes. Our algorithm shows robustness to such type of noise: its error rates remain the best or close to the best up to noise level 0.2 on all data sets. EigenCluster and Ward\u2019s method also show robustness, but their error rates are generally higher than those\nof our algorithm. The other algorithms report high errors even when the noise level is as low as 0.04. The second row shows the results when corruption noise is added to the similarities. We observe that the errors of our algorithm remain nearly the same up to noise level 0.2 over all the data sets, while the other algorithms report higher errors. Some algorithms (such as Complete Linkage on Wine) show comparable performance to our algorithms when there is no noise, but their errors generally increase rapidly as the noise level increases. This shows that our algorithm performs much better than the other algorithms in the presence of corruptions in the similarities.\nThe third row shows the results when Gaussian noise is added to the attributes. We observe that when the noise level increases, the errors of all algorithms increase. The errors of our algorithm do not increase much: they remain the best or close to the best up to the noise level 0.2 on all the data sets. Ward\u2019s method\nalso shows robustness, since the minimum variance criterion used is insensitive to this type of noise. The other algorithms generally show higher errors than our algorithms and Ward\u2019s method.\nIn conclusion, our algorithm generally outperforms the other algorithms when corruption noise is added to the data attributes or the similarities, or when Gaussian noise is added to the data attributes. Its robustness to Gaussian noise in similarities is not so significant since such noise with large variance can change the neighbor rankings of all points considerably. Still, it can tolerate such noise when the noise variance is not too large.\nRobustness to Parameter Tuning Our algorithm requires extra input parameters \u03b1 and \u03bd. There may be indirect ways to set their values, for example, by estimating the size of the smallest target cluster. Still, we are not aware of any efficient algorithm to compute the approximately correct values. Since these parameters play an important role in our algorithm, it is crucial to show the robustness of the algorithm to parameter tuning. Note that the two parameters are always used together as the additive term (\u03b1+ \u03bd), thus essentially the algorithm takes one parameter. So for evaluation, we vary the parameter (\u03b1 + \u03bd) linearly and run our algorithm over these values.\nFigure 13 shows the performance of the algorithm for different parameter values. We observe that the algorithm does not require the exact value of (\u03b1+ \u03bd) as it shows good performance over a continuous range of values. The range is sufficiently large for all data sets except Iris. The range for Iris is relatively small as there is little noise in it, and thus the parameter should be set to small values. In the other datasets we tried, we observed that it is easy to land in the right range with only a few runs."}, {"heading": "6.2.2 Inductive Setting", "text": "In this subsection, we present the evaluation results in the inductive setting. In this setting, the algorithm generates a hierarchy on a small random sample of the data set, and inserts the remaining points to generate a new hierarchy over the entire data set. We repeat the sampling and evaluation for 5 times and report the average results.\nWe compare our inductive algorithm with the random sample algorithm in (Eriksson, 2012). These algorithms sample some fraction of the similarities and use only these similarities. The percentage of sampled similarities can be tuned in these algorithms, so we compare their performance when they use the same amount of sampled similarities.\nFigure 14 shows the results for eight configurations (using 5% or 10% similarities on four different data sets). Our algorithm consistently outperforms the random sampling algorithm. Figure 15 shows the results on PFAM1 to PFAM10, which approximately satisfy the good neighborhood property (Voevodski et al.,\nFigure 14: Classification Error of different algorithms in the inductive setting. The y-axis represents the % error. The x-axis represents data sets, where the numbers before the names of the data sets denote the fraction of similarities used by the inductive algorithms.\nFigure 15: Classification Error on PFAM1 to PFAM10 data sets using 2.5% similarities. The y-axis in each case represents the % error, and the x-axis represents data sets.\n2012). On all PFAM data sets, the errors of our algorithm are low while those of the random sample algorithm are much higher. This shows the significant advantage of our algorithm when the data approximately satisfies the good neighborhood property."}, {"heading": "7 Discussion", "text": "In this work we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. In particular, if the data satisfies the good neighborhood properties, the algorithm will be successful in generating a hierarchy such that the target clustering is close to a pruning of that hierarchy.\nWe also show how to extend our algorithm to the inductive setting, where the given data is only a small random sample of the entire data set. Our algorithm achieves similar correctness guarantees, requiring only a small random sample whose size is independent of that of the entire data set.\nWe empirically show that with appropriate tuning of the noise parameters our algorithm consistently performs better than other hierarchical algorithms and are more robust to noise in the data. We also show the efficacy of the inductive version of our algorithm as a faster alternative when evaluation over the complete data is resource intensive.\nAdditionally, our subsequent work (Balcan and Liang, 2013) showed that the algorithm can be applied to the closely related community detection task and compares favorably with existing approaches.\nIt would be interesting to see if our algorithmic approach can be shown to work for other natural properties on the input similarity function. For example, it would be particularly interesting to analyze a noisy version of the max stability property in Balcan et al. (2008) which was shown to be a necessary and sufficient condition for single linkage to succeed, or of the average stability property which was shown to be a sufficient condition for average linkage to succeed. It would also be interesting to identify other natural conditions under different types of algorithms which are known to provide empirical noise robustness (e.g., the Wards method) would provably succeed. Finally, from an experimental point of view, an interesting open question is whether one can provide a wrapper for the algorithm to eliminate the need for manual tuning of the noise parameters.\nAcknowledgments We thank Avrim Blum for numerous useful discussions, and Konstantin Voevodski for providing us the PFAM data sets. We also thank the reviewers for their helpful comments and suggestions.\nThis work was supported in part by NSF grant CCF-0953192, AFOSR grant FA9550-09-1-0538, ONR grant N00014-09-1-0751, a Google Research Award, and a Microsoft Faculty Fellowship."}, {"heading": "B Additional Proofs for Section 5", "text": "Here we provide the details for proving that Algorithm 2 also succeeds for the weak good neighborhood. First, by a similar argument as that in Lemma 6, we can prove Lemma 9 showing that for a fixed p in X \\B and fixed x \u2208 Ap, the first condition of the weak good neighborhood is still satisfied on a sufficiently large sample (Recall the definition of Property 5). Similarly, we can prove Lemma 10 showing that the second condition of the weak good neighborhood is also satisfied. Then, the similarity K satisfies the weak good neighborhood property with respect to the clustering induced over the sample (Lemma 8). Our final guarantee, Theorem 4, then follows from the lemmas.\nLemma 9. Let K be a symmetric similarity function satisfying the weak (\u03b1, \u03b2, \u03bd)-good neighborhood for the clustering problem (X, `). Consider any fixed p \u2208 X \\ B and any fixed x \u2208 Ap. If the sample size satisfies n = \u0398 ( 1 \u03b1 ln 1 \u03b4 ) , then with probability at least 1\u2212 \u03b4, x has at most 2\u03b1n neighbors outside Ap \u2229 S out of the |Ap \u2229 S| nearest neighbors in S \\B.\nLemma 10. Let K be a symmetric similarity function satisfying the weak (\u03b1, \u03b2, \u03bd)-good neighborhood for the clustering problem (X, `). Consider any fixed p \u2208 X \\B and any fixed good point x \u2208 Ap. If the sample size satisfies n = \u0398 ( 1 \u03b1 ln 1 \u03b4 ) , then with probability at least 1 \u2212 \u03b4, x has at most 2\u03b1n neighbors outside C(x) \u2229 S out of the |Ap \u2229 S| nearest neighbors in S \\B.\nLemma 8 Let K be a symmetric similarity function satisfying the weak (\u03b1, \u03b2, \u03bd)-good neighborhood for the clustering problem (X, `). Furthermore, it satisfies that for any p \u2208 X \\ B, |Ap| > 24(\u03b1 + \u03bd)N . If the sample size satisfies n = \u0398 ( 1\nmin(\u03b1,\u03bd) ln 1 \u03b4min(\u03b1,\u03bd)\n) , then with probability at least 1 \u2212 \u03b4, K satisfies the\n(2\u03b1, 1516\u03b2, 2\u03bd)-good neighborhood with respect to the clustering induced over the sample S.\nProof. Consider the first condition of the weak good neighborhood property. First, by Chernoff bounds, when n \u2265 3\u03bd ln 4 \u03b4 , we have that with probability at least 1\u2212 \u03b4/4, at most 2\u03bdn bad points fall into the sample. Next, by Lemma 9 and union bound, when n = \u0398 ( 1 \u03b1 ln n \u03b4 ) we have that with probability at least 1 \u2212 \u03b4/4, for any point p \u2208 S \\B, any point x \u2208 Ap\u2229S has at most 2\u03b1n neighbors outside Ap\u2229S out of the |Ap\u2229S| nearest neighbors in S \\B. Since |Ap| > 24(\u03b1+\u03bd)N , we also have |Ap\u2229S| > 12(\u03b1+\u03bd)n with probability at least 1\u2212 \u03b4/4. So the first condition of the weak good neighborhood property is satisfied.\nNow consider the second condition. Fix Ci and a point p \u2208 (Ci \\ B) \u2229 S. When n = \u0398 ( 1 \u03b1 ln n \u03b4 ) , with probability at least 1\u2212 \u03b4/(8n), at least 1516\u03b2 fraction of points x in Ap \u2229 S have all but at most \u03b1N nearest neighbors from Ci \\ B out of their |Ci \\ B| nearest neighbors in X \\ B. Fix such a point x \u2208 Ap \u2229 S. By Lemma 10, with probability at least 1 \u2212 \u03b4/(8n2), it has all but at most 2\u03b1n nearest neighbors from (Ci \\ B) \u2229 S out of their |(Ci \\ B) \u2229 S| nearest neighbors in S \\ B. By union bound, we have that with probability at least 1\u2212 \u03b4/4, for any Ci and any p \u2208 Ci \\B, at least 1516\u03b2 fraction of points in Ap \u2229S have all\n7This does not change the time complexity and the correctness, but we observe that it helps speed up practical instances.\nbut at most 2\u03b1n nearest neighbors from (Ci \\B)\u2229S out of their |(Ci \\B)\u2229S| nearest neighbors in S \\B. So the second condition is also satisfied. Therefore, if n = \u0398 (\n1 min(\u03b1,\u03bd) ln n \u03b4\n) , then with probability at least 1\u2212 \u03b4, the similarity function satisfies\nthe (2\u03b1, 2\u03bd)-good neighborhood property with respect to the clustering induced over the sample S. The lemma then follows from the fact that n = \u0398 ( 1\nmin(\u03b1,\u03bd) ln 1 \u03b4min(\u03b1,\u03bd)\n) implies n = \u0398 ( 1\nmin(\u03b1,\u03bd) ln n \u03b4\n) .\nTheorem 4 LetK be a symmetric similarity function satisfying the weak (\u03b1, \u03b2, \u03bd)-good neighborhood for the clustering problem (X, `) with \u03b2 \u2265 1415 . Furthermore, it satisfies that for any p \u2208 X\\B, |Ap| > 24(\u03b1+\u03bd)N . Then Algorithm 2 with parameters n = \u0398 ( 1\nmin(\u03b1,\u03bd) ln 1 \u03b4\u00b7min(\u03b1,\u03bd)\n) produces a hierarchy with a pruning that\nis (\u03bd + \u03b4)-close to the target clustering with probability 1\u2212 \u03b4.\nProof. Note that by Lemma 8, with probability at least 1\u2212\u03b4/4, we have thatK satisfies the weak (2\u03b1, 1516\u03b2, 2\u03bd)good neighborhood with respect to the clustering induced over the sample. Then by Theorem 1, Algorithm 1 outputs a hierarchy T on the sample S with a pruning {u1, . . . , uk} such that ui \\B = (Ci \u2229 S) \\B.\nNow we want to show that fu1 , . . . , fuk have error at most \u03bd + \u03b4 with probability at least 1 \u2212 \u03b4/2. For convenience, let u(x) be a shorthand of u`(x). Then it is sufficient to show that with probability at least 1 \u2212 \u03b4/2, a (1 \u2212 \u03b4) fraction of points x \u2208 X \\ B have fu(x)(x) = 1. Fix Ci and a point x \u2208 Ci \\ B. By Lemma 9, with probability at least 1 \u2212 \u03b42/2, out of the |Ax \u2229 S| nearest neighbors of x in S \\ B, at most 2\u03b1n can be outside Ax. Then out of the 6(\u03b1 + \u03bd)n nearest neighbors of x in S, at most 2(\u03b1 + \u03bd)n points are outside Ax \u2229 S. By Lemma 1, ui contains Ax \u2229 S, so ui must contain at least 4(\u03b1 + \u03bd)n points in NS(x). Consequently, any ancestor w of ui, including ui, has more points in NS(x) than any other sibling of w. Then we must have fw(x) = 1 for any ancestor w of ui. In particular, fui(x) = 1. So, for any point x \u2208 X \\B, with probability at least 1\u2212\u03b42/2 over the draw of the random sample, fu(x)(x) = 1. By Markov inequality, with probability at least 1\u2212 \u03b4/2, a (1\u2212 \u03b4) fraction of points x \u2208 X \\B have fu(x)(x) = 1.\nC Strict Separation and Ward\u2019s Method\nHere we describe an example showing that Ward\u2019s minimum variance method fails in the presence of unbalanced clusters. The clustering instance satisfies the strict separation property and thus the more general good neighborhood properties, but on this instance Ward\u2019s method leads to large classification error.\nThe instance is presented in Figure 16. It consists three groups of points on a line: Group A has 4n points, Group B has n points, and Group C has n points. The distances between points in the same groups are 0, while the distances between points in A and points in B are 5, the distances between points in B and points in C are 6, the distances between points in A and points in C are 11.\nIt can be verified that the clustering {A \u222a B,C} satisfies the strict separation property. We now show that Ward\u2019s method will produce a tree that do not have this clustering as a pruning, and thus fails to cluster the instance. Recall that Ward\u2019s method starts with each point being a singleton cluster and at each step finds the pair of clusters that leads to minimum increase in total within-cluster variance after merging. Formally, it merges the two clusters U and V such that\n(U, V ) = argmin [Var(U \u222a V )\u2212Var(U)\u2212Var(V )]\nwhere Var(X) = min\nc \u2211 p\u2208X \u2016p\u2212 c\u201622.\nSince the distances between points in the same groups are 0, the method will first merge points in the same groups and forms three clusters A,B, and C. Now, merging A and B increases the variance by 20n, while merging B and C increases the variance by 18n. Therefore, B and C will be merged, and thus the best pruning in the tree produced is {A,B \u222a C}. This leads to an error of 1/6 \u2248 16.7%."}], "references": [{"title": "Clustering oligarchies", "author": ["M. Ackerman", "S. Ben-David", "D. Loker", "S. Sabato"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ackerman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ackerman et al\\.", "year": 2013}, {"title": "Basic local alignment search tool", "author": ["S.F. Altschul", "W. Gish", "W. Miller", "E.W. Myers", "D.J. Lipman"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Altschul et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Altschul et al\\.", "year": 1990}, {"title": "Modeling and detecting community hierarchies", "author": ["M.F. Balcan", "Y. Liang"], "venue": "In Proceedings of the International Workshop on Similarity-Based Pattern Analysis and Recognition,", "citeRegEx": "Balcan and Liang.,? \\Q2013\\E", "shortCiteRegEx": "Balcan and Liang.", "year": 2013}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["M.F. Balcan", "A. Blum", "S. Vempala"], "venue": "In Proceedings of the Annual ACM symposium on Theory of Computing,", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Clustering under approximation stability", "author": ["M.F. Balcan", "A. Blum", "A. Gupta"], "venue": "Journal of ACM,", "citeRegEx": "Balcan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2013}, {"title": "Separating populations with wide data: A spectral analysis", "author": ["A. Blum", "A. Coja-Oghlan", "A. Frieze", "S. Zhou"], "venue": "In Algorithms and Computation", "citeRegEx": "Blum et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2007}, {"title": "A structured family of clustering and tree construction methods", "author": ["D. Bryant", "V. Berry"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Bryant and Berry.,? \\Q2001\\E", "shortCiteRegEx": "Bryant and Berry.", "year": 2001}, {"title": "Rates of convergence for the cluster tree", "author": ["K. Chaudhuri", "S. Dasgupta"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Chaudhuri and Dasgupta.,? \\Q2010\\E", "shortCiteRegEx": "Chaudhuri and Dasgupta.", "year": 2010}, {"title": "A divide-and-merge methodology for clustering", "author": ["D. Cheng", "R. Kannan", "S. Vempala", "G. Wang"], "venue": "ACM Transaction on Database Systems,", "citeRegEx": "Cheng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2006}, {"title": "Performance guarantees for hierarchical clustering", "author": ["S. Dasgupta", "P. Long"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Dasgupta and Long.,? \\Q2005\\E", "shortCiteRegEx": "Dasgupta and Long.", "year": 2005}, {"title": "Hierarchical clustering using randomly selected measurements", "author": ["B. Eriksson"], "venue": "In Proceedings of the IEEE Statistical Signal Processing Workshop,", "citeRegEx": "Eriksson.,? \\Q2012\\E", "shortCiteRegEx": "Eriksson.", "year": 2012}, {"title": "Learning mixtures of product distributions over discrete domains", "author": ["J. Feldman", "R. O\u2019Donnell", "R.A. Servedio"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Feldman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2008}, {"title": "Maximum likelihood clustering with outliers. In Classification, Clustering, and Data Analysis", "author": ["M.T. Gallegos"], "venue": null, "citeRegEx": "Gallegos.,? \\Q2002\\E", "shortCiteRegEx": "Gallegos.", "year": 2002}, {"title": "A robust method for cluster analysis", "author": ["M.T. Gallegos", "G. Ritter"], "venue": "The Annals of Statistics,", "citeRegEx": "Gallegos and Ritter.,? \\Q2005\\E", "shortCiteRegEx": "Gallegos and Ritter.", "year": 2005}, {"title": "Robustness properties of k means and trimmed k means", "author": ["L. Garc\u0131\u0301a-Escudero", "A. Gordaliza"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Garc\u0131\u0301a.Escudero and Gordaliza.,? \\Q1999\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Escudero and Gordaliza.", "year": 1999}, {"title": "A general trimming approach to robust cluster analysis", "author": ["L. Garc\u0131\u0301a-Escudero", "A. Gordaliza", "C. Matr\u00e1n", "A. Mayo-Iscar"], "venue": "The Annals of Statistics,", "citeRegEx": "Garc\u0131\u0301a.Escudero et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Escudero et al\\.", "year": 2008}, {"title": "A review of robust clustering methods", "author": ["L. Garc\u0131\u0301a-Escudero", "A. Gordaliza", "C. Matr\u00e1n", "A. Mayo-Iscar"], "venue": "Advances in Data Analysis and Classification,", "citeRegEx": "Garc\u0131\u0301a.Escudero et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Escudero et al\\.", "year": 2010}, {"title": "Programmable clustering", "author": ["S. Gollapudi", "R. Kumar", "D. Sivakumar"], "venue": "In Symposium on Principles of Database Systems,", "citeRegEx": "Gollapudi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gollapudi et al\\.", "year": 2006}, {"title": "A comparison of some methods of cluster analysis", "author": ["J.C. Gower"], "venue": null, "citeRegEx": "Gower.,? \\Q1967\\E", "shortCiteRegEx": "Gower.", "year": 1967}, {"title": "CURE: an efficient clustering algorithm for large databases", "author": ["S. Guha", "R. Rastogi", "K. Shim"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "Guha et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Guha et al\\.", "year": 1998}, {"title": "Dissolution point and isolation robustness: robustness criteria for general cluster analysis methods", "author": ["C. Hennig"], "venue": "Journal of multivariate analysis,", "citeRegEx": "Hennig.,? \\Q2008\\E", "shortCiteRegEx": "Hennig.", "year": 2008}, {"title": "Algorithms for clustering", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "Jain and Dubes.,? \\Q1981\\E", "shortCiteRegEx": "Jain and Dubes.", "year": 1981}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM computing surveys,", "citeRegEx": "Jain et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1999}, {"title": "Hierarchical clustering schemes", "author": ["S.C. Johnson"], "venue": "Psychometrika,", "citeRegEx": "Johnson.,? \\Q1967\\E", "shortCiteRegEx": "Johnson.", "year": 1967}, {"title": "Step-wise clustering procedures", "author": ["B. King"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "King.,? \\Q1967\\E", "shortCiteRegEx": "King.", "year": 1967}, {"title": "The Hungarian method for the assignment algorithm", "author": ["H.W. Kuhn"], "venue": "Naval Research Logistics Quarterly,", "citeRegEx": "Kuhn.,? \\Q1955\\E", "shortCiteRegEx": "Kuhn.", "year": 1955}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "An experimental comparison of model-based clustering methods", "author": ["M. Meil\u0103", "D. Heckerman"], "venue": "Machine Learning,", "citeRegEx": "Meil\u0103 and Heckerman.,? \\Q2001\\E", "shortCiteRegEx": "Meil\u0103 and Heckerman.", "year": 2001}, {"title": "A polynomial time algorithm for lossy population recovery", "author": ["A. Moitra", "M. Saks"], "venue": "In Proceddings of the IEEE Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Moitra and Saks.,? \\Q2013\\E", "shortCiteRegEx": "Moitra and Saks.", "year": 2013}, {"title": "State of the art in pattern recognition", "author": ["G. Nagy"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Nagy.,? \\Q1968\\E", "shortCiteRegEx": "Nagy.", "year": 1968}, {"title": "The pfam protein families database", "author": ["M. Punta", "P.C. Coggill", "R.Y. Eberhardt", "J. Mistry", "J. Tate", "C. Boursnell", "N. Pang", "K. Forslund", "G. Ceric", "J. Clements", "A. Heger", "L. Holm", "E.L.L. Sonnhammer", "S.R. Eddy", "A. Bateman", "R.D. Finn"], "venue": "Nucleic Acids Research,", "citeRegEx": "Punta et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Punta et al\\.", "year": 2012}, {"title": "Numerical taxonomy. The principles and practice of numerical classification", "author": ["P.H.A. Sneath", "R.R. Sokal"], "venue": null, "citeRegEx": "Sneath and Sokal.,? \\Q1973\\E", "shortCiteRegEx": "Sneath and Sokal.", "year": 1973}, {"title": "Active clustering of biological sequences", "author": ["K. Voevodski", "M.F. Balcan", "H. R\u00f6glin", "S.-H. Teng", "Y. Xia"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Voevodski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Voevodski et al\\.", "year": 2012}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["J.H. Ward"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ward.,? \\Q1963\\E", "shortCiteRegEx": "Ward.", "year": 1963}, {"title": "Population recovery and partial identification", "author": ["A. Wigderson", "A. Yehudayoff"], "venue": "In Proceedings of the IEEE Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Wigderson and Yehudayoff.,? \\Q2012\\E", "shortCiteRegEx": "Wigderson and Yehudayoff.", "year": 2012}, {"title": "Mode analysis: a generalization of nearest neighbour which reduces chaining effects", "author": ["D. Wishart"], "venue": "Numerical Taxonomy,", "citeRegEx": "Wishart.,? \\Q1969\\E", "shortCiteRegEx": "Wishart.", "year": 1969}], "referenceMentions": [{"referenceID": 18, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 6, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 8, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 9, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 17, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 21, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 22, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 23, "context": "One of the oldest and most commonly used methods for clustering data, widely used in many scientific applications, is hierarchical clustering (Gower, 1967; Bryant and Berry, 2001; Cheng et al., 2006; Dasgupta and Long, 2005; Duda et al., 2000; Gollapudi et al., 2006; Jain and Dubes, 1981; Jain et al., 1999; Johnson, 1967; Narasimhan et al., 2006).", "startOffset": 142, "endOffset": 348}, {"referenceID": 9, "context": "Average linkage defines the similarity between two clusters as the average similarity between points in these two different clusters (Dasgupta and Long, 2005; Jain et al., 1999).", "startOffset": 133, "endOffset": 177}, {"referenceID": 22, "context": "Average linkage defines the similarity between two clusters as the average similarity between points in these two different clusters (Dasgupta and Long, 2005; Jain et al., 1999).", "startOffset": 133, "endOffset": 177}, {"referenceID": 3, "context": "In order to formally analyze correctness of our algorithm we use the framework introduced by Balcan et al. (2008). In this framework, we assume there is some target clustering (much like a k-class target function in the multi-class learning setting) and we say that an algorithm correctly clusters data satisfying property P if on any data set having property P , the algorithm produces a tree such that the target is some pruning of the tree.", "startOffset": 93, "endOffset": 114}, {"referenceID": 9, "context": "In agglomerative hierarchical clustering (Dasgupta and Long, 2005; Duda et al., 2000; Jain and Dubes, 1981; Jain et al., 1999), the goal is not to find a single partitioning of the data, but a hierarchy (generally represented by a tree) of partitionings which may reveal interesting structure in the data at multiple levels of granularity.", "startOffset": 41, "endOffset": 126}, {"referenceID": 21, "context": "In agglomerative hierarchical clustering (Dasgupta and Long, 2005; Duda et al., 2000; Jain and Dubes, 1981; Jain et al., 1999), the goal is not to find a single partitioning of the data, but a hierarchy (generally represented by a tree) of partitionings which may reveal interesting structure in the data at multiple levels of granularity.", "startOffset": 41, "endOffset": 126}, {"referenceID": 22, "context": "In agglomerative hierarchical clustering (Dasgupta and Long, 2005; Duda et al., 2000; Jain and Dubes, 1981; Jain et al., 1999), the goal is not to find a single partitioning of the data, but a hierarchy (generally represented by a tree) of partitionings which may reveal interesting structure in the data at multiple levels of granularity.", "startOffset": 41, "endOffset": 126}, {"referenceID": 29, "context": "As mentioned above, it is well known that standard agglomerative hierarchical clustering techniques are not tolerant to noise (Nagy, 1968; Narasimhan et al., 2006).", "startOffset": 126, "endOffset": 163}, {"referenceID": 35, "context": "Several algorithms have been proposed to make the hierarchical clustering techniques more robust to noise, such as Wishart\u2019s method (Wishart, 1969), and CURE (Guha et al.", "startOffset": 132, "endOffset": 147}, {"referenceID": 19, "context": "Several algorithms have been proposed to make the hierarchical clustering techniques more robust to noise, such as Wishart\u2019s method (Wishart, 1969), and CURE (Guha et al., 1998).", "startOffset": 158, "endOffset": 177}, {"referenceID": 33, "context": "Ward\u2019s minimum variance method (Ward, 1963) is also more preferable in the presence of noise.", "startOffset": 31, "endOffset": 43}, {"referenceID": 16, "context": "For general clustering beyond hierarchical clustering, there are also works proposing robust algorithms and analyzing robustness of the algorithms; see (Garc\u0131\u0301a-Escudero et al., 2010) for a review.", "startOffset": 152, "endOffset": 183}, {"referenceID": 14, "context": "In particular, the trimmed k-means algorithm (Garc\u0131\u0301a-Escudero and Gordaliza, 1999), a variant of the classical k-means algorithm, updates the centers after trimming points that are far away and thus are likely to be noise.", "startOffset": 45, "endOffset": 83}, {"referenceID": 3, "context": "On the theoretical side, Balcan et al. (2008) analyzed the \u03bd-strict separation property, a generalization of the simple strict separation property discussed above, requiring that after a small number of outliers have been removed all points are strictly more similar to points in their own cluster than to points in other clusters.", "startOffset": 25, "endOffset": 46}, {"referenceID": 3, "context": "On the theoretical side, Balcan et al. (2008) analyzed the \u03bd-strict separation property, a generalization of the simple strict separation property discussed above, requiring that after a small number of outliers have been removed all points are strictly more similar to points in their own cluster than to points in other clusters. They provided an algorithm for producing a hierarchy such that the target clustering is close to some pruning of the tree, but via a much more computationally expensive (non-agglomerative) algorithm. Our algorithm is simpler and substantially faster. As discussed in Section 2.1, the good neighborhood property is much broader than the \u03bd-strict separation property, so our algorithm is much more generally applicable compared to their algorithm specifically designed for \u03bd-strict separation. In a different statistical model, Chaudhuri and Dasgupta (2010) proposed a generalization of Wishart\u2019s method.", "startOffset": 25, "endOffset": 888}, {"referenceID": 15, "context": "An algorithm combining the above two approaches is then proposed in (Garc\u0131\u0301a-Escudero et al., 2008).", "startOffset": 68, "endOffset": 99}, {"referenceID": 20, "context": "(Hennig, 2008; Ackerman et al., 2013) studied the robustness of the classical algorithms such as k-means from the perspective of how the clusters are changed after adding some additional points.", "startOffset": 0, "endOffset": 37}, {"referenceID": 0, "context": "(Hennig, 2008; Ackerman et al., 2013) studied the robustness of the classical algorithms such as k-means from the perspective of how the clusters are changed after adding some additional points.", "startOffset": 0, "endOffset": 37}, {"referenceID": 27, "context": "This is popularly known as Classification Error (Meil\u0103 and Heckerman, 2001; Balcan et al., 2013; Voevodski et al., 2012).", "startOffset": 48, "endOffset": 120}, {"referenceID": 4, "context": "This is popularly known as Classification Error (Meil\u0103 and Heckerman, 2001; Balcan et al., 2013; Voevodski et al., 2012).", "startOffset": 48, "endOffset": 120}, {"referenceID": 32, "context": "This is popularly known as Classification Error (Meil\u0103 and Heckerman, 2001; Balcan et al., 2013; Voevodski et al., 2012).", "startOffset": 48, "endOffset": 120}, {"referenceID": 3, "context": "This is popularly known as Classification Error (Meil\u0103 and Heckerman, 2001; Balcan et al., 2013; Voevodski et al., 2012). We will be considering clustering algorithms whose only access to their data is via a pairwise similarity function K(x, x\u2032) that given two examples outputs a number in the range [\u22121, 1]. We will say that K is a symmetric similarity function ifK(x, x\u2032) = K(x\u2032, x) for all x, x\u2032. In this paper we assume that the similarity function K is symmetric. Our goal is to produce a hierarchical clustering that contains a pruning that is close to the target clustering. Formally, the goal of the algorithm is to produce a hierarchical clustering: that is, a tree on subsets such that the root is the set S, and the children of any node S\u2032 in the tree form a partition of S\u2032. The requirement is that there must exist a pruning h of the tree (not necessarily using nodes all at the same level) that has error at most . Balcan et al. (2008) have shown that this type of output is necessary in order to be able to analyze non-trivial properties of the similarity function.", "startOffset": 76, "endOffset": 950}, {"referenceID": 3, "context": "Given a similarity function satisfying the strict separation property (see Figure 1 for an example), we can efficiently construct a tree such that the ground-truth clustering is a pruning of this tree (Balcan et al., 2008).", "startOffset": 201, "endOffset": 222}, {"referenceID": 3, "context": "We start with a noisy version of the simple strict separation property (mentioned above) which was introduced in (Balcan et al., 2008) and we then define an interesting and natural generalization of it.", "startOffset": 113, "endOffset": 134}, {"referenceID": 3, "context": "Unfortunately the algorithm presented in (Balcan et al., 2008) is computationally very expensive: it first generates a large list of \u03a9(n2) candidate clusters and repeatedly runs pairwise tests in order to laminarize these clusters; its running time is a large unspecified polynomial.", "startOffset": 41, "endOffset": 62}, {"referenceID": 31, "context": "In this section, we compare our algorithm (called RMNL for convenience) with popular hierarchical clustering algorithms, including standard linkage algorithms (Sneath and Sokal, 1973; King, 1967; Everitt et al., 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al.", "startOffset": 159, "endOffset": 217}, {"referenceID": 24, "context": "In this section, we compare our algorithm (called RMNL for convenience) with popular hierarchical clustering algorithms, including standard linkage algorithms (Sneath and Sokal, 1973; King, 1967; Everitt et al., 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al.", "startOffset": 159, "endOffset": 217}, {"referenceID": 35, "context": ", 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al.", "startOffset": 40, "endOffset": 85}, {"referenceID": 7, "context": ", 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al.", "startOffset": 40, "endOffset": 85}, {"referenceID": 33, "context": ", 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al.", "startOffset": 118, "endOffset": 130}, {"referenceID": 19, "context": ", 2011), (Generalized) Wishart\u2019s Method (Wishart, 1969; Chaudhuri and Dasgupta, 2010), Ward\u2019s minimum variance method (Ward, 1963), CURE (Guha et al., 1998), and EigenCluster (Cheng et al.", "startOffset": 137, "endOffset": 156}, {"referenceID": 8, "context": ", 1998), and EigenCluster (Cheng et al., 2006).", "startOffset": 26, "endOffset": 46}, {"referenceID": 19, "context": "For reporting results, we follow the methodology in (Guha et al., 1998): for all algorithms accepting input parameters (including (Generalized) Wisharts\u2019 Method, CURE, and RMNL), the experiments are repeated on the same data over a range of input parameter values, and the best results are considered.", "startOffset": 52, "endOffset": 71}, {"referenceID": 26, "context": "We also consider the MNIST data set (LeCun et al., 1998) and use two subsets of the test set for our experiments: Digits0123 that contains the examples of the digits 0, 1, 2, 3, and Digits4567 that contains the examples of the digits 4, 5, 6, 7.", "startOffset": 36, "endOffset": 56}, {"referenceID": 32, "context": "We additionally consider the 10 data sets (PFAM1 to PFAM10) from (Voevodski et al., 2012), which are created by randomly choosing 8 families (of size between 1000 and 10000) from the biology database Pfam (Punta et al.", "startOffset": 65, "endOffset": 89}, {"referenceID": 30, "context": ", 2012), which are created by randomly choosing 8 families (of size between 1000 and 10000) from the biology database Pfam (Punta et al., 2012), version 24.", "startOffset": 123, "endOffset": 143}, {"referenceID": 1, "context": "The similarities for the PFAM data sets are generated by biological sequence alignment software BLAST (Altschul et al., 1990).", "startOffset": 102, "endOffset": 125}, {"referenceID": 25, "context": "To compute this error for a computed clustering in polynomial time, we first find its best match to the target clustering using the Hungarian Method (Kuhn, 1955) for min-cost bipartite matching in time O(n), and then calculate the error as the fraction of points misclassified in matched clusters.", "startOffset": 149, "endOffset": 161}, {"referenceID": 5, "context": "The corruption noise models data corruption or missing values, and is also frequently studied in machine learning and coding theory (Blum et al., 2007; Feldman et al., 2008; Wigderson and Yehudayoff, 2012; Moitra and Saks, 2013).", "startOffset": 132, "endOffset": 228}, {"referenceID": 11, "context": "The corruption noise models data corruption or missing values, and is also frequently studied in machine learning and coding theory (Blum et al., 2007; Feldman et al., 2008; Wigderson and Yehudayoff, 2012; Moitra and Saks, 2013).", "startOffset": 132, "endOffset": 228}, {"referenceID": 34, "context": "The corruption noise models data corruption or missing values, and is also frequently studied in machine learning and coding theory (Blum et al., 2007; Feldman et al., 2008; Wigderson and Yehudayoff, 2012; Moitra and Saks, 2013).", "startOffset": 132, "endOffset": 228}, {"referenceID": 28, "context": "The corruption noise models data corruption or missing values, and is also frequently studied in machine learning and coding theory (Blum et al., 2007; Feldman et al., 2008; Wigderson and Yehudayoff, 2012; Moitra and Saks, 2013).", "startOffset": 132, "endOffset": 228}, {"referenceID": 10, "context": "We compare our inductive algorithm with the random sample algorithm in (Eriksson, 2012).", "startOffset": 71, "endOffset": 87}, {"referenceID": 2, "context": "Additionally, our subsequent work (Balcan and Liang, 2013) showed that the algorithm can be applied to the closely related community detection task and compares favorably with existing approaches.", "startOffset": 34, "endOffset": 58}, {"referenceID": 2, "context": "Additionally, our subsequent work (Balcan and Liang, 2013) showed that the algorithm can be applied to the closely related community detection task and compares favorably with existing approaches. It would be interesting to see if our algorithmic approach can be shown to work for other natural properties on the input similarity function. For example, it would be particularly interesting to analyze a noisy version of the max stability property in Balcan et al. (2008) which was shown to be a necessary and sufficient condition for single linkage to succeed, or of the average stability property which was shown to be a sufficient condition for average linkage to succeed.", "startOffset": 35, "endOffset": 471}], "year": 2014, "abstractText": "One of the most widely used techniques for data clustering is agglomerative clustering. Such algorithms have been long used across many different fields ranging from computational biology to social sciences to computer vision in part because their output is easy to interpret. Unfortunately, it is well known, however, that many of the classic agglomerative clustering algorithms are not robust to noise. In this paper we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. We also show how to adapt our algorithm to the inductive setting where our given data is only a small random sample of the entire data set. Experimental evaluations on synthetic and real world data sets show that our algorithm achieves better performance than other hierarchical algorithms in the presence of noise.", "creator": "LaTeX with hyperref package"}}}