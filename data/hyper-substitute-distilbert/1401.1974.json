{"id": "1401.1974", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2014", "title": "Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts", "abstract": "we get a bayesian strategy framework titled multilevel clustering then generates group - level context information to formally discover pixel - contrast domains reflecting the folder contents and partitions itself into clusters. using the experimental process setting the building strength, our theory combines simplified quantitative value - measure with a balanced structure to accommodate content composition complement comparisons facing multiple levels. core proposed model framework properties the link underlying integrated graph transformation ( gdp ) matching the dirichlet process mixture models ( dpm ) in an interesting way : smoothing out all consensus results whereas the dpm simulation residues, simulated integrating out group - specific approaches aid in providing ndp mesh over content variables. simulation provide a polya - urn test of using model suggesting an efficient collapsed random inference procedure. extensive experiments on real - world datasets demonstrate visual advantage of utilizing context accuracy via that model in plain text and image search.", "histories": [["v1", "Thu, 9 Jan 2014 12:08:07 GMT  (5460kb)", "https://arxiv.org/abs/1401.1974v1", "Full version of ICML 2014"], ["v2", "Mon, 13 Jan 2014 06:28:03 GMT  (5101kb)", "http://arxiv.org/abs/1401.1974v2", "Full version of ICML 2014"], ["v3", "Mon, 27 Jan 2014 08:13:58 GMT  (5167kb)", "http://arxiv.org/abs/1401.1974v3", "Full version of ICML 2014"], ["v4", "Wed, 29 Jan 2014 01:54:57 GMT  (5170kb)", "http://arxiv.org/abs/1401.1974v4", "Full version of ICML 2014"]], "COMMENTS": "Full version of ICML 2014", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["tien-vu nguyen", "dinh quoc phung", "xuanlong nguyen", "svetha venkatesh", "hung hai bui"], "accepted": true, "id": "1401.1974"}, "pdf": {"name": "1401.1974.pdf", "metadata": {"source": "META", "title": "Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts", "authors": ["Vu Nguyen", "Dinh Phung", "XuanLong Nguyen", "Svetha Venkatesh", "Hung Hai Bui"], "emails": ["TVNGUYE@DEAKIN.EDU.AU", "DINH.PHUNG@DEAKIN.EDU.AU", "XUANLONG@UMICH.EDU", "SVETHA.VENKATESH@DEAKIN.EDU.AU", "BUI.H.HUNG@GMAIL.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 1.\n19 74\nv4 [\ncs .L\nG ]\n2 9"}, {"heading": "1. Introduction", "text": "In many situations, content data naturally present themselves in groups, e.g., students are grouped into classes, classes grouped into schools, words grouped into documents, etc. Furthermore, each content group can be associated with additional context information (teachers of the class, authors of the document, time and location stamps).\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nDealing with grouped data, a setting known as multilevel analysis (Hox, 2010; Diez-Roux, 2000), has diverse application domains ranging from document modeling (Blei et al., 2003) to public health (Leyland & Goldstein, 2001).\nThis paper considers specifically the multilevel clustering problem in multilevel analysis: to jointly cluster both the content data and their groups when there is group-level context information. By context, we mean a secondary data source attached to the group of primary content data. An example is the problem of clustering documents, where each document is a group of words associated with grouplevel context information such as time-stamps, list of authors, etc. Another example is image clustering where visual image features (e.g. SIFT) are the content and image tags are the context.\nTo cluster groups together, it is often necessary to perform dimensionality reduction of the content data by forming content topics, effectively performing clustering of the content as well. For example, in document clustering, using bag-of-words directly as features is often problematic due to the large vocabulary size and the sparsity of the in-document word occurrences. Thus, a typical approach is to first apply dimensionality reduction techniques such as LDA (Blei et al., 2003) or HDP (Teh et al., 2006b) to find word topics (i.e., distributions on words), then perform document clustering using the word topics and the document-level context information as features. In such a cascaded approach, the dimensionality reduction step (e.g., topic modeling) is not able to utilize the context information. This limitation suggests that a better alternative is to perform context-aware document clustering and topic modeling jointly. With a joint model, one can expect to obtain improved document clusters as well as context-guided content topics that are more predictive of the data.\nRecent work has attempted to jointly capture word topics and document clusters. Parametric approaches (Xie & Xing, 2013) are extensions of the LDA (Blei et al., 2003) and require specifying the number of topics and clusters in advance. Bayesian nonparametric approaches including the nested Dirichlet process (nDP) (Rodriguez et al., 2008) and the multi-level clustering hierarchical Dirichlet Process (MLC-HDP) (Wulsin et al., 2012) can automatically adjust the number of clusters. We note that none of these methods can utilize context data.\nThis paper propose the Multilevel Clustering with Context (MC2), a Bayesian nonparametric model to jointly cluster both content and groups while fully utilizing grouplevel context. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate both content and context observations. The MC2 model possesses properties that link the nested Dirichlet process (nDP) and the Dirichlet process mixture model (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-level context results in the nDP mixture over content variables. For inference, we provide an efficient collapsed Gibbs sampling procedure for the model.\nThe advantages of our model are: (1) the model automatically discovers the (unspecified) number of groups clusters and the number of topics while fully utilizing the context information; (2) content topic modeling is informed by group-level context information, leading to more predictive content topics; (3) the model is robust to partially missing context information. In our experiments, we demonstrate that our proposed model achieves better document clustering performances and more predictive word topics in realworld datasets in both text and image domains."}, {"heading": "2. Related Background", "text": "There have been extensive works on clustering documents in the literature. Due to limited scope of the paper, we only describe works closely related to probabilistic topic models. We note that standard topic models such as LDA (Blei et al., 2003) or its nonparametric Bayesian counter part, HDP (Teh et al., 2006b) exploits the group structure for word clustering. However these models do not cluster documents.\nAn approach to document clustering is to employ a twostage process. First, topic models (e.g. LDA or HDP) are applied to extract the topics and their mixture proportion for each document. Then, this is used as feature input to another clustering algorithm. Some examples of this approach include the use of LDA+Kmeans for image clustering (Xuan et al., 2011; Elango & Jayaraman, 2005) and\nHDP+Affinity Propagation for clustering human activities (Nguyen et al., 2013).\nA more elegant approach is to simultaneously cluster documents and discover topics. The first Bayesian nonparametric model proposed for this task is the nested Dirichlet Process (nDP) (Rodriguez et al., 2008) where documents in a cluster share the same distribution over topic atoms. Although the original nDP does not force the topic atoms to be shared across document clusters, this can be achieved by simply introducing a DP prior for the nDP base measure. The same observation was also made by (Wulsin et al., 2012) who introduced the MLC-HDP, a 3-level extension to the nDP. This model thus can cluster words, documents and document-corpora with shared topic atoms throughout the group hierarchy. Xie et al (Xie & Xing, 2013) recently introduced the Multi-Grain Clustering Topic Model which allows mixing between global topics and document-cluster topics. However, this is a parametric model which requires fixing the number of topics in advance. More crucially, all of these existing models do not attempt to utilize grouplevel context information."}, {"heading": "Modelling with Dirichlet Process", "text": "We provide a brief account of the Dirichlet process and its variants. The literature on DP is vast and we refer to (Hjort et al., 2010) for a comprehensive account. Here we focus on DPM, HDP and nDP which are related to our work.\nDirichlet process (Ferguson, 1973) is a basic building block in Bayesian nonparametrics. Let (\u0398,B, H) be a probability measure space, and \u03b3 is a positive number, a Dirichlet process DP (\u03b3,H) is a distribution over discrete random probability measure G on (\u0398,B). Sethuraman (Sethuraman, 1994) provides an alternative constructive definition which makes the discreteness property of a draw from a Dirichlet process explicit via the stick-breaking representation: G = \u2211\u221e\nk=1 \u03b2k\u03b4\u03c6k where \u03c6k iid \u223c H, k = 1, . . . ,\u221e and\n\u03b2 = (\u03b2k) \u221e k=1 are the weights constructed through a \u2018stickbreaking\u2019 process \u03b2k = vk \u220f s<k (1\u2212 vs) with vk iid \u223c Beta (1, \u03b3). It can be shown that \u2211\u221e\nk=1 \u03b2k = 1 with probability one, and as a convention (Pitman, 2002), we hereafter write \u03b2 \u223c GEM (\u03b3).\nDue to its discrete nature, Dirichlet process has been widely used in Bayesian mixture models as the prior distribution on the mixing measures, each is associated with an atom \u03c6k in the stick-breaking representation of G above. A likelihood kernel F (\u00b7) is used to generate data xi | \u03c6k iid \u223c F (\u00b7 | \u03c6k), resulting in a model known as the Dirichlet process mixture model (DPM), pioneered by the work of (Antoniak, 1974) and subsequently developed by many others. In section 3 we provide a precise definition for"}, {"heading": "DPM.", "text": "While DPM models exchangeable data within a single group, the Dirichlet process can also be constructed hierarchically to provide prior distributions over multiple exchangeable groups. Under this setting, each group is modelled as a DPM and these models are \u2018linked\u2019 together to reflect the dependency among them \u2013 a formalism which is generally known as dependent Dirichlet processes (MacEachern, 1999). One particular attractive approach is the hierarchical Dirichlet processes (Teh et al., 2006b) which posits the dependency among the group-level DPM by another Dirichlet process, i.e., Gj | \u03b1,G0 \u223c DP (\u03b1,G0) and G0 | \u03b3,H \u223c DP (\u03b3,H) where Gj is the prior for the j-th group, linked together via a discrete measure G0 whose distribution is another DP.\nYet another way of using DP to model multiple groups is to construct random measure in a nested structure in which the DP base measure is itself another DP. This formalism is the nested Dirichlet Process (Rodriguez et al., 2008), specifically Gj iid \u223c U where U \u223c DP (\u03b1\u00d7 DP (\u03b3H)). Modeling Gj (s) hierarchically as in HDP and nestedly as in nDP yields different effects. HDP focuses on exploiting statistical strength across groups via sharing atoms \u03c6k (s), but it does not partition groups into clusters. This statement is made precisely by noting that P (Gj = Gj\u2032 ) = 0 in HDP. Whereas, nDP emphasizes on inducing clusters on both observations and distributions, hence it partitions groups into clusters. To be precise, the prior probability of two groups being clustered together is P (Gj = Gj\u2032 ) = 1 a+1 . Finally we note that this original definition of nDP in (Rodriguez et al., 2008) does not force the atoms to be shared across clusters of groups, but this can be achieved by simply introducing a DP prior for the nDP base measure, a modification that we use in this paper. This is made clearly in our definition for nDP mixture in section 3."}, {"heading": "3. Multilevel Clustering with Contexts", "text": ""}, {"heading": "3.1. Model description and stick-breaking", "text": "Consider data presented in a two-level group structure as follows. Denote by J the number of groups; each group j contains Nj exchangeable data points, represented by wj = { wj1, wj2, . . . , wjNj } . For each group j, the groupspecific context data is denoted by xj . Assuming that the groups are exchangeable, the overall data is {(xj ,wj)} J\nj=1. The collection {w1, . . . ,wJ} represents observations of the group contents, and {x1, . . . , xJ} represents observations of the group-level contexts.\nWe now describe the generative process of MC2 that generates a two-level clustering of this data. We use a grouplevel DP mixture to generate an infinite cluster model for\ngroups. Each group cluster k is associated with an atom having the form of a pair (\u03c6k, Q\u2217k) where \u03c6k is a parameter that generates the group-level contexts within the cluster and Q\u2217k is a measure that generates the group contents within the same cluster.\nTo generate atomic pairs of context parameter and measurevalued content parameter, we introduce a product basemeasure of the form H \u00d7 DP(vQ0) for the group-level DP mixture. Drawing from a DP mixture with this base measure, each realization is a pair (\u03b8j , Qj); \u03b8j is then used to generate the context xj and Qj is used to repeatedly produce the set of content observations wji within the group j. Specifically,\nU \u223c DP (\u03b1(H \u00d7 DP(vQ0)))where Q0 \u223c DP (\u03b7S)\n(\u03b8j , Qj) iid \u223c U for each group j (1)\nxj \u223c F (.|\u03b8j), \u03d5ji iid \u223c Qj , wji \u223c Y (.|\u03d5ji)\nIn the above, H and S are respectively base measures for context and content parameters \u03b8j and\u03d5ji. The context and content observations are then generated via the likelihood kernels F (\u00b7 | \u03b8j) and Y (\u00b7 | \u03d5ji). To simplify inference, H and S are assumed to be conjugate to F and Y respectively. The generative process is illustrated in Figure 1."}, {"heading": "STICK-BREAKING REPRESENTATION", "text": "We now derive the stick-breaking construction for MC2 where all the random discrete measures are specified by a distribution over integers and a countable set of atoms. The random measure U in Eq. (7) has the stick-breaking form:\nU =\n\u221e\u2211\nk=1\n\u03c0k\u03b4(\u03c6k,Q\u2217k) (2)\nwhere \u03c0 \u223c GEM (\u03b1) and (\u03c6k, Q\u2217k) iid \u223c H \u00d7 DP (vQ0). Equivalently, this means \u03c6k is drawn i.i.d. from H and Q\u2217k drawn i.i.d. from DP (vQ0). Since Q0 \u223c DP (\u03b7S), Q0 and Q\u2217k have the standard HDP (Teh et al., 2006b)\nstick-breaking forms: Q0 = \u2211\u221e m=1 \u01ebm\u03b4\u03c8mwhere \u01eb \u223c GEM(\u03b7), \u03c8m iid \u223c S; Q\u2217k = \u2211\u221e m=1 \u03c4k,m\u03b4\u03c8m where \u03c4 k = (\u03c4k1, \u03c4k2, . . .) \u223c DP (v, \u01eb).\nFor each group j we sample the parameter pair (\u03b8j , Qj) iid \u223c U ; equivalently, this means drawing zj iid \u223c \u03c0 and letting \u03b8j = \u03c6zj and Qj = Q \u2217 zj . For the i-th content data within the group j, the content parameter \u03d5ji is drawn iid \u223c Qj = Q\u2217zj ; equivalently, this means drawing lji iid \u223c \u03c4zj and letting \u03d5ji = \u03c8lji . Figure 1 presents the graphical model of this stick-breaking representation."}, {"heading": "3.2. Inference and Polya Urn View", "text": "We use collapsed Gibbs sampling, integrating out \u03c6k(s), \u03c8m(s), \u03c0 and \u03c4k (s). Latent variables z, l, \u01eb and the hyperparameters \u03b1, v, \u03b7 will be resampled. We only describe the key inference steps in sampling z, l and \u01eb here and refer to Appendix A.2 for the rest of the details (including how to sample the hyper-parameters).\nSampling z. The required conditional distribution is p(zj = k | z\u2212j , l,x, \u03b1,H) \u221d\np (zj = k|z\u2212j, \u03b1) p (xj |zj = k, z\u2212j ,x\u2212j , H)\n\u00d7 p (lj\u2217|zj = k, l\u2212j\u2217, z\u2212j , \u01eb, v)\nThe first term can be recognized as a form of the Chinese restaurant process (CRP). The second term is the predictive likelihood for the context observations under the component \u03c6k after integrating out \u03c6k. This can be evaluated analytically due to conjugacy of F and H . The last term is the predictive likelihood for the group content-index lj\u2217 = {lji|i = 1 . . .Nj}. Since lji | zj = k iid \u223c Mult (\u03c4 k) where \u03c4 k \u223c Dir (v\u01eb1, . . . , v\u01ebM , \u01ebnew), the last term can also be evaluated analytically by integrating out \u03c4 k using the Multinomial-Dirichlet conjugacy property.\nSampling l. Let w\u2212ji be the same set as w excluding wji, let w\u2212ji(m) = {wj\u2032i\u2032 |(j\u2032i\u2032) 6= (ji) \u2227 lj\u2032i\u2032 = m} and l\u2212ij(k) = {lj\u2032i\u2032 |(j\u2032i\u2032) 6= (ji) \u2227 zj\u2032 = k}. Then p (lji = m | l\u2212ji, zj = k, z\u2212j, v,w, \u01eb, S) \u221d\np(wji|l, w\u2212ji, S) p(lji = m|l\u2212ji, zj = k, z\u2212j, \u01eb, v)\n=p (wji | w\u2212ji(m), S) p (lji = m | l\u2212ji(k), \u01eb, v)\nThe first term is the predictive likelihood under mixture component\u03c8m after integrating out \u03c8m, which can be evaluated analytically due to the conjugacy of Y and S. The second term is in the form of a CRP similar to the one that arises during inference for HDP (Teh et al., 2006b).\nSampling \u01eb. Sampling \u01eb requires information from both z and l.\np (\u01eb | l, z, v, \u03b7) \u221d p (l | \u01eb, v, z, \u03b7)\u00d7 p (\u01eb | \u03b7) (3)\nUsing a similar strategy in HDP, we introduce auxiliary variables (okm), then alternatively sample together with \u01eb:\np (okm = h | \u00b7) \u221d Stirl (h, nkm) (v\u01ebm)h, h = 0, 1, . . . , nkm\np (\u01eb | \u00b7) \u221d \u01eb\u03b7\u22121new\nM\u220f\nm=1\n\u01eb \u2211 k okm\u22121\nm\nwhere Stirl (h, nkm) is the Stirling number of the first kind, nkm is the count of seeing the pair (zj = k, lji = m) : \u2200i, j, and finally M is the current number of active content topics. It clear that okm can be sampled from a Multinomial distribution and \u01eb from an (M + 1)-dim Dirichlet distribution."}, {"heading": "POLYA URN VIEW", "text": "Our model exhibits a Polya-urn view using the analogy of a fleet of buses, driving customers to restaurants. Each bus represents a group and customers on the bus are data points within the group. For each bus j, zj acts as the index to the restaurant for its destination. Thus, buses form clusters at their destination restaurants according to a CRP: a new bus drives to an existing restaurant with the probability proportional to the number of other buses that have arrived at that restaurant, and with probability proportional to \u03b1, it goes to a completely new restaurant.\nOnce all the buses have delivered customers to the restaurants, all customers at the restaurants start to behave in the same manner as in a Chinese restaurant franchise (CRF) process: customers are assigned tables according to a restaurant-specific CRP; tables are assigned with dishes \u03c8m (representing the content topic atoms) according to a global franchise CRP. In addition to the usual CRF, at restaurant k, a single dessert \u03c6k (which represents the context-generating atom, drawing iid \u223c from H) will be served to all the customers at that restaurant. Thus, every customer on the same bus j will be served the same dessert \u03c6zj . We observe three sub-CRPs, corresponding to the three DP(s) in our model: the CRP at the dish level is due to the DP (\u03b7S), the CRP forming tables inside each restaurant is due to the DP(vQ0), and the CRP aggregating buses to restaurants is due to the DP (\u03b1(H \u00d7 DP(vQ0)))."}, {"heading": "3.3. Marginalization property", "text": "We study marginalization property for our model when either the content topics \u03d5ji (s) or context topics \u03b8j (s) are marginalized out. Our main result is established in Theorem 9 where we show an interesting link to nested DP and DPM via our model.\nLet H be a measure over some measurable spaces (\u0398,\u03a3). Let P be the set of all measures over (\u0398,\u03a3), suitably endowed with some \u03c3-algebra. Let G \u223c DP(\u03b1H) and\n\u03b8i iid \u223c G. The collection (\u03b8i) then follows the DP mixture distribution which is defined formally below.\nDefinition 1. (DPM) A DPM is a probability measure over \u0398n \u220b (\u03b81, . . . , \u03b8n) with the usual product sigma algebra \u03a3n such that for every collection of measurable sets {(S1, . . . , Sn) : Si \u2208 \u03a3, i = 1, . . . , n}:\nDPM(\u03b81 \u2208 S1, . . . , \u03b8n \u2208 Sn|\u03b1,H)\n=\n\u222b n\u220f\ni=1\nG (Si)DP (dG | \u03b1H)\nWe now state a result regarding marginalization of draws from a DP mixture with a joint base measure. Consider two measurable spaces (\u03981,\u03a31) and (\u03982,\u03a32) and let (\u0398,\u03a3) be their product space where \u0398 = \u03981\u00d7\u03982 and \u03a3 = \u03a31\u00d7\u03a32. Let H\u2217 be a measure over the product space \u0398 = \u03981 \u00d7\u03982 and let H1 be the marginal of H\u2217 over \u03981 in the sense that for any measurable set A \u2208 \u03a31, H1 (A) = H\u2217 (A\u00d7\u03982). Then drawing (\u03b8(1)i , \u03b8 (2) i ) from a DP mixture with base measure \u03b1H and marginalizing out (\u03b8(2)i ) is the same as drawing (\u03b8(1)i ) from a DP mixture with base measure H1. Formally Proposition 2. Denote by \u03b8i the pair ( \u03b8 (1) i , \u03b8 (2) i ) , there holds\nDPM (\n\u03b8 (1) 1 \u2208 S1, . . . , \u03b8 (1) n \u2208 Sn | \u03b1H1\n)\n= DPM (\u03b81 \u2208 S1 \u00d7\u03982, . . . , \u03b8n \u2208 Sn \u00d7\u03982 | \u03b1H \u2217)\nfor every collection of measurable sets {(S1, . . . , Sn) : Si \u2208 \u03a31, i = 1, . . . , n}.\nProof. see Appendix 7.\nNext we give a formal definition for the nDP mixture: \u03d5ji iid \u223c Qj , Qj iid \u223c U , U \u223c DP(\u03b1DP(vQ0)), Q0 \u223c DP (\u03b7S).\nDefinition 3. (nested DP Mixture) An nDPM is a probability measure over \u0398 \u2211J j=1\nNj \u220b (\u03d511, . . . , \u03d51N1 , . . . , \u03d5JNJ ) equipped with the usual product sigma algebra \u03a3N1 \u00d7 . . .\u00d7\u03a3NJ such that for every collection of measurable sets {(Sji) : Sji \u2208 \u03a3, j = 1, . . . , J, i = 1 . . . , Nj}:\nnDPM(\u03d5ji \u2208 Sji, \u2200i, j|\u03b1, v, \u03b7, S)\n=\n\u222b \u222b \n\n\nJ\u220f\nj=1\n\u222b Nj\u220f\ni=1\nQj (Sji)U (dQj)\n \n\n\u00d7 DP (dU | \u03b1DP (vQ0))DP (dQ0 | \u03b7, S)\nWe now have the sufficient formalism to state the marginalization result for our model.\nTheorem 4. Given \u03b1,H and \u03b1, v, \u03b7, S, let \u03b8 = (\u03b8j : \u2200j) and \u03d5 = (\u03d5ji : \u2200j, i) be generated as in Eq (7). Then, marginalizing out \u03d5 results in DPM (\u03b8 | \u03b1,H), whereas marginalizing out \u03b8 results in nDPM (\u03d5|\u03b1, v, \u03b7, S).\nProof. We sketch the main steps, Appendix 9 provides more detail. Let H\u2217 = H1 \u00d7 H2, we note that when either H1 or H2 are random, a result similar to Proposition 7 still holds by taking the expectation on both sides of the equality. Now let H1 = H and H2 = DP (vQ0) where Q0 \u223c DP(\u03b7S) yields the proof for the marginalization of \u03d5; let H1 = DP (vQ0) and H2 = H yields the proof for the marginalization of \u03b8."}, {"heading": "4. Experiments", "text": "We first evaluate the model via simulation studies, then demonstrate its applications on text and image modeling using three real-world datasets. Throughout this section, unless explicitly stated, discrete data is modeled by Multinomial with Dirichlet prior, while continuous data is modeled by Gaussian (unknown mean and unknown variance) with Gaussian-Gamma prior."}, {"heading": "4.1. Simulation studies", "text": "The main goal is to investigate the posterior consistency of the model, i.e., its ability to recover the true group clusters, context distribution and content topics. To synthesize the data, we use M = 13 topics which are the 13 unique letters in the ICML string \u201cINTERNATIONAL CONFERENCE MACHINE LEARNING\u201d. Similar to (Griffiths & Steyvers, 2004), each topic \u03c8m is a distribution over 35 words (pixels) and visualized as a 7 \u00d7 5 binary image. We generate K = 4 clusters of 100 documents each. For each cluster, we choose a set of topics corresponding to letters in the each of 4 words in the ICML string. The topic mixing distribution \u03c4k is an uniform distribution over the chosen topic letters. Each cluster is also assigned a context-generating univariate Gaussian distribution. These generating parameters are shown in Figure 2 (left). Altogether we have J = 400 documents; for each document we sample Nj = 50 words and a context variable xj drawing from the cluster-specific Gaussian.\nWe model the word wji with Multinomial and Gaussian for context xj . After 100 Gibbs iterations, the number of context and content topics (K = 4,M = 13) are recovered correctly: the learned context atoms \u03c6k and topic \u03c8m are almost identical to the ground truth (Figure 2, right) and the model successfully identifies the 4 clusters of documents with topics corresponding to the 4 words in the ICML string.\nTo demonstrate the importance of context observation, we\nthen run LDA and HDP with only the word observations (ignoring context) where the number of topic of LDA is set to 13. As can be seen from Figure 2 (right), LDA and HDP have problems in recovering the true topics. They cannot distinguish small differences between the overlapping character topics (e.g M vs N, or I vs T). Further analysis of the role of context in MC2 is provided in Appendix A.3."}, {"heading": "4.2. Experiments with Real-World Datasets", "text": "We use two standard NIPS and PNAS text datasets, and the NUS-WIDE image dataset.\nNIPS contains 1,740 documents with vocabulary size 13,649 (excluding stop words); timestamps (1987-1999), authors (2,037) and title information are available and used as group-level context. PNAS contains 79,800 documents, vocab size = 36,782 with publication timestamp (915- 2005). For NUS-WIDE we use a subset of the 13-class animals 1 comprising of 3,411 images (2,054 images for training and 1357 images for testing) with off-the-shelf features including 500-dim bag-of-word SIFT vector and 1000-dim bag-of-tag annotation vector."}, {"heading": "Text Modeling with Document-Level Contexts", "text": "We use NIPS and PNAS datasets with 90% for training and 10% for held-out perplexity evaluation. We compare the perplexity with HDP (Teh et al., 2006b) where no grouplevel context can be used, and npTOT (Dubey et al., 2012) where only timestamp information can be used. We note that unlike our model, npTOT requires replication of document timestamp for every word in the document, which is somewhat unnatural.\nWe use perplexity score (Blei et al., 2003) on\n1downloaded from http://www.ml-thu.net/~jun/data/\nheld-out data as performance metric, defined as2 exp { \u2212 \u2211J\nj=1 log p ( w test j | x\ntrain,wtrain ) / ( \u2211\nj N test j\n)}\n.\nTo ensure fairness and comparable evaluation, only words in held-out data is used to compute the perplexity. We use univariate Gaussian for timestamp and Multinomial distributions for words, tags and authors. We ran collapsed Gibbs for 500 iterations after 100 burn-in samples.\nTable 1 shows the results where MC2 achieves significant better performance. This shows that group-level context information during training provide useful guidance for the modelling tasks. Regarding the informative aspect of group-level context, we achieve better perplexity with timestamp information than with titles and authors. This may be explained by the fact that 1361 authors (among 2037) show up only once in the data while title provides little additional information than what already in that abstracts. Interestingly, without the group-level context information, our model still predicts the held-out words better than HDP. This suggests that inducing partitions over documents simultaneously with topic modelling is beneficial.\nBeyond the capacity of HDP and npTOT, our model can induce clusters over documents (value of K in Table 1). Figure 3 shows an example of one such document cluster discovered from NIPS data with authors as context.\nOur proposed model also allows flexibility in deriving useful understanding into the data and to evaluate on its predictive capacity (e.g., who most likely wrote this article, which authors work in the same research topic and so on). Another possible usage is to obtain conditional distributions among context topics \u03c6k (s) and content topics \u03c8m (s). For example if the context information is timestamp, the model immediately yields the distribution over time for\n2Appendix A.4 provides further details on how to derive this score from our model\na topic, showing when the topic rises and falls. Figure 4 illustrates an example of a distribution over time for a content topic discovered from PNAS dataset where timestamp was used as context. This topic appears to capture a congenital disorder known as Albinism. This distribution illustrates research attention to this condition over the past 100 years from PNAS data. To seek evidence for this result, we search the term \u201cAlbinism\u201d in Google Scholar, using the top 50 searching results and plot the histogram over time in the same figure. Surprisingly, we obtain a very close match between our results and the results from Google Scholar as evidenced in the figure."}, {"heading": "Image Clustering with Image-Level Tags", "text": "We evaluate the clustering capacity of MC2 using contexts on an image clustering task. Our dataset is NUS-WIDE described earlier. We use bag-of-word SIFT features from each image for its content. Since each image in this dataset\ncomes with a set of tags, we exploit them as context information, hence each context observation xj is a bag-of-tag annotation vector.\nFirst we perform the perplexity evaluation for this dataset using a similar setting as in the previous section. Table 2 presents the results where our model again outperforms HDP even when no context (tags) is used for training.\nNext we evaluate the clustering quality of the model using the provided 13 classes as ground truth. We report performance on four well-known clustering evaluation metrics: Purity, Normalized Mutual Information (NMI), RandIndex (RI), and Fscore (detailed in (Rand, 1971; Cai et al., 2011)). We use the following baselines for comparison:\n\u2022 Kmeans and Non-negative Matrix Factorization (NMF)(Lee & Seung, 1999). For these methods, we need to specify the number of clusters in advance, hence we vary this number from 10 to 40. We then report the min, max, mean and standard deviation.\n\u2022 Affinity Propagation (AP) (Frey & Dueck, 2007): AP\nrequires a similarity score between two documents and we use the Euclidean distance for this purpose.\n\u2022 Hierarchical Dirichlet Process (HDP) + AP: we first run HDP using content observations, and then apply Affinity Propagation with similarity score derived from the symmetric KL divergence between the mixture proportions from two documents.\nFigure 5 shows the result in which our model consistently delivers highest performance across all four metrics. For purity and NMI, our model beats all by a wide margin.\nTo gain some understanding on the clusters of images induced by our model, we run t-SNE (Van der Maaten & Hinton, 2008), projecting the feature vectors (both content and context) onto a 2D space. For visual clarity, we randomly select 7 out of 28 clusters and display in Figure 6 where it can be seen that they are reasonably well separated.\nEffect of partially observed and missing data\nMissing and unlabelled data is commonly encountered in\npractical applications. Here we examine the effect of context observability on document clustering performance. To do so, we again use the NUS-WIDE 13-animal subset as described previously, then vary the amount of observing context observation xj with missing proportion ranges from 0% to 100%.\nTable 3 reports the result. We make two observations: a) utilizing context results in a big performance gain as evidenced in the difference between the top and bottom row of the table, and b) as the proportion of missing context starts to increase, the performance degrades gracefully up to 50% missing. This demonstrates the robustness of model against the possibility of missing context data."}, {"heading": "5. Conclusion", "text": "We have introduced an approach for multilevel clustering when there are group-level context information. Our MC2 provides a single joint model for utilizing group-level contexts to form group clusters while discovering the shared topics of the group contents at the same time. We provide a collapsed Gibbs sampling procedure and perform extensive experiments on three real-world datasets in both text and image domains. The experimental results using our model demonstrate the importance of utilizing context information in clustering both at the content and at the group level. Since similar types of contexts (time, tags, locations, ages, genres) are commonly encountered in many real-world data sources, we expect that our model will also be further applicable in other domains.\nOur model contains a novel ingredient in DP-based Bayesian nonparametric modeling: we propose to use a base measure in the form of a product between a context-generating prior H and a content-generating prior DP(vQ0). Doing this results in a new model with one marginal being the DPM and another marginal being the nDP mixture, thus establishing an interesting bridge between the DPM and the nDP. Our product base measure construction can be generalized to yield new models suitable for data presenting in more complicated nested group structures (e.g., more than 2-level deep)."}, {"heading": "A. Appendix", "text": "This note provides supplementary information for the main paper. It has three parts: a) the proof for the marginalization property of our proposed model, b) detailed derivations for our inference, and c) equations to show how the perplexity in the experiment was computed."}, {"heading": "A.1. Proof for Marginalization Property (Theorem 4)", "text": "We start with a proposition on the marginalization result for DPM with the product measure then move on the final proof for our proposed model."}, {"heading": "A.1.1. MARGINALIZATION OF DPM WITH PRODUCT MEASURE", "text": "Let H be a measure over some measurable space (\u0398,\u03a3). Let P be the set of all measures over (\u0398,\u03a3), suitably endowed with some \u03c3-algebra. Let G \u223c DP(\u03b1H) be a draw from a Dirichlet process.\nLemma 5. Let S1 . . . Sn be n measurable sets in \u03a3. We form a measurable partition of \u0398, a collection of disjoint measurable sets, that generate S1, . . . , Sn as follows. If S is a set, let S1 = S and S\u22121 = \u0398\\S. Then S\u2217 = { \u22c2n\ni=1 S ci i |ci \u2208 {1,\u22121}} is a partition of \u0398 into a finite collection of disjoint measurable sets with the property that any Si can be written as a union of some sets in S\u2217. Let the element of S\u2217 be A1 . . . An\u2217 (note n\u2217 \u2264 2n). Then the expectation\nE G [G (S1) , . . . , G (Sn)] = (4)\n\u222b n\u220f\ni=1\nG (Si)DP (dG | \u03b1H) (5)\ndepends only on \u03b1 and H(Ai). In other words, the above expectation can be written as a function En(\u03b1,H(A1), . . . H(An\u2217)).\nIt is easy to see that since Si can always be expressed as the sum of some disjoints Ai, G (Si) can respectively be written as the sum of some G (Ai). Furthermore, by definition of a Dirichlet process, the vector (G (A1) , . . . , G (An\u2217)) distributed according to a finite Dirichlet distribution (\u03b1H (A1) , . . . , \u03b1H (An\u2217)), therefore the expectation E\nG [G (Si)] depends only on \u03b1 and\nH (Ai) (s).\nDefinition 6. (DPM) A DPM is a probability measure over \u0398n \u220b (\u03b81, . . . , \u03b8n) with the usual product sigma algebra \u03a3n such that for every collection of measurable sets {(S1, . . . , Sn) : Si \u2208 \u03a3, i = 1, . . . , n}:\nDPM(\u03b81 \u2208 S1, . . . , \u03b8n \u2208 Sn|\u03b1,H) = (6) \u222b\nG\nn\u220f\ni=1\nG (Si)DP (dG | \u03b1H)\nConsider two measurable spaces (\u03981,\u03a31) and (\u03982,\u03a32) and let (\u0398,\u03a3) be their product space where \u0398 = \u03981 \u00d7\u03982 and \u03a3 = \u03a31 \u00d7 \u03a32. We present the general theorem that states the marginal result from a product base measure.\nProposition 7. Let H\u2217 be a measure over the product space\u0398 = \u03981\u00d7\u03982. Let H1 be the marginal of H\u2217 over \u03981 in the sense that for any measurable set A \u2208 \u03a31, H1 (A) = H\u2217 (A\u00d7\u03982). Denote by \u03b8i the pair ( \u03b8 (1) i , \u03b8 (2) i ) , then:\nDPM (\n\u03b8 (1) 1 \u2208 S1, . . . , \u03b8 (1) n \u2208 Sn | \u03b1H1\n)\n= DPM (\u03b81 \u2208 S1 \u00d7\u03982, . . . , \u03b8n \u2208 Sn \u00d7\u03982 | \u03b1H \u2217)\nfor every collection of measurable sets {(S1, . . . , Sn) : Si \u2208 \u03a31, i = 1, . . . , n}.\nProof. Since {(S1, . . . , Sn) : Si \u2208 \u03a31, i = 1, . . . , n} are rectangles, expanding the RHS using Definition 6 gives:\nRHS =\n\u222b\nG (S1 \u00d7\u03982) . . .G (Sn \u00d7\u03982) dDP(dG|\u03b1,H\u2217)\nLet Ti = Si \u00d7 \u03982, the above expression is the expectation of \u220f\niG(Ti) when G \u223c DP (\u03b1H \u2217). Forming col-\nlection of the disjoint measurable sets T \u2217 = (B1 . . . Bn\u2217) that generates Ti, then note that Bi = Ai \u00d7 \u03982, and S\u2217 = (A1 . . . An\u2217) generates Si. By definition of H1, H1(Ai) = H \u2217(Ai \u00d7 \u03982) = H \u2217(Bi). Using the Lemma 5 above, RHS = En(\u03b1,H\u2217(B1) . . .H\u2217(Bn\u2217)), while LHS = En(\u03b1,H1(A1) . . . H1(An\u2217)) and they are indeed the same.\nWe note that H\u2217 can be any arbitrary measure on \u0398 and, in general, we do not require H\u2217 to factorize as product measure."}, {"heading": "A.1.2. MARGINALIZATION RESULT FOR OUR PROPOSED MODEL", "text": "Recall that we are considering a product base-measure of the form H\u2217 = H \u00d7DP(vQ0) for the group-level DP mixture. Drawing from a DP mixture with this base measure, each realization is a pair (\u03b8j , Qj); \u03b8j is then used to generate the context xj and Qj is used to repeatedly generate the\nset of content observations wji within the group j. Specifically,\nU \u223c DP (\u03b1(H \u00d7 DP(vQ0))) where Q0 \u223c DP (\u03b7S)\n(\u03b8j , Qj) iid \u223c U for j = 1, . . . , J (7)\n\u03d5ji iid \u223c Qj, for each j and i = 1, . . . , Nj\nIn the above, H and S are respectively base measures for context and content parameters \u03b8j and \u03d5ji. We start with a definition for nested Dirichlet Process Mixture (nDPM) to proceed further.\nDefinition 8. (nested DP Mixture) An nDPM is a probability measure over \u0398 \u2211 J j=1\nNj \u220b (\u03d511, . . . , \u03d51N1 , . . . , \u03d5JNJ ) equipped with the usual product sigma algebra \u03a3N1 \u00d7 . . .\u00d7\u03a3NJ such that for every collection of measurable sets {(Sji) : Sji \u2208 \u03a3, j = 1, . . . , J, i = 1 . . . , Nj}:\nnDPM(\u03d5ji \u2208 Sji, \u2200i, j|\u03b1, v, \u03b7, S)\n=\n\u222b \u222b \n\n\nJ\u220f\nj=1\n\u222b Nj\u220f\ni=1\nQj (Sji)U (dQj)\n \n\n\u00d7 DP (dU | \u03b1DP (vQ0))DP (dQ0 | \u03b7, S)\nWe now state the main marginalization result for our proposed model.\nTheorem 9. Given \u03b1,H and \u03b1, v, \u03b7, S, let \u03b8 = (\u03b8j : \u2200j) and \u03d5 = (\u03d5ji : \u2200j, i) be generated as in Eq (7). Then, marginalizing out \u03d5 results in DPM (\u03b8 | \u03b1,H), whereas marginalizing out \u03b8 results in nDPM (\u03d5|\u03b1, v, \u03b7, S).\nProof. First we make observation that if we can show Proposition 7 still holds when H1 is random with H2 is fixed and vice versa, then the proof required is an immediate corollary of Proposition 7 by letting H\u2217 = H1 \u00d7 H2 where we first let H1 = H , H2 = DP (vQ0) to obtain the proof for the first result, and then swap the order H1 = DP (vQ0) , H2 = H to get the second result.\nTo see that Proposition 7 still holds when H2 is a random measure and H1 is fixed, we let the product base measure H\u2217 = H1 \u00d7 H2 and further let \u00b5 be a prior probability measure for H2, i.e, H2 \u223c \u00b5 (\u00b7). Denote by \u03b8i the pair( \u03b8 (1) i , \u03b8 (2) i ) , consider the marginalization over H2:\n\u222b\nH2\nDPM (\u03b81 \u2208 S1 \u00d7\u03982, . . . , \u03b8n \u2208 Sn \u00d7\u03982 | \u03b1,H\u2217)\u00b5 (H2)\n=\n\u222b\n\u03a32\nDPM (\n\u03b8 (1) 1 \u2208 S1, . . . , \u03b8 (1) n \u2208 Sn | \u03b1,H1\n)\n\ufe38 \ufe37\ufe37 \ufe38\nconstant w.r.t H2\n\u00b5 (H2)\n= DPM (\n\u03b8 (1) 1 \u2208 S1, . . . , \u03b8 (1) n \u2208 Sn | \u03b1,H1\n) \u222b\n\u03a32\n\u00b5 (H2)\n= DPM (\n\u03b8 (1) 1 \u2208 S1, . . . , \u03b8 (1) n \u2208 Sn | \u03b1,H1\n)\nWhen H1 is random and H2 is fixed. Let \u03bb (\u00b7) be a prior probability measure for H1, ie., H1 \u223c \u03bb (\u00b7). It is clear that Proposition 7 holds for each draw H1 from \u03bb (\u00b7). This complete our proof."}, {"heading": "A.1.3. ADDITIONAL RESULT FOR CORRELATION", "text": "ANALYSIS IN NDPM\nWe now consider the correlation between \u03d5ik and \u03d5jk\u2032 for arbitrary i, j, k and k\u2032, i.e., we need to evaluate:\nP (\u03d5ik \u2208 A1, \u03d5jk\u2032 \u2208 A2 | \u03b1, \u03b7, v, S)\nfor two measurable sets A1, A2 \u2208 \u03a3 by integrating out over all immediate random measures. We use an explicit stick-breaking representation for U where U \u223c DP (\u03b1DP (vQ0)) as follows\nU =\n\u221e\u2211\nk=1\n\u03c0k\u03b4Q\u2217 k\n(8)\nwhere \u03c0 \u223c GEM (\u03b1) and Q\u2217k iid \u223c DP (vQ0). We use the notation \u03b4Q\u2217 k\nto denote the atomic measure on measure, placing its mass at measure Q\u2217k.\nFor i = j, we have:\nP (\u03d5ik \u2208 A1, \u03d5jk\u2032 \u2208 A2 | Q1, . . . , QJ) = Qi (A1)Qi (A2)\nSequentially take expectation over Qi and U :\n\u222b\nQi\nQi (A1)Qi (A2) dU (Qi) =\n\u222b\nQi\nQi (A1)Qi (A2) d\n( \u221e\u2211\nk=1\n\u03c0k\u03b4Q\u2217 k\n)\n=\n\u2211\nk\n\u03c0k [Q \u2217 k (A1)Q \u2217 k (A2)]\n\u222b\nU\n\u221e\u2211\nk=1\n\u03c0k [Q \u2217 k (A1)Q \u2217 k (A2)] dDP (U | \u03b1DP (vQ0)) =\nE\n{ \u2211\nk\n\u03c0k [Q \u2217 k (A1)Q \u2217 k (A2)]\n}\n=\n\u2211\nk\nE [\u03c0k]E [Q \u2217 k (A1)Q \u2217 k (A2)] =\nQ0 (A1 \u2229 A2) +Q0 (A1)Q0 (A2)\nv (v + 1)\n( \u2211\nk\nE [\u03c0k]\n)\n=\nQ0 (A1 \u2229 A2) +Q0 (A1)Q0 (A2)\nv (v + 1)\nIntegrating out Q0 \u223c DP (vS) we get:\nP (\u03d5ik \u2208 A1, \u03d5jk\u2032 \u2208 A2 | \u03b1, v, \u03b7, S) =\nE Q0|\u03b7,S\n[ Q0 (A1 \u2229A2) +Q0 (A1)Q0 (A2)\nv (v + 1)\n]\n=\n1\nv (v + 1)\n{\nS (A1 \u2229A2) + S (A1 \u2229 A2) + S (A1)S (A2)\n\u03b7 (\u03b7 + 1)\n}\n=\nS (A1 \u2229 A2)\nv (v + 1) +\nS (A1 \u2229 A2) + S (A1)S (A2)\nv (v + 1) \u03b7 (\u03b7 + 1)\nFor i 6= j, since Qi and Qj are conditionally independent given U , we get:\nP (\u03d5ik \u2208 A1, \u03d5jk\u2032 \u2208 A2 | Q1, . . . , QJ) = Qi (A1)Qj (A2)\nLet ak = Q\u2217k (A1) , bk = Q \u2217 k (A2) and using Definition (8), integrating out U conditional on Q0 with the stick-breaking representation in Eq (8):\nP (\u03d5ik \u2208 A1, \u03d5jk\u2032 \u2208 A2 | vQ0) = (\u222b\nU\nQi (A1) dU\n)(\u222b\nU\nQj (A2) dU\n)\n=\nE\n[ \u2211\nk\n\u03c0kQ \u2217 k (A1)\n][ \u2211\nk\u2032\n\u03c0k\u2032Q \u2217 k\u2032 (A2)\n]\n=\nE (\u03c01a1 + \u03c02a2 + . . .) (\u03c01b1 + \u03c02b2 + . . .) =\nE\n( \u2211\nk\n\u03c02kakbk\n)\n+ E\n\n \u2211\nk 6=k\u2032\n\u03c0k\u03c0k\u2032akbk\u2032\n\n =\nAE\n( \u2211\nk\n\u03c02k\n)\n+BE\n\n \u2211\nk 6=k\u2032\n\u03c0k\u03c0k\u2032\n\n =\nA \u2211\nk\nE [ \u03c02k ] + B\n(\n1\u2212 \u2211\nk\nE [ \u03c02k ]\n)\nwhere\nA = E [akbk]\n= E [Q\u2217k (A1)Q \u2217 k (A2)]\n= Q0 (A1 \u2229 A2) +Q0 (A1)Q0 (A2)\nv (v + 1)\nand since Q\u2217k (s) are iid draw from DP (vQ0) we have:\nB = E [akbk\u2032 ]\n= E [Q\u2217k (A1)Q \u2217 k\u2032 (A2)] = E [Q\u2217k (A1)]E [Q \u2217 k\u2032 (A2)]\n= Q0 (A1)Q0 (A2)\nLastly, since (\u03c01, \u03c02, . . .) \u223c GEM (\u03b1), using the property of its stick-breaking representation \u2211\nk E [ \u03c02k ] = 11+\u03b1 . Put\nthings together we obtain the expression for the correlation of \u03d5ik and \u03d5jk\u2032 for i 6= j conditional on Q0 as:\nP (\u03d5ik \u2208 A1, \u03d5jk\u2032 \u2208 A2 | vQ0) =\nQ0 (A1 \u2229 A2) +Q0 (A1)Q0 (A2)\n(1 + \u03b1) v (v + 1)\n+ \u03b1\n1 + \u03b1 Q0 (A1)Q0 (A2)\n=\nQ0 (A1 \u2229 A2)\n(1 + \u03b1) v (v + 1)\n+ \u03b1v (v + 1) + 1\n(1 + \u03b1) v (v + 1) Q0 (A1)Q0 (A2)\nNext, integrating out Q0 \u223c DP (vS) we get:\nP (\u03d5ik \u2208 A1, \u03d5jk\u2032 \u2208 A2 | \u03b1, v, \u03b7, S) =\n\u03b1v (v + 1) + 1\n(1 + \u03b1) v (v + 1) E [Q0 (A1)Q0 (A2)]\n+ E [Q0 (A1 \u2229A2)]\n(1 + \u03b1) v (v + 1)\n=\n\u03b1v (v + 1) + 1\n(1 + \u03b1) v (v + 1)\nS (A1 \u2229 A2) + S (A1)S (A2)\n\u03b7 (\u03b7 + 1)\n+ S (A1 \u2229 A2)\n(1 + \u03b1) v (v + 1)"}, {"heading": "A.2. Model Inference Derivations", "text": "We provide detailed derivations for model inference with the graphical model displayed in Fig 1. The variables \u03c6k, \u03c8m, \u03c0, \u03c4k are integrated out due to conjugacy property. We need to sample these latent variables z, l, \u01eb and hyper parameters \u03b1, v, \u03b7. For convenience of notation, we denote z\u2212j is a set of latent context variable z in all documents excluding document j, lj\u2217 is all of hidden variables lji in document j, and l\u2212j\u2217 is all of l in other documents rather than document j-th.\nSAMPLING z\nSampling context index zj needs to take into account the influence of the corresponding context topics:\np(zj = k | z\u2212j , l,x, \u03b1,H) \u221d p (zj = k | z\u2212j , \u03b1) \ufe38 \ufe37\ufe37 \ufe38\nCRP for context topic\n(9)\n\u00d7 p (xj | zj = k, z\u2212j ,x\u2212j , H) \ufe38 \ufe37\ufe37 \ufe38\ncontext predictive likelihood\n\u00d7 p (lj\u2217 | zj = k, l\u2212j\u2217, z\u2212j , \u01eb, v) \ufe38 \ufe37\ufe37 \ufe38\ncontent latent marginal likelihood\nThe first term can easily be recognized as a form of Chinese Restaurant Process (CRP):\np (zj = k | z\u2212j , \u03b1) =\n \n\nnk \u2212j\nn\u2217 \u2212j +\u03b1 if kold \u03b1 n\u2217 \u2212j +\u03b1 if knew\nwhere nk\u2212j is the number of data zj = k excluding zj , and n\u2217\u2212j is the count of all z, except zj .\nThe second expression is the predictive likelihood from the context observations under the context component \u03c6k. Specifically, let f (\u00b7 | \u03c6) and h (\u00b7) be respectively the density function for F (\u03c6) and H , the conjugacy between F and H allows us to integrate out the mixture component parameter \u03c6k , leaving us the conditional density of xj under the mixture component k given all the context data items exclude xj :\np (xj | zj = k, z\u2212j ,x\u2212j , H) = \u222b\n\u03c6k f (xj | \u03c6k)\n\u220f\nj\u2032 6=j,zj\u2032=k\nf (xj\u2032 | \u03c6k)h (\u03c6k) d\u03c6k\n\u222b\n\u03c6k\n\u220f\nj\u2032 6=j,zj\u2032=k\nf (xj\u2032 | \u03c6k) h (\u03c6k) d\u03c6k\n=f \u2212xj k (xj)\nFinally, the last term is the contribution from the multiple latent variables of corresponding topics to that context. Since lji | zj = k iid \u223c Mult (\u03c4 k) where \u03c4 k \u223c Dir (v\u01eb1, . . . , v\u01ebM , \u01ebnew), we shall attempt to integrate out \u03c4 k . Using the Multinomial-Dirichlet conjugacy property we proceed to compute the last term in Eq (9) as following:\np (lj\u2217 | zj = k, z\u2212j , l\u2212j\u2217, \u01eb, v) =\n\u222b\n\u03c4k\np (lj\u2217 | \u03c4 k)\n(10)\n\u00d7p (\u03c4 k | {lj\u2032\u2217 | zj\u2032 = k, j \u2032 6= j} , \u01eb, v)d\u03c4 k\nRecognizing the term p (\u03c4 k | {lj\u2032\u2217 | zj\u2032 = k, j\u2032 6= j} , \u01eb, v) is a posterior density, it is Dirichlet-distributed with the updated parameters\np (\u03c4 k | {lj\u2032\u2217 | zj\u2032 = k, j \u2032 6= j}) (11)\n= Dir (\nv\u01eb1 + c \u2212j k,1, . . . , v\u01ebM + c \u2212j k,M , v\u01ebnew\n)\nwhere c\u2212jk,m = \u2211 j\u2032 6=j \u2211Nj\u2032\ni=1 I (lj\u2032i = m, zj\u2032 = k) is the count of topic m being assigned to context k excluding document j. Using this result, p (lj\u2217 | \u03c4 k) is a predictive likelihood for lj\u2217 under the posterior Dirichlet parameters\n\u03c4 k in Eq 11 and therefore can be evaluated to be:\np (lj\u2217 | zj = k, z\u2212j , l\u2212j\u2217, \u01eb, v)\n=\n\u222b\n\u03c4k\np (lj\u2217 | \u03c4 k)\n\u00d7 Dir (\nv\u01eb1 + c \u2212j k,1, . . . , v\u01ebM + c \u2212j k,M , v\u01ebnew\n)\nd\u03c4 k\n=\n\u222b\n\u03c4k\nM\u220f\nm=1\n\u03c4 c j k,m k,m \u00d7 \u0393 ( \u2211M m=1 ( v\u01ebm + c \u2212j k,m ))\n\u220fM m=1 \u0393\n(\nv\u01ebm + c \u2212j k,m\n)\n\u00d7\nM\u220f\nm=1\n\u03c4 v\u01ebm+c\n\u2212j k,m \u22121\nk,m d\u03c4 k\n= \u0393 ( \u2211M m=1 ( v\u01ebm + c \u2212j k,m ))\n\u220fM m=1 \u0393\n(\nv\u01ebm + c \u2212j k,m\n)\n\u222b\n\u03c4k\nM\u220f\nm=1\n\u03c4 v\u01ebm+c\n\u2212j k,m +cj k,m \u22121\nk,m d\u03c4 k\n= \u0393 ( \u2211M m=1 ( v\u01ebm + c \u2212j k,m ))\n\u220fM m=1 \u0393\n(\nv\u01ebm + c \u2212j k,m\n)\n\u00d7\n\u220fM m=1 \u0393\n(\nv\u01ebm + c \u2212j k,m + c j k,m\n)\n\u0393 ( \u2211M\nm=1\n(\nv\u01ebm + c \u2212j k,m + c j k,m\n))\n= \u0393 ( \u2211M m=1 ( v\u01ebm + c \u2212j k,m ))\n\u0393 ( \u2211M\nm=1\n(\nv\u01ebm + c \u2212j k,m\n)\n+Nj\n)\n\u00d7\nM\u220f\nm=1\n\u0393 (\nv\u01ebm + c \u2212j k,m + c j k,m\n)\n\u0393 (\nv\u01ebm + c \u2212j k,m\n)\n=\n \n\nA = \u0393(\n\u2211 m[v\u01ebm+c \u2212j\nk,m]) \u0393( \u2211 m [v\u01ebm+ck,m]) \u220f m \u0393(v\u01ebm+ck,m) \u0393(v\u01ebm+c\u2212jk,m) if k old\nB = \u0393(\n\u2211 m v\u01ebm)\n\u0393( \u2211\nm v\u01ebm+Nj)\n\u220f\nm \u0393(v\u01ebm+cjk,m) \u0393(v\u01ebm)\nif k new\nnote that \u01eb = (\u01eb1, \u01eb2, ...\u01ebM , \u01ebnew), here \u01eb1:M = (\u01eb1, \u01eb2, ...\u01ebM ), when sampling zj we only use M active components from the previous iteration. In summary, the conditional distribution to sample zj is given as:\np (zj = k | z\u2212j , l,x, \u03b1,H) \u221d {\nnk\u2212j \u00d7 f \u2212xj k (xj)\u00d7A if k previousely used \u03b1\u00d7 f \u2212xji knew (xji)\u00d7B if k = knew\nImplementation note: to evaluate A and B, we make use of the marginal likelihood resulted from a MultinomialDirichlet conjugacy.\nSAMPLING l\nLet w\u2212ji be the same set as w excluding wji, i.e w\u2212ji = {wuv : u 6= j \u2229 v 6= i}, then we can write\np (lji = m | l\u2212ji, zj = k, v, w, S) \u221d\n(12)\np (wji | w\u2212ji, lji = m, \u03c1) \ufe38 \ufe37\ufe37 \ufe38\ncontent predictive likelihood\n\u00d7 p (lji = m | l\u2212ji, zj = k, \u01ebm, v) \ufe38 \ufe37\ufe37 \ufe38\nCRF for content topic\nThe first argument is computed as log likelihood predictive of the content with the component \u03c8m\np (wji | w\u2212ji, lji = m, \u03c1) =\n(13) \u222b\n\u03bbm s (wji | \u03bbm)\n[ \u220f\nu\u2208w\u2212ji(m) y(u | \u03bbm)\n]\ns(\u03bbm)d\u03bbm \u222b\n\u03bbm\n[ \u220f\nu\u2208w\u2212ji(m) y (u | \u03bbm)\n]\ns (\u03bbm) d\u03bbm\n, y\u2212wjim (wji)\nAnd the second term is inspired by Chinese Restaurant Franchise (CRF) as:\np (lji = m | l\u2212ji, \u01ebm, v) =\n{\nck,m + v\u01ebm if mold\nv\u01ebnew if m new\n(14)\nwhere ck,m is the number of data point |{lji|lji = m, zj = k, 1 \u2264 j \u2264 J, 1 \u2264 i \u2264 Nj}|. The final form to sample lji is given as:\np (lji = m | l\u2212ji, zj = k, w, v, \u01eb) \u221d {\n(ck,m + v\u01ebm)\u00d7 y \u2212wji m (wji) if mis used previously v\u01ebnew \u00d7 y \u2212wji m (wji) if m = mnew"}, {"heading": "Sampling \u01eb", "text": "Note that sampling \u01eb require both z and l.\np (\u01eb | l, z, v, \u03b7) \u221d p (l | \u01eb, v, z, \u03b7)\u00d7 p (\u01eb | \u03b7) (15)\nIsolating the content variables lkji generated by the same context zj = k into one group\nlkj = {lji : 1 \u2264 i \u2264 Nj , zj = k} the first term of 15 can be expressed following:\np (l | \u01eb, v, z, \u03b7) =\nK\u220f\nk=1\n\u222b\n\u03c4k\np ( lk\u2217\u2217 | \u03c4k ) p (\u03c4k | \u01eb) d\u03c4k\n=\nK\u220f\nk=1\n\u0393(v)\n\u0393 (v + nk\u2217)\nM\u220f\nm=1\n\u0393(v\u01ebm + nkm)\n\u0393(v\u01ebm)\nwhere nk\u2217 = |{wji | zj = k, i = 1, ...Nj}| and nkm = |{wji | zj = k, lji = m, 1 \u2264 j \u2264 J, 1 \u2264 i \u2264 Nj , }|.\nLet \u03b7r = \u03b7 R , \u03b7new = R\u2212MR \u03b7 and recall that \u01eb \u223c Dir (\u03b7r, . . . , \u03b7r, \u03b7new), the last term of Eq 15 is a Dirichlet density:\np (\u01eb | \u03b7) =Dir\n\n\u03b71, \u03b72, ...\u03b7M \ufe38 \ufe37\ufe37 \ufe38\nM\n, \u03b7new\n\n\n= \u0393(M \u00d7 \u03b7r + \u03b7new)\n[\u0393(\u03b7r)]M\u03b7new\nM\u220f\nm=1\n\u01eb\u03b7r\u22121m \u01eb \u03b7new\u22121 new\nUsing the result:\n\u0393(v\u01ebm + nkm)\n\u0393(v\u01ebm) =\nnkm\u2211\nokm=0\nStirl (okm, nkm) (v\u01ebm)okm\nThus, Eq 15 becomes:\np (\u01eb | l, z, v, \u03b7) =\u01eb\u03b7new\u22121new\nK\u220f\nk=1\n\u0393(v)\n\u0393 (v + nk\u2217)\n\u00d7\nM\u220f\nm=1\n\u01eb\u03b7m\u22121m\nnkm\u2211\nokm=0\nStirl (okm, nkm) (v\u01ebm) okm\n=\u01eb\u03b7new\u22121new\nnkm\u2211\nokm=0\nK\u220f\nk=1\n\u0393(v)\n\u0393 (v + nk\u2217)\n\u00d7\nM\u220f\nm=1\n\u01eb\u03b7m\u22121m Stirl (okm, nkm) (v\u01ebm) okm\np (\u01eb,o | l, z, v, \u03b7) =\u01eb\u03b7new\u22121new\nK\u220f\nk=1\n\u0393(v)\n\u0393 (v + nk\u2217)\n\u00d7\nM\u220f\nm=1\n\u01eb\u03b7m\u22121m Stirl (okm, nkm) (v\u01ebm) okm\nThe probability of the auxiliary variable okm is computed as:\np(okm) =\nnkm\u2211\nokm=0\nStirl (okm, nkm) (v\u01ebm)okm\nNow let o = (okm : \u2200k,m) we derive the following joint distribution:\np (\u01eb | o, l, z, v, \u03b7) = \u01eb\u03b7new\u22121new\nM\u220f\nm=1\n\u01eb \u2211 K okm+\u03b7m\u22121\nm\nAs R \u2192 \u221e, we have\np (\u01eb | o, l, z, v, \u03b7) \u221e = \u01eb\u03b7\u22121new\nM\u220f\nm=1\n\u01eb \u2211 K okm\u22121\nm\nFinally, we sample \u01eb jointly with the auxiliary variable okm by:\np (okm = h | \u00b7) \u221d Stirl (h, nkm) (v\u01ebm)h, h = 0, 1, . . . , nkm\np(\u01eb) \u221d \u01eb\u03b7\u22121new\nM\u220f\nm=1\n\u01eb \u2211 K okm\u22121\nm"}, {"heading": "Sampling hyperparameters", "text": "In the proposed model, there are three hyper-parameters which need to be sampled : \u03b1, v and \u03b7.\nSAMPLING \u03b7\nUsing similar strategy and using technique from Escobar and West (Escobar & West, 1995), we have\np (M | \u03b7, u) = Stirl (M,u) \u03b7M \u0393 (\u03b7)\n\u0393 (\u03b7 + u)\nwhere u = \u2211 m um with um = \u2211\nK okm is in the previous sampling \u01eb and M is the number of active content atoms. Let \u03b7 \u223c Gamma (\u03b71, \u03b72). Recall that:\n\u0393 (\u03b7)\n\u0393 (\u03b7 + u) =\n\u222b 1\n0\nt\u03b7 (1\u2212 t) u\u22121\n(\n1 + u\n\u03b7\n)\ndt\nthat we have just introduced an auxiliary variable t\np (t | \u03b7) \u221d t\u03b7 (1\u2212 t) u\u22121 = Beta (\u03b7 + 1, u)\nTherefore,\np (\u03b7 | t) \u221d \u03b7\u03b71\u22121+M exp {\u2212\u03b7\u03b72} \u00d7 t \u03b7 (1\u2212 t)\nu\u22121\n(\n1 + u\n\u03b7\n)\n= \u03b7\u03b71\u22121+M \u00d7 exp {\u2212\u03b7(\u03b72 \u2212 log t)} \u00d7 (1\u2212 t) u\u22121\n+ \u03b7\u03b71\u22121+M\u22121 exp {\u2212\u03b7(\u03b72 \u2212 log t)} \u00d7 (1\u2212 t) u\u22121 u\n\u221d \u03b7\u03b71\u22121+M exp {\u2212\u03b7(\u03b72 \u2212 log t)}\n+ u\u03b7\u03b71\u22121+M\u22121 exp {\u2212\u03b7(\u03b72 \u2212 log t)}\n= \u03c0tGamma (\u03b71 +M, \u03b72 \u2212 log t) (16)\n+ (1\u2212 \u03c0t)Gamma (\u03b71 +M \u2212 1, \u03b72 \u2212 log t)\nwhere \u03c0t satisfies this following equation to make the above expression a proper mixture density:\n\u03c0t 1\u2212 \u03c0t = \u03b71 +M \u2212 1 u (\u03b72 \u2212 log t) (17)\nTo re-sample \u03b7, we first sample t \u223c Beta (\u03b7 + 1, u), compute \u03c0t as in equation 17, and then use \u03c0t to select the correct Gamma distribution to sample \u03b7 as in Eq. 16.\nSAMPLING \u03b1\nAgain sampling \u03b1 is similar to Escobar et al (Escobar & West, 1995). Assuming \u03b1 \u223c Gamma (\u03b11, \u03b12) with the auxiliary variable t:\np (t | \u03b1,K) \u221dt\u03b11 (1\u2212 t) J\u22121\np (t | \u03b1,K) \u221dBeta (\u03b11 + 1, J)\nJ : number of document\np (\u03b7 | t,K) \u223c\u03c0tGamma (\u03b11 +K,\u03b12 \u2212 log(t))\n+ (1\u2212 \u03c0t)Gamma (\u03b11 +K \u2212 1, \u03b12 \u2212 log(t))\nwhere c, d are prior parameter for sampling \u03b7 following Gamma distribution and \u03c0t1\u2212\u03c0t = \u03b11+K\u22121 J(\u03b12\u2212log t)\nSAMPLING v\nSampling v is similar to sampling concentration parameter in HDP (Teh et al., 2006b). Denote ok\u2217 = \u2211\nm okm, where okm is defined previously during the sampling step for \u01eb, nk\u2217 = \u2211\nm nkm, where nkm is the count of |{lji | zji = k, lji = m}|. Using similar technique in (Teh et al., 2006b), we write:\np (o1\u2217, o2\u2217.., oK\u2217 | v, n1\u2217, ...nK\u2217) =\nK\u220f\nk=1\nStirl(nk\u2217, ok\u2217)\u03b1 ok\u2217 0\n\u00d7 \u0393(v)\n\u0393 (v + nk\u2217)\nwhere the last term can be expressed as\n\u0393(v)\n\u0393 (v + nk\u2217) =\n1\n\u0393(nk\u2217)\n\u222b 1\n0\nbvk (1\u2212 bk) nk\u2217\u22121\n(\n1 + nk\u2217 v\n)\ndbk\nAssuming v \u223c Gamma (v1, v2), define the auxiliary variables b = (bk | k = 1, . . . ,K) , bk \u2208 [0, 1] and t = (tk | k = 1, . . . ,K) , tk \u2208 {0, 1} we have\nq (v, b, t) \u221d vv1\u22121+ \u2211 k Mk exp {\u2212vv1}\n\u00d7\nK\u220f\nk=1\nbvk (1\u2212 bk) Mk\u22121 ( Mk v )tk\nWe will sample the auxiliary variables bk, tk in accordance with v that are defined below:\nq(bk | v) =Beta (v + 1, ok\u2217)\nq (tk | .) =Bernoulli\n( ok\u2217/v\n1 + ok\u2217/v\n)\nq(v | .) =Gamma\n(\nv1 + \u2211\nk\n(ok\u2217 \u2212 tk) , v2 \u2212 \u2211\nk\nlog bk\n)"}, {"heading": "A.3. Relative Roles of Context and Content Data", "text": "Regarding the inference of the cluster index zj (Eq. 9), to obtain the marginal likelihood (the third term in Eq. 9) one has to integrate out the words\u2019 topic labels lji. In doing so, it can be shown that the sufficient statistics coming from the content data toward the inference of the topic frequencies and the clustering labels will just be the empirical word frequency from each document. As each document becomes sufficiently long, the empirical word frequency quickly concentrates around its mean by the central limit theorem (CLT), so as soon as the effect of CLT kicks in, increasing document length further will do very little in improving this sufficient statistics.\nIncreasing the document length will probably not hurt, of course. But to what extent it contributes relative to the number of documents awaits a longer and richer story to be told.\nWe confirm this argument by varying the document length and the number of documents in the synthetic document and see how they affect the posterior of the clustering labels. Each experiment is repeated 20 times. We record the mean and standard deviation of the clustering performance by NMI score. As can be seen from Fig 7, using context observation makes the model more robust in recovering the true document clusters."}, {"heading": "A.4. Perplexity Evaluation", "text": "The standard perplexity proposed by Blei et al (Blei et al., 2003), used to evaluate the proposed model as following:\nperplexity ( wTest ) = exp\n{\n\u2212\n\u2211JTest j=1 log p ( wTestj )\n\u2211JTest j=1 N Test j\n}\nDuring individual sampling iteration t, we utilize the important sampling approach (Teh et al., 2006a) to compute p (wTest). The posterior estimation of \u03c8m in a MultinomialDirichlet case is defined below, note that it can be in other types of conjugacies (Gelman et al., 2003) (e.g. GaussianWishart, Binomial-Poisson):\n\u03c8tm,v = ntm,v + smooth\n\u2211V u=1 n t m,v + V \u00d7 smooth\n\u03c4 tk,m = ck,m + vv \u00d7 \u01ebm\n\u2211M m=1 (ck,m + vv \u00d7 \u01ebm)\nwhere ntm,v is number of times a word v, v \u2208 {1, ..., V } is assigned to context topic \u03c8m in iteration t, and ck,m is the count of the set {wji | zj = k, lji = m, 0 \u2264 j \u2264 J, 0 \u2264 i \u2264 Nj}. There is a constant smooth parameter (Asuncion et al., 2009) that influence on the count, roughly set as 0.1. Supposed that we estimate zTestj = k and l Test ji = m, then the probability p ( wTestj ) is computed as:\np ( wTestj ) =\nNTestj\u220f\ni=1\n1\nT\nT\u2211\nt=1\n\u03c4 tk,m\u03c8 t m,wTest\nji\nwhere T is the number of collected Gibbs samples.\nMC2 on Synthetic Data\nJ: number of document. NJ: number of word per document. NMI: normalized mutual information."}], "references": [{"title": "Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems", "author": ["C.E. Antoniak"], "venue": "The Annals of Statistics,", "citeRegEx": "Antoniak,? \\Q1974\\E", "shortCiteRegEx": "Antoniak", "year": 1974}, {"title": "On smoothing and inference for topic models", "author": ["A. Asuncion", "M. Welling", "P. Smyth", "Y.W. Teh"], "venue": "In Proceedings of the International Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Asuncion et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2009}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Locally consistent concept factorization for document clustering", "author": ["Cai", "Deng", "He", "Xiaofei", "Han", "Jiawei"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Cai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2011}, {"title": "Multilevel analysis in public health research", "author": ["Diez-Roux", "Ana V"], "venue": "Annual review of public health,", "citeRegEx": "Diez.Roux and V.,? \\Q2000\\E", "shortCiteRegEx": "Diez.Roux and V.", "year": 2000}, {"title": "A non-parametric mixture model for topic modeling over time", "author": ["Dubey", "Avinava", "Hefny", "Ahmed", "Williamson", "Sinead", "Xing", "Eric P"], "venue": "arXiv preprint arXiv:1208.4411,", "citeRegEx": "Dubey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dubey et al\\.", "year": 2012}, {"title": "Clustering images using the latent dirichlet allocation model", "author": ["Elango", "Pradheep K", "Jayaraman", "Karthik"], "venue": "University of Wisconsin,", "citeRegEx": "Elango et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Elango et al\\.", "year": 2005}, {"title": "Bayesian density estimation and inference using mixtures", "author": ["M.D. Escobar", "M. West"], "venue": "Journal of the american statistical association,", "citeRegEx": "Escobar and West,? \\Q1995\\E", "shortCiteRegEx": "Escobar and West", "year": 1995}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The Annals of Statistics,", "citeRegEx": "Ferguson,? \\Q1973\\E", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Clustering by passing messages between data", "author": ["B.J. Frey", "D. Dueck"], "venue": "points. Science,", "citeRegEx": "Frey and Dueck,? \\Q2007\\E", "shortCiteRegEx": "Frey and Dueck", "year": 2007}, {"title": "Bayesian data analysis", "author": ["Gelman", "Andrew", "Carlin", "John B", "Stern", "Hal S", "Rubin", "Donald B"], "venue": "CRC press,", "citeRegEx": "Gelman et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 2003}, {"title": "Finding scientific topics", "author": ["Griffiths", "Thomas L", "Steyvers", "Mark"], "venue": "Proceedings of the National academy of Sciences of the United States of America,", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Bayesian nonparametrics", "author": ["N.L. Hjort", "C. Holmes", "P. M\u00fcller", "S.G. Walker"], "venue": null, "citeRegEx": "Hjort et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hjort et al\\.", "year": 2010}, {"title": "Multilevel analysis: Techniques and applications", "author": ["Hox", "Joop"], "venue": null, "citeRegEx": "Hox and Joop.,? \\Q2010\\E", "shortCiteRegEx": "Hox and Joop.", "year": 2010}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Lee", "Daniel D", "Seung", "H Sebastian"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1999}, {"title": "Multilevel modelling of health statistics", "author": ["Leyland", "Alastair H", "Goldstein", "Harvey"], "venue": null, "citeRegEx": "Leyland et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Leyland et al\\.", "year": 2001}, {"title": "Dependent nonparametric processes", "author": ["S.N. MacEachern"], "venue": "In ASA Proceedings of the Section on Bayesian Statistical Science, pp", "citeRegEx": "MacEachern,? \\Q1999\\E", "shortCiteRegEx": "MacEachern", "year": 1999}, {"title": "Extraction of latent patterns and contexts from social honest signals using hierarchical dirichlet processes", "author": ["T.C. Nguyen", "D. Phung", "S. Gupta", "S. Venkatesh"], "venue": "IEEE International Conference on Pervasive Computing and Communications,", "citeRegEx": "Nguyen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2013}, {"title": "Conditionally dependent Dirichlet processes for modelling naturally correlated data sources", "author": ["D. Phung", "X. Nguyen", "H. Bui", "T.V. Nguyen", "S. Venkatesh"], "venue": "Technical report,", "citeRegEx": "Phung et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Phung et al\\.", "year": 2012}, {"title": "Poisson\u2013Dirichlet and GEM invariant distributions for split-and-merge transformations of an interval partition", "author": ["J. Pitman"], "venue": "Combinatorics, Probability and Computing,", "citeRegEx": "Pitman,? \\Q2002\\E", "shortCiteRegEx": "Pitman", "year": 2002}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["Rand", "William M"], "venue": "Journal of the American Statistical association,", "citeRegEx": "Rand and M.,? \\Q1971\\E", "shortCiteRegEx": "Rand and M.", "year": 1971}, {"title": "The nested Dirichlet process", "author": ["A. Rodriguez", "D.B. Dunson", "A.E. Gelfand"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rodriguez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 2008}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "Sethuraman,? \\Q1994\\E", "shortCiteRegEx": "Sethuraman", "year": 1994}, {"title": "A collapsed variational bayesian inference algorithm for latent dirichlet allocation", "author": ["Teh", "Yee W", "Newman", "David", "Welling", "Max"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Hierarchical Dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Laurens", "Hinton", "Geoffrey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "A hierarchical dirichlet process model with multiple levels of clustering for human eeg seizure modeling", "author": ["D. Wulsin", "S. Jensen", "B. Litt"], "venue": "Proceedings of the 29th International Conference on Machine learning,", "citeRegEx": "Wulsin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wulsin et al\\.", "year": 2012}, {"title": "Integrating document clustering and topic modeling", "author": ["Xie", "Pengtao", "Xing", "Eric P"], "venue": null, "citeRegEx": "Xie et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "Dealing with grouped data, a setting known as multilevel analysis (Hox, 2010; Diez-Roux, 2000), has diverse application domains ranging from document modeling (Blei et al., 2003) to public health (Leyland & Goldstein, 2001).", "startOffset": 159, "endOffset": 178}, {"referenceID": 2, "context": "Thus, a typical approach is to first apply dimensionality reduction techniques such as LDA (Blei et al., 2003) or HDP (Teh et al.", "startOffset": 91, "endOffset": 110}, {"referenceID": 2, "context": "Parametric approaches (Xie & Xing, 2013) are extensions of the LDA (Blei et al., 2003) and require specifying the number of topics and clusters in advance.", "startOffset": 67, "endOffset": 86}, {"referenceID": 21, "context": "Bayesian nonparametric approaches including the nested Dirichlet process (nDP) (Rodriguez et al., 2008) and the multi-level clustering hierarchical Dirichlet Process (MLC-HDP) (Wulsin et al.", "startOffset": 79, "endOffset": 103}, {"referenceID": 26, "context": ", 2008) and the multi-level clustering hierarchical Dirichlet Process (MLC-HDP) (Wulsin et al., 2012) can automatically adjust the number of clusters.", "startOffset": 80, "endOffset": 101}, {"referenceID": 2, "context": "We note that standard topic models such as LDA (Blei et al., 2003) or its nonparametric Bayesian counter part, HDP (Teh et al.", "startOffset": 47, "endOffset": 66}, {"referenceID": 17, "context": ", 2011; Elango & Jayaraman, 2005) and HDP+Affinity Propagation for clustering human activities (Nguyen et al., 2013).", "startOffset": 95, "endOffset": 116}, {"referenceID": 21, "context": "The first Bayesian nonparametric model proposed for this task is the nested Dirichlet Process (nDP) (Rodriguez et al., 2008) where documents in a cluster share the same distribution over topic atoms.", "startOffset": 100, "endOffset": 124}, {"referenceID": 26, "context": "The same observation was also made by (Wulsin et al., 2012) who introduced the MLC-HDP, a 3-level extension to the nDP.", "startOffset": 38, "endOffset": 59}, {"referenceID": 12, "context": "The literature on DP is vast and we refer to (Hjort et al., 2010) for a comprehensive account.", "startOffset": 45, "endOffset": 65}, {"referenceID": 8, "context": "Dirichlet process (Ferguson, 1973) is a basic building block in Bayesian nonparametrics.", "startOffset": 18, "endOffset": 34}, {"referenceID": 22, "context": "Sethuraman (Sethuraman, 1994) provides an alternative constructive definition which makes the discreteness property of a draw from a Dirichlet process explicit via the stick-breaking representation: G = \u2211\u221e k=1 \u03b2k\u03b4\u03c6k where \u03c6k iid \u223c H, k = 1, .", "startOffset": 11, "endOffset": 29}, {"referenceID": 19, "context": "It can be shown that \u2211\u221e k=1 \u03b2k = 1 with probability one, and as a convention (Pitman, 2002), we hereafter write \u03b2 \u223c GEM (\u03b3).", "startOffset": 77, "endOffset": 91}, {"referenceID": 0, "context": "A likelihood kernel F (\u00b7) is used to generate data xi | \u03c6k iid \u223c F (\u00b7 | \u03c6k), resulting in a model known as the Dirichlet process mixture model (DPM), pioneered by the work of (Antoniak, 1974) and subsequently developed by many others.", "startOffset": 175, "endOffset": 191}, {"referenceID": 16, "context": "Under this setting, each group is modelled as a DPM and these models are \u2018linked\u2019 together to reflect the dependency among them \u2013 a formalism which is generally known as dependent Dirichlet processes (MacEachern, 1999).", "startOffset": 200, "endOffset": 218}, {"referenceID": 21, "context": "This formalism is the nested Dirichlet Process (Rodriguez et al., 2008), specifically Gj iid \u223c U where U \u223c DP (\u03b1\u00d7 DP (\u03b3H)).", "startOffset": 47, "endOffset": 71}, {"referenceID": 21, "context": "Finally we note that this original definition of nDP in (Rodriguez et al., 2008) does not force the atoms to be shared across clusters of groups, but this can be achieved by simply introducing a DP prior for the nDP base measure, a modification that we use in this paper.", "startOffset": 56, "endOffset": 80}, {"referenceID": 5, "context": ", 2006b) where no grouplevel context can be used, and npTOT (Dubey et al., 2012) where only timestamp information can be used.", "startOffset": 60, "endOffset": 80}, {"referenceID": 2, "context": "We use perplexity score (Blei et al., 2003) on downloaded from http://www.", "startOffset": 24, "endOffset": 43}, {"referenceID": 5, "context": "1 (\u2212, 108) words npTOT (Dubey et al., 2012; Phung et al., 2012) 2491.", "startOffset": 23, "endOffset": 63}, {"referenceID": 18, "context": "1 (\u2212, 108) words npTOT (Dubey et al., 2012; Phung et al., 2012) 2491.", "startOffset": 23, "endOffset": 63}, {"referenceID": 3, "context": "We report performance on four well-known clustering evaluation metrics: Purity, Normalized Mutual Information (NMI), RandIndex (RI), and Fscore (detailed in (Rand, 1971; Cai et al., 2011)).", "startOffset": 157, "endOffset": 187}, {"referenceID": 2, "context": "Perplexity Evaluation The standard perplexity proposed by Blei et al (Blei et al., 2003), used to evaluate the proposed model as following:", "startOffset": 69, "endOffset": 88}, {"referenceID": 10, "context": "The posterior estimation of \u03c8m in a MultinomialDirichlet case is defined below, note that it can be in other types of conjugacies (Gelman et al., 2003) (e.", "startOffset": 130, "endOffset": 151}, {"referenceID": 1, "context": "There is a constant smooth parameter (Asuncion et al., 2009) that influence on the count, roughly set as 0.", "startOffset": 37, "endOffset": 60}], "year": 2014, "abstractText": "We present a Bayesian nonparametric framework for multilevel clustering which utilizes grouplevel context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-specific contexts results in the nDP mixture over content variables. We provide a Polyaurn view of the model and an efficient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.", "creator": "LaTeX with hyperref package"}}}