{"id": "1401.2411", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2014", "title": "Clustering, Coding, and the Concept of Similarity", "abstract": "the paper develops a uniquely related clustering and transformation which expresses a mapping decomposition with local probabilistic model in as related way. scaled descent model models real modular manifold with a riemannian metric, $ { g } _ { ij } ( { \\ - xx } ) $, which becomes recognise as another phrase describing dissimilarity. smooth schwartz polynomial collapses of standard vector process with an expectation probability measure which limits the density before the sample input strings. the decomposition between the iso models assumes written maximal polynomial, $ u ( { \\ fi t } ) $, ignoring its gradient, $ \\ nabla \u2032 ( { \\ bf x } ) $. please use the gradient invariant define the dissimilarity transformation, which determined whether our measure includes dissimilarity will predict instead the probability measure. finally, we write additive identities because they define a coordinate system on the embedded normal manifold, hopefully providing us a q - compression encoding of our original expression.", "histories": [["v1", "Fri, 10 Jan 2014 17:36:23 GMT  (2078kb,D)", "http://arxiv.org/abs/1401.2411v1", "55 pages, 13 figures"]], "COMMENTS": "55 pages, 13 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["l thorne mccarty"], "accepted": false, "id": "1401.2411"}, "pdf": {"name": "1401.2411.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "1. Introduction.", "text": "Clustering algorithms have been studied for several decades [DH73], and they remain one of the main ingredients in unsupervised learning [DHS01]. Intuitively, a cluster is both a geometric concept (e.g., a lowdimensional region in a high-dimensional space) and a probabilistic concept (e.g., a region of the input space in which the sample data density is high).\nRecently, a variant of the traditional clustering algorithms has attracted some attention, under the rubric of manifold learning : [TSL00] [RS00] [BN03]. In this variant, the learning task is to construct a low-dimensional manifold, embedded in the original high-dimensional space, on which the probability density of the input data is high. For\nDate: October, 2013; December, 2013. c\u00a9 L. Thorne McCarty. 1\nar X\niv :1\n40 1.\n24 11\nv1 [\ncs .L\nG ]\n1 0\nJa n\nexample, in one recent paper, Rifai, et al. [RDV+11], outline three hypotheses that motivate much of this work:\n1. The semi-supervised learning hypothesis, according to which learning aspects of the input distribution p(x) can improve models of the conditional distribution of the supervised target p(y|x), i.e., p(x) and p(y|x) share something . . . [citations omitted] 2. The (unsupervised) manifold hypothesis, according to which real world data presented in high dimensional spaces is likely to concentrate in the vicinity of non-linear sub-manifolds of much lower dimensionality . . . [citations omitted] 3. The manifold hypothesis for classification, according to which points of different classes are likely to concentrate along different sub-manifolds, separated by low density regions of the input space.\nThe authors then present a \u201cContractive Auto-Encoder (CAE)\u201d algorithm to exploit these hypotheses, and they combine this with an existing supervised learning algorithm to produce what they call a \u201cManifold Tangent Classifier (MTC),\u201d which performs very well on several datasets. It is interesting to note that these algorithms are based, explicitly, on concepts from differential geometry, but they draw only implicitly on probability theory. The informal language of probability theory abounds. For example, the authors write that the \u201cdata density concentrates near low-dimensional manifolds\u201d and \u201cdifferent classes correspond to disjoint manifolds separated by low density\u201d (see abstract). But there is no explicit probability model in the paper.\nIn this paper, we will develop a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way. The geometric model is a Riemannian manifold with a Riemannian metric, gij(x), which we interpret as a measure of dissimilarity. The probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data. The link between the two models is a potential function, U(x), and its gradient, \u2207U(x). We use the gradient to define the dissimilarity metric, which guarantees that our measure of dissimilarity will depend on the probability measure. Roughly speaking, the dissimilarity will be small in a region in which the probability density is high, and vice versa. Finally, we use the dissimilarity metric to define a coordinate system on the embedded Riemannian manifold, which gives us a low-dimensional encoding of our original data.\nSection 2 reviews the \u201cMathematical Background\u201d of the paper, including several theorems which will play a central role in the subsequent discussion. Section 3 then discusses \u201cPrototype Coding,\u201d our overall model, and explains how the dissimilarity metric and the lowdimensional coordinate system are related to the stochastic process with an invariant probability measure. Section 4 investigates the differential geometry component of the model more carefully, with a focus on the important concept of an \u201cIntegral Manifold.\u201d At this point in the paper, we restrict our analysis to R3 rather than Rn, although we will see later (in Section 7) that this is not actually a limitation on the scope of the theory. Instead, the restriction to three dimensions simplifies our calculations, and makes them much easier to visualize. Accordingly, in Section 5, we present the results of a number of experiments using Mathematica, including some full-color three-dimensional graphics of several examples which are intended to aid our intuitions about the main elements of the theory. Section 6 discusses an interesting technical result, which also helps to link the geometric model to the probabilistic model. Finally, Section 7 discusses \u201cFuture Work,\u201d including a further analysis of the connections between the present theory and the current literature on manifold learning."}, {"heading": "2. Mathematical Background.", "text": "Let\u2019s start with a model that will be familiar to most physicists: the Feynman-Kac formula [Fey48] [Kac49]. We will write this formula as follows:\n(1) u(t,x) = \u222b \u2126 f(Xt) exp [ \u2212 \u222b t 0 V (Xs) ds ] Wx(dX) Here, Xt \u2261 X(t, \u03c9) denotes a continuous path in Rn, and Wx denotes Wiener measure over all such paths beginning at X0 = x. If V : R\nn \u2192 R is bounded below, then u(t,x) is a solution to the Cauchy initial value problem:\n(2) \u2202u\n\u2202t =\n1 2 \u2206u\u2212 V (x)u with u(0, \u00b7) = f\nin which \u2206 denotes the standard Laplacian in Cartesian coordinates. Conversely, any bounded solution to (2) is equal to the function defined by (1). See [Str93], Section 4.3. Now, following Feynman\u2019s heuristic picture of formula (1), we can write a discrete approximation to Wiener measure as:\u222b\nexp [ \u2212\nm\u2211 k=1 sk \u2212 sk\u22121 2 ( |X(sk)\u2212X(sk\u22121)| sk \u2212 sk\u22121 )2] dX(s1) . . . dX(sm),\nmultiplied by a normalization factor, so that the exponential function in the integrand of (1) could be viewed, in the limit, as:\n(3) exp [ \u2212 \u222b t\n0\n1 2 |X\u0307(s)|2 + V (X(s)) ds ] See [Str93], Section 4.2, or [Str11], Section 8.1. The quantity inside the integral sign is, of course, the Hamiltonian of a classical dynamical system with the potential function: V (x).\nThis model obviously possesses some of the properties that we want: Equations (1) and (2) specify a stochastic process that depends on the potential function, V (x), and the exponent in formula (3) can be interpreted as an expression in differential geometry, which also depends on V (x). Furthermore, the paths that minimize the \u201cenergy\u201d in (3) will maximize the probability in (1). Now imagine that we can choose the potential function, V (x), in such a way as to generate an invariant probability measure on Rn. In other words, imagine that we can find a steady-state solution to equation (2). We can then project our stochastic process onto a nonlinear subspace of Rn \u2014 i.e., onto an embedded Riemannian manifold \u2014 and examine the probability density induced on that subspace. Feynman\u2019s heuristic picture of the relationship between (1) and (3) suggests that the subspaces of maximal probability will also be the subspaces of minimal energy, and the hope is that this will lead us to a solution to the clustering and coding problems in Rn.\nHowever, there are several problems with this model:\n\u2022 First, it is well known that Feynman\u2019s heuristic interpretation of formula (1) is mathematical nonsense, since there is no analogue of Lebesgue measure in an infinite-dimensional space. The relationship between (1) and (2) holds rigorously, as stated, ifWx is Wiener measure, or Brownian motion, but there is still a gulf between (1) and (3). To interpret the integral in (3) as an expression in differential geometry, the paths Xs \u2261 X(s) \u2261 X(s, \u03c9) must be continuous and differentiable. But, under Wiener measure, with probability one, the paths X(t, \u03c9) are continuous but nowhere differentiable. Thus there is a fundamental clash between the geometric model and the probabilistic model. Stroock calls this \u201ca fact which . . . haunts every attempt to deal with Brownian paths,\u201d [Str96], p. 140.\n\u2022 Second, assuming that we can overcome our first problem, it is not a simple matter to project a stochastic process from Rn\nonto an embedded Riemannian manifold. The mathematical problem itself has only been solved, in general, during the course\nof the past 20 or 30 years, and it is now part of a subject known as stochastic differential geometry. See [EM89] or [Hsu02]. But the calculations are not trivial.\n\u2022 Finally, it would be a mistake to assume that the FeynmanKac formula can be used directly to generate a stochastic process, with a proper probabilistic interpretation. Instead, we will need a new potential function, U(x), and we will need a further derivation from equations (1) and (2), in order to construct a stochastic process with an invariant probability measure. This also means that we will not be able to define our dissimilarity metric, directly, by minimizing the energy functional in formula (3).\nIn the remainder of this section, we will address these three problems, in reverse order. Our analysis will eventually lead us to a modification of the naive Feynman-Kac model, and to the definition of a dissimilarity metric which will achieve the goals articulated in Section 1.\n2.1. A Stochastic Process with an Invariant Measure. To see the problem with the basic Feynman-Kac formula, it is helpful to rewrite (1) using an operator:\n(4) [Ptf ](x) = \u222b \u2126 f(Xt) exp [ \u2212 \u222b t 0 V (Xs) ds ] Wx(dX)\nIt turns out that Pt1 6= 1, which means that we cannot use this operator to construct a Markov process with a proper probabilistic interpretation. Another manifestation of the same problem is the fact that V (x) has a natural interpretation as the \u201ckilling rate\u201d for the process, i.e., the probability per unit of time that a path starting at X0 = x will \u201cdie\u201d by time \u03b4t. Thus the process \u201cdissipates\u201d as time goes by.\nTo fix this problem, we need a new potential function. If \u00b5 is a probability density that satisfies 1\n2 \u2206\u00b5\u2212 V (x)\u00b5 = 0, then\nV (x) = 1\n2\n( \u2206\u00b5\n\u00b5\n) = 1\n2\n( \u2206 log \u00b5+ |\u2207 log \u00b5|2 ) The first equality is trivial, and the second equality follows from a straightforward computation, e.g., by expanding \u2206 log \u00b5 in Cartesian coordinates. This equation suggests that we should work with a potential function U(x) and define V (x) as follows:\n(5) V (x) = 1\n2\n( \u2206U(x) + |\u2207U(x)|2 )\nNow consider the following initial value problem:\n(6) \u2202w\n\u2202t =\n1 2 \u2206w + \u2207U(x) \u00b7\u2207w with w(0, \u00b7) = f\nLemma 1. w(t,x) is a solution to (6) if and only if eU(x)w(t,x) is a solution to (2) with initial value u(0, \u00b7) = eUf . Proof. By a straightforward computation, using the definition in (5) of V (x) in terms of U(x).\nWe now use both U and V to define a new operator:\n[Qtf ](x) =(7) exp [\u2212U(X0)] \u222b\n\u2126\nf(Xt) exp [ U(Xt)\u2212 \u222b t 0 V (Xs) ds ] Wx(dX)\nTheorem 1. If U is bounded above and V is bounded below, and if w(t,x) is a solution to (6) and eU(x)w(t,x) is also bounded, then w(t,x) is equal to [Qtf ](x) as defined in (7). Furthermore, Qt1 = 1 for all t \u2265 0, and (Qt)t\u22650 is a semigroup of operators which defines a Markov process on Rn with the invariant measure \u00b5 = eU(x).\nProof. See Theorem 4.3.36 in [Str93] or Theorem 10.3.33 in [Str11].\nIn the literature, (6) is known as a diffusion equation with a drift term \u2207U . It is a nice feature of our formalism that this drift term is the gradient of a potential U(x), and that the invariant measure turns out to be the exponential of the potential U(x). For a numerical example, if U(x) is a negative quadratic polynomial (which would be bounded above), then V (x) would be a positive quadratic polynomial (which would be bounded below), and the invariant measure would be a Gaussian. See Section 5.1 below.\nSources: These results appear in [Str93], Section 4.3, but the analysis there uses a different definition of V in terms of U . In the second edition of his book, Stroock switches to the more natural definition in (5) above, but with the opposite sign. See [Str11], Section 10.3. \u00d8ksendal also uses this example, with the same definition of V and the same sign, in Exercises 8.15 and 8.16 of his text [\u00d8ks03].\n2.2. Mapping a Diffusion to an Embedded Manifold. The equations in the previous section were all expressed in Cartesian coordinates, and the results would be different in a different coordinate system. For a simple example, if the standard 2-dimensional Laplacian were transformed into polar coordinates, it would acquire an additional first-order \u201cdrift\u201d term. This is a problem if we want to map a diffusion from Rn onto a nonlinear Riemannian manifold.\nOne approach to this problem is to analyze the diffusion by means of a stochastic differential equation, in two versions, one due to Ito\u0302, and one due to Stratonovich. We will write a 1-dimensional Ito\u0302 process as:\nX(t) = X(0) + \u222b t 0 \u03c3(s, \u03c9) dB(s, \u03c9) + \u222b t 0 b(s, \u03c9) ds\nwhere the first integral is an Ito\u0302 integral defined with respect to the Brownian motion B(t, \u03c9), and the second integral is an ordinary Riemann or Lebesgue integral. In differential notation, this would be:\n(8) dX(t) = \u03c3(t, \u03c9) dB(t, \u03c9) + b(t, \u03c9) dt Extending this notation to n dimensions, let B1(t, \u03c9), . . . ,Bd(t, \u03c9) be d independent Brownian motion processes, assume that b : Rn \u2192 Rn and \u03c3 : Rn \u2192 Rn\u00d7d are Lipschitz continuous, and define the n-dimensional Ito\u0302 process as follows:\n(9) dX(t) = \u03c311 . . . \u03c31d... ... \u03c3n1 . . . \u03c3 n d dB1(t)... dBd(t)  + b1... bn  dt We want to construct a differential operator associated with this process. Setting a = \u03c3\u03c3T , define L for all f \u2208 C2(Rn; R) by:\n(10) [Lf ](x) = 1 2 \u2211 i,j aij(x) \u22022f \u2202xi\u2202xj + \u2211 i bi(x) \u2202f \u2202xi\nTheorem 2. The operator L defined in (10) is the infinitesimal generator of the n-dimensional Ito\u0302 process given by (9).\nProof. See Definition 7.3.1 and Theorem 7.3.3 in [\u00d8ks03].\nIntuitively, \u03c3 is the \u201csquare root\u201d of a. Note also that, if a = \u03c3\u03c3T is the identity matrix and b = \u2207U , then (9) and (10) give us the same stochastic process in Rn as does (6).\nFor our purposes, however, the Ito\u0302 process has a defect: It is not invariant under coordinate transformations. This can be seen by an examination of Ito\u0302\u2019s formula, which functions as a \u201cchain rule\u201d for the stochastic calculus, but with a second-order correction term. Let F : Rn \u2192 R be a function with continuous second-order partial derivatives. Then Ito\u0302\u2019s formula asserts that:\ndF (X(t)) = \u2211 i \u2202F (X(t)) \u2202xi dXi(t) + 1 2 \u2211 i,j \u22022F (X(t)) \u2202xi \u2202xj dXi(t) dXj(t)\nSee [\u00d8ks03], Chapter 4. An alternative is to use the Stratonovich integral, which cancels out the correction term. A common notational\ndevice is to insert the symbol \u201c\u25e6\u201d in (9) to indicate that the stochastic integral is intended to be interpreted in the Stratonovich sense rather than the Ito\u0302 sense. Using this notation, the equation for dF (X(t)) would be written as:\n(11) dF (X(t)) = \u2211 i \u2202F (X(t)) \u2202xi \u25e6 dXi(t)\nin accordance with the usual rules of the Newton-Leibniz calculus. Since F could be an arbitrary coordinate transformation, the use of the Stratonovich formula in (11), instead of Ito\u0302\u2019s formula, makes it possible to combine the stochastic calculus with the traditional constructs of Riemannian geometry.\nFortunately, the Ito\u0302 integral and the Stratonovich integral can be developed in parallel, and it is possible to choose whichever version works best in a particular application. In the 1-dimensional case, we will write the Stratonovich version of a stochastic process as follows:\nX(t) = X(0) + \u222b t 0 \u03c3(s, \u03c9) \u25e6 dB(s, \u03c9) + \u222b t 0 b\u0303(s, \u03c9) ds\nNotice the notation \u201c\u25e6 dB(s, \u03c9)\u201d here, and the use of the function b\u0303(s, \u03c9) instead of b(s, \u03c9). Written as a differential, this would be:\n(12) dX(t) = \u03c3(t, \u03c9) \u25e6 dB(t, \u03c9) + b\u0303(t, \u03c9) dt Extending this notation to n dimensions, we have:\n(13) dX(t) = \u03c311 . . . \u03c31d... ... \u03c3n1 . . . \u03c3 n d  \u25e6 dB1(t)... dBd(t)  + b\u03031... b\u0303n  dt Lemma 2. The stochastic process defined by the Ito\u0302 integral in (9) is identical to the process defined by the Stratonovich integral in (13) if and only if\n(14) b\u0303i = bi \u2212 1 2 d\u2211 k=1 n\u2211 j=1 \u2202\u03c3ik \u2202xj \u03c3jk\nProof. See [Str66] or [Ito\u030275].\nWe thus have a simple mapping between the two formalisms, with the advantage that the stochastic differential equation in Stratonovich form is invariant under coordinate transformations.\nLemma 2 has an interesting consequence if we start out with the stochastic process given by (6). Recall that a = \u03c3\u03c3T is the identity and b = \u2207U in this case. Suppose we satisfy the condition a = I by\nsetting \u03c3 = I. Then the second term in (14) vanishes, and b\u0303 = b. However, if we subsequently apply a nonlinear coordinate transformation to our process, or project it onto a nonlinear subspace, then the Ito and Stratonovich equations will diverge, and we will want to use the Stratonovich equation from then on.\nLet us now reinterpret the preceding analysis as a general property of vector fields. Define the column vectors\nA0 = b\u03031... b\u0303n  and Ak = \u03c31k... \u03c3nk  for k = 1, . . . , d and rewrite (13) as:\n(15) dX(t) = ( A1| . . . |Ad ) \u25e6 dB(t) + A0 dt\nWe will think of a vector field as a differential operator, essentially the directional derivative with respect to a given vector V. Let us write this in shorthand notation as V\u2202. It then makes sense to talk about the \u201csquare\u201d of a vector field, which we can define as the composition of the differential operator with itself: (V\u2202)2 = V\u2202 \u25e6V\u2202. Expanding this formula in a coordinate system, we have:(\u2211\ni\nV i \u2202\n\u2202xi\n) \u25e6 (\u2211 j V j \u2202 \u2202xj ) =(16)\n\u2211 i,j V iV j \u22022 \u2202xi\u2202xj + \u2211 i,j \u2202V i \u2202xj V j \u2202 \u2202xi\nNow apply this equation to each of the vector fields Ak\u2202.\nTheorem 3. If L is the differential operator associated with the stochastic process defined in (15), then\n(17) L = 1 2 d\u2211 k=1 (Ak\u2202) 2 + A0\u2202\nProof. By a straightforward computation, using (10), (14) and (16).\nWith Theorem 3 as a guide, we can bypass the Ito\u0302 or Stratonovich stochastic differential equations entirely, and work directly with vector fields. This is our second (but closely related) approach to the problem of mapping diffusions to embedded manifolds. Let V0\u2202 and Vk\u2202,\nfor k = 1, . . . , d, be arbitrary vector fields, and define the differential operator\n(18) L = 1 2 d\u2211 k=1 (Vk\u2202) 2 + V0\u2202.\nThis is known as the Ho\u0308rmander form for the operator L, and it, too, can be shown to be invariant under coordinate transformations. See [Ho\u0308r67]. Thus L works just as well in an arbitrary manifold M as it does in Rn endowed with Cartesian coordinates. The only condition that we need to impose to guarantee that L, as defined in (18), gives us a nondegenerate diffusion in M is to require that the vector fields {V1(x)\u2202, . . . ,Vd(x)\u2202} span the tangent space on M at x. For these reasons, Stroock relies on the Ho\u0308rmander formalism extensively in his book on the analysis of Brownian paths on Riemannian manifolds [Str00].\nSources: For the basic results on stochastic differential equations, using Ito\u0302\u2019s formalism, the reader should consult [\u00d8ks03], but \u00d8ksendal\u2019s text provides only a cursory treatment of Stratonovich\u2019s formalism. The original paper by Stratonovich [Str66] is still very readable, but his theory was only given a solid mathematical foundation some years later by Ito\u0302 [Ito\u030275]. Chapter 8 of [Str03] is an excellent contemporary account of Stratonovich\u2019s theory, set in a broader context.\n2.3. Integral Curves and Martingales on Manifolds. There remains the problem that \u201chaunts every attempt to deal with Brownian paths,\u201d [Str96], p. 140. How do you reconcile the \u201csmooth\u201d curves of differential geometry with the \u201crough\u201d paths that provide the support for Wiener measure? One answer, suggested by Stroock, emerges from a study of the relationship between the integral curves of a vector field and the concept of a martingale.\nLet\u2019s examine this idea, first, in the ordinary Euclidean space Rn. Roughly speaking, a (continuous parameter) martingale Mt is a stochastic process which is \u201cconditionally constant\u201d in the sense that"}, {"heading": "E[ Mt | Fs ] = Ms for all 0 \u2264 s \u2264 t,", "text": "where the conditional expectation E is taken with respect to an nondecreasing family of sub-\u03c3-algebras {Fs}s\u22650 with the property that each Mt is Ft-measurable. Since we are only considering probability spaces (\u2126,F ,P) in which \u2126 is the set of continuous paths in Rn and for which the \u03c3-algebras F and {Fs}s\u22650 are fixed, we will suppress these references in our notation, and refer simply to a \u201cmartingale with respect\nto P,\u201d or a P-martingale. We are interested in the relationship between martingales and differential operators.\nDefinition 1. Let L be a second-order differential operator, and let Px be a probability measure on the space C([0,\u221e); Rn) of all continuous paths in Rn such that Px(X0 = x) = 1. We say that Px solves the martingale problem for L starting at x if\nMt \u2261 f(Xt)\u2212 \u222b t\n0\n[Lf ](Xs)ds\nis a Px-martingale for every f \u2208 C\u221e(Rn; R).\nNot surprisingly:\nLemma 3. If L = 1 2 \u2206, then the Wiener measure Wx solves the martingale problem for L starting at x.\nProof. See Corollary 7.1.20 and Remark 7.1.23 in [Str93].\nLet us now consider the operator L = b \u00b7\u2207 and the integral equation:\n(19) Yt = x + \u222b t 0 b(Ys) ds, 0 \u2264 t, where Yt \u2261 Y (t) is a continuous path in Rn. An equivalent differential equation is:\nY \u2032(t) = b(Y (t))(20)\nY (0) = x\nBy the existence and uniqueness theorem for ordinary differential equations, (19) and (20) have a unique solution, which would commonly be referred to as the integral curve of the vector field b starting at x. Intuitively, an integral curve is a curve whose tangent is identical to the given vector field at each point. Note, too, that an integral curve is a \u201csmooth\u201d curve if b is a smooth vector field. We have the following result:\nLemma 4. Let L = b\u00b7\u2207, and let Px be the unit point mass concentrated on the solution to (19) or (20) . Then Px solves the martingale problem for L starting at x.\nProof. See Exercise 7.1.32 in [Str93].\nWe now put these two examples together, and consider the differential operator:\n(21) L = 1 2 \u2206 + b \u00b7\u2207\nalong with the stochastic process determined by the integral equation:\n(22) Yt = Xt + \u222b t 0 b(Ys) ds, 0 \u2264 t\nIn this equation, we are assuming that Xt is our original stochastic process with the usual Wiener measureWx, and Yt is a derived process with a derived probability measure.\nTheorem 4. Let L be the differential operator given by (21), and let Qx be the probability measure determined by (22) when Xt is a stochastic process whose probability law is given by Wiener measure. Then Qx solves the martingale problem for L starting at x.\nProof. See Theorem 7.3.10 in [Str93].\nIntuitively, these results show that a stochastic process defined by (6), or (9), or (13), has a \u201cpure\u201d diffusion part and a \u201cpure\u201d drift part, and the drift part follows the integral curve of the drift vector.\nThe preceding analysis is not confined to Euclidean Rn, since a similar construction works when L is given in Ho\u0308rmander form by (18), see [ST94], and this means that all results can be replicated in an arbitrary Riemannian manifold, see [ST96]. The theory is explicated further in [Str00], where it serves as the foundation for Stroock\u2019s construction and analysis of Brownian motion on a Riemannian manifold. Specifically, Section 2.2.1 of [Str00] includes a generalization of Lemma 4 above, and Theorem 2.40 of [Str00] is a generalization of Theorem 4."}, {"heading": "3. Prototype Coding.", "text": "In discussing the mathematical background of the paper in the previous section, we were actually developing, implicitly, the main elements of our geometric and probabilistic models. The potential function, U(x), and its gradient, \u2207U(x), were introduced in connection with equations (6) and (7) and Theorem 1. Equation (6) is a diffusion equation with a drift term, \u2207U(x), and it has an invariant probability density equal to eU(x), modulo a normalization factor. The stochastic process described by equation (6) can also be written as an Ito\u0302 process, using equations (9) and (10) and Theorem 2, or it can be written in Stratonovich form, using equation (13) and Lemma 2. An alternative view of equation (6) is given by Stroock\u2019s result, Theorem 4, on the relationship between integral curves and martingales on manifolds.\nRecall that the main goal of our theory is to construct a lowerdimensional subspace of the original Euclidean space, Rn, which is \u201coptimal\u201d in some sense. To be specific, let\u2019s say that the subspace\nshould be a k-dimensional Riemannian manifold, embedded in Rn, with a local coordinate system centered at (0, 0, . . . , 0). We will use a form of prototype coding for the coordinate system, measuring the distance from the origin (i.e., the \u201cprototype\u201d) in k\u22121 specified directions. Extending this coordinate system to all of Rn, we can assume that these k \u2212 1 coordinate directions have been chosen from among n \u2212 1 coordinate directions in the full space. We will now follow the strategy suggested at the beginning of Section 2 for the naive FeynmanKac model. Choose U(x) and \u2207U(x) so that the invariant probability density for the stochastic process given by equation (6) matches the density of our sample input data in Rn. We can then project this stochastic process onto the embedded k-dimensional manifold, and examine the probability density induced on that manifold. The hope is that this procedure will lead us to the \u201cbest\u201d k-dimensional coordinate system for the purpose of encoding our initial data.\nHow to do this? Our first step was described briefly in the text following Theorem 2 above. We start with (6): a diffusion equation with a drift vector, \u2207U(x). We then write the differential operator associated with (6) in the form given by (10):\nL = 1 2 \u2211 i,j aij(x) \u22022 \u2202xi\u2202xj + \u2211 i bi(x) \u2202 \u2202xi\nby setting a(x) equal to the identity matrix, and setting b(x) = \u2207U(x). By Theorem 2, L is the infinitesimal generator of the n-dimensional Ito\u0302 process given by (9):\ndX(t) = \u03c3ik(x) dB1(t)...\ndBn(t)  + b1(x)... bn(x)  dt\nThe choice of \u03c3(x) is arbitrary, as long as a(x) = \u03c3(x)\u03c3(x)T is the identity matrix, which means that \u03c3(x) must be an orthogonal transformation. These equations are expressed in Cartesian coordinates.\nTo implement the idea of prototype coding, suppose we are given a radial coordinate, \u03c1, and the directional coordinates \u03b81, \u03b82, . . . , \u03b8n\u22121. For convenience, we will use the symbol \u0398 to refer to the entire sequence\nof directional coordinates. Assume the existence of n coordinate transformation functions, with the usual properties:\nx1 = x1(\u03c1, \u03b81, \u03b82, . . . , \u03b8n\u22121)\nx2 = x2(\u03c1, \u03b81, \u03b82, . . . , \u03b8n\u22121)\n. . .\nxn = xn(\u03c1, \u03b81, \u03b82, . . . , \u03b8n\u22121)\nLet J(\u03c1,\u0398) denote the Jacobian matrix of these transformation functions. We want to represent our stochastic process in this new coordinate system, and to do so we need to convert the Ito\u0302 equation, given by (9), into a Stratonovich equation in the form given by (13). We have two equalities:\ndX(t) = \u03c3ik(x)  \u25e6 dB1(t)... dBn(t)  + b\u03031(x)... b\u0303n(x)  dt(23)\ndX(t) = J(\u03c1,\u0398)  \u25e6  dX\u03c1(t) dX\u03b81(t)\n... dX\u03b8n\u22121(t) (24) The first equality is justified by Lemma 2. The second equality is justified by the Stratonovich formula for the \u201cchain rule,\u201d given by (11). The notation dX\u03c1(t), dX\u03b81(t), . . . , dX\u03b8n\u22121(t), in the second equation, expresses the fact that X\u03c1(t), X\u03b81(t), . . . , and X\u03b8n\u22121(t) are intended to represent the components of a new stochastic process defined on (\u03c1,\u0398).\nWe can now combine and solve equations (23) and (24) to obtain: dX\u03c1(t) dX\u03b81(t)\n... dX\u03b8n\u22121(t)\n = J(\u03c1,\u0398) \u22121\u03c3ik(x(\u03c1,\u0398))  \u25e6 dB1(t)... dBn(t)  + J(\u03c1,\u0398)\n\u22121 b\u03031(x(\u03c1,\u0398))... b\u0303n(x(\u03c1,\u0398))  dt We thus have a representation of our original stochastic process, in Stratonovich form, but expressed entirely in the new (\u03c1,\u0398) coordinate\nsystem. Note that the second term in this solution is just the transformation law for a contravariant vector, or a type (1, 0) tensor.\nNow consider the decomposition of a Stratonovich stochastic differential equation as in (15): dX\u03c1(t) dX\u03b81(t)\n... dX\u03b8n\u22121(t)  = (A1| . . . |An) \u25e6 dB(t) + A0 dt By matching the components of this equation with the components of the preceding equation, we can determine the vector fields A0\u2202 and A1\u2202, . . . ,An\u2202. Then, applying Theorem 3 and expanding the expression inside (17), we can compute a new infinitesimal generator, L, for our stochastic process, expressed again entirely in the (\u03c1,\u0398) coordinate system. Finally, whatever our result might be, it can be written in the following form:\n(25) L = 1 2 n\u22121\u2211 i,j=0 \u03b1ij(\u03c1,\u0398) \u22022 \u2202yi\u2202yj + n\u22121\u2211 i=0 \u03b2i(\u03c1,\u0398) \u2202 \u2202yi\nwhere y0 = \u03c1 and yi = \u03b8i, for i = 1, . . . , n \u2212 1. (To distinguish this equation for L from the L we started out with, we have written the coefficients of the differential operators as \u03b1ij(\u03c1,\u0398) and \u03b2i(\u03c1,\u0398) instead of aij(x) and bi(x).) Note that this is the infinitesimal generator of an Ito\u0302 process, but we derived it by an excursion through Stratonovich!\nBefore proceeding further, we need to analyze the (\u03c1,\u0398) coordinate system. How is it defined? What are its properties? First, we want the radial coordinate, \u03c1, to follow the drift vector, \u2207U(x). We have already seen how to do this. Suppose \u03c1\u0302(t) is the solution to the following differential equation, based on (20):\n\u03c1\u0302\u2032(t) = \u2207U(\u03c1\u0302(t)) \u03c1\u0302(0) = x0\nIn other words, \u03c1\u0302(t) is the integral curve of the vector field \u2207U(x) starting at x0. This is almost the construction that we want for our radial coordinate, but not quite. We will actually work with a generalization of the concept of an integral curve, known as an integral manifold. A one-dimensional integral manifold is, roughly speaking, just the image of an integral curve without the parametrization, and it always exists, for any vector field. Since we want to be able to alter the parametrization of \u03c1\u0302(t), arbitrarily, in order to choose a suitable\ncoordinate, \u03c1, the one-dimensional integral manifold is the device that we need.\nFor the directional coordinates, \u03b81, \u03b82, . . . , \u03b8n\u22121, the obvious generalization would be an integral manifold of dimension n \u2212 1, orthogonal to the integral manifold for \u03c1. But, for k \u2265 2, a k-dimensional integral manifold exists if and only if certain conditions are satisfied, known as the Frobenius integrability conditions. Fortunately, as we will see, if we are looking for an integral manifold orthogonal to a vector field that is proportional to the gradient of a potential function, such as \u2207U(x), then the Theorem of Frobenius gives us the results that we want. Our analysis here is based on the standard literature in differential geometry. See, e.g., [Spi99], Chapter 6; [BG68], Chapter 3; [AM77], Chapter 8. We will discuss these results in Section 4.\nTo summarize: At this point, we have a one-dimensional integral manifold for the \u03c1 coordinate, and an orthogonal n \u2212 1 dimensional integral manifold for the \u0398 coordinates. But we want to construct a lower -dimensional subspace by projecting our stochastic process onto a k \u2212 1 dimensional subset of the coordinates \u03b81, \u03b82, . . . , \u03b8n\u22121. Taken together with the \u03c1 coordinate, we want this operation to give us an \u201coptimal\u201d k dimensional subspace. The mathematical device that we need is a Riemannian metric, gij(x), which we will use to measure dissimilarity on the integral manifolds. And crucially: the dissimilarity metric should depend on the probability measure. Roughly speaking, the dissimilarity should be small in a region in which the probability density is high, and large in a region in which the probability density is low. We can then take the following steps:\n\u2022 To find a principal axis for the \u03c1 coordinate, we minimize the Riemannian distance, gij(x), along the drift vector. \u2022 To choose the principal directions for the \u03b81, \u03b82, . . . , \u03b8k\u22121 coor-\ndinates, we diagonalize the Riemannian matrix, ( gij(x) ), and select the eigenvectors associated with the k\u2212 1 smallest eigenvalues. \u2022 To compute the coordinate curves, we follow the geodesics of\nthe Riemannian metric, gij(x), in each of the k \u2212 1 principal directions.\nThus, overall, we are minimizing dissimilarity, and maximizing probability. We will show how to do this, using concrete examples, in Sections 5.1 and 5.2 of this paper.\nIn the following section, we will see how to construct an integral manifold orthogonal to \u2207U , and how to define a dissimilarity metric, gij(x), with the desired properties. Because of the prominent role\nplayed by the Riemannian dissimilarity metric in our theory, it is natural to describe it as a theory of differential similarity.\n4. Integral Manifolds in R3.\nFrom this point on, for purposes of exposition, we will restrict our investigations from Rn to R3. We will see later (in Section 7) that this is not a limitation on the scope of the theory, since our results can easily be generalized again to Rn. Instead, the restriction to three dimensions simplifies our calculations, and makes them easier to visualize, as we will see in Section 5.\nSince we are now working in three-dimensional Euclidean space, we are primarily interested in two-dimensional integral manifolds. Is there a two-dimensional integral manifold orthogonal to the drift vector,\u2207U? Consider, first, a more general case. Suppose G = (P (x), Q(x), R(x)) represents the coordinates of a vector field that is defined but not equal to (0, 0, 0) in some open region D \u2286 R3.\nTheorem 5. There exists a two-dimensional integral manifold in D with tangent plane everywhere orthogonal to G if and only if\nG \u00b7 (\u2207\u00d7G) = 0\nProof. See [BC64], Problem 29, p. 23; [Car71], pp. 97\u201398; [LR75], pp. 155\u2013156.\nIntuitively, this theorem states that G must be orthogonal to its own \u201ccurl,\u201d a condition that is satisfied if G is proportional to the gradient of a scalar potential. Thus, any G in the form N(x)\u2207U(x) would work.\nWe still need a method to compute this integral manifold, however, and to define a curvilinear coordinate system on it. One approach is to choose basis vectors for a two-dimensional subspace of the tangent space at x in the following form:\nV\u2202 = f(x) \u2202\n\u2202x +\n\u2202\n\u2202y (26)\nW\u2202 = g(x) \u2202\n\u2202x +\n\u2202\n\u2202z\nNow compute: V\u00d7W = (f, 1, 0)\u00d7 (g, 0, 1) = (1,\u2212f,\u2212g). If V\u00d7W is proportional to G, then G is orthogonal to the plane containing both V and W, and conversely. So we can set:\nV \u00d7W = 1 P (x) G = 1 P (x) (P (x), Q(x), R(x))\n= (1,\u2212f(x),\u2212g(x))\nand obtain the results f(x) = \u2212Q(x)/P (x) and g(x) = \u2212R(x)/P (x). If G = \u2207U(x), then Theorem 5 applies. In this case, the vector fields given by P (x)V = (\u2212Q(x), P (x), 0) and P (x)W = (\u2212R(x), 0, P (x)) provide what we want, namely, a basis for the tangent plane to the twodimensional integral manifold that is everywhere orthogonal to the drift vector, \u2207U .\nThis construction can also be justified directly by the Theorem of Frobenius. Geometrically, we interpret V\u2202 and W\u2202 as the basis vectors for a tangent subbundle, E, in R3. (Historically, a tangent subbundle was called a \u201cdistribution,\u201d but this term does not have the right connotations today.) We compute the Lie bracket of V\u2202 and W\u2202 as follows:\n[V\u2202,W\u2202] = V\u2202 \u25e6W\u2202 \u2212W\u2202 \u25e6V\u2202(27)\n=\n[ \u2202g\n\u2202y \u2212 \u2202f \u2202z + f(x, y, z) \u2202g \u2202x \u2212 g(x, y, z)\u2202f \u2202x\n] \u2202\n\u2202x\nNow the geometric version of the Theorem of Frobenius asserts that, if [V\u2202,W\u2202] \u201cbelongs to\u201d E whenever V\u2202 and W\u2202 \u201cbelong to\u201d E, then E can be extended to a full integral manifold in R3. But if V\u2202 and W\u2202 are defined by the equations in (26) and also \u201cbelong to\u201d E, then [V\u2202,W\u2202] \u201cbelongs to\u201d E if and only if the bracketed expression on the right-hand side of (27) is identically zero. This leads to the following classical statement of the Frobenius integrability conditions as a system of partial differential equations:\n\u2202g \u2202y + f(x, y, z) \u2202g \u2202x = \u2202f \u2202z + g(x, y, z) \u2202f \u2202x\nAs a further check on Theorem 5, we can verify by a direct computation that the preceding equation holds for f(x) and g(x), as defined previously, when G = \u2207U(x).\nTo simplify the notation, let us absorb the factor P (x) into the definition of the two tangential vector fields, and write:\n\u2207U(x) = (P (x), Q(x), R(x)) V(x) = (\u2212Q(x), P (x), 0) W(x) = (\u2212R(x), 0, P (x))\nIn this form, it is easy to see that\u2207U(x) is orthogonal to both V(x) and W(x). Note also that V(x) and W(x) are not orthogonal to each other, although the vector fields V\u2202 = V(x)/P (x) and W\u2202 = W(x)/P (x) commute, as we have seen, when viewed as differential operators. Now one way to use these tangential vector fields is to compute a global (\u03c1, \u03b8, \u03c6) coordinate system. For example, we can compute the integral\ncurves of the vector field V(x) and use these for a coordinate called \u03b8, and we can compute the integral curves of the vector field W(x) and use these for a coordinate called \u03c6. Note that the \u03b8 coordinate curves will all lie in the global xy plane, and the \u03c6 coordinate curves will all lie in the global xz plane, if we take this approach.\nBut another approach is to use these vector fields to construct a local coordinate system. Any linear combination of V(x) and W(x) could be taken as one of the basis vectors for the tangent subbundle, and we can vary this linear combination as we move around the integral manifold. To implement this idea, it is useful to define a Riemannian metric on the integral manifold. The most natural way to do this is to define a metric tensor on all of R3, using the inner products of \u2207U(x), V(x) and W(x), in that order. We thus define:\uf8eb\uf8edgij(x)\n =  P 2(x) +Q2(x) +R2(x) 0 00 P 2(x) +Q2(x) Q(x)R(x)\n0 Q(x)R(x) P 2(x) +R2(x)  To remain consistent with the coordinate notation introduced in Section 3, we let i and j range over 0, 1, 2, and we stipulate that y0 = \u03c1, y1 = \u03b8, y2 = \u03c6. Since P (x), Q(x), R(x), are the components of the drift vector, \u2207U(x), and since the diffusion equation in which \u2207U(x) appears has an invariant probability density that is determined by the exponential of the potential function, U(x), it should be clear that gij(x) has at least some of the properties that we have been looking for. We thus adopt this formula, provisionally, as the definition of our dissimilarity metric.\nThe matrix (gij(x)) is not diagonal, in general, but it can easily be diagonalized. The eigenvectors are:\n\u03be1 =  0\u2212R(x) Q(x)  , \u03be2 =  0Q(x) R(x)  , \u03be3 =  10 0  , and the corresponding eigenvalues are: \u03bb1(x) = P\n2(x) and \u03bb2(x) = \u03bb3(x) = P\n2(x) + Q2(x) + R2(x). This analysis leads to a spectral decomposition of (gij(x)) as:\n20 L. THORNE MCCARTY\n\u03bb1(x)\u03ba(x)  0 0 00 R2(x) \u2212Q(x)R(x) 0 \u2212Q(x)R(x) Q2(x)  +\n\u03bb2(x)\u03ba(x)  0 0 00 Q2(x) Q(x)R(x) 0 Q(x)R(x) R2(x)  + \u03bb2(x)  1 0 00 0 0 0 0 0  where \u03ba(x) = 1/(Q2(x) + R2(x)). Obviously, the third term in this expression corresponds to the \u03c1 coordinate. Since \u03bb1(x) is always the smallest eigenvalue, we will use the eigenvector \u03be1 to determine the initial direction of the \u03b8 coordinate on the surface of the integral manifold orthogonal to \u2207U(x). It is obvious, too, that \u03be1 and \u03be2 are orthogonal to each other, even though V(x) and W(x) are not.\nThe main application of our dissimilarity metric, however, is to compute geodesics on the surface of the integral manifold orthogonal to \u2207U(x). Recall that any linear combination of V(x) and W(x) yields a vector in the tangent subbundle, E, and thus we can construct vector fields in E in the form v(t)V(x) + w(t)W(x) for arbitrary functions v(t) and w(t). For a geodesic, we are looking for a curve \u03b3(t) with values in R3 which minimizes the \u201cenergy\u201d functional:\n(28) 1\n2 \u222b T 0 ( v(t) w(t) )( g11(\u03b3(t)) g12(\u03b3(t)) g21(\u03b3(t)) g22(\u03b3(t)) )( v(t) w(t) ) dt\nsubject to the constraint:\n(29) \u03b3 \u2032(t) = v(t)V(\u03b3(t)) + w(t)W(\u03b3(t))\nThis variational problem leads to a system of Euler-Lagrange equations for the curves \u03b3(t) = (x(t), y(t), z(t)) and (v(t), w(t)), plus three Lagrange multipliers. For initial conditions, we specify (x(0), y(0), z(0)) and we use the smallest eigenvalue of gij(x(0), y(0), z(0)) to determine the initial value (v(0), w(0)). This is a complicated system of equations, but it can be solved numerically in Mathematica.\nThe preceding analysis was based on a global coordinate system centered on the x axis, since our initial vector fields were determined by the equations P (x)V = (\u2212Q(x), P (x), 0) and P (x)W = (\u2212R(x), 0, P (x)). But we could also work with a coordinate system centered on the y axis, using the equations Q(x)V = (0,\u2212R(x), Q(x)) and Q(x)W = (Q(x),\u2212P (x), 0), or the z axis, using R(x)V = (R(x), 0,\u2212P (x)) and R(x)W = (0, R(x),\u2212Q(x)). In fact, it is useful to be able to switch from one such coordinate system to another, as we move around the\nintegral manifold. Since g00(x) = P 2(x)+Q2(x)+R2(x) = |\u2207U(x)|2, it is obvious that the \u03c1 coordinate is independent of the global coordinate system used to define it. But the same is true of gij(x) when i 6= 0 and j 6= 0. To see this, let y1 and y2 denote the \u0398 coordinates centered on the x axis, and let y\u03041 and y\u03042 denote the \u0398 coordinates centered on the y axis. The Jacobian matrix of the coordinate transformation from y\u0304k to yi can be computed as follows: \u2202yi/\u2202y\u0304k \n=  \u2202x/\u2202\u03c1 \u2212Q(x) \u2212R(x)\u2202y/\u2202\u03c1 P (x) 0 \u2202z/\u2202\u03c1 0 P (x) \u22121 \u2202x/\u2202\u03c1 0 Q(x)\u2202y/\u2202\u03c1 \u2212R(x) \u2212P (x) \u2202z/\u2202\u03c1 Q(x) 0  =\n 1 0 00 \u2212R(x)/P (x) \u22121 0 Q(x)/P (x) 0  Now let gij(x) and g\u0304kl(x) denote the dissimilarity metric based on the yi and y\u0304k coordinates, respectively. Restricting our attention to the 2\u00d7 2 matrix for the \u0398 coordinates, we compute:(\ng\u030411(x) g\u030412(x) g\u030421(x) g\u030422(x) ) = ( Q2(x) +R2(x) P (x)R(x) P (x)R(x) P 2(x) +Q2(x) )\n= ( \u2212R(x)/P (x) \u22121 Q(x)/P (x) 0 )T ( g11(x) g12(x) g21(x) g22(x) )( \u2212R(x)/P (x) \u22121 Q(x)/P (x) 0 ) But this is just an instantiation of the transformation law for a type (0,2) tensor:\ng\u0304kl(x) = 2\u2211\ni,j=1\n\u2202yi \u2202y\u0304k gij(x) \u2202yj \u2202y\u0304l\nThe same calculations obviously lead to the same results for all pairwise transformations among the three global coordinate systems. Thus, on a two-dimensional integral manifold, for a fixed \u03c1, the dissimilarity metric, gij(x), is independent of the global coordinate system used to define it.\n5. Experiments with Mathematica.\nTo sharpen our intuitions, and before developing the theory of differential similarity any further, let\u2019s look at some experiments in R3 using the computational and graphical facilities of Mathematica. Section 5.1 is a comprehensive study of the Gaussian case, which is the one example that can be solved analytically. Section 5.2 then considers what we will refer to as the \u201ccurvilinear Gaussian\u201d case. Here, we apply a quadratic potential function to the output of a cubic polynomial coordinate transformation, producing an example that cannot be solved analytically, but which still retains some degree of tractability. Finally, in Section 5.3, we put two \u201ccurvilinear Gaussians\u201d together in a mixture distribution.\nThe source code for these examples is available in three Mathematica notebooks:\nGaussian.nb CurvilinearGaussian.nb BimodalCurvilinearGaussian.nb\n5.1. The Gaussian Case. Consider, first, the case of a quadratic potential, for which most results can be obtained analytically in closed form. Define U(x) as follows:\nU(x, y, z) = \u22121 2 (ax2 + by2 + cz2)\nThen the gradient is: \u2207U(x, y, z) = (\u2212ax,\u2212by,\u2212cz), and the derived potential V (x) is:\nV (x, y, z) = 1 2 (a2x2 + b2y2 + c2z2)\u2212 1 2 (a+ b+ c)\n(We can ignore the constant term.) It is well known that the FeynmanKac formula, given by either (4) or (7), has a closed-form solution whenever U(x) and V (x) are quadratic polynomials. Furthermore, the invariant probability measure, eU(x), given by Theorem 1, is obviously a Gaussian. Adding in the normalization factor, the invariant probability density function is: (30) \u221a abc (2\u03c0)\u2212 3 2 exp [ \u22121\n2 (ax2 + by2 + cz2) ] Note that the covariance matrix in (30) is already in diagonalized form.\nFor a numerical example, set a = 1, b = 2, c = 4. Figure 1 then shows the surface defined by the equation U(x, y, z) = \u22122. Figure 2 shows a StreamPlot of the gradient vector field generated by\u2207U(x, y, z) at z = 0. This picture makes sense, intuitively. Notice that the drift vector is\n\u201ctransporting probability mass towards the origin,\u201d to counteract the dissipative effects of the diffusion term in the stochastic process. If the system is in perfect balance, of course, we have an invariant probability measure, which in this case is a Gaussian.\nThe Gaussian case is simple enough that we can solve the differential equations explicitly in Mathematica, using DSolve. First, the integral curve of the vector field \u2207U(x) = (P (x), Q(x), R(x)) starting at x0 = (x0, y0, z0) is given by:\n\u03c1\u0302(t) =  x0 e\u2212aty0 e\u2212bt z0 e \u2212ct  For the tangential vector fields, we will start with a global coordinate system centered on the x axis, so that V(x) = (\u2212Q(x), P (x), 0) = (by,\u2212ax, 0) and W(x) = (\u2212R(x), 0, P (x)) = (cz, 0,\u2212ax). Then the integral curve of the vector field V\u2202 starting at x1 = (x1, y1, z1) is given by:\n\u03b8\u0302(t) =  x1 cos\u221aab t + y1 \u221ab/a sin\u221aab ty1 cos\u221aab t \u2212 x1 \u221aa/b sin\u221aab t z1  and the integral curve of W\u2202 starting at x2 = (x2, y2, z2) is given by:\n\u03c6\u0302(t) =  x2 cos\u221aac t + z2 \u221ac/a sin\u221aac ty2 z2 cos \u221a ac t \u2212 x2 \u221a a/c sin \u221a ac t  Figure 3 shows the global coordinate system on a two-dimensional integral manifold that would be generated by these curves. Note that the \u03b8 coordinate curves lie in the xy plane, and the \u03c6 coordinate curves lie in the xz plane, as expected.\nGiven the curves \u03c1\u0302(t), \u03b8\u0302(t) and \u03c6\u0302(t), what does it mean to say that a point in R3 has the coordinates (\u03c1, \u03b8, \u03c6)? We adopt the following conventions: Starting with the x axis as the principal axis, choose a \u201cmaximal\u201d point x0 = (x0, 0, 0) and follow the curve \u03c1\u0302(t) towards the origin. There are two natural measures of distance along this curve: the Euclidean arc length, which in this case is just the value of the xcoordinate, and the Riemannian arc length, which is determined by our dissimilarity metric, gij(x). Our choice here is to use the Euclidean arc length to specify the \u03c1 coordinate. (We will subsequently see another role for the Riemannian arc length.) In the Gaussian case, therefore, \u03c1 has the value x0 e\n\u2212at, which ranges over the interval (0, x0] as t ranges from \u221e to 0. But by choosing a value for \u03c1, we are also choosing the\nintegral manifold on which \u03b8\u0302(t) and \u03c6\u0302(t) are defined. Therefore, to interpret the coordinates \u03b8 and \u03c6, starting at (\u03c1, 0, 0), we traverse the\ndistance \u03b8 along the \u03b8\u0302(t) curve from the slice \u03b8 = 0, and we traverse\nthe distance \u03c6 along the \u03c6\u0302(t) curve from the slice \u03c6 = 0, until we arrive\nat the point (\u03c1, \u03b8, \u03c6). Note, too, that we can traverse the \u03b8\u0302(t) and\n\u03c6\u0302(t) curves in either order, and still arrive at the same point, as long as we remain within a neighborhood of (\u03c1, 0, 0) in which these curves intersect. The black dots in Figure 3 may be helpful in visualizing this procedure.\nOnce again, the Gaussian case is simple enough that we can analyze the coordinate transformation from (\u03c1, \u03b8, \u03c6) to (x, y, z), and derive an explicit expression for its Jacobian matrix. First, let ~\u03b8s(x) = \u03b8\u0302x(s) denote the flow of the vector field V\u2202 starting at x, and similarly let ~\u03c6t(x) = \u03c6\u0302x(t) denote the flow of the vector field W\u2202 starting at x. Applying the composition, ~\u03b8s \u25e6 ~\u03c6t, of the flows ~\u03b8s and ~\u03c6t to the point x = (\u03c1, 0, 0), we obtain the following equations, for arbitrary s and t:\nx = ~x(\u03c1, s, t) = \u03c1 cos \u221a ab s cos \u221a ac t(31)\ny = ~y(\u03c1, s, t) = \u2212 \u03c1 \u221a a/b sin \u221a ab s cos \u221a ac t\nz = ~z(\u03c1, s, t) = \u2212 \u03c1 \u221a a/c sin \u221a ac t\nBy a simple calculation:\n\u2202\n\u2202s  ~x(\u03c1, s, t)~y(\u03c1, s, t) ~z(\u03c1, s, t)  =  b ~y(\u03c1, s, t)\u2212a ~x(\u03c1, s, t) 0  In other words, \u2202/\u2202s = V\u2202. By another simple calculation, setting s = 0 in (31), we have:\n\u2202\n\u2202t  ~x(\u03c1, 0, t)~y(\u03c1, 0, t) ~z(\u03c1, 0, t)  =  \u2212\u03c1 \u221aac sin\u221aac t0 \u2212a \u03c1 cos \u221a ac t  =  c ~z(\u03c1, 0, t)0 \u2212a ~x(\u03c1, 0, t)  In other words, \u2202/\u2202t = W\u2202 at s = 0. But we can now show that, if we move along the V\u2202 coordinate curves from the slice s = 0 at (\u03c1, 0, t) to a new slice, where s 6= 0, the components of W\u2202 do not change. This result follows from the commutative property of the coordinate vector fields: V\u2202 \u25e6W\u2202 = W\u2202 \u25e6V\u2202. See, e.g., [BG68], Theorem 3.7.1, and [Spi99], Lemma 5.13. We can obtain a similar result if we reverse the composition of the flows ~\u03b8s and ~\u03c6t, and apply ~\u03c6t \u25e6 ~\u03b8s to the point x = (\u03c1, 0, 0). In this case, we can compute \u2202/\u2202t = W\u2202 for all s and t, and \u2202/\u2202s = V\u2202 for t = 0. Again, if we move along the W\u2202 coordinate\ncurves from the slice t = 0 at (\u03c1, s, 0) to a new slice, where t 6= 0, the components of V\u2202 do not change.\nNow consider the coordinate transformation itself. Start anywhere on the slice s = 0 and follow the flow ~\u03b8s to s = \u03b8. Separately, start anywhere on the slice t = 0 and follow the flow ~\u03c6t to t = \u03c6. If we are within a sufficiently small neighborhood of (\u03c1, 0, 0), these curves will intersect at some point (x, y, z) on the integral manifold for \u03c1. Thus, applying the inverse function theorem, we can recover the coordinate values (\u03c1, \u03b8, \u03c6) = (\u03c1(x, y, z), \u03b8(x, y, z), \u03c6(x, y, z)). See, e.g., [BG68], Theorem 3.7.1, or [BC64], Theorem 1.5. Finally, even though we may not be able to write down an explicit formula for the coordinate transformation, in general, we can see from the equations above that the Jacobian matrix of (x, y, z) = (x(\u03c1, \u03b8, \u03c6), y(\u03c1, \u03b8, \u03c6), z(\u03c1, \u03b8, \u03c6)) can be written explicitly as:\n(32) J(\u03c1, \u03b8, \u03c6) =  x(\u03c1, \u03b8, \u03c6)/\u03c1 b y(\u03c1, \u03b8, \u03c6) c z(\u03c1, \u03b8, \u03c6)y(\u03c1, \u03b8, \u03c6)/\u03c1 \u2212a x(\u03c1, \u03b8, \u03c6) 0 z(\u03c1, \u03b8, \u03c6)/\u03c1 0 \u2212a x(\u03c1, \u03b8, \u03c6)  This is all we need to carry out the calculations described in Section 3, including the calculation of the coefficients \u03b1ij(\u03c1, \u03b8, \u03c6) and \u03b2i(\u03c1, \u03b8, \u03c6) in Equation (25). We will analyze these results further in Section 6.\nWe have referred to the x axis in Figure 3 as the \u201cprincipal axis\u201d because of its correspondence to the results of Principal Component Analysis (PCA) in traditional linear statistics. For the Gaussian probability density given by (30), with a = 1, b = 2, c = 4, the first component identified by PCA would be the x axis, and the second component would be the y axis. Thus the \u201cprincipal surface\u201d would be defined by the xy plane, which corresponds to the (\u03c1, \u03b8) surface in our curvilinear coordinate system. Figure 4 depicts this surface, with the \u03c1 and \u03b8 coordinates illustrated. The maximal point on the principal axis is (10, 0, 0), and the \u03b8 coordinate curves have been evenly spaced along the \u03c1 coordinate curve from (10, 0, 0) to (0, 0, 0). Similarly, the \u03c1 coordinate curves have been evenly spaced along the maximal \u03b8 coordinate curve, which passes through the point (10, 0, 0). Clearly, the (\u03c1, \u03b8) surface in Figure 4 encodes the same information as the xy plane does in traditional linear statistics.\nWhat if our potential function, U(x), while still a quadratic, was not already in diagonalized form? Figure 5 shows an example in which the potential function depicted in Figure 1 has been rotated through the angle \u03c0/3 around the line from (0, 0, 0) to (1, 1, 1). Under this rotation, the maximal point on the principal axis, (10, 0, 0), would be displaced to the position (20/3,\u221210/3, 20/3). If our basis vectors, V(x)\nand W(x), were also rotated in the same way, we could still compute closed form solutions to the differential equations, using DSolve, and this procedure would still give us explicit expressions for the functions \u03c1\u0302(t), \u03b8\u0302(t) and \u03c6\u0302(t), although these expressions would be more complex than they were previously. Continuing in this way, as before, we would eventually produce the (\u03c1, \u03b8) surface shown in Figure 6.\nBut this calculation would only be possible if we knew, a priori, what the rotation was. Suppose we had no information about the rotation. And, to make the task even more challenging, suppose we were not able to rely on symbolic methods in Mathematica, such as DSolve, but had to use numerical methods, such as NDSolve. Could we still compute the (\u03c1, \u03b8) surface shown in Figure 6?\nWe need to address a preliminary issue: When we were working with DSolve in the simple Gaussian case, we were able to compute an explicit expression for \u03c1\u0302(t) and convert it into a formula for the \u03c1 coordinate measured in Euclidean arc length. Basically, we were constructing a new parametrization of \u03c1\u0302(t). This is not easy to do in the general case, however, because it would require us to invert the general formula for arc length. Fortunately, there is a simpler approach, which works very well using NDSolve. In place of the differential equation derived from (20), we use the normalized version:\n\u03b3\u2032(t) = \u2207U(\u03b3(t)) \u2016\u2207U(\u03b3(t))\u2016\n\u03b3(0) = x0\nSince our tangent vector now has length 1, the integral curve that solves this equation will be parametrized by Euclidean arc length, but otherwise it will be identical to \u03c1\u0302(t). The formula for Riemannian arc length, using our dissimilarity metric, gij(x), is also very simple when \u03b3(t) is defined in this way:\n\u222b T 0 \u221a\u221a\u221a\u221a\u221a( 1 0 0 )  gij(\u03b3(t))  10 0  dt = \u222b T 0 \u221a g00(\u03b3(t)) dt\nNote that a similar normalized differential equation could be used with the vector fields V(x) and W(x), if we wanted to compute integral\ncurves \u03b8\u0302(t) and \u03c6\u0302(t) parametrized by Euclidean arc length, although the formula for the Riemannian arc length would be different.\nWe are now ready to compute the (\u03c1, \u03b8) surface in Figure 6, without knowledge of the rotation, and without the use of analytical methods in\nMathematica. We will start off with the basis vectors V(x) and W(x) centered on the x axis. There are three steps:\n(1) Find a principal axis for the \u03c1 coordinate.\nThe basic idea is to find a point (x0, y0, z0) at a fixed Euclidean distance from the origin, and an integral curve \u03b3(t) which solves the normalized differential equation for \u2207U(x) starting at x = (x0, y0, z0), and for which the Riemannian distance, gij(x), measured along \u03b3(t) for a fixed interval, t, is minimal. In short, we are looking for the least Riemannian distance for a fixed Euclidean distance.\nWe use NDSolve to compute \u03b3(t), and we use NIntegrate to compute the Riemannian distance along \u03b3(t). FindMinimum then searches for the minimal point (x0, y0, z0) satisfying these constraints. In our rotated Gaussian example, we can start the search at (10, 0, 0) with the constraint that (x0, y0, z0) must lie on the sphere x2 + y2 + z2 = 100, and FindMinimum will return the value (x0, y0, z0) = (6.66666,\u22123.33335, 6.66667). This is a reasonably good match with the analytical value, (x0, y0, z0) = (20/3,\u221210/3, 20/3).\n(2) Determine the initial directions of the \u03b8 coordinate.\nThe basic idea is to work with the eigenvector, \u03be1(x), associated with the smallest eigenvalue, \u03bb1(x), of the dissimilarity matrix, ( gij(x) ) , at the point x = (x0, y0, z0). For expository purposes, let\u2019s initially use the analytical value (x0, y0, z0) = (20/3,\u221210/3, 20/3). Then the smallest eigenvalue is 400/9, and its associated eigenvector is (0, 2, 1). However, the inner product of the basis vectors, V(x) and W(x), at (x0, y0, z0) is negative, which means that we need to flip the sign of, say, W(x), to determine the initial directions of the \u03b8 coordinate. We thus set (v(0), w(0)) = (2,\u22121), for one direction, and (v(0), w(0)) = (\u22122, 1), for the opposite direction.\nThere is only a minor difference if we use the numerical value (x0, y0, z0) = (6.66666,\u22123.33335, 6.66667). We then set (v(0), w(0)) = (1.99996,\u22121), for one direction, and, for the opposite direction, (v(0), w(0)) = (\u22121.99996, 1).\n(3) Compute the geodesic curves of the (\u03c1, \u03b8) coordinate system.\nIn the final step, we compute the geodesic curves that solve the variational problem given by (28) and (29), with the initial value (x(0), y(0), z(0)) = (6.66666,\u22123.33335, 6.66667) and\nwith (v(0), w(0)) equal to either (1.99996,\u22121) or (\u22121.99996, 1). Mathematica has a VariationalMethods package which computes the Euler-Lagrange equations symbolically from the specification of a variational problem. We use this package, and then solve the resulting equations numerically with NDSolve.\nThere is one complication: NDSolve encounters a singularity when the value of P (x) gets very small and the values of v(t) and w(t) grow very large. We will examine a solution to this problem in Section 5.2, below.\nThe end result of these three steps is a radial coordinate curve, \u03c1, and an outer transverse coordinate curve, \u03b8, which match perfectly the curves in Figure 6, within the expected tolerance of a numerical approximation and up to the location of the numerical singularities.\nThis match between the numerical results and the analytical results provides some evidence that our techniques are working correctly. We will now apply these techniques to an example for which analytical results are not available.\n5.2. The Curvilinear Gaussian. For this example, we start with a cubic polynomial: C(t) = t3\u2212t2\u2212t. We then define a cubic polynomial coordinate transformation from (x, y, z) to (u, v, w) as follows:\nu = u(x, y, z) = C(1.4 y) + 2x(y2 + z2)\nv = v(x, y, z) = C(1.2 z) + 2y(z2 + x2)\nw = w(x, y, z) = C(1.0 x) + 2z(x2 + y2)\nFinally, we define U(x) as a quadratic potential function in the variables u, v and w:\nU(x, y, z) = \u22121 2 (a u(x, y, z)2 + b v(x, y, z)2 + cw(x, y, z)2) \u2217 10\u22126\nThus U(x) is a sixth-degree polynomial in x, y and z, and the gradient, \u2207U(x), is a fifth-degree polynomial. There are no known closed-form solutions to the Feynman-Kac formula, given by either (4) or (7), when U(x) and V (x) are higher-order polynomials. However, it is possible to discretize the Feynman-Kac \u201cpath integral,\u201d and obtain approximate numerical solutions. See, for example, [Lya04].\nFor a numerical example, set a = 1, b = 2, c = 4. Figure 7 then shows the surface defined by the equation U(x, y, z) = \u221210. Figure 8 shows a StreamPlot of the gradient vector field generated by \u2207U(x, y, z) at z = \u221210. Figure 9 shows a stack of such stream plots, at the values z = 10, z = 0 and z = \u221210. Notice how the drift vector twists and\nturns to counteract the dissipative effects of the diffusion term, and maintain an invariant probability measure.\nFigure 10 is analogous to Figure 3 in the Gaussian case, and depicts the integral manifold that passes through the point (20, 0,\u221210). The coordinate curves in Figure 10 are generated by a global coordinate system centered on the x axis, with P (x)V = (\u2212Q(x), P (x), 0) and P (x)W = (\u2212R(x), 0, P (x)). These curves are thus analogous to the global \u03b8 and \u03c6 coordinate curves shown in Figure 3.\nFinally, Figure 11 shows the (\u03c1, \u03b8) surface computed by our numerical techniques, and analogous to the (\u03c1, \u03b8) surface in Figure 6. As before, we start off with the basis vectors V(x) and W(x) centered on the x axis, and we proceed through three steps:\n(1) Find a principal axis for the \u03c1 coordinate.\nAgain, we use NDSolve, NIntegrate and FindMinimum. We start the search at (20, 0,\u221210), and impose the constraint that (x0, y0, z0) must lie on the sphere x\n2 +y2 +z2 = 500. We obtain the result: (x0, y0, z0) = (20.4316, 1.27953,\u22128.99505).\n(2) Determine the initial directions of the \u03b8 coordinate.\nWorking with the eigenvector \u03be1(x) at the point x = (20.4316, 1.27953,\u22128.99505), we set (v(0), w(0)) = (7.03133,\u22121) and, alternatively, (v(0), w(0)) = (\u22127.03133, 1). The first pair of values will give us the \u03b8 coordinate curve in the clockwise direction, as seen from the vantage point of Figure 11; the second pair of values will give us the \u03b8 coordinate curve in the counterclockwise direction.\n(3) Compute the geodesic curves of the (\u03c1, \u03b8) coordinate system.\nUsing the initial values calculated in steps (1) and (2), we construct the Euler-Lagrange equations for the variational problem given by (28) and (29), and we solve them using NDSolve. We encounter singularities in both coordinate curves, however, and we cannot extend the geodesics into the negative x and positive z quadrant.\nThese three steps generate the principal \u03c1 coordinate curve and the outer \u03b8 coordinate curves shown in Figure 11, at least in the positive x and the negative z quadrant. The remaining \u03b8 coordinate curves in this quadrant are evenly spaced along the \u03c1 coordinate, and the remaining \u03c1 coordinate curves are evenly spaced along the outer \u03b8 coordinate, and they have been generated by the same three steps.\nTo complete Figure 11, we need another approach. One solution is to draw a second set of coordinate curves starting from the antipodal point in the opposite quadrant. Again, there are three steps:\n(1) Find a principal axis for the \u03c1 coordinate.\nTo find our initial starting point, (x0, y0, z0), we used NDSolve, NIntegrate and FindMinimum to search for the least Riemannian distance for a fixed Euclidean distance. In fact, the Riemannian distance from (20.4316, 1.27953,\u22128.99505) to the origin is 6.30863, but this distance is the same from any point on the integral manifold. For example, the Riemannian distance to the origin along the \u03c1 coordinate curves at 315\u25e6, 45\u25e6, and 90\u25e6, is also 6.30863, although the Euclidean distance is different in each case. Thus, to find the antipodal point in the negative x and positive z quadrant, we search along the integral manifold at a constant Riemannian distance to find the greatest Euclidean distance. FindMaximum gives us the result that we want: (x1, y1, z1) = (\u221219.2034,\u22121.25676, 9.25639).\n(2) Determine the initial directions of the \u03b8 coordinate.\nWorking with the eigenvector \u03be1(x) at the point (x1, y1, z1), we set (v(0), w(0)) = (7.36377,\u22121) for the \u03b8 coordinate curve in the clockwise direction, and (v(0), w(0)) = (\u22127.36377, 1) for the \u03b8 coordinate curve in the counterclockwise direction.\n(3) Compute the geodesic curves of the (\u03c1, \u03b8) coordinate system.\nUsing the initial values calculated in steps (1) and (2), we construct the Euler-Lagrange equations for the variational problem given by (28) and (29), and we solve them using NDSolve. Although we encounter singularities again in both cases, we can see from a close inspection of Figure 11 that there remains only a small gap to the location of the singularities along the \u03b8 coordinate curves that were previously constructed from (x0, y0, z0).\nThese three steps generate the principal \u03c1 coordinate curve and the outer \u03b8 coordinate curves shown in Figure 11, in the negative x and positive z quadrant. The remaining \u03b8 coordinate curves in this quadrant are evenly spaced along the \u03c1 coordinate, and the remaining \u03c1 coordinate curves are evenly spaced along the outer \u03b8 coordinate. The reader will note that the coordinate curves in both quadrants in Figure 11 match up reasonably well.\n5.3. The Bimodal Curvilinear Gaussian. Finally, we consider a bimodal case. Figure 12 shows two copies of the curvilinear Gaussian defined in Section 5.2. One copy has been translated from (0, 0, 0) to (20, 20,\u221210). The other copy has been translated from (0, 0, 0) to (\u221220,\u221220, 10) and rotated by \u03c0/2 around a line parallel to the y-axis. But the probability density is a mixture. If U1(x) is the potential function for the first copy and U2(x) is the potential function for the second copy, then the invariant probability density is given by:\neU(x) ' eU1(x) + eU2(x),\nmodulo an appropriate normalization factor. Figure 12 is actually showing the surface defined by the equation:\neU1(x,y,z) + eU2(x,y,z) = 0.0001\nThe advantage of this representation lies in the fact that our calculations for each copy will be almost independent of each other. Observe that the effective potential function for the mixture will be:\nU(x) ' log(eU1(x) + eU2(x))\nThus the gradient of U(x) in a neighborhood of (20, 20,\u221210) will be almost identical to the gradient of U1(x) computed by itself, and the gradient of U(x) in a neighborhood of (\u221220,\u221220, 10) will be almost identical to the gradient of U2(x) computed by itself.\nThe mixture distribution thus provides a useful representation of clusters. Analyzing the situation, intuitively, in terms of our dissimilarity metric, the two clusters in Figure 12 will be exponentially far apart."}, {"heading": "6. Diffusion Coefficients and Dissimilarity Metrics", "text": "Recall the main results from Section 3: We started with a diffusion process represented by an Ito stochastic differential equation, in Cartesian coordinates; we transformed this into a Stratonovich equation in the coordinates (\u03c1,\u0398); and we then converted this back into an Ito process characterized by a differential operator with coefficients \u03b1ij(\u03c1,\u0398) and \u03b2i(\u03c1,\u0398). The one necessary ingredient was the Jacobian matrix of the coordinate transformation.\nAs an illustration, let\u2019s try a brute force solution of these equations in the simple Gaussian case discussed in Section 5.1. The Jacobian is given by equation (32). For ease of reference, here is equation (23),\nrewritten for the three-dimensional coordinate system (\u03c1, \u03b8, \u03c6):\ndX(t) = \u03c3ik(x(\u03c1, \u03b8, \u03c6))  \u25e6 dB1(t)dB2(t) dB3(t)  + b\u03031(x(\u03c1, \u03b8, \u03c6))b\u03032(x(\u03c1, \u03b8, \u03c6)) b\u03033(x(\u03c1, \u03b8, \u03c6))  dt For the moment, we will assume that (\u03c3ik(x(\u03c1, \u03b8, \u03c6))) is an orthogonal transformation, but otherwise arbitrary. Our procedure is to combine and solve equations (23) and (24), and then expand the result using Theorem 3. When we do so, we discover that the \u201csum of squares\u201d inside equation (17) yields an expression consisting of 2679 terms! However, by using the fact that (\u03c3ik(x(\u03c1, \u03b8, \u03c6))) is an orthogonal transformation, we can eliminate all terms in which the factors \u03c3ik appear without derivatives. Furthermore, all the terms that include derivatives of \u03c3ik are cancelled out by similar terms in the expansion of A0\u2202 inside equation (17). The net result is equation (25), in the following form:\nL = 1 2 2\u2211 i,j=0 \u03b1ij(\u03c1, \u03b8, \u03c6) \u22022 \u2202yi\u2202yj + 2\u2211 i=0 \u03b2i(\u03c1, \u03b8, \u03c6) \u2202 \u2202yi\nwhere y0 = \u03c1, y1 = \u03b8 and y2 = \u03c6. Thus, the exact choice we make for the transformation (\u03c3ik(x(\u03c1, \u03b8, \u03c6))) turns out to be irrelevant. However, the diffusion coefficients \u03b1ij(\u03c1, \u03b8, \u03c6) and the drift coefficients \u03b2i(\u03c1, \u03b8, \u03c6) are still very complex, and they do not provide much insight into the structure of the solution, even in the simple Gaussian case.\nFor more insight, let\u2019s separate the \u03c1 coordinate from the \u0398 coordinates. The basic idea of the (\u03c1,\u0398) coordinate system was to align the \u03c1 coordinate with the drift vector, \u2207U(x), so that the trajectory of our stochastic process in the direction of the \u0398 coordinates would be orthogonal to the drift. The definition of our dissimilarity metric, ( gij(x) ), also exhibited a strong separation between the \u03c1 coordinate and the \u0398 coordinates. So there is a natural question here: What is the relationship between the representation of our stochastic process in \u0398 coordinates and the \u0398 submatrix of ( gij(x) )?\nThe answer is well known in the case of pure Brownian motion, without drift. The earliest example is in [Str71] and [Ito\u030275]. Stroock discovered that if you project Brownian motion in R3 onto the surface of a sphere of radius r centered at (0, 0, 0), the differential operator of the resulting stochastic process, in spherical coordinates, (r, \u03d1, \u03d5), is:\nL = 1 2 1 r2\n( \u22022\n\u2202\u03d12 +\n1\nsin2 \u03d1\n\u22022\n\u2202\u03d52 +\n1\ntan\u03d1\n\u2202\n\u2202\u03d1\n)\nwhich is the spherical Laplacian divided by 2. This result can be generalized to an arbitrary Riemannian manifold,M, embedded in Rn. For any f \u2208 C\u221e(M; R), the Laplace-Beltrami operator, \u2206M, is defined by:\n\u2206M f = divM (gradM f)\nin which the divergence, divM, and the gradient, gradM, can both be defined on M independently of a coordinate system.\nTheorem 6. Let M be an embedded submanifold of Rn, and let \u2206M be the Laplace-Beltrami operator on M. Then\nL = 1 2 \u2206M\nis the differential operator of a Brownian motion process in Rn that has been projected orthogonally onto M.\nProof. The proof starts by showing that L can always be written in Ho\u0308rmander form without the V0\u2202 term. In particular, we can write:\nL = 1 2 \u2206M = 1 2 n\u2211 k=1 (\u03a0M(ek\u2202) \u2202) 2\nwhere (e1, e2, . . . , en) is an orthonormal basis for R n and \u03a0M is the orthogonal projection operator from the tangent bundle in Rn onto the tangent bundle in M. See Section 4.2.1 of [Str00] or Theorem 3.1.4 in [Hsu02]. From this result, it follows that we can construct a diffusion process onM whose increments are precisely the projections, under \u03a0M, of the increments of a Brownian motion process in R\nn, and whose differential operator is L. For the details, see Theorem 4.37 in [Str00].\nThe diffusion process constructed in Theorem 6 is known as Brownian motion on M.\nLet us now analyze the stochastic process defined by equation (6), or (9), or (13), projected onto the \u03c1 and \u0398 coordinates separately. To simplify the calculations, we will initially focus our attention on the simple Gaussian case, in which \u2207U(x, y, z) = (\u2212ax,\u2212by,\u2212cz), and we will start with a construction borrowed from [Str71] and [Ito\u030275], but\nadapted to match this example. Consider the following matrix:\u03c0ij(x, y, z)  =  1 0 00 1 0 0 0 1  \u2212 1\n|\u2207U(x, y, z)|2  \u2212a x\u2212b y \u2212c z ( \u2212a x \u2212b y \u2212c z ) in which the product in the second line should be interpreted as the multiplication of a 3\u00d7 1 matrix times a 1\u00d7 3 matrix, yielding a 3\u00d7 3 matrix. It is easy to check that ( \u03c0ij(x)\n) is idempotent:\u03c0ik(x, y, z)  \u03c0kj (x, y, z)  = \u03c0ij(x, y, z) \nand that it maps the vector \u2207U(x, y, z) onto the origin:\u03c0ij(x, y, z)   \u2212ax\u2212by\n\u2212cz\n =  00\n0  Thus ( \u03c0ij(x) ) is a projection onto the plane tangent to the integral manifold at (x, y, z). We now apply this projection operator to the right-hand side of equation (23), as rewritten above. First, we set \u03c3 equal to the identity matrix, so that b\u0303 = b = \u2207U . (See the discussion following Lemma 2 in Section 2.2.) Then the projection operator ( \u03c0ij(x) ) annihilates the second term in (23), and we are left with:\ndX(t) = \u03c0ij(x(\u03c1, \u03b8, \u03c6))  \u25e6 dB1(t)dB2(t) dB3(t) (33) We now combine equation (33) with equation (24), and solve this system of equations to obtain:dX\u03c1(t)dX\u03b8(t)\ndX\u03c6(t)\n = J(\u03c1, \u03b8, \u03c6) \u22121\u03c0ij(x(\u03c1, \u03b8, \u03c6))  \u25e6 dB1(t)dB2(t) dB3(t) (34) As a verification that our calculations are on the right track, we note that the multiplication of the two matrices on the right-hand side of (34) produces a matrix in which the first row is identically zero. This means that dX\u03c1(t) = 0, which is exactly the result that we want.\nWe now continue the procedure outlined in Section 3, applying Theorem 3 to equation (34), and expanding the \u201csum of squares\u201d inside (17). This allows us to compute the coefficients \u03b1ij(\u03c1, \u03b8, \u03c6) and \u03b2i(\u03c1, \u03b8, \u03c6) in (25). It turns out that \u03b1ij(\u03c1, \u03b8, \u03c6) = 0 whenever i = 0 or j = 0, which is what we would expect. For the remaining diffusion coefficients, we compute:\n\u03b111(\u03c1, \u03b8, \u03c6) = a2 x(\u03c1, \u03b8, \u03c6)2 + c2 z(\u03c1, \u03b8, \u03c6)2\na2 x(\u03c1, \u03b8, \u03c6)2 |\u2207U |2\n\u03b122(\u03c1, \u03b8, \u03c6) = a2 x(\u03c1, \u03b8, \u03c6)2 + b2 y(\u03c1, \u03b8, \u03c6)2\na2 x(\u03c1, \u03b8, \u03c6)2 |\u2207U |2\n\u03b112(\u03c1, \u03b8, \u03c6) = \u03b121(\u03c1, \u03b8, \u03c6) = \u2212 b c y(\u03c1, \u03b8, \u03c6) z(\u03c1, \u03b8, \u03c6) a2 x(\u03c1, \u03b8, \u03c6)2 |\u2207U |2\nAlternatively, we can write the nonzero diffusion coefficients as a 2\u00d7 2 matrix:( \u03b1ij(\u03c1, \u03b8, \u03c6) ) = 1\na2 x(\u03c1, \u03b8, \u03c6)2 \u00d7\n(( 1 0 0 1 ) \u2212 1 |\u2207U |2 ( \u2212b y(\u03c1, \u03b8, \u03c6) \u2212c z(\u03c1, \u03b8, \u03c6) )( \u2212b y(\u03c1, \u03b8, \u03c6) \u2212c z(\u03c1, \u03b8, \u03c6) )) It turns out also that the the drift coefficient \u03b20(\u03c1, \u03b8, \u03c6) = 0, as we would expect, and for the other drift coefficients we compute:\n\u03b21(\u03c1, \u03b8, \u03c6) = b y(\u03c1, \u03b8, \u03c6)\n2 a x(\u03c1, \u03b8, \u03c6) |\u2207U |2 \u00d7(b+ c) + 1\n|\u2207U |2  a2 x(\u03c1, \u03b8, \u03c6)b2 y(\u03c1, \u03b8, \u03c6) c2 z(\u03c1, \u03b8, \u03c6)  \u00b7 \u2207U \n\u03b22(\u03c1, \u03b8, \u03c6) = c z(\u03c1, \u03b8, \u03c6)\n2 a x(\u03c1, \u03b8, \u03c6) |\u2207U |2 \u00d7(b+ c) + 1\n|\u2207U |2  a2 x(\u03c1, \u03b8, \u03c6)b2 y(\u03c1, \u03b8, \u03c6) c2 z(\u03c1, \u03b8, \u03c6)  \u00b7 \u2207U \nKeep in mind that these are the coefficients for the first-order terms \u2202/\u2202\u03b8 and \u2202/\u2202\u03c6.\nFor comparison, we will now compute the Laplace-Beltrami operator for the simple Gaussian case, using our Riemannian dissimilarity\nmetric, gij(\u03c1, \u03b8, \u03c6) = gij(x(\u03c1, \u03b8, \u03c6)), on the two-dimensional integral manifold given by the Theorem of Frobenius. In a local coordinate system, the Laplace-Beltrami operator is usually written as follows:\n\u2206M f = 1\u221a G n\u2211 j=1 \u2202 \u2202yj\n( \u221a G\nn\u2211 i=1 gij(y) \u2202f \u2202yi ) where G is the determinant of the matrix ( gij(y) ) and ( g\nij(y) ) is its inverse. Alternatively, we can expand the expression inside the parentheses, and write L in the form of equation (25):\nL = 1 2 \u2206M = 1 2 n\u2211 i,j=1 gij(y) \u22022 \u2202yi\u2202yj + n\u2211 i=1 hi(y) \u2202 \u2202yi ,\nwith hi(y) = 1\n2 \u221a G n\u2211 j=1\n\u2202 (\u221a Ggij(y) )\n\u2202yj\nWhen we do the calculations in the simple Gaussian case, with n = 2, we discover that the diffusion coefficients are identical:(\n\u03b1ij(\u03c1, \u03b8, \u03c6) ) = ( gij(\u03c1, \u03b8, \u03c6) ) and the drift coefficients are similar, but not identical:\nh1(\u03c1, \u03b8, \u03c6) = b y(\u03c1, \u03b8, \u03c6)\n2 a x(\u03c1, \u03b8, \u03c6) |\u2207U |2 \u00d7(a+ b+ c) + 1 |\u2207U |2  a2 x(\u03c1, \u03b8, \u03c6)b2 y(\u03c1, \u03b8, \u03c6) c2 z(\u03c1, \u03b8, \u03c6)  \u00b7 \u2207U \nh2(\u03c1, \u03b8, \u03c6) = c z(\u03c1, \u03b8, \u03c6)\n2 a x(\u03c1, \u03b8, \u03c6) |\u2207U |2 \u00d7(a+ b+ c) + 1 |\u2207U |2  a2 x(\u03c1, \u03b8, \u03c6)b2 y(\u03c1, \u03b8, \u03c6) c2 z(\u03c1, \u03b8, \u03c6)  \u00b7 \u2207U \nIn fact, there is a simple relationship between the coefficients \u03b2i(\u03c1, \u03b8, \u03c6) and hi(\u03c1, \u03b8, \u03c6):\n\u03b21(\u03c1, \u03b8, \u03c6)\u2212 h1(\u03c1, \u03b8, \u03c6) = \u2212 b y(\u03c1, \u03b8, \u03c6) 2x(\u03c1, \u03b8, \u03c6) |\u2207U |2 (35)\n\u03b22(\u03c1, \u03b8, \u03c6)\u2212 h2(\u03c1, \u03b8, \u03c6) = \u2212 c z(\u03c1, \u03b8, \u03c6) 2x(\u03c1, \u03b8, \u03c6) |\u2207U |2\nIs there an explanation for these results? The key is to recognize that the stochastic process defined by equation (6), or (9), or (13), is not Brownian motion. Brownian motion in Rn dissipates, and does not generate an invariant probability measure. Thus the projection of Brownian motion onto a Riemannian manifold, M, would dissipate as well. But the stochastic process defined by equation (6), when projected onto the manifold, M, would not dissipate, in general. This difference must be reflected in the drift coefficients for \u2202/\u2202\u03b8 and \u2202/\u2202\u03c6, as shown by equation (35).\nFigure 13 shows the \u201cdrift correction vector field\u201d generated by equation (35) on one quadrant of the integral manifold through (10, 0, 0), for the simple Gaussian case. The magnitude of the vector field is coded by color, with red indicating that the length of the vector is near zero. Keep in mind that we are looking at the difference between the two vector fields, ( \u03b2i(\u03c1, \u03b8, \u03c6) ) and (hi(\u03c1, \u03b8, \u03c6) ), as defined by equation (35). The vector fields themselves are oriented (approximately) in the opposite direction, but they have different magnitudes.\nWe have presented detailed calculations for the simple Gaussian case, so that our results would be easy to visualize. But the same calculations work for the general case, \u2207U(x) = (P (x), Q(x), R(x)). The projection operator, ( \u03c0ij(x(\u03c1, \u03b8, \u03c6)) ) , and the Jacobian matrix, ( J(\u03c1, \u03b8, \u03c6) ), can be defined in the same way, and the computational procedure from Section 3, applying Theorem 3 and expanding equation (17), still goes through. The expansion of the Laplace-Beltrami operator for the general dissimilarity metric, gij(\u03c1, \u03b8, \u03c6) = gij(x(\u03c1, \u03b8, \u03c6)), also goes through. We end up, again, with diffusion coefficients that are identical:\n( \u03b1ij(\u03c1, \u03b8, \u03c6) ) = ( gij(\u03c1, \u03b8, \u03c6) ) =\n1\nP 2( x(\u03c1, \u03b8, \u03c6) ) \u00d7\n(( 1 0 0 1 ) \u2212 1 |\u2207U |2 ( Q( x(\u03c1, \u03b8, \u03c6) ) R( x(\u03c1, \u03b8, \u03c6) ) )( Q( x(\u03c1, \u03b8, \u03c6) ) R( x(\u03c1, \u03b8, \u03c6) ) ))\nand drift coefficients that differ by a single, but more complex, term:\n\u03b21(\u03c1, \u03b8, \u03c6) \u2212 h1(\u03c1, \u03b8, \u03c6) =\n\u2212 1 2P 2 |\u2207U |2\n( P ( P \u2202Q\n\u2202x \u2212 Q \u2202P \u2202x\n) +R ( R \u2202Q\n\u2202x \u2212Q \u2202R \u2202x\n))\n\u03b22(\u03c1, \u03b8, \u03c6) \u2212 h2(\u03c1, \u03b8, \u03c6) =\n\u2212 1 2P 2 |\u2207U |2\n( P ( P \u2202R\n\u2202x \u2212 R \u2202P \u2202x\n) +Q ( Q \u2202R\n\u2202x \u2212R \u2202Q \u2202x\n))\nNote that \u2202P/\u2202x is the only partial derivative in these drift correction equations which is nonzero in the case \u2207U(x, y, z) = (\u2212ax,\u2212by,\u2212cz). Thus, for the simple Gaussian case, we can easily verify that the coefficients of the drift correction vector field reduce to the two terms:\n\u2212 b y(\u03c1, \u03b8, \u03c6) 2x(\u03c1, \u03b8, \u03c6) |\u2207U |2 and \u2212 c z(\u03c1, \u03b8, \u03c6) 2x(\u03c1, \u03b8, \u03c6) |\u2207U |2\nin agreement with equation (35)."}, {"heading": "7. Future Work", "text": "The theory of differential similarity combines a stochastic model with a geometric model, and it works because there is a common mathematical object in both models: the gradient, \u2207U(x), of a potential function, U(x). In the stochastic model, \u2207U(x) is the drift vector, which guarantees the existence of an invariant probability measure. In the geometric model, \u2207U(x) guarantees the existence of an orthogonal integral manifold. We have seen, in Section 6, that there is a theoretical connection between these two models, in which \u2207U(x) plays a crucial role, and we have seen the practical consequences of this connection in the computational examples in Sections 5.1 and 5.2.\nThe main deficiency in the theory, as presented in this paper, is the restriction of the geometric model to the three-dimensional case. We imposed this restriction to simplify the calculations, and to make it easy to visualize the examples in Mathematica. But the theory is not inherently limited to three dimensions. Theorem 5 in Section 4 was written using the vector cross product and the \u201ccurl,\u201d which is a threedimensional concept, but it is actually a special case of a general result in Rn which follows from the dual version of the Theorem of Frobenius, expressed in terms of differential forms. It follows that V\u2202 and W\u2202, our basis vectors for the tangent subbundle in R3, can be generalized to Rn. In particular, if \u2207U(x) = (P0(x), P1(x), . . . , Pn\u22121(x) ), we can\nwrite:\n\u2207U(x) = ( P0(x), P1(x), P2(x), . . . , Pn\u22122(x), Pn\u22121(x) ) V1(x) = ( \u2212P1(x), P0(x), 0, . . . , 0, 0 ) V2(x) = ( \u2212P2(x), 0, P0(x), . . . , 0, 0 )\n. . .\nVn\u22122(x) = ( \u2212Pn\u22122(x), 0, 0, . . . , P0(x), 0 ) Vn\u22121(x) = ( \u2212Pn\u22121(x), 0, 0, . . . , 0, P0(x) )\nIt is straightforward to verify that \u2207U(x) is orthogonal to each Vi(x), and that the tangent subbundle spanned by {Vi\u2202 = Vi(x)/P0(x)} satisfies the Frobenius integrability conditions. Thus the remaining results in Section 4 can be generalized as well. We will discuss these generalizations in a forthcoming paper, which has the working title: \u201cDifferential Similarity in Higher-Dimensional Spaces: Theory and Applications.\u201d\nOnce the theory is extended to higher dimensions, it will become apparent that there are various connections to recent work in manifold learning, as described in Section 1. Work in this area tends to follow either a geometric approach or a probabilistic approach, but not both. Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03]. Belkin and Niyogi [BN03], for example, work with the eigenvectors of the graph Laplacian and the eigenfunctions of the LaplaceBeltrami operator, and show that the solution to these eigenproblems yields an \u201coptimal\u201d embedding of a low-dimensional manifold into a higher-dimensional space, but their arguments are geometric rather than probabilistic. Examples of the probabilistic approach include: [HR02] [TB99] [CSP+10]. Tipping and Bishop [TB99] work with a mixture of low-dimensional Gaussians embedded in a higher-dimensional space, each with its own mean and covariance matrix, and they use the EM algorithm to estimate the parameters of this model. Chen, et al., [CSP+10] adopt a similar model, along with the assumption that the Gaussian mixture covers a low-dimensional manifold, and they estimate both the number of components in the mixture and the dimensionality of the subspaces, using Bayesian techniques. But neither paper makes use of the geometric structure of the embedded manifold.\nOne exception to this dichotomy between geometric and probabilistic approaches is a paper by Lee and Wasserman [LW10], which has some interesting connections to the present work. The paper starts out by defining a Markov chain on Rn with a transition kernel \u2126 (x, \u00b7) which gives preference to nearby points, y, that have a high probability density, p(y). This kernel is then used to define the one-step diffusion\noperator, A , and its m-step version, A ,m. The authors then construct a continuous time operator: At = lim \u21920A , t/ . The analogous mathematical object in our theory would be the operator Qt in equation (7). Lee and Wasserman are primarily interested in the eigenfunctions of A ,m and At, which have applications to various spectral clustering problems, following the work of Belkin and Niyogi [BN03] and others. They also use \u2126 (x, \u00b7) to define a diffusion distance, D2 ,m(x, z), and its continuous time version, D2t (x, z), but there does not seem to be a straightforward relationship between this distance and our dissimilarity metric, gij(x). The paper concludes with several examples that demonstrate the utility of these concepts.\nThe other important contribution of Lee and Wasserman [LW10] is their analysis of the statistical estimators for the population quantities, At and D 2 t . This is essential future work for our theory as well, since we need to apply our model to real data in order to fully validate it. One of the most important applications of manifold learning is to the field known as deep learning [BLPL06] [HOT06] [RPCL06]. In deep learning, a neural network with many layers (hence, \u201cdeep\u201d) is trained in two stages: the first stage uses an unsupervised learning algorithm to construct a set of \u201cfeatures\u201d bottom up; the second stage applies a supervised learning algorithm to the feature hierarchy, top down. In fact, the paper by Rifai, et al. [RDV+11], cited and quoted in Section 1, is an example of just such an approach. In their experiment, the authors extract a tangent plane at each training point using an unsupervised technique (CAE), and then train their network in a supervised manner using a (20 year old) technique that was designed to be sensitive to tangent directions (MTC). They write:\nTo the best of our knowledge this is the first time that the implicit relationship between an unsupervised learned mapping and the tangent space of a manifold is rendered explicit and successfully exploited for the training of a classifier.\nTo combine manifold learning and deep learning in this way, it is necessary to make a decision about the network interface: What kind of representation of the data should we use as the output of one layer and the input of the next? There are reasons to think that prototype coding (see Section 3) with our Riemannian dissimilarity metric, gij(x), will turn out to be a good choice, but this is obviously an important question on our agenda for future work."}], "references": [{"title": "Introduction to Differentiable Manifolds", "author": ["Louis Auslander", "Robert E. MacKenzie"], "venue": "Dover Publications,", "citeRegEx": "AM77", "shortCiteRegEx": null, "year": 1977}, {"title": "Pure and applied mathematics", "author": ["Richard L. Bishop", "Richard J. Crittenden. Geometry of Manifolds"], "venue": "Academic Press,", "citeRegEx": "BC64", "shortCiteRegEx": null, "year": 1964}, {"title": "Tensor Analysis on Manifolds", "author": ["Richard L. Bishop", "Samuel I. Goldberg"], "venue": "Macmillan,", "citeRegEx": "BG68", "shortCiteRegEx": null, "year": 1968}, {"title": "Greedy layerwise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems, volume 19, pages 153\u2013160", "citeRegEx": "BLPL06", "shortCiteRegEx": null, "year": 2006}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation, 15(6):1373\u20131396", "citeRegEx": "BN03", "shortCiteRegEx": null, "year": 2003}, {"title": "Dover Books on Mathematics Series", "author": ["Henri Cartan. Differential Forms"], "venue": "Dover Publications,", "citeRegEx": "Car71", "shortCiteRegEx": null, "year": 1971}, {"title": "Compressive sensing on manifolds using a nonparametric mixture of factor analyzers: Algorithm and performance bounds", "author": ["M. Chen", "J. Silva", "J.W. Paisley", "C. Wang", "D.B. Dunson", "L. Carin"], "venue": "IEEE Transactions on Signal Processing, 58(12):6140\u20136155", "citeRegEx": "CSP10", "shortCiteRegEx": null, "year": 2010}, {"title": "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data", "author": ["D. Donoho", "C. Grimes"], "venue": "Proceedings of National Academy of Sciences, 100:5591\u20135596", "citeRegEx": "DG03", "shortCiteRegEx": null, "year": 2003}, {"title": "John Willey & Sons", "author": ["Richard O. Duda", "Peter E. Hart. Pattern Classification", "Scene Analysis"], "venue": "New York,", "citeRegEx": "DH73", "shortCiteRegEx": null, "year": 1973}, {"title": "chapter 10: Unsupervised Learning and Clustering", "author": ["Richard O. Duda", "Peter E. Hart", "David G. Stork. Pattern Classification"], "venue": "Wiley & Sons, Inc., New York, 2nd edition,", "citeRegEx": "DHS01", "shortCiteRegEx": null, "year": 2001}, {"title": "Stochastic Calculus in Manifolds", "author": ["Michel Emery", "Paul A. Meyer"], "venue": "World Publishing Company,", "citeRegEx": "EM89", "shortCiteRegEx": null, "year": 1989}, {"title": "Space-time approach to non-relativistic quantum mechanics", "author": ["R.P. Feynman"], "venue": "Rev. Mod. Phys., 20:367\u2013387", "citeRegEx": "Fey48", "shortCiteRegEx": null, "year": 1948}, {"title": "Hypoelliptic second order differential equations", "author": ["L. H\u00f6rmander"], "venue": "Acta Mathematica, 119:147\u2013171", "citeRegEx": "H\u00f6r67", "shortCiteRegEx": null, "year": 1967}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, 18(7):1527\u20131554", "citeRegEx": "HOT06", "shortCiteRegEx": null, "year": 2006}, {"title": "Stochastic neighbor embedding", "author": ["G.E. Hinton", "S.T. Roweis"], "venue": "Advances in Neural Information Processing Systems, volume 15, pages 833\u2013 840", "citeRegEx": "HR02", "shortCiteRegEx": null, "year": 2002}, {"title": "Contemporary Mathematics", "author": ["Elton P. Hsu. Stochastic Analysis on Manifolds"], "venue": "American Mathematical Society,", "citeRegEx": "Hsu02", "shortCiteRegEx": null, "year": 2002}, {"title": "Stochastic differentials", "author": ["K. It\u00f4"], "venue": "Applied Mathematics & Optimization, 1(4):374\u2013381", "citeRegEx": "It\u00f475", "shortCiteRegEx": null, "year": 1975}, {"title": "On distributions of certain Wiener functionals", "author": ["M. Kac"], "venue": "Trans. Amer. Math. Soc., 65:1\u201313", "citeRegEx": "Kac49", "shortCiteRegEx": null, "year": 1949}, {"title": "Pure and Applied Mathematics", "author": ["David Lovelock", "Hanno Rund. Tensors", "Differential Forms", "Variational Principles"], "venue": "Wiley,", "citeRegEx": "LR75", "shortCiteRegEx": null, "year": 1975}, {"title": "Spectral connectivity analysis", "author": ["A.B. Lee", "L. Wasserman"], "venue": "Journal of the American Statistical Association, 105(491):1241\u20131255", "citeRegEx": "LW10", "shortCiteRegEx": null, "year": 2010}, {"title": "Path integral methods for parabolic partial differential equations with examples from computational finance", "author": ["A. Lyasoff"], "venue": "Mathematica Journal, 9(2):399\u2013422", "citeRegEx": "Lya04", "shortCiteRegEx": null, "year": 2004}, {"title": "Stochastic Differential Equations: An Introduction With Applications", "author": ["Bernt K. \u00d8ksendal"], "venue": "Springer, sixth edition,", "citeRegEx": "\u00d8ks03", "shortCiteRegEx": null, "year": 2003}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "Advances in Neural Information Processing Systems, volume 24, pages 2294\u20132302", "citeRegEx": "RDV11", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems, volume 19, pages 1137\u20131144", "citeRegEx": "RPCL06", "shortCiteRegEx": null, "year": 2006}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20132326", "citeRegEx": "RS00", "shortCiteRegEx": null, "year": 2000}, {"title": "volume 1", "author": ["Michael Spivak. A Comprehensive Introduction to Differential Geometry"], "venue": "Publish or Perish, third edition,", "citeRegEx": "Spi99", "shortCiteRegEx": null, "year": 1999}, {"title": "Diffusions as integral curves", "author": ["D.W. Stroock", "S. Taniguchi"], "venue": "or Stratonovich without It\u00f4. In The Dynkin Festschrift. Markov processes and their applications. In celebration of Eugene B. Dynkin\u2019s 70th birthday, pages 333\u2013369. Boston, MA: Birkh\u00e4user", "citeRegEx": "ST94", "shortCiteRegEx": null, "year": 1994}, {"title": "Diffusions as integral curves on manifolds and Lie groups", "author": ["D.W. Stroock", "S. Taniguchi"], "venue": "Probability theory and mathematical statistics. Lectures presented at the semester held in St. Petersburg, Russia, March 2\u2013April 23, 1993, pages 219\u2013226. Amsterdam: Gordon and Breach Publishers", "citeRegEx": "ST96", "shortCiteRegEx": null, "year": 1996}, {"title": "A new representation for stochastic integrals and equations", "author": ["R.L. Stratonovich"], "venue": "SIAM Journal on Control, 4(2):362\u2013371", "citeRegEx": "Str66", "shortCiteRegEx": null, "year": 1966}, {"title": "On the growth of stochastic integrals", "author": ["D.W. Stroock"], "venue": "Z. Wahrscheinlichkeitstheor. Verw. Geb., 18:340\u2013344", "citeRegEx": "Str71", "shortCiteRegEx": null, "year": 1971}, {"title": "Probability Theory: An Analytic View", "author": ["Daniel W. Stroock"], "venue": "Cambridge University Press,", "citeRegEx": "Str93", "shortCiteRegEx": null, "year": 1993}, {"title": "Gaussian measures in traditional and not so traditional settings", "author": ["D.W. Stroock"], "venue": "Bulletin (New Series) of the American Mathematical Society, 33(2):135\u2013155", "citeRegEx": "Str96", "shortCiteRegEx": null, "year": 1996}, {"title": "Mathematical Surveys and Monographs", "author": ["Daniel W. Stroock. An Introduction to the Analysis of Paths on a Riemannian Manifold"], "venue": "American Mathematical Society,", "citeRegEx": "Str00", "shortCiteRegEx": null, "year": 2000}, {"title": "It\u00f4\u2019s Perspective", "author": ["K Daniel W. Stroock. Markov Processes from"], "venue": "Annals of Mathematics Studies. Princeton University Press,", "citeRegEx": "Str03", "shortCiteRegEx": null, "year": 2003}, {"title": "Probability Theory: An Analytic View", "author": ["Daniel W. Stroock"], "venue": "Cambridge University Press, second edition,", "citeRegEx": "Str11", "shortCiteRegEx": null, "year": 2011}, {"title": "Mixtures of probabilistic principal component analyzers", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Neural Computation, 11(2):443\u2013482", "citeRegEx": "TB99", "shortCiteRegEx": null, "year": 1999}, {"title": "A Global Geometric Framework for Nonlinear Dimensionality Reduction", "author": ["J.B. Tenenbaum", "V. Silva", "J.C. Langford"], "venue": "Science, 290(5500):2319\u20132323", "citeRegEx": "TSL00", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 8, "context": "Clustering algorithms have been studied for several decades [DH73], and they remain one of the main ingredients in unsupervised learning [DHS01].", "startOffset": 60, "endOffset": 66}, {"referenceID": 9, "context": "Clustering algorithms have been studied for several decades [DH73], and they remain one of the main ingredients in unsupervised learning [DHS01].", "startOffset": 137, "endOffset": 144}, {"referenceID": 36, "context": "Recently, a variant of the traditional clustering algorithms has attracted some attention, under the rubric of manifold learning : [TSL00] [RS00] [BN03].", "startOffset": 131, "endOffset": 138}, {"referenceID": 24, "context": "Recently, a variant of the traditional clustering algorithms has attracted some attention, under the rubric of manifold learning : [TSL00] [RS00] [BN03].", "startOffset": 139, "endOffset": 145}, {"referenceID": 4, "context": "Recently, a variant of the traditional clustering algorithms has attracted some attention, under the rubric of manifold learning : [TSL00] [RS00] [BN03].", "startOffset": 146, "endOffset": 152}, {"referenceID": 22, "context": "[RDV11], outline three hypotheses that motivate much of this work:", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "Let\u2019s start with a model that will be familiar to most physicists: the Feynman-Kac formula [Fey48] [Kac49].", "startOffset": 91, "endOffset": 98}, {"referenceID": 17, "context": "Let\u2019s start with a model that will be familiar to most physicists: the Feynman-Kac formula [Fey48] [Kac49].", "startOffset": 99, "endOffset": 106}, {"referenceID": 30, "context": "See [Str93], Section 4.", "startOffset": 4, "endOffset": 11}, {"referenceID": 30, "context": "See [Str93], Section 4.", "startOffset": 4, "endOffset": 11}, {"referenceID": 34, "context": "2, or [Str11], Section 8.", "startOffset": 6, "endOffset": 13}, {"referenceID": 31, "context": "haunts every attempt to deal with Brownian paths,\u201d [Str96], p.", "startOffset": 51, "endOffset": 58}, {"referenceID": 10, "context": "See [EM89] or [Hsu02].", "startOffset": 4, "endOffset": 10}, {"referenceID": 15, "context": "See [EM89] or [Hsu02].", "startOffset": 14, "endOffset": 21}, {"referenceID": 30, "context": "36 in [Str93] or Theorem 10.", "startOffset": 6, "endOffset": 13}, {"referenceID": 34, "context": "33 in [Str11].", "startOffset": 6, "endOffset": 13}, {"referenceID": 30, "context": "Sources: These results appear in [Str93], Section 4.", "startOffset": 33, "endOffset": 40}, {"referenceID": 34, "context": "See [Str11], Section 10.", "startOffset": 4, "endOffset": 11}, {"referenceID": 21, "context": "16 of his text [\u00d8ks03].", "startOffset": 15, "endOffset": 22}, {"referenceID": 21, "context": "3 in [\u00d8ks03].", "startOffset": 5, "endOffset": 12}, {"referenceID": 21, "context": "See [\u00d8ks03], Chapter 4.", "startOffset": 4, "endOffset": 11}, {"referenceID": 28, "context": "See [Str66] or [It\u00f475].", "startOffset": 4, "endOffset": 11}, {"referenceID": 16, "context": "See [Str66] or [It\u00f475].", "startOffset": 15, "endOffset": 22}, {"referenceID": 12, "context": "See [H\u00f6r67].", "startOffset": 4, "endOffset": 11}, {"referenceID": 32, "context": "For these reasons, Stroock relies on the H\u00f6rmander formalism extensively in his book on the analysis of Brownian paths on Riemannian manifolds [Str00].", "startOffset": 143, "endOffset": 150}, {"referenceID": 21, "context": "Sources: For the basic results on stochastic differential equations, using It\u00f4\u2019s formalism, the reader should consult [\u00d8ks03], but \u00d8ksendal\u2019s text provides only a cursory treatment of Stratonovich\u2019s formalism.", "startOffset": 118, "endOffset": 125}, {"referenceID": 28, "context": "The original paper by Stratonovich [Str66] is still very readable, but his theory was only given a solid mathematical foundation some years later by It\u00f4 [It\u00f475].", "startOffset": 35, "endOffset": 42}, {"referenceID": 16, "context": "The original paper by Stratonovich [Str66] is still very readable, but his theory was only given a solid mathematical foundation some years later by It\u00f4 [It\u00f475].", "startOffset": 153, "endOffset": 160}, {"referenceID": 33, "context": "Chapter 8 of [Str03] is an excellent contemporary account of Stratonovich\u2019s theory, set in a broader context.", "startOffset": 13, "endOffset": 20}, {"referenceID": 31, "context": "There remains the problem that \u201chaunts every attempt to deal with Brownian paths,\u201d [Str96], p.", "startOffset": 83, "endOffset": 90}, {"referenceID": 30, "context": "23 in [Str93].", "startOffset": 6, "endOffset": 13}, {"referenceID": 30, "context": "32 in [Str93].", "startOffset": 6, "endOffset": 13}, {"referenceID": 30, "context": "10 in [Str93].", "startOffset": 6, "endOffset": 13}, {"referenceID": 26, "context": "The preceding analysis is not confined to Euclidean R, since a similar construction works when L is given in H\u00f6rmander form by (18), see [ST94], and this means that all results can be replicated in an arbitrary Riemannian manifold, see [ST96].", "startOffset": 137, "endOffset": 143}, {"referenceID": 27, "context": "The preceding analysis is not confined to Euclidean R, since a similar construction works when L is given in H\u00f6rmander form by (18), see [ST94], and this means that all results can be replicated in an arbitrary Riemannian manifold, see [ST96].", "startOffset": 236, "endOffset": 242}, {"referenceID": 32, "context": "The theory is explicated further in [Str00], where it serves as the foundation for Stroock\u2019s construction and analysis of Brownian motion on a Riemannian manifold.", "startOffset": 36, "endOffset": 43}, {"referenceID": 32, "context": "1 of [Str00] includes a generalization of Lemma 4 above, and Theorem 2.", "startOffset": 5, "endOffset": 12}, {"referenceID": 32, "context": "40 of [Str00] is a generalization of Theorem 4.", "startOffset": 6, "endOffset": 13}, {"referenceID": 25, "context": ", [Spi99], Chapter 6; [BG68], Chapter 3; [AM77], Chapter 8.", "startOffset": 2, "endOffset": 9}, {"referenceID": 2, "context": ", [Spi99], Chapter 6; [BG68], Chapter 3; [AM77], Chapter 8.", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": ", [Spi99], Chapter 6; [BG68], Chapter 3; [AM77], Chapter 8.", "startOffset": 41, "endOffset": 47}, {"referenceID": 1, "context": "See [BC64], Problem 29, p.", "startOffset": 4, "endOffset": 10}, {"referenceID": 5, "context": "23; [Car71], pp.", "startOffset": 4, "endOffset": 11}, {"referenceID": 18, "context": "97\u201398; [LR75], pp.", "startOffset": 7, "endOffset": 13}, {"referenceID": 2, "context": ", [BG68], Theorem 3.", "startOffset": 2, "endOffset": 8}, {"referenceID": 25, "context": "1, and [Spi99], Lemma 5.", "startOffset": 7, "endOffset": 14}, {"referenceID": 2, "context": ", [BG68], Theorem 3.", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": "1, or [BC64], Theorem 1.", "startOffset": 6, "endOffset": 12}, {"referenceID": 20, "context": "See, for example, [Lya04].", "startOffset": 18, "endOffset": 25}, {"referenceID": 29, "context": "The earliest example is in [Str71] and [It\u00f475].", "startOffset": 27, "endOffset": 34}, {"referenceID": 16, "context": "The earliest example is in [Str71] and [It\u00f475].", "startOffset": 39, "endOffset": 46}, {"referenceID": 32, "context": "1 of [Str00] or Theorem 3.", "startOffset": 5, "endOffset": 12}, {"referenceID": 15, "context": "4 in [Hsu02].", "startOffset": 5, "endOffset": 12}, {"referenceID": 32, "context": "37 in [Str00].", "startOffset": 6, "endOffset": 13}, {"referenceID": 29, "context": "To simplify the calculations, we will initially focus our attention on the simple Gaussian case, in which \u2207U(x, y, z) = (\u2212ax,\u2212by,\u2212cz), and we will start with a construction borrowed from [Str71] and [It\u00f475], but", "startOffset": 187, "endOffset": 194}, {"referenceID": 16, "context": "To simplify the calculations, we will initially focus our attention on the simple Gaussian case, in which \u2207U(x, y, z) = (\u2212ax,\u2212by,\u2212cz), and we will start with a construction borrowed from [Str71] and [It\u00f475], but", "startOffset": 199, "endOffset": 206}, {"referenceID": 36, "context": "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].", "startOffset": 44, "endOffset": 51}, {"referenceID": 24, "context": "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].", "startOffset": 52, "endOffset": 58}, {"referenceID": 4, "context": "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].", "startOffset": 59, "endOffset": 65}, {"referenceID": 7, "context": "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].", "startOffset": 66, "endOffset": 72}, {"referenceID": 4, "context": "Belkin and Niyogi [BN03], for example, work with the eigenvectors of the graph Laplacian and the eigenfunctions of the LaplaceBeltrami operator, and show that the solution to these eigenproblems yields an \u201coptimal\u201d embedding of a low-dimensional manifold into a higher-dimensional space, but their arguments are geometric rather than probabilistic.", "startOffset": 18, "endOffset": 24}, {"referenceID": 14, "context": "Examples of the probabilistic approach include: [HR02] [TB99] [CSP10].", "startOffset": 48, "endOffset": 54}, {"referenceID": 35, "context": "Examples of the probabilistic approach include: [HR02] [TB99] [CSP10].", "startOffset": 55, "endOffset": 61}, {"referenceID": 6, "context": "Examples of the probabilistic approach include: [HR02] [TB99] [CSP10].", "startOffset": 62, "endOffset": 69}, {"referenceID": 35, "context": "Tipping and Bishop [TB99] work with a mixture of low-dimensional Gaussians embedded in a higher-dimensional space, each with its own mean and covariance matrix, and they use the EM algorithm to estimate the parameters of this model.", "startOffset": 19, "endOffset": 25}, {"referenceID": 6, "context": ", [CSP10] adopt a similar model, along with the assumption that the Gaussian mixture covers a low-dimensional manifold, and they estimate both the number of components in the mixture and the dimensionality of the subspaces, using Bayesian techniques.", "startOffset": 2, "endOffset": 9}, {"referenceID": 19, "context": "One exception to this dichotomy between geometric and probabilistic approaches is a paper by Lee and Wasserman [LW10], which has some interesting connections to the present work.", "startOffset": 111, "endOffset": 117}, {"referenceID": 4, "context": "Lee and Wasserman are primarily interested in the eigenfunctions of A ,m and At, which have applications to various spectral clustering problems, following the work of Belkin and Niyogi [BN03] and others.", "startOffset": 186, "endOffset": 192}, {"referenceID": 19, "context": "The other important contribution of Lee and Wasserman [LW10] is their analysis of the statistical estimators for the population quantities, At and D 2 t .", "startOffset": 54, "endOffset": 60}, {"referenceID": 3, "context": "One of the most important applications of manifold learning is to the field known as deep learning [BLPL06] [HOT06] [RPCL06].", "startOffset": 99, "endOffset": 107}, {"referenceID": 13, "context": "One of the most important applications of manifold learning is to the field known as deep learning [BLPL06] [HOT06] [RPCL06].", "startOffset": 108, "endOffset": 115}, {"referenceID": 23, "context": "One of the most important applications of manifold learning is to the field known as deep learning [BLPL06] [HOT06] [RPCL06].", "startOffset": 116, "endOffset": 124}, {"referenceID": 22, "context": "[RDV11], cited and quoted in Section 1, is an example of just such an approach.", "startOffset": 0, "endOffset": 7}], "year": 2014, "abstractText": "This paper develops a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way. The geometric model is a Riemannian manifold with a Riemannian metric, gij(x), which we interpret as a measure of dissimilarity. The probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data. The link between the two models is a potential function, U(x), and its gradient, \u2207U(x). We use the gradient to define the dissimilarity metric, which guarantees that our measure of dissimilarity will depend on the probability measure. Finally, we use the dissimilarity metric to define a coordinate system on the embedded Riemannian manifold, which gives us a low-dimensional encoding of our original data.", "creator": "LaTeX with hyperref package"}}}