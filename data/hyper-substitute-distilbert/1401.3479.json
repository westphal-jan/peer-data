{"id": "1401.3479", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Complex Question Answering: Unsupervised Learning Approaches and Experiments", "abstract": "complex questions that require inferencing and comparison information such multiple models can be classed as lacking completely functional topic - defined, subjective multi - document summarization where initial goal is to produce maximum valid text resembling a compressed any larger project set than documents maintaining a minimum loss of relevant information. in some paper, we experiment only one empirical problem and explore unsupervised statistical linguistic selection machines : k - finding and retrieval maximization ( em ), accurately computing relative importance of the sentences. we compare the results of these researchers. first arguments show indeed the archival technology outperforms the other relational techniques and em performs approaches obtaining k - numbers. fortunately, specific difficulty of validation approaches depends rather on quantitative problem set used utilizing local weighting of these findings. one goal truly measure the importance and stability thus the user face we extract different kinds of scales ( ist. e. lexical, lexical complexity, cosine dependency, verbal spelling, tree kernel based syntactic method shallow - recall ) utilizing each meaningful additional written sentences. we use a local validation technique to tell spatial weights off additional features. to the best of those case, such study ignores any tree kernel functions to encode historical / semantic information but more complex categories such as indicating the relatedness between the completed sentences and the document metadata in comparison to generate query - free summaries ( or answers to complex errors ). including benefit of our methods of temporal summaries ( we. e. empirical, quasi - temporal and em ) we show significant effects distinguishing null _ shallow - grasp methods over the bag - of - proof ( bow ) features.", "histories": [["v1", "Wed, 15 Jan 2014 05:33:57 GMT  (406kb)", "http://arxiv.org/abs/1401.3479v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["yllias chali", "shafiq rayhan joty", "sadid a hasan"], "accepted": false, "id": "1401.3479"}, "pdf": {"name": "1401.3479.pdf", "metadata": {"source": "CRF", "title": "Complex Question Answering: Unsupervised Learning Approaches and Experiments", "authors": ["Yllias Chali", "Shafiq R. Joty", "Sadid A. Hasan"], "emails": ["chali@cs.uleth.ca", "rjoty@cs.ubc.ca", "hasan@cs.uleth.ca"], "sections": [{"heading": "1. Introduction", "text": "The vast increase in the amount of online text available and the demand for access to different types of information have led to a renewed interest in a broad range of Information Retrieval (IR) related areas that go beyond the simple document retrieval. These areas include question answering, topic detection and tracking, summarization, multimedia retrieval, chemical and biological informatics, text structuring, text mining, genomics, etc. Automated Question Answering (QA)\u2014the ability of a machine to answer questions, simple or complex, posed in ordinary human language\u2014is perhaps the most exciting technological development of the past six or seven years (Strzalkowski & Harabagiu, 2008). The\nc\u00a92009 AI Access Foundation. All rights reserved.\nexpectations are already tremendous, reaching beyond the discipline (a subfield of Natural Language Processing (NLP)) itself.\nAs a tool for finding documents on the web, search engines are proven to be adequate. Although there is no limitation in the expressiveness of the user in terms of query formulation, certain limitations exist in what the search engine does with the query. Complex question answering tasks require multi-document summarization through an aggregated search, or a faceted search, that represents an information need which cannot be answered by a single document. For example, if we look for the comparison of the average number of years between marriage and first birth for women in the U.S., Asia, and Europe, the answer is likely contained in multiple documents. Multi-document summarization is useful for this type of query and there is currently no tool on the market that is designed to meet this kind of information need.\nQA research attempts to deal with a wide range of question types including: fact, list, definition, how, why, hypothetical, semantically-constrained, and cross-lingual questions. Some questions, which we will call simple questions, are easier to answer. For example, the question: \u201cWho is the president of Bangladesh?\u201d asks for a person\u2019s name. This type of question (i.e. factoid) requires small snippets of text as the answer. Again, the question: \u201cWhich countries has Pope John Paul II visited?\u201d is a sample of a list question, asking only for a list of small snippets of text.\nAfter having made substantial headway in factoid and list questions, researchers have turned their attention to more complex information needs that cannot be answered by simply extracting named entities (persons, organizations, locations, dates, etc.) from documents. Unlike informationally simple factoid questions, complex questions often seek multiple different types of information simultaneously and do not presuppose that one single answer can meet all of its information needs. For example, with a factoid question like: \u201cHow accurate are HIV tests?\u201d it can be safely assumed that the submitter of the question is looking for a number or a range of numbers. However, with complex questions like: \u201cWhat are the causes of AIDS?\u201d the wider focus of this question suggests that the submitter may not have a single or well-defined information need and therefore may be amenable to receiving additional supporting information that is relevant to some (as yet) undefined informational goal (Harabagiu, Lacatusu, & Hickl, 2006). These questions require inferencing and synthesizing information from multiple documents.\nA well known QA systems is the Korean Naver\u2019s Knowledge iN search1, who were the pioneers in community QA. This tool allows users to ask just about any question and get answers from other users. Naver\u2019s Knowledge iN now has roughly 10 times more entries than Wikipedia. It is used by millions of Korean web users on any given day. Some people say Koreans are not addicted to the internet but to Naver. As of January 2008 the Knowledge Search database included more than 80 million pages of user-generated information. Another popular answer service is Yahoo! Answers which is a community-driven knowledge market website launched by Yahoo!. It allows users to both submit questions to be answered and answer questions from other users. People vote on the best answer. The site gives members the chance to earn points as a way to encourage participation and is based on the Naver model. As of December 2006, Yahoo! Answers had 60 million users and 65\n1. http://kin.naver.com/\nmillion answers. Google had a QA system2 based on paid editors which was launched in April 2002 and fully closed in December 2006.\nHowever, from a computational linguistics point of view information synthesis can be seen as a kind of topic-oriented informative multi-document summarization. The goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. Unlike indicative summaries (which help to determine whether a document is relevant to a particular topic), informative summaries must attempt to find answers.\nIn this paper, we focus on an extractive approach of summarization where a subset of the sentences in the original documents are chosen. This contrasts with abstractive summarization where the information in the text is rephrased. Although summaries produced by humans are typically not extractive, most of the state of the art summarization systems are based on extraction and they achieve better results than the automated abstraction. Here, we experimented with one empirical and two well-known unsupervised statistical machine learning techniques: K-means and EM and evaluated their performance in generating topicoriented summaries. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. In order to measure the importance and relevance to the user query we extract different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. We have used a gradient descent local search technique to learn the weights of the features.\nTraditionally, information extraction techniques are based on the BOW approach augmented by language modeling. But when the task requires the use of more complex semantics, the approaches based on only BOW are often inadequate to perform fine-level textual analysis. Some improvements on BOW are given by the use of dependency trees and syntactic parse trees (Hirao, , Suzuki, Isozaki, & Maeda, 2004; Punyakanok, Roth, & Yih, 2004; Zhang & Lee, 2003b), but these too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs. Shallow semantic representations, bearing more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti, Quarteroni, Basili, & Manandhar, 2007). As pinpointing the answer to a question relies on a deep understanding of the semantics of both, attempting an application of syntactic and semantic information to complex QA seems natural. To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). For all of our methods of generating summaries (i.e. empirical, K-means and EM) we show the effects of syntactic and shallow-semantic features over the BOW features.\nOver the past three years, complex questions have been the focus of much attention in both the automatic question-answering and Multi Document Summarization (MDS) communities. Typically, most current complex QA evaluations including the 2004 AQUAINT Relationship QA Pilot, the 2005 Text Retrieval Conference (TREC) Relationship QA Task, and the TREC definition (and others) require systems to return unstructured lists of can-\n2. http://answers.google.com/\ndidate answers in response to a complex question. However recently, MDS evaluations (including the 2005, 2006 and 2007 Document Understanding Conference (DUC)) have tasked systems with returning paragraph-length answers to complex questions that are responsive, relevant, and coherent.\nOur experiments based on the DUC 2007 data show that including syntactic and semantic features improves the performance. Comparison among the approaches are also shown. Comparing with DUC 2007 participants, our systems achieve top scores and there is no statistically significant difference between the results of our system and the results of DUC 2007 best system.\nThis paper is organized as follows: Section 2 focuses on the related work, Section 3 gives a brief description of our intended final model, Section 4 describes how the features are extracted, Section 5 discusses the learning issues and presents our learning approaches, Section 6 discusses how we remove the redundant sentences before adding them to the final summary, and Section 7 describes our experimental study. We conclude and discuss future directions in Section 8."}, {"heading": "2. Related Work", "text": "Researchers all over the world working on query-based summarization are trying different directions to see which methods provide the best results.\nThere are a number of sentence retrieval systems based on IR (Information Retrieval) techniques. These systems typically don\u2019t use a lot of linguistic information, but they still deserve special attention. Murdock and Croft (2005) propose a translation model specifically for monolingual data, and show that it significantly improves sentence retrieval over query likelihood. Translation models train on a parallel corpus and they used a corpus of question/answer pairs. Losada (2005) presents a comparison between multiple-Bernoulli models and multinomial models in the context of a sentence retrieval task and shows that a multivariate Bernoulli model can really outperform popular multinomial models for retrieving relevant sentences. Losada and Ferna\u0301ndez (2007) propose a novel sentence retrieval method based on extracting highly frequent terms from top retrieved documents. Their results reinforce the idea that top retrieved data is a valuable source to enhance retrieval systems. This is specially true for short queries because there are usually few query-sentence matching terms. They argue that this method improves significantly the precision at top ranks when handling poorly specified information needs.\nThe LexRank method addressed by Erkan and Radev (2004) was very successful in generic multi-document summarization. A topic-sensitive LexRank is proposed by Otterbacher, Erkan, and Radev (2005). As in LexRank, the set of sentences in a document cluster is represented as a graph where nodes are sentences, and links between the nodes are induced by a similarity relation between the sentences. The system then ranks the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.\nConcepts of coherence and cohesion enable us to capture the theme of the text. Coherence represents the overall structure of a multi-sentence text in terms of macro-level relations between clauses or sentences (Halliday & Hasan, 1976). Cohesion, as defined by Halliday and Hasan (1976), is the property of holding text together as one single grammat-\nical unit based on relations (i.e. ellipsis, conjunction, substitution, reference, and lexical cohesion) between various elements of the text. Lexical cohesion is defined as the cohesion that arises from the semantic relations (collocation, repetition, synonym, hypernym, hyponym, holonym, meronym, etc.) between the words in the text (Morris & Hirst, 1991). Lexical cohesion among words are represented by lexical chains which are the sequences of semantically related words. The summarization methods based on lexical chain first extract the nouns, compound nouns and named entities as candidate words (Li, Sun, Kit, & Webster, 2007). Then using WordNet3 the systems find the semantic similarity between the nouns and compound nouns. After that lexical chains are built in two steps:\n1. Building single document strong chains while disambiguating the senses of the words.\n2. Building a multi-chain by merging the strongest chains of the single documents into one chain.\nThe systems rank sentences using a formula that involves a) the lexical chain, b) keywords from the query and c) named entities. For example, Li et al. (2007) uses the following formula:\nScore = \u03b1P (chain) + \u03b2P (query) + \u03b3P (namedEntity)\nwhere P (chain) is the sum of the scores of the chains whose words come from the candidate sentence, P (query) is the sum of the co-occurrences of key words in a topic and the sentence, and P (namedEntity) is the number of name entities existing in both the topic and the sentence. The three coefficients \u03b1, \u03b2 and \u03b3 are set empirically. The top ranked sentences are then selected to form the summary.\nHarabagiu et al. (2006) introduce a new paradigm for processing complex questions that relies on a combination of (a) question decompositions; (b) factoid QA techniques; and (c) Multi-Document Summarization (MDS) techniques. The question decomposition procedure operates on a Markov chain. That is, by following a random walk with a mixture model on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topic-relevant passages that manifest these relations. Decomposed questions are then submitted to a state-of-the-art QA system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a MDS system. They show that question decompositions using this method can significantly enhance the relevance and comprehensiveness of summary-length answers to complex questions.\nThere are approaches that are based on probabilistic models (Pingali, K., & Varma, 2007; Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, & Vanderwende, 2007). Pingali et al. (2007) rank the sentences based on a mixture model where each component of the model is a statistical model:\nScore(s) = \u03b1\u00d7QIScore(s) + (1\u2212 \u03b1)\u00d7QFocus(s,Q) (1) 3. WordNet (http://wordnet.princeton.edu/) is a widely used semantic lexicon for the English language.\nIt groups English words (i.e. nouns, verbs, adjectives and adverbs) into sets of synonyms called synsets, provides short, general definitions (i.e. gloss definition), and records the various semantic relations between these synonym sets.\nWhere Score(s) is the score for sentence s. Query-independent score (QIScore) and query-dependent score (QFocus) are calculated based on probabilistic models. Toutanova et al. (2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency. The scoring function is learned by fitting weights for a set of feature functions of sentences in the document set and is trained to optimize a sentence pair-wise ranking criterion. The scoring function is further adapted to apply to summaries rather than sentences and to take into account redundancy among sentences.\nPingali et al. (2007) reduce the document-sentences by dropping words that do not contain any important information. Toutanova et al. (2007), Vanderwende, Suzuki, and Brockett (2006), and Zajic, Lin, Dorr, and Schwartz (2006) heuristically decompose the document-sentences into smaller units. They apply a small set of heuristics to a parse tree to create alternatives after which both the original sentence and (possibly multiple) simplified versions are available for selection.\nThere are approaches in multi-document summarization that do try to cluster sentences together. Guo and Stylios (2003) use verb arguments (i.e. subjects, times, locations and actions) for clustering. For each sentence this method establishes the indices information based on the verb arguments (subject is first index, time is second, location is third and action is fourth). All the sentences that have the same or closest \u2018subjects\u2019 index are put in a cluster and they are sorted out according to the temporal sequence from the earliest to the latest. Sentences that have the same \u2018spaces/locations\u2019 index value in the cluster are then marked out. The clusters are ranked based on their sizes and top 10 clusters are chosen. Then, applying a cluster reduction module the system generates the compressed extract summaries.\nThere are approaches in \u201cRecognizing Textual Entailment\u201d, \u201cSentence Alignment\u201d, and \u201cQuestion Answering\u201d that use syntactic and/or semantic information in order to measure the similarity between two textual units. This indeed motivated us to include syntactic and semantic features to get the structural similarity between a document sentence and a query sentence (discussed in Section 4.1). MacCartney, Grenager, de Marneffe, Cer, and Manning (2006) use typed dependency graphs (same as dependency trees) to represent the text and the hypothesis. They try to find a good partial alignment between the typed dependency graphs representing the hypothesis (contains n nodes) and the text (graph contains m nodes) in a search space of O((m + 1)n). They use an incremental beam search combined with a node ordering heuristic to do approximate global search in the space of possible alignments. A locally decomposable scoring function was chosen such that the score of an alignment is the sum of the local node and edge alignment scores. The scoring measure is designed to favor alignments which align semantically similar subgraphs, irrespective of polarity. For this reason, nodes receive high alignment scores when the words they represent are semantically similar. Synonyms and antonyms receive the highest score and unrelated words receive the lowest. Alignment scores also incorporate local edge scores which are based on the shape of the paths between nodes in the text graph which correspond to adjacent nodes in the hypothesis graph. In the final step they make a decision about whether or not the hypothesis is entailed by the text conditioned on the typed dependency graphs as well as the best alignment between them. To make this decision they use a supervised\nstatistical logistic regression classifier (with a feature space of 28 features) with a Gaussian prior parameter for regularization.\nHirao et al. (2004) represent the sentences using Dependency Tree Path (DTP) to incorporate syntactic information. They apply String Subsequence Kernel (SSK) to measure the similarity between the DTPs of two sentences. They also introduce Extended String Subsequence Kernel (ESK) to incorporate semantics in DTPs. Kouylekov and Magnini (2005) use the tree edit distance algorithms on the dependency trees of the text and the hypothesis to recognize the textual entailment. According to this approach, a text T entails a hypothesis H if there exists a sequence of transformations (i.e. deletion, insertion and substitution) applied to T such that we can obtain H with an overall cost below a certain threshold. Punyakanok et al. (2004) represent the question and the sentence containing answer with their dependency trees. They add semantic information (i.e. named entity, synonyms and other related words) in the dependency trees. They apply the approximate tree matching in order to decide how similar any given pair of trees are. They also use the edit distance as the matching criteria in the approximate tree matching. All these methods show the improvement over the BOW scoring methods."}, {"heading": "3. Our Approach", "text": "To accomplish the task of answering complex questions we extract various important features for each of the sentences in the document collection to measure its relevance to the query. The sentences in the document collection are analyzed in various levels and each of the document sentences is represented as a vector of feature-values. Our feature set includes lexical, lexical semantic, statistical similarity, syntactic and semantic features, and graph-based similarity measures (Chali & Joty, 2008b). We reimplemented many of these features which are successfully applied to many related fields of NLP.\nWe use a simple local search technique to fine-tune the feature weights. We also use the statistical clustering algorithms: EM and K-means to select the relevant sentences for summary generation. Experimental results show that our systems perform better when we include the tree kernel based syntactic and semantic features though summaries based on only syntactic or semantic feature do not achieve good results. Graph-based cosine similarity and lexical semantic features are also important for selecting relevant sentences. We find that the local search technique outperforms the other two and the EM performs better than the K-means based learning. In the later sections we describe all the subparts of our systems in details."}, {"heading": "4. Feature Extraction", "text": "In this section, we will describe the features that will be used to score the sentences. We provide detailed examples4 to show how we get the feature values. We will first describe the syntactic and semantic features that we are introducing in this work. We follow with a detailed description of the features more commonly used in the question answering and summarization communities.\n4. All the query and document sentences used in the examples are taken from the DUC 2007 collection."}, {"heading": "4.1 Syntactic and Shallow Semantic Features", "text": "For the task like query-based summarization that requires the use of more complex syntactic and semantics, the approaches with only BOW are often inadequate to perform fine-level textual analysis. The importance of syntactic and semantic features in this context is described by Zhang and Lee (2003a), Moschitti et al. (2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b).\nAn effective way to integrate syntactic and semantic structures in machine learning algorithms is the use of tree kernel functions (Collins & Duffy, 2001; Moschitti & Quarteroni, 2008) which has been successfully applied to question classification (Zhang & Lee, 2003a; Moschitti & Basili, 2006). Syntactic and semantic information are used effectively to measure the similarity between two textual units by MacCartney et al. (2006). To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences. Another good way to encode some shallow syntactic information is the use of Basic Elements (BE) (Hovy, Lin, Zhou, & Fukumoto, 2006) which uses dependency relations. Our experiments show that including syntactic and semantic features improves the performance on the sentence selection for complex question answering task (Chali & Joty, 2008a)."}, {"heading": "4.1.1 Encoding Syntactic Structures", "text": "Basic Element (BE) Overlap Measure Shallow syntactic information based on dependency relations was proved to be effective in finding similarity between two textual units (Hirao et al., 2004). We incorporate this information by using Basic Elements that are defined as follows (Hovy et al., 2006):\n\u2022 The head of a major syntactic constituent (noun, verb, adjective or adverbial phrases), expressed as a single item.\n\u2022 A relation between a head-BE and a single dependent, expressed as a triple: (head|modifier|relation).\nThe triples encode some syntactic information and one can decide whether any two units match or not- more easily than with longer units (Hovy et al., 2006). We extracted BEs for the sentences (or query) by using the BE package distributed by ISI5.\nOnce we get the BEs for a sentence, we computed the Likelihood Ratio (LR) for each BE following Zhou, Lin, and Hovy (2005). Sorting BEs according to their LR scores produced a BE-ranked list. Our goal is to generate a summary that will answer the users\u2019 questions. The ranked list of BEs in this way contains important BEs at the top which may or may not be relevant to the users\u2019 questions. We filter those BEs by checking whether they contain any word which is a query word or a QueryRelatedWords (defined in Section 4.3). For example, if we consider the following sentence we get the BE score of 0.77314.\nQuery: Describe steps taken and worldwide reaction prior to introduction of the Euro on January 1, 1999. Include predictions and expectations reported in the press.\n5. BE website:http://www.isi.edu/ cyl/BE\nSentence: The Frankfurt-based body said in its annual report released today that it has decided on two themes for the new currency: history of European civilization and abstract or concrete paintings.\nBE Score: 0.77314\nHere, the BE \u201cdecided|themes|obj\u201d is not considered as it does not contain any word from the query words or query relevant words but BE \u201creport|annual|mod\u201d is taken as it contains a query word \u201creport\u201d. In this way, we filter out the BEs that are not related to the query. The score of a sentence is the sum of its BE scores divided by the number of BEs in the sentence. By limiting the number of the top BEs that contribute to the calculation of the sentence scores we can remove the BEs with little importance and the sentences with fewer important BEs. If we set the threshold to 100 only the topmost 100 BEs in the ranked list can contribute to the normalized sentence BE score computation. In this paper, we did not set any threshold\u2014 we took all the BEs counted when calculating the BE scores for the sentences.\nTree Kernels Approach In order to calculate the syntactic similarity between the query and the sentence we first parse the sentence as well as the query into a syntactic tree (Moschitti, 2006) using a parser like Charniak (1999). Then we calculate the similarity between the two trees using the tree kernel. We reimplemented the tree kernel model as proposed by Moschitti et al. (2007).\nOnce we build the trees, our next task is to measure the similarity between the trees. For this, every tree T is represented by anm dimensional vector v(T ) = (v1(T ), v2(T ), \u00b7 \u00b7 \u00b7 vm(T )), where the i-th element vi(T ) is the number of occurrences of the i-th tree fragment in tree T . The tree fragments of a tree are all of its sub-trees which include at least one production with the restriction that no production rules can be broken into incomplete parts (Moschitti et al., 2007). Figure 1 shows an example tree and a portion of its subtrees.\nImplicitly we enumerate all the possible tree fragments 1, 2, \u00b7 \u00b7 \u00b7 ,m. These fragments are the axis of this m-dimensional space. Note that this could be done only implicitly since the number m is extremely large. Because of this, Collins and Duffy (2001) define the tree kernel algorithm whose computational complexity does not depend on m.\nThe tree kernel of two trees T1 and T2 is actually the inner product of v(T1) and v(T2):\nTK(T1, T2) = v(T1).v(T2) (2)\nWe define the indicator function Ii(n) to be 1 if the sub-tree i is seen rooted at node n and 0 otherwise. It follows:\nvi(T1) = \u2211\nn1\u2208N1 Ii(n1), vi(T2) =\n\u2211\nn2\u2208N2 Ii(n2) (3)\nWhere N1 and N2 are the set of nodes in T1 and T2 respectively. So, we can derive:\nTK(T1, T2) = v(T1).v(T2) = \u2211\ni\nvi(T1)vi(T2)\n= \u2211\nn1\u2208N1\n\u2211\nn2\u2208N2\n\u2211\ni\nIi(n1)Ii(n2)\n= \u2211\nn1\u2208N1\n\u2211\nn2\u2208N2 C(n1, n2) (4)\nwhere we define C(n1, n2) = \u2211\ni Ii(n1)Ii(n2). Next, we note that C(n1, n2) can be computed in polynomial time due to the following recursive definition:\n1. If the productions at n1 and n2 are different then C(n1, n2) = 0\n2. If the productions at n1 and n2 are the same, and n1 and n2 are pre-terminals, then C(n1, n2) = 1\n3. Else if the productions at n1 and n2 are not pre-terminals,\nC(n1, n2) =\nnc(n1) \u220f\nj=1\n(1 + C(ch(n1, j), ch(n2, j))) (5)\nwhere nc(n1) is the number of children of n1 in the tree; because the productions at n1 and n2 are the same we have nc(n1) = nc(n2). The i-th child-node of n1 is ch(n1, i).\nIn cases where the query is composed of two or more sentences we compute the similarity between the document sentence (s) and each of the query-sentences (qi) then we take the average of the scores as the syntactic feature value.\nSyntactic similarity value =\n\u2211n i=1 TK(qi, s)\nn\nWhere n is the number of sentences in the query q and s is the sentence under consideration. TK is the similarity value (tree kernel) between the sentence s and the query sentence q based on the syntactic structure. For example, for the following sentence s and query q we get the score:\nQuery (q): Describe steps taken and worldwide reaction prior to introduction of the Euro on January 1, 1999. Include predictions and expectations reported in the press.\nSentence (s): Europe\u2019s new currency, the euro, will rival the U.S. dollar as an international currency over the long term, Der Spiegel magazine reported Sunday.\nScores: 90, 41\nAverage Score: 65.5"}, {"heading": "4.1.2 Semantic Features", "text": "Though introducing syntactic information gives an improvement on BOW, by the use of syntactic parses, this too is not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs. Shallow semantic representations, bearing more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOWmodels (MacCartney et al., 2006; Moschitti et al., 2007).\nInitiatives such as PropBank (PB) (Kingsbury & Palmer, 2002) have made the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu, Pradhan, Ward, Martin, & Jurafsky, 2003) possible. Hence, attempting an application of SRL to QA seems natural as pinpointing the answer to a question relies on a deep understanding of the semantics of both. For example, consider the PB annotation:\n[ARG0 all][TARGET use][ARG1 the french franc][ARG2 as their currency]\nSuch annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g.\n[ARG0 the Vatican][TARGET use][ARG1 the Italian lira][ARG2 as their currency]\nIn order to calculate the semantic similarity between the sentences we first represent the annotated sentence (or query) using the tree structures like Figure 2 called Semantic Tree (ST) as proposed by Moschitti et al. (2007). In the semantic tree arguments are replaced with the most important word\u2013often referred to as the semantic head. We look for a noun first, then a verb, then an adjective, then adverb to find the semantic head in the argument. If none of these is present we take the first word of the argument as the semantic head.\nHowever, sentences rarely contain a single predicate, rather typically propositions contain one or more subordinate clauses. For instance, let us consider a slight modification of the second sentence: \u201cthe Vatican, located wholly within Italy uses the Italian lira as their currency.\u201d Here, the main predicate is \u201cuses\u201d and the subordinate predicate is \u201clocated\u201d. The SRL system outputs the following two annotations:\n(1) [ARG0 the Vatican located wholly within Italy][TARGET uses][ARG1 the Italian lira][ARG2 as their currency]\n(2) [ARG0 the Vatican][TARGET located] [ARGM-LOC wholly][ARGM-LOC within Italy] uses the Italian lira as their currency\ngiving the STs in Figure 3. As we can see in Figure 3(A), when an argument node corresponds to an entire subordinate clause we label its leaf with ST (e.g. the leaf of ARG0). Such ST node is actually the root of the subordinate clause in Figure 3(B). If taken separately, such STs do not express the whole meaning of the sentence. Hence, it is more accurate to define a single structure encoding the dependency between the two predicates as in Figure 3(C). We refer to this kind of nested STs as STNs.\nNote that the tree kernel (TK) function defined in Section 4.1.1 computes the number of common subtrees between two trees. Such subtrees are subject to the constraint that their nodes are taken with all or none of the children they have in the original tree. Though this definition of subtrees makes the TK function appropriate for syntactic trees, it is not well suited for the semantic trees (ST). For instance, although the two STs of Figure 2 share most of the subtrees rooted in the ST node, the kernel defined above computes no match.\nThe critical aspect of steps (1), (2), and (3) of the TK function is that the productions of two evaluated nodes have to be identical to allow the match of further descendants. This means that common substructures cannot be composed by a node with only some of its children as an effective ST representation would require. Moschitti et al. (2007) solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows portions of an ST to match.\nShallow Semantic Tree Kernel (SSTK) We reimplemented the SSTK according to the model given by Moschitti et al. (2007). The SSTK is based on two ideas: first, it changes\nthe ST, as shown in Figure 4 by adding SLOT nodes. These accommodate argument labels in a specific order with a fixed number of slots, possibly filled with null arguments that encode all possible predicate arguments. Leaf nodes are filled with the wildcard character * but they may alternatively accommodate additional information. The slot nodes are used in such a way that the adopted TK function can generate fragments containing one or more children like for example those shown in frames (b) and (c) of Figure 4. As previously pointed out, if the arguments were directly attached to the root node the kernel function would only generate the structure with all children (or the structure with no children, i.e. empty) (Moschitti et al., 2007).\nSecond, as the original tree kernel would generate many matches with slots filled with the null label we have set a new step 0 in the TK calculation: (0) if n1 (or n2) is a pre-terminal node and its child label is null, C(n1, n2) = 0;\nand subtract one unit to C(n1, n2), in step 3:\n(3) C(n1, n2) =\nnc(n1) \u220f\nj=1\n(1 + C(ch(n1, j), ch(n2, j))) \u2212 1 (6)\nThe above changes generate a new C which, when substituted (in place of original C ) in Eq. 4, gives the new SSTK.\nFor example, for the following sentence s and query q we get the semantic score:\nQuery (q): Describe steps taken and worldwide reaction prior to introduction of the Euro on January 1, 1999. Include predictions and expectations reported in the press.\nSentence (s): The Frankfurt-based body said in its annual report released today that it has decided on two themes for the new currency history of European civilization and abstract or concrete paintings.\nScores: 6, 12\nAverage Score: 9"}, {"heading": "4.2 Lexical Features", "text": "Here, we will discuss the lexical features that are most commonly used in the QA and summarization communities. We reimplemented all of them in this research."}, {"heading": "4.2.1 N-gram Overlap", "text": "N-gram overlap measures the overlapping word sequences between the candidate document sentence and the query sentence. With the view to measure the overlap scores, a query pool and a sentence pool are created. In order to create the query (or sentence) pool, we took the query (or document) sentence and created a set of related sentences by replacing its content words6 by their first-sense synonyms using WordNet. For example, given a stemmed document-sentence: \u201cJohn write a poem\u201d, the sentence pool contains: \u201cJohn compose a poem\u201d, \u201cJohn write a verse form\u201d along with the given sentence.\nWe measured the recall based n-gram scores for a sentence P using the following formula:\nNgramScore(P ) = maxi(maxj Ngram(si, qj)) (7)\nNgram(S,Q) =\n\u2211\ngramn\u2208S Countmatch (gramn) \u2211\ngramn\u2208S Count (gramn) (8)\nWhere n stands for the length of the n-gram (n = 1, 2, 3, 4), and Countmatch (gramn) is the number of n-grams co-occurring in the query and the candidate sentence, qj is the j-th sentence in the query pool, and si is the i-th sentence in the sentence pool of sentence P .\n1-gram Overlap Measure A 1-gram overlap score measures the number of words common in the sentence in hand and the query related words. This can be computed as follows:\n1gram Overlap Score =\n\u2211\nw1\u2208S Countmatch (w1) \u2211\nw1\u2208S Count (w1) (9)\nWhere S is the set of content words in the candidate sentence and Countmatch is the number of matches between the sentence content words and query related words. Count (gramn) is the number of w1.\nNote that in order to measure the 1-gram score we took the query related words instead of the exact query words. The motivation behind this is the sentence which has word(s) that are not exactly the query words but their synonyms, hypernyms, hyponym or gloss words, will get counted.\nExample:\nQuery Describe steps taken and worldwide reaction prior to introduction of the Euro on January 1, 1999. Include predictions and expectations reported in the press.\nSentence The Frankfurt-based body said in its annual study released today that it has decided on two themes for the new currency: history of European civilization and abstract or concrete paintings.\n6. hence forth content words are the nouns, verbs, adverbs and adjectives.\n1-gram Score 0.06666 (After normalization7).\nNote that the above sentence has a 1-gram overlap score of 0.06666 even though it has no exact word common with the query words. It got this score because the sentence word study is a synonym of the query word report.\nOther N-gram Overlap Measures\nAs above, we can calculate the other n-gram overlap scores. For example, considering the following query sentence and document sentence (From DUC 2007 collection), we have 4 matching 2-grams: (\u201c1 1999\u201d,\u201cof Euro\u201d, \u201con January\u201d and \u201cJanuary 1\u201d). Hence, employing the formula given above, we get the following 2-gram score after normalization. 3-gram score is also found accordingly.\nQuery Sentence: Describe steps taken and worldwide reaction prior to introduction of the Euro on January 1, 1999. Include predictions and expectations reported in the press.\nDocument Sentence: Despite skepticism about the actual realization of a single European currency as scheduled on January 1, 1999, preparations for the design of the Euro note have already begun.\n2-gram: 0.14815\n3-gram: 0.0800"}, {"heading": "4.2.2 LCS and WLCS", "text": "A sequence W = [w1, w2, ..., wn] is a subsequence of another sequence X = [x1, x2, ..., xm] , if there exists a strict increasing sequence [i1, i2, ..., in] of indices of X such that for all j = 1, 2, ..., n we have xij = wj (Cormen, Leiserson, & Rivest, 1989). Given two sequences S1 and S2, the longest common subsequence (LCS) of S1 and S2 is a common subsequence with maximum length (Lin, 2004).\nThe longer the LCS of two sentences is, the more similar the two sentences are. We used LCS-based F-measure to estimate the similarity between the document sentence S of length m and the query sentence Q of length n as follows:\nRlcs(S,Q) = LCS(S,Q)\nm (10)\nPlcs(S,Q) = LCS(S,Q)\nn (11)\nFlcs(S,Q) = (1\u2212 \u03b1)\u00d7 Plcs(S,Q) + \u03b1\u00d7Rlcs(S,Q) (12)\nWhere LCS(S,Q) is the length of a longest common subsequence of S and Q, and \u03b1 is a constant that determines the importance of precision and recall. While computing the LCS measure each document sentence and query sentence are viewed as a sequence of words.\n7. We normalize each of the feature values corresponding to a sentence with respect to the entire context of a particular document.\nThe intuition is that the longer the LCS of these two is the more similar they are. Here the recall (Rlcs(S,Q)) is the ratio of the length of the longest common subsequence of S and Q to the document sentence length that measures the completeness. Whereas the precision (Plcs(S,Q)) is the ratio of the length of the longest common subsequence of S and Q to the query sentence length which is a measure of exactness. To obtain the equal importance to precision and recall we set the value of \u03b1 as 0.5. Equation 12 is called the LCS-based F-measure. Notice that Flcs is 1 when, S=Q; and Flcs is 0 when there is nothing in common between S and Q.\nOne advantage of using LCS is that it does not require consecutive matches but insequence matches that reflect sentence level word order as n-grams. The other advantage is that it automatically includes longest in-sequence common n-grams. Therefore, no predefined n-gram length is necessary. Moreover, it has the property that its value is less than or equal to the minimum of the unigram (i.e. 1-gram) F-measure of S and Q. Unigram recall reflects the proportion of words in S that are also present in Q; while unigram precision is the proportion of words in Q that are also in S. Unigram recall and precision count all co-occurring words regardless of their orders; while LCS counts in-sequence co-occurrences.\nBy only awarding credit to in-sequence unigram matches, LCS measure also captures sentence level structure in a natural way. Consider the following example:"}, {"heading": "S1 John shot the thief", "text": "S2 John shot the thief\nS3 the thief shot John\nUsing S1 as reference sentence, and S2 and S3 as the sentences under consideration S2 and S3 would have the same 2-gram score since they both have one bigram (i.e. \u201cthe thief\u201d) in common with S1. However, S2 and S3 have very different meanings. In case of LCS S2 has a score of 3/4=0.75 and S3 has a score of 2/4=0.5 with \u03b1 = 0.5. Therefore, S2 is better than S3 according to LCS.\nHowever, LCS suffers one disadvantage in that it only counts the main in-sequence words; therefore, other alternative LCSes and shorter sequences are not reflected in the final score. For example, given the following candidate sentence:\nS4 the thief John shot\nUsing S1 as its reference, LCS counts either \u201cthe thief\u201d or \u201cJohn shot\u201d but not both; therefore, S4 has the same LCS score as S3 while 2-gram would prefer S4 over S3.\nIn order to measure the LCS score for a sentence we took a similar approach as the previous section using WordNet (i.e. creation of sentence pool and query pool). We calculated the LCS score using the following formula:\nLCS score = maxi(maxj Flcs(si, qj)) (13)\nWhere qj is the j-th sentence in the query pool, and si is the i-th sentence in the sentence pool.\nThe basic LCS has a problem in that it does not differentiate LCSes of different spatial relations within their embedding sequences (Lin, 2004). For example, given a reference sequence S and two candidate sequences Y1 and Y2 as follows:\nS: A B C D E F G\nY1 : A B C D H I K\nY2 : A H B K C I D\nY1 and Y2 have the same LCS score. However, Y1 should be better choice than Y2 because Y1 has consecutive matches. To improve the basic LCS method we can store the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS. We call it weighted LCS (WLCS) and use k to indicate the length of the current consecutive matches ending at words xi and yj. Given two sentences X and Y, the WLCS score of X and Y can be computed using the similar dynamic programming procedure as stated by Lin (2004). We use WLCS as it has the advantage of not measuring the similarity by taking the words in a higher dimension like string kernels which indeed reduces the time complexity. As before, we computed the WLCS-based F-measure in the same way using both the query pool and the sentence pool.\nWLCS score = maxi(maxj Fwlcs(si, qj)) (14)\nExample:\nQuery Sentence: Describe steps taken and worldwide reaction prior to introduction of the Euro on January 1, 1999. Include predictions and expectations reported in the press.\nDocument Sentence: Despite skepticism about the actual realization of a single European currency as scheduled on January 1, 1999, preparations for the design of the Euro note have already begun.\nWe find 6 matching strings: (\u201cof on 1 Euro 1999 January\u201d) in the longest common subsequence considering this sentence and related sentences. For WLCS we set the weight as 1.2. After normalization, we get the following LCS and WLCS scores for the sentence applying the above formula.\nLCS Score: 0.27586\nWLCS Score: 0.15961"}, {"heading": "4.2.3 Skip-Bigram Measure", "text": "A skip-bigram is any pair of words in their sentence order allowing for arbitrary gaps. Skipbigram measures the overlap of skip-bigrams between a candidate sentence and a query sentence (Lin, 2004). We rely on the query pool and the sentence pool as before using WordNet. Considering the following sentences:"}, {"heading": "S1 John shot the thief", "text": "S2 John shoot the thief\nS3 the thief shoot John\nS4 the thief John shot\nwe get that each sentence has C(4,2)=6 skip-bigrams8. For example, S1 has the following skip-bigrams: (\u201cJohn shot\u201d, \u201cJohn the\u201d, \u201cJohn thief\u201d, \u201cshot the\u201d, \u201cshot thief\u201d and \u201cthe thief\u201d) S2 has three skip bi-gram matches with S1 (\u201cJohn the\u201d, \u201cJohn thief\u201d, \u201cthe thief\u201d), S3 has one skip bi-gram match with S1 (\u201cthe thief\u201d), and S4 has two skip bi-gram matches with S1 (\u201cJohn shot\u201d, \u201cthe thief\u201d).\nThe skip bi-gram score between the document sentence S of length m and the query sentence Q of length n can be computed as follows:\nRskip2(S,Q) = SKIP2(S,Q)\nC(m, 2) (15)\nPskip2(S,Q) = SKIP2(S,Q)\nC(n, 2) (16)\nFskip2(S,Q) = (1\u2212 \u03b1)\u00d7 Pskip2(S,Q) + \u03b1\u00d7Rskip2(S,Q) (17)\nWhere SKIP2(S,Q) is the number of skip bi-gram matches between S and Q, and \u03b1 is a constant that determines the importance of precision and recall. We set the value of \u03b1 as 0.5 to associate the equal importance to precision and recall. C is the combination function. We call the equation 17 the skip bigram-based F-measure. We computed the skip bigram-based F-measure using the formula:\nSKIP BIGRAM = maxi(maxj Fskip2(si, qj)) (18)\nFor example, given the following query and the sentence, we get 8 skip-bigrams: (\u201con 1\u201d, \u201cJanuary 1\u201d, \u201cJanuary 1999\u201d, \u201cof Euro\u201d, \u201c1 1999\u201d, \u201con 1999\u201d, \u201con January\u201d and \u201cof on\u201d). Applying the equations above, we get skip bi-gram score of 0.05218 after normalization.\nQuery Describe steps taken and worldwide reaction prior to introduction of the Euro on January 1, 1999. Include predictions and expectations reported in the press.\nSentence Despite skepticism about the actual realization of a single European currency as scheduled on January 1, 1999, preparations for the design of the Euro note have already begun.\nSkip bi-gram Score: 0.05218\n8. C(n, r) = n! r!\u00d7(n\u2212r)!\nNote that skip bi-gram counts all in-order matching word pairs while LCS only counts one longest common subsequence. We can put the constraint on the maximum skip distance, dskip, between two in-order words to form a skip bi-gram which avoids the spurious matches like \u201cthe the\u201d or \u201cof from\u201d. For example, if we set dskip to 0 then it is equivalent to bi-gram overlap measure (Lin, 2004). If we set dskip to 4 then only word pairs of at most 4 words apart can form skip bi-grams. In our experiment we set dskip = 4 in order to ponder at most 4 words apart to get the skip bi-grams.\nModifying the equations: 15, 16, and 17 to allow the maximum skip distance limit is straightforward: following Lin (2004) we count the skip bi-gram matches, SKIP2(S,Q), within the maximum skip distance and replace the denominators of the equations with the actual numbers of within distance skip bi-grams from the reference sentence and the candidate sentence respectively."}, {"heading": "4.2.4 Head and Head Related-words Overlap", "text": "The number of head words common in between two sentences can indicate how much they are relevant to each other. In order to extract the heads from the sentence (or query), the sentence (or query) is parsed by Minipar9 and from the dependency tree we extract the heads which we call exact head words. For example, the head word of the sentence: \u201cJohn eats rice\u201d is \u201ceat\u201d.\nWe take the synonyms, hyponyms, and hypernyms10 of both the query-head words and the sentence-head words and form a set of words which we call head-related words. We measured the exact head score and the head-related score as follows:\nExactHeadScore =\n\u2211\nw1\u2208HeadSetCountmatch (w1) \u2211\nw1\u2208HeadSetCount (w1) (19)\nHeadRelatedScore =\n\u2211\nw1\u2208HeadRelSet Countmatch (w1) \u2211\nw1\u2208HeadRelSet Count (w1) (20)\nWhere HeadSet is the set of head words in the sentence and Countmatch is the number of matches between the HeadSet of the query and the sentence. HeadRelSet is the set of synonyms, hyponyms, and hypernyms of head words in the sentence and Countmatch is the number of matches between the head-related words of the query and the sentence. For example, below we list the head words for a query and a sentence and their measures:\nQuery: Describe steps taken and worldwide reaction prior to introduction of the Euro on January 1, 1999. Include predictions and expectations reported in the press.\nHeads for Query: include, reaction, step, take, describe, report, Euro, introduction, press, prediction, 1999, expectation\nSentence: The Frankfurt-based body said in its annual report released today that it has decided on two themes for the new currency: history of European civilization and abstract or concrete paintings.\n9. http://www.cs.ualberta.ca/ lindek/minipar.htm 10. hypernym and hyponym levels are restricted to 2 and 3 respectively.\nHeads for Sentence: history, release, currency, body, report,painting, say, civilization, theme, decide.\nExact Head Score: 111 = 0.09\nHead Related Score: 0"}, {"heading": "4.3 Lexical Semantic Features", "text": "We form a set of words which we call QueryRelatedWords by taking the content words from the query, their first-sense synonyms, the nouns\u2019 hypernyms/hyponyms, and the nouns\u2019 gloss definitions using WordNet."}, {"heading": "4.3.1 Synonym Overlap", "text": "The synonym overlap measure is the overlap between the list of synonyms of the content words extracted from the candidate sentence and query related words. This can be computed as follows:\nSynonym Overlap Score =\n\u2211\nw1\u2208SynSetCountmatch (w1) \u2211\nw1\u2208SynSetCount (w1) (21)\nWhere SynSet is the synonym set of the content words in the sentence and Countmatch is the number of matches between the SynSet and query related words."}, {"heading": "4.3.2 Hypernym/Hyponym Overlap", "text": "The hypernym/hyponym overlap measure is the overlap between the list of hypernyms (level 2) and hyponyms (level 3) of the nouns extracted from the sentence in consideration and query related words. This can be computed as follows:\nHypernym/hyponym overlap score =\n\u2211\nh1\u2208HypSetCountmatch (h1) \u2211\nh1\u2208HypSetCount (h1) (22)\nWhere HypSet is the hyponym/hyponym set of the nouns in the sentence and Countmatch is the number of matches between the HypSet and query related words."}, {"heading": "4.3.3 Gloss Overlap", "text": "The gloss overlap measure is the overlap between the list of content words that are extracted from the gloss definition of the nouns in the sentence in consideration and query related words. This can be computed as follows:\nGloss Overlap Score =\n\u2211\ng1\u2208GlossSetCountmatch (g1) \u2211\ng1\u2208GlossSetCount (g1) (23)\nWhere GlossSet is the set of content words (i.e. nouns, verbs and adjectives) taken from the gloss definition of the nouns in the sentence and Countmatch is the number of matches between the GlossSet and query related words.\nExample: For example, given the query the following sentence gets synonym overlap score of 0.33333, hypernym/hyponym overlap score of 0.1860465 and gloss overlap score of 0.1359223.\nQuery Describe steps taken and worldwide reaction prior to introduction of the Euro on January 1, 1999. Include predictions and expectations reported in the press.\nSentence The Frankfurt-based body said in its annual report released today that it has decided on two themes for the new currency: history of European civilization and abstract or concrete paintings.\nSynonym Overlap Score: 0.33333\nHypernym/Hyponym Overlap Score: 0.1860465\nGloss Overlap Score: 0.1359223"}, {"heading": "4.4 Statistical Similarity Measures", "text": "Statistical similarity measures are based on the co-occurrence of similar words in a corpus. Two words are termed as similar if they belong to the same context. We used the thesaurus provided by Dr. Dekang Lin11 for these purpose. We have used two statistical similarity measures:\nDependency-based similarity measure This method uses the dependency relations among words in order to measure the similarity (Lin, 1998b). It extracts the dependency triples and then uses a statistical approach to measure the similarity. Using the given corpus one can retrieve the most similar words for a given word. The similar words are grouped into clusters.\nNote that for a word there can be more than one cluster. Each cluster represents the sense of the word and its similar words for that sense. So, selecting the right cluster for a word is itself a problem. Our goals are: i) to create a bag of similar words to the query words and ii) once we get the bag of similar words (dependency based) for the query words to measure the overlap score between it and the sentence words.\nCreating Bag of Similar Words: For each query-word we extract all of its clusters from the thesaurus. Now in order to determine the right cluster for a query word we measure the overlap score between the query related words (i.e. exact words, synonyms, hypernyms/hyponyms and gloss) and the clusters. The hypothesis is that the cluster that has more words in common with the query related words is the right cluster under the assumption that the first synonym is the correct sense. We choose the cluster for a word which has the highest overlap score.\nOverlap scorei =\n\u2211\nw1\u2208QueryRelatedWordsCountmatch (w1) \u2211\nw1\u2208QueryRelatedWordsCount (w1) (24) Cluster = argmaxi(Overlap Scorei) (25)\n11. http://www.cs.ualberta.ca/ lindek/downloads.htm\nwhere QueryRelatedWords is the set of exact words, synonyms, hyponyms/hypernyms, and gloss words for the words in the query (i.e query words) and Countmatch is the number of matches between the query related words and the ith cluster of similar words.\nMeasuring Overlap Score:\nOnce we get the clusters for the query words we measured the overlap between the cluster words and the sentence words which we call dependency based similarity measure:\nDependencyMeasure =\n\u2211\nw1\u2208SenWordsCountmatch (w1) \u2211\nw1\u2208SenWordsCount (w1) (26)\nWhere SenWords is the set of words for the sentence and Countmatch is the number of matches between the sentence words and the cluster of similar words.\nProximity-based similarity measure\nThis similarity is computed based on the linear proximity relationship between words only (Lin, 1998a). It uses the information theoretic definition of similarity to measure the similarity. The similar words are grouped into clusters. We took the similar approach to measure this feature as the previous section except that we used a different thesaurus.\nExample: Considering the following query and sentence we get the following measures:\nQuery: Describe steps taken and worldwide reaction prior to introduction of the Euro on January 1, 1999. Include predictions and expectations reported in the press.\nSentence: The Frankfurt-based body said in its annual report released today that it has decided on two themes for the new currency: history of European civilization and abstract or concrete paintings.\nDependency-based Similarity Score: 0.0143678\nProximity-based Similarity Score: 0.04054054"}, {"heading": "4.5 Graph-based Similarity Measure", "text": "Erkan and Radev (2004) used the concept of graph-based centrality to rank a set of sentences for producing generic multi-document summaries. A similarity graph is produced for the sentences in the document collection. In the graph each node represents a sentence. The edges between nodes measure the cosine similarity between the respective pair of sentences. The degree of a given node is an indication of how important the sentence is. Figure 5 shows an example of a similarity graph for 4 sentences.\nOnce the similarity graph is constructed, the sentences are ranked according to their eigenvector centrality. The LexRank performed well in the context of generic summarization. To apply LexRank to query-focused context a topic-sensitive version of LexRank is proposed by Otterbacher et al. (2005). We followed a similar approach in order to calculate this feature. The score of a sentence is determined by a mixture model of the relevance of the sentence to the query and the similarity of the sentence to other high-scoring sentences.\nRelevance to the Question We first stem out all the sentences in the collection and compute the word IDFs (Inverse Document Frequency) using the following formula:\nidfw = log\n(\nN + 1\n0.5 + sfw\n)\n(27)\nWhere N is the total number of sentences in the cluster, and sfw is the number of sentences that the word w appears in.\nWe also stem out the questions and remove the stop words. The relevance of a sentence s to the question q is computed by:\nrel(s|q) = \u2211 w\u2208q log (tfw,s + 1)\u00d7 log (tfw,q + 1)\u00d7 idfw (28)\nWhere tfw,s and tfw,q are the number of times w appears in s and q, respectively.\nMixture Model In the previous section we measured the relevance of a sentence to the question but a sentence that is similar to the high scoring sentences in the cluster should also have a high score. For instance, if a sentence that gets a high score based on the question relevance model is likely to contain an answer to the question then a related sentence, which may not be similar to the question itself, is also likely to contain an answer (Otterbacher et al., 2005).\nWe capture this idea by the following mixture model:\np(s|q) = { d\u00d7 rel(s|q)\u2211 z\u2208C rel(z|q) + (1\u2212 d)\u00d7 \u2211\nv\u2208C\nsim(s, v) \u2211\nz\u2208C sim(z, v)\n}\n\u00d7 p(v|q) (29)\nWhere p(s|q), the score of a sentence s given a question q, is determined as the sum of its relevance to the question and the similarity to the other sentences in the collection. C is the set of all sentences in the collection. The value of the parameter d which we call\n\u201cbias\u201d is a trade-off between two terms in the equation and is set empirically. For higher values of d we prefer the relevance to the question to the similarity to other sentences. The denominators in both terms are for normalization. Although it is computationally expensive, equation 29 calculates the sum over the entire collection since it is required for the model to sense the global impact through the voting of all sentences. We measure the cosine similarity weighted by word IDFs as the similarity between two sentences in a cluster:\nsim(x, y) =\n\u2211 w\u2208x,y tfw,x \u00d7 tfw,y \u00d7 (idfw)2 \u221a\n\u2211 xi\u2208x (tfxi,x \u00d7 idfxi) 2 \u00d7\n\u221a\n\u2211 yi\u2208y (tfyi,y \u00d7 idfyi) 2\n(30)\nEquation 29 can be written in matrix notation as follows:\np = [dA+ (1\u2212 d)B]Tp (31)\nA is the square matrix such that for a given index i, all the elements in the i-th column are proportional to rel(i|q). B is also a square matrix such that each entry B(i,j) is proportional to sim(i,j). Both matrices are normalized so that row sums add up to 1. Note that as a result of this normalization all rows of the resulting square matrix Q = [dA + (1 \u2212 d)B] also add up to 1. Such a matrix is called stochastic and defines a Markov chain. If we view each sentence as a state in a Markov chain then Q(i,j) specifies the transition probability from state i to state j in the corresponding Markov chain. The vector p we are looking for in Eq. 31 is the stationary distribution of the Markov chain. An intuitive interpretation of the stationary distribution can be understood by the concept of a random walk on the graph representation of the Markov chain. With probability d a transition is made from the current node to the nodes that are similar to the query. With probability (1-d) a transition is made to the nodes that are lexically similar to the current node. Every transition is weighted according to the similarity distributions. Each element of the vector p gives the asymptotic probability of ending up at the corresponding state in the long run regardless of the starting state. The stationary distribution of a Markov chain can be computed by a simple iterative algorithm called power method (Erkan & Radev, 2004). It starts with a uniform distribution. At each iteration the eigenvector is updated by multiplying with the transpose of the stochastic matrix. Since the Markov chain is irreducible and aperiodic the algorithm is guaranteed to terminate."}, {"heading": "5. Ranking Sentences", "text": "We use several methods in order to rank sentences to generate summaries applying the features described in Section 4. In this section we will describe the systems in detail."}, {"heading": "5.1 Learning Feature-weights: A Local Search Strategy", "text": "In order to fine-tune the weights of the features, we have used a local search technique. Initially we set all the feature-weights, w1, \u00b7 \u00b7 \u00b7 , wn, as equal values (i.e. 0.5) (see Algorithm 1). Then we train the weights using the DUC 2006 data set. Based on the current weights we score the sentences and generate summaries accordingly. We evaluate the summaries using\nInput: Stepsize l, Weight Initial Value v Output: A vector ~w of learned weights Initialize the weight values wi to v. for i \u2190 1 to n do\nrg1 = rg2 = prev = 0 while (true) do\nscoreSentences(~w) generateSummaries() rg2 = evaluateROUGE() if rg1 \u2264 rg2 then\nprev = wi wi+ = l rg1 = rg2\nelse break end\nend\nend return ~w\nAlgorithm 1: Tuning weights using Local Search technique\nthe automatic evaluation tool ROUGE (Lin, 2004) (described in Section 7) and the ROUGE value works as the feedback to our learning loop. Our learning system tries to maximize the ROUGE score in every step by changing the weights individually by a specific step size (i.e. 0.01). That means, to learn weight wi we change the value of wi keeping all other weight values (wj\u2200j 6=i) stagnant. For each weight wi the algorithm achieves the local maximum (i.e. hill climbing) of ROUGE value.\nOnce we have learned the feature-weights we compute the final scores for the sentences using the formula:\nscorei = ~xi. ~w (32)\nWhere ~xi is the feature vector for i-th sentence, ~w is the weight vector, and scorei is the score of i-th sentence."}, {"heading": "5.2 Statistical Machine Learning Approaches", "text": "We experimented with two unsupervised statistical learning techniques with the features extracted in the previous section for the sentence selection problem:\n1. K-means learning\n2. Expectation Maximization (EM) learning"}, {"heading": "5.2.1 The K-means Learning", "text": "K-means is a hard clustering algorithm that defines clusters by the center of mass of their members. We start with a set of initial cluster centers that are chosen randomly and go\nthrough several iterations of assigning each object to the cluster whose center is closest. After all objects have been assigned we recompute the center of each cluster as the centroid or mean (\u00b5) of its members. The distance function we use is squared Euclidean distance instead of the true Euclidean distance.\nSince the square root is a monotonically growing function squared Euclidean distance has the same result as the true Euclidean distance but the computation overload is smaller when the square root is dropped.\nOnce we have learned the means of the clusters using the K-means algorithm our next task is to rank the sentences according to a probability model. We have used Bayesian model in order to do so. Bayes\u2019 law says:\nP (qk|x,\u0398) = p(x|qk,\u0398)P (qk|\u0398)\np(x|\u0398)\n= p(x|qk,\u0398)P (qk|\u0398)\n\u2211K k=1 p(x|qk,\u0398)p(qk|\u0398)\n(33)\nwhere qk is a cluster, x is a feature vector representing a sentence, and \u0398 is the parameter set of all class models. We set the weights of the clusters as equiprobable (i.e. P (qk|\u0398) = 1/K). We calculated p(x|qk,\u0398) using the gaussian probability distribution. The gaussian probability density function (pdf) for the d-dimensional random variable x is given by:\np(\u00b5,\u03a3)(x) = e\n\u22121 2 (x\u2212\u00b5)T\u03a3\u22121(x\u2212\u00b5)\n\u221a 2\u03c0 d\u221a det(\u03a3)\n(34)\nwhere \u00b5, the mean vector, and \u03a3, the covariance matrix, are the parameters of the gaussian distribution. We get the means (\u00b5) from the K-means algorithm and we calculate the covariance matrix using the unbiased covariance estimation procedure:\n\u03a3\u0302j = 1 N \u2212 1 N \u2211\ni=1\n(xi \u2212\u00b5j)(xi \u2212\u00b5j)T (35)"}, {"heading": "5.2.2 The EM Learning", "text": "The EM algorithm for gaussian mixture models is a well known method for cluster analysis. A useful outcome of this model is that it produces a likelihood value of the clustering model and the likelihood values can be used to select the best model from a number of different models providing that they have the same number of parameters (i.e. same number of clusters).\nInput: A sample of n data-points (x) each represented by a feature vector of length L Input: Number of Clusters K Output: An array S of K-means-based Scores Data: Array dnK , \u00b5K , \u03a3K Data: Array CK , ynK Randomly choose K data-points as K initial means: \u00b5k, k = 1, \u00b7 \u00b7 \u00b7 ,K. repeat\nfor i \u2190 1 to n do for j \u2190 1 to K do\ndij = \u2016xi \u2212\u00b5j\u20162 = (xi \u2212\u00b5j)T (xi \u2212\u00b5j)\nend if dik < dil,\u2200l 6= k then\nassign xi to Ck. end\nend for i \u2190 1 to K do\n\u00b5i =\n\u2211\nxj\u2208Ci xj\n|C i| end\nuntil no further change occurs ; /* calculating the covariances for each cluster */ for i \u2190 1 to K do m = |C i| for j \u2190 1 to m do\n\u03a3i + = (C ij \u2212\u00b5i) \u2217 (C ij \u2212\u00b5i)T end \u03a3i \u2217 = (1/(m\u2212 1))\nend /* calculating the scores for sentences */ for i \u2190 1 to n do for j \u2190 1 to K do\nyij = e \u22121 2 (xi\u2212\u00b5j) T \u03a3\u22121 j (xi\u2212\u00b5j)\n\u221a 2\u03c0 d\u221a det(\u03a3j)\nend for j \u2190 1 to K do\nzij = (yij \u2217 wj)/ \u2211K j=1 yij \u2217 wj ; // where, wj = 1/K end m = max(\u00b5k) \u2200k Push zim to S\nend return S\nAlgorithm 2: Computing K-means based similarity measure\nA significant problem with the EM algorithm is that it converges to a local maximum of the likelihood function and hence the quality of the result depends on the initialization. This problem along with a method for improving the initialization is discussed later in this section.\nEM is a \u201csoft\u201d version of the K-means algorithm described above. As with K-means we start with a set of random cluster centers c1 \u00b7 \u00b7 \u00b7 ck. In each iteration we do a soft assignment of the data-points to every cluster by calculating their membership probabilities. EM is an iterative two step procedure: 1. Expectation-step and 2. Maximization-step. In the expectation step we compute expected values for the hidden variables hi,j which are cluster membership probabilities. Given the current parameters we compute how likely it is that an object belongs to any of the clusters. The maximization step computes the most likely parameters of the model given the cluster membership probabilities.\nThe data-points are considered to be generated by a mixture model of k-gaussians of the form:\nP (x) = k \u2211\ni=1\nP (C = i)P (x|C = i) = k \u2211\ni=1\nP (C = i)P (x|\u00b5i,\u03a3i) (36)\nwhere the total likelihood of model \u0398 with k components, given the observed data points X = x1, \u00b7 \u00b7 \u00b7 ,xn, is:\nL(\u0398|X) = n \u220f\ni=1\nk \u2211\nj=1\nP (C = j)P (xi|\u0398j) = n \u220f\ni=1\nk \u2211\nj=1\nwjP (xi|\u00b5j,\u03a3j) (37)\n\u21d4 n \u2211\ni=1\nlog k \u2211\nj=1\nwjP (xi|\u00b5j,\u03a3j) ( taking the log likelihood ) (38)\nwhere P is the probability density function (i.e. eq 34). \u00b5j and \u03a3j are the mean and covariance matrix of component j, respectively. Each component contributes a proportion, wj , of the total population such that: \u2211K j=1wj = 1.\nLog likelihood can be used instead of likelihood as it turns the product into a sum. We describe the EM algorithm for estimating a gaussian mixture.\nSingularities The covariance matrix \u03a3 above must be non-singular or invertible. The EM algorithm may converge to a position where the covariance matrix becomes singular (|\u03a3| = 0) or close to singular, that means it is not invertible anymore. If the covariance matrix becomes singular or close to singular then EM may result in wrong clusters. We restrict the covariance matrices to become singular by testing these cases at each iteration of the algorithm as follows:\nif ( \u221a |\u03a3| > 1e\u22129) then update \u03a3 else do not update \u03a3\nDiscussion: Starting values for the EM algorithm\nThe convergence rate and success of clustering using the EM algorithm can be degraded by a poor choice of starting values for the means, covariances, and weights of the components. We experimented with one summary (for document number D0703A from DUC 2007) in order to test the impact of these initial values on the EM algorithm. The cluster means are initialized with a heuristic that spreads them randomly around Mean(DATA) with standard deviation \u221a\nCov(DATA) \u2217 10. Their initial covariance is set to Cov(DATA) and the initial values of the weights are wj = 1/K where K is the number of clusters.\nThat is, for d-dimensional data-points the parameters of j\u2212th component are as follows:\n~\u00b5j = rand(1, \u00b7 \u00b7 \u00b7 , d) \u2217 \u221a\n\u03a3(DATA) \u2217 10 + ~\u00b5(DATA) \u03a3j = \u03a3(DATA)\nwj = 1/K\nThe highly variable nature of the results of the tests is reflected in the very inconsistent values for the total log likelihood and the results of repeated experiments indicated that using random starting values for initial estimates of the means frequently gave poor results. There are two possible solutions to this problem. In order to get good results from using random starting values (as specified by the algorithm) we will run the EM algorithm several times and choose the initial configuration for which we get the maximum log likelihood among all configurations. Choosing the best one among several runs is a very computer intensive process. So, to improve the outcome of the EM algorithm on gaussian mixture models, it is necessary to find a better method of estimating initial means for the components.\nThe best starting position for the EM algorithm, in regard to the estimates of the means, would be to have one estimated mean per cluster which is closer to the true mean of that cluster.\nTo achieve this aim we explored the widely used \u201cK-means\u201d algorithm as a cluster (means) finding method. That is, the means found by the K-means clustering above will be utilized as the initial means for the EM and we calculate the initial covariance matrices using the unbiased covariance estimation procedure (Equation 35).\nRanking the Sentences\nOnce the sentences are clustered by the EM algorithm, we identify the sentences which are question-relevant by checking their probabilities, P (qr|xi,\u0398) where qr denotes the cluster \u201cquestion-relevant\u201d. If for a sentence xi, P (qr|xi,\u0398) > 0.5 then xi is considered to be question-relevant. The cluster which has the mean values greater than the other one is considered as the question-relevant cluster.\nOur next task is to rank the question-relevant sentences in order to include them in the summary. This can be done easily by multiplying the feature vector ~xi with the weight vector ~w that we learned by applying the local search technique (Equation 32).\nInput: A Sample of n data-points ( x) each represented by a feature vector of length L Input: Number of Clusters K Output: An array S of EM-based Scores Start with K initial Gaussian models: N(\u00b5k,\u03a3k) k = 1, \u00b7 \u00b7 \u00b7 ,K, with equal priors set to P (qk) = 1/K. repeat\n/* Estimation step: compute the probability P (q (i) k |xj,\u0398(i)) for each\ndata point xj, j = 1, \u00b7 \u00b7 \u00b7 , n, to belong to the class q(i)k */ for j \u2190 1 to n do\nfor k \u2190 1 to K do\nP (q (i) k |xj,\u0398(i)) =\nP (q (i) k |\u0398(i))p(xj|q (i) k ,\u0398 (i))\np(xj|\u0398(i))\n= P (q\n(i) k |\u0398(i))p(xj|\u00b5 (i) k ,\u03a3 (i) k )\n\u2211K k=1 P (q (i) k |\u0398(i))p(xj|\u00b5 (i) k ,\u03a3 (i) k )\nend\nend /* Maximization step: */ for k \u2190 1 to K do for j \u2190 1 to n do\n// update the means:\n\u00b5i+1k =\n\u2211n j=1xjP (q (i) k |xj ,\u0398(i))\n\u2211n j=1 P (q (i) k |xj,\u0398(i))\n// update the variances:\n\u03a3 (i+1) k =\n\u2211n j=1 P (q (i) k |xj ,\u0398(i))(xj \u2212\u00b5 (i+1) k )(xj \u2212\u00b5 (i+1) k ) T\n\u2211N j=1 P (q (i) k |xj,\u0398(i))\n// update the priors:\nP (qk(i+ 1)|\u0398(i+1)) = 1\nn\nn \u2211\nj=1\nP (q (i) k |xj,\u0398(i))\nend\nend until the total likelihood increase falls under some desired threshold ; return S\nAlgorithm 3: Computing EM-based similarity measure"}, {"heading": "6. Redundancy Checking and Generating Summary", "text": "Once the sentences are scored the easiest way to create summaries is just to output the topmost N sentences until the required summary length is reached. In that case, we are ignoring other factors: such as redundancy and coherence.\nAs we know that text summarization clearly entails selecting the most salient information and putting it together in a coherent summary. The answer or summary consists of multiple separately extracted sentences from different documents. Obviously, each of the selected text snippets should individually be important. However, when many of the competing sentences are included in the summary the issue of information overlap between parts of the output comes up and a mechanism for addressing redundancy is needed. Therefore, our summarization systems employ two levels of analysis: first a content level where every sentence is scored according to the features or concepts it covers, and second a textual level, when, before being added to the final output, the sentences deemed to be important are compared to each other and only those that are not too similar to other candidates are included in the final answer or summary. Goldstein, Kantrowitz, Mittal, and Carbonell (1999) observed this in what the authors called \u201cMaximum-Marginal-Relevance (MMR)\u201d. Following Hovy et al. (2006) we modeled this by BE overlap between an intermediate summary and a to-be-added candidate summary sentence.\nWe call this overlap ratio R, where R is between 0 and 1 inclusively. Setting R = 0.7 means that a candidate summary sentence, s, can be added to an intermediate summary, S, if the sentence has a BE overlap ratio less than or equal to 0.7."}, {"heading": "7. Experimental Evaluation", "text": "This section describes the results of experiments conducted using DUC12 2007 dataset provided by NIST 13. Some of the questions these experiments address include:\n\u2022 How do the different features affect the behavior of the summarizer system?\n\u2022 Which one of the algorithms (K-means, EM and Local Search) performs better for this particular problem?\nWe used the main task of DUC 2007 for evaluation. The task was:\n\u201cGiven a complex question (topic description) and a collection of relevant documents, the task is to synthesize a fluent, well-organized 250-word summary of the documents that answers the question(s) in the topic.\u201d\nThe documents of DUC 2007 came from the AQUAINT corpus comprising newswire articles from the Associated Press and New York Times (1998-2000) and Xinhua News Agency (1996-2000). NIST assessors developed topics of interest to them and choose a set of 25 documents relevant (document cluster) to each topic. Each topic and its document cluster were given to 4 different NIST assessors including the developer of the topic. The assessor created a 250-word summary of the document cluster that satisfies the information\n12. http://www-nlpir.nist.gov/projects/duc/ 13. National Institute of Standards and Technology\nneed expressed in the topic statement. These multiple \u201creference summaries\u201d are used in the evaluation of summary content.\nThe purpose of our experiments is to study the impact of different features. To accomplish this we generated summaries for the 45 topics of DUC 2007 by each of our seven systems defined as below:\n\u2022 The LEX system generates summaries based on only lexical features (Section 4.2): n-gram (n=1,2,3,4), LCS, WLCS, skip bi-gram, head, head synonym and BE overlap.\n\u2022 The LEXSEM system considers only lexical semantic features (Section 4.3): synonym, hypernym/hyponym, gloss, dependency-based and proximity-based similarity.\n\u2022 The SYN system generates summary based on only syntactic feature (Section 4.1.1).\n\u2022 The COS system generates summary based on the graph-based method (Section 4.5).\n\u2022 The SYS1 system considers all the features except the syntactic and semantic features (All features except section 4.1).\n\u2022 The SYS2 system considers all the features except the semantic feature (All features except section 4.1.2) and\n\u2022 TheALL system generates summaries taking all the features (Section 4) into account."}, {"heading": "7.1 Automatic Evaluation", "text": "ROUGE We carried out automatic evaluation of our summaries using the ROUGE (Lin, 2004) toolkit, which has been widely adopted by DUC for automatic summarization evaluation. ROUGE stands for \u201cRecall-Oriented Understudy for Gisting Evaluation\u201d. It is a collection of measures that determines the quality of a summary by comparing it to reference summaries created by humans. The measures count the number of overlapping units such as n-gram, word-sequences, and word-pairs between the system-generated summary to be evaluated and the ideal summaries created by humans. The available ROUGE measures are: ROUGE-N (N=1,2,3,4), ROUGE-L, ROUGE-W and ROUGE-S. ROUGE-N is n-gram recall between a candidate summary and a set of reference summaries. ROUGE-L measures the longest common subsequence (LCS) which takes into account sentence level structure similarity naturally and identifies longest co-occurring insequence n-grams automatically. ROUGE-W measures the weighted longest common subsequence (WLCS) providing an improvement to the basic LCS method of computation to credit the sentences having the consecutive matches of words. ROUGE-S is the overlap of skip-bigrams between a candidate summary and a set of reference summaries where skip-bigram is any pair of words in their sentence order allowing for arbitrary gaps. Most of these ROUGE measures have been applied in automatic evaluation of summarization systems and achieved very promising results (Lin, 2004).\nFor all our systems, we report the widely accepted important metrics: ROUGE-2 and ROUGE-SU. We also present the ROUGE-1 scores since this has never been shown to not correlate with human judgement. All the ROUGE measures were calculated by running\nROUGE-1.5.5 with stemming but no removal of stopwords. ROUGE run-time parameters were set as the same as DUC 2007 evaluation setup. They are:\nROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2 -m -l 250 -a\nWe also show 95% confidence interval of the important evaluation metrics for our systems to report significance for doing meaningful comparison. We use the ROUGE tool for this purpose. ROUGE uses a randomized method named bootstrap resampling to compute the confidence interval. We used 1000 sampling points in the bootstrap resampling.\nWe report the evaluation scores of one baseline system (The BASE column) in each of the tables in order to show the level of improvement our systems achieve. The baseline system generates summaries by returning all the leading sentences (up to 250 words) in the \u3008TEXT \u3009 field of the most recent document(s).\nWhile presenting the results we highlight the top two F-scores and bottom one F-score to indicate significance at a glance."}, {"heading": "7.1.1 Results and Discussion", "text": "The K-means Learning Table 1 shows the ROUGE-1 scores for different combinations of features in the K-means learning. It is noticeable that the K-means performs best for the graph-based cosine similarity feature. Note that including syntactic feature does not improve the score. Also, including syntactic and semantic features increases the score but not by a significant amount. Summaries based on only lexical features give us good ROUGE-1 evaluation.\nTable 2 shows the ROUGE-2 scores for different combinations of features in the K-means learning. Just like ROUGE-1 graph-based cosine similarity feature performs well here. We get a significant improvement in ROUGE-2 score when we include syntactic feature with all other features. Semantic features do not affect the score much. Lexical Semantic features perform well here.\nAs Table 3 shows: ROUGE-SU scores are the best for all features without syntactic and semantic. Including syntactic/semantic features with other features degrades the scores. Summaries based on only lexical features achieve good scores.\nTable 4 shows the 95% confidence interval (for F-measures in K-means learning) of the important ROUGE evaluation metrics for all our systems in comparison to the confidence interval of the baseline system. It can be seen that our systems have performed significantly better than the baseline system in most of the cases.\nThe EM learning Table 5 to Table 7 show different ROUGE measures for the feature combinations in the context of the EM learning. It can be easily noticed that for all these measures we get significant amount of improvement in ROUGE scores when we include syntactic and semantic features along with other features. We get 3-15% improvement over SYS1 in F-score when we include syntactic feature and 2-24% improvement when we include syntactic and semantic features. The cosine similarity measure does not perform as well as it did in the K-means experiments. Summaries considering only the lexical features achieve good results.\nTable 8 shows the 95% confidence interval (for F-measures in EM learning) of the important ROUGE evaluation metrics for all our systems in comparison to the confidence interval of the baseline system. We can see that our systems have performed significantly better than the baseline system in most of the cases.\nLocal Search Technique The ROUGE scores based on the feature combinations are given in Table 9 to Table 11. Summaries generated by including all features perform the\nbest scores for all the measures. We get 7-15% improvement over SYS1 in F-score when we include syntactic feature and 8-19% improvement over SYS1 in F-score when we include syntactic and semantic features. In this case also lexical features (LEX) perform well but not better than all features (ALL).\nTable 12 shows the 95% confidence interval (for F-measures in local search technique) of the important ROUGE evaluation metrics for all our systems in comparison to the confidence interval of the baseline system. We find that our systems have performed significantly better than the baseline system in most of the cases."}, {"heading": "7.1.2 Comparison", "text": "From the results reported above we can see for all three algorithms our systems clearly outperform the baseline system. Table 13 shows the F-scores of the reported ROUGE measures while Table 14 reports the 95% confidence intervals for the baseline system, the best system in DUC 2007, and our three techniques taking all features (ALL) into consideration. We can see that the method based on local search technique outperforms the other two and the EM algorithm performs better than the K-means algorithm. If we analyze deeply, we find that in all cases but ROUGE-SU with local search the confidence intervals do not overlap with the best DUC 2007 system."}, {"heading": "7.2 Manual Evaluation", "text": "For a sample of 105 summaries14 drawn from our different systems\u2019 generated summaries we conduct an extensive manual evaluation in order to analyze the effectiveness of our approaches. The manual evaluation comprised a Pyramid-based evaluation of contents and a user evaluation to get the assessment of linguistic quality and overall responsiveness."}, {"heading": "7.2.1 Pyramid Evaluation", "text": "In the DUC 2007 main task, 23 topics were selected for the optional community-based pyramid evaluation. Volunteers from 16 different sites created pyramids and annotated the peer summaries for the DUC main task using the given guidelines15. 8 sites among them created the pyramids. We used these pyramids to annotate our peer summaries to compute the modified pyramid scores16. We used the DUCView.jar17 annotation tool for this purpose. Table 15 to Table 17 show the modified pyramid scores of all our systems for the three algorithms. A baseline system\u2019s score is also reported. The peer summaries of the baseline system are generated by returning all the leading sentences (up to 250 words) in the \u3008TEXT \u3009 field of the most recent document(s). From these results we see that all our systems perform better than the baseline system and inclusion of syntactic and semantic features yields better scores. For all three algorithms we can also notice that the lexical semantic features are the best in terms of modified pyramid scores."}, {"heading": "7.2.2 User Evaluation", "text": "10 university graduate students judged the summaries for linguistic quality and overall responsiveness. The given score is an integer between 1 (very poor) and 5 (very good) and is guided by consideration of the following factors: 1. Grammaticality, 2. Non-redundancy, 3. Referential clarity, 4. Focus and 5. Structure and Coherence. They also assigned a content responsiveness score to each of the automatic summaries. The content score is an integer between 1 (very poor) and 5 (very good) and is based on the amount of information in the summary that helps to satisfy the information need expressed in the topic narrative. These measures were used at DUC 2007. Table 18 to Table 20 present the average linguistic quality and overall responsive scores of all our systems for the three algorithms. The same baseline system\u2019s scores are given for meaningful comparison. From a closer look at these results, we find that most of our systems perform worse than the baseline system in terms of linguistic quality but achieve good scores in case of overall responsiveness. It is also obvious from the tables that the exclusion of syntactic and semantic features often causes lower scores. On the other hand, lexical and lexical semantic features show good overall responsiveness scores for all three algorithms.\n14. We have 7 systems for each of the 3 algorithms, cumulatively we have 21 systems. Randomly we chose 5 summaries for each of these 21 systems. 15. http://www1.cs.columbia.edu/ becky/DUC2006/2006-pyramid-guidelines.html 16. This equals the sum of the weights of the Summary Content Units (SCUs) that a peer summary matches,\nnormalized by the weight of an ideally informative summary consisting of the same number of contributors as the peer.\n17. http://www1.cs.columbia.edu/ ani/DUC2005/Tool.html"}, {"heading": "8. Conclusion and Future Work", "text": "In this paper we presented our works on answering complex questions. We extracted eighteen important features for each of the sentences in the document collection. Later we used a simple local search technique to fine-tune the feature weights. For each weight, wi, the algorithm achieves the local maximum of the ROUGE value. In this way, once we learn the weights we rank the sentences by multiplying the feature-vector with the weight-vector. We also experimented with two unsupervised learning techniques: 1) EM and 2) K-means with the features extracted. We assume that we have two clusters of sentences: 1. queryrelevant and 2. query-irrelevant. We learned the means of the clusters using the K-means algorithm then we used Bayesian model in order to rank the sentences. The learned means in the K-means algorithm are used as the initial means in the EM algorithm. We applied the EM algorithm to cluster the sentences into two classes : 1) query-relevant and 2) query-irrelevant. We take out the query-relevant sentences and rank them using the learned weights (i.e. in local search). For each of our methods of generating summaries we filter out the redundant sentences using a redundancy checking module and generate summaries by taking the top N sentences.\nWe also experimented with the effects of different kinds of features. We evaluated our systems automatically using ROUGE and report the significance of our results through 95% confidence intervals. We conducted two types of manual evaluation: 1) Pyramid and 2) User Evaluation to further analyze the performance of our systems. Our experimental results mostly show the following: (a) our approaches achieve promising results, (b) the empirical approach based on a local search technique outperforms the other two learning techniques and EM performs better than the K-means algorithm, (c) our systems achieve better results when we include the tree kernel based syntactic and semantic features, and (d) in all cases but ROUGE-SU with local search the confidence intervals do not overlap with the best DUC 2007 system.\nWe are now experimenting with the supervised learning techniques (i.e. SVM, MAXENT, CRF etc) and analyzing how they perform for this problem. Prior to that, we produced huge amount of labeled data automatically using similarity measures such as ROUGE (Toutanova et al., 2007).\nIn the future we plan to decompose the complex questions into several simple questions before measuring the similarity between the document sentence and the query sentence. This will certainly serve to create more limited trees and subsequences which might increase the precision. Thus, we expect that by decomposing complex questions into the sets of subquestions that they entail systems can improve the average quality of answers returned and achieve better coverage for the question as a whole."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their useful comments on the earliest version of this paper. Special thanks go to our colleagues for proofreading the paper. We are also grateful to all the graduate students who took part in the user evaluation process. The research reported here was supported by the Natural Sciences and Engineering Research Council (NSERC) research grant and the University of Lethbridge."}, {"heading": "Appendix A. Stop Word List", "text": "reuters ap jan feb mar apr may jun jul aug sep oct nov dec tech news index mon tue wed thu fri sat \u2019s a a\u2019s able about above according accordingly across actually after afterwards again against ain\u2019t all allow allows almost alone along already also although always am amid among amongst an and another any anybody anyhow anyone anything anyway anyways anywhere apart appear appreciate appropriate are aren\u2019t around as aside ask asking associated at available away awfully b be became because become becomes becoming been before beforehand behind being believe below beside besides best better between beyond both brief but by c c\u2019mon c\u2019s came can can\u2019t cannot cant cause causes certain certainly changes clearly co com come comes concerning consequently consider considering contain containing contains corresponding could couldn\u2019t course currently d definitely described despite did didn\u2019t different do does doesn\u2019t doing don\u2019t done down downwards during e each edu eg e.g. eight either else elsewhere enough entirely especially et etc etc. even ever every everybody everyone everything everywhere ex exactly example except f far few fifth five followed following follows for former formerly forth four from further furthermore g get gets getting given gives go goes going gone got gotten greetings h had hadn\u2019t happens hardly has hasn\u2019t have haven\u2019t having he he\u2019s\nhello help hence her here here\u2019s hereafter hereby herein hereupon hers herself hi him himself his hither hopefully how howbeit however i i\u2019d i\u2019ll i\u2019m i\u2019ve ie i.e. if ignored immediate in inasmuch inc indeed indicate indicated indicates inner insofar instead into inward is isn\u2019t it it\u2019d it\u2019ll it\u2019s its itself j just k keep keeps kept know knows known l lately later latter latterly least less lest let let\u2019s like liked likely little look looking looks ltd m mainly many may maybe me mean meanwhile merely might more moreover most mostly mr. ms. much must my myself n namely nd near nearly necessary need needs neither never nevertheless new next nine no nobody non none noone nor normally not nothing novel now nowhere o obviously of off often oh ok okay old on once one ones only onto or other others otherwise ought our ours ourselves out outside over overall own p particular particularly per perhaps placed please plus possible presumably probably provides q que quite qv r rather rd re really reasonably regarding regardless regards relatively respectively right\ns said same saw say saying says second secondly see seeing seem seemed seeming seems seen self selves sensible sent serious seriously seven several shall she should shouldn\u2019t since six so some somebody somehow someone something sometime sometimes somewhat somewhere soon sorry specified specify specifying still sub such sup sure t t\u2019s take taken tell tends th than thank thanks thanx that that\u2019s thats the their theirs them themselves then thence there there\u2019s thereafter thereby therefore therein theres thereupon these they they\u2019d they\u2019ll they\u2019re they\u2019ve think third this thorough thoroughly those though three through throughout thru thus to together too took toward towards tried tries truly try trying twice two u un under unfortunately unless unlikely until unto up upon us use used useful uses using usually uucp v value various very via viz vs w want wants was wasn\u2019t way we we\u2019d we\u2019ll we\u2019re we\u2019ve welcome well went were weren\u2019t what what\u2019s whatever when whence whenever where where\u2019s whereafter whereas whereby wherein whereupon wherever whether which while whither who who\u2019s whoever whole whom whose why will willing wish with within without won\u2019t wonder would would wouldn\u2019t x y yes yet you you\u2019d you\u2019ll you\u2019re you\u2019ve your yours yourself yourselves z zero"}], "references": [{"title": "Combined syntactic and semantic kernels for text classification", "author": ["S. Bloehdorn", "A. Moschitti"], "venue": "In 29th European Conference on IR Research,", "citeRegEx": "Bloehdorn and Moschitti,? \\Q2007\\E", "shortCiteRegEx": "Bloehdorn and Moschitti", "year": 2007}, {"title": "Structure and semantics for expressive text kernels", "author": ["S. Bloehdorn", "A. Moschitti"], "venue": "In CIKM-2007,", "citeRegEx": "Bloehdorn and Moschitti,? \\Q2007\\E", "shortCiteRegEx": "Bloehdorn and Moschitti", "year": 2007}, {"title": "Improving the performance of the random walk model for answering complex questions", "author": ["Y. Chali", "S.R. Joty"], "venue": "In Proceedings of the 46th Annual Meeting of the ACL-HLT. Short Paper Section,", "citeRegEx": "Chali and Joty,? \\Q2008\\E", "shortCiteRegEx": "Chali and Joty", "year": 2008}, {"title": "Selecting sentences for answering complex questions", "author": ["Y. Chali", "S.R. Joty"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Chali and Joty,? \\Q2008\\E", "shortCiteRegEx": "Chali and Joty", "year": 2008}, {"title": "A Maximum-Entropy-Inspired Parser", "author": ["E. Charniak"], "venue": "Technical Report CS-99-12", "citeRegEx": "Charniak,? \\Q1999\\E", "shortCiteRegEx": "Charniak", "year": 1999}, {"title": "Convolution Kernels for Natural Language", "author": ["M. Collins", "N. Duffy"], "venue": "In Proceedings of Neural Information Processing Systems,", "citeRegEx": "Collins and Duffy,? \\Q2001\\E", "shortCiteRegEx": "Collins and Duffy", "year": 2001}, {"title": "Introduction to Algorithms", "author": ["T.R. Cormen", "C.E. Leiserson", "R.L. Rivest"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 1989}, {"title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Erkan and Radev,? \\Q2004\\E", "shortCiteRegEx": "Erkan and Radev", "year": 2004}, {"title": "Summarizing Text Documents: Sentence Selection and Evaluation Metrics", "author": ["J. Goldstein", "M. Kantrowitz", "V. Mittal", "J. Carbonell"], "venue": "In Proceedings of the 22nd International ACM Conference on Research and Development in Information Retrieval,", "citeRegEx": "Goldstein et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Goldstein et al\\.", "year": 1999}, {"title": "A New Multi-document Summarization System", "author": ["Y. Guo", "G. Stylios"], "venue": "In Proceedings of the Document Understanding Conference. NIST", "citeRegEx": "Guo and Stylios,? \\Q2003\\E", "shortCiteRegEx": "Guo and Stylios", "year": 2003}, {"title": "Shallow Semantic Parsing Using Support Vector Machines", "author": ["K. Hacioglu", "S. Pradhan", "W. Ward", "J.H. Martin", "D. Jurafsky"], "venue": "Technical Report TR-CSLR2003-03 University of Colorado", "citeRegEx": "Hacioglu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hacioglu et al\\.", "year": 2003}, {"title": "Answering complex questions with random walk models", "author": ["S. Harabagiu", "F. Lacatusu", "A. Hickl"], "venue": "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Harabagiu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Harabagiu et al\\.", "year": 2006}, {"title": "Dependency-based sentence alignment for multiple document summarization", "author": ["T. Hirao", "J. Suzuki", "H. Isozaki", "E. Maeda"], "venue": "In Proceedings of Coling", "citeRegEx": "Hirao et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hirao et al\\.", "year": 2004}, {"title": "Automated Summarization Evaluation with Basic Elements", "author": ["E. Hovy", "C.Y. Lin", "L. Zhou", "J. Fukumoto"], "venue": "In Proceedings of the Fifth Conference on Language Resources and Evaluation Genoa, Italy", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "From Treebank to PropBank", "author": ["P. Kingsbury", "M. Palmer"], "venue": "In Proceedings of the international conference on Language Resources and Evaluation Las Palmas, Spain", "citeRegEx": "Kingsbury and Palmer,? \\Q2002\\E", "shortCiteRegEx": "Kingsbury and Palmer", "year": 2002}, {"title": "Recognizing textual entailment with tree edit distance algorithms. In Proceedings of the PASCAL Challenges Workshop: Recognising Textual Entailment Challenge", "author": ["M. Kouylekov", "B. Magnini"], "venue": null, "citeRegEx": "Kouylekov and Magnini,? \\Q2005\\E", "shortCiteRegEx": "Kouylekov and Magnini", "year": 2005}, {"title": "A Query-Focused Multi-Document Summarizer Based on Lexical Chains", "author": ["J. Li", "L. Sun", "C. Kit", "J. Webster"], "venue": "In Proceedings of the Document Understanding Conference Rochester. NIST", "citeRegEx": "Li et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author": ["C.Y. Lin"], "venue": "In Proceedings of Workshop on Text Summarization Branches Out, Post-Conference Workshop of Association for Computational Linguistics,", "citeRegEx": "Lin,? \\Q2004\\E", "shortCiteRegEx": "Lin", "year": 2004}, {"title": "An Information-Theoretic Definition of Similarity", "author": ["D. Lin"], "venue": "In Proceedings of International Conference on Machine Learning,", "citeRegEx": "Lin,? \\Q1998\\E", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "Automatic Retrieval and Clustering of Similar Words", "author": ["D. Lin"], "venue": "In Proceedings of the International Conference on Computational Linguistics and Association for Computational Linguistics,", "citeRegEx": "Lin,? \\Q1998\\E", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "Language modeling for sentence retrieval: A comparison between multiple-bernoulli models and multinomial models", "author": ["D. Losada"], "venue": "In Information Retrieval and Theory Workshop", "citeRegEx": "Losada,? \\Q2005\\E", "shortCiteRegEx": "Losada", "year": 2005}, {"title": "Highly frequent terms and sentence retrieval", "author": ["D. Losada", "R.T. Fern\u00e1ndez"], "venue": "In Proc. 14th String Processing and Information Retrieval Symposium,", "citeRegEx": "Losada and Fern\u00e1ndez,? \\Q2007\\E", "shortCiteRegEx": "Losada and Fern\u00e1ndez", "year": 2007}, {"title": "Learning to recognize features of valid textual entailments", "author": ["B. MacCartney", "T. Grenager", "M. de Marneffe", "D. Cer", "C.D. Manning"], "venue": "In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL,", "citeRegEx": "MacCartney et al\\.,? \\Q2006\\E", "shortCiteRegEx": "MacCartney et al\\.", "year": 2006}, {"title": "Lexical cohesion computed by thesaural relations as an indicator of structure of text", "author": ["J. Morris", "G. Hirst"], "venue": "Computational Linguistics,", "citeRegEx": "Morris and Hirst,? \\Q1991\\E", "shortCiteRegEx": "Morris and Hirst", "year": 1991}, {"title": "Efficient convolution kernels for dependency and constituent syntactic trees", "author": ["A. Moschitti"], "venue": "In Proceedings of the 17th European Conference on Machine Learning Berlin, Germany", "citeRegEx": "Moschitti,? \\Q2006\\E", "shortCiteRegEx": "Moschitti", "year": 2006}, {"title": "A Tree Kernel approach to Question and Answer Classification in Question Answering Systems", "author": ["A. Moschitti", "R. Basili"], "venue": "In Proceedings of the 5th international conference on Language Resources and Evaluation Genoa, Italy", "citeRegEx": "Moschitti and Basili,? \\Q2006\\E", "shortCiteRegEx": "Moschitti and Basili", "year": 2006}, {"title": "Kernels on linguistic structures for answer extraction. In Proceedings of the 46th Conference of the Association for Computational Linguistics (ACL\u201908)", "author": ["A. Moschitti", "S. Quarteroni"], "venue": null, "citeRegEx": "Moschitti and Quarteroni,? \\Q2008\\E", "shortCiteRegEx": "Moschitti and Quarteroni", "year": 2008}, {"title": "Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classificaion", "author": ["A. Moschitti", "S. Quarteroni", "R. Basili", "S. Manandhar"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Moschitti et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Moschitti et al\\.", "year": 2007}, {"title": "A translation model for sentence retrieval", "author": ["V. Murdock", "W.B. Croft"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "Murdock and Croft,? \\Q2005\\E", "shortCiteRegEx": "Murdock and Croft", "year": 2005}, {"title": "Using Random Walks for Questionfocused Sentence Retrieval", "author": ["J. Otterbacher", "G. Erkan", "D.R. Radev"], "venue": "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Otterbacher et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Otterbacher et al\\.", "year": 2005}, {"title": "Mapping dependencies trees: An application to question answering", "author": ["V. Punyakanok", "D. Roth", "W. Yih"], "venue": "In Proceedings of AI & Math Florida, USA", "citeRegEx": "Punyakanok et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2004}, {"title": "Advances in Open Domain Question Answering", "author": ["T. Strzalkowski", "S. Harabagiu"], "venue": null, "citeRegEx": "Strzalkowski and Harabagiu,? \\Q2008\\E", "shortCiteRegEx": "Strzalkowski and Harabagiu", "year": 2008}, {"title": "The pythy summarization system: Microsoft research at duc", "author": ["K. Toutanova", "C. Brockett", "M. Gamon", "J. Jagarlamudi", "H. Suzuki", "L. Vanderwende"], "venue": "In proceedings of the Document Understanding Conference Rochester. NIST", "citeRegEx": "Toutanova et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2007}, {"title": "Microsoft Research at DUC2006: Task-Focused Summarization with Sentence Simplification and Lexical Expansion", "author": ["L. Vanderwende", "H. Suzuki", "C. Brockett"], "venue": "In Proceedings of the Document Understanding Conference Rochester. NIST", "citeRegEx": "Vanderwende et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Vanderwende et al\\.", "year": 2006}, {"title": "Sentence Compression as a Component of a Multi-Document Summarization System", "author": ["D.M. Zajic", "J. Lin", "B.J. Dorr", "R. Schwartz"], "venue": "In Proceedings of the Document Understanding Conference Rochester. NIST", "citeRegEx": "Zajic et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zajic et al\\.", "year": 2006}, {"title": "Question Classification using Support Vector Machines", "author": ["A. Zhang", "W. Lee"], "venue": "In Proceedings of the Special Interest Group on Information Retrieval,", "citeRegEx": "Zhang and Lee,? \\Q2003\\E", "shortCiteRegEx": "Zhang and Lee", "year": 2003}, {"title": "A Language Modeling Approach to Passage Question Answering", "author": ["D. Zhang", "W.S. Lee"], "venue": "In Proceedings of the Twelfth Text REtreival Conference,", "citeRegEx": "Zhang and Lee,? \\Q2003\\E", "shortCiteRegEx": "Zhang and Lee", "year": 2003}, {"title": "A BE-based Multi-dccument Summarizer with Query Interpretation", "author": ["L. Zhou", "C.Y. Lin", "E. Hovy"], "venue": "In Proceedings of Document Understanding Conference", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 25, "context": "Murdock and Croft (2005) propose a translation model specifically for monolingual data, and show that it significantly improves sentence retrieval over query likelihood.", "startOffset": 0, "endOffset": 25}, {"referenceID": 19, "context": "Losada (2005) presents a comparison between multiple-Bernoulli models and multinomial models in the context of a sentence retrieval task and shows that a multivariate Bernoulli model can really outperform popular multinomial models for retrieving relevant sentences.", "startOffset": 0, "endOffset": 14}, {"referenceID": 19, "context": "Losada (2005) presents a comparison between multiple-Bernoulli models and multinomial models in the context of a sentence retrieval task and shows that a multivariate Bernoulli model can really outperform popular multinomial models for retrieving relevant sentences. Losada and Fern\u00e1ndez (2007) propose a novel sentence retrieval method based on extracting highly frequent terms from top retrieved documents.", "startOffset": 0, "endOffset": 295}, {"referenceID": 7, "context": "The LexRank method addressed by Erkan and Radev (2004) was very successful in generic multi-document summarization.", "startOffset": 32, "endOffset": 55}, {"referenceID": 7, "context": "The LexRank method addressed by Erkan and Radev (2004) was very successful in generic multi-document summarization. A topic-sensitive LexRank is proposed by Otterbacher, Erkan, and Radev (2005). As in LexRank, the set of sentences in a document cluster is represented as a graph where nodes are sentences, and links between the nodes are induced by a similarity relation between the sentences.", "startOffset": 32, "endOffset": 194}, {"referenceID": 7, "context": "The LexRank method addressed by Erkan and Radev (2004) was very successful in generic multi-document summarization. A topic-sensitive LexRank is proposed by Otterbacher, Erkan, and Radev (2005). As in LexRank, the set of sentences in a document cluster is represented as a graph where nodes are sentences, and links between the nodes are induced by a similarity relation between the sentences. The system then ranks the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question. Concepts of coherence and cohesion enable us to capture the theme of the text. Coherence represents the overall structure of a multi-sentence text in terms of macro-level relations between clauses or sentences (Halliday & Hasan, 1976). Cohesion, as defined by Halliday and Hasan (1976), is the property of holding text together as one single grammat-", "startOffset": 32, "endOffset": 882}, {"referenceID": 15, "context": "For example, Li et al. (2007) uses the following formula: Score = \u03b1P (chain) + \u03b2P (query) + \u03b3P (namedEntity) where P (chain) is the sum of the scores of the chains whose words come from the candidate sentence, P (query) is the sum of the co-occurrences of key words in a topic and the sentence, and P (namedEntity) is the number of name entities existing in both the topic and the sentence.", "startOffset": 13, "endOffset": 30}, {"referenceID": 11, "context": "Harabagiu et al. (2006) introduce a new paradigm for processing complex questions that relies on a combination of (a) question decompositions; (b) factoid QA techniques; and (c) Multi-Document Summarization (MDS) techniques.", "startOffset": 0, "endOffset": 24}, {"referenceID": 11, "context": "Harabagiu et al. (2006) introduce a new paradigm for processing complex questions that relies on a combination of (a) question decompositions; (b) factoid QA techniques; and (c) Multi-Document Summarization (MDS) techniques. The question decomposition procedure operates on a Markov chain. That is, by following a random walk with a mixture model on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topic-relevant passages that manifest these relations. Decomposed questions are then submitted to a state-of-the-art QA system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a MDS system. They show that question decompositions using this method can significantly enhance the relevance and comprehensiveness of summary-length answers to complex questions. There are approaches that are based on probabilistic models (Pingali, K., & Varma, 2007; Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, & Vanderwende, 2007). Pingali et al. (2007) rank the sentences based on a mixture model where each component of the model is a statistical model:", "startOffset": 0, "endOffset": 1068}, {"referenceID": 28, "context": "Toutanova et al. (2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency.", "startOffset": 0, "endOffset": 24}, {"referenceID": 28, "context": "Toutanova et al. (2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency. The scoring function is learned by fitting weights for a set of feature functions of sentences in the document set and is trained to optimize a sentence pair-wise ranking criterion. The scoring function is further adapted to apply to summaries rather than sentences and to take into account redundancy among sentences. Pingali et al. (2007) reduce the document-sentences by dropping words that do not contain any important information.", "startOffset": 0, "endOffset": 522}, {"referenceID": 28, "context": "Toutanova et al. (2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency. The scoring function is learned by fitting weights for a set of feature functions of sentences in the document set and is trained to optimize a sentence pair-wise ranking criterion. The scoring function is further adapted to apply to summaries rather than sentences and to take into account redundancy among sentences. Pingali et al. (2007) reduce the document-sentences by dropping words that do not contain any important information. Toutanova et al. (2007), Vanderwende, Suzuki, and Brockett (2006), and Zajic, Lin, Dorr, and Schwartz (2006) heuristically decompose the document-sentences into smaller units.", "startOffset": 0, "endOffset": 641}, {"referenceID": 28, "context": "Toutanova et al. (2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency. The scoring function is learned by fitting weights for a set of feature functions of sentences in the document set and is trained to optimize a sentence pair-wise ranking criterion. The scoring function is further adapted to apply to summaries rather than sentences and to take into account redundancy among sentences. Pingali et al. (2007) reduce the document-sentences by dropping words that do not contain any important information. Toutanova et al. (2007), Vanderwende, Suzuki, and Brockett (2006), and Zajic, Lin, Dorr, and Schwartz (2006) heuristically decompose the document-sentences into smaller units.", "startOffset": 0, "endOffset": 683}, {"referenceID": 16, "context": "(2007), Vanderwende, Suzuki, and Brockett (2006), and Zajic, Lin, Dorr, and Schwartz (2006) heuristically decompose the document-sentences into smaller units.", "startOffset": 61, "endOffset": 92}, {"referenceID": 9, "context": "Guo and Stylios (2003) use verb arguments (i.", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "Guo and Stylios (2003) use verb arguments (i.e. subjects, times, locations and actions) for clustering. For each sentence this method establishes the indices information based on the verb arguments (subject is first index, time is second, location is third and action is fourth). All the sentences that have the same or closest \u2018subjects\u2019 index are put in a cluster and they are sorted out according to the temporal sequence from the earliest to the latest. Sentences that have the same \u2018spaces/locations\u2019 index value in the cluster are then marked out. The clusters are ranked based on their sizes and top 10 clusters are chosen. Then, applying a cluster reduction module the system generates the compressed extract summaries. There are approaches in \u201cRecognizing Textual Entailment\u201d, \u201cSentence Alignment\u201d, and \u201cQuestion Answering\u201d that use syntactic and/or semantic information in order to measure the similarity between two textual units. This indeed motivated us to include syntactic and semantic features to get the structural similarity between a document sentence and a query sentence (discussed in Section 4.1). MacCartney, Grenager, de Marneffe, Cer, and Manning (2006) use typed dependency graphs (same as dependency trees) to represent the text and the hypothesis.", "startOffset": 0, "endOffset": 1179}, {"referenceID": 12, "context": "Hirao et al. (2004) represent the sentences using Dependency Tree Path (DTP) to incorporate syntactic information.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Hirao et al. (2004) represent the sentences using Dependency Tree Path (DTP) to incorporate syntactic information. They apply String Subsequence Kernel (SSK) to measure the similarity between the DTPs of two sentences. They also introduce Extended String Subsequence Kernel (ESK) to incorporate semantics in DTPs. Kouylekov and Magnini (2005) use the tree edit distance algorithms on the dependency trees of the text and the hypothesis to recognize the textual entailment.", "startOffset": 0, "endOffset": 343}, {"referenceID": 12, "context": "Hirao et al. (2004) represent the sentences using Dependency Tree Path (DTP) to incorporate syntactic information. They apply String Subsequence Kernel (SSK) to measure the similarity between the DTPs of two sentences. They also introduce Extended String Subsequence Kernel (ESK) to incorporate semantics in DTPs. Kouylekov and Magnini (2005) use the tree edit distance algorithms on the dependency trees of the text and the hypothesis to recognize the textual entailment. According to this approach, a text T entails a hypothesis H if there exists a sequence of transformations (i.e. deletion, insertion and substitution) applied to T such that we can obtain H with an overall cost below a certain threshold. Punyakanok et al. (2004) represent the question and the sentence containing answer with their dependency trees.", "startOffset": 0, "endOffset": 735}, {"referenceID": 26, "context": "The importance of syntactic and semantic features in this context is described by Zhang and Lee (2003a), Moschitti et al.", "startOffset": 82, "endOffset": 104}, {"referenceID": 18, "context": "The importance of syntactic and semantic features in this context is described by Zhang and Lee (2003a), Moschitti et al. (2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b).", "startOffset": 105, "endOffset": 129}, {"referenceID": 0, "context": "(2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b).", "startOffset": 8, "endOffset": 40}, {"referenceID": 0, "context": "(2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b).", "startOffset": 8, "endOffset": 69}, {"referenceID": 0, "context": "(2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b). An effective way to integrate syntactic and semantic structures in machine learning algorithms is the use of tree kernel functions (Collins & Duffy, 2001; Moschitti & Quarteroni, 2008) which has been successfully applied to question classification (Zhang & Lee, 2003a; Moschitti & Basili, 2006).", "startOffset": 8, "endOffset": 105}, {"referenceID": 0, "context": "(2007), Bloehdorn and Moschitti (2007a), Moschitti and Basili (2006) and Bloehdorn and Moschitti (2007b). An effective way to integrate syntactic and semantic structures in machine learning algorithms is the use of tree kernel functions (Collins & Duffy, 2001; Moschitti & Quarteroni, 2008) which has been successfully applied to question classification (Zhang & Lee, 2003a; Moschitti & Basili, 2006). Syntactic and semantic information are used effectively to measure the similarity between two textual units by MacCartney et al. (2006). To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences.", "startOffset": 8, "endOffset": 538}, {"referenceID": 12, "context": "1 Encoding Syntactic Structures Basic Element (BE) Overlap Measure Shallow syntactic information based on dependency relations was proved to be effective in finding similarity between two textual units (Hirao et al., 2004).", "startOffset": 202, "endOffset": 222}, {"referenceID": 13, "context": "We incorporate this information by using Basic Elements that are defined as follows (Hovy et al., 2006): \u2022 The head of a major syntactic constituent (noun, verb, adjective or adverbial phrases), expressed as a single item.", "startOffset": 84, "endOffset": 103}, {"referenceID": 13, "context": "The triples encode some syntactic information and one can decide whether any two units match or not- more easily than with longer units (Hovy et al., 2006).", "startOffset": 136, "endOffset": 155}, {"referenceID": 13, "context": "The triples encode some syntactic information and one can decide whether any two units match or not- more easily than with longer units (Hovy et al., 2006). We extracted BEs for the sentences (or query) by using the BE package distributed by ISI5. Once we get the BEs for a sentence, we computed the Likelihood Ratio (LR) for each BE following Zhou, Lin, and Hovy (2005). Sorting BEs according to their LR scores produced a BE-ranked list.", "startOffset": 137, "endOffset": 371}, {"referenceID": 24, "context": "Tree Kernels Approach In order to calculate the syntactic similarity between the query and the sentence we first parse the sentence as well as the query into a syntactic tree (Moschitti, 2006) using a parser like Charniak (1999).", "startOffset": 175, "endOffset": 192}, {"referenceID": 27, "context": "The tree fragments of a tree are all of its sub-trees which include at least one production with the restriction that no production rules can be broken into incomplete parts (Moschitti et al., 2007).", "startOffset": 174, "endOffset": 198}, {"referenceID": 4, "context": "Tree Kernels Approach In order to calculate the syntactic similarity between the query and the sentence we first parse the sentence as well as the query into a syntactic tree (Moschitti, 2006) using a parser like Charniak (1999). Then we calculate the similarity between the two trees using the tree kernel.", "startOffset": 213, "endOffset": 229}, {"referenceID": 4, "context": "Tree Kernels Approach In order to calculate the syntactic similarity between the query and the sentence we first parse the sentence as well as the query into a syntactic tree (Moschitti, 2006) using a parser like Charniak (1999). Then we calculate the similarity between the two trees using the tree kernel. We reimplemented the tree kernel model as proposed by Moschitti et al. (2007). Once we build the trees, our next task is to measure the similarity between the trees.", "startOffset": 213, "endOffset": 386}, {"referenceID": 5, "context": "Because of this, Collins and Duffy (2001) define the tree kernel algorithm whose computational complexity does not depend on m.", "startOffset": 17, "endOffset": 42}, {"referenceID": 22, "context": "Shallow semantic representations, bearing more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOWmodels (MacCartney et al., 2006; Moschitti et al., 2007).", "startOffset": 157, "endOffset": 206}, {"referenceID": 27, "context": "Shallow semantic representations, bearing more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOWmodels (MacCartney et al., 2006; Moschitti et al., 2007).", "startOffset": 157, "endOffset": 206}, {"referenceID": 22, "context": "Shallow semantic representations, bearing more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOWmodels (MacCartney et al., 2006; Moschitti et al., 2007). Initiatives such as PropBank (PB) (Kingsbury & Palmer, 2002) have made the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu, Pradhan, Ward, Martin, & Jurafsky, 2003) possible. Hence, attempting an application of SRL to QA seems natural as pinpointing the answer to a question relies on a deep understanding of the semantics of both. For example, consider the PB annotation: [ARG0 all][TARGET use][ARG1 the french franc][ARG2 as their currency] Such annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g. [ARG0 the Vatican][TARGET use][ARG1 the Italian lira][ARG2 as their currency] In order to calculate the semantic similarity between the sentences we first represent the annotated sentence (or query) using the tree structures like Figure 2 called Semantic Tree (ST) as proposed by Moschitti et al. (2007). In the semantic tree arguments are replaced with the most important word\u2013often referred to as the semantic head.", "startOffset": 158, "endOffset": 1138}, {"referenceID": 24, "context": "Moschitti et al. (2007) solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows portions of an ST to match.", "startOffset": 0, "endOffset": 24}, {"referenceID": 24, "context": "Moschitti et al. (2007) solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows portions of an ST to match. Shallow Semantic Tree Kernel (SSTK) We reimplemented the SSTK according to the model given by Moschitti et al. (2007). The SSTK is based on two ideas: first, it changes", "startOffset": 0, "endOffset": 255}, {"referenceID": 27, "context": "empty) (Moschitti et al., 2007).", "startOffset": 7, "endOffset": 31}, {"referenceID": 17, "context": "Given two sequences S1 and S2, the longest common subsequence (LCS) of S1 and S2 is a common subsequence with maximum length (Lin, 2004).", "startOffset": 125, "endOffset": 136}, {"referenceID": 17, "context": "The basic LCS has a problem in that it does not differentiate LCSes of different spatial relations within their embedding sequences (Lin, 2004).", "startOffset": 132, "endOffset": 143}, {"referenceID": 17, "context": "Given two sentences X and Y, the WLCS score of X and Y can be computed using the similar dynamic programming procedure as stated by Lin (2004). We use WLCS as it has the advantage of not measuring the similarity by taking the words in a higher dimension like string kernels which indeed reduces the time complexity.", "startOffset": 132, "endOffset": 143}, {"referenceID": 17, "context": "Skipbigram measures the overlap of skip-bigrams between a candidate sentence and a query sentence (Lin, 2004).", "startOffset": 98, "endOffset": 109}, {"referenceID": 17, "context": "For example, if we set dskip to 0 then it is equivalent to bi-gram overlap measure (Lin, 2004).", "startOffset": 83, "endOffset": 94}, {"referenceID": 17, "context": "For example, if we set dskip to 0 then it is equivalent to bi-gram overlap measure (Lin, 2004). If we set dskip to 4 then only word pairs of at most 4 words apart can form skip bi-grams. In our experiment we set dskip = 4 in order to ponder at most 4 words apart to get the skip bi-grams. Modifying the equations: 15, 16, and 17 to allow the maximum skip distance limit is straightforward: following Lin (2004) we count the skip bi-gram matches, SKIP2(S,Q), within the maximum skip distance and replace the denominators of the equations with the actual numbers of within distance skip bi-grams from the reference sentence and the candidate sentence respectively.", "startOffset": 84, "endOffset": 411}, {"referenceID": 7, "context": "5 Graph-based Similarity Measure Erkan and Radev (2004) used the concept of graph-based centrality to rank a set of sentences for producing generic multi-document summaries.", "startOffset": 33, "endOffset": 56}, {"referenceID": 7, "context": "5 Graph-based Similarity Measure Erkan and Radev (2004) used the concept of graph-based centrality to rank a set of sentences for producing generic multi-document summaries. A similarity graph is produced for the sentences in the document collection. In the graph each node represents a sentence. The edges between nodes measure the cosine similarity between the respective pair of sentences. The degree of a given node is an indication of how important the sentence is. Figure 5 shows an example of a similarity graph for 4 sentences. Once the similarity graph is constructed, the sentences are ranked according to their eigenvector centrality. The LexRank performed well in the context of generic summarization. To apply LexRank to query-focused context a topic-sensitive version of LexRank is proposed by Otterbacher et al. (2005). We followed a similar approach in order to calculate this feature.", "startOffset": 33, "endOffset": 834}, {"referenceID": 29, "context": "For instance, if a sentence that gets a high score based on the question relevance model is likely to contain an answer to the question then a related sentence, which may not be similar to the question itself, is also likely to contain an answer (Otterbacher et al., 2005).", "startOffset": 246, "endOffset": 272}, {"referenceID": 17, "context": "the automatic evaluation tool ROUGE (Lin, 2004) (described in Section 7) and the ROUGE value works as the feedback to our learning loop.", "startOffset": 36, "endOffset": 47}, {"referenceID": 13, "context": "Following Hovy et al. (2006) we modeled this by BE overlap between an intermediate summary and a to-be-added candidate summary sentence.", "startOffset": 10, "endOffset": 29}, {"referenceID": 17, "context": "1 Automatic Evaluation ROUGE We carried out automatic evaluation of our summaries using the ROUGE (Lin, 2004) toolkit, which has been widely adopted by DUC for automatic summarization evaluation.", "startOffset": 98, "endOffset": 109}, {"referenceID": 17, "context": "Most of these ROUGE measures have been applied in automatic evaluation of summarization systems and achieved very promising results (Lin, 2004).", "startOffset": 132, "endOffset": 143}, {"referenceID": 32, "context": "Prior to that, we produced huge amount of labeled data automatically using similarity measures such as ROUGE (Toutanova et al., 2007).", "startOffset": 109, "endOffset": 133}], "year": 2009, "abstractText": "Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. In this paper, we experiment with one empirical method and two unsupervised statistical machine learning techniques: K-means and Expectation Maximization (EM), for computing relative importance of the sentences. We compare the results of these approaches. Our experiments show that the empirical approach outperforms the other two techniques and EM performs better than K-means. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. In order to measure the importance and relevance to the user query we extract different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. We use a local search technique to learn the weights of the features. To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). For each of our methods of generating summaries (i.e. empirical, K-means and EM) we show the effects of syntactic and shallow-semantic features over the bag-of-words (BOW) features.", "creator": "dvips(k) 5.92b Copyright 2002 Radical Eye Software"}}}