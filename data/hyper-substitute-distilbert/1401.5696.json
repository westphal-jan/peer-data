{"id": "1401.5696", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Unsupervised Methods for Determining Object and Relation Synonyms on the Web", "abstract": "the criterion group selecting ambiguous phrases and objects, termed construct resolution, is critical in high - power information extraction. this paper measures synonym resolution amid physical context of secure information networks, hence both binary - tagged context examples nor domain knowledge is available. similarly paper mentions a scalable, fully - automatic system whenever runs any random ( kn log size ) time in each number of blocks, n, reflecting the shortest proportion of synonyms after word, k. the system, polynomial induction, introduces structured probabilistic relational logic for predicting whatever two strings define co - referential based ; grammatical similarity linking the assertions with them. introducing a set of two template assertions extracted and the library, iso resolves objects @ 78 % precision and 68 % stability, independently resolves relations with 44 % precision and 35 % clarity. several sizes of resolvers probabilistic searches are criticized, and criticisms are that under appropriate conditions individual variations can improve f1 ~ 78 %. an extension to xml basic problem consists of it simultaneously handle polysemous names with 97 % precision and 95 % retrieval on an data set representing that trec corpus.", "histories": [["v1", "Wed, 15 Jan 2014 05:33:07 GMT  (448kb)", "http://arxiv.org/abs/1401.5696v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander pieter yates", "oren etzioni"], "accepted": false, "id": "1401.5696"}, "pdf": {"name": "1401.5696.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Methods for Determining Object and Relation Synonyms on the Web", "authors": ["Alexander Yates", "Oren Etzioni"], "emails": ["yates@temple.edu", "etzioni@cs.washington.edu"], "sections": [{"heading": "1. Introduction", "text": "Web Information Extraction (WIE) systems (Zhu, Nie, Wen, Zhang, & Ma, 2005; Agichtein, 2006; Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, & Yates, 2005) extract assertions that describe a relation and its arguments from Web text. For example:\n(is capital of, D.C., United States)\nWIE systems can extract hundreds of millions of assertions containing millions of different strings from the Web (e.g., the TextRunner system by Banko, Cafarella, Soderland, Broadhead, & Etzioni, 2007). One problem that becomes a real challenge at this scale is that WIE systems often extract assertions that describe the same real-world object or relation using different names. For example, a WIE system might also extract\nc\u00a92009 AI Access Foundation. All rights reserved.\n(is capital city of, Washington, U.S.)\nwhich describes the same relationship as above but contains a different name for the relation and each argument.\nSynonyms are prevalent in text, and the Web corpus is no exception. Our data set of two million assertions extracted from a Web crawl contained over a half-dozen different names each for the United States and Washington, D.C., and three for the is capital of relation. The top 80 most commonly extracted objects had an average of 2.9 extracted names per entity, and several had as many as 10 names. The top 100 most commonly extracted relations had an average of 4.9 synonyms per relation.\nWe refer to the problem of identifying synonymous object and relation names as synonym resolution. Previous techniques have focused on one particular aspect of the problem, either objects or relations. In addition, these techniques often depend on a large set of training examples, or are tailored to a specific domain by assuming knowledge of the domain\u2019s schema. Due to the number and diversity of the relations extracted, these techniques are not feasible for WIE systems. Schemata are not available for the Web, and hand-labeling training examples for each relation would require a prohibitive manual effort.\nIn response, we present Resolver, a novel, domain-independent, unsupervised synonym resolution system that applies to both objects and relations. Resolver clusters synonymous names together using a probabilistic model informed by string similarity and the similarity of the assertions containing the names. Its similarity metric outperforms those used by similar systems for cross-document entity coreference (e.g., Mann & Yarowsky, 2003) and paraphrase discovery (Lin & Pantel, 2001; Hasegawa, Sekine, & Grishman, 2004) on their respective tasks of object and relation synonym resolution. The key questions answered by Resolver include:\n1. Is it possible to effectively cluster strings in a large set of extractions into sets of synonyms without using domain knowledge, manually labeled training data, or other external resources that are unavailable in the context of Web Information Extraction? Experiments below include an empirical demonstration that Resolver can resolve objects with 78% precision and 68% recall, and relations with 90% precision and 35% recall.\n2. How can we scale synonym resolution to large, high-dimensional data sets? Resolver provides a scalable clustering algorithm that runs in time O(KN logN) in the number of extractions, N , and the maximum number of synonyms per word, K. In theory it compares well with even fast approximate solutions for clustering large data sets in large-dimensional spaces, and in practice Resolver has been successfully run on a set of assertions extracted from over 100 million Web pages.\n3. How can we formalize unsupervised synonym resolution, and is there a practical benefit to doing so? Resolver provides an unsupervised, generative probabilistic model for predicting whether two object or relation names co-refer, and experiments show that this significantly outperforms previous metrics for distributional similarity. In particular, it outperforms a related metric based on mutual information (Lin & Pantel, 2001) by 193% in AUC on object clustering, and by 121% on relation clustering.\n4. Is it possible to use the special properties of functions and inverse functions to improve the precision of a synonym resolution algorithm? The basic version of Resolver\u2019s probabilistic model for object synonymy is independent of the relation in the extraction. However, it is intuitively clear that certain relations, especially functions and inverse functions, provide especially strong evidence for and against synonymy. Several extensions to the Resolver system show that without hurting recall, the precision of object merging can be improved by 3% using functions.\n5. Can Resolver handle polysemous names, which have different meanings in different contexts? While the basic version of Resolver assumes that every name has a single meaning, we present an extension to the basic system that is able to automatically handle polysemous names. On a manually-cleaned data set of polysemous named entities from the TREC corpus, Resolver achieves a precision of 97.3% and a recall of 94.7% in detecting proper noun coreference relationships, and is able to outperform previous work in accuracy while requiring only a large, unannotated corpus as input.\nThe next section discusses previous work in synonym resolution. Section 3 describes the problem of synonym resolution formally and introduces notation and terminology that will be used throughout. Section 4 introduces Resolver\u2019s probabilistic model. Section 5 describes Resolver\u2019s clustering algorithm. Section 6 presents experiments with the basic Resolver system that compare its performance with the performance of previous work in synonym resolution. Section 7 describes several extensions to the basic Resolver system, together with experiments illustrating the gains in precision and recall. Section 8 develops an extension to Resolver that relaxes the assumption that every string has a single referent, and it compares Resolver experimentally to previous work in crossdocument entity resolution. Finally, Section 9 discusses conclusions and areas for future work."}, {"heading": "2. Previous Work", "text": "Synonym resolution encompasses two tasks, finding synonyms for extracted objects and relations. Synonym resolution for objects is very similar to the task of cross-document entity resolution (Bagga & Baldwin, 1998), in which the objective is to cluster occurrences of named entities from multiple documents into coreferential groups. Pedersen and Kulkarni (Pedersen & Kulkarni, 2007; Kulkarni & Pedersen, 2008) cluster people\u2019s names in Web documents and in emails using agglomerative clustering and a heuristic similarity function. Li, Morie, and Roth (2004a, 2004b) use an Expectation-Maximization with a graphical model and databases of common nicknames, honorifics, titles, etc.to achieve high accuracy on a cross-document entity resolution task. Mann and Yarowsky (2003) use a combination of extracted features and term vectors including proper names in context to cluster ambiguous names on the Web. They use the Cosine Similarity Metric (Salton & McGill, 1983) together with hierarchical agglomerative clustering. Resolver\u2019s main contribution to this body of work is that it proposes a new, formal similarity measure that works for both objects and relations, and it demonstrates both theoretically and empirically that it can scale up to millions of extractions. The Web People Search Task (WEPS) (Artile, Sekine, & Gonzalo, 2008), part of SemEval 2007, involved 16 systems trying to determine clusters of documents\ncontaining references to the same entity for ambiguous person names like \u201cKennedy.\u201d In Section 6, we show that Resolver significantly outperforms the Cosine Similarity Metric in clustering experiments. Further experiments below (Section 8) show that Resolver is able to achieve similar, slightly higher performance than Li et al. on their dataset, while not relying on any resources besides a large corpus.\nCoreference resolution systems, like synonym resolution systems, try to merge references to the same object, and they apply to arbitrary noun phrases rather than just to named entities. Because of the difficulty of this general problem, most work has considered techniques informed by parsers (e.g., Lappin & Leass, 1994) or training data (e.g., Ng & Cardie, 2002; McCarthy & Lehnert, 1995). Cardie and Wagstaff (1999) use a set of extracted grammatical and semantic features and an ad-hoc clustering algorithm to perform unsupervised coreference resolution, achieving better performance on the MUC-6 coreference task than a supervised system. More recently, Haghighi and Klein (2007) use a graphical model combining local salience features and global entity features to perform unsupervised coreference, achieving an F1 score of 70.1 on MUC-6. Two systems use automatically extracted information to help make coreference resolution decisions, much like Resolver does. Kehler, Appelt, Taylor, and Simma (2004) use statistics over automatically-determined predicateargument structures to compare contexts between pronouns and their potential antecedents. They find that adding this information to a system that relies on morpho-syntactic evidence for pronoun resolution provides little or no benefit. Bean and Riloff (2004) use targeted extraction patterns to find semantic constraints on the relationship between pronouns and their antecedents, and show that they can use these to improve an anaphora-resolution system. Coreference resolution is a more difficult and general task than synonym resolution for objects since it deals with arbitrary types of noun phrases. However, systems for coreference resolution also have more information available to them in the form of local sequence and salience information, which is lost in the extraction process, and they do not address relation synonymy.\nSynonym resolution for relations is often called paraphrase discovery or paraphrase acquisition in NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005). Previous work in this area (Barzilay & Lee, 2003; Barzilay & McKeown, 2001; Shinyama & Sekine, 2003; Pang, Knight, & Marcu, 2003) has looked at the use of parallel, aligned corpora, such as multiple translations of the same text or multiple news reports of the same story, to find paraphrases. Brockett and Dolan (2005) have used manually-labeled data to train a supervised model of paraphrases. The PASCAL Recognising Textual Entailment Challenge (Dagan, Glickman, & Magnini, 2006) proposes the task of recognizing when two sentences entail one another, given manually labeled training data, and many authors have submitted responses to this challenge. Resolver avoids the use of labor-intensive resources, and relies solely on automatically acquired extractions from a large corpus.\nSeveral unsupervised systems for paraphrase discovery have focused on using corpusbased techniques to cluster synonymous relations. Sekine (2005) uses a heuristic similarity measure to cluster relations. Davidov and Rappoport (2008) use a heuristic clustering method to find groups of relation patterns that can be used to extract instances. Hasegawa et al. (2004) automatically extract relationships from a large corpus and cluster relations, using the Cosine Similarity Metric (Salton & McGill, 1983) and a hierarchical clustering technique like Resolver\u2019s. The DIRT system (Lin & Pantel, 2001) uses a similarity mea-\nsure based on mutual information statistics to identify relations that are similar to a given one. Resolver provides a formal probabilistic model for its similarity technique, and it applies to both objects and relations. Section 4.3 contains a fuller description of the differences between Resolver and DIRT, and Section 6 describes experiments which show Resolver\u2019s superior performance in precision and recall over clustering using the mutual information similarity metric employed by DIRT, as well as the Cosine Similarity Metric.\nResolver\u2019s method of determining the similarity between two strings is an example of a broad class of metrics called distributional similarity metrics (Lee, 1999), but it has significant advantages over traditional distributional similarity metrics for the synonym resolution task. All of these metrics are based on the underlying assumption, called the Distributional Hypothesis, that \u201cSimilar objects appear in similar contexts.\u201d (Hindle, 1990) Previous distributional similarity metrics, however, have been designed for comparing words based on terms appearing in the same document, rather than extracted properties. This has two important consequences: first, extracted properties are by nature sparser because they appear only in a narrow window around words and because they consist of longer strings (at the very least, pairs of words); second, each extracted shared property provides stronger evidence for synonymy than an arbitrary word that appears together with each synonym, because the extraction mechanism is designed to find meaningful relationships. Resolver\u2019s metric is designed to take advantage of the relational model provided by Web Information Extraction. Section 4.3 more fully describes the difference between Resolver\u2019s metric and the Cosine Similarity Metric (Salton & McGill, 1983), an example of a traditional distributional similarity metric. Experiments in Section 6 demonstrate that Resolver outperforms the Cosine Similarity Metric.\nThere are many unsupervised approaches for object resolution in databases, but unlike our algorithm these approaches depend on a known, fixed, and generally small schema. Ravikumar and Cohen (2004) present an unsupervised approach to object resolution using Expectation-Maximization on a hierarchical graphical model. Several other recent approaches leverage domain-specific information and heuristics for object resolution. For example, many (Dong, Halevy, & Madhavan, 2005; Bhattacharya & Getoor, 2005, 2006) rely on evidence from observing which strings appear as arguments to the same relation simultaneously (e.g., co-authors of the same publication). While this is useful information when resolving authors in the citation domain, it is rare to find relations with similar properties in extracted assertions. None of these approaches applies to the problem of resolving relations. Winkler (1999) provides a survey of this area. Several supervised learning techniques make entity resolution decisions (Kehler, 1997; McCallum & Wellner, 2004; Singla & Domingos, 2006), but of course these systems depend on the availability of training data, and even on a significant number of labeled examples per relation of interest.\nOne promising new approach to clustering in a relational domain is the Multiple Relational Clusterings (MRC) algorithm (Kok & Domingos, 2007). This approach, though not specific to synonym resolution, can find synonyms in a set of unlabeled, relational extractions without domain-specific heuristics. The approach is quite recent, and so far no detailed experimental comparison has been conducted.\nResolver\u2019s probabilistic model is partly inspired by the ball-and-urns abstraction of information extraction presented by Downey, Etzioni, and Soderland (2005) Resolver\u2019s task and probability model are different from theirs, but many of the same modeling as-\nsumptions (such as the independence of extractions) are made in both cases to simplify the derivation of the models.\nPrevious work on Resolver (Yates & Etzioni, 2007) discussed the basic version of the probabilistic model and initial experimental results. This work expands on the previous work in that it includes a new experimental comparison with an established mutual information-based similarity metric; a new extension to the basic system (property weighting); full proofs for three claims; and a description of a fast algorithm for calculating the Extracted Shared Property model."}, {"heading": "3. The Formal Synonym Resolution Problem", "text": "A synonym resolution system for WIE takes a set of extractions as input and returns a set of clusters, with each cluster containing synonymous object strings or relation strings. More precisely, the input is a data set D containing extracted assertions of the form a = (r, o1, . . . , on), where r is a relation string and each oi is an object string representing the arguments to the relation. Throughout this work, all assertions are assumed to be binary, so n = 2.\nThe output of a synonym resolution system is a clustering, or set of clusters, of the strings in D. Let S be the set of all distinct strings in D. A clustering of S is a set C \u2282 2S such that all the clusters in C are distinct, and they cover the whole set:\n\u22c3\nc\u2208C\n= S\n\u2200c1, c2 \u2208 C. c1 \u2229 c2 = \u2205 Each cluster in the output clustering constitutes the system\u2019s conjecture that all strings inside the cluster are synonyms, and no string outside that cluster is a synonym of any string in the cluster."}, {"heading": "3.1 The Single-Sense Assumption", "text": "The formal representation of synonym resolution described above makes an important simplifying assumption: it is assumed that every string belongs to exactly one cluster. In language, however, strings often have multiple meanings; i.e., they are polysemous. Polysemous strings cannot be adequately represented using a clustering in which each string belongs to exactly one cluster. For most of this paper, we will make the single-sense assumption, but Section 8 illustrates an extension to Resolver that does away with this assumption.\nAs an example of the representational trouble posed by polysemy, consider the name \u201cPresident Roosevelt.\u201d In certain contexts, this name is synonymous with \u201cPresident Franklin D. Roosevelt,\u201d and in other contexts it is synonymous with \u201cPresident Theodore Roosevelt.\u201d However, \u201cPresident Franklin D. Roosevelt\u201d is never synonymous with \u201cPresident Theodore Roosevelt.\u201d There is no clustering of the three names, using the notion of clustering described above, such that all synonymy relationships are accurately represented.\nOthers have described alternate kinds of clustering that take polysemy into account. For example, \u201csoft clustering\u201d allows a string to be assigned to as many different clusters as it\nhas senses. One variation on this idea is to assign a probability distribution to every string, describing the prior probability that the string belongs in each cluster (Li & Abe, 1998; Pereira, Tishby, & Lee, 1993). Both of these representations capture only prior information about strings. That is, they represent the idea that a particular string can belong to a cluster, or the probability that it belongs to a cluster, but not whether a particular instance of the string actually does belong to a cluster. A third type of clustering, the most explicit representation, stores each instance of a string separately. Each string instance is assigned to the cluster that is most appropriate for the instance\u2019s context. Word sense disambiguation systems that assign senses from WordNet (Miller, Beckwith, Fellbaum, Gross, & Miller., 1990) implicitly use this kind of clustering (e.g., Ide & Veronis, 1998; Sinha & Mihalcea, 2007)."}, {"heading": "3.2 Subproblems in Synonym Resolution", "text": "The synonym resolution problem can be divided into two subproblems: first, how to measure the similarity, or probability of synonymy, between pairs of strings in S; and second, how to form clusters such that all of the elements in each cluster have high similarity to one another, and relatively low similarity to elements in other clusters.\nResolver uses a generative, probabilistic model for finding the similarity between strings. For strings si and sj , let Ri,j be the random variable for the event that si and sj refer to the same entity. Let R t i,j denote the event that Ri,j is true, and R f i,j denote the event that it is false. Let Dx denote the set of extractions in D which contain string x. Given D and S, the first subtask of synonym resolution is to find P (Rti,j |Dsi , Dsj ) for all pairs si and sj . The second subtask takes S and the probability scores for pairs of strings from S as input. Its output is a clustering of S. Sections 4 and 5 cover Resolver\u2019s solutions to each subtask respectively."}, {"heading": "4. Models for String Comparisons", "text": "Our probabilistic model provides a formal, rigorous method for resolving synonyms in the absence of training data. It has two sources of evidence: the similarity of the strings themselves (i.e., edit distance) and the similarity of the assertions they appear in. This second source of evidence is sometimes referred to as distributional similarity (Hindle, 1990).\nSection 4.1 presents a simple model for predicting whether a pair of strings are synonymous based on string similarity. Section 4.2 then presents a model called the Extracted Shared Property (ESP) Model for predicting whether a pair of strings co-refer based on their distributional similarity. Section 4.3 compares the ESP model with other methods for computing distributional similarity to give an intuition for how it behaves. Finally, Sections 4.4 and 4.5 present a method for combining the ESP model and the string similarity model to come up with an overall prediction for synonymy decisions between two clusters of strings."}, {"heading": "4.1 String Similarity Model", "text": "Many objects appear with multiple names that are substrings, acronyms, abbreviations, or other simple variations of one another. Thus string similarity can be an important source of\nevidence for whether two strings co-refer (Cohen, 1998). Resolver\u2019s probabilistic String Similarity Model (SSM) assumes a similarity function sim(s1, s2): STRING\u00d7STRING \u2192 [0, 1]. The model sets the probability of s1 co-referring with s2 to a smoothed version of the similarity:\nP (Rti,j |sim(s1, s2)) = \u03b1 \u2217 sim(s1, s2) + 1\n\u03b1+ \u03b2\nAs \u03b1 increases, the probability estimate transitions from 1/\u03b2 (at \u03b1 = 0) to the value of the similarity function (for very large \u03b1). The particular choice of \u03b1 and \u03b2 make little difference to Resolver\u2019s results, as long as they are chosen such that the resulting probability can never be one or zero. In the experiments below, \u03b1 = 20 and \u03b2 = 5. The Monge-Elkan string similarity function (Monge & Elkan, 1996) is used for objects, and the Levenshtein string edit-distance function is used for relations (Cohen, Ravikumar, & Fienberg, 2003)."}, {"heading": "4.2 The Extracted Shared Property Model", "text": "The Extracted Shared Property Model (ESP) outputs the probability that two strings corefer based on the similarity of the extracted assertions in which they appear. For example, if the extractions (invented, Newton, calculus) and (invented, Leibniz, calculus) both appeared in the data, then Newton and Leibniz would be judged to have similar contexts in the extracted data.\nMore formally, let a pair of strings (r, s) be called a property of an object string o if there is an assertion (r, o, s) \u2208 D or (r, s, o) \u2208 D. A pair of strings (s1, s2) is an instance of a relation string r if there is an assertion (r, s1, s2) \u2208 D. Equivalently, the property p = (r, s) applies to o, and the instance i = (s1, s2) belongs to r. The ESP model outputs the probability that two strings co-refer based on how many properties (or instances) they share.\nAs an example, consider the strings Mars and Red Planet, which appear in our data 659 and 26 times respectively. Out of these extracted assertions, they share four properties. For example, (lacks, Mars, ozone layer) and (lacks, Red Planet, ozone layer) both appear as assertions in our data. The ESP model determines the probability that Mars and Red Planet refer to the same entity after observing k, the number of properties that apply to both; n1, the total number of extracted properties for Mars; and n2, the total number of extracted properties for Red Planet.\nESP models the extraction of assertions as a generative process, much like the URNS model (Downey et al., 2005). For each string si, a certain number, Pi, of properties of the string are written on balls and placed in an urn. Extracting ni assertions that contain si amounts to selecting a subset of size ni from these labeled balls.\n1 Properties in the urn are called potential properties to distinguish them from extracted properties.\nTo model synonymy decisions, ESP uses a pair of urns, containing Pi and Pj balls respectively, for the two strings si and sj . Some subset of the Pi balls have the exact same labels as an equal-sized subset of the Pj balls. Let the size of this subset be Si,j . Crucially, the ESP model assumes that synonymous strings share as many potential properties as possible, though only a few of the potential properties will be extracted for both. For non-\n1. Unlike the URNS model, balls are drawn without replacement. The TextRunner data contains only one mention of any extraction, so drawing without replacement tends to model the data more accurately.\nsynonymous strings, the set of shared potential properties is a strict subset of the potential properties of each string. Thus the central modeling choice in the ESP model is: if si and sj are synonymous (i.e., Ri,j = R t i,j) then the number of shared potential properties (Si,j) is equal to the number of potential properties in the smaller urn (min(Pi,Pj)), and if the two strings are not synonymous (Ri,j = R f i,j) then the number of shared potential properties is strictly less than the number of properties in the smaller urn (Si,j < min(Pi,Pj)). The ESP model makes several simplifying assumptions in order to make probability predictions. As is suggested by the ball-and-urn abstraction, it assumes that each ball for a string is equally likely to be selected from its urn. Because of data sparsity, almost all properties are very rare, so it would be difficult to get a better estimate for the prior probability of selecting a particular potential property. Second, balls are drawn from one urn independent of draws from any other urn. And finally, it assumes that without knowing the value of k, every value of Si,j is equally likely, since we have no better information.\nGiven these assumptions, we can derive an expression for P (Rti,j). The derivation is sketched below; see Appendix A for a complete derivation. First, note that there are (\nPi ni\n)(\nPj nj\n)\ntotal ways of extracting ni and nj assertions for si and sj . Given a particular value\nof Si,j , the number of ways in which ni and nj assertions can be extracted such that they share exactly k is given by\nCount(k,ni,nj|Pi,Pj,Si,j) = ( Si,j k ) \u2211 r,s\u22650 ( Si,j\u2212k r+s )( r+s r )( Pi\u2212Si,j ni\u2212(k+r) )( Pj\u2212Si,j nj\u2212(k+s) )\n(1)\nBy our assumptions,\nP (k|ni, nj , Pi, Pj , Si,j) = Count(k,ni,nj|Pi,Pj,Si,j) (\nPi ni\n)(\nPj nj\n) (2)\nLet Pmin = min(Pi,Pj). The result below follows from Bayes\u2019 Rule and our assumptions above:\nProposition 1 If two strings si and sj have Pi and Pj potential properties (or instances), and they appear in extracted assertions Di and Dj such that |Di| = ni and |Dj | = nj, and they share k extracted properties (or instances), the probability that si and sj co-refer is:\nP (Rti,j |Di, Dj , Pi, Pj) = P (k|ni, nj , Pi, Pj , Si,j = Pmin) \u2211\nSi,j k\u2264Si,j\u2264Pmin\nP (k|ni, nj , Pi, Pj , Si,j) (3)\nSubstituting equation 2 into equation 3 gives us a complete expression for the probability we are looking for.\nNote that the probability for Rti,j depends on two hidden parameters, Pi and Pj . Since in unsupervised synonym resolution there is no labeled data to estimate these parameters from, these parameters are tied to the number of times the respective strings si and sj are extracted: Pi = N \u00d7 ni. The discussion of experimental methods in Section 6 explains how the parameter N is set.\nAppendix B illustrates a technique for calculating the ESP model efficiently."}, {"heading": "4.3 Comparison of ESP with Other Distributional Similarity Metrics", "text": "The Discovery of Inference Rules from Text (DIRT) (Lin & Pantel, 2001) system is the most similar previous work to Resolver in its goals, but DIRT\u2019s similarity metric is very different from ESP. Like ESP, DIRT operates over triples of extracted strings and produces similarity scores for relations by comparing the distributions of one relation\u2019s arguments to another\u2019s. The DIRT system, however, has its own extraction mechanism based on a dependency parser. Here we focus on the differences in the two systems\u2019 similarity metrics, and compare performance on the same set of extracted triples produced by TextRunner, since the extracted triples used by DIRT were not available to us. We refer to the mutualinformation-based similarity metric employed by the DIRT system as sMI . It is important to note that sMI as we describe it here is our own implementation of the similarity metric described by Lin and Pantel (2001), and is not the complete DIRT system.\nWe now briefly describe sMI as it applies to a set of extractions. sMI originally was applied to only relation strings, and for simplicity we describe it that way here, but it is readily generalized to a metric for computing the similarity between two argument-1 strings or two argument-2 strings. For notational convenience, let Dx=s be the set of extractions that contain string s at position x. For example, D2=Einstein would contain the extraction (discovered, Einstein, Relativity), but not the extraction (talked with, Bohr, Einstein). Similarly, let Dx=s1,y=s2 be the set of extractions that contain s1 and s2 at positions x and y respectively. Finally, let the projection of a set of extractions D = {(d1, d2, d3)} onto one of its dimensions x be given by:\nprojx(D) = {s|\u2203d1,d2,d3 .dx = s \u2227 (d1, d2, d3) \u2208 D}\nsMI uses a mutual information score to determine how much weight to give to each string in the set of extractions during its similarity computation. For a string s at position x, the mutual information between it and a relation r at position 1 is given by:\nmi1,x(r, s) = log ( |D1=r,x=s| \u00d7 |D| |D1=r| \u00d7 |Dx=s| )\nsMI calculates the similarity between two relations by first calculating the similarity between the sets of first arguments to the relations, and then the similarity between the sets of second arguments. Let r1 and r2 be two relations, and let the position of the argument being compared be x. The similarity function used is:\nsimx(r1, r2) =\n\u2211\na\u2208projx(D1=r1 )\u2229projx(D1=r2 )\nmi1,x(r1, a) +mi1,x(r2, a)\n\u2211\na\u2208projx(D1=r1 )\nmi1,x(r1, a) + \u2211\na\u2208projx(D1=r2 )\nmi1,x(r2, a)\nThe final similarity score for two relations is the geometric average of the similarity scores for each argument:\nsMI(r1, r2) = \u221a sim2(r1, r2)\u00d7 sim3(r1, r2) (4) Applying the sMI metric to entities rather than relations simply requires projecting onto different dimensions of the relevant tuple sets.\nThe most significant difference between the sMI similarity metric and the ESP model is that the sMI metric compares the x arguments from one relation to the x arguments of the other, and then compares the y arguments from one relation to the y arguments of the other, and finally combines the scores. In contrast, ESP compares the (x, y) argument pairs of one relation to the (x, y) pairs of the other. While the sMI metric has the advantage that it is more likely to find matches between two relations in sparse data, it has the disadvantage that the matches it does find are not necessarily strong evidence for synonymy. In effect, it is capturing the intuition that synonyms have the same argument types for their domains and ranges, but it is certainly possible for non-synonyms to have similar domains and ranges. Antonyms are an obvious example. Synonyms are not defined by their domains and ranges, but rather by the mapping between them, and ESP better captures the similarity in this mapping. Experiments below (Section 6) compare the ESP as a similarity metric against sMI , as given in Equation 4.\nAs previously mentioned, there is a large body of previous work on similarity metrics (e.g., Lee, 1999). We now compare ESP with one of the more popular of these metrics, the Cosine Similarity Metric (CSM), which has previously been used in synonym resolution work (Mann & Yarowsky, 2003; Hasegawa et al., 2004). Like most traditional distributional similarity metrics, CSM operates over context vectors, rather than extracted triples. However, the ESP model is very similar to CSM in this regard. For each extracted string, it in effect creates a binary vector of properties, ones representing properties that apply to the string and zeros representing those that do not. For example, the string Einstein would have a context vector with a one in the position for the property (discovered, Relativity), and a zero in the position for the property (invented, light bulb). Both ESP and CSM calculate similarities by comparing these vectors.\nThe specific metric used to compute CSM for two vectors ~x and ~y is given by:\nsimCSM (~x, ~y) = ~x \u00b7 ~y\n||~x|| \u00d7 ||~y||\n=\n\u2211\ni xiyi \u221a\n\u2211\ni x 2 i \u00d7\n\u221a\n\u2211\ni y 2 i\nOften, techniques like term weighting or TFIDF (Salton & McGill, 1983) are used with CSM to create vectors that are not boolean, but rather have dimensions with different weights according to how informative those dimensions are. We experimented with TFIDF-like weighting schemes, where the number of times an extraction was extracted is used as the term frequency, and the number of different strings a property applies to is used as the document frequency. However, we found that these weighting schemes had negative effects on performance, so from here on we ignore them. For two boolean vectors, CSM reduces to a simple computation on the number of shared properties k and the number of extractions for each string, n1 and n2 respectively. It is given by:\nsimCSM\u2212boolean(~x, ~y) = k\u221a n1n2\n(5)\nCSM determines how similar two context vectors are in each dimension, and then adds the scores up in a weighted sum. In contrast, ESP is highly non-linear in the number\nof shared properties. As the number of matching contexts grows, the weight for each additional matching context also grows. Figure 1 compares the behavior of ESP and CSM as the number of shared properties between two strings increases. Holding the number of extractions fixed and assuming boolean vectors for CSM, it behaves as a linear function of the number of shared properties. On the other hand, the ESP has the shape of a thresholding function: it has a very low value until a threshold point around k = 10, at which point its probability estimate starts increasing rapidly. The effect is that ESP has much lower similarity scores than CSM for small numbers of matching contexts, and much higher scores for larger numbers of matching contexts. The threshold at which it switches depends on n1 and n2, as well as P1 and P2, but we can show experimentally that our method for estimating P1 and P2, though simple, can be effective. Experiments in Section 6 compare the ESP model with CSM, as computed using Equation 5."}, {"heading": "4.4 Combining the Evidence", "text": "For each potential synonymy relationship, Resolver considers two pieces of probabilistic evidence. Let Eei,j be the evidence for ESP, and let E s i,j be the evidence for SSM. Our method for combining the two uses the Na\u0308\u0131ve Bayes assumption that each piece of evidence\nis conditionally independent, given the synonymy relationship:\nP (Esi,j , E e i,j |Ri,j) = P (Esi,j |Ri,j)P (Eei,j |Ri,j) (6)\nGiven this simplifying assumption, we can combine the evidence to find the probability of a coreference relationship by applying Bayes\u2019 Rule to both sides (we omit the i, j indices for brevity):\nP (Rt|Es, Ee) = P (R t|Es)P (Rt|Ee)(1\u2212 P (Rt)) \u2211\ni\u2208{t,f} P (R i|Es)P (Ri|Ee)(1\u2212 P (Ri)) (7)"}, {"heading": "4.5 Comparing Clusters of Strings", "text": "Our algorithm merges clusters of strings with one another, using the above models. However, these models give probabilities for synonymy decisions between two individual strings, not two clusters of strings.\nWe have experimented with several different methods of determining the probability of synonymy from the individual probability scores for each pair of strings, one taken from each cluster. Initially, we followed the work of Snow, Jurafsky, and Ng (2006) in incorporating transitive closure constraints in probabilistic modeling, and we made the same independence assumptions. This approach provides a formal probabilistic framework for the problem that is simple and efficient to calculate. In other experiments, we found that simply taking the mean or geometric mean (or even the harmonic mean) of the string pair scores provided slightly improved results. For completeness, we now provide a brief explanation of the probabilistic method for combining string pair scores into cluster pair scores.\nLet a clustering be a set of synonymy relationships between pairs of strings such that the synonymy relationships obey the transitive closure property. We let the probability of a set of assertions D given a clustering C be:\nP (D|C) = \u220f\nRti,j\u2208C\nP (Di \u222aDj |Rti,j)\u00d7 \u220f\nRfi,j\u2208C\nP (Di \u222aDj |Rfi,j) (8)\nThe metric used to determine if two clusters should be merged is the likelihood ratio, or the probability for the set of assertions given the merged clusters over the probability given the original clustering. Let C \u2032 be a clustering that differs from C only in that two clusters in C have been merged in C \u2032, and let \u2206C be the set of synonymy relationships in C \u2032 that are true, but the corresponding ones in C are false. This metric is given by:\nP (D|C \u2032)/P (D|C) = \u220f Rti,j\u2208\u2206C P (Rti,j |Di \u222aDj)(1\u2212 P (Rti,j)) \u220f\nRti,j\u2208\u2206C (1\u2212 P (Rti,j |Di \u222aDj))P (Rti,j)\n(9)\nThe probability P (Rti,j |Di \u222a Dj) may be supplied by SSM, ESP, or the combination model. In our experiments, we let the prior for the SSM model be 0.5. For the ESP and combined models, we set the prior to P (Rti,j) = 1 min(Pi,Pj) , where Pi and Pj are the number of potential properties for si and sj respectively."}, {"heading": "5. Resolver\u2019s Clustering Algorithm", "text": "Synonym resolution for the Web requires a clustering algorithm that can scale to a huge number of strings in a sparse, high-dimensional space. Those requirements are difficult for any clustering algorithm. On the other hand, very few words have more than a handful of synonyms, so clusters tend to be quite small. Greedy agglomerative approaches are wellsuited to this type of clustering problem, since they start with the smallest possible clusters and merge them as needed.\nThe Resolver clustering algorithm is a version of greedy agglomerative clustering, with a key modification that allows it to scale to sparse, high-dimensional spaces and huge numbers of elements. A standard greedy clustering algorithm begins by comparing each pair of data points, and then greedily merges the closest pair. The biggest hurdle to scaling such an algorithm is that the initial step of comparing every pair of data points requires O(N2) comparisons for N points. Several proposed techniques have been able to speed up this process in practice by filtering out some of the initial pairs of points to be compared; we build on this work to provide a novel technique with a new bound of O(N logN) comparisons, under very mild assumptions.\nOur algorithm is outlined in Figure 2. It begins by calculating similarity scores between pairs of strings, in steps 1-4. Then the scores are sorted and the best cluster pairs are merged until no pair of clusters has a score above threshold. The novel part of the algorithm, step 4, compares pairs of clusters that share the same property, as long as no more than Max clusters share that same property. This step limits the number of comparisons made between clusters, and it is the reason for the algorithm\u2019s improved efficiency, as explained below.\nThis algorithm compares every pair of clusters that have the potential to be merged, assuming two properties of the data. First, it assumes that pairs of clusters with no shared properties are not worth comparing. Since the number of shared properties is a key source of evidence for our approach, these clusters almost certainly will not be merged, even if they are compared, so the assumption is quite reasonable. Second, the approach assumes that clusters sharing only properties that apply to very many strings (at least Max) need not be compared. Since properties shared by many strings provide little evidence that the strings are synonymous, this assumption is reasonable for synonym resolution.\nWe use Max = 50 in our experiments. Less than 0.1% of the distinct properties are thrown out using this cutoff, but because these discarded properties apply to many strings (at least Max), and because the number of comparisons grows with the square of the number of strings that a property applies to, the restriction drastically cuts down on the total number of comparisons made. Table 1 shows the number of comparisons made by the na\u0308\u0131ve method of comparing all pairs of strings in a set of over 2 million extractions, and the number of comparisons that Resolver makes in these experiments. Our algorithm achieves a reduction by a factor of 136 for objects and 486 for relations in the number of comparisons made. An unoptimized implementation of Resolver is able to cluster the strings in these extractions in approximately 30 minutes. Resolver was also run on a larger set containing over 100 million extractions and over 1 million distinct strings, and was able to cluster these in approximately 3.5 days on a single machine."}, {"heading": "5.1 Algorithm Analysis", "text": "Let D be the set of extracted assertions. The following analysis2 shows that one iteration of merges takes time O(|D| log |D|). Let NC be the number of comparisons between strings in step 4. To simplify the analysis, we consider only those properties that contain a relation string and an argument 1 string. Let Properties be the set of all such properties that apply to fewer than Max strings, and let Stringsp be the set of all strings that a particular\n2. If the Max parameter is allowed to vary with log |D|, rather than remaining constant, the same analysis leads to a slightly looser bound that is still better than O(|D|2).\nproperty p applies to. The number of comparisons is given by the size of the union of the set of comparisons made for each property, which is upper-bounded by the sum of the maximum number of comparisons made for each property:\nNC =\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u22c3\np\u2208Properties\n{pair = {s1, s2}|pair \u2282 Stringsp}\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 \u2211\np\u2208Properties\n|{pair = {s1, s2}|pair \u2282 Stringsp}|\n= \u2211\np\u2208Properties\n|Stringsp| \u00d7 (|Stringsp| \u2212 1) 2\nSince each Stringsp contains at most Max elements, we can upper-bound this expression by\nNC \u2264 \u2211\np\u2208Properties\n|Stringsp| \u00d7 (Max\u2212 1) 2\n= (Max\u2212 1)\n2 \u00d7\n\u2211\np\u2208Properties\n|Stringsp|\n\u2264 (Max\u2212 1) 2 \u00d7 |D|\nThe last step bounds \u2211 p |Stringsp| with |D|, since the number of extractions is equal to the number of times that each property is extracted. Since \u2211\np |Stringsp| is summing only over properties that apply to fewer than Max strings, |D| may be greater than this sum. Overall, the analysis shows that NC is linear in |D|. Note that in general this bound is quite loose because most properties apply to only a small number of strings, far fewer than Max.\nStep 5 requires time O(|D| log |D|) to sort the comparison scores and perform one iteration of merges. If the largest cluster has size K, in the worst case the algorithm will take\nK iterations (and in the best case it will take logK). In our experiments, the algorithm never took more than 9 iterations.\nThe analysis thus far has related the computational complexity to |D|, the size of the input data set of extractions. Most existing techniques, however, have been analyzed in terms of |S|, the number of distinct strings to be clustered. In order to relate the two kinds of analysis, we observe that linguistic data naturally obeys a Zipf distribution for the frequency of its distinct strings. That is, the most common string appears many times in the extractions; the next-most common appears roughly (\n1 2 )z times as often for some parameter\nz; the next most common appears roughly ( 1 3 )z times as often; and so on. The parameter z is known as the Zipf parameter, and for naturally-occurring text it has typically been observed to be around 1 (Zipf, 1932; Manning & Schuetze, 1999). If we can characterize the Zipf distribution for the input data set of extractions, we can rewrite the number of extractions |D| in terms of the number of distinct strings |S|, since |D| = \u2211s\u2208S frequency(s). Following this line of thought to its conclusion, we find that when z < 1, as it is for our data set, |D| grows linearly with |S|, and a complexity of O(|D| log |D|) is equivalent to a complexity of O(|S| log |S|). When z = 1, O(|D| log |D|) is equivalent to a bound of O(|S| log2 |S|). And when z > 1, the bound is O(|S|z log |S|). Not until z = 2 is the asymptotic bound of O(|S|2 log |S|) worse than the O(|S|2) bound for comparing all string pairs, and such a high value of z is highly unlikely for naturally occurring text. For more details and a complete analysis, see Appendix C."}, {"heading": "5.2 Relation to Other Speed-Up Techniques", "text": "McCallum, Nigam, and Ungar (2000) proposed a widely-used technique for pre-processing a data set to reduce the number of comparisons made during clustering. They use a cheap comparison metric to place objects into overlapping \u201ccanopies,\u201d and then use a more expensive metric to cluster objects appearing in the same canopy. The Resolver clustering algorithm is in fact an adaptation of the canopy method: like the Canopies method, it uses an index to eliminate many of the comparisons that would otherwise need to be made. Our method adds the restriction that strings are not compared when they share only highfrequency properties. The Canopy method works well on high-dimensional data with many clusters, which is the case with our problem. Our contribution has been to observe that if we restrict comparisons in a novel and well-justified way, we can obtain a new theoretical bound on the complexity of clustering text data.\nThe merge/purge algorithm (Hernandez & Stolfo, 1995) assumes the existence of a particular attribute such that when the data set is sorted on this attribute, matching pairs will all appear within a narrow window of one another. This algorithm is O(M logM) where M is the number of distinct strings. However, there is no attribute or set of attributes that comes close to satisfying this assumption in the context of domain-independent information extraction.\nResolver\u2019s clustering task can in part be reduced to a task of nearest-neighbor search, for which several recent systems have developed fast new algorithms. The reduction works as follows: the nearest-neighbor retrieval techniques can be used to find the most similar string for every distinct string in the corpus, and then Resolver\u2019s merge criteria can decide which of these M pairs to actually merge. Several of the fastest nearest-neighbor techniques\nperform approximate nearest-neighbor search: given an error tolerance , such techniques will return a neighbor for a query node q that is at most 1 + times as far from q as the true nearest neighbor of q.\nExamples of nearest-neighbor techniques can be divided into those that use hash-based or tree-based indexing schemes. Locality-Sensitive Hashing uses a combination of hashing functions to retrieve approximate nearest neighbors in time O(n 1\n1+ ) for error tolerance . So if for a given query point q we are willing to accept neighbors that are at a distance of at most twice the distance of its true nearest neighbor ( = 2), then the running time will be O(n 1 2 ) = O( \u221a n) to find a single nearest neighbor (Gionis, Indyk, & Motwani, 1999). More recently, tree-based index structures such as metric cover trees (Beygelzimer, Kakade, & Langford, 2006) and hybrid spill trees (Liu, Moore, Gray, & Yang, 2004), have offered competitive or even better performance than Locality-Sensitive Hashing. The tree-based algorithms have a complexity of O(d logn), where d is the dimensionality of the space, to find a single nearest neighbor. Metric trees offer exact nearest-neighbor search, and spill trees offer faster search in practice at the cost of finding approximate solutions and using more space for their index. These indexing schemes are powerful tools for nearest-neighbor search, but their dependence on the dimensionality of the space makes it costly to apply them in our case. Resolver operates in a space of hundreds of thousands of dimensions (the number of distinct extract properties), while the fastest of these techniques have been applied to spaces of around a few thousand dimensions (Liu et al., 2004). Resolver determines the exact nearest neighbor, and in fact the exact distance between all relevant pairs of points under the mild assumptions stated above, while operating in a huge-dimensional space.\n5.3 Resolver Implementation\nResolver currently exists as a Java package containing 23,338 lines of code. It has separate modules for calculating the Extracted Shared Property Model and the String Similarity Model, as well as for clustering extractions. The basic version of the system accepts a file containing tuples of strings as input, one tuple per line. Optionally, it accepts manually labeled clusters as input as well, and will use those to output precision and recall scores. The output of the system is two files containing all object clusters and relation clusters of size two or more, respectively. Optionally, the system also outputs precision and recall scores. Several other options allow the user to run extensions to the basic Resolver system, which are discussed below in Section 7.\nResolver is currently a part of the TextRunner demonstration system. The demonstration system is available for keyword searches over the Web at http://www.cs.washington.edu/research/textrunner/. This demonstration system contains extractions from several hundred million Web documents. The extractions were fed into Resolver and the resulting clusters were added to the TextRunner index so that keyword searches return results for any member of the cluster containing the keyword being searched for, and the displayed results are condensed such that members of the same cluster are not repeated."}, {"heading": "6. Experiments", "text": "Several experiments below test Resolver and ESP, and demonstrate their improvement over related techniques in paraphrase discovery, sMI (Lin & Pantel, 2001) and the Cosine Similarity Metric (CSM) (Salton & McGill, 1983; Hasegawa et al., 2004; Mann & Yarowsky, 2003). The first experiment compares the performance of the various similarity metrics, and shows that Resolver\u2019s output clusters are significantly better than ESP\u2019s or SSM\u2019s, and that ESP\u2019s clusters are in turn significantly better than sMI \u2019s or CSM\u2019s. The second experiment measures the sensitivity of the ESP model to its hidden parameter, and shows that for a very wide range of parameter settings, it is able to outperform both the sMI and CSM models."}, {"heading": "6.1 Experimental Setup", "text": "The models are tested on a data set of 2.1 million assertions extracted from a Web crawl. All models run over all assertions, but compare only those objects or relations that appear at least 25 times in the data, to give the distributional similarity models sufficient data for estimating similarity. Although this restriction limits the applicability of Resolver, we note that it is intuitive that this should be necessary for unsupervised clustering, since such systems by definition start with no knowledge about a string. They must see some number of examples before it is reasonable to expect them to make decisions about them. We also note that Downey, Schoenmackers, and Etzioni (2007) have shown for a different problem how bootstrapping techniques can leverage performance on high-frequency examples to build accurate models for low-frequency items.\nOnly proper nouns3 are compared, and only those relation strings that contain no punctuation or capital letters are compared. This helps to restrict the experiment to strings that are less prone to extraction errors. However, the models do use the other strings as features. In all, the data contains 9,797 distinct proper object strings and 10,151 distinct proper relation strings that appear at least 25 times. We created a gold standard data set by manually clustering a subset of 6,000 object and 2,000 relation strings. In total, our gold standard data sets contains 318 true object clusters and 330 true relation clusters with at least 2 elements each.\nAs noted previously (Section 3.1), polysemous strings pose a particular representational trouble for creating a gold standard data set, since there is no correct clustering that captures all of the synonymy relationships for polysemous strings, in general. We adopted the following data-oriented strategy: polysemous strings were not clustered with other strings unless there was a match for every sense of the strings that appeared in the data. For example, there have been two U.S. Presidents named \u201cRoosevelt\u201d: Theodore Roosevelt and Franklin Delano Roosevelt. After applying the criterion above, the gold standard data contained a cluster for FDR and President Franklin Roosevelt, since both referred to Franklin Delano Roosevelt unambiguously in this dataset. Likewise, President Theodore Roosevelt and Teddy Roosevelt were put into their own cluster. The terms Roosevelt and President Roosevelt, however, were used in various places to refer to both men, and so they could not\n3. The following heuristic was used to detect proper nouns: if the string consisted of only alphabetic characters, whitespace, and periods, and if the first character of every word is capitalized, it is considered a proper noun. Otherwise, it is not.\nbe clustered with either the Franklin Roosevelt cluster or the Theodore Roosevelt cluster. Since they had the same set of senses in the data, the gold standard contained a separate cluster containing just these two strings. Section 8.2 describes an extension to Resolver that handles polysemous names. Our criterion for polysemy prevented 480 potential merges in our gold standard data set between object clusters that might be synonymous. The prevented merges usually affected acronyms, first names, and words like \u201cAgency\u201d that might refer to a number of institutions, and they represent less than 10% of the strings in the gold standard object data set.\nIn addition to a gold standard data set for evaluation, we manually created a data set of development data containing 5 correct pairs of objects, 5 correct pairs of relations, and also 5 examples of incorrect pairs for each. These 20 examples were not used in the evaluation data. The development data was used to estimate a value for the ESP model\u2019s hidden parameter N , called its property multiple (see Section 4.2). We used a simple hillclimbing search procedure to find a value for N separately for objects and relations, and found that N = 30 worked best for objects on development data, and N = 500 for relations. Although the amount of data required to set this parameter effectively is very small, it is nevertheless an important topic for future work to come up with a method that will estimate this parameter in a completely unsupervised manner in order to fully automate Resolver.\nFor our comparisons, we calculated the Cosine Similarity Metric (CSM) using the technique described in Section 4.3 and Equation 5, and the sMI metric as defined in Equation 4."}, {"heading": "6.2 Clustering Analysis", "text": "Our first experiment compares the precision and recall of clusterings output by five similarity metrics: two kinds of previous work used in paraphrase discovery, CSM and sMI ; two components of Resolver, ESP and SSM; and the full Resolver system.\nThe precision and recall of a clustering is measured as follows: hypothesis clusters are matched with gold clusters such that each hypothesis cluster matches no more than one gold cluster, and vice versa. This mapping is computed so that the number of elements in hypothesis clusters that intersect with elements in the matching gold clusters is maximized. All such intersecting elements are marked correct. Any elements in a hypothesis cluster that do not intersect with the corresponding gold cluster are marked incorrect, or irrelevant if they do not appear in the gold clustering at all. Likewise, gold cluster elements are marked as found if the matching hypothesis cluster contains the same element, or not found otherwise. The precision is defined as the number of correct hypothesis elements in clusters containing at least two relevant (correct or incorrect) elements, divided by the total number of relevant hypothesis elements in clusters containing at least two relevant items. The recall is defined as the number of found gold elements in gold clusters of size at least two, divided by the total number of gold elements in clusters of size at least two. We consider only clusters of size two or more in order to focus on the interesting cases.\nEach model requires a threshold parameter to determine which scores are suitable for merging. For these experiments we arbitrarily chose a threshold of 3 for the ESP model (that is, the data needs to be 3 times more likely given the merged cluster than the unmerged clusters in order to perform the merge) and chose thresholds for the other models by hand\ndifferent from the score in the row above at p < 0.05 using the chi-squared test with one degree of freedom. Using the same test, Resolver is also significantly different from ESP, sMI , and CSM in recall on objects, and from sMI , CSM and SSM in recall on relations. Resolver\u2019s F1 on objects is a 19% increase over SSM\u2019s F1. Resolver\u2019s F1 on relations is a 28% increase over SSM\u2019s F1. No significance tests were performed on the F1 values.\nso that the difference between them and ESP would be roughly even between precision and recall, although for relations it was harder to improve the recall. Table 2 shows the precision and recall of our models."}, {"heading": "6.3 Sensitivity Analysis", "text": "The ESP model requires a parameter for the number of potential properties of a string, but the performance of ESP is not strongly sensitive to the exact value of this parameter. As described in Section 4.2, we assume that the number of potential properties is a multiple N of the number of extractions for a string. In the above experiments, we chose values of N = 30 for objects and N = 500 for relations, since they worked well on held-out data. However, as Tables 3 and 4 show, the actual values of these parameters may vary in a large range, while still enabling ESP to outperform sMI and CSM.\nIn these experiments, we measured precision and recall for just the similarity metrics, without performing any clustering. We used the similarity metrics to sort the pairs of strings (but only those pairs that share at least some property) in descending order of similarity. We then place a threshold T on the similarity, and measure precision as the number of correct synonym pairs with similarity greater than T divided by the total number of pairs with similarity greater than T . We measure recall by the number of correct synonym pairs with similarity greater than T divided by the total number of correct synonym pairs. By varying T , we can create a precision-recall curve and measure the area underneath the curve.\nThese tables highlight two significant results. First, for both objects and relations the ESP model outperforms CSM and sMI by a large amount for parameter settings that vary by close to a factor of two in either direction from the value we determined on development data. Thus although we required a small amount of data to determine a value for this parameter, the performance of ESP is not overly sensitive to the exact value. Second, the\nof parameter settings. Likewise, Resolver significantly outperforms SSM in AUC. The maximum possible area is less than one because many correct string pairs share no properties, and are therefore not compared by the clustering algorithm. The third column shows the score as a fraction of the maximum possible area under the curve, which for relations is 0.094. The improvement over baseline shows how much the ESP curves improve over sMI , and how much Resolver improves over SSM.\nESP model clearly provides a significant boost to the performance of the SSM model, as Resolver\u2019s performance significantly improves over SSM\u2019s."}, {"heading": "6.4 Discussion", "text": "In all experiments, ESP outperforms both CSM and sMI . The sensitivity analysis shows that this remains true for a wide range of hidden parameters for ESP, for both objects and relations. Moreover, ESP\u2019s improvement over the comparison metrics holds true when the metrics are used in clustering the data. sMI \u2019s performance is largely the same as CSM in every experiment. Somewhat surprisingly, sMI performs worse on relation clustering than on object clustering, even though it is designed for relation similarity.\nThe results show that the three distributional similarity models perform below the SSM model on its own for both objects and relations, both in the similarity experiments and the clustering experiments. The one exception is in the clustering experiment for relations, where SSM had a poor recall, and thus had lower F1 score than ESP and CSM. This is to be expected, since ESP, sMI , and CSM make predictions based on a very noisy signal. For example, Canada shares more properties with United States in our data than U.S. does, even though Canada appears less often than U.S. Importantly, though, there is a significant improvement in both precision and recall when using a combined model over using SSM alone. Resolver\u2019s F1 is 19% higher than SSM\u2019s on objects, and 28% higher on relations in the clustering experiments.\nInterestingly, the distributional similarity metrics (ESP, sMI , and CSM) perform significantly worse in the task of ranking string pairs than in the clustering task. One reason is that the task of ranking string pairs does not measure performance when comparing a cluster of two strings against a cluster of two other strings. In a greedy clustering process such as the one used by Resolver, large groups of correct clusters can be formed as long as the similarity metrics rank some correct pair of strings near the top, and are able to improve their estimates of similarity when comparing clusters. This issue requires further investigation.\nThere is clearly room for improvement on the synonym resolution task. Error analysis shows that most of Resolver\u2019s mistakes are due to three kinds of errors:\n1. Extraction errors. For example, US News gets extracted separately from World Report, and then Resolver clusters them together because they share almost all of the same properties.\n2. Similarity vs. Identity. For example, Larry Page and Sergey Brin get merged, as do Angelina Jolie and Brad Pitt, and Asia and Africa.\n3. Multiple word senses. For example, there are two President Bushes; also, there are many terms like President and Army that can refer to multiple distinct entities.\nExtraction systems are improving their accuracy over time, and we do not further address these errors. The next two sections develop techniques to address the second and third of these kinds of errors, respectively."}, {"heading": "7. Similar and Identical Pairs", "text": "As the error analysis above suggests, similar objects that are not exact synonyms make up a large fraction of Resolver\u2019s errors. This section describes three techniques for dealing with such errors.\nFor example, Resolver is likely to make a mistake with the pair Virginia and West Virginia. They share many properties because they have the same type (U.S. states), and they have high string similarity. Perhaps the easiest approach for determining that these two are not synonymous is simply to collect more data about them. While they are highly similar, they will certainly not share all of their properties; they have different governors, for example. However, for highly similar pairs such as these two, the amount of data required to decide that they are not identical may be huge, and simply unavailable.\nFortunately, there are more sophisticated techniques for making decisions with the available data. One approach is to consider the distribution of words that occur between candidate synonyms. Similar words are likely to be separated by conjunctions (e.g., \u201cVirginia and West Virginia\u201d) and domain-specific relations that hold between two objects of the same type (e.g., \u201cVirginia is larger than West Virginia\u201d). On the other hand, synonyms are more likely to be separated by highly specialized phrases such as \u201ca.k.a.\u201d Section 7.1 describes a method for using this information to distinguish between similar and identical pairs.\nA second approach is to consider how candidate synonyms behave in the context of relations with special distributions, like functions or inverse functions. For example, the \u201cx is capital of y\u201d relation is an inverse function: every y argument has at most one x argument4. If capitals are extracted for both West Virginia and Virginia, then they may be ruled out as a synonymous pair when the capitals are seen to be different. On the other hand, if Virginia and VA share the same capital, that is much stronger evidence that the two are the same than if they shared some other random property, such as that a town called Springfield is located there. Section 7.2 describes a method for eliminating similar pairs because they have different values for the same function or inverse function, and Section 7.3 illustrates a technique for assigning different weights to different evidence based on how close to functional the property is. Section 7.4 gives results for each of these techniques."}, {"heading": "7.1 Web Hitcounts for Synonym Discovery", "text": "While names for two similar objects may often appear together in the same sentence, it is relatively rare for two different names of the same object to appear in the same sentence. Moreover, synonymous pairs tend to appear in idiosyncratic contexts that are quite different from the contexts seen between similar pairs. Resolver exploits this fact by querying the Web to determine how often a pair of strings appears together in certain contexts in a large corpus. When the hitcount is high, Resolver can prevent the merge.\nSpecifically, given a candidate synonym pair s1 and s2, the Coordination-Phrase Filter uses a discriminator phrase (Etzioni et al., 2005) of the form \u201cs1 and s2\u201d. It then computes\n4. It is also a function.\na variant of pointwise mutual information, given by\ncoordination score(s1, s2) = hits(s1 and s2)\n2\nhits(s1)\u00d7 hits(s2)\nThe filter removes from consideration any candidate pair for which the coordination score is above a threshold, which is determined on a small development set. The results of coordination-phrase filtering are presented below.\nThe Coordination-Phrase Filter uses just one possible context between candidate synonym pairs. A simple extension is to use multiple discriminator phrases that include common context phrases like \u201cor\u201d and \u201cunlike.\u201d A more complex approach could measure the distribution of words found between a candidate pair, and compare that distribution with the distributions found between known similar or known identical pairs. These are important avenues for further investigation.\nOne drawback of this approach is that it requires text containing a pair of objects in close proximity. For a pair of rare strings, such data will be extremely unlikely to occur \u2014 this type of test exacerbates the data sparsity problem. The following two sections describe two techniques that do not suffer from this particular problem."}, {"heading": "7.2 Function Filtering", "text": "Functions and inverse functions can help to distinguish between similar and identical pairs. For example, Virginia and West Virginia have different capitals: respectively, Richmond and Charleston. If both of these facts are extracted, and if Resolver knows that the capital of relation is an inverse function, it ought to prevent Virginia and West Virginia from merging.\nGiven a candidate synonym pair x1 and x2, the Function Filter prevents merges between strings that have different values for the same function. More precisely, it decides that two strings y1 and y2 match if their string similarity is above a high threshold. It prevents a merge between x1 and x2 if there exists a function f and extractions f(x1, y1) and f(x2, y2), and there are no such extractions such that y1 and y2 match (and vice versa for inverse functions). Experiments described in Section 7.4 show that the Function Filter can improve the precision of Resolver without significantly affecting its recall.\nThe Function Filter requires knowledge about which relations are actually functions or inverse functions. Others have investigated techniques for determining such properties of relations automatically (Popescu, 2007); in the experiments, a pre-defined list of functions is used. Table 5 lists the set of functions used in the experiments for the Function Filter. These functions were selected by manually inspecting a set of 500 common relations from TextRunner\u2019s extractions, and selecting those that were reliably functional. Only a few met the criteria, partly because of polysemy in the data, and partly because of extraction noise."}, {"heading": "7.3 Function Weighting", "text": "While the Function Filter uses functions and inverse functions as negative evidence, it is also possible to use them as positive evidence. For example, the relation married is not strictly one-to-one, but for most people the set of spouses is very small. If a pair of\nstrings are extracted with the same spouse\u2014e.g., FDR and President Roosevelt share the property (married, Eleanor Roosevelt)\u2014this is far stronger evidence that the two strings are identical than if they shared some random property, such as (spoke to, reporters).\nThere are several possibilities for incorporating this insight into Resolver. First, any such technique will need some method for estimating the \u201cfunction-ness\u201d of a property, or how close the property is to being functional. We define the degree of a relation to be the number of y values that are expected to hold true for a given x value. We call a property high-degree if it is expected to apply to many strings (highly non-functional), and low-degree if it is expected to apply to few strings (close to functional).\nThe degree of a property may be estimated from the relation involved in the property and the set of extractions for that relation, or it may be based on how many objects the property applies to. For example, if there are 100 unique extractions for the married relation, and there are 80 unique x argument strings in those 100 extractions, then on average each x string participates in 100/80 = 1.25 married relations. One method might assign every property containing the married relation this statistic as the degree. On the other hand, suppose there are two extractions for the property (married, John Smith). A second method is to assign a degree of 2 to this property.\nThere are also two possible ways to incorporate the degree information into the ESP model. The ESP model may be altered so that it directly models the degrees of the properties during the process of selecting balls from urns, but this vastly complicates the model and may make it much more computationally expensive. A second option is to reweight the number of shared properties between strings based on a TF-IDF style weighting of the properties, and calculate the ESP model using this parameter instead. This requires modifying the ESP model so that it can handle non-integer values for the number of shared properties.\nIn experiments so far, one set of these options was explored, while others remain for future investigation. The Weighted Extracted Shared Property Model (W-ESP) sets the degree of a property to be the number of extractions for that property. Second, if strings si and sj share all properties p \u2208 P , it sets the value for the number of shared properties between si and sj to be\n\u2211\np\u2208P\n1\ndegree(p)\nThe ESP model has been changed to handle continuous values for the number of shared properties by changing all factorials to gamma functions, and using Stirling\u2019s approximation whenever possible.\nusing the Weighted Extracted Shared Property Model, and Resolver plus both types of filtering. Bold indicates the score is significantly different from Resolver\u2019s score at p < 0.05 using the chi-squared test with one degree of freedom. Resolver+ Coordination Phrase Filtering\u2019s F1 on objects is a 28% increase over SSM\u2019s F1, and a 7% increase over Resolver\u2019s F1.\nUnlike the Function Filter, the W-ESP model does not require additional knowledge about which relations are functional. And unlike the Coordination-Phrase Filter, it does not require Web hitcounts or a training phase. It works on extracted data, as is."}, {"heading": "7.4 Experiments", "text": "The extensions to Resolver attempt to address the confusion between similar and identical pairs. Experiments with the extensions, using the same datasets and metrics as in Section 6 demonstrate that the Function Filter (FF) and the Coordination-Phrase Filter (CPF) boost Resolver\u2019s precision. Unfortunately, the W-ESP model yielded essentially no improvement of Resolver.\nTable 6 contains the results of our experiments. With coordination-phrase filtering, Resolver\u2019s F1 is 28% higher than SSM\u2019s on objects, and 6% higher than Resolver\u2019s F1 without filtering. While function filtering is a promising idea, FF provides a smaller benefit than CPF on this dataset, and the merges that it prevents are, with a few exceptions, a subset of the merges prevented by CPF. This is in part due to the limited number of functions available in the data.\nBoth the Function Filter and the Coordination-Phrase Filter consistently blocked merges between highly similar countries, continents, planets, and people in our data, as well as some other smaller classes. The biggest difference is that CPF more consistently has hitcounts for the similar pairs that tend to be confused with identical pairs. Perhaps as the amount of extracted data grows, more functions and extractions with functions will be extracted, allowing the Function Filter to improve.\nPart of the appeal of the W-ESP model is that it requires none of the additional inputs that the other two models require, and it applies to each property, rather than to a subset of the relations like the Function Filter. Like TFIDF weighting for the Cosine Similarity Metric, the W-ESP model uses information about the distribution of the properties in the data to weight each property. For the data extracted by TextRunner, neither W-ESP nor TFIDF weighting seems to have a positive effect. More experiments are required to test\nwhether W-ESP might prove more beneficial on other data sets where TFIDF does have a positive effect.\n8. Resolver and Cross-Document Entity Resolution\nUp to this point, we have made the single-sense assumption, or the assumption that every token has exactly one meaning. While this assumption is defensible in small domains, where named entities and relations rarely have multiple meanings, even there it can cause problems: for example, the names Clinton and Bush each refer to two major players in American politics, as well as a host of other people. When extractions are taken from multiple domains, this assumption becomes more and more problematic.\nWe now describe a refinement of the Resolver system that handles the task of CrossDocument Entity Resolution (Bagga & Baldwin, 1998), in which tokens or names may have multiple referents, depending on context. An experiment below compares Resolver with an existing entity resolution system (Li et al., 2004a), and demonstrates that Resolver can handle polysemous named entities with high accuracy. This extension could theoretically be applied to highly polysemous tokens such as common nouns, but this has not yet been empirically demonstrated.\n8.1 Clustering Polysemous Names with Resolver\nRecall that the synonym resolution task is defined as finding clusters in the set of distinct strings S found in a set of extractions D (Section 3). Cross-Document Entity Resolution differs from synonym resolution in that it requires a clustering of the set of all string occurrences, rather than the set of distinct strings. For example, suppose a document contains two occurrences of the token DP, one where it means \u201cDesign Pattern\u201d and one where it means \u201cDynamic Programming.\u201d Synonym resolution systems treat DP as a single item, and will implicitly cluster both occurrences of DP together. A Cross-Document Entity Resolution system treats each occurrence of DP separately, and therefore has the potential to put each occurrence in a separate cluster when they mean different things. In this way, a Cross-Document Entity Resolution system has the potential to handle polysemous names correctly.\nBecause of the change in task definition, the sources of evidence for similarity are sparser. For each occurrence of a named entity in its input, Resolver has just a single TextRunner extraction describing the occurrence. To achieve reasonable performance, it needs more information about the context in which a named entity appears. We change Resolver\u2019s representation of entity occurrences to include the nearest E named entities in the text surrounding the occurrence. That is, each entity occurrence x is represented as a set of named entities y, where y appears among the nearest E entities in the text surrounding x. Suppose, for instance, that e1 is an occurrence of DP with Bellman and Viterbi in its context, and e2 is another occurrence with OOPSLA and Factory in its context. e1 would be represented by the set {Bellman,Viterbi}, and e2 would be represented by the set {Factory,OOPSLA}.\nTable 7 summarizes the major ways in which we extended Resolver to handle polysemous names. With these extensions in place, Resolver can proceed to cluster occurrences of entities more or less the same way that it clusters entity names for synonym resolution.\nThe SSM model works as above, and the ESP model calculates probabilities of coreference based on sets of named entities in context rather than extracted properties. The clustering algorithm eliminates comparisons between occurrences that share no part of their contexts, or only very common contextual elements. In the end, Resolver produces sets of coreferential entity occurrences, which can be used to annotate extractions containing these entity occurrences for coreference relationships."}, {"heading": "8.2 Experiment with Cross-Document Entity Resolution", "text": "We tested Resolver\u2019s ability to handle polysemous names on a data set of 300 documents from 1998-2000 New York Times articles in the TREC corpus (Voorhees, 2002). Li et al. (2004b) automatically ran a named-entity tagger on these documents and manually corrected them to identify approximately 4,000 occurrences of people\u2019s names. They then manually annotated the occurrences to form a gold standard set of coreferential clusters.\nFor each named entity occurrence in this data set, we extracted the set of the closest E named entities, with E set to 100, to represent the context for the named entity occurrence. We then ran Resolver to cluster the entity occurrences. We set ESP\u2019s latent parameter N to 30, as in the experiments above. We did not have any development data to set the merge threshold, so we used the following strategy: we arbitrarily picked a single occurrence of a common name from this data set (Al Gore), found a somewhat uncommon variant of the name (Vice President Al Gore), and set the threshold at a value just below the similarity score for this pair (7.5). For every round of merging in Resolver\u2019s clustering algorithm, we filtered the top 20 proposed merges using the Coordination Phrase Filter, with the same threshold as used in the previous experiments.\nLi et al. propose a generative model of entity coreference that we compare against. Their model requires databases of information about titles, first names, last names, genders, nicknames, and common transformations of these attributes of people\u2019s names to help compute the probability of coreference. It uses Expectation-Maximization over the given data set to compute parameters, and an inference algorithm that is O(N2) in the number of word occurrences N . Full details are provided by Li et al. (2004b).\nFollowing Li et al., we evaluate clusters using precision and recall calculated as follows: let Op be the set of entity occurrence pairs that are predicted to be coreferential (i.e., they belong to the same cluster), and let Oa denote the set of correct coreferential pairs, as calculated from the manual clustering. Then precision P = |Op\u2229Oa|\n|Op| , recall R = |Op\u2229Oa| |Oa| ,\nand F1 = 2PR P+R .\nTable 8 shows the results of running Resolver on this data set, as well as the best results reported by Li et al. (2004b) on the same data. 5 In follow-up work, Li et al. (2004a) demonstrate that their unsupervised model outperforms three supervised techniques that learn parameters for how much different attributes (first name, honorifics, etc.) contribute to the similarity of occurrence pairs.\nIn terms of absolute performance, Resolver is quite accurate in dealing with the polysemous names in this data set. Its performance on this data set is significantly higher than on TextRunner extractions, partly because it has extra information available in terms of the contexts of occurrences, and partly because it is starting out with manually labelled named entities, rather than noisy extractions.\nResolver\u2019s precision is significantly higher than Li et al.\u2019s, with roughly equal recall. Because of the large sample sizes, the differences in precision and recall are both statistically significant (two-tailed Chi-Square test with one degree of freedom, p < 0.01). In comparison with Li et al.\u2019s system, Resolver\u2019s SSM model is much less sophisticated, but it compensates by using Web data and a strong measure of distributional similarity. It does not need to rely on manually curated databases for expert knowledge about the domain, or in this case, the similarity of people\u2019s names."}, {"heading": "9. Conclusion and Future Work", "text": "We have shown that the unsupervised and scalable Resolver system is able to find clusters of coreferential object names in extracted relations with a precision of 78% and a recall of\n5. In follow-up work, Li et al. (2004a) report an F1 score of 95.1 for this task using what appears to be the same model and the same data, but the result is calculated by testing the model on 6 random splits of the data and averaging the score. We do not have access to these random splits. One possible reason that the reported results are different is that splitting up the test data reduces the number of coreference relations that need to be found and the potential number of incorrect coreference relations that can cause a system confusion.\n68% with the aid of coordination-phrase filtering, and can find clusters of coreferential relation names with precision of 90% and recall of 35%. We have demonstrated significant improvements over using existing similarity metrics for this task by employing a novel probabilistic model of synonymy. On a much cleaner set of extractions from the TREC corpus, we demonstrated that Resolver was able to achieve 97% precision and 95% recall by employing an extension that allowed it to cluster different senses of the same name into different groups.\nPerhaps the most critical aspect to extending Resolver is refining its ability to handle polysemy. Further experiments are needed to test its ability to handle new types of polysemous named entities and extracted data that has not been manually cleaned, as is the case for Li et al.\u2019s data. In addition, we plan to incorporate the ESP model into a system for unsupervised coreference resolution, including common nouns and pronouns. We will extend the model to include aspects of local salience, which can be important in coreference decisions for noun phrases other than proper names.\nCurrently we are setting the ESP model\u2019s single hidden parameter using a development set. While the required amount of data is very small, the model might be more accurate and easier to use if the hidden parameter were set by sampling the data, rather than using a development set that must be manually assembled. That is, Resolver could inspect a substantial portion of the data, and then measure how often new properties appear in the remaining data. The rate of appearance of new properties should offer a strong signal for how to set the hidden parameter.\nSeveral extensions to Resolver have dealt with ruling out highly similar non-synonyms (Section 7), with varying degrees of success at boosting Resolver\u2019s precision. We have also considered another extension to Resolver that seeks to use \u201cmutual recursion\u201d to boost recall, much like semi-supervised information extraction techniques use \u201cmutual bootstrapping\u201d between entities and patterns to increase recall (Riloff & Jones, 1999). The method begins by clustering objects, then clusters relations using the merged object clusters as properties (rather than the raw object strings), then clusters objects again using relation clusters as properties, and so on. Although we have so far been unable to boost the performance of Resolver using this technique on TextRunner data, experiments on artificial simulations suggest that under suitable conditions, mutual recursion could boost recall by as much as 16%. It remains an important area of future work to determine if there is natural data for which this technique is indeed useful, and to investigate other methods for increasing Resolver\u2019s recall."}, {"heading": "Acknowledgments", "text": "This research was supported in part by Temple University, NSF grants IIS-0535284 and IIS-0312988, ONR grant N00014-08-1-0431 as well as gifts from Google, and was carried out at the University of Washington\u2019s Turing Center and Temple University\u2019s Center for Information Science and Technology. We would like to thank the anonymous reviewers and the JAIR associate editor in charge of this paper for their helpful comments and suggestions. We would also like to thank the KnowItAll group at the University of Washington for their feedback and support."}, {"heading": "Appendix A. Derivation of the Extracted Shared Property Model", "text": "The Extracted Shared Property (ESP) Model is introduced in Section 4. It is a method for calculating the probability that two strings are synonymous, given that they share a certain number of extractions in a data set. This appendix gives a derivation of the model.\nLet si and sj be two strings, each with a set of extracted properties Ei and Ej . Let Ui and Uj be the set of potential properties for each string, contained in their respective urns. Let Si,j be the number of properties shared between the two urns, or |Ui \u2229 Uj |. Let Ri,j be the random variable for the synonymy relationship between si and sj , with Ri,j = R t i,j denoting the event that they are, and Rfi,j that they are not. The ESP model states that the probability of Rti,j is the probability of selecting the observed number of matching properties from two urns containing all matching properties, divided by the probability of selecting the observed number of matching properties from two urns which may contain some matching and some non-matching properties:\nProposition 2 If two strings si and sj have |Ui| = Pi and |Uj | = Pj potential properties (or instances), with min(Pi, Pj) = Pmin; and they appear in extracted assertions Ei and Ej such that |Ei| = ni and |Ej | = nj; and they share k extracted properties (or instances), the probability that si and sj co-refer is:\nP (Rti,j |Ei, Ej , Pi, Pj) =\n(\nPmin k\n) \u2211\nr,s\u22650\n(\nSi,j\u2212k r+s\n)(\nr+s r\n)(\nPi\u2212Pmin ni\u2212(k+r) )(Pj\u2212Pmin nj\u2212(k+s) )\n\u2211\nk\u2264Si,j\u2264Pmin (Si,j k ) \u2211 r,s\u22650 ( Si,j\u2212k r+s )( r+s r )( Pi\u2212Si,j ni\u2212(k+r) )( Pj\u2212Si,j nj\u2212(k+s)\n) (10)\nThe ESP model makes several simplifying assumptions:\n1. Balls are drawn from the urns without replacement.\n2. Draws from one urn are independent of draws from any other urn.\n3. Each ball for a string is equally likely to be selected from its urn: if U = {u1, . . . , um} and X denotes a random draw from U , P (X = ui) = 1 |U | for every ui.\n4. The prior probability for Si,j , given the number of properties in Ui and Uj , is uniform: \u22000\u2264s\u2264min(Pi,Pj)P (Si,j = s|Pi, Pj) = 1min(Pi,Pj)+1\n5. Given extracted properties for two strings and the number of potential properties for each, the probability of synonymy depends only on the number of extracted properties for each, and the number of shared properties in the extractions: P (Rti,j |Ei, Ej , Pi, Pj) = P (Rti,j |k, ni, nj , Pi, Pj).\n6. Two strings are synonymous if and only if they share as many potential properties as possible: Rti,j \u2261 (|Ui \u2229 Uj | = min(Pi, Pj)).\nBefore proving Proposition 2, we prove a simple property of urns under the assumptions above.\nLemma 1 Given n draws without replacement from an urn containing a set of properties U , the probability of selecting a particular set S \u2282 U is 1\n(|U||S|) if |S| = n, and zero otherwise.\nProof of Lemma 1: Let U = {u1, . . . , um} denote the elements of U , and let X1, . . . , Xn denote the independent draws from the urn. If n = 1, then P (S = {ui}) = P (X1 = ui) = 1|U | by assumption 3 above. Now suppose that n = n0, and that the lemma holds for every n\u2032 < n0.\nP (S = {x1, . . . , xn0 |xi \u2208 U}) = \u2211\ni\nP (Sn0\u22121 = {x1, . . . , xi\u22121, xi+1, . . . , xn0})\u00d7\nP (Xn = xi)\n= \u2211\ni\n1 (\n|U | n0\u22121\n)\n1\n|U | \u2212 n0 + 1\n= \u2211\ni\n(n0 \u2212 1)!(|U | \u2212 n0 + 1)! |U |!\n1\n|U | \u2212 n0 + 1\n= n0(n0 \u2212 1)!(|U | \u2212 n0 + 1)(|U | \u2212 n0)!\n|U |!(|U | \u2212 n0 + 1)\n= n0!(|U | \u2212 n0)!\n|U |!\n= 1\n(\n|U | n0\n)\n2\nProof of Proposition 2: We begin by transforming the desired expression, P (Rti,j |Ei, Ej , Pi, Pj), into something that can be derived from the urn model. By assumptions 5 and 6, we get\nP (Rti,j |Ei, Ej , Pi, Pj) = P (Si,j = Pmin|k, ni, nj , Pi, Pj) (11)\nThen, by applying Bayes Rule, we get\nP (Si,j = Pmin|k, ni, nj , Pi, Pj) =\nP (k|Si,j = Pmin, ni, nj , Pi, Pj)P (Si,j = Pmin|ni, nj , Pi, Pj) \u2211\nk\u2264Si,j\u2264Pmin P (k|ni, nj , Pi, Pj)P (Si,j |ni, nj , Pi, Pj)\n(12)\nSince we have assumed a uniform prior for Si,j (assumption 4), the prior terms vanish, leaving\nP (Rti,j |Ei, Ej , Pi, Pj) = P (k|Si,j = Pmin, ni, nj , Pi, Pj) \u2211\nk\u2264Si,j\u2264Pmin P (k|ni, nj , Pi, Pj)\n(13)\nThe second step of the derivation is to find a suitable expression for\nP (k|Si,j , ni, nj , Pi, Pj)\nThe probability can be written out fully as:\nP (k|Si,j , ni, nj , Pi, Pj) =\n\u2211\nEi\u2282Ui:|Ei|=ni Ej\u2282Uj :|Ej |=nj\n|Ei\u2229Ej |=k\nP (Ei, Ej |Si,j , ni, nj , Pi, Pj)\n\u2211\nEi\u2282Ui:|Ei|=ni Ej\u2282Uj :|Ej |=nj\nP (Ei, Ej |Si,j , ni, nj , Pi, Pj) (14)\nBy assumption 2, P (Ei, Ej) = P (Ei)P (Ej). By Lemma 1, all P (Ei) terms are equal, since they are all sets of size ni, and likewise for P (Ej) terms. Thus, to get the desired probability expression, we simply need to count the number of ways of taking subsets from the two urns such that they share k properties.\nP (k|Si,j , ni, nj , Pi, Pj) =\n\u2211\nEi\u2282Ui:|Ei|=ni Ej\u2282Uj :|Ej |=nj\n|Ei\u2229Ej |=k\n1\n\u2211\nEi\u2282Ui:|Ei|=ni Ej\u2282Uj :|Ej |=nj\n1 (15)\n= Count(k, ni, nj |Si,j , Pi, Pj) Count(ni, nj |Si,j , Pi, Pj)\n(16)\nThere are ( Pi ni ) ways of picking each set Ei, so\nCount(ni, nj |Si,j , Pi, Pj) = ( Pi ni )( Pj nj )\n(17)\nTo complete the derivation, we need an expression for Count(k, ni, nj | Si,j , Pi, Pj). This involves splitting the relevant sets into several parts. First Ui and Uj each contain some shared and unshared properties. Let Ti,j = Ui\u2229Uj , Vi = Ui\u2212Ti,j , and Vj = Uj\u2212Ti,j . Second, the selected sets from each urn, Ei and Ej , each have properties that come from the set of shared properties and the set of unshared properties. Let K = Ei\u2229Ej , Fi = (Ei\u2229Ti,j)\u2212K, and Fj = (Ej \u2229 Ti,j)\u2212K.\nWith these sets defined, each set Ei and Ej is composed of three distinct subsets: the shared subset (K); a subset also selected from the shared potential properties, Ti,j , but which is not shared (Fi and Fj); and the remaining elements, which are chosen from the complements of the shared properties (Vi and Vj). Since the subsets are distinct, we can count them separately and multiply the results to arrive at the final count.\nThe number of ways of selecting the shared subset is clearly (Si,j\nk\n)\n. The sizes of Fi and Fj are unknown, however, so we must sum over all possibilities. Let r = |Fi|, and s = |Fj |. There are Si,j \u2212 k remaining shared potential properties in Ti,j from which to choose the r + s elements of Fi and Fj , and then ( r+s s )\nways to split the two into distinct subsets. There are ni\u2212 (k+ r) elements left to choose in Ei, and nj \u2212 (k+ s) elements left to choose in Ej . These must be selected from the unshared potential properties in Vi and Vj , which have sizes Pi \u2212 Si,j and Pj \u2212 Si,j respectively. Putting these pieces together, we have\nCount(k, ni, nj |Si,j , Pi, Pj) = (\nSi,j k\n)\n\u2211\nr,s\n( Si,j \u2212 k r + s )( r + s s )( Pi \u2212 Si,j ni \u2212 (k + r) )( Pj \u2212 Si,j nj \u2212 (k + s) )\n(18)\nThe ranges for r and s are somewhat involved. They must obey the following constraints:\n1. r, s \u2265 0\n2. r \u2265 ni \u2212 k \u2212 Pi + Si,j\n3. s \u2265 nj \u2212 k \u2212 Pj + Si,j\n4. r \u2264 ni \u2212 k\n5. s \u2264 nj \u2212 k\n6. r + s \u2264 Si,j \u2212 k\nPlugging Equation 18 into Equation 16, and that in turn into Equation 13 yields the desired result. 2"}, {"heading": "Appendix B. Fast Calculation of the Extracted Shared Property Model", "text": "The ESP model can be expensive to calculate if done the wrong way. We use two techniques to speed up the calculation immensely. For reference, the full formulation of the model is:\nP (Rti,j |k, ni, nj , Pi, Pj) =\n(\nPmin k\n) \u2211\nr,s\u22650\n(\nSi,j\u2212k r+s\n)(\nr+s r\n)(\nPi\u2212Pmin ni\u2212(k+r) )(Pj\u2212Pmin nj\u2212(k+s) )\n\u2211\nk\u2264Si,j\u2264Pmin (Si,j k ) \u2211 r,s\u22650 ( Si,j\u2212k r+s )( r+s r )( Pi\u2212Si,j ni\u2212(k+r) )( Pj\u2212Si,j nj\u2212(k+s)\n) (19)\nNote that the equation involves three sums, ranging over O(Pmin), O(ni), and O(nj) values respectively. In effect, this is O(n3) in the number of extractions for a string. Furthermore, each step requires the expensive operation of calculating binomial coefficients. Fortunately, there are several easy ways to drastically speed up this calculation.\nFirst, Stirling\u2019s approximation can be used to calculate factorials (and therefore the binomial function). Stirling\u2019s approximation is given by:\nn! \u2248 \u221a \u03c0 ( 2n+ 1\n3\n)(\nnn en\n)\nTo avoid underflow and overflow errors, log probabilities are used everywhere possible. This calculation can then be done using a few simple multiplications and logarithm calculations. Stirling\u2019s formula converges to n! like O( 1n); in practice it proved to be accurate enough of an approximation of n! for n > 100. In ESP\u2019s implementation, all other values of n! are calculated once, and stored for future use.\nSecond, the calculation of P (k|n1, n2, P1, P2) can be sped up by simplifying the expression to get rid of two of the sums. The result is the following equivalent expression, assuming without loss of generality that P2 \u2264 P1:\nP (k|n1, n2, P1, P2) = ( P2+1 n2+1 ) \u2211n2 r=k ( r k )( P1\u2212r n1\u2212k ) (\nP2 n2\n)(\nP1 n1\n) (20)\nThis simplification removes two of the sums, and therefore changes the complexity of calculating ESP from O(P2n2n1) to O(n2). This was sufficient for our data set, but on larger data sets it might be necessary to introduce sampling techniques to improve the efficiency even further."}, {"heading": "Appendix C. A Better Bound on the Number of Comparisons Made by", "text": "the Resolver Clustering Algorithm\nSection 4 showed that the Resolver clustering algorithm initially makes O(N log N) comparisons between strings in the data, where N is the number of extractions. Heuristic methods like the Canopies method (McCallum et al., 2000) require O(M2) comparisons, where M is the number of distinct strings in the data. We claim that O(N log N) is asymptotically better than O(M2) for Zipf-distributed data.\nZipf-distributed data is controlled by a shape parameter, which we call z. The claim above holds true for any shape parameter z < 2, as shown below. Fortunately, in natural data the shape parameter is usually very close to z = 1, and in Resolver data it was observed to be z < 1.\nLet S be the set of distinct strings in a set of extractions D. For each s \u2208 S, let freq(s) denote the number of times that s appears in the extractions. Thus |D| = \u2211s\u2208S freq(s). Let M = |S| and N = |D|.\nProposition 3 If S has an observed Zipf distribution with shape parameter z, then\n1. if z < 1, N = \u0398(M)\n2. if z = 1, N = \u0398(M logM)\n3. if z > 1, N = \u0398(M z)\nProof: Let s1, . . . , sM be the elements of S in rank order from highest frequency string (s1) to lowest frequency string (sM ). Since S has an observed Zipf distribution with shape parameter z, freq(si) = Mz\niz . Given the assumptions, z and M determine the number of extractions made:\nNM,z = \u2211\ns\u2208S\nfreq(s) (21)\n= \u2211\n1\u2264i\u2264M\nM z\niz (22)\nWe can build a recurrence relation for the value of N as M changes (holding z constant) by noting that\nN2M,z = \u2211\n1\u2264i\u22642M\n(2M)z\niz (23)\n= (2M)z \u2211\n1\u2264i\u2264M\n1 iz + (2M)z\n\u2211\nM+1\u2264i\u22642M\n1 iz (24)\n= 2zNM,z + fz(M) (25)\nwhere fz(M) = \u2211 M+1\u2264i\u22642M (2M)z\niz . There are two important properties of fz(M).\n1. Note that every term in the sum for fz(M) is less than (2M)z Mz = 2 z. Thus fz(M) is\nbounded above by 2z \u00b7M , so if z is held constant, fz(M) = O(M).\n2. Every term in the sum is at least 1, so fz(M) \u2265 M and fz(M) = \u2126(M); combining these two facts yields fz(M) = \u0398(M).\nThese two properties of fz(M) will be used below. We can now use the recurrence relation and the Master Recurrence Theorem (Cormen, Leiserson, & Rivest, 1990) to prove the three claims of the proposition. For reference, the Master Recurrence Theorem states the following:\nTheorem 1 Let a \u2265 1 and b \u2265 1 be constants, let f(n) be a function, and let T (n) be defined on the non-negative integers by the recurrence\nT (n) = aT (n/b) + f(n)\nThen T(n) can be bounded asymptotically as follows.\n1. If f(n) = O(nlogb a\u2212 ) for some constant > 0, then T (n) = \u0398(nlogb a)\n2. If f(n) = \u0398(nlogb a), then T (n) = \u0398(nlogb a logn)\n3. If f(n) = \u2126(nlogb a+ ), for some constant > 0, and if af(n/b) \u2264 cf(n) for some constant c < 1 and all sufficiently large n, then T (n) = \u0398(f(n)).\nFirst consider the case where z > 1. The recurrence for NM,z can clearly be made to fit the form for Theorem 1 by setting a = 2z, b = 2, and f = fz(M). Since fz(M) is bounded above by 2z \u00b7 M = O(M), it is also clearly bounded above by O(M logb a\u2212 ) = O(M z\u2212 ), where we can take to be anything in (0, z \u2212 1). Thus the case one of Theorem 1 applies, and NM,z = \u0398(M\nlogb a) = \u0398(M z). Next consider the case where z = 1. Since fz=1(M) = \u0398(M) and \u0398(M\nlogb a) = \u0398(M), case two of Theorem 1 applies. Thus NM,z=1 = \u0398(M\nlogb a logM) = \u0398(M logM). Finally, consider the case where z < 1. Unfortunately, the regularity condition in case 3 of Theorem 1 does not hold for fz(M). Instead of using Theorem 1, we resort to a proof by induction.\nSpecifically, we show by induction that whenever z < 1 and M \u2265 2, NM,z \u2264 c \u00b7M , where c = Max(2\nz+1 2 , 2z 2\u22122z ). First, consider the case where M = 2:\nN2,z = 2 \u2211\ni=1\n2z iz\n= 2z + 1 = 2z + 1\n2 \u00b7 2\n\u2264 c \u00b7M\nWe now prove the induction case.\nNM,z \u2264 2zNM/2,z + fz(M/2) \u2264 2zcM/2 + fz(M/2) (by the induction hypothesis) \u2264 2zcM/2 + 2zM/2 = cM \u00b7 (2z\u22121 + 2z\u22121/c)\n\u2264 cM \u00b7 (2z\u22121 + 2z\u22121 2\u2212 2 z\n2z ) (by the definition of c)\n= cM\n2\nThe data used in Resolver experiments in Section 4 had a shape parameter z < 1, so the bound on the number of comparisons made was O(N logN) = O(M logM). For z = 1, the bound is O(N logN) = O(M logM log(M logM)) = O(M log2M). For z > 1, the bound is O(M z logM). Not until z = 2 would the asymptotic performance of O(M2 logM) have been worse than O(M2). If past experience is any guide, such a high value for z is unlikely for extractions from naturally occurring text."}], "references": [{"title": "Web Information Extraction and User Information Needs: Towards Closing the Gap", "author": ["E. Agichtein"], "venue": "IEEE Data Engineering Bulletin issue on Web-Scale Data,", "citeRegEx": "Agichtein,? \\Q2006\\E", "shortCiteRegEx": "Agichtein", "year": 2006}, {"title": "Web people search: results of the first evaluation and the plan for the second", "author": ["J. Artile", "S. Sekine", "J. Gonzalo"], "venue": "In Proceeding of the 17th international conference on World Wide Web", "citeRegEx": "Artile et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Artile et al\\.", "year": 2008}, {"title": "Entity-based cross-document coreferencing using the vector space model", "author": ["A. Bagga", "B. Baldwin"], "venue": "In COLING-ACL", "citeRegEx": "Bagga and Baldwin,? \\Q1998\\E", "shortCiteRegEx": "Bagga and Baldwin", "year": 1998}, {"title": "Open information extraction from the web", "author": ["M. Banko", "M.J. Cafarella", "S. Soderland", "M. Broadhead", "O. Etzioni"], "venue": "In IJCAI", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment", "author": ["R. Barzilay", "L. Lee"], "venue": "In Proc. of NAACL-HLT", "citeRegEx": "Barzilay and Lee,? \\Q2003\\E", "shortCiteRegEx": "Barzilay and Lee", "year": 2003}, {"title": "Extracting paraphrases from a parallel corpus", "author": ["R. Barzilay", "K. McKeown"], "venue": "In Proceedings of ACL/EACL", "citeRegEx": "Barzilay and McKeown,? \\Q2001\\E", "shortCiteRegEx": "Barzilay and McKeown", "year": 2001}, {"title": "Unsupervised learning of contextual role knowledge for coreference resolution", "author": ["D. Bean", "E. Riloff"], "venue": "In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL)", "citeRegEx": "Bean and Riloff,? \\Q2004\\E", "shortCiteRegEx": "Bean and Riloff", "year": 2004}, {"title": "Cover trees for nearest neighbor", "author": ["A. Beygelzimer", "S. Kakade", "J. Langford"], "venue": "In Proceeings of the 23rd International Conference on Machine Learning (ICML)", "citeRegEx": "Beygelzimer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2006}, {"title": "Relational Clustering for Multi-type Entity Resolution", "author": ["I. Bhattacharya", "L. Getoor"], "venue": "In 11th ACM SIGKDD Workshop on Multi Relational Data Mining", "citeRegEx": "Bhattacharya and Getoor,? \\Q2005\\E", "shortCiteRegEx": "Bhattacharya and Getoor", "year": 2005}, {"title": "Query-time entity resolution", "author": ["I. Bhattacharya", "L. Getoor"], "venue": "In KDD", "citeRegEx": "Bhattacharya and Getoor,? \\Q2006\\E", "shortCiteRegEx": "Bhattacharya and Getoor", "year": 2006}, {"title": "Support vector machines for paraphrase identification and corpus construction", "author": ["C. Brockett", "W.B. Dolan"], "venue": "In International Workshop on Paraphrasing", "citeRegEx": "Brockett and Dolan,? \\Q2005\\E", "shortCiteRegEx": "Brockett and Dolan", "year": 2005}, {"title": "Noun Phrase Coreference as Clustering", "author": ["C. Cardie", "K. Wagstaff"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora", "citeRegEx": "Cardie and Wagstaff,? \\Q1999\\E", "shortCiteRegEx": "Cardie and Wagstaff", "year": 1999}, {"title": "Providing database-like access to the web using queries based on textual similarity", "author": ["W.W. Cohen"], "venue": "In Proceedings of the ACM SIGMOD International Conference on Management of Data", "citeRegEx": "Cohen,? \\Q1998\\E", "shortCiteRegEx": "Cohen", "year": 1998}, {"title": "A comparison of string distance metrics for name-matching tasks. In IIWeb", "author": ["W. Cohen", "P. Ravikumar", "S. Fienberg"], "venue": null, "citeRegEx": "Cohen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2003}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 1990}, {"title": "The PASCAL Recognising Textual Entailment Challenge", "author": ["I. Dagan", "O. Glickman", "B. Magnini"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Unsupervised Discovery of Generic Relationships Using Pattern Clusters and its Evaluation by Automatically Generated SAT Analogy Questions", "author": ["D. Davidov", "A. Rappoport"], "venue": "In Proceedings of the ACL", "citeRegEx": "Davidov and Rappoport,? \\Q2008\\E", "shortCiteRegEx": "Davidov and Rappoport", "year": 2008}, {"title": "Reference reconciliation in complex information spaces", "author": ["X. Dong", "A. Halevy", "J. Madhavan"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2005}, {"title": "A Probabilistic Model of Redundancy in Information Extraction", "author": ["D. Downey", "O. Etzioni", "S. Soderland"], "venue": null, "citeRegEx": "Downey et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Downey et al\\.", "year": 2005}, {"title": "Sparse information extraction: Unsupervised language models to the rescue", "author": ["D. Downey", "S. Schoenmackers", "O. Etzioni"], "venue": "In ACL", "citeRegEx": "Downey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Downey et al\\.", "year": 2007}, {"title": "Unsupervised named-entity extraction from the web: An experimental study", "author": ["O. Etzioni", "M. Cafarella", "D. Downey", "S. Kok", "A. Popescu", "T. Shaked", "S. Soderland", "D. Weld", "A. Yates"], "venue": "Artificial Intelligence,", "citeRegEx": "Etzioni et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Etzioni et al\\.", "year": 2005}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In Proceedings of the 25th Conference on Very Large Databases (VLDB)", "citeRegEx": "Gionis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gionis et al\\.", "year": 1999}, {"title": "Unsupervised Coreference Resolution in a Nonparametric Bayesian Model", "author": ["A. Haghighi", "D. Klein"], "venue": "In Proceedings of the ACL", "citeRegEx": "Haghighi and Klein,? \\Q2007\\E", "shortCiteRegEx": "Haghighi and Klein", "year": 2007}, {"title": "Discovering relations among named entities from large corpora", "author": ["T. Hasegawa", "S. Sekine", "R. Grishman"], "venue": "In Proceedings of the ACL", "citeRegEx": "Hasegawa et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hasegawa et al\\.", "year": 2004}, {"title": "The merge/purge problem for large databases", "author": ["M.A. Hernandez", "S.J. Stolfo"], "venue": null, "citeRegEx": "Hernandez and Stolfo,? \\Q1995\\E", "shortCiteRegEx": "Hernandez and Stolfo", "year": 1995}, {"title": "Noun classification from predicage-argument structures", "author": ["D. Hindle"], "venue": null, "citeRegEx": "Hindle,? \\Q1990\\E", "shortCiteRegEx": "Hindle", "year": 1990}, {"title": "Word Sense Disambiguation: The State of the Art", "author": ["N. Ide", "J. Veronis"], "venue": "Computational Linguistics,", "citeRegEx": "Ide and Veronis,? \\Q1998\\E", "shortCiteRegEx": "Ide and Veronis", "year": 1998}, {"title": "Probabilistic coreference in information extraction", "author": ["A. Kehler"], "venue": "In EMNLP", "citeRegEx": "Kehler,? \\Q1997\\E", "shortCiteRegEx": "Kehler", "year": 1997}, {"title": "The (non)utility of predicateargument frequencies for pronoun interpretation", "author": ["A. Kehler", "D. Appelt", "L. Taylor", "A. Simma"], "venue": "In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL)", "citeRegEx": "Kehler et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kehler et al\\.", "year": 2004}, {"title": "Statistical predicate invention", "author": ["S. Kok", "P. Domingos"], "venue": "In Proceedings of the Twenty-Fourth International Conference on Machine Learning", "citeRegEx": "Kok and Domingos,? \\Q2007\\E", "shortCiteRegEx": "Kok and Domingos", "year": 2007}, {"title": "Name Discrimination and Email Clustering Using Unsupervised Clustering of Similar Contexts", "author": ["A. Kulkarni", "T. Pedersen"], "venue": "Journal of Intelligent Systems (Special Issue: Recent Advances in Knowledge-Based Systems and Their Applications),", "citeRegEx": "Kulkarni and Pedersen,? \\Q2008\\E", "shortCiteRegEx": "Kulkarni and Pedersen", "year": 2008}, {"title": "An algorithm for pronominal anaphora resolution", "author": ["S. Lappin", "H.J. Leass"], "venue": "Computational Linguistics,", "citeRegEx": "Lappin and Leass,? \\Q1994\\E", "shortCiteRegEx": "Lappin and Leass", "year": 1994}, {"title": "Measures of distributional similarity", "author": ["L. Lee"], "venue": "In Proceedings of the 37th ACL", "citeRegEx": "Lee,? \\Q1999\\E", "shortCiteRegEx": "Lee", "year": 1999}, {"title": "Word clustering and disambiguation based on co-occurence data", "author": ["H. Li", "N. Abe"], "venue": "In COLING-ACL,", "citeRegEx": "Li and Abe,? \\Q1998\\E", "shortCiteRegEx": "Li and Abe", "year": 1998}, {"title": "Identification and tracing of ambiguous names: Discriminative and generative approaches", "author": ["X. Li", "P. Morie", "D. Roth"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Li et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Li et al\\.", "year": 2004}, {"title": "Robust reading: Identification and tracing of ambiguous names", "author": ["X. Li", "P. Morie", "D. Roth"], "venue": "In Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL),", "citeRegEx": "Li et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Li et al\\.", "year": 2004}, {"title": "DIRT \u2013 Discovery of Inference Rules from Text", "author": ["D. Lin", "P. Pantel"], "venue": null, "citeRegEx": "Lin and Pantel,? \\Q2001\\E", "shortCiteRegEx": "Lin and Pantel", "year": 2001}, {"title": "An investigation of practical approximate nearest neighbor algorithms", "author": ["T. Liu", "A.W. Moore", "A. Gray", "K. Yang"], "venue": "In Proceedings of the 22nd Annual Conference on Neural Information Processing Systems (NIPS)", "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "Unsupervised personal name disambiguation", "author": ["G. Mann", "D. Yarowsky"], "venue": "In CoNLL", "citeRegEx": "Mann and Yarowsky,? \\Q2003\\E", "shortCiteRegEx": "Mann and Yarowsky", "year": 2003}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Schuetze"], "venue": null, "citeRegEx": "Manning and Schuetze,? \\Q1999\\E", "shortCiteRegEx": "Manning and Schuetze", "year": 1999}, {"title": "Efficient clustering of high-dimensional data sets with application to reference matching", "author": ["A. McCallum", "K. Nigam", "L. Ungar"], "venue": null, "citeRegEx": "McCallum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Conditional models of identity uncertainty with application to noun coreference", "author": ["A. McCallum", "B. Wellner"], "venue": null, "citeRegEx": "McCallum and Wellner,? \\Q2004\\E", "shortCiteRegEx": "McCallum and Wellner", "year": 2004}, {"title": "Using decision trees for coreference resolution", "author": ["J. McCarthy", "W. Lehnert"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence", "citeRegEx": "McCarthy and Lehnert,? \\Q1995\\E", "shortCiteRegEx": "McCarthy and Lehnert", "year": 1995}, {"title": "Introduction to WordNet: An on-line lexical database", "author": ["G.A. Miller", "R. Beckwith", "C. Fellbaum", "D. Gross", "K.J. Miller"], "venue": "International Journal of Lexicography,", "citeRegEx": "Miller et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1990}, {"title": "The Field Matching Problem: Algorithms and Applications", "author": ["A.E. Monge", "C. Elkan"], "venue": "In Knowledge Discovery and Data Mining,", "citeRegEx": "Monge and Elkan,? \\Q1996\\E", "shortCiteRegEx": "Monge and Elkan", "year": 1996}, {"title": "Improving machine learning approaches to coreference resolution", "author": ["V. Ng", "C. Cardie"], "venue": null, "citeRegEx": "Ng and Cardie,? \\Q2002\\E", "shortCiteRegEx": "Ng and Cardie", "year": 2002}, {"title": "Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences", "author": ["B. Pang", "K. Knight", "D. Marcu"], "venue": "In Proceedings of HLT/NAACL", "citeRegEx": "Pang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2003}, {"title": "Unsupervised Discrimination of Person Names in Web Contexts", "author": ["T. Pedersen", "A. Kulkarni"], "venue": "In Proceedings of the Eighth International Conference on Intelligent Text Processing and Computational Linguistics", "citeRegEx": "Pedersen and Kulkarni,? \\Q2007\\E", "shortCiteRegEx": "Pedersen and Kulkarni", "year": 2007}, {"title": "Distributional clustering of English words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "In Proceedings of the 31st ACL", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Information Extraction from Unstructured Web Text", "author": ["Popescu", "A.-M"], "venue": "Ph.D. thesis,", "citeRegEx": "Popescu and A..M.,? \\Q2007\\E", "shortCiteRegEx": "Popescu and A..M.", "year": 2007}, {"title": "A hierarchical graphical model for record linkage", "author": ["P. Ravikumar", "W.W. Cohen"], "venue": "In UAI", "citeRegEx": "Ravikumar and Cohen,? \\Q2004\\E", "shortCiteRegEx": "Ravikumar and Cohen", "year": 2004}, {"title": "Learning Dictionaries for Information Extraction by Multilevel Bootstrapping", "author": ["E. Riloff", "R. Jones"], "venue": "In Proceedings of the Sixteenth National Conference on Artificial Intelligence,", "citeRegEx": "Riloff and Jones,? \\Q1999\\E", "shortCiteRegEx": "Riloff and Jones", "year": 1999}, {"title": "Introduction to Modern Information", "author": ["Yates", "G. Etzioni Salton", "M. McGill"], "venue": null, "citeRegEx": "Yates et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Yates et al\\.", "year": 1983}, {"title": "Automatic Paraphrase Discovery based on Context and Keywords", "author": ["S. Hill. Sekine"], "venue": null, "citeRegEx": "Sekine,? \\Q2005\\E", "shortCiteRegEx": "Sekine", "year": 2005}, {"title": "Entity Resolution with Markov Logic", "author": ["P. Paraphrasing. Singla", "P. Domingos"], "venue": null, "citeRegEx": "Singla and Domingos,? \\Q2006\\E", "shortCiteRegEx": "Singla and Domingos", "year": 2006}, {"title": "Overview of the TREC-2002 question-answering track", "author": ["E. COLING/ACL. Voorhees"], "venue": "In TREC. Winkler, W", "citeRegEx": "Voorhees,? \\Q2002\\E", "shortCiteRegEx": "Voorhees", "year": 2002}, {"title": "Selective Studies and the Principle of Relative Frequency in Language", "author": ["G.K. Zipf"], "venue": null, "citeRegEx": "Zipf,? \\Q1932\\E", "shortCiteRegEx": "Zipf", "year": 1932}], "referenceMentions": [{"referenceID": 0, "context": "Web Information Extraction (WIE) systems (Zhu, Nie, Wen, Zhang, & Ma, 2005; Agichtein, 2006; Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, & Yates, 2005) extract assertions that describe a relation and its arguments from Web text.", "startOffset": 41, "endOffset": 174}, {"referenceID": 38, "context": "Mann and Yarowsky (2003) use a combination of extracted features and term vectors including proper names in context to cluster ambiguous names on the Web.", "startOffset": 0, "endOffset": 25}, {"referenceID": 53, "context": "Synonym resolution for relations is often called paraphrase discovery or paraphrase acquisition in NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005).", "startOffset": 114, "endOffset": 156}, {"referenceID": 9, "context": "Cardie and Wagstaff (1999) use a set of extracted grammatical and semantic features and an ad-hoc clustering algorithm to perform unsupervised coreference resolution, achieving better performance on the MUC-6 coreference task than a supervised system.", "startOffset": 0, "endOffset": 27}, {"referenceID": 9, "context": "Cardie and Wagstaff (1999) use a set of extracted grammatical and semantic features and an ad-hoc clustering algorithm to perform unsupervised coreference resolution, achieving better performance on the MUC-6 coreference task than a supervised system. More recently, Haghighi and Klein (2007) use a graphical model combining local salience features and global entity features to perform unsupervised coreference, achieving an F1 score of 70.", "startOffset": 0, "endOffset": 293}, {"referenceID": 9, "context": "Cardie and Wagstaff (1999) use a set of extracted grammatical and semantic features and an ad-hoc clustering algorithm to perform unsupervised coreference resolution, achieving better performance on the MUC-6 coreference task than a supervised system. More recently, Haghighi and Klein (2007) use a graphical model combining local salience features and global entity features to perform unsupervised coreference, achieving an F1 score of 70.1 on MUC-6. Two systems use automatically extracted information to help make coreference resolution decisions, much like Resolver does. Kehler, Appelt, Taylor, and Simma (2004) use statistics over automatically-determined predicateargument structures to compare contexts between pronouns and their potential antecedents.", "startOffset": 0, "endOffset": 618}, {"referenceID": 6, "context": "Bean and Riloff (2004) use targeted extraction patterns to find semantic constraints on the relationship between pronouns and their antecedents, and show that they can use these to improve an anaphora-resolution system.", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "Bean and Riloff (2004) use targeted extraction patterns to find semantic constraints on the relationship between pronouns and their antecedents, and show that they can use these to improve an anaphora-resolution system. Coreference resolution is a more difficult and general task than synonym resolution for objects since it deals with arbitrary types of noun phrases. However, systems for coreference resolution also have more information available to them in the form of local sequence and salience information, which is lost in the extraction process, and they do not address relation synonymy. Synonym resolution for relations is often called paraphrase discovery or paraphrase acquisition in NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005). Previous work in this area (Barzilay & Lee, 2003; Barzilay & McKeown, 2001; Shinyama & Sekine, 2003; Pang, Knight, & Marcu, 2003) has looked at the use of parallel, aligned corpora, such as multiple translations of the same text or multiple news reports of the same story, to find paraphrases. Brockett and Dolan (2005) have used manually-labeled data to train a supervised model of paraphrases.", "startOffset": 0, "endOffset": 1076}, {"referenceID": 6, "context": "Bean and Riloff (2004) use targeted extraction patterns to find semantic constraints on the relationship between pronouns and their antecedents, and show that they can use these to improve an anaphora-resolution system. Coreference resolution is a more difficult and general task than synonym resolution for objects since it deals with arbitrary types of noun phrases. However, systems for coreference resolution also have more information available to them in the form of local sequence and salience information, which is lost in the extraction process, and they do not address relation synonymy. Synonym resolution for relations is often called paraphrase discovery or paraphrase acquisition in NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005). Previous work in this area (Barzilay & Lee, 2003; Barzilay & McKeown, 2001; Shinyama & Sekine, 2003; Pang, Knight, & Marcu, 2003) has looked at the use of parallel, aligned corpora, such as multiple translations of the same text or multiple news reports of the same story, to find paraphrases. Brockett and Dolan (2005) have used manually-labeled data to train a supervised model of paraphrases. The PASCAL Recognising Textual Entailment Challenge (Dagan, Glickman, & Magnini, 2006) proposes the task of recognizing when two sentences entail one another, given manually labeled training data, and many authors have submitted responses to this challenge. Resolver avoids the use of labor-intensive resources, and relies solely on automatically acquired extractions from a large corpus. Several unsupervised systems for paraphrase discovery have focused on using corpusbased techniques to cluster synonymous relations. Sekine (2005) uses a heuristic similarity measure to cluster relations.", "startOffset": 0, "endOffset": 1687}, {"referenceID": 6, "context": "Bean and Riloff (2004) use targeted extraction patterns to find semantic constraints on the relationship between pronouns and their antecedents, and show that they can use these to improve an anaphora-resolution system. Coreference resolution is a more difficult and general task than synonym resolution for objects since it deals with arbitrary types of noun phrases. However, systems for coreference resolution also have more information available to them in the form of local sequence and salience information, which is lost in the extraction process, and they do not address relation synonymy. Synonym resolution for relations is often called paraphrase discovery or paraphrase acquisition in NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005). Previous work in this area (Barzilay & Lee, 2003; Barzilay & McKeown, 2001; Shinyama & Sekine, 2003; Pang, Knight, & Marcu, 2003) has looked at the use of parallel, aligned corpora, such as multiple translations of the same text or multiple news reports of the same story, to find paraphrases. Brockett and Dolan (2005) have used manually-labeled data to train a supervised model of paraphrases. The PASCAL Recognising Textual Entailment Challenge (Dagan, Glickman, & Magnini, 2006) proposes the task of recognizing when two sentences entail one another, given manually labeled training data, and many authors have submitted responses to this challenge. Resolver avoids the use of labor-intensive resources, and relies solely on automatically acquired extractions from a large corpus. Several unsupervised systems for paraphrase discovery have focused on using corpusbased techniques to cluster synonymous relations. Sekine (2005) uses a heuristic similarity measure to cluster relations. Davidov and Rappoport (2008) use a heuristic clustering method to find groups of relation patterns that can be used to extract instances.", "startOffset": 0, "endOffset": 1774}, {"referenceID": 6, "context": "Bean and Riloff (2004) use targeted extraction patterns to find semantic constraints on the relationship between pronouns and their antecedents, and show that they can use these to improve an anaphora-resolution system. Coreference resolution is a more difficult and general task than synonym resolution for objects since it deals with arbitrary types of noun phrases. However, systems for coreference resolution also have more information available to them in the form of local sequence and salience information, which is lost in the extraction process, and they do not address relation synonymy. Synonym resolution for relations is often called paraphrase discovery or paraphrase acquisition in NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005). Previous work in this area (Barzilay & Lee, 2003; Barzilay & McKeown, 2001; Shinyama & Sekine, 2003; Pang, Knight, & Marcu, 2003) has looked at the use of parallel, aligned corpora, such as multiple translations of the same text or multiple news reports of the same story, to find paraphrases. Brockett and Dolan (2005) have used manually-labeled data to train a supervised model of paraphrases. The PASCAL Recognising Textual Entailment Challenge (Dagan, Glickman, & Magnini, 2006) proposes the task of recognizing when two sentences entail one another, given manually labeled training data, and many authors have submitted responses to this challenge. Resolver avoids the use of labor-intensive resources, and relies solely on automatically acquired extractions from a large corpus. Several unsupervised systems for paraphrase discovery have focused on using corpusbased techniques to cluster synonymous relations. Sekine (2005) uses a heuristic similarity measure to cluster relations. Davidov and Rappoport (2008) use a heuristic clustering method to find groups of relation patterns that can be used to extract instances. Hasegawa et al. (2004) automatically extract relationships from a large corpus and cluster relations, using the Cosine Similarity Metric (Salton & McGill, 1983) and a hierarchical clustering technique like Resolver\u2019s.", "startOffset": 0, "endOffset": 1906}, {"referenceID": 32, "context": "Resolver\u2019s method of determining the similarity between two strings is an example of a broad class of metrics called distributional similarity metrics (Lee, 1999), but it has significant advantages over traditional distributional similarity metrics for the synonym resolution task.", "startOffset": 151, "endOffset": 162}, {"referenceID": 25, "context": "\u201d (Hindle, 1990) Previous distributional similarity metrics, however, have been designed for comparing words based on terms appearing in the same document, rather than extracted properties.", "startOffset": 2, "endOffset": 16}, {"referenceID": 27, "context": "Several supervised learning techniques make entity resolution decisions (Kehler, 1997; McCallum & Wellner, 2004; Singla & Domingos, 2006), but of course these systems depend on the availability of training data, and even on a significant number of labeled examples per relation of interest.", "startOffset": 72, "endOffset": 137}, {"referenceID": 12, "context": "Ravikumar and Cohen (2004) present an unsupervised approach to object resolution using Expectation-Maximization on a hierarchical graphical model.", "startOffset": 14, "endOffset": 27}, {"referenceID": 12, "context": "Ravikumar and Cohen (2004) present an unsupervised approach to object resolution using Expectation-Maximization on a hierarchical graphical model. Several other recent approaches leverage domain-specific information and heuristics for object resolution. For example, many (Dong, Halevy, & Madhavan, 2005; Bhattacharya & Getoor, 2005, 2006) rely on evidence from observing which strings appear as arguments to the same relation simultaneously (e.g., co-authors of the same publication). While this is useful information when resolving authors in the citation domain, it is rare to find relations with similar properties in extracted assertions. None of these approaches applies to the problem of resolving relations. Winkler (1999) provides a survey of this area.", "startOffset": 14, "endOffset": 731}, {"referenceID": 12, "context": "Ravikumar and Cohen (2004) present an unsupervised approach to object resolution using Expectation-Maximization on a hierarchical graphical model. Several other recent approaches leverage domain-specific information and heuristics for object resolution. For example, many (Dong, Halevy, & Madhavan, 2005; Bhattacharya & Getoor, 2005, 2006) rely on evidence from observing which strings appear as arguments to the same relation simultaneously (e.g., co-authors of the same publication). While this is useful information when resolving authors in the citation domain, it is rare to find relations with similar properties in extracted assertions. None of these approaches applies to the problem of resolving relations. Winkler (1999) provides a survey of this area. Several supervised learning techniques make entity resolution decisions (Kehler, 1997; McCallum & Wellner, 2004; Singla & Domingos, 2006), but of course these systems depend on the availability of training data, and even on a significant number of labeled examples per relation of interest. One promising new approach to clustering in a relational domain is the Multiple Relational Clusterings (MRC) algorithm (Kok & Domingos, 2007). This approach, though not specific to synonym resolution, can find synonyms in a set of unlabeled, relational extractions without domain-specific heuristics. The approach is quite recent, and so far no detailed experimental comparison has been conducted. Resolver\u2019s probabilistic model is partly inspired by the ball-and-urns abstraction of information extraction presented by Downey, Etzioni, and Soderland (2005) Resolver\u2019s task and probability model are different from theirs, but many of the same modeling as-", "startOffset": 14, "endOffset": 1612}, {"referenceID": 25, "context": "This second source of evidence is sometimes referred to as distributional similarity (Hindle, 1990).", "startOffset": 85, "endOffset": 99}, {"referenceID": 12, "context": "evidence for whether two strings co-refer (Cohen, 1998).", "startOffset": 42, "endOffset": 55}, {"referenceID": 18, "context": "ESP models the extraction of assertions as a generative process, much like the URNS model (Downey et al., 2005).", "startOffset": 90, "endOffset": 111}, {"referenceID": 36, "context": "It is important to note that sMI as we describe it here is our own implementation of the similarity metric described by Lin and Pantel (2001), and is not the complete DIRT system.", "startOffset": 120, "endOffset": 142}, {"referenceID": 23, "context": "We now compare ESP with one of the more popular of these metrics, the Cosine Similarity Metric (CSM), which has previously been used in synonym resolution work (Mann & Yarowsky, 2003; Hasegawa et al., 2004).", "startOffset": 160, "endOffset": 206}, {"referenceID": 56, "context": "The parameter z is known as the Zipf parameter, and for naturally-occurring text it has typically been observed to be around 1 (Zipf, 1932; Manning & Schuetze, 1999).", "startOffset": 127, "endOffset": 165}, {"referenceID": 37, "context": "Resolver operates in a space of hundreds of thousands of dimensions (the number of distinct extract properties), while the fastest of these techniques have been applied to spaces of around a few thousand dimensions (Liu et al., 2004).", "startOffset": 215, "endOffset": 233}, {"referenceID": 23, "context": "Several experiments below test Resolver and ESP, and demonstrate their improvement over related techniques in paraphrase discovery, sMI (Lin & Pantel, 2001) and the Cosine Similarity Metric (CSM) (Salton & McGill, 1983; Hasegawa et al., 2004; Mann & Yarowsky, 2003).", "startOffset": 196, "endOffset": 265}, {"referenceID": 20, "context": "Specifically, given a candidate synonym pair s1 and s2, the Coordination-Phrase Filter uses a discriminator phrase (Etzioni et al., 2005) of the form \u201cs1 and s2\u201d.", "startOffset": 115, "endOffset": 137}, {"referenceID": 55, "context": "We tested Resolver\u2019s ability to handle polysemous names on a data set of 300 documents from 1998-2000 New York Times articles in the TREC corpus (Voorhees, 2002).", "startOffset": 145, "endOffset": 161}, {"referenceID": 34, "context": "Li et al. (2004b) automatically ran a named-entity tagger on these documents and manually corrected them to identify approximately 4,000 occurrences of people\u2019s names.", "startOffset": 0, "endOffset": 18}, {"referenceID": 34, "context": "Li et al. (2004b) automatically ran a named-entity tagger on these documents and manually corrected them to identify approximately 4,000 occurrences of people\u2019s names. They then manually annotated the occurrences to form a gold standard set of coreferential clusters. For each named entity occurrence in this data set, we extracted the set of the closest E named entities, with E set to 100, to represent the context for the named entity occurrence. We then ran Resolver to cluster the entity occurrences. We set ESP\u2019s latent parameter N to 30, as in the experiments above. We did not have any development data to set the merge threshold, so we used the following strategy: we arbitrarily picked a single occurrence of a common name from this data set (Al Gore), found a somewhat uncommon variant of the name (Vice President Al Gore), and set the threshold at a value just below the similarity score for this pair (7.5). For every round of merging in Resolver\u2019s clustering algorithm, we filtered the top 20 proposed merges using the Coordination Phrase Filter, with the same threshold as used in the previous experiments. Li et al. propose a generative model of entity coreference that we compare against. Their model requires databases of information about titles, first names, last names, genders, nicknames, and common transformations of these attributes of people\u2019s names to help compute the probability of coreference. It uses Expectation-Maximization over the given data set to compute parameters, and an inference algorithm that is O(N2) in the number of word occurrences N . Full details are provided by Li et al. (2004b).", "startOffset": 0, "endOffset": 1630}, {"referenceID": 34, "context": "Following Li et al., we evaluate clusters using precision and recall calculated as follows: let Op be the set of entity occurrence pairs that are predicted to be coreferential (i.e., they belong to the same cluster), and let Oa denote the set of correct coreferential pairs, as calculated from the manual clustering. Then precision P = |Op\u2229Oa| |Op| , recall R = |Op\u2229Oa| |Oa| , and F1 = 2PR P+R . Table 8 shows the results of running Resolver on this data set, as well as the best results reported by Li et al. (2004b) on the same data.", "startOffset": 10, "endOffset": 518}, {"referenceID": 34, "context": "Following Li et al., we evaluate clusters using precision and recall calculated as follows: let Op be the set of entity occurrence pairs that are predicted to be coreferential (i.e., they belong to the same cluster), and let Oa denote the set of correct coreferential pairs, as calculated from the manual clustering. Then precision P = |Op\u2229Oa| |Op| , recall R = |Op\u2229Oa| |Oa| , and F1 = 2PR P+R . Table 8 shows the results of running Resolver on this data set, as well as the best results reported by Li et al. (2004b) on the same data. 5 In follow-up work, Li et al. (2004a) demonstrate that their unsupervised model outperforms three supervised techniques that learn parameters for how much different attributes (first name, honorifics, etc.", "startOffset": 10, "endOffset": 575}, {"referenceID": 34, "context": "In follow-up work, Li et al. (2004a) report an F1 score of 95.", "startOffset": 19, "endOffset": 37}, {"referenceID": 40, "context": "Heuristic methods like the Canopies method (McCallum et al., 2000) require O(M2) comparisons, where M is the number of distinct strings in the data.", "startOffset": 43, "endOffset": 66}], "year": 2009, "abstractText": "The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fullyimplemented system that runs in O(KN log N) time in the number of extractions, N , and the maximum number of synonyms per word, K. The system, called Resolver, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, Resolver resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of Resolver\u2019s probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve F1 by 5%. An extension to the basic Resolver system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus.", "creator": "dvips(k) 5.96 Copyright 2007 Radical Eye Software"}}}