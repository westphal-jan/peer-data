{"id": "1402.3849", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2014", "title": "Scalable Kernel Clustering: Approximate Kernel k-means", "abstract": "kernel - based clustering modes have proven ability to resolve the non - linear structure in real algebraic data. utilizing global kernel - based clustering algorithms, kernel k - means having gained popularity with to its simple iterative nature besides techniques of modification. however, its run - time demands and benefits must increase quadratically in terms via the size of the resource container, and hence, large data loads cannot be clustered efficiently. interpreting this paper, we derive an approximation machine based on complexity, therefore the global kernel ko - strategy. you learn the cluster kernel using reasonable squared similarity between a very smaller elements atop all the points in the data container. we demonstrates that the proposed iteration improved correct clustering rates of the traditional low rank kernel approximation based clustering schemes. we also demonstrate additionally its response time and query requirements are significantly lower than those at generic k - means, with only a stable spike in musical audio quality below some connected domain large node sets. we then analyze ensemble adjustment techniques to correctly integrate the variance from our database.", "histories": [["v1", "Sun, 16 Feb 2014 22:19:40 GMT  (114kb)", "http://arxiv.org/abs/1402.3849v1", "15 pages, 6 figures,extension of the work \"Approximate Kernel k-means: Solution to large scale kernel clustering\" published in KDD 2011"]], "COMMENTS": "15 pages, 6 figures,extension of the work \"Approximate Kernel k-means: Solution to large scale kernel clustering\" published in KDD 2011", "reviews": [], "SUBJECTS": "cs.CV cs.DS cs.LG", "authors": ["radha chitta", "rong jin", "timothy c havens", "anil k jain"], "accepted": false, "id": "1402.3849"}, "pdf": {"name": "1402.3849.pdf", "metadata": {"source": "CRF", "title": "Scalable Kernel Clustering: Approximate Kernel k -means", "authors": ["Radha Chitta", "Rong Jin", "Timothy C. Havens", "Anil K. Jain"], "emails": ["chittara@msu.edu,", "rongjin@cse.msu.edu,", "jain@cse.msu.edu,", "thavens@mtu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 2.\n38 49\nv1 [\ncs .C\nV ]\n1 6\nFe b\n20 14\n1\nScalable Kernel Clustering: Approximate Kernel k -means\nRadha Chitta, Rong Jin, Timothy C. Havens and Anil K. Jain\nAbstract\u2014Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data. Among various kernel-based clustering algorithms, kernel k -means has gained popularity due to its simple iterative nature and ease of implementation. However, its run-time complexity and memory footprint increase quadratically in terms of the size of the data set, and hence, large data sets cannot be clustered efficiently. In this paper, we propose an approximation scheme based on randomization, called the Approximate Kernel k-means. We approximate the cluster centers using the kernel similarity between a few sampled points and all the points in the data set. We show that the proposed method achieves better clustering performance than the traditional low rank kernel approximation based clustering schemes. We also demonstrate that it\u2019s running time and memory requirements are significantly lower than those of kernel k -means, with only a small reduction in the clustering quality on several public domain large data sets. We then employ ensemble clustering techniques to further enhance the performance of our algorithm.\nIndex Terms\u2014Clustering, Large Scale Clustering, Kernel Clustering, k -means, scalability, ensemble clustering, metaclustering.\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "Recent advances in data generation, collection and storage technologies have resulted in a digital data explosion. A study by IDC and EMC Corp1 predicted the creation of 8 trillion gigabytes of digital data by the year 2015. Massive amounts of data are generated through online services like blogs, e-mails and social networks in the form of text, images, audio and video. Clustering is one of the principal tools to efficiently organize such large amounts of data and to enable convenient access. It has found use in a multitude of applications such as web search, social network analysis, image retrieval, medical imaging, gene expression analysis, recommendation systems and market analysis [2]. Most algorithms capable of clustering large data sets assume that the clusters in the data set are linearly separable and group the objects based on\n\u2022 R. Chitta, R. Jin, and A.K. Jain are with the Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, 48824. T.C. Havens is with the Department of Electrical and Computer Engineering, Michigan Tech University, Houghton, MI, 49931. E-mail: chittara@msu.edu, {rongjin,jain}@cse.msu.edu, thavens@mtu.edu\nA previous version of this paper appeared as [1]. In this version, we extend the proposed method to use ensemble clustering techniques and further enhance its performance. We also provide a tighter bound on the kernel approximation error when compared to that of the naive Nystrom approximation method. Empirical results on two additional large data sets along with more baseline techniques are presented to demonstrate the accuracy and scalability of the proposed algorithm. 1. Refer http://idcdocserv.com/1142\ntheir pairwise Euclidean distances. On the other hand, kernel-based clustering algorithms employ a non-linear distance measure, defined in terms of a positive-definite kernel, to compute the similarity. The kernel function embeds the objects in a highdimensional feature space, in which the clusters are more likely to be separable. Kernel clustering algorithms, therefore, have the ability to capture the non-linear structure in real world data sets and, thus, usually perform better than the Euclidean distance based clustering algorithms [3].\nA number of kernel-based clustering methods such as spectral clustering [4], kernel SelfOrganizing Maps (SOM) [5] and kernel neural gas [6] have been proposed. In this study, we focus on kernel k-means [7], [8] due to its simplicity and efficiency. In addition, several studies have established the equivalence of kernel k-means and other kernel-based clustering methods, suggesting that they yield similar results [9]\u2013[11].\nKernel k-means [7], a non-linear extension of the classical k-means algorithm, replaces the Euclidean distance function d2(xa, xb) = \u2016xa\u2212xb\u20162 employed in the k-means algorithm with a non-linear kernel distance function defined as\nd2\u03ba(xa, xb) = \u03ba(xa, xa) + \u03ba(xb, xb)\u2212 2\u03ba(xa, xb), where xa \u2208 \u211cd and xb \u2208 \u211cd are two data points and \u03ba(., .) : \u211cd \u00d7 \u211cd \u2192 \u211c is the kernel function. While the kernel distance function enables the clustering algorithm to capture the non-linear structure in\n2 data, it requires computation and storage of an n \u00d7 n kernel matrix in memory, where n is the number of data points to be clustered. This renders kernel k-means non-scalable to data sets with more than a few thousands of data points, on a standard workstation. In this paper, we address the challenge posed by the large kernel matrix. The Nystrom method for kernel approximation has been successfully employed in several learning problems [12]\u2013[15]. The naive Nystrom approximation method [16] randomly samples a small number of points from the data set and computes a low rank approximation of the kernel matrix using the similarity between all the points and the sampled points. The spectral clustering algorithm was adapted to use this low rank approximate kernel [14], [17]\u2013[19]. The clusters are found by approximating the top eigenvectors of the kernel using the similarity between a subset of randomly selected data points. The proposed algorithm, named Approximate kernel k-means (aKKm), follows along the idea of the Nystrom approximation and avoids computing the full n \u00d7 n kernel matrix. We randomly select a subset ofm data points (m \u226a n), and approximate the cluster centers using vectors in the subspace spanned by this subset. This approximation requires the computation and storage of only the n \u00d7 m portion of the kernel matrix, leading to a significant speedup of kernel k-means. We demonstrate, both theoretically and empirically, that aKKm yields similar clustering performance as kernel k-means using the full kernel matrix. Unlike the spectral clustering algorithm based on the naive Nystrom extension [14], our method uses information from all the eigenvectors of the approximate kernel matrix (without explicitly computing them), thereby yielding more accurate clustering results. We further improve the efficacy of our algorithm through ensemble clustering methods."}, {"heading": "2 BACKGROUND", "text": "We first briefly describe some of the related work on large scale clustering and kernel-based clustering, and then outline the kernel k-means algorithm."}, {"heading": "2.1 Large scale clustering", "text": "A number of methods have been developed to efficiently cluster large data sets. Incremental clustering [20], [21] and divide-and-conquer based clustering algorithms [22], [23] were designed to operate in a single pass over the data points, thereby reducing the time required for clustering. Sampling based methods, such as CLARA [24] and\nCURE [25], reduce the computation time by finding the cluster centers based on a small number of randomly selected data points. The coreset algorithms [26] represent the data set using a small set of core data points and find the cluster centers using only these core data points. Clustering algorithms such as BIRCH [27] and CLARANS [28] improve the clustering efficiency by summarizing the data set into data structures like trees and graphs, thus enabling efficient data access. With the evolution of cloud computing, parallel processing techniques for clustering are gaining popularity [29], [30]. These techniques speedup the clustering process by first dividing the task into a number of independent sub-tasks that can be performed simultaneously, and then efficiently merging these solutions into the final solution. For instance, in [30], the MapReduce framework [31] is employed to speedup the k-means and the kmedians clustering algorithms. The data set is split among many processors and a small representative data sample is obtained from each of the processors. These representative data points are then clustered to obtain the cluster centers or medians."}, {"heading": "2.2 Kernel-based clustering", "text": "Most of the existing methods for large scale clustering compute the pairwise dissimilarities between the data points using the Euclidean distance measure. As a result, they cannot accurately cluster data sets that are not linearly separable. Kernel based clustering techniques address this limitation by employing a non-linear kernel distance function to capture the non-linear structure in data [3]. Various kernel-based clustering algorithms have been developed, including kernel k-means [7], [8], spectral clustering [4], kernel SOM [5] and kernel neural gas [6]. Scalability is a major challenge faced by all the kernel-based algorithms, as they require computation of the full kernel matrix whose size is quadratic in the number of data points. To the best of our knowledge, only a few attempts have been made to scale kernel clustering algorithms to large data sets. In [32], the memory requirement is reduced by dividing the kernel matrix into blocks and using one block of the kernel matrix at a time. Although this technique handles the memory complexity, it still requires the computation of the full kernel matrix. The leaders clustering algorithm is integrated with kernel k-means to reduce its computational complexity in [33]. However, this method is data orderdependent and does not always produce accurate\n3 results. Sampling methods, such as the Nystrom method [15], have been employed to obtain low rank approximation of the kernel matrix to address this challenge [14], [18]. In [34], random projection is combined with sampling to further improve the clustering efficiency. However, these methods rely on the approximation of the top eigenvectors of the kernel matrix. In our method, we propose to use the approximate kernel matrix directly to find the clusters and show that this leads to more accurate clustering of the data."}, {"heading": "2.3 Kernel k -means", "text": "Let X = {x1, x2, ..., xn} be the input data set consisting of n data points, where xi \u2208 \u211cd, C be the number of clusters and K \u2208 \u211cn\u00d7n be the kernel matrix with Kij = \u03ba(xi, xj), where \u03ba(\u00b7, \u00b7) is the kernel function. Let H\u03ba be the Reproducing Kernel Hilbert Space (RKHS) endowed by the kernel function \u03ba(\u00b7, \u00b7), and | \u00b7 |H\u03ba be the functional norm for H\u03ba. The objective of kernel k-means is to minimize the clustering error, defined as the sum of squared distances between the data points and the center of the cluster to which each point is assigned. Hence, the kernel k-means problem can be cast as the following optimization problem [11]:\nmin U\u2208P max {ck(\u00b7)\u2208H\u03ba}Ck=1\nC\u2211\nk=1\nn\u2211\ni=1\nUki|ck(\u00b7)\u2212 \u03ba(xi, \u00b7)|2H\u03ba , (1)\nwhere U = (u1, . . . ,uC) \u22a4 is the cluster membership matrix, ck(\u00b7) \u2208 H\u03ba, k \u2208 [C] are the cluster centers, and domain P = {U \u2208 {0, 1}C\u00d7n : U\u22a41 = 1}, where 1 is a vector of all ones. Let nk = u \u22a4 k 1 be the number of data points assigned to the kth cluster, and U\u0302 = (u\u03021, . . . , u\u0302C) \u22a4 = [diag(n1, . . . , nC)] \u22121U, U\u0303 = (u\u03031, . . . , u\u0303C) \u22a4 = [diag( \u221a n1, . . . , \u221a nC)] \u22121U, denote the \u21131 and \u21132 normalized membership matrices, respectively.\nThe problem in (1) can be relaxed to the following optimization problem over U [11]:\nmin U tr(K)\u2212 tr(U\u0303KU\u0303\u22a4), (2) and the optimal cluster centers found using\nck(\u00b7) = n\u2211\ni=1\nU\u0302ki\u03ba(xi, \u00b7), k \u2208 [C]. (3)\nAs indicated in (2), a naive implementation of kernel k-means requires computation and storage of the full n \u00d7 n kernel matrix K , restricting its scalability. The objective of our work is to reduce the computational complexity and the memory requirements of kernel k-means."}, {"heading": "3 APPROXIMATE KERNEL k -MEANS", "text": "A simple and naive approach for reducing the complexity of kernel k-means is to randomly sample m points from the data set to be clustered, and find the cluster centers based only on the sampled points; then assign every unsampled data point to the cluster whose center is nearest. We refer to this two-step process as the two-step kernel kmeans (tKKm), detailed in Algorithm 1. Though this approach has reduced run-time complexity and memory requirements, its performance does not match that of the kernel k-means algorithm, unless it is provided with a sufficiently large sample of data points.\nWe propose a superior approach for reducing the complexity of kernel k-means based on the fact that kernel k-means requires the full n \u00d7 n kernel matrixK only because the the cluster centers {ck(\u00b7), k \u2208 [C]} are represented as linear combinations of all the data points to be clustered (see (3)) [35]. In other words, the cluster centers lie in the space spanned by all the data points, i.e., ck(\u00b7) \u2208 Ha = span(\u03ba(x1, \u00b7), . . . , \u03ba(xn, \u00b7)), k \u2208 [C]. We can avoid computing the full kernel matrix if we restrict the cluster centers to a smaller subspace Hb \u2282 Ha. Hb should be constructed such that (i) Hb is small enough to allow efficient computation, and (ii) Hb is rich enough to yield similar clustering results as those obtained using Ha. We employ a simple approach of randomly sampling m data points (m \u226a n), denoted by X\u0302 = {x\u03021, . . . , x\u0302m}, and construct the subspace Hb = span(x\u03021, . . . , x\u0302m). Given the subspace Hb, we modify (1) as\nmin U\u2208P max {ck(\u00b7)\u2208Hb}Ck=1\nC\u2211\nk=1\nn\u2211\ni=1\nUki|ck(\u00b7)\u2212 \u03ba(xi, \u00b7)|2H\u03ba . (4)\nLet KB \u2208 \u211cn\u00d7m represent the kernel similarity matrix between data points in X and the sampled data points in X\u0302 , and K\u0302 \u2208 \u211cm\u00d7m represent the kernel similarity between the sampled data points. The following lemma allows us to reduce (4) to an optimization problem involving only the cluster membership matrix U . Lemma 1. Given the cluster membership matrix U , the optimal cluster centers in (4) are given by\nck(\u00b7) = m\u2211\ni=1\n\u03b1ki\u03ba(x\u0302i, \u00b7), (5)\nwhere \u03b1 = U\u0302KBK\u0302 \u22121. The optimization problem for U is given by\nmin U tr(K)\u2212 tr(U\u0303KBK\u0302\u22121K\u22a4B U\u0303\u22a4). (6) Proof: Let \u03d5i = (\u03ba(xi, x\u03021), . . . , \u03ba(xi, x\u0302m)) and \u03b1i = (\u03b1i1, . . . , \u03b1im) be the i-th rows of matri-\n4\nces KB and \u03b1 respectively. As ck(\u00b7) \u2208 Hb = span(x\u03021, . . . , x\u0302m), we can write ck(\u00b7) as\nck(\u00b7) = m\u2211\ni=1\n\u03b1ki\u03ba(x\u0302i, \u00b7).\nand write the objective function in (9) as C\u2211\nk=1\nn\u2211\ni=1\nUki|ck(\u00b7)\u2212 \u03ba(xi, \u00b7)|2H\u03ba\n= tr(K) + C\u2211\nk=1\n( nk\u03b1 \u22a4 k K\u0302\u03b1k \u2212 2u\u22a4k KB\u03b1k ) . (7)\nBy minimizing over \u03b1k, we have\n\u03b1k = K\u0302 \u22121K\u22a4B u\u0302k, k \u2208 [C]\nand therefore, \u03b1 = U\u0302KBK\u0302 \u22121. We complete the proof by substituting the expression for \u03b1 into (7).\nAs indicated by Lemma 1, we need to compute only KB for finding the cluster memberships\n2. When m \u226a n, this computational cost is significantly smaller than that of computing the full kernel matrix. On the other hand, when m = n, i.e., all the data points are selected for constructing the subspace Hb, we have K\u0302 = KB = K and the problem in (6) reduces to (2). We refer to the proposed algorithm as Approximate Kernel k-means (aKKm), outlined in Algorithm 2. Fig. 1 illustrates and compares this algorithm with tKKm on a 2- dimensional synthetic data set. Note that the problem in (6) can also be viewed as approximating the kernel matrix K in (4) by KBK\u0302\n\u22121K\u22a4B , which is essentially the Nystrom method for low rank matrix approximation. However, our method offers two advantages over previous learning methods which employ the Nystrom approximation. Firstly, we do not need to explicitly compute the top eigenvectors of the approximate kernel matrix, resulting in a higher speedup over kernel k-means. Secondly, our method uses the approximate kernel matrix directly to estimate the clusters instead of the top eigenvectors. We demonstrate through our analysis that this leads to a more accurate solution than that obtained by the earlier methods.\n2. K\u0302 is part of KB and therefore does not need to be computed separately.\nAlgorithm 1 Two-step Kernel k-means (tKKm)\nInput: \u2022 X = (x1, . . . , xn): the set of n data points to be clustered\n\u2022 \u03ba(\u00b7, \u00b7) : \u211cd \u00d7\u211cd 7\u2192 \u211c: kernel function \u2022 m: the number of randomly sampled data points (m \u226a n)\n\u2022 C: the number of clusters Output: Cluster membership matrix U \u2208 {0, 1}C\u00d7n\n1: Randomly select m data points from X , denoted by X\u0302 = (x\u03021, . . . , x\u0302m). 2: Compute the cluster centers, denoted by ck(\u00b7), k \u2208 [C], by\napplying kernel k-means to X\u0302 . 3: for i = 1, . . . , n do 4: Update the ith column of U by Uk\u2217i = 1 where\nk\u2217 = argmin k\u2208[C] |ck(\u00b7)\u2212 \u03ba(xi, \u00b7)|H\u03ba .\n5: end for\nAlgorithm 2 Approximate Kernel k-means (aKKm)\nInput: \u2022 X = (x1, . . . , xn): the set of n data points to be clustered\n\u2022 \u03ba(\u00b7, \u00b7) : \u211cd \u00d7\u211cd 7\u2192 \u211c: kernel function \u2022 m: the number of randomly sampled data points (m \u226a n) \u2022 C: the number of clusters \u2022 MAXITER: maximum number of iterations\nOutput: Cluster membership matrix U \u2208 {0, 1}C\u00d7n\n1: Randomly sample m data points from X , denoted by X\u0302 = (x\u03021, . . . , x\u0302m). 2: Compute KB = [\u03ba(xi, x\u0302j)]n\u00d7m and K\u0302 = [\u03ba(x\u0302i, x\u0302j)]m\u00d7m. 3: Compute T = KBK\u0302 \u22121. 4: Randomly initialize the membership matrix U . 5: Set t = 0. 6: repeat 7: Set t = t+ 1. 8: Compute the \u21131 normalized membership matrix U\u0302 by U\u0302 =\n[diag(U1)]\u22121U . 9: Calculate \u03b1 = U\u0302T . 10: for i = 1, . . . , n do 11: Update the ith column of U by Uk\u2217i = 1 where\nk\u2217 = argmin k\u2208[C]\n\u03b1\u22a4k K\u0302\u03b1k \u2212 2\u03d5 \u22a4 i \u03b1k .\nwhere \u03b1j and \u03d5j are the jth rows of matrices \u03b1 and KB , respectively.\n12: end for 13: until the membership matrix U does not change or t >\nMAXITER"}, {"heading": "4 ENSEMBLE APPROXIMATE KERNEL k - MEANS", "text": "We improve the quality of the aKKm solution by using ensemble clustering.\nThe objective of ensemble clustering [36] is to combine multiple partitions of the given data set. A popular ensemble clustering algorithm is the MetaClustering algorithm (MCLA) [37], which maximizes the average normalized mutual information. It is based on hypergraph partitioning. Given r cluster membership matrices, {U1, . . . , U r}, where U q = (uq1, . . . ,u q C)\n\u22a4, the objective of this algorithm is to find a consensus membership matrix U (c) that maximizes the Average Normalized Mutual\nInformation, defined as\nANMI = 1\nr\nr\u2211\nq=1\nNMI(U (c), U q), (8)\nwhere NMI(Ua, U b), the Normalized Mutual Information (NMI) [38] between two partitions a and b, represented by the membership matrices Ua and U b respectively, is defined by\nNMI(Ua, U b) =\nC\u2211 i=1 C\u2211 j=1 na,bi,j log\n( n.n\na,b i,j\nnai n b j\n)\n\u221a\u221a\u221a\u221a (\nC\u2211 i=1 nai log na i n\n)( C\u2211\nj=1\nnbj log nb j n\n) .\n(9) In equation (9), nai represents the number of data points that have been assigned label i in partition a, and na,bi,j represents the number of data points that have been assigned label i in partition a and label j in partition b. NMI values lie in the range [0, 1]. An NMI value of 1 indicates perfect matching between the two partitions whereas 0 indicates perfect mismatch.\nMaximizing (8) is a combinatorial optimization problem and solving it exhaustively is computationally infeasible. MCLA obtains an approximate consensus solution by representing the set of partitions as a hypergraph. Each vector uqk, k \u2208 [C], q \u2208 [r] represents a vertex in a regular undirected graph, called the meta-graph. Vertex ui is connected to vertex uj by an edge whose weight is proportional to the Jaccard similarity between the two vectors ui and uj :\nsi,j = u \u22a4 i uj\n\u2016ui\u20162 + \u2016uj\u20162 \u2212 u\u22a4i uj . (10)\nThis meta-graph is partitioned using a graph partitioning algorithm such as METIS [39] to obtain\nC balanced meta-clusters \u03c01, \u03c02, . . . \u03c0C . Each metacluster \u03c0k = { u (1) k ,u (2) k , . . .u (sk) k } , containing sk vertices, is represented by the mean vector\n\u00b5k = 1\nsk\nsk\u2211\ni=1\nu (i) k . (11)\nThe value \u00b5ki represents the association between data point xi and the k\nth cluster. Each data point xi is assigned to the meta-cluster with which it is associated the most, breaking ties randomly, i.e\nU (c) k\u2217i =\n{ 1 if k\u2217 = argmax\nk\u2208[C]\n\u00b5ki\n0 otherwise (12)\nThe aKKm algorithm is combined with MCLA to enhance its accuracy. We execute the aKKm algorithm r times with different samples from the data set and MCLA is used to integrate the partitions obtained from each execution into a consensus partition.\nMore specifically, we independently draw r samples {X\u03021, . . . , X\u0302r}, where each X\u0302 i = {x\u0302i1, . . . , x\u0302im} contains m data points. Each x\u0302ij is uniformly sampled without replacement. After the sampling of X\u0302 i is performed, the samples are replaced and the next sample X\u0302j is obtained. For each sample X\u0302 i, we first compute the kernel matrices KiB = [\u03ba(xa, x\u0302 i b)]n\u00d7m and K\u0302i = [\u03ba(x\u0302ia, x\u0302 i b)]m\u00d7m, and then execute aKKm to obtain the cluster membership matrix U i. We then combine the partitions {U i}ri=1 using MCLA to obtain the consensus cluster membership U (c).\nThis ensemble clustering algorithm is described in Algorithm 3 and illustrated in Fig. 2. On the synthetic data set in Fig. 1(a), we obtain the partitions similar to Fig. 1(e) by using a sample of 20 data points instead of 50 data points. This illustrates that the efficiency of the algorithm is improved by using\n6 ensemble clustering.\nAlgorithm 3 Ensemble aKKm\nInput: \u2022 X = (x1, . . . , xn): the set of n data points to be clustered\n\u2022 \u03ba(\u00b7, \u00b7) : \u211cd \u00d7\u211cd 7\u2192 \u211c: kernel function \u2022 m: the number of randomly sampled data points (m \u226a n) \u2022 C: number of clusters \u2022 r: number of ensemble partitions \u2022 MAXITER: maximum number of iterations\nOutput: Consensus cluster membership matrix U (c) \u2208 {0, 1}C\u00d7n\n1: for i = 1, . . . , r do 2: Randomly select m data points from X , denoted by X\u0302i. 3: Run Algorithm 2 using X\u0302i as the sampled points and obtain the cluster membership matrix U i. 4: end for MCLA: 5: Concatenate the membership matrices {U i}ri=1 to obtain an rC \u00d7 n matrix U = (u1,u2, . . . ,urC)\n\u22a4. 6: Compute the Jaccard similarity si,j between the vectors ui\nand uj , i, j \u2208 [rC] using (10). 7: Construct a complete weighted meta-graph G = (V, E),\nwhere vertex set V = {u1,u2, . . . ,urC} and each edge (ui,uj) is weighted by si,j . 8: Partition G into C meta-clusters {\u03c0k} C k=1 and compute the\nmean vectors {\u00b5k} C k=1 using (11).\n9: for i = 1, . . . , n do 10: Update the ith column of U (c) in accordance with (12). 11: end for"}, {"heading": "5 ANALYSIS OF APPROXIMATE KERNEL k -MEANS", "text": "In this section, we first show that the computational complexity of the aKKm algorithm is less than that of the kernel k-means algorithm and then we derive bounds on the difference in the clustering error achieved by our algorithm and the kernel k-means algorithm."}, {"heading": "5.1 Computational complexity", "text": "Approximate kernel k-means: The aKKm algorithm consists of two parts: kernel computation and clustering. As only an n\u00d7m portion of the kernel needs to be computed and stored, the cost of kernel computation is O(ndm), a dramatic reduction over the O(n2d) complexity of classical kernel k-means. The memory requirement also reduces to O(mn). The most expensive clustering operation is the matrix inversion K\u0302\u22121 and calculation of T = KBK\u0302\n\u22121, which has a computational cost of O(m3 + m2n). The cost of computing \u03b1 and updating the membership matrix U isO(mnCl), where l is the number of iterations needed for convergence. Hence, the overall cost of clustering is O(m3+m2n+mnCl). We can further reduce this cost by avoiding the matrix\ninversion K\u0302\u22121 and formulating the calculation of \u03b1 = U\u0302T = U\u0302KBK\u0302\n\u22121 as the following optimization problem:\nmin \u03b1\u2208\u211cC\u00d7m\n1 2 tr(\u03b1K\u0302\u03b1) \u2212 tr(U\u0302KB\u03b1\u22a4) (13)\nIf K\u0302 is well conditioned (i.e., the minimum eigenvalue of K\u0302 is significantly larger than zero), we can solve the optimization problem in (13) using the simple gradient descent method with a convergence rate of O (log(1/\u03b5)), where 1 \u2212 \u03b5 is the desired accuracy. As the cost of each step in the gradient descent method is O(m2C), the overall computational cost is only O(m2Cl log(1/\u03b5)) \u226a O(m3) when Cl \u226a m. Using this approximation, we can reduce the overall computational cost to O(m2Cl +mnCl +m2n) \u223c O(n) when m \u226a n.\nEnsemble aKKm: In the ensemble aKKm algorithm, an additional cost of O(nC2r2) is incurred for combining the partitions using MCLA [37]. However, we empirically observed that the sample size m required to achieve a satisfactory clustering accuracy is reduced considerably when compared to aKKm. This leads to a further reduction in the running time."}, {"heading": "5.2 Clustering error", "text": "Let binary random variables \u03be = (\u03be1, \u03be2, ..., \u03ben) \u22a4 \u2208 {0, 1}n represent the random sampling process, where \u03bei = 1 if xi \u2208 X\u0302 and 0 otherwise. The following proposition allows us to write the clustering error in terms of the random variable \u03be. Proposition 1. Given the cluster membership matrix U = (u1, . . . ,uC)\n\u22a4, the clustering error can be expressed as\n\u0141(U, \u03be) = tr(K) + C\u2211\nk=1\n\u0141k(U, \u03be), (14)\nwhere \u0141k(U, \u03be) is \u0141k(U, \u03be) = min \u03b1k\u2208\u211cn\n\u22122u\u22a4k K(\u03b1k\u25e6\u03be)+nk(\u03b1k\u25e6\u03be)\u22a4K(\u03b1k\u25e6\u03be). Note that \u03be = 1, where 1 is a vector of all\nones, implies that all the data points are chosen for constructing the subspace Hb, which is equivalent to kernel k-means using the full kernel matrix. As a result, \u0141(U,1) is the clustering error of the standard kernel k-means algorithm.\nThe following theorem bounds the expectation of the clustering error. Theorem 1. Given the membership matrix U , we have\n7 the expectation of \u0141(U, \u03be) bounded as follows E\u03be[\u0141(U, \u03be)] \u2264 \u0141(U,1)\n+ tr ( U\u0303 [ K\u22121 + m\nn [diag(K)]\u22121\n]\u22121 U\u0303\u22a4 ) ,\nwhere \u0141(U,1) = tr(K)\u2212 tr(U\u0303KU\u0303\u22a4).\nProof: We first bound E\u03be[\u0141k(U, \u03be)] as 1\nnk E\u03be[\u0141k(U, \u03be)]\n= E\u03be [ min \u03b1 \u22122u\u0302\u22a4k K(\u03b1 \u25e6 \u03be) + (\u03b1 \u25e6 \u03be)\u22a4K(\u03b1 \u25e6 \u03be) ] \u2264 min \u03b1 E\u03be [ \u22122u\u0302\u22a4k K(\u03b1 \u25e6 \u03be) + (\u03b1 \u25e6 \u03be)\u22a4K(\u03b1 \u25e6 \u03be) ]\n= min \u03b1 \u22122m n u\u0302 \u22a4 k K\u03b1+\nm2 n2 \u03b1\u22a4K\u03b1\n+ m\nn\n( 1\u2212 m\nn\n) \u03b1\u22a4diag(K)\u03b1\n\u2264 min \u03b1 \u22122m n u\u0302 \u22a4 k K\u03b1+ m n \u03b1\u22a4 (m n K + diag(K) ) \u03b1.\nBy minimizing over \u03b1, we obtain\n\u03b1\u2217 = (m n K + diag(K) )\u22121 Ku\u0302k.\nThus, E\u03be[\u0141k(U, \u03be)] is bounded as\nE\u03be[\u0141k(U, \u03be)] + nku\u0302 \u22a4 k Ku\u0302k\n\u2264 nku\u0302\u22a4k ( K \u2212K [ K + n\nm diag(K)\n]\u22121 K ) u\u0302k\n= u\u0303\u22a4k\n( K\u22121 + m\nn [diag(K)]\u22121\n)\u22121 u\u0303k.\nWe complete the proof by adding up E\u03be[\u0141k(U, \u03be)] and using the fact that\n\u0141k(U,1) = min \u03b1\n\u22122u\u22a4k K\u03b1+ nk\u03b1\u22a4K\u03b1 = \u2212u\u0303\u22a4k Ku\u0303k.\nThe following corollary interprets the result of the above theorem in terms of the eigenvalues of K . Let \u03bb1 \u2265 \u03bb2 \u2265 . . . \u2265 \u03bbn \u2265 0 and Z = (z1, z2, . . . , zn) be the eigenvalues and the corresponding eigenvectors of K . Corollary 1. Given the membership matrix U , we have\nE\u03be[\u0141(U, \u03be)]\n\u0141(U,1) \u2264 1 +\n\u2211C i=1 \u03bbi/[1 + \u03bbim/n]\ntr(K)\u2212\u2211Ci=1 \u03bbi\n\u2264 1 + C/m\u2211n i=C+1 \u03bbi/n .\nProof: As \u03ba(x, x) \u2264 1 for any x, we have diag(K) I , where I is an identity matrix. As U\u0303 is an \u21132 normalized matrix, we have\ntr ( U\u0303 [ K\u22121 + m\nn [diag(K)]\u22121\n]\u22121 U\u0303\u22a4 )\n\u2264 tr ( U\u0303 [ K\u22121 + m n I ]\u22121 U\u0303\u22a4 ) \u2264 C\u2211\ni=1\n\u03bbi 1 +m\u03bbi/n \u2264 Cn m\nand\n\u0141(U,1) = tr(K \u2212 UKU\u22a4) \u2265 tr(K)\u2212 C\u2211\ni=1\n\u03bbi.\nWe complete the proof by combining the above inequalities. As an illustration of the result of Corollary 1, consider a special kernel matrix K that has its first a eigenvalues equal to n/a and the remaining eigenvalues equal to zero; i.e. \u03bb1 = . . . = \u03bba = n/a and \u03bba+1 = . . . = \u03bbn = 0. Assuming a > 2C, i.e. the number of non-zero eigenvalues of K is larger than twice the number of clusters, we have E\u03be[L(U, \u03be)]\u2212 \u0141(U,1)\n\u0141(U,1) \u2264 1 + Ca m(a\u2212 C) \u2264 1 + 2C m .\n(15) This indicates that when the number of non-zero eigenvalues of K is significantly larger than the number of clusters, the difference in the clustering errors between the standard kernel k-means and our approximation scheme decreases at the rate of O(1/m)."}, {"heading": "5.3 Parameter sensitivity", "text": "One of the important factors that determines the performance of the approximate kernel k-means algorithm is the sample size m. Sampling introduces a trade-off between clustering quality and efficiency. As m increases, the clustering quality improves but the speedup achieved by the algorithm suffers. The following theorem gives an estimate of m. Theorem 2. Let \u03a31 = diag(\u03bb1, . . . , \u03bbC), \u03a32 = diag(\u03bbC+1, . . . , \u03bbn), Z1 = (z1, . . . , zC) and Z = (zC+1, . . . , zn). Let\n\u03c4 = n max 1\u2264i\u2264n |Z(i)|2 (16) denote the coherence of the kernel matrix K (adapted from [16]). For any \u01eb \u2208 (0, 1), the spectral norm of the approximation error is bounded above, with probability 1\u2212 \u03b4, as\n\u2016K \u2212KBK\u0302\u22121K\u22a4B\u20162 \u2264 \u03bbC+1 ( 1 + 8\u03c4 ln 2\n\u03b4\n\u221a Cn\nm\n) ,\nprovided m \u2265 \u03c4Cmax(C1 ln p, C2 ln(3/\u03b4)), for some positive constants C1 and C2. A small sample size m suffices if the coherence measure \u03c4 is low and there is a large gap in the eigenspectrum. The approximation error reduces at a rate of O(1/ \u221a m)3. The reader is referred to the appendix for the proof of this theorem. In our experiments, we examined the performance of our algorithm for different sample sizes\n3. This improves over the approximation error bound of the naive Nystrom method presented in [16].\n8 Data set Size Dimensionality Kernel function Imagenet 20,000 17,000 Pyramid MNIST 70,000 784 Neural\nForest Cover Type 581,012 54 RBF Network Intrusion 4,898,431 50 Polynomial\nTABLE 1: Data set summary\n(m) ranging from 0.001% to 15% of the data set size n, and observed that setting m equal to 0.01% to 0.05% of n leads to a satisfactory performance."}, {"heading": "6 EXPERIMENTAL RESULTS", "text": "In this section, we show that aKKm is an efficient and scalable variant of the kernel k-means algorithm. It has lower run-time and memory requirements but is on par with kernel k-means in terms of the clustering error and clustering quality. We tested our algorithm on four data sets with varying sizes: Imagenet, MNIST, Forest Cover Type, and Network Intrusion data sets (Table 1). Using small and medium-sized data sets (Imagenet and MNIST) for which the full kernel calculation is feasible on a single processor, we demonstrate that our algorithm\u2019s clustering performance is similar to that of the kernel k-means algorithm. We then demonstrate scalability using the large Forest Cover Type and Network Intrusion data sets. Finally, we show that the performance can be improved by using the ensemble aKKm algorithm. All algorithms were implemented in MATLAB4 and run on an 2.8 GHz processor. The memory used was explicitly limited to 40 GB."}, {"heading": "6.1 Performance comparison with kernel k - means", "text": "We use the Imagenet and MNIST data sets to demonstrate that the approximate kernel k-means algorithm\u2019s clustering performance is similar to that of the kernel k-means algorithm."}, {"heading": "6.1.1 Datasets", "text": "\u2022 Imagenet: The Imagenet data set [41] consists of over 1.2 million images that are organized according to the WordNet hierarchy. Each node in this hierarchy represents a concept (known as the \u201csynset\u201d). We chose 20, 000 images from 12 synsets. We extracted keypoints from each image using the VLFeat library [42] and represented each keypoint as a 128 dimensional\n4. We used the k-means implementation in the MATLAB Statistics Toolbox and the Nystrom approximation based spectral clustering implementation [40] available at http://alumni.cs.ucsb.edu/\u223cwychen/sc.html. The remaining algorithms were implemented in-house.\nSIFT descriptor; an average of 3, 055 keypoints were extracted from each image. \u2022 MNIST: The MNIST data set [43] is a subset of the database of handwritten digits available from NIST. It contains 60, 000 training images and 10, 000 test images from 10 classes. Each image is represented using a 784-dimensional feature vector. For the purpose of clustering, we combined the training and test images to form a data set with 70, 000 images."}, {"heading": "6.1.2 Experimental setup", "text": "We first compare the approximate kernel k-means (aKKm) algorithm with the kernel k-means algorithm to show that they achieve similar performance. We then compare it to the two-step kernel k-means (tKKm) algorithm and demonstrate that aKKm is superior. We also gauge it\u2019s performance against that of (i) the Nystrom spectral clustering algorithm (nysSC) [14], which clusters the top C eigenvectors of a low rank approximate kernel obtained through the Nystrom approximation technique, (ii) the leaders based kernel k-means algorithm (lKKm) [33] which finds a few representative patterns (called leaders) based on a user-defined distance threshold and then runs kernel k-means on the leaders, and (iii) the k-means algorithm to show that it achieves a better clustering accuracy. For the Imagenet data set, we employ the spatial pyramid kernel [44] to calculate the pairwise similarity with the number of pyramid levels set to be 4. This has been shown to be effective for object recognition and image retrieval. It took 24, 236 seconds to compute the multi-resolution histograms and the pyramid representation of the data. On the MNIST data set, we use the neural kernel defined as \u03ba(x, y) = tanh(ax\u22a4y + b), with the parameters a and b set to 0.0045 and 0.11 respectively, as suggested in [32]. We evaluate the efficiency of the aKKm algorithm for different sample sizes ranging from 100 to 5, 000. We directly compute K\u0302\u22121 instead of using the approximation method in (13) to demonstrate that our algorithm is efficient in spite of a naive implementation. The number of clusters C is set equal to the number of true classes in the data set. We measure the time taken for computing the kernel matrix (when applicable) and clustering the data points. We measure the clustering performance using error reduction, defined as the ratio of the difference between the initial clustering error (on random initialization) and the final clustering error (after running the clustering algorithm) to the initial\n9 clustering error. The larger the error reduction, the greater is the cluster compactness. To evaluate the difference between the clustering results of the aKKm and tKKm algorithms, and the kernel k-means, we calculate the Adjusted Rand Index (ARI) [45], a measure of similarity between two data partitions. The adjusted Rand index value lies in [0, 1]. A value close to 1 indicates better matching between the two partitions than a value close to 0. Finally, we measure the clustering accuracy in terms of the NMI with respect to the true class labels. All the results are averaged over 10 runs of the algorithms."}, {"heading": "6.1.3 Experimental results", "text": "Imagenet: Tables 2(a) and 3, and Figs. 4(a) and 5(a) compare the performance of the aKKm algorithm on the Imagenet data set with the kernel k-means, k-means, lKKm, tKKm, and nysSC clustering algorithms. Table 2(a) lists the running time of all the algorithms. The kernel computation time is common to both the aKKm and tKKm algorithms. We observe that a significant speedup (over 90%) is achieved by both the algorithms in kernel computation when compared to kernel k-means. tKKm is the most efficient in terms of the clustering speedup. aKKm takes longer as it needs to compute the inverse matrix K\u0302\u22121. However, when the algorithms are compared with the requirement that they yield the same clustering performance, we will see later that aKKm is more efficient. For m \u2264 1, 000, aKKm is even faster than the k-means algorithm executed on the pyramid features. This is due to the high dimensionality of the pyramid features, which plays an important role in the complexity of k-means. In Table 3 (columns 2-3), we observe that the ARI values of the aKKm algorithm are much higher than those of the tKKm algorithm. This shows that aKKm obtains partitions that are similar to the kernel k-means partitions. When m = 5, 000, the partitions obtained by aKKm are, on an average, 84% similar to the partitions generated by kernel k-means. Figs. 4(a) and 5(a) show the error reduction and NMI plots, respectively. We again observe that the aKKm algorithm achieves performance similar to that of the kernel k-means. The tKKm algorithm achieves much lower error reduction. The tKKm, lKKm and nysSC algorithms yield lower NMI values. With just 100 sampled points, aKKm significantly outperforms tKKm provided with 1, 000 sampled points, and achieves the same perfor-\nmance as the nysSC algorithm. This observation indicates that it is insufficient to estimate the cluster centers using only the randomly sampled data points as in the tKKm method, further justifying the design of the aKKm algorithm. As expected, all the kernel-based algorithms perform better than the k-means algorithm.\nMNIST: Table 2(b) shows the results for the MNIST data set using the neural kernel. Unlike the Imagenet data set, more time is spent in clustering than in kernel calculation due to the simplicity of the kernel function. As observed in the Imagenet data set, a significant amount of time was saved by the aKKm algorithm as well as by the tKKm algorithm, when compared to the kernel k-means algorithm. When the sample size is small (m < 5, 000), the aKKm algorithm is also faster than the k-means algorithm. Though the tKKm algorithm is more efficient than the aKKm algorithm, the ARI values in Table 3 (columns 4-5) indicate that the tKKm algorithm produces inferior partitions. The partitions generated by the aKKm algorithm are more similar to those generated by the kernel k-means even for small sample sizes. The aKKm algorithm achieves similar performance as kernel k-means when m = 500, whereas the tKKm algorithm cannot achieve this until m \u2265 5, 000. As seen in Fig. 4(b), approximately equal amounts of error reduction are achieved by both the kernel k-means and the aKKm algorithm for m \u2265 500. In the NMI plot shown in Fig. 5(b), we first observe that all the kernel-based algorithms, except tKKm and lKKm, perform better than the k-means algorithm. The aKKm algorithm\u2019s performance is better than that of the lKKm and nysSC algorithms, and comparable to that of kernel kmeans, when m \u2265 500."}, {"heading": "6.2 Performance of Approximate kernel k - means using different sampling strategies", "text": "Table 4(a) and Figs. 3(a) and 3(b) compare the diagonal sampling, column norm sampling, and kmeans sampling strategies with the uniform random sampling technique. In Table 4(a), we assume that the n \u00d7 n kernel matrix is pre-computed and only include the time taken for sorting the diagonal entries (for diagonal sampling), or computing the column norms (for column norm sampling), and the time taken for choosing the first m indices, in the sampling time. For the k-means sampling, we show the time taken to execute k-means and find the representative sample. As expected, the\n10\nsampling time for all the non-uniform sampling techniques is greater than the time required for random sampling. Because of the high complexity of column norm sampling and k-means sampling, their sampling time is significantly greater than those of the other two methods. Figs. 3(a) and 3(b) show that the column-norm sampling produces inferior error reduction and NMI results, compared to the random sampling. The diagonal and k-means sampling achieve similar results as random sampling. These results show that uniform random sampling is not only the most efficient way to use the approximate kernel k-means, but it also produces partitions that are as good as or better than the more \u201cintelligent\u201d sampling schemes. In Table 4(b) and Figs. 3(c) and 3(d), we gauge the various non-uniform sampling techniques against the uniform random sampling strategy. As expected, they take much longer than uniform sampling. Similar error reduction and NMI values are achieved by all the sampling strategies when provided with a sufficiently large sample (m \u2265 2, 000). Therefore, the additional time spent for nonuniform sampling does not lead to any significant improvement in the performance."}, {"heading": "6.3 Scalability", "text": "Using the large Forest Cover Type and Network Intrusion data sets, we demonstrate that the proposed algorithm is scalable to large data sets. The aKKm algorithm uses less than 40 GB of memory, thereby dramatically reducing the memory requirements of clustering."}, {"heading": "6.3.1 Datasets", "text": "\u2022 Forest Cover Type: This data set [46] is composed of cartographic variables obtained from the US Geological Survey (USGS) and the US Forest Service (USFS) data. Each of the 581, 012 data points represents the attributes of a 30\u00d730meter cell of the forest floor. There are a total of 12 attributes, including qualitative\n11\nmeasures like soil type and wilderness area, and quantitative measures like slope, elevation, and distance to hydrology. These 12 attributes are represented using 54 features. The data are grouped into 7 classes, each representing a different forest cover type. The true cover type was determined from the USFS Region 2 Resource Information System (RIS) data. \u2022 Network Intrusion: The Network Intrusion data set [47] contains 4,898,431 50-dimensional patterns representing TCP dump data from seven weeks of local-area network traffic. The data are classified into 23 classes, one class representing legitimate traffic and the remaining 22 classes representing different types of illegitimate traffic."}, {"heading": "6.3.2 Experimental setup", "text": "For these data sets, it is currently infeasible to compute and store the full kernel on a single system due to memory and computational time constraints. The aKKm algorithm alleviates this complexity issue. We compare the performance of the aKKm algorithm on these data sets in terms of the running time, error reduction and NMI, with that of the k-means, the tKKm, and the nysSC algorithms. We found that the lKKm algorithm takes longer than 24 hours to find the leaders for these large data sets, clearly demonstrating its non-scalability. Therefore, we eliminated this algorithm from the set of baseline algorithms. We evaluate the efficiency of the aKKm algorithm for different sample sizes ranging from 100 to 5, 000. On the Network Intrusion data set, the value of m is increased only up to 2, 000, as greater values of m require more than 40 GB memory. On the Cover Type data set, we employ the RBF kernel to compute the pairwise similarity, with the parameter \u03c3 set to 0.35. The 3-degree polynomial kernel is employed for the Network Intrusion data set. The kernels and their parameters are tuned to achieve optimal performance. The number of clusters is again set equal to the true number of classes in the data set."}, {"heading": "6.3.3 Experimental results", "text": "Forest Cover Type: We compare the running time of the algorithms in Table 2(c). As in the MNIST data set, kernel calculation time is minimal when compared to clustering time and the aKKm algorithm is less efficient than the tKKm algorithm in terms of the clustering time. When compared to the nysSC algorithm, its running time is higher when the sample size m is small, but as the time taken by the nysSC algorithm increases cubically with m, the aKKm algorithm becomes more efficient as m is increased. It is faster than the k-means algorithm when m < 500. Figs. 4(c) and 5(c) show the effectiveness of our algorithm in terms of error reduction and NMI,\n12\nrespectively. In Fig. 4(c), we observe that a much higher error reduction is achieved by the aKKm algorithm than the tKKm algorithm even when m = 100. Its NMI values are also much higher than those of the baseline algorithms. The nysSC algorithm\u2019s NMI is similar to that of the aKKm algorithm only when m \u2265 5, 000, when the nysSC algorithm is computationally more expensive.\nNetwork Intrusion: Table 2(d) shows the running time of all the algorithms. As observed in the results for the earlier data sets, aKKm takes longer than tKKm but is much faster than the nysSC algorithm. Fig. 4(d) shows that aKKm achieves a better error reduction than tKKm. It also performs better than both the tKKm and nysSC algorithms in terms of NMI, as shown in Fig. 5(d). These results demonstrate that the aKKm algorithm is an efficient technique for clustering large data sets."}, {"heading": "6.4 Ensemble aKKm", "text": "We employ the ensemble aKKm algorithm to combine 10 ensemble partitions. The times taken for combining the partitions (averaged over 10 runs), for each of the data sets, are shown in Table 5. We find that these times are small when compared to the clustering times and hence, do not significantly\nimpact the overall running time. Fig. 6 shows the improvement in NMI achieved through the use of ensembles on the four data sets. A significant improvement is observed, especially when the sample size m is small. For example, in Fig. 6(b), an NMI of around 0.48 is obtained on the MNIST data set, even for a small sample size of m = 100. The NMI increases by about 15% and becomes almost equal to the average NMI obtained for a sample size m = 1, 000. On the Cover Type and Network Intrusion data sets, there are significant improvements in the NMI values. We obtain a good clustering accuracy for small values of m, thereby enhancing the efficiency of the approximate kernel k-means algorithm."}, {"heading": "7 CONCLUSIONS", "text": "We have proposed an efficient approximation for the kernel k-means algorithm, suitable for large data sets. The key idea is to avoid computing the full kernel matrix by restricting the cluster centers to a subspace spanned by a small set of randomly sampled data points. We show theoretically and empirically that the proposed algorithm is efficient in terms of both computational complexity and memory requirement, and is able to yield similar clustering results as the kernel k-means algorithm using the full kernel matrix. In most cases, the performance of our algorithm is better than that of other popular large scale kernel clustering algorithms. By integrating ensemble clustering meth-\n13\nods with the proposed algorithm, its efficiency is further enhanced. In the future, we plan to further enhance the scalability of kernel clustering by devising more efficient kernel approximation techniques. We also plan to extend these ideas to semi-supervised clustering."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This research was supported by the Office of Naval Research (ONR Grant N00014-11-1-0100). Havens is supported by the National Science Foundation under Grant #1019343 to the Computing Research Association for the CI Fellows Project.\nAPPENDIX Proof of Theorem 2 To prove this theorem, we use the following results from [16], [48], and [49]: Lemma 2. (Theorem 1 from [16]) Let A be a n \u00d7 n positive semi-definite matrix and S \u2208 {0, 1}n\u00d7m be a random sampling matrix. Let A be partitioned as\nA = [ Z1 Z2 ] [ \u03a31 0 0 \u03a32 ] [ Z\u22a41 Z\u22a42 ] ,\nwhere Z1 \u2208 \u211cn\u00d7C , Z2 \u2208 \u211cn\u00d7(n\u2212C), \u03a31 \u2208 \u211cC\u00d7C and \u03a32 \u2208 \u211cn\u00d7(n\u2212C). Define \u21261 = Z\u22a41 S and \u21262 = Z\u22a42 S. Assume \u21261 has full row rank. Then the spectral approximation error of the Nystrom extension of A using S as the column sampling matrix satisfies \u2225\u2225A\u2212AS(S\u22a4AS)\u2020S\u22a4A \u2225\u2225 \u2264 \u2016\u03a32\u20162 ( 1 + \u2225\u2225\u2225\u21262\u2126\u20201 \u2225\u2225\u2225 2\n2\n) .\nLemma 3. (Theorem 1.2 from [48]) Let Z \u2208 \u211cn\u00d7n include the eigenvectors of a positive semi-definite A with coherence \u03c4 , where coherence is defined in (16). Let Z1 \u2208 \u211cn\u00d7C represent the first C columns of Z , containing the first C eigenvectors of A, and S \u2208 {0, 1}n\u00d7m represent the first m columns of a random permutation matrix of size n. We have, with probability atleast 1\u2212\u03b4,\u2225\u2225\u2225\u2225 1\nm Z\u22a41 SS \u22a4Z1 \u2212 I \u2225\u2225\u2225\u2225 2 < 1 2 ,\nprovided that m \u2265 C\u03c4 max(C1 ln k, C2 ln(3/\u03b4)), for some fixed positive constants C1 and C2.\nLemma 4. (Lemma 2 from [49]) Let H be a Hilbert space and \u03be be a random variable on (Z, \u03c1) with values in H . Assume \u2016\u03be\u2016 \u2264 M < \u221e almost surely. Denote \u03c32(\u03be) = E(\u2016\u03be\u20162). Let {zi}mi=1 be independent random drawers of \u03c1. For any 0 < \u03b4 < 1, with confidence 1\u2212 \u03b4,\u2225\u2225\u2225\u2225\u2225 1 m m\u2211\ni=1\n(\u03bei \u2212 E[\u03bei]) \u2225\u2225\u2225\u2225\u2225 \u2264 2M ln(2/\u03b4) m + \u221a 2\u03c32(\u03be) ln(2/\u03b4) m .\nProof: Let ai and bi represent the i th rows of Z1 and Z2, respectively. Let \u2206 be the subset of rows of Z1 and Z2 selected by S. Using Lemma 4, we have\n\u2225\u2225Z\u22a42 SS\u22a4Z1 \u2225\u2225 2 = \u2225\u2225\u2225\u2225\u2225 \u2211\nk\u2208\u2206\n( bka \u22a4 k \u2212 E[bka\u22a4k ] ) \u2225\u2225\u2225\u2225\u2225 2 (17)\n\u2264 2M ln(2/\u03b4) + \u221a 2m\u03c32 ln(2/\u03b4),\nwhere M = max j \u2225\u2225bja\u22a4j \u2225\u2225 2 \u2264 max j \u221a |bj |2 |aj |2 \u2264 \u03c4 \u221a C/n and \u03c32 = E [\u2225\u2225bja\u22a4j \u2225\u2225 2 ] \u2264 \u03c4C/n. Substituting \u21262\u2126 \u2020 1 = Z \u22a4 2 SS \u22a4Z1(Z \u22a4 1 SS \u22a4Z1) \u22121 in the result of Lemma 2, we have with probability 1\u2212 2\u03b4,\u2225\u2225A\u2212AS(S\u22a4AS)\u2020S\u22a4A\n\u2225\u2225 (18) \u2264 \u03bbC+1 ( 1 + \u2225\u2225Z\u22a42 SS\u22a4Z1 \u2225\u22252 2 \u2225\u2225(Z\u22a41 SS\u22a4Z1)\u22121 \u2225\u22252 2 ) We obtain the result in the theorem by combining the result of Lemma 3 with equations (17) and (18), and substituting A, AS and S\u22a4AS with K , KB and K\u0302 respectively."}], "references": [{"title": "Approximate kernel k-means: solution to large scale kernel clustering", "author": ["R. Chitta", "R. Jin", "T. Havens", "A. Jain"], "venue": "Proceedings of the SIGKDD conference on Knowledge Discovery and Data mining, 2011, pp. 895\u2013903.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A. Jain"], "venue": "Pattern Recognition Letters, vol. 31, no. 8, pp. 651\u2013666, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation of the performance of clustering algorithms in kernel-induced feature space", "author": ["D. Kim", "K. Lee", "D. Lee", "K. Lee"], "venue": "Pattern Recognition, vol. 38, no. 4, pp. 607\u2013 611, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and Computing, vol. 17, no. 4, pp. 395\u2013416, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "The kernel self-organising map", "author": ["D. MacDonald", "C. Fyfe"], "venue": "Proceedings of the International Conference on Knowledge-Based Intelligent Engineering Systems and Allied Technologies, vol. 1, 2002, pp. 317\u2013320.  14", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Kernel neural gas algorithms with application to cluster analysis", "author": ["A.K. Qinand", "P.N. Suganthan"], "venue": "Pattern Recognition, vol. 4, pp. 617\u2013620, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Mercer kernel-based clustering in feature space", "author": ["M. Girolami"], "venue": "IEEE Transactions on Neural Networks, vol. 13, no. 3, pp. 780\u2013784, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Scholkopf", "A. Smola", "K. Muller"], "venue": "Neural Computation, vol. 10, no. 5, pp. 1299\u20131314, 1996.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "A unified view of kernel k-means, spectral clustering and graph cuts", "author": ["I. Dhillon", "Y. Guan", "B. Kulis"], "venue": "University of Texas at Austin, Tech. Rep., 2004, (Tech. rep. TR-04-25).", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "On the equivalence of nonnegative matrix factorization and spectral clustering", "author": ["C. Ding", "X. He", "H. Simon"], "venue": "Proceedings of the SIAM Data Mining Conference, 2005, pp. 606\u2013610.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Spectral relaxation for k-means clustering", "author": ["H. Zha", "C. Ding", "M. Gu", "X. He", "H. Simon"], "venue": "Advances in Neural Information Processing Systems, vol. 14, pp. 1057\u20131064, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Spectral methods in machine learning and new strategies for very large datasets", "author": ["M. Belabbas", "P. Wolfe"], "venue": "Proceedings of the National Academy of Sciences, vol. 106, no. 2, pp. 369\u2013374, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "On the Nystrom method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M. Mahoney"], "venue": "The Journal of Machine Learning Research, vol. 6, pp. 2153\u20132175, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Spectral grouping using the Nystrom method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 214\u2013225, 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Using the Nystrom method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "Advances in Neural Information Processing Systems, 2001, pp. 682\u2013688.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "The spectral norm error of the naive nystrom extension", "author": ["A. Gittens"], "venue": "Arxiv preprint arXiv:1110.5305, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel spectral clustering in distributed systems", "author": ["W.-Y. Chen", "Y. Song", "H. Bai", "C.-J. Lin", "E. Chang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 3, pp. 568 \u2013586, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Segmentation for SAR Image Based on a New Spectral Clustering Algorithm", "author": ["L. Liu", "X. Wen", "X. Gao"], "venue": "Life System Modeling and Intelligent Computing, pp. 635\u2013643, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast approximate spectral clustering", "author": ["D. Yan", "L. Huang", "M. Jordan"], "venue": "EECS Department, University of California, Berkeley, Tech. Rep. UCB/EECS-2009-45, 2009. [Online]. Available: http://www.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-45.html", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Incremental clustering for dynamic information processing", "author": ["F. Can"], "venue": "ACM Transactions on Information Systems, vol. 11, no. 2, pp. 143\u2013164, 1993.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Incremental clustering for very large document databases: Initial MARIAN experience", "author": ["F. Can", "E. Fox", "C. Snavely", "R. France"], "venue": "Information Sciences, vol. 84, no. 1-2, pp. 101\u2013 114, 1995.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "A framework for clustering evolving data streams", "author": ["C. Aggarwal", "J. Han", "J. Wang", "P. Yu"], "venue": "Proceedings of the International Conference on Very Large Databases, 2003, pp. 81\u201392.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Clustering data streams: Theory and practice", "author": ["S. Guha", "A. Meyerson", "N. Mishra", "R. Motwani", "L. O\u2019Callaghan"], "venue": "IEEE Transactions on Knowledge and Data Engineering, pp. 515\u2013528, 2003.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Finding Groups in Data: An Introduction to Cluster Analysis", "author": ["L. Kaufman", "P. Rousseeuw"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Cure: an efficient clustering algorithm for large databases", "author": ["S. Guha", "R. Rastogi", "K. Shim"], "venue": "Information Systems, vol. 26, no. 1, pp. 35\u201358, 2001.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "Proceedings of the ACM Symposium on Theory of Computing, 2004, pp. 291\u2013300.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "BIRCH: an efficient data clustering method for very large databases", "author": ["T. Zhang", "R. Ramakrishnan", "M. Livny"], "venue": "ACM SIGMOD Record, vol. 25, no. 2, pp. 103\u2013114, 1996.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "CLARANS: A method for clustering objects for spatial data mining", "author": ["R. Ng", "J. Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering, pp. 1003\u20131016, 2002.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Google news personalization: scalable online collaborative filtering", "author": ["A. Das", "M. Datar", "A. Garg", "S. Rajaram"], "venue": "Proceedings of the International Conference on World Wide Web, 2007, pp. 271\u2013280.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast clustering using mapreduce", "author": ["A. Ene", "S. Im", "B. Moseley"], "venue": "Proceedings of the International conference on Knowledge discovery and data mining, 2011, pp. 681\u2013689.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Evaluating mapreduce for multi-core and multiprocessor systems", "author": ["C. Ranger", "R. Raghuraman", "A. Penmetsa", "G. Bradski", "C. Kozyrakis"], "venue": "IEEE Symposium on High Performance Computer Architecture, 2007, pp. 13\u201324.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "A large scale clustering scheme for kernel k-means", "author": ["R. Zhang", "A. Rudnicky"], "venue": "Proceedings of the International Conference on Pattern Recognition, 2002, pp. 289\u2013292.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Speeding-up the kernel k-means clustering method: A prototype based hybrid approach", "author": ["T. Sarma", "P. Viswanath", "B. Reddy"], "venue": "Pattern Recognition Letters, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast spectral clustering with random projection and sampling", "author": ["T. Sakai", "A. Imiya"], "venue": "Machine Learning and Data Mining in Pattern Recognition, pp. 372\u2013384, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "A generalized representer theorem", "author": ["B. Scholkopf", "R. Herbrich", "A. Smola"], "venue": "Proceedings of Computational Learning Theory, 2001, pp. 416\u2013426.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2001}, {"title": "A survey of clustering ensemble algorithms", "author": ["S. Vega-Pons", "J. Ruiz-Schulcloper"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence, vol. 25, no. 3, pp. 337\u2013 372, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Cluster ensembles - a knowledge reuse framework for combining multiple partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 583\u2013617, 2003.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2003}, {"title": "Entropy and correlation: Some comments", "author": ["T. Kvalseth"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, vol. 17, no. 3, pp. 517\u2013519, 1987.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1987}, {"title": "A software package for partitioning unstructured graphs, partitioning meshes, and computing fill-reducing orderings of sparse matrices", "author": ["G. Karypis", "V. Kumar"], "venue": "University of Minnesota (998), 1998.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1998}, {"title": "Parallel spectral clustering in distributed systems", "author": ["W. Chen", "Y. Song", "H. Bai", "C. Lin", "E. Chang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 3, pp. 568\u2013586, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L. Li", "K. Li", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Vlfeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http://www.vlfeat.org, 2008.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1998}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, vol. 2, 2006, pp. 2169\u20132178.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of Classification, vol. 2, no. 1, pp. 193\u2013218, 1985.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1985}, {"title": "Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables", "author": ["J. Blackard", "D. Dean"], "venue": "Computers and Electronics in Agriculture, vol. 24, no. 3, pp. 131\u2013152, 1999.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1999}, {"title": "Cost-based modeling for fraud and intrusion detection:  15 Results from the jam project", "author": ["S. Stolfo", "W. Fan", "W. Lee", "A. Prodromidis", "P. Chan"], "venue": "Proceedings of DARPA Information Survivability Conference and Exposition, vol. 2, 2000, pp. 130\u2013144.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2000}, {"title": "Sparsity and incoherence in compressive sampling", "author": ["E. Candes", "J. Romberg"], "venue": "Inverse Problems, vol. 23, no. 3, pp.  969\u2013985, 2007.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "Geometry on probability spaces", "author": ["S. Smale", "D.-X. Zhou"], "venue": "Constructive Approximation, vol. 30, pp. 311\u2013323, 2009.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "It has found use in a multitude of applications such as web search, social network analysis, image retrieval, medical imaging, gene expression analysis, recommendation systems and market analysis [2].", "startOffset": 196, "endOffset": 199}, {"referenceID": 0, "context": "A previous version of this paper appeared as [1].", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "Kernel clustering algorithms, therefore, have the ability to capture the non-linear structure in real world data sets and, thus, usually perform better than the Euclidean distance based clustering algorithms [3].", "startOffset": 208, "endOffset": 211}, {"referenceID": 3, "context": "A number of kernel-based clustering methods such as spectral clustering [4], kernel Self-", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "Organizing Maps (SOM) [5] and kernel neural gas [6] have been proposed.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "Organizing Maps (SOM) [5] and kernel neural gas [6] have been proposed.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "In this study, we focus on kernel k-means [7], [8] due to its simplicity and efficiency.", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "In this study, we focus on kernel k-means [7], [8] due to its simplicity and efficiency.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "In addition, several studies have established the equivalence of kernel k-means and other kernel-based clustering methods, suggesting that they yield similar results [9]\u2013[11].", "startOffset": 166, "endOffset": 169}, {"referenceID": 10, "context": "In addition, several studies have established the equivalence of kernel k-means and other kernel-based clustering methods, suggesting that they yield similar results [9]\u2013[11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 6, "context": "Kernel k-means [7], a non-linear extension of the classical k-means algorithm, replaces the Euclidean distance function d(xa, xb) = \u2016xa\u2212xb\u2016 employed in the k-means algorithm with a non-linear kernel distance function defined as d\u03ba(xa, xb) = \u03ba(xa, xa) + \u03ba(xb, xb)\u2212 2\u03ba(xa, xb), where xa \u2208 Rd and xb \u2208 Rd are two data points and \u03ba(.", "startOffset": 15, "endOffset": 18}, {"referenceID": 11, "context": "The Nystrom method for kernel approximation has been successfully employed in several learning problems [12]\u2013[15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "The Nystrom method for kernel approximation has been successfully employed in several learning problems [12]\u2013[15].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "The naive Nystrom approximation method [16] randomly samples a small number of points from the data set and computes a low rank approximation of the kernel matrix using the similarity between all the points and the sampled points.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "The spectral clustering algorithm was adapted to use this low rank approximate kernel [14], [17]\u2013[19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "The spectral clustering algorithm was adapted to use this low rank approximate kernel [14], [17]\u2013[19].", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "The spectral clustering algorithm was adapted to use this low rank approximate kernel [14], [17]\u2013[19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "Unlike the spectral clustering algorithm based on the naive Nystrom extension [14], our method uses information from all the eigenvectors of the approximate kernel matrix (without explicitly computing them), thereby yielding more accurate clustering results.", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "Incremental clustering [20], [21] and divide-and-conquer based clustering algorithms [22], [23] were designed to operate in a single pass over the data points, thereby reducing the time required for clustering.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "Incremental clustering [20], [21] and divide-and-conquer based clustering algorithms [22], [23] were designed to operate in a single pass over the data points, thereby reducing the time required for clustering.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Incremental clustering [20], [21] and divide-and-conquer based clustering algorithms [22], [23] were designed to operate in a single pass over the data points, thereby reducing the time required for clustering.", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "Incremental clustering [20], [21] and divide-and-conquer based clustering algorithms [22], [23] were designed to operate in a single pass over the data points, thereby reducing the time required for clustering.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "Sampling based methods, such as CLARA [24] and CURE [25], reduce the computation time by finding the cluster centers based on a small number of randomly selected data points.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "Sampling based methods, such as CLARA [24] and CURE [25], reduce the computation time by finding the cluster centers based on a small number of randomly selected data points.", "startOffset": 52, "endOffset": 56}, {"referenceID": 25, "context": "The coreset algorithms [26] represent the data set using a small set of core data points and find the cluster centers using only these core data points.", "startOffset": 23, "endOffset": 27}, {"referenceID": 26, "context": "Clustering algorithms such as BIRCH [27] and CLARANS [28] improve the clustering efficiency by summarizing the data set into data structures like trees and graphs, thus enabling efficient data access.", "startOffset": 36, "endOffset": 40}, {"referenceID": 27, "context": "Clustering algorithms such as BIRCH [27] and CLARANS [28] improve the clustering efficiency by summarizing the data set into data structures like trees and graphs, thus enabling efficient data access.", "startOffset": 53, "endOffset": 57}, {"referenceID": 28, "context": "With the evolution of cloud computing, parallel processing techniques for clustering are gaining popularity [29], [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 29, "context": "With the evolution of cloud computing, parallel processing techniques for clustering are gaining popularity [29], [30].", "startOffset": 114, "endOffset": 118}, {"referenceID": 29, "context": "For instance, in [30], the MapReduce framework [31] is employed to speedup the k-means and the kmedians clustering algorithms.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "For instance, in [30], the MapReduce framework [31] is employed to speedup the k-means and the kmedians clustering algorithms.", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "Kernel based clustering techniques address this limitation by employing a non-linear kernel distance function to capture the non-linear structure in data [3].", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "Various kernel-based clustering algorithms have been developed, including kernel k-means [7], [8], spectral clustering [4], kernel SOM [5] and kernel neural gas [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "Various kernel-based clustering algorithms have been developed, including kernel k-means [7], [8], spectral clustering [4], kernel SOM [5] and kernel neural gas [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Various kernel-based clustering algorithms have been developed, including kernel k-means [7], [8], spectral clustering [4], kernel SOM [5] and kernel neural gas [6].", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "Various kernel-based clustering algorithms have been developed, including kernel k-means [7], [8], spectral clustering [4], kernel SOM [5] and kernel neural gas [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Various kernel-based clustering algorithms have been developed, including kernel k-means [7], [8], spectral clustering [4], kernel SOM [5] and kernel neural gas [6].", "startOffset": 161, "endOffset": 164}, {"referenceID": 31, "context": "In [32], the memory requirement is reduced by dividing the kernel matrix into blocks and using one block of the kernel matrix at a time.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "The leaders clustering algorithm is integrated with kernel k-means to reduce its computational complexity in [33].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "Sampling methods, such as the Nystrom method [15], have been employed to obtain low rank approximation of the kernel matrix to address this challenge [14], [18].", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "Sampling methods, such as the Nystrom method [15], have been employed to obtain low rank approximation of the kernel matrix to address this challenge [14], [18].", "startOffset": 150, "endOffset": 154}, {"referenceID": 17, "context": "Sampling methods, such as the Nystrom method [15], have been employed to obtain low rank approximation of the kernel matrix to address this challenge [14], [18].", "startOffset": 156, "endOffset": 160}, {"referenceID": 33, "context": "In [34], random projection is combined with sampling to further improve the clustering efficiency.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "Hence, the kernel k-means problem can be cast as the following optimization problem [11]:", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "The problem in (1) can be relaxed to the following optimization problem over U [11]:", "startOffset": 79, "endOffset": 83}, {"referenceID": 34, "context": "We propose a superior approach for reducing the complexity of kernel k-means based on the fact that kernel k-means requires the full n \u00d7 n kernel matrixK only because the the cluster centers {ck(\u00b7), k \u2208 [C]} are represented as linear combinations of all the data points to be clustered (see (3)) [35].", "startOffset": 296, "endOffset": 300}, {"referenceID": 35, "context": "The objective of ensemble clustering [36] is to combine multiple partitions of the given data set.", "startOffset": 37, "endOffset": 41}, {"referenceID": 36, "context": "A popular ensemble clustering algorithm is the MetaClustering algorithm (MCLA) [37], which maximizes the average normalized mutual information.", "startOffset": 79, "endOffset": 83}, {"referenceID": 37, "context": "where NMI(U, U ), the Normalized Mutual Information (NMI) [38] between two partitions a and b, represented by the membership matrices U and U b respectively, is defined by", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "NMI values lie in the range [0, 1].", "startOffset": 28, "endOffset": 34}, {"referenceID": 38, "context": "This meta-graph is partitioned using a graph partitioning algorithm such as METIS [39] to obtain C balanced meta-clusters \u03c01, \u03c02, .", "startOffset": 82, "endOffset": 86}, {"referenceID": 36, "context": "Ensemble aKKm: In the ensemble aKKm algorithm, an additional cost of O(nCr) is incurred for combining the partitions using MCLA [37].", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "denote the coherence of the kernel matrix K (adapted from [16]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "This improves over the approximation error bound of the naive Nystrom method presented in [16].", "startOffset": 90, "endOffset": 94}, {"referenceID": 40, "context": "\u2022 Imagenet: The Imagenet data set [41] consists of over 1.", "startOffset": 34, "endOffset": 38}, {"referenceID": 41, "context": "We extracted keypoints from each image using the VLFeat library [42] and represented each keypoint as a 128 dimensional", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "We used the k-means implementation in the MATLAB Statistics Toolbox and the Nystrom approximation based spectral clustering implementation [40] available at http://alumni.", "startOffset": 139, "endOffset": 143}, {"referenceID": 42, "context": "\u2022 MNIST: The MNIST data set [43] is a subset of the database of handwritten digits available from NIST.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "We also gauge it\u2019s performance against that of (i) the Nystrom spectral clustering algorithm (nysSC) [14], which clusters the top C eigenvectors of a low rank approximate kernel obtained through the Nystrom approximation technique, (ii) the leaders based kernel k-means algorithm (lKKm) [33] which finds a few representative patterns (called leaders) based on a user-defined distance threshold and then runs kernel k-means on the leaders, and (iii) the k-means algorithm to show that it achieves a better clustering accuracy.", "startOffset": 101, "endOffset": 105}, {"referenceID": 32, "context": "We also gauge it\u2019s performance against that of (i) the Nystrom spectral clustering algorithm (nysSC) [14], which clusters the top C eigenvectors of a low rank approximate kernel obtained through the Nystrom approximation technique, (ii) the leaders based kernel k-means algorithm (lKKm) [33] which finds a few representative patterns (called leaders) based on a user-defined distance threshold and then runs kernel k-means on the leaders, and (iii) the k-means algorithm to show that it achieves a better clustering accuracy.", "startOffset": 287, "endOffset": 291}, {"referenceID": 43, "context": "For the Imagenet data set, we employ the spatial pyramid kernel [44] to calculate the pairwise similarity with the number of pyramid levels set to be 4.", "startOffset": 64, "endOffset": 68}, {"referenceID": 31, "context": "11 respectively, as suggested in [32].", "startOffset": 33, "endOffset": 37}, {"referenceID": 44, "context": "To evaluate the difference between the clustering results of the aKKm and tKKm algorithms, and the kernel k-means, we calculate the Adjusted Rand Index (ARI) [45], a measure of similarity between two data partitions.", "startOffset": 158, "endOffset": 162}, {"referenceID": 0, "context": "The adjusted Rand index value lies in [0, 1].", "startOffset": 38, "endOffset": 44}, {"referenceID": 45, "context": "\u2022 Forest Cover Type: This data set [46] is composed of cartographic variables obtained from the US Geological Survey (USGS) and the US Forest Service (USFS) data.", "startOffset": 35, "endOffset": 39}, {"referenceID": 46, "context": "\u2022 Network Intrusion: The Network Intrusion data set [47] contains 4,898,431 50-dimensional patterns representing TCP dump data from seven weeks of local-area network traffic.", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "To prove this theorem, we use the following results from [16], [48], and [49]: Lemma 2.", "startOffset": 57, "endOffset": 61}, {"referenceID": 47, "context": "To prove this theorem, we use the following results from [16], [48], and [49]: Lemma 2.", "startOffset": 63, "endOffset": 67}, {"referenceID": 48, "context": "To prove this theorem, we use the following results from [16], [48], and [49]: Lemma 2.", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "(Theorem 1 from [16]) Let A be a n \u00d7 n positive semi-definite matrix and S \u2208 {0, 1}n\u00d7m be a random sampling matrix.", "startOffset": 16, "endOffset": 20}, {"referenceID": 47, "context": "2 from [48]) Let Z \u2208 Rn\u00d7n include the eigenvectors of a positive semi-definite A", "startOffset": 7, "endOffset": 11}, {"referenceID": 48, "context": "(Lemma 2 from [49]) Let H be a Hilbert space and \u03be be a random variable on (Z, \u03c1) with values in H .", "startOffset": 14, "endOffset": 18}], "year": 2014, "abstractText": "Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data. Among various kernel-based clustering algorithms, kernel k -means has gained popularity due to its simple iterative nature and ease of implementation. However, its run-time complexity and memory footprint increase quadratically in terms of the size of the data set, and hence, large data sets cannot be clustered efficiently. In this paper, we propose an approximation scheme based on randomization, called the Approximate Kernel k-means. We approximate the cluster centers using the kernel similarity between a few sampled points and all the points in the data set. We show that the proposed method achieves better clustering performance than the traditional low rank kernel approximation based clustering schemes. We also demonstrate that it\u2019s running time and memory requirements are significantly lower than those of kernel k -means, with only a small reduction in the clustering quality on several public domain large data sets. We then employ ensemble clustering techniques to further enhance the performance of our algorithm.", "creator": "LaTeX with hyperref package"}}}