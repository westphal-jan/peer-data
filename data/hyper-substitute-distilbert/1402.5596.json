{"id": "1402.5596", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2014", "title": "Exact Post Model Selection Inference for Marginal Screening", "abstract": "we introduced a proposal enabling post model selection analytics, via numerical screening, in behavioral regression. at the core of implicit framework is your result that characterizes the exact error of linear functions of objective estimate $ is $, conditional on the model being selected ( ` ` m \\ x \" clause ). this allows us to specify variable sample intervals and return tests for regression coefficients over account without the selection procedure. in contrast to recent publications in high - index statistics, our results are sparse ( non - asymptotic ), require no criterion - like procedure considering its parameter matrix $ \u00d7 $. furthermore, the computational cost of marginal regression, the confidence about underlying hypothesis reliability equals negligible compared vs the expense under linear regression, fundamentally making our methods as suitable for sequencing large datasets. although we worked on marginal inhibition to induce ~ applicability dependent linear condition on selection variance, this formulation is comparatively more explicitly standardized. we show how users supplement the aforementioned approach to provide other selection procedures including gradient matching methods, non - diagonal prime squares, and marginal screening + utility.", "histories": [["v1", "Sun, 23 Feb 2014 10:30:21 GMT  (99kb,D)", "https://arxiv.org/abs/1402.5596v1", null], ["v2", "Fri, 28 Feb 2014 00:28:21 GMT  (99kb,D)", "http://arxiv.org/abs/1402.5596v2", null]], "reviews": [], "SUBJECTS": "stat.ME cs.LG math.ST stat.ML stat.TH", "authors": ["jason d lee", "jonathan e taylor"], "accepted": true, "id": "1402.5596"}, "pdf": {"name": "1402.5596.pdf", "metadata": {"source": "CRF", "title": "Exact Post Model Selection Inference for Marginal Screening", "authors": ["Jason D. Lee", "Jonathan E. Taylor"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Consider the model\nyi = \u00b5(xi) + i, i \u223c N (0, \u03c32I), (1)\nwhere \u00b5(x) is an arbitrary function, and xi \u2208 Rp. Our goal is to perform inference on (XTX)\u22121XT\u00b5, which is the best linear predictor of \u00b5. In the classical setting of n > p , the least squares estimator\n\u03b2\u0302 = (XTX)\u22121XT y (2)\nar X\niv :1\n40 2.\n55 96\nv2 [\nst at\n.M E\n] 2\n8 Fe\nis a commonly used estimator for (XTX)\u22121XT\u00b5. Under the linear model assumption \u00b5 = X\u03b20, the exact distribution of \u03b2\u0302 is\n\u03b2\u0302 \u223c N (\u03b20, \u03c32(XTX)\u22121). (3)\nUsing the normal distribution, we can test the hypothesis H0 : \u03b2 0 j = 0 and form confidence intervals for \u03b20j using the z-test. However in the high-dimensional p > n setting, the least squares estimator is an underdetermined problem, and the predominant approach is to perform variable selection or model selection [3]. There are many approaches to variable selection including AIC/BIC, greedy algorithms such as forward stepwise regression, orthogonal matching pursuit, and regularization methods such as the Lasso. The focus of this paper will be on the model selection procedure known as marginal screening, which selects the k most correlated features xj with the response y.\nMarginal screening is the simplest and most commonly used of the variable selection procedures [13, 28, 20]. Marginal screening requires only O(np) computation and is several orders of magnitude faster than regularization methods such as the Lasso; it is extremely suitable for extremely large datasets where the Lasso may be computationally intractable to apply. Furthermore, the selection properties are comparable to the Lasso [12]. In the ultrahigh dimensional setting p = O(en k ), marginal screening is shown to have the SURE screening property, P (S \u2282 S\u0302), that is marginal screening selects a superset of the truly relevant variables [9, 11, 10]. Marginal screening can also be combined with a second variable selection procedure such as the Lasso to further reduce the dimensionality; our statistical inference methods extend to the Marginal Screening+Lasso method.\nSince marginal screening utilizes the response variable y, the confidence intervals and statistical tests based on the distribution in (3) are not valid; confidence intervals with nominal 1\u2212\u03b1 coverage may no longer cover at the advertised level:\nPr ( \u03b20j \u2208 C1\u2212\u03b1(x) ) < 1\u2212 \u03b1.\nSeveral authors have previously noted this problem including recent work in [17, 18, 19, 2]. A major line of work [17, 18, 19] has described the difficulty of inference post model selection: the distribution of post model selection estimates is complicated and cannot be approximated in a uniform sense by their asymptotic counterparts.\nIn this paper, we describe how to form exact confidence intervals for linear regression coefficients post model selection. We assume the model\n(1), and operate under the fixed design matrix X setting. The linear regression coefficients constrained to a subset of variables S is linear in \u00b5, eTj (X T SXS) \u22121XTS \u00b5 = \u03b7 T\u00b5 for some \u03b7. We derive the conditional distribution of \u03b7T y for any vector \u03b7, so we are able to form confidence intervals and test regression coefficients.\nIn Section 2 we discuss related work on high-dimensional statistical inference, and Section 3 introduces the marginal screening algorithm and shows how z intervals may fail to have the correct coverage properties. Section 4 and 5 show how to represent the marginal screening selection event as constraints on y, and construct pivotal quantities for the truncated Gaussian. Section 6 uses these tools to develop valid hypothesis tests and confidence intervals.\nAlthough the focus of this paper is on marginal screening, the \u201ccondition on selection\u201d framework, first proposed for the Lasso in [16], is much more general; we use marginal screening as a simple and clean illustration of the applicability of this framework. In Section 7, we discuss several extensions including how to apply the framework to other variable/model selection procedures and to nonlinear regression problems. Section 7 covers\n1. marginal screening+Lasso, a screen and clean procedure that first uses marginal screening and cleans with the Lasso,\n2. orthogonal matching pursuit (OMP)\n3. non-negative least squares (NNLS)."}, {"heading": "2 Related Work", "text": "Most of the theoretical work on high-dimensional linear models focuses on consistency. Such results establish, under restrictive assumptions on X, the Lasso \u03b2\u0302 is close to the unknown \u03b20 [24] and selects the correct model [33, 30, 15]. We refer to the reader to [3] for a comprehensive discussion about the theoretical properties of the Lasso.\nThere is also recent work on obtaining confidence intervals and significance testing for penalized M-estimators such as the Lasso. One class of methods uses sample splitting or subsampling to obtain confidence intervals and p-values [31, 23]. In the post model selection literature, the recent work of [2] proposed the POSI approach, a correction to the usual t-test confidence intervals by controlling the familywise error rate for all parameters in any possible submodel. The POSI approach will produce valid confidence intervals for any possible model selection procedure; however for a given model\nselection procedure such as marginal regression, it will be conservative. In addition, the POSI methodology is extremely computationally intensive and currently only applicable for p \u2264 30.\nA separate line of work establishes the asymptotic normality of a corrected estimator obtained by \u201cinverting\u201d the KKT conditions [29, 32, 14]. The corrected estimator b\u0302 has the form b\u0302 = \u03b2\u0302 + \u03bb\u0398\u0302z\u0302, where z\u0302 is a subgradient of the penalty at \u03b2\u0302 and \u0398\u0302 is an approximate inverse to the Gram matrix XTX. The two main drawbacks to this approach are 1) the confidence intervals are valid only when the M-estimator is consistent, and thus require restricted eigenvalue conditions on X, 2) obtaining \u0398\u0302 is usually much more expensive than obtaining \u03b2\u0302, and 3) the method is specific to regularized estimators, and does not extend to marginal screening, forward stepwise, and other variable selection methods.\nMost closely related to our work is the \u201ccondition on selection\u201d framework laid out in [16] for the Lasso. Our work extends this methodology to other variable selection methods such as marginal screening, marginal screening followed by the Lasso (marginal screening+Lasso), orthogonal matching pursuit, and non-negative least squares. The primary contribution of this work is the observation that many model selection methods, including marginal screening and Lasso, lead to \u201cselection events\u201d that can be represented as a set of constraints on the response variable y. By conditioning on the selection event, we can characterize the exact distribution of \u03b7T y. This paper focuses on marginal screening, since it is the simplest of variable selection methods, and thus the applicability of the \u201cconditioning on selection event\u201d framework is most transparent. However, this framework is not limited to marginal screening and can be applied to a wide a class of model selection procedures including greedy algorithms such as matching pursuit and orthogonal matching pursuit. We discuss some of these possible extensions in Section 7, but leave a thorough investigation to future work.\nA remarkable aspect of our work is that we only assume X is in general position, and the test is exact, meaning the distributional results are true even under finite samples. By extension, we do not make any assumptions on n and p, which is unusual in high-dimensional statistics [3]. Furthermore, the computational requirements of our test are negligible compared to computing the linear regression coefficients.\nOur test assumes that the noise variance \u03c32 is known. However, there are many methods for estimating \u03c32 in high dimensions. A data splitting technique is used in [8], while [27] proposes a method that computes the regression estimate and an estimate of the variance simultaneously. We refer the reader to [25] for a survey and comparison of the various methods,\nand assume \u03c32 is known for the remainder of the paper."}, {"heading": "3 Marginal Screening", "text": "Let X \u2208 Rn\u00d7p be the design matrix, y \u2208 Rn the response variable, and assume the model\nyi = \u00b5(xi) + i, i \u223c N (0, \u03c32I).\nWe will assume that X is in general position and has unit norm columns. The algorithm estimates \u03b2\u0302 via Algorithm 1. The marginal screening algo-\nAlgorithm 1 Marginal screening algorithm\n1: Input: Design matrix X, response y, and model size k. 2: Compute |XT y|. 3: Let S\u0302 be the index of the k largest entries of |XT y|. 4: Compute \u03b2\u0302S\u0302 = (X T S\u0302 XS\u0302) \u22121XT S\u0302 y\nrithm chooses the k variables with highest absolute dot product with y, and then fits a linear model over those k variables. We will assume k \u2264 min(n, p). For any fixed subset of variables S, the distribution of \u03b2\u0302S = (X T SXS)\n\u22121XTS y is\n\u03b2\u0302S \u223c N (\u03b2?S , \u03c32(XTSXS)\u22121) (4) \u03b2?S := (X T SXS) \u22121XTS \u00b5. (5)\nWe will use the notation \u03b2?j\u2208S := (\u03b2 ? S)j , where j is indexing a variable in the set S. The z-test intervals for a regression coefficient are\nC(\u03b1, j, S) :=( \u03b2\u0302j\u2208S \u2212 \u03c3z1\u2212\u03b1/2(XTSXS)jj , \u03b2\u0302j\u2208S + \u03c3z1\u2212\u03b1/2(XTSXS)jj ) (6)\nand each interval has 1\u2212\u03b1 coverage, meaning Pr ( \u03b2?j\u2208S \u2208 C(\u03b1, j, S) ) = 1\u2212\u03b1.\nHowever if S\u0302 is chosen using a model selection procedure that depends on y, the distributional result (5) no longer holds and the z-test intervals will not cover at the 1\u2212 \u03b1 level. It is possible that\nPr ( \u03b2? j\u2208S\u0302 \u2208 C(\u03b1, j, S\u0302) ) < 1\u2212 \u03b1.\nSimilarly, the test of the hypothesis H0 : \u03b2 ? j\u2208S\u0302 = 0 will not control type I error at level \u03b1, meaning Pr (reject H0|H0) > \u03b1."}, {"heading": "3.1 Failure of z-test confidence intervals", "text": "We will illustrate empirically that the z-test intervals do not cover at 1\u2212 \u03b1 when S\u0302 is chosen by marginal screening in Algorithm 1. For this experiment\nwe generated X from a standard normal with n = 20 and p = 200. The signal vector is 2 sparse with \u03b201 , \u03b2 0 2 = SNR, y = X\u03b2\n0 + , and \u223c N(0, 1). The confidence intervals were constructed for the k = 2 variables selected by the marginal screening algorithm. The z-test intervals were constructed via (6) with \u03b1 = .1, and the adjusted intervals were constructed using Algorithm 3. The results are described in Figure 1. The y-axis plots the coverage proportion or the fraction of times the true parameter value fell in the confidence interval. Each point represents 500 independent trials. The x-axis varies the SNR parameter over the values 0.1, .2, .5, 1, 2, 5, 10. From the figure, we see that the z intervals can have coverage proportion drastically less than the nominal level of 1 \u2212 \u03b1 = .9, and only for SNR=10 does the coverage tend to .9. This motivates the need for intervals that have the correct coverage proportion after model selection."}, {"heading": "4 Representing the selection event", "text": "Since Equation (5) does not hold for a selected S\u0302 when the selection procedure depends on y, the z-test intervals are not valid. Our strategy will be to understand the conditional distribution of y and contrasts (linear functions of y) \u03b7T y, then construct inference conditional on the selection event E\u0302. We will use E\u0302(y) to represent a random variable, and E to represent an element of the range of E\u0302(y). In the case of marginal screening, the selection event E\u0302(y) corresponds to the set of selected variables S\u0302 and signs s:\nE\u0302(y) = { y : sign(xTi y)x T i y > \u00b1xTj y for all i \u2208 S\u0302 and j \u2208 S\u0302c } = { y : s\u0302ix T i y > \u00b1xTj y and s\u0302ixTi y \u2265 0 for all i \u2208 S\u0302 and j \u2208 S\u0302c\n} = { y : A(S\u0302, s\u0302)y \u2264 b(S\u0302, s\u0302) } (7)\nfor some matrix A(S\u0302, s\u0302) and vector b(S\u0302, s\u0302)1. We will use the selection event E\u0302 and the selected variables/signs pair (S\u0302, s\u0302) interchangeably since they are in bijection.\nThe space Rn is partitioned by the selection events, Rn = \u2294\n(S,s)\n{y : A(S, s)y \u2264 b(S, s)}.\nThe vector y can be decomposed with respect to the partition as follows y = \u2211 S,s y 1 (A(S, s)y \u2264 b(S, s)) (8)\nThe previous equation establishes that y is a different constrained Gaussian for each element of the partition, where the partition is specified by a possible subset of variables and signs (S, s). The above discussion can be summarized in the following theorem.\nTheorem 4.1. The distribution of y conditional on the selection event is a constrained Gaussian,\ny|{E\u0302(y) = E} d= z \u2223\u2223{A(S, s)z \u2264 b}, z \u223c N (\u00b5, \u03c32I).\nProof. The event E is in bijection with a pair (S, s), and y is unconditionally Gaussian. Thus the conditional y \u2223\u2223{A(S, s)y \u2264 b(S, s)} is a Gaussian constrained to the set {A(S, s)y \u2264 b(S, s)}.\n1b can be taken to be 0 for marginal screening, but this extra generality is needed for other model selection methods"}, {"heading": "5 Truncated Gaussian test", "text": "This section summarizes the recent tools developed in [16] for testing contrasts2 \u03b7T y of a constrained Gaussian y. The results are stated without proof and the proofs can be found in [16].\nThe distribution of a constrained Gaussian y \u223c N(\u00b5,\u03a3) conditional on affine constraints {Ay \u2264 b} has density 1Pr(Ay\u2264b)f(y;\u00b5,\u03a3)1 (Ay \u2264 b), involves the intractable normalizing constant Pr(Ay \u2264 b). In this section, we derive a one-dimensional pivotal quantity for \u03b7T\u00b5. This pivot relies on characterizing the distribution of \u03b7T y as a truncated normal. The key step to deriving this pivot is the following lemma:\nLemma 5.1. The conditioning set can be rewritten in terms of \u03b7T y as follows: {Ay \u2264 b} = {V\u2212(y) \u2264 \u03b7T y \u2264 V+(y),V0(y) \u2265 0} where\n\u03b1 = A\u03a3\u03b7\n\u03b7T\u03a3\u03b7 (9)\nV\u2212 = V\u2212(y) = max j: \u03b1j<0 bj \u2212 (Ay)j + \u03b1j\u03b7T y \u03b1j\n(10)\nV+ = V+(y) = min j: \u03b1j>0 bj \u2212 (Ay)j + \u03b1j\u03b7T y \u03b1j . (11)\nV0 = V0(y) = min j: \u03b1j=0 bj \u2212 (Ay)j (12)\nMoreover, (V+,V\u2212,V0) are independent of \u03b7T y.\nThe geometric picture gives more intuition as to why V+ and V\u2212 are independent of \u03b7T y. Without loss of generality, we assume ||\u03b7||2 = 1 and y \u223c N(\u00b5, I) (otherwise we could replace y by \u03a3\u2212 1 2 y). Now we can decompose y into two independent components, a 1-dimensional component \u03b7T y and an (n\u2212 1)-dimensional component orthogonal to \u03b7:\ny = \u03b7T y + P\u03b7\u22a5y.\nThe case of n = 2 is illustrated in Figure 2. Since the two components are independent, the distribution of \u03b7T y is the same as \u03b7T y|{P\u03b7\u22a5y}. If we condition on P\u03b7\u22a5y, it is clear from Figure 2 that in order for y to lie in the set, it is necessary for V\u2212 \u2264 \u03b7T y \u2264 V+, where V\u2212 and V+ are functions of P\u03b7\u22a5y.\n2A contrast of y is a linear function of the form \u03b7T y.\nCorollary 5.2. The distribution of \u03b7T y conditioned on {Ay \u2264 b,V+(y) = v+,V\u2212(y) = v\u2212} is a (univariate) Gaussian truncated to fall between V\u2212 and V+, i.e.\n\u03b7T y | {Ay \u2264 b,V+(y) = v+,V\u2212(y) = v\u2212} d= W\nwhere W \u223c TN(\u03b7T\u00b5, \u03b7T\u03a3\u03b7, v\u2212, v+). TN(\u00b5, \u03c3, a, b) is the normal distribution truncated to lie between a and b.\nIn Figure 3, we plot the density of the truncated Gaussian, noting that its shape depends on the location of \u00b5 relative to [a, b] as well as the width relative to \u03c3.\nThe following pivotal quantity3 follows from Corollary 5.2 via the probability integral transform.\nTheorem 5.3. Let \u03a6(x) denote the CDF of a N(0, 1) random variable, and let F [a,b] \u00b5,\u03c32 denote the CDF of TN(\u00b5, \u03c3, a, b), i.e.:\nF [a,b] \u00b5,\u03c32 (x) = \u03a6((x\u2212 \u00b5)/\u03c3)\u2212 \u03a6((a\u2212 \u00b5)/\u03c3) \u03a6((b\u2212 \u00b5)/\u03c3)\u2212 \u03a6((a\u2212 \u00b5)/\u03c3) . (13)\n3The distribution of a pivotal quantity does not depend on unobserved parameters.\nThen F [V\u2212,V+] \u03b7T\u00b5, \u03b7T\u03a3\u03b7 (\u03b7T y) is a pivotal quantity, conditional on {Ay \u2264 b}:\nF [V\u2212,V+] \u03b7T\u00b5, \u03b7T\u03a3\u03b7\n(\u03b7T y) \u2223\u2223 {Ay \u2264 b} \u223c Unif(0, 1) (14)\nwhere V\u2212 and V+ are defined in (10) and (11)."}, {"heading": "6 Inference for marginal screening", "text": "In this section, we apply the theory summarized in Sections 4 and 5 to marginal screening. In particular, we will construct confidence intervals for the selected variables.\nTo summarize the developments so far, recall that our model (1) says that y \u223c N(\u00b5, \u03c32I). The distribution of interest is y|{E\u0302(y) = E}, and by Theorem 4.1, this is equivalent to y|{A(S, s)z \u2264 b(S, s)}, where y \u223c N(\u00b5, \u03c32I). By applying Theorem 5.3, we obtain the pivotal quantity\nF [V\u2212,V+] \u03b7T\u00b5, \u03c32||\u03b7||22\n(\u03b7T y) \u2223\u2223 {E\u0302(y) = E} \u223c Unif(0, 1) (15)\nfor any \u03b7, where V\u2212 and V+ are defined in (10) and (11)."}, {"heading": "6.1 Hypothesis tests for selected variables", "text": "In this section, we describe how to form confidence intervals for the components of \u03b2?\nS\u0302 = (XT S\u0302 XS\u0302) \u22121XT S\u0302 \u00b5. The best linear predictor of \u00b5 that uses\nonly the selected variables is \u03b2? S\u0302 , and \u03b2\u0302S\u0302 = (X T S\u0302 XS\u0302) \u22121XT S\u0302 y is an unbiased estimate of \u03b2? S\u0302\n. In this section, we propose hypothesis tests and confidence intervals for \u03b2?\nS\u0302 . If we choose\n\u03b7j = ((X T S\u0302 XS\u0302) \u22121XT S\u0302 ej) T , (16)\nthen \u03b7Tj \u00b5 = \u03b2 ? j\u2208S\u0302 , so the above framework provides a method for inference about the jth variable in the model S\u0302. This choice of \u03b7 is not fixed before marginal screening selects S\u0302, but it is measurable with respect to the \u03c3algebra generated by the partition. Since it is measurable, \u03b7 is constant on each partition, so the pivot is uniformly distributed on each element of the partition, and thus uniformly distributed for all y.\nIf we assume the linear model \u00b5 = X\u03b20 for some \u03b20 \u2208 Rp, S0 := support(\u03b20) \u2282 S\u0302, and XS\u0302 is full rank, then by the following computation \u03b2? S\u0302 = \u03b20 S\u0302 :\n\u03b2? S\u0302 = (XT S\u0302 XS\u0302) \u22121XT S\u0302 XS\u03b2 0 S\n= (XT S\u0302 XS\u0302) \u22121XT S\u0302 XS\u0302\u03b2 0 S\u0302 = \u03b20 S\u0302\nIn [9], the screening property S0 \u2282 S\u0302 for the marginal screening algorithm is\nestablished under mild conditions. Thus under the screening property, our method provides hypothesis tests and confidence intervals for \u03b20\nS\u0302 .\nBy applying Theorem 5.3, we obtain the following (conditional) pivot for \u03b2?\nj\u2208S\u0302 :\nF [V\u2212,V+] \u03b2? j\u2208S\u0302 , \u03c32||\u03b7j ||2(\u03b7 T j y) \u2223\u2223\u2223{E\u0302(y) = E} \u223c Unif(0, 1). (17) The quantities j and \u03b7j are both random through E\u0302, a quantity which is fixed after conditioning, therefore Theorem 5.3 holds even for this choice of \u03b7.\nConsider testing the hypothesis H0 : \u03b2 ? j\u2208S\u0302 = \u03b2j . A valid test statis-\ntic is given by F [V\u2212,V+] \u03b2j , \u03c32||\u03b7j ||2(\u03b7 T j y), which is uniformly distributed under the null hypothesis and y|{E\u0302(y) = E}. Thus, this test would reject when F\n[V\u2212,V+] \u03b2j , \u03c32||\u03b7j ||2(\u03b7 T j y) > 1\u2212 \u03b12 or F [V\u2212,V+] \u03b2j , \u03c32||\u03b7j ||2(\u03b7 T j y) < \u03b1 2 .\nTheorem 6.1. The test of H0 : \u03b2 ? j\u2208S\u0302 = \u03b2j that accepts when\n\u03b1 2 < F [V\u2212,V+] \u03b2j , \u03c32||\u03b7j ||2(\u03b7 T j y) < 1\u2212 \u03b1 2\nis an \u03b1 level test of H0.\nProof. Under H0, we have \u03b2 ? j\u2208S\u0302 = \u03b2j , so by (17) F [V\u2212,V+] \u03b2j , \u03c32||\u03b7j ||2(\u03b7 T j y) \u2223\u2223{E\u0302(y) = E} is uniformly distributed. Thus\nPr( \u03b1\n2 < F\n[V\u2212,V+] \u03b2j , \u03c32||\u03b7j ||2(\u03b7 T j y) \u2264 1\u2212 \u03b1\n2 \u2223\u2223{E\u0302(y) = E,H0)} = 1\u2212 \u03b1, and the type 1 error is exactly \u03b1. Under H0, but not conditional on selection event E\u0302, we have\nPr( \u03b1\n2 < F\n[V\u2212,V+] \u03b2j , \u03c32||\u03b7j ||2(\u03b7 T j y) \u2264 1\u2212 \u03b1\n2 \u2223\u2223H0)} = \u2211 E Pr( \u03b1 2 < F [V\u2212,V+] \u03b2j , \u03c32||\u03b7j ||2(\u03b7 T j y) \u2264 1\u2212 \u03b1 2\n\u2223\u2223{E\u0302(y) = E,H0)}Pr(E\u0302(y) = E|H0) = \u2211 E (1\u2212 \u03b1)Pr(E\u0302(y) = E|H0)\n= (1\u2212 \u03b1) \u2211 E Pr(E\u0302(y) = E|H0) = 1\u2212 \u03b1.\nFor each element of the partition E, the conditional (on selection) hypothesis test is level 1 \u2212 \u03b1, so by summing over the partition the unconditional test is level 1\u2212 \u03b1.\nOur hypothesis test is not conservative, in the sense that the type 1 error is exactly \u03b1; also, it is non-asymptotic, since the statement holds for fixed n and p. We summarize the hypothesis test in this section in the following algorithm.\nAlgorithm 2 Hypothesis test for selected variables\n1: Input: Design matrix X, response y, model size k. 2: Use Algorithm 1 to select a subset of variables S\u0302 and signs s\u0302 =\nsign(XT S\u0302 y).\n3: Specify the null hypothesis H0 : \u03b2 ? j\u2208S\u0302 = \u03b2j . 4: Let A = A(S\u0302, s\u0302) and b = b(S\u0302, s\u0302) using (7). Let \u03b7j = (X T S\u0302 )\u2020ej . 5: Compute F [V\u2212,V+] \u03b2j , \u03c32||\u03b7j ||2(\u03b7 T j y), where V\u2212 and V+ are computed via (10)\nand (11) using the A, b, and \u03b7 previously defined.\n6: Output: Reject if F [V\u2212,V+] \u03b2j , \u03c32||\u03b7j ||2(\u03b7 T j y) > \u03b1 2 or F [V\u2212,V+] \u03b2j , \u03c32||\u03b7j ||2(\u03b7 T j y) < 1\u2212 \u03b12 ."}, {"heading": "6.2 Confidence intervals for selected variables", "text": "Next, we discuss how to obtain confidence intervals for \u03b2? j\u2208S\u0302 . The standard way to obtain an interval is to invert a pivotal quantity [4]. In other words, since\nPr\n( \u03b1\n2 \u2264 F [V \u2212,V+] \u03b2? j\u2208S\u0302 , \u03c32||\u03b7j ||2(\u03b7 T j y) \u2264 1\u2212 \u03b1 2 \u2223\u2223 {E\u0302 = E}) = \u03b1, one can define a (1\u2212 \u03b1) (conditional) confidence interval for \u03b2?\nj,E\u0302 as{\nx : \u03b1 2 \u2264 F [V \u2212,V+] x, \u03c32||\u03b7j ||2(\u03b7 T j y) \u2264 1\u2212 \u03b1 2\n} . (18)\nIn fact, F is monotone decreasing in x, so to find its endpoints, one need only solve for the root of a smooth one-dimensional function. The monotonicity is a consequence of the fact that the truncated Gaussian distribution is a natural exponential family and hence has monotone likelihood ratio in \u00b5 [21].\nWe now formalize the above observations in the following result, an immediate consequence of Theorem 5.3.\nCorollary 6.2. Let \u03b7j be defined as in (16), and let L\u03b1 = L\u03b1(\u03b7j , (S\u0302, s\u0302)) and U\u03b1 = U\u03b1(\u03b7j , (S\u0302, s\u0302)) be the (unique) values satisfying\nF [V\u2212,V+] L\u03b1, \u03c32||\u03b7j ||2(\u03b7 T j y) = 1\u2212 \u03b1 2 F [V\u2212,V+] U\u03b1, \u03c32||\u03b7j ||2(\u03b7 T j y) = \u03b1 2 (19)\nThen [L\u03b1, U\u03b1] is a (1\u2212 \u03b1) confidence interval for \u03b2?j\u2208S\u0302, conditional on E\u0302:\nP ( \u03b2? j\u2208S\u0302 \u2208 [L\u03b1, U\u03b1] \u2223\u2223 {E\u0302 = E}) = 1\u2212 \u03b1. (20) Proof. The confidence region of \u03b2?\nj\u2208S\u0302 is the set of \u03b2j such that the test\nof H0 : \u03b2 ? j\u2208S\u0302 accepts at the 1 \u2212 \u03b1 level. The function F [V\u2212,V+] x, \u03c32||\u03b7j ||2(\u03b7 T j y) is monotone in x, so solving for L\u03b1 and U\u03b1 identify the most extreme values where H0 is still accepted. This gives a 1\u2212 \u03b1 confidence interval.\nIn relation to the literature on False Coverage Rate (FCR) [1], our procedure also controls the FCR.\nLemma 6.3. For each j \u2208 S\u0302,\nPr ( \u03b2? j\u2208S\u0302 \u2208 [L j \u03b1, U j \u03b1] ) = 1\u2212 \u03b1. (21)\nFurthermore, the FCR of the intervals { [Lj\u03b1, U j \u03b1] } j\u2208E\u0302 is \u03b1.\nProof. By (20), the conditional coverage of the confidence intervals are 1\u2212\u03b1. The coverage holds for every element of the partition {E\u0302(y) = E}, so\nPr ( \u03b2? j\u2208S\u0302 \u2208 [L j \u03b1, U j \u03b1] )\n= \u2211 E Pr ( \u03b2? j\u2208S\u0302 \u2208 [L\u03b1, U\u03b1] \u2223\u2223 {E\u0302 = E})Pr(E\u0302 = E) = \u2211 E (1\u2212 \u03b1)Pr(E\u0302 = E)\n= (1\u2212 \u03b1) \u2211 E Pr(E\u0302 = E) = 1\u2212 \u03b1.\nWe summarize the algorithm for selecting and constructing confidence intervals below."}, {"heading": "6.3 Experiments on Diabetes dataset", "text": "In Figure 1, we have already seen that the confidence intervals constructed using Algorithm 3 have exactly 1\u2212\u03b1 coverage proportion. In this section, we\nperform an experiment on real data where the linear model does not hold, the noise is not Gaussian, and the noise variance is unknown. The diabetes dataset contains n = 442 diabetes patients measured on p = 10 baseline variables [6]. The baseline variables are age, sex, body mass index, average blood pressure, and six blood serum measurements, and the response y is a quantitative measure of disease progression measured one year after the baseline. The goal is to use the baseline variables to predict y, the measure of disease progression after one year, and determine which baseline variables are statistically significant for predicting y.\nSince the noise variance \u03c32 is unknown, we estimate it by \u03c32 = \u2016y\u2212y\u0302\u2016n\u2212p , where y\u0302 = X\u03b2\u0302 and \u03b2\u0302 = (XTX)\u22121XT y. For each trial we generated new responses y\u0303i = X\u03b2\u0302 + \u0303, and \u0303 is bootstrapped from the residuals ri = yi \u2212 y\u0302i. This is known as the residual bootstrap, and is a standard method for assessing statistical procedures when the underlying model is unknown [7]. We used marginal screening to select k = 2 variables, and then fit linear regression on the selected variables. The adjusted confidence intervals were\nAlgorithm 3 Confidence intervals for selected variables\n1: Input: Design matrix X, response y, model size k. 2: Use Algorithm 1 to select a subset of variables S\u0302 and signs s\u0302 =\nsign(XT S\u0302 y).\n3: Let A = A(S\u0302, s\u0302) and b = b(S\u0302, s\u0302) using (7). Let \u03b7j = (X T S\u0302 )\u2020ej . 4: Solve for Lj\u03b1 and U j \u03b1 using Equation (19) where V\u2212 and V+ are computed\nvia (10) and (11) using the A, b, and \u03b7j previously defined.\n5: Output: Return the intervals [Lj\u03b1, U j \u03b1] for j \u2208 S\u0302.\nconstructed using Algorithm 3 with the estimated \u03c32. The nominal coverage level is varied across 1 \u2212 \u03b1 \u2208 {.5, .6, .7, .8, .9, .95, .99}. From Figure 6, we observe that the adjusted intervals always cover at the nominal level, whereas the z-test is always below. The experiment was repeated 2000 times."}, {"heading": "7 Extensions", "text": "The purpose of this section is to illustrate the broad applicability of the condition on selection framework. This framework was first proposed in [16]\nto form valid hypothesis tests and confidence intervals after model selection via the Lasso. However, the framework is not restricted to the Lasso, and we have shown how to apply it to marginal screening. For expository purposes, we focused the paper on marginal screening where the framework is particularly easy to understand. In the rest of this section, we show how to apply the framework to marginal screening+Lasso, orthogonal matching pursuit, and non-negative least squares. This is a non-exhaustive list of selection procedures where the condition on selection framework is applicable, but we hope this incomplete list emphasizes the ease of constructing tests and confidence intervals post-model selection via conditioning."}, {"heading": "7.1 Marginal screening + Lasso", "text": "The marginal screening+Lasso procedure was introduced in [9] as a variable selection method for the ultra-high dimensional setting of p = O(en k ). Fan et al. [9] recommend applying the marginal screening algorithm with k = n \u2212 1, followed by the Lasso on the selected variables. This is a two-stage procedure, so to properly account for the selection we must encode the selection event of marginal screening followed by Lasso. This can be done by representing the two stage selection as a single event. Let (S\u0302m, s\u0302m) be the variables and signs selected by marginal screening, and the (S\u0302L, z\u0302L) be the variables and signs selected by Lasso [16]. In Proposition 2.2 of [16], it is shown how to encode the Lasso selection event (S\u0302L, z\u0302L) as a set of constraints {ALy \u2264 bL} 4, and in Section 4 we showed how to encode the marginal screening selection event (S\u0302m, s\u0302m) as a set of constraints {Amy \u2264 bm}. Thus the selection event of marginal screening+Lasso can be encoded as {ALy \u2264 bL, Amy \u2264 bm}. Using these constraints, the hypothesis test and confidence intervals described in Algorithms 2 and 3 are valid for marginal screening+Lasso."}, {"heading": "7.2 Orthogonal Matching Pursuit", "text": "Orthogonal matching pursuit (OMP) is a commonly used variable selection method. At each iteration, OMP selects the variable most correlated with the residual r, and then recomputes the residual using the residual of least squares using the selected variables. The description of the OMP algorithm is given in Algorithm 4.\n4The Lasso selection event is with respect to the Lasso optimization problem after marginal screening.\nAlgorithm 4 Orthogonal matching pursuit (OMP)\n1: Input: Design matrix X, response y, and model size k. 2: for: i = 1 to k 3: pi = arg maxj=1,...,p |rTi xj |. 4: S\u0302i = \u222aij=1 {pi}. 5: ri+1 = (I \u2212XS\u0302iX \u2020 S\u0302i )y. 6: end for 7: Output: S\u0302 := {p1, . . . , pk}, and \u03b2\u0302S\u0302 = (X T S\u0302 XS\u0302) \u22121XT S\u0302 y\nSimilar to Section 4, we can represent the OMP selection event as a set of linear constraints on y.\nE\u0302(y) = { y : sign(xTpiri)x T piri > \u00b1x T j ri, for all j 6= pi and all i \u2208 [k] } = {y : s\u0302ixTpi(I \u2212XS\u0302i\u22121X \u2020 S\u0302i\u22121 )y > \u00b1xTj (I \u2212XS\u0302i\u22121X \u2020 S\u0302i\u22121 )y and\ns\u0302ix T pi(I \u2212XS\u0302i\u22121X \u2020 S\u0302i\u22121 )y > 0, for all j 6= pi, and all i \u2208 [k] } = { y : A(S\u03021, . . . , S\u0302k, s\u03021, . . . , s\u0302k) \u2264 b(S\u03021, . . . , S\u0302k, s\u03021, . . . , s\u0302k) } .\nThe selection event encodes that OMP selected a certain variable and the sign of the correlation of that variable with the residual, at steps 1 to k. The primary difference between the OMP selection event and the marginal screening selection event is that the OMP event also describes the order at which the variables were chosen. The marginal screening event only describes that the variable was among the top k most correlated, and not whether a variable was the most correlated or kth most correlated.\nSince the selection event can be represented as constraints on y, the hypothesis test and confidence intervals described in Algorithms 2 and 3 are valid for OMP selected \u03b2\u0302S\u0302 ."}, {"heading": "7.3 Nonnegative Least Squares", "text": "Non-negative least squares (NNLS) is a simple modification of the linear regression estimator with non-negative constraints on \u03b2:\narg min \u03b2:\u03b2\u22650\n1 2 \u2016y \u2212X\u03b2\u20162 . (22)\nUnder a positive eigenvalue conditions on X, several authors [26, 22] have shown that NNLS is comprable to the Lasso in terms of prediction and\nestimation errors. The NNLS estimator also does not have any tuning parameters, since the sign constraint provides a natural form of regularization. NNLS has found applications when modeling non-negative data such as prices, incomes, count data. Non-negativity constraints arise naturally in non-negative matrix factorization, signal deconvolution, spectral analysis, and network tomography; we refer to [5] for a comprehensive survey of the applications of NNLS.\nWe show how our framework can be used to form exact hypothesis tests and confidence intervals for NNLS estimated coefficients. The primal dual solution pair (\u03b2\u0302, \u03bb\u0302) is a solution iff the KKT conditions are satisfied,\n\u03bb\u0302i := \u2212xTi (y \u2212X\u03b2\u0302) \u2265 0 for all i \u03b2\u0302 \u2265 0.\nLet S\u0302 = {i : \u2212xTi (y \u2212 X\u03b2\u0302) = 0}. By complementary slackness \u03b2\u0302\u2212S\u0302 = 0, where \u2212S\u0302 is the complement to the \u201cactive\u201d variables S\u0302 chosen by NNLS. Given the active set we can solve the KKT equation for the value of \u03b2\u0302S\u0302 ,\n\u2212XT S\u0302 (y \u2212X\u03b2\u0302) = 0\n\u2212XT S\u0302 (y \u2212XS\u0302 \u03b2\u0302S\u0302) = 0\n\u03b2\u0302S\u0302 = X \u2020 S\u0302 y,\nwhich is a linear contrast of y. The NNLS selection event is\nE\u0302(y) = {y : XT S\u0302 (y \u2212X\u03b2\u0302) = 0, XT\u2212S\u0302(y \u2212X\u03b2\u0302) > 0}\n= {y : XT S\u0302 (y \u2212X\u03b2\u0302) \u2265 0,\u2212XT S\u0302 (y \u2212X\u03b2\u0302) \u2265 0, XT\u2212S\u0302(y \u2212X\u03b2\u0302) > 0} = {y : XT S\u0302 (I \u2212XS\u0302X \u2020 S\u0302 )y \u2265 0,\u2212XT S\u0302 (I \u2212XS\u0302X \u2020 S\u0302 )y \u2265 0, XT\u2212S\u0302(I \u2212XS\u0302X \u2020 S\u0302 )y > 0} = {y : A(S\u0302)y \u2264 0}.\nThe selection event encodes that for a given y the NNLS optimization program will select a subset of variables S\u0302(y). Similar to the case in OMP and marginal screening, we can use Algorithms 2 and 3, since the selection event is represented by a set of linear constraints {y : A(S\u0302)y \u2264 0}."}, {"heading": "8 Conclusion", "text": "Due to the increasing size of datasets, marginal screening has become an important method for fast variable selection. However, the standard hypothesis tests and confidence intervals used in linear regression are invalid\nafter using marginal screening to select important variables. We have described a method to perform hypothesis and form confidence intervals after marginal screening. The conditional on selection framework is not restricted to marginal screening, and also applies to OMP, marginal screening + Lasso, and NNLS."}, {"heading": "Acknowledgements", "text": "Jonathan Taylor was supported in part by NSF grant DMS 1208857 and AFOSR grant 113039. Jason Lee was supported by a NSF graduate fellowship, and a Stanford Graduate Fellowship."}], "references": [{"title": "False discovery rate\u2013adjusted multiple confidence intervals for selected parameters", "author": ["Yoav Benjamini", "Daniel Yekutieli"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Valid post-selection inference", "author": ["Richard Berk", "Lawrence Brown", "Andreas Buja", "Kai Zhang", "Linda Zhao"], "venue": "Annals of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Statistics for Highdimensional Data", "author": ["Peter Lukas B\u00fchlmann", "Sara A van de Geer"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Statistical inference, volume 70", "author": ["George Casella", "Roger L Berger"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Nonnegativity constraints in numerical analysis", "author": ["Donghui Chen", "Robert J Plemmons"], "venue": "In Symposium on the Birth of Numerical Analysis,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Least angle regression", "author": ["Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani"], "venue": "The Annals of statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "An introduction to the bootstrap, volume 57", "author": ["Bradley Efron", "Robert Tibshirani"], "venue": "CRC press,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Variance estimation using refitted cross-validation in ultrahigh dimensional regression", "author": ["Jianqing Fan", "Shaojun Guo", "Ning Hao"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Sure independence screening for ultrahigh dimensional feature space", "author": ["Jianqing Fan", "Jinchi Lv"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Ultrahigh dimensional feature selection: beyond the linear model", "author": ["Jianqing Fan", "Richard Samworth", "Yichao Wu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Sure independence screening in generalized linear models with np-dimensionality", "author": ["Jianqing Fan", "Rui Song"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "A comparison of the lasso and marginal regression", "author": ["Christopher R Genovese", "Jiashun Jin", "Larry Wasserman", "Zhigang Yao"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Confidence intervals and hypothesis testing for high-dimensional regression", "author": ["Adel Javanmard", "Andrea Montanari"], "venue": "arXiv preprint arXiv:1306.3171,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "On model selection consistency of penalized m-estimators: a geometric theory", "author": ["Jason Lee", "Yuekai Sun", "Jonathan E Taylor"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Exact inference after model selection via the lasso", "author": ["Jason D Lee", "Dennis L Sun", "Yuekai Sun", "Jonathan E Taylor"], "venue": "arXiv preprint arXiv:1311.6238,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "The finite-sample distribution of post-model-selection estimators and uniform versus nonuniform approximations", "author": ["Hannes Leeb", "Benedikt M P\u00f6tscher"], "venue": "Econometric Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "P\u00f6tscher. Model selection and inference: Facts and fiction", "author": ["Hannes Leeb", "Benedikt M"], "venue": "Econometric Theory,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Can one estimate the conditional distribution of post-model-selection estimators", "author": ["Hannes Leeb", "Benedikt M P\u00f6tscher"], "venue": "The Annals of Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Sign-constrained least squares estimation for high-dimensional regression", "author": ["Nicolai Meinshausen"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "P-values for high-dimensional regression", "author": ["Nicolai Meinshausen", "Lukas Meier", "Peter B\u00fchlmann"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu"], "venue": "Statistical Science,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "A study of error variance estimation in lasso regression", "author": ["Stephen Reid", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Non-negative least squares for high-dimensional linear models: Consistency and sparse recovery without regularization", "author": ["Martin Slawski", "Matthias Hein"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Scaled sparse linear regression", "author": ["Tingni Sun", "Cun-Hui Zhang"], "venue": "Biometrika, 99(4):879\u2013898,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Significance analysis of microarrays applied to the ionizing radiation response", "author": ["Virginia Goss Tusher", "Robert Tibshirani", "Gilbert Chu"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "On asymptotically optimal confidence regions and tests for high-dimensional models", "author": ["Sara van de Geer", "Peter B\u00fchlmann", "Ya\u2019acov Ritov"], "venue": "arXiv preprint arXiv:1303.0518,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming (lasso)", "author": ["M.J. Wainwright"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "High dimensional variable selection", "author": ["Larry Wasserman", "Kathryn Roeder"], "venue": "Annals of statistics,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Confidence intervals for lowdimensional parameters with high-dimensional data", "author": ["Cun-Hui Zhang", "S Zhang"], "venue": "arXiv preprint arXiv:1110.2563,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "On model selection consistency of lasso", "author": ["P. Zhao", "B. Yu"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}], "referenceMentions": [{"referenceID": 2, "context": "However in the high-dimensional p > n setting, the least squares estimator is an underdetermined problem, and the predominant approach is to perform variable selection or model selection [3].", "startOffset": 187, "endOffset": 190}, {"referenceID": 12, "context": "Marginal screening is the simplest and most commonly used of the variable selection procedures [13, 28, 20].", "startOffset": 95, "endOffset": 107}, {"referenceID": 25, "context": "Marginal screening is the simplest and most commonly used of the variable selection procedures [13, 28, 20].", "startOffset": 95, "endOffset": 107}, {"referenceID": 11, "context": "Furthermore, the selection properties are comparable to the Lasso [12].", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "In the ultrahigh dimensional setting p = O(en k ), marginal screening is shown to have the SURE screening property, P (S \u2282 \u015c), that is marginal screening selects a superset of the truly relevant variables [9, 11, 10].", "startOffset": 205, "endOffset": 216}, {"referenceID": 10, "context": "In the ultrahigh dimensional setting p = O(en k ), marginal screening is shown to have the SURE screening property, P (S \u2282 \u015c), that is marginal screening selects a superset of the truly relevant variables [9, 11, 10].", "startOffset": 205, "endOffset": 216}, {"referenceID": 9, "context": "In the ultrahigh dimensional setting p = O(en k ), marginal screening is shown to have the SURE screening property, P (S \u2282 \u015c), that is marginal screening selects a superset of the truly relevant variables [9, 11, 10].", "startOffset": 205, "endOffset": 216}, {"referenceID": 16, "context": "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].", "startOffset": 76, "endOffset": 91}, {"referenceID": 17, "context": "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].", "startOffset": 76, "endOffset": 91}, {"referenceID": 18, "context": "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].", "startOffset": 76, "endOffset": 91}, {"referenceID": 1, "context": "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].", "startOffset": 76, "endOffset": 91}, {"referenceID": 16, "context": "A major line of work [17, 18, 19] has described the difficulty of inference post model selection: the distribution of post model selection estimates is complicated and cannot be approximated in a uniform sense by their asymptotic counterparts.", "startOffset": 21, "endOffset": 33}, {"referenceID": 17, "context": "A major line of work [17, 18, 19] has described the difficulty of inference post model selection: the distribution of post model selection estimates is complicated and cannot be approximated in a uniform sense by their asymptotic counterparts.", "startOffset": 21, "endOffset": 33}, {"referenceID": 18, "context": "A major line of work [17, 18, 19] has described the difficulty of inference post model selection: the distribution of post model selection estimates is complicated and cannot be approximated in a uniform sense by their asymptotic counterparts.", "startOffset": 21, "endOffset": 33}, {"referenceID": 15, "context": "Although the focus of this paper is on marginal screening, the \u201ccondition on selection\u201d framework, first proposed for the Lasso in [16], is much more general; we use marginal screening as a simple and clean illustration of the applicability of this framework.", "startOffset": 131, "endOffset": 135}, {"referenceID": 21, "context": "Such results establish, under restrictive assumptions on X, the Lasso \u03b2\u0302 is close to the unknown \u03b20 [24] and selects the correct model [33, 30, 15].", "startOffset": 100, "endOffset": 104}, {"referenceID": 30, "context": "Such results establish, under restrictive assumptions on X, the Lasso \u03b2\u0302 is close to the unknown \u03b20 [24] and selects the correct model [33, 30, 15].", "startOffset": 135, "endOffset": 147}, {"referenceID": 27, "context": "Such results establish, under restrictive assumptions on X, the Lasso \u03b2\u0302 is close to the unknown \u03b20 [24] and selects the correct model [33, 30, 15].", "startOffset": 135, "endOffset": 147}, {"referenceID": 14, "context": "Such results establish, under restrictive assumptions on X, the Lasso \u03b2\u0302 is close to the unknown \u03b20 [24] and selects the correct model [33, 30, 15].", "startOffset": 135, "endOffset": 147}, {"referenceID": 2, "context": "We refer to the reader to [3] for a comprehensive discussion about the theoretical properties of the Lasso.", "startOffset": 26, "endOffset": 29}, {"referenceID": 28, "context": "One class of methods uses sample splitting or subsampling to obtain confidence intervals and p-values [31, 23].", "startOffset": 102, "endOffset": 110}, {"referenceID": 20, "context": "One class of methods uses sample splitting or subsampling to obtain confidence intervals and p-values [31, 23].", "startOffset": 102, "endOffset": 110}, {"referenceID": 1, "context": "In the post model selection literature, the recent work of [2] proposed the POSI approach, a correction to the usual t-test confidence intervals by controlling the familywise error rate for all parameters in any possible submodel.", "startOffset": 59, "endOffset": 62}, {"referenceID": 26, "context": "A separate line of work establishes the asymptotic normality of a corrected estimator obtained by \u201cinverting\u201d the KKT conditions [29, 32, 14].", "startOffset": 129, "endOffset": 141}, {"referenceID": 29, "context": "A separate line of work establishes the asymptotic normality of a corrected estimator obtained by \u201cinverting\u201d the KKT conditions [29, 32, 14].", "startOffset": 129, "endOffset": 141}, {"referenceID": 13, "context": "A separate line of work establishes the asymptotic normality of a corrected estimator obtained by \u201cinverting\u201d the KKT conditions [29, 32, 14].", "startOffset": 129, "endOffset": 141}, {"referenceID": 15, "context": "Most closely related to our work is the \u201ccondition on selection\u201d framework laid out in [16] for the Lasso.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "By extension, we do not make any assumptions on n and p, which is unusual in high-dimensional statistics [3].", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "A data splitting technique is used in [8], while [27] proposes a method that computes the regression estimate and an estimate of the variance simultaneously.", "startOffset": 38, "endOffset": 41}, {"referenceID": 24, "context": "A data splitting technique is used in [8], while [27] proposes a method that computes the regression estimate and an estimate of the variance simultaneously.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "We refer the reader to [25] for a survey and comparison of the various methods,", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "This section summarizes the recent tools developed in [16] for testing contrasts2 \u03b7T y of a constrained Gaussian y.", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "The results are stated without proof and the proofs can be found in [16].", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "In [9], the screening property S0 \u2282 \u015c for the marginal screening algorithm is", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "The standard way to obtain an interval is to invert a pivotal quantity [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "In relation to the literature on False Coverage Rate (FCR) [1], our procedure also controls the FCR.", "startOffset": 59, "endOffset": 62}, {"referenceID": 5, "context": "The diabetes dataset contains n = 442 diabetes patients measured on p = 10 baseline variables [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "This is known as the residual bootstrap, and is a standard method for assessing statistical procedures when the underlying model is unknown [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 15, "context": "This framework was first proposed in [16]", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "1 Marginal screening + Lasso The marginal screening+Lasso procedure was introduced in [9] as a variable selection method for the ultra-high dimensional setting of p = O(en k ).", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "[9] recommend applying the marginal screening algorithm with k = n \u2212 1, followed by the Lasso on the selected variables.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Let (\u015cm, \u015dm) be the variables and signs selected by marginal screening, and the (\u015cL, \u1e91L) be the variables and signs selected by Lasso [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "2 of [16], it is shown how to encode the Lasso selection event (\u015cL, \u1e91L) as a set of constraints {ALy \u2264 bL} 4, and in Section 4 we showed how to encode the marginal screening selection event (\u015cm, \u015dm) as a set of constraints {Amy \u2264 bm}.", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "Under a positive eigenvalue conditions on X, several authors [26, 22] have shown that NNLS is comprable to the Lasso in terms of prediction and", "startOffset": 61, "endOffset": 69}, {"referenceID": 19, "context": "Under a positive eigenvalue conditions on X, several authors [26, 22] have shown that NNLS is comprable to the Lasso in terms of prediction and", "startOffset": 61, "endOffset": 69}, {"referenceID": 4, "context": "Non-negativity constraints arise naturally in non-negative matrix factorization, signal deconvolution, spectral analysis, and network tomography; we refer to [5] for a comprehensive survey of the applications of NNLS.", "startOffset": 158, "endOffset": 161}], "year": 2014, "abstractText": "We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response y, conditional on the model being selected (\u201ccondition on selection\u201d framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in highdimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix X. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit, non-negative least squares, and marginal screening+Lasso.", "creator": "LaTeX with hyperref package"}}}