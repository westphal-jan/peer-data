{"id": "1403.5556", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2014", "title": "Learning to Optimize via Information-Directed Sampling", "abstract": "this paradigm proposes information directed measurement - - - new platform for optimal attribute exploration and relevance in online optimization problems in circumstances a selection - maker must learn achieved partial equality. irrelevant data quantifies the amount learned whereas recognizing an opportunity through incomplete expectation theoretic imperative : finite mutual indifference relates the true optimal action and the employers'predicted next observation. actions are then selected by retaining a myopic objective therefore favors earning high immediate input problems acquiring intelligence. techniques show this algorithm exceptionally remarkably objective and is effectively efficient in simulation trials. we compare novel and general regret trials that scale assuming minimal entropy of the ultimate learning distribution. furthermore, as we provided us several examples, information directed sampling sometimes dramatically outperforms subjective analyses like ucb algorithms and thompson expressions which don'- recover cumulative information provided by automated actions.", "histories": [["v1", "Fri, 21 Mar 2014 02:02:25 GMT  (17kb)", "https://arxiv.org/abs/1403.5556v1", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v2", "Sun, 8 Jun 2014 20:40:38 GMT  (172kb,D)", "http://arxiv.org/abs/1403.5556v2", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v3", "Thu, 3 Jul 2014 01:09:22 GMT  (174kb,D)", "http://arxiv.org/abs/1403.5556v3", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v4", "Tue, 22 Jul 2014 17:59:30 GMT  (175kb,D)", "http://arxiv.org/abs/1403.5556v4", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v5", "Fri, 12 Aug 2016 06:53:32 GMT  (257kb,D)", "http://arxiv.org/abs/1403.5556v5", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v6", "Wed, 24 May 2017 00:06:24 GMT  (257kb,D)", "http://arxiv.org/abs/1403.5556v6", "arXiv admin note: substantial text overlap witharXiv:1403.5341"], ["v7", "Fri, 7 Jul 2017 05:51:15 GMT  (259kb,D)", "http://arxiv.org/abs/1403.5556v7", "arXiv admin note: substantial text overlap witharXiv:1403.5341"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1403.5341", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel russo 0001", "benjamin van roy"], "accepted": true, "id": "1403.5556"}, "pdf": {"name": "1403.5556.pdf", "metadata": {"source": "CRF", "title": "Learning to Optimize Via Information-Directed Sampling", "authors": ["Daniel Russo", "Benjamin Van Roy"], "emails": ["daniel.russo@kellogg.northwestern.edu", "bvr@stanford.edu"], "sections": [{"heading": null, "text": "in which a decision-maker must balance between exploration and exploitation while learning from partial feedback. Each action is sampled in a manner that minimizes the ratio between squared expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation.\nWe establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution. We illustrate through simple analytic examples how information-directed sampling accounts for kinds of information that alternative approaches do not adequately address and that this can lead to dramatic performance gains. For the widely studied Bernoulli, Gaussian, and linear bandit problems, we demonstrate state-of-the-art simulation performance."}, {"heading": "1 Introduction", "text": "In the classical multi-armed bandit problem, a decision-maker repeatedly chooses from among a finite set of actions. Each action generates a random reward drawn independently from a probability distribution associated with the action. The decision-maker is uncertain about these reward distributions, but learns about them as rewards are observed. Strong performance requires striking a balance between exploring poorly understood actions and exploiting previously acquired knowledge to attain high rewards. Because selecting one action generates no information pertinent to other actions, effective algorithms must sample every action many times.\nThere has been significant interest in addressing problems with more complex information structures, in which sampling one action can inform the decision-maker\u2019s assessment of other actions. Effective algorithms must take advantage of the information structure to learn more efficiently. The most popular approaches to such problems extend upper-confidence-bound (UCB) algorithms and Thompson sampling, which were originally devised for the classical multi-armed bandit problem. In some cases, such as the linear bandit problem, strong performance guarantees have been established for these approaches. For some problem classes, compelling empirical results have also been presented for UCB algorithms and Thompson sampling, as well as the knowledge gradient algorithm. However, as we will demonstrate through simple analytic examples, these approaches can perform very poorly when faced with more complex information structures. Shortcomings stem from the fact that they do not adequately account for particular kinds of information.\nIn this paper, we propose a new approach \u2013 information-directed sampling (IDS) \u2013 that is designed to address this. IDS quantifies the amount learned by selecting an action through an\nar X\niv :1\n40 3.\n55 56\nv7 [\ncs .L\nG ]\n7 J\nul 2\ninformation-theoretic measure: the mutual information between the true optimal action and the next observation. Each action is sampled in a manner that minimizes the ratio between squared expected single-period regret and this measure of information gain. Through this information measure, IDS accounts for kinds of information that alternatives fail to address.\nAs we will demonstrate through simple analytic examples, IDS can dramatically outperform UCB algorithms, Thompson sampling, and the knowledge-gradient algorithm. Further, by leveraging the tools of our recent information theoretic analysis of Thompson sampling [61], we establish an expected regret bound for IDS that applies across a very general class of models and scales with the entropy of the optimal action distribution. We also specialize this bound to several classes of online optimization problems, including problems with full feedback, linear optimization problems with bandit feedback, and combinatorial problems with semi-bandit feedback, in each case establishing that bounds are order optimal up to a poly-logarithmic factor.\nWe benchmark the performance of IDS through simulations of the widely studied Bernoulli, Gaussian, and linear bandit problems, for which UCB algorithms and Thompson sampling are known to be very effective. We find that even in these settings, IDS outperforms UCB algorithms and Thompson sampling. This is particularly surprising for Bernoulli bandit problems, where UCB algorithms and Thompson sampling are known to be asymptotically optimal in the sense proposed by Lai and Robbins [49].\nIDS solves a single-period optimization problem as a proxy to an intractable multi-period problem. Solution of this single-period problem can itself be computationally demanding, especially in cases where the number of actions is enormous or mutual information is difficult to evaluate. We develop numerical methods for particular classes of online optimization problems. In some cases, our numerical methods do not compute exact or near-exact solutions but generate efficient approximations that are intended to capture key benefits of IDS. There is much more work to be done to design efficient algorithms for various problem classes and we hope that our analysis and initial collection of numerical methods will provide a foundation for further developments.\nIt is worth noting that the problem formulation we work with, which is presented in Section 3, is very general, encompassing not only problems with bandit feedback, but also a broad array of information structures for which observations can offer information about rewards of arbitrary subsets of actions or factors that influence these rewards. Because IDS and our analysis accommodate this level of generality, they can be specialized to problems that in the past have been studied individually, such as those involving pricing and assortment optimization (see, e.g., [14, 57, 65]), though in each case, developing a computationally efficient version of IDS may require innovation."}, {"heading": "2 Literature review", "text": "UCB algorithms are the primary approach considered in the segment of the stochastic multi-armed bandit literature that treats problems with dependent arms. UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.g. Lipschitz continuous) model [17, 45, 68]. Recently, an algorithm known as Thompson sampling has received a great deal of interest. Agrawal and Goyal [6] provided the first analysis for linear contextual bandit problems. Russo and Van Roy [58, 59] consider a more general class of models, and show that standard analysis of upper confidence bound algorithms leads to bounds on the expected regret of Thompson sampling. Very recent work of Gopalan et al. [33] provides asymptotic frequentist bounds on the growth rate of regret for problems with dependent arms. Both UCB algorithms and Thompson\nsampling have been applied to other types of problems, like reinforcement learning [40, 53] and Monte Carlo tree search [10, 46].\nIn one of the first papers on multi-armed bandit problems with dependent arms, Agrawal et al. [3] consider a general model in which the reward distribution associated with each action depends on a common unknown parameter. When the parameter space is finite, they provide a lower bound on the asymptotic growth rate of the regret of any admissible policy as the time horizon tends to infinity and show that this bound is attainable. These results were later extended by Agrawal et al. [4] and Graves and Lai [34] to apply to the adaptive control of Markov chains and to problems with infinite parameter spaces. These papers provide results of fundemental importance, but seem to have been overlooked by much of the recent literature.\nThough the use of mutual information to guide sampling has been the subject of much research, dating back to the work of Lindley [50], to our knowledge, only two other papers [36, 70] have used the mutual information between the optimal action and the next observation to guide action selection. Each focuses on optimization of expensive-to-evaluate black-box functions. Here, black\u2013box indicates the absence of strong structural assumptions such as convexity and that the algorithm only has access to function evaluations, while expensive-to-evaluate indicates that the cost of evaluation warrants investing considerable effort to determine where to evaluate. These papers focus on settings with low-dimensional continuous action spaces, and with a Gaussian process prior over the objective function, reflecting the belief that \u201csmoother\u201d objective functions are more plausible than others. This approach is often called \u201cBayesian optimization\u201d in the machine learning community [13]. Both Villemonteix et al. [70] and Hennig and Schuler [36] propose selecting each sample to maximize the mutual information between the next observation and the optimal solution. Several papers [37, 38, 39] have extended this line of work since an initial version of our paper appeared online. The numerical routines in these papers use approximations to mutual information, and may give insight into how to design efficient computational approximations to IDS.\nSeveral features distinguish our work from that of Villemonteix et al. [70] and Hennig and Schuler [36]. First, these papers focus on pure exploration problems: the objective is simply to learn about the optimal solution \u2013 not to attain high cumulative reward. Second, and more importantly, they focus only on problems with Gaussian process priors and continuous action spaces. For such problems, simpler approaches like UCB algorithms [67], probability of improvement [47], and expected improvement [51] are already extremely effective. As noted by Brochu et al. [13], each of these algorithms simply chooses points with \u201cpotentially high values of the objective function: whether because the prediction is high, the uncertainty is great, or both.\u201d By contrast, a major motivation of our work is that a richer information measure is needed to address problems with more complicated information structures. Finally, we provide a variety of general theoretical guarantees for information-directed sampling, whereas Villemonteix et al. [70] and Hennig and Schuler [36] propose their algorithms as heuristics without guarantees. Section 9.1 shows that our theoretical guarantees extend to pure exploration problems.\nThe knowledge gradient (KG) algorithm uses a different measure of information to guide action selection: the algorithm computes the impact of a single observation on the quality of the decision made by a greedy algorithm, which simply selects the action with highest posterior expected reward. This measure was proposed by Mockus et al. [51] and studied further by Frazier et al. [29] and Ryzhov et al. [64]. KG seems natural since it explicitly seeks information that improves decision quality. Computational studies suggest that for problems with Gaussian priors, Gaussian rewards, and relatively short time horizons, KG performs very well. However, there are no general guarantees for KG, and even in some simple settings, it may not converge to optimality. In fact, it may select a suboptimal action in every period, even as the time horizon tends to infinity. IDS also measures the information provided by a single observation, but our results imply it converges to optimality.\nKG is discussed further in Subsection 4.3.3. Our work also connects to a much larger literature on Bayesian experimental design (see [20] for a review). Contal et al. [22] study problems with Gaussian process priors and a method that guides exploration using the mutual information between the objective function and the next observation. This work provides a regret bound, though, as the authors\u2019 erratum indicates, the proof of the regret bound is incorrect. Recent work has demonstrated the effectiveness of greedy or myopic policies that always maximize a measure of the information gain from the next sample. Jedynak et al. [41] and Waeber et al. [71] consider problem settings in which this greedy policy is optimal. Another recent line of work [31, 32] shows that measures of information gain sometimes satisfy a decreasing returns property known as adaptive sub-modularity, implying the greedy policy is competitive with the optimal policy. Our algorithm also only considers the information gain due to the next sample, even though the goal is to acquire information over many periods. Our results establish that the manner in which IDS encourages information gain leads to an effective algorithm, even for the different objective of maximizing cumulative reward.\nFinally, our work connects to the literature on partial monitoring. First introduced by [54] the partial monitoring problem encompasses a broad range of online optimization problems with limited or partial feedback. Recent work [11] has focused on classifying the minimax-optimal scaling of regret in the problem\u2019s time horizon as a function of the level of feedback the agent receives. That work focuses most attention on cases where the agent receives very restrictive feedback, and in particular, cannot observe the reward their action generates. Our work also allows the agent to observe rich forms of feedback in response to actions they select, but we focus on a more standard decision-theoretic framework in which the agent associates their observations with a reward as specified by a utility function.\nThe literature we have discussed primarily focuses on contexts where the goal is to converge on an optimal action in a manner that limits exploration costs. Such methods are not geared towards problems where time preference plays an important role. A notable exception is the KG algorithm, which takes a discount factor as input to account for time preference. Francetich and Kreps [26, 27] discuss a variety of heuristics for the discounted problem. Recent work [62] generalizes Thompson sampling to address discounted problems. We believe that IDS can also be extended to treat discounted problems, though we do not pursue that in this paper.\nThe regret bounds we will present build on our information-theoretic analysis of Thompson sampling [61], which can be used to bound the regret of any policy in terms of its information ratio. The information ratio of IDS is always smaller than that of TS, and therefore, bounds on the information ratio of TS provided in Russo and Van Roy [61] yield regret bounds for IDS. This observation and a preliminary version of our results was first presented in a conference paper [60]. Recent work by Bubeck et al. [18] and Bubeck and Eldan [16] build on ideas from [61] in another direction by bounding the information ratio when the reward function is convex and using that bound to study the order of regret in adversarial bandit convex optimization."}, {"heading": "3 Problem formulation", "text": "We consider a general probabilistic, or Bayesian, formulation in which uncertain quantities are modeled as random variables. The decision-maker sequentially chooses actions(At)t\u2208N from a finite action set A and observes the corresponding outcomes (Yt,At)t\u2208N. There is a random outcome Yt,a \u2208 Y associated with each action a \u2208 A and time t \u2208 N. Let Yt \u2261 (Yt,a)a\u2208A be the vector of outcomes at time t \u2208 N. There is a random variable \u03b8 such that, conditioned on \u03b8, (Yt)t\u2208N is an iid sequence. This can be thought of as a Bayesian formulation, where randomness in \u03b8 captures\nthe decision-maker\u2019s prior uncertainty about the true nature of the system, and the remaining randomness in Yt captures idiosyncratic randomness in observed outcomes.\nThe agent associates a reward R(y) with each outcome y \u2208 Y, where the reward function R : Y \u2192 R is fixed and known. Let Rt,a = R(Yt,a) denote the realized reward of action a at time t. Uncertainty about \u03b8 induces uncertainty about the true optimal action, which we denote by A\u2217 \u2208 arg max\na\u2208A E [R1,a|\u03b8]. The T\u2013period regret of the sequence of actions A1, .., AT is the random\nvariable\nRegret(T ) := T\u2211 t=1 (Rt,A\u2217 \u2212Rt,At) , (1)\nwhich measures the cumulative difference between the reward earned by an algorithm that always chooses the optimal action and actual accumulated reward up to time T . In this paper we study expected regret\nE [Regret(T )] = E [ T\u2211 t=1 (Rt,A\u2217 \u2212Rt,At) ] , (2)\nwhere the expectation is taken over the randomness in the actions At and the outcomes Yt, and over the prior distribution over \u03b8. This measure of performance is commonly called Bayesian regret or Bayes risk.\nThe action At is chosen based on the history of observations Ft = (A1, Y1,A1 , . . . , At\u22121, Yt\u22121,At\u22121) up to time t. Formally, a randomized policy \u03c0 = (\u03c0t)t\u2208N is a sequence of deterministic functions, where \u03c0t(Ft) specifies a probability distribution over the action set A. Let D(A) denote the set of probability distributions over A. The action At is a selected by sampling independently from \u03c0t(Ft). With some abuse of notation, we will typically write this distribution as \u03c0t, where \u03c0t(a) = P(At = a|Ft) denotes the probability assigned to action a given the observed history. We explicitly display the dependence of regret on the policy \u03c0, letting E [Regret(T, \u03c0)] denote the expected value given by (2) when the actions (A1, .., AT ) are chosen according to \u03c0.\nFurther notation. Set \u03b1t(a) = P (A\u2217 = a|Ft) to be the posterior distribution of A\u2217. For two probability measures P and Q over a common measurable space, if P is absolutely continuous with respect to Q, the Kullback-Leibler divergence between P and Q is\nDKL(P ||Q) = \u02c6 log ( dP\ndQ\n) dP (3)\nwhere dPdQ is the Radon\u2013Nikodym derivative of P with respect to Q. For a probability distribution P over a finite set X , the Shannon entropy of P is defined as H(P ) = \u2212 \u2211 x\u2208X P (x) log (P (x)). The mutual information under the posterior distribution between two random variables X1 : \u2126 \u2192 X1, and X2 : \u2126\u2192 X2, denoted by\nIt(X1;X2) := DKL (P ((X1, X2) \u2208 \u00b7|Ft) || P (X1 \u2208 \u00b7|Ft)P (X2 \u2208 \u00b7|Ft)) , (4)\nis the Kullback-Leibler divergence between the joint posterior distribution of X1 and X2 and the product of the marginal distributions. Note that It(X1;X2) is a random variable because of its dependence on the conditional probability measure P (\u00b7|Ft).\nTo reduce notation, we define the information gain from an action a to be gt(a) := It(A\u2217;Yt,a). As shown for example in Lemma 5.5.6 of Gray [35], this is equal to the expected reduction in entropy of the posterior distribution of A\u2217 due to observing Yt(a):\ngt(a) = E [H(\u03b1t)\u2212H(\u03b1t+1)|Ft, At = a] , (5)\nwhich plays a crucial role in our results. Let \u2206t(a) := E [Rt,A\u2217 \u2212Rt,a|Ft] denote the expected instantaneous regret of action a at time t.\nWe use overloaded notation for gt(\u00b7) and \u2206t(\u00b7). For an action sampling distribution \u03c0 \u2208 D(A), gt(\u03c0) := \u2211 a\u2208A \u03c0(a)gt(a) denotes the expected information gain when actions are selected according\nto \u03c0, and \u2206t(\u03c0) = \u2211 a\u2208A \u03c0(a)\u2206t(a) is defined analogously. Finally, we sometimes use the shorthand notation Et[\u00b7] = Et[\u00b7|Ft] for conditional expectations under the posterior distribution, and similarly write Pt(\u00b7) = P(\u00b7|Ft)."}, {"heading": "4 Algorithm design principles", "text": "The primary contribution of this paper is information-directed sampling (IDS), a general principle for designing action-selection algorithms. We will define IDS in this section, after discussing motivations underlying its structure. Further, through a set of examples, we will illustrate how alternative design principles fail to account for particular kinds of information and therefore can be dramatically outperformed by IDS."}, {"heading": "4.1 Motivation", "text": "Our goal is to minimize expected regret over a time horizon T . This is achieved by a Bayes-optimal policy, which, in principle, can be computed via dynamic programming. Unfortunately, computing, or even storing, this Bayes-optimal policy is generally infeasible. For this reason, there has been significant interest in developing computationally efficient heuristics.\nAs with much of the literature, we are motivated by contexts where the time horizon T is \u201clarge.\u201d For large T and moderate times t T , the mapping from belief state to action prescribed by the Bayes-optimal policy does not vary significantly from one time period to the next. As such, it is reasonable to restrict attention to stationary heuristic policies. IDS falls in this category.\nIDS is motivated largely by a desire to overcome shortcomings of currently popular design principles. In particular, it accounts for kinds of information that alternatives fail to adequately address:\n1. Indirect information. IDS can select an action to obtain useful feedback about other actions even if there will be no useful feedback about the selected action.\n2. Cumulating information. IDS can select an action to obtain feedback that does not immediately enable higher expected reward but can eventually do so when combined with feedback from subsequent actions.\n3. Irrelevant information. IDS avoids investments in acquiring information that will not help to determine which actions ought to be selected.\nExamples presented in Section 4.3 aim to contrast the manner in which IDS and alternative approaches treat these kinds of information.\nIt is worth noting that we refer to IDS as a design principle rather than an algorithm. The reason is that IDS does not specify steps to be carried out in terms of basic computational operations but only an abstract objective to be optimized. As we will discuss later, for many problem classes of interest, like the Bernoulli bandit, the Gaussian bandit, and the linear bandit, one can develop tractable algorithms that implement IDS. The situation is similar for upper confidence bounds, Thompson sampling, expected improvement maximization, and knowledge gradient; these are abstract design principles that lead to tractable algorithms for specific problem classes."}, {"heading": "4.2 Information-directed sampling", "text": "IDS balances between obtaining low expected regret in the current period and acquiring new information about which action is optimal. It does this by minimizing over all action sampling distributions \u03c0 \u2208 D(A) the ratio between the square of expected regret \u2206t(\u03c0)2 and information gain gt(\u03c0) about the optimal action A\u2217. In particular, the policy \u03c0IDS = ( \u03c0IDS1 , \u03c0 IDS 2 , . . . ) is defined by:\n\u03c0IDSt \u2208 arg min \u03c0\u2208D(A)\n{ \u03a8t (\u03c0) := \u2206t(\u03c0)2\ngt(\u03c0)\n} . (6)\nWe call \u03a8t(\u03c0) the information ratio of an action sampling distribution \u03c0. It measures the squared regret incurred per-bit of information acquired about the optimum. IDS myopically minimizes this notion of cost-per-bit of information in each period.\nNote that IDS is stationary randomized policy: randomized in that each action is randomly sampled from a distribution and stationary in that this action distribution is determined by the posterior distribution of \u03b8 and otherwise independent of the time period. It is natural to wonder whether randomization plays a fundamental role or if a stationary deterministic policy can offer similar behavior. The following example sheds light on this matter. Example 1 (A known standard). Consider a problem with two actions A = {a1, a2}. Rewards from a1 are known to be distributed Bernoulli(1/2). The distribution of rewards from a2 is Bernoulli(3/4) with prior probability p0 and is Bernoulli(1/4) with prior probability 1\u2212 p0.\nConsider a stationary deterministic policy for this problem. With such a policy, each action At is a deterministic function of pt\u22121, the posterior probability conditioned on observations made through period t \u2212 1. Suppose that for some p0 > 0, the policy selects A1 = a1. Since this is an uninformative action, pt = p0 and At = a1 for all t, and thus, expected regret grows linearly with time. If, on the other hand, A1 = a2 for all p0 > 0 then At = a2 for all t, which again results in expected regret that grows linearly with time. It follows that, for any deterministic stationary policy, there exists a prior probability p0 such that expected regret grows linearly with time.\nIn Section 5, we will establish a sub-linear bound on expected regret of IDS. The result implies that, when applied to the preceding example, the expected regret of IDS does not grow linearly as does that of any stationary deterministic policy. This suggests that randomization plays a fundamental role.\nIt may appear that the need for randomization introduces great complexity since the solution of the optimization problem (6) takes the form of a distribution over actions. However, an important property of this problem dramatically simplifies solutions. In particular, as we will establish in Section 6, there is always a distribution with support of at most two actions that attains the minimum in (6)."}, {"heading": "4.3 Alternative design principles", "text": "Several alternative design principles have figured prominently in the literature. However, each of them fails to adequately address one or more of the categories of information enumerated in Section 4.1. This motivated our development of IDS. In this section, we will illustrate through a set of examples how IDS accounts for such information while alternatives fail."}, {"heading": "4.3.1 Upper confidence bounds and Thompson sampling", "text": "Upper confidence bound (UCB) and Thompson sampling (TS) are two of the most popular principles for balancing between exploration and exploitation. As data is collected, both approaches do\nnot only estimate the rewards generated by different actions, but carefully track the uncertainty in their estimates. They then continue to experiment with all actions that could plausibly be optimal given the observed data. This guarantees actions are not prematurely discarded, but, in contrast to more naive approaches likes the -greedy algorithm, also ensures that samples are not wasted on clearly suboptimal actions.\nWith a UCB algorithm, actions are selected through two steps. First, for each action a \u2208 A an upper confidence bound Bt(a) is constructed. Then, the algorithm selects an action At \u2208 arg maxa\u2208ABt(a) with maximal upper confidence bound. The upper confidence bound Bt(a) represents the greatest mean reward value that is statistically plausible. In particular, Bt(a) is typically constructed to be optimistic (Bt(a) \u2265 E[Rt,a|\u03b8]) and asymptotically consistent (Bt(a)\u2192 E [Rt,a|\u03b8] as data about the action a accumulates).\nA TS algorithm simply samples each actions according to the posterior probability that it is optimal. In particular, at each time t, an action is sampled from \u03c0TSt = \u03b1t. This means that for each a \u2208 A, P(At = a|Ft) = P(A\u2217 = a|Ft) = \u03b1t(a). This algorithm is sometimes called probability matching because the action selection distribution is matched to the posterior distribution of the optimal action.\nFor some problem classes, UCB and TS lead to efficient and empirically effective algorithms with strong theoretical guarantees. Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].\nUnfortunately, as the following examples demonstrate, UCB and TS do not pursue indirect information and because of that can perform very poorly relative to IDS for some natural problem classes. A common feature of UCB and TS that leads to poor performance in these examples is that they restrict attention to sampling actions that have some chance of being optimal. This is the case with TS because each action is selected according to the probability that it is optimal. With UCB, the upper-confidence-bound of an action known to be suboptimal will always be dominated by others.\nOur first example is somewhat contrived but designed to make the point transparent.\nExample 2. (a revealing action) Let A = {0, 1, . . . ,K} consist of K+1 actions and suppose that \u03b8 is drawn uniformly at random from a finite set \u0398 = {1, . . . ,K} of K possible values. Consider a problem with bandit-feedback Yt,a = Rt,a. Under \u03b8, the reward of action a is\nRt,a =  1 \u03b8 = a 0 \u03b8 6= a, a 6= 0 1 2\u03b8 a = 0.\nNote that action 0 is known to never yield the maximal reward, and is therefore never selected by TS or UCB. Instead, these algorithms will select among actions {1, . . . ,K}, ruling out only a single action at a time until a reward 1 is earned and the optimal action is identified. Their expected regret therefore grows linearly in K. IDS is able to recognize that much more is learned by drawing action 0 than by selecting one of the other actions. In fact, selecting action 0 immediately identifies the optimal action. IDS selects this action, learns which action is optimal, and selects that action in all future periods. Its regret is independent of K.\nOur second example may be of greater practical significance. It represents the simplest case of a sparse linear model.\nExample 3. (sparse linear model) Consider a linear bandit problem where A \u2282 Rd and the reward from an action a \u2208 A is aT \u03b8\u2217. The true parameter \u03b8 is known to be drawn uniformly at\nrandom from the set of 1-sparse vectors \u0398 = {\u03b8\u2032 \u2208 {0, 1}d : \u2016\u03b8\u2032\u20160 = 1}. For simplicity, assume d = 2m for some m \u2208 N. The action set is taken to be the set of vectors in {0, 1}d normalized to be a unit vector in the L1 norm: A = { x \u2016x\u20161 : x \u2208 {0, 1} d, x 6= 0 } .\nFor this problem, when an action a is selected and y = aT \u03b8 \u2208 {0, 1/\u2016a\u20160} is observed, each \u03b8\u2032 \u2208 \u0398 with aT \u03b8\u2032 6= y is ruled out. Let \u0398t denote the set of all parameters in \u0398 that are consistent with the rewards observed up to time t and let It = {i \u2208 {1, . . . , d} : \u03b8\u2032i = 1, \u03b8\u2032 \u2208 \u0398t} denote the corresponding set of possible positive components.\nNote that A\u2217 = \u03b8. That is, if \u03b8 were known, choosing the action \u03b8 would yield the highest possible reward. TS and UCB algorithms only choose actions from the support of A\u2217 and therefore will only sample actions a \u2208 A that, like A\u2217, have only a single positive component. Unless that is also the positive component of \u03b8, the algorithm will observe a reward of zero and rule out only one element of It. In the worst case, the algorithm requires d samples to identify the optimal action.\nNow, consider an application of IDS to this problem. The algorithm essentially performs binary search: it selects a \u2208 A with ai > 0 for half of the components i \u2208 It and ai = 0 for the other half as well as for any i /\u2208 It. After just log2(d) time steps the true value of the parameter vector \u03b8 is identified.\nTo see why this is the case, first note that all parameters in \u0398t are equally likely and hence the expected reward of an action a is 1|It| \u2211 i\u2208It ai. Since ai \u2265 0 and \u2211 i ai = 1 for each a \u2208 A, every action whose positive components are in It yields the highest possible expected reward of 1/|It|. Therefore, binary search minimizes expected regret in period t for this problem. At the same time, binary search is assured to rule out half of the parameter vectors in \u0398t at each time t. This is the largest possible expected reduction, and also leads to the largest possible information gain about A\u2217. Since binary search both minimizes expected regret in period t and uniquely maximizes expected information gain in period t, it is the sampling strategy followed by IDS.\nIn this setting we can explicitly calculate the information ratio of each policy, and the difference between them highlights the advantages of information-directed sampling. We have\n\u03a81(\u03c0TS1 ) = (d\u2212 1)2/d2\nlog(d) d + d\u22121 d log ( d d\u22121 ) \u223c dlog(d) \u03a81(\u03c0IDS1 ) = 1log(2) ( 1\u2212 1 d )2 \u223c 1log(2)\nwhere h(d) \u223c f(d) if h(d)/f(d) \u2192 1 as d \u2192 \u221e. When the dimension d is large, \u03a81(\u03c0IDS1 ) is much smaller.\nOur final example involves an assortment optimization problem. Example 4. (assortment optimization) Consider the problem of repeatedly recommending an assortment of products to a customer. The customer has unknown type \u03b8 \u2208 \u0398 where |\u0398| = n. Each product is geared toward customers of a particular type, and the assortment a \u2208 A = \u0398m of m products offered is characterized by the vector of product types a = (a1, . . . , am). We model customer responses through a random utility model in which customers are more likely to derive high value from a product geared toward their type. When offered an assortment of products a, the customer associates with the ith product utility U (t)\u03b8,i (a) = \u03b21{ai=\u03b8} + W (t) i , where W ti follows an extreme\u2013value distribution and \u03b2 \u2208 R is a known constant. This is a standard multinomial logit discrete choice model. The probability a customer of type \u03b8 chooses product i is given by\nexp{\u03b21{ai=\u03b8}}\u2211m j=1 exp{\u03b21{aj=\u03b8}} .\nWhen an assortment a is offered at time t, the customer makes a choice It = arg maxi U (t)\u03b8i (a) and leaves a review U (t)\u03b8It(a) indicating the utility derived from the product, both of which are observed by\nthe recommendation system. The reward to the recommendation system is the normalized utility of the customer U (t)\u03b8It(a)/\u03b2.\nIf the type \u03b8 of the customer were known, then the optimal recommendation would be A\u2217 = (\u03b8, \u03b8, . . . , \u03b8), which consists only of products targeted at the customer\u2019s type. Therefore, both TS and UCB would only offer assortments consisting of a single type of product. Because of this, TS and UCB each require order n samples to learn the customer\u2019s true type. IDS will instead offer a diverse assortment of products to the customer, allowing it to learn much more quickly.\nTo render issues more transparent, suppose that \u03b8 is drawn uniformly at random from \u0398 and consider the behavior of each type of algorithm in the limiting case where \u03b2 \u2192\u221e. In this regime, the probability a customer chooses a product of type \u03b8 if it is available tends to 1, and the normalized review \u03b2\u22121U (t)\u03b8It(a) tends to 1{aIt=\u03b8}, an indicator for whether the chosen product is of type \u03b8. The initial assortment offered by IDS will consist of m different and previously untested product types. Such an assortment maximizes both the algorithm\u2019s expected reward in the next period and the algorithm\u2019s information gain, since it has the highest probability of containing a product of type \u03b8. The customer\u2019s response almost perfectly indicates whether one of those items was of type \u03b8. The algorithm continues offering assortments containing m unique, untested, product types until a review near U (t)\u03b8It(a) \u2248 1 is received. With extremely high probability, this takes at most dn/me time periods. By diversifying the m products in the assortment, the algorithm learns a factor of m times faster.\nAs in the previous example, we can explicitly calculate the information ratio of each policy, and the difference between them highlights the advantages of IDS. The information ratio of IDS is more than m times smaller:\n\u03a81(\u03c0TS1 ) =\n( 1\u2212 1n )2 1 n log(n) + n\u22121 n log ( n n\u22121 ) \u223c nlog(n) \u03a81(\u03c0IDS1 ) = ( 1\u2212 mn )2 m n log( n m) + n\u2212m n log ( n n\u2212m ) \u2264 n m log( nm) ."}, {"heading": "4.3.2 Other information-directed approaches", "text": "Another natural information-directed algorithm aims to maximize the information acquired about the uncertain model parameter \u03b8. In particular, consider an algorithm that selects the action at time t that maximizes the weighted combination of the expected reward the action generates and the information it generates about the uncertain model parameter \u03b8: Et[Rt,a] + \u03bbIt(Yt,a; \u03b8). Throughout this section we will refer to this algorithm as \u03b8-IDS. While such an algorithm can perform well on particular examples, the next example highlights that it may invest in acquiring information about \u03b8 that is irrelevant to the decision problem.\nExample 5. (unconstrained assortment optimization) Consider again the problem of repeatedly recommending assortments of products to a customer with unknown preferences. The recommendation system can choose any subset of products a \u2282 {1, .., n} to display. When offered assortment a at time t, the customer chooses the item Jt = arg maxi\u2208a \u03b8i and leaves the review Rt,a = \u03b8Jt where \u03b8i is the utility associated with product i. The recommendation system observes both Jt and the review Rt,a, and has the goal of learning to offer the assortment that yields the best outcome for the customer and maximizes the review Rt,a. Suppose that \u03b8 is drawn as a uniformly random permutation of the vector (1, 12 , 1 3 , . . . , 1 n). The customer is known to assign utility 1 to her most preferred item, 1/2 to the next best item, 1/3 to the third best, and so on, but the rank ordering is unknown.\nIn this example, no learning is required to offer an optimal assortment: since there is no constraint on the size of the assortment, it\u2019s always best to offer the full collection of products a = {1, . . . , n} and allow the customer to choose the most preferred. Offering this assortment reveals which item is most preferred by the customer, but it reveals nothing about her preferences about others. When applied to this problem, \u03b8-IDS begins by offering the full assortment A1 = {1, . . . , n}, which yields a reward of 1, and, by revealing the top item, yields information of I1(Y1,A1 ; \u03b8) = log(n). But, if 1/2 < \u03bb log(n \u2212 1), which is guaranteed for sufficiently large n, it continues experimenting with suboptimal assortments. In the second period, it will offer the assortment A2 consisting of all products except arg maxi \u03b8i. Playing this assortment reveals the customer\u2019s second most preferred item, and yields information gain I2(Y2,A2 ; \u03b8) = log(n \u2212 1). This process continues until the first period k where \u03bb log(n+ 1\u2212 k) < 1\u2212 1/k.\nIn order to learn to offer effective assortments, \u03b8-IDS tries to learn as much as possible about the customer\u2019s preferences. In doing this, the algorithm inadvertently invests experimentation effort in information that is irrelevant to choosing an optimal assortment. On the other hand, IDS recognizes that the optimal assortment A\u2217 = {1, . . . , n} does not depend on full knowledge of the vector \u03b8, and therefore does not invest in identifying \u03b8.\nAs shown in Section 9.2, our analysis can be adapted to provide regret bounds in for a version of IDS that uses information gain with respect to \u03b8, rather than with respect to A\u2217. These regret bounds depends on the entropy of \u03b8, whereas the bound for IDS depends on the entropy of A\u2217, which can be much smaller."}, {"heading": "4.3.3 Expected improvement and the knowledge gradient", "text": "We now consider two algorithms which measure the quality of the best decision that can be made based on current information, and encourage gathering observations that are expected to immediately increase this measure. The first is the expected improvement algorithm, which is one of the most widely used techniques in the active field of Bayesian optimization (see [13]). Define \u00b5t,a = E[Rt,a|Ft] to be the expected reward generated by a under the posterior, and Vt = maxa\u2032 \u00b5t,a\u2032 to be the best objective value attainable given current information. The expected improvement of action a is defined to be E[max{f\u03b8(a), Vt}|Ft], where f\u03b8(a) = E[Rt,a|\u03b8] is the expected reward generated by action a under the unknown true parameter \u03b8. The EGO algorithm aims to identify high performing actions by sequentially sampling those that yield the highest expected improvement. Similar to UCB algorithms, this encourages the selection of actions that could potentially offer great performance. Unfortunately, like these UCB algorithms, this measure of improvement does not place value on indirect information: it won\u2019t select an action that provides useful feedback about other actions unless the mean-reward of that action might exceed Vt. For example, the expected improvement algorithm cannot treat the problem described in Example 2 in a satisfactory manner.\nThe knowledge gradient algorithm [64] uses a modified improvement measure. At time t, it computes vKGt,a := E [Vt+1|Ft, At = a]\u2212 Vt for each action a. If Vt measures the quality of the decision that can be made based on current information, then vKGt,a captures the immediate improvement in decision quality due to sampling action a and observing Yt,a. For a problem with time horizon T , the knowledge gradient (KG) policy selects an action in time period t by maximizing \u00b5t,a + (T \u2212 t)vKGt,a over actions a \u2208 A.\nUnlike expected-improvement, the measure vKGt,a of the value of sampling an action places value on indirect information. In particular, even if an action is known to yield low expected reward,\nsampling that action could lead to a significant increase in Vt by providing information about other actions. Unfortunately, there are no general guarantees for KG, and it sometimes struggles with cumulating information; individual observations that provide information about A\u2217 may not be immediately useful for making decisions in the next period, and therefore may lead to no improvement in Vt.\nExample 1 provides one simple illustration of this phenomenon. In that example, action 1 is known to yield a mean reward of 1/2. When p0 \u2264 1/4, upon sampling action 2 and observing a reward 1, the posterior expected reward of action 2 becomes\nE[R2,a2 |R1,a2 = 1] = p0(3/4)\np0(3/4) + (1\u2212 p0)(1/4) \u2264 1/2.\nIn particular, a single sample could never be influential enough to change which action has the highest posterior expected reward. Therefore, vKGt,a2 = 0, and the KG decision rule selects action 1 in the first period. Since nothing is learned from the resulting observation, it will continue selecting action 1 in all subsequent periods. Even as the time horizon T tends to infinity, the KG policy would never select action 2. Its cumulative regret over T time periods is equal to (p0/4)T , which grows linearly with T .\nIn this example, although sampling action 2 will not immediately shift the decision-maker\u2019s prediction of the best action (arg maxa E[\u03b8a|F1]), these samples influences her posterior beliefs and reduce uncertainty about which action is optimal. As a result, IDS will always assign positive probability to sampling the second action. More broadly, IDS places value on information that is pertinent to the decision problem, even if that information won\u2019t directly improve performance on its own. This is useful when one must combine multiple pieces of information in order to effectively learn.\nTo address problems like Example 1, Frazier and Powell [28] propose KG* \u2013 a modified form of KG that considers the value of sampling a single action many times. This helps to address some cases \u2013 those where a single sample of an action provides no value even though sampling the action several times could be quite valuable \u2013 but this modification may not adequately assess information gain in more general problems. KG* may explore very inefficiently, for instance, in the sparse linear bandit problem considered in Example 3. However, the performance of KG* on this problem depends critically on how ties are broken among actions maximizing \u00b5t,a+ (T \u2212 t)vKGt,a . To avoid such ambiguity, we instead consider the following modification to Example 3.\nExample 6. (sparse linear model with an outside option) Consider a modification to Example 3 in which the agent has access to a outside option that generates known, but suboptimal, rewards. The action set is A = {O}\u222aA\u2032, where the outside option O is know to always yield reward 1/2, and A\u2032 = { x \u2016x\u20161 : x \u2208 {0, 1} d, x 6= 0 } is the action space considered in Example 3. As before, the reward generated by action a \u2208 A\u2032 is aT \u03b8 where \u03b8 is drawn uniformly at random from the set of 1-sparse vectors \u0398 = {\u03b8\u2032 \u2208 {0, 1}d : \u2016\u03b8\u2032\u20160 = 1}. As before, assume for simplicity that d = 2m for some m \u2208 N.\nWhen the horizon T is long, the inclusion of the outside option should be irrelevant. Indeed, nothing is learned by sampling the outside option, and it is known apriori to be suboptimal: it generates a reward of 1/2 whereas the optimal action generates a reward of 1 = maxa\u2208A\u2032 aT \u03b8. An optimal algorithm for Example 6 should sample actions in A\u2032 until the optimal action is identified and should do so in a manner that minimizes the expected regret incurred.\nNote that in the absence of observation noise, KG and KG* are equivalent, and so we will not distinguish between these two algorithms. We will see that when T is large, KG always samples\nactions in A\u2032 until the optimal action is identified. However, the expected number of samples required, and the expected regret incurred, both scale linearly with the dimension d. IDS, on the other hand, identifies the optimal action after only log2(d) steps.\nTo see why, note that unless the agent has exactly identified the parameter \u03b8, selecting the outside option will always generate the highest expected reward in the next period. Only selecting an action with a single positive component could immediately reveal \u03b8, so in response KG only places value on the information generated by such actions. When the horizon is large, KG prioritizes such information over the safe reward offered by the outside option. It therefore engages in exhaustive search, sequentially checking each component i \u2208 {1, . . . , d} of \u03b8 until \u03b8i = 1 is observed.\nTo show this more precisely, let us reintroduce some notation used in Example 3. As before, when an action a \u2208 A\u2032 is selected and y = aT \u03b8 \u2208 {0, 1/\u2016a\u20160} is observed, each \u03b8\u2032 \u2208 \u0398 with aT \u03b8\u2032 6= y is ruled out. Let \u0398t denote the set of all parameters in \u0398 that are consistent with the rewards observed up to time t and let It = {i \u2208 {1, . . . , d} : \u03b8\u2032i = 1, \u03b8\u2032 \u2208 \u0398t} denote the corresponding set of possible positive components.\nThe best reward value attainable given current information is\nVt = max {\n1/2, max a\u2208A\u2032\nE[\u03b8Ta|Ft] } = max {\n1/2, 1 |\u0398t|\n} .\nTherefore, Vt = 1/2 whenever |\u0398t| > 1. For remaining indices i \u2208 It, selecting the standard basis vector ei and observing \u03b8T ei = 1 establishes that \u03b8 = ei. As a result, vKGt,ei = (1/2)P(e T i \u03b8 = 1|Ft) =\n1 2|\u0398t| > 0. For other actions, observing a T \u03b8 \u2208 {0, 1/\u2016a\u20160} is not sufficient to identify \u03b8, and so vKGt,a = 0. When the horizon T is large, KG always select one of the actions with vKGt,a > 0, and so the algorithm selects only standard basis vectors until \u03b8 is revealed\nIDS, on the other hand, essentially performs binary search. In the first period, it selects some permutation of the action a = (2/d, ..., 2/d, 0, ..., 0). By observing either aT \u03b8 = 0 or aT \u03b8 = 1, it rules out half the parameter vectors in \u03981. Continuing this binary search process, the parameter is identified using O(log2(d)) steps.\nTo see why this is the case, let us focus on the first period. As in Example 3, selecting an action with d/2 positive components, like a = (2/d, ..., 2/d, 0, ..., 0), offers strictly maximal information gain I1(A\u2217; aT \u03b8) = log(2) and weakly maximal reward E[aT \u03b8] = 1/d among all actions a \u2208 A\u2032. As a result, any action in A\u2032 that is not a permutation of a is strictly dominated and is never sampled by IDS. It turns out that IDS also has zero probability of selecting the outside option in this case. Indeed, IDS selects O with probability 1 \u2212 \u03b1\u2217, where \u03b1\u2217 can be attained as the minimizer of the information ratio\n\u03b1\u2217 = arg min \u03b1\u2208[0,1]\n((1\u2212 \u03b1)/2 + \u03b1(1\u2212 1/d))2\n\u03b1 log2(d) = arg min \u03b1\u2208[0,1] 1 2 \u221a \u03b1 + \u221a \u03b1 (1 2 \u2212 1 d ) = 1.\nThis process continues inductively. Another step of binary search again rules out half the components in |\u03981|, leading to an information gain of log(2) bits. The regret incurred is even lower\u2013now only (1\u2212 2/d) \u2013and hence the algorithm again has zero probability of selecting the outside option. Iterating this process, the true positive component of \u03b8 is identified after log2(d) steps."}, {"heading": "5 Regret bounds", "text": "This section establishes regret bounds for information-directed sampling for several of the most widely studied classes of online optimization problems. These regret bounds follow from our recent information theoretic-analysis of Thompson sampling [61]. In the next subsection, we establish a\nregret bound for any policy in terms of its information ratio. Because the information-ratio of IDS is always smaller than that of TS, the bounds on the information ratio of TS provided in Russo and Van Roy [61] immediately yield regret bounds for IDS for a number of important problem classes."}, {"heading": "5.1 General bound", "text": "We begin with a general result that bounds the regret of any policy in terms of its information ratio and the entropy of the optimal action distribution. Recall that we have defined the information ratio of an action sampling distribution to be \u03a8t(\u03c0) := \u2206t(\u03c0)2/gt(\u03c0); it is the squared expected regret the algorithm incurs per-bit of information it acquires about the optimum. The entropy of the optimal action distribution H(\u03b11) captures the magnitude of the decision-maker\u2019s initial uncertainty about which action is optimal. One can then interpret the next result as a bound on regret that depends on the cost of acquiring new information and the total amount of information that needs to be acquired.\nProposition 1. For any policy \u03c0 = (\u03c01, \u03c02, \u03c03, . . .) and time T \u2208 N,\nE [Regret (T, \u03c0)] \u2264 \u221a \u03a8T (\u03c0)H(\u03b11)T .\nwhere\n\u03a8T (\u03c0) \u2261 1 T T\u2211 t=1 E\u03c0[\u03a8t(\u03c0t)]\nis the average expected information ratio under \u03c0.\nWe will use the following immediate corollary of Proposition 1, which relies on a uniform bound on the information ratio of the form \u03a8t(\u03c0t) \u2264 \u03bb rather than a bound on the average expected information ratio.\nCorollary 1. Fix a deterministic \u03bb \u2208 R and a policy \u03c0 = (\u03c01, \u03c02, . . .) such that \u03a8t(\u03c0t) \u2264 \u03bb almost surely for each t \u2208 {1, .., T}. Then,\nE [Regret (T, \u03c0)] \u2264 \u221a \u03bbH(\u03b11)T ."}, {"heading": "5.2 Specialized bounds on the minimal information ratio", "text": "We now establish upper bounds on the information ratio of IDS in several important settings, which yields explicit regret bounds when combined with Corollary 1. These bounds show that, in any period, the algorithm\u2019s expected regret can only be large if it is expected to acquire a lot of information about which action is optimal. In this sense, it effectively balances between exploration and exploitation in every period. For each problem setting, we will compare our upper bounds on expected regret with known lower bounds.\nThe bounds on the information ratio also help to clarify the role it plays in our results: it roughly captures the extent to which sampling some actions allows the decision maker to make inferences about other actions. In the worst case, the ratio depends on the number of actions, reflecting the fact that actions could provide no information about others. For problems with full information, the information ratio is bounded by a numerical constant, reflecting that sampling one action perfectly reveals the rewards that would have been earned by selecting any other action. The problems of online linear optimization under \u201cbandit feedback\u201d and under \u201csemi-bandit feedback\u201d lie between these two extremes, and the ratio provides a natural measure of each problem\u2019s\ninformation structure. In each case, our bounds reflect that IDS is able to automatically exploit this structure.\nThe proofs of these bounds follow from our recent analysis of Thompson sampling, and the implied regret bounds are the same as those established for Thompson sampling. In particular, since \u03a8t(\u03c0IDSt ) \u2264 \u03a8t(\u03c0TSt ) where \u03c0TS is the Thompson sampling policy, it is enough to bound \u03a8t(\u03c0TSt ). Several bounds on the information-ratio of TS were provided by Russo and Van Roy [61], and we defer to that paper for the proofs. While the analysis is similar in the cases considered here, IDS outperforms Thompson sampling in simulation, and, as we highlighted in the previous section, is sometimes provably much more informationally efficient.\nIn addition to the bounds stated here, recent work by Bubeck et al. [18] and Bubeck and Eldan [16] bounds the information ratio when the reward function is convex, and uses this to study the order of regret in adversarial bandit convex optimization. This points to a broader potential of using information-ratio analysis to study the information-complexity of general online optimization problems.\nTo simplify the exposition, our results are stated under the assumption that rewards are uniformly bounded. This effectively controls the worst-case variance of the reward distribution, and as shown in the appendix of Russo and Van Roy [61], our results can be extended to the case where reward distributions are sub-Gaussian.\nAssumption 1. sup y\u2208Y R(y)\u2212 inf y\u2208Y R(y) \u2264 1."}, {"heading": "5.2.1 Worst case bound", "text": "The next proposition shows that \u03a8t(\u03c0IDSt ) is never larger than |A|/2. That is, there is always an action sampling distribution \u03c0 \u2208 D(A) such that \u2206t(\u03c0)2 \u2264 (|A|/2)gt(\u03c0). As we will show in the coming sections, the ratio between regret and information gain can be much smaller under specific information structures.\nProposition 2. For any t \u2208 N, \u03a8t(\u03c0IDSt ) \u2264 |A|/2 almost surely.\nCombining Proposition 2 with Corollary 1 shows that E [ Regret ( T, \u03c0IDS )] \u2264 \u221a\n1 2 |A|H(\u03b11)T ."}, {"heading": "5.2.2 Full information", "text": "Our focus in this paper is on problems with partial feedback. For such problems, what the decision maker observes depends on the actions selected, which leads to a tension between exploration and exploitation. Problems with full information arise as an extreme point of our formulation where the outcome Yt,a is perfectly revealed by observing Yt,a\u0303 for some a\u0303 6= a; what is learned does not depend on the selected action. The next proposition shows that under full information, the minimal information ratio is bounded by 1/2.\nProposition 3. Suppose for each t \u2208 N there is a random variable Zt : \u2126\u2192 Z such that for each a \u2208 A, Yt,a = (a, Zt). Then for all t \u2208 N, \u03a8t(\u03c0IDSt ) \u2264 12 almost surely.\nCombining this result with Corollary 1 shows E [ Regret(T, \u03c0IDS) ] \u2264 \u221a\n1 2H(\u03b11)T . Further, a worst\u2013case bound on the entropy of \u03b11 shows that E [ Regret(T, \u03c0IDS) ] \u2264 \u221a 1 2 log(|A|)T . Dani et al. [23] show this bound is order optimal, in the sense that for any time horizon T and number of actions |A| there exists a prior distribution over \u03b8 under which inf\u03c0 E [Regret(T, \u03c0)] \u2265 c0 \u221a log(|A|)T where c0 is a numerical constant that does not depend on |A| or T . The bound here improves upon this\nworst case bound since H(\u03b11) can be much smaller than log(|A|) when the prior distribution is informative."}, {"heading": "5.2.3 Linear optimization under bandit feedback", "text": "The stochastic linear bandit problem has been widely studied (e.g. [1, 24, 56]) and is one of the most important examples of a multi-armed bandit problem with \u201ccorrelated arms.\u201d In this setting, each action is associated with a finite dimensional feature vector, and the mean reward generated by an action is the inner product between its known feature vector and some unknown parameter vector. Because of this structure, observations from taking one action allow the decision\u2013maker to make inferences about other actions. The next proposition bounds the minimal information ratio for such problems.\nProposition 4. If A \u2282 Rd, \u0398 \u2282 Rd, and E [Rt,a|\u03b8] = aT \u03b8 for each action a \u2208 A, then \u03a8t(\u03c0IDSt ) \u2264 d/2 almost surely for all t \u2208 N.\nThis result shows that E [ Regret(T, \u03c0IDS) ] \u2264 \u221a\n1 2H(\u03b11)dT \u2264 \u221a 1 2 log(|A|)dT for linear bandit\nproblems. Dani et al. [23] again show this bound is order optimal in the sense that, for any time horizon T and dimension d, when the action set is A = {0, 1}d there exists a prior distribution over \u03b8 such that inf\u03c0 E [Regret(T, \u03c0)] \u2265 c0 \u221a log(|A|)dT where c0 is a constant that is independent of d and T . The bound here improves upon this worst case bound since H(\u03b11) can be much smaller than log(|A|) when the prior distribution in informative."}, {"heading": "5.2.4 Combinatorial action sets and \u201csemi-bandit\u201d feedback", "text": "To motivate the information structure studied here, consider a simple resource allocation problem. There are d possible projects, but the decision\u2013maker can allocate resources to at most m \u2264 d of them at a time. At time t, project i \u2208 {1, .., d} yields a random reward Xt,i, and the reward from selecting a subset of projects a \u2208 A \u2282 {a\u2032 \u2282 {0, 1, . . . , d} : |a\u2032| \u2264 m} is m\u22121 \u2211 i\u2208AXt,i. In the linear bandit formulation of this problem, upon choosing a subset of projects a the agent would only observe the overall reward m\u22121 \u2211 i\u2208aXt,i. It may be natural instead to assume that the outcome of each selected project (Xt,i : i \u2208 a) is observed. This type of observation structure is sometimes called \u201csemi-bandit\u201d feedback [8].\nA naive application of Proposition 4 to address this problem would show \u03a8\u2217t \u2264 d/2. The next proposition shows that since the entire parameter vector (\u03b8t,i : i \u2208 a) is observed upon selecting action a, we can provide an improved bound on the information ratio.\nProposition 5. Suppose A \u2282 {a \u2282 {0, 1, . . . , d} : |a| \u2264 m}, and that there are random variables (Xt,i : t \u2208 N, i \u2208 {1, . . . , d}) such that\nYt,a = (Xt,i : i \u2208 a) and Rt,a = 1 m \u2211 i\u2208a Xt,i.\nAssume that the random variables {Xt,i : i \u2208 {1, . . . , d}} are independent conditioned on Ft and Xt,i \u2208 [\u221212 , 1 2 ] almost surely for each (t, i). Then for all t \u2208 N, \u03a8t(\u03c0 IDS t ) \u2264 d2m2 almost surely.\nIn this problem, there are as many as ( d m ) actions, but because IDS exploits the structure relating actions to one another, its regret is only polynomial in m and d. In particular, combining Proposition 5 with Corollary 1 shows E [ Regret(T, \u03c0IDS) ] \u2264 1m \u221a d 2H(\u03b11)T . SinceH(\u03b11) \u2264 log |A| =\nO(m log( dm)) this also yields a bound of order \u221a d m log ( d m ) T . As shown by Audibert et al. [8], the\nlower bound1 for this problem is of order \u221a\nd mT , so our bound is order optimal up to a \u221a log( dm)\nfactor."}, {"heading": "6 Computational methods", "text": "IDS offers an abstract design principle that captures some key qualitative properties of the Bayesoptimal solution while accommodating tractable computation for many relevant problem classes. However, additional work is required to design efficient computational methods that implement IDS for specific problem classes. In this section, we provide guidance and examples.\nWe will focus in this section on the problem of generating an action At given the posterior distribution over \u03b8 at time t. This sidesteps the problem of computing and representing a posterior distribution, which can present its own challenges. Though IDS could be combined with approximate Bayesian inference methods, we will focus here on the simpler context in which posterior distributions can be efficiently computed and stored, as is the case when working with tractable finite uncertainty sets or appropriately chosen conjugate priors. It is worth noting, however, that two of our algorithms approximate IDS using samples from the posterior distribution, and this may be feasible through the use of Markov chain Monte Carlo even in cases where the posterior distribution cannot be computed or even stored."}, {"heading": "6.1 Evaluating the information ratio", "text": "Given a finite action set A = {1, . . . ,K}, we can view an action distribution \u03c0 as a K-dimensional vector of probabilities. The information ratio can then be written as\n\u03a8t(\u03c0) =\n( \u03c0>~\u2206 )2 \u03c0>~g ,\nwhere ~\u2206 and ~g are K-dimensional vectors with components ~\u2206k = \u2206t(k) and ~gk = gt(k) for k \u2208 A. In this subsection, we discuss the computation of ~\u2206 and ~g for use in evaluation of the information ratio.\nThere is no general efficient procedure for computing ~\u2206 and ~g given a posterior distribution, because that would require computing integrals over possibly high-dimensional spaces. Such computation can often be carried out efficiently by leveraging the functional form of the specific posterior distribution and often require numerical integration. In order to illustrate the design of problemspecific computational procedures, we will present two simple examples in this subsection.\nWe begin with a conceptually simple model involving finite uncertainty sets.\nExample 7. (finite sets) Consider a problem in which \u03b8 takes values in \u0398 = {1, . . . , L}, the action set is A = {1, . . . ,K}, the observation set is Y = {1, . . . , N}, and the reward function\n1In their formulation, the reward from selecting action a is \u2211\ni\u2208a Xt,i, which is m times larger than in our formulation. The lower bound stated in their paper is therefore of order \u221a mdT . They don\u2019t provide a complete proof of their result, but note that it follows from standard lower bounds in the bandit literature. In the proof of Theorem 5 in that paper, they construct an example in which the decision maker plays m bandit games in parallel, each with d/m actions. Using that example, and the standard bandit lower bound (see Theorem 3.5 of Bubeck and Cesa-Bianchi [15]), the agent\u2019s regret from each component must be at least \u221a d m T , and hence her overall expected\nregret is lower bounded by a term of order m \u221a\nd m T =\n\u221a mdT ."}, {"heading": "R : Y 7\u2192 R is arbitrary. Let p1 be the prior probability mass function of \u03b8 and let q\u03b8,a(y) be the probability, conditioned on \u03b8, of observing y when action a is selected.", "text": "Note that the posterior probability mass function pt, conditioned on observations made prior to period t, can be computed recursively via Bayes\u2019 rule:\npt+1(\u03b8)\u2190 pt(\u03b8)q\u03b8,At(Yt,At)\u2211\n\u03b8\u2032\u2208\u0398 pt(\u03b8\u2032)q\u03b8\u2032,At(Yt,At) .\nGiven the posterior distribution pt along with the model parameters (L,K,N,R, q), Algorithm 1 computes ~\u2206 and ~g. Line 1 computes the optimal action for each value of \u03b8. Line 2 calculates the probability that each action is optimal. Line 3 computes the marginal distribution of Y1,a and line 4 computes the joint probability mass function of (A\u2217, Y1,a). Lines 5 and 6 use the aforementioned probabilities to compute ~\u2206 and ~g.\nAlgorithm 1 finiteIR(L,K,N,R, p, q) 1: \u0398a \u2190 {\u03b8|a = arg maxa\u2032 \u2211 y q\u03b8,a\u2032(y)R(y)} \u2200\u03b8\n2: p(a\u2217)\u2190 \u2211 \u03b8\u2208\u0398a\u2217 p(\u03b8) \u2200a \u2217\n3: pa(y)\u2190 \u2211 \u03b8 p(\u03b8)q\u03b8,a(y) \u2200a, y, \u03b8\n4: pa(a\u2217, y)\u2190 1p(a\u2217) \u2211 \u03b8\u2208\u0398a\u2217 q\u03b8,a(y) \u2200a, y, a \u2217\n5: R\u2217 \u2190 \u2211 a \u2211 \u03b8\u2208\u0398a \u2211 y p(\u03b8)q\u03b8,a(y)R(y)\n6: ~ga \u2190 \u2211 a\u2217,y pa(a\u2217, y) log pa(a\u2217,y) p(a\u2217)pa(y) \u2200a\n7: ~\u2206a \u2190 R\u2217 \u2212 \u2211 \u03b8 p(\u03b8) \u2211 y q\u03b8,a(y)R(y) \u2200a 8: return ~\u2206, ~g\nNext, we consider the beta-Bernoulli bandit.\nExample 8. (beta-Bernoulli bandit) Consider a multi-armed bandit problem with binary rewards: A = {1, . . . ,K}, Y = {0, 1}, and R(y) = y. Model parameters \u03b8 \u2208 RK specify the mean reward \u03b8a of each action a. Components of \u03b8 are independent and each beta-distributed with prior parameters \u03b211 , \u03b221 \u2208 RK+\nBecause the beta distribution is a conjugate prior for the Bernoulli distribution, the posterior distribution of each \u03b8a is a beta distribution. The posterior parameters \u03b21t,a, \u03b22t,a \u2208 R+ can be computed recursively:\n(\u03b21t+1,a, \u03b22t+1,a)\u2190 {\n(\u03b21t,a + Yt,a, \u03b22t,a + (1\u2212 Yt,a)) if At = a (\u03b21t,a, \u03b22t,a) otherwise.\nGiven the posterior parameters (\u03b21t , \u03b22t ), Algorithm 2 computes ~\u2206 and ~g. Line 5 of the algorithm computes the posterior probability mass function of A\u2217. It is easy to\nderive the expression used:\nPt(A\u2217 = a) = Pt  \u22c2 a\u2032 6=a {\u03b8a\u2032 \u2264 \u03b8a}  = \u02c6 1\n0 fa(x)Pt  \u22c2 a\u2032 6=a {\u03b8a\u2032 \u2264 x} \u2223\u2223\u2223\u2223\u03b8a = x  dx\n= \u02c6 1\n0 fa(x) \u220f a\u2032 6=a Fa\u2032(x)  dx = \u02c6 1\n0 [ fa(x) Fa(x) ] F (x)dx,\nwhere fa, Fa, and F are defined as in lines 1-3 of the algorithm, with arguments (K,\u03b21t , \u03b22t ). Using expressions that can be derived in a similar manner, for each pair of actions Lines 6-7 compute Ma\u2032|a := Et [\u03b8a\u2032 |\u03b8a = maxa\u2032\u2032 \u03b8a\u2032\u2032 ], the expected value of \u03b8a\u2032 given that action a is optimal. Lines 8-9 computes the expected reward of the optimal action \u03c1\u2217 = Et [maxa \u03b8a] and uses that to compute, for each action,\n~\u2206a = Et [ max a\u2032 \u03b8a \u2212 \u03b8a ] = \u03c1\u2217 \u2212 \u03b21t,a (\u03b21t,a + \u03b22t,a) .\nFinally, line 10 computes ~g. The expression makes use of the following fact, which is a consequence of standard properties of mutual information2:\nIt(A\u2217;Yt,a) = \u2211 a\u2217\u2208A Pt(A\u2217 = a\u2217)DKL (Pt(Yt,a = \u00b7|A\u2217 = a\u2217) ||Pt(Yt,a = \u00b7)) . (7)\nThat is, the mutual information between A\u2217 and Yt,a is the expected Kullback-Leibler divergence between the posterior predictive distribution Pt(Yt,a = \u00b7) and the predictive distribution conditioned on the identity of the optimal action Pt(Yt,a = \u00b7|A\u2217 = a\u2217). For our beta-Bernoulli model, the information gain ~ga is the expected Kullback-Leibler divergence between a Bernoulli distribution with mean Ma|A\u2217 and the posterior distribution at action a, which is Bernoulli with parameter \u03b21t,a/(\u03b21t,a + \u03b22t,a).\nAlgorithm 2, as we have presented it, is somewhat abstract and can not readily be implemented on a computer. In particular, lines 1-4 require computing and storing functions of a continuous variable and several lines require integration of continuous functions. However, near-exact approximations can be efficiently generated by evaluating integrands at discrete grid of points {x1, . . . , xn} \u2282 [0, 1]. The values of fa(x), Fa(x), Ga(x) and F (x) can be computed and stored for each value in this grid. The compute time can also be reduced via memoization, since values change only for one action per time period. The compute time of such an implementation scales with K2n where K is the number of actions and n is the number of points used in the discretization of [0, 1]. The bottleneck is Line 7.\n2Some details related to the derivation of this fact when Yt,a is a general random variable can be found in the appendix of Russo and Van Roy [61].\nAlgorithm 2 betaBernoulliIR(K,\u03b21, \u03b22) 1: fa(x)\u2190 beta.pdf(x|\u03b21a, \u03b22a) \u2200a, x 2: Fa(x)\u2190 beta.cdf(x|\u03b21a, \u03b22a) \u2200a, x 3: F (x)\u2190 \u220f a Fa(x) \u2200x\n4: Ga(x)\u2190 \u00b4 x\n0 yfa(y)dy \u2200a, x 5: p\u2217(a)\u2190 \u00b4 1 0 [ fa(x) Fa(x) ] F (x)dx \u2200a\n6: Ma|a \u2190 1p\u2217(a) \u00b4 1 0 [ xfa(x) Fa(x) ] F (x)dx \u2200a 7: Ma\u2032|a \u2190 1p\u2217(a) \u00b4 1 0 [ fa(x)F (x) Fa(x)Fa\u2032 (x) ] Ga\u2032(x)dx \u2200a, a\u2032 6= a\n8: \u03c1\u2217 \u2190 \u2211 a p \u2217(a)Ma|a 9: ~\u2206a \u2190 \u03c1\u2217 \u2212 \u03b2 1 a\n\u03b21a+\u03b22a \u2200a 10: ~ga \u2190 \u2211 a\u2032 p \u2217(a\u2032) ( Ma|a\u2032 log ( Ma|a\u2032(\u03b21a + \u03b22a)/\u03b21a ) + (1\u2212Ma|a\u2032) log ( (1\u2212Ma|a\u2032)(\u03b21a + \u03b22a)/\u03b22a )) \u2200a\n11: return ~\u2206, ~g"}, {"heading": "6.2 Optimizing the information ratio", "text": "Let us now discuss how to generate an action given ~\u2206 and ~g 6= 0. If ~g = 0, the optimal action is known with certainty, and therefore the action selection problem is trivial. Otherwise, IDS selects an action by solving\nmin \u03c0\u2208SK\n( \u03c0>~\u2206 )2 \u03c0>~g\n(8)\nwhere SK = {\u03c0 \u2208 RK+ : \u2211 k \u03c0k = 1} is the K-dimensional unit simplex, and samples from the resulting distribution \u03c0. The following result establishes that (8) is a convex optimization problem and, surprisingly, has an optimal solution with at most two non-zero components. Therefore, while IDS is a randomized policy, it suffices to randomize over two actions.\nProposition 6. For all ~\u2206, ~g \u2208 RK+ such that ~g 6= 0, the function \u03c0 7\u2192 ( \u03c0>~\u2206 )2 /\u03c0>~g is convex\non { \u03c0 \u2208 RK : \u03c0>~g > 0 } . Moreover, this function is minimized over SK by some \u03c0\u2217 for which |{k : \u03c0\u2217k > 0}| \u2264 2.\nAlgorithm 3 leverages Proposition 6 to efficiently choose an action in a manner that minimizes (6). The algorithm takes as input ~\u2206 \u2208 RK+ and ~g \u2208 RK+ , which provide the expected regret and information gain of each action. The sampling distribution that minimizes (6) is computed by iterating over all pairs of actions (a, a\u2032) \u2208 A \u00d7 A, and for each, computing the probability q that minimizes the information ratio among distributions that sample a with probability q and a\u2032 with probability 1 \u2212 q. This one-dimensional optimization problem requires little computation since the objective is convex; q can be computed by solving for the first-order necessary condition or approximated by a bisection method. The compute time of this algorithm scales with K2.\nAlgorithm 3 IDSAction(K, ~\u2206, ~g) 1: qa,a\u2032 \u2190 arg minq\u2032\u2208[0,1] [ q\u2032~\u2206a + (1\u2212 q\u2032)~\u2206a\u2032 ]2 / [q\u2032~ga + (1\u2212 q\u2032)~ga\u2032 ] \u2200a < K, a\u2032 > a\n2: (a\u2217, a\u2217\u2217)\u2190 arg mina<K,a\u2032>a [ qa,a\u2032 ~\u2206a + (1\u2212 qa,a\u2032)~\u2206a\u2032 ]2 / [ qa,a\u2032~ga + (1\u2212 qa,a\u2032)~ga\u2032 ] 3: Sample b \u223c Bernoulli(qa\u2217,a\u2217\u2217) 4: return ba\u2217 + (1\u2212 b)a\u2217\u2217"}, {"heading": "6.3 Approximating the information ratio", "text": "Though reasonably efficient algorithms can be devised to implement IDS for various problem classes, some applications, such as those arising in high-throughput web services, call for extremely fast computation. As such, it is worth considering approximations to the information ratio that retain salient features while enabling faster computation. In this section, we discuss some useful approximation concepts.\nThe dominant source of complexity in computing ~\u2206 and ~g is in the calculation of requisite integrals, which can require integration over high-dimensional spaces. One approach to addressing this challenge is to replace integrals with sample-based estimates. Algorithm 4 does this. In addition to the number of actions K and routines for evaluation q and R, the algorithm takes as input M representative samples of \u03b8. In the simplest use scenario, these would be independent samples drawn from the posterior distribution. The steps correspond to those of Algorithm 1, but with the set of possible models approximated by the set of representative samples. For many problems, even when exact computation of ~\u2206 and ~g is intractable due to required integration over high-dimensional spaces, Algorithm 1 can generate close approximations from a moderate number of samples M .\nAlgorithm 4 SampleIR(K, q,R,M, \u03b81, . . . , \u03b8M )\n1: \u0398\u0302a \u2190 {m|a = arg maxa\u2032 \u2211 y q\u03b8m,a\u2032(y)R(y)} 2: p\u0302(a\u2217)\u2190 |\u0398\u0302a\u2217 |/M \u2200a\u2217 3: p\u0302a(y)\u2190 \u2211 m qa,\u03b8m(y)/M \u2200y\n4: p\u0302a(a\u2217, y)\u2190 \u2211 m\u2208\u0398a qa,\u03b8m(y)/M \u2200a \u2217, y\n5: R\u0302\u2217 \u2190 \u2211 a,y p\u0302a(a, y)R(y)\n6: ~ga \u2190 \u2211 a\u2217,y p\u0302a(a\u2217, y) log p\u0302a(a\u2217,y) p\u0302(a\u2217)p\u0302a(y) \u2200a\n7: ~\u2206a \u2190 R\u2217 \u2212M\u22121 \u2211 m \u2211 y q\u03b8m,a(y)R(y) \u2200a 8: return ~\u2206, ~g\nThe information ratio is designed to effectively address indirect information, cumulating information, and irrelevant information, for a very broad class of learning problems. It can sometimes be helpful to replace the information ratio with alternative information measures that adequately address these issues for more specialized classes of problems. As an example, we will introduce the variance-based information ratio, which is suitable for some problems with bandit feedback, satisfies our regret bounds for such problems, and can facilitate design of more efficient numerical methods.\nTo motivate the variance-based information ratio, note that when rewards our bounded, with\nR(y) \u2208 [0, 1] for all y, our information measure term is lower-bounded according to\ngt(a) = It(A\u2217;Yt,a) = \u2211 a\u2217\u2208A Pt(A\u2217 = a\u2217)DKL (Pt(Yt,a = \u00b7|A\u2217 = a\u2217) ||Pt(Yt,a = \u00b7))\n\u2265 \u2211 a\u2217\u2208A Pt(A\u2217 = a\u2217)DKL (Pt(Rt,a = \u00b7|A\u2217 = a\u2217) ||Pt(Rt,a = \u00b7))\n(a) \u2265 2 \u2211 a\u2217\u2208A Pt(A\u2217 = a\u2217)(Et[Rt,a|A\u2217 = a\u2217]\u2212 Et[Rt,a])2 = 2Et[(Et[Rt,a|A\u2217]\u2212 Et[Rt,a])2] = 2Vart(Et[Rt,a|A\u2217]),\nwhere Vart(X) = Et[(X \u2212 Et[X])2] denotes the variance of X under the posterior distribution. Inequality (a) is a simple corollary of Pinsker\u2019s inequality, and is given as Fact 9 in Russo and Van Roy [61]. Let vt(a) := Vart(Et[Rt,a|A\u2217]), which represents the variance of the conditional expectation Et[Rt,a|A\u2217] under the posterior distribution. This measures how much the expected reward generated by action a varies depending on the identity of the optimal action A\u2217. The above lower bound on mutual information indicates that actions with high variance vt(a) must yield substantial information about which action is optimal. It is natural to consider an approximation to IDS that uses a variance-based information ratio:\nmin \u03c0\u2208SK\n( \u03c0>~\u2206 )2 \u03c0>~v ,\nwhere ~va = vt(a). While variance-based IDS will not minimize the information ratio, the next proposition establishes that it satisfies the bounds on the information ratio given by Propositions 2 and 4.\nProposition 7. Suppose supy R(y)\u2212 infy R(y) \u2264 1 and\n\u03c0t \u2208 arg min \u03c0\u2208SK\n\u2206t(\u03c0)2\nvt(\u03c0) .\nThen \u03a8t(\u03c0t) \u2264 |A|/2. Moreover, if A \u2282 Rd, \u0398 \u2282 Rd, and E [Rt,a|\u03b8] = aT \u03b8 for each action a \u2208 A, then \u03a8t(\u03c0t) \u2264 d/2.\nWe now consider a couple examples that illustrate computation of ~v and benefits of using this approximation. Our first example is the independent Gaussian bandit problem.\nExample 9. (independent Gaussian bandit) Consider a multi-armed bandit problem with A = {1, . . . ,K}, Y = R, and R(y) = y. Model parameters \u03b8 \u2208 RK specify the mean reward \u03b8a of each action a. Components of \u03b8 are independent and Gaussian-distributed, with prior means \u00b51 \u2208 RK and covariances \u03c321 \u2208 RK . When an action At is applied, the observation Yt is drawn independently from N(\u03b8At , \u03b72).\nThe posterior distribution of \u03b8 is Gaussian, with independent components. Parameters can be\ncomputed recursively according to\n\u00b5t+1,a \u2190  ( \u00b5t,a \u03c32t,a + Yt,a \u03b72 ) / ( 1 \u03c32t,a + 1 \u03b72 ) if At = a\n\u00b5t,a otherwise.\n\u03c3t+1,a \u2190  ( 1 \u03c32t,a + 1 \u03b72 )\u22121 if At = a\n\u03c3t,a otherwise.\nGiven arguments (K,\u00b5t, \u03c3t), Algorithm 5 computes ~\u2206 and ~v for the independent Gaussian bandit problem. Note that this algorithm is very similar to Algorithm 2, which was designed for the beta-Bernoulli bandit. One difference is that Algorithm 5 computes the variance-based information measure. In addition, the Gaussian distribution exhibits special structure that simplifies the computation of Ma\u2032|a := Et [\u03b8a\u2032 |\u03b8a = maxa\u2032\u2032 \u03b8a\u2032\u2032 ] . In particular, the computation of Ma\u2032|a uses the following closed form expression for the expected value of a truncated Gaussian distribution with mean \u00b5\u0303 and variance \u03c3\u03032:\nE [X|X \u2264 x] = \u00b5\u0303\u2212 \u03c3\u0303\u03c6 ( x\u2212 \u00b5\u0303 \u03c3\u0303 ) /\u03a6 ( x\u2212 \u00b5\u0303 \u03c3\u0303 ) = \u00b5\u0303\u2212 \u03c3\u03032f(x)/F (x),\nwhere X \u223c N(\u00b5\u0303, \u03c3\u03032) and f and F are the probability density and cumulative distribution functions. The analogous calculation that would be required to compute the standard information ratio is more complex.\nAlgorithm 5 independentGaussianVIR(K,\u00b5, \u03c3) 1: fa(x)\u2190 Gaussian.pdf(x|\u00b5a, \u03c32a) \u2200a, x 2: Fa(x)\u2190 Gaussian.cdf(x|\u00b5a, \u03c32a) \u2200a, x 3: F (x)\u2190 \u220f a Fa(x) \u2200x\n4: p\u2217(a)\u2190 \u00b4 1\n0 [ fa(x) Fa(x) ] F (x)dx \u2200a\n5: Ma|a \u2190 1p\u2217(a) \u00b4\u221e \u2212\u221e [ xfa(x) Fa(x) ] F (x)dx \u2200a 6: Ma\u2032|a \u2190 \u00b5a\u2032 \u2212 \u03c32 a\u2032 p\u2217(a) \u00b4\u221e \u2212\u221e [ fa(x)fa\u2032 (x) Fa(x)Fa\u2032 (x) ] F (x)dx \u2200a, a\u2032 6= a\n7: \u03c1\u2217 \u2190 \u2211 a p \u2217(a)Ma|a 8: \u2206a \u2190 \u03c1\u2217 \u2212 \u00b5a \u2200a 9: va \u2190 \u2211 a\u2032 p \u2217(a\u2032) ( Ma|a\u2032 \u2212 \u00b5a )2 \u2200a\n10: return ~\u2206, ~v\nWe next consider the linear bandit problem.\nExample 10. (linear bandit) Consider a multi-armed bandit problem with A = {1, . . . ,K}, Y = R, and R(y) = y. Model parameters \u03b8 \u2208 RK are drawn from a Gaussian prior with mean \u00b51 and covariance matrix \u03a31. There is a known matrix \u03a6 = [\u03a61, \u00b7 \u00b7 \u00b7 ,\u03a6K ] \u2208 Rd\u00d7K such that, when an action At is applied, the observation Yt,At is drawn independently from N(\u03a6At\u03b8, \u03b72), where \u03a6At denotes the Atth column of \u03a6.\nThe posterior distribution of \u03b8 is Gaussian and can be computed recursively:\n\u00b5t+1 = (\u03a3\u22121t + \u03a6At\u03a6>At/\u03b7 2)\u22121(\u03a3\u22121t \u00b5t + Yt,At\u03a6At/\u03b72)\n\u03a3t+1 = (\u03a3\u22121t + \u03a6At\u03a6>At/\u03b7 2)\u22121.\nWe will develop an algorithm that leverages the fact that, for the linear bandit, vt(a) takes on a particularly simple form:\nvt(a) = Vart(Et[Rt,a|A\u2217]) = Vart(Et[\u03a6>a \u03b8|A\u2217]) = Vart(\u03a6>a Et[\u03b8|A\u2217]) = \u03a6>a Et[(\u00b5A \u2217 t \u2212 \u00b5t)(\u00b5A \u2217 t \u2212 \u00b5t)>]\u03a6a\n= \u03a6>a Lt\u03a6a,\nwhere \u00b5at = Et[\u03b8|A\u2217 = a] and Lt = Et[(\u00b5A \u2217 t \u2212\u00b5t)(\u00b5A \u2217 t \u2212\u00b5t)>]. Algorithm 6 presents a sample-based approach to computing ~\u2206 and ~v. In addition to model dimensions K and d and the problem data matrix \u03a6, the algorithm takes as input M representative values of \u03b8, which in the simplest use scenario, would be independent samples drawn from the posterior distribution N(\u00b5t,\u03a3t). The algorithm approximates posterior means \u00b5t and \u00b5at as well as Lt by averaging suitable expressions over these samples. Due to the quadratic structure of vt(a), these calculations are substantially simpler than those that would be carried out by Algorithm 4, specialized to this context.\nAlgorithm 6 linearSampleVIR(K, d,M, \u03b81, . . . , \u03b8M ) 1: \u00b5\u0302\u2190 \u2211 m \u03b8\nm/M 2: \u0398\u0302a \u2190 {m : (\u03a6>\u03b8m)a = maxa\u2032(\u03a6\u03b8m)a\u2032} \u2200a 3: p\u0302\u2217(a)\u2190 |\u0398\u0302a|/M \u2200a 4: \u00b5\u0302a \u2190 \u2211 \u03b8\u2208\u0398\u0302a \u03b8/|\u0398\u0302a| \u2200a\n5: L\u0302\u2190 \u2211 a p\u0302 \u2217(a) (\u00b5\u0302a \u2212 \u00b5\u0302) (\u00b5\u0302a \u2212 \u00b5\u0302)>\n6: \u03c1\u2217 \u2190 \u2211 a p\u0302 \u2217(a)\u03a6>a \u00b5\u0302a 7: ~va \u2190 \u03a6>a L\u0302\u03a6>a \u2200a 8: ~\u2206a \u2190 \u03c1\u2217 \u2212 \u03a6>a \u00b5\u0302 \u2200a 9: return ~\u2206, ~v\nIt is interesting to note that Algorithms 4 and 6 do not rely on any special structure in the posterior distribution. Indeed, these algorithms should prove effective regardless of the form taken by the posterior. This points to a broader opportunity to use IDS or approximations to address complex models for which posteriors can not be efficiently computed or even stored, but for which it is possible to generate posterior samples via Markov chain Monte Carlo methods. We leave this as a future research opportunity."}, {"heading": "7 Computational results", "text": "This section presents computational results from experiments that evaluate the effectiveness of information-directed sampling in comparison to alternative algorithms. In Section 4.3, we showed that alternative approaches like UCB algorithms, Thompson sampling, and the knowledge gradient algorithm can perform very poorly when faced with complicated information structures and for this reason can be dramatically outperformed by IDS. In this section, we focus instead on simpler settings where current approaches are extremely effective. We find that even for these simple and widely studied settings, information-directed sampling displays state-of-the-art performance. For each experiment, the algorithm used to implement IDS is presented in the previous section.\nIDS, Thompson sampling (TS), and some UCB algorithms, do not take the horizon T as input, and are instead designed to work well for all sufficiently long horizons. Other algorithms we simulate were optimized for the particular horizon of the simulation trial. The KG and KG* algorithms in particular, treat the simulation horizon as known, and explore less aggressively in later periods. We have tried to clearly delineate which algorithms are optimized for simulation horizon. We believe one can also design variants of IDS, TS, and UCB algorithms that reduce exploration as the time remaining diminishes, but leave this for future work."}, {"heading": "7.1 Beta-Bernoulli bandit", "text": "Our first experiment involves a multi-armed bandit problem with independent arms and binary rewards. The mean reward of each arm is drawn from Beta(1, 1), which is the uniform distribution, and the means of separate arms are independent. Figure 1a and Table 1 present the results of 1000 independent trials of an experiment with 10 arms and a time horizon of 1000. We compared the performance of IDS to that of six other algorithms, and found that it had the lowest average regret of 18.0.\nThe UCB1 algorithm of Auer et al. [9] selects the action a which maximizes the upper confidence bound \u03b8\u0302t(a)+ \u221a 2 log(t)/Nt(a) where \u03b8\u0302t(a) is the empirical average reward from samples of action a and Nt(a) is the number of samples of action a up to time t. The average regret of this algorithm is 130.7, which is dramatically larger than that of IDS. For this reason UCB1 is omitted from Figure 1a.\nThe confidence bounds of UCB1 are constructed to facilitate theoretical analysis. For practical performance Auer et al. [9] proposed using an algorithm called UCB-Tuned. This algorithm selects the action a which maximizes the upper confidence bound \u03b8\u0302t(a) + \u221a min{1/4 , V t(a)} log(t)/Nt(a), where V t(a) is an upper bound on the variance of the reward distribution at action a. While this method dramatically outperforms UCB1, it is still outperformed by IDS. The MOSS algorithm of Audibert and Bubeck [7] is similar to UCB1 and UCB\u2013Tuned, but uses slightly different confidence bounds. It is known to satisfy regret bounds for this problem that are minimax optimal up to a numerical constant factor.\nIn previous numerical experiments [21, 43, 44, 66], Thompson sampling and Bayes UCB exhibited state-of-the-art performance for this problem. Each also satisfies strong theoretical guarantees, and is known to be asymptotically optimal in the sense defined by Lai and Robbins [49]. Unsurprisingly, they are the closest competitors to IDS. The Bayes UCB algorithm, studied in kaufmann et al. [43], constructs upper confidence bounds based on the quantiles of the posterior distribution: at time step t the upper confidence bound at an action is the 1\u2212 1t quantile of the posterior distribution of that action3.\nA somewhat different approach is the knowledge gradient (KG) policy of Powell and Ryzhov [55], which uses a one-step lookahead approximation to the value of information to guide experimentation. For reasons described in Section 4.3.3, KG does not explore sufficiently to identify the optimal arm in this problem, and therefore its regret grows linearly with time. Because KG explores very little, its realized regret is highly variable, as depicted in Table 1. In 200 out of the 2000 trials, the regret of KG was lower than .7, reflecting that the best arm was almost always chosen. In the worst 200 out of the 2000 trials, the regret of KG was larger than 159.\nKG is particularly poorly suited to problems with discrete observations and long time horizons. The KG* heuristic of Ryzhov et al. [63] offers much better performance in some of these problems.\n3Their theoretical guarantees require choosing a somewhat higher quantile, but the authors suggest choosing this quantile, and use it in their own numerical experiments.\nAt time t, KG* calculates the value of sampling an arm for M \u2208 {1, .., T \u2212 t} periods and choosing the arm with the highest posterior mean in subsequent periods. It selects an action by maximizing this quantity over all possible arms and possible exploration lengths M . Our simulations require computing T = 1, 000 decisions per trial, and a direct implementation of KG* requires order T 3 basic operations per decision. To enable efficient simulation, we use a heuristic approach to computing KG* proposed by Kami\u0144ski [42]. The approximate KG* algorithm we implement uses golden section search to maximize a non-concave function, but is still empirically effective.\nFinally, as demonstrated in Figure 1a, variance-based IDS offers performance very similar to standard IDS for this problem.\nIt is worth pointing out that, although Gittins\u2019 indices characterize the Bayes optimal policy for infinite horizon discounted problems, the finite horizon formulation considered here is computationally intractable [30]. A similar index policy [52] designed for finite horizon problems could be applied as a heuristic in this setting. However, with long time horizons, the associated computational requirements become onerous."}, {"heading": "7.2 Independent Gaussian bandit", "text": "Our second experiment treats a different multi-armed bandit problem with independent arms. The reward value at each action a follows a Gaussian distribution N(\u03b8a, 1). The mean \u03b8a \u223c N(0, 1) is drawn from a Gaussian prior, and the means of different reward distributions are drawn independently. We ran 2000 simulation trials of a problem with 10 arms. The results are displayed in Figure 1b and Table 2.\nFor this problem, we compare variance-based IDS against Thompson sampling, Bayes UCB, and KG. We use the variance-based variant of IDS because it affords us computational advantages.\nWe also simulated the GPUCB of Srinivas et al. [67]. This algorithm maximizes the upper confidence bound \u00b5t(a) + \u221a \u03b2t\u03c3t(a) where \u00b5t(a) and \u03c3t(a) are the posterior mean and standard deviation of \u03b8a. They provide regret bounds that hold with probability at least 1 \u2212 \u03b4 when \u03b2t = 2 log ( |A|t2\u03c02/6\u03b4 ) . This value of \u03b2t is far too large for practical performance, at least in this problem setting. The average regret of GPUCB4 is 157.6, which is roughly almost three times that of V-IDS. For this reason, we considered a tuned version of GPUCB that sets \u03b2t = c log(t). We ran 1000 trials of many different values of c to find the value c = .9 with the lowest average regret for this problem. This tuned version of GPUCB had average regret of 53.8, which is slight better than IDS.\nThe work on knowledge gradient (KG) focuses almost entirely on problems with Gaussian reward distributions and Gaussian priors. We find KG performs better in this experiment than it did in the Bernoulli setting, and its average regret is competitive with that of IDS.\nAs in the Bernoulli setting, KG\u2019s realized regret is highly variable. The median regret of KG is the lowest of any algorithm, but in 100 of the 2000 trials its regret exceeded 283 \u2013 seemingly reflecting that the algorithm did not explore enough to identify the best action. The KG* heuristic explores more aggressively, and performs very well in this experiment.\n4We set \u03b4 = 0 in the definition of \u03b2t, as this choice leads to a lower value of \u03b2t and stronger performance.\nKG is particularly effective over short time spans. Unlike information-directed sampling, KG takes the time horizon T as an input, and explores less aggressively when there are fewer time periods remaining. Table 3 compares the regret of KG and IDS over different time horizons. Even though IDS does not take the time horizon into account, it is competitive with KG, even over short horizons. We believe that IDS can be modified to exploit fixed and known time horizons more effectively, though we leave the matter for future research."}, {"heading": "7.3 Asymptotic optimality", "text": "The previous subsections present numerical examples in which IDS outperforms Bayes UCB and Thompson sampling for some problems with independent arms. This is surprising since each of these algorithms is known, in a sense we will soon formalize, to be asymptotically optimal for these problems. This section presents simulation results over a much longer time horizon that suggest IDS scales in the same asymptotically optimal way.\nWe consider again a problem with binary rewards and independent actions. The action ai \u2208 {a1, . . . , aK} yields in each time period a reward that is 1 with probability \u03b8i and 0 otherwise. The seminal work of Lai and Robbins [49] provides the following asymptotic lower bound on regret of any policy \u03c0:\nlim inf T\u2192\u221e E [Regret(T, \u03c0)|\u03b8] log T \u2265 \u2211 a6=A\u2217 \u03b8A\u2217 \u2212 \u03b8a DKL(\u03b8A\u2217 || \u03b8a) := c(\u03b8).\nNote that we have conditioned on the parameter vector \u03b8, indicating that this is a frequentist lower bound. Nevertheless, when applied with an independent uniform prior over \u03b8, both Bayes UCB and Thompson sampling are known to attain this lower bound [43, 44].\nOur next numerical experiment fixes a problem with three actions and with \u03b8 = (.3, .2, .1). We compare algorithms over a 10,000 time periods. Due to the expense of running this experiment, we were only able to execute 200 independent trials. Each algorithm uses a uniform prior over \u03b8. Our results, along with the asymptotic lower bound of c(\u03b8) log(T ), are presented in Figure 2."}, {"heading": "7.4 Linear bandit problems", "text": "Our final numerical experiment treats a linear bandit problem. Each action a \u2208 R5 is defined by a 5 dimensional feature vector. The reward of action a at time t is aT \u03b8 + t where \u03b8 \u223c N(0, 10I) is drawn from a multivariate Gaussian prior distribution, and t \u223c N(0, 1) is independent Gaussian noise. In each period, only the reward of the selected action is observed. In our experiment, the action set A contains 30 actions, each with features drawn uniformly at random from [\u22121/ \u221a 5, 1/ \u221a\n5]. The results displayed in Figure 3 and Table 5 compare regret across 2,000 independent trials.\nWe simulate variance-based IDS using the implementation presented in Algorithm 6. We\ncompare its regret to six competing algorithms. Like IDS, GP-UCB and Thompson sampling satisfy strong regret bounds for this problem5. Both algorithms are significantly outperformed by IDS.\nWe also include Bayes UCB [43] and a version of GP-UCB that was tuned, as in Subsection 7.2, to minimize its average regret. Each of these displays performance that is competitive with that of IDS. These algorithms are heuristics, in the sense that the way their confidence bounds are constructed differ significantly from those of linear UCB algorithms that are known to satisfy theoretical guarantees.\nAs discussed in Subsection 7.2, unlike IDS, KG takes the time horizon T as an input, and explores less aggressively when there are fewer time periods remaining. Table 5 compares IDS to KG over several different time horizons. Even though IDS does not exploit knowledge of the time horizon, it is competitive with KG over short time horizons.\nIn this experiment, KG* appears to offer a small improvement over standard KG, but as shown in the next subsection, it is much more computationally burdensome. To save computational resources, we have only executed 500 independent trails of the KG* algorithm.\n5Regret analysis of GP-UCB can be found in [67]. Regret bounds for Thompson sampling can be found in [6, 59, 61]"}, {"heading": "10 0.00298 0.000008 0.00002 0.00001 0.000146 0.001188", "text": ""}, {"heading": "30 0.012597 0.000005 0.000009 0.000005 0.000097 0.003157", "text": ""}, {"heading": "50 0.023084 0.000006 0.000009 0.000005 0.000094 0.005146", "text": ""}, {"heading": "70 0.03913 0.000006 0.000009 0.000005 0.000098 0.006364", "text": ""}, {"heading": "7.5 Runtime Comparison", "text": "We now compare the time required to compute decisions using the algorithms we have applied. In our experiments, Thompson sampling and UCB algorithms are extremely fast, sometimes requiring only a few microseconds to reach a decision. As expected, our implementation of IDS requires significantly more compute time. However, IDS often reaches a decision in only a small fraction of second, which is tolerable in many application areas. In addition, IDS may be accelerated considerably via parallel processing or an optimized implementation.\nThe results for KG are mixed. For independent Gaussian models, certain integrals can be computed via closed form expressions, allowing KG to execute quickly. There is also a specialized numerical procedure for implementing KG for correlated (or linear) Gaussian models, but computation is an order of magnitude slower than in the independent case. For correlated Gaussian models, the KG* policy is much slower than both KG and IDS. For beta-Bernoulli problems, KG can be computed very easily, but yields poor performance. A direct implementation of the KG* policy was too slow to simulate, and so we have used a heuristic approach presented in [42], which uses golden section search to maximize a function that is not necessarily unimodal. This method is labeled \u201cApprox KG*\u201d in Table 6.\nTable 6 displays results for the Bernoulli experiment described in Subsection 7.1. It shows the average time required to compute a decision in a 1000 period problem with 10, 30, 50 and 70 arms. IDS was implementing using Algorithm 2 to evaluate the information ratio, and Algorithm 3 to optimize it. The numerical integrals in Algorithm 2 were approximated using quadrature with 1000 equally spaced points. Table 7 presents results of the corresponding experiment in the Gaussian case. Finally, Table 8 displays results for the linear bandit experiments described in Subsection 7.4, which make use of Algorithm 6 and Markov chain Monte Carlo sampling withM = 10, 000 samples. The table provides the average time required to compute a decision in a 250 period problem."}, {"heading": "8 Conclusion", "text": "This paper has proposed information-directed sampling \u2013 a new algorithm for online optimization problems in which a decision maker must learn from partial feedback. We establish a general regret bound for the algorithm, and specialize this bound to several widely studied problem classes. We show that it sometimes greatly outperforms other popular approaches, which don\u2019t carefully"}, {"heading": "15 3 0.004305 0.000178 0.000139 0.000048 0.002709 0.311935", "text": ""}, {"heading": "30 5 0.008635 0.000064 0.000048 0.000038 0.004789 0.589998", "text": ""}, {"heading": "50 20 0.026222 0.000077 0.000083 0.000068 0.008356 1.051552", "text": ""}, {"heading": "100 30 0.079659 0.000115 0.000148 0.00013 0.017034 2.067123", "text": "measure the information provided by sampling actions. Finally, for some simple and widely studied classes of multi-armed bandit problems we demonstrate simulation performance surpassing popular approaches.\nMany important open questions remain, however. IDS solves a single-period optimization problem as a proxy to an intractable multi-period problem. Solution of this single-period problem can itself be computationally demanding, especially in cases where the number of actions is enormous or mutual information is difficult to evaluate. An important direction for future research concerns the development of computationally elegant procedures to implement IDS in important cases. Even when the algorithm cannot be directly implemented, however, one may hope to develop simple algorithms that capture its main benefits. Proposition 1 shows that any algorithm with small information ratio satisfies strong regret bounds. Thompson sampling is a simple algorithm that, we conjecture, sometimes has nearly minimal information ratio. Perhaps simple schemes with small information ratio could be developed for other important problem classes, like the sparse linear bandit problem.\nIn addition to computational considerations, a number of statistical questions remain open. One question raised is whether IDS attains the lower bound of Lai and Robbins [49] for some bandit problems with independent arms. Beyond the empirical evidence presented in Subsection 7.3, there are some theoretical reasons to conjecture this is true. Next, a more precise understanding of problem\u2019s information complexity remains an important open question for the field. Our regret bound depends on the problem\u2019s information complexity through a term we call the information ratio, but it\u2019s unclear if or when this is the right measure. Finally, it may be possible to derive lower bounds using the same information theoretic style of argument used in the derivation of our upper bounds."}, {"heading": "9 Extensions", "text": "This section presents a number of ways in which the results and ideas discussed throughout this paper can be extended. We will consider the use of algorithms like information-directed sampling for pure\u2013exploration problems, a form of information-directed sampling that aims to acquire information about \u03b8 instead of A\u2217, and a version of information directed-sampling that uses a tuning parameter to control how aggressively the algorithm explores. In each case, new theoretical guarantees can be easily established by leveraging our analysis of information-directed sampling."}, {"heading": "9.1 Pure exploration problems", "text": "Consider the problem of adaptively gathering observations ( A1, Y1,A1 , . . . , AT\u22121, YT\u22121,AT\u22121 ) so as to minimize the expected loss of the best decision at time T ,\nE [ min a\u2208A \u2206T (a) ] . (9)\nRecall that we have defined \u2206t(a) := E [Rt,A\u2217 \u2212Rt,a|Ft] to be the expected regret of action a at time t. This is a \u201cpure exploration problem,\u201d in the sense that one is interested only in the terminal regret (9) and not in the algorithm\u2019s cumulative regret. However, the next proposition shows that bounds on the algorithm\u2019s cumulative expected regret imply bounds on E [mina\u2208A\u2206T (a)].\nProposition 8. If actions are selected according to a policy \u03c0, then\nE [ min a\u2208A \u2206T (a) ] \u2264 E [Regret (T, \u03c0)] T .\nProof. By the tower property of conditional expectation, E [\u2206t+1(a)|Ft] = \u2206t(a). Therefore, Jensen\u2019s inequality shows E [mina\u2208A\u2206t+1(a)|Ft] \u2264 mina\u2208A\u2206t(a) \u2264 \u2206t(\u03c0t). Taking expectations and iterating this relation shows that\nE [ min a\u2208A \u2206T (a) ] \u2264 E [ min a\u2208A \u2206t(a) ] \u2264 E [\u2206t(\u03c0t)] \u2200t \u2208 {1, . . . , T}. (10)\nThe result follows by summing both sides of (10) over t \u2208 {1, . . . , T} and dividing each by T .\nInformation-directed sampling is designed to have low cumulative regret, and therefore balances between acquiring information and taking actions with low expected regret. For pure exploration problems, it\u2019s natural instead to consider an algorithm that always acquires as much information about A\u2217 as possible. The next proposition provides a theoretical guarantee for an algorithm of this form. The proof of this result combines our analysis of information-directed sampling with Proposition 8.\nProposition 9. If actions are selected so that\nAt \u2208 arg max a\u2208A gt(a),\nand \u03a8\u2217t \u2264 \u03bb almost surely for each t \u2208 {1, . . . , T}, then\nE [ min a\u2208A \u2206T (a) ] \u2264 \u221a \u03bbH(\u03b11) T .\nProof. To simplify notation, let \u2206\u2217t = mina\u2208A\u2206t(a) denote the minimal expected regret at time t, and g\u2217t = maxa\u2208A gt(a) denote the information gain under the current algorithm.\nSince \u2206t(\u03c0IDSt )2 \u2264 \u03bbgt(\u03c0IDSt ), it is immediate that \u2206\u2217t \u2264 \u221a \u03bbg\u2217t . Therefore\nE[\u2206\u2217T ] (a) \u2264 ( 1 T ) E T\u2211 t=1 \u2206\u2217t \u2264 (\u221a \u03bb T ) E T\u2211 t=1 \u221a g\u2217t (b) \u2264 (\u221a \u03bb T )\u221a\u221a\u221a\u221aTE T\u2211 t=1 g\u2217t (c) \u2264 \u221a \u03bbH(\u03b11) T .\nInequality (a) uses equation (10) in the proof of Proposition 8, (b) uses the Cauchy-Schwartz inequality, and (c) follows as in the proof of Proposition 1."}, {"heading": "9.2 Using information gain about \u03b8", "text": "Information-directed sampling optimizes a single-period objective that balances earning high immediate reward and acquiring information. Information is quantified using the mutual information between the true optimal action A\u2217 and the algorithm\u2019s next observation Yt,a. In this subsection, we will consider an algorithm that instead quantifies the amount learned through selecting\nan action a using the mutual information It (\u03b8;Yt,a) between the algorithm\u2019s next observation and the unknown parameter \u03b8. As highlighted in Subsection 4.3.2, such an algorithm could invest in acquiring information that is irrelevant to the decision problem. However, in some cases, such an algorithm can be computationally simple while offering reasonable statistically efficiency.\nWe introduce a modified form of the information ratio\n\u03a8\u03b8t (\u03c0) := \u2206t(\u03c0)2\u2211\na\u2208A \u03c0(a)It (\u03b8;Yt,a) (11)\nwhich replaces the expected information gain about A\u2217, gt(\u03c0) = \u2211 a\u2208A \u03c0(a)It (A\u2217;Yt,a), with the expected information gain about \u03b8.\nProposition 10. For any action sampling distribution \u03c0\u0303 \u2208 D(A),\n\u03a8\u03b8t (\u03c0\u0303) \u2264 \u03a8t(\u03c0\u0303). (12)\nFurthermore, if \u0398 is finite, and there is some \u03bb \u2208 R and policy \u03c0 = (\u03c01, \u03c02, . . .) satisfying \u03a8\u03b8t (\u03c0t) \u2264 \u03bb almost surely, then\nE [Regret(T, \u03c0)] \u2264 \u221a \u03bbH(\u03b8)T . (13)\nEquation (12) relies on the inequality It (A\u2217;Yt,a) \u2264 It (\u03b8;Yt,a), which itself follows from the data processing inequality of mutual information because A\u2217 is a function of \u03b8. The proof of the second part of the proposition is almost identical to the proof of Proposition 1, and is omitted.\nWe have provided several bounds on the information ratio of \u03c0IDS of the form \u03a8t(\u03c0IDSt ) \u2264 \u03bb. By this proposition, such bounds imply that if \u03c0 = (\u03c01, \u03c02, . . .) satisfies\n\u03c0t \u2208 arg min \u03c0\u2208D(A) \u03a8\u03b8t (\u03c0)\nthen, \u03a8\u03b8t (\u03c0t) \u2264 \u03a8\u03b8t (\u03c0IDSt ) \u2264 \u03a8t(\u03c0IDSt ) \u2264 \u03bb, and the regret bound (13) applies."}, {"heading": "9.3 A tunable version of information-directed sampling", "text": "In this section, we present an alternative form of information-directed sampling that depends on a tuning parameter \u03bb \u2208 R. As \u03bb varies, the algorithm strikes a different balance between exploration and exploration. The following proposition provides regret bounds for this algorithm provided \u03bb is sufficiently large.\nProposition 11. Fix any \u03bb \u2208 R such that \u03a8t(\u03c0IDSt ) \u2264 \u03bb almost surely for each t \u2208 {1, . . . , T}. If \u03c0 = (\u03c01, \u03c02, ..) is defined so that\n\u03c0t \u2208 arg min \u03c0\u2208D(A)\n{ \u03c1(\u03c0) := \u2206t(\u03c0)2 \u2212 \u03bbgt(\u03c0) } , (14)\nthen E [Regret(T, \u03c0)] \u2264 \u221a \u03bbH(\u03b1)T .\nProof. We have that\n\u03c1(\u03c0t) (a) \u2264 \u03c1(\u03c0IDSt ) (b) \u2264 0,\nwhere (a) follows since \u03c0IDSt is feasible for the optimization problem (14), and (b) follows since\n0 = \u2206t(\u03c0IDSt )2 \u2212\u03a8t(\u03c0IDSt )gt(\u03c0IDSt ) \u2265 \u2206t(\u03c0IDSt )2 \u2212 \u03bbgt(\u03c0IDSt ).\nSince \u03c1t(\u03c0t) \u2264 0, it must be the case that \u03bb \u2265 \u2206t(\u03c0t)2/gt(\u03c0t) Def= \u03a8t(\u03c0t). The result then follows by applying Proposition 1."}, {"heading": "Acknowledgements", "text": "We thank the anonymous referees for feedback and stimulating exchanges, Junyang Qian for correcting miscalculations in an earlier draft pertaining to Examples 3 and 4, and Eli Gutin and Yashodan Kanoria for helpful suggestions. This work was generously supported by a research grant from Boeing, a Marketing Research Award from Adobe, and the Burt and Deedee McMurty Stanford Graduate Fellowship."}, {"heading": "A Proof of Proposition 6", "text": "Proposition 6. For all ~\u2206, ~g \u2208 RK+ such that ~g 6= 0, the function \u03c0 7\u2192 ( \u03c0>~\u2206 )2 /\u03c0>~g is convex\non { \u03c0 \u2208 RK : \u03c0>~g > 0 } . Moreover, this function is minimized over SK by some \u03c0\u2217 for which |{k : \u03c0\u2217k > 0}| \u2264 2.\nProof. First, we show the function \u03a8 : \u03c0 7\u2192 ( \u03c0T\u2206 )2 /\u03c0T g is convex on { \u03c0 \u2208 RK |\u03c0T g > 0 } . As\nshown in Chapter 3 of Boyd and Vandenberghe [12], f : (x, y) 7\u2192 x2/y is convex over {(x, y) \u2208 R2 : y > 0}. The function h : \u03c0 7\u2192 (\u03c0T\u2206, \u03c0T g) \u2208 R2 is affine. Since convexity is preserved under composition with an affine function, the function \u03a8 = g \u25e6 h is convex.\nWe now prove the second claim. Consider the optimization problems\nminimize \u03a8(\u03c0) subject to \u03c0T e = 1, \u03c0 \u2265 0 (15) minimize \u03c1(\u03c0) subject to \u03c0T e = 1, \u03c0 \u2265 0 (16)\nwhere \u03c1(\u03c0) := ( \u03c0T\u2206 )2 \u2212 ( \u03c0T g ) \u03a8\u2217,\nand \u03a8\u2217 \u2208 R denotes the optimal objective value for the minimization problem (15). The set of optimal solutions to (15) and (16) correspond. Note that\n\u03a8(\u03c0) = \u03a8\u2217 =\u21d2 \u03c1(\u03c0) = 0\nbut for any feasible \u03c0, \u03c1(\u03c0) \u2265 0 since \u2206(\u03c0)2 \u2265 \u03a8\u2217g(\u03c0). Therefore, any optimal solution \u03c00 to (15) is an optimal solutions to (16) and satisfies \u03c1(\u03c00) = 0. Similarly, if \u03c1(\u03c0) = 0 then simple algebra shows that \u03a8(\u03c0) = \u03a8\u2217 and hence that \u03c0 is an optimal solution to (15)\nWe will now show that there is a minimizer of \u03c1(\u00b7) with at most two nonzero components, which implies the same is true of \u03a8(\u00b7). Fix a minimizer \u03c0\u2217 of \u03c1(\u00b7). Differentiating \u03c1(\u03c0) with respect to \u03c0 at \u03c0 = \u03c0\u2217 yields\n\u2202\n\u2202\u03c0 \u03c1(\u03c0\u2217) = 2\n( \u2206T\u03c0\u2217 ) \u2206\u2212\u03a8\u2217g\n= 2L\u2217\u2206\u2212\u03a8\u2217g\nwhere L\u2217 = \u2206T\u03c0\u2217 is the expected instantaneous regret of the sampling distribution \u03c0\u2217. Let d\u2217 = mini \u2202\u2202\u03c0i \u03c1(\u03c0\n\u2217) denote the smallest partial derivative of \u03c1 at \u03c0\u2217. It must be the case that any i with \u03c0\u2217i > 0 satisfies d\u2217 = \u2202\u2202\u03c0i \u03c1(\u03c0\n\u2217), as otherwise transferring probability from action ai could lead to strictly lower cost. This shows that\n\u03c0\u2217i > 0 =\u21d2 gi = \u2212d\u2217 \u03a8\u2217 + 2L\u2217 \u03a8\u2217 \u2206i. (17)\nLet i1, .., im be the indices such that \u03c0\u2217ik > 0 ordered so that gi1 \u2265 gi2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 gim . Then we can choose a \u03b2 \u2208 [0, 1] so that\nm\u2211 k=1 \u03c0\u2217ikgik = \u03b2gi1 + (1\u2212 \u03b2)gim .\nBy equation (17), this implies as well that \u2211m k=1 \u03c0 \u2217 ik\n\u2206ik = \u03b2\u2206i1 + (1 \u2212 \u03b2)\u2206im , and hence that the sampling distribution that plays ai1 with probability \u03b2 and aim otherwise has the same instantaneous expected regret and the same expected information gain as \u03c0\u2217. That is, starting with a general sampling distribution \u03c0\u2217 that maximizes \u03c1(\u03c0), we showed there is a sampling distribution with support over at most two actions attains the same objective value and hence that also maximizes \u03c1(\u03c0)."}, {"heading": "B Proof of Proposition 1", "text": "The following fact expresses the mutual information between A\u2217 and Yt,a as the as the expected reduction in the entropy of A\u2217 due to observing Yt,a.\nFact 1. (Lemma 5.5.6 of Gray [35])\nIt (A\u2217;Yt,a) = E [H(\u03b1t)\u2212H(\u03b1t+1)|At = a,Ft]\nProposition 1. For any policy \u03c0 = (\u03c01, \u03c02, \u03c03, . . .) and time T \u2208 N,\nE [Regret (T, \u03c0)] \u2264 \u221a \u03a8T (\u03c0)H(\u03b11)T .\nwhere\n\u03a8T (\u03c0) \u2261 1 T T\u2211 t=1 E\u03c0[\u03a8t(\u03c0t)]\nis the average expected information ratio under \u03c0.\nProof. Since the policy \u03c0 is fixed throughout, we will simplify notation and write \u03a8t \u2261 \u03a8t(\u03c0t), \u2206t \u2261 \u2206t(\u03c0t) and gt = gt(\u03c0t) throughout this proof. First observe that entropy bounds expected cumulative information gain: E T\u2211 t=1 gt = E T\u2211 t=1 E [H(\u03b1t)\u2212H(\u03b1t+1)|Ft] = E T\u2211 t=1 (H(\u03b1t)\u2212H(\u03b1t+1)) = H(\u03b11)\u2212H(\u03b1T+1) \u2264 H(\u03b11),\nwhere the first equality relies on Fact 1 and the tower property of conditional expectation and the final inequality follows from the non-negativity of entropy. Then,\nE [Regret (T, \u03c0)] = E T\u2211 t=1 \u2206t = E T\u2211 t=1 \u221a \u03a8t \u221a gt ( \u03c0IDSt ) \u2264 \u221a\u221a\u221a\u221aE T\u2211 t=1 \u03a8t \u221a\u221a\u221a\u221aE T\u2211 t=1 gt\n\u2264 \u221a H(\u03b11) \u221a\u221a\u221a\u221aE T\u2211 t=1 \u03a8t\n= \u221a\u221a\u221a\u221a( 1 T E T\u2211 t=1 \u03a8t ) H(\u03b11)T ,\nwhere the first inequality follows from Holder\u2019s inequality."}, {"heading": "C Proof of Proposition 7", "text": "Proposition 7. Suppose supy R(y)\u2212 infy R(y) \u2264 1 and\n\u03c0t \u2208 arg min \u03c0\u2208SK\n\u2206t(\u03c0)2\nvt(\u03c0) ,\nThen the following hold:\n1. \u03a8t(\u03c0t) \u2264 |A|/2.\n2. \u03a8t(\u03c0t) \u2264 d/2 when A \u2282 Rd, \u0398 \u2282 Rd, and E [Rt,a|\u03b8] = aT \u03b8 for each action a \u2208 A.\nThe proof of this proposition essentially reduces to techniques in Russo and Van Roy [61], but some new analysis is required to show the results in that paper apply to variance-based IDS. A full proof is provided below.\nWe will make use of the following fact, which is a matrix-analogue of the Cauchy-Schwartz inequality. For any rank r matrix M \u2208 Rn\u00d7n with singular values \u03c31, . . . , \u03c3r, let\n\u2016M\u2016\u2217 := r\u2211 i=1 \u03c3i, \u2016M\u2016F := \u221a\u2211n k=1 \u2211n j=1M 2 i,j = \u221a\u2211r i=1 \u03c3 2 i , Trace(M) := n\u2211 i=1 Mii,\ndenote respectively the Nuclear norm, Frobenius norm and trace of M .\nFact 2. For any matrix M \u2208 Rk\u00d7k,\nTrace (M) \u2264 \u221a Rank(M)\u2016M\u2016F.\nWe now prove Proposition 7\nProof. Preliminaries: As noted in Section 6.3, gt(a) \u2265 2vt(a) for all t and a. Therefore for any \u03c0 \u2208 D(A)\n\u03a8t(\u03c0) = \u2206t(\u03c0)2 gt(\u03c0) \u2264 \u2206t(\u03c0) 2 2vt(\u03c0) .\nTherefore, if\n\u03c0t = arg min \u03c0\u2208D(A)\n\u2206t(\u03c0)2\nvt(\u03c0)\nis the action-sampling distribution chosen by variance based IDS, then\n\u03a8t(\u03c0t) \u2264 \u2206t(\u03c0t)2 2vt(\u03c0t) \u2264 \u2206t(\u03c0 TS t )2 2vt(\u03c0TSt ) ,\nwhere \u03c0TSt is the action-sampling distribution of Thompson sampling at time t. As a result, to show \u03a8t(\u03c0t) \u2264 \u03bb/2, it\u2019s enough to show \u2206t(\u03c0TSt )2 \u2264 \u03bbvt(\u03c0TSt ). We show that this holds always for \u03bb = |A|, and then show it holds for \u03bb = d when A \u2282 Rd, \u0398 \u2282 Rd, and E [Rt,a|\u03b8] = aT \u03b8 for all a \u2208 A.\nRecall that by definition, \u03c0TSt (a) = Pt(A\u2217 = a) for each a \u2208 A. Therefore\n\u2206t(\u03c0TSt ) = Et[Rt,A\u2217 ]\u2212 \u2211 a\u2208A \u03c0TSt (a)Et[Rt,a]\n= \u2211 a\u2217\u2208A Pt(A\u2217 = a\u2217)E[Rt,a\u2217 |A\u2217 = a\u2217]\u2212 \u2211 a\u2208A Pt(A\u2217 = a)Et[Rt,a]\n= \u2211 a\u2208A Pt(A\u2217 = a) (Et[Rt,a|A\u2217 = a]\u2212 Et[Rt,a]) (18)\nand\nvt(\u03c0TSt ) = \u2211 a\u2208A \u03c0TSt (a)Vart(E[Rt,a|A\u2217])\n= \u2211 a\u2208A \u03c0TSt (a) \u2211 a\u2217\u2208A Pt(A\u2217 = a\u2217) (Et[Rt,a|A\u2217 = a\u2217]\u2212 Et[Rt,a])2\n= \u2211\na,a\u2217\u2208A Pt(A\u2217 = a)Pt(A\u2217 = a\u2217) (Et[Rt,a|A\u2217 = a\u2217]\u2212 Et[Rt,a])2 . (19)\nProof part 1: By the Cauchy-Schwartz inequality, we conclude\n\u2206t(\u03c0TSt )2 = (\u2211 a\u2208A Pt(A\u2217 = a) (Et[Rt,a|A\u2217 = a]\u2212 Et[Rt,a]) )2\n\u2264 |A| \u2211 a\u2208A Pt(A\u2217 = a)2 (Et[Rt,a|A\u2217 = a]\u2212 Et[Rt,a])2\n\u2264 |A| \u2211\na,a\u2032\u2208A Pt(A\u2217 = a)Pt(A\u2217 = a\u2032)\n( Et[Rt,a|A\u2217 = a\u2032]\u2212 Et[Rt,a] )2 = |A|vt(\u03c0TSt ).\nAs argued above, this implies \u03a8t(\u03c0t) \u2264 |A|/2.\nProof of part 2: This argument can be extended to provide a tighter bound under a linearity assumption. Now assume A \u2282 Rd, \u0398 \u2282 Rd, and E [Rt,a|\u03b8] = aT \u03b8. Write A = {a1, . . . , aK} and define M \u2208 RK\u00d7K by\nMi,j = \u221a Pt(A\u2217 = ai)Pt(A\u2217 = aj) (Et[Rt,ai |A\u2217 = aj ]\u2212 Et[Rt,ai ])\n= \u221a \u03b1t(ai)\u03b1t(aj) (Et[Rt,ai |A\u2217 = aj ]\u2212 Et[Rt,ai ])\nfor all i, j \u2208 {1, ..,K}. Then, by (18) and (19),\n\u2206t(\u03c0TSt ) = Trace(M),\nand vt(\u03c0TSt ) = \u2016M\u20162F.\nThis shows, by Fact 2 that \u2206t(\u03c0TSt )2 \u2264 Rank(M)vt(\u03c0TSt )\nWe now show Rank(M) \u2264 d. Define\n\u00b5 = E [\u03b8|Ft] \u00b5j = E [\u03b8|Ft, A\u2217 = aj ] .\nThen, by the linearity of the expectation operator, Et[Rt,ai |A\u2217 = aj ] \u2212 Et[Rt,ai ] = (\u00b5j \u2212 \u00b5)Tai. Therefore, Mi,j = \u221a \u03b1t(ai)\u03b1t(aj)((\u00b5j \u2212 \u00b5)Tai) and\nM =  \u221a \u03b1t(a1) ( \u00b51 \u2212 \u00b5 )T ... ...\u221a\n\u03b1t(aK)) ( \u00b5k \u2212 \u00b5 )T\n [ \u221a \u03b1t(a1)a1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u221a \u03b1t(aK)aK ] .\nSince M is the product of a K by d matrix and a d by K matrix, it has rank at most d."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Y. Abbasi-Yadkori", "D. P\u00e1l", "C. Szepesv\u00e1ri"], "venue": "Advances in Neural Information Processing Systems, 24,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Online-to-confidence-set conversions and application to sparse stochastic bandits", "author": ["Y. Abbasi-Yadkori", "D. P\u00e1l", "C. Szepesv\u00e1ri"], "venue": "Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Asymptotically efficient adaptive allocation schemes for controlled iid processes: Finite parameter space", "author": ["R. Agrawal", "D. Teneketzis", "V. Anantharam"], "venue": "IEEE Transactions on Automatic Control, 34(3):258\u2013267,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Asymptotically efficient adaptive allocation schemes for controlled Markov chains: Finite parameter space", "author": ["R. Agrawal", "D. Teneketzis", "V. Anantharam"], "venue": "IEEE Transactions on Automatic Control, 34(12):1249\u20131259,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Further optimal regret bounds for Thompson sampling", "author": ["S. Agrawal", "N. Goyal"], "venue": "Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, pages 99\u2013107,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["S. Agrawal", "N. Goyal"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 127\u2013135,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["J.-Y. Audibert", "S. Bubeck"], "venue": "Proceedings of the 22nd Annual Conference on Learning Theory (COLT), pages 217\u2013226,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Regret in online combinatorial optimization", "author": ["J.-Y. Audibert", "S. Bubeck", "G. Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, 47(2):235\u2013256,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Bayesian mixture modelling and inference based Thompson sampling in Monte-Carlo tree search", "author": ["A. Bai", "F. Wu", "X. Chen"], "venue": "Advances in Neural Information Processing Systems 26. Curran Associates, Inc.,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Partial monitoringclassification, regret bounds, and algorithms", "author": ["G. Bart\u00f3k", "D.P. Foster", "D. P\u00e1l", "A. Rakhlin", "C. Szepesv\u00e1ri"], "venue": "Mathematics of Operations Research, 39(4): 967\u2013997,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge university press,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. de Freitas"], "venue": "Technical Report TR-2009-23,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Dynamic pricing under a general parametric choice model", "author": ["J. Broder", "P. Rusmevichientong"], "venue": "Operations Research, 60(4):965\u2013980,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and trends in machine learning, 5(1):1\u2013122,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-scale exploration of convex functions and bandit convex optimization", "author": ["S. Bubeck", "R. Eldan"], "venue": "Proceedings of the 29th Annual Conference on Learning Theory (COLT), pages 583\u2013589,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "X-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesv\u00e1ri"], "venue": "Journal of Machine Learning Research, 12:1655\u20131695, June", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Bandit convex optimization:  \u221a T regret in one dimension", "author": ["S. Bubeck", "O. Dekel", "T. Koren", "Y. Peres"], "venue": "Proceedings of the 28st Annual Conference on Learning Theory (COLT),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Kullback-Leibler upper confidence bounds for optimal sequential allocation", "author": ["O. Capp\u00e9", "A. Garivier", "O.-A. Maillard", "R. Munos", "G. Stoltz"], "venue": "Annals of Statistics, 41(3):1516\u20131541,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian experimental design: A review", "author": ["K. Chaloner", "I. Verdinelli"], "venue": "Statistical Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "An empirical evaluation of Thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian process optimization with mutual information", "author": ["E. Contal", "V. Perchet", "N. Vayatis"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "The price of bandit information for online optimization", "author": ["V. Dani", "S.M. Kakade", "T.P. Hayes"], "venue": "Advances in Neural Information Processing Systems, pages 345\u2013352,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["V. Dani", "T.P. Hayes", "S.M. Kakade"], "venue": "Proceedings of the 21st Annual Conference on Learning Theory (COLT), pages 355\u2013366,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Parametric bandits: The generalized linear case", "author": ["S. Filippi", "O. Capp\u00e9", "A. Garivier", "C. Szepesv\u00e1ri"], "venue": "Advances in Neural Information Processing Systems, 23:1\u20139,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Choosing a good toolkit, I: Formulation, heuristics, and asymptotic properties", "author": ["A. Francetich", "D.M. Kreps"], "venue": "preprint,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Choosing a good toolkit, II: Simulations and conclusions", "author": ["A. Francetich", "D.M. Kreps"], "venue": "preprint,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Paradoxes in learning and the marginal value of information", "author": ["P.I. Frazier", "W.B. Powell"], "venue": "Decision Analysis, 7(4):378\u2013403,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "A knowledge-gradient policy for sequential information collection", "author": ["P.I. Frazier", "W.B. Powell", "S. Dayanik"], "venue": "SIAM Journal on Control and Optimization, 47(5):2410\u20132439,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-Armed Bandit Allocation Indices", "author": ["J. Gittins", "K. Glazebrook", "R. Weber"], "venue": "John Wiley & Sons, Ltd,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "Journal of Artificial Intelligence Research, 42(1):427\u2013486,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["D. Golovin", "A. Krause", "D. Ray"], "venue": "Advances in Neural Information Processing Systems, pages 766\u2013774,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Thompson sampling for complex online problems", "author": ["A. Gopalan", "S. Mannor", "Y. Mansour"], "venue": "Proceedings of The 31st International Conference on Machine Learning, pages 100\u2013108,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Asymptotically efficient adaptive choice of control laws in controlled Markov chains", "author": ["T.L. Graves", "T.L. Lai"], "venue": "SIAM journal on control and optimization, 35(3):715\u2013743,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Entropy and information theory", "author": ["R.M. Gray"], "venue": "Springer,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Entropy search for information-efficient global optimization", "author": ["P. Hennig", "C.J. Schuler"], "venue": "The Journal of Machine Learning Research, 98888(1):1809\u20131837,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Predictive entropy search for multi-objective Bayesian optimization", "author": ["D. Hern\u00e1ndez-Lobato", "J.M. Hern\u00e1ndez-Lobato", "A. Shah", "R.P. Adams"], "venue": "arXiv preprint arXiv:1511.05467,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Predictive entropy search for efficient global optimization of black-box functions", "author": ["J.M. Hern\u00e1ndez-Lobato", "M.W. Hoffman", "Z. Ghahramani"], "venue": "Advances in neural information processing systems, pages 918\u2013926,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Predictive entropy search for Bayesian optimization with unknown constraints", "author": ["J.M. Hern\u00e1ndez-Lobato", "M.A. Gelbart", "M.W. Hoffman", "R.P. Adams", "Z. Ghahramani"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, Lille, France,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "Journal of Machine Learning Research, 11:1563\u20131600,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Twenty questions with noise: Bayes optimal policies for entropy loss", "author": ["B. Jedynak", "P.I. Frazier", "R. Sznitman"], "venue": "Journal of Applied Probability,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Refined knowledge-gradient policy for learning probabilities", "author": ["B. Kami\u0144ski"], "venue": "Operations Research Letters, 43(2):143\u2013147,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "On Bayesian upper confidence bounds for bandit problems", "author": ["E. kaufmann", "O. Capp\u00e9", "A. Garivier"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Thompson sampling: an asymptotically optimal finite time analysis", "author": ["E. Kaufmann", "N. Korda", "R. Munos"], "venue": "International Conference on Algorithmic Learning Theory,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-armed bandits in metric spaces", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfal"], "venue": "Proceedings of the 40th ACM Symposium on Theory of Computing,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "Cs. Szepesv\u00e1ri"], "venue": "In ECML,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise", "author": ["H.J. Kushner"], "venue": "Journal of Basic Engineering, 86(1):97\u2013106,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1964}, {"title": "Adaptive treatment allocation and the multi-armed bandit problem", "author": ["T.L. Lai"], "venue": "The Annals of Statistics, pages 1091\u20131114,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1987}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in applied mathematics, 6(1):4\u201322,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1985}, {"title": "On a measure of the information provided by an experiment", "author": ["D.V. Lindley"], "venue": "Annals of Mathematical Statistics, 78(4):986\u20131005,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1956}, {"title": "The application of Bayesian methods for seeking the extremum", "author": ["J. Mockus", "V. Tiesis", "A. Zilinskas"], "venue": "Towards Global Optimization, 2(117-129):2,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1978}, {"title": "Computing a classic index for finite-horizon bandits", "author": ["J. Ni\u00f1o-Mora"], "venue": "INFORMS Journal on Computing, 23(2):254\u2013267,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "More) efficient reinforcement learning via posterior 40  sampling", "author": ["I. Osband", "D. Russo", "B. Van Roy"], "venue": "Advances in Neural Information Processing Systems 26. Curran Associates, Inc.,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["A. Piccolboni", "C. Schindelhauer"], "venue": "International Conference on Computational Learning Theory, pages 208\u2013223. Springer,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2001}, {"title": "Optimal learning, volume 841", "author": ["W.B. Powell", "I.O. Ryzhov"], "venue": "John Wiley & Sons,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Linearly parameterized bandits", "author": ["P. Rusmevichientong", "J.N. Tsitsiklis"], "venue": "Mathematics of Operations Research, 35(2):395\u2013411,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic assortment optimization with a multinomial logit choice model and capacity constraint", "author": ["P. Rusmevichientong", "Z.-J.M. Shen", "D.B. Shmoys"], "venue": "Operations research, 58(6): 1666\u20131680,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Eluder dimension and the sample complexity of optimistic exploration", "author": ["D. Russo", "B. Van Roy"], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2256\u20132264. Curran Associates, Inc.,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to optimize via posterior sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "Mathematics of Operations Research, 39(4):1221\u20131243,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to optimize via information-directed sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1583\u20131591. Curran Associates, Inc.,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "An information-theoretic analysis of Thompson sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "Journal of Machine Learning Research, 17(68):1\u201330,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2016}, {"title": "On the robustness of a one-period look-ahead policy in multi-armed bandit problems", "author": ["I. Ryzhov", "P. Frazier", "W. Powell"], "venue": "Procedia Computer Science, 1(1):1635\u20131644,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2010}, {"title": "The knowledge gradient algorithm for a general class of online learning problems", "author": ["I.O. Ryzhov", "W.B. Powell", "P.I. Frazier"], "venue": "Operations Research, 60(1):180\u2013195,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimal dynamic assortment planning with demand learning", "author": ["D. Saur\u00e9", "A. Zeevi"], "venue": "Manufacturing & Service Operations Management, 15(3):387\u2013404,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2013}, {"title": "A modern Bayesian look at the multi-armed bandit", "author": ["S.L. Scott"], "venue": "Applied Stochastic Models in Business and Industry, 26(6):639\u2013658,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2010}, {"title": "Information-theoretic regret bounds for Gaussian process optimization in the bandit setting", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"], "venue": "IEEE Transactions on Information Theory, 58(5):3250 \u20133265, may", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic simultaneous optimistic optimization", "author": ["M. Valko", "A. Carpentier", "R. Munos"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 19\u201327,", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2013}, {"title": "Finite-time analysis of kernelised contextual bandits", "author": ["M. Valko", "N. Korda", "R. Munos", "I. Flaounas", "N. Cristianini"], "venue": "Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2013}, {"title": "An informational approach to the global optimization of expensive-to-evaluate functions", "author": ["J. Villemonteix", "E. Vazquez", "E. Walter"], "venue": "Journal of Global Optimization, 44(4):509\u2013534,", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2009}, {"title": "Bisection search with noisy responses", "author": ["R. Waeber", "P.I. Frazier", "S.G. Henderson"], "venue": "SIAM Journal on Control and Optimization, 51(3):2261\u20132279,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 60, "context": "Further, by leveraging the tools of our recent information theoretic analysis of Thompson sampling [61], we establish an expected regret bound for IDS that applies across a very general class of models and scales with the entropy of the optimal action distribution.", "startOffset": 99, "endOffset": 103}, {"referenceID": 48, "context": "This is particularly surprising for Bernoulli bandit problems, where UCB algorithms and Thompson sampling are known to be asymptotically optimal in the sense proposed by Lai and Robbins [49].", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": ", [14, 57, 65]), though in each case, developing a computationally efficient version of IDS may require innovation.", "startOffset": 2, "endOffset": 14}, {"referenceID": 56, "context": ", [14, 57, 65]), though in each case, developing a computationally efficient version of IDS may require innovation.", "startOffset": 2, "endOffset": 14}, {"referenceID": 63, "context": ", [14, 57, 65]), though in each case, developing a computationally efficient version of IDS may require innovation.", "startOffset": 2, "endOffset": 14}, {"referenceID": 0, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 106, "endOffset": 117}, {"referenceID": 23, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 106, "endOffset": 117}, {"referenceID": 55, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 106, "endOffset": 117}, {"referenceID": 24, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 138, "endOffset": 142}, {"referenceID": 1, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 161, "endOffset": 164}, {"referenceID": 65, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 207, "endOffset": 211}, {"referenceID": 65, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 268, "endOffset": 276}, {"referenceID": 67, "context": "UCB algorithms have been applied to problems where the mapping from action to expected reward is a linear [1, 24, 56], generalized linear [25], or sparse linear [2] model; is sampled from a Gaussian process [67] or has small norm in a reproducing kernel Hilbert space [67, 69]; or is a smooth (e.", "startOffset": 268, "endOffset": 276}, {"referenceID": 16, "context": "Lipschitz continuous) model [17, 45, 68].", "startOffset": 28, "endOffset": 40}, {"referenceID": 44, "context": "Lipschitz continuous) model [17, 45, 68].", "startOffset": 28, "endOffset": 40}, {"referenceID": 66, "context": "Lipschitz continuous) model [17, 45, 68].", "startOffset": 28, "endOffset": 40}, {"referenceID": 5, "context": "Agrawal and Goyal [6] provided the first analysis for linear contextual bandit problems.", "startOffset": 18, "endOffset": 21}, {"referenceID": 57, "context": "Russo and Van Roy [58, 59] consider a more general class of models, and show that standard analysis of upper confidence bound algorithms leads to bounds on the expected regret of Thompson sampling.", "startOffset": 18, "endOffset": 26}, {"referenceID": 58, "context": "Russo and Van Roy [58, 59] consider a more general class of models, and show that standard analysis of upper confidence bound algorithms leads to bounds on the expected regret of Thompson sampling.", "startOffset": 18, "endOffset": 26}, {"referenceID": 32, "context": "[33] provides asymptotic frequentist bounds on the growth rate of regret for problems with dependent arms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "sampling have been applied to other types of problems, like reinforcement learning [40, 53] and Monte Carlo tree search [10, 46].", "startOffset": 83, "endOffset": 91}, {"referenceID": 52, "context": "sampling have been applied to other types of problems, like reinforcement learning [40, 53] and Monte Carlo tree search [10, 46].", "startOffset": 83, "endOffset": 91}, {"referenceID": 9, "context": "sampling have been applied to other types of problems, like reinforcement learning [40, 53] and Monte Carlo tree search [10, 46].", "startOffset": 120, "endOffset": 128}, {"referenceID": 45, "context": "sampling have been applied to other types of problems, like reinforcement learning [40, 53] and Monte Carlo tree search [10, 46].", "startOffset": 120, "endOffset": 128}, {"referenceID": 2, "context": "[3] consider a general model in which the reward distribution associated with each action depends on a common unknown parameter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] and Graves and Lai [34] to apply to the adaptive control of Markov chains and to problems with infinite parameter spaces.", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "[4] and Graves and Lai [34] to apply to the adaptive control of Markov chains and to problems with infinite parameter spaces.", "startOffset": 23, "endOffset": 27}, {"referenceID": 49, "context": "Though the use of mutual information to guide sampling has been the subject of much research, dating back to the work of Lindley [50], to our knowledge, only two other papers [36, 70] have used the mutual information between the optimal action and the next observation to guide action selection.", "startOffset": 129, "endOffset": 133}, {"referenceID": 35, "context": "Though the use of mutual information to guide sampling has been the subject of much research, dating back to the work of Lindley [50], to our knowledge, only two other papers [36, 70] have used the mutual information between the optimal action and the next observation to guide action selection.", "startOffset": 175, "endOffset": 183}, {"referenceID": 68, "context": "Though the use of mutual information to guide sampling has been the subject of much research, dating back to the work of Lindley [50], to our knowledge, only two other papers [36, 70] have used the mutual information between the optimal action and the next observation to guide action selection.", "startOffset": 175, "endOffset": 183}, {"referenceID": 12, "context": "This approach is often called \u201cBayesian optimization\u201d in the machine learning community [13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 68, "context": "[70] and Hennig and Schuler [36] propose selecting each sample to maximize the mutual information between the next observation and the optimal solution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[70] and Hennig and Schuler [36] propose selecting each sample to maximize the mutual information between the next observation and the optimal solution.", "startOffset": 28, "endOffset": 32}, {"referenceID": 36, "context": "Several papers [37, 38, 39] have extended this line of work since an initial version of our paper appeared online.", "startOffset": 15, "endOffset": 27}, {"referenceID": 37, "context": "Several papers [37, 38, 39] have extended this line of work since an initial version of our paper appeared online.", "startOffset": 15, "endOffset": 27}, {"referenceID": 38, "context": "Several papers [37, 38, 39] have extended this line of work since an initial version of our paper appeared online.", "startOffset": 15, "endOffset": 27}, {"referenceID": 68, "context": "[70] and Hennig and Schuler [36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[70] and Hennig and Schuler [36].", "startOffset": 28, "endOffset": 32}, {"referenceID": 65, "context": "For such problems, simpler approaches like UCB algorithms [67], probability of improvement [47], and expected improvement [51] are already extremely effective.", "startOffset": 58, "endOffset": 62}, {"referenceID": 46, "context": "For such problems, simpler approaches like UCB algorithms [67], probability of improvement [47], and expected improvement [51] are already extremely effective.", "startOffset": 91, "endOffset": 95}, {"referenceID": 50, "context": "For such problems, simpler approaches like UCB algorithms [67], probability of improvement [47], and expected improvement [51] are already extremely effective.", "startOffset": 122, "endOffset": 126}, {"referenceID": 12, "context": "[13], each of these algorithms simply chooses points with \u201cpotentially high values of the objective function: whether because the prediction is high, the uncertainty is great, or both.", "startOffset": 0, "endOffset": 4}, {"referenceID": 68, "context": "[70] and Hennig and Schuler [36] propose their algorithms as heuristics without guarantees.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[70] and Hennig and Schuler [36] propose their algorithms as heuristics without guarantees.", "startOffset": 28, "endOffset": 32}, {"referenceID": 50, "context": "[51] and studied further by Frazier et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] and Ryzhov et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[64].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Our work also connects to a much larger literature on Bayesian experimental design (see [20] for a review).", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "[22] study problems with Gaussian process priors and a method that guides exploration using the mutual information between the objective function and the next observation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] and Waeber et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "[71] consider problem settings in which this greedy policy is optimal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Another recent line of work [31, 32] shows that measures of information gain sometimes satisfy a decreasing returns property known as adaptive sub-modularity, implying the greedy policy is competitive with the optimal policy.", "startOffset": 28, "endOffset": 36}, {"referenceID": 31, "context": "Another recent line of work [31, 32] shows that measures of information gain sometimes satisfy a decreasing returns property known as adaptive sub-modularity, implying the greedy policy is competitive with the optimal policy.", "startOffset": 28, "endOffset": 36}, {"referenceID": 53, "context": "First introduced by [54] the partial monitoring problem encompasses a broad range of online optimization problems with limited or partial feedback.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "Recent work [11] has focused on classifying the minimax-optimal scaling of regret in the problem\u2019s time horizon as a function of the level of feedback the agent receives.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "Francetich and Kreps [26, 27] discuss a variety of heuristics for the discounted problem.", "startOffset": 21, "endOffset": 29}, {"referenceID": 26, "context": "Francetich and Kreps [26, 27] discuss a variety of heuristics for the discounted problem.", "startOffset": 21, "endOffset": 29}, {"referenceID": 60, "context": "The regret bounds we will present build on our information-theoretic analysis of Thompson sampling [61], which can be used to bound the regret of any policy in terms of its information ratio.", "startOffset": 99, "endOffset": 103}, {"referenceID": 60, "context": "The information ratio of IDS is always smaller than that of TS, and therefore, bounds on the information ratio of TS provided in Russo and Van Roy [61] yield regret bounds for IDS.", "startOffset": 147, "endOffset": 151}, {"referenceID": 59, "context": "This observation and a preliminary version of our results was first presented in a conference paper [60].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "[18] and Bubeck and Eldan [16] build on ideas from [61] in another direction by bounding the information ratio when the reward function is convex and using that bound to study the order of regret in adversarial bandit convex optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] and Bubeck and Eldan [16] build on ideas from [61] in another direction by bounding the information ratio when the reward function is convex and using that bound to study the order of regret in adversarial bandit convex optimization.", "startOffset": 26, "endOffset": 30}, {"referenceID": 60, "context": "[18] and Bubeck and Eldan [16] build on ideas from [61] in another direction by bounding the information ratio when the reward function is convex and using that bound to study the order of regret in adversarial bandit convex optimization.", "startOffset": 51, "endOffset": 55}, {"referenceID": 34, "context": "6 of Gray [35], this is equal to the expected reduction in entropy of the posterior distribution of A\u2217 due to observing Yt(a):", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 126, "endOffset": 145}, {"referenceID": 18, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 126, "endOffset": 145}, {"referenceID": 43, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 126, "endOffset": 145}, {"referenceID": 47, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 126, "endOffset": 145}, {"referenceID": 48, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 126, "endOffset": 145}, {"referenceID": 16, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 23, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 24, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 32, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 55, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 58, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 65, "context": "Specific UCB and TS algorithms are known to be asymptotically efficient for multi-armed bandit problems with independent arms [5, 19, 44, 48, 49] and satisfy strong regret bounds for some problems with dependent arms [17, 24, 25, 33, 56, 59, 67].", "startOffset": 217, "endOffset": 245}, {"referenceID": 12, "context": "The first is the expected improvement algorithm, which is one of the most widely used techniques in the active field of Bayesian optimization (see [13]).", "startOffset": 147, "endOffset": 151}, {"referenceID": 62, "context": "The knowledge gradient algorithm [64] uses a modified improvement measure.", "startOffset": 33, "endOffset": 37}, {"referenceID": 27, "context": "To address problems like Example 1, Frazier and Powell [28] propose KG* \u2013 a modified form of KG that considers the value of sampling a single action many times.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "\u03b1\u2217 = arg min \u03b1\u2208[0,1] ((1\u2212 \u03b1)/2 + \u03b1(1\u2212 1/d))2", "startOffset": 15, "endOffset": 20}, {"referenceID": 0, "context": "\u03b1 log2(d) = arg min \u03b1\u2208[0,1] 1 2 \u221a \u03b1 + \u221a \u03b1 (1 2 \u2212 1 d ) = 1.", "startOffset": 22, "endOffset": 27}, {"referenceID": 60, "context": "These regret bounds follow from our recent information theoretic-analysis of Thompson sampling [61].", "startOffset": 95, "endOffset": 99}, {"referenceID": 60, "context": "Because the information-ratio of IDS is always smaller than that of TS, the bounds on the information ratio of TS provided in Russo and Van Roy [61] immediately yield regret bounds for IDS for a number of important problem classes.", "startOffset": 144, "endOffset": 148}, {"referenceID": 60, "context": "Several bounds on the information-ratio of TS were provided by Russo and Van Roy [61], and we defer to that paper for the proofs.", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "[18] and Bubeck and Eldan [16] bounds the information ratio when the reward function is convex, and uses this to study the order of regret in adversarial bandit convex optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] and Bubeck and Eldan [16] bounds the information ratio when the reward function is convex, and uses this to study the order of regret in adversarial bandit convex optimization.", "startOffset": 26, "endOffset": 30}, {"referenceID": 60, "context": "This effectively controls the worst-case variance of the reward distribution, and as shown in the appendix of Russo and Van Roy [61], our results can be extended to the case where reward distributions are sub-Gaussian.", "startOffset": 128, "endOffset": 132}, {"referenceID": 22, "context": "[23] show this bound is order optimal, in the sense that for any time horizon T and number of actions |A| there exists a prior distribution over \u03b8 under which inf\u03c0 E [Regret(T, \u03c0)] \u2265 c0 \u221a log(|A|)T where c0 is a numerical constant that does not depend on |A| or T .", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1, 24, 56]) and is one of the most important examples of a multi-armed bandit problem with \u201ccorrelated arms.", "startOffset": 0, "endOffset": 11}, {"referenceID": 23, "context": "[1, 24, 56]) and is one of the most important examples of a multi-armed bandit problem with \u201ccorrelated arms.", "startOffset": 0, "endOffset": 11}, {"referenceID": 55, "context": "[1, 24, 56]) and is one of the most important examples of a multi-armed bandit problem with \u201ccorrelated arms.", "startOffset": 0, "endOffset": 11}, {"referenceID": 22, "context": "[23] again show this bound is order optimal in the sense that, for any time horizon T and dimension d, when the action set is A = {0, 1}d there exists a prior distribution over \u03b8 such that inf\u03c0 E [Regret(T, \u03c0)] \u2265 c0 \u221a log(|A|)dT where c0 is a constant that is independent of d and T .", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "This type of observation structure is sometimes called \u201csemi-bandit\u201d feedback [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "[8], the", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "5 of Bubeck and Cesa-Bianchi [15]), the agent\u2019s regret from each component must be at least \u221a d m T , and hence her overall expected regret is lower bounded by a term of order m \u221a d m T = \u221a mdT .", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": ", xn} \u2282 [0, 1].", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "The compute time of such an implementation scales with K2n where K is the number of actions and n is the number of points used in the discretization of [0, 1].", "startOffset": 152, "endOffset": 158}, {"referenceID": 60, "context": "2Some details related to the derivation of this fact when Yt,a is a general random variable can be found in the appendix of Russo and Van Roy [61].", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "1: qa,a\u2032 \u2190 arg minq\u2032\u2208[0,1] [ q\u2032~ \u2206a + (1\u2212 q\u2032)~ \u2206a\u2032 ]2 / [q~ga + (1\u2212 q)~ga\u2032 ] \u2200a < K, a\u2032 > a 2: (a\u2217, a\u2217\u2217)\u2190 arg mina<K,a\u2032>a [ qa,a\u2032 ~ \u2206a + (1\u2212 qa,a\u2032)~ \u2206a\u2032 ]2 / [ qa,a\u2032~ga + (1\u2212 qa,a\u2032)~ga\u2032 ]", "startOffset": 21, "endOffset": 26}, {"referenceID": 0, "context": "R(y) \u2208 [0, 1] for all y, our information measure term is lower-bounded according to", "startOffset": 7, "endOffset": 13}, {"referenceID": 60, "context": "Inequality (a) is a simple corollary of Pinsker\u2019s inequality, and is given as Fact 9 in Russo and Van Roy [61].", "startOffset": 106, "endOffset": 110}, {"referenceID": 8, "context": "[9] selects the action a which maximizes the upper confidence bound \u03b8\u0302t(a)+ \u221a 2 log(t)/Nt(a) where \u03b8\u0302t(a) is the empirical average reward from samples of action a and Nt(a) is the number of samples of action a up to time t.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] proposed using an algorithm called UCB-Tuned.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "The MOSS algorithm of Audibert and Bubeck [7] is similar to UCB1 and UCB\u2013Tuned, but uses slightly different confidence bounds.", "startOffset": 42, "endOffset": 45}, {"referenceID": 20, "context": "In previous numerical experiments [21, 43, 44, 66], Thompson sampling and Bayes UCB exhibited state-of-the-art performance for this problem.", "startOffset": 34, "endOffset": 50}, {"referenceID": 42, "context": "In previous numerical experiments [21, 43, 44, 66], Thompson sampling and Bayes UCB exhibited state-of-the-art performance for this problem.", "startOffset": 34, "endOffset": 50}, {"referenceID": 43, "context": "In previous numerical experiments [21, 43, 44, 66], Thompson sampling and Bayes UCB exhibited state-of-the-art performance for this problem.", "startOffset": 34, "endOffset": 50}, {"referenceID": 64, "context": "In previous numerical experiments [21, 43, 44, 66], Thompson sampling and Bayes UCB exhibited state-of-the-art performance for this problem.", "startOffset": 34, "endOffset": 50}, {"referenceID": 48, "context": "Each also satisfies strong theoretical guarantees, and is known to be asymptotically optimal in the sense defined by Lai and Robbins [49].", "startOffset": 133, "endOffset": 137}, {"referenceID": 42, "context": "[43], constructs upper confidence bounds based on the quantiles of the posterior distribution: at time step t the upper confidence bound at an action is the 1\u2212 t quantile of the posterior distribution of that action3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "A somewhat different approach is the knowledge gradient (KG) policy of Powell and Ryzhov [55], which uses a one-step lookahead approximation to the value of information to guide experimentation.", "startOffset": 89, "endOffset": 93}, {"referenceID": 61, "context": "[63] offers much better performance in some of these problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "To enable efficient simulation, we use a heuristic approach to computing KG* proposed by Kami\u0144ski [42].", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "It is worth pointing out that, although Gittins\u2019 indices characterize the Bayes optimal policy for infinite horizon discounted problems, the finite horizon formulation considered here is computationally intractable [30].", "startOffset": 215, "endOffset": 219}, {"referenceID": 51, "context": "A similar index policy [52] designed for finite horizon problems could be applied as a heuristic in this setting.", "startOffset": 23, "endOffset": 27}, {"referenceID": 65, "context": "[67].", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "The seminal work of Lai and Robbins [49] provides the following asymptotic lower bound on regret of any policy \u03c0:", "startOffset": 36, "endOffset": 40}, {"referenceID": 42, "context": "Nevertheless, when applied with an independent uniform prior over \u03b8, both Bayes UCB and Thompson sampling are known to attain this lower bound [43, 44].", "startOffset": 143, "endOffset": 151}, {"referenceID": 43, "context": "Nevertheless, when applied with an independent uniform prior over \u03b8, both Bayes UCB and Thompson sampling are known to attain this lower bound [43, 44].", "startOffset": 143, "endOffset": 151}, {"referenceID": 42, "context": "We also include Bayes UCB [43] and a version of GP-UCB that was tuned, as in Subsection 7.", "startOffset": 26, "endOffset": 30}, {"referenceID": 65, "context": "5Regret analysis of GP-UCB can be found in [67].", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "Regret bounds for Thompson sampling can be found in [6, 59, 61]", "startOffset": 52, "endOffset": 63}, {"referenceID": 58, "context": "Regret bounds for Thompson sampling can be found in [6, 59, 61]", "startOffset": 52, "endOffset": 63}, {"referenceID": 60, "context": "Regret bounds for Thompson sampling can be found in [6, 59, 61]", "startOffset": 52, "endOffset": 63}, {"referenceID": 41, "context": "A direct implementation of the KG* policy was too slow to simulate, and so we have used a heuristic approach presented in [42], which uses golden section search to maximize a function that is not necessarily unimodal.", "startOffset": 122, "endOffset": 126}, {"referenceID": 48, "context": "One question raised is whether IDS attains the lower bound of Lai and Robbins [49] for some bandit problems with independent arms.", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "As shown in Chapter 3 of Boyd and Vandenberghe [12], f : (x, y) 7\u2192 x2/y is convex over {(x, y) \u2208 R2 : y > 0}.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "Then we can choose a \u03b2 \u2208 [0, 1] so that m \u2211", "startOffset": 25, "endOffset": 31}, {"referenceID": 34, "context": "6 of Gray [35])", "startOffset": 10, "endOffset": 14}, {"referenceID": 60, "context": "The proof of this proposition essentially reduces to techniques in Russo and Van Roy [61], but some new analysis is required to show the results in that paper apply to variance-based IDS.", "startOffset": 85, "endOffset": 89}], "year": 2017, "abstractText": "We propose information-directed sampling \u2013 a new approach to online optimization problems in which a decision-maker must balance between exploration and exploitation while learning from partial feedback. Each action is sampled in a manner that minimizes the ratio between squared expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation. We establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution. We illustrate through simple analytic examples how information-directed sampling accounts for kinds of information that alternative approaches do not adequately address and that this can lead to dramatic performance gains. For the widely studied Bernoulli, Gaussian, and linear bandit problems, we demonstrate state-of-the-art simulation performance.", "creator": "LaTeX with hyperref package"}}}