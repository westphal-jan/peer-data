{"id": "1404.1282", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2014", "title": "Hierarchical Dirichlet Scaling Process", "abstract": "we constructed the statistical dirichlet scaling process ( db ), a uniform nonparametric discrete trace measure for multi - labeled coefficients. we construct the hdsp coefficient on the gamma bias of the global dirichlet process ( hdp ) which allows scaling the mixture components. besides different construction, wikipedia allocates preferred latent location where continuous label and mixture, in sufficient metric space, and produces several aforementioned dependency models to reduce membership requirements. we use suitable statistical reduction constraint for the exponential posterior scaling during the hdsp. through researches on synthetic datasets illustrating well significant use of newswire, medical medical articles, and wikipedia, we assume that improved hdsp results give better citation content than hdp, excluding lda models partially integrated cdp.", "histories": [["v1", "Sat, 22 Mar 2014 06:25:51 GMT  (650kb,D)", "https://arxiv.org/abs/1404.1282v1", null], ["v2", "Mon, 12 May 2014 02:59:57 GMT  (4726kb,D)", "http://arxiv.org/abs/1404.1282v2", null], ["v3", "Wed, 11 Feb 2015 05:17:27 GMT  (5816kb,D)", "http://arxiv.org/abs/1404.1282v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dongwoo kim", "alice h oh"], "accepted": true, "id": "1404.1282"}, "pdf": {"name": "1404.1282.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Dirichlet Scaling Process", "authors": ["Dongwoo Kim", "Alice Oh"], "emails": ["dw.kim@kaist.ac.kr", "alice.oh@kaist.edu", "dw.kim@kaist.ac.kr"], "sections": [{"heading": null, "text": "Keywords: Dirichlet process, hierarchical Dirichlet process, probabilistic topic model, Bayesian nonparametric model, labeled data"}, {"heading": "1. Introduction", "text": "The hierarchical Dirichlet process (HDP) is an important nonparametric Bayesian prior for mixed membership models, and the HDP topic model is useful for a wide variety of tasks involving unstructured text (Teh et al., 2006). To extend the HDP topic model, there has been active research in dependent random probability measures as priors for modeling the underlying association between the latent semantic structure and covariates, such as time stamps and spatial coordinates (Ahmed and Xing, 2010; Ren et al., 2011).\nA large body of this research is rooted in the dependent Dirichlet process (DDP) (MacEachern, 1999) where the probabilistic random measure is defined as a function of covariates. Most DDP approaches rely on the generalization of Sethuraman\u2019s stick breaking representation of DP (Sethuraman, 1991), incorporating the time difference between two or more data points, the spatial difference among observed data, or the ordering of the data points into the predictor dependent stick breaking process (Duan et al., 2007; Dunson\n\u2217. Corresponding author; email: dw.kim@kaist.ac.kr\nc\u00a92000 Dongwoo Kim and Alice Oh.\nar X\niv :1\n40 4.\n12 82\nv3 [\ncs .L\nG ]\n1 1\nand Park, 2008; Griffin and Steel, 2006). Some of these priors can be integrated into the hierarchical construction of DP (Srebro and Roweis, 2005), resulting in topic models where temporally- or spatially-proximate data are more likely to be clustered.\nThese existing DP approaches, however, cannot model datasets with various types of covariates, including categorical and numerical labels. One reason is that categorical labels cannot be used to directly define the similarity between two documents, unlike temporal or spatial information. Also, labels and documents do not have a one-to-one correspondence, as there may be zero, one, or more labels per document. Furthermore, existing DP approaches cannot be applied to datasets with more than one type of covariates, for example numerical and categorical labels.\nWe suggest the hierarchical Dirichlet scaling process (HDSP) as a new way of modeling a corpus with various types of covariates such as categories, authors, and numerical ratings. The HDSP models the relationship between topics and covariates by generating dependent random measures in a hierarchy, where the first level is a Dirichlet process, and the second level is a Dirichlet scaling process (DSP). The first level DP is constructed in the traditional way of a stick breaking process, and the second level DSP with a normalized gamma process. With the normalized gamma process, each topic proportion of a document is independently drawn from a gamma distribution and then normalized. Unlike the stick breaking process, the normalized gamma process keeps the same order of the atoms as the first level measure, which allows the topic proportions in the random measure to be controlled. The DSP then uses that controllability to guide the topic proportions of a document by replacing the rate parameter of the gamma distribution with a scaling function that defines the correlation structure between topics and labels. The choice of the scaling function reflects the characteristics of the corpus. We show two scaling functions, the first one for a corpus with categorical labels, and the second for a corpus with both categorical and numerical labels.\nThe HDSP models the topic proportions of a document as a dependent variable of observable side information. This modeling approach differs from the traditional definition of a generative process where the observable variables are generated from a latent variable or parameter. For example, Zhu et al. (2009) and Mcauliffe and Blei (2007) propose generative processes where the observable labels are generated from a topic proportion of a document. However, a more natural model of the human writing process is to decide what to write about (e.g., categories) before writing the content of a document. This same approach is also successfully demonstrated in Mimno and McCallum (2012).\nThe outline of this paper is as follows. In Section 2, we describe related work and position our work within the topic modeling literature. In Section 3, we describe the gamma process construction of the HDP and how scale parameters are used to develop the HDSP with two different scaling functions. In Section 4, we derive a variational inference for the latent variables. In Section 5, we verify our approach on a synthetic dataset and demonstrate the improved predictive power on real world corpora. In Section 6, we discuss our conclusions and possible directions for future work."}, {"heading": "2. Related Work", "text": "For model construction, the model most closely related to HDSP is the discrete infinite logistic normal (DILN) model (Paisley et al., 2012) in which the correlations among topics are modeled through the normalized gamma construction. DILN allocates a latent location for each topic in the first level, and then draws the second level random measures from the normalized gamma construction of the DP. Those random measures are then scaled by an exponentiated Gaussian process defined on the latent locations. DILN is a nonparametric counterpart of the correlated topic model (Blei and Lafferty, 2007) in which the logistic normal prior is used to model the correlations between topics. The HDSP is also constructed through the normalized gamma distribution with an informative scaling parameter, but our goal in HDSP is to model the correlations between topics and labels.\nThe Dirichlet-multinomial regression topic model (DMR-TM) (Mimno and McCallum, 2012) also models the label dependent topic proportions of documents, but it is a parametric model. The DMR-TM places a log-linear prior on the parameter of the Dirichlet distribution to incorporate arbitrary types of observed labels. The DMR-TM takes the \u201cupstream\u201d approach in which the latent variable or latent topics are conditionally generated from the observed label information. The author-topic model (Rosen-Zvi et al., 2004) also takes the same approach, but it is a specialized model for authors of documents. Unlike the \u201cdownstream\u201d generative approach used in the supervised topic model (Mcauliffe and Blei, 2007), the maximum margin topic model (Zhu et al., 2009), and the relational topic model (Chang and Blei, 2009), the upstream approach does not require specifying the probability distribution over all possible values of observed labels.\nThe HDSP is a new way of constructing a dependent random measure in a hierarchy. In the field of Bayesian nonparametrics, the introduction of DDP (Sethuraman, 1991) has led to increased attention in constructing dependent random measures. Most such approaches develop priors to allow covariate dependent variation in the atoms of the random measure (Gelfand et al., 2005; Rao and Teh, 2009) or in the weights of atoms (Griffin and Steel, 2006; Duan et al., 2007; Dunson and Park, 2008). These priors replace the first level of the HDP to incorporate a document-specific covariate for generating a dependent topic proportion. These approaches focus on the spatial distances or the ordering of the covariate, so they cannot be generalized for arbitrary types of label information. The HDSP, on the other hand, can model any types of labels. The HDSP allows covariate dependent variation in the weights of atoms, where the variation is controlled by the scaling function that defines the correlation between atoms and labels. A proper definition of the scaling function gives the flexibility to model various types of labels.\nSeveral topic models for labeled documents use the credit attribution approach where each observed word token is assigned to one of the observed labels. Labeled LDA (L-LDA) allocates one dimension of the topic simplex per label and generates words from only the topics that correspond to the labels in each document (Ramage et al., 2009). An extension of this model, partially labeled LDA (PLDA), adds more flexibility by allocating a predefined number of topics per label and including a background label to handle documents with no labels (Ramage et al., 2011). The Dirichlet process with mixed random measures (DP-MRM) is a nonparametric topic model which generates an unbounded number of topics\nper label but still excludes topics from labels that are not observed in the document (Kim et al., 2012)."}, {"heading": "3. Hierarchical Dirichlet Scaling Process", "text": "In this section, we describe the hierarchical Dirichlet scaling process (HDSP). First we review the HDP with an alternative construction using the normalized gamma process construction for the second level DP. We then present the HDSP where the second level DP is replaced by Dirichlet scaling process (DSP). Finally, we describe two scaling functions for the DSP to incorporate categorical and numerical labels."}, {"heading": "3.1 The normalized gamma process construction of HDP", "text": "The HDP1 consists of two levels of the DP where the random measure drawn from the upper level DP is the base distribution of the lower level DP. The formal definition of the hierarchical representation is as follows:\nG0 \u223c DP(\u03b1,H), Gm \u223c DP(\u03b2,G0), (1)\nwhere H is a base distribution, \u03b1, and \u03b2 are concentration parameters for each level respectively, and index m represents multiple draws from the second level DP. For the mixed membership model, xmn, observation n in group m, can be drawn from\n\u03b8mn \u223c Gm, xmn \u223c f(\u03b8mn), (2)\nwhere f(\u00b7) is a data distribution parameterized by \u03b8. In the context of topic models, the base distribution H is usually a Dirichlet distribution over the vocabulary, so the atoms of the first level random measure G0 are an infinite set of topics drawn from H. The second level random measure Gm is distributed based on the first level random measure G0, so the second level shares the same set of topics, the atoms of the first level random measure.\nThe constructive definition of the DP can be represented as a stick breaking process (Sethuraman, 1991), and in the HDP inference algorithm based on stick breaking, the first level DP is given by the following conditional distributions:\nVk \u223c Beta(1, \u03b1) pk = Vk j<k\u220f j=1 (1\u2212 Vj)\n\u03c6k \u223c H G0 = \u221e\u2211 k=1 pk\u03b4\u03c6k , (3)\n1. In this paper, we limit our discussions of the HDP to the two level construction of the DP and refer to it simply as the HDP.\nwhere Vk defines a corpus level topic distribution for topic \u03c6k. The second level random measures are conditionally distributed on the first level discrete random measure G0:\n\u03c0ml \u223c Beta(1, \u03b2) pml = \u03c0ml j<l\u220f j=1 (1\u2212 \u03c0mj)\n\u03b8ml \u223c G0 Gm = \u221e\u2211 l=1 pml\u03b4\u03b8ml , (4)\nwhere the second level atom \u03b8ml corresponds to one of the first level atoms \u03c6k. This stick breaking construction is the most widely used method for the hierarchical construction (Wang et al., 2011; Teh et al., 2006).\nAn alternative construction of the HDP is based on the normalized gamma process (Paisley et al., 2012). While the first level construction remains the same, the gamma process changes the second level construction from Eq. 4 to\n\u03c0mk \u223c Gamma(\u03b2pk, 1)\nGm = \u221e\u2211 k=1 \u03c0mk\u2211\u221e j=1 \u03c0mj \u03b4\u03c6k, (5)\nwhere Gamma(x; a, b) = bax(a\u22121)e\u2212bx/\u0393(a). Unlike the stick breaking construction, the atom of the \u03c0mk of the gamma process is the same as the atom of the kth stick of the first level. Therefore, during inference, the model does not need to keep track of which second level atoms correspond to which first level atoms. Furthermore, by placing a proper random variable on the rate parameter of the gamma distribution, the model can infer the correlations among the topics (Paisley et al., 2012) through the Gaussian process (Rasmussen and Williams, 2005).\nThe normalized gamma process itself is not an appropriate construction method for the approximate posterior inference algorithm based on the variational truncation method (Blei and Jordan, 2006) because, unlike the stick breaking process, the probability mass of a random measure constructed by the normalized gamma process is not limited to the first few number of atoms. But once the base distribution of second level DP is constructed by the stick breaking process of first level DP, the total mass of the second level base distribution G0 is limited to the first few number of atoms, and then the truncation based posterior inference algorithm approximates the true posterior of the normalized gamma construction."}, {"heading": "3.2 Hierarchical Dirichlet scaling process", "text": "The HDSP generalizes the HDP by modeling mixture proportions dependent on covariates. As a topic model, the HDSP assumes that topics and labels are correlated, and the topic proportions of a document are proportional to the correlations between the topics and the observed labels of the document. We develop the Dirichlet scaling process (DSP) with the normalized gamma construction of the DP, where the rate parameter of the gamma distribution is replaced by the scaling function. This scaling function serves the central role of defining the correlation structure between a topic and labels. Formally, the HDSP\nconsists of DP and DSP in a hierarchy:\nG0 \u223c DP(\u03b1,H) (6) Gm \u223c DSP(\u03b2,G0, rm, sw(\u00b7)), (7)\nwhere the first level random measure G0 is drawn from the DP with concentration parameter \u03b1 and base distribution H. The second level random measure Gm for document m is drawn from the DSP parameterized by the concentration parameter \u03b2, base distribution G0, observed labels of document rm, and scaling function s(\u00b7) with scaling parameter w.\nAs in the HDP, the first level of HDSP is a DP where the base distribution is the product of two distributions for data distribution and scaling parameter w. Specifically, the base distribution H is Dir(\u03b7)\u2297Lw where \u03b7 is the parameter of the word-topic distribution, and Lw is a prior distribution for the scaling parameter w. The form of the resulting random measure is\nG0 = \u221e\u2211 k=1 pk\u03b4{\u03c6k,wk}, (8)\nwhere pk is the stick length for topic k, pk = Vk \u220fk\u2032<k k\u2032=1(1\u2212Vk\u2032) and {\u03c6k, wk} is the atom of stick k. At the second level construction, wk becomes the parameter to guide the proportion of topic k\u2019s for each document.\nAt the second level of HDSP, label-dependent random measures are drawn from the DSP. First, as in the HDP, draw a random measure G\u2032m \u223c DP(\u03b2,H) for document m. Second, scale the weights of the atoms based on a scaling function parameterized by wk and the observed labels. Let rmj be the value of observed label j in document m, then G \u2032 m is scaled as follows:\nGm({\u03c6k, lk}) \u221d G\u2032m({\u03c6k, lk})\u00d7 swk(rmj) (9)\nwhere swk(\u00b7) is the scaling function parameterized by the scaling parameter wk. Topic k is scaled by the scaling weight, swk(rmj), and therefore, the topic proportions of a document\nis proportional to the scaling weights of the observed labels. The scaling function should be carefully chosen to reflect the underlying relationship between topics and labels. We show two concrete examples of scaling functions in Section 3.3.\nThe constructive definition of HDSP is similar to the HDP, but the difference comes from the scaling function. The stick breaking process is used to construct the first level random measure:\nVk \u223c Beta(1, \u03b1) pk = Vk j<k\u220f j=1 (1\u2212 Vj)\n\u03c6k \u223c Dir(\u03b7), wk \u223c Lw G0 = \u221e\u2211 k=1 pk\u03b4{\u03c6k,wk}, (10)\nwhere the pair {\u03c6k, wk} drawn i.i.d. from two base distributions forms an atom of the resulting measure.\nBased on the discrete first level random measure, the second level random measure is constructed by the normalized gamma process. As in the HDP, the weight of atom k is drawn from a gamma distribution with parameter \u03b2pk, and then scaled by the scaling weight swk(rm)\n\u03c0mk \u223c Gamma(\u03b2pk, 1)\u00d7 swk(rm). (11)\nThe scaling weight can be directly incorporated into the second parameter of the gamma distribution because the scaled gamma random variable y = kx \u223c Gamma(a, 1) is equal to y \u223c Gamma(a, k\u22121),\n\u03c0mk \u223c Gamma(\u03b2pk, swk(rm)\u22121). (12)\nThen, the random variables are normalized to form a proper probability random measure\nGm = \u221e\u2211 k=1 \u03c0mk\u2211\u221e j=1 \u03c0mj \u03b4\u03c6k . (13)\nFor the mixed membership model, nth observation in mth group is drawn as follows:\n\u03c6k \u223c Gm, xmn \u223c f(\u03c6k), (14)\nwhere f is a data distribution parameterized by \u03c6k. For topic modeling, Gm and xmn correspond to document m and word n in document m, respectively."}, {"heading": "3.3 Scaling functions", "text": "Now we propose two scaling functions to express the correlation between topics and labels of documents. A scaling method is properly defined by two factors: 1) a proper prior over the scaling parameter wk, 2) a plausible scaling function between topic specific scaling parameter wk and the observed labels of document rm.\nScaling function 1: We design the first scaling function to model categorical side information such as authors, tags, and categories. For a corpus with J unique labels, then wk is a J-dimensional parameter where each dimension matches to a corresponding label. We define the scaling function as the product of scaling parameters that correspond to the observed labels:\nswk(rm) = J\u220f j=1 w rmj kj wkj \u223c inv-Gamma(aw, bw) (15)\nwhere rmj is an indicator variable whose value is one when label j is observed in document m and zero otherwise. wkj is a scaling parameter of topic k for label j. We place a inverse gamma prior over the weight variable wkj .\nWith this scaling function, the proportion of topic k for document m is scaled as follows:\n\u03c0mk \u223c Gamma(\u03b2pk, 1)\u00d7 J\u220f j=1 w rmj kj . (16)\nThe scaled gamma distribution is equal to the gamma distribution with the rate parameter of inverse scaling factor, so we can rewrite the above equation as follows:\n\u03c0mk \u223c Gamma(\u03b2pk, J\u220f j=1 w \u2212rmj kj ). (17)\nFinally, we normalize these random variables to make a probabilistic random measure summed up to unity for document m:\n\u03c0\u0304mk = \u03c0mk\u2211 k\u2032 \u03c0mk\u2032 . (18)\nScaling function 2: The above scaling function models categorical side information, but many datasets, such as product reviews have numerical ratings as well as categorical information. We propose the second scaling function that can model both numerical and categorical information. Again, let wk be J-dimensional scaling parameter where each dimension matches to a corresponding label. The second scaling function is defined as follows:\nswk(rm) = 1 exp( \u2211 j wkjrmj) , (19)\nwhere wkj is the scaling parameter of label j for topic k, and rmj is the observed value of label j of document m. We place a normal prior over the scaling parameter wk. The scaling function is an inverse log-linear to the weighted sum of document\u2019s labels. Unlike the previous scaling function which only considers whether a label is observed in a document, this scaling function incorporates the value of the observed label. With this scaling function, the proportion of topic k for document m is scaled as follows\n\u03c0mk \u223c Gamma(\u03b2pk, 1)\u00d7 1 exp( \u2211 j wkjrmj) . (20)\nAgain, we can rewrite this equations as \u03c0mk \u223c Gamma(\u03b2pk, exp( \u2211 j wkjrmj)). (21)\n\u03c0mk is proportional to the inverse weighted sum of observed labels. Again, we normalize \u03c0mk to construct a proper random measure.\nThe choice of scaling function reflects the modeler\u2019s perspective with respect to the underlying relationship between topics and labels. The first scaling function scales each topic by the product of the scaling parameters of the observed labels. This reflects the modeler\u2019s assumption that a document with a set of observed labels is likely to exhibit topics that have high correlation with all of the observed labels. With the second scaling function, the scaling weight changes exponentially as the value of label changes. This reflects the modeler\u2019s assumption that two documents with the same set of observed labels but with different values are likely to exhibit different topics."}, {"heading": "3.4 HDSP as a dependent Dirichlet process", "text": "We can view the HDSP as an alternative construction of the hierarchical dependent Dirichlet process (DDP) via a hierarchy consisting of a stick breaking process and a normalized gamma process. Let us compare the HDSP approach to the general DDP approach for topic modeling. The formal definition of DDP is:\nG0(\u00b7) \u223c DDP(\u03b1,H), (22)\nwhere the resulting random measure G0 is a function of some covariates. Using G0 as the base distribution of a DP for a document with a covariate, the random measure corresponding to document m is constructed as follows:\nGm \u223c DP(\u03b2,G0(rm)), (23)\nwhere G0(rm) is the base distribution for the document with same covariate rm (Srebro and Roweis, 2005).\nSimilarly, the HDSP constructs a dependent random measure with covariates. However, unlike the DDP-DP approach, G0 is no longer a function of covariates. The HDSP defines a single global random measure G0 and then scales G0 based on the covariates with the scaling function. The advantage of the HDSP is that it only requires a proper, but relatively simple, scaling function that reflects the correlation between covariates and topics, whereas the DDP requires a complex dependent process for different types of covariates (Griffin and Steel, 2006)."}, {"heading": "4. Variational Inference for HDSP", "text": "The posterior inference for Bayesian nonparametric models is important because it is intractable to compute the posterior over an infinite dimensional space. Approximation algorithms, such as marginalized MCMC (Escobar and West, 1995; Teh et al., 2006) and variational inference (Blei and Jordan, 2006; Teh et al., 2008), have been developed for\nthe Bayesian nonparametric mixture models. We develop a mean field variational inference (Jordan et al., 1999; Wainwright and Jordan, 2008) algorithm for approximate posterior inference of the HDSP topic model. The objective of variational inference is to minimize the KL divergence between a distribution over the hidden variables and the true posterior, which is equivalent to maximizing the lower bound of the marginal log likelihood of observed data.\nIn this section, we first derive the inference algorithm for the first scaling function with a fully factorized variational family. Variational inference algorithms can be easily modularized with the fully factorized variational family, and the variation in a model only affects the update rules for the modified parts of the model. Therefore, for the second scaling function, we only need to update the part of the inference algorithm related to the new scaling function."}, {"heading": "4.1 Variational inference for the first scaling function", "text": "For the first scaling function, we use a fully factorized variational distribution and perform a mean-field variational inference. There are five latent variables of interest: the corpus level stick proportion Vk, the document level stick proportion \u03c0mk, the scaling parameter between topic and label wkj , the topic assignment for each word zmn, and the word topic distribution \u03c6k. Thus the variational distribution q(z, \u03c0, V, w, \u03c6) can be factorized into\nq(z, \u03c0, V, w, \u03c6) = T\u220f k=1 M\u220f m=1 J\u220f j=1 Nm\u220f n=1 q(zmn)q(\u03c0mk)q(Vk)q(\u03c6k)q(wkj), (24)\nwhere the variational distributions are\nq(zmn) = Multinomial(zmn|\u03b3mn) q(\u03c0mk) = Gamma(\u03c0mk|a\u03c0mk, b\u03c0mk) q(Vk) = \u03b4Vk q(\u03c6k) = Dirichlet(\u03c6k|\u03b7k) q(wkj) = InvGamma(wkj |awkj , bwkj).\nFor the corpus level stick proportion Vk, we use the delta function as a variational distribution for simplicity and tractability in inference steps as demonstrated in (Liang et al., 2007). Infinite dimensions over the posterior is a key problem in Bayesian nonparametric models and requires an approximation method. In variational treatment, we truncate the unbounded dimensionality to T by letting VT = 1. Thus the model still keeps the infinite dimensionality while allowing approximation to be carried out under the bounded variational distributions.\nUsing standard variational theory, we derive the evidence lower bound (ELBO) of the marginal log likelihood of the observed data D = (xm, rm)Mm=1,\nlog p(D|\u03b1, \u03b2, aw, bw, \u03b7) \u2265 Eq[log p(D, z, \u03c0, V, w, \u03c6)] +H(q) = L(q), (25)\nwhere H(q) is the entropy for the variational distribution. By taking the derivative of this lower bound, we derive the following coordinate ascent algorithm.\nDocument-level Updates: At the document level, we update the variational distribution for the topic assignment zmn and the document level stick proportion \u03c0mk. The update for q(zmn|\u03b3mn) is\n\u03b3mnk \u221d exp (Eq[ln \u03b7k,xmn ] + Eq[ln\u03c0mk]) . (26)\nUpdating q(\u03c0mk|a\u03c0mk, b\u03c0mk) requires computing the expectation term E[ln \u2211T\nk=1 \u03c0mk]. Following Blei and Lafferty (2007), we approximate the lower bound of the expectation by using the first-order Taylor expansion,\n\u2212Eq[ln T\u2211 k=1 \u03c0mk] \u2265 \u2212 ln \u03bem \u2212 \u2211T k=1 Eq[\u03c0mk]\u2212 \u03bem \u03bem , (27)\nwhere the update for \u03bem = \u2211K k=1 Eq[\u03c0mk]. Then, the update for \u03c0mk is\na\u03c0mk = \u03b2pk + Nm\u2211 n=1 \u03b3mnk\nb\u03c0mk = \u220f j Eq[w \u2212rmj kj ] + Nm \u03bem . (28)\nNote again rmj is equal to 1 when jth label is observed in mth document, otherwise 0. Corpus-level Updates: At the corpus level, we update the variational distribution for the scaling parameter wkj , corpus level stick length Vk and word topic distribution \u03b7ki. The optimal form of a variational distribution can be obtained by exponentiating the variational lower bound with all expectations except the parameter of interest (Bishop and Nasrabadi, 2006). For wkj , we can derive the optimal form of variational distribution as follows\nq(wkj) \u223c InvGamma(a\u2032, b\u2032) (29) a\u2032 = Eq[\u03b2pk] \u2211 m rmj + a w\nb\u2032 = \u2211 m\u2032 \u220f j\u2032/j Eq[w\u22121j\u2032k]Eq[\u03c0m\u2032k] + b w,\nwhere m\u2032 = {m : rmj = 1} and j\u2032/j = {j\u2032 : rmj\u2032 = 1, j\u2032 6= j}. See Appendix A for the complete derivation. There is no closed form update for Vk, instead we use the steepest ascent algorithm to jointly optimize Vk. The gradient of Vk is\n\u2202L \u2202Vk = \u2212 \u03b1\u2212 1 1\u2212 Vk\n(30)\n\u2212 \u03b2pk Vk { \u2211 m,j rmjEq[ln\u03c0mk]\u2212 Eq[\u03c0mk] + \u03c8(\u03b2pk)}\n+ \u2211 k\u2032>k \u03b2pk\u2032 1\u2212 Vk { \u2211 m,j rmjEq[ln\u03c0mk\u2032 ]\u2212 Eq[\u03c0mk\u2032 ] + \u03c8(\u03b2pk\u2032)},\nwhere \u03c8(\u00b7) is a digamma function. Finally, the update for the word topic distribution q(\u03c6k|\u03b7k) is\n\u03b7ki = \u03b7 + \u2211 m,n \u03b3mnk1(xmn = i), (31)\nwhere i is a word index, and 1 is an indicator function (Blei et al., 2003). The expectations of latent variables under the variational distribution q are\nEq[\u03c0mk] = a\u03c0mk/b\u03c0mk Eq[ln\u03c0mk] = \u03c8(a\u03c0mk)\u2212 ln b\u03c0mk Eq[wkj ] = bwkj/(awkj \u2212 1) Eq[w\u22121kj ] = a w kj/b w kj Eq[lnwkj ] = ln bwkj \u2212 \u03c8(awkj) Eq[ln\u03c6ki] = \u03c8(\u03b7ki)\u2212 \u03c8(\n\u2211 i \u03b7ki)."}, {"heading": "4.2 Variational inference for the second scaling function", "text": "Introducing a new scaling function requires a new approximation method. We first choose the part of ELBO which requires new treatment as the scaling function changes. From Equation 25, we take the terms that are related to the scaling function s:\nLs = Eq[ M\u2211 m=1 \u221e\u2211 k=1 ln p(\u03c0mk|Vk, s, rm)] + Eq[ln p(s)]\u2212 Eq[ln q(s)] (32)\n= \u2211 m [\u03b2pkEq[ln s(rm)] + (\u03b2pk \u2212 1)Eq[ln\u03c0mk]\u2212 Eq[s(rm)]Eq[\u03c0mk]\u2212 ln \u0393(\u03b2pk)]\n+ Eq[p(s)]\u2212 Eq[q(s)].\nTo update the scaling parameters, we need a proper prior and variational distribution. For the second scaling function, the normal distribution with zero mean and variance \u03c3 is used as a prior of wkj , and the delta function is used as the variational distribution of wkj . Newton-Raphson optimization method are used to update the weight parameters. The Newton-Raphson optimization finds a stationary point of a function by iterating:\nwnewk \u2190 woldk \u2212H(wk)\u22121 \u2202L \u2202wk , (33)\nwhere H(wk) and \u2202` \u2202wk are the Hessian matrix and gradient at the point woldk . The lower bound with respect to the parameter wkj is,\nLwkj = \u2211 m \u03b2pk\u2211 j wkjrmj + (\u03b2pk \u2212 1)Eq[ln\u03c0mk]\u2212 exp(w>k rm)Eq[\u03c0mk]\u2212 ln \u0393(\u03b2pk)  . (34)\nThen, the gradient and Hessian matrix of wkj are\n\u2202L \u2202wkj = \u2211 m [ \u03b2pkrmj \u2212 rmj exp(w>k rm)Eq[\u03c0mk] ] \u2212 wjk \u03c3 (35)\n\u22022L \u2202wkj\u2032\u2202wkj = \u2211 m [ \u2212rmj\u2032rmj exp(w>k rm)Eq[\u03c0mk] ] \u2212 1(j = j\u2032)\u03c3\u22121. (36)\nBecause the wkj is depends on both label j and topic k, we iteratively update wkj until converged.\nThe update rules for the variational parameter of \u03c0mk and Vk need to be changed to accommodate the change of scaling function. The variational parameters of \u03c0mk are approximated by using the first-order Taylor expansion,\na\u03c0mk = \u03b2pk + Nm\u2211 n=1 \u03b3mnk (37)\nb\u03c0mk = Eq[ln s(rm)] + Nm \u03bem ,\nwhere \u03bem is \u2211K\nk=1 Eq[\u03c0mk]. To update Vk, we take the same approach in which the variational distribution is a delta function of current Vk. Again, we use the steepest ascent algorithm to jointly optimize Vk, and the gradient of Vk is\n\u2202L \u2202Vk = \u2212 \u03b1\u2212 1 1\u2212 Vk \u2212 \u03b2pk Vk {\u2211 m Eq[ln s(rm)]Eq[ln\u03c0mk]\u2212 Eq[\u03c0mk] + \u03c8(\u03b2pk) }\n(38)\n+ \u2211 k\u2032>k \u03b2pk\u2032 1\u2212 Vk {\u2211 m Eq[ln s(rm)]Eq[ln\u03c0mk\u2032 ]\u2212 Eq[\u03c0mk\u2032 ] + \u03c8(\u03b2pk\u2032) } .\nThe update rules for \u03c0mk and Vk only requires the expectation of the log scaling function. The update rules for the other parameters remain the same as the previous section.\nIntroducing a new scaling function requires a new inference algorithm, and this can be cumbersome. Once one defines a tractable expectation of the log of a scaling function, a recently suggested Black-box method (Ranganath et al., 2014) can be an alternative to update the function-specific parameters instead of deriving function-specific approximation algorithms."}, {"heading": "5. Experiments", "text": "In this section, we describe how the HDSP performs with real and synthetic data. We fit the HDSP topic model with three different types of data and compare the results with several comparison models. First, we test the model with synthetic data to verify the approximate inference. Second, we train the model with categorical data whose label information is represented by binary values. Third, we train the model with mixed-type of data whose label information has both numerical and categorical values."}, {"heading": "5.1 Synthetic data", "text": "There is no naturally-occurring dataset with the observable weights between topics and labels, so we synthesize data based on the model assumptions to verify our model and the approximate inference. First, we check the difference between the original topics and the inferred topics via simple visualization. Then, we focus on the differences between the inferred and synthetic weights. For all experiments with synthetic data, the datasets are generated by following the model assumptions with the first scaling function, and the posterior inferences are done with the first scaling function. We set the truncation level T at twice the number of topics. We terminate the variational inference when the fractional change of the lower bound falls below 10\u22123, and we average all results over 10 individual runs with different initializations.\nWith the first experiment, we show that HDSP correctly recovers the underlying topics and scaling parameter between topics and labels. For the dataset, we generate 500 documents using the following steps. We define five topics over ten terms shown in Figure 2(a)\nand the scaling parameter of five topics and four labels shown in Figure 2(d). For each document, we randomly draw Nm from the Poisson distribution and rmj from the Bernoulli distribution. The average length of a document is 20, and the average number of labels per document is 2. We generate topic proportions of corpus and documents by using Equations 10 and 13. For each word in a document, we draw the topic and the word by using Equation 14. We set both \u03b1 and \u03b2 to 1.\nFigure 2 shows the results of the HDP and the HDSP on the synthetic dataset. Figure 2(b) and Figure 2(c) are the heat maps of topics inferred from each model. We match the inferred topics to the original topics using KL divergence between the two sets of topic distributions. There are no significant differences between the inferred topics of HDSP and HDP. In addition to the topics, HDSP infers the scaling parameters between topics and labels, which are shown in Figure 2(e). The results show that the relative differences between original scaling parameters are preserved in the inferred parameters through the variational inference.\nWith the second experiment, we show that the inferred parameters preserve the relative differences between labels and topics in the dataset. For this experiment, we generate 1,000 documents with ten randomly drawn topics from Dirichlet(0.1) with the vocabulary size of 20. To generate the weights between topics and labels, we randomly place the topics and labels into three dimensional euclidean space, and use the distance between a topic and label as a scaling parameter. The location of topics and labels are uniformly drawn from three dimensional euclidean space, so the total volume is x3, then we vary the x value\nfrom 1 to 20 for each experiment. As the volume of space increases, the potential scaling parameter between a topic and label increases, and the scaling effect on a topic proportion of document also increases.\nWe compute the mean absolute error (MAE) and the spearman\u2019s rank correlation coefficient \u03c1 between the original parameters and the inferred parameters. The spearman\u2019s \u03c1 is designed to measure the ranking correlation of two lists. Figure 3 shows the results. The MAE increases as the volume of the space increases. However, spearman\u2019s \u03c1 stabilizes, indicating that the relative differences are preserved even when the MAE increases. Since there are an infinite number of configurations of scaling parameters that generate the same expectation E[p(\u03c0m|\u03b2p,wj)] given \u03c0m and \u03b2p, preserving the relative differences verifies our model\u2019s capability of capturing the underlying structure of topics and labels."}, {"heading": "5.2 Categorical data", "text": "We evaluate the performance of HDSP and compare it with the HDP, labeled LDA (L-LDA), partially labeled LDA (PLDA), and author-topic model (ATM). For the HDSP, we use both scaling functions and denote the model with the second scaling function as wHDSP. We use three multi-labeled corpora: RCV2 (newswire from Reuter\u2019s), OHSUMED3 (a subset of the Medline journal articles), and NIPS (proceedings of NIPS conference). For RCV and OHSUMED, we use multi-category information of documents as labels, and for NIPS, we use authors of papers as labels. The average number of labels per article is 3.2 for RCV, 5.2 for OHSUMED, and 2.4 for NIPS. Table 1 contains the details of the datasets."}, {"heading": "5.2.1 Experimental settings", "text": "For the HDP and HDSP, we initialize the word-topic distribution with three iterations of LDA for fast convergence to the posterior while preventing the posterior from falling into a local mode of LDA and then reorder these topics by the size of the posterior word count. For all experiments, we set the truncation level T to 200. We terminate variational inference when the fractional change of the lower bound falls below 10\u22123, and we optimize all hyperparameters during inference except \u03b7. For the L-LDA and PLDA, we implement the collapsed Gibbs sampling algorithm. For each model, we run 5,000 iterations, the first 3,000 as burn-in and then using the samples thereafter with gaps of 100 iterations. For PLDA, we set the number of topics for each label to two and five (PLDA-2, PLDA-5). For\n2. http://trec.nist.gov/data/reuters/reuters.html 3. http://ir.ohsu.edu/ohsumed/ohsumed.html\nthe ATM, we set the number of topics to 50, 100, and 150. We try five different values for the topic Dirichlet parameter \u03b7: \u03b7 = 0.1, 0.25, 0.5, 0.75, 1.0. Finally all results are averaged over 20 runs with different random initialization. We do not report the standard errors because they are small enough to ignore."}, {"heading": "5.2.2 Evaluation metric", "text": "The goal of our model is to construct the dependent random probability measure given multiple labels. Therefore, our interest is to see the increments of predictive performance when the label information is given.\nThe predictive probability given label information for held-out documents are approximated by the conditional marginal,\np(x\u2032|r\u2032,Dtrain) = (39)\u222b q N\u220f n=1 T\u2211 k=1 p(x\u2032n|\u03c6k)p(z\u2032n = k|\u03c0\u2032)p(\u03c0\u2032|V, r\u2032)dq(V,w, \u03c6),\nwhere Dtrain = {xtrain, rtrain} is the training data, x\u2032 is the vector of N words of a held-out document, r\u2032 are the labels of the held-out document, z\u2032n is the latent topic of word n, and \u03c0\u2032k is the kth topic proportion of the held-out document. Since the integral is intractable, we approximate the probability\np(x\u2032|r\u2032,Dtrain) \u2248 N\u220f n=1 T\u2211 k=1 \u03c0\u0303k\u03c6\u0303k,x\u2032n , (40)\nwhere \u03c6\u0303k and \u03c0\u0303k are the variational expectations of \u03c6k and \u03c0k given label r \u2032. This approximated likelihood is then used to compute the perplexity of the held-out document\nperplexity = exp {\u2212 ln p(x\u2032|r\u2032,Dtrain) N } . (41)\nLower perplexity indicates better performance. We also take the same approach to compute the perplexity for L-LDA, PLDA and HDP, but HDP does not use the labels of held-out documents. To measure the predictive performance, we leave 20% of the documents for testing and use the remaining 80% to train the models."}, {"heading": "5.2.3 Experimental results", "text": "Figure 4 shows the predictive performance of our model against the comparison models. For the OHSUMED and RCV corpora, both HDSP and wHDSP outperform all others. Among these models, L-LDA restricts the modeling flexibility the most; the PLDA relaxes that restriction by adding an additional latent label and allowing multiple topics per label. HDSP and wHDSP further increase the modeling flexibility by allowing all topics to be generated from each label. This is reflected in the results of predictive performance of the three models; L-LDA shows the worst performance, then PLDA, and HDSP and wHDSP show the lowest perplexity. For the NIPS data, we compare HDSP and wHDSP to ATM, and again, HDSP and wHDSP show the lowest perplexity.\nTo visualize the relationship between topics and labels, we embed the inferred topics and the labels into the two dimensional euclidean space by using multidimensional scaling (Kruskal, 1964) on the inferred parameters of HDSP. In Figure 5, we choose and display a few representative topics and authors from NIPS. For instance, Geoffrey Hinton and Yann LeCun are closely located to the neural network related topics such as \u2018learning, network error\u2019 and \u2018recognition character, network\u2019, and the reinforcement learning researcher Richard Sutton is closely located to the \u2018state, learning policy\u2019 topic. Figure 6 shows the embedded labels and topics from OHSUMED. The labels \u2018Preschool\u2019, \u2018Pregnancy\u2019, and \u2018Infant\u2019 are closely located to one another with similar topics. While the model explicitly models the\ncorrelation between topics and labels, embedding them together shows that the correlation among labels, as well as among topics, can also be inferred.\nFigure 7 and 8 show the expected topic distributions of HDSP given different sets of labels. When multiple labels are given, the model expects high probabilities for the topics that are similar to all given labels. For example, when \u2018Market\u2019 and \u2018Sports\u2019 labels are given, the model expects high probabilities on sports related topics and relatively high probability on \u2018Market\u2019 related topics based on the weights between topics and two labels."}, {"heading": "5.2.4 Modeling data with missing labels", "text": "We also test our model with partially labeled data which have not been previously covered in topic modeling. Many real-world data fall into this category where some of the data are labeled, others are incompletely labeled, and the rest are unlabeled. For this experiment, we randomly remove existing labels from the RCV and OHSUMED corpora. To remove observed labels in the training corpus, we use Bernoulli trials with varying parameters to analyze how the proportion of observed labels affects the heldout predictive performance of the model.\nFigure 9 shows the predictive perplexity with varying parameters of Bernoulli distribution from 0.1 to 0.9. For both scaling functions, the perplexity decreases as the model observes more labels. Compared to the PLDA (with the parameter setting for optimal performance), the HDSP achieves similar perplexity with only 20% of the labels. One notable\nphenomenon is that the HDSP outperforms wHDSP on both datasets when the number of observed labels is less than 50% of the total number of labels."}, {"heading": "5.3 Mixed-Type data", "text": "In this section, we present the performance of the second scaling function with a corpus of product reviews which has real-valued ratings and category information.\nThe first scaling function is only applicable to categorical side information, so we use the second scaling function (wHDSP) which can model numerical as well as categorical side information of documents. To evaluate the performance of wHDSP with numerical side information, we train the model with the Amazon review data collected from seven categories of electronic products: air conditioner, canister vacuum, coffee machine, digital SLR, laptop, MP3 player, and space heater. Amazon uses a five-star rating system, so each review contains one numerical rating ranging from one to five. Table 2 shows the number of reviews for each rating and category. Recall that r is a vector whose values denote the observation of the labels. For each review, we set the dimension of r to eight in which the first dimension is a numerical rating of a review, and then the remaining seven dimensions match the seven product categories. We set the value of each dimension to one if the review belongs to the corresponding category, and zero otherwise.\nTo evaluate the performance of wHDSP, we classify the ratings of the reviews based on a trained model. We use 90% of the corpus to train models and the remaining 10% of the corpus to test the models. To classify the rating of each review in the test set,\nwe compute the perplexity of the given review with varying ratings from one to five, and choose the rating that shows the lowest perplexity. Generally, computing the perplexity of\nheldout document requires complex approximation schemes (Wallach et al., 2009), but we compute the perplexity based on the expected topic distribution given category and rating information, which requires a finite number of computations.\nWe compare the wHDSP with the supervised LDA (SLDA), LDA+SVM, as well as classifiers Naive Bayes, SVM, and decision trees (CART). For the LDA+SVM approach, we first train the LDA model and then use the inferred topic proportion and categories as features of the SVM. For the SLDA model, the category information cannot be used because the model is designed to learn and predict the single response variable. For both models, we set the number of topics to 50, 100, and 200.\nIn many applications, classifying negative feedback of users is more important than classifying positive feedback. From the negative feedback, companies can identify possible\nproblems of their products and services and use the information to design their next product or improve their services. In most online reviews, however, the proportion of negative feedback is smaller than the proportion of positive feedback. For example, in the Amazon data, about 51% of reviews are rated as five-star, and 72% rated as four or five. A classifier trained by such skewed data is likely to be biased toward the majority class.\nWe report the classification results of Amazon dataset in Table 3 and Table 4. Table 3 shows the results for each rating in terms of F1, and Table 4 shows the results in terms of micro and macro F1. The wHDSP outperforms the other models in terms of macro F1 but performs worse than sLDA in terms of micro F1. As we noted earlier, classifying negative reviews may be more important in many applications. Both the SLDA with 100 topics and the wHDSP are comparable in classifying the most negative (one-star) reviews. However, the confusion matrices and Table 3 indicate that the SLDA dichotomously learns the decision boundaries where the most reviews are classified into one-star or five-star. For example, the SLDA with 50 and 100 topics did not classify any two-star and three-star reviews correctly. The wHDSP learns the decision boundaries for classifying subtle differences between fivestar rating reviews. These patterns are shown clearly with the confusion matrices in Figure 10 where the diagonal entries are the numbers of correctly classified reviews. For example, the wHDSP classified only eight one-star reviews as five-star reviews, but the SLDA50 assigned 68 reviews as five-star reviews.\nWe perform a rating prediction task with and without the category information of reviews to see the effect of using both the category and rating information on the wHDSP and LDA+SVM approaches. The results represented by wHDSP* in Table 4 and Figure 10(b) show the performances of rating prediction with the wHDSP trained without category information. For wHDSP*, the model performs worse than wHDSP, which indicates the model, without category information, cannot distinguish the review ratings which depend\non topical context. The LDA+SVM without categories achieves 0.309 macro F1 and 0.533 micro F1, which are comparable to the LDA+SVM with the category information. Unlike the wHDSP, the decision boundaries of SVM are not improved with the additional category information. The result supports that for learning decision boundaries between ratings over different categories, the approach of including category information to train topics is more effective than using topics and the category information independently."}, {"heading": "6. Discussions", "text": "We have presented the hierarchical Dirichlet scaling process (HDSP), a Bayesian nonparametric prior for a mixed membership model that lets us analyze underlying semantics and observable side information. The combination of the stick breaking process with the normalized gamma process in HDSP is a more controllable construction of the hierarchical Dirichlet process because each atom of the second level measure inherits from the first level measure in order. HDSP also allows more flexibility and the capability of modeling side information by the scaling functions that plug into the rate parameter of the gamma distribution. The choice of the scaling function is the most important part of the model in terms of establishing a link between topics and observed labels. We developed two scaling functions but the choice of scaling function depends on the modeler\u2019s intention. For example, the well known linking functions from the generalized linear model can be used as scaling functions, or one can use several scaling functions together on purpose. We showed that the application of HDSP to topic modeling correctly recovers the topics and topic-label weights of synthetic data. Experiments with the real dataset show that the first scaling function is more suited for partially labeled data, and the second first scaling function is more suited for a dataset with both numerical and categorical labels.\nHierarchical Dirichlet scaling process opens up a number of interesting research questions that should be addressed in future work. First, in the two scaling functions we proposed to model the correlation structure between topics and side information, we simply defined the relationship between topic k and label j through the scaling parameter wkj . However, this approach does not consider the correlation within topics and labels. Taking inspiration from previous work (Blei and Lafferty, 2007; Mimno et al., 2007; Paisley et al., 2012) that showed correlations among topics, we can define a scaling function with a prior over the topics and labels to capture their complex relationships. Second, our posterior inference algorithm based on mean-field variational inference is tested with tens of thousands documents. However, modern data analysis requires inference of massive and/or streaming data. For a fast and efficient posterior inference, we can apply parallel or distributed algorithms based on a stochastic update (Hoffman et al., 2013; Ahn et al., 2014). Furthermore, we fix the number of labels before training but we need to find a way to model the unbounded number of labels for streaming data."}, {"heading": "Appendix A. Variational inference for HDSP", "text": "In this section, we provide the detailed derivation for mean-field variational inference for HDSP with the first scaling function. First, the evidence of lower bound for HDSP is obtained by taking a Jensen\u2019s inequality on the marginal log likelihood of the observed\ndata,\nln \u222b p(D,\u0398)d\u0398 \u2265 \u222b Q(\u03a8) ln\nP (D,\u0398) Q(\u03a8) d\u0398, (42)\nwhere D is the training set of documents and labels. \u03a8 denotes the set of variational parameters, \u0398 denotes the set of model parameters.\nWe define a fully factorized variational distribution Q as follows:\nQ := T\u220f k=1 q(\u03c6k)q(Vk) J\u220f j=1 q(wkj) M\u220f m=1 q(\u03c0mk) Nm\u220f n=1 q(zmn), (43)\nwhere\nq(zmn) = Multinomial(zmn|\u03b3mn1, \u03b3mn2, ..., \u03b3mnT ) (44) q(\u03c0mk) = Gamma(\u03c0mk|a\u03c0mk, b\u03c0mk) q(wkj) = InvGamma(wkj |awkj , bwkj) q(\u03c6k) = Dirichlet(\u03c6k|\u03b7k1, \u03b7k2, ..., \u03b7kI) q(Vk) = \u03b4Vk .\nThe evidence of lower bound (ELBO) is\nL(D,\u03a8) = Eq[ln p(D,\u03a8)] + H[Q] (45)\n= Eq[ M\u2211 m=1 Nm\u2211 n=1 ln p(xmn|zmn,\u03a6)] + Eq[ M\u2211 m=1 Nm\u2211 n=1 ln p(zmn|\u03c0m)]\n+ Eq[ M\u2211 m=1 \u221e\u2211 k=1 ln p(\u03c0mk|Vk, wk, rm)] + Eq[ \u221e\u2211 k=1 ln p(Vk|\u03b1)] + Eq[ J\u2211 j=1 \u221e\u2211 k=1 ln p(wkj |aw, bw)]\n+ Eq[ \u221e\u2211 k=1 ln p(\u03c6k|\u03b7)]\u2212 Eq[lnQ]\n= M\u2211 m=1 Nm\u2211 n=1 T\u2211 k=1 \u03b3mnkEq[ln p(xmn|\u03c6k)] + M\u2211 m=1 N\u2211 n=1 T\u2211 k=1 \u03b3mnkEq[ln p(zmn = k|\u03c0m)]\n+ M\u2211 m=1 T\u2211 k=1 Eq[ln p(\u03c0mk|Vk, wk, rm)] + T\u2211 k=1 Eq[ln p(Vk|\u03b1)] + \u221e\u2211 k=1 J\u2211 j=1 Eq[ln p(wkj |aw, bw)]\u2212 Eq[lnQ]\nwhere the expectations of latent variables under the variational distribution Q are\nEq[\u03c0mk] = a\u03c0mk/b\u03c0mk Eq[ln\u03c0mk] = \u03c8(a\u03c0mk)\u2212 ln b\u03c0mk Eq[wkj ] = bwkj/(awkj \u2212 1) Eq[w\u22121kj ] = a w kj/b w kj Eq[lnwkj ] = ln bwkj \u2212 \u03c8(awkj)\nEq[ln\u03c6ki] = \u03c8(\u03b7kd)\u2212 \u03c8( \u2032\u2211 i \u03c8ki\u2032)\nThen, we derive the equations further\nL(D,\u03a8) = M\u2211 m=1 Nm\u2211 n=1 T\u2211 k=1 \u03b3mnk{\u03c8(\u03b7kxmn)\u2212 \u03c8( \u2211 d \u03b7kd)} (46)\n+ M\u2211 m=1 Nm\u2211 n=1 T\u2211 k=1 \u03b3mnk{Eq[ln\u03c0mk]\u2212 Eq[ln T\u2211 k=1 \u03c0mk]}\n+ M\u2211 m=1 T\u2211 k=1 \u2212\u03b2pk \u2211 j rmj{ln(bwkj)\u2212 \u03c8(awkj)}+ (\u03b2pk \u2212 1){\u03c8(a\u03c0mk)\u2212 ln(b\u03c0mk)}\n\u2212 \u220f j ( awkj bwkj )rmj a\u03c0mk b\u03c0mk \u2212 ln \u0393(\u03b2pk)\n+ T\u2211 k=1 ln \u0393(\u03b1+ 1)\u2212 ln \u0393(\u03b1) + (\u03b1\u2212 1) ln(1\u2212 Vk)\n+ T\u2211 k=1 J\u2211 j=1 aw ln bw \u2212 ln \u0393(aw)\u2212 (aw + 1){ln bw \u2212 \u03c8(aw)} \u2212 aw \u2212 Eq[lnQ].\nTaking the derivatives of this lower bound with respect to each variational parameter, we can obtain the coordinate ascent updates.\nThe optimal form of the variational distribution can be obtained by exponentiating the variational lower bound with all expectations except the parameter of interest (Bishop and Nasrabadi, 2006). For \u03c0mk, we can derive the optimal form of variational distribution as follows\nq(\u03c0mk) \u221d exp { Eq\u2212\u03c0mk [ln p(\u03c0mk|z, \u03c0\u2212mk, rm, V )] } (47)\n\u221d exp { Eq\u2212\u03c0mk [ln p(z|\u03c0m) + ln p(\u03c0m|rm, V )] } \u221d \u03c0\u03b2pk+ \u2211Nm n=1 \u03b3mnk\u22121\nmk e (b\u03c0mk\n\u220f j Eq [w \u2212rmj kj ]+ Nm \u03bem )\u03c0mk\nwhere update for \u03bem is \u2212 ln \u03bem \u2212 ( \u2211T\nk=1 Eq[\u03c0mk]\u2212 \u03bem)/\u03bem. Therefore, the optimal form of variational distribution for \u03c0mk is\nq(\u03c0mk) \u223c Gamma(\u03b2pk + Nm\u2211 n=1 \u03b3mnk, b \u03c0 mk \u220f j Eq[w \u2212rmj kj ] + Nm \u03bem ). (48)\nWe take the same approach described in (Paisley et al., 2012), and the only difference comes from the product of the inverse distance term.\nFor wkj , we can derive the optimal form of the variational distribution as follows\nq(wkj) \u221d exp { Eq\u2212wkj [ln p(wkj |\u03c0,w\u2212jk, a w, bw)] }\n(49)\n\u221d exp Eq\u2212wkj [ M\u2211 m=1 ln p(\u03c0mk|\u03b2pk, J\u220f j=1 w \u2212rmj kj ) + ln p(wkj |aw, bw)]  \u221d exp\nEq\u2212wkj [\u2212\u03b2pk M\u2211 m=1 rmj lnwkj \u2212 \u2211 m \u220f j w \u2212rmj kj \u03c0mk \u2212 (aw + 1) lnwkj \u2212 bw wkj ]  \u221d w\u2212Eq [\u03b2pk] \u2211M m=1 rmj\u2212aw\u22121\nkj e (\u2212\n\u2211 {m:rmj=1} \u220f {j\u2032:rmj\u2032=1/j} Eq [w\u22121j\u2032k]Eq [\u03c0mk]\u2212b w) 1 wkj\nTherefore, the optimal form of variational distribution for wkj is\nq(wkj) \u223c InvGamma(Eq[\u03b2pk] \u2211 m rmj + a w, \u2211 m\u2032 \u220f j\u2032/j Eq[w\u22121j\u2032k]Eq[\u03c0m\u2032k] + b w) (50)\nwhere m\u2032 = {m : rmj = 1} and j\u2032/j = {j\u2032 : rmj\u2032 = 1, j\u2032 6= j}."}, {"heading": "B. Posterior Word Count", "text": "Like the HDP and other nonparametric topic models, our model also uses only a few topics even though we set the truncation level to 200. Figure 11 shows the posterior word count for the different values of the Dirichlet topic parameter \u03b7. As the result indicates our model uses 50 to 100 topics. The HDSP tends to use more topics than the HDP."}], "references": [{"title": "Timeline: A dynamic hierarchical dirichlet process model for recovering birth/death and evolution of topics in text stream", "author": ["Amr Ahmed", "Eric P. Xing"], "venue": "In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Ahmed and Xing.,? \\Q2010\\E", "shortCiteRegEx": "Ahmed and Xing.", "year": 2010}, {"title": "Distributed stochastic gradient mcmc", "author": ["Sungjin Ahn", "Babak Shahbaba", "Max Welling"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML),", "citeRegEx": "Ahn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2014}, {"title": "Pattern recognition and machine learning, volume 1. springer", "author": ["Christopher M Bishop", "Nasser M Nasrabadi"], "venue": "New York,", "citeRegEx": "Bishop and Nasrabadi.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and Nasrabadi.", "year": 2006}, {"title": "Variational inference for dirichlet process mixtures", "author": ["David M Blei", "Michael I Jordan"], "venue": "Bayesian Analysis,", "citeRegEx": "Blei and Jordan.,? \\Q2006\\E", "shortCiteRegEx": "Blei and Jordan.", "year": 2006}, {"title": "A correlated topic model of science", "author": ["David M Blei", "John D Lafferty"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Blei and Lafferty.,? \\Q2007\\E", "shortCiteRegEx": "Blei and Lafferty.", "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Relational topic models for document networks", "author": ["Jonathan Chang", "David M Blei"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Chang and Blei.,? \\Q2009\\E", "shortCiteRegEx": "Chang and Blei.", "year": 2009}, {"title": "Generalized spatial dirichlet process models", "author": ["Jason A Duan", "Michele Guindani", "Alan E Gelfand"], "venue": null, "citeRegEx": "Duan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2007}, {"title": "Bayesian density estimation and inference using mixtures", "author": ["Michael D Escobar", "Mike West"], "venue": "Journal of the american statistical association,", "citeRegEx": "Escobar and West.,? \\Q1995\\E", "shortCiteRegEx": "Escobar and West.", "year": 1995}, {"title": "Bayesian nonparametric spatial modeling with dirichlet process mixing", "author": ["Alan E Gelfand", "Athanasios Kottas", "Steven N MacEachern"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gelfand et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gelfand et al\\.", "year": 2005}, {"title": "Order-based dependent dirichlet processes", "author": ["Jim E Griffin", "MF J Steel"], "venue": "Journal of the American statistical Association,", "citeRegEx": "Griffin and Steel.,? \\Q2006\\E", "shortCiteRegEx": "Griffin and Steel.", "year": 2006}, {"title": "Stochastic variational inference", "author": ["Matthew D Hoffman", "David M Blei", "Chong Wang", "John Paisley"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "An introduction to variational methods for graphical models", "author": ["Michael I. Jordan", "Zoubin Ghahramani", "Tommi S. Jaakkola", "Lawrence K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Dirichlet process with mixed random measures: a nonparametric topic model for labeled data", "author": ["Dongwoo Kim", "Suin Kim", "Alice Oh"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "Kim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis", "author": ["Joseph B Kruskal"], "venue": null, "citeRegEx": "Kruskal.,? \\Q1964\\E", "shortCiteRegEx": "Kruskal.", "year": 1964}, {"title": "The infinite pcfg using hierarchical dirichlet processes", "author": ["Percy Liang", "Slav Petrov", "Michael I Jordan", "Dan Klein"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "Liang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2007}, {"title": "Dependent nonparametric processes", "author": ["Steven N MacEachern"], "venue": "In ASA Proceedings of the Section on Bayesian Statistical Science,", "citeRegEx": "MacEachern.,? \\Q1999\\E", "shortCiteRegEx": "MacEachern.", "year": 1999}, {"title": "Supervised topic models", "author": ["Jon D Mcauliffe", "David M Blei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mcauliffe and Blei.,? \\Q2007\\E", "shortCiteRegEx": "Mcauliffe and Blei.", "year": 2007}, {"title": "Generalized linear models", "author": ["Peter McCullagh"], "venue": "European Journal of Operational Research,", "citeRegEx": "McCullagh.,? \\Q1984\\E", "shortCiteRegEx": "McCullagh.", "year": 1984}, {"title": "Topic models conditioned on arbitrary features with dirichlet-multinomial regression", "author": ["David Mimno", "Andrew McCallum"], "venue": "arXiv preprint arXiv:1206.3278,", "citeRegEx": "Mimno and McCallum.,? \\Q2012\\E", "shortCiteRegEx": "Mimno and McCallum.", "year": 2012}, {"title": "Mixtures of hierarchical topics with pachinko allocation", "author": ["David Mimno", "Wei Li", "Andrew McCallum"], "venue": "In Proceedings of the 24th international conference on Machine learning (ICML),", "citeRegEx": "Mimno et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2007}, {"title": "The discrete infinite logistic normal distribution", "author": ["John Paisley", "Chong Wang", "David M Blei"], "venue": "Bayesian Analysis,", "citeRegEx": "Paisley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2012}, {"title": "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["Daniel Ramage", "David Hall", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume", "citeRegEx": "Ramage et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ramage et al\\.", "year": 2009}, {"title": "Partially labeled topic models for interpretable text mining", "author": ["Daniel Ramage", "Christopher D. Manning", "Susan Dumais"], "venue": "In Proceedings of the 17th ACM International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Ramage et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ramage et al\\.", "year": 2011}, {"title": "Black Box Variational Inference", "author": ["Rajesh Ranganath", "Sean Gerrish", "David Blei"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ranganath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2014}, {"title": "Spatial normalized gamma processes", "author": ["Vinayak Rao", "Yee W Teh"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Rao and Teh.,? \\Q2009\\E", "shortCiteRegEx": "Rao and Teh.", "year": 2009}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["Carl Edward Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2005\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2005}, {"title": "Logistic stick-breaking process", "author": ["Lu Ren", "Lan Du", "Lawrence Carin", "David Dunson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ren et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2011}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": null, "citeRegEx": "Rosen.Zvi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "A constructive definition of dirichlet priors", "author": ["Jayaram Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "Sethuraman.,? \\Q1991\\E", "shortCiteRegEx": "Sethuraman.", "year": 1991}, {"title": "Time-varying topic models using dependent dirichlet processes", "author": ["Nathan Srebro", "Sam Roweis"], "venue": "UTML, TR# 2005,", "citeRegEx": "Srebro and Roweis.,? \\Q2005\\E", "shortCiteRegEx": "Srebro and Roweis.", "year": 2005}, {"title": "Hierarchical dirichlet processes", "author": ["Yee Whye Teh", "Michael I Jordan", "Matthew J Beal", "David M Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Collapsed variational inference for HDP", "author": ["Yee Whye Teh", "Kenichi Kurihara", "Max Welling"], "venue": null, "citeRegEx": "Teh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2008}, {"title": "Evaluation methods for topic models", "author": ["Hanna M Wallach", "Iain Murray", "Ruslan Salakhutdinov", "David Mimno"], "venue": "Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Online variational inference for the hierarchical dirichlet process", "author": ["Chong Wang", "John W Paisley", "David M Blei"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Medlda: maximum margin supervised topic models for regression and classification", "author": ["Jun Zhu", "Amr Ahmed", "Eric P Xing"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Zhu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 31, "context": "Introduction The hierarchical Dirichlet process (HDP) is an important nonparametric Bayesian prior for mixed membership models, and the HDP topic model is useful for a wide variety of tasks involving unstructured text (Teh et al., 2006).", "startOffset": 218, "endOffset": 236}, {"referenceID": 0, "context": "To extend the HDP topic model, there has been active research in dependent random probability measures as priors for modeling the underlying association between the latent semantic structure and covariates, such as time stamps and spatial coordinates (Ahmed and Xing, 2010; Ren et al., 2011).", "startOffset": 251, "endOffset": 291}, {"referenceID": 27, "context": "To extend the HDP topic model, there has been active research in dependent random probability measures as priors for modeling the underlying association between the latent semantic structure and covariates, such as time stamps and spatial coordinates (Ahmed and Xing, 2010; Ren et al., 2011).", "startOffset": 251, "endOffset": 291}, {"referenceID": 16, "context": "A large body of this research is rooted in the dependent Dirichlet process (DDP) (MacEachern, 1999) where the probabilistic random measure is defined as a function of covariates.", "startOffset": 81, "endOffset": 99}, {"referenceID": 29, "context": "Most DDP approaches rely on the generalization of Sethuraman\u2019s stick breaking representation of DP (Sethuraman, 1991), incorporating the time difference between two or more data points, the spatial difference among observed data, or the ordering of the data points into the predictor dependent stick breaking process (Duan et al.", "startOffset": 99, "endOffset": 117}, {"referenceID": 30, "context": "Some of these priors can be integrated into the hierarchical construction of DP (Srebro and Roweis, 2005), resulting in topic models where temporally- or spatially-proximate data are more likely to be clustered.", "startOffset": 80, "endOffset": 105}, {"referenceID": 10, "context": "and Park, 2008; Griffin and Steel, 2006). Some of these priors can be integrated into the hierarchical construction of DP (Srebro and Roweis, 2005), resulting in topic models where temporally- or spatially-proximate data are more likely to be clustered. These existing DP approaches, however, cannot model datasets with various types of covariates, including categorical and numerical labels. One reason is that categorical labels cannot be used to directly define the similarity between two documents, unlike temporal or spatial information. Also, labels and documents do not have a one-to-one correspondence, as there may be zero, one, or more labels per document. Furthermore, existing DP approaches cannot be applied to datasets with more than one type of covariates, for example numerical and categorical labels. We suggest the hierarchical Dirichlet scaling process (HDSP) as a new way of modeling a corpus with various types of covariates such as categories, authors, and numerical ratings. The HDSP models the relationship between topics and covariates by generating dependent random measures in a hierarchy, where the first level is a Dirichlet process, and the second level is a Dirichlet scaling process (DSP). The first level DP is constructed in the traditional way of a stick breaking process, and the second level DSP with a normalized gamma process. With the normalized gamma process, each topic proportion of a document is independently drawn from a gamma distribution and then normalized. Unlike the stick breaking process, the normalized gamma process keeps the same order of the atoms as the first level measure, which allows the topic proportions in the random measure to be controlled. The DSP then uses that controllability to guide the topic proportions of a document by replacing the rate parameter of the gamma distribution with a scaling function that defines the correlation structure between topics and labels. The choice of the scaling function reflects the characteristics of the corpus. We show two scaling functions, the first one for a corpus with categorical labels, and the second for a corpus with both categorical and numerical labels. The HDSP models the topic proportions of a document as a dependent variable of observable side information. This modeling approach differs from the traditional definition of a generative process where the observable variables are generated from a latent variable or parameter. For example, Zhu et al. (2009) and Mcauliffe and Blei (2007) propose generative processes where the observable labels are generated from a topic proportion of a document.", "startOffset": 16, "endOffset": 2482}, {"referenceID": 10, "context": "and Park, 2008; Griffin and Steel, 2006). Some of these priors can be integrated into the hierarchical construction of DP (Srebro and Roweis, 2005), resulting in topic models where temporally- or spatially-proximate data are more likely to be clustered. These existing DP approaches, however, cannot model datasets with various types of covariates, including categorical and numerical labels. One reason is that categorical labels cannot be used to directly define the similarity between two documents, unlike temporal or spatial information. Also, labels and documents do not have a one-to-one correspondence, as there may be zero, one, or more labels per document. Furthermore, existing DP approaches cannot be applied to datasets with more than one type of covariates, for example numerical and categorical labels. We suggest the hierarchical Dirichlet scaling process (HDSP) as a new way of modeling a corpus with various types of covariates such as categories, authors, and numerical ratings. The HDSP models the relationship between topics and covariates by generating dependent random measures in a hierarchy, where the first level is a Dirichlet process, and the second level is a Dirichlet scaling process (DSP). The first level DP is constructed in the traditional way of a stick breaking process, and the second level DSP with a normalized gamma process. With the normalized gamma process, each topic proportion of a document is independently drawn from a gamma distribution and then normalized. Unlike the stick breaking process, the normalized gamma process keeps the same order of the atoms as the first level measure, which allows the topic proportions in the random measure to be controlled. The DSP then uses that controllability to guide the topic proportions of a document by replacing the rate parameter of the gamma distribution with a scaling function that defines the correlation structure between topics and labels. The choice of the scaling function reflects the characteristics of the corpus. We show two scaling functions, the first one for a corpus with categorical labels, and the second for a corpus with both categorical and numerical labels. The HDSP models the topic proportions of a document as a dependent variable of observable side information. This modeling approach differs from the traditional definition of a generative process where the observable variables are generated from a latent variable or parameter. For example, Zhu et al. (2009) and Mcauliffe and Blei (2007) propose generative processes where the observable labels are generated from a topic proportion of a document.", "startOffset": 16, "endOffset": 2512}, {"referenceID": 10, "context": "and Park, 2008; Griffin and Steel, 2006). Some of these priors can be integrated into the hierarchical construction of DP (Srebro and Roweis, 2005), resulting in topic models where temporally- or spatially-proximate data are more likely to be clustered. These existing DP approaches, however, cannot model datasets with various types of covariates, including categorical and numerical labels. One reason is that categorical labels cannot be used to directly define the similarity between two documents, unlike temporal or spatial information. Also, labels and documents do not have a one-to-one correspondence, as there may be zero, one, or more labels per document. Furthermore, existing DP approaches cannot be applied to datasets with more than one type of covariates, for example numerical and categorical labels. We suggest the hierarchical Dirichlet scaling process (HDSP) as a new way of modeling a corpus with various types of covariates such as categories, authors, and numerical ratings. The HDSP models the relationship between topics and covariates by generating dependent random measures in a hierarchy, where the first level is a Dirichlet process, and the second level is a Dirichlet scaling process (DSP). The first level DP is constructed in the traditional way of a stick breaking process, and the second level DSP with a normalized gamma process. With the normalized gamma process, each topic proportion of a document is independently drawn from a gamma distribution and then normalized. Unlike the stick breaking process, the normalized gamma process keeps the same order of the atoms as the first level measure, which allows the topic proportions in the random measure to be controlled. The DSP then uses that controllability to guide the topic proportions of a document by replacing the rate parameter of the gamma distribution with a scaling function that defines the correlation structure between topics and labels. The choice of the scaling function reflects the characteristics of the corpus. We show two scaling functions, the first one for a corpus with categorical labels, and the second for a corpus with both categorical and numerical labels. The HDSP models the topic proportions of a document as a dependent variable of observable side information. This modeling approach differs from the traditional definition of a generative process where the observable variables are generated from a latent variable or parameter. For example, Zhu et al. (2009) and Mcauliffe and Blei (2007) propose generative processes where the observable labels are generated from a topic proportion of a document. However, a more natural model of the human writing process is to decide what to write about (e.g., categories) before writing the content of a document. This same approach is also successfully demonstrated in Mimno and McCallum (2012). The outline of this paper is as follows.", "startOffset": 16, "endOffset": 2857}, {"referenceID": 21, "context": "Related Work For model construction, the model most closely related to HDSP is the discrete infinite logistic normal (DILN) model (Paisley et al., 2012) in which the correlations among topics are modeled through the normalized gamma construction.", "startOffset": 130, "endOffset": 152}, {"referenceID": 4, "context": "DILN is a nonparametric counterpart of the correlated topic model (Blei and Lafferty, 2007) in which the logistic normal prior is used to model the correlations between topics.", "startOffset": 66, "endOffset": 91}, {"referenceID": 19, "context": "The Dirichlet-multinomial regression topic model (DMR-TM) (Mimno and McCallum, 2012) also models the label dependent topic proportions of documents, but it is a parametric model.", "startOffset": 58, "endOffset": 84}, {"referenceID": 28, "context": "The author-topic model (Rosen-Zvi et al., 2004) also takes the same approach, but it is a specialized model for authors of documents.", "startOffset": 23, "endOffset": 47}, {"referenceID": 17, "context": "Unlike the \u201cdownstream\u201d generative approach used in the supervised topic model (Mcauliffe and Blei, 2007), the maximum margin topic model (Zhu et al.", "startOffset": 79, "endOffset": 105}, {"referenceID": 35, "context": "Unlike the \u201cdownstream\u201d generative approach used in the supervised topic model (Mcauliffe and Blei, 2007), the maximum margin topic model (Zhu et al., 2009), and the relational topic model (Chang and Blei, 2009), the upstream approach does not require specifying the probability distribution over all possible values of observed labels.", "startOffset": 138, "endOffset": 156}, {"referenceID": 6, "context": ", 2009), and the relational topic model (Chang and Blei, 2009), the upstream approach does not require specifying the probability distribution over all possible values of observed labels.", "startOffset": 40, "endOffset": 62}, {"referenceID": 29, "context": "In the field of Bayesian nonparametrics, the introduction of DDP (Sethuraman, 1991) has led to increased attention in constructing dependent random measures.", "startOffset": 65, "endOffset": 83}, {"referenceID": 9, "context": "Most such approaches develop priors to allow covariate dependent variation in the atoms of the random measure (Gelfand et al., 2005; Rao and Teh, 2009) or in the weights of atoms (Griffin and Steel, 2006; Duan et al.", "startOffset": 110, "endOffset": 151}, {"referenceID": 25, "context": "Most such approaches develop priors to allow covariate dependent variation in the atoms of the random measure (Gelfand et al., 2005; Rao and Teh, 2009) or in the weights of atoms (Griffin and Steel, 2006; Duan et al.", "startOffset": 110, "endOffset": 151}, {"referenceID": 10, "context": ", 2005; Rao and Teh, 2009) or in the weights of atoms (Griffin and Steel, 2006; Duan et al., 2007; Dunson and Park, 2008).", "startOffset": 54, "endOffset": 121}, {"referenceID": 7, "context": ", 2005; Rao and Teh, 2009) or in the weights of atoms (Griffin and Steel, 2006; Duan et al., 2007; Dunson and Park, 2008).", "startOffset": 54, "endOffset": 121}, {"referenceID": 22, "context": "Labeled LDA (L-LDA) allocates one dimension of the topic simplex per label and generates words from only the topics that correspond to the labels in each document (Ramage et al., 2009).", "startOffset": 163, "endOffset": 184}, {"referenceID": 23, "context": "An extension of this model, partially labeled LDA (PLDA), adds more flexibility by allocating a predefined number of topics per label and including a background label to handle documents with no labels (Ramage et al., 2011).", "startOffset": 202, "endOffset": 223}, {"referenceID": 13, "context": "per label but still excludes topics from labels that are not observed in the document (Kim et al., 2012).", "startOffset": 86, "endOffset": 104}, {"referenceID": 29, "context": "The constructive definition of the DP can be represented as a stick breaking process (Sethuraman, 1991), and in the HDP inference algorithm based on stick breaking, the first level DP is given by the following conditional distributions:", "startOffset": 85, "endOffset": 103}, {"referenceID": 34, "context": "This stick breaking construction is the most widely used method for the hierarchical construction (Wang et al., 2011; Teh et al., 2006).", "startOffset": 98, "endOffset": 135}, {"referenceID": 31, "context": "This stick breaking construction is the most widely used method for the hierarchical construction (Wang et al., 2011; Teh et al., 2006).", "startOffset": 98, "endOffset": 135}, {"referenceID": 21, "context": "An alternative construction of the HDP is based on the normalized gamma process (Paisley et al., 2012).", "startOffset": 80, "endOffset": 102}, {"referenceID": 21, "context": "Furthermore, by placing a proper random variable on the rate parameter of the gamma distribution, the model can infer the correlations among the topics (Paisley et al., 2012) through the Gaussian process (Rasmussen and Williams, 2005).", "startOffset": 152, "endOffset": 174}, {"referenceID": 26, "context": ", 2012) through the Gaussian process (Rasmussen and Williams, 2005).", "startOffset": 37, "endOffset": 67}, {"referenceID": 3, "context": "The normalized gamma process itself is not an appropriate construction method for the approximate posterior inference algorithm based on the variational truncation method (Blei and Jordan, 2006) because, unlike the stick breaking process, the probability mass of a random measure constructed by the normalized gamma process is not limited to the first few number of atoms.", "startOffset": 171, "endOffset": 194}, {"referenceID": 30, "context": "Using G0 as the base distribution of a DP for a document with a covariate, the random measure corresponding to document m is constructed as follows: Gm \u223c DP(\u03b2,G0(rm)), (23) where G0(rm) is the base distribution for the document with same covariate rm (Srebro and Roweis, 2005).", "startOffset": 251, "endOffset": 276}, {"referenceID": 10, "context": "The advantage of the HDSP is that it only requires a proper, but relatively simple, scaling function that reflects the correlation between covariates and topics, whereas the DDP requires a complex dependent process for different types of covariates (Griffin and Steel, 2006).", "startOffset": 249, "endOffset": 274}, {"referenceID": 8, "context": "Approximation algorithms, such as marginalized MCMC (Escobar and West, 1995; Teh et al., 2006) and variational inference (Blei and Jordan, 2006; Teh et al.", "startOffset": 52, "endOffset": 94}, {"referenceID": 31, "context": "Approximation algorithms, such as marginalized MCMC (Escobar and West, 1995; Teh et al., 2006) and variational inference (Blei and Jordan, 2006; Teh et al.", "startOffset": 52, "endOffset": 94}, {"referenceID": 3, "context": ", 2006) and variational inference (Blei and Jordan, 2006; Teh et al., 2008), have been developed for", "startOffset": 34, "endOffset": 75}, {"referenceID": 32, "context": ", 2006) and variational inference (Blei and Jordan, 2006; Teh et al., 2008), have been developed for", "startOffset": 34, "endOffset": 75}, {"referenceID": 12, "context": "We develop a mean field variational inference (Jordan et al., 1999; Wainwright and Jordan, 2008) algorithm for approximate posterior inference of the HDSP topic model.", "startOffset": 46, "endOffset": 96}, {"referenceID": 15, "context": "For the corpus level stick proportion Vk, we use the delta function as a variational distribution for simplicity and tractability in inference steps as demonstrated in (Liang et al., 2007).", "startOffset": 168, "endOffset": 188}, {"referenceID": 4, "context": "Following Blei and Lafferty (2007), we approximate the lower bound of the expectation by using the first-order Taylor expansion, \u2212Eq[ln T \u2211", "startOffset": 10, "endOffset": 35}, {"referenceID": 2, "context": "The optimal form of a variational distribution can be obtained by exponentiating the variational lower bound with all expectations except the parameter of interest (Bishop and Nasrabadi, 2006).", "startOffset": 164, "endOffset": 192}, {"referenceID": 5, "context": "where i is a word index, and 1 is an indicator function (Blei et al., 2003).", "startOffset": 56, "endOffset": 75}, {"referenceID": 24, "context": "Once one defines a tractable expectation of the log of a scaling function, a recently suggested Black-box method (Ranganath et al., 2014) can be an alternative to update the function-specific parameters instead of deriving function-specific approximation algorithms.", "startOffset": 113, "endOffset": 137}, {"referenceID": 14, "context": "To visualize the relationship between topics and labels, we embed the inferred topics and the labels into the two dimensional euclidean space by using multidimensional scaling (Kruskal, 1964) on the inferred parameters of HDSP.", "startOffset": 176, "endOffset": 191}, {"referenceID": 33, "context": "heldout document requires complex approximation schemes (Wallach et al., 2009), but we compute the perplexity based on the expected topic distribution given category and rating information, which requires a finite number of computations.", "startOffset": 56, "endOffset": 78}, {"referenceID": 4, "context": "Taking inspiration from previous work (Blei and Lafferty, 2007; Mimno et al., 2007; Paisley et al., 2012) that showed correlations among topics, we can define a scaling function with a prior over the topics and labels to capture their complex relationships.", "startOffset": 38, "endOffset": 105}, {"referenceID": 20, "context": "Taking inspiration from previous work (Blei and Lafferty, 2007; Mimno et al., 2007; Paisley et al., 2012) that showed correlations among topics, we can define a scaling function with a prior over the topics and labels to capture their complex relationships.", "startOffset": 38, "endOffset": 105}, {"referenceID": 21, "context": "Taking inspiration from previous work (Blei and Lafferty, 2007; Mimno et al., 2007; Paisley et al., 2012) that showed correlations among topics, we can define a scaling function with a prior over the topics and labels to capture their complex relationships.", "startOffset": 38, "endOffset": 105}, {"referenceID": 11, "context": "For a fast and efficient posterior inference, we can apply parallel or distributed algorithms based on a stochastic update (Hoffman et al., 2013; Ahn et al., 2014).", "startOffset": 123, "endOffset": 163}, {"referenceID": 1, "context": "For a fast and efficient posterior inference, we can apply parallel or distributed algorithms based on a stochastic update (Hoffman et al., 2013; Ahn et al., 2014).", "startOffset": 123, "endOffset": 163}, {"referenceID": 2, "context": "The optimal form of the variational distribution can be obtained by exponentiating the variational lower bound with all expectations except the parameter of interest (Bishop and Nasrabadi, 2006).", "startOffset": 166, "endOffset": 194}, {"referenceID": 21, "context": "We take the same approach described in (Paisley et al., 2012), and the only difference comes from the product of the inverse distance term.", "startOffset": 39, "endOffset": 61}], "year": 2015, "abstractText": "We present the hierarchical Dirichlet scaling process (HDSP), a Bayesian nonparametric mixed membership model. The HDSP generalizes the hierarchical Dirichlet process (HDP) to model the correlation structure between metadata in the corpus and mixture components. We construct the HDSP based on the normalized gamma representation of the Dirichlet process, and this construction allows incorporating a scaling function that controls the membership probabilities of the mixture components. We develop two scaling methods to demonstrate that different modeling assumptions can be expressed in the HDSP. We also derive the corresponding approximate posterior inference algorithms using variational Bayes. Through experiments on datasets of newswire, medical journal articles, conference proceedings, and product reviews, we show that the HDSP results in a better predictive performance than labeled LDA, partially labeled LDA, and author topic model and a better negative review classification performance than the supervised topic model and SVM.", "creator": "LaTeX with hyperref package"}}}