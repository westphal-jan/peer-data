{"id": "1405.1675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2014", "title": "Structured Learning Modulo Theories", "abstract": "presenting problems to abundant mixture of boolean and numerical variables is a long - standing occupation of artificial intelligence. however, implicit inference and learning in hybrid analysis is a particularly relevant task. learning ability so model this kind of domains is thus about \" learnt to design \" tasks, dat is, learning applications and the intelligence isn to recover from examples how to perform large de novo inference called novel simulations. here this manual we mention structured learning modulo theories, classic squared - margin diagram : working in hybrid cases ; on cognitive modulo theories, which allows implementations combine better reasoning and predictions over more linear arithmetical constructions. we produce our specifications governing artificial and composite world scenarios.", "histories": [["v1", "Wed, 7 May 2014 17:41:43 GMT  (304kb,D)", "http://arxiv.org/abs/1405.1675v1", "40 pages, 12 figures, submitted to Artificial Intelligence Journal Special Issue on Combining Constraint Solving with Mining and Learning"], ["v2", "Thu, 18 Dec 2014 15:57:29 GMT  (332kb,D)", "http://arxiv.org/abs/1405.1675v2", "46 pages, 11 figures, submitted to Artificial Intelligence Journal Special Issue on Combining Constraint Solving with Mining and Learning"]], "COMMENTS": "40 pages, 12 figures, submitted to Artificial Intelligence Journal Special Issue on Combining Constraint Solving with Mining and Learning", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["stefano teso", "roberto sebastiani", "andrea passerini"], "accepted": false, "id": "1405.1675"}, "pdf": {"name": "1405.1675.pdf", "metadata": {"source": "CRF", "title": "Structured Learning Modulo Theories", "authors": ["Stefano Teso", "Roberto Sebastiani", "Andrea Passerini"], "emails": ["teso@disi.unitn.it", "roberto.sebastiani@unitn.it", "passerini@disi.unitn.it"], "sections": [{"heading": null, "text": "Modelling problems containing a mixture of Boolean and numerical variables is a long-standing interest of Artificial Intelligence. However, performing inference and learning in hybrid domains is a particularly daunting task. The ability to model this kind of domains is crucial in \u201clearning to design\u201d tasks, that is, learning applications where the goal is to learn from examples how to perform automatic de novo design of novel objects. In this paper we present Structured Learning Modulo Theories, a max-margin approach for learning in hybrid domains based on Satisfiability Modulo Theories, which allows to combine Boolean reasoning and optimization over continuous linear arithmetical constraints. We validate our method on artificial and real world scenarios.\nKeywords: Satisfiability Modulo Theory, Structured-Output Learning, Optimization Modulo Theory, Constructive Machine Learning, Learning with Constraints"}, {"heading": "1. Introduction", "text": "Research in machine learning has progressively widened its scope, from simple scalar classification and regression tasks to more complex problems involving multiple related variables. Methods developed in the related fields of statistical relational learning (SRL) [1] and structured-output learning [2] allow to perform learning, reason and make inference about relational entities\nEmail addresses: teso@disi.unitn.it (Stefano Teso), roberto.sebastiani@unitn.it (Roberto Sebastiani), passerini@disi.unitn.it (Andrea Passerini)\nPreprint submitted to Artificial Intelligence September 21, 2017\nar X\niv :1\n40 5.\n16 75\nv1 [\ncs .A\nI] 7\nM ay\ncharacterized by both hard and soft constraints. Most methods rely on some form of (finite) First-Order Logic (FOL) to encode the learning problem, and define the constraints as weighted logical formulae. One issue with these approaches is that First-Order Logic is not suited for efficiently reasoning over hybrid domains, characterized by both continuous and discrete variables. The Booleanization of an n-bit integer variable requires n distinct Boolean variables, which account for 2n distinct states, making naive translation impractical. In addition, standard FOL automated reasoning techniques offer no mechanism to deal efficiently with operators among numerical variables, like comparisons (e.g. \u201cless-than\u201d, \u201cequal\u201d) and arithmetical operations (e.g. summation), limiting the range of realistically applicable constraints to those based solely on logical connectives. On the other hand, many real-world domains are inherently hybrid and require to reason over inter-related continuous and discrete variables. This is especially true in constructive machine learning tasks, where the focus is on the de-novo design of objects with certain characteristics to be learned from examples (e.g. a recipe for a dish, with ingredients, proportions, etc.).\nIn order to side-step the limitations of FOL automated-reasoning tools, researchers in automated reasoning and formal verification have developed more appropriate logical languages and reasoning tools that allow for natively reasoning over mixtures of Boolean and numerical variables (or even more complex structures). These languages are grouped under the umbrella term of Satisfiability Modulo Theories (SMT) [3]. Each such language corresponds to a decidable fragment of First-Order Logic augmented with an additional background theory T . There are many such background theories, including those of linear arithmetic over the rationals LA(Q) or over the integers LA(Z), among others [3]. In SMT, a formula can contain Boolean variables (i.e. 0-ary logical predicates) and connectives, mixed with symbols defined by the theory T , e.g. rational variables and arithmetical operators. For instance, the SMT(LA(Q)) syntax allows to write constraints such as:\ntouching i j\u2194 ((xi + dxi = xj) \u2228 (xj + dxj = xi))\nwhere the variables are Boolean (touching i j) or rational (xi, xj, dxi,dxj). More specifically, SMT is a decision problem, which consists in finding an assignment to the variables of a quantifier-free formula, both Boolean and theory-specific ones, that makes the formula true, and it can be seen as an extension of SAT.\nRecently, researchers have leveraged SMT from decision to optimization. In particular, MAX-SAT Modulo Theories (MAX-SMT) [4, 5, 6] generalizes MAX-SAT [7] to SMT formulae, and consists in finding a theory-consistent truth assignment to the atoms of the input SMT formula \u03d5 which maximizes the total weight of the satisfied clauses of \u03d5. More generally, Optimization Modulo Theories (OMT) [8], consists in finding a model for \u03d5 which minimizes the value of some (arithmetical) term, and strictly subsumes MAX-SMT. Most important for the scope of this paper is that there are high quality MAX-SMT (and OMT) solvers, which (at least for the LA(Q) theory) can handle problems with a large number of hybrid variables.\nThere is relatively little previous work on hybrid SRL methods. Most current approaches are direct generalizations of existing SRL methods [1]. Hybrid Markov Logic networks [9] extend Markov Logic by including continuous variables, and allow to embed numerical comparison operators (namely 6=, \u2265 and \u2264) into the constraints by defining an ad hoc translation of said operators to a continuous form amenable to numerical optimization. Inference relies on an MCMC procedure that interleaves calls to a MAX-SAT solver and to a numerical optimization procedure. This results in a very expensive iterative process, which can hardly scale with the size of the problem. Conversely, MAX-SMT and OMT tools are built on top of SMT solvers and are hence specifically designed to tightly integrate theory-specific and SAT solvers [3]. Some probabilistic-logical methods, e.g. ProbLog [10] and PRISM [11], have also been modified to deal with continuous random variables. These models, however, rely on probabilistic assumptions that make it difficult to implement fully expressive constraints in, e.g. linear arithmetic, in their formalism. Church [12] is a very expressive probabilistic programming language that can potentially represent arbitrary constraints on both continuous and discrete variables. Its focus is on modelling the generative process underlying the program, and inference is based on sampling techniques. This makes inference involving continuous optimization subtasks prohibitively expensive, as will be discussed in the experimental evaluation.\nIn this paper we propose Learning Modulo Theories (LMT), a class of novel hybrid statistical relational learning methods. By combining the flexibility of structured output Support Vector Machines [13] and the expressivity and reasoning power of Satisfiability Modulo Theories, LMT is able to perform learning and inference in mixed Boolean-numerical domains. Thanks to the efficiency of the underlying OMT solver, and of the cutting plane algorithm we employ for weight learning, LMT is capable of addressing con-\nstructive learning problems which cannot be efficiently addressed by existing methods. Furthermore, LMT is generic, and can in principle be applied to any of the existing background theories. This paper builds on a previous work in which MAX-SMT was used for interactive preference elicitation [14]. Here we focus on supervised structured-output learning and leverage the expressivity of full OMT.\nThe rest of the paper is organized as follows. In Section 2 we review some related work. Section 3 provides an introduction to SMT, MAX-SMT and OMT technology. The LMT method is described in Section 4 and Section 5 reports an experimental evaluation showing the potential of the framework. Finally, conclusions are drawn in Section 6."}, {"heading": "2. Related Work", "text": "There is a body of work concerning integration of relational and numerical data from a feature representation perspective, in order to effectively incorporate numerical features into statistical relational learning models. Lippi and Frasconi [15] incorporate neural networks as feature generators within Markov Logic Networks, where neural networks act as numerical functions complementing the Boolean formulas of standard MLNs. Semantic Based Regularization [16] is a framework for integrating logic constraints within kernel machines, by turning them into real-valued constraints using appropriate transformations (T-norms). The resulting optimization problem is no more convex in general, and they suggest a stepwise approach adding constraints in an incremental fashion, in order to solve progressively more complex problems. In Probabilistic soft logic [17], arbitrarily complex similarity measures between objects are combined with logic constraints, again using t-norms for the continuous relaxation of Boolean operators. In Gaussian Logic [18], numeric variables are modeled with multivariate Gaussian distributions. Their parameters are tied according to logic formulas defined over these variables, and combined with weighted first order formulas modeling the discrete part of the domain (as in standard MLNs).\nPerforming inference over joint continuous-discrete relational domains is a very challenging task. The few existing attempts aim at extending statistical relational learning methods to the hybrid domain.\nHybrid Markov Logic Networks [9] extend Markov Logic Networks to deal with numeric variables. A Hybrid Markov Logic Network consists of both First Order Logic formulas and numeric terms. In running inference,\noptimization of numeric variables is performed by a general-purpose global optimization algorithm (L-BFGS). This approach is extremely flexible and allows to encode arbitrary numeric constraints, like soft equalities and inequalities with quadratic or exponential costs. A major drawback of this flexibility is the computational cost, as each single inference step on continuous variables requires to solve a global optimization problem, making the approach infeasible for addressing medium to large scale problems.\nRelational Hybrid models [19] allow to represent relational models combining discrete and continuous distributions. The authors present a family of lifted relational variational algorithms for performing efficient inference.\nThe PRISM [20] system provides primitives for Gaussian distributions. It has also been recently extended [11] to perform inference over continuous random variables by a symbolic procedure which avoids the enumeration of individual proofs. The extension allows to encode models like Hybrid Bayesian Networks [21] and Kalman Filters, in terms of linear equality contraints over real variables. Being built on top of the PRISM system, the approach assumes the exclusive explanation and independence property: no two different proofs for the same goal can be true simultaneously, and all random processes within a proof are independent. Some research directions for lifting these restrictions have been suggested [22].\nHybrid ProbLog [10] is an extension of the probabilistic logic language ProbLog [23] to deal with continuous variables. A ProbLog program consists of a set of probabilistic Boolean facts, and a set of deterministic first order logic formulas representing the background knowledge. Hybrid ProbLog introduces a set of probabilistic continuous facts, containing both discrete and continuous variables. Each continuous variable is attached with a probability density function. The authors show how to compute the probability of success of a query, by partitioning the continuous space into admissible intervals, within which values are interchangeable with respect to the provability of the query. The drawback of this approach is that in order to make this computation feasible, severe limitations have to be imposed on the use of continuous variables. No algebraic operations or comparisons are allowed between continuous variables, which should remain uncoupled. Some of these limitations have been overcome in a recent approach [24] which performs inference by forward (i.e. from facts to rules) rather than backward reasoning, which is the typical inference process in (probabilistic) logic programming engines (SLD-resolution and its probabilistic extensions). Forward reasoning is more amenable to be adapted to sampling strategies for performing approximate\ninference and dealing with continuous variables. Church [12] is a very expressive probabilistic programming language that can easily accomodate hybrid discrete-continuous distributions and arbitrary constraints. In order to deal with the resulting complexity, inference is again performed by sampling techniques. This makes reasoning involving hard continuous constraints prohibitively expensive, as will be shown in the experimental evaluation.\nAn advantage of these probabilistic inference approaches is that they allow to return marginal probabilities in addition to most probable explanations. As most structured-output approaches over which it builds, LMT is currently limited to the latter task. We are planning to extend it to also perform probability computation, as discussed in the conclusions of the paper."}, {"heading": "3. From Satisfiability to Optimization Modulo Theories", "text": "Propositional satisfiability (SAT), is the problem of deciding whether a logical formula over Boolean variables and logical connectives can be satisfied by some truth value assignment of the Boolean variables. In the last two decades we have witnessed an impressive advance in the efficiency of SAT solvers, which nowadays can handle industrial-derived formulae in the order of up to 106 \u2212 107 variables and clauses. Modern SAT solvers are based on the conflict-driven clause-learning (CDCL) schema [25], and adopt a variety of very-efficient search techniques [26]. 1\nMAX-SAT [7] leverages SAT from decision to optimization. In a (weighted partial) MAX-SAT problem a subset of the clauses of the input formula \u03d5 \u2014 called \u201csoft\u201d clauses\u2014 are given a positive numerical weight, and the problem consists in finding a truth assignment to the atoms of the \u03d5 which satisfies all the other clauses \u2014called \u201chard\u201d clauses\u2014 and maximizes the total weight of the satisfied soft clauses. A wide variety of MAX-SAT tools have been proposed in the literature and are currently available.\nIn the contexts of automated reasoning (AR) and formal verification (FV), important decision problems are effectively encoded into and solved as Satisfiability Modulo Theories (SMT) problems [28]. SMT is the problem of deciding the satisfiability of a (typically quantifier-free) first-order formula\n1CDCL SAT-solving algorithms, and SMT-solving ones thereof, require input formulas to be in conjunctive normal form (CNF); however they all very-effectively pre-convert input formulas into CNF [27]. Hence, wlog we assume input formulae to have any form.\nwith respect to some (decidable) background theory T (which can also be a combination of theories \u22c3 i Ti). Theories of practical interest are, e.g., those of equality and uninterpreted functions (EUF), of linear arithmetic over the rationals (LA(Q)) or over the integers (LA(Z)), of non-linear arithmetic over the reals (NLA), of arrays (AR), of bit-vectors (BV), and their combinations.\nExample 3.1. Consider the following toy LA(Z) \u222a EUF \u222a AR-formula:\n\u03d5 def = (d \u2265 0)\u2227(d < 1)\u2227((f(d) = f(0))\u2192 (read(write(V, i, x), i+d) = x+1))\nwhere d, i, x are integer variables, V is an array variable, f is an uninterpreted function symbol, read(), write() are the standard functions of AR theory. 2 \u03d5 is inconsistent in the combined theory LA(Z) \u222a EUF \u222a AR. In fact, from the first two atoms (d \u2265 0) (d < 1) we deduce in LA(Z) that d = 0, from which we deduce in EUF that the second atom (f(d) = f(0)) must be true; hence, by Boolean reasoning, we deduce that the last atom (read(write(V, i, x), i + d) = x + 1) must be true; in LA(Z) we deduce (read(write(V, i, x), i) = x + 1) and hence \u00ac(read(write(V, i, x), i) = x). The latter contradicts one of the axioms in AR, 3 so that we can deduce that \u03d5 is unsatisfiable in LA(Z) \u222a EUF \u222a AR.\nIn the last decade efficient SMT solvers have been developed following the so-called lazy approach, that combines the power of modern conflict-driven clause-learning (CDCL) SAT solvers with the expressivity of dedicated decision procedures (T -solvers) for several first-order theories of interest. Modern lazy SMT solvers \u2014like e.g. CVC44, MathSAT55, Yices6, Z37\u2014 combine synergically a variety of solving techniques coming from very hetherogeneous domains, ranging from SAT (e.g. for Boolean reasoning and BV theory),\n2read(V, i) returns the value of the i-th element of array V , and write(V, i, x) returns a new array resulting from writing value x in the i-th element of array V .\n3\u201c\u2200 V, i, x. (read(write(V, i, x), i) = x)\u201d is an axiom of AR, meaning \u201cthe value you read in the i-th element of the array resulting from writing x into the i-th element in the array V , is x.\n4http://cvc4.cs.nyu.edu/ 5http://mathsat.fbk.eu/ 6http://yices.csl.sri.com/ 7ttp://research.microsoft.com/en-us/um/redmond/projects/z3/ml/z3.html\nfirst-order theorem proving (e.g., for EUF and AR) operational research and CSP (e.g., for LA(Q), LA(Z) or NLA(R)), computer algebra (e.g., for NLA(R)). (We refer the reader to [29, 3] for an overview on lazy SMT solving, and to the URLs of the above solvers for a description of their supported theories and functionalities.)\nMore recently, also SMT has been leveraged from decision to optimization. First, MAX-SAT Modulo Theories (MAX-SMT) generalizes MAX-SAT to SMT formulae, by finding a theory-consistent truth assignment to the atoms of the input SMT formula \u03d5 which maximizes the total weight of the satisfied soft clauses. A few MAX-SMT tools have been proposed in the literature and are currently availables [4, 5, 6], which are built on top of state-of-the-art SMT solvers. 8\nSecond, and more generally, Optimization Modulo Theories (OMT) [4, 8], is the problem of finding a model for an SMT formula \u03d5 which minimizes the value of some (arithmetical) cost function. In [8] they are presented some general OMT procedure adding to SMT the capability of finding models minimizing cost functions in LA(Q). This problem is denoted OMT(LA(Q)) if only the LA(Q) theory is involved in the SMT formula, OMT(LA(Q)\u222aT ) if some other (combination of) theories are involved. 9 Such procedures combine standard lazy SMT-solving with LP minimization techniques, implementing mixed binary-rearch/branch&bound minimization strategies inside the CDCL schema of the underlining SAT solver, so as to fully exploit the pruning power of the CDCL paradighm. These procedures have been implemented into the OptiMathSAT 10 tool, a sub-branch of MathSAT5. Importantly, OMT(LA(Q)) and OMT(LA(Q)\u222aT ) are strictly more expressive than MAX-SMT, since the latter can be straightforwardly and effectively encoded into the former, but not vice versa [8]. 11\nExample 3.2. Consider the following toy LA(Q)-formula \u03d5: (cost = x+y)\u2227(x \u2265 0)\u2227(y \u2265 0)\u2227(A\u2228(4x+y\u22124 \u2265 0))\u2227(\u00acA\u2228(2x+3y\u22126 \u2265 0))\n8Also Yices and Z3 provide support for MAX-SMT, but we are not aware of any public document describing the procedures used there.\n9 In [8] LA(Q) and T are combinations of Nelson-Oppen theories, as described in [30]. (E.g., EUF , LA(Q), AR match this definition.)\n10 http://disi.unitn.it/~rseba/optimathsat5.tar.gz 11The effectiveness of such encoding is witnessed by the experimental evaluation of [6], where the specialized MAX-SMT tool LL introduced there did not beat OptiMathSAT on MAX-SMT benchmarks, despite both tools were built on top of MathSAT5.\nand the OMT(LA(Q)) problem of finding the model of \u03d5 (if any) which makes the value of cost minimum. In fact, depending on the truth value of A, there are two possible alternative sets of constraints to minimize:\n{A, (cost = x+ y), (x \u2265 0), (y \u2265 0), (2x+ 3y \u2212 6 \u2265 0)} {\u00acA, (cost = x+ y), (x \u2265 0), (y \u2265 0), (4x+ y \u2212 4 \u2265 0)}\nwhose minimum-cost models are, respectively:\n{A = True, x = 0.0, y = 2.0, cost = 2.0} {A = False, x = 1.0, y = 0.0, cost = 1.0}\nfrom which we can conclude that the latter is a minimum-cost model for \u03d5.\nOverall, for the scope of this paper, it is important to highlight the fact that MAX-SMT and OMT solvers are available which, thanks to the underlying SAT and SMT technologies, can handle problems with a large number of hybrid variables (at least for the LA(Q) theory).\nTo this extent, we notice that the underlying theories and T -solvers provide the meaning and the reasoning capabilities for specific predicates and function symbols (e.g., \u201c\u2265\u201d and \u201c+\u201d, or \u201cread(...)\u201d, \u201cwrite(...)\u201d) that would otherwise be very difficult to describe, or to reason over, with logic-based automated reasoning tools \u2014e.g., traditional first-order theorem provers cannot handle arithmetical reasoning efficiently\u2014 or with arithmetical ones \u2014e.g., DLP, ILP, MILP, LGDP tools [31, 32, 33] or CLP tools [34, 35, 36] do not handle symbolic theory-reasoning on theories like EUF or AR. Also, the underlying CDCL SAT solver allows SMT solvers to handle very efficiently big amounts of Boolean reasoning, which are typically out of the reach of both first-order theorem provers and arithmetical tools.\nThese facts motivate our choice of using SMT/OMT technology, and hence the tool OptiMathSAT, as workhorse engines for reasoning in hybrid domains.\nAnother prospective advantage of SMT technology is that modern SMT solvers (e.g. MathSAT5, Z3,...) have an incremental interface, which allows for solving sequences of \u201csimilar\u201d formulas without restarting the search from scratch at each new formula, rather reusing \u201ccommon\u201d parts of the search performed for previous formulas (see, e.g., [37]). This drastically improves overall performances on sequences of similar formulas. An incremental extension of OptiMathSAT, fully exploiting that of MathSAT5, is currently\nunder development, and we expect a significant improvement in performances when invoked on sequences of similar formulas, as we do in this paper.\nNotice that a current limitation of SMT solvers is that, unlike traditional theorem provers, they typically handle efficiently only quantifier-free formulas. Although some SMT solvers (e.g., Z3) provide some support for quantified formulas, and some attempts of extending SMT to quantified formulas by combining them with first-order theorem provers have been presented in the literature (e.g., [38, 39, 40]), the state of the art of these estensions is far from being satisfactory yet. Nonetheless, the method we present in the paper can be easily adapted to deal with this type of extensions once they\u2019ll reach the required level of maturity."}, {"heading": "4. The Method", "text": "Notation. We consider the problem of learning from a set of n complex objects {zi}ni=1 that include a combination of Boolean and rational variables:\nz \u2208 ({>,\u22a5} \u00d7 . . .\u00d7 {>,\u22a5})\ufe38 \ufe37\ufe37 \ufe38 Boolean part \u00d7 (Q\u00d7 . . .\u00d7Q)\ufe38 \ufe37\ufe37 \ufe38 rational part\nWe indicate Boolean variables using predicates such as touching(i, j), and write rational variables as lower-case letters, e.g. cost, distance, x, y. Please note that our method requires the grounding of all Boolean predicates prior to learning and inference. In the present formulation, we assume objects to be composed of two parts, that is z = (I,O). Here I is the input (or observed) part, while O is the output (or query) part (we depart from the conventional x/y notation for indicating inputs/outputs pairs to avoid name clashes with the cost variables). The learning problem is defined by a set of m constraints {\u03d5k}mk=1. Each constraint \u03d5k is either a Boolean- or rationalvalued function of the object z. For each Boolean-valued constraint \u03d5k, we denote its indicator function as 1k(z), which evaluates to 1 if the constraint is satisfied and to \u22121 otherwise (the choice of \u22121 to represent falsity is customary in the max-margin literature). Similarly, we refer to the cost of a real-valued constraint \u03d5k as ck(z) \u2208 Q. The feature space representation of an object z is given by the feature vector \u03c8(z). Each soft constraint \u03d5k has an associated finite weight wk \u2208 Q (to be learned from the data), while hard constraints have no associated weight. We denote the vector of learned weights as w := (w1, w2, . . . , wm), and its Euclidean norm as \u2016w\u2016."}, {"heading": "4.1. Background on Structured Output SVMs", "text": "Structured output SVMs [13] are a very flexible framework that generalizes max-margin methods to the prediction of complex outputs such as strings, trees and graphs. In this setting the association between inputs I and outputs O is controlled by a so-called compatibility function f(I,O) := w>\u03c8(I,O) defined as a linear combination of the joint feature space representation \u03c8(I,O) of the input-output pair. Inference amounts to finding the most compatible output O\u2217 for a given input I, which equates to solving the following optimization problem:\nO\u2217 = argmaxOf(I,O) = argmaxOw >\u03c8(I,O) (1)\nPerforming inference on structured domains is non-trivial, since the maximization ranges over an exponential (and possibly unbounded) number of candidate outputs.\nLearning is formulated within the regularized empirical risk minimization framework. In order to learn the weights from a training set of n examples, one needs to define a non-negative loss function \u2206(I,O,O\u2032) that, for any given observation I, quantifies the penalty incurred when predicting O\u2032 instead of the correct output O. Learning can be expressed as the problem of finding the weights w that minimize the per-instance error \u03bei and the model complexity [13]:\nargmin w,\u03be\n1 2 \u2016w\u20162 + C n n\u2211 i=1 \u03bei (2)\ns.t. w>(\u03c8(I i,Oi)\u2212\u03c8(I,O\u2032)) \u2265 \u2206(I i,Oi,O\u2032)\u2212\u03bei, \u2200 i = 1, . . . , n; O\u2032 6= Oi Here the constraints require that the compatibility between any input I i and its corresponding correct output Oi is always higher than that with all wrong outputs O\u2032 by a margin, with \u03bei playing the role of per-instance violations. This is the so-called n-slack max-margin formulation of structured SVMs, but there are a number of alternatives; in this paper we build on the 1-slack margin rescaling formulation, see [41] for a more extensive exposition.\nWeight learning is a quadratic program, and can be solved very efficiently with a cutting-plane (CP) algorithm [13]. Since in Eq (2) there is an exponential number of constraints, it is infeasible to naively account for all of them during learning. Based on the observations that the constraints obey a subsumption relation, the CP algorithm [41] sidesteps the issue by keeping\na working set of active constraints: at each iteration, it augments the working set with the most violated constraint, and then solves the corresponding reduced quadratic program using a standard SVM solver. This procedure is guaranteed to find an -approximate solution to the QP in a polynomial number of iterations, independently of the cardinality of Y and of the number of examples n [13]. The 1-slack margin rescaling version of the CP algorithm can be found in Algorithm 1.\nData: Training instances {z1, . . . ,zn}, hyperparameters C, . Result: Learned weights w, slack variable \u03be.\n1 W \u2190 \u2205, \u03be \u2190 0 ; 2 repeat 3 (w, \u03be)\u2190 argminw,\u03be 12\u2016w\u2016 2 2 + C\u03be 4\ns.t.\u2200O\u03041, . . . , O\u0304n 1 n w> \u2211 i (\u03c8(Ii,Oi)\u2212\u03c8(I, O\u0304i)) \u2265\n1\nn \u2211 i \u2206(Ii,Oi, O\u0304i)\u2212 \u03be\n5 for i = 1, . . . , n do 6 O\u2032 \u2190 argmaxO\u2032 \u2206(Ii,Oi,O\n\u2032)\u2212w> (\u03c8(Ii,Oi)\u2212\u03c8(Ii,O\u2032)) ; 7 end 8 W \u2190W \u222a {(O\u20321, . . . ,O\u2032n)} ; 9 until 1n \u2211 i \u2206(Ii,Oi,O \u2032 i)\u2212 1nw >\u2211 i(\u03c8(Ii,Oi)\u2212\u03c8(I,Oi)) \u2264 \u03be + ; Algorithm 1: Cutting-plane algorithm for training structural SVMs, according to the 1-slack formulation presented in [41].\nThe CP algorithm is generic, meaning that it can be adapted to any structured prediction problem as long as it is provided with: (i) a joint feature space representation \u03c8 of input-output pairs (and consequently a compatibility function f); (ii) an oracle to perform inference, i.e. to solve Equation (1); and (iii) an oracle to retrieve the most violated constraint of the QP, i.e. to solve the separation problem:\nargmaxO\u2032w T\u03c8(I i,O \u2032) + \u2206(Oi,O \u2032) (3)\nThe two oracles are used as sub-routines during the optimization procedure. For a more detailed account, please refer to [13].\nOne key aspect of the structured output SVMs is that efficient implementations of the two oracles are fundamental for the learning task to be\ntractable in practice. In the following sections we show how to define a feature space for hybrid Boolean-numerical learning problems, and how to use OMT solvers to efficiently perform inference and separation."}, {"heading": "4.2. Learning Modulo Theories with OMT", "text": "In order to introduce the LMT framework, we start with a toy learning example. We are given a unit-length bounding box, [0, 1]\u00d7[0, 1], that contains a given, fixed block (rectangle), as in Figure 1 (a). The block is identified by the four constants (x1, y1, dx1, dy1), where (x1, y1) indicates the bottom-left corner of the rectangle, and dx1, dy1 its width and height, respectively. Now, suppose that we are assigned the task of fitting another block, identified by the variables (x2, y2, dx2, dy2), in the same bounding box, so to minimize the following cost function:\ncost := w1 \u00d7 dx2 + w2 \u00d7 dy2 (4)\nwith the additional requirements that (i) the two blocks \u201ctouch\u201d either from above, below, or sideways, and (ii) do not overlap.\nIt is easy to see that the weights control the shape and location of the optimal solution. If both weights are positive, then the cost is minimized by any block of null size located along the perimeter of block 1. If both weights are negative and w1 w2, then the optimal block will be placed as to occupy as much horizontal space as possible, while if w1 w2 it will prefer to occupy as much vertical space as possible, as in Figure 1 (b,c). If w1 and w2 are close, then the optimal solution depends on the relative amount of available vertical and horizontal space in the bounding box.\nThis toy example illustrates two key points. First, the problem involves both a mixture of numerical (coordinates, sizes of block 2) and Boolean variables, hard rules that control the feasible space of the optimization procedure\n(conditions (i) and (ii)), and costs \u2014 or soft rules \u2014 which control the shape of the optimization landscape. This is the kind of problem that can be solved in terms of Optimization Modulo Linear Arithmetic, OMT(LA(Q)). Second, it is possible to estimate the weights w1, w2 from data in order to learn what kind of blocks are to be considered optimal. We will see that this task can be framed within the structured output SVMs framework.\nLet us formalize this toy example in the language of LMT. Here the input I to the problem is the observed box (x1, y1, dx1, dy1) while the output O is the generated block (x2, y2, dx2, dy2). In order to encode the set of constraints {\u03d5k}, it is convenient to first introduce a background knowledge of predicates expressing facts about the relative positioning of blocks. To this end we add a fresh predicate left(i, j), that encodes the fact that \u201ca generic block of index i touches a second block j from the left\u201d, defined as follows:\nleft(i, j) := xi + dxi = xj\u2227 ((yj \u2264 yi \u2264 yj + dyj)\u2228 (yj \u2264 yi + dyi \u2264 yj + dyj))\nSimilarly, we add analogous predicates for the other directions: right(i, j), below(i, j), over(i, j) (see Table 2 for the full definitions). The hard constraints represent the fact that the output O should be a valid block within the bounding box (all the constraints \u03d5k are implicitly conjoined):\n0 \u2264 x2, y2, dx2, dy2 \u2264 1 (x2 + dx2) \u2264 1 \u2227 (y2 + dy2) \u2264 1\nThen we require the output block O to \u201ctouch\u201d the input block I:\nleft(1, 2) \u2228 right(1, 2) \u2228 below(1, 2) \u2228 over(1, 2)\nNote that whenever this rule is satisfied, both conditions (i) and (ii) hold, i.e. touching blocks never overlap. Finally, we encode the cost function cost = w1dx2 + w2dy2, completing the description of the optimization problem.\nNow, suppose we were given a training set of instances analogous to those pictured in Figure 1 (c), i.e. where the supervision includes output blocks that preferentially fill as much vertical space as possible. The learning algorithm should be able to learn this preference by inferring appropriate weights. This kind of learning task can be cast within the structured SVM framework, by defining an appropriate joint feature space \u03c8 and oracles for the inference and separation problems.\nLet us focus on the feature space first. Our definition is grounded on the concept of reward assigned to an object z with respect to the set of formulae {\u03d5k}mk=1. We construct the feature vector\n\u03c8(z) := (\u03c81(z), . . . , \u03c8m(z)) >\nby collating m per-formula rewards \u03c8k(z), where:\n\u03c8k(z) :=\n{ 1k(z) if \u03d5k is Boolean\n\u2212ck(z) if \u03d5k is arithmetical\nReturning to the toy example, the feature space of an instance z is simply \u03c8(z) = (\u2212dx2,\u2212dy2)>, which reflects the size of the output block O.\nAccording to this definition both satisfied and unsatisfied rules contribute to the total reward, and two objects z, z\u2032 that satisfy/violate similar sets of constraints will be close in feature space. The compatibility function f(z) := w>\u03c8(z) computes the (weighted) total reward assigned to z with respect to the constraints. Using this definition, the maximization in the inference (Equation 1) can be seen as attempting to find the output O that maximizes the total reward with respect to the input I and the rules, or equivalently the one with minimum cost. Since \u03c8 can be expressed in terms\nof Satisfiability Modulo Linear Arithmetic, the latter minimization problem can be readily cast as an OMT problem. Translating back to the example, maximizing the compatibility function f boils down to:\nargmax w>\u03c8(z) = argmax (\u2212dx2,\u2212dy2)w = argmin (dx2, dy2)w\nwhich is exactly the cost minimization problem in Equation 4. By selecting a loss function expressible in OMT(LA(Q)), such as the following Hamming loss in feature space:\n\u2206(I,O,O\u2032) := \u2211\nk : \u03d5k is Boolean |1k(I,O)\u2212 1k(I,O\u2032)|+\u2211 k : \u03d5k is arithmetical |ck(I,O)\u2212 ck(I,O\u2032)|\n= \u2016\u03c8(I,O)\u2212\u03c8(I,O\u2032)\u20161\nit turns out that also the separation oracle, Equation. (3), can be cast in the OMT framework. These obervations enable us to apply an OMT solver to implement the two oracles required by the CP algorithm, and thus to efficiently solve the learning task. In particular, the current implementation is based on a vanilla copy of SVMstruct 12, which acts as a cutting-plane solver, whereas inference is implemented with the OptiMathSAT OMT solver."}, {"heading": "5. Experimental Evaluation", "text": "In the following we evaluate LMT on two novel applications that would be difficult to address with less expressive methods, and that stress the ability of LMT to deal with rather complex mixed Boolean-numerical problems."}, {"heading": "5.1. Stairway to Heaven", "text": "In this section we are interested in learning how to assemble different kinds of stairways from examples. For the purpose of this paper, a stairway is simply a collection of m blocks (rectangles) located within a two-dimensional, unit-sized bounding box [0, 1]\u00d7 [0, 1]. Clearly not all possible arrangements of blocks form a stairway; a stairway must satisfy the following conditions: (i) the first block touches either the top or the bottom corner of the left edge\n12http://www.cs.cornell.edu/people/tj/svm light/svm struct.html.\nof the bounding box; (ii) the last block touches the opposite corner at the right edge of the bounding box; (iii) there are no gaps between consecutive blocks; (iv) consecutive blocks must actually form a step and, (v) no two blocks overlap. Note that the property of \u201cbeing a stairway\u201d is a collective property of the m blocks.\nMore formally, each block i = 1, . . . ,m consists of four rational variables: the origin (xi, yi), which indicates the bottom-left corner of the block, a width dxi and a height dyi; the top-right corner of the block is (xi + dxi, yi + dyi). A stairway is simply an assignment to all 4 \u00d7m variables that satisfies the above conditions.\nOur definition does not impose any constraint on the orientation of stairways: it is perfectly legitimate to have left stairways that start at the bottomleft corner of the bounding box and reach the top-right corner, and right stairways that connect the top-left corner to the bottom-right one. For instance, a left 2-stairway can be defined with the following block assignment (see Figure 3 (a)):\n(x1, y1, dx1, dy1) =\n( 0, 1\n2 , 1 2 , 1 2\n) (x2, y2, dx2, dy2) = ( 1\n2 , 0,\n1 2 , 1 2 ) Similarly, a right 2-stairway is obtained with the assignment (Figure 3 (b)):\n(x1, y1, dx1, dy1) =\n( 0, 0, 1\n2 , 1 2\n) (x2, y2, dx2, dy2) = ( 1\n2 , 1 2 , 1 2 , 1 2 ) We also note that the above conditions do not impose any explicit restriction on the width and height of individual blocks (as long as consecutive ones are in contact and there is no overlap). Consequently we allow for both ladder stairways, where the total amount of vertical and horizontal surface of the individual blocks is minimal, as in Figure 3 (a) and (b); and for pillar stairways, where either the vertical or horizontal block lengths are maximized, as in Figure 3 (c). There are of course an uncountable number of intermediate stairways that do not belong to any of the above categories.\nInference amounts to generating a set of variable assignments to all blocks, so that none of conditions (i)-(v) is violated, and the cost of the soft rules is minimized. This can be easily encoded as an OMT(LA(Q)) problem. As a first step, we define a background knowledge of useful predicates. We use four predicates to encode the fact that a block i may touch one of the four corners\nof the bounding box, namely bottom left(i), bottom right(i), top left(i), and top right(i), which can be written as, e.g.:\nbottom right(i) := (xi + dxi) = 1 \u2227 yi = 0\nWe also define predicates to describe the relative positions of two blocks i and j, such as left(i, j):\nleft(i, j) := (xi + dxi) = xj\u2227 ((yj \u2264 yi \u2264 yj + dyj)\u2228 (yj \u2264 yi + dyi \u2264 yj + dyj))\nSimilarly, we also define below(i, j) and over(i, j). Finally, and most importantly, we combine the above predicates to define the concept of step, i.e. two blocks i and i + 1 that are both touching and positioned as to form a stair:\nleft step(i, j) := (left(i, j) \u2227 (yi + dyi) > (yj + dyj)) \u2228 (over(i, j) \u2227 (xi + dxi) < (xj + dxj))\nWe define right step(i,j) in the same manner. For a complete description of the background knowledge, see Table 4.\nThe background knowledge allows to encode the property of being a left stairway as:\ntop left(1) \u2227 \u2227\ni\u2208[1,m\u22121]\nleft step(i, i+ 1) \u2227 bottom right(m)\nAnalogously, any right stairway satisfies the following condition: bottom left(1) \u2227 \u2227\ni\u2208[1,m\u22121]\nright step(i, i+ 1) \u2227 top right(m)\nHowever, our inference procedure does not have access to this knowledge. We rather encode an appropriate set of soft rules (costs) which, along with the associated weights, should bias the optimization towards block assignments that form a stairway of the correct type.\nWe include a few hard rules to constrain the space of admissible block assignments. We require that all blocks fall within the bounding box:\n\u2200i 0 \u2264 xi, dxi, yi, dyi \u2264 1\n\u2200i 0 \u2264 (xi + dxi) \u2264 1 \u2227 0 \u2264 (yi + dyi) \u2264 1\nWe also require that blocks do not overlap:\n\u2200i 6= j (xi + dxi \u2264 xj) \u2228 (xj + dxj \u2264 xi) \u2228 (yi + dyi \u2264 yj) \u2228 (yj + dyj \u2264 yi)\nFinally, we require (without loss of generality) blocks to be ordered from left to right, \u2200i xi \u2264 xi+1.\nNote that only condition (v) is modelled as a hard constraint. The others are implicitly part of the problem cost. Our cost model is based on the observation that it is possible to discriminate between the different stairway types using only four factors: minimum and maximum step size, and amount of horizontal and vertical material. For instance, in the cost we account for both the maximum step height of all left steps (a good stair should not have too high steps):\nmaxshl = m\u00d7 max i\u2208[1,m\u22121] { (yi + dyi)\u2212 (yi+1 + dyi+1) if i, i+ 1 form a left step 1 otherwise\nand the minimum step width of all right steps (good stairs should have sufficiently large steps):\nminswr = m\u00d7 min i\u2208[1,m\u22121] { (xi+1 + dxi+1)\u2212 (xi + dxi) if i, i+ 1 form a left step 0 otherwise\nThe value of these costs depends on whether a pair of blocks actually forms a left step, a right step, or no step at all. Note that these costs are multiplied by the number of blocks m. This allows to renormalize costs according to the number of steps; e.g. the step height of a stair with m uniform steps is half that of a stair with m/2 steps. Finally, we write the average amount of vertical material as vmat = 1\nm \u2211 i dyi. All the other costs can be written\nsimilarly; see Table 5 for the complete list. As we will see, the normalization of individual costs allows to learn weights which generalize to stairs with a larger number of blocks with respect to those seen during training.\nPutting all the pieces together, the complete cost is:\ncost := (maxshl,minshl,maxshr,minshr,\nmaxswl,minswl,maxswr,minswr,\nvmat, hmat) w\nMinimizing the weighted cost implicitly requires the inference engine to decide whether it is preferable to generate a left or a right stairway, thanks to the minshl, . . . ,minswr components, and whether the stairway should be a ladder or pillar, due to vmat and hmat. The actual weights are learned, allowing the learnt model to reproduce whichever stairway type is present in the training data.\nTo test the stairway scenario, we focused on learning one model for each of six kinds of stairway: left ladder, right ladder, left pillar and right pillar with a preference for horizontal blocks, and left pillar and right pillar with vertical blocks. In this setting, the input I is empty, and the model should generate all m blocks as output O during test.\nWe generated \u201cperfect\u201d stairways of 2 to 6 blocks for each stairway type to be used as training instances. We then learned a model using all training instances up to a fixed number of blocks: a model using examples with up to 3 blocks, another with examples of up to 4, etc., for a total of 4 models per stairway type. Then we analyzed the generalization ability of the learnt models by generating stairways with a larger number of blocks (up to 10) than those in the training set. The results can be found in Figure 5.1.\nThe experiment shows that LMT is able to solve the stairway construction problem, and can learn appropriate models for all stairway types, as expected. As can be seen in Figure 5.1, the generated stairways can present some imperfections when the training set is too small (e.g., only two training examples; first row of each table), especially in the 10 output blocks case. However, the situation quickly improves when the training set increases: models learned with four training examples are always able to produce perfect 10-block stairways of the same kind. Note again that the learner has no explicit notion of what a stairway is, but just the values of step width, height and material for some training examples of stairways.\nAll in all, albeit simple, this experiment showcases the ability of LMT to handle learning in hybrid Boolean-numerical domains, whereas other formalisms are not particularly suited for the task. As previously mentioned, the Church [12] language allows to encode arbitrary constraints over both numeric and Boolean variables. The stairway problem can indeed be encoded in Chuch in a rather straightforward way. However, the sampling strategies used for inference are not conceived for performing optimization with hard continuous constraints. Even the simple task of generating two blocks,\nconditioned on the fact that they form a step, is prohibitively expensive. 13"}, {"heading": "5.2. Learning to Draw Characters", "text": "In this section we are concerned with automatic character drawing, a novel structured-output learning problem that consists in learning to translate any input noisy hand-drawn character into its symbolic representation. More specifically, given an unlabeled black-and-white image of a handwritten letter or digit, the goal is to construct an equivalent vectorial representation of the same character.\nIn this paper, we assume the character to be representable by a polyline made of a given number m of directed segments, i.e. segments identified by a starting point (xb, yb) and an ending point (xe, ye). The input image I is seen as the set P of coordinates of the pixels belonging to the character, while the output O is a set of m directed segments {(xbi , ybi , xei , yei )}mi=1. Just like in the previous section, we assume all coordinates to fall within the unit bounding box.\nIntuitively, any good output O should satisfy the following requirements: (i) it should be as similar as possible to the noisy input character; and (ii) it should actually \u201clook like\u201d the corresponding vectorial character. Here we interpret the first condition as implying that the generated segments should cover as many pixels of the input image as possible (although alternative interpretations are possible). Under this interpretation, we can informally write the inference problem as follows:\nargmaxO (coverage(I,O), orientation(O)) w\nwhere the orientation term encodes information on the orientation of the segments which should be useful for computing the \u201clooking like\u201d condition. In the following, we will detail how to formulate and compute these two quantities.\nSince the output is supposed to be a polyline, we constrain consecutive segments to be connected, i.e. to share an endpoint:\n\u2200i connected(i, i+ 1)\nWe also want the segments to be no larger than the image, nor smaller than a pixel: \u2200i min length \u2264 length(i) \u2264 1. Additionally, we constrain\n13We interrupted the inference process after 24 hours of computation.\n(without loss of generality) each segment to be ordered from left to right, i.e. xbi \u2264 xei . Finally, we restrict the segments to be either horizontal, vertical or 45\u25e6 diagonal, that is:\n\u2200i horizontal(i) \u2228 vertical(i) \u2228 diagonal(i)\nThis restriction allows us express all numerical constraints in linear terms. All the predicates used above are defined as in Table 8.\nUnder these assumptions, we can encode the coverage reward as:\ncoverage(I,O) := 1 |P | \u2211 p\u2208P 1(covered(p))\nwhere covered(p) is true if pixel p is covered by at least one of the m segments: covered(p) := \u2228\ni\u2208[1,m]\ncovered(p, i)\nHere we normalize coverage to allow generalizing between training and test images with different number of observed pixels. The fact that a segment i = (xbi , y b i , x e i , y e i ) covers pixel p = (x, y) implicitly depends on the orientation of the segment, and is computed using constructs like:\nIf horizontal(i) then covered(p, i) := xbi \u2264 x \u2264 xei \u2227 y = ybi The coverage formulae for the other segment types can be found in Table 8.\nAs for the orientation term, it should contain features related to the vectorial representation of characters. These include both the direction of the individual segments and the connections between pairs of segments. As an example, consider this possible description of \u201clooking like an A\u201d:\nincreasing(1) \u2227 h2t(1, 2) \u2227 increasing(2) \u2227 h2t(2, 3) \u2227 decreasing(3) \u2227 h2h(3, 4) \u2227 horizontal(4) \u2227 h2t(4, 5) \u2227 decreasing(5)\nHere increasing(i) and decreasing(i) indicate the direction of segment i, and can be written as:\nincreasing(i) := yei > y b i decreasing(i) := yei < y b i\nThen we have connections between pairs of segments. We encode connection types following the convention used for Bayesian Networks, where the head of a directed segment is the edge containing the arrow (represented by ending point (xe, ye)) and the tail is the opposite edge (the starting point (xb, yb)). For instance, h2t(i, j) indicates that i is head-to-tail with respect to j, h2h(i, j) that they are head-to-head:\nh2t(i, j) := (xei = x b j) \u2227 (yei = ybj) h2h(i, j) := (xei = x e j) \u2227 (yei = yej )\nFor a pictorial representation of the \u201clooking like an A\u201d constraint, see Figure 5.2 (b). We include a number of other, similar predicates in the background knowledge; for a full list, see Table 8.\nFor example, suppose we have a 8\u00d7 8 image of an upper-case \u201cA\u201d, as in\nFigure 5.2 (a). A character drawing algorithm should decide how to overlay 5 segments on top of the input image according to the previous two criteria. A good output would look like the one pictured in Figure 5.2 (c).\nHowever, the formula for the \u201clooking like an A\u201d constraint is not available at test time and should be learned from the data. In order to do so, the orientation term includes possible directions (increasing, decreasing, right) for all m segments and all possible connection types between consecutive segments (h2t, h2h, t2t, t2h14). Note that we do not include detailed segment orientation (i.e., horizontal, vertical, diagonal) in the feature space to accommodate for alternative vectorial representations of the same letter. For instance, the first segment in an \u2018A\u2019, which due to the left-toright rule necessarily fits the lower left portion of the character, is bound to be increasing, but may be equally likely vertical or diagonal (see e.g.\nwhere each feature is the indicator function of the corresponding Boolean variable, e.g. increasing(1) := 1(increasing(1)) (see Table 9).\nWe evaluated LMT on the character drawing problem by carrying out an extensive experiment using a set of noisy B&W 16 \u00d7 20 character images15. The dataset includes 39 instances of handwritten images of each alphanumerical character. We downscaled the images to 8 \u00d7 8 for speeding up experiments. Learning to draw characters is a very challenging constructive problem, and the reduced resolution makes it even harder as characters are less accurate and sometimes miss some relevant parts. In this experiment we learn a model for each of the first five letters of the alphabet (A to E), and assess the ability of LMT to generalize over unseen handwritten images of the same character.\nWe selected for each letter five images at random out of the 39 available to be employed as training instances. For each of these, we used OptiMathSAT to generate a \u201cperfect\u201d vectorial representation according to a human-provided letter template (similar to the \u201clooking like an A\u201d rule above), obtaining a set of five fully supervised images. The resulting supervision obtained with this procedure is, in some cases, very noisy \u2014 as can be seen in Figures 10, 11, and 12 (first row of each table) \u2014 and depends crucially on the quality of the character image. This is particularly relevant for the \u201cB\u201d, where the downscaled images tend to loose relevant portions of the character.\nThen we learned a model for each letter, and used it to infer the vectorial representation for each of the remaining 33 images. The number of segments m was known during both training and inference. In particular, we used 5 segments for A, C, and E, 4 segments for the D, and 6 for the B. The output for all letters can be found in Figures 10, 11, and 12 (second to last rows of each Table).\nThe images show that LMT is able to address the character drawing problem and produce reasonable outputs for all the target letters. For the simplest letter, \u201cD\u201d (drawn with only four segments), LMT is able to learn a correct template from a set of quite different training instances, and achieve the required generalization; while three of the five training instances have diagonal segments, the learned model opted for using vertical and horizontal segments only on all test instances to maximize the coverage (only one case\n15Dataset taken from http://cs.nyu.edu/\u223croweis/data.html\nlooks clearly wrong; fourth row, first column of Figure 11.) The results for the C are also very promising.\nMore complex letters like the A and E still see a large degree of success, with a few exceptions where LMT does not attempt to cover the whole image and ends up drawing the segments too close. This follows from two facts: (i) the supervision contains a large number of bad examples, which are mimicked in the outputs; and (ii) not to overly complicate the experiment, we did not include any cost term favoring non-consecutive segments that are distant enough (although it would be possible).\nThe worst results were obtained for the B, where the learned model failed to capture the layout of B. This is due to the effect of rescaling on both the training and test images. Summarizing, excluding all pathologically bad inputs, LMT is able to learn an appropriate model and generalize.\nTo speed up the computations we imposed a 2 minute timeout on the separation oracle used for training. Consequently most invocations to the separation routine did return an approximate solution. Analyzing the weights, we found that this change had little effect on the learned model (data not shown). This can be explained by observing that the cutting-plane algorithm does not actually require the separation oracle to be perfect: as long as the approximation is consistent with the hard rules (which is necessarily the case even in sub-optimal solutions), the constraint added to the working set W at each iteration still restricts the quadratic program in a sound manner (see Algorithm 1). As a consequence, the learning procedure is still guaranteed to find an -approximate solution, but it may take more iterations to converge. This trick allows training to terminate very quickly (in the order of minutes for a set of five training examples). Furthermore, it enables the user to fine tune the trade-off between separation complexity and number of QP sub-problems to be solved.\nIn this paper we only consider the case where the training data is labeled with a fully observed vectorial letter. Our method, however, can in principle be extended to work with partially observed supervision, e.g. with training instances labeled only with the character type, in order to discover the vectorial representation. More on this point can be found in Section 6."}, {"heading": "6. Conclusions", "text": "In this work we presented a novel class of methods for structured learning problems with mixed Boolean and numerical variables. These methods,\ntermed Learning Modulo Theories, are based on a combination of structuredoutput SVMs and Satisfiability Modulo Theories. In contrast to classical First-Order Logic, SMT allows to natively describe, and reason over, numerical variables and mixed logical/arithmetical constraints. By leveraging the existing industrial-grade MAX-SMT and OMT solvers, LMT is well-suited for dealing with hybrid constructive learning problems, avoiding the combinatorial explosion of the state space that would affect an equivalent FOL formulation. Experimental results on both artificial and real world datasets show the potential of this approach. This learning framework has a wide range of potential applications, from robotics to computational creativity and design.\nThis work can be extended in a number of directions. First, the current formulation assumes knowledge of the desired output for training examples. This requirement can be loosened by introducting latent variables for the unobserved part of the output, to be maximized over during training [42]. Second, OMT is currently limited to quantifier free formulas and linear algebra for what concerns numeric theories. The former requires to ground all predicates before performing inference, while the latter prevents the formulation of non-linear constraints, e.g. on areas and Euclidean distances. Some attempts to extend SMT solvers to both quantified formulas [38, 39, 40] and non-linear arithmetic [43, 44] have been presented in the literature; although the state of the art of these estensions is not satisfactory yet, we can think of extending our techniques in these directions as soon as the underlying SMT technology gets mature enough. Finally, LMT is currently focused on the task of finding the maximal configuration, and cannot compute marginal probabilities. We are planning to introduce support for probability computation by leveraging ideas from weighted model counting [45]."}, {"heading": "Acknowledgements", "text": "We would like to thank Luc De Raedt, Guy Van den Broeck and Bernd Gutmann for useful discussions."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Modelling problems containing a mixture of Boolean and numerical variables<lb>is a long-standing interest of Artificial Intelligence. However, performing<lb>inference and learning in hybrid domains is a particularly daunting task.<lb>The ability to model this kind of domains is crucial in \u201clearning to design\u201d<lb>tasks, that is, learning applications where the goal is to learn from examples<lb>how to perform automatic de novo design of novel objects. In this paper we<lb>present Structured Learning Modulo Theories, a max-margin approach for<lb>learning in hybrid domains based on Satisfiability Modulo Theories, which<lb>allows to combine Boolean reasoning and optimization over continuous linear<lb>arithmetical constraints. We validate our method on artificial and real world<lb>scenarios.<lb>", "creator": "LaTeX with hyperref package"}}}