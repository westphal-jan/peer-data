{"id": "1405.3167", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2014", "title": "Clustering, Hamming Embedding, Generalized LSH and the Max Norm", "abstract": "we study the negative partial regression \u03b5 and \u03c3 embedding, testing on the asymmetric case ( \u03bc - clustering \u2212 asymmetric harmonic embedding ), understanding marginal relationship to regression as studied cases ( charikar 2002 ) and integrating the max - entropy problem, possibly generating results underlying biased symmetric and variance versions.", "histories": [["v1", "Tue, 13 May 2014 14:36:59 GMT  (38kb)", "http://arxiv.org/abs/1405.3167v1", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["behnam neyshabur", "yury makarychev", "nathan srebro"], "accepted": false, "id": "1405.3167"}, "pdf": {"name": "1405.3167.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["bneyshabur@ttic.edu", "yury@ttic.edu", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n31 67\nv1 [\ncs .L\nG ]\n1 3\nM ay\n2 01\n4\nKeywords: Clustering, Hamming Embedding, LSH, Max Norm"}, {"heading": "1 Introduction", "text": "Convex relaxations play an important role in designing efficient learning and recovery algorithms, as well as in statistical learning and online optimization. It is thus desirable to understand the convex hull of hypothesis sets, to obtain tractable relaxation to these convex hulls, and to understand the tightness of such relaxations.\nIn this paper we consider convex relaxations of two important problems, namely clustering and hamming embedding, and study the convex hulls of the corresponding hypothesis classes: of cluster incidence matrices and of similarity measures with a short hamming embedding. In section 2 we introduce these classes formally, and understand the relationship between them, showing how hamming embedding can be seen as a generalization of clustering. In section 3 we discuss their convex hull and its relationship to notion of Locality Sensitive Hashing (LSH) as studied by [6]. There has been several studies on different aspects of LSH (e.g.[16,10,7]).\nMore specifically, we focus on the asymmetric versions of these classes, which correspond to co-clustering (e.g. [11,3]) and asymmetric hamming embedding as recently introduced by [15]. We define the corresponding notion of an Asymmetric LSH, and show how it could be much more powerful then standard (symmetric) LSH (section 4).\nOur main conclusion is that the convex hull of asymmetric clustering and hamming embedding is tightly captured by a shift-invariant modification of the max-norm\u2014a tractable SDP-representable relaxation (Theorem 2 in section 5). We contrast this with the symmetric case, in which the corresponding SDP relaxation is not tight, highlighting an important distinction between symmetric and asymmetric clustering, embedding and LSH."}, {"heading": "2 Clustering and Hamming Embedding", "text": "In this section we introduce the problems of clustering and hamming embedding, providing a unified view of both problems, with hamming embedding being viewed as a direct generalization of clustering. Our starting point, in any case, is a given similarity function sim : S \u00d7 S \u2192 [\u22121,+1] over a (possibly infinite) set of objects S. \u201cClustering\u201d, as we think of it here, is the problem of partitioning the elements of S into disjoint clusters so that items in the same cluster are similar while items in different clusters are not similar. \u201cHamming Embedding\u201d is the problem of embedding S into some hamming space such that the similarity between objects is captured by the hamming distance between their mappings."}, {"heading": "2.1 Clustering", "text": "We represent a clustering of S as a mapping h : S \u2192 \u0393 , where \u0393 is a discrete alphabet. We can think of h as a function that assigns a cluster identity to each element, where the meaning of the different identities is arbitrary. The alphabet \u0393 might have a fixed finite cardinality |\u0393 | = k, if we would like to have a clustering with a specific number of clusters. E.g., a binary alphabet corresponds to standard graph partitioning into two clusters. If |\u0393 | = k, we can assume that \u0393 = [k]. The alphabet \u0393 might be infinitely countable (e.g. \u0393 = N), in which case we are not constraining the number of clusters.\nThe cluster incidence function \u03bah : S\u00d7S \u2192 {\u00b11} associated with a clustering h is defined as \u03bah(x, y) = 1 if h(x) = h(y) and \u03bah(x, y) = \u22121 otherwise. For a finite space S of cardinality n = |S| we can think of \u03bah \u2208 {\u00b11}n\u00d7n as a permuted block-diagonal matrix. We denote the set of all valid cluster incidence functions over S with an alphabet of size k (i.e. with at most k clusters) as MS,k = {\u03bah | h : S \u2192 [k]}, where k = \u221e is allowed.\nWith this notion in hand, we can think of clustering as a problem of finding a cluster incidence function \u03bah that approximates a given similarity sim, as quantified by objectives minEx,y[|\u03bah(x, y)\u2212 sim(x, y)|] or maxEx,y[sim(x, y)\u03bah(x, y)] (this is essentially the correlation clustering objective). Since objectives themselves are convex in \u03ba, but the constraint that \u03ba is a valid cluster incidence function is not a convex constraint, a possible approach is to relax the constraint that \u03ba is a valid cluster incidence function, or in the finite case, a cluster incidence matrix. This is the approach taken by, e.g. [12,13], who relax the constraint to a trace-norm and max-norm constraint respectively. One of the questions we will be exploring here is whether this is the tightest relaxation possible, or whether there is a significantly tighter relaxation."}, {"heading": "2.2 Hamming Embedding and Binary Matrix Factorization", "text": "In the problem of binary hamming embedding (also known as binary hashing), we want to find a mapping from each object x \u2208 S to binary string b(x) \u2208 {\u00b11}d\nsuch that similarity between strings is approximated by the hamming distance between their images:\nsim(x, y) \u2248 1\u2212 2\u03b4Ham(b(x), b(y)) d\n(1)\nCalculating the hamming distance of two binary hashes is an extremely fast operation, and so such a hash is useful for very fast computation of similarities between massive collections of objects. Furthermore, hash tables can be used to further speed up retrieval of similar objects.\nBinary hamming embedding can be seen as a generalization of clustering as follows: For each position i = 1, . . . , d in the hash, we can think of bi(x) as a clustering into two clusters (i.e. with \u0393 = {\u00b11}). The hamming distance is then an average of the d cluster incidence functions:\n1\u2212 2\u03b4Ham(b(x), b(y)) d = 1 d\nd \u2211\ni=1\n\u03babi(x, y).\nOur goal then is to approximate a similarity function by an average of d binary clusterings. For d = 1 this is exactly a binary clustering. For d > 1, we are averaging multiple binary clusterings.\nSince we have \u3008b(x), b(y)\u3009 = d \u2212 2\u03b4Ham(b(x), b(y)), we can formulate the binary hashing problem as a binary matrix factorization where the goal is to approximate the similarity matrix by a matrix of the form RR\u22a4, where R is a d-dimensional binary matrix:\nmin R\n\u2211\nij\nerr(sim(i, j), X(i, j))\ns.t X = RR\u22a4 R \u2208 {\u00b11}n\u00d7d (2)\nwhere err(x, y) is some error function such as err(x, y) = |x\u2212 y|. Going beyond binary clustering and binary embedding, we can consider hamming embeddings over larger alphabets. That is, we can consider mappings b : S \u2192 \u0393 d, where we aim to approximate the similarity as in (1), recalling that the hamming distance always counts the number of positions in which the strings disagree. Again, we have that the length d hamming embeddings over a (finite or infinitely countable) alphabet \u0393 correspond to averages of d cluster incidence matrices over the same alphabet \u0393 ."}, {"heading": "3 Locality Sensitive Hashing Schemes", "text": "Moving on from a finite average of clusterings, with a fixed number of components, as in hamming embedding, to an infinite average, we arrive at the notion of LSH as studied by [6].\nGiven a collection S of objects, an alphabet \u0393 and a similarity function sim : S \u00d7 S \u2192 [\u22121, 1] such that for any x \u2208 S we have sim(x, x) = 1,a locality sensitive hashing scheme (LSH) is a probability distribution on the family of clustering functions (hash functions) H = {h : S \u2192 \u0393} such that [6]:\nEh\u2208H[\u03bah(x, y)] = sim(x, y). (3)\n[6] discuss similarity functions sim : S \u00d7 S \u2192 [0, 1] as so require\nPh\u2208H[h(x) = h(y)] = sim(x, y).\nThe definition (3) is equivalent, except it applies to the transformed similarity function 2sim(x, y)\u2212 1.\nThe set of all locality sensitive hashing schemes with an alphabet of size k is nothing but the convex hull of the set MS,k of cluster incidence matrices.\nThe importance of an LSH, as an object in its own right as studied by [6], is that a hamming embedding can be obtained from an LSH by randomly generating a finite number of hash functions from the distribution over the family H. In particular, if we draw h1, . . . , hd i.i.d. from an LSH, then the length-d hamming embedding b(x) = [h1(x), . . . , hd(x)] has expected square error\nE[(sim(x, y)\u2212 1 d \u2211 \u03bahd(x, y)) 2] \u2264 1 d , (4)\nwhere the expectation is w.r.t. the sampling, and this holds for all x, y, and so also for any average over them."}, {"heading": "3.1 \u03b1-LSH", "text": "If the goal is to obtain an low-error embedding, the requirement (3) might be too harsh. If we are willing to tolerate a fixed offset between our embedding and the target similarity, we can instead require that\n\u03b1Eh\u2208H[\u03bah(x, y)]\u2212 \u03b8 = sim(x, y). (5)\nwhere \u03b1, \u03b8 \u2208 R, \u03b1 > 0. A distribution over h that obeys (5) is called an \u03b1-LSH. We can now verify that, for h1, . . . , hd drawn i.i.d. from an \u03b1-LSH, and any x, y \u2208 S:\nE\n[\n( sim(x, y)\u2212 (\u03b1 d \u2211 \u03bahd(x, y)\u2212 \u03b8) )2\n]\n\u2264 \u03b1 2\nd . (6)\nThe length of the LSH required to acheive accurate approximation of a similarity function thus scales quadartically with \u03b1, and it is therefor desireable to obtain an \u03b1-LSH with as low an \u03b1 as possible (note that sim(x, x) = 1, implies \u03b8 = \u03b1\u22121, and so we must allow a shift if we want to allow \u03b1 6= 1).\nUnfortunately, even the requirement (5) of an \u03b1-LSH is quite limiting and difficult to obey, as captured by the following theorem, which is based on lemmas 2 and 3 of [6]:\nClaim 1. For any finite or countable alphabet \u0393 , k = |\u0393 | \u2265 2, a similarity function sim has an \u03b1-LSH over \u0393 for some \u03b1 if and only if D(x, y) = 1\u2212sim(x,y)2 is embeddable to hamming space with no distortion.\nProof. Given metric spaces (X, d) and (X, d\u2032) any map f : X \u2192 X \u2032 is called a metric embedding. The distortion of such an embedding is defined as:\n\u03b2 = max x,y\u2208X\nd(x, y)\nd\u2032(f(x), f(y)) . max x,y\u2208X\nd\u2032(f(x), f(y))\nd(x, y)\nWe first show that if there exist an \u03b1-LSH for function sim(x, y) then 1\u2212sim(x,y)2 is embeddable to hamming space with no distortion. An \u03b1-LSH for function sim(x, y) corresponds to an LSH for function 1 \u2212 1\u2212sim(x,y)\u03b1 . Using lemma 3 in [6], we can say that 1\u2212sim(x,y)\u03b1 can be isometrically embedded in the Hamming cube which means 1 \u2212 sim(x, y) can be embedded in Hamming cube with no distortion.\nEh\u223cDH [\u03bah(x, y)] = 2Ph\u223cDH [h(x) = h(y)]\u2212 1 = 1\u2212 2Ph\u223cDH [h(x) 6= h(y)] = 1\u2212 2dH(x, y)\n= 1\u2212 2d(x, y) \u03b2\n= 2 \u03b2 ( \u03b2 2 \u2212 d(x, y))\n= 2 \u03b2 (1\u2212 d(x, y) + \u03b2 2 \u2212 1)\n= sim(x, y) + \u03b8\n\u03b8 + 1\nAs a result of Claim 1, it can be shown that given any large enough set of low dimensional unit vectors, there is no \u03b1-LSH for the Euclidian inner product.\nClaim 2. Let {x(1), . . . , x(n)} be an arbitrary set of unit vectors in the unit sphere. Let Zij = \u3008x(i), x(j)\u3009 for 1 \u2264 i, j \u2264 n. If d < log2 n, then there is no \u03b1-LSH for Z.\nProof. According to [8] (see also [4]), if d < log2 n then in any set of n points in d-dimensional Euclidian space, there exist at least three points that form an obtuse triangle. Equivalently, there exist three vectors x, y and z in any set of n different d-dimensional unit vectors such that:\n\u3008z \u2212 x, z \u2212 y\u3009 < 0\nWe rewrite the above inequality as:\n(1\u2212 \u3008z, x\u3009) + (1\u2212 \u3008z, y\u3009) < (1\u2212 \u3008x, y\u3009)\nThe above inequality implies that the distance measure \u2206ij = (1\u2212Zij)/2 is not a metric. Consequently, according to Claim 1 since \u2206ij = (1 \u2212 Zij)/2 is not a metric, there is no \u03b1-LSH for the matrix Z.\nAs noted by [6] (and stated in claim 2), we can therefore unfortunately conclude that there is no \u03b1-LSH for several important similarity measures such as the Euclidian inner product, Overlap coefficient and Dice\u2019s coefficient. Note that based on Claim 1, even a finite positive semidefinite similarity matrix is not necessarily \u03b1-LSHable."}, {"heading": "3.2 Generalized \u03b1-LSH", "text": "In the following section, we will see how to break the barrier imposed by Claim 1 by allowing asymmetry, highlighting the extra power asymmetry affords us. But before doing so, let us consider a different attempt at relaxing the definition of an \u03b1-LSH, motivated by to the work of [5] and [1]: in order to uncouple the shift \u03b8 from the scaling \u03b1, we will allow for a different, arbitrary, shift on the self-similarities sim(x, x) (i.e. on the diagonal of sim).\nWe say that a probability distribution over H = {h : S \u2192 \u0393} is a Generalized \u03b1-LSH, for \u03b1 > 0 if there exist \u03b8, \u03b3 \u2208 R such that for all x, y:\n\u03b1Eh\u2208H[\u03bah(x, y))] = sim(x, y) + \u03b8 + \u03b31x=y\nWith this definition, then any symmetric similarity function, at least over a finite domain, admits a Generalized \u03b1-LSH, with a sufficiently large \u03b1:\nClaim 3. For a finite set S, |S| = n, for any symmetric sim : S \u00d7 S \u2192 [\u22121, 1] with sim(x, x) = 1, there exists a Generalized \u03b1-LSH over a binary alphabet \u0393 (|\u0393 | = 2) where \u03b1 = O((1\u2212\u03bbmin) logn)-LSH, and \u03bbmin is the smallest eigenvalue of the matrix sim.\nProof. We observe that sim\u2212\u03bbminI is a positive semidefinite matrix. According to [5], if a matrix Z with unit diagonal is positive semidefinite, then there is a probability distribution over a family H of hash functions such that for any x 6= y:\nEh\u2208H[h(x)h(y)] = Z(x, y)\nC logn .\nWe let Z(x, y) = (sim(x, y) \u2212 \u03bbmin1x=y)/(1 \u2212 \u03bbmin). Matrix Z is positive semidefinite and has unit diagonal. Hence, there is a probability distribution over a family H of hash functions such that\nEh\u2208H[h(x)h(y)] = sim(x, y)\u2212 \u03bbmin1x=y C(1\u2212 \u03bbmin) logn ,\nequivalently\n(C(1 \u2212 \u03bbmin) log n) \u00b7 Eh\u2208H[\u03bah(x, y))] = sim(x, y)\u2212 \u03bbmin1x=y.\nIt is important to note that \u03bbmin could be negative, and as low as \u03bbmin = \u2212\u2126(n). The required \u03b1 might therefor be as large as \u2126(n), yielding a terrible LSH."}, {"heading": "4 Asymmetry", "text": "In order to allow for greater power, we now turn to Asymmetric variants of clustering, hamming embedding, and LSH.\nGiven two collections of objects S, T , which might or might not be identical, and an alphabet \u0393 , an asymmetric clustering (or co-clustering [11]) is specified by pair of mappings f : S \u2192 \u0393 and g : T \u2192 \u0393 and is captured by the asymmetric cluster incidence matrix \u03baf,g(x, y) where \u03baf,g(x, y) = 1 if f(x) = g(y) and \u03baf,g(x, y) = \u22121 otherwise. We denote the set of all valid asymmetric cluster incidence functions over S, T with an alphabet of size k as M(S,T ),k = {\u03baf,g | f : S \u2192 [k], g : T \u2192 [k]}, where we again also allow k = \u221e to correspond to a countable alphabet \u0393 = N.\nLikewise, an asymmetric binary embedding of S, T with alphabet \u0393 consists of a pair of functions f : S \u2192 \u0393 d, g : T \u2192 \u0393 d, where we approximate a similarity as:\nsim(x, y) \u2248 1\u2212 2\u03b4Ham(f(x), g(y)) d = 1 d\nd \u2211\ni=1\n\u03bafi,gi(x, y). (7)\nThat is, in asymmetric hamming embedding, we approximate a similarity as an average of d asymmetric cluster incidence matrices from M(S,T ),k.\nIn a recent work, [15] showed that even when S = T and the similarity function sim is a well-behaved symmetric similarity function, asymmetric binary embedding could be much more powerful in approximating the similarity, using shorter lengths d, both theoretically and empirically on data sets of interest. That is, these concepts are relevant and useful not only in an a-priory asymmetric case where S 6= T or sim is not symmetric, but also when the target similarity is symmetric, but we allow an asymmetric embedding. We will soon see such gaps also when considering the convex hulls of MS,k and M(S,T ),k, i.e. when considering LSHs. Let us first formally define an asymmetric \u03b1-LSH.\nGiven two collections of objects S and T , an alphabet \u0393 , a similarity function sim : S\u00d7T \u2192 [\u22121, 1], and \u03b1 > 0, we say that an \u03b1-ALSH is a distribution over pairs of functions f : S \u2192 \u0393 , g : T \u2192 \u0393 , or equivalently over M(S,T ),|\u0393 |, such that for some \u03b8 \u2208 R and all x \u2208 S, y \u2208 T :\n\u03b1E(f,g)\u2208F\u00d7G [\u03baf,g(x, y))]\u2212 \u03b8 = sim(x, y). (8)\nTo understand the power of asymmetric LSH, recall that many symmetric similarity functions do not have an \u03b1-LSH for any \u03b1. On the other hand, any similarity function over finite domains necessarily has an \u03b1-ALSH:\nClaim 4. For any similarity function sim : S \u00d7 T \u2192 [\u22121, 1] over finite S, T , there exists an \u03b1-ALSH with \u03b1 \u2264 min{|S|, |T |}\nThis is corollary of Theorem 2 that will be proved later in section 5. The proof follows from Theorem 2 the following upper bound on the max-norm:\n\u2016Z\u2016max \u2264 rank(Z).\u2016Z\u20162\u221e\nwhere \u2016Z\u20162\u221e = maxx,y |Z(x, y)|. In section 3, we saw that similarity functions that do not admit an \u03b1-LSH, still admit Generalized \u03b1-LSH. However, the gap between the \u03b1 required for a Generalized \u03b1-LSH and that required for an \u03b1-ALSH might be as large as \u2126(|S|):\nTheorem 1. For any even n, there exists a set S of n objects and a similarity Z : S \u00d7 S \u2192 R such that\n\u2013 there is a binary 3KR-ALSH for Z, where KR \u2248 1.79 is Krivine\u2019s constant; \u2013 there is no Generalized \u03b1-LSH for any \u03b1 < n\u2212 1.\nProof. Let S = [n] and Z be the following similarity matrix:\nZ = 2In\u00d7n +\n[\n\u22121n 2 \u00d7n 2 1n 2 \u00d7n 2\n1n 2 \u00d7n 2 \u22121n 2 \u00d7n 2\n]\nNow we use Theorem 2, which we will prove later (our proof of Theorem 2 does not rely on the proof of this theorem). Using triangle inequality property of the norm, we have \u2016Z\u2016max \u2264 \u2016Z\u2212 2In\u00d7n\u2016max+ \u20162In\u00d7n\u2016max = 3; and by Theorem 2 there is a 3KR-ALSH for Z. Looking at the decomposition of Z, it is not difficult to see that the smallest eigenvalue of Z is 2\u2212 n. So in order to have a positive semidefinite similarity matrix, we need \u03b3 to be at least n\u22122 and \u03b8 to be at least \u22121 (otherwise the sum of elements of Z + \u03b8+(n\u2212 2)I will be less than zero and so Z + \u03b8 + (n \u2212 2)I will not be positive semidefinite). So \u03b1 = \u03b8 + \u03b3 is at least n\u2212 1."}, {"heading": "5 Convex Relaxations, \u03b1-LSH and Max-norm", "text": "We now turn to two questions which are really the same: can we get a tight convex relaxation of the set M(S,T ),k of (asymmetric) clustering incidence functions, and can we characterize the values of \u03b1 for which we can get an \u03b1-ALSH for a particular similarity measure.\nFor notational simplicity, we will now fix S and T and use Mk to denote M(S,T ),k."}, {"heading": "5.1 The Ratio Function", "text": "The tightest possible convex relaxation of Mk is simply its convex hull convMk. Assuming P 6= NP, convMk is not polynomially tractable. What we ask here is\nwhether he have a tractable tight relaxation of convMk. To measure tightness of some convex B \u2287 Mk, for each Z \u2208 B, we will bound its cluster ratio:\n\u03c1k(Z) = min{r|Z \u2208 r convMk} = min{r|Z/r \u2208 convMk}.\nThat is, by how much to we have to inflate Mk so that includes Z \u2208 B. The supremum \u03c1k(B) = supZ\u2208B \u03c1k(Z) is then the maximal inflation ratio between convMk and B, i.e. such that convMk \u2286 B \u2286 \u03c1k convMk. Similarly, we define the centralized cluster ratio as:\n\u03c1\u0302k(Z) = min \u03b8\u2208R\nmin{r|Z \u2212 \u03b8 \u2208 r convMk}.\nThis is nothing but the lowest \u03b1 for which we have an \u03b1-ALSH:\nClaim 5. For any similarity function sim(x, y), \u03c1\u0302k(sim) is equal to the smallest \u03b1 s.t. there exists an \u03b1-ALSH for sim over alphabet of cardinality k.\nProof. We write the problem of minimizing \u03b1 in \u03b1-ALSH as:\nmin \u03b8\u2208R,\u03b1\u2208R+\n\u03b1\ns.t sim(x, y) = \u03b1E(f,g)\u2208F\u00d7G [\u03baf,g(x, y)]\u2212 \u03b8 (9)\nWe know that:\nE(f,g)\u2208F\u00d7G [\u03baf,g(x, y)] = \u2211\nf\u2208MS,k\n\u2211\ng\u2208MT,k\n\u03baf,g(x, y)p(f, g)\nwhere p(f, g) is the joint probability of hash functions f and g. Define \u00b5(f, g) = \u03b1p(f, g) and write:\n\u03b1 = \u03b1 \u2211\nf\u2208MS,k\n\u2211\ng\u2208MT,k\np(f, g) = \u2211\nf\u2208MS,k\n\u2211\ng\u2208MT,k\n\u03b1p(f, g) = \u2211\nf\u2208MS,k\n\u2211\ng\u2208MT,k\n\u00b5(f, g)\nWe have:\n\u03b1 \u2211\nf\u2208MS,k\n\u2211\ng\u2208MT,k \u03baf,g(x, y)p(f, g)\u2212 \u03b8 =\n\u2211\nf\u2208MS,k\n\u2211\ng\u2208MT,k \u03baf,g(x, y)\u00b5(f, g)\u2212 \u03b8\nSubstituting the last two equalities into formulation 9 gives us the formulation for centralized cluster ratio.\nOur main goal in this section is to obtain tight bounds on \u03c1k(Z) and \u03c1\u0302k(Z).\nThe Ratio Function and Cluster Norm The convex hull convMk is related to the cut-norm, and its generalization the cluster-norm, and although the two are not identical, its worth understanding the relationship.\nFor k = 2, the ratio function is a norm, and is in fact the dual of a modified cut-norm:\n\u03c1\u22172(W ) = \u2016W\u2016C,2 = max u:S\u2192{\u00b11},v:S\u2192{\u00b11}\n\u2211\nx\u2208S,y\u2208T W (x, y)u(x)v(y) (10)\nThe norm \u2016W\u2016C,2 is a variant of the cut-norm, and is always within a factor of four from the cut-norm as defined in, e.g. [1]. The set convM2 in this case is the unit ball of the modified cut-norm.\nFor k > 2, the ratio function is not a norm, since Mk, for k > 2, is not symmetric about the origin: we might have Z \u2208 Mk but \u2212Z 6\u2208 Mk and so \u03c1k(Z) 6= \u03c1k(\u2212Z). A ratio function defined with respect the symmetric convex hull of conv(Mk \u222a \u2212Mk), is a norm, and is dual to the following cluster norm, which is a generalization of the modified cut-norm:\n\u2016W\u2016C,k = max u:S\u2192\u0393,v:S\u2192\u0393\n\u2211\nx\u2208S,y\u2208T W (x, y)\u03bau,v(x, y) (11)"}, {"heading": "5.2 A Tight Convex Relaxation using the Max-Norm", "text": "Recall that the max-norm (also known as the \u03b32 : \u21131 \u2192 \u2113\u221e norm) of a matrix is defined as [18]:\n\u2016Z\u2016max = min UV \u22a4 max(\u2016U\u201622,\u221e, \u2016V \u201622,\u221e)\nwhere \u2016U\u20162,\u221e is the maximum \u21132 norm of rows of the matrix U . The maxnorm is SDP representable and thus tractable [17]. Even when S and T are not finite, and thus sim is not a finite matrix, the max-norm can be defined as above, where now U and V can be thought of as mappings from S and T respectively into a Hilbert space, with sim(x, y) = (UV \u22a4)(x, y) = \u3008U(x), V (y)\u3009 and \u2016U\u20162,\u221e = supx \u2016U(x)\u2016.\nWe also define the centralized max-norm, which, even though it is not a norm, we denote as:\n\u2016Z\u2016m\u0302ax = min \u03b8 \u2016Z \u2212 \u03b8\u2016max\nThe centralized max-norm is also SDP-representable. Our main result is that the max-norm provides a tight bound on the ratio function:\nTheorem 2. For any similarity function sim : S \u00d7 T \u2192 R we have that: 1\n2 \u2016sim\u2016m\u0302ax \u2264\n1 2 \u03c1\u03022(sim) \u2264 \u03c1\u0302(sim) \u2264 \u03c1\u0302k(sim) \u2264 \u03c1\u03022(sim) \u2264 K\u2016sim\u2016m\u0302ax\nand also\n1 3 \u2016sim\u2016max \u2264 \u03c1(sim) \u2264 \u03c1k(sim) \u2264 \u03c12(sim) \u2264 K\u2016sim\u2016max\nwhere all inequalities are tight and we have 1.67 \u2264 KG \u2264 K \u2264 KR \u2264 1.79 (KG is Grothendieck\u2019s constant and KR is Krivine\u2019s constant).\nConsidering the dual view of \u03c1(sim), the theorem can also be viewed in two ways: First, we see that the centralized max-norm provides a tight characterization (up to a small constant factor) of the smallest \u03b1 for which we can obtain an \u03b1-ALSH. In particular, since for domains (i.e. finite matrices) the max-norm is always finite, this establishes that we always have an \u03b1-ALSH, as claimed in Claim 4. We also used it in Theorem 1 to establish the existence of an \u03b1-ALSH for a specific, small, \u03b1.\nSecond, bounding the ratio function establishes that the max-norm ball is a tight tractable relaxation of convMk:\n{Z \u2016 \u2016Z\u2016max \u2264 1/K} \u2286 convMk \u2286 {Z \u2016 \u2016Z\u2016max \u2264 3} (12)\nThird, we see the effect of the alphabet size k (number of clusters) on the convex hull is very limited.\nThe Symmetric Case It is not difficult to show that the lower bounds for \u03b1LSH are the same as for \u03b1-ALSH and the inequalities are tight. However, there are no upper bounds for \u03b1-LSH similar to those for \u03b1-ALSH. Specifically, let \u03b1\u0302 and \u03b1\u0302g be the smallest values of \u03b1 such that there is an \u03b1-LSH for sim and there is a generalized \u03b1-LSH for sim, respectively. Note that for some similarity functions sim there is no \u03b1-LSH at all; that is, \u03b1\u0302 = \u221e and \u2016sim\u2016max < \u221e. Also, as Theorem 1 shows, there is a similarity function sim such that\n\u2016sim\u2016max = O(1) but \u03b1\u0302g \u2265 n\u2212 1.\nMoreover, it follows from the result of [2] that there is no efficiently computable upper bound \u03b2 for \u03b1\u0302g such that\n\u03b2\nlogc n \u2264 \u03b1\u0302g \u2264 \u03b2\n(under a standard complexity assumption that NP 6\u2286 DTIME(nlog3 n)). That is, neither the max-norm nor any other efficiently computable norm of sim gives a constant factor approximation for \u03b1\u0302g.\nIn the remainder of this section we prove a series of lemmas corresponding to the inequalities in Theorem 2."}, {"heading": "5.3 Proofs", "text": "Lemma 1. For any two sets S and T of objects and any function sim : S\u00d7T \u2192 R, we have that \u03c1\u03022(sim) \u2264 2\u03c1\u0302(sim) and the inequality is tight.\nProof. Using Claim 5, all we need to do is to prove that given the function sim, if there exist an \u03b1-ALSH with arbitrary cardinality, then we can find a binary 2\u03b1\u2212ALSH . In order to do so, we assume that there exists an \u03b1-ALSH for family F and G of hash functions such that:\n\u03b1E(f,g)\u2208F\u00d7G [\u03baf,g(x, y)] = sim(x, y) + \u03b8\nwhere f : S \u2192 \u0393 and g : T \u2192 \u0393 are hash functions. Now let H be a family of pairwise independent hash functions of the form \u0393 \u2192 {\u00b11} such that each element \u03b3 \u2208 \u0393 , has the equal chance of being mapped into -1 or 1. Now, we have that:\n2\u03b1Eh\u2208H,(f,g)\u2208F\u00d7G[\u03bahof,hog(x, y)] = 2\u03b1Eh\u2208H,(f,g)\u2208F\u00d7G[\u03bahof,hog(x, y)]\n= 2\u03b1Eh\u2208H,(f,g)\u2208F\u00d7G[h(f(x))h(g(y))] = 2\u03b1(2Ph\u2208H,(f,g)\u2208F\u00d7G[h(f(x)) = h(g(y))]\u2212 1) = 2\u03b1P(f,g)\u2208F\u00d7G [f(x) = g(y)]\n= sim(x, y) + \u03b8 + \u03b1\n= sim(x, y) + \u03b8\u0303\nThe tightness can be demonstrated by the example sim(x, y) = 2x=y \u2212 1 when S is not finite.\nLemma 2. For any two sets S and T of objects and any function sim : S\u00d7T \u2192 R, we have that \u2016sim\u2016max \u2264 \u03c12(sim) and the inequality is tight. Proof. Without loss of generality, we assume that \u0393 = {\u00b11}. We want to solve the following optimization problem:\n\u03c12(sim) = min \u00b5:MS,2\u00d7MT,2\u2192R+\n\u2211\nf\u2208MS,2\n\u2211\ng\u2208MT,2 \u00b5(f, g)\ns.t. sim(x, y) = \u2211\nf\u2208MS,2\n\u2211\ng\u2208MT,2 \u03baf,g(x, y)\u00b5(f, g)\nFor any x \u2208 S and y \u2208 T , we define two new function variables \u2113x : MS,2 \u00d7 MT,2 \u2192 R and ry : MS,2 \u00d7MT,2 \u2192 R:\n\u2113x(f, g) = \u221a \u00b5(f, g)f(x) ry(f, g) = \u221a \u00b5(f, g)g(y)\nSince cluster incidence matrix can be written as \u03baf,g(x, y) = f(x)g(y), we have sim(x, y) = \u3008\u2113x, ry\u3009 and \u2016\u2113x\u201622 = \u2211 f\u2208MS,2 \u2211\ng\u2208MT,2 \u00b5(f, g). Therefore, we rewrite the optimization problem as:\n\u03c12(sim) = min t,\u2113,r,\u00b5:MS,2\u00d7MT,2\u2192R+ t\ns.t. \u3008lx, ry\u3009 = sim(x, y) \u2016\u2113x\u201622 \u2264 t \u2016ry\u201622 \u2264 t \u2113x(f, g) = \u221a \u00b5(f, g)f(x)\nry(f, g) = \u221a \u00b5(f, g)g(y)\nFinally, we relax the above problem by removing the last two constraints:\n\u2016sim\u2016max = min t,\u2113,r t\ns.t. \u3008lx, ry\u3009 = sim(x, y) \u2016\u2113x\u201622 \u2264 t (13) \u2016rx\u201622 \u2264 t\nThe above problem is a max-norm problem and the solution is \u2016sim\u2016max. Therefore, \u2016sim\u2016max \u2264 \u03c12(sim). Taking the function sim(x, y) to be a binary cluster incidence function will indicate the tightness of the inequality.\nLemma 3. (Krivine\u2019s lemma [14]) For any two sets of unit vectors {ui} and {vj} in a Hilbert space H, there are two sets of unit vectors {u\u2032i} and {v\u2032j} in a Hilbert space H \u2032 such that for any ui and vj, sin(c\u3008ui, vj\u3009) = \u3008u\u2032i, v\u2032j\u3009 where c = sinh\u22121(1).\nLemma 4. For any two sets S and T of objects and any function sim : S\u00d7T \u2192 R, we have that \u03c12(sim) \u2264 K\u2016sim\u2016max where 1.67 \u2264 KG \u2264 K \u2264 KR \u2264 1.79 (KG is Grothendieck\u2019s constant and KR is Krivine\u2019s constant).\nProof. A part of the proof is similar to [1]. Let \u2113x and ry be the solution to the max-norm formulation 13. If we use Lemma 3 on the normalized \u2113x/\u2016\u2113x\u20162 and ry/\u2016ry\u20162 in Hilbert space H and we call the new vectors \u2113\u2032x and r\u2032y in Hilbert space H \u2032, we have that:\nsin\n(\nc.Z(x, y)\n\u2016\u2113x\u20162\u2016rx\u20162\n)\n= \u3008\u2113\u2032x, r\u2032y\u3009\nIf z is a random vector chosen uniformly from H \u2032, by Lemma 3, we have:\nE([sign(\u3008\u2113\u2032x, z\u3009)].[sign(\u3008r\u2032y , z\u3009)]) = 2 \u03c0 arcsin(\u3008\u2113\u2032x, r\u2032y\u3009)) =\n2c\n\u03c0\u2016\u2113x\u20162\u2016ry\u20162 sim(x, y)\nNow if we set the hashing function f(x) = s(x).[sign(\u3008\u2113\u2032x, z\u3009)] where s(x) = 1 with probability 12 + \u2016\u2113x\u20162 2 \u221a t and s(x) = \u22121 with probability 12 \u2212 \u2016\u2113x\u20162 2 \u221a t we have that:\nE[f(x).sign(\u3008r\u2032y , z\u3009)] = ( 1\n2 + \u2016\u2113x\u20162 2 \u221a t\n)\n2c\n\u03c0\u2016\u2113x\u20162\u2016ry\u20162 sim(x, y)\n\u2212 ( 1 2 \u2212 \u2016\u2113x\u20162 2 \u221a t )\n2c\n\u03c0\u2016\u2113x\u20162\u2016ry\u20162 sim(x, y)\n= 2c\n\u03c0 \u221a t\u2016ry\u20162 sim(x, y)\nIf we do the same procedure on g(y) = s\u2032(x).[sign(\u3008r\u2032y , z\u3009)], we will have:\nE[f(x).g(y)] = 2c\nt\u03c0 sim(x, y)\nBy setting \u00b5(f, g) = \u03c0\u2016sim\u2016max2c p(f, g) where p(f, g) is the probability distribution over the defined f and g, we can see that such \u00b5(f, g) is a feasible solution for the formulation of cluster ratio and we have:\n\u03c12(sim) \u2264 \u2211\nf\u2208MS,2\n\u2211\ng\u2208MT,2 \u00b5(f, g) =\n\u03c0 2c \u2016sim\u2016max = KR\u2016sim\u2016max\nThe inequality KG \u2264 K is known due to [1]."}, {"heading": "A Random Matrices", "text": "In this section we investigate the locality sensitive hashing schemes on random p.s.d matrices. We generate a random n\u00d7n positive semidefinite matrix Z of rank at most d by choosing n d-dimensional unit vectors x(i) uniformly at random from the unit ball and set Zij = \u3008x(i), x(j)\u3009. Since we are generating the data randomly and E[Zij ] = 0, we don\u2019t expect to observe major changes by thresholding the matrix. So our analysis is limited to the LSH without thresholding, i.e. \u03b8 = 0.\nSince based on Theorem 2, we already know given any set of unit vectors x(1), . . . , x(n) and Zij = \u3008x(i), x(j)\u3009, there is an KR-ALSH for the matrix Z, we are just interested in investigating the symmetric LSH for these random vectors.\nA.1 LSH\nFor the symmetric LSH, we only have two possibilities: either having LSH with \u03b1 = 1 or not having any LSH. We also know from Claim 2 that there is no \u03b1 LSH if d < log2 n because in that case D(x\n(i), x(j)) = 1 \u2212 Zij is not metric. So we want to know the conditions under which the distance will be a metric and also the conditions for having \u03b1-LSH with high probability.\nLemma 5. [9] If x is a d-dimensional unit vector and x\u0303 is its projection onto another unit vector that is sampled uniformly at random from the unit sphere, then for any t > 1, we have E[\u2016x\u0303\u201622] = 1d and moreover, P(\u2016x\u0303\u201622 \u2265 td ) \u2264 e 1\u2212t+log t 2 .\nLemma 6. Let {x(1), . . . , x(n)} be a set of unit vectors sampled uniformly at random from the unit sphere and for any 1 \u2264 i, j \u2264 n let Zij = \u3008x(i), x(j)\u3009. If d \u2265 72 loge n + loge 1\u03b4 , then the distance measure \u2206ij = 1 \u2212 Zij is metric with probability at least 1\u2212 \u03b4.\nProof. The distance measure \u2206ij = 1\u2212 \u3008x(i), x(j)\u3009 is not a metric if and only if there exist i, j and k such that\n(1\u2212 \u3008x(i), x(j)\u3009) + (1\u2212 \u3008x(i), x(k)\u3009) < (1\u2212 \u3008x(j), x(k)\u3009)\nA simple reordering of the above inequality gives us:\nCijk = \u3008x(i), x(j)\u3009+ \u3008x(i), x(k)\u3009 \u2212 \u3008x(j), x(k)\u3009) > 1\nFor this inequality to hold, the absolute value of at least one of the inner products \u3008x(i), x(j)\u3009, \u3008x(i), x(k)\u3009, \u3008x(j), x(k)\u3009 must be at least 13 . Now we have:\nP(\u2206 is not a metric) = P(\u2203ijk : Cijk > 1) \u2264 P(\u2203ij : |\u3008x(i), x(j)\u3009| > 1/3)\n\u2264 n 2\n2 P(|\u3008x(1), x(2)\u3009| > 1/3)\nSince both x(1) and x(2) are random vectors, the probability P(|\u3008x(1), x(2)\u3009| > 1/3) is equal to the probability that the projection of a random d-dimensional vector onto a 1-dimensional subspace is at least 1/3 in absolute value. By Lemma 5, we have:\nP(\u2206 is not a metric) \u2264 n 2\n2 P(|\u3008x(1), x(2)\u3009| > 1/3)\n\u2264 n 2\n2 P(\u3008x(1), x(2)\u30092 > 1/9)\n\u2264 n 2\n2 e\n1+log(d/9)\u2212(d/9) 2\n\u2264 n2e\u2212 d36 \u2264 \u03b4\nLemma 7. ([19], Theorem 5.39) Let {x(1), . . . , x(n)} be a set of unit vectors sampled uniformly at random from the unit sphere and t \u2208 (0, 1). Let Zij = \u3008x(i), x(j)\u3009 for 1 \u2264 i, j \u2264 n. If d \u2265 C1n/t2 then with probability at least 1 \u2212 2e\u2212C2t\n2N , we have |\u03bbi \u2212 1| \u2264 t for all eigenvalues \u03bbi of Z. Here, C1 > 0 and C2 > 0 are some absolute constants.\nTheorem 3. Let {x(1), . . . , x(n)} be a set of unit vectors sampled uniformly at random from the unit sphere. Let Zij = \u3008x(i), x(j)\u3009 for 1 \u2264 i, j \u2264 n. If d \u2265 Cn log2 n then with probability at least 1 \u2212 eC\u2032n/ log2 n, there is an LSH for Z. Here, C > 0 and C\u2032 > 0 are some absolute constants.\nProof. Apply Lemma 7 with t = 1C0 logn (where C0 is a sufficiently large constant). We get that if d \u2265 (C20C1)n log2 n then with probability at least 1\u2212e\u2212(C2/C21)N/ log2 n the smallest eigenvalue is greater than or equal to 1\u2212 1C logn . Therefore, matrix Y = C logn (Z \u2212 (1\u2212 1C logn )I) is a positive semidefinite matrix with unit diagonal. Now according to [5], there exists a distribution over a family H of hash functions such that for any i 6= j, Eh\u2208H[hihj ] = YijC log n . We have,\nEh\u2208H[hihj ] = Yij\nC logn = Zij \u2212\n(\n1\u2212 1 C logn\n)\nIij = Zij\nMoreover, for every i, we have Eh\u2208H[hihi] = 1 = Zii.\nA.2 Generalized LSH\nIn this section, we try to investigate the conditions to have Generalized LSH with high probability.\nLemma 8. Let {x(1), . . . , x(n)} be a set of unit vectors. Let Zij = \u3008x(i), x(j)\u3009 for 1 \u2264 i, j \u2264 n. There is a generalized \u03b1-LSH for matrix Z with \u03b1 = O(log n).\nProof. Matrix Z is positive semi-definite and thus its smallest eigenvalue \u03bbmin is non-negative. Applying Claim 3, we get the statement of the lemma.\nTheorem 4. Let {x(1), . . . , x(n)} be a set of unit vectors sampled uniformly at random from the unit sphere, let 0 < \u03b1 < O(log n). Let Zij = \u3008x(i), x(j)\u3009 for 1 \u2264 i, j \u2264 n. If d \u2265 Cn log2 n/\u03b12 then with probability at least 1\u2212 eC\u2032n\u03b12/ log2 n, there is a generalized \u03b1-LSH for Z. Here C > 0 and C\u2032 > 0 are some absolute constants.\nProof. The proof is a straightforward generalization of Theorem 3."}], "references": [{"title": "Approximating the cut-norm via grothendieck\u2019s inequality", "author": ["N. Alon", "A. Naor"], "venue": "SIAM Journal on Computing, 35(4):787\u2013803,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "On nonapproximability for quadratic programs", "author": ["Sanjeev Arora", "Eli Berger", "Hazan Elad", "Guy Kindler", "Muli Safra"], "venue": "In Foundations of Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "A generalized maximum entropy approach to bregman co-clustering and matrix approximation", "author": ["A. Banerjee", "I. Dhillon", "J. Ghosh", "S. Merugu", "D. S D.S. Modha"], "venue": "SIGKDD, pages 509\u2013514,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Two new approaches to obtaining estimates in the danzergrunbaum problem", "author": ["L.V. Buchok"], "venue": "Mathematical Notes, 87(4):489\u2013496,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Maximizing quadratic programs: Extending grothendieck\u2019s inequality", "author": ["M. Charikar", "A. Wirth"], "venue": "In FOCS, pages 54\u201360,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "STOC,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Lsh-preserving functions and their applications", "author": ["F. Chierichetti", "R. Kumar"], "venue": "SODA,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "\u00dcber zwei probleme bez\u00fcglich konvexer k\u00f6rper von p", "author": ["L Danzer", "B Gr\u00fcnbaum"], "venue": "erd\u00f6s und von vl klee. Mathematische Zeitschrift, 79(1):95\u201399,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1962}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["S. Dasgupta", "A. Gupta"], "venue": "Random Structures & Algorithms, 22(1):60\u201365,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "S.V. Mirrokni"], "venue": "In Proc. 20th SoCG, pages 253\u2013262,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Information-theoretic co-clustering", "author": ["I.S. Dhillon", "M. Subramanyam", "S.M. Dharmendra"], "venue": "SIGKDD,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Clustering partially observed graphs via convex optimization", "author": ["A. Jalali", "Y. Chen", "S. Sanghavi", "H. Xuo"], "venue": "ICML,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Clustering using max-norm constrained optimization", "author": ["A. Jalali", "N. Srebro"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Sur la constante de grothendieck", "author": ["J.L. Krivine"], "venue": "C. R. Acad. Sci. Paris Ser. A-B 284, pages 445\u2013446,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1977}, {"title": "The power of asymmetry in binary hashing", "author": ["B. Neyshabur", "P. Yadollahpour", "Y. Makarychev", "R. Salakhutdinov", "N. Srebro"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Rajeev Motwani Piotr Indyk"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Maximum margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": "NIPS,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Rank, trace-norm and max-norm", "author": ["N. Srebro", "A. Shraibman"], "venue": "COLT,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 5, "context": "We study the convex relaxation of clustering and hamming embedding, focusing on the asymmetric case (co-clustering and asymmetric hamming embedding), understanding their relationship to LSH as studied by [6] and to the max-norm ball, and the differences between their symmetric and asymmetric versions.", "startOffset": 204, "endOffset": 207}, {"referenceID": 5, "context": "In section 3 we discuss their convex hull and its relationship to notion of Locality Sensitive Hashing (LSH) as studied by [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 15, "context": "[16,10,7]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 9, "context": "[16,10,7]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 6, "context": "[16,10,7]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 10, "context": "[11,3]) and asymmetric hamming embedding as recently introduced by [15].", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[11,3]) and asymmetric hamming embedding as recently introduced by [15].", "startOffset": 0, "endOffset": 6}, {"referenceID": 14, "context": "[11,3]) and asymmetric hamming embedding as recently introduced by [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "[12,13], who relax the constraint to a trace-norm and max-norm constraint respectively.", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[12,13], who relax the constraint to a trace-norm and max-norm constraint respectively.", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "Moving on from a finite average of clusterings, with a fixed number of components, as in hamming embedding, to an infinite average, we arrive at the notion of LSH as studied by [6].", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "Given a collection S of objects, an alphabet \u0393 and a similarity function sim : S \u00d7 S \u2192 [\u22121, 1] such that for any x \u2208 S we have sim(x, x) = 1,a locality sensitive hashing scheme (LSH) is a probability distribution on the family of clustering functions (hash functions) H = {h : S \u2192 \u0393} such that [6]: Eh\u2208H[\u03bah(x, y)] = sim(x, y).", "startOffset": 294, "endOffset": 297}, {"referenceID": 5, "context": "(3) [6] discuss similarity functions sim : S \u00d7 S \u2192 [0, 1] as so require Ph\u2208H[h(x) = h(y)] = sim(x, y).", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "(3) [6] discuss similarity functions sim : S \u00d7 S \u2192 [0, 1] as so require Ph\u2208H[h(x) = h(y)] = sim(x, y).", "startOffset": 51, "endOffset": 57}, {"referenceID": 5, "context": "The importance of an LSH, as an object in its own right as studied by [6], is that a hamming embedding can be obtained from an LSH by randomly generating a finite number of hash functions from the distribution over the family H.", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "Unfortunately, even the requirement (5) of an \u03b1-LSH is quite limiting and difficult to obey, as captured by the following theorem, which is based on lemmas 2 and 3 of [6]:", "startOffset": 167, "endOffset": 170}, {"referenceID": 5, "context": "Using lemma 3 in [6], we can say that 1\u2212sim(x,y) \u03b1 can be isometrically embedded in the Hamming cube which means 1 \u2212 sim(x, y) can be embedded in Hamming cube with no distortion.", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "According to [8] (see also [4]), if d < log2 n then in any set of n points in d-dimensional Euclidian space, there exist at least three points that form an obtuse triangle.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "According to [8] (see also [4]), if d < log2 n then in any set of n points in d-dimensional Euclidian space, there exist at least three points that form an obtuse triangle.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "As noted by [6] (and stated in claim 2), we can therefore unfortunately conclude that there is no \u03b1-LSH for several important similarity measures such as the Euclidian inner product, Overlap coefficient and Dice\u2019s coefficient.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "But before doing so, let us consider a different attempt at relaxing the definition of an \u03b1-LSH, motivated by to the work of [5] and [1]: in order to uncouple the shift \u03b8 from the scaling \u03b1, we will allow for a different, arbitrary, shift on the self-similarities sim(x, x) (i.", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "But before doing so, let us consider a different attempt at relaxing the definition of an \u03b1-LSH, motivated by to the work of [5] and [1]: in order to uncouple the shift \u03b8 from the scaling \u03b1, we will allow for a different, arbitrary, shift on the self-similarities sim(x, x) (i.", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "According to [5], if a matrix Z with unit diagonal is positive semidefinite, then there is a probability distribution over a family H of hash functions such that for any x 6= y: Eh\u2208H[h(x)h(y)] = Z(x, y) C logn .", "startOffset": 13, "endOffset": 16}, {"referenceID": 10, "context": "Given two collections of objects S, T , which might or might not be identical, and an alphabet \u0393 , an asymmetric clustering (or co-clustering [11]) is specified by pair of mappings f : S \u2192 \u0393 and g : T \u2192 \u0393 and is captured by the asymmetric cluster incidence matrix \u03baf,g(x, y) where \u03baf,g(x, y) = 1 if f(x) = g(y) and \u03baf,g(x, y) = \u22121 otherwise.", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "In a recent work, [15] showed that even when S = T and the similarity function sim is a well-behaved symmetric similarity function, asymmetric binary embedding could be much more powerful in approximating the similarity, using shorter lengths d, both theoretically and empirically on data sets of interest.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "2 A Tight Convex Relaxation using the Max-Norm Recall that the max-norm (also known as the \u03b32 : l1 \u2192 l\u221e norm) of a matrix is defined as [18]: \u2016Z\u2016max = min UV \u22a4 max(\u2016U\u20162,\u221e, \u2016V \u20162,\u221e) where \u2016U\u20162,\u221e is the maximum l2 norm of rows of the matrix U .", "startOffset": 136, "endOffset": 140}, {"referenceID": 16, "context": "The maxnorm is SDP representable and thus tractable [17].", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "Moreover, it follows from the result of [2] that there is no efficiently computable upper bound \u03b2 for \u03b1\u0302g such that \u03b2 log n \u2264 \u03b1\u0302g \u2264 \u03b2 (under a standard complexity assumption that NP 6\u2286 DTIME(nlog3 )).", "startOffset": 40, "endOffset": 43}, {"referenceID": 13, "context": "(Krivine\u2019s lemma [14]) For any two sets of unit vectors {ui} and {vj} in a Hilbert space H, there are two sets of unit vectors {ui} and {v\u2032 j} in a Hilbert space H \u2032 such that for any ui and vj, sin(c\u3008ui, vj\u3009) = \u3008ui, v\u2032 j\u3009 where c = sinh\u22121(1).", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "A part of the proof is similar to [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "g\u2208MT,2 \u03bc(f, g) = \u03c0 2c \u2016sim\u2016max = KR\u2016sim\u2016max The inequality KG \u2264 K is known due to [1].", "startOffset": 82, "endOffset": 85}], "year": 2014, "abstractText": "We study the convex relaxation of clustering and hamming embedding, focusing on the asymmetric case (co-clustering and asymmetric hamming embedding), understanding their relationship to LSH as studied by [6] and to the max-norm ball, and the differences between their symmetric and asymmetric versions.", "creator": "LaTeX with hyperref package"}}}