{"id": "1405.3843", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2014", "title": "Logistic Regression: Tight Bounds for Stochastic and Online Optimization", "abstract": "the uniform loss probability is often suggested in machine learning and statistics as a marginal loop strictly convex sequence opposite the 0 - index loss. together related paper we investigate the problems of ensuring these smoothness and convexity operators make bounded uncertainty hypothesis appealing to bigger widely considered options than as nash hinge entropy. both show but in contrast relative optimal global bounds, no long as infinite probability of prediction / optimization iterations is sub divided, local logistic limitation provides no improvement over a normally non - ideal likelihood function such as the hinge loss. in particular we show approximately the transition rate applying stochastic chaos algorithms is bounded far below by a polynomial in the diameter of the vertex set and the size of prediction iterations, and provide a matching tight definite bound. keynes defines as adaptive succession problem of lambert and bell ( 2012 ).", "histories": [["v1", "Thu, 15 May 2014 13:29:27 GMT  (27kb)", "http://arxiv.org/abs/1405.3843v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["elad hazan", "tomer koren", "kfir y levy"], "accepted": false, "id": "1405.3843"}, "pdf": {"name": "1405.3843.pdf", "metadata": {"source": "CRF", "title": "Logistic Regression: Tight Bounds for Stochastic and Online Optimization\u2217", "authors": ["Elad Hazan", "Tomer Koren", "Kfir Y. Levy"], "emails": ["ehazan@ie.technion.ac.il,", "tomerk@technion.ac.il,", "kfiryl@tx.technion.ac.il."], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n38 43\nv1 [\ncs .L\nG ]\n1 5"}, {"heading": "1 Introduction", "text": "In many applications, such as estimation of click-through-rate in web advertising, and predicting whether a patient has a certain disease, the logistic loss is often the loss of choice. It appeals as a convex surrogate of the 0-1 loss, and as a tool that not only yields categorical prediction but also able to estimate the underlying probabilities of the categories. Moreover, Friedman et al. (2000) and Collins et al. (2002) have shown that logistic regression is strongly connected to boosting.\nA long standing debate in the machine learning community has been the optimal choice of surrogate loss function for binary prediction problems (see Langford (2009), Bulatov (2007)). Amongst the arguments in support of the logistic loss are its smoothness and strictconvexity properties, which unlike other loss functions (such as the hinge loss), permit the\n\u2217The research leading to these results has received funding from the European Union\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u25e6 336078 \u2013 ERC-SUBLRN.\n\u2020Technion\u2014Israel Institute of Technology, Haifa 32000, Israel. Emails: ehazan@ie.technion.ac.il, tomerk@technion.ac.il, kfiryl@tx.technion.ac.il.\nuse of more efficient optimization methods. In particular, the logistic loss is exp-concave, and thus second-order methods are applicable and give rise to theoretically superior convergence and/or regret bounds.\nMore technically, under standard assumptions on the training data, the logistic loss is 1- Lipschitz and e\u2212D-exp-concave over the set of linear n-dimensional classifiers whose L2-norm is at most D. Thus, the Online Newton Step algorithm (Hazan et al., 2007) can be applied to the logistic regression problem and gives a convergence rate of O\u0303(eDn/T ) over T iterations. On the other hand, first order methods can be used to attain a rate of O(D/ \u221a T ), which is attainable in general for any Lipschitz convex loss function. The exponential dependence on D of the first bound suggests that second order methods might present poor performance in practical logistic regression problems, even when compared to the slow 1/ \u221a T rate of firstorder methods. The gap between the two rates raises the question: is a fast convergence rate of the form O\u0303(poly(D)/T ) achievable for logistic regression?\nThis question has received much attention lately. Bach (2013), relying on a property called \u201cgeneralized self-concordance\u201d, gave an algorithm with convergence rate ofO(D4/\u00b5\u2217T ), where \u00b5\u2217 is the smallest eigenvalue of the Hessian at the optimal point. This translates to a O(poly(D)/T ) rate whenever the expected loss function is \u201clocally strongly convex\u201d at the optimum. More recently, Bach and Moulines (2013) extended this result and presented an elegant algorithm that attains a rate of the form O(\u03c13D4n/T ), without assuming strong convexity (neither global or local) \u2014 but rather depending on a certain data-dependent constant \u03c1.\nIn this paper, we resolve the above question and give tight characterization of the achievable convergence rates for logistic regression. We show that as long as the target accuracy \u01eb is not exponentially small in D, a rate of the form O\u0303(poly(D)/T ) is not attainable. Specifically, we prove a lower bound of \u2126( \u221a D/T ) on the convergence rate, that can also be achieved (up to a \u221a D factor) by stochastic gradient descent algorithms. In particular, this shows that in the worst case, the magnitude of data-dependent parameters used in previous works are exponentially large in the diameter D. The latter lower bound only applies for multidimensional regression (i.e., when n \u2265 2); surprisingly, in one-dimensional logistic regression we find a rate of \u0398(T\u22122/3) to be tight. As far as we know, this is the first natural setting demonstrating such a phase transition in the optimal convergence rates, with respect to the dimensionality of the problem.\nWe also consider the closely-related online optimization setting, where on each round t = 1, 2, . . . , T an adversary chooses a certain logistic function and our goal is to minimize the T -round regret, with respect to the best fixed decision chosen with the benefit of hindsight. In this setting, McMahan and Streeter (2012) investigated the one-dimensional case and showed that if the adversary is restricted to pick binary (i.e. \u00b11) labels, a simple followthe-leader algorithm attains a regret bound of O( \u221a D + log T ). This discovery led them to conjecture that bounds of the form O(poly(D) log T ) should be achievable in the general multi-dimensional case with continuous labels set.\nOur results extend to the online optimization setup and resolve the COLT 2012 open problem of McMahan and Streeter (2012) on the negative side. Namely, we show that as long as\nthe number of rounds T is not exponentially large in D, an upper bound of O(poly(D) logT ) cannot be attained in general. We obtain lower bounds on the regret of \u2126( \u221a DT ) in the multi-dimensional case and \u2126(D2/3T 1/3) in the one-dimensional case, when allowing the adversary to use a continuous label set. We are not aware of any other natural problem that exhibits such a dichotomy between the minimax regret rates in the one-dimensional and multi-dimensional cases.\nIt is interesting to note that our bounds apply to a finite interval of time, namely when T = O(eD), which is arguably the regime of interest for reasonable values of D. This is the reason our lower bounds do not contradict the logarithmic known regret bounds.\nWe prove the tightness of our one-dimensional lower bounds, in both the stochastic and online settings, by devising an online optimization algorithm specialized for one-dimensional online logistic regression that attains a regret of O(D3 T 1/3). This algorithm maintains approximations of the observed logistic loss functions, and use these approximate losses to form the next prediction by a follow-the-regularized-leader (FTRL) procedure. As opposed to previous works that utilize approximate losses based on local structure (Zinkevich, 2003; Hazan et al., 2007), we find it necessary to employ approximations that rely on the global structure of the logistic loss.\nThe rest of the paper is organized as follows. In Section 2 we describe the settings we consider and give the necessary background. We present our lowers bounds in Section 3, and in Section 4 we prove our upper bound for one dimensional logistic regression. In Section 5 we give complete proofs of our results. We conclude in Section 6."}, {"heading": "2 Setting and Background", "text": "In this section we formalize the settings of stochastic logistic regression and online logistic regression and give the necessary background on both problems."}, {"heading": "2.1 Stochastic Logistic Regression", "text": "In the problem of stochastic logistic regression, there is an unknown distribution D over instances x \u2208 Rn. For simplicity, we assume that \u2016x\u2016 \u2264 1. The goal of an optimization algorithm is to minimize the expected loss of a linear predictor w \u2208 Rn,\nL(w) = Ex\u223cD[ \u2113(w, x) ] , (1)\nwhere \u2113 is the logistic loss function1,\n\u2113(w, x) = log ( 1 + exp(x \u00b7 w) )\nthat expresses the negative log-likelihood of the instance x under the logit model. While we may try to optimize L(w) over the entire Euclidean space, for generalization purposes we usually restrict the optimization domain to some bounded set. In this paper, we focus on optimizing the expected loss over the set W = {w \u2208 Rn : \u2016w\u2016 \u2264 D}, the Euclidean ball of radius D. We define the excess loss of a linear predictor w \u2208 W as the difference L(w) \u2212 minw\u2217\u2208W L(w\u2217) between the expected loss of w and the expected loss of the best predictor in the class W.\nAn algorithm for the stochastic optimization problem, given a sample budget T as a parameter, may use a sample x1, . . . , xT of T instances sampled independently from the distribution D, and produce an approximate solution wT . The rate of convergence of the algorithm is then defined as the expected excess loss of the predictor wT , given by\nE[L(wT )] \u2212 min w\u2217\u2208W L(w\u2217) ,\nwhere the expectation is taken with respect to both the random choice of the training set and the internal randomization of the algorithm (which is allowed to be randomized)."}, {"heading": "2.2 Online Logistic Regression", "text": "Another optimization framework we consider is that of online logistic optimization, which we formalize as the following game between a player and an adversary. On each round t = 1, 2, . . . , T of the game, the adversary first picks an instance xt \u2208 Rn, the player then chooses a linear predictor wt \u2208 W = {w \u2208 Rn : \u2016w\u2016 \u2264 D}, observes xt and incurs loss\n\u2113(wt, xt) = log ( 1 + exp(xt \u00b7 wt) ) .\nFor simplicity we again assume that \u2016xt\u2016 \u2264 1 for all t. The goal of the player is to minimize his regret with respect to a fixed prediction from the set W, which is defined as\nRegretT = T\u2211\nt=1\n\u2113(wt, xt) \u2212 min w\u2217\u2208W\nT\u2211\nt=1\n\u2113(w\u2217, xt) .\n1The logistic loss is commonly defined as \u2113(w;x, y) = log ( 1 + exp(\u2212yx \u00b7 w) ) for instances (x, y) \u2208 R n \u00d7 [\u22121, 1]. For ease of notation and without loss of generality, we ignore the variable y in the instance (x, y) by absorbing it into x."}, {"heading": "2.3 Information-theoretic Tools", "text": "As a part of our lower bound proofs, we utilize two impossibility theorems that assert the minimal number of samples needed in order to distinguish between two distributions. We prove the following lower bound on the performance of any algorithm for this task.\nTheorem 1. Assume a coin with bias either p or p + \u01eb, where p \u2208 (0, 1 2 ], is given. Any algorithm that correctly identifies the coin\u2019s bias with probability at least 3/4, needs no less than p/16\u01eb2 tosses.\nThe theorem applies to both deterministic and randomized algorithms; in case of random algorithms the probability is with respect to both the underlying distribution of the samples, and the randomization of the algorithm. The proof of Theorem 1 is given, for completeness, in Appendix A."}, {"heading": "3 Lower Bounds for Logistic Regression", "text": "In this section we derive lower bounds for the convergence rate of stochastic logistic regression. For clarity, we lower bound the number of observations T required in order to attain excess loss of at most \u01eb, which we directly translate to a bound for the convergence rate. The stochastic optimization lower bounds are then used to obtain corresponding bounds for the online setting.\nIn Section 3.1 we prove a lower bound for the one dimensional case, in Section 3.2 we prove another lower bound for the multidimensional case, and in Section 3.3 we present our lower bounds for the online setting."}, {"heading": "3.1 One-dimensional Lower Bound for Stochastic Optimization", "text": "We now show that any algorithm for one-dimensional stochastic optimization with logistic loss, must observe at least \u2126(D/\u01eb1.5) instances before it provides an instance with \u01eb expected excess loss. This directly translates to a convergence rate of \u2126(D2/3/T 2/3). Formally, the main theorem of this section is the following.\nTheorem 2. Consider the one dimensional stochastic logistic regression setting with a fixed sample budget T = O(eD). For any algorithm A there exists a distribution D for which the expected excess loss of A\u2019s output is at least \u2126(D2/3/T 2/3).\nThe proof of Theorem 2 is given at the end of this section; here we give an informal proof sketch. Consider distributions D over the two-element set {1\u2212 \u03b8\n2 ,\u2212\u03b8}. For w \u2208 [D/2, D] and\n\u03b8 \u226a 1, the losses of these instances are approximately linear/quadratic with opposed slopes (see Fig. 1(a)). Consequently, we can build a distribution with an expected loss which is quadratic in w; upon perturbing the latter distribution by \u00b1\u01eb we get two distributions D+,D\u2212 with expected losses L+, L\u2212 that are approximately linear in w with slopes \u00b1\u01eb (see Fig. 1(b)). An algorithm that attains a low expected excess loss on both these distributions can be\nused to distinguish between them, we then utilize an information theoretic impossibility theorem to bound the number of observations needed in order to distinguish between two distributions.\nIn Fig. 2 we present two distributions, which we denote by D+ and D\u2212. We denote by L+, L\u2212 the expected logistic loss of a predictor w \u2208 W with respect to D+,D\u2212, i.e.,\nL\u03c7(w) = ED\u03c7[\u2113(w, x)]\n=\n( \u03b8\n2 + \u03c7\n\u01eb\nD\n) \u2113 ( w, 1\u2212 \u03b8\n2\n) + ( 1\u2212 \u03b8\n2 \u2212 \u03c7 \u01eb D\n) \u2113 (w,\u2212\u03b8) , \u03c7 \u2208 {\u22121, 1} .\nThe following lemma states that it is impossible attain a low expected excess loss on both D+ and D\u2212 simultaneously. Here we only give a sketch of the proof; the complete proof is deferred to Section 5.1.\nLemma 3. Given D \u2265 1 and \u2126(e\u2212D) \u2264 \u01eb \u2264 1/25, consider the distributions D+,D\u2212 defined in Fig. 2. Then the following holds:\nL+(w)\u2212 min w\u2217\u2208W L+(w \u2217) \u2265 \u01eb/20 , \u2200 w \u2208 [3 4 D,D] , L\u2212(w)\u2212 min w\u2217\u2208W L\u2212(w \u2217) \u2265 \u01eb/20 , \u2200 w \u2208 [\u2212D, 3 4 D] .\nProof (sketch). First we show that for w \u2208 [1 2 D,D], the losses of the instances 1\u2212 \u03b8 2 ,\u2212\u03b8 are approximately linear/quadratic, i.e.,\n\u2223\u2223\u2113(w, 1\u2212 \u03b8 2 )\u2212 (1\u2212 \u03b8 2 )w \u2223\u2223 \u2264 \u01eb 40 , \u2200 w \u2208 [1 2 D,D] ,\n\u2223\u2223\u2113(w,\u2212\u03b8)\u2212 ( log 2\u2212 \u03b8\n2 w + 1 8 (\u03b8w)2 )\u2223\u2223 \u2264 \u01eb 40 , \u2200 w \u2208 [1 2 D,D] .\nUsing the above approximations and \u03b8 = \u221a \u01eb/D, we show that L+(w) \u2248 \u01ebw/D + \u01ebw2/8D2 and L\u2212(w) \u2248 \u2212\u01ebw/D + \u01ebw2/8D2 for w \u2208 [12D,D], where \u201c\u2248\u201d denotes equality up to an additive term of \u01eb/40. Thus,\nL+(w)\u2212 min w\u2217\u2208W L+(w \u2217) \u2265 L+(w)\u2212 L+(D/2) \u2265 \u01eb/20 , \u2200 w \u2208 [34D,D] , L\u2212(w)\u2212 min w\u2217\u2208W L\u2212(w \u2217) \u2265 L\u2212(w)\u2212 L\u2212(D) \u2265 \u01eb/20 , \u2200 w \u2208 [12D, 34D] .\nShowing that L\u2212 is monotonically decreasing in [\u2212D, 12D], extends the latter inequality to [\u2212D, 3\n4 D].\nWe are now ready to prove Theorem 2.\nProof of Theorem 2. Consider some algorithm A; we will show that if A observes T samples from a distribution D which is either D+ or D\u2212, then the expected excess loss \u01eb\u0303 that A can guarantee is lower bounded by \u2126(D2/3T\u22122/3).\nThe excess loss is non negative; therefore, if A guarantees an expected excess loss smaller than \u01eb\u0303 := \u01eb/80, then by Markov\u2019s inequality it achieves an excess loss smaller than \u01eb/20, w.p. \u2265 3/4. Denoting by wT the predictor that A outputs after T samples, then according to Theorem 3, attaining an excess loss smaller than \u01eb/20 on the distribution D+ (respectively D\u2212) implies wT \u2264 34D (respectively wT > 34D).\nSince A achieves an excess loss smaller than \u01eb/20 w.p. \u2265 3/4 for any distribution D we can use its output to identify the right distribution w.p. \u2265 3/4. This can be done as follows:\nIf wT \u2264 34D, Return: \u201cD+\u201d ; If wT > 3 4 D, Return: \u201cD\u2212\u201d .\nAccording to Theorem 1 distinguishing between these two distributions (\u201ccoins\u201d) w.p. \u2265 3/4 requires that the number of observations T to be lower bounded as follows:\nT \u2265 \u03b8/2\u2212 \u01eb/D 16(2\u01eb/D)2 \u2265 1 256 D \u01eb1.5 ,\nWe used \u03b8/2 \u2212 \u01eb/D as a lower bound on the bias of D\u2212; since \u03b8 = \u221a \u01eb/D and \u01eb \u2264 1/25 it\nfollows that \u03b8/2 \u2212 \u01eb/D \u2265 \u221a\u01eb/4D. We also used 2\u01eb/D as the bias between the \u201ccoins\u201d D+, D\u2212. Using the above inequality together with \u01eb\u0303 = \u01eb/80 yields a lower bound of 14000D2/3T\u22122/3 on the expected excess loss."}, {"heading": "3.2 Multidimensional Lower Bound for Stochastic Optimization", "text": "We now construct two distribution over instance vectors from the unit ball of R2, and prove that any algorithm that attains an expected excess loss at most \u01eb on both distributions requires \u2126(D/\u01eb2) samples in the worst case. This directly translates to a convergence rate of \u2126( \u221a D/T ). For n > 2 dimensions, we can embed the same construction in the unit ball of Rn, thus our bound holds in any dimension greater than one. The main theorem of this section is the following.\nTheorem 4. Consider the multidimensional stochastic logistic regression setting with D \u2265 2 and a fixed sample budget T = O(eD). For any algorithm A there exists a distribution D such that the expected excess loss of A\u2019s output is at least \u2126( \u221a D/T ).\nTheorem 4 is proved at the end of this section. We bring here an informal description of the proof:\nConsider distributions that choose instances among the set {x0, xl, xr} depicted in Fig. 3. The shaded areas in Fig. 3 depict regions in the domain W where either \u2113(\u00b7, xl) or \u2113(\u00b7, xr) is approximately linear. The dark area represents the region in which both loss functions are approximately linear. By setting the probability of x0 much larger than the others we can construct a distribution over the instances {x0, xl, xr} such that the minima of the induced expected loss function lies in the black area. Perturbing this distribution by \u00b1\u01eb over the odds of choosing xl, xr we attain two distributions D+,D\u2212 whose induced expected losses L+, L\u2212 are almost linear over in the dark area, with opposed \u00b1\u01eb slopes. An algorithm that attains a low expected excess loss on both distributions can be used to distinguish between them. This allows us to use information theoretic arguments to lower bound the number of samples needed for the optimization algorithm. In Fig. 4 we present the distributions D+,D\u2212. We\ndenote by L+ and L\u2212 the expected loss functions induced by D+ and D\u2212 respectively, that are given by\nL\u03c7(w) = p \u00b7 \u2113(w, x0) + 1 + \u03c7\u01eb\n2 (1\u2212 p) \u00b7 \u2113(w, xl) + 1\u2212 \u03c7\u01eb 2 (1\u2212 p) \u00b7 \u2113(w, xr), \u03c7 \u2208 {\u22121, 1}\nIn the following lemma we state that it is impossible attain a low expected excess loss on both D+ and D\u2212 simultaneously.\nLemma 5. Given D \u2265 2 and \u2126(e\u2212D) \u2264 \u01eb \u2264 1/10D, consider D+,D\u2212 as defined in Fig. 4. Then the following holds:\nL+(w)\u2212 min w\u2217\u2208W L+(w \u2217) \u2265 \u01eb/20 , \u2200 w : w[1] \u2264 0 , and L\u2212(w)\u2212 min w\u2217\u2208W L\u2212(w \u2217) \u2265 \u01eb/20 , \u2200 w : w[1] \u2265 0 .\nHere we only give a sketch of the proof; for the complete proof, refer to Section 5.2.\nProof (sketch). Let L0 be the unperturbed (\u01eb = 0) version of L+, L\u2212, i.e.,\nL0(w) = p\u2113(w, x0) + 1\u2212 p 2 \u2113(w, xl) + 1\u2212 p 2 \u2113(w, xr) .\nNote that L0 is constructed such that its minima is attained at w0 = (0, 0.9D), which belongs to the shaded area in Fig. 3. Thus, in the neighborhood of this minima both \u2113(w, xl), \u2113(w, xr) are approximately linear. Using linear approximations of \u2113(w, xl), \u2113(w, xr) around w0, we\nshow that the value of L+ at wa = (0.3D, 0.9D) is smaller by \u01eb/20 than the minimal value of L0, hence\nmin w\u2217\u2208W\nL+(w \u2217) \u2264 L+(wa) \u2264 L0(w0)\u2212 \u01eb/20 . (2)\nMoreover, L+ is shown to be the sum of L0 and a function which is positive whenever w[1] \u2264 0, thus\nL+(w) \u2265 L0(w) , \u2200 w : w[1] \u2264 0 . (3)\nCombining Eqs. (2) and (3) we get\nL+(w)\u2212 min w\u2217\u2208W L+(w \u2217) \u2265 L0(w)\u2212\n( L0(w0)\u2212 \u01eb/20 ) \u2265 \u01eb/20 , \u2200 w : w[1] \u2264 0 ,\nwhere the last inequality follows from w0 being the minimizer of L0(w). A similar argument shows that for predictors w such that w[1] \u2265 0, it holds that L\u2212(w) \u2212 minw\u2217\u2208W L\u2212(w\u2217) \u2265 \u01eb/20.\nFor the proof of Theorem 4 we require a lemma that lower-bounds the minimal number of samples needed in order to distinguish between the distributions D+,D\u2212 defined in Fig. 4. To this end, we use the following modified version of Theorem 1.\nLemma 6. Let p \u2208 (0, 1/2]. Consider a distribution supported on three atoms with probabilities {q0, (1 \u2212 q0)(p + \u03c7\u01eb), (1 \u2212 q0)(1 \u2212 p \u2212 \u03c7\u01eb)}, with \u03c7 being either 0 or 1. Any algorithm that identifies the distribution correctly with probability at least 3/4, needs no less than p/16(1\u2212 q0)\u01eb2 samples.\nTheorem 6 can be proved similarly to Theorem 1 (see Appendix A). We are now ready to prove Theorem 4.\nProof of Theorem 4. Consider some algorithm A; we will show that if A observes T samples from a distribution D which is either D+ or D\u2212, then the expected excess loss \u01eb\u0303 that A can guarantee is lower bounded by \u2126( \u221a D/T ).\nThe excess loss is non negative; therefore if A guarantees an expected excess loss smaller than \u01eb\u0303 = \u01eb/80, then by Markov\u2019s inequality it achieves an excess loss smaller than \u01eb/20, w.p. \u2265 3/4. Denoting by wT the predictor that A outputs after T samples, then according to Theorem 5, attaining an excess loss smaller than \u01eb/20 on distribution D+(respectively D\u2212) implies wT [1] > 0 (respectively wT [1] < 0).\nSince A achieves an excess loss smaller than \u01eb/20 w.p. \u2265 3/4 for any D among D+,D\u2212 we can use its output to identify the right distribution w.p. \u2265 3/4. This can be done as follows:\nif wT [1] \u2265 0, return \u201cD+\u201d ; if wT [1] < 0, return \u201cD\u2212\u201d .\nAccording to Theorem 6, distinguishing between these two distributions w.p.\u2265 3/4 requires that the number of observations T to be upper bounded as follows:\nT \u2265 0.5(1\u2212 \u01eb) 16(1\u2212 p)(2\u01eb)2 \u2265 D 256 1 \u01eb2 ,\nWe used 0.5(1 \u2212 \u01eb) as a lower bound on the bias of distribution D\u2212 conditioned that the instance x0 was not chosen; since \u01eb \u2264 1/10D, D \u2265 2 it follows that 0.5(1 \u2212 \u01eb) \u2265 0.25. We also used 2\u01eb as the bias between the distributions D+ and D\u2212 conditioned that the label x0 was not chosen. Finally we used 1\u2212 p \u2264 1/D. The above inequality together with \u01eb\u0303 = \u01eb/80 yields a lower bound of 1\n1300\n\u221a D/T on the expected excess loss."}, {"heading": "3.3 Lower Bounds for Online Optimization", "text": "In Section 3 we proved two lower bounds for the convergence rate of stochastic logistic regression. Standard online-to-batch conversion (Cesa-Bianchi et al., 2004) shows that any online algorithm attaining a regret of R(T ) can be used to attain a convergence rate of R(T )/T for stochastic optimization. Hence, the lower bounds stated in Theorems 2 and 4 imply the following:\nCorollary 7. Consider the one dimensional online logistic regression setting with T = O(eD). For any algorithm A there exists a sequence of loss functions such that A suffers a regret of at least \u2126(D2/3T 1/3).\nCorollary 8. Consider the multidimensional online logistic regression setting with T = O(eD), D \u2265 2. For any algorithm A there exists a sequence of loss functions such that A suffers a regret of at least \u2126( \u221a DT )."}, {"heading": "4 Upper Bound for One-dimensional Regression", "text": "In this section we consider online logistic regression in one dimension; here an adversary chooses instances xt \u2208 [\u22121, 1], then a learner chooses predictors wt \u2208 W = {w \u2208 R : |w| \u2264 D}, and suffers a logistic loss \u2113(wt, xt) = log(1 + extwt). We provide an upper bound of O(T 1/3) for logistic online regression in one dimension, thus showing that the lower bound found in Theorem 2 is tight. Formally, we prove:\nTheorem 9. Consider the one dimensional online regression with logistic loss. Then a player that chooses predictors wt \u2208 W according to Algorithm 1 with \u03b7 = T\u22121/3 and D \u2265 2, achieves the following guarantee:\nRegretT =\nT\u2211\nt=1\nlog(1 + extwt)\u2212 min w\u2208W\nT\u2211\nt=1\nlog(1 + extw) = O(D3 T 1/3) .\nUsing standard online-to-batch conversion techniques Cesa-Bianchi et al. (2004), we can translate the upper bound given in the above lemma to an upper bound for stochastic optimization.\nCorollary 10. Consider the one dimensional stochastic logistic regression setting with D \u2265 2 and a budget of T samples. Then for any distribution D over instances, an algorithm that chooses predictors w1, . . . , wt \u2208 W according to Algorithm 1 with \u03b7 = T\u22121/3 and outputs wT = 1 T \u2211T \u03c4=1w\u03c4 , achieves the following guarantee:\nE[L(wT )] \u2212 min w\u2217\u2208[\u2212D,D] L(w\u2217) = O(D3/T 2/3) .\nFollowing Zinkevich (2003) and Hazan et al. (2007), we approximate the losses received by the adversary, and use the approximate losses in a follow-the-regularized-leader (FTRL) procedure in order to choose the predictors.\nFirst note the following lemma due to Zinkevich (2003) (proof is found in Hazan et al. (2007)):\nLemma 11. Let \u21131, . . . , \u2113T be an arbitrary sequence of loss functions, and let w1, . . . , wT \u2208 K. Let, \u2113\u03031, . . . , \u2113\u0303T be a sequence of loss function that satisfy \u2113\u0303t(wt) = \u2113t(wt), and \u2113\u0303t(w) \u2264 \u2113t(w) for all w \u2208 K. Then\nT\u2211\nt=1\n\u2113t(wt)\u2212min w\u2208K\nT\u2211\nt=1\n\u2113t(w) \u2264 T\u2211\nt=1\n\u2113\u0303t(wt)\u2212min w\u2208K\nT\u2211\nt=1\n\u2113\u0303t(w) .\nThus, the regret on the original losses is bounded by the regret of the approximate losses. For the logistic losses, \u2113(w, xt) = log(1 + e\nxtw), we define approximate losses \u2113\u0303t that satisfy the conditions of the last lemma. Depending on xt, wt, we divide into 3 cases:\n\u2113\u0303t(w) =\n \n\na0 + ytw + \u03b2 2 y2tw 21 w\u22640 if wt \u2265 0 and xt \u2265 1D ; a0 + ytw + \u03b2 2 y2tw\n21 w\u22650 if wt \u2264 0 and xt \u2264 \u2212 1D ; a0 + ytw + \u03b2 2 y2t (w \u2212 wt)2 if |xt| \u2264 1D or xtwt \u2264 0 ,\n(4)\nwhere,\nyt = \u2202\u2113(w, xt)\n\u2202w\n\u2223\u2223\u2223\u2223 wt = gtxt , gt = extwt 1 + extwt , \u03b2 = 1/8D , a0 = log(1 + e xtwt)\u2212 gtxtwt .\nThus, if |xt| \u2264 1/D or xtwt \u2264 0, then we use a quadratic approximation, else we use a loss that changes from linear to quadratic on w = 0. Note that if the approximation loss \u2113\u0303t is partially linear, then the magnitude of its slope |yt| is greater than 1/2D.\nThe approximations are depicted in Fig. 5. In Fig. 5(a) the approximate loss changes from linear to quadratic in w = 0 , where in Fig. 5(b) the approximate loss is quadratic everywhere. The following technical lemma states that the losses {\u2113\u0303t} satisfy the conditions of Theorem 11.\nLemma 12. Assume that D \u2265 2. Let \u2113(\u00b7, x1), . . . , \u2113(\u00b7, xT ) be a sequence of logistic loss functions and let w1, . . . , wT \u2208 W. The approximate losses \u2113\u03031, . . . , \u2113\u0303T defined above satisfy \u2113\u0303t(wt) = \u2113(wt, xt) and \u2113\u0303t(w) \u2264 \u2113(w, xt) for all w \u2208 W.\nTheorem 12 is proved in Section 5.4. We are now ready to describe our algorithm that obtains a regret of O(D3T 1/3) for one-dimensional online regression, given in Algorithm 1.\nAlgorithm 1 FTRL for logistic losses\nInput: Learning rate \u03b7 > 0, diameter D let R(w) = 1\n16D w2\nfor t = 1, 2 . . . T do set wt = argminw\u2208[\u2212D,D] {\u2211t\u22121 \u03c4=1 \u2113\u0303\u03c4 (w) + 1 \u03b7 R(w) }\nobserve xt \u2208 [\u22121, 1] and suffer loss \u2113(wt, xt) = log(1 + extwt) compute \u2113\u0303t according to Eq. (4)\nend for\nWe conclude with a proof sketch of Theorem 9; the complete proof is deferred to Section 5.3.\nProof of Theorem 9 (sketch). First we show that the regret of Algorithm 1 is upper bounded by the sum of differences \u2211T t=1 \u2113\u0303 \u2032 t(wt)(wt\u2212wt+1), and then divide the analysis into two cases. In the first case we show that the accumulated regret in rounds where \u2113\u0303t is quadratic around wt is upper bounded by O(D log T ). The second case analyses rounds in which \u2113\u0303t is linear around wt; due to the regularization, in the first such T\n2/3 rounds our regret is bounded by O(T 1/3) and if the number of such rounds is greater than T 2/3 we show that the quadratic part of the accumulated losses is large enough so the above sum of differences is smaller than O(D3T 1/3). Since the approximations \u2113\u0303t may change from linear to quadratic in w = 0, our analysis splits into two cases: the case where consecutive predictors wt, wt+1 have the same sign, and the case where they have opposite signs."}, {"heading": "5 Proofs", "text": ""}, {"heading": "5.1 Proof of Theorem 3", "text": "Proof. We assume that the following holds:\n\u2126(e\u2212D) = 40e\u22120.45D \u2264 \u01eb \u2264 1 25 .\nIn the proof we use the following:\n\u03b8D \u2264 0.2; 1\u2212 \u03b8 2 \u2265 0.9 ,\nthe first follows since: \u03b8D = \u221a \u01eb \u2264 \u221a 1 25\n= 0.2, combing the latter with D \u2265 1 we get 1\u2212 \u03b8\n2 \u2265 0.9. Next we prove the lemma in three steps:\nStep 1: Linear/quadratic approximation in [D/2, D]. We show that for w \u2208 [D/2, D], the logistic losses of the instances (1 \u2212 \u03b8\n2 ),\u2212\u03b8 are linear/quadratic, up to an additive term\nof \u2206 \u2264 \u01eb/40: \u2223\u2223\u2223\u2223\u2113(w, 1\u2212 \u03b8 2 )\u2212 (1\u2212 \u03b8 2 )w \u2223\u2223\u2223\u2223 = log(1 + e \u2212(1\u2212 \u03b8 2 )w) \u2264 e\u2212(1\u2212 \u03b82 )w \u2264 e\u22120.45D \u2264 \u2206, \u2200w \u2208 [D/2, D]\n(5)\u2223\u2223\u2223\u2223\u2113(w,\u2212\u03b8)\u2212 ( log 2\u2212 \u03b8\n2 w +\n(\u03b8w)2\n8 )\u2223\u2223\u2223\u2223 \u2264 maxw\u0304\u2208[\u2212D,D] (\u03b8w\u0304)4 192 \u2264 (\u03b8D) 4 192 \u2264 \u2206, \u2200w \u2208 [\u2212D,D]\n(6)\nrecalling \u2113(w, x) = log(1 + exw), in the first equality of Eq. (5) we used, log(1 + ez) = z + log(1 + e\u2212z), next we used log(1 + z) \u2264 z , finally we used w \u2265 D/2 and (1\u2212 \u03b8\n2 ) \u2265 0.9.\nIn Eq. (6) we used the second order taylor approximation of the loss around 0, and the RHS of the second inequality is an upper bound to the error of this approximation. We define \u2206 = max{e\u22120.45D, (\u03b8D)4 192 }; using \u03b8 = \u221a \u01eb D , 40e\u22120.45D \u2264 \u01eb \u2264 1 25 and D \u2265 1 we can bound:\n\u2206 \u2264 \u01eb/40 .\nStep 2: proving the lemma for w \u2208 [D/2, D]. Recall the notation L+(w), L\u2212(w) for the expected losses according to D+,D\u2212; using Eqs. (5) and (6), we can write:\nL+(w) =\n( \u03b8\n2 +\n\u01eb\nD\n)( 1\u2212 \u03b8\n2\n) w + ( 1\u2212 \u03b8\n2 \u2212 \u01eb D\n)( log 2\u2212 \u03b8\n2 w +\n(\u03b8w)2\n8\n) \u00b1\u2206\n= \u01eb\nD w +\n( 1\u2212 \u03b8\n2 \u2212 \u01eb D\n) log 2 + ( 1\u2212 \u03b8\n2 \u2212 \u01eb D\n) (\u03b8w)2\n8 \u00b1\u2206, \u2200w \u2208 [D/2, D] .\nUsing the latter expression for L+ we can bound the excess loss for w \u2208 [D/2, D] as follows:\nL+(w)\u2212 min w\u2217\u2208W L+(w \u2217) \u2265 L+(w)\u2212 L+(D/2)\n\u2265 \u01eb D\n( w \u2212 D\n2\n) + \u03b82\n8\n( 1\u2212 \u03b8\n2 \u2212 \u01eb D\n)( w2 \u2212 D 2\n4\n) \u2212 2\u2206\n\u2265 \u01eb D\n( w \u2212 D\n2\n) + \u03b82\n10\n( w2 \u2212 D 2\n4\n) \u2212 2\u2206 ,\nwhere in the last inequality we used \u03b8/2 \u2264 0.1 and \u01eb/D \u2264 1/25. Hence, for w \u2265 3D/4, we have\nL+(w)\u2212min w\u2217\nL+(w \u2217) \u2265 \u01eb\n4 +\n\u03b82\n10\n5D2\n16 \u2212 2\u2206 \u2265 \u01eb 20 ,\nwhere we used \u2206 \u2264 \u01eb/40. Similarly to L+ we can show that\nL\u2212(w) = \u2212 \u01eb\nD w +\n( 1\u2212 \u03b8\n2 +\n\u01eb\nD\n) log 2 + ( 1\u2212 \u03b8\n2 +\n\u01eb\nD\n) (\u03b8w)2\n8 \u00b1\u2206 , \u2200w \u2208 [D/2, D] .\nUsing the latter expression for L\u2212 we can bound the excess loss for w \u2208 [D/2, D] as follows: L\u2212(w)\u2212 min\nw\u2217\u2208W L\u2212(w\n\u2217) \u2265 L\u2212(w)\u2212 L\u2212(D)\n\u2265 \u2212 \u01eb D (w \u2212D) + \u03b8\n2\n8\n( 1\u2212 \u03b8\n2 \u2212 \u01eb D\n) (w2 \u2212D2)\u2212 2\u2206\n\u2265 \u2212 \u01eb D (w \u2212D) + \u03b8\n2\n8\n( D2\n4 \u2212D2\n) \u2212 2\u2206 .\nHence, for w \u2208 [D 2 , 3D 4 ], we have:\nL\u2212(w)\u2212min w\u2217\nL\u2212(w \u2217) \u2265 \u01eb 4 \u2212 \u03b8\n2\n8\n3D2\n4 \u2212 2\u2206 \u2265 \u01eb 20 . (7)\nStep 3: Extending the lemma to w \u2208 [\u2212D,D]. We are left to prove:\nL\u2212(w)\u2212min w\u2217\nL\u2212(w \u2217) \u2265 \u01eb\n20 , \u2200w \u2208 [\u2212D,D/2] .\nAccording to Eq. (7), it suffices to prove L\u2212(w) \u2265 L\u2212(D/2), \u2200w \u2208 [\u2212D,D/2]. Since L\u2212 is convex, showing that the derivative of L\u2212 at D/2 is negative implies that L\u2212(w) \u2265 L\u2212(D/2), \u2200w \u2264 D/2. Deriving L\u2212(w) at D/2 we get:\nd\ndw L\u2212(w) \u2223\u2223\u2223\u2223 D/2 = ( \u03b8 2 \u2212 \u01eb D )( 1\u2212 \u03b8 2 ) 1 1 + e\u2212(1\u2212 \u03b8 2) D 2 \u2212 \u03b8 ( 1\u2212 \u03b8 2 + \u01eb D ) 1 1 + e \u03b8D 2\n\u2264 ( \u03b8\n2 \u2212 \u01eb D\n)( 1\u2212 \u03b8\n2\n) \u2212 \u03b8 ( 1\u2212 \u03b8\n2 +\n\u01eb\nD\n)( 1\n2 \u2212 \u03b8D 8\n)\n\u2264 \u2212 \u01eb D + \u03b82D 8 = \u2212 \u01eb D + \u01eb 8D \u2264 0 ,\nwhere in the first inequality we used (1+ ex)\u22121 \u2264 1, \u2200x, and (1+ ex)\u22121 \u2265 1 2 \u2212 x 4 , \u2200x \u2265 0, this is since (1 + ex)\u22121 is convex for x \u2265 0 and 1 2 \u2212 x 4 is its tangent at x = 0. In the last line we\nused \u03b8 = \u221a \u01eb/D."}, {"heading": "5.2 Proof of Theorem 5", "text": "Proof. We assume that the following holds:\n\u2126(e\u2212D) = 100e\u22120.6D/ \u221a 2 \u2264 \u01eb \u2264 1\n10D = O(1/D) .\nIn the proof we will need to use: 1 6D \u2264 1\u2212p 2 \u2264 1 2D\n, this can be shown by simple algebra using the definition of p in Fig. 4 and using D \u2265 2. Next we prove the lemma in three steps:\nStep 1: Define L0(w) and find its minima. Define:\nL0(w) = p\u2113(w, x0) + 1\u2212 p 2 \u2113(w, xl) + 1\u2212 p 2 \u2113(w, xr) ,\nwhere p is defined in Fig. 4. Note that L0 is the unperturbed version (\u01eb = 0) of L+, L\u2212. We want to show that w0 = (0, 0.9D) is the global minimizer of L0; since L0(w) is convex it is sufficient to show that \u2207L0(w0) = 0. Deriving L0 we get\n\u2207L0(w) = p\nD\n1 1 + ew[2]/D (0,\u22121)+1\u2212 p 2 \u221a 2\n( 1\n1 + e(w[1]\u2212w[2])/ \u221a 2 (\u22121, 1) + 1 1 + e\u2212(w[1]+w[2])/ \u221a 2 (1, 1)\n) .\nsubstituting p so that p 1\u2212p = D\u221a 2\n1+e0.9\n1+e\u22120.9D/ \u221a 2 , and w0 = (0, 0.9D) confirms that the gradient is\nindeed zero at w0.\nStep 2: Bounding the minimal loss of L+(w). We would like to upper bound the minimal value of L+(w) as follows:\nmin w\u2217\u2208W\nL+(w \u2217) \u2264 L0(w0)\u2212 \u01eb/20 .\nWe do so by showing that for wa = (0.3D, 0.9D) it holds that L+(wa) \u2264 L0(w0)\u2212 \u01eb/20. First, notice that we can write wa = w0 + ua, where ua = (0.3D, 0). Recalling \u2113(w, x) = log(1 + ex\u00b7w), we use x0 \u00b7 wa = x0 \u00b7 w0 = \u22120.9 to get:\n\u2113(wa, x0) = \u2113(w0, x0) . (8)\nMoreover:\n\u2113(wa, xl) = xl \u00b7 wa + log(1 + e\u2212xl\u00b7wa) = xl \u00b7 w0 + xl \u00b7 ua + log(1 + e\u2212 0.6D\u221a 2 ) (9)\n\u2264 \u2113(w0, xl)\u2212 0.3D\u221a\n2 + e\n\u2212 0.6D\u221a 2 ,\nrecalling \u2113(w, x) = log(1+ ex\u00b7w), in the equalities we used log(1+ ez) = z+ log(1+ e\u2212z), and xl \u00b7 wa = 0.6D\u221a2 ; In the inequality we used xl \u00b7 ua = \u2212 0.3D\u221a 2 , next we used z \u2264 log(1 + ez), and also log(1 + z) \u2264 z. Similarly to Eq. (9), we can show:\n\u2113(wa, xr) \u2264 \u2113(w0, xr) + 0.3D\u221a\n2 + e\n\u2212 1.2D\u221a 2 . (10)\nNow, plugging Eqs. (8) to (10) into the definition of L+(wa), we get:\nL+(wa) \u2264 p\u2113(w0, x0) + 1\u2212 p 2 (1 + \u01eb)\n( \u2113(w0, xl)\u2212\n0.3D\u221a 2 + e \u2212 0.6D\u221a 2\n)\n+ 1\u2212 p 2\n(1\u2212 \u01eb) ( \u2113(w0, xr) +\n0.3D\u221a 2 + e \u2212 1.2D\u221a 2\n)\n\u2264 L0(w0)\u2212 1\u2212 p 2 (0.3 \u221a 2D)\u01eb+ e \u2212 0.6D\u221a 2\n\u2264 L0(w0)\u2212 \u221a 2\n20 \u01eb+ e\n\u2212 0.6D\u221a 2\n< L0(w0)\u2212 \u01eb/20 , (11)\nwe used \u2113(w0, xl) = \u2113(w0, xr), and 1\u2212p 2 \u2265 1 6D\n, we also used \u01eb \u2265 100e\u22120.6D/ \u221a 2. So we showed\nthat L+(wa) is upper bounded by L0(w0) \u2212 \u01eb/20, thus upper bounding the minimum of L+(w).\nStep 3: Bound the excess loss of predictors w : w[1] \u2264 0. In order to so, it is sufficient to show that the value of such predictors is greater by \u01eb/20 than the upper bound we found for minw\u2217\u2208W L+(w \u2217). Let us write L+(w) as a sum of L0(w) and a perturbation:\nL+(w) = L0(w) + (1\u2212 p)\u01eb\n2\n( log(1 + exl\u00b7w)\u2212 log(1 + exr \u00b7w) )\n= L0(w) + (1\u2212 p)\u01eb\n2\n( log ( 1 + e 1\u221a 2 (w[2]\u2212w[1]))\u2212 log ( 1 + e 1\u221a 2 (w[2]+w[1])))\n\u2265 L0(w) , \u2200w[1] \u2264 0 .\nThe inequality follows since w[1] \u2264 0 and log(1+ez) is monotonically increasing, therefore the perturbation summand is positive. Combining the above inequality with the upper bound found in step 2 above we get:\nL+(w)\u2212 min w\u2217\u2208W L+(w \u2217) \u2265 L0(w)\u2212\n( L0(w0)\u2212 \u01eb/20 ) \u2265 \u01eb/20 , \u2200w : w[1] \u2264 0 ,\nand the last inequality follows from w0 being the minimizer of L0(w). We can similarly show that for predictors w such that w[1] \u2265 0, then L\u2212(w)\u2212minw\u2217\u2208W L\u2212(w\u2217) \u2265 \u01eb/20 applies."}, {"heading": "5.3 Proof of Theorem 9", "text": "Since the approximate losses \u2113\u0303t defined in Eq. (4) satisfy the conditions of Theorem 11 then it suffices to prove the lower bound for the regret of the \u2113\u0303t\u2019s.\nDenoting, Ft(w) = \u2211t\u22121\n\u03c4=1 \u2113\u0303\u03c4 (w)+R(w), then Algorithm 1 chooses wt = argminw\u2208W Ft(w). Letting ut be the global minimizer of Ft, the following is equivalent to Algorithm 1:\nAlgorithm 2 Equivalent form-FTRL\nCalculate: ut = argminw\u2208R \u2211t\u22121 \u03c4=1 \u2113\u0303\u03c4 (w) + \u03b7 \u22121R(w) Choose: wt = argminw\u2208W |w \u2212 ut|\nAlgorithm 2 first finds ut, the global minima of Ft, and then projects ut onto W. The expression for ut in Algorithm 2 is useful since it enables us to calculate the differences |ut\u22121\u2212 ut|, which upper bound the differences between predictors: |wt\u22121 \u2212wt|; these differences are useful in bounding the regret of FTRL as seen in the next lemma due to Kalai and Vempala (2005) (proof can be found in Hazan (2011) or in Shalev-Shwartz (2011)):\nLemma 13. Let a regularizer function R, and ft, for t = 1, . . . , T , be a sequence of cost functions and let wt = argminw\u2208K \u2211t\u22121 \u03c4=1 f\u03c4 (w) + \u03b7 \u22121R(w), Then:\nT\u2211\nt=1\nft(wt)\u2212 T\u2211\nt=1\nft(v) \u2264 T\u2211\nt=1\n\u2207ft(wt) \u00b7 (wt \u2212 wt+1) + \u03b7\u22121(R(v)\u2212R(w1)), \u2200v \u2208 K\nNote that in our one-dimensional case the gradient \u2207\u2113\u0303t(w) is simply the derivative \u2113\u0303\u2032t(w). Also note that we can bound the FTL-BTL differences: \u2113\u0303\u2032t(wt)(wt \u2212 wt+1) as follows:\n\u2113\u0303\u2032t(wt)(wt \u2212 wt+1) \u2264 |\u2113\u0303\u2032t(wt)(wt \u2212 wt+1)| \u2264 |yt||wt \u2212 wt+1| \u2264 |yt||ut \u2212 ut+1| , (12)\nwhere we used \u2113\u0303\u2032t(wt) = yt (see Eq. (4)), we also used |wt+1 \u2212wt| \u2264 |ut+1 \u2212 ut| which follows from wt being the projection of ut onto W = [\u2212D,D].\nCombining Theorem 13 with Eq. (12), the regret of Algorithm 1 is bounded as follows:\nRegretT \u2264 T\u2211\nt=1\n|yt(wt \u2212 wt+1)|+ \u03b7\u22121 D\n16 \u2264\nT\u2211\nt=1\n|yt(ut \u2212 ut+1)|+ D\n16 T 1/3 , (13)\nwhere we used R(w) = 1 16D w2 \u2264 D 16 , \u2200w \u2208 W, and \u03b7 = T\u22121/3. In Sections 5.3.1 and 5.3.2, we analyze the differences |yt(wt \u2212 wt+1)|, we divide the analysis into two cases:\n1. rounds in which utut+1 \u2265 0: Section 5.3.1.\n2. rounds in which utut+1 < 0: Section 5.3.2.\n5.3.1 Rounds in which utut+1 \u2265 0 Assume without loss of generality that ut, ut+1, are both positive. Hence, ui = argminw\u22650 Fi(w), i \u2208 {t, t+ 1}. For w \u2265 0, the losses are either linear with a positive slope \u2265 1/2D, or quadratic losses, this can be seen easily from Eq. (4). Lets introduce some notation:\nyqt = yt1{|xt| \u2264 1/D or xtwt \u2264 0 } + yt1{wt \u2264 0, xt \u2264 \u22121/D }; y l t = yt1{ wt \u2265 0, xt \u2265 1/D};\nThe notation\u201cq\u201d, stands for quadratic losses on w \u2265 0, the \u201cl\u201d notation is for losses that are linear on w \u2265 0. We will also use the following notation w\u0302t:\nw\u0302t =\n   0; if wt \u2264 0, xt \u2264 \u2212 1\nD wt; otherwise\n(14)\nUsing these new notations, and the expression for the \u2113\u0303t\u2019s in Eq. (4), then \u2200w \u2265 0:\nFt(w) =\nt\u22121\u2211\n\u03c4=1\n\u2113\u0303\u03c4 (w) + \u03b7 \u22121\u03b2\n2 w2 =\nt\u22121\u2211\n\u03c4=1\ny\u03c4w + \u03b2\n2\nt\u22121\u2211\n\u03c4=1\n(yq\u03c4) 2(w \u2212 w\u0302\u03c4 )2 + \u03b7\u22121\n\u03b2 2 w2 ,\nwhere we used R(w) = 1 16D w2 = \u03b2 2 w2. From the last expression we can derive an analytic expression for ut:\nut = argmin w\u22650\nFt(w) = \u2212 1\n\u03b2\n\u2211t\u22121 \u03c4=1 y\u03c4 \u2212 \u03b2 \u2211t\u22121 \u03c4 (y q \u03c4 )\n2w\u0302\u03c4\u2211t\u22121 \u03c4 (y q \u03c4 )2 + \u03b7\u22121 . (15)\nNext we analyze the sum of differences \u2211t\n\u03c4=1 y\u03c4 (w\u03c4 \u2212 w\u03c4+1), the analysis divides into two sub-cases, first we analyze rounds in which \u2113\u0303t is quadratic, and then we analyze rounds where \u2113\u0303t is linear:\nRounds when \u2113\u0303t is quadratic for w \u2265 0: In that case yt = yqt and we have:\n\u2113\u0303t(w) = y q tw +\n\u03b2 2 (yqt ) 2(w \u2212 wt)2, \u2200w \u2265 0 ,\nfor such a quadratic loss \u2113\u0303t, then Eq. (15) provides an analytic expression for ut+1, subtracting ut is can be shown that:\nut \u2212 ut+1 = 1\n\u03b2\nyqt + \u03b2(y q t ) 2(ut \u2212 wt)\u2211t\u22121 \u03c4=1(y q \u03c4 )2 + \u03b7\u22121 , (16)\nIf both ut, ut+1 \u2265 D, it means that wt = wt+1 = D, and therefore:\nyqt (wt \u2212 wt+1) = 0 . (17)\nIf either ut < D or ut+1 < D, we have:\n|yqt (wt \u2212 wt+1)| \u2264 |yqt (ut \u2212 ut+1)| = 1\n\u03b2\n(yqt ) 2\n\u2211t \u03c4=1(y q \u03c4 )2 + \u03b7\u22121 |1 + \u03b2yqt (ut \u2212 wt)|\n\u2264 2 \u03b2\n(yqt ) 2\n\u2211t \u03c4=1(y q \u03c4 )2 + \u03b7\u22121 , (18)\nwhere we used the inequality |ut \u2212 ut+1| \u2264 4D (can be derived from the expressions for ut, ut+1 ), and thus if either ut, ut+1 is smaller than D it follows |ut| \u2264 5D, we then use |wt| \u2264 D, |ut| \u2264 5D, \u03b2 = 18D , and |y q t | \u2264 1 to show that |1 + \u03b2yqt (ut \u2212 wt)| \u2264 2.\nThus, for rounds in which utut+1 \u2265 0 and \u2113\u0303t is quadratic, we can bound the regret by:\n2\n\u03b2\nT\u2211\nt=1\n(yqt ) 2\n\u2211t \u03c4=1(y q \u03c4 )2 + \u03b7\u22121 \u2264 16D log(T + 1) , (19)\nwhere we used \u03b2 = 1/8D together with the following lemma, taken from Hazan et al. (2007):\nLemma 14. Let vt \u2208 R, for t = 1, . . . , T , be a sequence of scalars such that for some r, |vt| \u2264 r. Then:\nT\u2211\nt=1\nv2t\u2211t \u03c4=1 v 2 \u03c4 + \u01eb \u2264 log(r2T/\u01eb+ 1) .\nRounds when \u2113\u0303t is linear for w \u2265 0: In that case xt \u2265 1D , yt = ylt and we have:\n\u2113\u0303t(w) = y l tw, \u2200w \u2265 0 ,\nfor such a linear loss \u2113\u0303t, then Eq. (15) provides an analytic expression for ut+1, subtracting ut is can be shown that:\nut \u2212 ut+1 = 1\n\u03b2 ylt\u2211t\u22121 \u03c4=1(y q \u03c4 )2 + \u03b7\u22121 . (20)\nWe are left to bound the sum of differences yt(wt \u2212wt+1) at times in which \u2113\u0303t is linear (and therefore yt = y l t, and xt \u2265 1D ); according to Eq. (20) each such difference is bounded by:\nylt(wt \u2212 wt+1) = 1\n\u03b2\n(ylt) 2\n\u2211t\u22121 \u03c4=1(y q \u03c4 )2 + \u03b7\u22121 . (21)\nDefine n+(t), to be the number of positive linear losses received at the first t rounds:\nn+(t) = t\u2211\n\u03c4=1\n1{x\u03c4\u2265 1D ,wt\u22650} .\nSuppose that n+(T ) \u2264 T 2/3, and recall \u03b7 = T 1/3 and |ylt| \u2264 1, then: T\u2211\nt=1\nylt(wt \u2212 wt+1)1{xt\u2265 1D ,wt\u22650} = 1\n\u03b2\nT\u2211\nt=1\n(ylt) 2\n\u2211t\u22121 \u03c4=1(y q \u03c4 )2 + \u03b7\u22121\n1{xt\u2265 1D ,wt\u22650} \u2264 1 \u03b2 \u03b7n+(T )\n\u2264 1 \u03b2 T\u22121/3T 2/3 = 8DT 1/3 . (22)\nSuppose on the contrary that n+(T ) \u2265 T 2/3, so till time t0 for which n+(t0) = T 2/3, we accumulate a regret bounded by 8DT 1/3. Next, we analyze the FTL-BTL differences at rounds in which xt \u2265 1D , wt \u2265 0, and t \u2265 t0. From Eq. (15) for ut, it can be seen that ut \u2265 0 implies:\nt\u22121\u2211\n\u03c4=1\ny\u03c4 \u2212 \u03b2 t\u22121\u2211\n\u03c4=1\n(yq\u03c4 ) 2w\u0302\u03c4 \u2264 0 .\nthe latter equation can be written as follows:\nt\u22121\u2211\n\u03c4=1\n(\u2212y\u03c4 )1{y\u03c4<0} + \u03b2 t\u22121\u2211\n\u03c4=1\n(yq\u03c4 ) 2w\u0302\u03c4 \u2265\nt\u22121\u2211\n\u03c4=1\ny\u03c41{y\u03c4\u22650} . (23)\nThe RHS of the last equation can be lower bounded as follows:\nt\u22121\u2211\n\u03c4=1\ny\u03c41{y\u03c4\u22650} \u2265 t\u22121\u2211\n\u03c4=1\nyl\u03c4 \u2265 1\n2D n+(t\u2212 1) , (24)\nwhere we used the definition of n+(t), and y l \u03c4 \u2265 1/2D. The LHS of Eq. (23) is upper bounded as follows:\nt\u22121\u2211\n\u03c4=1\n(\u2212y\u03c4 )1{y\u03c4<0} + \u03b2 t\u22121\u2211\n\u03c4=1\n(yq\u03c4 ) 2w\u0302\u03c4 \u2264\nt\u22121\u2211\n\u03c4=1\n|yq\u03c4 |+ 1\n8\nt\u22121\u2211\n\u03c4=1\n|yq\u03c4 | \u2264 9\n8\n\u221a\u221a\u221a\u221a(t\u2212 1) t\u22121\u2211\n\u03c4=1\n(yq\u03c4 )2 , (25)\nin the first inequality we used \u2211t\u22121 \u03c4=1(\u2212y\u03c4 )1{y\u03c4<0} \u2264 \u2211t\u22121\n\u03c4=1 |yq\u03c4 |, also |w\u0302\u03c4 | \u2264 D, \u03b2 = 1/8D, and finally (yq\u03c4 ) 2 \u2264 |yq\u03c4 | (since |yq\u03c4 | \u2264 1); in the second inequality we used ||z||1 \u2264 \u221a\nN ||z||22, \u2200z \u2208 R N . Combining Eqs. (23) to (25) we get:\nt\u22121\u2211\n\u03c4=1\n(yq\u03c4) 2 \u2265 1 10D2 n2+(t\u2212 1) t\u2212 1 \u2265 1 10D2 n2+(t\u2212 1) T . (26)\nUsing the inequality in Eq. (26) inside Eq. (21), then the sum of differences yt(wt \u2212 wt+1) for the rounds with a linear loss (hence yt = y l t) and t > t0, we can upper bound:\nT\u2211\nt=t0+1\nylt(wt \u2212 wt+1) \u2264 1\n\u03b2\nT\u2211\nt=t0+1\n(ylt) 2\n\u2211t\u22121 \u03c4=1(y q \u03c4 )2 + \u03b7\u22121\n\u2264 80D3T T\u2211\nt=t0+1\n1\nn2+(t\u2212 1) 1{xt\u2265 1D ,wt\u22650}\n\u2264 80D3T n+(T )\u2211\ni=T 2/3\n1 i2 \u2264 80D3T 2 T 2/3 = 160D3T 1/3 , (27)\nwhere we assumed n+(t0) = T 2/3 \u2264 n+(T ), and used \u03b2 = 1/8D, (ylt)2 \u2264 1, finally we applied:\nn2\u2211\ni=n1\n1 i2 \u2264 1 n21 +\n\u222b \u221e\ny=n1\n1\ny2 dy =\n1\nn21 +\n1 n1 \u2264 2 n1 .\nHence during rounds where utut+1 \u2265 0, then Eqs. (19), (22) and (27) upper bound the regret of Algorithm 1 by:\n16D log(T + 1) + 8DT 1/3 + 160D3T 1/3 .\n5.3.2 Rounds in which utut+1 < 0\nAssume without loss of generality that, ut \u2265 0, and ut+1 < 0, thus, ut = argminw\u22650 Ft(w) and ut+1 = argminw\u22640 Ft+1(w). Since ut \u2265 0, then according to Eq. (15) we have:\nt\u22121\u2211\n\u03c4=1\ny\u03c4 \u2212 \u03b2 t\u22121\u2211\n\u03c4\n(yq\u03c4 ) 2w\u0302\u03c4 \u2264 0 .\nSince ut+1 \u2264 0, we must have: t\u22121\u2211\n\u03c4=1\ny\u03c4 \u2212 \u03b2 t\u22121\u2211\n\u03c4=1\n(yq\u03c4 ) 2w\u0302\u03c4 + yt \u2212 \u03b2(yqt )2w\u0302t \u2265 0 ,\nor else the global minima would be positive. The last two inequalities imply that:\n| t\u22121\u2211\n\u03c4=1\ny\u03c4 \u2212 \u03b2 t\u22121\u2211\n\u03c4=1\n(yq\u03c4 ) 2w\u0302\u03c4 | \u2264 yt \u2212 \u03b2(yqt )2w\u0302t \u2264 yt . (28)\nCombining the last equation with Eq. (15), we get:\nut \u2264 1\n\u03b2 yt\u2211t\u22121 \u03c4 (y q \u03c4)2 + \u03b7\u22121 ,\nand therefore:\nytut \u2264 1\n\u03b2 y2t\u2211t\u22121 \u03c4 (y q \u03c4 )2 + \u03b7\u22121 .\nSimilar to the analysis made in Section 5.3.1 we can show that:\nT\u2211\nt=1\nytut1{utut+1<0} \u2264 16D log(T + 1) + 8DT 1/3 + 160D3T 1/3 .\nsymmetrically, we can show:\nT\u2211\nt=1\nytut+11{utut+1<0} \u2265 \u221216D log(T + 1)\u2212 8DT 1/3 \u2212 160D3T 1/3 .\nFrom the last two inequalities, it follows:\nT\u2211\nt=1\nyt(ut \u2212 ut+1)1{utut+1<0} \u2264 32D log(T + 1) + 16DT 1/3 + 320D3T 1/3 ."}, {"heading": "5.3.3 Concluding the proof", "text": "According to Sections 5.3.1 and 5.3.2, the regret of Algorithm 1 is upper bounded by:\nRegretT \u2264 48D log(T + 1) + 24DT 1/3 + 480D3T 1/3 + D\n16 T 1/3 ,\nwhere the last term is due to the regularization."}, {"heading": "5.4 Proof of Theorem 12", "text": "Proof. For ease of notation we use the following shorthand for the logistic loss:\n\u2113t(w) := \u2113(w, xt) = log(1 + e xtw)\nThe proof is divided into 4 cases:\nCase 0. Denote by \u2113\u0303 (0) t , an approximate loss of the logistic around w = 0, thus:\n\u2113\u0303 (0) t (w) = \u2113t(0) + \u2113 \u2032 t(0)w +\n\u03b2 2 x2tw 2 = log(2) + xt 2 w + \u03b2 2 x2tw 2 (29)\nwhere we used \u2113t(w) = log(1 + e xtw). Next, we show that \u2113\u0303 (0) t (w) \u2264 \u2113t(w), \u2200w \u2208 [\u2212D,D]. Lets write \u2113t(w)\u2212 \u2113\u0303(0)t (w), explicitly:\n\u2113t(w)\u2212 \u2113\u0303(0)t (w) = log(1 + extw)\u2212 log(2)\u2212 xtw 2 \u2212 \u03b2 2 (xtw) 2 = log(1 + ez)\u2212 log(2)\u2212 z 2 \u2212 \u03b2 2 z2\n= log( e\u2212z/2 + ez/2\n2 )\u2212 \u03b2 2 z2\nand we denoted z = xtw. Thus, it is sufficient to show that log( e\u2212z/2+ez/2 2 )\u2212 \u03b2 2 z2 \u2265 0, \u2200z \u2208 [\u2212D,D]. Assume z \u2208 [\u221210, 10], then from the taylor expansion of log( e\u2212z/2+ez/2 2\n) around zero, there exists z\u0304 : |z\u0304| \u2264 10 such that:\nlog( e\u2212z/2 + ez/2\n2 )\u2212 \u03b2 2 z2 =\nz2\n8 \u2212 z\u0304\n4\n192 \u2212 \u03b2 2 z2 \u2265 (1 8 \u2212 1 16D )z2 \u2212 z\n4\n192\n\u2265 z 2 16 \u2212 z 4 192 \u2265 0, \u2200z \u2208 [\u221210, 10]\nwhere we used \u03b2 = 1 8D , D \u2265 2, and |z\u0304| \u2264 |z| \u2264 10. Assuming 10 \u2264 |z| \u2264 D:\nlog( e\u2212z/2 + ez/2\n2 )\u2212 \u03b2 2 z2 \u2265 log(e|z|/2)\u2212 \u03b2 2 z2 \u2212 log(2) = |z| 2 \u2212 1 16D z2 \u2212 log(2)\n\u2265 |z| 2 \u2212 |z| 8 \u2212 log(2) \u2265 0, \u220010 \u2264 |z| \u2264 D\nwe used \u03b2 = 1 8D , in the second inequality we used |z| \u2264 D, and in the last inequality we used |z| \u2265 10. So we have shown:\n\u2113\u0303 (0) t (w) \u2264 \u2113t(w), \u2200w \u2208 [\u2212D,D] (30)\nCase 1: wt \u2265 0, xt \u2265 1D . For that case, the approximate loss \u2113\u0303t of Eq. (4) can be written as follows:\n\u2113\u0303t(w) =    \u2113t(wt) + \u2113 \u2032 t(wt)(w \u2212 wt); if w \u2208 [0, D]\n\u2113t(wt) + \u2113 \u2032 t(wt)(w \u2212 wt) +\n\u03b2 2 y2tw 2; if w \u2208 [\u2212D, 0] (31)\nwhere \u2113t(w) = log(1 + e xtw), yt = \u2113 \u2032 t(wt) = xtextwt 1+extwt . It is easily noticed that \u2113\u0303t(wt) = \u2113t(wt). Also note that for positive instances \u2113\u0303t(w) is the tangent of \u2113t(w) at wt, since \u2113t(w) is convex it follows that: \u2113\u0303t(w) \u2264 \u2113t(w), \u2200w \u2208 [0, D] We are left to prove the latter inequality holds for negative instances. Recalling \u2113\u0303\n(0) t from\nEq. (29), we will show that:\n\u2113\u0303t(w) \u2264 \u2113\u0303(0)t (w) \u2264 \u2113t(w) \u2200w \u2208 [\u2212D, 0] (32) Thus, concluding the proof. The lefthand inequality of Eq. (32) can be derived as follows:\n\u2113\u0303t(w) = \u2113t(wt) + \u2113 \u2032 t(wt)(w \u2212 wt) +\n\u03b2 2 y2tw 2 = \u2113\u0303t(0) + \u2113 \u2032 t(wt)w + \u03b2 2 y2tw 2 (33)\n\u2264 \u2113t(0) + \u2113\u2032t(0)w + \u03b2\n2 x2tw 2 = \u2113\u0303 (0) t (w), \u2200w \u2264 0, wt \u2208 [0, D]\nwhere we used \u2113\u0303t(0) \u2264 \u2113t(0), 0 \u2264 \u2113\u2032t(0) \u2264 \u2113\u2032t(wt), and w \u2264 0, moreover we used |yt| = | xtextwt 1+extwt\n| \u2264 |xt|. The righthand inequality of Eq. (32), is proved in the former case, see Eq. (30).\nThe proof for the case wt \u2264 0, xt \u2264 \u2212 1D is similar.\nCase 2: |xt| \u2264 1D . For that case, the approximate loss \u2113\u0303t of Eq. (4) can be written as follows:\n\u2113\u0303t(w) = \u2113t(wt) + \u2113 \u2032 t(wt)(w \u2212 wt) +\n\u03b2\n2\n( \u2113\u2032t(wt) )2 (w \u2212 wt)2 (34)\nwhere we used, yt = \u2113 \u2032 t(wt). Noticeably \u2113\u0303t(wt) = \u2113t(wt). To prove \u2113\u0303t(w) \u2264 \u2113t(w), we require the following lemma from Hazan et al. (2007):\nLemma 15. For a function f : K \u2192 R, where K has diameter D, such that \u2200w \u2208 K, ||\u2207f(w)|| \u2264 G, and e\u2212\u03b1f(w) is concave, the following holds for \u03b3 = 1\n2 min{ 1 4GD , \u03b1}:\nf(w) \u2265 f(w0) +\u2207f(w0)T (w \u2212 w0) + \u03b3\n2 (\u2207f(w0)T (w \u2212 w0))2, \u2200w,w0 \u2208 K\nIn Hazan et al. (2007) it is also shown that for one dimensional functions, if \u03b1 \u2264 minw\u2208K f \u2032\u2032(w)(\nf \u2032(w) )2 ,\nthen e\u2212\u03b1f(w) is concave in K. In the case of logistic loss \u2113t(w) = log(1 + extw), the norm of its derivative is bounded by 1, moreover:\n\u2113\u2032\u2032t (w)( \u2113\u2032t(w) )2 = e \u2212xtw (35)\nSince |xt| \u2264 1D , and |w| \u2264 D, then \u03b10 := e\u22121 \u2264 minw\u2208[\u2212D,D] \u2113\u2032\u2032t (w)( \u2113\u2032t(w) )2 , implying \u03b3 = 1 2 min{ 1 4D , e\u22121} = 1 8D . Applying Theorem 15 to the logistic loss \u2113t(w), and w0 = wt, we get:\n\u2113t(w) \u2265 \u2113t(wt) + \u2113\u2032t(wt)(w \u2212 wt) + 1\n2\n1\n8D (\u2113\u2032t(wt)(w \u2212 wt))2 := \u2113\u0303t(w), \u2200w \u2208 [\u2212D,D]\nwhich proved the lemma for that case.\nCase 3: xtwt \u2264 0. Assume, without loss of generality, that wt > 0 and xt < 0. For that case, the approximate loss \u2113\u0303t has the same form as in Eq. (34). It is easily noticed that \u2113\u0303t(wt) = \u2113t(wt). Notice that in [0, D] we have:\ne\u2212xtw = e|xt|w \u2265 1 2\nwhere we used xt < 0, and w \u2208 [0, D]. According to Eq. (35) it implies that e\u22120.5\u2113t(w) is concave in [0, D]; applying Theorem 15, we get:\n\u2113t(w) \u2265 \u2113t(wt) + \u2113\u2032t(wt)(w \u2212 wt) + 1\n2\n1\n8D (\u2113\u2032t(wt)(w \u2212 wt))2 := \u2113\u0303t(w), \u2200w \u2208 [0, D]\nand we used 1 8D = 1 2 min{ 1 4D , 1 2 }. Next we show that \u2113\u0303t(w) \u2264 \u2113\u0303(0)t (w), \u2200w \u2208 [\u2212D, 0], where \u2113\u0303 (0) t is defined in Eq. (29). Writing \u2113\u0303t(w) we get:\n\u2113\u0303t(w) = \u2113t(wt) + \u2113 \u2032 t(wt)(w \u2212 wt) +\n\u03b2 2 (\u2113\u2032t(wt)(w \u2212 wt))2\n= \u2113\u0303t(0) + ( extwt 1 + extwt (1\u2212 \u03b2xtwte xtwt 1 + extwt ) ) xtw + ( extwt 1 + extwt )2 \u03b2 2 x2tw 2\nwhere we used \u2113\u2032t(w) = xt extwt 1+extwt . Let\u2019s denote z = xtwt < 0, and note that for z \u2264 0, the following holds:\nez\n1 + ez \u2264 1 2 ,\nez\n1 + ez (1\u2212 ze\nz\n1 + ez ) \u2264 1, \u2200z \u2264 0\nUsing the latter expression for \u2113\u0303t, and the two inequalities above:\n\u2113\u0303t(w) \u2264 \u2113\u0303t(0) + 1\n2 xtw +\n\u03b2 2 x2tw 2 \u2264 \u2113t(0) + 1 2 xtw + \u03b2 2 x2tw 2 := \u2113\u0303 (0) t (w), \u2200w \u2208 [\u2212D, 0]\nwhere we used z = xtwt \u2264 0, and \u2113\u0303t(0) \u2264 \u2113t(0). Combining the latter inequality with Eq. (30), proves:\n\u2113\u0303t(w) \u2264 \u2113\u0303(0)t (w) \u2264 \u2113t(w), \u2200w \u2208 [\u2212D, 0]\nwhich concludes the proof."}, {"heading": "6 Summary and Open Questions", "text": "We have given tight bounds for stochastic and online logistic regression that preclude the existence of fast rates for logistic regression without exponential factors. As a consequence, we have also resolved the COLT 2012 open problem of McMahan and Streeter (2012). Our lower bounds can be extended to the multidimensional setting in which the instances are normalized and the labels are binary.\nOur results suggest that second-order methods might present poor performance in practical logistic regression problems. Indeed, in the derivation of our lower bounds we have constructed a distribution over instances such that the induced expected loss function is approximately linear around its optimum.\nAn interesting feature of our results is that our regret/convergence bounds apply to a finite range of T , and are different than the known asymptotic bounds. Arguably, the range of T for which our results apply is the important one in practice (sub-exponential in the size of the hypothesis class). Are there other natural settings in which regret bounds for bounded number of iterations differ from the asymptotic bound?"}, {"heading": "A Proof of Theorem 1", "text": "Suppose a randomize algorithm A that given m tosses decides upon one of the coins, and denote by DA the conditional distribution of the algorithm over his decision given the m coin tosses. We also let Dp, Dp+\u01eb denote the respective Bernoulli distributions corresponding to a single toss; let Dmp , Dmp+\u01eb be the product distributions of a sequence of m independent tosses, and let Dmp,A, Dmp+\u01eb,A be the joint distributions over the sequence of m independent tosses and the decision of the randomized algorithm. For the proof we need the following standard lemma.\nLemma 16. For all events B in the space of m independent tosses and the decision of the algorithm:\n\u2223\u2223Dmp,A(B)\u2212Dmp+\u01eb,A(B) \u2223\u2223 \u2264\n\u221a m\u01eb2\np .\nProof. We first bound the KL-divergence between Dp and Dp+\u01eb. Using the fact log z \u2264 z\u22121 for z > 0, we obtain\nKL(Dp+\u01eb || Dp) = (p+ \u01eb) log p + \u01eb p + (1\u2212 p\u2212 \u01eb) log 1\u2212 p\u2212 \u01eb 1\u2212 p\n\u2264 (p + \u01eb) ( p+ \u01eb\np \u2212 1\n) + (1\u2212 p\u2212 \u01eb) ( 1\u2212 p\u2212 \u01eb 1\u2212 p \u2212 1 )\n= \u01eb2\np(1\u2212 p) .\nSince the decision of the algorithm only depends on the m tosses that A observes, we may write:\nDmp,A = Dmp DA, Dmp+\u01eb,A = Dmp+\u01ebDA (36)\nThus, we can write:\nKL(Dmp+\u01eb,A || Dmp,A) = KL(Dmp+\u01eb || Dmp ) = mKL(Dp+\u01eb || Dp) \u2264 m\u01eb2\np(1\u2212 p)\nthe first equality follows from Eq. (36) combined with the definition of the KL-divergence, the second equality holds since the KL-divergence is additive over distribution products. Finally, recalling Pinsker\u2019s inequality we conclude that for all events B in the joint space of tosses and algorithm\u2019s decision:\n\u2223\u2223Dmp,A(B)\u2212Dmp+\u01eb,A(B) \u2223\u2223 \u2264 \u221a 1 2 KL(Dmp+\u01eb,A || Dmp,A) \u2264\n\u221a m\u01eb2 2p(1\u2212 p) \u2264 \u221a m\u01eb2 p .\nwhere in the last inequality we used p \u2208 (0, 1 2 ]. We can now prove Theorem 1.\nProof. Having an algorithm A that discovers the correct coin w.p\u2265 3/4, let B be the event that the algorithm decides that nature uses the first coin after m tosses, then clearly:\n\u2223\u2223Dmp,A(B)\u2212Dmp+\u01eb,A(B) \u2223\u2223 \u2265 1/4\ncombining the latter with Theorem 16 proves Theorem 1."}], "references": [{"title": "Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression", "author": ["F. Bach"], "venue": "arXiv preprint arXiv:1303.6149,", "citeRegEx": "Bach.,? \\Q2013\\E", "shortCiteRegEx": "Bach.", "year": 2013}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)", "author": ["F. Bach", "E. Moulines"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bach and Moulines.,? \\Q2013\\E", "shortCiteRegEx": "Bach and Moulines.", "year": 2013}, {"title": "Log loss or hinge loss? http://yaroslavvb.blogspot.co.il/2007/06/log-loss-or-hinge-l", "author": ["Y. Bulatov"], "venue": null, "citeRegEx": "Bulatov.,? \\Q2007\\E", "shortCiteRegEx": "Bulatov.", "year": 2007}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "Logistic regression, adaboost and bregman distances", "author": ["M. Collins", "R.E. Schapire", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Collins et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2002}, {"title": "Additive logistic regression: a statistical view of boosting", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "The annals of statistics,", "citeRegEx": "Friedman et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2000}, {"title": "A survey: The convex optimization approach to regret minimization", "author": ["E. Hazan"], "venue": "Optimization for Machine Learning,", "citeRegEx": "Hazan.,? \\Q2011\\E", "shortCiteRegEx": "Hazan.", "year": 2011}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "Moreover, Friedman et al. (2000) and Collins et al.", "startOffset": 10, "endOffset": 33}, {"referenceID": 3, "context": "(2000) and Collins et al. (2002) have shown that logistic regression is strongly connected to boosting.", "startOffset": 11, "endOffset": 33}, {"referenceID": 3, "context": "(2000) and Collins et al. (2002) have shown that logistic regression is strongly connected to boosting. A long standing debate in the machine learning community has been the optimal choice of surrogate loss function for binary prediction problems (see Langford (2009), Bulatov (2007)).", "startOffset": 11, "endOffset": 268}, {"referenceID": 2, "context": "A long standing debate in the machine learning community has been the optimal choice of surrogate loss function for binary prediction problems (see Langford (2009), Bulatov (2007)).", "startOffset": 165, "endOffset": 180}, {"referenceID": 7, "context": "Thus, the Online Newton Step algorithm (Hazan et al., 2007) can be applied to the logistic regression problem and gives a convergence rate of \u00d5(en/T ) over T iterations.", "startOffset": 39, "endOffset": 59}, {"referenceID": 0, "context": "Bach (2013), relying on a property called \u201cgeneralized self-concordance\u201d, gave an algorithm with convergence rate ofO(D4/\u03bc\u2217T ), where \u03bc\u2217 is the smallest eigenvalue of the Hessian at the optimal point.", "startOffset": 0, "endOffset": 12}, {"referenceID": 0, "context": "Bach (2013), relying on a property called \u201cgeneralized self-concordance\u201d, gave an algorithm with convergence rate ofO(D4/\u03bc\u2217T ), where \u03bc\u2217 is the smallest eigenvalue of the Hessian at the optimal point. This translates to a O(poly(D)/T ) rate whenever the expected loss function is \u201clocally strongly convex\u201d at the optimum. More recently, Bach and Moulines (2013) extended this result and presented an elegant algorithm that attains a rate of the form O(\u03c1Dn/T ), without assuming strong convexity (neither global or local) \u2014 but rather depending on a certain data-dependent constant \u03c1.", "startOffset": 0, "endOffset": 362}, {"referenceID": 0, "context": "Bach (2013), relying on a property called \u201cgeneralized self-concordance\u201d, gave an algorithm with convergence rate ofO(D4/\u03bc\u2217T ), where \u03bc\u2217 is the smallest eigenvalue of the Hessian at the optimal point. This translates to a O(poly(D)/T ) rate whenever the expected loss function is \u201clocally strongly convex\u201d at the optimum. More recently, Bach and Moulines (2013) extended this result and presented an elegant algorithm that attains a rate of the form O(\u03c1Dn/T ), without assuming strong convexity (neither global or local) \u2014 but rather depending on a certain data-dependent constant \u03c1. In this paper, we resolve the above question and give tight characterization of the achievable convergence rates for logistic regression. We show that as long as the target accuracy \u01eb is not exponentially small in D, a rate of the form \u00d5(poly(D)/T ) is not attainable. Specifically, we prove a lower bound of \u03a9( \u221a D/T ) on the convergence rate, that can also be achieved (up to a \u221a D factor) by stochastic gradient descent algorithms. In particular, this shows that in the worst case, the magnitude of data-dependent parameters used in previous works are exponentially large in the diameter D. The latter lower bound only applies for multidimensional regression (i.e., when n \u2265 2); surprisingly, in one-dimensional logistic regression we find a rate of \u0398(T\u22122/3) to be tight. As far as we know, this is the first natural setting demonstrating such a phase transition in the optimal convergence rates, with respect to the dimensionality of the problem. We also consider the closely-related online optimization setting, where on each round t = 1, 2, . . . , T an adversary chooses a certain logistic function and our goal is to minimize the T -round regret, with respect to the best fixed decision chosen with the benefit of hindsight. In this setting, McMahan and Streeter (2012) investigated the one-dimensional case and showed that if the adversary is restricted to pick binary (i.", "startOffset": 0, "endOffset": 1862}, {"referenceID": 0, "context": "Bach (2013), relying on a property called \u201cgeneralized self-concordance\u201d, gave an algorithm with convergence rate ofO(D4/\u03bc\u2217T ), where \u03bc\u2217 is the smallest eigenvalue of the Hessian at the optimal point. This translates to a O(poly(D)/T ) rate whenever the expected loss function is \u201clocally strongly convex\u201d at the optimum. More recently, Bach and Moulines (2013) extended this result and presented an elegant algorithm that attains a rate of the form O(\u03c1Dn/T ), without assuming strong convexity (neither global or local) \u2014 but rather depending on a certain data-dependent constant \u03c1. In this paper, we resolve the above question and give tight characterization of the achievable convergence rates for logistic regression. We show that as long as the target accuracy \u01eb is not exponentially small in D, a rate of the form \u00d5(poly(D)/T ) is not attainable. Specifically, we prove a lower bound of \u03a9( \u221a D/T ) on the convergence rate, that can also be achieved (up to a \u221a D factor) by stochastic gradient descent algorithms. In particular, this shows that in the worst case, the magnitude of data-dependent parameters used in previous works are exponentially large in the diameter D. The latter lower bound only applies for multidimensional regression (i.e., when n \u2265 2); surprisingly, in one-dimensional logistic regression we find a rate of \u0398(T\u22122/3) to be tight. As far as we know, this is the first natural setting demonstrating such a phase transition in the optimal convergence rates, with respect to the dimensionality of the problem. We also consider the closely-related online optimization setting, where on each round t = 1, 2, . . . , T an adversary chooses a certain logistic function and our goal is to minimize the T -round regret, with respect to the best fixed decision chosen with the benefit of hindsight. In this setting, McMahan and Streeter (2012) investigated the one-dimensional case and showed that if the adversary is restricted to pick binary (i.e. \u00b11) labels, a simple followthe-leader algorithm attains a regret bound of O( \u221a D + log T ). This discovery led them to conjecture that bounds of the form O(poly(D) log T ) should be achievable in the general multi-dimensional case with continuous labels set. Our results extend to the online optimization setup and resolve the COLT 2012 open problem of McMahan and Streeter (2012) on the negative side.", "startOffset": 0, "endOffset": 2349}, {"referenceID": 7, "context": "As opposed to previous works that utilize approximate losses based on local structure (Zinkevich, 2003; Hazan et al., 2007), we find it necessary to employ approximations that rely on the global structure of the logistic loss.", "startOffset": 86, "endOffset": 123}, {"referenceID": 3, "context": "Standard online-to-batch conversion (Cesa-Bianchi et al., 2004) shows that any online algorithm attaining a regret of R(T ) can be used to attain a convergence rate of R(T )/T for stochastic optimization.", "startOffset": 36, "endOffset": 63}, {"referenceID": 3, "context": "Using standard online-to-batch conversion techniques Cesa-Bianchi et al. (2004), we can translate the upper bound given in the above lemma to an upper bound for stochastic optimization.", "startOffset": 53, "endOffset": 80}, {"referenceID": 6, "context": "Following Zinkevich (2003) and Hazan et al. (2007), we approximate the losses received by the adversary, and use the approximate losses in a follow-the-regularized-leader (FTRL) procedure in order to choose the predictors.", "startOffset": 31, "endOffset": 51}, {"referenceID": 6, "context": "First note the following lemma due to Zinkevich (2003) (proof is found in Hazan et al. (2007)): Lemma 11.", "startOffset": 74, "endOffset": 94}, {"referenceID": 6, "context": "The expression for ut in Algorithm 2 is useful since it enables us to calculate the differences |ut\u22121\u2212 ut|, which upper bound the differences between predictors: |wt\u22121 \u2212wt|; these differences are useful in bounding the regret of FTRL as seen in the next lemma due to Kalai and Vempala (2005) (proof can be found in Hazan (2011) or in Shalev-Shwartz (2011)):", "startOffset": 315, "endOffset": 328}, {"referenceID": 6, "context": "The expression for ut in Algorithm 2 is useful since it enables us to calculate the differences |ut\u22121\u2212 ut|, which upper bound the differences between predictors: |wt\u22121 \u2212wt|; these differences are useful in bounding the regret of FTRL as seen in the next lemma due to Kalai and Vempala (2005) (proof can be found in Hazan (2011) or in Shalev-Shwartz (2011)):", "startOffset": 315, "endOffset": 356}, {"referenceID": 6, "context": "where we used \u03b2 = 1/8D together with the following lemma, taken from Hazan et al. (2007): Lemma 14.", "startOffset": 69, "endOffset": 89}, {"referenceID": 6, "context": "To prove l\u0303t(w) \u2264 lt(w), we require the following lemma from Hazan et al. (2007): Lemma 15.", "startOffset": 61, "endOffset": 81}, {"referenceID": 6, "context": "To prove l\u0303t(w) \u2264 lt(w), we require the following lemma from Hazan et al. (2007): Lemma 15. For a function f : K \u2192 R, where K has diameter D, such that \u2200w \u2208 K, ||\u2207f(w)|| \u2264 G, and e\u2212\u03b1f(w) is concave, the following holds for \u03b3 = 1 2 min{ 1 4GD , \u03b1}: f(w) \u2265 f(w0) +\u2207f(w0) (w \u2212 w0) + \u03b3 2 (\u2207f(w0) (w \u2212 w0)), \u2200w,w0 \u2208 K In Hazan et al. (2007) it is also shown that for one dimensional functions, if \u03b1 \u2264 minw\u2208K f \u2032\u2032(w) ( f \u2032(w) )2 , then e\u2212\u03b1f(w) is concave in K.", "startOffset": 61, "endOffset": 336}], "year": 2014, "abstractText": "The logistic loss function is often advocated in machine learning and statistics as a smooth and strictly convex surrogate for the 0-1 loss. In this paper we investigate the question of whether these smoothness and convexity properties make the logistic loss preferable to other widely considered options such as the hinge loss. We show that in contrast to known asymptotic bounds, as long as the number of prediction/optimization iterations is sub exponential, the logistic loss provides no improvement over a generic non-smooth loss function such as the hinge loss. In particular we show that the convergence rate of stochastic logistic optimization is bounded from below by a polynomial in the diameter of the decision set and the number of prediction iterations, and provide a matching tight upper bound. This resolves the COLT open problem of McMahan and Streeter (2012).", "creator": "LaTeX with hyperref package"}}}