{"id": "1405.5474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2014", "title": "New Perspectives in Sinographic Language Processing Through the Use of Character Structure", "abstract": "implicit grammar have a dense implicit hierarchical graphical structure containing both semantic encoding phonetic information. we use fuzzy structure to enhance the learning properties and obtain computation results in standard nlp operations. first as all, to consider small problem of denying message matrices define sparse classes of characters. next, restricting matrix dependent inclusion of a string in a grammar, provides algorithms with fairly practical derivation of allographic weights. we provide this graph with special weights : width ( the relation between subcharacter and character ) and phoneticity ( composition relation ) we calculate \" equally identifiable subcharacter paths \" for each character. meanwhile, adding multimedia information contained onto these paths to html we find as increase the fewer effective text copying machines. scientists evaluate our thesis on combining sample classification module on multimedia corpora ( polish and japanese ) including a basis of 18 million colors and detect an improvement every 3 % around an already high baseline of 89. 6 % precision, obtained by a linear svm sequence. other simple applications and assumptions enhance the system repeatedly explored.", "histories": [["v1", "Wed, 21 May 2014 16:49:50 GMT  (2054kb)", "http://arxiv.org/abs/1405.5474v1", "17 pages, 5 figures, presented at CICLing 2013"]], "COMMENTS": "17 pages, 5 figures, presented at CICLing 2013", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yannis haralambous"], "accepted": false, "id": "1405.5474"}, "pdf": {"name": "1405.5474.pdf", "metadata": {"source": "CRF", "title": "New Perspectives in Sinographic Language Processing Through the Use of Character Structure", "authors": ["Yannis Haralambous"], "emails": ["yannis.haralambous@telecom-bretagne.eu"], "sections": [{"heading": "1 Introduction", "text": "The Chinese script is used mainly in the Chinese and Japanese languages. Chinese characters (or \u201csinographs\u201d) are notorious for their large number (over 84 thousand have been encoded in Unicode [1]) and their complexity (they can have from 1 stroke, like \u4e00, to as many as 64 strokes, like ;). Despite its complexity, the Chinese script is quite efficient, since semantic and phonetic information is stored in stroke patterns, easily recognizable by native readers. In this paper we will deal with a specific kind of stroke pattern, namely those that exist also as stand-alone characters\u2014we call them subcharacters. We will study the phonetic similarity (called phoneticity) and the semantic relatedness (called semanticity) between a character and its subcharacters. After having built a graph of subcharacter inclusions, and attaching various kinds of information to it, we introduce an enhanced NLP task feature model: together with individual characters we use data contained in their subcharacter graphs. Indeed, in sinographic ? The final publication is available at http://link.springer.com.\nlanguage processing it is customary to combine character-level and word-level processing. The advantage to our approach is that an additional level is added to these two: the level of subcharacters. By exploring this additional level, we go deeper into the inherent structure of sinographic characters\u2014this inherent structure is completely lost in conventional NLP approaches.\nWe believe that this new feature model will prove useful in various branches of statistical NLP. As a first step in that direction, we evaluate our tools by applying them to a text classification task."}, {"heading": "1.1 Related Work", "text": "There have been various attempts at describing sinographs in a systematic way: [2] and [3] use generative grammars, [4] uses a Prolog-like language, [5] uses the biological metaphor of gene theory, [6] describes sinographs as objects with features (the Chaon model), [7] defines a formal language type called planar regular language, [8] and [9] describe the topology of sinographs in XML, [10] uses projections of stroke bounding boxes, and [11] use combinatorics of IDS operators.1\nThe OCR community has also shown a strong interest in the structure of sinographs [12].\nAs for considering Chinese script as a network, this has been done in several papers, such as [13, 14] (where edges represent components combined in the same sinograph) or [15], where components and sinographs form a bipartite graph; [16], where edges represent sinographs combined into words; [17], dealing with purely phonemic networks. [14] give an example of a small phono-semantic graph (a bipartite graph where edges connect semantic and phonetic components), but do not enter into the calculations of phoneticity and semanticity. Hanzi Grid [18, 19] maps component inclusions to relations in ontologies.\nFinally the cognitive psychology community is also heavily interested in the (cognitive) processing of phonetic and semantic components [20, 21], and even in the effects of stroke order and radicals on linguistic knowledge [22]."}, {"heading": "2 Definitions", "text": ""}, {"heading": "2.1 Strokes", "text": "A sinograph consists of a number of strokes, arranged inside an (imaginary) square according to specific patterns. Strokes can be classified as belonging to 36 calligraphic stroke classes. The latter have been encoded in Unicode (table cjk strokes). Furthermore, strokes are always drawn in a very specific order.\n1 Ideographic Description Sequences do not describe sinographs per se, but provide operators \u2018 \u2019 \u00f7 \u25ca \u00ff \u0178 \u2044 \u20ac \u2039 \u203a fi fl for graphically combining existing sinographs in groups of two or three. IDS operators can be arbitrarily nested and have been encoded in Unicode (table ideographic description characters)."}, {"heading": "2.2 Components, Subcharacters, Radicals", "text": "In a manner similar to etymological roots in Western languages, readers of sinographs recognize patterns of strokes, so that the meaning of an unknown sinograph can be identified, more or less effectively, by the stroke patterns it contains.\nFrequently appearing patterns of strokes are called components. In this paper we will deal only with components that also exist as isolated sinographs. Using the term \u201ccharacter\u201d in the sense of \u201cUnicode character,\u201d we will call such components, subcharacters. In other words, subcharacters are components having a Unicode identity.\nClassification of sinographs in dictionaries traditionally uses a set of several hundred subcharacters called radicals. We do not discuss radicals in this paper."}, {"heading": "2.3 Allographic Classes", "text": "An important property of components is that they can change shape when combined with other components (for example, \u706b becomes \u706c when combined, like in \u70b9). Some of these variant shapes have been encoded in Unicode. Also, some sinographs can have variant forms. In particular, during the Chinese writing reform [23], about 1,753 sinographs obtained simplified shapes (for example, \u8514 became \u535c), which are encoded separately in Unicode.\nAs these variations in shape do not affect semantic or phonetic properties (at least not at the level of statistical language processing), we merge characters and their variants into sets called allographic classes. For example, \u7cf8 belongs to class [\u2eaf,\u2eb0,\u7cf9,\u7cf8,\u7e9f] where the two first characters belong to the table of Unicode radicals and the others are graphical variants.\nWe obtain 18,686 allographic classes, out of which 87.356% are singletons, the highest number of characters per class is 15 and the average is 1.1382.\nIn the remainder of this paper we will use italics for characters (c, s, . . .) and bold letters for allographic classes (c, s, . . .). The term \u201csubcharacter\u201d will mean a character or an allographic class, depending on the context."}, {"heading": "2.4 Semanticity and Phoneticity", "text": "Subcharacters can play a semantic role (when one of their meanings is close to one of the meanings of the sinograph) and/or a phonetic role (when one of their readings is identical or close, in a given language, to one of the readings of the sinograph). In this paper we will deal with Mandarin and Japanese.\nThese two properties of subcharacters in relation to characters can be quantified and are then called semanticity and phoneticity [24]."}, {"heading": "3 Resources", "text": ""}, {"heading": "3.1 Frequency Lists", "text": "First some notation: let T be a sinographic text (or a corpus considered as a single text). A frequency list A, generated out of T , is an M -tuple of pairs\n(ci, fA(ci)), where the frequency fA(ci) of character ci is defined as #ci#T , that is the number of occurrences of ci in T divided by the length of T . The ci must be pairwise different. A is sorted in decreasing order of frequencies: fA(ci) \u2265 fA(cj) when i < j. If N \u2208 N, let A1...N be the subtuple of the first N pairs.\nLet char(A) be the underlying set of characters of A. Let A1...N be the sublist of the N first characters of A. Let\ncomcharN (A,A \u2032) :=\n(char(A1...N ) \u2229 char(A\u2032)) \u222a (char(A\u20321...N ) \u2229 char(A))\nbe the set of N -common characters between two lists A and A\u2032. In other words, among the first N characters of A we take those that also belong to A\u2032 and vice versa. We define the N -common coverage factor between A and A\u2032 as:\ncomcovN (A,A \u2032) :=\n#comchar(A,A\u2032)\nN .\nUsing comcharN (A,A\u2032) as the underlying character set, we obtain sub-lists Ac of A and A\u2032c of A\u2032 (although not noted, Ac depends not only on A but also on A\u2032 and on N , and A\u2032c also on A and N).\nFinally, we define a distance of character frequency lists dN as follows:\ndN (A,A \u2032) := 1\u2212 comcovN (A,A\u2032) \u00b7\n\u03c1(Ac, A \u2032 c) + 1\n2\nwhere \u03c1 is the Spearman ranking correlation coefficient (\u03c1 takes values in [\u22121, 1]). We have used three publicly available frequency lists: uni, the language-\nindependent Unihan [25]; sin, the traditional Chinese Sinica compiled by Academia Sinica Taipei; and noz, a Japanese list, taken from [26].\nWe have also compiled our own frequency lists out of five corpora: Chinese Wikipedia (wps for simplified Chinese and wpt for traditional Chinese), Japanese Wikipedia wpj, Chinese Project Gutenberg gut, Chinese GoogleBooks goo, Leeds Chinese Internet Corpus cic [27].\nAs we needed a frequency list suitable for all sinographic languages, we calculated distance dN between them (for N = 3, 000). In Fig. 1 the reader can see a graph of frequency lists with edges proportional to values of dN . One can identify three linguistic clusters, while the Unihan list can be considered as an outlier. After removing the Unihan list we aggregated the remaining lists to form a \u201cUniversal Frequency List,\u201d ufl, as the normalized average of noz, wpj, wps,\nwpt, she, goo, sin and gut, defined as follows: if fX(c) is the frequency of character c in corpus X, and #X is the size of the corpus in characters, then:\nchar(ufl) = \u22c3\nX\u2208X char(X), fufl(c) = \u2211 X\u2208X fX(c) \u00b7#char(X) #char(ufl) ,\nwhere X = {noz, wpj, wps, wpt, she, goo, sin, gut}."}, {"heading": "3.2 Character Descriptions and Subcharacter Inclusions", "text": "Wenlin Institute kindly provided us with the CDL database of sinographs. This XML file provides an ordered stroke list for each sinograph, and for each stroke, its calligraphic type and coordinates of endpoints. We modified stroke order for the few exceptional cases where components are overlapping.2\nBecause of the many affine transformations a component is subject to, this topological sinograph description is not suitable for effectively detecting subcharacters. On the other hand, use of topological properties is unavoidable, since some sinographs have the same strokes in the same order and combinatorial arrangement but differ by the (relative) size of strokes, the typical example being \u58eb (scholar) and \u571f (earth), where the bottom stroke of the latter is longer than that of the former. For this reason we used a different representation of sinographs, based on relative size of strokes and extrapolated intersection locations. For example, here is the relation between the two first strokes of the sinograph \u8a00, which are of type d (= \u201cdot\u201d) and h (= \u201chorizontal\u201d):\nd (1.4,9.1,0.0,0.6) h\n(see Fig. 2 for the description of the numeric values). Using this affine transformation invariant representation of sinographs, we\nextracted 868,856 subcharacter inclusions from the Wenlin CDL database, where inclusion s\u2192 c of subcharacter s into character cmeans that the code lines of our representation of s are contained, in their identical form, in the representation of c. After merging with data from the CHISE project [28] and with data kindly provided by Cornelia Schindelin [29], and after having removed identities, we got a list of 824,120 strict inclusions. The number is quite high because inclusion chains like:\u4e3f\u2192\u52f9\u2192\u5310\u2192\u8514 provide automatically all triangulations\u4e3f\u2192\u5310, \u52f9\u2192\u8514, etc., and there are exponentially many of them. We have detriangulated by taking systematically the longest path, and hence reduced the number of inclusions to 185,801. 2 An important fact about components is that in the sequence of strokes drawn in traditional stroke order, components form subsequences without overlapping : the first stroke of a component is drawn after the last stroke of the preceding component. There are a few exceptions to this rule: sinographs like\u56e2 (subcharacter\u56d7 containing \u624d) where the drawing of the lower horizontal stroke of \u56d7 is postponed until the drawing of the internal subcharacter \u624d has been completed.\nlated intersection locations. The blue lines are skeletons of strokes given by CDL. The values (1.4,9.1,0.0,0.6) correspond to: d((x0,y0)\u2212(x1,y1))\nd((x2,y2)\u2212(x1,y1)) (that is: distance of the in-\ntersection point from (x1, y1) with stroke length as unit), |x4\u2212x3||x2\u2212x1| (stroke box width ratio), |y4\u2212y3||y2\u2212y1| (stroke box height ratio) and d((x0,y0)\u2212(x3,y3)) d((x4,y4)\u2212(x3,y3))\n(again, distance of the intersection point from (x3, y3) with second stroke length as unit), where d is Euclidean distance. When strokes are parallel or orthogonal, some of the parameters are infinite and we write E instead."}, {"heading": "3.3 The Inclusion Graph", "text": "We construct a graph, using all sinographs as vertices and representing the inclusions as edges, giving us 74,601 vertices and 185,801 edges. Both the in-degree and out-degree properties of this graph (see Fig. 3) follow a power law distribution of parameters \u03b1\u2212 = 1.138 (in-degree) and \u03b1+ = 1.166 (out-degree). Remarkably, these are a bit low compared to typical scale-free networks like the Web or proteins, which are in the 2\u20133 range [30].\nAs higher Unicode planes contain rare sinographs which are of little use to common NLP tasks, and for reasons of computational efficiency, we restricted ourselves to the subgraph of sinographs contained in Unicode\u2019s BMP (Basic Multilingual Plane). This subset covers only 91.65% of the sinographs contained in the frequency list, but its frequency-weighted coverage3 is as high as 99.9995%, showing that our choice of BMP is justified.\nBy lifting4 sinograph inclusions to allographic classes we obtain a graph C of 18,686 allographic classes and 39,719 class inclusions.\nIn all, 99.8% of the allographic classes have incoming inclusions (the highest in-degree is 12 for class [\u8550]), 14 classes are \u201csources\u201d (zero in-degree nodes): [\u4e00], [\u4e28], [\u4e36], [\u4e3f], [\u4e85], [\u529b], [\u5ddb], [\u4e40,\u4e41,\u4e5b,\u4e5a,\u4e59], [\u4e43], [\u5201], [\u53c8], [\u53ca], [\u5ddc] and [\u5ef4]. 87.05% of the classes are leaves (zero out-degree nodes) and the highest out-degree is 996 for class [\u6c35,\u6c34]).\n3 If standard coverage is #{ci\u2208char(ufl)} #{ci\u2208BMP} where ufl is our Universal Frequency List,\nthen frequency-weighted coverage is defined as \u2211 ci\u2208char(ufl) fufl(ci)\u2211\nci\u2208BMP fufl(ci) . 4 If c1, c2 \u2208 C we have c1 \u2192 c2 iff there is at least one character pair c1 \u2208 c1, c2 \u2208 c2 such that c1 \u2192 c2.\nLogarithmic in-degree distribution\nLogarithmic out-degree distribution\ngraphic classes."}, {"heading": "4 Phoneticity", "text": "Unihan provides phonetic data for sinographs in several languages. For this study, we used data for Mandarin Chinese, Japanese On (readings originating from China, and imported to Japan together with the writing system) and Japanese Kun (native Japanese readings). We define phoneticity as the degree of phonetic similarity between subcharacter and character. To calculate it we need a phonetic distance.\nFor Mandarin Chinese we implemented the phonetic distance described in [31].\nFor Japanese we have defined our own phonetic distance.We obtained this distance by applying a methodology given by [32] and [33]: indeed, we defined a distance for syllables, using seven features: consonant place of articulation (weight 4), consonant voicing (1), consonant manner of articulation (4), consonant palatalization (1), vowel frontness (5), vowel height (1), and vowel rounding (1). Distance between syllables is Euclidean in feature space. When sinograph readings have an unequal number of syllables5 we use a sliding window approach to find the shortest phonetic distance between the shorter word and a subword of equal length to the longer one.\nIf d is a phonetic distance between sinographs, let dmin be the distance between allographic classes defined as the minimum distance between class members. We define the phoneticity coefficient \u03d5(c1, c2) := 1\u2212dmin(c1,c2) N where N is a normalization constant such that Im(\u03d5) = [0, 1]. We will write \u03d5(s \u2192 c)\n5 Contrary to Chinese language where sinograph readings are monosyllabic, in Japanese they can have up to 12 syllables (the longest readings are nuhitorioshitanameshigaha for \u97bc and hitohenotsutsusodeudenuki for \u8920, both being Kun readings).\nfor \u03d5(s, c) when s \u2192 c is a class inclusion. In Fig. 4, we show the distribution density of \u03d5 for Mandarin, Japanese On and Kun. One can see that On mimics the distribution of Mandarin (with a lesser \u03d5 value for the right peak) while Kun has a completely different distribution with a single peak around \u03d5 = 0.4. Indeed, the historical relation of On and Mandarin is reflected in the similarity of distributions, while in Kun phonetic distance between subcharacter and character is random.\nHere is an example:\nMandarin Jap. On Jap. Kun\n\u4efb r\u00e8n nin makaseru, ninau, taeru\n\u4eba\nOO\nr\u00e9n \u2248 OO\nnin =\nOO\nhito 6= OO\nwhere the difference between the phonetics of the two characters in Mandarin is at the tone level only, and since tones are not phonemic in Japanese, the sinographs are homophones in On. Incidentally, this inclusion has very low semanticity: \u4eba means \u201cman\u201d and \u4efb, \u201cto trust.\u201d\nMandarin\nJapanese On\nJapanese Kun"}, {"heading": "4.1 The Least Phonetic Chain", "text": "Under the hypothesis that subcharacters with higher phoneticity have statistically lower semanticity and vice versa, we consider the subcharacters with the lowest phoneticity, as these have an increased potential of having higher semanticity. We define the least phonetic chain (pi)i\u22650 of a class c as follows: let p0 = c and given pi let pi+1 := argminz\u03d5(z\u2192 pi), that is, the subcharacter of pi with least phoneticity (see Fig. 5 for an example). We will use this construct in our text classification strategies.\nEdge labels are \u03d5 values; between parentheses, the subcharacter\u2019s multiplicity."}, {"heading": "5 Semanticity", "text": "As semantic resources for sinographic languages we used three WordNets: the Academia Sinica BOW [34] for traditional Chinese, the Chinese WordNet [35] for simplified Chinese and the Japanese WordNet [36]. All three provide English WordNet synset IDs. The single-sinograph entries they contain are rather limited: 3,075 for traditional Chinese, 2,440 for simplified Chinese and 4,941 for Japanese. From these we obtain a first mapping of allographic classes into synset IDs. In all, 2,852 allographic classes are covered, out of which 1,063 have a single synset ID, 581 have two IDs (IDs from different WordNets are not merged), etc. The highest number of IDs is 45, for [\u523a] (= \u201cthorn\u201d). Only 154 classes share the same synset ID in all three WordNets.\nThe Unihan database also provides meanings for a large number of sinographs, but these meanings are not mapped to WordNet. We used the following method to attach synset IDs to them: let wi be a Chinese or Japanese word attached to synset ID \u03c3i in one of the three WordNets; let ei,k be one of the terms of the English WordNet synset with ID \u03c3i; if ei,k can be found in Unihan as meaning of a character cj and if cj \u2208 wi then we attach synset ID \u03c3i to cj . We lift this information to allographic classes.\nHere is an example:\nWordNet 1.6 WordNet 3.0 Unihan\n\u98a8\u75b9 BOW // 110160032\n\u2208\n111412304\n\u2208\n\u75b9\n\u2208\nkDefinition \u75b9\n\u2208\n33\nmeasles#1\n\u2208\nmeasles\nSince \u75b9 has the meaning \u201cmeasles\u201d in Unihan and is contained in the word \u98a8\u75b9 which belongs to the measles synset, we attach the measles synset ID to \u75b9.\nThanks to this method, 1,392 additional allographic classes were mapped to at least one WordNet synset ID, raising the number of semantically annotated classes to 4,244."}, {"heading": "5.1 Extracting Semantic Relations", "text": "We attempted three methods of estimating semanticity of subcharacters: 1. By measuring distance between WordNet nodes using semantic similarity\nmeasures, such as those of [37\u201339] or simply the inverse of the shortest path length in the WordNet graph. This method was unfruitful.\n2. By using the following algorithm: whenever there was a semantic relation (hyponymy, meronymy, antonymy, etc.) between twoWordNet synsets \u03c31 and \u03c32, and we have a sinograph belonging to a word from \u03c32 and one of its subcharacters in a word from \u03c31, we added a unit of \u201csemantic weight\u201d to the inclusion. We counted these over the three WordNets and obtained 6,816 allographic class inclusions of various weights.\nHere is an example:\n114662574 (mineral#1)\n\u7926\u7269oo \u79263\n114696793 (Greenland spar#1)\nhyponym\nOO\n\u51b0\u6676\u77f3oo \u77f33\nOO\n\u201cGreenland spar\u201d is an hyponym of \u201cmineral,\u201d \u51b0\u6676\u77f3 belongs to the synset of the former and \u7926\u7269 to that of the latter. Characters \u77f3 and \u7926 appear in these two words, and the former being a subcharacter of the latter, we attach a unit of \u201csemantic weight\u201d to it. The (log of the) total amount of units is our tentative semanticity measure.\n3. To a lesser degree, we used the Ka\u0304ng X\u0131\u0304 radical (see next section) as an additional semanticity indicator. We did the following: for every inclusion, we compared the Ka\u0304ng X\u0131\u0304 radicals on both sides: when equal, then this may be a hint that the given subcharacter has higher semanticity than others.\nExample: in the inclusion\u6bcf\u2192\u6bd3, both sinographs have the same Ka\u0304ng X\u0131\u0304 radical \u6bcc, so we assume that\u6bcf\u2192\u6bd3 has higher semanticity than\u342c\u2192\u6bd3 (which, incidentally, also has higher phoneticity since it is pronounced li\u00fa which is closer to y\u00f9 (\u6bd3) than m\u011bi (\u6bcf))."}, {"heading": "6 Evaluation", "text": "To evaluate an application of our graph to a common NLP task we attempted text classification on two corpora:\n1. The Sogou Corpus of Chinese news [43], a collection of 2.36 million online articles (611 million sinographs) taken from various sources. We removed all sinographs not in the cjk unified ideographs Unicode table and extracted 5,000 texts per class:\nCategory Avg length Min length Max length Sports 728.23 500 1,000 Finance 714.51 500 1,000 News 699.63 500 1,000 Entertainment 696.36 500 1,000 Global 709.68 500 1,000\n2. A corpus we built from the online archives of Japanese Reuters [44], covering the period from 2007 to today. We removed all sinographs not in the cjk characters Unicode table (including the kana) and extracted 5,000 texts per class. Because of the nature of Reuters news, these texts are shorter than the Chinese ones:\nCategory Avg length Min length Max length Sports 105.54 74 521 Finance 293.79 188 1,262 News 315.69 166 876 Entertainment 94.73 44 588 Global 202.44 44 1,262\nOur baseline is obtained as follows: we take unigrams for all sinographs appearing at least 10 times in the corpus. This results in 4,275 unigrams for the Chinese and 2,010 for the Japanese corpus. Then we apply a linear SVM with 10-fold cross validation. Here are the results obtained:\nAccuracy # of SVs Baseline Chinese 89.605% 4,933 Baseline Japanese 86.925% 4,237"}, {"heading": "6.1 Strategy 1: The Most Semantic Chain", "text": "Let \u03b9 : s\u2192 c be a class inclusion. We calculate three quantities:\n1. f1(\u03b9): the frequency of character pairs (s \u2208 s, c \u2208 c) contained in words s \u2208 w1, c \u2208 w2 belonging to synsets w1 \u2208 \u03a31, w2 \u2208 \u03a32 such that there is a semantic relation \u03a31 \u2192 \u03a32 in WordNet (see Section 5);\n2. f2(\u03b9): the frequency of character pairs (s \u2208 s, c \u2208 c) contained in words s \u2208 w1, c \u2208 w2 belonging to synsets w1 \u2208 \u03a31, w2 \u2208 \u03a32 such that there is a two-step semantic relation \u03a31 \u2192 \u03a3\u2032 \u2192 \u03a32 in WordNet;\n3. r(\u03b9): let r(s, c) be 1 when s and c share the same Ka\u0304ng X\u0131\u0304 radical, and 0 otherwise. r(\u03b9) will be 1nm \u2211 s\u2208s \u2211 c\u2208c r(s, c).\nThe semanticity S(\u03b9) of inclusion \u03b9 : s\u2192 c will be:\nS(\u03b9) = 12 log(1 + f1(\u03b9)) + 1 4 log(1 + f2(\u03b9)) + 1 4r(\u03b9),\nnormalized so that its values stay in the interval [0, 1]. Here, coefficients 12 , 1 4 and 1 4 have been obtained heuristically by a grid method applied to a series of tests.\nThe most semantic chain (si)i\u22650 of class c is calculated as follows: s0 = c, and, given si, si+1 := argmaxz S(z \u2192 si), that is, the subcharacter of si with maximal semanticity.\nLet w(c) be the weight of unigram c, obtained by frequency in the baseline case. Let w(c) := maxc\u2208c w(c) be the weight of class c. For every member si of the most semantic chain of c, we added a weight w(si) = 1iS(si\u22121 \u2192 si) (with s0 = c) to unigram c.\nWe obtained the following results:\nAccuracy # of SVs Un.w/ mod. Un.added Chinese 92.62% 3,287 594 20 Japanese 89.99% 3,728 524 152\nwhere the two last columns contain the number of updated unigrams and the number of new unigrams. Notice that the increase in performance is similar for Chinese and Japanese, while the number of support vectors needed is higher in Japanese, probably due to the shorter length of texts making the classification task harder."}, {"heading": "6.2 Strategy 2: Combining Most Semantic Chain and Least Phonetic Chain", "text": "With the notation of previous sections, let \u03d5(s \u2192 c) be the phoneticity of inclusion s \u2192 c (calculated as explained in Section 4). Recall that the least phonetic chain (pi)i\u22650 is obtained by taking p0 = c, and, given pi, pi+1 := argminz \u03d5(z,pi). For each pi we define a new class weight w\u2032(pi) as follows:\nw\u2032(pi) = w(pi) + 1 i\u03d5(pi\u22121 \u2192 pi).\nWe obtained the following results:\nAccuracy # of SVs Un.w/ mod. Un.added Chinese 92.435% 3,299 851 245 Japanese 90.125% 3,737 745 453\nThe value for Chinese is a bit lower than in Strategy 1, but we get a better result for Japanese. We have an increase in the number of new unigrams, due to the fact that we have significantly more phonetically annotated inclusions than semantically annotated ones."}, {"heading": "7 Unknown Character Semantic Approximation", "text": "Besides the usual NLP tasks where a semantic relatedness distance is needed, our system can also be applied to the processing of unknown sinographs. Let u be an\nunknown sinograph (i.e., we know neither reading nor meaning), and u its class. In our graph, there is subgraph Gu generated by paths leading to u. The way the graph is built, our subgraph has no cycles. When we leave u and head towards the leaves, for every path si there will be a first node ni for which we have semantic annotation (for example, a WordNet synset ID). Furthermore, we can attach weights to the ni by using the product of semanticity expectations (calculated in a way similar to phoneticity expectations) of the edges of path uni and the distance from u on the path uni.\nWe obtain a vector in the space of WordNet synset IDs. This is not a precise semantic, but it can be used in various statistical NLP tasks.6"}, {"heading": "8 Perspectives", "text": "Further work will involve three main areas of focus:\n1. further analyzing the graph and extracting knowledge about sinographic languages;\n2. similarly processing various higher-order graphs (n-grams, words, concepts, . . . ) and studying their interactions;\n3. last, but not least, in the frame of the HanziGraph Project, manually validating the semanticity of the 39,719 class inclusions of our graph by a team of native Chinese/Japanese speakers, using the dedicated Web site http://www.hanzigraph.net."}], "references": [{"title": "The Unicode Standard, Version 6.0", "author": ["J.D. Allen", "eds"], "venue": "Unicode Consortium", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Structural patterns of Chinese characters", "author": ["O. Fujimura", "R. Kagaya"], "venue": "Proceedings of the International Conference on Computational Linguistics, S\u00e5nga-S\u00e4by, Sweden.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1969}, {"title": "Toward a generative grammar of Chinese character structure and stroke order", "author": ["J.C.S. Wang"], "venue": "PhD thesis, University of Wisconsin-Madison", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1983}, {"title": "Coordinate-independent font description using Kanji as an example", "author": ["M.J. D\u00fcrst"], "venue": "Electronic Publishing 6(3)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "\u6f22\u5b57\u57fa\u56e0\u6731\u90a6\u5fa9\u6f22\u5b57\u57fa\u56e0\u5de5\u7a0b (Genetic engineering of Chinese characters) (2003) http://cbflabs.com/down/show.php?id=26", "author": ["B.F. Chu"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Surface or essence: Beyond the coded character set model", "author": ["S. Moro"], "venue": "Proceedings of the Glyph and Typesetting Workshop, Kyoto, Japon", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "A Computational Theory of Writing Systems", "author": ["R. Sproat"], "venue": "Studies in Natural Language Processing. Cambridge University Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "SCML: A Structural Representation for Chinese Characters", "author": ["D.G. Peebles"], "venue": "PhD thesis, Dartmouth College", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Wenlin CDL: Character Description Language", "author": ["T. Bishop", "R. Cook"], "venue": "Multilingual 18", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Seeking meaning in a space made out of strokes, radicals, characters and compounds", "author": ["Y. Haralambous"], "venue": "Proceedings of ISSM\u201910-11, Aizu-Wakamatsu, Japan.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Decomposition for ISO/IEC 10646 ideographic characters", "author": ["L. Qin", "C.S. Tong", "L. Yin", "L.N. Ling"], "venue": "COLING\u201902: Proceedings of the 3rd workshop on Asian language resources and international standardization, Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Chinese character recognition: history, status and prospects", "author": ["R. Dai", "C. Liu", "B. Xiao"], "venue": "Frontiers of Computer Science in China 1", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Network of words", "author": ["Y. Fujiwara", "Y. Suzuki", "T. Morioka"], "venue": "Artificial Life and Robotics 7", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Chinese character structure analysis based on complex networks", "author": ["J. Li", "J. Zhou"], "venue": "Physica A: Statistical Mechanics and its Applications 380", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Substructure shape analysis for Kanji character recognition", "author": ["J. Rocha", "H. Fujisawa"], "venue": "Advances in Structural and Syntactical Pattern Recognition. Springer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "A character-net based Chinese text segmentation method", "author": ["L. Zhou", "Q. Liu"], "venue": "SEMANET \u201902 Proceedings of the 2002 workshop on Building and using semantic networks, Association for Computational Linguistics", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Statistical properties of Chinese phonemic networks", "author": ["S. Yu", "H. Liu", "C. Xu"], "venue": "Physica A: Statistical Mechanics and its Applications 390", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Hanzi, Concept and Computation: A Preliminary Survey of Chinese Characters as a Knowledge Resource in NLP", "author": ["S.K. Hsieh"], "venue": "PhD thesis, Universit\u00e4t T\u00fcbingen", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Hanzi grid: toward a knowledge infrastructure for Chinese character-based cultures", "author": ["Y.M. Chou", "S.K. Hsieh", "C.R. Huang"], "venue": "Proceedings of the 1st international conference on Intercultural collaboration IWIC\u201907, Kyoto, Japan, Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Submorphemic processing in reading Chinese", "author": ["M. Taft", "X. Zhu"], "venue": "Journal of Experimental Psychology: Learning, Memory and Cognition 23", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Chinese character decoding: a semantic bias", "author": ["C. Williams", "T. Bever"], "venue": "Read Writ", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "The effects of stroke order and radicals on the knowledge of Japanese Kanji orthography, phonology and semantics", "author": ["K. Tamaoka", "H. Yamada"], "venue": "Psychologia 43", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Planning Chinese Characters", "author": ["S. Zhao", "Baldauf", "R.B. Jr."], "venue": "Reaction, Evolution or Revolution? Volume 9 of Language Policy. Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Sinographemdidaktik", "author": ["A. Guder-Manitius"], "venue": "Aspekte einer systematischen Vermittlung der chinesischen Schrift im Unterricht Chinesisch als Fremdsprache. Volume 7 of SinoLinguistica. Julius Groos Verlag, T\u00fcbingen", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Unicode Standard Annex #38", "author": ["J.H. Jenkins", "R. Cook"], "venue": "Unicode Han Database. Technical report, The Unicode Consortium", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "A Japanese logographic character frequency list for cognitive science research", "author": ["N. Chikamatsu", "S. Yokoyama", "H. Nozaki", "E. Long", "S. Fukuda"], "venue": "Behavior Research Methods, Instruments, & Computers 32(3)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Creating general-purpose corpora using automated search engine queries", "author": ["S. Sharoff"], "venue": "In Baroni, M., Bernardini, S., eds.: WaCky! Working papers on the Web as Corpus, Bologna, GEDIT", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "CHISE: Character processing based on character ontology", "author": ["T. Morioka"], "venue": "LKR\u201908: Proceedings of the 3rd international conference on Large-scale knowledge resources: Construction and application, Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Zur Phonetizit\u00e4t chinesischer Schriftzeichen in der Didaktik des Chinesischen als Fremdsprache", "author": ["C. Schindelin"], "venue": "Volume 13 of SinoLinguistica. iudicium, M\u00fcnchen", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Networks", "author": ["M.J. Newman"], "venue": "An introduction. Oxford University Press", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "\u4ee5\u6700\u4f73\u5316\u53ca\u6a5f\u7387\u5206\u4f48\u5224 \u65b7\u6f22\u5b57\u8072\u7b26\u4e4b\u7814\u7a76 (Automatic identification of phonetic complements for Chinese characters based on optimization and probability distribution)", "author": ["C.H. Chang", "S.Y. Li", "S. Lin", "C.Y. Huang", "J.M. Chen"], "venue": "Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), Puli, Nantou, Taiwan.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Phonetic distance based cross-lingual search", "author": ["S. Sriram", "P.P. Talukdar", "S. Badaskar", "K. Bali", "A.G. Ramakrishnan"], "venue": "Proc. of the 5th International Conf. on Natural Language Processing (KBCS 2004), Hyderabad, India.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "A new algorithm for the alignment of phonetic sequences", "author": ["G. Kondrak"], "venue": "NAACL 2000: Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2000}, {"title": "Sinica BOW: Integrating bilingual WordNet and SUMO ontology", "author": ["C.R. Huang"], "venue": "International Conference on Natural Language Processing and Knowledge Engineering.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Chinese WordNet (2008) http://www.aturstudio.com/wordnet/ windex.php", "author": ["Z Gao"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Development of the Japanese WordNet", "author": ["H. Isahara", "F. Bond", "K. Uchimoto", "M. Utiyama", "K. Kanzaki"], "venue": "Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC\u201908).", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["J.J. Jiang", "D.W. Conrath"], "venue": "Proceedings on International Conference on Research in Computational Linguistics, Taiwan 1997.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1997}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "Proceedings of 15th International Conference On Machine Learning, Madison WI, 1998.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1998}, {"title": "Using information content to evaluate semantic similarity in a taxonomy", "author": ["P. Resnik"], "venue": "Proceedings of the 14th International Joint Conference on Artificial Intelligence, Montr\u00e9al.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1995}, {"title": "Visible speech: The diverse oneness of writing systems", "author": ["J. DeFrancis"], "venue": "University of Hawaii Press, Honolulu", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1989}, {"title": "S\u00e9mantisme et classification dans l\u2019\u00e9criture chinoise", "author": ["F. Bott\u00e9ro"], "venue": "Les syst\u00e8mes de classement des caract\u00e8res par cl\u00e9s du Shuown Jiezi au Kangxi Zidian. Volume 37 of M\u00e9moires de l\u2019Institut des Hautes \u00c9tudes Chinoises. Coll\u00e8ge de France, Paris", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1996}, {"title": "Hantology: conceptual system discovery based on orthographic convention", "author": ["Y.M. Chou", "C.R. Huang"], "venue": "Ontology and the lexicon, a Natural Language Processing perspective, Cambridge University Press", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "A method of part-of-speech guessing of Chinese unknown words based on combined features", "author": ["H.J. Zhang", "S.M. Shi", "C. Feng", "H.Y. Huang"], "venue": "International Conference on Machine Learning and Cybernetics.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Chinese characters (or \u201csinographs\u201d) are notorious for their large number (over 84 thousand have been encoded in Unicode [1]) and their complexity (they can have from 1 stroke, like \u4e00, to as many as 64 strokes, like ;).", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "There have been various attempts at describing sinographs in a systematic way: [2] and [3] use generative grammars, [4] uses a Prolog-like language, [5] uses the biological metaphor of gene theory, [6] describes sinographs as objects with features (the Chaon model), [7] defines a formal language type called planar regular language, [8] and [9] describe the topology of sinographs in XML, [10] uses projections of stroke bounding boxes, and [11] use combinatorics of IDS operators.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "There have been various attempts at describing sinographs in a systematic way: [2] and [3] use generative grammars, [4] uses a Prolog-like language, [5] uses the biological metaphor of gene theory, [6] describes sinographs as objects with features (the Chaon model), [7] defines a formal language type called planar regular language, [8] and [9] describe the topology of sinographs in XML, [10] uses projections of stroke bounding boxes, and [11] use combinatorics of IDS operators.", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "There have been various attempts at describing sinographs in a systematic way: [2] and [3] use generative grammars, [4] uses a Prolog-like language, [5] uses the biological metaphor of gene theory, [6] describes sinographs as objects with features (the Chaon model), [7] defines a formal language type called planar regular language, [8] and [9] describe the topology of sinographs in XML, [10] uses projections of stroke bounding boxes, and [11] use combinatorics of IDS operators.", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "There have been various attempts at describing sinographs in a systematic way: [2] and [3] use generative grammars, [4] uses a Prolog-like language, [5] uses the biological metaphor of gene theory, [6] describes sinographs as objects with features (the Chaon model), [7] defines a formal language type called planar regular language, [8] and [9] describe the topology of sinographs in XML, [10] uses projections of stroke bounding boxes, and [11] use combinatorics of IDS operators.", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "There have been various attempts at describing sinographs in a systematic way: [2] and [3] use generative grammars, [4] uses a Prolog-like language, [5] uses the biological metaphor of gene theory, [6] describes sinographs as objects with features (the Chaon model), [7] defines a formal language type called planar regular language, [8] and [9] describe the topology of sinographs in XML, [10] uses projections of stroke bounding boxes, and [11] use combinatorics of IDS operators.", "startOffset": 198, "endOffset": 201}, {"referenceID": 6, "context": "There have been various attempts at describing sinographs in a systematic way: [2] and [3] use generative grammars, [4] uses a Prolog-like language, [5] uses the biological metaphor of gene theory, [6] describes sinographs as objects with features (the Chaon model), [7] defines a formal language type called planar regular language, [8] and [9] describe the topology of sinographs in XML, [10] uses projections of stroke bounding boxes, and [11] use combinatorics of IDS operators.", "startOffset": 267, "endOffset": 270}, {"referenceID": 7, "context": "There have been various attempts at describing sinographs in a systematic way: [2] and [3] use generative grammars, [4] uses a Prolog-like language, [5] uses the biological metaphor of gene theory, [6] describes sinographs as objects with features (the Chaon model), [7] defines a formal language type called planar regular language, [8] and [9] describe the topology of sinographs in XML, [10] uses projections of stroke bounding boxes, and [11] use combinatorics of IDS operators.", "startOffset": 334, "endOffset": 337}, {"referenceID": 8, "context": "There have been various attempts at describing sinographs in a systematic way: [2] and [3] use generative grammars, [4] uses a Prolog-like language, [5] uses the biological metaphor of gene theory, [6] describes sinographs as objects with features (the Chaon model), [7] defines a formal language type called planar regular language, [8] and [9] describe the topology of sinographs in XML, [10] uses projections of stroke bounding boxes, and [11] use combinatorics of IDS operators.", "startOffset": 342, "endOffset": 345}, {"referenceID": 9, "context": "There have been various attempts at describing sinographs in a systematic way: [2] and [3] use generative grammars, [4] uses a Prolog-like language, [5] uses the biological metaphor of gene theory, [6] describes sinographs as objects with features (the Chaon model), [7] defines a formal language type called planar regular language, [8] and [9] describe the topology of sinographs in XML, [10] uses projections of stroke bounding boxes, and [11] use combinatorics of IDS operators.", "startOffset": 390, "endOffset": 394}, {"referenceID": 10, "context": "There have been various attempts at describing sinographs in a systematic way: [2] and [3] use generative grammars, [4] uses a Prolog-like language, [5] uses the biological metaphor of gene theory, [6] describes sinographs as objects with features (the Chaon model), [7] defines a formal language type called planar regular language, [8] and [9] describe the topology of sinographs in XML, [10] uses projections of stroke bounding boxes, and [11] use combinatorics of IDS operators.", "startOffset": 442, "endOffset": 446}, {"referenceID": 11, "context": "The OCR community has also shown a strong interest in the structure of sinographs [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "As for considering Chinese script as a network, this has been done in several papers, such as [13, 14] (where edges represent components combined in the same sinograph) or [15], where components and sinographs form a bipartite graph; [16], where edges represent sinographs combined into words; [17], dealing with purely phonemic networks.", "startOffset": 94, "endOffset": 102}, {"referenceID": 13, "context": "As for considering Chinese script as a network, this has been done in several papers, such as [13, 14] (where edges represent components combined in the same sinograph) or [15], where components and sinographs form a bipartite graph; [16], where edges represent sinographs combined into words; [17], dealing with purely phonemic networks.", "startOffset": 94, "endOffset": 102}, {"referenceID": 14, "context": "As for considering Chinese script as a network, this has been done in several papers, such as [13, 14] (where edges represent components combined in the same sinograph) or [15], where components and sinographs form a bipartite graph; [16], where edges represent sinographs combined into words; [17], dealing with purely phonemic networks.", "startOffset": 172, "endOffset": 176}, {"referenceID": 15, "context": "As for considering Chinese script as a network, this has been done in several papers, such as [13, 14] (where edges represent components combined in the same sinograph) or [15], where components and sinographs form a bipartite graph; [16], where edges represent sinographs combined into words; [17], dealing with purely phonemic networks.", "startOffset": 234, "endOffset": 238}, {"referenceID": 16, "context": "As for considering Chinese script as a network, this has been done in several papers, such as [13, 14] (where edges represent components combined in the same sinograph) or [15], where components and sinographs form a bipartite graph; [16], where edges represent sinographs combined into words; [17], dealing with purely phonemic networks.", "startOffset": 294, "endOffset": 298}, {"referenceID": 13, "context": "[14] give an example of a small phono-semantic graph (a bipartite graph where edges connect semantic and phonetic components), but do not enter into the calculations of phoneticity and semanticity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Hanzi Grid [18, 19] maps component inclusions to relations in ontologies.", "startOffset": 11, "endOffset": 19}, {"referenceID": 18, "context": "Hanzi Grid [18, 19] maps component inclusions to relations in ontologies.", "startOffset": 11, "endOffset": 19}, {"referenceID": 19, "context": "Finally the cognitive psychology community is also heavily interested in the (cognitive) processing of phonetic and semantic components [20, 21], and even in the effects of stroke order and radicals on linguistic knowledge [22].", "startOffset": 136, "endOffset": 144}, {"referenceID": 20, "context": "Finally the cognitive psychology community is also heavily interested in the (cognitive) processing of phonetic and semantic components [20, 21], and even in the effects of stroke order and radicals on linguistic knowledge [22].", "startOffset": 136, "endOffset": 144}, {"referenceID": 21, "context": "Finally the cognitive psychology community is also heavily interested in the (cognitive) processing of phonetic and semantic components [20, 21], and even in the effects of stroke order and radicals on linguistic knowledge [22].", "startOffset": 223, "endOffset": 227}, {"referenceID": 22, "context": "In particular, during the Chinese writing reform [23], about 1,753 sinographs obtained simplified shapes (for example, \u8514 became \u535c), which are encoded separately in Unicode.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "These two properties of subcharacters in relation to characters can be quantified and are then called semanticity and phoneticity [24].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "We have used three publicly available frequency lists: uni, the languageindependent Unihan [25]; sin, the traditional Chinese Sinica compiled by Academia Sinica Taipei; and noz, a Japanese list, taken from [26].", "startOffset": 91, "endOffset": 95}, {"referenceID": 25, "context": "We have used three publicly available frequency lists: uni, the languageindependent Unihan [25]; sin, the traditional Chinese Sinica compiled by Academia Sinica Taipei; and noz, a Japanese list, taken from [26].", "startOffset": 206, "endOffset": 210}, {"referenceID": 26, "context": "We have also compiled our own frequency lists out of five corpora: Chinese Wikipedia (wps for simplified Chinese and wpt for traditional Chinese), Japanese Wikipedia wpj, Chinese Project Gutenberg gut, Chinese GoogleBooks goo, Leeds Chinese Internet Corpus cic [27].", "startOffset": 261, "endOffset": 265}, {"referenceID": 27, "context": "After merging with data from the CHISE project [28] and with data kindly provided by Cornelia Schindelin [29], and after having removed identities, we got a list of 824,120 strict inclusions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 28, "context": "After merging with data from the CHISE project [28] and with data kindly provided by Cornelia Schindelin [29], and after having removed identities, we got a list of 824,120 strict inclusions.", "startOffset": 105, "endOffset": 109}, {"referenceID": 29, "context": "Remarkably, these are a bit low compared to typical scale-free networks like the Web or proteins, which are in the 2\u20133 range [30].", "startOffset": 125, "endOffset": 129}, {"referenceID": 30, "context": "For Mandarin Chinese we implemented the phonetic distance described in [31].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "We obtained this distance by applying a methodology given by [32] and [33]: indeed, we defined a distance for syllables, using seven features: consonant place of articulation (weight 4), consonant voicing (1), consonant manner of articulation (4), consonant palatalization (1), vowel frontness (5), vowel height (1), and vowel rounding (1).", "startOffset": 61, "endOffset": 65}, {"referenceID": 32, "context": "We obtained this distance by applying a methodology given by [32] and [33]: indeed, we defined a distance for syllables, using seven features: consonant place of articulation (weight 4), consonant voicing (1), consonant manner of articulation (4), consonant palatalization (1), vowel frontness (5), vowel height (1), and vowel rounding (1).", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "We define the phoneticity coefficient \u03c6(c1, c2) := 1\u2212dmin(c1,c2) N where N is a normalization constant such that Im(\u03c6) = [0, 1].", "startOffset": 121, "endOffset": 127}, {"referenceID": 33, "context": "As semantic resources for sinographic languages we used three WordNets: the Academia Sinica BOW [34] for traditional Chinese, the Chinese WordNet [35] for simplified Chinese and the Japanese WordNet [36].", "startOffset": 96, "endOffset": 100}, {"referenceID": 34, "context": "As semantic resources for sinographic languages we used three WordNets: the Academia Sinica BOW [34] for traditional Chinese, the Chinese WordNet [35] for simplified Chinese and the Japanese WordNet [36].", "startOffset": 146, "endOffset": 150}, {"referenceID": 35, "context": "As semantic resources for sinographic languages we used three WordNets: the Academia Sinica BOW [34] for traditional Chinese, the Chinese WordNet [35] for simplified Chinese and the Japanese WordNet [36].", "startOffset": 199, "endOffset": 203}, {"referenceID": 36, "context": "By measuring distance between WordNet nodes using semantic similarity measures, such as those of [37\u201339] or simply the inverse of the shortest path length in the WordNet graph.", "startOffset": 97, "endOffset": 104}, {"referenceID": 37, "context": "By measuring distance between WordNet nodes using semantic similarity measures, such as those of [37\u201339] or simply the inverse of the shortest path length in the WordNet graph.", "startOffset": 97, "endOffset": 104}, {"referenceID": 38, "context": "By measuring distance between WordNet nodes using semantic similarity measures, such as those of [37\u201339] or simply the inverse of the shortest path length in the WordNet graph.", "startOffset": 97, "endOffset": 104}, {"referenceID": 0, "context": "S(\u03b9) = 12 log(1 + f1(\u03b9)) + 1 4 log(1 + f2(\u03b9)) + 1 4r(\u03b9), normalized so that its values stay in the interval [0, 1].", "startOffset": 108, "endOffset": 114}], "year": 2014, "abstractText": "Chinese characters have a complex and hierarchical graphical structure carrying both semantic and phonetic information. We use this structure to enhance the text model and obtain better results in standard NLP operations. First of all, to tackle the problem of graphical variation we define allographic classes of characters. Next, the relation of inclusion of a subcharacter in a characters, provides us with a directed graph of allographic classes. We provide this graph with two weights: semanticity (semantic relation between subcharacter and character) and phoneticity (phonetic relation) and calculate \u201cmost semantic subcharacter paths\u201d for each character. Finally, adding the information contained in these paths to unigrams we claim to increase the efficiency of text mining methods. We evaluate our method on a text classification task on two corpora (Chinese and Japanese) of a total of 18 million characters and get an improvement of 3% on an already high baseline of 89.6% precision, obtained by a linear SVM classifier. Other possible applications and perspectives of the system are discussed.", "creator": "TeX"}}}