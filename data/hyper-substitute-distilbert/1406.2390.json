{"id": "1406.2390", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2014", "title": "Unsupervised Deep Haar Scattering on Graphs", "abstract": "thus introduce a deep scattering modeling, wherein computes invariants with iterated contractions adapted to nonlinear objectives. it describes a generic adaptive network model, requires specific properties thus be analyzed mathematically. complete cascade of wavelet transform convolutions naturally computed with modified complex filter model, and modified because displacement. unsupervised channels of diffusion optimize the selection factor, by maximizing direct average discriminability of filter outputs. for hybrid wavelets, regression only solved with a polynomial exponential smoothing algorithm. translation _ rotation invariance learning recently shown giving angular indices introducing cross - twisted digits.", "histories": [["v1", "Mon, 9 Jun 2014 23:51:30 GMT  (29kb,D)", "http://arxiv.org/abs/1406.2390v1", null], ["v2", "Mon, 3 Nov 2014 15:25:16 GMT  (1277kb,D)", "http://arxiv.org/abs/1406.2390v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["xu chen", "xiuyuan cheng", "st\u00e9phane mallat"], "accepted": true, "id": "1406.2390"}, "pdf": {"name": "1406.2390.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning by Deep Scattering Contractions", "authors": ["Mia Xu Chen", "Xiuyuan Cheng", "St\u00e9phane Mallat"], "emails": ["xuchen@princeton.edu,", "xiuyuan.cheng@ens.fr,", "mallat@di.ens.fr"], "sections": [{"heading": "1 Introduction", "text": "The curse of high-dimensional learning results from the huge space volume. A uniform sampling requires a number of examples which increases exponentially with the dimension. Scattering transforms reduce the space volume by iteratively applying contractive operators, with a deep convolution network architecture [2, 5]. These contractions compute invariants and progressively decrease the space dimension to circumvent the curse of dimensionality. We introduce a deep scattering architecture implemented with a standard multirate filter bank, where learning is introduced with permutation operators. It rotates the space with orthogonal wavelet transforms and implements directional contractions with modulus operators. It provides a simple deep convolution network model, whose properties can be analyzed mathematically, because filter coefficients do not need to be modified.\nSection 2 begins by introducing scattering transforms with a non-linear multirate filter bank. It computes wavelet transform convolutions and modulus non-linearities. Section 3 describes adapted scattering transforms, which optimize contraction directions with permutations. For Haar wavelets, it amounts to a pair matching problem, as explained in Section 4, which yields hierachical invariants over multiscale groups. The scattering transform is then calculated with a cascade of additions and subtractions over pairs of points.\nBy adapting contractions with simple operators, a goal of this paper is to highlight important principles which govern the structure of deep convolution networks, and relate them to standard signal processing tools. Contractions are potentially dangerous because they can strongly reduce distance along certain directions and hence the discriminability of elements in different classes. Supervised data provides information to evaluate distances across classes, and thus adapt contractions to avoid reducing them too much.\nThe authors\u2019 names are in alphabetical order. This work was supported by the ERC grant InvariantClass 320959.\nar X\niv :1\n40 6.\n23 90\nv1 [\ncs .L\nG ]\n9 J\nun 2\n01 4\nUnsupervised learning can only implement weaker contraction criteria on training samples. We show that such optimizations amounts to computing sparse signal representations, similarly to sparse auto-encoders [4]. These results are illustrated by numerical experiments on modified MNIST digit images in Section 5. It shows that unsupervised scattering networks can learn translation and rotation invariant descriptors, with no prior information."}, {"heading": "2 Scattering Transforms", "text": "A scattering transform is computed with a cascade of wavelet transforms and modulus non-linearities [2]. We reintroduce this transformation as a product of multirate filter bank operators, which is closer to deep convolution networks [5], and allows to generalize it for learning."}, {"heading": "2.1 Modulus Filter-Bank", "text": "A multirate filter bank computes convolutions of x \u2208 Rd with a low-pass filter h and a high-pass filter g, and subsamples the output by a factor 2:\nHx(n) = x ? h(2n) and Gx(n) = x ? g(2n) .\nTo avoid boundary issues, we consider x as a d-periodic signal, where d is a power of 2. The low-pass filter h is real but g may be complex valued. Both filters have a finite impulse response of size K. We write Wx = (Hx,Gx) the resulting multirate filtering. In the following, we shall impose that\n\u2200x \u2208 Rd , \u2016Wx\u20162 = \u2016Hx\u20162 + \u2016Gx\u20162 = \u2016x\u20162.\nA fast wavelet transform iteratively applies W to the low-pass output Hx, as illustrated in Figure 1(a). The resulting orthogonal wavelet transform at the scale 2J is\nWJx = (H Jx,GHjx)0\u2264j<J .\nThe first component is the averaged signal\nHJx = x ? \u03c6J(2 jn),\nwhere \u03c6J is a low-pass filter of support K2J . The wavelet coefficients are\nGHjx = x ? \u03c8j(2 jn),\nwhere \u03c8j is a dilated band-pass filter whose support is of size K2j .\nComplex nearly analytic wavelet transforms are implemented with a nearly analytic complex filters g. It however requires to slightly modify the filter bank by first extending the dimension of x from d to 2d with a linear interpolation [6]. Real orthogonal wavelet transforms are implemented with real conjugate mirror filters (h, g) which satisfy\n|h\u0302(\u03c9)|2 + |h\u0302(\u03c9 + \u03c0)|2 = 2 with h\u0302(0) = \u2211 n h(n) = \u221a 2. (1)\nThe high pass filter is g(n) = (\u22121)nh(1\u2212n), and \u2211 n g(n) = 0. One can prove thatW = (H,G) is then an orthogonal operator in Rd and that {\u03c6J(n \u2212 k2J), \u03c8j(n \u2212 k2j)}0\u2264k\u2264d2\u2212j is a wavelet orthonormal basis of Rd [1].\nHaar filters are simple examples defined by h(n) = 2\u22121/2(\u03b4(n) + \u03b4(n \u2212 1)) and g(n) = 2\u22121/2(\u03b4(n) \u2212 \u03b4(n \u2212 1)). The resulting Haar scaling filter is \u03c6J = 2\u2212J/21[0,2J\u22121] and the Haar wavelet is \u03c8j = 2\u2212j/2(1[0,2j\u22121\u22121] \u2212 1[2j\u22121,2j\u22121]). A modulus filter bank inserts a contraction with a modulus operator applied to the high frequency component Gx of x:\n|G|x(n) = |x ? g(2n)| .\nSuppressing the phase or the sign performs a non-linear demodulation which shifts part of the signal energy towards the lower frequencies. This modulus is not applied to Hx which is already a low frequency signal. With an abuse of notation, we denote by |x| the vector obtained by applying the entry-wise modulus operator to a vector x \u2208 Rd, that is, |x| = {|x(n)|}1\u2264n\u2264d, and we write |W |x = (Hx, |G|x). If x is real positive, which is the case in a scattering cascade, then Hx is positive real so Hx = |Hx|. It is thus as if the modulus was also applied to Hx.\nA modulus is contractive in the sense that ||a|\u2212|b|| \u2264 |a\u2212b|, for any (a, b) \u2208 C2. Since \u2016Wx\u2212Wx\u2032\u2016 = \u2016x\u2212 x\u2032\u2016, it results that |W | is contractive\n\u2016|W |x\u2212 |W |x\u2032\u2016 \u2264 \u2016x\u2212 x\u2032\u2016, and it preserves the norm \u2016|W |x\u2016 = \u2016x\u2016. It is important to realize that a modulus strongly contracts the signal space. Indeed, Gx is an oscillatory signal whose sign or phase changes rapidly. For real signals, a modulus reduces by 2 the range of variation of its coefficients."}, {"heading": "2.2 Scattering as Iterated Wavelet Transforms", "text": "The scattering transform of x \u2208 Rd is defined by a product of modulus filtering operators:\nSJx = |W |Jx \u2208 Rd . (2) Since |W | is contractive and preserves the norm, it results that SJ = |W |J is also contractive and preserves the norm:\n\u2016SJx\u2212 SJx\u2032\u2016 \u2264 \u2016x\u2212 x\u2032\u2016 and \u2016SJx\u2016 = \u2016x\u2016 . (3)\nThe operator product (2) iteratively computes Sjx = |W |Sj\u22121x for 1 \u2264 j \u2264 J , with S0x = x. Each time, |W | splits Sj\u22121x in two vectors (HSjx, |G|Sjx). It resuts that Sjx is a concatenation of 2j vectors {Sj,kx}0\u2264k<2j of dimension 2\u2212jd each, with\nSj,2kx = H Sj\u22121,kx and Sj,2k+1x = |G|Sj\u22121,kx . (4)\nThis tree subdecomposition is illustrated in Figure 1(b). Each SJ,k can thus be written a product of 2J operators equal to H or |G|. Since h and g have a size K, the scattering operator SJ has a support of size K2J . Each SJx(n) is computed from K2J consecutive values of x. If we supress the modulus nonlineary and replace |G| by G then SJ has totally different properties. It is then a linear wavelet packet orthogonal transform introduced in [7].\nA scattering operator is implemented as a product of multirate filtering modulus, but its properties depend upon the underlying wavelet transform. A wavelet transform modulus at the scale 2l is defined by\n|Wl|x = (H lx, |G|Hjx)0\u2264j<l .\nThe modulus is applied only to the output of G:\n|G|Hjx = |x ? \u03c8j(2jn)| .\nThe modulus suppresses the wavelet tranform phase and hence computes an envelop which is approximatively invariant to translations smaller than 2j .\nA scattering transform SJ can be factorized as a product of wavelet transform modulus:\nSJ = |W1| ...|WJ\u22121| |WJ | .\nIndeed, for each 0 \u2264 k < 2J there exists 0 \u2264 m(k) \u2264 J such that\nSJ,k = H J\u2212\n\u2211 l jl m(k)\u220f l=1 |G|Hjl with \u2211 l jl \u2264 J .\nEach |G|Hjl is calculated by the wavelet transform modulus |Wjl |. Figure 1(b) illustrates the factorization of the scattering operator SJ into the modulus of wavelet transforms |Wj |, for J = 3.\nThe index m(k) is the number of times that |G| appears in the calculation of SJ,k. There are ( J m ) vectors SJ,k of orderm. Since the contraction is produced by |G|, the indexm can be interpreted as a contraction factor. It plays an essential role to analyze the properties of these deep network coefficients. For a random vector X in Rd, we define \u03c32m = \u2211 k,m(k)=m Var(SJ,kX) as the total variance of all coefficients of order m. Each modulus is applied to the output of a filter G which produces a zero-mean random vector. It thus strongly reduces the variance, and \u03c32m typically decreases exponentially with m. Table 1 gives its value for a Gaussian white random vector, which is an average measure of volume reduction in the signal space. One can show that \u03c32m \u2248 (1\u2212 2\u03c0 ) m \u00b7 ( J m ) , which is verified numericaly in Table 1. Beyond m = 4 the variance of coefficients becomes negligible and the corresponding scattering coefficients thus carry little information. Eliminating coefficients of order m > 4 reduces the number of scattering coefficients from d to about (log42 d)/24."}, {"heading": "3 Adapted Scattering", "text": "Scattering transforms are computed by deep convolution network, where the network weights are specified by the filters h and g. Learning network weights in convolution networks can thus be interpreted as a filter adaptation. As long as h remains an averaging filter and g is a high pass filter, modifying the filter values has marginal effects. Cascading such subsampled filtering still defines a wavelet transform. The number of vanishing moments or the wavelet regularity may be modified [1], but it does not modify much the wavelet coefficient properties. Major modifications of wavelet coefficients are however obtained by modifying the orbit (ordering of coefficients) along which convolutions are performed. For example, rotation invariant scattering transforms are calculated by also computing convolutions along rotation parameters in a deep scattering network [8]. This suggests to adapt wavelet transforms with permutations of the network variables before applying convolution operators. Let \u03c0 be a permutation of {1, ..., d}. It acts on x \u2208 Rd as a linear orthogonal operator, written z = \u03c0x \u2208 Rd, with z(n) = x(\u03c0(n)). An adapted scattering inserts a permutation \u03c0j of {1, ..., d}, before each multirate modulus convolution:\nSjx = |W |\u03c0jSj\u22121x,\nwhere \u03c0j makes a permutation of the d coordinates of Sj\u22121x before applying |W |. It results that"}, {"heading": "SJ = |W |\u03c0J ... |W |\u03c02 |W |\u03c01 .", "text": "Since each \u03c0j is a linear orthogonal operator, SJ remains contractive and preserves the signal norm:\n\u2016SJx\u2212 SJx\u2032\u2016 \u2264 \u2016x\u2212 x\u2032\u2016 and \u2016SJx\u2016 = \u2016x\u2016 . (5)\nGeometrically, W\u03c0j is a rotation of the signal space. It depends on \u03c0j , which thus modifies the \u201cdirections\u201d in which the modulus contractions are acting. The group of permutations is a group of rotations among which we shall search for particular rotations which optimize the contraction directions for learning. Optimizing permutations are typically NP -hard problems. However, the problem is not as bad as it seems because |W | computes convolutions with filters h and g of small support K. The output values thus depend on local ordering properties. For Haar filters, Section 4.1 shows that the ordering reduces to a pairing problem.\nA scattering SJ operators is not invertible because the modulus looses a complex phase or a sign. In the real case, for almost all pairs of unitary operators (A,B) it has been proved [11] that the operator (|A|, |B|) is invertible on Rd. If W is real, one can thus expect that (|W\u03c00|, |W\u03c01|) are invertible for \u201csufficiently different\u201d permutations \u03c00 and \u03c00. A necessary and sufficient condition is given on \u03c00 and \u03c01 for this result to hold for a Haar filtering."}, {"heading": "3.1 Adapted Haar Scattering", "text": "A Haar modulus filtering computes\nHx(n) = x(2n) + x(2n+ 1)\u221a 2 and |G|x(n) = |x(2n)\u2212 x(2n+ 1)|\u221a 2 .\nObserve that (Hx(n), |G|x(n)) is a permutation invariant representation of (x(2n), x(2n + 1)), which specifies the two values of x(2n) and x(2n+ 1) because\nmax(x(2n), x(2n+ 1)) = Hx(n) + |G|x(n)\u221a\n2 and min(x(2n), x(2n+ 1)) = Hx(n)\u2212 |G|x(n)\u221a 2 .\nAn adapted Haar scattering is thus a cascade of permutation invariant operators over matched pairs.\nWe say that two permutations \u03c00 and \u03c01 are \u201cinterlacing\u201d if there exists no strict subset \u2126 of {1, . . . , d} such that \u03c00 and \u03c01 are pairing elements within \u2126. The following theorem derives a condition to recover a signal from 2J vectors of invariant Haar scattering coefficients.\nTheorem 3.1. Suppose that x \u2208 Rd takes more than 2 different values.\n(1) If W is a Haar filtering, then (|W |\u03c00, |W |\u03c01) is invertible if \u03c00 and \u03c01 are interlacing.\n(2) If \u03c00 and \u03c01 are interlacing then any x \u2208 Rd can be recovered from the 2J scattering transforms SJx computed from the 2J families of permutations (\u03c0 11 , ..., \u03c0 J J ) defined by all binary vectors\n( 1, ..., J) with j \u2208 {0, 1}.\nProof. Property (2) is proved by applying (1) recursively to (|W |\u03c00Sjx, |W |\u03c01Sjx) for J \u2212 1 \u2265 j \u2265 0.\nTo prove (1), notice that if n1, n2, n3 is a triplet where (n1, n2) is a pair in \u03c00 and (n1, n3) a pair in \u03c01 then the values x(n1), x(n2), x(n3) are uniquely determined from (|W |\u03c00x, |W |\u03c01x), unless x(n1) 6= x(n2) and x(n2) = x(n3). The interlacing condition implies that \u03c01 pairs n2 to an index n4 which can not be n3 or n1. Moreover, the four values of x(n1), x(n2), x(n3), x(x4) are specified unless x(n4) = x(n1) 6= x(n2) = x(n3). This interlacing argument can be used to extend to {1, . . . , d} the set of all indices ni for which x(ni) is specified, unless x takes only two values. If that is not the case then x can be recovered, which proves the theorem."}, {"heading": "4 Unsupervised Learning", "text": "Learning a scattering metric amounts to finding the most appropriate directions along which to contract the space, and hence optimize permutations. For Haar wavelets, we show that this optimization can be reduced to pairing problems. We then concentrate on unsupervised learning."}, {"heading": "4.1 Haar Pairing", "text": "To a permutation \u03c0, we associate a pairing p\u03c0 defined by the unordered set of d/2 pairs {(\u03c0(2n), \u03c0(2n+ 1)}n\u2264d/2. Many permutations \u03c00 and \u03c01 have the same pairing p\u03c00 = p\u03c01 . A Haar scattering metric is only specified by these pairing operators. Indeed, if p\u03c00j = p\u03c01j for 1 \u2264 j \u2264 J then one can verify by induction on J that there exists a permutatioon \u03c0 such that S0J = |W |\u03c00JxJ ... |W |\u03c001 and S1J = |W |\u03c01J ... |W |\u03c011 satisfy S0J = \u03c0S1J . It results that the scattering metrics are identical:\n\u2200(x, x\u2032) \u2208 R2d , \u2016S0Jx\u2212 S0Jx\u2016 = \u2016S1Jx\u2212 S1Jx\u2032\u2016.\nThe optimization of a Haar scattering metric is reduced to an optimization of pairing operators. Ordering problems are typically NP -hard, but not pairing problems. If C(p\u03c0) is an additive cost of each pair\nC(p\u03c0) = d/2\u2211 n=1 c(\u03c0(2n), \u03c0(2n+ 1)) (6)\nthen p\u03c0\u2217 = arg minp\u03c0 C(p\u03c0) is computed with O(d 3) operations with the Blossom Algorithm of Edmonds [9]. Computations in this paper use the implementation in [10]. The next section explains how to construct such additive costs for unsupervised learning."}, {"heading": "4.2 Unsupervised Contraction Learning", "text": "A contraction can reduce too much the distance between two points which do not belong to the same class, and thus strongly reduce their discriminability. In general, this can not be fully avoided with unsupervised data. However, one can optimize contractions by maximizing the average distance between training samples, so that this loss of discriminability becomes less likely. We show that it implies computing sparse signal representations.\nA scattering progressively contracts the scattering metric \u2016Sj\u22121x \u2212 Sj\u22121x\u2032\u2016, by applying |W |\u03c0j for 1 \u2264 j \u2264 J . We consider unlabeled examples as realizations of a random vector X \u2208 Rd, which is an unknown mixture of different classes. The operator |W |\u03c0j strongly contracts the signal space but it should reduce as little as possible the average Euclidean distance between realizations of Sj\u22121X , to avoid confusing elements of different unknown classes. We thus want to maximize the variance \u03c32(SjX) of SjX . Since \u03c32(SjX) = E\u2016SjX\u20162 \u2212 \u2016E(SjX)\u20162 and E\u2016SjX\u20162 = E\u2016Sj\u22121X\u20162, this is equivalent to finding a permutation \u03c0j which minimizes\n\u2016E(SjX)\u20162 = \u2016E(|W |\u03c0jSj\u22121X)\u20162. From q realizations {xi}i\u2264q , we estimate \u2016E(SjX)\u2016 with the empirical sum \u2016 \u2211q i=1 Sjxi\u2016. It is a mixed l1 norm along realizations xi and an l2 norm across the scattering coordinates. This is a sparsity norm which is optimized so that W\u03c0jSj\u22121X has a sparse representation. Sparsity appears in this optimization because the distance between two vectors having many zero coordinates is not much reduced by applying a modulus on the coordinate of these vectors. Indeed, ||a| \u2212 |b|| = |a\u2212 b| if a or b is zero. The mixed l2 and l1 norm can also be replaced by a simpler l1 sparsity norm.\nFor a Haar scattering transform, the sparsity norm is an additive cost on the pairing p\u03c0j :\n\u2016 q\u2211 i=1 Sjxi\u20162 = C(p\u03c0j ) = d/2\u2211 p=1 c(\u03c0j(2p), \u03c0j(2n+ 1)),\nwith\nc(k, k\u2032) = ( q\u2211 i=1 Sj\u22121xi(k) + Sj\u22121x(k \u2032) )2 + ( q\u2211 i=1 |Sj\u22121xi(k)\u2212 Sj\u22121x(k\u2032)| )2 .\nIt is solved with the Blossom algorithm with O(d3) operations. The unsupervised learning algorithm iteratively computes an optimized pairing p\u03c0j for j going from 1 to J . One can also impose constraints on the pairing to reduce computations and incorporate some prior information. An inner-node pairing in the filter bank tree imposes that \u03c0j pairs coefficients within each vector Sj,kx, and performs the same pairing for all 0 \u2264 k < 2j . There is typically not a unique pairing which minimizes the unsupervised contraction cost. Let us consider for example a random vector X which is stationary and has a period d. Any translation of the pairing yields the same cost because of the stationarity. These pairing being equally valid, one can reduce the classification variance by using them all. We thus use a bagging algorithm which estimates several ordered scattering from several training sets, and aggregates these multiple scattering vectors to compute a linear SVM classification."}, {"heading": "5 Numerical Experiments", "text": "Unsupervised contraction learning does not differentiate subclasses and can thus mostly learn sources of variability which are in common for most of the classes. Geometric variability such as translations, rotations or deformations are such examples. The MNIST digit recognition data bases provides a simple framework to study the learning of these sources of variability.\nLearning of translations and deformations is evaluated in Section 5.1 on the original MNIST database, which has 60,000 training samples and 10,000 testing samples. A modified MNIST data basis with 3D digit rotations is studied in Section 5.2."}, {"heading": "5.1 MNIST Digit Recognition", "text": "We consider a random permutation of the MNIST image pixels, illustrated in Figure 2a. Each image is thus considered as a non-ordered bag of pixels. These experiments tests several aspect of the algorithm: the ability to recover spatial neighborhood information and the classification accuracy without location information.\nThe permutation learning is first performed with inner-node pairing optimizations, which means that for each level j, the same pairing function is used to associate the coefficients of each Sj,kx for 0 \u2264 k < 2j . These pairings perform a multiscale estimation of relative spatial locations. We say that two coefficients Sj,k(n) and Sj,k(n\u2032) are spatially connected if they are computed with operators whose supports of size 2j are spatially connected in the original image domain. We only consider coefficients whose amplitude are non-negligible and thus play a role in the classification. Over the first three levels 1 \u2264 j \u2264 3, a 100% of the pairs are connected, which shows that all pairing are spatialy connected. For j = 4 and j = 5 the proportion of connected pairs are respectively 85% and 67%. The connectivity ratio decreases with the scale because long range correlations are weaker and spatially non-connected pairs may become more similar than spatially connected ones.\nMNIST images have d \u2264 210 pixels so scattering operators can be computed at scales 2J \u2264 210. The classification error decreases when J increases and the best accuracy is obtained for J = 10, which corresponds to a maximally invariant representation. We compute N adapted scattering operators with unsupervised pairing overN different subsets of training samples. TheN different scattering vectors SJx are aggregated and fed into a linear SVM. The energy of scattering coefficients decrease exponentially with m, as shown by Table 1 for a Gaussian process. Table 2b shows that best classification results are obtained by keeping only coefficients of order m \u2264 4. Since 2J = d, we thus keep about (log2 d)4/24 scattering coefficients as opposed to d. These figures are computed with an aggregation of N = 10\ndifferent scattering vectors, but the optimum for m = 4 does not depend upon N . Table 2c gives the classification error rate as a function of N . The performance increases slowsly for N \u2265 10, and does not improve beyond N = 50, which is much smaller than 2J .\nAlmost all MNIST classification algorithms use prior information on the spatial location of pixels to build spatially localized descriptors, and deep convolution networks as well as translation invariant scattering transforms further use prior information on translation invariance. State of the art results, without making any modification of training vectors, is achieved by deep convolutional neural networks (0.53%, [5]), and scattering networks with Gabor wavelet (0.43% [2]). This unsupervised learning, uses no prior information on pixel location or on translation invariance. It reaches an error of 0.9% by optimizing inner-node pairing. If no constraint is imposed on the pairing algorithm, which may thus associates coefficients computed across different nodes in the filter bank tree, then the error increases to 1%. Indeed, for MNIST, intra-class variabilities are mostly due to translations and deformations. Appropriate invariants can be thus computed with inner-node pairing. Providing more flexibiilty increases the pairing variance, which explains the slight error increase. This flexibility can however produce useful invariants for non-translation invariants."}, {"heading": "5.2 MNIST with 3D Rotations", "text": "To test the algorithm ability to build invariant to different source of geometric variability, we use the 3D rotated MNIST data basis constructed in [3]. Digit \u20189\u2019 is removed from the data set as it is equivalent to the digit \u20186\u2019 after rotation. Each digit is projected on a 3D sphere sampled over d = 4096 points, and randomly rotated on the sphere, with a rotation variance \u03c32 = 0.2 [3].\nTranslations in the plane are now replaced by rotations over the sphere, on which one can not define convolution operators after sampling. The classification algorithms in [3] introduces an elegant solution which replaces convolution operators (diagonal in Fourier) by operators which are diagonal over the Laplacian eigenvectors on a graph. These algorithms use the 3D neighborhood of points on the sphere to define the graph connectivity. Table 2 gives the results reported in [3], with 19% error for a nearest neighbor algorithm, 5.6% for a two-layer fully connected neural network, and 6% for two best locally connected network algorithms. The adapted Haar scattering algorithm yields a smaller error of 2.2%, when computed at the maximum scale 2J = d, by aggregating N = 10 scattering transforms, up the order m = 4. The Haar scattering thus reduces the error by an important factor although it uses no prior information on the 3D connectivity of points, which illustrates the learning abilities of this deep network structure."}], "references": [{"title": "A Wavelet Tour of Signal Processing: The Sparse Way", "author": ["S. Mallat"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Invariant Scattering Convolution Networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Trans. PAMI, 35(8): 1872- 1886, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1872}, {"title": "Spectral Networks and Deep Locally Connected Networks on Graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "ICLR 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Convolutional Networks and Applications in Vision", "author": ["Y. LeCun", "K. Kavukvuoglu", "C. Farabet"], "venue": "Proc. IEEE Int. Sump. Circuits and Systems 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "A dual tree complex wavelet transform", "author": ["G. Selesnick", "I. Baraniuk", "Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Wavelet analysis and signal processing\u201d, in Wavelets and Their Applications, Jones and Barlett", "author": ["R. Coifman", "Y. Meyer", "M. Wickerhauser"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination", "author": ["L. Sifre", "S. Mallat"], "venue": "CVPR 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Paths, trees, and flowers", "author": ["J. Edmonds"], "venue": "Canadian Journal of Mathematics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1965}, {"title": "Gabow\u2019s, \u201cAn Efficient Implementation of Edmond\u2019s Algorithm for Maximum Matching on Graphs.", "author": ["H.E. Rothberg of"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1976}, {"title": "On signal reconstruction without phase", "author": ["R. Balana", "P. Casazza", "D. Edidinb"], "venue": "Applied and Computational Harmonic Analysis, 2006. 9", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 1, "context": "Scattering transforms reduce the space volume by iteratively applying contractive operators, with a deep convolution network architecture [2, 5].", "startOffset": 138, "endOffset": 144}, {"referenceID": 4, "context": "Scattering transforms reduce the space volume by iteratively applying contractive operators, with a deep convolution network architecture [2, 5].", "startOffset": 138, "endOffset": 144}, {"referenceID": 3, "context": "We show that such optimizations amounts to computing sparse signal representations, similarly to sparse auto-encoders [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "A scattering transform is computed with a cascade of wavelet transforms and modulus non-linearities [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "We reintroduce this transformation as a product of multirate filter bank operators, which is closer to deep convolution networks [5], and allows to generalize it for learning.", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "It however requires to slightly modify the filter bank by first extending the dimension of x from d to 2d with a linear interpolation [6].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "One can prove thatW = (H,G) is then an orthogonal operator in R and that {\u03c6J(n \u2212 k2), \u03c8j(n \u2212 k2)}0\u2264k\u2264d2\u2212j is a wavelet orthonormal basis of R [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 6, "context": "It is then a linear wavelet packet orthogonal transform introduced in [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "The number of vanishing moments or the wavelet regularity may be modified [1], but it does not modify much the wavelet coefficient properties.", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "For example, rotation invariant scattering transforms are calculated by also computing convolutions along rotation parameters in a deep scattering network [8].", "startOffset": 155, "endOffset": 158}, {"referenceID": 10, "context": "In the real case, for almost all pairs of unitary operators (A,B) it has been proved [11] that the operator (|A|, |B|) is invertible on R.", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "then p\u03c0\u2217 = arg minp\u03c0 C(p\u03c0) is computed with O(d ) operations with the Blossom Algorithm of Edmonds [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "Computations in this paper use the implementation in [10].", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "53%, [5]), and scattering networks with Gabor wavelet (0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "43% [2]).", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "To test the algorithm ability to build invariant to different source of geometric variability, we use the 3D rotated MNIST data basis constructed in [3].", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "2 [3].", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "The classification algorithms in [3] introduces an elegant solution which replaces convolution operators (diagonal in Fourier) by operators which are diagonal over the Laplacian eigenvectors on a graph.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Table 2 gives the results reported in [3], with 19% error for a nearest neighbor algorithm, 5.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "[3] Spectral Net.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Adapted Haar Scat.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Table 2: Percentage of errors on MNIST 3D rotation data set [3], with a nearest neighbor classifier, a fully connected two layer neural network, a locally connected network, a spectral network [3], and an adapted Haar scattering.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "Table 2: Percentage of errors on MNIST 3D rotation data set [3], with a nearest neighbor classifier, a fully connected two layer neural network, a locally connected network, a spectral network [3], and an adapted Haar scattering.", "startOffset": 193, "endOffset": 196}], "year": 2014, "abstractText": "We introduce a deep scattering network, which computes invariants with iterated contractions adapted to training data. It defines a deep convolution network model, whose contraction properties can be analyzed mathematically. A cascade of wavelet transform convolutions are computed with a multirate filter bank, and adapted with permutations. Unsupervised learning of permutations optimize the contraction directions, by maximizing the average discriminability of training data. For Haar wavelets, it is solved with a polynomial complexity pairing algorithm. Translation and rotation invariance learning is shown with classification experiments on hand-written digits.", "creator": "LaTeX with hyperref package"}}}