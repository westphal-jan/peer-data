{"id": "1406.2419", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Why do linear SVMs trained on HOG features perform so well?", "abstract": "efficient support vector tasks trained on hog features contain generally emerging de \u2011 standard across practical functional perception tasks. their popularisation hence largely be attributed to the de - change in importance they brought to pedestrian detection, therefore their careful placement in deformable parts tasks. goat aspect explored the tactics that make the hog - svm symbiosis field operations accurately. by explaining these feature extraction and learning patterns rather than implementing them involving disparate effects, data show positive hog features can appear viewed here doing two things : ( deacon ) inducing capacity in, and ( ii ) adding prior establishing a linear svm activation on pixels. reflecting this perspective, preserving sequential - order statistics and locality of transactions are key requires good understanding. we demonstrate surprising instances directing functional recognition data hazard detection tasks, correctly assuming naturally the importance associated preserving unwanted local second - occur interactions.", "histories": [["v1", "Tue, 10 Jun 2014 04:34:43 GMT  (1827kb,D)", "http://arxiv.org/abs/1406.2419v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hilton bristow", "simon lucey"], "accepted": false, "id": "1406.2419"}, "pdf": {"name": "1406.2419.pdf", "metadata": {"source": "META", "title": "Why do linear SVMs trained on HOG features perform so well?", "authors": ["Hilton Bristow", "Simon Lucey"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Despite visual object detectors improving by leaps and bounds in the last decade, they still largely rely on the same underlying principles: linear Support Vector Machines (SVMs) trained on Histogram of Oriented Gradient (HOG) features. When HOG features were introduced, they improved upon existing methods for pedestrian detection by an order of magnitude [5]. Since then, the HOG-SVM pipeline has been used in the deformable parts model of [9], its derivatives, and high achievers of the PASCAL VOC challenge. A keyword search for \u201cHOG\u201d and \u201cSVM\u201d in the PASCAL 2012 results page returns 25 and 18 hits respectively. HOG is now complicit in detection pipelines across almost every visual detection/classification task [5, 7, 20, 21].\nIn this paper we peel back the layers of complexity from HOG features to understand what underlying interactions drive good performance. In particular,\n\u2022 HOG features can be viewed as an affine weighting on the margin of a quadratic kernel SVM, \u2022 underlying this prior and added capacity is the preservation of local pixel interactions and secondorder statistics, \u2022 using these foundational components alone, we show it is possible to learn a high performing classifier, with no further assumptions on images, edges or filters."}, {"heading": "2. Representing HOG features as a linear", "text": "transform of pixels\nHOG features can be described as taking a nonlinear function of the edge orientations in an image and pooling them into small spatial regions to remove sensitivity to exact localisation of the edges. A pictorial representation of this pipeline is shown in Figure 1. This type of representation has proven particularly successful at being tolerant to non-rigid changes in object geometry whilst maintaining high selectivity [8].\nThe exact choice of non-linear function is discretionary, however Dalal and Triggs try f(x) = |x| and f(x) = |x|2. Both functions remove the edge direction, leaving only a function of the magnitude. In this paper, we choose to use the square of edge directions. Our choice is motivated by a number of reasons. First, the square function leads to greater flexibility in manipulation, which we show becomes vital in our later reformulation. Second, there are proponents of the squaring function (referred to as \u201csquare\u201d or \u201cL2\u201d pooling) in convolutional network literature [1, 14], whose architectures have a similar pipeline to HOG features per layer, and have shown exciting results on large-scale visual recognition challenges [16]. Finally, the square function has a good basis in statistical models of the primary visual cortex [13].\nFollowing the notation of [3], the transform from pixels to output in HOG features can be written as,\n\u03a6f (x) = Db \u2217 [(gf \u2217 x) (gf \u2217 x)] , (1)\nar X\niv :1\n40 6.\n24 19\nv1 [\ncs .C\nV ]\n1 0\nJu n\n20 14\nwhere a vectorized input image x \u2208 RD is convolved with an oriented edge filter gf [6], rectified through the Hadamard operator (pointwise square), then finally blurred with b and downsampled by the sparse selection matrix D to achieve pooling/histogramming. Performing this operation over a bank of oriented edge filters and concatenating the responses leads to the final descriptor,\n\u03a6(x) = [\u03a61(x) \u03a62(x) . . . \u03a6F (x)] . (2)\nThis reformulation omits the contrast normalization step, which we address later in the piece. We showed previously in [3] that each sub-descriptor can be expressed in the form,\n\u03a6f (x) = DBM(Gf \u2297Gf )(x\u2297 x) , (3)\nwhere M is a selection matrix and capitalized B, G are matrix forms of their convolutional prototypes. The full response to a bank of filters can be written as,\n\u03a6(x) = L(x\u2297 x) , (4)\nwhere the projection matrix L is formed by concatenating the bank,\nL = BM(G1 \u2297G1)... BM(GF \u2297GF )  . (5) Under this reformulation, HOG features can be viewed as an affine weighting of quadratic interactions between pixels in the image. This suggests that the good performance of HOG features is down to two things: (i) second-order interactions between pixels, and (ii) the choice of prior that forms the affine weighting L."}, {"heading": "3. Capacity of the Classifier", "text": "When used in a support vector classification setting, the projection matrix L can be absorbed into the\nmargin,\nw\u2217 = arg min w,\u03bei\u22650\n1 2 wT (LTL)\u22121w + C l\u2211 i=1 \u03bei (6)\nsubject to yiw T (xi \u2297 xi) > 1\u2212 \u03bei, i = 1 . . . l\nleaving as the data term only interactions between the weights w and the Kronecker expansion of the image.\nRemark 1 When the weighting is the identity matrix, ( i.e. L = I), Equation (6) reduces to an SVM with a unary quadratic kernel.\nThe first part of the HOG story, the induced prior, can thus be quantified as being quadratic. The weighting matrix is therefore intrinsically a prior on the margin of a quadratic kernel SVM."}, {"heading": "4. Secord-Order Interactions", "text": "The term (x\u2297 x) in Equation (4) can alternatively be written as,\n(x\u2297 x) = vec(xxT ) , (7)\nwhich is the vectorized covariance matrix of all pixel interactions. [19] showed that the covariance of a local image distribution is often enough to discriminate it from other distributions. However, when dealing with high-dimensional distributions, computing a full-rank covariance matrix is often difficult. [10] circumvent this problem by assuming stationarity of background image statistics (a translated image is still an image), as well as limiting the bandwidth of interactions between pixels. Simoncelli [18] showed that these assumptions are reasonable, since correlations between pixels fall quickly with distance (see Figure 2).\nTo improve conditioning and prevent overfitting the classifier to hallucinated interactions, we consider the most general set of local second-order features: the set of all local unary second-order interactions in an image,\n\u03a8(x) = [vec{S1(x)}T , . . . , vec{SD(x)}T ]T (8)\nwhere,\nSi(x) = Pixx TPTi , (9)\nPi is simply an M \u00d7 D matrix that extracts an M pixel local region centred around the ith pixel of the image x. By retaining local second-order interactions, the feature length grows from D for raw pixels to M2D.\nFortunately, inspection of Equation (8) reveals a large amount of redundant information. This redundancy stems from the re-use of pixel interactions in surrounding local pixel locations. Taking this into account, and without loss of information, one can compact the local second-order feature to MD elements, so that Equation (8) becomes,\n\u03a8\u2217(x) =  (e1 \u2217 x) T \u25e6 xT\n... (eM \u2217 x)T \u25e6 xT  . (10) where {em}Mm=1 is the set of M impulse filters that encode the local interactions in the signal."}, {"heading": "5. Local Second-Order Interactions", "text": "Consider two classes A and B. Class A represents the distribution of all natural images. Class B represents a noise distribution which has the same frequency spectrum as natural images, namely 1f [18]. Both distributions are power normalized. We sample 25000 training and testing examples from each class, and train two classifiers: one preserving the raw pixel information and one preserving local second-order interactions of the pixels. The goal of the classifiers is to predict \u201cnatural\u201d or \u201cnoise.\u201d An illustration of the experimental setup and the results are presented in Figure 3. The pixel classifier fails to discriminate between the two distributions. There is no information in either the spatial or Fourier domain to linearly separate the classes (i.e. the distributions overlap). By preserving local quadratic interactions of the pixels, however,\nthe classifier can discriminate natural from synthetic almost perfectly.\nWhilst the natural image and noise distributions have the same frequency spectra, natural images are not random: they contain structure such as lines, edges and contours. This experiment shows that image structure is inherently local, and more importantly, that local second-order interactions of pixels can exploit this structure. Without encoding an explicit prior on edges, pooling, histogramming or blurring, local quadratic interactions have sufficient capacity to exploit the statistics of natural images, and separate them from noise."}, {"heading": "6. Replacing prior with posterior:", "text": "learning over pixels\nQuadratic kernel SVMs have not historically performed well on recognition tasks when learned using pixel information. The image prior that HOG encodes, and the affine weighting that it can be distilled into, is integral to obtaining good generalisation performance. We know, however, that a prior is simply used to reflect our belief in the posterior distribution in the absence of actual data. In the case of HOG, the prior encodes insensitivity to local non-rigid deformations so that the entire space of deformation does not need to be sampled to make informed decisions.\nThis is usually a reasonable assumption to make, since sampling the posterior sufficiently may be infeasible. Take, for example, the task of pedestrian detection. The full posterior comprises all possible combinations of pose, clothing, lighting, race, gender, identity, background and any other attribute that manifests in a change to the visual appearance of a person. Multiscale sliding window HOG detectors (with pose elasticity in the case of DPMs) work to project out as much of this intra-class variation as possible.\nIs it possible to learn a detector using only the assumptions that underlie HOG features: the preservation of local second-order interactions? How much data\nis required to render the HOG prior unnecessary, and what sort of data is required? Can the data just be perturbations of the training data? Does the resulting classifier learn anything more specialized than one learned on HOG features? In the following section we aim to provide answers to these questions.\nHypothesis 1 If HOG features are doing what we intend - providing tolerance to local geometric misalignment - then it should be possible to reproduce their effects by sampling from a sufficiently large dataset containing geometrically perturbed instances of the original training set.\nLearned Features: It is the firm belief of many that learned features are the way forward [11]. Convolutional network literature has heavily relied on learned features for a number of years already [15, 17]. We take feature learning to its most primitive form, setting only the capacity and distribution of features, and letting the classifier learn the rest."}, {"heading": "7. Methods", "text": "We designed an experiment where we could control the amount of geometric misalignment observed between the training and testing examples. We used the Cohn Kanade+ expression recognition dataset, consisting of 68-point landmark, broad expression and FACS\nlabels across 123 subjects and 593 sequences. Each sequence varies in length and captures the neutral and peak formation of facial expression. In this paper we consider only the task of broad expression classification (i.e. we discard FACS encodings). To test the invariance of different types of features to geometric misalignment, we first register each training example to a canonical pose, then synthesize similarity warps of the examples with increasing RMS point error.\nWhy Faces?: HOG features have been used across a broad range of visual recognition tasks, including object recognition, scene recognition, pose estimation, etc. Faces are unique, however, since they are a heavily studied domain with many datasets containing subjects photographed under controlled lighting and pose conditions, and labelled with ground-truth facial landmarks. This enables a great degree of flexibility in experimental design, since we can programatically set the amount of geometric misalignment observed while controlling for pose, lighting, expression and identity.\nWe synthesize sets with 300, 1500, 15000 and 150000 training examples. The larger the synthesized set, the greater the coverage of geometric variation. We use HOG features according to Felzenszwalb et al . [9] with 18 orientations and a spatial aggregation size of 4. For the reformulation of Equation (2), we use Gabor filters with 18 orientations at 4 scales, and a 4 \u00d7 4 blur kernel. The local quadratic features have a spatial sup-\nport equal to the amount of RMS point error (i.e. at 10 pixels error, correlations are collected over 10 \u00d7 10 regions). All training images are 80 \u00d7 80 pixels and cropped around only the faces. Figure 4 illustrates the degree of geometric misalignment introduced.\nContrast Normalization: So far we have neglected to mention contrast normalization, the final stage in the HOG feature extraction pipeline, and a component that has traditionally received much attention, particularly in neuroscience literature [4]. Whilst contrast normalization is an integral part of HOG features, we do not believe it plays a significant role in their invariance to geometry. Since our model does not explain the highly nonlinear process of contrast normalization, we instead power normalize images in the expression recognition experiment, and pre-whiten images in the pedestrian detection experiment.\nLearning: The storage requirements of local quadratic features quickly explode with increasing geometric error and synthesized examples. At 10 pixels RMS error, 150000 training examples using local quadratic features takes 715 GB of storage. To train on such a large amount of data, we implemented a parallel support vector machine [2] with a dual coordinate descent method as the main solver [12]. Training on a Xeon server using 4 cores and 24 GB of RAM took between 1 \u2013 5 days, depending on problem size. We used multiple machines to grid search parameters and run different problem sizes.\nFigure 5 shows a breakdown of the results for synthesized sets of geometric variation. Pixels (shown in shades of green) perform consistently poorly, even with large amounts of data. HOG features (in blue, and reformulation in aqua) consistently perform well. The performance of HOG saturates after just 1500 training examples. [23] talk about the saturation of HOG at length, noting that more data sometimes decreases its performance.\nLocal quadratic features (shown in red) have a marked improvement in performance with increasing amounts of data (roughly 10% per order of magnitude\nof training data). Synthesizing variation used to be quite popular in some vision circles, particularly face recognition through the use of AAMs [22], however it seems to have gone out of fashion in object recognition. Our results suggest that a model with sufficient capacity could benefit from synthesized data, even on general object recognition tasks (see Pedestrian Detection).\nOnly when the dataset contains \u2265 100000 examples do the local quadratic features begin to model non-trivial correlations correctly. With 150000 training samples, local quadratic features perform within 3% of HOG features.\nPedestrian Detection: We close with an example showing how the ideas of locality and second-order interactions can be used to learn a pedestrian detector. We don\u2019t intend to outperform HOG features. Instead we show that our insights are important for good performance, in contrast with a pixel-based classifier.\nWe follow a similar setup to our earlier expression recognition experiment on INRIA person. We generate synthetic similarity warps of each image, making sure they remain aligned with respect to translation. Figure 6 illustrates how the addition of synthesized examples does not change the dataset mean appreciably (misalignment would manifest as blur). We train the SVM on 40,000 positive examples and 80,000 negative examples, without hard negative mining.\nThe results are striking. Unsurprisingly, the pixelbased classifier has high detection error, whilst the HOG classifier performs well. The local-quadratic classifier falls between the two, with an equal error rate of 22%. The improved performance can be attributed solely to the added classifier capacity and its ties to correlations within natural image statistics."}, {"heading": "8. Discussion", "text": "We began the piece on the premise that HOG features embody interactions requisite to good recognition performance. Their popularity and success on current object recognition benchmarks supports this, at least empirically. Expanding on the work of [3], we showed that HOG features can be reformulated as an affine weighting on the margin of a quadratic kernel SVM. One can take away two messages from this: (i) a quadratic classifier has sufficient capacity to enable discrimination of visual object classes, and (ii) the actual image prior is captured by the weighting matrix. We performed an experiment where classifiers were tasked with discriminating between \u201cnatural\u201d and \u201cnoise\u201d images, and found that the quadratic classifier preserving only local pixel interactions was able to separate the two classes, suggesting that the structure of natural im-\nages can be exploited by local second-order statistics. Armed with only these principles, we set out to discover whether it was possible to learn an accurate classifier in a controlled expression recognition setting, and quantify how much data was required for varying amounts of geometric misalignment. Figure 5 illustrates that with enough synthesized data, a local quadratic classifier can learn non-trivial pixel interactions necessary for predicting expressions. Finally, we applied these insights to a pedestrian detection task, and show in Figure 7 that a significant fraction of HOG\u2019s performance can be attributed to preserving local second-order pixels interactions, and not the image specific prior (i.e. edges) that it encodes. Inspecting the local quadratic classifier visualization from Figure 8, one can see that emphasis (strong positive support weights represented by lighter shades) is placed on the object boundaries - specifically around the head and shoulders and between the legs - just as HOG does."}, {"heading": "9. Conclusions", "text": "Linear SVMs trained on HOG features have pervaded many visual perception tasks. We hypothesized that preserving local second-order interactions are at the heart of their success. This is motivated by similar findings within the human visual system. With these simple assumptions combined with large amounts of training data, it is possible to learn a classifier that performs well on a constrained expression recognition task, and within ballpark figures of a HOG-based classifier tailored specifically to an image prior on a pedestrian detection experiment. As the size of datasets continues to grow, we will be able to rely less and less on prior assumptions, and instead let data drive the models we use. Local second-order interactions are one of the simplest encoders of natural image statistics that ensure such models have the capacity to make informed predictions."}], "references": [{"title": "Quadratic polynomials learn better image features", "author": ["J. Bergstra", "G. Desjardins", "P. Lamblin", "Y. Bengio"], "venue": "Technical report, Universite de Montreal,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers", "author": ["S. Boyd"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "V1-Inspired features induce a weighted margin in SVMs", "author": ["H. Bristow", "S. Lucey"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Normalization as a canonical neural computation", "author": ["M. Carandini", "D.J. Heeger"], "venue": "Nature reviews. Neuroscience,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Uncertainty Relation for Resolution in Space, Spatial Frequency, and Orientation Optimized by Two-Dimensional Visual Cortical Filters", "author": ["J. Daugman"], "venue": "Optical Society of America,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1985}, {"title": "Face recognition using Histograms of Oriented Gradients", "author": ["O. D\u00e9niz", "G. Bueno", "J. Salido", "F. De la Torre"], "venue": "Pattern Recognition Letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "How does the brain solve visual object recognition", "author": ["J.J. DiCarlo", "D. Zoccolan", "N.C. Rust"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "Pattern Analysis and Machine Learning (PAMI),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Discriminative decorrelation for clustering and classification", "author": ["B. Hariharan", "J. Malik", "D. Ramanan"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Learning multiple layers of representation", "author": ["G.E. Hinton"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["C.-J. Hsieh", "K.-W. Chang", "C.-J. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Complex cell pooling and the statistics of natural", "author": ["A. Hyv\u00e4rinen", "U. K\u00f6ster"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Learning invariant features through topographic filter maps. Computer Vision and Pattern Recognition", "author": ["K. Kavukcuoglu", "M. Ranzato", "R. Fergus"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.-l. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Natural Image Statistics and Neural Representation", "author": ["E. Simoncelli", "B. Olshausen"], "venue": "Annual Review of Neuroscience,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Region covariance: A fast descriptor for detection and classification", "author": ["O. Tuzel", "F. Porikli", "P. Meer"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Sun database: Largescale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K. Ehinger"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Articulated pose estimation with flexible mixtures-of-parts", "author": ["Y. Yang", "D. Ramanan"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Face recognition across pose: A review", "author": ["X. Zhang", "Y. Gao"], "venue": "Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Do We Need More Training Data or Better Models for Object Detection", "author": ["X. Zhu", "C. Vondrick", "D. Ramanan", "C. Fowlkes"], "venue": "British Machine Vision Conference (BMVC),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "When HOG features were introduced, they improved upon existing methods for pedestrian detection by an order of magnitude [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 8, "context": "Since then, the HOG-SVM pipeline has been used in the deformable parts model of [9], its derivatives, and high achievers of the PASCAL VOC challenge.", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "HOG is now complicit in detection pipelines across almost every visual detection/classification task [5, 7, 20, 21].", "startOffset": 101, "endOffset": 115}, {"referenceID": 6, "context": "HOG is now complicit in detection pipelines across almost every visual detection/classification task [5, 7, 20, 21].", "startOffset": 101, "endOffset": 115}, {"referenceID": 19, "context": "HOG is now complicit in detection pipelines across almost every visual detection/classification task [5, 7, 20, 21].", "startOffset": 101, "endOffset": 115}, {"referenceID": 20, "context": "HOG is now complicit in detection pipelines across almost every visual detection/classification task [5, 7, 20, 21].", "startOffset": 101, "endOffset": 115}, {"referenceID": 7, "context": "This type of representation has proven particularly successful at being tolerant to non-rigid changes in object geometry whilst maintaining high selectivity [8].", "startOffset": 157, "endOffset": 160}, {"referenceID": 0, "context": "Second, there are proponents of the squaring function (referred to as \u201csquare\u201d or \u201cL2\u201d pooling) in convolutional network literature [1, 14], whose architectures have a similar pipeline to HOG features per layer, and have shown exciting results on large-scale visual recognition challenges [16].", "startOffset": 132, "endOffset": 139}, {"referenceID": 13, "context": "Second, there are proponents of the squaring function (referred to as \u201csquare\u201d or \u201cL2\u201d pooling) in convolutional network literature [1, 14], whose architectures have a similar pipeline to HOG features per layer, and have shown exciting results on large-scale visual recognition challenges [16].", "startOffset": 132, "endOffset": 139}, {"referenceID": 15, "context": "Second, there are proponents of the squaring function (referred to as \u201csquare\u201d or \u201cL2\u201d pooling) in convolutional network literature [1, 14], whose architectures have a similar pipeline to HOG features per layer, and have shown exciting results on large-scale visual recognition challenges [16].", "startOffset": 289, "endOffset": 293}, {"referenceID": 12, "context": "Finally, the square function has a good basis in statistical models of the primary visual cortex [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 2, "context": "Following the notation of [3], the transform from pixels to output in HOG features can be written as,", "startOffset": 26, "endOffset": 29}, {"referenceID": 5, "context": "where a vectorized input image x \u2208 R is convolved with an oriented edge filter gf [6], rectified through the Hadamard operator (pointwise square), then finally blurred with b and downsampled by the sparse selection matrix D to achieve pooling/histogramming.", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "We showed previously in [3] that each sub-descriptor can be expressed in the form,", "startOffset": 24, "endOffset": 27}, {"referenceID": 18, "context": "[19] showed that the covariance of a local image distribution is often enough to discriminate it from other distributions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] circumvent this problem by assuming stationarity of background image statistics (a translated image is still an image), as well as limiting the bandwidth of interactions between pixels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Simoncelli [18] showed that these assumptions are reasonable, since correlations between pixels fall quickly with distance (see Figure 2).", "startOffset": 11, "endOffset": 15}, {"referenceID": 17, "context": "Class B represents a noise distribution which has the same frequency spectrum as natural images, namely 1 f [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "Learned Features: It is the firm belief of many that learned features are the way forward [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "Convolutional network literature has heavily relied on learned features for a number of years already [15, 17].", "startOffset": 102, "endOffset": 110}, {"referenceID": 16, "context": "Convolutional network literature has heavily relied on learned features for a number of years already [15, 17].", "startOffset": 102, "endOffset": 110}, {"referenceID": 8, "context": "[9] with 18 orientations and a spatial aggregation size of 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Contrast Normalization: So far we have neglected to mention contrast normalization, the final stage in the HOG feature extraction pipeline, and a component that has traditionally received much attention, particularly in neuroscience literature [4].", "startOffset": 244, "endOffset": 247}, {"referenceID": 1, "context": "To train on such a large amount of data, we implemented a parallel support vector machine [2] with a dual coordinate descent method as the main solver [12].", "startOffset": 90, "endOffset": 93}, {"referenceID": 11, "context": "To train on such a large amount of data, we implemented a parallel support vector machine [2] with a dual coordinate descent method as the main solver [12].", "startOffset": 151, "endOffset": 155}, {"referenceID": 22, "context": "[23] talk about the saturation of HOG at length, noting that more data sometimes decreases its performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Synthesizing variation used to be quite popular in some vision circles, particularly face recognition through the use of AAMs [22], however it seems to have gone out of fashion in object recognition.", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "Expanding on the work of [3], we showed that HOG features can be reformulated as an affine weighting on the margin of a quadratic kernel SVM.", "startOffset": 25, "endOffset": 28}], "year": 2014, "abstractText": "Linear Support Vector Machines trained on HOG features are now a de facto standard across many visual perception tasks. Their popularisation can largely be attributed to the step-change in performance they brought to pedestrian detection, and their subsequent successes in deformable parts models. This paper explores the interactions that make the HOG-SVM symbiosis perform so well. By connecting the feature extraction and learning processes rather than treating them as disparate plugins, we show that HOG features can be viewed as doing two things: (i) inducing capacity in, and (ii) adding prior to a linear SVM trained on pixels. From this perspective, preserving second-order statistics and locality of interactions are key to good performance. We demonstrate surprising accuracy on expression recognition and pedestrian detection tasks, by assuming only the importance of preserving such local second-order interactions.", "creator": "LaTeX"}}}