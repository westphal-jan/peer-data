{"id": "1406.3474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2014", "title": "Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network", "abstract": "we show then effective multi - task automated behavior enhancing human pose estimation from monocular image with deep structural complex network. in reality, we simultaneously report a pose - joint clusters and a see - window body - part cluster in a deep network architecture. our show that including sensitive body - part assembly data helps to enhance the network, shifting patterns to others as a coordinated pattern. we report rigorous and state - of - art optimization on network specific components. addison also empirically verified that mechanically learned neurons in the middle point of our network appropriately tuned alongside localized body types.", "histories": [["v1", "Fri, 13 Jun 2014 10:11:18 GMT  (3676kb,D)", "http://arxiv.org/abs/1406.3474v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["sijin li", "zhi-qiang liu", "antoni b chan"], "accepted": false, "id": "1406.3474"}, "pdf": {"name": "1406.3474.pdf", "metadata": {"source": "CRF", "title": "Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network", "authors": ["Sijin Li", "Zhi-Qiang Liu", "Antoni B. Chan"], "emails": ["sijin.li@my.cityu.edu.hk", "SMZLIU@cityu.edu.hk", "abchan@cityu.edu.hk"], "sections": [{"heading": null, "text": "work for human pose estimation from monocular image with deep convolutional neural network. In particular, we simultaneously learn a pose-joint regressor and a slidingwindow body-part detector in a deep network architecture. We show that including the body-part detection task helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several data sets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts."}, {"heading": "1. Introduction", "text": "Human pose estimation is a popular research topic in computer vision for its wide potential in many applications, such as video games, gesture control, action understanding, pose retrieval. Human pose estimation from depth images is much more mature than estimation from 2D image. Some algorithms [21] based on depth maps have already been used in practice. However the majority of visual media are in 2D format, and most mobile devices are only equipped with 2D camera. Therefore, it is very useful to estimate human pose from 2D image.\n2D pose estimation from images is more difficult than estimation from depth maps due to ambiguities of appearance and self-occlusion. In general, human pose estimation approaches can be classified into two types: methods based on part-based graphical models, and methods based on regression. In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27]. One popular graphical model for human pose estimation is the pictorial structure model [10] (PSM), which uses pairwise connections\nbetween parts to form a tree. Exact inference is possible and the solution is guaranteed to be globally optimal [10], but the inference is still very expensive for real-time applications. In general, there are two definitions of parts, namely using joints as parts and using limbs as parts. Using joint points as parts avoids the need to predict the orientation of parts, although appearances around joints are more ambiguous.\nFor both definitions of parts, the appearance model is critical for learning a good PSM [3, 7]. Simple appearance models using linear filters are not capable of capturing the parts\u2019 appearances, while complicated features are expensive to evaluate at each sliding window. Several methods have been proposed to alleviate this problem by truncating the pose space [7, 20]. On the other hand, [28] extends the traditional PSM by allowing each body part to have multiple modes. Also, multimodal models, such as mixtures of PSM or hierarchical PSM [6, 13, 18, 19], have been proposed. The computation complexity increases rapidly along with the number of modes.\nIn the second approach, pose estimation is viewed as a regression task [2]. These methods train their model to learn a mapping between feature space and pose space. A good feature that encodes pose information is more critical for these methods. Currently, these approaches can only handle small amounts of training data, since calculating a prediction requires solving an expensive optimization problem.\nIn recent years, deep neural network architectures have achieved success in many computer vision tasks [9, 14, 22]. Convolutional neural networks (CNN) are one of the most popular architectures used in computer vision problems because of their reduced number of parameters compared to fully connected models and intuitive structure, which allows the network to learn translation invariant features. In addition, convolution is a \u201ccheap\u201d operation that can be computed efficiently. However, because of the larger capacity (i.e., more parameters) of a deep neural network, it is hard to train a network that generalizes well with limited data.\nIn this paper, we propose a heterogeneous multi-task\n1\nar X\niv :1\n40 6.\n34 74\nv1 [\ncs .C\nV ]\n1 3\nJu n\n20 14\nframework for human pose estimation using a deep convolutional neural network. We frame pose estimation as a regression task, while also defining several accessory tasks to guide the network to learn useful features for pose estimation. In particular, these accessory tasks are sliding window detectors for different body-parts. In our framework, the heterogeneous tasks (regression and detection) are trained simultaneously, and we show that the regression network benefits greatly from the accessory detection tasks, and converges to much better local minima than the network trained with only regression tasks. We also empirically show that the activation patterns of neurons in the middle layers preserve location information and are selective to localized body-part shapes."}, {"heading": "2. Related work", "text": "Multi-task learning is typically applied when multiple tasks resemble each other and training data for each task is limited [8, 26, 29]. We refer reader to [8, 29] for a review. In the following, we will briefly compare with previous multi-task approaches and regression networks that are most related to our work.\nIn [26], a heterogeneous multi-task model is trained by encouraging the parameters for the regression task and the classification task to share the same sparsity pattern. They found that joint-training tends to find the most useful features in the input for both tasks. Instead of sharing a sparsity pattern, our framework forces the heterogeneous tasks to share the same feature layers, which results in learning shared feature representation that is good for both tasks.\nIn [9], a deep convolutional network is trained for scene labeling, by defining a multi-category classification task for each pixel. Instead, we define our detection tasks over sliding windows in the image. Since we allow each window to contain multiple body parts, each detection task is essentially a binary classification task in a window.\n[23] trains a deep CNN to learn a pose-sensitive embedding with nonlinear NCA (neighbourhood components analysis) regression, and predicts the location of the head and hands by finding the nearest neighbor with the learned embedding features. In contrast to [23], we introduce accessory tasks for learning shared \u201cpose features\u201d, and output the joint locations directly from the regression network.\nIn [22], a multi-stage system with deep convolutional networks is built for predicting facial point locations. In order to embed a structure prior of the face, they use a set of neural networks that focus on different regions of the input image.\nSimilarly, [24] trained cascaded convolutional networks for human pose estimation. Instead of increasing the number of stages for refinement, here we explore how to improve the performance of a single regression network by introducing accessory tasks. Our multi-task strategy could\nalso be used in conjunction with the multi-stage strategy. In [25] semi-supervised learning is used to guide the network to learn an internal representation that reflects the similarity between training samples. The authors propose that the unsupervised network can either share layers with a supervised network, or serve as an input into the supervised network. In contrast, we design multiple classification tasks for body parts detection at different location, while all the tasks share the same learned feature space.\nFinally, in order to investigate the feature representation learned by the neural network, [15] estimates the \u201coptimal\u201d input that maximizes the activation of a selected neuron, and find that the \u201coptimal\u201d input resembles a human face. In contrast to [15], we visualize a feature by averaging image patches that are associated with the neurons with maximum responses in an upper-layer, and obtain similar results."}, {"heading": "3. Heterogeneous Multi-task Learning", "text": "Our heterogeneous multi-task framework consists of two types of tasks: 1) a pose regression task, where the aim is to predict the locations of human body joints in an image; 2) a set of body-part detection tasks, where the goal is to classify whether a window in the image contains the specific body part. In the following, we assume that a bounding box around the human has already been provided, e.g., using an upper body detector [1]. All the coordinates are with respect to the bounding box containing the human. Our framework is summarized in Figure 1."}, {"heading": "3.1. Joint point regression", "text": "The regression task is to predict the location of joint points for each human body part. The coordinates of each joint point are taken as the target values. We normalize all the coordinates with the size of bounding box so that their values will be in range of [0, 1]. We use the squared-error as the cost function for our regression task,\nEr(J\u0302i, Ji) = \u2016Ji \u2212 J\u0302i\u201622, (1)\nwhere Ji and J\u0302i are the ground truth and predicted positions for the i-th joint, respectively."}, {"heading": "3.2. Body part detection", "text": "For the body part detection tasks, the goal is to determine whether a given window in the image contains a specific body part. Let P be the total number of body parts, and let L be the number of overlapping windows inside the bounding box. For the p-th body part, we train L classifiers, namely Cp,1, ..., Cp,L, to determine whether the l-th window contains body part p. Note that we train a separate classifier for each location L, which allows the part detector to learn a location-specific appearance for the part, as well as location-specific contextual information with other parts. For example, a lower arm in the upper corner of the bounding box will more likely be vertical or diagonal.\nIn our training set, the annotated body parts are represented as sticks. Hence, to train the body-part detectors, we need to first identify the windows in the training set that contain each body part. A window is considered to contain a body part if the portion of the body part inside the window is at least a particular length, relative to the total length of the part. Specifically, we use the following formula to convert the stick annotation of body part p into a binary label indicating its presence/absence in the l-th window,\nyp,l = { 1, if len(windowl \u2229 stickp) > \u03b2 \u00b7 len(stickp) 0, otherwise,\n(2)\nwhere stickp is the segment of the p-th body part, and windowl \u2229 stickp is the portion of stickp inside windowl. \u03b2 is a fixed threshold, which we empirically set \u03b2 = 0.3 in all of our experiments. Finally, calculating the binary indicator yp,l for each window l, results in a binary indicator map for part p. Figure 2 shows an example converting the upper-arm annotation into an indicator map. Note that we\nallow multiple body parts to appear in the same window, and also allow one body part to appear in several windows.\nFor each detection task for part p and window l, we minimize the cross-entropy error function,\nEd(y\u0302p,l, yp,l) = \u2212yp,l log(y\u0302p,l)\u2212 (1\u2212 yp,l) log(1\u2212 y\u0302p,l), (3)\nwhere yp,l is the ground-truth label, and y\u0302p,l is the corresponding detection probability from the classifier."}, {"heading": "3.3. Global cost function", "text": "Our global cost function is the linear combination of the regression cost function for all joints and the detection cost function for all parts and windows, over all training images,\n\u03a6 = \u03bbr \u2211 t \u2211 i Er(J\u0302 (t) i , J (t) i ) + \u03bbd \u2211 t \u2211 p \u2211 l Ed(y\u0302 (t) p,l , y (t) p,l),\n(4)\nwhere \u03bbr and \u03bbd are the weights for regression and detection tasks, respectively, and the superscript (t) indicates the index of the training image."}, {"heading": "3.4. Network Structure", "text": "The design of our network is based on the following considerations:\n\u2022 Low level feature sharing: We allow the detection tasks and regression tasks to share the same learned feature representation. This is motivated by the following two reasons. First, features learned for the detection task should also be helpful for identifying parts or joints in the regression task. Second, feature sharing will reduce the number of parameters and encourage the network to generalize on a larger range of samples.\n\u2022 Preservation of location information: The detection task is to determine whether a local window contains\nthe specific body part, while the regression task is to predict the coordinates of the joint position. Hence, the features extracted from the lower layers should not be translation invariant, i.e., the positions of the features should be preserved in the feature map. \u2022 Integration of context information: Sometimes it\nis difficult to distinguish different body parts by only looking at the bounding box of the body parts. For example, when wearing long-sleeves, the upper arm and lower arm can have very similar appearance, and hence it is hard to distinguish them by only looking at the windows containing these two parts. Including context information about neighboring parts can help to improve the part detector. Hence, the input for each local part detector is the whole bounding box image (the whole human). Our network structure is shown in Figure 3. The input is an RGB image with human. The first 6 hidden layers are shared by both regression and detection tasks. In the shared layers, we only use convolutional layers and pooling layers to ensure the activation of neurons are affected by only local patterns in the input. We also choose to use a small filter and stride size to keep more location information.\nEach convolutional layer consists of several maps. Filter weights are shared within each map, which means the neurons within the same map are sensitive to the same patterns at different location in the previous layer. Neurons at the same position (but belonging to different maps) will always contribute to the same unit in the next layer. The max-pooling layer is added after each convolutional layer to increase non-linearity and to integrate local information.\nThe value of neuron i in a convolutional layer or regression layer is calculated by\nv(i) = fact( \u2211 j\u2208Ri wi,jv(j)), (5)\nwhereRi is the set of neurons from which neuron i receives input, wi,j is the weight between neuron i and neuron j, and\nfact is the activation function of that layer. Most of the neurons in our network are Rectified Linear Units (ReLu) [17], where fact(x) = max(0, x). [17] showed that ReLus are good for recognition tasks and fast to train. We use the hyperbolic tangent as the activation function in the last layer of the regression task, and the logistic function in all the last layer of detection tasks."}, {"heading": "3.5. Training", "text": "We jointly train the regression and detection networks with the global cost function in (4). We use backpropagation [16] to update the weights. Given a training image, predictions for both tasks are calculated, and the corresponding gradients are back-propagated through the network. For layers with several output layers, the gradient from their output layers are summed together for weight updating. \u201cDropout\u201d [12] is also used in the first fully connected layers for the regression and detection tasks to prevent over-fitting. The dropout probability is set to be 0.5 in the experiments. In each iteration, the neurons in dropout layers will be randomly selected with probability 0.5 to forward their activation to the output units, and only the selected neurons will participate in the back-propagation during this iteration. In the testing stage, all the neurons are used for prediction with their activation value multiplied by 0.5 for normalization. This strategy turns out to be very effective, since without \u201cdropout\u201d, our network will severely overfit. We refer reader to [14] for more details about the training procedure."}, {"heading": "4. Experiments", "text": "We present experiments using our method HMLPE (heterogeneous multi-task learning for pose estimation)."}, {"heading": "4.1. Training data", "text": "We collect training data from several data sets, including Buffy Stickmen [7], ETHZ Stickmen [4], Leed Sport Pose (LSP [13]), Synchronic Activities Stickmen (SA [6]),\nFrames Labeled In Cinema (FLIC [19]), We Are Family(WAF) [5]. For Buffy, LSP, FLIC we only use their respective training sets, while we use the whole ETHZ, SA, and WAF datasets for training. In total, we have collected 8427 images for training.\nWe represent the human body with a set of joints, and use the segments between those joints to represent body parts. For data sets with only stick labels, we use the nearest end of stick or average of nearest ends as the joint point. We define 8 joints (nose, neck, left and right shoulders, left and right elbows, and left and right wrists), and 7 body parts (head, left and right shoulder, left and right upper arms, and left and right lower arms). Since Buffy, ETHZ, SA, WAF only provide the upper-end and lower-end of the head, we use the middle point as the nose position. We illustrate our parts and joints definition in Figure 2.\nBounding boxes for the training images are generated according to the ground-truth labels. We select a bounding box for each training image that contains all the annotated body parts, and then resize the image inside the bounding box to 128 \u00d7 128. We then augment the dataset by randomly selecting 16 bounding box of size 112 \u00d7 112 inside the extracted human image, and apply a mirror transformation to double the training set. In total, the training set is augmented by a factor of 32.\nIn the current experiments, images with occluded body parts are removed, although our framework could be extended to handle training poses with occlusion."}, {"heading": "4.2. Experiment setup", "text": "For our HMLPE, the pose regression task predicts 8 joint positions (16 outputs in total), and the detection task has 7 body parts. For the detection task in HMLPE, we use 64 local windows of equal size uniformly distributed in the bounding box. The window size is set to 30 \u00d7 30 in all experiments, which is comparable to the size of a body parts found in the training set. We pre-train the network using the training data discussed in the previous section, in order to obtain an initial network. Then, we use the initial network as the starting point for training the network using the training data of a specific dataset, either Buffy or FLIC. The initial network serves as a prior to help regularize the network. We train and evaluate our network on a Dell T3400 with GTX 770 4G. Training the network takes 1 to 2 days, while the evaluation for 4000 images takes 5-6 seconds."}, {"heading": "4.3. Evaluation on Buffy Set", "text": "We use the same upper body detector as [7, 11]. In order to get the human bounding box, the width and height of the upper body detection windows are scaled by a fixed factor (swidth = 1.7, sheight = 4.2), which were empirically set according to the training set. The scaled detection window is used as the human bounding box, and the image\nis cropped and resized to 112\u00d7 112. We use Percentage of Correct Part (PCP) to measure the accuracy of pose estimation. As pointed out in [11], the previous PCP evaluation measure does not compute PCP correctly. We use the evaluation tool provided by [11] to calculate the corrected PCP, where an estimated body part with end points (e1, e2) is considered as correct if\n\u2016e1 \u2212 g1\u20162 \u2264 \u03b1 \u00b7 L and \u2016e2 \u2212 g2\u20162 \u2264 \u03b1 \u00b7 L or\n\u2016e2 \u2212 g1\u20162 \u2264 \u03b1 \u00b7 L and \u2016e1 \u2212 g2\u20162 \u2264 \u03b1 \u00b7 L (6)\nwhere (g1, g2) and L are ground truth position and length of the part, and \u03b1 is the parameter for PCP. We use the standard value of \u03b1 = 0.5.\nTable 1 presents the PCP results of lower and upper arms (since we have different definitions of torso and head parts, we do not show the evaluation here). On the whole Buffy test set (276 images), HMLPE achieves better results than [7, 11] on the more difficult parts, lower arms (4.8% improvement), but gets a slightly worse result than [11] on upper arms (1.1% lower) .\nEvaluation on the whole Buffy test set includes errors due to mis-detection of the upper body. To investigate the pose estimation performance alone, we also present results on the subset of the Buffy test set where the upper body detector predicts the correct bounding box. In this case, HMLPE achieves slightly better results than [28] (0.7% better on lower arms and 0.5% better on upper arms).\nWe also run the code from [28] on our full training set, using different number of components per part (denoted as M = {6, 9, 12}). The default setting of M = 6 gets worse results than the model trained with only the Buffy training set, most likely because the full training set contains more variance in poses. Increasing the number of components improves the accuracy, but at an increased cost of training, e.g., 4 days were needed to train the M = 9 model. Using M = 12, [28] has better PCP (4%) on the lower arms compared to HMLPE. On the other hand, HMLPE has better PCP (4.5%) than [28] on upper arms."}, {"heading": "4.4. Evaluation on FLIC Data set", "text": "Next we evaluate on the FLIC test set. We use the same torso box as [19] with scale factors (swidth = 3.5, sheight = 4.5) set empirically from the training set. [19] uses the following accuracy to evaluate their performance,\naccJi(r) = 100\nNsample Nsample\u2211 t=1 1\n( 100 \u00b7 \u2016J (t)i \u2212 J\u0302 (t) i \u20162\n\u2016J (t)lhip \u2212 J (t) rsho\u20162\n\u2264 r ) .\n(7) where J (t)i and J\u0302 (t) i are the ground truth annotation and predicted position for the i-th joint point of test image t. Since [19] compares their methods with several previous approaches, and show that their model performs the best under this criteria, we only compare with [19]. The accuracy results are shown in Figure 4. HMLPE has better accuracy with a looser criteria (larger r); for r = 20, the accuracy of HLMPE on wrists and elbows is about 6% higher than MODEC. On the other hand, HLMPE has worse accuracy with a strict criteria (when r = 6, HMLPE is about 7% and 5% lower than MODEC on wrists and elbows). These results suggest that HLMPE can robustly estimate the general pose, but is less accurate at estimating the exact location of each joint. Also, we have trained MODEC on our full training set, but did not observe any improvement.\nIn addition, we measure PCP on the FLIC dataset to facilitate future comparisons (see Table 2)."}, {"heading": "4.5. Effect of multi-task training", "text": "Next we study the effect of multi-task training, i.e., the joint learning of the regression and detection tasks. We set\ndifferent values for the weights of the regression and detection tasks. All parameters except the weights on the cost function are kept the same. We show training and testing error in Figure 5 and in Table 3.\nFirstly, the network with only the regression task performs poorly on both the training and testing sets.1 Even using tiny weights on the detection tasks help to improve the convergence, leading to a significant performance increase. Within a certain range, increasing weights on the detection tasks leads to lower errors on the test set. For larger weights on the detection tasks, the performance decreases. This is reasonable since the gradient will be dominated by detection task in this case.\nThese results suggest that the regression task benefits greatly from the feature representation induced by the detection tasks. The gradients from detection tasks not only guide the network to converge to a better minimum on the training set, but also help to enhance the generalization. Although the network needs to learn 7*8*8 detectors from limited training data, sharing features among the detection and regression tasks seems to be an effective way for learning useful features for both tasks."}, {"heading": "5. Visualization of features", "text": "In this section, we investigate the features learned by the network. Since the first convolutional layer operates on the input image, the filter response can reflect what low-level patterns in the image to which the neurons are sensitive. The learned filters are in Fig. 6a, and as expected, they look like edge or gradient detectors for different orientations.\nFor the 2nd and 3rd layers (mid- and high-level features), we use a different approach than [15], which finds the input that maximizes one specific neuron. Instead, we use the property that our network is only locally connected in the first 6 layers. That is, the activation of some neurons in the middle layers are only affected by a sub-region of the input image. In addition, the connection is regular, we can backtrack through the network to find the region of the image from which a neuron received its input. We present the backtracking algorithm in Algorithm 1. Since filter weights are shared within the same feature map, neurons in the same map are \u201cexpecting\u201d the same local patterns in the previous layer. Based on these properties, we consider the activation of one feature map at a time. Instead of solving an\n1Training the network with different initializations gave similar results.\npose regression task part detection tasks training error test error training error test error\noptimization problem, we select the patches in the original image that contribute to the maximum activation in one feature map. Figure 7 shows the backtracked patches on a Buffy test image for different features in the 3rd convolutional layer. Surprisingly, we find some feature maps work like body part detectors \u2014 the maximal activation in some maps occurs more frequently on neurons that take inputs from region of body parts, such as head, shoulders and arms.\nTo visualize the feature of a map, we average all its corresponding backtracked patches from all training images. The average backtracked patches for each map in the 2nd and 3rd convolutional layers are shown in Figure 6b and 6c. The average backtracked patches show more clear patterns of body parts like head, shoulder, upper arms. In particular, the visualizations of the mid-level features in Fig. 6b look like body part detectors, such as head (feature 1), neck (feature 9), arms (feature 5), and shoulders (feature 14). Similarly, the high-level features in Fig. 6c look like localized body parts, e.g., heads in different positions (features 2, 3, and 11), left and right shoulders (features 1 and 10), and arms (features 6, 9, and 15). There are also a few highlevel features that do not correspond to specific body parts. For example, feature 8 in Fig. 6c has two horizontal bands of color, and appears to respond to horizontal background structures, such as windows and the tops of door frames (see Fig. 7). This feature could be useful for identifying context information, such as the location of the top of the door relative to the top of the head."}, {"heading": "6. Conclusion", "text": "In this paper, we have proposed a heterogeneous multitask learning framework with deep convolutional neural\nnetwork for human pose estimation. Our framework consists of two tasks: pose regression and body-part detection via sliding-window classifiers. We empirically show that jointly training pose regression with the detection tasks guides the network to learn meaningful features for pose estimation, and makes the network generalize well on testing data. Finally, we visualize the mid- and high-level features using the average of backtracked patches from the maximally responding neurons. We found that these neurons are selective to shape patterns resembling localized human body parts.\nIn future work, we will extend our network for learning poses with occlusion, and combine our framework with unsupervised learning for pre-training the network. In addition, we would like to extend our framework for estimating human pose from video sequences, as well as other structured objects.\nAcknowledgements This work was supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (CityU 123212, CityU 118810, and CityU 119313).\nAlgorithm 1 Backtracked patches Require: layer list, R = (mx,my,mx,my) {(mx,my) are the location of maximum activation} for l in reversed(layer list) do Rlx \u2190 Rlx \u00b7 l.stride Rly \u2190 Rly \u00b7 l.stride Rux \u2190 Rux \u00b7 l.stride+ l.filter size\u2212 1 Ruy \u2190 Ruy \u00b7 l.stride+ l.filter size\u2212 1\nend for"}], "references": [{"title": "Twin gaussian processes for structured prediction", "author": ["L. Bo", "C. Sminchisescu"], "venue": "Int. J. Comput. Vision,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Human pose estimation from still images using body parts dependent joint regressors", "author": ["M. Dantone", "J. Gall", "C. Leistner", "L. van Gool"], "venue": "In CVPR. IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Better appearance models for pictorial structures", "author": ["M. Eichner", "V. Ferrari"], "venue": "In BMVC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "We are family: Joint pose estimation of multiple persons", "author": ["M. Eichner", "V. Ferrari"], "venue": "In ECCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Human pose co-estimation and applications", "author": ["M. Eichner", "V. Ferrari"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "2d articulated human pose estimation and retrieval in (almost) unconstrained still images", "author": ["M. Eichner", "M. Marin-Jimenez", "A. Zisserman", "V. Ferrari"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE TPAMI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Pictorial structures for object recognition", "author": ["P.F. Felzenszwalb", "Huttenlocher", "D.P"], "venue": "IJCV, pages 55\u201379,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Computationally efficient regression on a dependency graph for human pose estimation", "author": ["K. Hara", "R. Chellappa"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Learning effective human pose estimation from inaccurate annotation", "author": ["S. Johnson", "M. Everingham"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Cor-  rado", "J. Dean", "A. Ng"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Poselet conditioned pictorial structures", "author": ["L. Pishchulin", "M. Andriluka", "P. Gehler", "B. Schiele"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Modec: Multimodal decomposable models for human pose estimation", "author": ["B. Sapp", "B. Taskar"], "venue": "In In Proc. CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Cascaded models for articulated pose estimation", "author": ["B. Sapp", "A. Toshev", "B. Taskar"], "venue": "In ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Real-time human pose recognition in parts from single depth", "author": ["J. Shotton", "A. Fitzgibbon", "M. Cook", "T. Sharp", "M. Finocchio", "R. Moore", "A. Kipman", "A. Blake"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Deep convolutional network cascade for facial point detection", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Pose-sensitive embedding by nonlinear nca regression", "author": ["G.W. Taylor", "R. Fergus", "G. Williams", "I. Spiro", "C. Bregler"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "R. Collobert"], "venue": "In ICML,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Heterogeneous multitask learning with joint sparsity constraints", "author": ["X. Yang", "S. Kim", "E.P. Xing"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Articulated pose estimation with flexible mixtures-of-parts", "author": ["Y. Yang", "D. Ramanan"], "venue": "In CVPR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Articulated human detection with flexible mixtures of parts", "author": ["Y. Yang", "D. Ramanan"], "venue": "IEEE TPAMI,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Learning gaussian processes from multiple tasks", "author": ["K. Yu", "V. Tresp", "A. Schwaighofer"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}], "referenceMentions": [{"referenceID": 19, "context": "Some algorithms [21] based on depth maps have already been used in practice.", "startOffset": 16, "endOffset": 20}, {"referenceID": 4, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 5, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 8, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 11, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 18, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 25, "context": "In the first approach using part-based graphical models, the human body structure is embedded into the connections between nodes of the graphical model, and the pose is estimated by finding the pose configuration that best matches the observation as measured by a score function or distribution [6, 7, 10, 13, 20, 27].", "startOffset": 295, "endOffset": 317}, {"referenceID": 8, "context": "One popular graphical model for human pose estimation is the pictorial structure model [10] (PSM), which uses pairwise connections between parts to form a tree.", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "Exact inference is possible and the solution is guaranteed to be globally optimal [10], but the inference is still very expensive for real-time applications.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "For both definitions of parts, the appearance model is critical for learning a good PSM [3, 7].", "startOffset": 88, "endOffset": 94}, {"referenceID": 5, "context": "For both definitions of parts, the appearance model is critical for learning a good PSM [3, 7].", "startOffset": 88, "endOffset": 94}, {"referenceID": 5, "context": "Several methods have been proposed to alleviate this problem by truncating the pose space [7, 20].", "startOffset": 90, "endOffset": 97}, {"referenceID": 18, "context": "Several methods have been proposed to alleviate this problem by truncating the pose space [7, 20].", "startOffset": 90, "endOffset": 97}, {"referenceID": 26, "context": "On the other hand, [28] extends the traditional PSM by allowing each body part to have multiple modes.", "startOffset": 19, "endOffset": 23}, {"referenceID": 4, "context": "Also, multimodal models, such as mixtures of PSM or hierarchical PSM [6, 13, 18, 19], have been proposed.", "startOffset": 69, "endOffset": 84}, {"referenceID": 11, "context": "Also, multimodal models, such as mixtures of PSM or hierarchical PSM [6, 13, 18, 19], have been proposed.", "startOffset": 69, "endOffset": 84}, {"referenceID": 16, "context": "Also, multimodal models, such as mixtures of PSM or hierarchical PSM [6, 13, 18, 19], have been proposed.", "startOffset": 69, "endOffset": 84}, {"referenceID": 17, "context": "Also, multimodal models, such as mixtures of PSM or hierarchical PSM [6, 13, 18, 19], have been proposed.", "startOffset": 69, "endOffset": 84}, {"referenceID": 0, "context": "In the second approach, pose estimation is viewed as a regression task [2].", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "In recent years, deep neural network architectures have achieved success in many computer vision tasks [9, 14, 22].", "startOffset": 103, "endOffset": 114}, {"referenceID": 12, "context": "In recent years, deep neural network architectures have achieved success in many computer vision tasks [9, 14, 22].", "startOffset": 103, "endOffset": 114}, {"referenceID": 20, "context": "In recent years, deep neural network architectures have achieved success in many computer vision tasks [9, 14, 22].", "startOffset": 103, "endOffset": 114}, {"referenceID": 6, "context": "Multi-task learning is typically applied when multiple tasks resemble each other and training data for each task is limited [8, 26, 29].", "startOffset": 124, "endOffset": 135}, {"referenceID": 24, "context": "Multi-task learning is typically applied when multiple tasks resemble each other and training data for each task is limited [8, 26, 29].", "startOffset": 124, "endOffset": 135}, {"referenceID": 27, "context": "Multi-task learning is typically applied when multiple tasks resemble each other and training data for each task is limited [8, 26, 29].", "startOffset": 124, "endOffset": 135}, {"referenceID": 6, "context": "We refer reader to [8, 29] for a review.", "startOffset": 19, "endOffset": 26}, {"referenceID": 27, "context": "We refer reader to [8, 29] for a review.", "startOffset": 19, "endOffset": 26}, {"referenceID": 24, "context": "In [26], a heterogeneous multi-task model is trained by encouraging the parameters for the regression task and the classification task to share the same sparsity pattern.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [9], a deep convolutional network is trained for scene labeling, by defining a multi-category classification task for each pixel.", "startOffset": 3, "endOffset": 6}, {"referenceID": 21, "context": "[23] trains a deep CNN to learn a pose-sensitive embedding with nonlinear NCA (neighbourhood components analysis) regression, and predicts the location of the head and hands by finding the nearest neighbor with the learned embedding features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In contrast to [23], we introduce accessory tasks for learning shared \u201cpose features\u201d, and output the joint locations directly from the regression network.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "In [22], a multi-stage system with deep convolutional networks is built for predicting facial point locations.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "Similarly, [24] trained cascaded convolutional networks for human pose estimation.", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "In [25] semi-supervised learning is used to guide the network to learn an internal representation that reflects the similarity between training samples.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Finally, in order to investigate the feature representation learned by the neural network, [15] estimates the \u201coptimal\u201d input that maximizes the activation of a selected neuron, and find that the \u201coptimal\u201d input resembles a human face.", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "In contrast to [15], we visualize a feature by averaging image patches that are associated with the neurons with maximum responses in an upper-layer, and obtain similar results.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "Most of the neurons in our network are Rectified Linear Units (ReLu) [17], where fact(x) = max(0, x).", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "[17] showed that ReLus are good for recognition tasks and fast to train.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "We use backpropagation [16] to update the weights.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "\u201cDropout\u201d [12] is also used in the first fully connected layers for the regression and detection tasks to prevent over-fitting.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "We refer reader to [14] for more details about the training procedure.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "We collect training data from several data sets, including Buffy Stickmen [7], ETHZ Stickmen [4], Leed Sport Pose (LSP [13]), Synchronic Activities Stickmen (SA [6]),", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "We collect training data from several data sets, including Buffy Stickmen [7], ETHZ Stickmen [4], Leed Sport Pose (LSP [13]), Synchronic Activities Stickmen (SA [6]),", "startOffset": 93, "endOffset": 96}, {"referenceID": 11, "context": "We collect training data from several data sets, including Buffy Stickmen [7], ETHZ Stickmen [4], Leed Sport Pose (LSP [13]), Synchronic Activities Stickmen (SA [6]),", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "We collect training data from several data sets, including Buffy Stickmen [7], ETHZ Stickmen [4], Leed Sport Pose (LSP [13]), Synchronic Activities Stickmen (SA [6]),", "startOffset": 161, "endOffset": 164}, {"referenceID": 17, "context": "Frames Labeled In Cinema (FLIC [19]), We Are Family(WAF) [5].", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "Frames Labeled In Cinema (FLIC [19]), We Are Family(WAF) [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "We use the same upper body detector as [7, 11].", "startOffset": 39, "endOffset": 46}, {"referenceID": 9, "context": "We use the same upper body detector as [7, 11].", "startOffset": 39, "endOffset": 46}, {"referenceID": 9, "context": "RoDG-Boost[11] 51.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "Eichner[7] 50.", "startOffset": 7, "endOffset": 10}, {"referenceID": 26, "context": "MoP [28] M = 6 51.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "MoP [28] M = 9 56.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "MoP [28] M = 12 60.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "MoP [28] 57.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "As pointed out in [11], the previous PCP evaluation measure does not compute PCP correctly.", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "We use the evaluation tool provided by [11] to calculate the corrected PCP, where an estimated body part with end points (e1, e2) is considered as correct if", "startOffset": 39, "endOffset": 43}, {"referenceID": 5, "context": "On the whole Buffy test set (276 images), HMLPE achieves better results than [7, 11] on the more difficult parts, lower arms (4.", "startOffset": 77, "endOffset": 84}, {"referenceID": 9, "context": "On the whole Buffy test set (276 images), HMLPE achieves better results than [7, 11] on the more difficult parts, lower arms (4.", "startOffset": 77, "endOffset": 84}, {"referenceID": 9, "context": "8% improvement), but gets a slightly worse result than [11] on upper arms (1.", "startOffset": 55, "endOffset": 59}, {"referenceID": 26, "context": "In this case, HMLPE achieves slightly better results than [28] (0.", "startOffset": 58, "endOffset": 62}, {"referenceID": 26, "context": "We also run the code from [28] on our full training set, using different number of components per part (denoted as M = {6, 9, 12}).", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "Using M = 12, [28] has better PCP (4%) on the lower arms compared to HMLPE.", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "5%) than [28] on upper arms.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "We use the same torso box as [19] with scale factors (swidth = 3.", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "[19] uses the following accuracy to evaluate their performance,", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Since [19] compares their methods with several previous approaches, and show that their model performs the best under this criteria, we only compare with [19].", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "Since [19] compares their methods with several previous approaches, and show that their model performs the best under this criteria, we only compare with [19].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "For the 2nd and 3rd layers (mid- and high-level features), we use a different approach than [15], which finds the input that maximizes one specific neuron.", "startOffset": 92, "endOffset": 96}], "year": 2014, "abstractText": "We propose an heterogeneous multi-task learning framework for human pose estimation from monocular image with deep convolutional neural network. In particular, we simultaneously learn a pose-joint regressor and a slidingwindow body-part detector in a deep network architecture. We show that including the body-part detection task helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several data sets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts.", "creator": "LaTeX with hyperref package"}}}