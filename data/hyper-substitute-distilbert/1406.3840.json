{"id": "1406.3840", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2014", "title": "Optimal Resource Allocation with Semi-Bandit Feedback", "abstract": "tools study a sequential resource allocation problem involving a queue demand initial recurring cases. at each cost - span the experts immediately distribute available resources among the jobs at order to maximise once expected number of completed tasks. allocating scheduling resources to increase given quantity minus the probability that recession arises, simply implies increasing cut - off. secondly, we assume a linear model where accumulation probability increases normally until it equals one, after which many additional resources is wasteful. analysts assume the difficulty computing each job arises unexpected and complicated because first algorithm for her problem and prove bigger and lower bounds describing its recovery. despite its apparent simplicity, the models has a big audience : we showcase how an opt optimistic management can improve its learning habits favorably whereas the results one normally displays for similar problems as the problem becomes tool - restricted.", "histories": [["v1", "Sun, 15 Jun 2014 18:41:47 GMT  (28kb)", "http://arxiv.org/abs/1406.3840v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tor lattimore", "koby crammer", "csaba szepesv\\'ari"], "accepted": false, "id": "1406.3840"}, "pdf": {"name": "1406.3840.pdf", "metadata": {"source": "CRF", "title": "Optimal Resource Allocation with Semi-Bandit Feedback", "authors": ["Tor Lattimore", "Csaba Szepesv\u00e1ri"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n38 40\nv1 [\ncs .L\nG ]\n1 5\nJu n\nWe study a sequential resource allocation problem involving a fixed number of recurring jobs. At each time-step the manager should distribute available resources among the jobs in order to maximise the expected number of completed jobs. Allocating more resources to a given job increases the probability that it completes, but with a cut-off. Specifically, we assume a linear model where the probability increases linearly until it equals one, after which allocating additional resources is wasteful. We assume the difficulty of each job is unknown and present the first algorithm for this problem and prove upper and lower bounds on its regret. Despite its apparent simplicity, the problem has a rich structure: we show that an appropriate optimistic algorithm can improve its learning speed dramatically beyond the results one normally expects for similar problems as the problem becomes resource-laden."}, {"heading": "1 INTRODUCTION", "text": "Assume that there are K jobs and at each time-step t a learner must distribute the available resources with Mk,t \u2265 0 going to job k, subject to a budget constraint,\nK \u2211\nk=1\nMk,t \u2264 1.\nThe probability that the kth job completes in time-step t is min {1,Mk,t/\u03bdk}, where the unknown cut-off parameter \u03bdk \u2208 (0,\u221e] determines the difficulty of job k. After every time-step the resources are replenished and all jobs are restarted regardless of whether or not they completed successfully in the previous time-step. The goal of the learner\n\u2217On sabbatical leave from the Department of Computing Science, University of Alberta, Canada\nis to maximise the expected number of jobs that successfully complete up to some known time horizon n.\nDespite the simple model, the problem is surprisingly rich. Given its information structure, the problem belongs to the class of stochastic partial monitoring problems, which was first studied by Agrawal et al. [1989]1, where in each time step the learner receives noisy information about a hidden \u201cparameter\u201d while trying to maximise the sum of rewards and both the information received and the rewards depend in a known fashion on the actions and the hidden parameter. While partial monitoring by now is relatively well understood, either in the stochastic or the adversarial framework when the action set is finite [Barto\u0301k et al., 2011, Foster and Rakhlin, 2012, Barto\u0301k, 2013], the case of continuous action sets has received only limited attention [Broder and Rusmevichientong, 2012, and references therein]. To illustrate the difficulty of the problem, notice that over-assigning resources to a given job means that the job completes with certainty and provides little information about the job\u2019s difficulty. On the other hand, if resources are under-assigned, then the information received allows one to learn about the payoff associated with all possible arms, which is reminiscent of bandit problems where the arms have \u201ccorrelated payoffs\u201d (e.g., Filippi et al. 2010, Russo and Roy 2013 and the references therein). Finally, allocating less resources yields high-variance estimates.\nOur motivation to study this particular framework comes from the problem of cache allocation. In particular, data collected offline from existing and experimental allocation strategies showed a relatively good fit to the above parametric model. In this problem each job is a computer process, which is successful in a given time-step if there were no cache misses (cache misses are very expensive). Besides this specific resource allocation problem, we also envision other applications, such as load balancing in networked environments, or any other computing applications where some precious resource (bandwidth, radio spectrum, CPU, etc.) is to be subdivided amongst competing processes. In fact, we anticipate numerous extensions and\n1The name was invented later by (perhaps) [Rustichini, 1999].\nadaptations for specific applications, such as in the case of bandits (see, Bubeck and Cesa-Bianchi [2012] for an overview of this rich literature). Finally, let us point out that although our problem is superficially similar to the socalled budgeted bandit problems (or, budget limited bandit problems), there are some major differences: in budgeted bandits, the information structure is still that of bandit problems and the resources are not replenished. Either learning stops when the budget is exhausted (e.g., Tran-Thanh et al. 2012, Ding et al. 2013, Badanidiyuru et al. 2013)2, or performance is measured against the total resources consumed in an ongoing fashion (e.g., Gyo\u0308rgy et al. 2007).\nThe main contribution besides the introduction of a new problem is a new optimistic algorithm for this problem that is shown to suffer poly-logarithmic regret with respect to optimal omniscient algorithm that knows the parameters (\u03bdk)k in advance. The structure of the bound depends significantly on the problem dynamics, ranging from a (relatively) easy full-information-like setting, corresponding to a resource-laden regime, to a bandit-like setting, corresponding to the resource-scant setting. Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too.\nProblems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache. However, they all assume the model is fully known and no learning is required. Bitirgen et al. [2008] used ANNs to predict individual program performance as a function of resources. Finally, Ipek et al. [2008] used reinforcement learning to allocate DRAM to multi-processors."}, {"heading": "2 PRELIMINARIES", "text": "In each time-step t the learner chooses Mk,t \u2265 0 subject to the constraint,\n\u2211K k=1 Mk,t \u2264 1. Then all jobs are exe-\ncuted and Xk,t \u2208 {0, 1} indicates the success or failure of job k in time-step t and is sampled from a Bernoulli distribution with parameter \u03b2(Mk,t/\u03bdk) := min {1,Mk,t/\u03bdk}. The goal is to maximise the expected number of jobs that successfully complete, \u2211Kk=1 \u03b2(Mk,t/\u03bdk). We define the gaps \u2206j,k = \u03bd \u22121 j \u2212 \u03bd\u22121k . We assume throughout for conve-\n2Besides Badanidiyuru et al. [2013], all works consider finite action spaces and unstructured reward functions.\nnience, and without loss of generality, that \u03bd1 < \u03bd2 < \u00b7 \u00b7 \u00b7 < \u03bdK . It can be shown that the optimal allocation distributes the resources to jobs in increasing order of difficulty.\nM\u2217k = min\n{ 1\u2212 k\u22121 \u2211\ni=1\nM\u2217i , \u03bdk\n}\n.\nWe let \u2113 be the number of jobs that are fully allocated under the optimal policy: \u2113 = max {i : M\u2217i = \u03bdi}. The overflow is denoted by S\u2217 = M\u2217\u2113+1, which we assume to vanish if \u2113 = K . The expected reward (number of completed jobs) when following the optimal allocation is\nK \u2211\nk=1\nM\u2217k \u03bdk = \u2113+ S\u2217 \u03bd\u2113+1 ,\nwhere we define \u03bdK+1 = \u221e in the case that \u2113 = K . The (expected n-step cumulative) regret of a given allocation algorithm is the difference between the expected number of jobs that complete under the optimal policy and those that complete given the algorithm,\nRn = E\n[\nn \u2211\nt=1\nrt\n]\n, rt =\nK \u2211\nk=1\n\u03b2(M\u2217k /\u03bdk)\u2212 K \u2211\nk=1\n\u03b2(Mk,t/\u03bdk)\n=\n( \u2113+ S\u2217\n\u03bd\u2113+1\n) \u2212 K \u2211\nk=1\n\u03b2(Mk,t/\u03bdk)."}, {"heading": "3 OVERVIEW OF ALGORITHM", "text": "We take inspiration from the optimal policy for known \u03bdk, which is to fully allocate the jobs with the smallest \u03bdk (easiest jobs) and allocate the remainder/overflow to the next easiest job. At each time-step t we replace the unknown \u03bdk by a high-probability lower bound \u03bdk,t\u22121 \u2264 \u03bdk. This corresponds to the optimistic strategy, which assumes that each job is as easy as reasonably possible. The construction of a confidence interval about \u03bdk is surprisingly delicate. There are two main challenges. First, the function \u03b2(Mk,t/\u03bdk) is non-differentiable at Mk,t = \u03bdk, and for Mk,t \u2265 \u03bdk the job will always complete and little information is gained. This is addressed by always using a lower estimate of \u03bdk in the algorithm. The second challenge is that Mk,t will vary with time, so the samples Xk,t are not identically distributed. This would normally be unproblematic, since martingale inequalities can be applied, but the specific structure of this problem means that a standard sample average estimator is a little weak in the sense that its estimation accuracy can be dramatically improved. In particular, we will propose an estimator that is able to take advantage of the fact that the variance of Xk,t decreases to zero as Mk,t approaches \u03bdk from below.\nAs far as the estimates are concerned, rather than estimate the parameters \u03bdk, it turns out that learning the reciprocal\n\u03bd\u22121k is both more approachable and ultimately more useful for proving regret bounds. Fix k and let Mk,1, . . . ,Mk,t \u2264 \u03bdk be a sequence of allocations with Mk,s \u2264 \u03bdk and Xk,s \u223c Bernoulli (Mk,s/\u03bdk). Then a natural (unbiased) estimator of \u03bd\u22121k is given by\n1\n\u03bd\u0302k,t :=\n1\nt\nt \u2211\ns=1\nXk,s Mk,s .\nThe estimator has some interesting properties. First, the random variable Xk,s/Mk,s \u2208 [0, 1/Mk,s] has a large range for small Mk,s, which makes it difficult to control the error \u03bd\u0302\u22121k,t \u2212 \u03bd\u22121k via the usual Azuma/Bernstein inequalities. Secondly, if Mk,s is close to \u03bdk, then the range of Xk,s/Mk,s is small, which makes estimation easier. Additionally, the variance is greatly decreased for Mk,s close to \u03bdk. This suggests that samples for which Mk,s is large are more useful than those where Mk,s is small, which motivates the use of the weighted estimator,\n1\n\u03bd\u0302k,t :=\n\u2211t s=1 wsXk,s \u2211t s=1 wsMk,s ,\nwhere ws will be chosen in a data-dependent way, but with the important characteristic that ws is large for Mk,s close to \u03bdk. The pseudo-code of the main algorithm is shown on Algorithm Listing 1. It accepts as input the horizon n, the number of jobs, and a set {\u03bdk,0}Kk=1 for which 0 < \u03bdk,0 \u2264 \u03bdk for each k. In Section 7 we present a simple (and efficient) algorithm that relaxes the need for the lower bounds \u03bdk,0.\nRemark 1. Later (in Lemma 6) we will show that with high probability 1 \u2264 wk,s \u2264 O(s). By definition the confidence bounds \u03bdk,t and \u03bd\u0304k,t are non-decreasing/increasing respectively. These results are sufficient to guarantee that the new algorithm is numerically stable. It is also worth noting that the running time of Algorithm 1 is O(1) per time step, since all sums can be computed incrementally."}, {"heading": "4 UPPER BOUNDS ON THE REGRET", "text": "The regret of Algorithm 1 depends in a subtle way on the parameters \u03bdk. There are four natural cases, which will appear in our main result.\nCase 1: Insufficient budget for any jobs. In this case \u2113 = 0 and the optimal algorithm allocates all available resources to the easiest task, which means M\u22171 = 1. Knowing that \u2113 = 0, the problem can be reduced to a K-armed Bernoulli bandit by restricting the action space to Mk,t = 1 for all k. Then a bandit algorithm such as UCB1 [Auer et al., 2002] will achieve logarithmic (problem dependent) regret with some dependence on the gaps \u22061,k = 1\u03bd1 \u2212 1 \u03bdk . In particular, the regret looks like Rn \u2208 O ( \u2211K k=2 log n \u22061,k ) .\nAlgorithm 1 Optimistic Allocation Algorithm\n1: input: n,K , {\u03bdk,0}Kk=1 2: \u03b4 \u2190 (nK)\u22122 and \u03bd\u0304k,0 = \u221e for each k 3: for t \u2208 1, . . . , n do 4: /* Optimistically choose Mk,t using \u03bdk,t\u22121 */ 5: (\u2200k \u2208 1, . . . ,K) initialise Mk,t \u2190 0 6: for i \u2208 1, . . . ,K do 7: k \u2190 argmin\nk:Mk,t=0 \u03bdk,t\u22121\n8: Mk,t \u2190 min { \u03bdk,t\u22121, 1\u2212 \u2211K j=1 Mj,t }\n9: end for 10: (\u2200k \u2208 1, . . . ,K) observe Xk,t 11: (\u2200k \u2208 1, . . . ,K) compute weighted estimates:\nwk,t \u2190 1\n1\u2212 Mk,t\u03bd\u0304k,t\u22121\n1\n\u03bd\u0302k,t \u2190\n\u2211t s=1 wk,sXk,s \u2211t s=1 wk,sMk,s\n12: (\u2200k \u2208 1, . . . ,K) update confidence intervals:\nRk,t \u2190 max s\u2264t wk,s V\u0302 2 k,t \u2190\n\u2211\ns\u2264t\nwk,sMk,s \u03bdk,t\u22121\n\u03b5\u0303k,t \u2190 f(Rk,t, V\u0302\n2 k,t, \u03b4)\n\u2211t s=1 wk,sMk,s\n1\n\u03bdk,t \u2190 min\n{\n1\n\u03bdk,t\u22121 ,\n1\n\u03bd\u0302k,t + \u03b5\u0303k,t\n}\n1\n\u03bd\u0304k,t \u2190 max\n{\n1\n\u03bd\u0304k,t\u22121 ,\n1\n\u03bd\u0302k,t \u2212 \u03b5\u0303k,t\n}\n13: end for\n14: function f (R,V 2, \u03b4) 15: \u03b40 \u2190 \u03b43(R+1)2(V 2+1)2 16: return R+13 log 2 \u03b40\n+ \u221a\n2(V 2 + 1) log 2\u03b40 + ( R+1 3 )2 log2 2\u03b40\n17: end function\nCase 2: Sufficient budget for all jobs. In this case \u2113 = K and the optimal policy assigns Mk,t = \u03bdk for all k, which enjoys a reward of K at each time-step. Now Algorithm 1 will choose Mk,t = \u03bdk,t\u22121 for all time-steps and by Theorem 4 stated below we will have \u03bdk,t\u22121/\u03bdk \u2208 O(1 \u2212 1t logn). Consequently, the regret may be bounded by Rn \u2208 O ( log2 n ) with no dependence on the gaps.\nCase 3: Sufficient budget for all but one job. Now the algorithm must learn which jobs should be fully allocated. This introduces a weak dependence on the gaps \u2206\u2113,k for k > \u2113, but choosing the overflow job is trivial. Again we expect the regret to be O(log2 n), but with an additional modest dependence on the gaps.\nCase 4: General case. In the completely general case even\nthe choice of the overflow job is non-trivial. Ultimately it turns out that in this setting the problem decomposes into two sub-problems. Choosing the jobs to fully allocate, and choosing the overflow job. The first component is fast, since we can make use of the faster learning when fully allocating. Choosing the overflow reduces to the bandit problem as described in case 1.\nOur main result is the following theorem bounding the regret of our algorithm.\nTheorem 2. Let \u03b4 be as in the algorithm, \u03b7k = min {1, \u03bdk} /\u03bdk,0, \u03b4\u0303k = \u03b448\u03b74\nk n6 , ck,1 = 27 log 2\u03b4\u0303k , ck,2 =\n6 log 2 \u03b4\u0303k , uk,j = ck,1 \u03bdk,0\u2206j,k . Then Algorithm 1 suffers regret at most\nRn \u2264 1 + \u2113 \u2211\nk=1\nck,1\u03b7k(1 + log n)\n+ 1{\u2113 < K} [ K \u2211\nk=\u2113+2\nck,2 \u03bdk,0\u2206\u2113+1,k +\n\u2113+1 \u2211\nk=1\nck,1\u03b7k(1 + logn)\n+\nK \u2211\nk=\u2113+2\nck,1\u03b7k(1 + log u\u2113+1,k) +\nK \u2211\nk=\u2113+1\nck,1\u03b7k(1 + log u\u2113,k)\n]\n.\nIf we assume \u03b7k \u2208 O(1) for each k (reasonable as discussed in Section 7), then the regret bound looks like\nRn \u2208 O ( \u2113 log2 n+ K \u2211\nk=\u2113+1\n(\nlog 1\n\u03bdk\u2206\u2113,k\n)\nlogn (1)\n+\nK \u2211\nk=\u2113+2\n(\nlog 1\n\u03bdk\u2206\u2113+1,k\n)\nlogn+\nK \u2211\nk=\u2113+1\nlogn\n\u2206\u2113+1,k\n)\n,\nwhere the first term is due to the gap between \u03bdk,t and \u03bdk, the second due to discovering which jobs should be fully allocated, while the third and fourth terms are due to mistakes when choosing the overflow job.\nThe proof is broken into two components. In the first part we tackle the convergence of \u03bd\u0302t,k to \u03bdk and analyse the width of the confidence intervals, which are data-dependent and shrink substantially faster when Mk,t is chosen close to \u03bdk. In the second component we decompose the regret in terms of the width of the confidence intervals. While we avoided large constants in the algorithm itself, in the proof we focus on legibility. Optimising the constants would complicate an already long result."}, {"heading": "5 ESTIMATION", "text": "We consider a single job with parameter \u03bd and analyse the estimator and confidence intervals used by Algorithm 1. We start by showing that the confidence intervals contain the truth with high-probability and then analyse the rate at which the intervals shrink as more more data is observed.\nSomewhat surprisingly the rate has a strong dependence on the data with larger allocations leading to faster convergence.\nLet {Ft}\u221et=0 be a filtration and let M1, . . . ,Mn be a sequence of positive random variables such that Mt is Ft\u22121measurable. Define Xt to be sampled from a Bernoulli distribution with parameter \u03b2(Mt/\u03bd) for some \u03bd \u2208 [\u03bd0,\u221e] and assume that Xt is Ft-measurable. Our goal is to construct a sequence of confidence intervals {[\u03bdt, \u03bd\u0304t]}nt=1 such that \u03bd \u2208 [\u03bdt, \u03bd\u0304t] with high probability and \u03bd\u0304t \u2212 \u03bdt \u2192 0 as fast as possible. We assume a known lower bound \u03bd0 \u2264 \u03bd and define \u03bd\u03040 = \u221e. Recall that the estimator used by Algorithm 1 is defined by\nws = 1\n1\u2212 Mt\u03bd\u0304t\u22121 ,\n1 \u03bd\u0302t =\n\u2211t s=1 wsXs \u2211t s=1 wsMs .\nFix a number 0 < \u03b4 < 1 and define \u03b5\u0303t = f(Rt, V\u0302 2 t , \u03b4)/ \u2211t s=1 wsMs, where the function f is defined in Algorithm 1, Rt = maxs\u2264t ws and V\u0302 2t = \u2211t\ns=1 wsMs \u03bdt\u22121 . The lower and upper confidence bounds on\n\u03bd\u22121 are defined by,\n1\n\u03bdt =min\n{\n1 \u03bdt\u22121 , 1 \u03bd\u0302t + \u03b5\u0303t\n}\n, 1\n\u03bd\u0304t =max\n{\n1 \u03bd\u0304t\u22121 , 1 \u03bd\u0302t \u2212 \u03b5\u0303t\n}\n.\nWe define \u03b5t = \u03bd \u22121 t \u2212 \u03bd\u0304\u22121t to be the (decreasing) width of the confidence interval. Note that both \u03bdt and \u03bd\u0304t depend on \u03b4, although this dependence is not shown to minimise clutter.\nTheorem 3. If Ms is chosen such that Ms \u2264 \u03bds\u22121 for all s then P {\u2203s \u2264 t s.t. \u03bd 6\u2208 [\u03bds, \u03bd\u0304s]} \u2264 t\u03b4 holds for any 0 < \u03b4 < 1.\nProof of Theorem 3. Let Ft be the event Ft = {\u03bd \u2208 [\u03bdt, \u03bd\u0304t]}. Note that since [\u03bdt, \u03bd\u0304t] \u2282 [\u03bdt\u22121, \u03bd\u0304t\u22121] \u2282 \u00b7 \u00b7 \u00b7 \u2282 [\u03bd0, \u03bd\u03040], Ft \u2282 Ft\u22121 \u2282 \u00b7 \u00b7 \u00b7 \u2282 F0. Hence, Ft = \u2229s\u2264tFs and it suffices to prove that P {F ct } \u2264 t\u03b4.3\nDefine Ys = wsXs \u2212 wsMs\u03bd and St = \u2211t s=1 Ys and V 2 t = \u2211t s=1 Var[Ys|Fs\u22121]. We proceed by induction. Assume P {\nF ct\u22121 } \u2264 (t \u2212 1)\u03b4, which is trivial for t = 1. Now, on Ft\u22121,\nV 2t (a) =\nt \u2211\ns=1\nVar[Ys|Fs\u22121] (b) =\nt \u2211\ns=1\nw2sMs \u03bd\n(\n1\u2212 Ms \u03bd\n)\n(c) =\nt \u2211\ns=1\nwsMs \u03bd\n(\n1\u2212 Ms\u03bd 1\u2212 Ms\u03bd\u0304s\u22121\n)\n(d) \u2264 t \u2211\ns=1\nwsMs \u03bd\n(e) \u2264 V\u0302 2t ,\nwhere (a) is the definition of V 2t , (b) follows since ws is Fs\u22121-measurable, (c) follows by substituting the definition of ws, (d) and (e) are true since given Ft\u22121 we\n3For an event E, we use Ec to denote its complement.\nknow that \u03bds\u22121 \u2264 \u03bd \u2264 \u03bd\u0304s\u22121. Therefore f(Rt, V 2t , \u03b4) \u2264 f(Rt, V\u0302 2 t , \u03b4), which follows since f is monotone increasing in its second argument. Therefore,\nP\n{\u2223\n\u2223 \u2223 \u2223\n1 \u03bd\u0302t \u2212 1 \u03bd\n\u2223 \u2223 \u2223 \u2223 \u2265 \u03b5\u0303t \u2227 Ft\u22121 }\n= P\n{\u2223\n\u2223 \u2223 \u2223 \u2223\n\u2211t s=1 wsXs \u2211t s=1 wsMs \u2212 1 \u03bd\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2265 f(Rt, V\u0302 2 t , \u03b4)\n\u2211t s=1 wsMs\n\u2227 Ft\u22121 }\n\u2264 P {\u2223 \u2223 \u2223\n\u2223 \u2223\nt \u2211\ns=1\nwsXs \u2212 t \u2211\ns=1\nwsMs \u03bd\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2265 f(Rt, V 2t , \u03b4) \u2227 Ft\u22121 }\n= P { |St| \u2265 f(Rt, V 2t , \u03b4) \u2227 Ft\u22121 } . (2)\nBy the union bound we have\nP\n{ |St| \u2265 f(Rt, V\u0302 2t , \u03b4) \u2228 F ct\u22121 }\n\u2264 P { |St| \u2265 f(Rt, V 2t , \u03b4) \u2227 Ft\u22121 } + P { F ct\u22121 }\n(a) \u2264 \u03b4 + P {\nF ct\u22121 } \u2264 \u03b4 + (t\u2212 1)\u03b4 = t\u03b4 ,\nwhere (a) follows from Theorem 13 in the Appendix. Therefore P {\n|St| \u2264 f(Rt, V 2t , \u03b4) \u2227 Ft\u22121 } \u2265 1 \u2212 t\u03b4 and so with probability at least 1\u2212 t\u03b4 we have that Ft\u22121 and\n\u2223 \u2223 \u2223 \u2223 1 \u03bd\u0302t \u2212 1 \u03bd \u2223 \u2223 \u2223 \u2223 \u2264 f(Rt, V\u0302 2 t , \u03b4)\n\u2211t s=1 wsMs\n= \u03b5\u0303t,\nin which case\n1\n\u03bdt = min\n{\n1 \u03bdt\u22121 , 1 \u03bd\u0302t + \u03b5\u0303t\n}\n\u2265 1 \u03bd ,\nand similarly 1\u03bd\u0304t \u2264 1 \u03bd , which implies Ft. Therefore P {F ct } \u2264 t\u03b4 as required.\nWe now analyse the width \u03b5t \u2261 \u03bd\u22121t \u2212 \u03bd\u0304\u22121t of the confidence interval obtained after t samples are observed. We say that a job is fully allocated at time-step s if Ms = \u03bds\u22121. The first theorem shows that the width \u03b5t drops with order O(1/T (t)), where T (t) =\n\u2211t s=1 1{Ms = \u03bds\u22121} is the\nnumber of fully allocated time-steps. The second theorem shows that for any \u03b1 > 0, the width \u03b5t drops with order O( \u221a 1/(\u03b1U\u03b1(t))), where U\u03b1(t) = \u2211t\ns=1 1{Ms \u2265 \u03b1}. The dramatic difference in speeds is due to the low variance Var[Xt|Ft\u22121] when Mt is chosen close to \u03bd. For the next results define \u03b7 = min {1, \u03bd} /\u03bd0 and \u03b4\u0303 = \u03b448\u03b74n6 . Theorem 4. \u03b5t \u2264 c1\n\u03bd0(T (t) + 1) where c1 = 27 log 2\u03b4\u0303 .\nTheorem 5. \u03b5t \u2264 \u221a c2 \u03b1\u03bd0U\u03b1(t) where c2 = 6 log 2\u03b4\u0303 .\nThe proofs are based on the following lemma that collects some simple observations:\nLemma 6. The following hold for any t \u2265 1:\n1. wtMt \u2264 1\u03b5t\u22121 , with equality if Mt = \u03bdt\u22121. 2. 1 \u2264 Rt \u2264 1\u03bd0\u03b5t\u22121 . 3. \u03b5t \u2265 1tmin{1,\u03bd} . 4. 1\u2212 \u03bdt\u03bd \u2264 \u03bdt\u03b5t.\nProof. Using the definition of ws and the fact that Ms is always chosen to be smaller or equal to \u03bds\u22121, we get\nws \u2261 ( 1\u2212 Ms \u03bd\u0304s\u22121 )\u22121 (a) \u2264 ( 1\u2212 \u03bds\u22121 \u03bd\u0304s\u22121 )\u22121 =\n1\n\u03b5s\u22121\u03bds\u22121 .\nThe first claim follows since the inequality (a) can be replaced by equality if Ms = \u03bds\u22121. The second follows from the definition of Rt and the facts that (\u03b5s)s is nonincreasing and (\u03bds)s is non-decreasing. For the third claim we recall that Rt = maxs\u2264t ws and Ms \u2264 \u03bd. Therefore,\n\u03b5t (a) \u2265 min { \u03b5t\u22121, Rt\n\u2211t s=1 wsMs\n}\n(b) \u2265 min { \u03b5t\u22121, 1\ntmin {1, \u03bd}\n}\n,\nwhere (a) follows from the definition of \u03b5t and naive bounding of the function f , (b) follows since Rt \u2265 ws for all s \u2264 t and because Ms \u2264 min {1, \u03bd} for all s. Trivial induction and the fact that \u03b50 = \u03bd \u22121 0 \u2265 \u03bd\u22121 completes the proof of the third claim. For the final claim we use the facts that \u03bd\u22121t \u2264 \u03bd\u22121 + \u03b5t. Therefore, 1\u2212 \u03bdt\u03bdt = \u03bdt ( 1 \u03bdt \u2212 1\u03bd )\n\u2264 \u03bdt\u03b5t.\nLemma 7. \u03b5t \u2264 6Rt log\n2 \u03b4\u0303\n\u2211t s=1 wsMs\n+\n\u221a \u221a \u221a \u221a\n8 log 2 \u03b4\u0303\n\u03bd0 \u2211t s=1 wsMs .\nProof. Let \u03b4t = \u03b4/(3(Rt + 1)2(V\u0302 2t + 1) 2) < 1. By the definition of \u03b5t,\n\u03b5t \u2264 2f(Rt, V\u0302\n2 t , \u03b4)\n\u2211t s=1 wsMs\n(a) \u2264 4(Rt+1) 3 log 2 \u03b4t\n+ 2 \u221a\n2(V\u0302 2t + 1) log 2 \u03b4t\n\u2211t s=1 wsMs\n(b) \u2264 6Rt log\n2 \u03b4t\n+ \u221a\n8 \u03bd0 \u2211t s=1 wsMs log 2 \u03b4t\n\u2211t s=1 wsMs\n= 6Rt log\n2 \u03b4t\n\u2211t s=1 wsMs\n+\n\u221a\n8 log 2\u03b4t \u03bd0 \u2211t s=1 wsMs ,\nwhere in (a) we used the definition of f , in (b) we substituted the definition of V\u0302 2t and used the facts that Rt \u2265 1 and \u03bd0 \u2264 \u03bdt\u22121 and we also used a naive bound. The proof is completed by proving 2/\u03b4t \u2264 2/\u03b4\u0303. Indeed, by Lemma 6,\n1 \u2264 Rt \u2264 1\u03b5t\u22121\u03bd0 \u2264 1 \u03b5t\u03bd0 . We also have V\u0302 2t \u2264 tR2t . Thus,\n2 \u03b4t =\n6(Rt + 1) 2(V\u0302 2t + 1) 2\n\u03b4 \u2264 6 \u03b4\n(\n16t2\n(\u03b5t\u03bd0) 4\n)\n(a) \u2264 2 \u03b4\u0303 ,\nwhere in (a) we used Lemma 6(3).\nProof of Theorem 4. By Lemma 7,\n\u03b5t \u2264 6Rt log\n2 \u03b4\u0303\n\u2211t s=1 wsMs\n+\n\u221a\n8\n\u03bd0 \u2211t s=1 wsMs log\n2 \u03b4\u0303 . (3)\nWe proceed by induction. Assume that \u03b5s\u22121 \u2264 c1 \u03bd0(T (s\u22121)+1) , which is trivial for s = 1. By Lemma 6(1),\nt \u2211\ns=1\nwsMs \u2265 T (t) \u2211\ns=1\ns\u03bd0 c1 = \u03bd0T (t)(T (t) + 1) 2c1 . (4)\nTherefore, \u221a\n8\n\u03bd0 \u2211t s=1 wsMs log\n2\n\u03b4\u0303\n(a) \u2264 1 \u03bd0T (t)\n\u221a\n4c1 log 2\n\u03b4\u0303 . (5)\nNow we work on the first term in (3). If \u03b5t\u22121 \u2264 c1\u03bd0(T (t)+1) , then we are done, since \u03b5s is non-increasing. Otherwise, we use Lemma 6(2) to obtain,\n6Rt \u2211t\ns=1 wsMs log\n2 \u03b4\u0303 \u2264 6 \u03bd0\u03b5t\u22121 \u2211t s=1 wsMs log 2 \u03b4\u0303\n(a) \u2264 3 \u03bd0T (t) log 2 \u03b4\u0303 , (6)\nwhere in (a) we used (4) and the lower bound on \u03b5t\u22121. Substituting (5) and (6) into (3) we have\n\u03b5t \u2264 1\n\u03bd0T (t)\n\u221a\n4c1 log 2\n\u03b4\u0303 +\n3\n\u03bd0T (t) log\n2 \u03b4\u0303 .\nChoosing c1 = 27 log 2\u03b4\u0303 leads to\n\u03b5t \u2264 1\n\u03bd0T (t)\n\u221a\n4 \u00b7 27 log2 2 \u03b4\u0303 +\n3\n\u03bd0T (t) log\n2\n\u03b4\u0303\n\u2264 27 \u03bd0(T (t) + 1) log 2 \u03b4\u0303 = c1 \u03bd0(T (t) + 1) ,\nwhich completes the induction and proof.\nProof of Theorem 5. Firstly, by Lemma 7,\n\u03b5t \u2264 6Rt\n\u2211t s=1 wsMs\nlog 2\n\u03b4\u0303 +\n\u221a\n8\n\u03bd0 \u2211t s=1 wsMs log\n2 \u03b4\u0303 .\nThe second term is easily bounded by using the fact that ws \u2265 1 and the definition of U\u03b1(t),\n\u221a\n8\n\u03bd0 \u2211t s=1 wsMs log\n2 \u03b4\u0303 \u2264\n\u221a\n8\n\u03bd0U\u03b1(t)\u03b1 log\n2 \u03b4\u0303 .\nFor the first term we assume \u03b5t\u22121 \u2265 \u221a\nc2 \u03bd0U\u03b1(t)\u03b1 , since\notherwise we can apply monotonicity of \u03b5t. Therefore\n6Rt \u2211t\ns=1 wsMs log\n2 \u03b4\u0303 \u2264 6 \u03bd0\u03b5t\u22121 \u2211t s=1 wsMs log 2 \u03b4\u0303\n\u2264 \u221a\nU\u03b1(t)\u03b1\u03bd0 c2\n\u00b7 6 log 2 \u03b4\u0303\n\u03bd0U\u03b1(t)\u03b1 \u2264 6\n\u221a\n1\nc2\u03b1\u03bd0U\u03b1(t) log\n2 \u03b4\u0303 .\nNow choose c2 = 6 log 2\u03b4\u0303 to complete the result."}, {"heading": "6 PROOF OF THEOREM 2", "text": "We are now ready to use the results of Section 5 to bound the regret of Algorithm 1. The first step is to decompose the regret into two cases depending on whether or not the confidence intervals contain the truth. The probability that they do not is low, so this contributes negligibly to the regret. When the confidence intervals are valid we break the problem into two components. The first is the selection of the processes to fully allocation, which leads to the O(log2 n) part of the bound. The second component involves analysing the selection of the overflow process, where the approach is reminiscent of the analysis for the UCB algorithm for stochastic bandits [Auer et al., 2002].\nLet Fk,t denote the event when none of the confidence intervals underlying job k fail up to time t:\nFk,t = {\u2200s \u2264 t : \u03bd \u2208 [\u03bdk,s, \u03bd\u0304k,s]} .\nThe algorithm uses \u03b4 = (nK)\u22122, which is sufficient by a union bound and Theorem 3 to ensure that,\nP {Gc} \u2264 1 nK\n, where G = K \u22c2\nk=1\nFk,n . (7)\nThe regret can be decomposed into two cases depending on whether G holds:\nRn = E\nn \u2211\nt=1\nrt (a) = E1{Gc}\nn \u2211\nt=1\nrt + E1{G} n \u2211\nt=1\nrt (8)\n(b) \u2264 E1{Gc}nK + E1{G} n \u2211\nt=1\nrt (c) \u2264 1 + E1{G} n \u2211\nt=1\nrt,\nwhere (a) follows from the definition of expectation, (b) is true by bounding rt \u2264 K for all t, and (c) follows from (7). For the remainder we assume G holds and use Theorems 4 and 5 combined with the definition of the algorithm to control the second term in (8). The first step is to decompose the regret in round t:\nrt = \u2113 \u2217 +\nS\u2217\n\u03bd\u2113+1 \u2212\nK \u2211\nk=1\n\u03b2\n(\nMk,t \u03bdk\n)\n.\nBy the assumption that G holds we know for all t \u2264 n and k that \u03bd\u0304\u22121k,t \u2264 \u03bd\u22121k \u2264 \u03bd\u22121k,t . Therefore Mk,t \u2264 \u03bdk,t\u22121 \u2264 \u03bdk, which means that \u03b2(Mk,t/\u03bdk) = Mk,t/\u03bdk. Define \u03c0t(i) \u2208 {1, . . . ,K} such that \u03bd\u03c0t(i),t\u22121 \u2264 \u03bd\u03c0t(i+1),t\u22121. Also let\nAt = {k : Mk,t = \u03bdk,t\u22121} , A\u2264jt = At \u2229 {\u03c0i(t) : 1 \u2264 i \u2264 j} ,\nTk(t) =\nt \u2211\ns=1\n1{k \u2208 At} and Bt = \u03c0t(\u2113+ 1).\nInformally, At is the set of jobs that are fully allocated at time-step t, A\u2264jt is a subset of At containing the j jobs believed to be easiest, Tk(t) is the number of times job k has been fully allocated at time-step t, and Bt is the (\u2113 + 1)th easiest job at time-step t (this is only defined if \u2113 < K and will only be used in that case).\nLemma 8. For all t, |At| \u2265 \u2113 and if |At| = \u2113, then MBt,t \u2265 S\u2217.\nProof. |At| = max { j : \u2211j i=1 \u03bd\u03c0t(i),t\u22121 \u2264 1 } . But \u03bdk,t\u22121 \u2264 \u03bdk for all k and t, so \u2211\u2113\ni=1 \u03bd\u03c0t(i),t\u22121 \u2264 \u2211\u2113\nk=1 \u03bdk,t\u22121 \u2264 \u2211\u2113 k=1 \u03bdk \u2264 1. Therefore |At| \u2265 \u2113. If |At| = \u2113, then Bt /\u2208 At is the overflow job and so MBt,t = 1 \u2212 \u2211 k\u2208At \u03bdk,t\u22121 \u2265 1 \u2212 \u2211\nk\u2208A\u2217 \u03bdk,t\u22121 \u2265 1\u2212\u2211k\u2208A\u2217 \u03bdk \u2261 S\u2217\nWe now decompose the regret, while still assuming that G holds:\nn \u2211\nt=1\nrt =\nn \u2211\nt=1\n(\n\u2113+ S\u2217\n\u03bd\u2113+1 \u2212\nK \u2211\nk=1\nMk,t \u03bdk\n)\n\u2264 n \u2211\nt=1\n\u2211\nk\u2208A\u2264\u2113t\n(\n1\u2212 Mk,t \u03bdk\n)\n(9)\n+ 1{\u2113 < K} n \u2211\nt=1\n(\nS\u2217\n\u03bd\u2113+1 \u2212 MBt,t \u03bdBt\n)\n. (10)\nLet us bound the first sum:\nn \u2211\nt=1\n\u2211\nk\u2208A\u2264\u2113t\n(\n1\u2212 Mk,t \u03bdk\n)\n=\nn \u2211\nt=1\nK \u2211\nk=1\n1\n{ k \u2208 A\u2264\u2113t }\n(\n1\u2212 \u03bdk,t\u22121 \u03bdk\n)\n(a) \u2264 n \u2211\nt=1\nK \u2211\nk=1\n1\n{ k \u2208 A\u2264\u2113t } \u03bdk,t\u22121\u03b5k,t\u22121\n(b) \u2264 n \u2211\nt=1\nK \u2211\nk=1\n1\n{ k \u2208 A\u2264\u2113t } ck,1\u03bdk,t\u22121\n\u03bdk,0Tk(t) , (11)\nwhere (a) follows by Lemma 6 and (b) by Theorem 4.\nLemma 9. If k > j, then\nn \u2211\nt=1\n1\n{ k \u2208 A\u2264jt } \u2264 ck,1 \u03bdk,0\u2206j,k =: uj,k.\nProof. Assume k \u2208 A\u2264jt . Therefore \u03bdk,t\u22121 \u2264 \u03bdj . But if uj,k < \u2211t s=1 1 { k \u2208 A\u2264js } \u2264 Tk(t\u2212 1) + 1, then\n1 \u03bdk,t\u22121 \u2264 1 \u03bdk + \u03b5k,t\u22121 = 1 \u03bdj + \u03b5k,t\u22121 \u2212\u2206j,k\n(a) \u2264 1 \u03bdj + ck,1 \u03bdk,0(Tk(t\u2212 1) + 1) \u2212\u2206j,k < 1 \u03bdj ,\nwhere (a) follows from Theorem 4. Therefore k \u2208 A\u2264jt implies that \u2211t s=1 1 { k \u2208 A\u2264js }\n\u2264 uj,k and so \u2211n\nt=1 1\n{ k \u2208 A\u2264jt } \u2264 uj,k as required.\nContinuing (11) by applying Lemma 9 with j = \u2113:\nn \u2211\nt=1\nK \u2211\nk=1\n1\n{ k \u2208 A\u2264\u2113t } ck,1\u03bdk,t\u22121\n\u03bdk,0Tk(t)\n=\nn \u2211\nt=1\n\u2211\nk\u2208A\u2217 1\n{ k \u2208 A\u2264\u2113t } ck,1\u03bdk,t\u22121\n\u03bdk,0Tk(t)\n+\nn \u2211\nt=1\n\u2211\nk/\u2208A\u2217 1\n{ k \u2208 A\u2264\u2113t } ck,1\u03bdk,t\u22121\n\u03bdk,0Tk(t) (12)\n(a) \u2264 \u2211\nk\u2208A\u2217\nn \u2211\nt=1\nck,1\u03b7k t + \u2211\nk/\u2208A\u2217\nu\u2113,k \u2211\nt=1\nck,1\u03b7k t\n\u2264 \u2113 \u2211\nk=1\nck,1\u03b7k(1 + logn) +\nK \u2211\nk=\u2113+1\nck,1\u03b7k(1 + log u\u2113,k),\nwhere (a) follows by Lemma 9 and the fact that k \u2208 A\u2264\u2113t implies that \u03bdk,t\u22121\u03bdk,0 \u2264 \u03b7k. Now if \u2113 = K , then the second term in (9) is zero and the proof is completed by substituting the above result into (9) and then into (8). So now we assume \u2113 > K and bound the second term in (9) as follows:\nn \u2211\nt=1\n( S\u2217\n\u03bd\u2113+1 \u2212 MBt,t \u03bdBt\n) \u2264 n \u2211\nt=1\n1{Bt \u2208 At} ( 1\u2212 \u03bdBt,t\u22121 \u03bdBt )\n+\nn \u2211\nt=1\n1{Bt /\u2208 At} ( S\u2217\n\u03bd\u2113+1 \u2212 S\n\u2217\n\u03bdBt\n)\n, (13)\nwhere we used Lemma 8 and S\u2217 \u2264 1 and that if Bt \u2208 At,\nthen MBt,t = \u03bdBt,t\u22121. Bounding each term separately:\nn \u2211\nt=1\n1{Bt \u2208 At} ( 1\u2212 \u03bdBt,t\u22121 \u03bdBt )\n(a) \u2264 K \u2211\nk=1\nn \u2211\nt=1\n1\n{ k \u2208 A\u2264\u2113+1t }\n(\n1\u2212 \u03bdk,t\u22121 \u03bdk\n)\n(b) \u2264 K \u2211\nk=1\nn \u2211\nt=1\n1\n{ k \u2208 A\u2264\u2113+1t } \u03bdk,t\u22121\u03b5k,t\u22121 (14)\n(c) \u2264 K \u2211\nk=1\nn \u2211\nt=1\n1\n{ k \u2208 A\u2264\u2113+1t } ck,1\u03bdk,t\u22121\n\u03bdk,0Tk(t)\n(d) \u2264 \u2113+1 \u2211\nk=1\nck,1\u03b7k(1 + logn) +\nK \u2211\nk=\u2113+2\nck,1\u03b7k(1 + log u\u2113+1,k),\nwhere (a) follows since Bt \u2208 At implies that Bt \u2208 A\u2264\u2113+1t , (b) follows from Lemma 6(4), (c) by Theorem 4, and (d) follows from Lemma 9 and the same analysis as (12). For the second term we need the following lemma, which uses Theorem 5 and a reasoning analogues to that of Auer et al. [2002] to bound the regret of the UCB algorithm for stochastic bandits:\nLemma 10. Let Uk(t) = \u2211t s=1 1{Mk,s \u2265 S\u2217} and k > \u2113+ 1. If Uk(t) \u2265 ck,2S\u2217\u03bdk,0\u22062\u2113+1,k =: vk, then k 6= Bt.\nProof. If \u03bdk,t\u22121 > \u03bd\u2113+1, then k 6= Bt. Furthermore, if Uk(t) > vk, then\n1 \u03bdk,t\u22121 \u2264 1 \u03bdk + \u03b5k,t\u22121 = 1 \u03bd\u2113+1 \u2212\u2206\u2113+1,k + \u03b5k,t\u22121\n(a) \u2264 1 \u03bd\u2113+1 \u2212\u2206\u2113+1,k + \u221a ck,2 \u03bdk,0S\u2217Uk(t) < 1 \u03bd\u2113+1 ,\nwhere (a) follows from Theorem 5.\nTherefore n \u2211\nt=1\n1{Bt /\u2208 At} ( S\u2217\n\u03bd\u2113+1 \u2212 S\n\u2217\n\u03bdBt\n)\n(a) \u2264 S\u2217 K \u2211\nk=1\nn \u2211\nt=1\n1{k = Bt /\u2208 At}\u2206\u2113+1,k\n(b) \u2264 S\u2217 K \u2211\nk=\u2113+2\nn \u2211\nt=1\n1{k = Bt /\u2208 At}\u2206\u2113+1,k\n(c) \u2264 S\u2217 K \u2211\nk=\u2113+2\nn \u2211\nt=1\n1{k = Bt \u2227Mk,t \u2265 S\u2217}\u2206\u2113+1,k\n(d) \u2264 K \u2211\nk=\u2113+2\nS\u2217\u2206\u2113+1,kvk (e) =\nK \u2211\nk=\u2113+2\nck,2 \u03bdk,0\u2206\u2113+1,k , (15)\nwhere (a) follows from the definition of \u2206\u2113+1,k and the fact that if Bt /\u2208 At, then |At| = \u2113, (b) follows since \u2206\u2113+1,k is\nnegative for k \u2264 \u2113+1, (c) by Lemma 8, (d) by Lemma 10, and (e) by the definition of vk. Substituting (14) and (15) into (13) we have\nn \u2211\nt=1\n( S\u2217\n\u03bd\u2113+1 \u2212 MBt,t \u03bdBt\n) \u2264 \u2113+1 \u2211\nk=1\nck,1\u03b7k(1 + logn)\n+\nK \u2211\nk=\u2113+2\nck,1\u03b7k(1 + log u\u2113+1,k) +\nK \u2211\nk=\u2113+2\nck,2 \u03bdk,0\u2206\u2113+1,k .\nWe then substitute this along with (12) into (9) and then (8) to obtain\nRn \u2264 1 + \u2113 \u2211\nk=1\nck,1\u03b7k(1 + log n)\n+ 1{\u2113 < K} [ K \u2211\nk=\u2113+2\nck,2 \u03bdk,0\u2206\u2113+1,k\n+ \u2113+1 \u2211\nk=1\nck,1\u03b7k(1 + logn)\n+\nK \u2211\nk=\u2113+2\nck,1\u03b7k(1 + log u\u2113+1,k) +\nK \u2211\nk=\u2113+1\nck,1\u03b7k(1 + log u\u2113,k)\n]\n."}, {"heading": "7 INITIALISATION", "text": "Previously we assumed a known lower bound \u03bdk,0 \u2264 \u03bdk for each k. In this section we show that these bounds are easily obtained using a halving trick. In particular, the following algorithm computes a lower bound \u03bd0 \u2264 \u03bd for a single job with unknown parameter \u03bd.\nAlgorithm 2 Initialisation of \u03bd0 1: for t \u2208 1, . . . ,\u221e do 2: Allocate Mt = 2\u2212t and observe Xt 3: if Xt = 0 then return \u03bd0 \u2190 2\u2212t. 4: end for\nA naive way to eliminate the need for the lower bounds (\u03bdk,0)k is simply to run Algorithm 2 for each job sequentially. Then the following proposition shows that \u03b7 \u2208 O(1) is reasonable, which justifies the claim made in (1) that the \u03b7k terms appearing in Theorem 2 are O(1).\nProposition 11. If \u03b7 = min{1,\u03bd}\u03bd0 , then E\u03b7 \u2264 4.\nProof. Let pt be the probability that the algorithm ends after time-step t, which is\npt = (1\u2212 \u03b2(2\u2212t/\u03bd)) t\u22121 \u220f\ns=1\n\u03b2(2\u2212s/\u03bd).\nTherefore\nE\u03b7 = E\n[\nmin {1, \u03bd} \u03bd0\n] = \u221e \u2211\nt=1\npt \u00b7 min {1, \u03bd}\nMt\n= min {1, \u03bd} \u221e \u2211\nt=1\n2t(1 \u2212 \u03b2(2\u2212t/\u03bd)) t\u22121 \u220f\ns=1\n\u03b2(2\u2212s/\u03bd)\n\u2264 4,\nwhere the final inequality follows from an arduous, but straight-forward, computation.\nThe problem with the naive method is that the expected running time of Algorithm 2 is O(log 1\u03bd ), which may be arbitrary large for small \u03bd and lead to a high regret during the initialisation period. Fortunately, the situation when \u03bd is small is easy to handle, since the amount of resources required to complete such a job is also small. The trick is to run K offset instances of Algorithm 2 alongside a modified version of Algorithm 1. First we describe the parallel implementations of Algorithm 2. For job k, start Algorithm 2 in time-step k, which means that the total amount of resources used by the parallel copies of Algorithm 2 in time-step t is bounded by K \u2211\nk=1\n1{t \u2265 k} 2k\u2212t\u22121\n\u2264 min { 1, 2K\u2212t } . (16)\nJob Mk,1 Mk,2 Mk,3 Mk,4 1 1/2 1/4 1/8 1/16 2 0 1/2 1/4 1/8\n3 0 0 1/2 1/4 K\u2211\nk=1\nMk,t 1/2 3/4 7/8 7/16\nAlgorithm 1 is implemented starting from time-step 1, but only allocates resources to jobs for which the initialisation process has completed. Estimates are computed using only the samples for which Algorithm 1 chose the allocation, which ensures that they are based on allocations with Mk,t \u2264 \u03bdk. Note that the analysis of the modified algorithm does not depend on the order in which the parallel processes are initialised. The regret incurred by the modified algorithm is given in order notation in (1). The proof is omitted, but relies on two observations. First, that the expected number of time-steps that a job is not (at least) fully allocated while it is being initialised is 2. The second is that the resources available to Algorithm 1 at time-step t converges exponentially fast to 1 by (16)."}, {"heading": "8 MINIMAX LOWER BOUNDS", "text": "Despite the continuous action space, the techniques used when proving minimax lower bounds for standard stochastic bandits [Auer et al., 1995] can be adapted to our setting.\nTheorem 12. Given fixed n and 8n \u2265 K \u2265 2 and an arbitrary algorithm, there exists an allocation problem for which the expected regret satisfies Rn \u2265 \u221a nK\n16 \u221a 2 .\nProof. Let 1 \u2265 \u03b5 > 0 be a constant to be chosen later. We consider a set of K allocation problems where in problem\nk, \u03bdj = 2 for all j 6= k and \u03bdk = 21+\u03b5 . The optimal action in problem k is to assign all available resources to job k when the expected reward is 1+\u03b52 . The interaction between the algorithm and a problem k defines a measure Pk on the set of outcomes (successes, allocations). We write Ek for expectations with respect to measure Pk. We have\nEk\n[\nn \u2211\nt=1\nMk,t\n] \u2212 E0 [ n \u2211\nt=1\nMk,t\n]\n\u2264 n \u221a 1\n2 KL(P0,Pk) ,\n(17)\nwhere KL(P0,Pk) is the Kullback-Leibler divergence (or relative entropy) between P0 and Pk. The divergence is bounded by\nKL(P0,Pk) (a) \u2264 E0 [ n \u2211\nt=1\n\u03b52M2k,t 4\n(\n1 Mk,t 2 + 1 1\u2212 Mk,t2\n)]\n(b) = E0\n[\nn \u2211\nt=1\n\u03b52Mk,t 2\u2212Mk,t\n]\n(c) \u2264 \u03b52E0 [ n \u2211\nt=1\nMk,t\n]\n, (18)\nwhere (a) follows from the telescoping property of the KL divergence and by bounding the KL divergence by the \u03c7squared distance, (b) is trivial and (c) follows since Mk,t \u2264 1. The n-step expected regret given environment k is\nRn(k) = n(1 + \u03b5)\n2 \u2212 Ek\nn \u2211\nt=1\nK \u2211\nj=1\nMj,t \u03bdj\n(b) \u2265 \u03b5 2\n(\nn\u2212 Ek n \u2211\nt=1\nMk,t\n)\n(19)\nwhere (b) follows by recalling that \u03bdj = 2 unless j = k, when \u03bdj = 2/(1 + \u03b5). Therefore,\nsup k\nRn(k) (a) \u2265 1 K\nK \u2211\nk=1\nRn(k)\n(b) \u2265 1 K\nK \u2211\nk=1\n\u03b5\n2\n(\nn\u2212 Ek n \u2211\nt=1\nMk,t\n)\n(c) \u2265 1 K\nK \u2211\nk=1\n\u03b5\n2\n n\u2212 E0 n \u2211\nt=1\nMk,t \u2212 n\u03b5\n\u221a \u221a \u221a \u221a 1\n2 E0\nn \u2211\nt=1\nMk,t\n\n\n(d) \u2265 \u03b5 2K\n nK \u2212 n\u2212 n\u03b5 K \u2211\nk=1\n\u221a \u221a \u221a \u221a 1\n2 E0\nn \u2211\nt=1\nMk,t\n\n\n(e) \u2265 \u03b5 2K\n\nnK \u2212 n\u2212 n\u03b5\n\u221a \u221a \u221a \u221a K\n2\nK \u2211\nk=1\nE0\nn \u2211\nt=1\nMk,t\n\n\n(f) \u2265 \u03b5 2K\n(\nnK \u2212 n\u2212 n\u03b5 \u221a nK\n2\n)\n(g) \u2265 \u03b5n 4 \u2212 \u03b5 2n 3 2 2 \u221a 2K 1 2 ,\nwhere (a) follows since the max is greater than the average, (b) follows from (19), (c) is obtained by combining (17)\nand (18), (d) follows from the fact that \u2211K k=1 Mk,t \u2264 1, (e) is true by Jensen\u2019s inequality and (f/g) are straightforward. Choosing \u03b5 = \u221a\nK/(8n) leads to supk Rn(k) \u2265\u221a nK/(16 \u221a 2) as required."}, {"heading": "9 EXPERIMENTS", "text": "Data points were generated using the modified algorithm described in Section 7 and by taking the mean of 300 samples. With this many samples the standard error is relatively low (and omitted for readability). We should note that the variance in the regret of the modified algorithm is reasonably large, because the regret depends linearly on the random \u03b7k. For known lower bounds the variance is extremely low. To illustrate the behaviour of the algorithm we performed four experiments on synthetic data with K = 2, which are plotted below as TL (top left), TR, BL, BR (bottom right) respectively. In TL we fixed n = 104, \u03bd1 = 2 and plotted the regret as a function of \u03bd2 \u2208 [2, 10]. The experiment shows the usual bandit-like dependence on the gap 1/\u22061,2. In TR we fixed \u03bd1 = 4/10, \u03bd2 = 6/10 and plotted Rn/ log\n2 n as a function of n. The experiment lies within case 2 described in Section 4 and shows that the algorithm suffers regret Rn \u2248 45 log2 n as predicted by Theorem 2. In BL we fixed n = 105, \u03bd1 = 4/10 and plotted the regret as a function of \u03bd2 \u2208 [4/10, 1]. The results show the algorithm suffering O(log2 n) regret for both processes until the critical point when \u03bd2 > 6/10 when the second process can no longer be fully allocated, which is quickly learned and the algorithm suffers O(log2 n) regret for only one process. In BR we fixed \u03bd1 = 4/10 and \u03bd2 = 6/10 and plotted the regret as a function of n for two algorithms. The first algorithm (solid blue) is the modified version of Algorithm 1 as described in Section 7. The second (dotted red) is the same, but uses the unweighted estimator wk,t = 1 for all k and t. The result shows that both algorithms suffer sub-linear regret, but that the weighted estimator is a significant improvement over the unweighted one.\n2 3 4 5 6\n140\n160\n180\n\u03bd2\nR n\n0 1e6\n20\n40\nn\nR n lo g 2 n\n0.4 0.6 0.8 1\n2e3\n5e3\n\u03bd2\nR n\n0 1e5 0\n1e4\nn\nR n"}, {"heading": "10 CONCLUSIONS", "text": "We introduced the linear stochastic resource allocation problem and a new optimistic algorithm for this setting. Our main result shows that the new algorithm enjoys a (squared) logarithmic problem-dependent regret. We also presented a minimax lower bound of \u2126( \u221a nK), which is\nconsistent with the problem-dependent upper bound. The simulations confirm the theory and highlight the practical behaviour of the new algorithm. There are many open questions and possibilities for future research. Most important is whether the log2 n can be reduced to logn. Problemdependent lower bounds would be interesting. The algorithm is not anytime (although a doubling trick presumably works in theory). Developing and analysing algorithms when the horizon it not known, and have high-probability bounds are both of interest. We also wonder if Thompson sampling can be efficiently implemented for some reasonable prior, and if it enjoys the same practical and theoretical guarantees in this domain as it does for bandits. Other interesting extensions are when resources are not replenished, or the state of the jobs follow a Markov process. Finally, we want to emphasise that we have made just the first steps towards developing this new and interesting setting. We hope to see significant activity extending and modifying the model/algorithm for specific problems.\nAcknowledgements This work was supported by the Alberta Innovates Technology Futures, NSERC, by EU Framework 7 Project No. 248828 (ADVANCE), and by Israeli Science Foundation grant ISF- 1567/10. Part of this work was done while Csaba Szepesva\u0301ri was visiting Technion."}, {"heading": "A TECHNICAL INEQUALITIES", "text": "The proof of the following theorem is given in the supplementary material.\nTheorem 13. Let \u03b4 \u2208 (0, 1) and X1, . . . , Xn be a sequence of random variables adapted to filtration {Ft} with E[Xt|Ft\u22121] = 0. Let Rt be Ft\u22121-measurable such that |Xt| \u2264 Rt almost surely, R = maxt\u2264n Rt. Define S =\n\u2211n t=1 Xt, V 2 = \u2211n t=1 Var[Xt|Ft\u22121], and\n\u03b4r,v = \u03b4\n3(r + 1)2(v + 1)2 ,\nf(r, v) = r + 1\n3 log\n2\n\u03b4r,v\n+\n\u221a\n2(v + 1) log 2\n\u03b4r,v +\n(\nr + 1\n3\n)2\nlog2 2\n\u03b4r,v .\nThen P { |S| \u2265 f(R, V 2) } \u2264 \u03b4.\nProof of Theorem 13. Note that f(r, v) is strictly monotone increasing in both r and v. We now use a peeling\nargument. We have,\nP { |Sn| \u2265 f(R, V 2) }\n(a) =\n\u221e \u2211\nr=1\n\u221e \u2211\nv=1\nP { |Sn| \u2265 f(R, V 2), \u2308 V 2 \u2309 = v, \u2308R\u2309 = r }\n(b) \u2264 \u221e \u2211\nr=1\n\u221e \u2211\nv=1\nP { |Sn| \u2265 f(r \u2212 1, v \u2212 1), \u2308 V 2 \u2309 = v, \u2308R\u2309 = r }\n(c) \u2264 \u221e \u2211\nr=1\n\u221e \u2211\nv=1\n2 exp\n(\n\u2212 f(r \u2212 1, v \u2212 1) 2\n2v + 2rf(r\u22121,v\u22121)3\n)\n(d) \u2264 \u221e \u2211\nr=1\n\u221e \u2211\nv=1\n\u03b4r\u22121,v\u22121 (e) =\n\u03b4\n3\n\u221e \u2211\nr=1\n\u221e \u2211\nv=1\n1\nv2r2\n(f) \u2264 \u03b4 ,\nwhere (a) follows from the positivity of R and V , (b) by the monotonicity of f , (c) by Theorem 14 stated below (a martingale version Bernstein\u2019s inequality), (d) by Lemma 15, (e) by the definition of \u03b4r,v, (f) is trivial.\nTheorem 14 (Theorem 3.15 of McDiarmid 1998, see also Freedman 1975 and Bernstein 1946). Let X1, . . . , Xn be a sequence of random variables adapted to the filtration {Ft} with E[Xt|Ft\u22121] = 0. Further, let Rt be Ft\u22121measurable such that Xt \u2264 Rt almost surely, R = maxt\u2264n Rt and V 2 = \u2211n t=1 Var[Xt|Ft\u22121]. Then for any \u03b5, r, v > 0,\nP\n{\nn \u2211\nt=1\nXt \u2265 \u03b5, V 2 \u2264 v,R \u2264 r } \u2264 exp ( \u2212 \u03b5 2\n2v + 2\u03b5r3\n)\n.\nWe note that although this inequality is usually stated for deterministic Rt, the extension is trivial: Just define Yt = Xt 1{Rt \u2264 r} and apply the standard inequality to Yt. The result then follows since on R \u2264 r, Yt = Xt for all t and thus\n\u2211n t=1 Xt = \u2211n t=1 Yt.\nLemma 15. If \u03b5 \u2265 r3 log 2\u03b4 + \u221a 2v log 2\u03b4 + r2 9 log 2 2 \u03b4 , then 2 exp (\n\u2212 \u03b52 2v+ 2\u03b5r\n3\n)\n\u2264 \u03b4."}, {"heading": "B TABLE OF NOTATION", "text": "K number of jobs\nn time horizon\n\u03bdk parameter characterising difficulty of job k \u03b2(p) function \u03b2(p) := min {1, p} Mk,t resources assigned to job k in time-step t\nXk,t outcome of job k in time-step t\n\u03bdk,t lower bound on \u03bdk at time-step t\n\u03bd\u0304k,t upper bound on \u03bdk at time-step t\n\u03b4 bound on probability that some confidence intervals fails\n\u03c0t(i) ith easiest job at time-step t sorted by \u03bdk,t\u22121 \u2113 number of fully allocated jobs under optimal\nallocation\nS\u2217 optimal amount of resources assigned to overflow process A\u2217 contains the \u2113 easiest jobs (sorted by \u03bdk)\nAt set of jobs with Mk,t = \u03bdk,t\u22121 at time-step t\nBt equal to \u03c0t(\u2113+ 1) \u03b7k min{1,\u03bdk}\n\u03bdk,0"}], "references": [{"title": "Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space", "author": ["Rajeev Agrawal", "Demosthenis Teneketzis", "Venkatachalam Anantharam"], "venue": "IEEE Transaction on Automatic Control,", "citeRegEx": "Agrawal et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 1989}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Finitetime analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f3 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Bandits with knapsacks", "author": ["Ashwinkumar Badanidiyuru", "Robert Kleinberg", "Aleksandrs Slivkins"], "venue": "In FOCS, pages 207\u2013216,", "citeRegEx": "Badanidiyuru et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Badanidiyuru et al\\.", "year": 2013}, {"title": "A near-optimal algorithm for finite partialmonitoring games against adversarial opponents", "author": ["G\u00e1bor Bart\u00f3k"], "venue": "In COLT,", "citeRegEx": "Bart\u00f3k.,? \\Q2013\\E", "shortCiteRegEx": "Bart\u00f3k.", "year": 2013}, {"title": "Minimax regret of finite partial-monitoring games in stochastic environments", "author": ["G\u00e1bor Bart\u00f3k", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "COLT", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2011}, {"title": "The Theory of Probabilities (Russian)", "author": ["Sergei Bernstein"], "venue": "Moscow,", "citeRegEx": "Bernstein.,? \\Q1946\\E", "shortCiteRegEx": "Bernstein.", "year": 1946}, {"title": "Dynamic pricing under a general parametric choice model", "author": ["Josef Broder", "Paat Rusmevichientong"], "venue": "Operations Research,", "citeRegEx": "Broder and Rusmevichientong.,? \\Q2012\\E", "shortCiteRegEx": "Broder and Rusmevichientong.", "year": 2012}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Foundations and Trends in Machine Learning", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Now Publishers Incorporated,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Multi-armed bandit with budget constraint and variable costs", "author": ["Wenkui Ding", "Tao Qin", "Xu-Dong Zhang", "Tie-Yan Liu"], "venue": "In AAAI,", "citeRegEx": "Ding et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2013}, {"title": "Parametric bandits: The generalized linear case", "author": ["Sarah Filippi", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "Filippi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Filippi et al\\.", "year": 2010}, {"title": "No internal regret via neighborhood watch", "author": ["Dean P. Foster", "Alexander Rakhlin"], "venue": "Journal of Machine Learning Research - Proceedings Track (AISTATS),", "citeRegEx": "Foster and Rakhlin.,? \\Q2012\\E", "shortCiteRegEx": "Foster and Rakhlin.", "year": 2012}, {"title": "On tail probabilities for martingales", "author": ["David A. Freedman"], "venue": "The Annals of Probability, 3(1):100\u2013118,", "citeRegEx": "Freedman.,? \\Q1975\\E", "shortCiteRegEx": "Freedman.", "year": 1975}, {"title": "Continuous time associative bandit problems", "author": ["Andr\u00e1s Gy\u00f6rgy", "Levente Kocsis", "Ivett Szab\u00f3", "Csaba Szepesv\u00e1ri"], "venue": "In IJCAI-07,", "citeRegEx": "Gy\u00f6rgy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gy\u00f6rgy et al\\.", "year": 2007}, {"title": "Self-optimizing memory controllers: A reinforcement learning approach", "author": ["Engin Ipek", "Onur Mutlu", "Jos\u00e9 F. Mart\u0131\u0301nez", "Rich Caruana"], "venue": "SIGARCH Comput. Archit. News,", "citeRegEx": "Ipek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ipek et al\\.", "year": 2008}, {"title": "Organizing the last line of defense before hitting the memory wall for cmps", "author": ["Chun Liu", "Anand Sivasubramaniam", "Mahmut Kandemir"], "venue": "In Software, IEE Proceedings-,", "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "Concentration. In Probabilistic methods for algorithmic discrete mathematics, pages 195\u2013248", "author": ["Colin McDiarmid"], "venue": null, "citeRegEx": "McDiarmid.,? \\Q1998\\E", "shortCiteRegEx": "McDiarmid.", "year": 1998}, {"title": "Eluder dimension and the sample complexity of optimistic exploration", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "In NIPS,", "citeRegEx": "Russo and Roy.,? \\Q2013\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2013}, {"title": "Minimizing regret: The general case", "author": ["Aldo Rustichini"], "venue": "Games and Economic Behavior,", "citeRegEx": "Rustichini.,? \\Q1999\\E", "shortCiteRegEx": "Rustichini.", "year": 1999}, {"title": "A new memory monitoring scheme for memory-aware scheduling and partitioning", "author": ["G Edward Suh", "Srinivas Devadas", "Larry Rudolph"], "venue": "In High-Performance Computer Architecture,", "citeRegEx": "Suh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Suh et al\\.", "year": 2002}, {"title": "Knapsack based optimal policies for budget-limited multi-armed bandits", "author": ["Long Tran-Thanh", "Archie C. Chapman", "Alex Rogers", "Nicholas R. Jennings"], "venue": "In AAAI,", "citeRegEx": "Tran.Thanh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tran.Thanh et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "In fact, we anticipate numerous extensions and The name was invented later by (perhaps) [Rustichini, 1999].", "startOffset": 88, "endOffset": 106}, {"referenceID": 0, "context": "Given its information structure, the problem belongs to the class of stochastic partial monitoring problems, which was first studied by Agrawal et al. [1989]1, where in each time step the learner receives noisy information about a hidden \u201cparameter\u201d while trying to maximise the sum of rewards and both the information received and the rewards depend in a known fashion on the actions and the hidden parameter.", "startOffset": 136, "endOffset": 158}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989].", "startOffset": 252, "endOffset": 274}, {"referenceID": 6, "context": "adaptations for specific applications, such as in the case of bandits (see, Bubeck and Cesa-Bianchi [2012] for an overview of this rich literature).", "startOffset": 76, "endOffset": 107}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data.", "startOffset": 253, "endOffset": 589}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache.", "startOffset": 253, "endOffset": 721}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache. However, they all assume the model is fully known and no learning is required. Bitirgen et al. [2008] used ANNs to predict individual program performance as a function of resources.", "startOffset": 253, "endOffset": 948}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache. However, they all assume the model is fully known and no learning is required. Bitirgen et al. [2008] used ANNs to predict individual program performance as a function of resources. Finally, Ipek et al. [2008] used reinforcement learning to allocate DRAM to multi-processors.", "startOffset": 253, "endOffset": 1056}, {"referenceID": 0, "context": "Again, to contrast this work to previous works, note that the results we obtain for the full-information-like setting are distinct from those possible in the finite action case, where the full-information setting allows one to learn with finite regret [Agrawal et al., 1989]. On the technical side, we believe that our study and use of weighted estimators in situations where some samples are more informative than others might be of independent interest, too. Problems of allocating resources to jobs were studied in the community of architecture and operating systems. Liu et al. [2004] build static profile-based allocation of L2cache banks to different processes using their current miss rate data. Suh et al. [2002] proposed a hit-rate optimisation using hardware counters which used a model-based estimation of hit-rate vs allocated cache. However, they all assume the model is fully known and no learning is required. Bitirgen et al. [2008] used ANNs to predict individual program performance as a function of resources. Finally, Ipek et al. [2008] used reinforcement learning to allocate DRAM to multi-processors. 2 PRELIMINARIES In each time-step t the learner chooses Mk,t \u2265 0 subject to the constraint, \u2211K k=1 Mk,t \u2264 1. Then all jobs are executed and Xk,t \u2208 {0, 1} indicates the success or failure of job k in time-step t and is sampled from a Bernoulli distribution with parameter \u03b2(Mk,t/\u03bdk) := min {1,Mk,t/\u03bdk}. The goal is to maximise the expected number of jobs that successfully complete, \u2211Kk=1 \u03b2(Mk,t/\u03bdk). We define the gaps \u2206j,k = \u03bd \u22121 j \u2212 \u03bd\u22121 k . We assume throughout for conveBesides Badanidiyuru et al. [2013], all works consider finite action spaces and unstructured reward functions.", "startOffset": 253, "endOffset": 1630}, {"referenceID": 2, "context": "Then a bandit algorithm such as UCB1 [Auer et al., 2002] will achieve logarithmic (problem dependent) regret with some dependence on the gaps \u22061,k = 1 \u03bd1 \u2212 1 \u03bdk .", "startOffset": 37, "endOffset": 56}, {"referenceID": 2, "context": "The second component involves analysing the selection of the overflow process, where the approach is reminiscent of the analysis for the UCB algorithm for stochastic bandits [Auer et al., 2002].", "startOffset": 174, "endOffset": 193}, {"referenceID": 1, "context": "For the second term we need the following lemma, which uses Theorem 5 and a reasoning analogues to that of Auer et al. [2002] to bound the regret of the UCB algorithm for stochastic bandits: Lemma 10.", "startOffset": 107, "endOffset": 126}, {"referenceID": 1, "context": "8 MINIMAX LOWER BOUNDS Despite the continuous action space, the techniques used when proving minimax lower bounds for standard stochastic bandits [Auer et al., 1995] can be adapted to our setting.", "startOffset": 146, "endOffset": 165}], "year": 2014, "abstractText": "We study a sequential resource allocation problem involving a fixed number of recurring jobs. At each time-step the manager should distribute available resources among the jobs in order to maximise the expected number of completed jobs. Allocating more resources to a given job increases the probability that it completes, but with a cut-off. Specifically, we assume a linear model where the probability increases linearly until it equals one, after which allocating additional resources is wasteful. We assume the difficulty of each job is unknown and present the first algorithm for this problem and prove upper and lower bounds on its regret. Despite its apparent simplicity, the problem has a rich structure: we show that an appropriate optimistic algorithm can improve its learning speed dramatically beyond the results one normally expects for similar problems as the problem becomes resource-laden.", "creator": "Creator"}}}