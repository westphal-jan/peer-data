{"id": "1406.4631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2014", "title": "A Sober Look at Spectral Learning", "abstract": "normal learning recently exhibit lots historical excitement in machine learning, either because it is the lowest known method to produce reasonable precision ( necessarily suitable specifications ) for several latent variable models. in contrast, maximum event errors may get confused in local optima due to the non - linear nature of the optimal set by prediction variable judgments. in this paper, we suggest an interim measurement of spectral reconstruction ( sl ) entitled expectation maximization ( em ), which imply an important gap marking mean precision and the expectation. first, sl findings tend to negative probabilities. second, em often yields better parameters than spectral theory if cp does not generate or occur stuck in local decisions. we discuss if incorrect projections of the model parameters indicate various amount of simulation data can yield negative probabilities. students simply raise the common belief what maximum likelihood analyses are necessarily inconsistent.", "histories": [["v1", "Wed, 18 Jun 2014 08:25:03 GMT  (128kb,D)", "http://arxiv.org/abs/1406.4631v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["han zhao", "pascal poupart"], "accepted": false, "id": "1406.4631"}, "pdf": {"name": "1406.4631.pdf", "metadata": {"source": "META", "title": "A Sober Look at Spectral Learning", "authors": ["Han Zhao", "Pascal Poupart"], "emails": ["HAN.ZHAO@UWATERLOO.CA", "PPOUPART@UWATERLOO.CA"], "sections": [{"heading": "1. Introduction", "text": "Spectral Learning is a general approach that uses spectral decompositions (e.g., singular value decomposition and tensor decomposition) for parameter estimation based on the method of moments (Hsu et al., 2012; Parikh & Xing, 2011; Anandkumar et al., 2012a;c;b). Spectral learning has generated a lot of excitement in recent years due to its performance guarantees in latent variable models. The presence of discrete latent variables generally leads to a non-concave log-likelihood function, which is problematic for maximum likelihood estimators. Spectral learning is the first known method to be consistent (under suitable conditions) for several latent variable models including mixtures of Gaussians (MoGs), hidden Markov models (HMMs) and latent Dirichlet allocation (LDA). Furthermore, finite sample bounds guarantee that the approach will find nearly optimal parameters or make nearly optimal pre-\ndictions with high probability given a sufficient amount of training data (Hsu et al., 2012).\nWe report some experiments that suggest an important gap between the theory and the practice. Despite its theoretical guarantees, spectral learning often generates negative probabilities. This is an important issue that has received little attention so far. We show some empirical results that suggest that a poor choice of the rank of the model parameters and insufficient training data increase the likelihood of negative probabilities. We also investigate how well spectral learning performs in comparison to common approaches such as EM that do not enjoy the same theoretical guarantees. Interestingly, even though EM is subject to local optima and spectral learning is not, EM often outperforms spectral learning. Contrary to the common belief, we suggest that EM may be consistent in several settings. We discuss two situations under the assumption that the observation space is finite. When the true parameters are identifiable, increasing the amount of data often leads to a unimodal (though still non-concave) likelihood function, which explains why maximum likelihood estimators do not suffer from local optima. When the true parameters are unidentifiable (i.e., several equivalent solutions), the likelihood function remains multimodal, but if all the peaks of the likelihood function are at equivalent solutions, maximum likelihood estimators do not suffer from local optima. We also discuss two advantages of maximum likelihood estimators over spectral learning: a) maximum likelihood is a better objective to optimize than moment consistency and b) the data efficiency of maximum likelihood tends to be higher since it uses all empirical moments of the data (not just a few low order moments)."}, {"heading": "2. Spectral Learning for HMMs", "text": "Consider an HMM described as follows. Let x1, x2, x3, . . . denote a sequence of discrete observations where xt \u2208 [n] = {1, . . . , n} is the observation at time step t, and h1, h2, h3, . . . denotes a sequence of hidden states where ht \u2208 [m] = {1, . . . ,m} is the hidden state at time step t. The parameters of an HMM are (\u03c0, T,O) where \u03c0 \u2208 Rm is the initial state distribution, T \u2208 Rm\u00d7m is\nar X\niv :1\n40 6.\n46 31\nv1 [\ncs .L\nG ]\n1 8\nJu n\n20 14\nthe transition matrix and O \u2208 Rn\u00d7m is the observation matrix. More specifically, we have Pr(h1 = i) = \u03c0i, Pr(ht+1 = i|ht = j) = Tij and Pr(xt = i|ht = j) = Oij . Based on (\u03c0, T,O), we define an observable operator Ax = Tdiag(Ox,1, . . . , Ox,m) \u2208 Rm\u00d7m for each observation x \u2208 [n]. The joint probability of an observation sequence of length t can be computed based on these operators as follows:\nPr(x1, . . . , xt) = 1 T mAxt . . . Ax1\u03c0 (1)\nHsu et al. (2012) proposed a spectral algorithm called LearnHMM to estimate a transformed set of operators based on some low order empirical moments of the data. The following moment matrices are estimated from the data:\nP1 \u2208 Rn, [P1]i = Pr(x1 = i) P2,1 \u2208 Rn\u00d7n, [P2,1]ij = Pr(x2 = i, x1 = j) P3,x,1 \u2208 Rn\u00d7n, [P3,x,1]ij = Pr(x3 = i, x2 = x, x1 = j)\nLearnHMM requires a matrix U \u2208 Rn\u00d7m such that UTO is invertible. It is often chosen to be the first m left singular vectors that preserve the range of O. The following operators are then computed:\nb1 = U TP1 bT\u221e = P T 1 (U TP2,1) + Bx = U TP3,x,1(U TP2,1) + \u2200x \u2208 [n] (2)\nIf T and O are of rank m and \u03c0 is element-wise positive, it can be shown that\nPr(x1, . . . , xt) = b T \u221eBxt . . . Bx1b1\nThe classic parameters (\u03c0,O, T ) can also be recovered from the operators (Hsu et al., 2012).\nIn practice, since we do not know the exact moments, we obtain approximate moment matrices P\u03021, P\u03022,1, P\u03023,x,1 from the data and approximate operators b\u03021, b\u0302T\u221e, B\u0302x. Hsu et al. (2012) proved that joint probability estimates are consistent in the sense that\nlim N\u2192\u221e \u2211 x1,...,xt |Pr(x1, . . . , xt)\u2212 P\u0302r(x1, . . . , xt)| = 0 (3)\nwhere N is the sample size. They also showed that \u2200 > 0, the sample size needed to get an -bound on the estimate is polynomial in t and m. Several extensions and variants of this approach have been proposed for many latent variable models (Parikh et al., 2012; Parikh & Xing, 2011; Anandkumar et al., 2012a;c)."}, {"heading": "3. Negative Probabilities", "text": "We implemented LearnHMM and tested it on small and large synthetic discrete HMMs. The small HMM has 4 hidden states, 8 observations and the test set consists of 4096\nobservation sequences of length 4. The large HMM has 50 hidden states, 100 observations and a test set of 10,000 observation sequences of length 50. Fig. 1a and Fig. 1d show the normalized L1 error when estimating the probability of the test sequences as we vary the amount of training data and the rank hyperparameter m. The normalized L1 error is defined as follows:\nL1 = \u2211\n(x1,...,xt)\u2208T\n|Pr(x1, . . . , xt)\u2212 P\u0302r(x1, . . . , xt)| 1 t\nwhere T is the set of test sequences. We also report the proportion of negative probabilities\nNEG PROP = |{P\u0302r(x1, . . . , xt) < 0 | (x1, . . . , xt) \u2208 T }|\n|T |\ncomputed by LearnHMM in Fig. 1b and Fig. 1e. Negative probabilities are an important problem as they occur frequently. Increasing the amount of data and choosing a more accurate rank parameter tends to decrease the L1 error and the proportion of negative probabilities.\nNegative probabilities do not invalidate the theoretical guarantees of spectral learning. They simply reflect the fact that the theoretical guarantees are expressed in terms of bounds on additive error (see Theorem 6 in Hsu et al. (2012)). When the true probability is close to 0 and the bound is loose, it may guarantee that the estimated probability is in some interval that is partly negative. The problem of negative probabilities is well-known in the literature on observable operator models and was acknowledged by Boots et al. (2011) who rounded up all negative outputs to a number slightly above zero followed by normalization. In some spectral learning algorithms such as Excess Correlation Analysis for latent Dirichlet allocation (Anandkumar et al., 2012a), the parameters are estimated up to a sign. This means that an exact estimate of a distribution will normally be all positive or all negative and the sign can be flipped in the case of an entirely negative distribution. However, since the parameters are estimated approximately, the sign of the probabilities will often be mixed. It is not clear anymore whether the sign should be flipped. A simple heuristic consists of adding the probabilities of all outcomes and if the sum is negative, then flip the sign of all probabilities. After that, the negative probabilities can be rounded up to a number slightly higher than 0 followed by normalization. Here, there is a risk that the sign of the probabilities will be flipped when it should not. If most of the mass is negative due to the approximate nature of spectral learning, then the sign should not be flipped. Those heuristics will ensure that the final probabilities are positive, but they may increase the additive error. Spectral learning (with those heuristics) remains consistent in the limit, but the finite sample bounds need to be revised (this is an open problem).\nCan we modify spectral learning to ensure that all probabilities are non-negative? The root of the problem is that spectral learning implicitly solves a system of non-linear equations without restricting the space to non-negative solutions. Could we simply add additional constraints to ensure non-negativity? We conjecture that it will be NP-hard. Consider the problem of matrix factorization (i.e., find matrices A and B such that C=AB). If the entries of A and B may be any real number, then a solution can be found in polynomial time by singular value decomposition. However, if we want A and B to be non-negative then this becomes a problem of non-negative matrix factorization, which is NP-hard (Vavasis, 2009). Similarly, spectral learning finds operators (from which transition and observation matrices can be recovered) by singular value decomposition in polynomial time. If we add non-negativity constraints for the resulting transition and observation matrices, we conjecture that the problem will become NP-hard."}, {"heading": "4. Empirical Comparison with EM", "text": "We compared empirically spectral learning to expectation maximization (EM) on synthetic HMMs. The theory suggests that spectral learning should perform better since it is consistent while EM is subject to local optima, but the results are mixed. On the small synthetic HMM (Fig. 1c), with the true rank and sufficient data, spectral learning outperforms EM, but on the large synthetic HMM (Fig. 1f), EM outperforms spectral learning. The amount of training\ndata was the same for both problems. We suspect that the amount of training data was insufficient for spectral learning to estimate reasonable operators for the larger model. Furthermore, since spectral learning inverts a matrix to recover a similarity transform of the observable operators, it is unstable and sensitive to noise. We also noticed a large amount of negative probabilities. In general, spectral learning is very sensitive to the amount of data and the rank parameter as discussed in the previous section.\nWe also note that spectral learning does not optimize any desirable objective. Since it implicitly solves a non-linear system of equations induced by moment matching, if the moments matrices are too approximate, the resulting operators may be far from those that produced the data. If the system of equations is highly sensitive to perturbations, then spectral learning may yield terrible results. In contrast, EM directly maximizes the likelihood of the data. So even when there is little data it will find parameters that are likely to generate the data. The main issue with small datasets for EM is overfitting. We did not use regularization to mitigate overfitting in this experiment.\nAnother difference between spectral learning and EM is the information from the data that is used in training. Spectral learning does not use the raw training data. It uses only the first few empirical moments, which can be viewed as insufficient statistics. In contrast, EM trains with the raw data and therefore implicitly takes into account all the empirical moments including the higher order moments that spectral learning ignores."}, {"heading": "5. Local Optima", "text": "We were surprised by the fact that EM performed as well as it did since it may get stuck in arbitrarily bad local optima. While EM produced different results for different random restarts as shown by the error bars in Fig. 1c and 1f, the results were generally quite good and consistent. The variation for different random results can be explained by (minor) local optima or different amounts of overfitting. To investigate this further we constructed a single-parameter HMM for which we can visualize the likelihood function. This HMM has 2 states and 2 observations. We assume that the observation distribution is known and fixed (Pr(xi|hi) = 0.7 \u2200i) while the transition distribution is symmetric with a single parameter \u03b8 = Pr(hi+1 = n|hi = n) \u2200n, i indicating the probability that the current state remains unchanged. The likelihood function can be computed analytically for discrete latent variable models since it consists of an unnormalized mixture of Dirichlets. However, this mixture has one Dirichlet component per joint assignment of the latent variables. For a sequence of t time steps, this would yield an exponentially large mixture in t. However, when there is only one parameter \u03b8, several mixture components can be collapsed together and the number of different components in the mixture grows quadratically in t. Figure 2 shows the analytical likelihood function for data sequences of increasing length. Each curve is a mixture of Dirichlets corresponding to the likelihood function for observation sequences of different length. Since the likelihood functions are (unnormalized) mixtures of Dirichlets, we expect to see multimodal curves, but most of the curves in Fig. 2 are unimodal. This means that maximum likelihood estimators such as EM will perform very well. The fact that mixtures of Dirichlets tend to form unimodal curves as we increase the amount of data can be explained by the consistency of Bayesian learning (Casella & Berger, 1990). When we start with a uniform prior, the posterior in Bayesian learning is the likelihood function. Since Bayesian learning is consistent for discrete observation models, the posterior converges to a Dirac distribution in the limit as long as the true parameters are identifiable (i.e., unique solution) (Casella\n& Berger, 1990). Hence the likelihood function converges to a Dirac distribution too and we conjecture that EM is consistent in this setting.\nTo test our conjecture that EM is consistent we did another experiment with an HMM of 2 states, 2 observations and 4 parameters. Fig. 3 shows the likelihood of the training data for the solutions found by EM in comparison to the true parameters. Our conjecture would not hold if EM found solutions with lower likelihood than for the true parameters because this would mean that it got stuck in a local optimum. However as the the amount of training data increases EM consistently finds solutions with higher likelihood than for the true parameters and the variance of the likelihood vanishes. This suggests that EM found solutions that are all equivalent (i.e. no local optimum that is worse than the other optima). The fact that the likelihood is higher than for the true parameters simply indicates overfitting. While we suspect that EM is consistent in some settings under suitable conditions (that remain to be proven formally), we note that EM is inconsistent for some continuous observation models such as HMMs with continuous observations and mixture of Gaussians (MoGs). For MoGs, it is will known that EM may converge to a mixture of a Dirac distribution centered at one data point with a widespread Gaussian that fits the rest of the data (Bishop, 2006). This singular solution has infinite data likelihood, but it does not correspond to the true parameters, confirming the inconsistency of EM."}, {"heading": "6. Conclusion", "text": "Spectral learning is an exciting and promising line of research. In this work we showed that there is an important gap between the theory and the practice. We highlighted several open problems regarding negative probabilities and conjectured that EM may be consistent in some settings."}], "references": [{"title": "A Spectral Algorithm for Latent Dirichlet Allocation", "author": ["Anandkumar", "Anima", "Foster", "Dean P", "Hsu", "Daniel"], "venue": "In NIPS, pp", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Tensor Decompositions for Learning Latent Variable Models", "author": ["Anandkumar", "Anima", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M", "Telgarsky", "Matus"], "venue": "In arXiv preprint arXiv:1210.7559,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A Method of Moments for Mixture Models and Hidden Markov Models", "author": ["Anandkumar", "Animashree", "Hsu", "Daniel", "Kakade", "Sham M"], "venue": "arXiv preprint arXiv:1203.0683,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Pattern recognition and machine learning, volume 1. springer", "author": ["Bishop", "Christopher M"], "venue": "New York,", "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Boots et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "A spectral algorithm for learning Hidden Markov Models", "author": ["Hsu", "Daniel", "Kakade", "Sham M", "Zhang", "Tong"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "A Spectral Algorithm for Latent Tree Graphical Models", "author": ["Parikh", "Ankur", "Xing", "Eric P"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Parikh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2011}, {"title": "A Spectral Algorithm for Latent Junction Trees", "author": ["Parikh", "Ankur", "Teodoru", "Gabi", "Tech", "Georgia", "Ishteva", "Mariya", "Xing", "Eric P"], "venue": "arXiv preprint arXiv:1210.4884,", "citeRegEx": "Parikh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2012}, {"title": "On the complexity of nonnegative matrix factorization", "author": ["Vavasis", "Stephen A"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Vavasis and A.,? \\Q2009\\E", "shortCiteRegEx": "Vavasis and A.", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "Furthermore, finite sample bounds guarantee that the approach will find nearly optimal parameters or make nearly optimal predictions with high probability given a sufficient amount of training data (Hsu et al., 2012).", "startOffset": 198, "endOffset": 216}, {"referenceID": 5, "context": "Bx1b1 The classic parameters (\u03c0,O, T ) can also be recovered from the operators (Hsu et al., 2012).", "startOffset": 80, "endOffset": 98}, {"referenceID": 5, "context": "Hsu et al. (2012) proved that joint probability estimates are consistent in the sense that", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "They simply reflect the fact that the theoretical guarantees are expressed in terms of bounds on additive error (see Theorem 6 in Hsu et al. (2012)).", "startOffset": 130, "endOffset": 148}, {"referenceID": 1, "context": "The problem of negative probabilities is well-known in the literature on observable operator models and was acknowledged by Boots et al. (2011) who rounded up all negative outputs to a number slightly above zero followed by normalization.", "startOffset": 124, "endOffset": 144}], "year": 2014, "abstractText": "Spectral learning recently generated lots of excitement in machine learning, largely because it is the first known method to produce consistent estimates (under suitable conditions) for several latent variable models. In contrast, maximum likelihood estimates may get trapped in local optima due to the non-convex nature of the likelihood function of latent variable models. In this paper, we do an empirical evaluation of spectral learning (SL) and expectation maximization (EM), which reveals an important gap between the theory and the practice. First, SL often leads to negative probabilities. Second, EM often yields better estimates than spectral learning and it does not seem to get stuck in local optima. We discuss how the rank of the model parameters and the amount of training data can yield negative probabilities. We also question the common belief that maximum likelihood estimators are necessarily inconsistent.", "creator": "LaTeX with hyperref package"}}}