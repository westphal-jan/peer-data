{"id": "1409.5330", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2014", "title": "Learning and approximation capability of orthogonal super greedy algorithm", "abstract": "we consider the approximation feasible opposite conventional super greedy algorithms ( cfa ) becoming suitable applications in supervised harvesting. osga is concerned its selecting minimum uniformly one machine in continuous iteration step, plus, of the, greatly reduces thus additional confusion when compared with cheaper conventional orthogonal filtering algorithm ( oga ). we perform versions of larger function classes that without dominating the convex hull of the dictionary, osga does normally employ the approximation capability of oga provided the correction is incoherent. satisfied on these, we find certain general generalization error bound for procedural learning. our solutions maintain that in the name of supervised learning, osga provides better path to further reduce efficient computational congestion versus oga in the premise on maintaining its effective processing capability.", "histories": [["v1", "Thu, 18 Sep 2014 15:09:47 GMT  (49kb)", "http://arxiv.org/abs/1409.5330v1", "30 pages,14 figures"]], "COMMENTS": "30 pages,14 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jian fang", "shaobo lin", "zongben xu"], "accepted": false, "id": "1409.5330"}, "pdf": {"name": "1409.5330.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jian Fang", "Shaobo Lin", "Zongben Xu"], "emails": ["sblin1983@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n53 30\nv1 [\ncs .L\nG ]\n1 8\nSe p\nWe consider the approximation capability of orthogonal super greedy algorithms (OSGA) and its applications in supervised learning. OSGA is concerned with selecting more than one atoms in each iteration step, which, of course, greatly reduces the computational burden when compared with the conventional orthogonal greedy algorithm (OGA). We prove that even for function classes that are not the convex hull of the dictionary, OSGA does not degrade the approximation capability of OGA provided the dictionary is incoherent. Based on this, we deduce a tight generalization error bound for OSGA learning. Our results show that in the realm of supervised learning, OSGA provides a possibility to further reduce the computational burden of OGA in the premise of maintaining its prominent generalization capability. Keywords: supervised learning, nonlinear approximation, orthogonal super greedy algorithm, orthogonal greedy algorithm."}, {"heading": "1. Introduction", "text": "A greedy algorithm is a stepwise inference process that follows the problem solving heuristic of making the locally optimal choice at each stape with the hope of finding a global optimum. The use of greedy algorithms in the context of nonlinear approximation [1] is very appealing since it greatly reduces the computational burden when compared\n\u2729The research was supported by the National 973 Programming (2013CB329404), the Key Program of National Natural Science Foundation of China (Grant No. 11131006).\n\u2217Corresponding author: sblin1983@gmail.com\nPreprint submitted to Elsevier September 19, 2014\nwith standard model selection using general dictionaries. This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].\nGreedy learning, or more specifically, applying greedy algorithms to tackle supervised learning problems, has been proved to possess charming generalization capability with lower computational burden than the widely used coefficient-based regularization methods [1]. From approximation to learning, greedy learning can be usually formulated as a four-stage stepwise learning strategy [32]. The first one is the \u201cdictionary-selection\u201d stage which constructs a suitable set of basis functions. The second one is the \u201cgreedydefinition\u201d stage that sets the measurement criterion to choose new atoms (or elements) from the dictionary in each greedy step. The third one is the \u201citerative-rule\u201d stage that defines the estimator based on the selected \u201cgreedy atoms\u201d and the estimator obtained in the previous greedy step. The last one is the \u201cstopping-criterion\u201d stage which focuses on how to terminate the learning process.\nSince greedy learning\u2019s inception in supervised learning [14], the aforementioned four stages were comprehensively studied for various purposes. For the \u201cdictionary-selection\u201d stage, Chen et al. [4] and Lin et al. [17] proposed that the kernel based dictionary is a good choice for greedy learning. For the \u201cgreedy-definition\u201d stage, Xu et al. [32] pointed out that the metric of greedy-definition is not uniquely the greediest one. They provided a threshold to discriminate whether or not a selection is greedy and analyzed the feasibility of such a discrimination measurement. For the \u201citerative-rule stage\u201d, Barron et al. [1] declared that both relaxed greedy iteration and orthogonal greedy iteration can achieve a fast learning rate for greedy learning. For the \u201cstopping-criterion\u201d stage, Barron et al. [1] provided an l0 complexity regularization strategy and Chen et al. [4] proposed an l1 complexity constraint strategy. All these results showed that as a new learning scheme, greedy learning deserves avid studying due to its stepwise learning character [14].\nAlthough the importance of a single stage of greedy learning was widely studied [1, 4, 17, 31], the relationship between these stages and their composite effects for learning also need classifying. In the recent work [32], Xu et al. established a relationship between the \u201cgreedy-definition\u201d and \u201cstopping-criterion\u201d stages and successfully reduced the com-\nputational cost of greedy learning without sacrificing the generalization capability. This implies that the study of these relationships may bring additional benefits of greedy learning. In this paper, we aim to study the relationship between the \u201cdictionary-selection\u201d and \u201cgreedy-definition\u201d stages of orthogonal greedy algorithms (OGA). Our idea mainly stems from an interesting observation. We observe that if the selected dictionary is an orthogonal basis, then it is not necessary to define greedy learning as a stepwise strategy. Indeed, due to the orthogonal property, we can select all required atoms from the dictionary simultaneously. Conversely, if the dictionary is redundant (or linear dependent), then greedy learning must be defined as a stepwise strategy due to the redundant property which usually causes a disorder of the learning process. This implies that specific features of a dictionary can be used to modify the greedy definition.\nTherefore, if the coherence, a specific feature of a dictionary, is utilized to describe the dictionary, we can improve the performance of OGA in the direction of either reducing the computational burden or enhancing the generalization capability. In this paper, we study the learning capability of orthogonal super greedy algorithm (OSGA) which was proposed by Liu and Temlyakov [18]. OSGA selects more than one atoms from a dictionary in each iteration step and hence reduces the computational burden of OGA. The aim of the present paper can be explained in two folds. The first one is to study the approximation capability of OSGA and the other is to pursue the pros and cons of OSGA in the context of supervised learning.\nFor OSGA approximation, it was shown in [18] (see also [19]) that for incoherent dictionaries, OSGA reduces the computational burden when compared with OGA. It can be found in [18, Theorem 2] that such a significant computational burden-reduction does not degrade the approximation capability if the target functions belong to the convex hull of the dictionary. However, such an assumption to the target functions is very stringent if the dimension of variable is large [1]. Our purpose is to circumvent the above problem by deducing convergence rates for functions not simply related to the convex hull of the dictionary. Interestingly, we find that, even for functions out of the convex hull of the dictionary, the approximation capability of OSGA is similar as that of OGA [1].\nFor OSGA learning, we find that if the dictionary is incoherent, then OSGA learning\nwith appropriate step-size can reduce the computational burden of OGA learning further. In particular, using the established approximation results of OSGA, we can deduce an almost same learning rate as that of OGA. This means that studying the relationship between the \u201cdictionary-selection\u201d and \u201cgreedy-definition\u201d stages can build more efficient learning schemes with the same rate of convergence as OGA.\nThe paper is organized as follows. In Section 2, we review notations and preliminary results in greedy-type algorithms that are frequently referred to throughout the paper. In Section 3, we show the main result of this paper where a general approximation theorem for OSGA and its applications in supervised learning are established. In Section 4, we present a line of simulations to verify our viewpoints. In Section 5, we give proofs of the main results. In the last section, we further discuss the OSGA learning and draw a simple conclusion of this paper."}, {"heading": "2. Greedy-type algorithms", "text": "Let H be a Hilbert space endowed with norm and inner product \u2016 \u00b7 \u2016 and \u3008\u00b7, \u00b7, \u3009, respectively. Let D = {g}g\u2208D be a given dictionary. Define L1 = {f : f = \u2211\ng\u2208D agg}. The norm of L1 is defined by \u2016f\u2016L1 := inf { \u2211 g\u2208D |ag| : f = \u2211 g\u2208D agg } . We shall assume here and later that the elements of the dictionary are normalized according to \u2016g\u2016 = 1. There exist several types of greedy algorithms [25]. The four most commonly used are the pure greedy, orthogonal greedy, relaxed greedy and stepwise projection algorithms, which are often denoted by their acronyms PGA, OGA, RGA and SPA, respectively. In all the above greedy algorithms, we begin by setting f0 := 0. The new approximation fk (k \u2265 1) is defined based on fk\u22121 and its residual rk\u22121 := f \u2212 fk\u22121. In OGA, fk is defined as\nfk = PVkf,\nwhere PVk is the orthogonal projection onto Vk = span{g1, . . . , gk} and gk is defined as\ngk = argmax g\u2208D\n|\u3008rk\u22121, g\u3009|.\nLet\nM = M(D) = sup g 6=h,g,h\u2208D |\u3008g, h\u3009|\nbe the coherence of the dictionary D. Let s \u2265 1 be a natural number. Initially, set f s0 = 0 and rs0 = f , then the OSGA proposed in [18] for each k \u2265 1 can be inductively define as the following.\n1) g(k\u22121)s+1, . . . , gks \u2208 D are chosen according to\nmin i\u2208Ik |\u3008rsk\u22121, gi\u3009| \u2265 sup g\u2208D,g 6=gi,i\u2208Ik |\u3008rsk\u22121, g\u3009|,\nwhere Ik = [(k \u2212 1)s+ 1, ks]. 2) Let Vks = span{g1, . . . , gks} and define\nf sk := PVksf, (2.1)\nand\nrsk = f \u2212 f sk .\nThe following Lemma 2.1 proved in [18] shows that OSGA can achieve the optimal\napproximation rate of ks term nonlinear approximation [24].\nLemma 2.1. Let D be a dictionary with coherence M . Then, for s \u2264 (2M)\u22121 + 1, the OSGA estimator (2.1) provides an approximation of f \u2208 L1 with the following error bound:\n\u2016rsk\u20162 \u2264 40.5\u2016f\u2016L1(sk)\u22121, k = 1, 2, . . . ."}, {"heading": "3. Approximation and learning by OSGA", "text": "In this section, after presenting some basic conceptions of the statistical learning theory, we deduce a general approximation theorem concerning OSGA and pursue its applications in supervised learning."}, {"heading": "3.1. Statistical learning theory", "text": "In most of machine learning problems, data are taken from two sets: the input space X \u2286 Rd and the output space Y \u2286 R. The relation between the variable x \u2208 X and the variable y \u2208 Y is not deterministic, and is described by a probability distribution \u03c1 on Z := X \u00d7 Y that admits the decomposition\n\u03c1(x, y) = \u03c1X(x)\u03c1(y|x),\nin which \u03c1(y|x) denotes the conditional (given x) probability measure on Y , and \u03c1X(x) the marginal probability measure on X . Let z = (xi, yi) n i=1 be a set of finite random samples of size n, n \u2208 N, drawn identically, independently according to \u03c1 from Z. The set of examples z is called a training set. Without loss of generality, we assume that |yi| \u2264 L for a prescribed (and fixed) L > 0. The goal of supervised learning is to derive a function f : X \u2192 Y from a training set such that f(x) is an effective and reliable estimate of y when x is given. A natural measurement of the error incurred by using f(x) for this purpose is the generalization error, given by\nE(f) := \u222b\nZ\n(f(x)\u2212 y)2d\u03c1,\nwhich is minimized by the regression function [6], defined by\nf\u03c1(x) :=\n\u222b\nY\nyd\u03c1(y|x).\nThis ideal minimizer f\u03c1 exists in theory only. In practice, we do not know \u03c1, and we can only access random examples from X \u00d7 Y sampled according to \u03c1. Let L2\u03c1\nX be the Hilbert space of \u03c1X square integrable function onX , with norm denoted\nby \u2016 \u00b7 \u2016\u03c1. With the assumption that f\u03c1 \u2208 L2\u03c1 X , it is well known [5] that, for every f \u2208 L2\u03c1X , there holds\nE(f)\u2212 E(f\u03c1) = \u2016f \u2212 f\u03c1\u20162\u03c1. (3.1)\nThe task of the least square regression problem is then to construct functions f z that approximates f\u03c1, in the norm \u2016 \u00b7 \u2016\u03c1, using finite samples."}, {"heading": "3.2. Approximation capability of OSGA", "text": "A consensus in the nonlinear approximation community is that greedy-type algorithms can break the \u201ccurse of dimensionality\u201d [24]. Lemma 2.1 seems to verify this assertion, since a dimensional independent convergence rate was deduced. We find, however, this is not exactly true since, in practice, the condition that the target functions belong to the convex hull of the dictionary becomes more and more stringent as the dimension of variable grows [1]. The similar phenomenon concerning OGA approximation was successfully\ntackled in [1] by proving convergence results for a variety of function classes and not simply those are related to the convex hull of the dictionary.\nAlong the flavor of [1], we study the behavior of OSGA approximation when the target\nfunctions f \u2208 H are more general. We consider the real interpolation spaces [3]\nRp = [H,L1]\u03b8,\u221e, 0 < \u03b8 < 1,\nwith p defined by 1\np = \u03b8 + 1\u2212 \u03b8 2 = 1 + \u03b8 2 .\nRecall that f \u2208 [X, Y ]\u03b8,\u221e if and only if for all t > 0, there holds [10]\nK(f, t) \u2264 Ct\u03b8, (3.2)\nwhere\nK(f, t) = K(f, t, X, Y ) = inf h\u2208Y\n{\u2016f \u2212 h\u2016X + t\u2016h\u2016Y }\nis the so-called K-functional. The smallest C satisfying (3.2) defines a norm for the interpolation space [X, Y ]\u03b8,\u221e. Based on these preliminaries, we can obtain the following Theorem 3.1.\nTheorem 3.1. Let D be a dictionary with coherence M . Then for all f \u2208 H, any h \u2208 L1 and arbitrary s \u2264 (2M)\u22121 + 1, the OSGA estimator (2.1) satisfies\n\u2016rsk\u20162 \u2264 \u2016f \u2212 h\u20162 + 27\n2 \u2016h\u20162L1(sk) \u22121, k = 1, 2, . . . , (3.3)\nand therefore,\n\u2016rsk\u2016 \u2264 K ( f, 3 \u221a 6\n2 (sk)\u22121/2, H,L1\n)\n, k = 1, 2, . . . . (3.4)\nFrom the definition of the interpolation space Rp and (3.4), it follows that f \u2208 Rp implies the rate of decay\n\u2016rsk\u2016 \u2264 C1(ks)\u2212\u03b8/2.\nA similar error estimate for OGA has been provided in [1, Theorem 2.3], which says that for all f \u2208 H and any h \u2208 L1, the error of the OGA estimator satisfies\n\u2016rN\u20162 \u2264 \u2016f \u2212 h\u20162 + 4\u2016h\u20162L1N\u22121, N = 1, 2, . . . . (3.5)\nWe note that OSGA adds s new atoms at each iteration and makes one orthogonal projection at each iteration. After k iterations of OSGA, there are totally sk atoms to build up the estimator. For comparison, OGA adds one atom at each iteration and makes one orthogonal projection at each iteration. While, it is obvious that there need sk iterations of OGA to deduce an estimator with sk atoms. Thus, the computational cost of OSGA is near s times lower than OGA. (3.3) together with (3.5) yields that such a computational burden reduction does not degenerate the approximation capability. The reason of this is that the specific feature of the dictionary, M-coherence, is used in OSGA. It can be found in Theorem 3.1 that if M > 1/2, OSGA coincides with OGA."}, {"heading": "3.3. OSGA learning", "text": "It was pointed out in [18] that OSGA can be applied in compressed sensing very well. In this subsection, we pursue its applications in supervised learning. It can also be found in [1, Theorem 3.1] that the error estimate formed as (3.3) plays an important role in analyzing the generalization capability of greedy-type algorithms. Based on this, we can deduce the generalization error of OSGA in the context of regression.\nGiven training samples z, we define the empirical norm and inner product as\n\u2016f\u20162n = 1\nn\nn \u2211\ni=1\n|f(xi)|2,\nand\n\u3008f, g\u3009n = 1\nn\nn \u2211\ni=1\nf(xi)g(xi),\nrespectively. The OSGA learning scheme studied in this subsection is shown in the following Algorithm 1.\nAlgorithm 1 OSGA learning. Initialization: Data z = (xi, yi) n i=1, step-size s, iteration number m, inner product \u3008\u00b7, \u00b7\u3009n, f0 = 0, V0 = \u2205, r s z,0(x) satisfies r0(xi) = yi, and y(x) satisfies y(xi) = yi. Stage1: Dictionary-selection: Select\nDN := {gi : i = 1, . . . , N}\nwith \u2016gi\u2016n = 1. Stage2: Greedy definition: Choose g(k\u22121)s+1, . . . , gks \u2208 DN according to\nmin i\u2208Ik\n|\u3008rs z,k\u22121, gi\u3009n| \u2265 sup\ng\u2208D,g 6=gi,i\u2208Ik\n|\u3008rs z,k\u22121, g\u3009n|, (3.6)\nwhere rs z,k\u22121 is the residual defined by r s z,k\u22121 := y \u2212 f sz,k\u22121 and Ik = [(k \u2212 1)s+ 1, ks]. Stage3. Iterative rule: Let Vks = Span(g1, ..., gks). Compute the k step approximation f s z,k as:\nf s z,k = Pz,Vks(y) (3.7)\nand the residual: rs z,k := y\u2212 f sz,k, where Pz,Vks is the orthogonal projection onto space Vks in the metric of \u3008\u00b7, \u00b7\u3009n.\nStage4. Stopping criterion : If k = m then stop and obtain the final estimator f s z,m, otherwise set k = k + 1 and repeat Stage 1-Stage 4.\nIt is shown in Algorithm 1 that the only difference between OSGA and OGA learning [1] is that in OSGA there are s atoms selected in the \u201cgreedy-definition\u201d stage. Therefore the computational burden of OGA is further reduced. The first result in this subsection is to illustrate that such a reduction do not degrade the generalization capability of OGA learning, provided the dictionary is incoherent and the step-size is appropriated tuned.\nTheorem 3.2. Let f s z,m be defined as in Algorithm 1, and DN be a dictionary with coherence M . If s \u2264 (2M)\u22121 + 1, then for all functions h in span(DN), there holds\nE(\u2016\u03a0Lf sz,m \u2212 f\u03c1\u20162\u03c1) \u2264 8\u2016h\u2212 f\u03c1\u20162\u03c1 + 108\nsm \u2016h\u20162L1,N + C\nms log n\nn ,\nwhere \u03a0Lu := min{L, |u|}sgn(u) is the truncation operator at level L,\n\u2016f\u2016L1,N := inf{ N \u2211\ni=1\n|ai| : f = N \u2211\ni=1\naigi},\nand C is a constant depending only on f\u03c1 and L.\nIn Theorem 3.2, we propose a truncation operator on the OSGA estimator. It should be noted that such a truncation operator does not require any computation. Furthermore, as y \u2208 [\u2212L, L], it is easy to deduce [33] that\n\u2016\u03a0Lf sz,m \u2212 f\u03c1\u20162\u03c1 \u2264 \u2016f sz,m \u2212 f\u03c1\u20162\u03c1.\nTheorem 3.2 provides an oracle-type error estimate for the OSGA learning, since the final error estimate can only be deduced some oracle about the regression function. We further notice that up to the constant, the deduced oracle inequality is the same as that deduced in [1] with k in [1, Theorem 3.1] replaced by ms in Theorem 3.2. Therefore, as a computational burden reduction version of OGA learning, OSGA learning does not degrade the generalization capability of OGA learning in the sense that they can obtain the same learning rate.\nTo classify the learning rate, we should give some assumptions (oracle) to the regression function. Along [1]\u2019s flavor, for r > 0, we define the space Lr1 as the set of all functions f such that, for all N , there exists h \u2208 span{DN} satisfying\n\u2016h\u2016L1 \u2264 B, and \u2016f \u2212 h\u2016\u03c1 \u2264 BN\u2212r. (3.8)\nThe infimum of all such B defines a norm (for f ) on Lr1. Furthermore, let a \u2265 1 is fixed we assume that the size of dictionary, N , satisfies N \u223c na. If f\u03c1 \u2208 Lr1, we can deduce the following learning rate estimate of OSGA learning.\nCorollary 3.3. Suppose that the assumptions of Theorem 3.2 holds. If f\u03c1 \u2208 Lr1, N \u223c na and a \u2265 1\n4r , then we can choose m satisfying m \u223c n1/2 s such that\nE(\u2016\u03a0Lf sz,m \u2212 f\u03c1\u20162\u03c1) \u2264 C (n/ log n)\u22121/2 .\nwhere C is a constant depending only on L and f\u03c1.\nIt should be highlighted that the main difficulty of OSGA learning is to select an appropriate iteration number, m. Corollary 3.3 proposes a strategy of selecting the best m, but the main flaw is that such a choice depends heavily on the prior f\u03c1 \u2208 Lr1. In practice, it is usually impossible to verified. Thus, we turn to pursue a universal strategy to fix m. Hence, we use the same l0 complexity regularization strategy as that in [1] to choose m.\nWe define the estimator f\u0302 = \u03a0f z,m\u2217 , where m\n\u2217is chosen to minimize (over all m > 0)\nthe penalized empirical risk\n\u2016y \u2212 \u03a0Lf sz,m\u20162n + \u03ba ms logn\nn , (3.9)\nwith \u03ba a constant depending only on L and a.\nNoting Theorem 3.1, using the almost same method as that in [1, Theorem 3.1] we\ncan deduce the following Theorem 3.4.\nTheorem 3.4. If the assumptions in Theorem 3.2 hold, then there exists a \u03ba depending only on L and a such that for all m > 0 and h \u2208 span(DN), there holds\nE(\u2016f\u0302 \u2212 f\u03c1\u20162\u03c1) \u2264 8\u2016h\u2212 f\u03c1\u20162\u03c1 + 108\nsm \u2016h\u20162L1,N + C\nms logn\nn ,\nwhere C is a constant depending only on \u03ba and L.\nFor the sake of brevity, we omit the proof of Theorem 3.4. We refer the readers to the proof of Theorem 3.1 in [1] for the details. If some assumptions are added to the regression function f\u03c1, then we can also deduce the following learning rate estimate.\nCorollary 3.5. If f\u03c1 \u2208 Lr1, a \u2265 14r and the assumptions of Theorem 3.4 holds, there exists a \u03ba depending only on L and a such that for all m > 0 and h \u2208 span(DN),\nE(\u2016f\u0302 \u2212 f\u03c1\u20162\u03c1) \u2264 C (n/ logn)\u22121/2 .\nwhere C is a constant depending only on \u03ba, L and f\u03c1.\nCorollary 3.5 together with [1, Corollary 3.6] shows that OSGA does not degenerate the learning performance when compared with OGA by using the l0 complexity regularization strategy to fix the iteration number. However, it has already been pointed out in [1, Remark 3.5] that \u03ba should satisfy \u03ba \u2265 2568L4(a + 5). Such a pessimistic estimate\nmakes the l0 complexity regularization strategy (3.9) always infeasible. In practice, this may result in selecting a too small value for m\u2217. Many of the programmers\u2019 spirit will be dampened by this restriction, and shy away from running OSGA for large m. Therefore, the value of the previous results is only to classify the theoretical feasibility. To facilitate the use of OSGA, we need to find another strategy to choose m rather than the l0 complexity regularization (3.9). A widely used approach is the so-called \u201ccross-validation\u201d [15, Chapter 8], which has also proposed for OGA learning by Barron et al. in [1]."}, {"heading": "4. Simulation Supports", "text": "In this section, we present several toy simulations to illustrate the feasibility, effectiveness, and efficiency of OSGA learning. The main purpose can be divided into three aspects. The first one is to reveal that there exists a relationship between the \u201cdictionaryselection\u201d stage and \u201cgreedy-definition\u201d stages for greedy learning. Since the incoherence assumption is too strict to describe the property of the dictionary and difficult to verify, especially for supervised learning [21], we do not implement the simulation for dictionaries with such a pessimistic assumption. Instead, we utilize two widely used dictionaries such as the trigonometric polynomial dictionary and Gaussian radial basis function dictionary to justify our viewpoint. The second one is to analyze the pros and cons of OSGA learning. In particular, we compare both the training time and test time between OSGA learning and OGA learning with different dictionaries for different regression functions. The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14]. It should be noted that the aim of this simulation is to compare different learning strategies for fixed dictionaries, thus we only employ two fixed dictionaries rather than pursuing the best dictionary. Given the dictionary, we then analyze different performances of the aforementioned different learning schemes."}, {"heading": "4.1. Experiment Setup", "text": "Data sets: we employ two regression functions f\u03c1 as\nf1(x) = sinc(40x\u2212 10) + sinc(60x\u2212 30) + sinc(20x\u2212 1) + cos(10x),\nwhere sinc(t) = sin t t , and\nf2(x) =\n    \n    1/3\u2212 x, if 0 \u2264 x < 1/3, x2, if 1/3 \u2264 x \u2264 2/3; \u22121, 2/3 < x \u2264 1.\nIt is easy to see that f1 is an infinitely differential function and f2 is a discontinuous function. We generated the training sample set z = {(xi, yi)}5000i=1 through independently and randomly sampling xi from U(0, 1) , and the corresponding y\u2032is to be yi = f\u03c1(xi) + \u01eb, with \u03b5 \u223c N(0, 0.1) being the white noise. The learning performances of different algorithms were then tested by applying the resultant estimators to the test set ztest = {(x(t)i , y (t) i )}5000i=1 which was generated similarly to z but with a promise that y\u2032is were always taken to be y (t) i = f\u03c1(x (t) i ).\nDictionary: In each simulation, we adopt two types of dictionaries. The first one is constructed by the Gaussian radial basis function: exp{\u2212\u03c3|x \u2212 ti|2} with {ti}500i=1 being drawn identically and independently from U(0, 1) and appropriately selected \u03c3. Indeed, we set \u03c3 = 200 for f1 and \u03c3 = 1000 for f2. The other one is constructed by the trigonometric polynomial: cos kt with k \u2208 {1, 2, ..., 500}. Methods: For OSGA and OGA learning, we applied QR decomposition to acquired the least squared estimates [22]. For greedy boosting, we used the L2boost algorithm [14] with the step size 0.0005. For L2 coefficient regularization (or ridge regression), we use its analytic regularized least square solution [5]. For L1 (or lasso) and L1/2 coefficient regularization schemes, we utilize the iterative soft [8] and half [30] thresholding algorithms to obtain the corresponding estimators, respectively.\nMeasurements: Since the aim of the toy simulations is to justify the feasibility of OSGA, we don\u2019t access any concrete parameter-selection strategies. Therefore, we draw our conclusion in the basis of the most appropriate parameters. Under this circumstance,\nwe do not divide the training set into training data and validation data and use validation data to choose parameters as in [15], instead, we use the test set to fix parameters directly. To be detailed, the iteration steps for OSGA, OGA and greedy boosting, and the regularization parameters \u03bb \u2208 {2\u221210, 2\u22129, ..., 210} for Lq coefficients regularization with q = 1/2, 1, 2 are selected when the prediction error on the test data is minimized. We recorded the rooted mean squared error (RMSE) of test error, the sparsity of the coefficients, and the training time under the selected parameters to measure the performances of the mentioned learning schemes.\nEnvironment: All the simulations and experiments were conducted in Matlab R2013a on a desktop computer with Windows 7/Intel(R)/Core(TM) i7-3770K RAM and 3.50GHz CPU, and the statistics were averaged based on 100 independent trials."}, {"heading": "4.2. The relationship between dictionary-selection and greedy-definition", "text": "Theorem 3.2 theoretically presents that if the relationship between the \u201cdictionaryselection\u201d and \u201cgreedy-definition\u201d stages is considered, then the efficiency of greedy learning can be essentially improved. However, such a theoretical result is built on the incoherence property of the dictionary. As is shown in [21], the incoherence assumption in the background of supervised learning is too strict to describe the property of dictionaries. We guess that there may exist a much looser measurement than it within our purpose. To verify this conjecture, we employ both trigonometric polynomial dictionary (TPD) and Gaussian radial basis dictionary (GRD) to be the carriers of OSGA. It can be found in [2] that the TPD dictionary together with the random samples can develop a wellconditioned sampling matrix [2, Theorem 5.1], while the sampling matrix constructed by GRD is usually ill-conditioned [20]. We compare the TPD and GRD for OSGA learning, so as to experimentally study how the \u201cdictionary-selection\u201d stage influences the \u201cgreedydefinition\u201d stage. Fig.1 and Fig.2 summarize the learning rate of OSGA for the continuous regression function f1 and the discontinuous regression function f2, respectively.\nIt is seen from Fig.1(a) that when the TPD is selected, the RMSE does not increase as the step-size s increases. However, Fig.1(b) shows that when the GRD is selected, the RMSE increases obviously. A similar trend can also be observed when applying\nthe OSGA on the discontinuous regression function f2, as shown in Fig.2. From an experimental viewpoint, such differences demonstrate that a dictionary with certain good properties do help to redefine what is greedy in the \u201cgreedy-definition\u201d stage and improve the efficiency of greedy learning while a \u201cbad\u201d dictionary can not bring such benefits. All these simulations reveal that there does exist a relationship between the \u201cdictionaryselection\u201d and\u201cgreedy-definition\u201d stages.\nIt should be noted from Fig.1 (b) and Fig.2 (b) that even for the GRD, OSGA does not degrade the generalization capability of OGA very much. Indeed, taking Fig.1 for example, from s = 1 to s = 10, the RMSE only increases from 0.0073 to 0.0097. This phenomenon shows that at the cost of a small loss of generalization capability, OSGA\nprovides a possibility to reduce the computation burden of OGA learning, even for some \u201cbad\u201d dictionaries. The main reason of this phenomenon, from our point of view, can be stated as follows. As s increases, due to the high-correlation of the atoms of GRD, the selected atoms via (3.6) are high-correlated to the residual rs z,k\u22121, so they are highcorrelated with each other. Therefore, the approximation capability of the spanned space Vsk are not much better than Vs(k\u22121)+1, where Vs(k\u22121)+1 denotes the span of dictionary in which only one atom is added to Vs(k\u22121) = {g1, . . . , gs(k\u22121)} according to the classical greedy definition. However, as the atoms of Vsk is more than that of Vs(k\u22121)+1, and the high-correlation of GRD, the capacity of Vsk is only larger than that of Vs(k\u22121)+1 to a limited extent. Thus, according to the known bias and variance trade-off principle [6], the bias decreases a little while the variance increases a little, which makes the final generalization error varies only a little. As a consequence, more atoms are required to reach a good prediction as compared with OGA."}, {"heading": "4.3. The pros and cons of OSGA Learning", "text": "The main motivation to introduce OSGA to tackle supervised learning problem is that OSGA can reduce the computational burden of OGA, provided the dictionary possess some prominent property. The main purpose of this series of simulations is to verify this pros of OSGA learning. Furthermore, we also experimentally analyze the cons of OSGA learning. To this end, we aim to compare both the training time and test time of OSGA learning with different step-size, s. As the test time only depends on the sparsity of the coefficients of the deduced estimator, we record both the training time (Fig.3) and sparsity of the estimator (4) as a function of s.\nIt can be found in Fig.3 that, to deduce the OSGA estimator, the training time monotonously decreases with respect to s. This implies that as far as the training time is concerned, OSGA learning outperforms than OGA learning. The reason is that OSGA learning can skip many least square estimation for a large s, as compared with the standard OGA. This conclusion is regarded as the main pros of OSGA learning. On the other hand, as shown in Fig.4, the number of the selected atoms of the OSGA estimator may be a bit larger than OGA, which is witnessed in Fig.4 (a), (b), (d). Under this circumstance, the\ntest cost of OSGA learning is larger than that of OGA learning, which can be considered as the main cons of OSGA learning.\nThe above two simulations only take the computational burden for OSGA into account.\nWe further do the following simulations to consider both the computational burden and generalization capability. Since the sparsity of the OSGA estimator is the product of the iteration number m and step-size s, the training time also depends heavily on the sparsity. Therefore, we employ a simulation by setting RMSE as a function of the sparsity, k. Such a simulation, shown in Fig.5 presents a summary of OSGA learning. It is seen that the number of atoms required for the smallest RMSE, may grow as the step-size s increases (see (a), (b), (d) in Fig.5). Furthermore, the obtained least RMSE for different s varies very little. These show that OSGA learning can reduce the computational burden of OSGA without sacrificing the generalization capability very much, and the price (or risk) to do such a reduction is that the test time may increase. It should be highlighted in Fig.5 that, for the well developed dictionary, TPD, and a suitable regression function, all the training time, test time and RMSE can be reduced by utilizing OSGA. The reason for these phenomenons is similar as that presented in the bottom of the last subsection. For \u201cbad\u201d dictionary such as GRD, the bias decreases a little while the variance increases a little as s increases. Due to the high-correlation of GRD, the quantity of decreased bias is smaller than that of increased variance. Thus, it requires more atoms and makes the generalization error a little larger. For a \u201cgood\u201d dictionary such as TPD, and an appropriate regression function, the quantity of decreased bias can be larger than that of increased variance, which leads to both smaller sparsity of the estimator and less generalization error."}, {"heading": "4.4. The generalization ability of OSGA Learning", "text": "Finally, we assess the generalization capacity of OSGA learning as compared with some typical dictionary-based learning methods. As the purpose of this paper is not to pursue the best dictionary, we just employ two fixed dictionaries as GRD and TPD. Specifically, we run the OSGA-1 (or OGA), OSGA-2, OSGA-5, OSGA-10, Lasso, ridge regression, half regression, and greedy boosting on the same data and dictionaries. Here, OSGA-s denotes that there are s atoms selected in the \u201cgreedy-definition\u201d stage of OSGA. The results are summarized in Tables 1 and 2. It can be found in Tables 1 and 2 that for GRD, the test error of OSGA-s increases as s increases, while for TPD, the test error of\nOSGA-s monotonously decreases with respect to s, which verifies our assertion proposed in Section 4.2 further. Moreover, it is shown in Tables 1 and 2 that the performance of OSGA is similar to other competitive methods. This shows that OSGA can reduce the computational burden without sacrificing the generalization capability very much.\nIt should be noted that the results of OSGA is searched in the whole finite discrete parameter space, and therefore it is easy for OSGA to select the best parameter. However, for both greedy boosting and Lq coefficient regularization, their main parameters are distributed in continuous (or infinite) spaces, which makes the best parameters be difficult to achieve. This phenomenon can be regarded as another advantage of OSGA, and also give a reason why Lq coefficient regularization and greedy boosting\u2019s generalization capability seems worse than OSGA in the second column of Table 2. We believe that if a more elaborate parametric selection method is given, then the RMSE of these methods\ncan reduce."}, {"heading": "5. Proofs", "text": "can be found in [12] and [11], respectively.\nLemma 5.1. Assume a dictionary D has coherence M . Then we have for any distinct gj \u2208 D, j = 1, 2, . . . , s and for any aj, j = 1, 2 . . . , s, the inequalities\n(1\u2212M(s\u2212 1)) s \u2211\ni=1\na2i \u2264 \u2225 \u2225 \u2225 \u2225\n\u2225\ns \u2211\ni=1\naigi\n\u2225 \u2225 \u2225 \u2225 \u2225 2 \u2264 (1 +M(s\u2212 1)) s \u2211\ni=1\na2i .\nLemma 5.2. Assume a dictionary D has coherence M . Let {gi}si=1 \u2282 D and G(s) := span{g1, . . . , gs}. Then we have\n1\n1 +M(s\u2212 1)\ns \u2211\ni=1\n\u3008f, gi\u30092 \u2264 \u2016PG(s)(f)\u20162 \u2264 1\n1\u2212M(s\u2212 1)\ns \u2211\ni=1\n\u3008f, gi\u30092.\nProof. It follows from the definition of PG(s)(f) that\n\u2016PG(s)(f)\u2016 = max \u03c8\u2208G(s),\u2016\u03c8\u2016\u22641 |\u3008f, \u03c8\u3009|.\nLet \u03c8 = \u2211s i=1 aigi. Then it follows form Lemma 5.1 that for arbitrary \u03c8 \u2208 G(s) and \u2016\u03c8\u2016 \u2264 1, there holds |\u3008f, \u03c8\u3009|2 = \u2223 \u2223 \u2223\n\u2223 \u2223\n\u2329\nf, s \u2211\ni=1\naigi\n\u232a\u2223\n\u2223 \u2223 \u2223 \u2223\n2\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 s \u2211\ni=1\nai \u3008f, gi\u3009 \u2223 \u2223 \u2223 \u2223\n\u2223\n2\n\u2264 s \u2211\ni=1\na2i\ns \u2211\ni=1\n\u3008f, gi\u30092 \u2264 1\n1\u2212M(s\u2212 1)\ns \u2211\ni=1\n\u3008f, gi\u30092.\nTherefore, we get\n\u2016PG(s)(f)\u20162 \u2264 1\n1\u2212M(s\u2212 1)\ns \u2211\ni=1\n\u3008f, gi\u30092.\nTo bound \u2016PG(s)(f)\u20162 from below, noting \u2225\n\u2225 \u2225 \u2225 \u2225\ns \u2211\ni=1\n\u3008f, gi\u3009gi\n\u2225 \u2225 \u2225 \u2225 \u2225 2 \u2264 (1 +M(s\u2212 1)) s \u2211\ni=1\n\u3008f, gi\u30092,\nwe have \u2225\n\u2225 \u2225 \u2225 \u2225\ns \u2211\ni=1 \u3008f, gi\u3009 (1 +M(s\u2212 1))1/2 |\u2211si=1\u3008f, gi\u30092| 1/2 gi\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2264 1.\nThus, there holds\n\u2016PG(s)(f)\u20162 = max \u03c8\u2208G(s),\u2016\u03c8\u2016\u22641\n|\u3008f, \u03c8\u3009| \u2265 \u2223 \u2223 \u2223\n\u2223 \u2223\n\u2329\nf,\ns \u2211\ni=1 \u3008f, gi\u3009 (1 +M(s\u2212 1))1/2 |\u2211si=1\u3008f, gi\u30092| 1/2 gi\n\u232a\u2223\n\u2223 \u2223 \u2223 \u2223\n2\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 s \u2211\ni=1\n\u3008f, gi\u30092\n(1 +M(s\u2212 1))1/2 | \u2211s i=1\u3008f, gi\u30092| 1/2\n\u2223 \u2223 \u2223 \u2223 \u2223 2\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 (1 +M(s\u2212 1))\u22121/2 ( s \u2211\ni=1\n\u3008f, gi\u30092 )1/2\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2\n= (1 +M(s\u2212 1))\u22121 s \u2211\ni=1\n\u3008f, gi\u30092.\nThis finishes the proof of Lemma 5.2\nLemma 5.3. Let (an)n 6=0 be a set of decreasing nonnegative numbers that satisfy a0 \u2264 R and ak \u2264 ak\u22121 ( 1\u2212 ak\u22121 R ) for all k > 0. Then, for all n > 0, there holds an \u2264 Rn+1 .\nBy the help of the above lemmas, we are in a position to give the proof of Theorem\n3.1.\nProof of Theorem 3.1. Let Pf be the projection of f onto L1. Noting that every element of L1 can be approximated arbitrarily well by elements of the form\n\u03c6 = \u221e \u2211\nj=1\ncjgj, gj \u2208 D, \u221e \u2211\nj=1\n|cj | \u2264 \u2016Pf\u2016L1 + \u03b4, |c1| \u2265 |c2| \u2265 . . . ,\nwhere \u03b4 > 0 is arbitrary positive number. It will be clear from the following argument that it is sufficient to consider elements Pf of the above form. Suppose v is such that\n|cv| \u2265 2(\u2016Pf\u2016L1 + \u03b4)\ns \u2265 |cv+1|.\nThen, the aforementioned assumption on the sequence {cj} yields that v \u2264 s2 and |cs+1| < \u2016Pf\u2016L1+\u03b4\ns . We claim that elements g1, . . . , gv will be chosen among \u03d51, . . . , \u03d5s at the first\niteration. Indeed, for j \u2208 [1, v], we have |\u3008Pf , gj\u3009| = \u2223 \u2223 \u2223 \u2223\n\u2223\n\u2329\n\u221e \u2211\nk=1\nckgk, gj\n\u232a\u2223\n\u2223 \u2223 \u2223 \u2223 =\n\u2223 \u2223 \u2223 \u2223 \u2223 cj + \u2211\nk 6=j\nck\u3008gk, gj\u3009 \u2223 \u2223 \u2223 \u2223\n\u2223\n\u2265 |cj| \u2212 \u2223 \u2223 \u2223 \u2223\n\u2223\n\u2211\nk 6=j\nck\u3008gk, gj\u3009 \u2223 \u2223 \u2223 \u2223\n\u2223\n\u2265 |cj | \u2212M(\u2016Pf\u2016L1 + \u03b4 \u2212 |cj|)\n= |cj|(1 +M)\u2212M((\u2016Pf\u2016L1 + \u03b4) \u2265 2(\u2016Pf\u2016L1 + \u03b4)\ns (1 +M)\u2212M(\u2016Pf\u2016L1 + \u03b4).\nFor all g distinct from g1, . . . , gs, we have\n|\u3008Pf , g\u3009| = \u2223 \u2223 \u2223 \u2223\n\u2223\n\u221e \u2211\nk=1\nck\u3008gk, g\u3009 \u2223 \u2223 \u2223 \u2223\n\u2223\n< (\u2016Pf\u2016L1 + \u03b4)\ns +M(\u2016Pf\u2016L1 + \u03b4) = (\u2016Pf\u2016L1 + \u03b4)(M +\n1 s ).\nSince s \u2264 1 2M + 1, we obtain\n|\u3008Pf , gi\u3009| \u2265 |\u3008Pf , g\u3009|\nfor all g distinct from g1, . . . , gs. This implies that\n|\u3008f, gi\u3009| \u2265 |\u3008f, g\u3009|\nfor all g distinct from g1, . . . , gs. Thus, we do not pick any g \u2208 D distinct from g1, . . . , gs until we have chosen all g1, . . . , gv.\nNow we proceed the proof of Theorem 3.1. Denote Fm = span(\u03d5i, i \u2208 Im). Then, Hm\u22121, Fm \u2208 Hm. Therefore,\nrsm = f \u2212 PHm(f) = rsm\u22121 + f sm\u22121 \u2212 PHm(rsm\u22121 + f sm\u22121) = rsm\u22121 \u2212 PHm(rsm\u22121).\nIt is clear that the inclusion Fm \u2282 Hm implies\n\u2016rsm\u2016 \u2264 \u2016rsm\u22121 \u2212 PFm(rsm\u22121)\u2016.\nUsing the notation pm = PFm(r s m\u22121). We continue\n\u2016rsm\u22121\u20162 = \u2016rsm\u22121 \u2212 pm\u20162 + \u2016pm\u20162\nand\n\u2016rsm\u20162 \u2264 \u2016rsm\u22121\u20162 \u2212 \u2016pm\u20162.\nIt is obvious that for arbitrary h \u2208 L1,\n\u2016rsm\u22121\u20162 = \u3008rsm\u22121, f\u3009 = \u3008rsm\u22121, h+ f \u2212 h\u3009 = \u3008rsm\u22121, h\u3009+ \u3008rsm\u22121, f \u2212 h\u3009.\nThe known Cauchy-Schwarz inequality implies that\n\u3008rsm\u22121, f \u2212 h\u3009 \u2264 \u2016rsm\u22121\u2016 \u00b7 \u2016f \u2212 h\u2016. (5.1)\nNow we turn to bound \u3008rsm\u22121, h\u3009. Denote Jl = [(l \u2212 1)s + v + 1, ls + v], G(Jl) = span{gi}i\u2208Jl, and\nqs = qs(r s m\u22121) = sup\ngi\u2208D,i\u2208[1,s]\n\u2016PG(s)(rsm\u22121)\u2016,\nwe then write for m \u2265 2,\n\u3008rsm\u22121, h\u3009 = \u2329 rsm\u22121, h\u2212 v \u2211\nj=1\ncjgj\n\u232a\n=\n\u2329\nrsm\u22121, \u221e \u2211\nj=v+1\ncjgj\n\u232a\n=\n\u221e \u2211\nl=1\n\u3008rsm\u22121, \u2211\nj\u2208Jl\ncjgj\u3009 = \u221e \u2211\nl=1\n\u2211\nj\u2208Jl\ncj\u3008rsm\u22121, gj\u3009\n\u2264 \u221e \u2211\nl=1\n(\n\u2211\nj\u2208Jl\nc2j\n)1/2( \u2211\nj\u2208Jl\n\u3008rsm\u22121, gj\u30092 )1/2 .\nHence, Lemma 5.2 implies that\n\u3008rsm\u22121, h\u3009 \u2264 \u221e \u2211\nl=1\n(\n\u2211\nj\u2208Jl\nc2j\n)1/2\n(1 +M(s\u2212 1))1/2PG(Jl)(rm\u22121)\n\u2264 \u221e \u2211\nl=1\n(\n\u2211\nj\u2208Jl\nc2j\n)1/2\n(1 +M(s\u2212 1))1/2qs.\nAs the sequence {cj} has the property\n|cv+1| \u2265 |cv+2| \u2265 \u00b7 \u00b7 \u00b7 , \u221e \u2211\nj=v+1\n|cj| \u2264 \u2016h\u2016L1 + \u03b4, |cv+1| \u2264 2(\u2016h\u2016L1 + \u03b4)\ns ,\nwe may apply the simple inequality\n(\n\u2211\nj\u2208Jl\nc2j\n)1/2\n\u2264 |c(l\u22121)s+v+1|s1/2\nso that we have\n\u221e \u2211\nl=1\n(\n\u2211\nj\u2208Jl\nc2j\n)1/2\n\u2264 s1/2 \u221e \u2211\nl=1\n|c(l\u22121)s+v+1| \u2264 s1/2  \n2(\u2016h\u2016L1 + \u03b4) s +\n\u221e \u2211\nl=2\ns\u22121 \u2211\nj\u2208Jl\u22121\n|cj|\n\n\n\u2264 3(\u2016h\u2016L1 + \u03b4)s\u22121/2.\nTherefore, we obtain\n\u3008rsm\u22121, h\u3009 \u2264 qs(1 +M(s\u2212 1))1/23(\u2016h\u2016L1 + \u03b4)s\u22121/2. (5.2)\nIt follows from (5.1) and (5.2) that\n\u2016rsm\u22121\u20162 \u2264 \u2016rsm\u22121\u2016\u2016f \u2212 h\u2016+ qs(1 +M(s\u2212 1))1/23(\u2016h\u2016L1 + \u03b4)s\u22121/2\n\u2264 1 2 (\u2016rsm\u22121\u20162 + \u2016f \u2212 h\u20162) + qs(1 +M(s\u2212 1))1/23(\u2016h\u2016L1 + \u03b4)s\u22121/2.\nDenote am = \u2016rsm\u20162 \u2212 \u2016f \u2212 h\u20162, then (5.2) implies that\nqs \u2265 am\u22121s\n1/2\n3(\u2016h\u2016L1 + \u03b4)(1 +M(s\u2212 1))1/2 .\nNote that if for some k0, we have \u2016rsk0\u22121\u2016 \u2264 \u2016f \u2212 h\u2016, then the theorem holds trivially for all N \u2265 k0 \u2212 1. We therefore assume that ak\u22121 is positive, so that we can write\nq2s \u2265 a2m\u22121s\n9(\u2016h\u2016L1 + \u03b4)2(1 +M(s\u2212 1)) .\nThus, we obtain\n\u2016rsm\u20162 \u2264 \u2016rsm\u22121\u20162 \u2212 \u2016pm\u20162 \u2264 \u2016rsm\u22121\u20162 \u2212 q2s \u2264 \u2016rsm\u22121\u20162 \u2212 a2m\u22121s\n9(\u2016h\u2016L1 + \u03b4)2(1 +M(s\u2212 1)) ,\nwhich, by subtracting \u2016f \u2212 h\u20162 in the both sides, gives\nam \u2264 am\u22121 ( 1\u2212 am\u22121s 9(\u2016h\u2016L1 + \u03b4)2(1 +M(s\u2212 1)) ) .\nThe above inequality together with Lemma 5.3 yields that\nam \u2264 9(\u2016h\u2016L1 + \u03b4)2(1 +M(s\u2212 1))\ns m\u22121, (5.3)\nprovided that\na1 \u2264 9(\u2016h\u2016L1 + \u03b4)2(1 +M(s\u2212 1))\ns . (5.4)\nTo prove (5.4), we remark that either a0 \u2264 9(\u2016h\u2016L1+\u03b4) 2(1+M(s\u22121))\ns , so that the same holds\nfor a1, or a0 \u2265 9(\u2016h\u2016L1+\u03b4) 2(1+M(s\u22121))\ns , in which case a1 < 0 by (5.3), which means that we\nare already in the trivial case \u2016rs1\u2016 \u2264 \u2016f \u2212 h\u2016 for which there is noting to prove. As \u03b4 is arbitrary positive number and s \u2264 1\n2M + 1, we have\n\u2016rsm\u20162 \u2264 \u2016f \u2212 h\u20162 + 9(\u2016h\u2016L1 + \u03b4)2(1 +M(s\u2212 1))(sm)\u22121 \u2264 \u2016f \u2212 h\u20162 + 27\n2 \u2016h\u20162L1(sm) \u22121.\nThis finishes the proof of Theorem 3.1.\nTo prove Theorem 3.2, we need the following lemma, which can be found in [15,\nTheorem 11.3]. Lemma 5.4. Suppose that Fn is a linear vector space of functions f : X \u2192 Y which may depend on x1, . . . , xn. Let \u00b5 = \u00b5(x1, . . . , xn) be the vector space dimension of Fn. If we define f\nz,Fn as\nf z,Fn := arg min\nf\u2208Fn\n1 n\nn \u2211\ni=1\n|f(xi)\u2212 yi|2,\nthen we have\nE(\u2016\u03a0f z,Fn \u2212 f\u03c1\u20162\u03c1) \u2264 CL2\n\u00b5 logn\nn + 8 min f\u2208Fn \u2016f\u03c1 \u2212 f\u20162\u03c1,\nwhere C is a universal constant.\nBased on Lemma 5.4, we give the proof of Theorem 3.2. Proof of Theorem 3.2. We divide the OSGA(s) into two parts. The one is to choose sm atoms from the dictionary using m steps and the other is to implement a least\nsquare algorithm on an sm-dimensional linear space Vsm. Once the sm-dimensional linear space is fixed, it follows from Lemma 5.4 that\nE(\u2016\u03a0f s z,m \u2212 f\u03c1\u20162\u03c1) \u2264 L2\nsm logn\nn + min f\u2208Vsm \u2016f\u03c1 \u2212 f\u20162\u03c1. (5.5)\nThus, we only need to give an upper bound of minf\u2208Vsm \u2016f\u03c1 \u2212 f\u20162\u03c1. By Theorem 3.1, we obtain for arbitrary h \u2208 spanDN ,\n1 n\nn \u2211\ni=1\n(yi \u2212 f sz,m(xi))2 \u2212 1\nn\nn \u2211\ni=1\n(yi \u2212 f\u03c1(xi))2\n\u2264 1 n\nn \u2211\ni=1\n(yi \u2212 h(xi))2 \u2212 1\nn\nn \u2211\ni=1\n(yi \u2212 f\u03c1(xi))2 + 27\n2 \u2016h\u20162L1,N (ms) \u22121.\nIt follows from (3.1) that\n\u2016f s z,m \u2212 f\u03c1\u20162\u03c1 = E\n(\n1 n\nn \u2211\ni=1\n(yi \u2212 f sz,m(xi))2 \u2212 1\nn\nn \u2211\ni=1\n(yi \u2212 f\u03c1(xi))2 )\n= E(|y \u2212 f s z,m(x)|2)\u2212E(|y \u2212 f\u03c1(x)|2)\nand\n\u2016h\u2212 f\u03c1\u20162\u03c1 = E ( 1\nn\nn \u2211\ni=1\n(yi \u2212 h(xi))2 \u2212 1\nn\nn \u2211\ni=1\n(yi \u2212 f\u03c1(xi))2 )\n= E(|y \u2212 h(x)|2)\u2212E(|y \u2212 f\u03c1(x)|2).\nThe above two equalities yield that\nmin f\u2208Vsm\n\u2016f\u03c1 \u2212 f\u20162\u03c1 \u2264 \u2016f\u03c1 \u2212 f sz,m\u20162\u03c1 \u2264 \u2016h\u2212 f\u03c1\u20162\u03c1 + 27\n2sm \u2016h\u20162L1,N ,\nwhich together with (5.5) completes the proof of Theorem 3.2."}, {"heading": "6. Concluding Remarks", "text": "The main contributions of the present paper can be summarized as follows. Firstly, we have proposed that studying the relationship between the \u201cdictionary-selection\u201d and \u201cgreedy-definition\u201d stages can improve the learning performance of greedy learning. In fact, we borrowed the idea of orthogonal super greedy algorithm (OSGA) for incoherent dictionaries from nonlinear approximation and compressive sensing [18] to the supervised\nlearning problem and analyze the pros and cons of OSGA learning. Secondly, we have established an approximation theorem of OSGA approximation and show that OSGA is also available to a variety of target function classes which is not simply related to the convex hull of the dictionary. Such an approximation theorem is the main tool to generalize the application of OSGA from approximation to learning. Thirdly, we have theoretically proved that, for incoherent dictionaries, OSGA learning can reduce the computational burden of OGA learning without sacrificing its generalization capability. Precisely, our error estimate for OSGA learning yields a learning rate as (n/ logn)\u22121/2, which is the same as that of OGA [1]. Finally, we have studied the numeral performance of OSGA. Our results show that when applied in supervised learning problem, OSGA yields a similar prediction accuracy as both OGA and other dictionary-based learning schemes, but has the potentials to reduce the price in both training and test time.\nTo make sense of the OSGA learning presented in this paper, we conclude this paper\nby the following remarks concerning some crucial issues of OSGA learning.\nRemark 6.1. In Theorem 3.2, we study the learning capability of OSGA under the assumption that the dictionary is incoherent. However, in Section 4, we employ the simulations by utilizing two fixed dictionaries: TPD and GRD. It is easy to see that neither TPD nor GRD satisfies the conditions of Theorem 3.2. However, the numerical results show that implementing OSGA in TPD can improve the learning performance compared with the classical OGA. This fact shows that the theoretical results about OSGA are a bit pessimistic and the incoherence constraint to the dictionary can be relaxed further. Thus, we are usually asked for an essentially constraint to the dictionary instead of the incoherence. Under this constraint, OSGA can essentially improve the learning performance in the sense that OSGA reduces the computational burden of OGA without sacrificing its generalization capability. Admittedly, this is a very difficult but important issue about OSGA learning. We will keep working on this interesting project, and report our progress in a future publication.\nRemark 6.2. Practitioners have asked us frequently the following question: How to choose the step-size parameter s for OSGA learning? This is a very good question. Admittedly, it is often unlikely to tackle only the dictionary with very small coherence. Thus, judiciously choosing a value for s is crucial. If s is chosen to be too large, although the training time is reduced, the generalization capability may be weakened. If s is selected to bee too small, then OSGA cannot essentially outperform the classical OGA. We think the best choice of s depends heavily on the essential condition developed in Remark 6.1. Once the essential condition is found, the best choice of s can be consequently determined. We will also keep working on this practical issue and report our progress in a future publication.\nRemark 6.3. In the simulations, we present an example that in the one-dimensional case, the TPD dictionary perfectly guarantees the effectiveness of OSGA learning. However, in high-dimensionally cases, there lack of such \u201cgood\u201d and easy-implemented dictionaries, which more or less influences the application of OSGA learning. Therefore, how to develop generally \u201cgood\u201d and easy-implemented dictionaries for OSGA learning deserves further studies."}], "references": [{"title": "Approximation and learning by greedy algorithms", "author": ["A.R. Barron", "A. Cohen", "W. Dahmen", "R. DeVore"], "venue": "Ann. Statist., 36 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Random sampling of multivariate trigonometric polynomials", "author": ["R.F. Bass", "K. Gr\u00f6chenig"], "venue": "SIAM J. Math. Anal., 36 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Interpolation of Operators", "author": ["C. Bennett", "R. Sharpley"], "venue": "Academic Press, Boston", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1988}, {"title": "Learning rates of multi-kernel regression by orthogonal greedy algorithm", "author": ["H. Chen", "L. Li", "Z. Pan"], "venue": "J. Statist. Plan. & Infer., 143 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "On the mathematical foundations of learning", "author": ["F. Cucker", "S. Smale"], "venue": "Bull. Amer. Math. Soc., 39 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning Theory: An Approximation Theory Viewpoint", "author": ["F. Cucker", "D.X. Zhou"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Subspace pursuit for compressive sensing signal recontruction", "author": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Trans. Inf. Theory, 55 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Commun. Pure Appl. Math., 57 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Iteratively re-weighted least squares minimization for sparse recovery", "author": ["I. Daubechies", "R. DeVore", "M. Fornasier", "C. G\u00fcnt\u00fcrk"], "venue": "Commun. Pure Appl. Math., 63 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Constructive Approximation", "author": ["R. DeVore", "G. Lorentz"], "venue": "Springer-Verlag, Berlin", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Some remarks on greedy algorithms", "author": ["R. DeVore", "V. Temlyakov"], "venue": "Adv. Comput. Math., 5 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "On Lebesgue-type inequalities for greedy approximation", "author": ["D. Donoho", "M. Elad", "V. Temlyakov"], "venue": "J. Approx. Theory, 147 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse solution of underdetermined systems of linear equations by stagewise orthogonal matching pursuit", "author": ["D.L. Donoho", "Y. Tsaig", "O. Drori", "J.L. Starck"], "venue": "IEEE Trans. Inf. Theory, 58 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J. Friedman"], "venue": "Ann. Statis., 29 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "A Distribution-Free Theory of Nonparametric Regression", "author": ["L. Gy\u00f6rfy", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": "Springer, Berlin", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Random sampling of sparse trigonometric polynomials II- Orthogonal matching pursuit versus basis pursit", "author": ["S. Kunis", "H. Rauhut"], "venue": "Found. Comput. Math., 8 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning capability of relaxed greedy algorithms", "author": ["S.B. Lin", "Y.H. Rong", "X.P. Sun", "Z.B. Xu"], "venue": "IEEE Trans. Neural Netw. & Learn. Syst., 24 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "The orthogonal super greedy algorithm and applications in compressed sensing", "author": ["E. Liu", "V. Temlyakov"], "venue": "IEEE. Trans. Inf. Theory, 58 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Super greedy type algorithms", "author": ["E. Liu", "V. Temlyakov"], "venue": "Adv. Comput. Math., 37 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Some properties of Gaussian reproducing kernel Hilbert spaces and their implications for function approximation and learning theory", "author": ["H. Minh"], "venue": "Constr. Approx., 32 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning out of leaders", "author": ["M. Mougeot", "D. Picard", "K. Tribouley"], "venue": "J. Royal Statis. Soc. Series B, 74 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Numerical Analysis", "author": ["T. Sauer"], "venue": "Addison-Wesley Longman, London", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "A generalized representer theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "D. Helmbold and B.Williamson, edited, Proceedings of the 14th Annual Conference on Computational Learning Theory, pp 416-426. Springer, New York", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Nonlinear methods of approximation", "author": ["V. Temlyakov"], "venue": "Found. Comput. Math., 3, ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Greedy approximation", "author": ["V. Temlakov"], "venue": "Acta Numer., 17 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "On performance of greedy algorithms", "author": ["V. Temlyakov", "P. Zheltov"], "venue": "J. Approx. Theory, 163 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "J. ROY. Statist. Soc. Ser. B, 58 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1995}, {"title": "Greed is good: algorithmic results for sparse approximation", "author": ["J.A. Tropp"], "venue": "IEEE Trans. Inf. Theory, 50 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Computational methods for sparse solution of linear inverse problems", "author": ["J.A. Tropp", "S. Wright"], "venue": "in: Proceedings of the IEEE, 98: 948-958", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "L1/2 regularization: a thresholding representation theory and a fast solver", "author": ["Z.B. Xu", "X.Y. Chang", "F.M. Xu", "H. Zhang"], "venue": "IEEE. Trans. Neural netw & Learn. system., 23 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient greedy learning for massive data", "author": ["C. Xu", "S.B. Lin", "J. Fan"], "venue": "Manuscript", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Greedy metrics in orthogonal greedy learning", "author": ["L. Xu", "S.B. Lin", "J.S. Zeng", "Z.B. Xu"], "venue": "Manuscript", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Approximation with polynomial kernels and SVM classifiers", "author": ["D.X. Zhou", "K. Jetter"], "venue": "Adv. Comput. Math., 25 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "The use of greedy algorithms in the context of nonlinear approximation [1] is very appealing since it greatly reduces the computational burden when compared The research was supported by the National 973 Programming (2013CB329404), the Key Program of National Natural Science Foundation of China (Grant No.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 90, "endOffset": 101}, {"referenceID": 15, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 90, "endOffset": 101}, {"referenceID": 27, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 90, "endOffset": 101}, {"referenceID": 12, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 119, "endOffset": 127}, {"referenceID": 28, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 119, "endOffset": 127}, {"referenceID": 11, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 153, "endOffset": 161}, {"referenceID": 25, "context": "This property triggers avid research activities of greedy algorithms in signal processing [7, 16, 28], inverse problem [13, 29] and sparse approximation [12, 26].", "startOffset": 153, "endOffset": 161}, {"referenceID": 0, "context": "Greedy learning, or more specifically, applying greedy algorithms to tackle supervised learning problems, has been proved to possess charming generalization capability with lower computational burden than the widely used coefficient-based regularization methods [1].", "startOffset": 262, "endOffset": 265}, {"referenceID": 31, "context": "From approximation to learning, greedy learning can be usually formulated as a four-stage stepwise learning strategy [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "Since greedy learning\u2019s inception in supervised learning [14], the aforementioned four stages were comprehensively studied for various purposes.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "[4] and Lin et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] proposed that the kernel based dictionary is a good choice for greedy learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] pointed out that the metric of greedy-definition is not uniquely the greediest one.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] declared that both relaxed greedy iteration and orthogonal greedy iteration can achieve a fast learning rate for greedy learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] provided an l complexity regularization strategy and Chen et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] proposed an l complexity constraint strategy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "All these results showed that as a new learning scheme, greedy learning deserves avid studying due to its stepwise learning character [14].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "Although the importance of a single stage of greedy learning was widely studied [1, 4, 17, 31], the relationship between these stages and their composite effects for learning also need classifying.", "startOffset": 80, "endOffset": 94}, {"referenceID": 3, "context": "Although the importance of a single stage of greedy learning was widely studied [1, 4, 17, 31], the relationship between these stages and their composite effects for learning also need classifying.", "startOffset": 80, "endOffset": 94}, {"referenceID": 16, "context": "Although the importance of a single stage of greedy learning was widely studied [1, 4, 17, 31], the relationship between these stages and their composite effects for learning also need classifying.", "startOffset": 80, "endOffset": 94}, {"referenceID": 30, "context": "Although the importance of a single stage of greedy learning was widely studied [1, 4, 17, 31], the relationship between these stages and their composite effects for learning also need classifying.", "startOffset": 80, "endOffset": 94}, {"referenceID": 31, "context": "In the recent work [32], Xu et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "In this paper, we study the learning capability of orthogonal super greedy algorithm (OSGA) which was proposed by Liu and Temlyakov [18].", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "For OSGA approximation, it was shown in [18] (see also [19]) that for incoherent dictionaries, OSGA reduces the computational burden when compared with OGA.", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "For OSGA approximation, it was shown in [18] (see also [19]) that for incoherent dictionaries, OSGA reduces the computational burden when compared with OGA.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "However, such an assumption to the target functions is very stringent if the dimension of variable is large [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "Interestingly, we find that, even for functions out of the convex hull of the dictionary, the approximation capability of OSGA is similar as that of OGA [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 24, "context": "There exist several types of greedy algorithms [25].", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "Initially, set f s 0 = 0 and r 0 = f , then the OSGA proposed in [18] for each k \u2265 1 can be inductively define as the following.", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "1 proved in [18] shows that OSGA can achieve the optimal approximation rate of ks term nonlinear approximation [24].", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "1 proved in [18] shows that OSGA can achieve the optimal approximation rate of ks term nonlinear approximation [24].", "startOffset": 111, "endOffset": 115}, {"referenceID": 5, "context": "Z (f(x)\u2212 y)d\u03c1, which is minimized by the regression function [6], defined by f\u03c1(x) := \u222b", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "With the assumption that f\u03c1 \u2208 L\u03c1 X , it is well known [5] that, for every f \u2208 L2\u03c1X , there holds E(f)\u2212 E(f\u03c1) = \u2016f \u2212 f\u03c1\u2016\u03c1.", "startOffset": 54, "endOffset": 57}, {"referenceID": 23, "context": "Approximation capability of OSGA A consensus in the nonlinear approximation community is that greedy-type algorithms can break the \u201ccurse of dimensionality\u201d [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "We find, however, this is not exactly true since, in practice, the condition that the target functions belong to the convex hull of the dictionary becomes more and more stringent as the dimension of variable grows [1].", "startOffset": 214, "endOffset": 217}, {"referenceID": 0, "context": "tackled in [1] by proving convergence results for a variety of function classes and not simply those are related to the convex hull of the dictionary.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "Along the flavor of [1], we study the behavior of OSGA approximation when the target functions f \u2208 H are more general.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "We consider the real interpolation spaces [3] Rp = [H,L1]\u03b8,\u221e, 0 < \u03b8 < 1, with p defined by 1 p = \u03b8 + 1\u2212 \u03b8 2 = 1 + \u03b8 2 .", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "Recall that f \u2208 [X, Y ]\u03b8,\u221e if and only if for all t > 0, there holds [10] K(f, t) \u2264 Ct, (3.", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "OSGA learning It was pointed out in [18] that OSGA can be applied in compressed sensing very well.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "It is shown in Algorithm 1 that the only difference between OSGA and OGA learning [1] is that in OSGA there are s atoms selected in the \u201cgreedy-definition\u201d stage.", "startOffset": 82, "endOffset": 85}, {"referenceID": 32, "context": "Furthermore, as y \u2208 [\u2212L, L], it is easy to deduce [33] that \u2016\u03a0Lf s z,m \u2212 f\u03c1\u2016\u03c1 \u2264 \u2016f s z,m \u2212 f\u03c1\u2016\u03c1.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "We further notice that up to the constant, the deduced oracle inequality is the same as that deduced in [1] with k in [1, Theorem 3.", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "Along [1]\u2019s flavor, for r > 0, we define the space L1 as the set of all functions f such that, for all N , there exists h \u2208 span{DN} satisfying \u2016h\u2016L1 \u2264 B, and \u2016f \u2212 h\u2016\u03c1 \u2264 BN.", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "Hence, we use the same l complexity regularization strategy as that in [1] to choose m.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "1 in [1] for the details.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "in [1].", "startOffset": 3, "endOffset": 6}, {"referenceID": 20, "context": "Since the incoherence assumption is too strict to describe the property of the dictionary and difficult to verify, especially for supervised learning [21], we do not implement the simulation for dictionaries with such a pessimistic assumption.", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 124, "endOffset": 127}, {"referenceID": 26, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 135, "endOffset": 139}, {"referenceID": 22, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 158, "endOffset": 162}, {"referenceID": 8, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 182, "endOffset": 185}, {"referenceID": 29, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 236, "endOffset": 240}, {"referenceID": 13, "context": "The last one is to compare the performance of OSGA with other typical dictionary learning strategy such as the OGA learning [1], Lasso [27], ridge regression [23], bridge regression [9] (for example, the half coefficient regularization [30]) and greedy boosting [14].", "startOffset": 262, "endOffset": 266}, {"referenceID": 21, "context": "Methods: For OSGA and OGA learning, we applied QR decomposition to acquired the least squared estimates [22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "For greedy boosting, we used the L2boost algorithm [14] with the step size 0.", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "For L2 coefficient regularization (or ridge regression), we use its analytic regularized least square solution [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "For L1 (or lasso) and L1/2 coefficient regularization schemes, we utilize the iterative soft [8] and half [30] thresholding algorithms to obtain the corresponding estimators, respectively.", "startOffset": 93, "endOffset": 96}, {"referenceID": 29, "context": "For L1 (or lasso) and L1/2 coefficient regularization schemes, we utilize the iterative soft [8] and half [30] thresholding algorithms to obtain the corresponding estimators, respectively.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "we do not divide the training set into training data and validation data and use validation data to choose parameters as in [15], instead, we use the test set to fix parameters directly.", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "As is shown in [21], the incoherence assumption in the background of supervised learning is too strict to describe the property of dictionaries.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "It can be found in [2] that the TPD dictionary together with the random samples can develop a wellconditioned sampling matrix [2, Theorem 5.", "startOffset": 19, "endOffset": 22}, {"referenceID": 19, "context": "1], while the sampling matrix constructed by GRD is usually ill-conditioned [20].", "startOffset": 76, "endOffset": 80}, {"referenceID": 5, "context": "Thus, according to the known bias and variance trade-off principle [6], the bias decreases a little while the variance increases a little, which makes the final generalization error varies only a little.", "startOffset": 67, "endOffset": 70}, {"referenceID": 11, "context": "The first and third lemmas can be found in [12] and [11], respectively.", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "The first and third lemmas can be found in [12] and [11], respectively.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "In fact, we borrowed the idea of orthogonal super greedy algorithm (OSGA) for incoherent dictionaries from nonlinear approximation and compressive sensing [18] to the supervised 26", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "Precisely, our error estimate for OSGA learning yields a learning rate as (n/ logn), which is the same as that of OGA [1].", "startOffset": 118, "endOffset": 121}], "year": 2014, "abstractText": "We consider the approximation capability of orthogonal super greedy algorithms (OSGA) and its applications in supervised learning. OSGA is concerned with selecting more than one atoms in each iteration step, which, of course, greatly reduces the computational burden when compared with the conventional orthogonal greedy algorithm (OGA). We prove that even for function classes that are not the convex hull of the dictionary, OSGA does not degrade the approximation capability of OGA provided the dictionary is incoherent. Based on this, we deduce a tight generalization error bound for OSGA learning. Our results show that in the realm of supervised learning, OSGA provides a possibility to further reduce the computational burden of OGA in the premise of maintaining its prominent generalization capability.", "creator": "LaTeX with hyperref package"}}}