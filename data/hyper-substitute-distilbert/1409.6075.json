{"id": "1409.6075", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2014", "title": "The Information Theoretically Efficient Model (ITEM): A model for computerized analysis of large datasets", "abstract": "this treatise discusses the information theoretically efficient database ( item ), typically proposed system to detect an information theoretically primitive or logistic regression from a empirical view. more economically, this model emerges synthesized from compromise ( every linear correlation transform of the standard variable makes not necessarily linear in the independent variables. this research shows that with model animals, many resulting models can fit produced on modern computers exhibiting a reduced amount of configuration. these constraints require also manufactured from fluctuations, otherwise as such they tend to produce interpretable models with only a minor definition of features, all of which will advised to be well behaved.", "histories": [["v1", "Mon, 22 Sep 2014 03:39:23 GMT  (492kb,D)", "https://arxiv.org/abs/1409.6075v1", null], ["v2", "Mon, 6 Oct 2014 11:12:07 GMT  (996kb,D)", "http://arxiv.org/abs/1409.6075v2", null], ["v3", "Tue, 4 Nov 2014 05:41:04 GMT  (1622kb,D)", "http://arxiv.org/abs/1409.6075v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tyler ward"], "accepted": false, "id": "1409.6075"}, "pdf": {"name": "1409.6075.pdf", "metadata": {"source": "CRF", "title": "The Information Theoretically Efficient Model (ITEM): A model for computerized analysis of large datasets", "authors": ["Tyler Ward"], "emails": ["ward.tyler@gmail.com"], "sections": [{"heading": null, "text": "The Information Theoretically Efficient Model (ITEM): A model\nfor computerized analysis of large datasets\nTyler Ward ward.tyler@gmail.com\nNovember 5, 2014\nar X\niv :1\n40 9.\n60 75\nv3 [\ncs .L\nG ]\n4 N\nov 2\n01 4\nContents"}, {"heading": "1 Introduction 4", "text": "1.1 Statement of the Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.1.1 The Analogy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.1.2 The Example Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.1.3 The Example Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9"}, {"heading": "2 Efficiency Considerations 10", "text": "2.1 Computational Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.2 Information Theoretic Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"}, {"heading": "3 Model Choice 12", "text": "3.1 Parametric Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.1.1 Structural Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.1.2 Multinomial Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.2 Multinomial Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.3 Optimality of Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.4 Splined Multinomial Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . 17"}, {"heading": "4 An Introduction to ITEM 19", "text": "4.1 ITEM Curve Families . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n4.2 Comparison to Generalized Additive Models . . . . . . . . . . . . . . . . . . . . . . . 22\n4.3 Information Theoretical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.3.1 Efficient Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n4.3.2 Resistance to Overfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23"}, {"heading": "5 Implementation of ITEM 24", "text": "5.1 Fitting ITEM curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n5.2 Annealing ITEM curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n5.3 Adaptive Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n5.4 Hybrid Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n5.4.1 Markov Matrix Multplication . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n5.4.2 Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n5.4.3 Hybrid Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30"}, {"heading": "6 Results 32", "text": "6.1 Initial Curve Fitting Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n6.2 Fitting Computational Cost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n6.3 Fitting Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n6.4 Model Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41"}, {"heading": "7 Advanced Fitting Topics 43", "text": ""}, {"heading": "8 Conclusion 49", "text": ""}, {"heading": "9 Reference Implementation 49", "text": ""}, {"heading": "10 Future Work 50", "text": "References 51\nI Appendix: Information-Theoretic Efficiency 52\nAbstract\nThis document discusses the Information Theoretically Efficient Model (ITEM), a computerized system to generate an information theoretically efficient multinomial logistic regression from a general dataset. More specifically, this model is designed to succeed even where the logit transform of the dependent variable is not necessarily linear in the independent variables. This research shows that for large datasets, the resulting models can be produced on modern computers in a tractable amount of time. These models are also resistant to overfitting, and as such they tend to produce interpretable models with only a limited number of features, all of which are designed to be well behaved."}, {"heading": "1 Introduction", "text": "A good overview of the state of the art can be found in [6]. That paper introduces the general problem ITEM was designed to solve, and very briefly surveys the various methods available in the literature that can solve this problem."}, {"heading": "1.1 Statement of the Problem", "text": "Consider the usual regression situation: we have data (~xi, ~yi) for i = 1, 2, ..., N . Here ~xi = (xi,1, xi,2, ...., xi,K) T and ~yi = (yi,1, yi,2, ...yi,W ) T are the regressor and response variables. Suppose also that xi,k \u2208 R and ~yi is a probability vector. In shorthand, X = {~xi} and Y = {~yi}.\nA model is some function AXY (xw) whose output is a prediction for yw. In most common situations, the modelAXY must be deduced from the dataX = {~x1, ~x2, ....., ~xN} and Y = {~y1, ~y2, ..., ~yN}, this process is referred to as fitting. Then the model would be used to make predictions for ~xw with w > N , i.e. predict ~y for the data points where we only know ~x, this is referred to as projection or running. ITEM is one such model, but several others will also be discussed.\nIn addition to the requirements above, a two more factors are added from an engineering standpoint:\n\u2022 A large amount of data is available (at least 100,000 observations)\n\u2022 The data has more than a few dimensions (~xi has at least 5 dimensions)\nFor one dimensional data, various solutions are available, but generally, people will simply look at the results manually and make any adjustments that are needed. Beyond a few dimensions, this no longer works efficiently. It uses too much labor, and the projections on to the individual dimensions become less and less tractable. Building up a model from a set of N regressors requires roughly N2 operations, as for each regressor added, all other regressors need to be checked. With a moderate\nnumber of regressors (dozens to hundreds) this is too time consuming for people, but could be done by a computer. The primary goal of ITEM is to perform this multidimensional analysis and fitting automatically. Since human guidance is limited, it is important for the model to be able to resist overfitting while simultaneously using the data efficiently to produce a model as accurate as possible. The dataset must be large, because it will generally require a lot of data to support good predictions across many dimensions."}, {"heading": "1.1.1 The Analogy", "text": "Before proceeding further, this entire paper is devoted to producing good models. It is worth taking a brief detour to discuss what is meant by a good model. For this purpose, it is helpful to use a car analogy. For a person buying a car, there are several statistics that might be important.\n1. acceleration\n2. top speed\n3. fuel efficiency\n4. material efficiency\n5. automated construction\nEach of these factors helps drive the appeal (and cost) of a given car. For instance, a car could be made out of carbon fiber and titanium, thus getting better speed and efficiency, but by using these extremely expensive materials, the cost may be prohibitive. Similarly, a car could be constructed with extremely advanced manufacturing techniques, but if it cannot be mass produced, then again the car would be too expensive for typical use. Among these five characteristics, often improving one will make others worse.\nThis tradeoff will define a sort of efficient frontier, a 5 dimensional surface enclosing some volume. Each point on this frontier represents some tradeoff among the 5 qualities, but there is no point reachable with this technology that is better in all 5 categories. One definition of the quality of automotive technology is to simply say that better technology allows for a larger frontier, one that completely encloses the frontier for an inferior set of technology. A model has a similar set of factors that together help to determine its quality.\n1. fitting computational efficiency\n2. running computational efficiency\n3. data efficiency\n4. parameter efficiency\n5. automated construction\nA good model would be computationally inexpensive to fit and to run. Just like in the case of a car, these are nearly the same requirement. A car with good acceleration will typically have a high top speed, similarly a model that is efficient to fit will typically (though not always) also be efficient to run. These two factors will be grouped together as computational efficiency.\nData efficiency and parameter efficiency are similar concepts as well, so they will be grouped together as information theoretic efficiency. Data efficiency means essentially that statistically significant features of the data will be included in the model. We require that a feature will be picked up by the model without requiring excessive amounts of data, it will be added as soon as enough data is present for it to be significant. The concept of parameter efficiency means that the model will not pick up extra features beyond those indicated by the data. Essentially, this is a requirement that the model use as few parameters as possible, while simultaneously fitting the data as well as possible. Obviously there is some trade off between these two features, but together they define a part of the efficient frontier for models. We would like the model to be on or near this frontier, and for this frontier to itself pass close to the true distribution of the data.\nThe last requirement is simply that the model be constructed automatically, with limited human involvement. The whole purpose of this exercise is to produce a good model without expending a large amount of human effort to do so.\nIt is not possible for a model to excel in all five of these areas. Some models are better in one, or another, and for most distributions of data there is a definite limit to how high the information theoretic efficiency of a model can be, just as a car\u2019s fuel efficiency runs into thermodynamic limits. The goal of the ITEM system is to construct models that are very good (though by no means ideal) in all 5 of these areas."}, {"heading": "1.1.2 The Example Problem", "text": "For the purposes of this paper, a residential mortgage model will be considered. In the United States, a residential mortgage is a loan made by a lender (typically a bank) to a borrower (typically an individual) for the purpose of buying residential real estate, typically a house, but sometimes a condo or other living space. When making such a loan, it is important for the lender to be able to project the probable future behavior of the borrower. In order to do that, the lender can examine a large dataset of historical loan behavior, and attempt to build a model based on this historical record.\nOne such dataset is the Freddie Mac loan level dataset [7], which will be used to fit this example model. This data set has approximately 600 million observations, so it is large enough to show an example of a real model. Other similar data sets are also available, most of which contain one to several billion observations. Since this is real data, projections based on it have actual economic value. For instance, projections of future behavior determine the interest rate offered to the borrower, and help the lender determine how to hedge efficiently.\nThe dataset is a collection of loan-month observations. By this, it is meant that each loan in the dataset has one observation per month. This observation is little more than the knowledge of whether or not the borrower made his payment that month. These observations will be considered independently, ignoring the fact that some of them are related to each other through the underlying loan. The logic here is that if the model is accurate and the dataset is comprehensive, then the loan-months will be independent conditional on the data presented. Another way of saying this is that the model residuals will be independent. This is an assumption that can be relaxed, but that is outside the scope of this paper.\nEach of these loan-months has several regressors, including FICO, several flags related to the type of mortgage, loan age, Debt-To-Income ratios, refinance incentive, and others. Each loanmonth also has a status. This status is a combination of the number of payments the borrower is in arrears, as well as some additional information about potential foreclosure proceedings and similar items. For the purposes of this paper, only three statuses will be considered.\n\u2022 Status \u201dC\u201d: The borrower is \u201dcurrent\u201d, and is not behind on his payments\n\u2022 Status \u201dP\u201d: The borrower has repaid the entire loan, the loan no longer exists\n\u2022 Status \u201d3\u201d: The borrower is one payment behind schedule (\u201d30 days delinquent\u201d)\nNow consider the status that the loan will have next month. If the loan-month is historical data, this value may be known, otherwise it it necessary to project it. The purpose of the example model is to predict next month\u2019s status for any loan-month in which we know this month\u2019s status. In this way, a Markov Chain can be built up, progressively projecting the status further and further into the future.\nIn each month, the borrower transitions from his current delinquency state to a new state depending on the number of payments made. For a borrower who is Current (status \u201dC\u201d), he can make 1 payment (remaining \u201dC\u201d), 0 payments (to become \u201d3\u201d), or all the payments (to become \u201dP\u201d). Similarly, if a borrower is already in status \u201d3\u201d, he could become more delinquent by missing additional payments and so on. A separate model can be fit for each loan status, modeling the transitions available to loan-months in that status, so for the purpose of this paper it is enough to consider only the status \u201dC\u201d.\nTherefore, there are three transitions available, in shorthand written thus:\n\u2022 C \u2192 C Going from Current to Current\n\u2022 C \u2192 P Going from Current to Prepaid\n\u2022 C \u2192 3 Going from Current to 30 days delinquent\nThe transition C \u2192 C is more common than the others by several orders of magnitude, but most of the economic behavior of these loans is related to the C \u2192 P and C \u2192 3 transitions.\nLong before large datasets were available, mortgages were modeled using pool level approaches, which simply averaged a group of loans together and then applied the model. Some of these models are still around, however they are quickly falling out of favor. The essential issue with the pool level approach is that it is subject to numerous problems related to this averaging. For instance, there is a world of difference between two pools with 700 FICO, one where every loan has exactly a 700 and another where half the loans have 600 and the other half have 800. In many cases, the pool behavior is driven by just a handful of loans, and the information about this handful of loans is destroyed by the averaging process."}, {"heading": "1.1.3 The Example Data", "text": "The fields presented here are a handful of the more important factors driving mortgage behavior. A complete mortgage model would include dozens of effects, but it is not helpful to list them all out here.\n\u2022 loanId: A unique ID identifying the loan.\n\u2022 month: The month of the observation.\n\u2022 FICO: The borrower\u2019s FICO score at origination.\n\u2022 MTMLTV: The LTV of the loan modified for house price, amortization and curtailment.\n\u2022 age: The age of the loan, in months.\n\u2022 incentive: The interest rate improvement the borrower would expect from refinancing.\n\u2022 isOwner: Is this home occupied by its owner.\n\u2022 startStatus: What was the status of the loan at month start.\n\u2022 endStatus: What was the status of the loan at month end.\nEach row in the table above represents one observation. A given loan would be represented by many observations. Some of the columns (such as FICO) that don\u2019t change with time will depend only on the loanId, others will depend on loanId and month. Computing several of these columns could be a very involved process. For instance, MTMLTV requires knowing the value of the home as well as any amortization or curtailment undertaken by the borrower. Incentive requires an estimate of what rate the borrower would be offered if he decided to refi in the given month. When fitting on historical data, these calculations are often much easier, as the historical data is known so fewer sub-models are needed to make forecasts.\nFor the purposes of this paper, it is not important to understand the specifics behind these regressors, other than to know a few salient facts.\n1. There are a large number of them.\n2. Some of these regressors require substantial calculation to derive.\n3. Some of these regressors are highly path dependent, depending on previous status transitions."}, {"heading": "2 Efficiency Considerations", "text": "For the space of models in question, efficiency will be very important. When discussed here, there are two types of efficiency considered.\n\u2022 Computational Efficiency\n\u2022 Information Theoretic Efficiency\nA good model in this space is one that very closely approximates the actual phenomenon under consideration using minimal human intervention and a reasonable amount of computing resources."}, {"heading": "2.1 Computational Efficiency", "text": "Within the world of mortgage modeling, the computational space is huge. There are roughly 60 million mortgages in the country. Mortgage models typically require path dependent effects. As just one example, one of the best predictors for whether or not a borrower will miss a payment is the number of months since the last missed payment. Regressors such as this ensure that while each loan-month transition probability may have a closed form, there is no (known) closed form for the distribution of loan status at any time more than 1 month in the future. Therefore, if loan level accuracy is desired, then numerous (e.g. 1000+) simulations will be needed. A typical loan (on a typical path) will require an average of approximately 100 months of projection before liquidation, since the typical mortgage is refinanced approximately once every 5-10 years. In addition, the financial firms that uses these models typically need to examine a significant number (e.g. 20) of interest rate and house price scenarios. So the number of loan-months we would like to compute each day is:\n\u2022 nloan = 6 \u2217 107\n\u2022 npath = 104\n\u2022 nmonth = 102\n\u2022 nscenario = 2 \u2217 101\nnloanMonth = nloan \u2217 npath \u2217 nmonth \u2217 nscenario = 1.2 \u2217 1014 (1)\nGiven that a single call to ex takes on the order of 100 CPU clock cycles, assume that a model takes at least 10,000 cycles per loan-month. This implies that computing the whole universe would take at least 1.2 \u2217 1018 clock cycles. This is about 500 million CPU seconds, or approximately 110,000 CPU hours. Clearly, running on a compute grid will be necessary to perform any meaningful fraction of this computation.\nThroughout this paper, it will be important to consider the computational cost of the functions being used. Generally, only entire (in the sense of complex analysis) functions should be used, as they are highly suitable for numerical optimization. Being entire, they are defined everywhere (so there is no \u201dedge\u201d for the optimizer to hit), and they have infinitely many continuous derivatives, ensuring that they are smooth enough to be optimized easily. Of these functions, polynomials are typically cheaper to compute than other functions in this family. Unfortunately, polynomials are often unsuitable due to issues with oscillations, closure, and unboundedness. Of the non-polynomial entire functions, the exponential function is unusually inexpensive, and therefore will be used in preference to other options whenever it is appropriate.\nIt is these efficiency considerations, as much as the related explainability considerations that lead the models chosen in this field to be overwhelmingly parametric."}, {"heading": "2.2 Information Theoretic Efficiency", "text": "Within this space, the data itself is not believed to be generated by any exact closed form formula. Therefore, it is important that the family of functions making up the model be able to approximate the physical phenomenon in question very closely. Appendix II has some definitions and discussion of these factors.\nThe important point to take away from Appendix II is that most models will not converge to the exact distribution of the data. In this problem space, the dataset is large, so if the model fitting itself is efficient in the sense that the variance of the parameters is small, then the error in the model will come to be dominated by the mismatch between the model form itself, and the distribution of\nthe data. The primary purpose of ITEM is to eliminate some of this mismatch, and thus produce a more accurate model.\nIf a distribution is the limit of a sequence of models, it is clear that in realistic cases it is the limit of such a sequence as the number of parameters goes to infinity. If it were a limit of a sequence of functions g(~x|\u03b8) with a fixed set of parameters \u03b8, then it would be equal to g(~x|\u03b8MLE), which is by assumption not the case. So it is important that this sequence converge quickly, without including an unnecessarily large number of parameters. In future work, this question could be explored more completely in the context of Kolmogorov Complexity, but for the purposes of this paper it is enough to have heuristic means by which to prevent the unnecessary waste of parameters."}, {"heading": "3 Model Choice", "text": "There are several broad families of models that could be applied to this space. A sample is explored below.\n1. Linear Regression\n2. Nonlinear Regression\n3. Kernel Smoothing\n4. Neural Network\n5. Random Forest\nIn this problem space, the predicted quantity in question is a probability. Therefore, linear regression is totally unsuitable, as it can give rise to probabilities that are greater than 1.0 or less than 0.0. Even if this were to be bounded, it doesn\u2019t have the expected saturation behavior. We would expect that whatever function we choose would steadily approach some level of probability as the regressors become more or less favorable.\nComputational cost and engineering considerations eliminate kernel smoothing and neural networks from consideration. Neural networks themselves would blow far beyond the 10,000 CPU cycle computational budget envisioned for this model. In addition, there are issues with explainability. Kernel smoothing suffers from the correlation among loans. Due to macroeconomic variables, the entire loan population is moving with time, so most loans in the future will not be very near to any significant number of loans from the past. There are also issues with tiling this dataset properly to be able to run it on a grid, and with the fairly high dimensionality of the data itself. The space is simply too large (and too mobile) for kernel smoothing to work well.\nLastly, the random forest could work, but it has issues with explainability and also parameter efficiency. Simply put, the random forest will use far too many parameters, and run the risk of very dangerous overfitting. With regards to explainability, in the mortgage world, people talk about the \u201dcredit box\u201d, which is in typical parameter space, quite literally a box. For instance, (LTV < 80, Fico > 720) is a reasonable credit box. Even defining structures such as this becomes very hard with a random forest approach, requiring extensive simulation. For something like multinomial logistic, it is pretty easy (though not trivial) to define level sets of the function. These level sets may be high dimensional, so some projection is requied in order to get anything that is very explainable. In the absence of the need for very high precision, a credit box approach can serve as a simple substitue.\nFor all of these reasons, modern mortgage models are typically parametric."}, {"heading": "3.1 Parametric Models", "text": "Within mortgage modeling, there are two widely used families of parametric models.\n1. Structural models\n2. Multinomial regressions\nThere are advantages and disadvantages of each which will be discussed in turn."}, {"heading": "3.1.1 Structural Models", "text": "For a good overview of a structural model, see [8]. When modelers talk of structural models, they typically mean a model that is not the result of a maximum likelihood optimization. Occasionally, (as in [8]) a structural model is more explicitly defined as hedonic (see [9]) model driven by known (e.g. economic) factors. Usually a true structural model is both of these things, and will take the form of a collection of unrelated terms that are estimated through manual examination of the dataset, or by drawing some analogy between the data and relevant policy or law. The problem with this approach is that it is hugely expensive in terms of man hours, tends to have explainability issues (i.e. why did you add this rather than that), and almost always results in parameters that are not very optimal. It doesn\u2019t help that only a handful of effects can be discovered through this manual process, and people are not good at visualizing data in higher dimensions.\nStructural models are effective when the data is very limited, and when there is insufficient computational power to fit an accurate model on a large dataset. Historically, this was very much the case, which is why older mortgage models tend to be structural. However, when the dataset size is very large and computational resources are readily available for fitting, structural models\ntend to be less efficient than nonlinear regressiosn both in terms of information theory and in terms of computational performance.\nThis inefficiency is caused by several factors. The first is that (by our definition here), in a structural model, the parameters are provided by hand and are not the result of an MLE estimation. It is certainly possible that \u03b8structural = \u03b8MLE , but in practice it is unlikely. For this reason, the model will almost always be biased, and the variance of the parameter estimates will be needlessly high. In addition, it is possible that the parameters are added in an efficiently parsimonious fashion so as to not overparameterize the model, but again, without carefully searching the problem space, this is unlikely to happen of its own accord in practice. In addition, it is certainly possible that the model could be constructed from entire functions chosen for efficiency, but again, this is not common in practice. As a result, these models tend to be inaccurate, bloated with excess parameters, and computationall inefficient."}, {"heading": "3.1.2 Multinomial Regression", "text": "A nonlinear regression needs to properly represent the problem space. Primarily, this means that it needs to produce a probability vector for any input. A broad class of functions that fit this description is the class of multinomial cumulative distribution functions. This family of functions takes a vector of values (often referred to as \u201dpower scores\u201d or \u201dpropensity scores\u201d) as input, and produces a probability vector as output. Common examples of this family are logistic (associated with the logit function), and the gaussian CDF (associated with the probit function).\nFor these models, if there is a strong belief that the data actually follows one of these distributions, then this should certainly be considered. For most datasets, and particularly for mortgages, there is no reason to believe that one of these distributions is a more natural fit than the other. For this reason, the logit model is typically used because it is computationally cheaper than the probit model."}, {"heading": "3.2 Multinomial Logistic Regression", "text": "Since modern mortgage modeling is typically based on multinomial logistic regression, it is helpful to review the technique here. One can see an overview of the multinomial logistic regression model in [3].\nConsider first the function multinomial logistic function.\nL(~v) = e~v\n|e~v| (2)\nIt is understood in the definition above that the exponential of a vector is taken element wise to produce a vector, and that \u03b2 is a matrix of parameters. The values of ~v are known as power scores. If we set the probability vector ~y = L(~v), then this equation is underdetermined since the values of ~y sum to 1. The equivalent invariant for ~v is that subtracting the same value from all elements of ~v leaves the values of ~y unchanged. Traditionally, the largest power score is subtracted from all the power scores, guaranteeing one power score is 0 and the rest are negative. This improves the numerical stability of the logistic function itself and ensures that overflow results in the correct limiting behavior rather than NaN. With that convention in place, L(~v) defines a bijection, so it has an inverse.\nL\u22121(~y) = ln( ~y\ny0 ) (3)\nWhere here y0 is the probability corresponding to the 0 element of ~v. These power scores are then derrived from the regressors using a simple dot product.\n~v = \u03b2 \u00b7 ~x (4)\nModeling the data in this way makes the following assumptions.\n\u2022 ~y is linear with respect to ~x in logit space.\n\u2022 The terms of ~x interact through summation in logit space.\nThese limitations are critically important, in that they are typically not satisfied by any real world phenomenon. This problem is resolved by introducing an arbitrary vector valued function F (~x) that transforms the values of ~x into new regressors which are linear with respect to ~y in logit space and do interact by summation.\nNow the model looks like this.\nL(\u03b2 \u00b7 F (~xi)) = e\u03b2\u00b7F (~xi)\n|e\u03b2\u00b7F (~xi)| (5)\nThis equation is now perfectly general. Any probability vector can be written in such a form provided F (~x) can be discovered, and also that one is willing to allow \u03b2 to have entries in the generalized reals (R \u222a {+\u221e,\u2212\u221e}).\nThis function has a (relatively) simple closed form derivative. Start by computing the derivative with respect to the power score.\nd dsi L(~s)w = (\u03b4i,w \u2212 L(~s)i)L(~s)wdsi (6)\nNow if si = \u03b2i \u00b7 F (~x) then the derivative with respect to \u03b2i,j follows.\nd\nd\u03b2i,k L(\u03b2 \u00b7 F (~x))w = (\u03b4i,w \u2212 L(\u03b2 \u00b7 F (~x))i)L(\u03b2 \u00b7 F (~x))wF (~x)k (7)\nSimilarly, if F (~x) is parameterized by ~\u03b1, then\nd\nd\u03b1i L(\u03b2 \u00b7 F~\u03b1(~x))w = (\u03b4i,w \u2212 L(\u03b2 \u00b7 F~\u03b1(~x))i)L(\u03b2 \u00b7 F (~x))w(\u03b2w \u00b7\nd\nd\u03b1i F~\u03b1(~x)k) (8)\nLikewise, the derivative of the log likelihood can be computed.\nd\ndsi \u2212 ~y \u00b7 ln(L(~s)) = \u2211 w yw \u2217 (\u03b4i,w \u2212 L(~s)i)dsi (9)\nIn the mortgage modeling world (returning to the example), the vector ~xi is the data for the i\u2019th loan-month, and the vector ~yi represents the probability that the loan will be in each of the available loan statuses in the next month. The parameters \u03b2 depend on the starting state, for this example we can assume that the model under consideration is for computing the C \u2192 \u2217 transitions. The loan-months are then simply partitioned by starting state, and the appropriate value of \u03b2 used for each.\nThe difficulty now is reduced to finding a suitable value of F . The model would be able to converge to any distribution of ~y if the corresponding value of F could be found. If no such value of F can be found, then the model will converge to a distribution that is a non-infinitesimal distance (in KL divergence, for instance) from the true distribution of ~y. Given the function F , the remaining fitting of the model is an entirely mechanical maximum likelihood estimation."}, {"heading": "3.3 Optimality of Logistic Regression", "text": "The logistic regression model is extremely efficient for several reasons. First of all, it is critical that only analytic functions be considered. Analytic functions are always C\u221e, and thus it is always possible for an automatic nonlinear optimizer to solve for its parameters. In addition, analytic\nfunctions are closed under composition, so it is easy to build analytic functions using simpler analytic functions as building blocks. It should be possible to make a good model using C\u221e functions that are not analytic, but such functions are rare and often difficult to construct. As a practical matter, all reasonable functions that are C\u221e are also analytic, so this is a distinction without a meaningful difference.\nMultinomial logistic regression is analytic, and it also has computational efficiency that is in some sense optimal. See Appendix I for more details. In brief, any analytic CDF will be based on one or more analytic functions that are not polynomials. The computationally cheapest such function (subject to certain conditions) is the exponential function. Therefore a simple formula incorporating exponentials will be more computationally efficient than any other option. The logistic function is just such a function. In addition to the above mentioned traits, the logistic function has arbitrarily many simple closed form derivatives. This greatly accelerates the process of fitting model parameters and reduces the associated errors.\nLastly, if the proper value for F can be discovered, then the logistic regression will converge arbitrarily closely to any distribution. In that sense, it is completely general. In addition, if the function F can be recovered efficiently from the data at hand, then this regression will also be information theoretically efficient in that it will produce parameters with low variance and converge quickly towards the actual data distribution, thus producing a model with small residuals. The ITEM system attempts to recover this value of F automatically."}, {"heading": "3.4 Splined Multinomial Logistic Regression", "text": "Returning to the mortgage modeling problem described in Section 1.1.2.\nFor the multinomial logistic models, the problem usually begins with variable selection. Here, expert knowledge is important. Some of the regressors are fairly obvious, but others reflect real economic motives and drives, but are only obvious when examining certain interactions between the dataset and historical data from other sources. In any case, a candidate list of regressors is drawn up. It is helpful to eliminate regressors that do not have much impact, and often modelers do so using p-tests and Lasso style regularization to come to an initial fit. Initially, the function F is simply taken to be the identity function, or perhaps some simple splines chosen a-priori.\nOnce this is done, the next step is to look at the residuals. Observing a typical set of residuals, it will generally be the case that there is some smooth deviation from the projection. It appears that the projection is following one curve, but the actual data is following another. The mean of the two (for a suitable definition of mean) must be the same, but the curves do not in general coincide. This situation comes about when the response to a variable is not linear in logit space.\nThe solution to this problem is generally to construct some splines, the linear step spline defined below is typical.\nsa,b(x) = min(1,max(0, x\u2212 a b\u2212 a )) (10)\nThen the model would be recalculated using this spline instead of (or perhaps in addition to) the original regressor.\nL(...) = e\u03b20+\u03b21\u00b7sa,b(x1)+...\n|...| (11)\nBy adding several of these splines, the response curve for x1 could slowly be transformed into any particular curve, making the approximation better and better. In addition, this spline is bounded, so it naturally defends the model against extreme outliers.\nThis spline is an example of a (non-analytic) CDF. Unfortunately, because this curve is not analytic with respect to a and b, it is usually not possible to run an optimizer over it in order to find the optimal values of these parameters. Therefore, the values of a and b are typically chosen manually through inspection of the data and model residuals. This is extremely expensive in terms of man-hours, and results in splines that are almost never truly optimal. In addition, when adding splines, it is not typically feasible to determine which regressor most needs a spline, so often the splines actually included (even with optimal parameters) are less helpful than other splines which were not included. Using a sequence of these splines, the model can approach the true distribution arbitrarily closely, but it cannot do so without almost unlimited manual labor.\nOften, modeling groups attempt to avoid this unlimited investment of manual effort by automatically fitting splines. Typically, before the process even begins it has failed. Step splines such as this cannot be fit automatically (with typical algorithms at least) because they are not analytical in their parameters. Often, this gives rise to an attemp to use cubic splines (which suffer from the same problem) or polynomials.\nPolynomials deserve special mention, because they are analytic. However, just the same, they will not work. First of all, polynomials are never bounded, so even a few outliers will completely destroy the fitting process. Secondly, polynomials of degree d make a closed group, adding more polynomials of the same or lesser degree simply produces another polynomial of the same degree. Therefore, in order to add \u201dmore\u201d curves, it is necessary to instead add polynomials of progressively higher and higher degree, which brings us to the next issue. Polynomials are strongly oscillitory, having all manner of harmonics and \u201dringing\u201d issues which rapidly make the model nonsensical.\nPolynomial ringing is the wild oscillitory behavior caused by small moves in two points that are very close together and that the polynomial is constrained to pass through.\nWhen this work is tied to the notion that these response curves need to be splines or polynomials, there is never any real hope of success.\nWith an algorithm that can select proper curves (not necessarily splines) automatically, the model would be much improved. This is the goal of ITEM."}, {"heading": "4 An Introduction to ITEM", "text": "The ITEM model builds on the splined multinomial logistic regression by providing a means to fit splines automatically. To get good results, the family of curves chosen must meet the following minimum critera.\n1. The family must be defined by a few (e.g. 2) parameters\n2. It must be bounded and entire in all inputs (parameters as well as x).\n3. It must be very smooth and have at most one local maximum internal to the dataset (i.e. no harmonics)\n4. The family must form a complete basis. Ideally, realistic curves are well approximated by a few members\n5. It must be efficient to calculate\n6. It must have an efficient closed form derivative.\nThe first requirement is necessary for information-theoretic reasons. If the model is to use the AIC to determine which curves to include, this will be greatly undermined if the curves require a huge number of parameters. In addition, the optimizers that will make these curves will have a much harder time finding good results.\nThe second requirement (together with the first) is basically the same as saying that an optimizer can be used to find the parameters. Recall that a curve is entire if it is analytic and defined for all points. For this purpose, we will consider a curve to be entire if it is defined on the whole real line. Similarly, no properly entire function can be bounded (Liouville\u2019s Theorem), but one could be bounded on the real line, which is the important factor here.\nThe analytic requirement ensures that the curve has well defined derivatives. The boundedness and requirement that the function be entire protect against computational problems when an unrealistic starting point is given to the optimizer. The requirement that it be entire also ensures that there are no large zones where the derivative is zero (such as in the spline case). For instance, when optimizing a spline or similar curve, if both a and b are chosen to be below the whole dataset, the optimizer will find all progress impossible because the derivative is identically zero. Choosing curves bounded at 1 in particular helps to make the associated values of \u03b2 easier to interpret.\nIn addition to the computational utility of using analytic functions, there is a physical justification as well. In most realistic datasets, there will be some degree of measurement error. If these measurement errors are (for instance) gaussian, then any properly specified model must be composed of only analytic response functions. We know this because the model response to the data must be a convolution between the error function and the actual functional form of the underlying phenomenon. Analytic functions are contagious over convolutions, convolving an analytic function with any other reasonable (e.g. bounded with finitely many discontinuities) function produces an analytic function. Therefore, any model operating on a dataset with analytic (again, typically gaussian) measurement errors is automatically misspecified if it includes any unanalytic response functions. This is one reason to avoid splines, the sharp turns they make are simply not possible if the visible data has any noise relative to the state driving the modeled phenomenon.\nThe third requirement ensures that results for similar inputs will be similar, a basic requirement for any model.\nThe fourth requirement ensures that any curve can be suitably approximated by a sum of enough curves from this family. This is necessary if the model is to actually converge to the true distribution for large dataset sizes.\nThe fifth requirement is a basic requirement that the model shouldn\u2019t be wasting money, once all the other requirements are satisfied.\nMost of the curves that modelers try to use here fail at least one of these tests. Requirement 1 eliminates kernel smoothing, requirement 2 eliminates splines, requirmements 3 and 4 together with 1 eliminate polynomials."}, {"heading": "4.1 ITEM Curve Families", "text": "Notice that for any cumulative distribution function CDF (x) that is analytic in x, the function CDF (a2(x \u2212 b)) will satisfy all requirements, provided it is efficient to calculate. The logistic function is an efficent CDF, and it has closed form derivatives, so that is a natural choice. The\nvalue of a2 is used so that this CDF is always upwards sloping, hence the associated \u03b2 has the expected sign, positive for positive correlation, negative for negative correlation. This function also has closed form derivatives, which make it much easier to handle within the optimizer.\nThe formula is then\nCa,b(x) = 1\n1 + e\u2212a2(x\u2212b) (12)\nThe derivatives are\nd\nda Ca,b(x) = 2a(x\u2212 b)Ca,b(x)Ca,b(\u2212x) (13)\nand\nd\ndb Ca,b(x) = \u2212a2Ca,b(x)Ca,b(\u2212x) (14)\nUnfortunately, this function family is not enough to achieve good results. The problem comes down to the procedure for doing the fit itself. In the case of a regressor with a response that is strongly peaked within the distribution (e.g. it looks like a gaussian), no single CDF can improve the results significantly. Two of them taken together could approximate a gaussian, but if the fitting procedure is adding curves one at a time, this does not help. Therefore, it is necessary to include a spike-like family of curves as well. Fairly obviously, the spike family should be actual gaussians.\nGa,b(x) = e \u2212(x\u2212a)2 2b2 (15)\nIts derivatives are\nGa,b(x) = 2(x\u2212 a)\n2b2 Ga,b(x) (16)\nand\nGa,b(x) = 2(x\u2212 a)2\n2b3 Ga,b(x) (17)\nGiven that the gaussian family is used, if the centrality parameter of the gaussian is set outside of the bulk of the dataset, the result would be a bounded monotonically increasing function within the dataset. It then stands to reason that this might eliminate the need for the Logistic family. Unfortunately, this is not the case. The Gaussian family will tend to have a very large derivative near the edge of the dataset, whereas many features of datasets like the one in the example problem saturate rapidly well within the range covered by the data, and have a much more classic S-curve shape. Therefore the Gaussian family itself will yield poor results, and it is worth keeping the Logistic family as well.\nWith these families in hand, it is then necessary to fit them."}, {"heading": "4.2 Comparison to Generalized Additive Models", "text": "ITEM may be considered to be a special case of a Generalized Additive Model [1], albeit one that is parametric. Since the nature of the problem has eliminated from consideration all nonparametric models, a proper nonparametric GAM could not be considered for this problem space. The issue is that in order to make any predictions, a true GAM would require the whole dataset resident in memory, this is not realistic for multi-TB data sets.\nA slight modification of a GAM could be used, in that the data for each parameter could be bucketed and averaged, then these averages connected with (for instance) a cubic spline. This would have the advantage of being comparatively quick to fit and also relatively efficient to use. The disadvantage is that if the bucket size is too small, there will be a lot of oscillation, but a bucket size too large will lose a lot of detail. An algorithm similar to that used by ITEM to determine the number of buckets using an information criterion could be used, but that would considerably slow the fitting. There are also many potential issues related to the fact that for typical problems, highly non-uniform buckets are likely to be called for, greatly expanding the needed number of parameters. Some of these parameters would then be knot points, which would potentially have numerous issues due to unanalytic results when the knots get close together. It remains to be seen whether a bucketing approach such as this could achieve information theoretic efficiency on par with the ITEM curves themselves. Where there is a strong suspicion that the data follows an S-curve, for instance, ITEM would be expected to have a significant advantage. It can closely approximate an S-curve with just two parameters, whereas a more typical bucketed GAM would require many."}, {"heading": "4.3 Information Theoretical Considerations", "text": "Once an automated procedure is unleashed upon curve selection, it is critically important to handle two information-theoretical considerations."}, {"heading": "4.3.1 Efficient Convergence", "text": "The model must efficiently converge to the true distribution of the dataset. The functions chosen above were selected such that most typical data distributions will be well approximated by a small number of functions that can be selected one at a time. The choice of functions changes the assumptions related to our model from the classic logistic model pair to a new set of greatly relaxed assumptions.\n\u2022 The function L\u22121(~y) is not pathologicial (* see below).\n\u2022 The terms of ~x interact through summation in logit space.\nNotice the first condition. By \u201dnot pathological\u201d we really mean just that it is efficiently approximated by a limited number of logistic and gaussian functions. For instance the condition that its Fourier transform decays rapidly in frequency is sufficient. Rather than requiring linearity, we require only that the function is not pathological, a much weaker condition. The second condition remains unchanged. If there are complicated interaction terms between the variables, it will still be up to a human operator to discover that. No similar scheme can be attempted to uncover interaction terms since the number of such terms at each step is equal to the power set of the elements of x times the number of interaction functions, a space much too large to search effectively and posessing no obvious metrics to allow an optimization.\nHowever, subject to these assumptions, the model will now converge to the true distribution if given enough parameters."}, {"heading": "4.3.2 Resistance to Overfitting", "text": "To achieve optimal accuracy, the model must not incorporate curves that are not well supported by the dataset. This inclusion of extraneous features is called overfitting. This requirement has two portions.\n1. The model should stop calculating when there is no point in further work\n2. The model must not include features that are not well supported by the dataset\nThe key here is to use an information criterion. At each stage of the fitting, include the best available curve, but demand that it pass an information criterion before allowing it to be included into the model. When the best curve can no longer pass the criterion, the model is complete. In the case of ITEM, this is done using the AIC/BIC. These criteria differ by only a constant of (2\u2212 ln(N))k, with the BIC being the stricter criterion. In the results section, the choice of stopping\ncondition is discussed. Typically, it won\u2019t matter, an AIC criterion may include marginally more curves than BIC, but the difference will be small. In many tests, they include the exact same curves. There is some argument to be made that the BIC could be used simply because it is somewhat computationally cheaper since it may stop drawing curves earlier, and it is better to err on the side of fewer rather than more curves when in doubt."}, {"heading": "5 Implementation of ITEM", "text": "There are many subtle points in the actual fitting of the ITEM model, so that procedure will be described here."}, {"heading": "5.1 Fitting ITEM curves", "text": "Regressors can be divided into two types.\n\u2022 Binary Flags\n\u2022 Real Numbers\nBinary flags are just what they sound like, being either 1 or 0. They don\u2019t need to be further considered here, since they have no need of curves, a simple fit to produce a corresponding \u03b2 will suffice.\nReal numbers are typically double precision IEEE floating point numbers. It is important that these values have a well defined (i.e. not discrete) metric. A discrete metric simply means that the regressor is categorical, see [10]. Having a non-discrete metric over a space means that there is some sensible notion of distance. Given such a notion, it should be possible to approximate the apparent power score of the data (as evidenced by the observed transitions) by a smooth curve. The exact shape of the dataset itself is not important, but it is important that it (for example) have a fourier transform with rapidly decaying high frequencies. For instance, it should not be oscillatory with a high frequency. These pathologies are not common.\nIf a regressor happens to be a categorical variable (i.e. it is drawn from an enumeration of N values, but without a well defined metric), then it should be replaced with N \u2212 1 flags. It is important to not form a complete basis with any flags that are introduced, as that would cause a linear correlation between the sum of the flags (guaranteed to be 1) and the intercept term of the regression (also guaranteed to be 1).\nFor numerical stability, it is very important to cap the log likelihood at some maximum constant. Approximately 20 is reasonable. This prevents the case where a specific loan with extremely odd parameters drives the entire fit by producing a vanishingly low probability for an even that actually occurs in the dataset. Adding a small random noise (see Annealing, 5.2) can also help here.\nThe basic fitting procedure is as follows.\n1. Fit all flags and betas for existing curves.\n2. If model has maximum number of curves, exit.\n3. For each regressor\n4. For each transition\n5. For each curve type (i.e. logistic, gaussian)\n6. Use an optimizer to find the curve that minimizes the log likelihood.\n7. Consider all curves thus computed, take the on that reduces the log likelihood the most.\n8. For this best curve, examine the AIC, verify that the test passes.\n9. If AIC test fails, exit, otherwise add the curve and go to step 1\nNotice that it is critically important that the curve families were chosen such that a single curve will always improve the fit. This ensures that the optimization procedure above can always make progress and will not get stuck in a situation where fitting two curves at once would succeed, except in the most marginal cases.\nDuring the fitting, there are a few pitfalls. The first thing to remember is that the logistic function has an inverse. Therefore, it is most efficient to convert the model probabilities into power scores, then do an (N + 2) parameter fit where N is the number of parameters in the curve (i.e, 2 for the curves mentioned above). This will be fitting the 2 curve parameters, the curve beta, and an intercept adjustment. All 4 are needed in order to arrive at the actual optimum. For instance, without fitting the intercept adjustment, it will typically be impossible to draw any curve that improves the fit, since any curve will increase or decrease the population-wide average, making the fit worse.\nOnce the power scores are availble, on each iteration simply update scorei = scorei+ intercept+ \u03b2 \u2217 Ca,b(x). This will require only the regressor being fit against and the 4 parameters, it will not require the rest of the regressors which would otherwise bog down the calculation. Then convert these scores back to probabilities, and compute the log likelihood. Remember to use the analytical derivatives. Most of these calculations will not need to process the whole dataset, see the section on adaptive optimzation (Adaptive Optimization 5.3).\nEach curve so fit will cost 3 parameters, 2 for the curve, 1 for the beta, the intercept is an adjustment of an existing parameter, so it is free. These curves can be made available to other transitions at a cost of 1 parameter each, and they could even be chosen such that when applied across all transitions the model is most improved. Both of these paths should be avoided, as either one will result in a model that is less understandable, though admittedly the information-theoretic efficiency may be marginally improved. When sharing curves like this, a few parameters are avoided, but each transition now gets a curve that is not ideal for it, and it may be hard to explain why some features from one transition are suddenly showing up in another. This is a judgement call, and the one place where efficiency has been sacrificed for interpretability within the model."}, {"heading": "5.2 Annealing ITEM curves", "text": "Once all curves are fit, and there is either insufficient information to fit further curves or the model has reached the operator supplied curve limit, the model is complete. At this point, however, the model may not actually be optimal. The exact curves chosen are path dependent, and may not be a global optimum.\nThis is analogous to metal that has been formed and then cooled. If it was cooled quickly, the internal arrangement of atoms may not be at the minimum energy. Essentially, the crystal was left at a local minimum somewhere along the way to the global minimum. Given more time (and sufficient energy), it could eventually, through random chance, bounce out of the local minimum and continue towards the global minimum. At low temperatures, there is not enough energy for this to occur in any reasonable timeframe. The solution in metalurgy is called annealing, simply heat the metal and then cool it very slowly. There is a procedure ([2]) that can improve the total fit quality at the cost of computational time during the fit.\nThis pass is usually not necessary, but if extra CPU cycles are available, it may be attempted. Dramatic improvements from this process should not be expected, but neither are dramatic changes. Most models go through many iterations of fitting and analysis (or bug fixing), so fitting performance is important. However, for a final version, it may be worthwhile to spend the extra resources to get a marginally better product. The fit below assumes that curve parameters are not optimized during step 1. From experience, adding so many parameters to the optimization rapidly bumps against machine precision issues and causes the results to be worse than an iterative approach.\n1. Run a full optimization on all \u03b2 values in the model (but not curve parameters).\n2. If loop count reached, exit.\n3. Loop over each regressor.\n4. Remove all the curves from this regressor, then (as above) generate the same number of new curves.\n5. Once all regressors have been processed in this way, go back to step (1).\nFor this to really work well, some small amount of randomness needs to be added to the model. In this way, any curves that came about simply because of the exact content of this dataset will have a chance to be replaced by other curves that may be better, but are unreachable from this dataset. The proposed method is to add to each observed probability a gaussian noise with a very small standard deviation (around 1.0e-4 to 1.0e-6 depending on dataset size), and then renormalize the results. This has the effect of making events that didn\u2019t happen (probability 0 in the dataset) have some small probability of occuring, and thus they cannot be completely discounted by the model. The size of this standard deviation should be inversely proportional to the dataset size. Essentially, in a dataset with a million points, nothing should be considered less than approximately a one in a million chance, for instance. The data itself simply does not support such a strong assertion.\nAnother way of introducing randomness, is in the selection fo the starting points for the centrality parameters of the various curves to be drawn. This could be started at the actual dataset mean, but that may prevent improvement in some cases, where a local optimimum prevents the the global optimimum from being obtained. An example related to seasonality will be discussed in Section 6.3. One way to avoid this is to instead select a random data point, and start the centrality parameter at that point\u2019s value. This has the effect of allowing some out of the way portions of the parameter space to be explored, but it does not tend to explore places lacking in data. Since the curve fitting process is iterative, there will be many chances to find a good curve, especially if annealing is used. For curves that have an easily reachable global optimum, this won\u2019t matter anyway.\nFor the example built here, annealing results will not be discussed. In future work, good values of the noise term and annealing performance analysis can be discussed."}, {"heading": "5.3 Adaptive Optimization", "text": "For the ITEM model, all the optimizations are taken over the expected value of the log-likelihood of the dataset. These functions look like this.\nf(\u03b2) = 1\nN N\u2211 k=0 ~yk \u00b7 ln(~L(\u03b2 \u00b7 F (~xk))) (18)\nIn this equation, N is the number of observations, and L(\u03b2 \u00b7 F (~xk)) is the model estimated value of the transition probabilities. Considering just the partial sums of this function.\nfM (\u03b2) = 1\nM M\u2211 k=0 ~yk \u00b7 ln(\u03b2 \u00b7 F (~xk)) (19)\nIt is clear that for large M , fM (\u03b2)\u2212f(\u03b2) is distributed as N(0, \u03c3\u221aM ) where \u03c3 is simply the standard deviation of the average element from the dataset. Optimizers are driven not by the value of f(\u03b2), but by the differences between these values, e.g. f(\u03b21) \u2212 f(\u03b22). The standard deviation of this difference in the case of partial sums may be computed directly, call it \u03c3(\u03b21, \u03b22,M)\nFirst, define a term wise difference.\nd(\u03b21, \u03b22, k) = ~yk \u00b7 (ln(~L(\u03b21 \u00b7 ~xk))\u2212 ln(~L(\u03b2 \u00b7 \u03b22, ~xk)))) (20)\nThen the std. deviation of the partial sum can be defined as follows.\n\u03c3(\u03b21, \u03b22,M) = 1\u221a M\n\u221a (d(\u03b21, \u03b22, k)\u2212 \u00b5)2 (21)\nWhere here, \u00b5 can be taken to simply be the sample mean. Due to the sizes of the numbers typically encountered, this will present no problem. Then, for some constant C, successive values of M (starting at M0) are considered until\n|fM (\u03b21)\u2212 fM (\u03b22)| > C\u03c3(\u03b21, \u03b22,M) (22)\nHere the choice for the value of C reflects how certain the modeler would like to be that the new point is actually better than the old point. For values of C around 5 (the standard metric for proof in scientific circles) the odds of spurious point selection are very small, roughly 10\u22126. In any case, the model is not very sensitive to this parameter, setting it smaller doesn\u2019t help performance much, and setting it larger doesn\u2019t improve accuracy meaningfully. The value of M0 needs to be chosen large enough that the law of large numbers will take effect, and the data sample will be representative of the whole dataset. Again, the model is not very sensitive to this parameter. Making it approximately 1% of the data set size is a reasonable setting if the dataset is large. Alternatively something of moderate size such as 104 should work for most data sets. Again, setting this parameter very small doesn\u2019t improve the efficiency much, so if it is a moderately sized fraction of the whole dataset size, little is gained by reducing it further.\nNote that for each successive value of M, the previous results need not be recalculated, only the additional elements need to be considered. If the dataset is large, then it will almost never be necessary to examine the whole dataset. Only in the last few iterations of the optimizer will the comparisons be close enough together that the entire dataset must be considered.\nFurthermore, this gives a natural stopping condition. Whenever the points considered by the optimizer satisfy\n|fN (\u03b21)\u2212 fN (\u03b22)| < \u03c3(\u03b21, \u03b22, N) (23)\nApplied over the whole dataset N , then there is simply no further progress possible, so the current results should be returned as-is.\nThe end result of this algorithm is that for a constant value of x and y tolerance (i.e. stop optimizing when the parameter is bracked close enough to its optimum, etc...) the cost of optimization grows sub linearly. For very large dataset sizes, this cost would approach a constant, as the optimizer would never reach the end of the dataset for any iteration."}, {"heading": "5.4 Hybrid Projection", "text": "Once a model is fit, it must be used to project future behavior. Though this process will not be covered in depth in this paper, an efficient means for performing this projection will be described."}, {"heading": "5.4.1 Markov Matrix Multplication", "text": "If the model contains m potential states, then at each time period, the m\u00d7m Markov matrix at a time t (here denoted M(t)) may be computed. Then the next state ~s(t) may be computed from ~s(t\u2212 1) through matrix multiplication.\n~s(t) = M(t\u2212 1)~s(t\u2212 1) (24)\nThis method has two primary disadvantages.\n1. It forbids the use of non-Markovian regressors\n2. It wastes resources on computations for extremely rare states\nIn mortgage modeling, both of these disadvantages are particularly severe. In general, a regressor such as \u201dnumber of months since last delinquent\u201d is incredibly important, but non-Markovian. In addition, some states (e.g. \u201dIn Foreclosure\u201d) are extremely rare. If the matrix multiplication method is used, then the majority of the computations each month go towards states that are immaterial for most loans, on most interest rate paths. This method is especially expensive since interest rates and housing prices need to be simulated anyway, so it doesn\u2019t even eliminate the need for simulation."}, {"heading": "5.4.2 Simulation", "text": "An alternative to matrix multiplication is to each month select a random transition to make, weighted by transition probability. Now instead of ~s(t), the equation includes ~s\u2032(t) where ~s\u2032(t) is composed of a single 1 and all other elements are zero. This means that only one column of M(t) needs to be calculated. Call this selected matrix M \u2032(t), it is composed of all zeros except for a single 1 element in the randomly selected (weighted by probability) location along the column corresponding to the 1 element in ~s\u2032(t\u2212 1). Then ~s\u2032(t) is now especially easy to calculate.\n~s\u2032(t) = M \u2032(t\u2212 1)~s\u2032(t\u2212 1) (25)\nThis computation involves only a single column of M(t\u22121), and is thus much cheaper (generally about m times cheaper) than the computation in equation 24. Furthermore, since there is no longer any uncertainty about the loan status in any month, non-Markovian regressors no longer pose a problem. Unfortunately, this method introduces substantial additional simulation noise, which may require the use of additional paths, thus increasing computational cost.\nIn typical usage, especially if loan level noise is not a problem (i.e. only aggregate behavior of a pool is needed), simulation is several times cheaper than matrix multiplication, even when it requires the use of extra paths due to excessive noise. This is especially true since many of the most expensive computations are related to some of the rarest states (e.g. \u201dForeclosure\u201d) in many loan level models."}, {"heading": "5.4.3 Hybrid Simulation", "text": "Ideally, one would like a mechanism as inexpensive as simulation, but with as little noise as matrix multiplication. In addition, this method should allow the use of non-Markovian regressors, just like simulation does.\nReturning again to the mortgage modeling example, notice that most loans will be in status C most of the time. For any loan that starts in status C, we could assume that it will always remain\nin C. If we do so, then the non-Markovian regressors are not an issue, since the status at each time is entirely clear. Comparing this to the simulation approach, we can have a misprediction in one of two ways.\n1. The loan transitions to P\n2. The loan transitions to 3\nIn the first case above, since P is an absorbing state, no harm has been done. Though the non-Markovian regressors are no longer correct, they are also no longer needed. In the second case, it would be necessary to then begin real simulation in order to correctly calculate future states. Taking T (t,X) to be the probability that the loan transitions from C to X in month t, the following three values are needed.\n1. P (t, C\u2217): The probability that the loan is always in C\n2. P (t, P \u2217): The probability that the loan is in P after previously having always been C\n3. T (t, 3\u2217): The probability that in the current month, the loan goes to 3 having always been C\nThese values can be easily represented in terms of the previous months\u2019 values.\nP (t, C\u2217) = P (t\u2212 1, C\u2217)T (t\u2212 1, C\u2217) (26)\nSimilar equations exist for P and for 3 as well.\nIf the loan matures at time \u03c4 , then compute P (\u03c4, 3\u2217) (using formulas such as those above), which is the probability that the loan ever reaches state 3. For each time t, compute\nT \u2032(t, 3) = T (t, 3)\nP (\u03c4, 3) (27)\nThis is the probability that the loan enters state 3 at time t given that it ever enters state 3. Now select a time t\u2032 randomly, weighted by T \u2032(t, 3), and begin simulation from time t\u2032, given that the loan was current up until then. In this way, compute ~s\u2032(t) for all t \u2264 \u03c4 , taking ~s\u2032(t) = C where t < t\u2032. Similarly, define the status vector S(t) = {P (t, C\u2217), P (t, P \u2217), 0}. Now the status vector ~s(t) can be computed as\n~s(t) = P (\u03c4, 3)~s\u2032(t) + (1\u2212 P (\u03c4, 3))S(t) (28)\nThis computation has the advantages of both simulation and Markov matrix multiplication, and none of the weaknesses. On average, only approximately 1.5 columns of M need to be computed, rather than m. In addition, non-Markovian regressors can be used freely, since in both branches of this computation, the complete state vector up to time t is always known with certainty. The simulation noise of this method is much, much less than the simulation noise introduced by raw simulation, since the rare events (transitions through 3) are over sampled, and the common events (C \u2192 C) are computed exactly.\nLastly, this approach allows the practitioner to spend resources where they are most needed. I propose the following method, performed for each interest rate and house price path. For each loan number n, compute w(n) = P (\u03c4, 3), with the understanding that P (\u03c4, 3) is a function of n.\n1. Compute \u03b3 = \u2211\nnw(n), where the sum is taken over all n loans.\n2. Determine some threshold, call it \u03b5, perhaps \u03b5 = \u03b3nq for some integer q.\n3. Loop through all loans, compute \u03b1(n) = min(1, w(n)\u03b5 )\n4. For each loan, draw a random value \u03bd(n) \u2208 [0, 1)\n5. If \u03bd(n) > \u03b1(n), skip this loan\n6. Otherwise reduce w(n) by \u03b5, and compute a path (as defined in equation 28) for this loan\n7. Continue looping until all nq simulations have been performed, or no loan has w(n) > \u03b52 .\nThis procedure is designed to allocate calculations efficiently. In general, loans that have a higher probability crossing status 3 will get more simulations. The end result is that all loans will have a similar amount of simulation noise at the end of the process. This avoids the very common problem where most loans have very little uncertainty, but a few have a lot, so most computational resources are wasted on the loans where it makes little difference.\nAn alternate version would involve no randomness, and would simply apply w(n)\u03b5 (rounded up, presumably) computations to each loan."}, {"heading": "6 Results", "text": "The ITEM model was implemented as discussed above, and applied to the Freddie Mac loan level dataset. For the purposes of this demonstration, only a limited set of regressors were used. In a true production setting, a much larger set would be used, but the results would be otherwise very simliar. The inclusion of several of the most important regressors will be enough to prove the point.\nAdditionally, it should be noted that only the first few million observations from the database were used. The observation count is limited by the RAM of the test machine. In a live production setting, the observations used would be selected randomly, in addition, they would be shuffled. Without the shuffling, it is known that the observations used correspond to the oldest loans. In addition, the adaptive optimization is somewhat undermined by the fact that the first few blocks of calculations represent only a few (about 10k) loans originated around the same time. Therefore, no curve is reachable that doesn\u2019t improve the fit of these loans. This deficiency was allowed to remain in order to show the behavior of a multi-curve age regressor, which otherwise would have ended up with only two curves rather than the 5 shown here."}, {"heading": "6.1 Initial Curve Fitting Convergence", "text": "One important aspect of fitting is that it must be parsimonious with the curves to be added. The model performs well on this score. The table below describes the curves added when the model is allowed to fit on 1 million data points from the dataset. The model was allowed to select curves from age, incentive, fico, LTV and calendar month (to capture seasonal effects). Among these 5 regressors, 13 curves were drawn. The model was allowed to draw as many curves as it found useful, but stopped after 13.\nIn the table above, the first 5 of these curves are shown, as well as curve 10 and curve 13. The AIC shows exponential decay behavior, though the decay slows for curves 6 through 10. It is interesting to note that curve 5 improves the AIC more than curve 4. The improvement in the age fit allowed for a better selection of incentive curve in the next step than was possible previously, this is a good example of the path dependency that annealing is designed to handle. After an intial improvement, the results quickly level off, as expected.\nThe BIC for each of these curves is also recorded in the table. It differs from the AIC by a constant of roughly 35 for a dataset of this size. If the BIC is used instead of the AIC, then only 10 curves (rather than 13) would have been drawn. The difference in final log likelihood is immaterial (0.130071 vs 0.130020), but the computational cost would have been reduced by roughly 20%.\nAfter these 13 (or 10) curves, the model is unable to find any better curves. The primary reason for this is that the std. deviation of the log likelihood is large enough that all future optimizations rapidly halt, as there is not enough data to conclusively point the way to a better curve. With a\nlarger dataset, it would be expected that this situation would improve. Even so, the curve count is expected to be highly sub-linear in dataset size. Notice also that ITEM did something that a typical human analyst would not do, it did not use all of the regressors, instead opting to place more curves on the most important few regressors rather than placing a few on on each.\nThe concentration of the curves on a few regressors is an interesting phenomenon. What is really happening here is that several of the regressors have strong colinearity and a dramatic difference in predictive power. Any weak regressor will appear to be pure noise if the distribution of stronger regressors is not uniform when projected onto this regressor. This prevents weaker regressors from getting any curves on them until the stronger regressors are adequately fit. At the same time, some strong regressors are strongly colinear with each other, meaning that each curve drawn on one of the regressors actually greatly improves the smoothness of the other regressor. Consequently, the algorithm moves back and forth between the two strong colinear regressors, ignoring all others until those regressors have been fully accounted for. You can see this oscillation in the order in which the curves were fit.\nIn a larger dataset, with 5 million points (see table above), fewer curves are added, but better results are achieved. The reason for this is related to the defense against overfitting built into the model. The model will not refine a parameter beyond the point where it moves the log likelihood by less than one sample standard deviation. This protects against over fitting by avoiding cures that might otherwise have a spuriously high AIC score. In a larger dataset, not only is the noise within the dataset suppressed (thus allowing easier fitting to the smooth model curves), but the\nparameters can be refined further due to the smaller standard deviation of the mean of the log likelihood. This means that each curve is in general a better fit, which can offset the ability of the dataset to otherwise support additional curves. Also, note that there was no difference between AIC and BIC, they both would have included the same set of curves.\nNote here that the centrality parameter of these curves shows little variation. This is apparently due to the presence of numerous local minima in the noisy dataset. One solution to this problem is to pick the centrality parameter by simply selecting the relevant value from a random point in the dataset. This will have the effect of starting the curves (typically) in concentrated parts of the dataset, but the added noise should help to overcome this problem related to local minima."}, {"heading": "6.2 Fitting Computational Cost", "text": "For the purposes of this test, the fitting is allowed to draw any number of curves on any of 5 regressors for 3 transitions. This fitting was performed several times with different dataset sizes to see how the fitting time grows with dataset size. It was also retried with several configuration options related to adaptive optimization to measure the performance with and without the improvements brought about by adaptive optimization. All tests were performed on a 2009 Mac Pro with 4 physical CPU cores, 8 virtual cores.\nThe column Adaptive notes whether or not this test optimizes by examining the whole dataset on each iteration, or simply examines a subset if that is enough to determine which point is better.\nThe column \u201dSigma Stop\u201d indicates whether or not the optimization stops when the points are all within one standard deviation of each other.\nThe table above includes three time estimates. One is the time per curve, another is the total time, and the third is the time required to draw 8 curves. This last (curve 8) time is perhaps the fairest measure. The values are noisy, so it is hard to draw any firm conclusions, but the adaptive optimization does generally perform better. The total time is much better for the adaptive optimization, since it avoids drawing a large number of relatively less important curves for larger dataset sizes. Notice that the rows where both of these flags are false indicate typical optimization algorithms. The adaptive optimization with a 1-sigma stop performs almost 3x faster (20% faster on a curve for curve basis) already by the time the dataset gets to 10 million points. With a much larger dataset near a billion points, this difference would become much, much larger. The adaptive optimization does not lose any meaningful degree of accuracy, and in fact is far more parsimonious with curves as well simply due to the fact that it avoids fitting curves that cannot match well on several subsets of the data set.\nIt is expected that this trend continues into very large dataset sizes, saving substantial computation time. In this case, ten times more data requires approximately 4 times as much computation time. For very large dataset sizes, the cost of adaptive optimization (at a given level of precision) would approach a constant. It is expected that the cost of a round of annealing would be similar to the cost of drawing all these curves initially.\nThe real limitation in this context is RAM. The calculation requires approximately 500 MB of RAM per million observations. Given that the computer used for this exercise has only 8 GB of RAM, it is hard to push much beyond 10 million observations, or roughly 1% of the dataset. Similarly, for reasons of simplicity, this sample is not being chosen uniformly from the entire dataset, but is rather composed of the first observations from the set. This is done just for speed and efficiency when loading the data. In a real production setting, a representative sample (possibly the whole dataset) would be precompiled, and that would then be used. It is estimated that a computer with roughly 500-1000 GB of RAM (and about 40 CPU cores) could optimize a model over the entire dataset. A system such as this is well within the reach of even a small corporation, and might cost approximately $50,000. Such an optimization would take roughly 6 hours, easily accomplished as an overnight job. Smaller optimizations on representative subsets of the data could be finished in minutes, allowing for very rapid prototyping and development.\nAs can be seen from the table above, the cost per row of these optimizations is very moderate. It requires roughly 1700 clock cycles to do a row of curve fitting, and roughly 10,000 to do a row of coefficient optimization. In this case, the coefficient optimization was fitting only flags, that cost would be expected to rise somewhat as more curves are introduced. The curve fitting cost is far more important, as more than 90% of the fitting time is consumed by curve fitting rather than coefficient fitting.\nThe code used for these examples is not fully optimized. In particular, it uses correctly rounded ex and ln functions. Replacing the correctly rounded exponential function with one with a relative error of approximately 10\u221215 more than triples the performance of the computation at no material cost to accuracy. Additional optimizations are certainly possible. The cycles/row computation assumes 100% utilization of all four cores in the CPU of the test machine, but in practice the utilization rate is somewhat lower (about 70%), primarily due to the small size of the dataset preventing a more efficient division. Additional tuning would increase that value to nearly 100%, reducing the cost still further. For the curve fitting, it should be possible to process a single observation in less than 200 clock cycles on a typical modern CPU in Java. Straight C or C++ code would perform similarly, though it might be possible to do better with carefully hand tuned assembly code that takes full advantage of the SIMD units within newer CPUs.\nSimilarly, the coefficient optimization is essentially the same operation that would be used to project future behavior. Even with no additional code improvements, this cost is in line with the resource budget set out at the beginning of the paper. Careful code optimization should keep the calculations under budget even in a full scale model using numerous regressors and interaction terms."}, {"heading": "6.3 Fitting Results", "text": "With the example above (using 1 million data points), already the results are greatly improved by the curves that were drawn. Notice that all of these fits were computed entirely without human intervention other than for a human to define the regressors, and give the model the list of regressors it may use. A typical mortgage model built within industry takes at least a man-month of data analysis, fiting and validation. The ITEM model automates most of these tasks, accomplishing in minutes what a human would typically do in a few days in a more traditional setting.\nIn these charts, the curve labled Before is from the model with all flags fit, but before any of the curves have been drawn. So this model does not include the effect of age or incentive. The curve labeled After is from the model with the curves added. The charts show clearly that even a few curves have dramatically improved the fitting of these regressors.\nNote that the age curve shown is strongly influenced by the fact that the loans under consideration are all originated near the same time. Therefore, the spike seen around age 36 months is strongly influenced by the prevailing interest rates at that time. With a larger dataset, this correlation between age and time would smooth out, and a more classic age curve would be recovered. However, for the purposes of this demonstration, it is enough to show that ITEM can construct a model that represents the data. It is not necessary to show that the data set chosen is a truly fair representation of the mortgage universe.\nThe seasonality curve shows a good example where again annealing might be helpful.\nIt can be seen in this example that the seasonality effect was not captured. Ideally, that spike near the end should be captured, but give all the smaller spikes throughout the year, the optimizer would be likely to hit a very unsatisfying local minimum rather than finding the actual behavior related to December and January. Also, in this example, the low season for prepayment spans the year end, so it is not localized when looking at the seasonality in this way. The model might still draw curves here that would help, but it would take a large amount of data and would take at least two curves to really improve this significantly. This is an example where a small amount of randomness could help dramatically, by allowing the optimizer to find the spike at the end."}, {"heading": "6.4 Model Curves", "text": "A full and complete examination of all curves drawn by this model is beyond the scope of this paper. However, for illustrative purposes, the curve related to incentive for the 5 million point dataset is shown below.\nAs can be seen from Figure 5, the response of the model to incentive is an extremely smooth curve that rapidly saturates in both the positive and negative directions. This curve has been represented here as a multiple. Assuming the C\u2212 > P probability is not too large, then with a given value of incentive, that probability is multiplied by the value of this curve. Only relative multiples matter here, so it doesn\u2019t matter if the curve passes through 1 or not. This shows that a loan with strong negative incentive is about one third as likely to prepay as a loan with strong positive incentive. This fits with intuition, and in fact is a close approximation of the historical data seen in Figure 3. Notice that in Figure 3 the highest vs. lowest prepayment multiple is approximately 7 to 1, but that graph is confounded by numerous other factors (occupancy type, age, etc...), whereas the response above is purely for incentive acting on its own.\nThe primary advantage of the ITEM model is that it expresses all responses to input variables in terms of these extremely smooth curves, and thus tends not to over fit. This can be seen in the response to Age, which has 5 different curves fit to it for the C\u2212 > P transition.\nEven here, the curves combine to make a single very smooth curve. This response is largely driven by the correlation between age and interest rates, due to the limited fitting sample. Arguably, this is a worst case scenario. As the sample size increases, and more variation in the loan-months is pulled in, this age curve will flatten out and look smoother. However, even as it is, the age curve makes intuitive sense. Loans tend to not prepay for the first few years of life, then they enter a period of fast prepays before finally tailing off into old age. This curve captures that phenomenon surprisingly well already."}, {"heading": "7 Advanced Fitting Topics", "text": "The previous sections explored some basic fitting routines and results. This section will concentrate on a single large fit (roughly 15 million data points) drawn randomly from the dataset. The loans in this dataset are selected by hasing the loan id from the dataset using SHA-256, then reducing it mod 50 and taking any loan with a reduction congruent to 0. This selection procedure should ensure a well distributed random sample.\nIn this exercise, several experimental fitting choices were made in an attempt to improve the fit.\n1. The ordering of the data points was randomized\n2. The starting curve centrality parameter was set to the regressor value of a random selection from the sample\n3. The starting curve slope parameter was randomized in sign and magnitude\n4. Annealing was run at the conclusion of the initial round of fitting\nThe addition of these random choices would be expected to make the model less likely to get caught up in local minima, and indeed the quality of the fit is qualitatively better than those seen before. One primary reason for this is that the optimizer finds a saddle point at zero beta, which means that it has a very difficult time reversing the sign of beta during the optimization. Always starting the beta as a positive number would therefore tend to bias the selection strongly towards results that would naturally have a positive beta, reducing the fit quality. Using a random starting point and random sign of beta helps this problem greatly.\nOne unexpected side-effect is that gaussian curves become significantly more common than logistic curves. The likely reason for this is that since the beta sign cannot be easily flipped, a logistic that starts off with the wrong sign will be unable to improve the results. However, a Gaussian can overcome the incorrect beta sign by simply moving the centrality parameter to be very high or very low. In the future, perhaps an adjustment to attempt both signs on every iteration would reduce this bias towards Gaussians.\nSurprisingly, annealing was unable to improve the quality of this fit. The issue appears to be the high degree of colinearity in the regressors. With so much colinearity, dropping all the curves from a given regressor does not provide a very clean slate, and it may be found that only curves that interlock with existing curves in a very particular way are very effective. Presumably, annealing would be more effective if it was used earlier in the process, as here it was used only at the end. In addition, in a less contrived example where more regressors are considered, it is expected that annealing will prove more useful.\nThe tables below show the in sample validation against several key variables. Notice that the qualitative fit is not much better, but the regressors (especially age) that are strongly correlated with time make a lot more sense due to the better sampling that reduces the contemporality of the selected loans.\nThe one curve that stands out is LTV. Though the model was allowed to draw curves on LTV, it did not do so, yet the LTV results fit very well. Primarily, this is due to colinearity between LTV and other regressors, primarily FICO. If more up-to-date LTV values were available, this regressor woudl be far more useful. Below, the model curves for these regressors are shown. Here the model transition probabilities are charted directly for some sensible choice of the regressors not being charted. The effect of the excessive use of Gaussians is immediately obvious, as the curves start to do unusual things once they leave the domain where the data lies. For loans that could be\nextreme outliers, this could matter. A solution as simple as capping/flooring each regressor at the 1-percentile level would suffice to eliminate any danger here.\nNotice in particular how FICO begins to bend back after roughly 800. There is simply no appreciable level of defaults at that FICO level, and very little data of any form. Improvements to the fitting routines might eliminate the usage of Gaussians there, and greatly reduce the propensity for this sort of issue. A similar story holds for incentive.\nAgain, the important result here is not that these curves are dramatically better than other curves show by other mortgage researchers. The advantage of this approach is that the curves are\ngenerated with no human intervention. Using this as the starting point, it is easy to correct minor issues (e.g. cap and floor some regressors) and thus very quickly arrive at a very high quality model."}, {"heading": "8 Conclusion", "text": "The ITEM model was designed to automate repetitive tasks and allow for an efficient model to be built automatically. In the examples above, all fits were performed automatically, with no manual intervention beyond the initial definition of the regressors. In a more typical mortgage model, a modeler would typically spend months analyzing data, selecting regressors, and examining model residuals. Each iteration of this process would require several days, and many dead ends would be explored, largely due to inefficient regressor and parameter choices. Generally, another process would be required to remove insignificant parameters, which would then be followed up again by further rounds of fitting and analysis.\nThe ITEM model automates this entire cycle. It runs thousands of iterations of fit, extend, analyze residuals, and then fit again, all within the space of minutes rather than days. The model does not add insignificant parameters, so there are none to be removed, though some annealing may be needed. Similarly, ITEM does not choose suboptimal curves or regressors, so it explores fewer dead ends. Since the model is additive in logit space, projecting it onto any individual regressor gives a fair representation of its behavior, greatly easing the job of validation.\nThe workflowing using ITEM is to simply select the dataset, define the regressors, allow ITEM to fit a model, and then go straight to final model validation."}, {"heading": "9 Reference Implementation", "text": "Attached to this submission is a Java reference implementation of the ITEM model core. This reference implementation provides only the core modeling functionality. In order to run this model, the practitioner will need to define enumerations (see edu.columbia.tjw.item.base.StandardCurveType as an example) describing the statuses available to the modeled phenomenon, and also describing the regressors available. In addition, the practitioner will need to provide a class to produce grids of data implementing edu.columbia.tjw.item.ItemFittingGrid, and related interfaces.\nOnce these interfaces are implemented, the model may be fit using the ItemFitter. The resulting model is suitable for projection, but no general projection code is provided in this reference implementation. Similarly, at this time, the annealing code is not provided, but could be made available if there is interest.\nThis reference implementation is made available as a Maven package:\n\u2022 groupId: edu.columbia.tjw\n\u2022 artifactId: item\n\u2022 version: 1.0\nIf there is interest, this archive could be published to the Maven central repository."}, {"heading": "10 Future Work", "text": "Anticipated future work includes some results for annealing passes and the effect of noise introduction during fitting. In addition, the model could be applied to a larger set of regressors to show a more fully featured mortgage model. Applying the model to large regressor sets can reveal previously unknown behavior which may be worth research in its own right. The hybrid simulation approach could also be investigated more fully."}, {"heading": "I Appendix: Information-Theoretic Efficiency", "text": "This paper mentions information-theoretic efficiency in several places. This term can have several meaning, so this appendix will define the meaning for the purposes of this paper.\nThe first thing to note about modeling in general, is that it is closely related to the concept of relative entropy, i.e. entropy against a party that has access to some models or data. In fact, a model is little more than a compression function, with all the limitations that entails. For instance, suppose the datasets X and Y are available. If one wished to transmit this data to another person, it might be more efficient to first fit a model AXY (~x) = ~y + \u03b5, and then transmit X, AXY , and \u03b5. Ideally, the entropy of the model and the residual together would be smaller than the entropy of the response ~y. We know, however, that any compression algorithm that makes at least one dataset shorter must make at least one dataset longer. So it is known apriori that there are some datasets for which this model will give such bad predictions that it actually increases the entropy. Therefore, there can be no perfect model, as any model could be defeated by carefully chosen data.\nBecause of this, most of the approaches in this paper are heuristic, with some motivation from fields such as analysis but no complete proofs.\nThe relative entropy of a data set is simply its log likelihood (up to some constant factors), so a model that improves the log likelihood fo the dataset enough, would be worthwhile, and one that doesn\u2019t is not. Various information criteria (e.g. AIC, BIC) are based on a calculation of the total entropy contained in the parameter set \u03b8 and the residuals \u03b5, and comparing that value to what it would be for an alternate parameters set \u03b8\u2032 and \u03b5\u2032. The best model is then the one with the lowest AIC, it has extracted as much information as possible from the dataset without passing the point where additional model complexity is counter productive.\nWith this in mind, suppose that in the typical fashion, the phenomenon actually follows some well defined but unknown distribution, with some associated noise term. Assume that the noise is pure noise, in that no model can improve its entropy, taking into consideration the entropy of the model itself.\n~y = f(~x, ~w) + \u03b5 (29)\nHere, ~x is some observable data, but ~w is unobservable. This situation might be simplified by absorbing the unobservable information into the noise term \u03b5. The noise terms may no longer be i.i.d., but they should at least be mean zero.\n~y = f(~x) + \u03b5 (30)\nN.B. that the noise term is no longer pure noise, it is possible that a model could reduce it if there is correlation between ~w and ~x. This will result in a lower log likelihood, but will also result in a misattribution of some behavior away from ~w towards ~x. However, being aware of the dangers, suppose the model assumes that ~y follows some parameterized distribution g with some unknown parameters \u03b8.\n~y \u2248 g(~x|\u03b8) + \u03b5 (31)\nThe parameter (in practice, perhaps many parameters) \u03b8 is unknown, but it could be estimated, perhaps with maximum likelihood estimation. This would recover an estimator for \u03b8, namely \u03b8\u0302. Now, to state that a model is information theoretically efficient, we will in this paper mean the combination of three requirements, expressed in terms of the above quantities and also the sample size N .\n1. limN\u2192\u221e g(~x|\u03b8) = f(~x)\n2. limN\u2192\u221e \u03b8\u0302 = \u03b8\n3. The variance of \u03b8\u0302 reaches the Cramer-Rao bound asymptotically.\nBriefly, requirements (2) and (3) above are the standard requirements for an efficient estimator. The first condition states simply that the model g will converge to the actual function defining the phenomenon, given enough data, i.e. the model is not misspecified. For parameterized models, this is almost never true. Only in extremely rare circumstances would it be the case that the actual physical phenomenon being modeled converges to exactly the functional form chosen for the model. As the dataset becomes large, the majority of the residual error in the model would be due to this misspecification.\nIf the model is known to be misspecified and does not converge to the true distribution, then it may still be possible to define a sequence of models such that.\nlim n\u2192\u221e\ngn(~x|\u03b8n) = f(~x) (32)\nWhere here, the varable n is determining how many variables are included in the model. In other words, gn(~x|\u03b8n) is now a family of models with an unbounded number of parameters. In this case, information theoretic efficiency is expanded to include now four separate items. It is understood that all the following equations hold only as N \u2192\u221e.\n1. limn\u2192\u221e gn(~x|\u03b8n) = f(~x)\n2. |gn(~x|\u03b8n)\u2212 f(~x)| is minimized for all n\n3. \u03b8\u0302 = \u03b8\n4. The variance of \u03b8\u0302 reaches the Cramer-Rao bound asymptotically.\nWhere again, the first item is ideal but unlikely in practice. Similarly, the second item is an idealization, but can be taken to mean just that the model is parsimonious with parameters, getting meaningful improvement with each parameter added. The last two items are as before, measures of unbiased and asymptotically efficient estimation. A maximum likelihood estimation is (under most conditions) guaranteed to be both asymptotically efficient and unbiased, so the last two conditions will then be satisfied. Any parameterized model is also virtually guaranteed to not converge to exactly the actual phenomenon under study. What remains then is to consider the distance between the model and the phenomenon for any given number of parameters, and when information theoretic efficiency is discussed, this is typically what will be meant. For this notion of closeness, a typical metric would be Kullback-Leibler divergence though any equivalent metric would do.\nSince no model can be perfect due to the compression arguments above, there is no true solution to this problem and no ideal model. It is therefore necessary to from here proceed with heuristics for which there is some theoretical basis to believe that they may in many cases produce good models. One common assumption is that f(~x) is smooth and continuous with respect to ~x, depending on the model family, other assumptions may be needed as well."}], "references": [{"title": "Generalized Additive Models", "author": ["Hastie", "Tibshirani"], "venue": "Statistical Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1986}, {"title": "Sparse Multinomial Logistic Regression: Fast Algorithms and Generalization Bounds", "author": ["B. Krishnapuram et"], "venue": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE IN- TELLIGENCE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Regression Shrinkage and Selection via the Lasso Journal of the Royal Statistical Society", "author": ["R. Tibshirani"], "venue": "Series B Volume", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "2 Comparison to Generalized Additive Models ITEM may be considered to be a special case of a Generalized Additive Model [1], albeit one that is parametric.", "startOffset": 120, "endOffset": 123}], "year": 2014, "abstractText": "This document discusses the Information Theoretically Efficient Model (ITEM), a computerized system to generate an information theoretically efficient multinomial logistic regression from a general dataset. More specifically, this model is designed to succeed even where the logit transform of the dependent variable is not necessarily linear in the independent variables. This research shows that for large datasets, the resulting models can be produced on modern computers in a tractable amount of time. These models are also resistant to overfitting, and as such they tend to produce interpretable models with only a limited number of features, all of which are designed to be well behaved.", "creator": "LaTeX with hyperref package"}}}