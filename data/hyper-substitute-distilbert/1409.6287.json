{"id": "1409.6287", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2014", "title": "On tensor rank of conditional probability tables in Bayesian networks", "abstract": "defining difficult way in modeling with bayesian networks requires the concepts governing numerical parameters of bayesian networks. before specific number of parameters were guaranteed to specify a conditional ladder table ( \u03bc ) there is seemingly larger parent set. in introductory paper we show that, most cpts from software applications of bayesian neurons can actually find very confidently specified by those that require seemingly less parameters. elliptic function has via consequence not truly intuitive model elicitation but also rather efficient probabilistic calculations with these predictions.", "histories": [["v1", "Mon, 22 Sep 2014 19:32:15 GMT  (19kb,D)", "http://arxiv.org/abs/1409.6287v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ji\\v{r}\\'i vomlel", "petr tichavsk\\'y"], "accepted": false, "id": "1409.6287"}, "pdf": {"name": "1409.6287.pdf", "metadata": {"source": "CRF", "title": "On tensor rank of conditional probability tables in Bayesian networks", "authors": ["Ji\u0159\u0131\u0301 Vomlel", "Petr Tichavsk\u00fd"], "emails": ["vomlel@utia.cas.cz", "tichavsk@utia.cas.cz"], "sections": [{"heading": "1 INTRODUCTION", "text": "On the most cumbersome tasks in modeling with Bayesian networks is the elicitation of conditional probabilities. These probabilities can be estimated from collected data or elicited with the help of domain experts. A general conditional probability table (CPT), P (Xi|pa(Xi)), where Xi is a discrete random variable and pa(Xi) are its parent variables, requires ni = (|Xi| \u2212 1) \u00b7 \u220f Xj\u2208pa(Xi) |Xj | parameters. The cardinality |X| of a variable X is the number of values the variable X can take. Since ni is exponential with respect to number of parents the task of probability elicitation can be quite demanding for tables with a large parent set.\nTo ease the task of probability elicitation special types of CPTs were proposed. They include, so called, canonical models [2] whose typical examples are noisy-or, noisy-max and noisy-threshold, etc. For these special types of CPTs the number of parameters is often linear or quadratic with respect to the number of parents. Furthermore, it is possible to perform more efficient computations with them [5, 3, 7, 8].\nOn the other hand the vast majority of Bayesian networks used in real applications that are available in Bayesian network repositories uses only general CPTs. The main motivation of this paper is to show that actually many of these general tables have a hidden simple structure that requires a lower number of parameters than general CPTs. We will apply the CP-tensor decomposition [4, 1] for CPTs [7, 8] of models from a BN repository. We will see that many of the CPTS of these Bayesian networks can be approximated pretty well by tables with a low rank.\nThis observation has two practical consequences. First, more attention should be paid to discovering the internal structure of CPTs that are either estimated from data or elicited with the help of domain experts. Second, the internal structure of CPTs can be exploited for more efficient probabilistic inference with these networks.\n1 Institute of Information Theory and Automation of the AS CR, Prague, Czech Republic, email: vomlel@utia.cas.cz 2 Institute of Information Theory and Automation of the AS CR, Prague, Czech Republic, email: tichavsk@utia.cas.cz\nThe rest of this paper is organized as follows. In Section 2 we briefly introduce the CP tensor decomposition. The results of experiments with the state-of-the-art numerical algorithms for the CPtensor decomposition are reported in Section 3. We conclude the paper by a summary in Section 4."}, {"heading": "2 CP TENSOR DECOMPOSITION", "text": "Each CPT can be viewed as a tensor. A tensor is simply a mapping A : I \u2192 R, where I = I1 \u00d7 . . . \u00d7 Ik, k is a natural number called the order of tensor A, and Ij , j = 1, . . . , k are index sets. Typically, Ij are sets of integers of cardinality nj . Then we can say that tensor A has dimensions n1, . . . , nk. Tensor A has rank one if it can be written as an outer product of vectors:\nA = a1 \u2297 . . .\u2297 ak\nwith the outer product \u2297 being defined for all (i1, . . . , ik) \u2208 I1 \u00d7 . . . \u00d7 Ik as Ai1,...,ik = a1,i1 \u00b7 . . . \u00b7 ak,ik , where aj = (aj,i)i\u2208Ij , j = 1, . . . , k are real valued vectors.\nCP-tensor decomposition [4, 1] is decomposition of a tensor into a sum of r tensors of rank one. The minimum number r such that the decomposition is possible is called the rank of tensor A.\nIn [7] the CP tensor decomposition (called tensor rank-one decomposition there) was applied to CPTs of canonical models. See [7] or [8] for a more detailed explanation, which is out of scope of this paper. Instead, we will just illustrate savings one may get with the CP tensor decomposition of CPTs. Assume a CPT defined for a binary variable with k \u2212 1 binary parents. To fully specify such a table we need 2k\u22121 parameters. If this table can be approximated by a table with rank r we need k(r\u22121)+r parameters instead. If r = 2 then we need k+2 parameters. We can immediately see that instead of an exponential number of parameters we need just a linear number of them."}, {"heading": "3 NUMERICAL EXPERIMENTS", "text": "For the experiments we used 15 Bayesian networks from a Bayesian network repository3, namely:\nalarm.net, hailfinder.net, barley2.net, pathfinder.net, munin1.net, munin2.net, munin3.net, munin4.net, mildew.net, hepar2.net, andes.net, win95pts.net, water.net, link.net, and insurance.net.\nIn the experiments we considered all CPTs for variables having at least three parents. It was 492 CPTs in total. For the CP tensor de-\n3 We used networks from the repository at http://www.bnlearn.com/ bnrepository/, where networks from other repositories after a qualitycheck and fixing are stored.\nar X\niv :1\n40 9.\n62 87\nv1 [\ncs .A\nI] 2\n2 Se\np 20\n14\ncomposition we used the Fast Damped Gauss-Newton (LevenbergMarquard) algorithm [6] implemented in the Matlab package TENSORBOX 4. In the first experiment we used this algorithm with 100 different random starting points and one precomputed (using nvec method). In the second experiment we used 10+1 starting points.\nFirst, we present the results for an example of a 3 \u00d7 3 \u00d7 4 \u00d7 3 CPT from the hailfinder.net network. In this CPT the child variable Boundaries (3 states) has three parents OutflowFrMt (3 states), MorningBound (3 states), and WndHodograph (4 states). In Figure 1 we compare the maximum distance\nmax (i1,...,ik) |A\u0302i1,...,ik \u2212Ai1,...,ik |\nof the approximation A\u0302 to the original CPT A as a function of the rank of the approximation. We can see that the distance rapidly decreases with rank (full line), which is not the case of a table of the same dimensions with random values (dashed line).\nNext, we summarize results for all 492 tables from 15 different Bayesian networks. In Figure 2 we present the plot describing dependence of percentage of CPTs that can be approximated by tables of a given rank with maximum error (taken over all possible configurations of parents) smaller than 0.001. See the full line. From this figure we can see that about 40% of the CPTs can be pretty well approximated by tables of rank 2, about 75% by tables of rank 6, and only about 15% require rank more than 10 to get a very good fit.\nOne may conjecture that any CPT can be approximated well by a table with a similarly low rank as CPTs of networks from repositories. To show that this is not the case we repeated the computations for CPTs of the same dimensions but with their entries replaced by random numbers. See dashed line in Figure 2. We can see that it is not possible to approximate randomly created CPTs with similarly low rank as for CPTs of Bayesian networks from the repository."}, {"heading": "4 CONCLUSIONS", "text": "We presented numerical experiments on 15 Bayesian network models from real applications. The results suggest that most of the conditional probability tables can be very well approximated by tables that have lower rank than one would expect from a general table of the same dimensions. Our results are in line with results presented in [9] where the authors experimentally verified that noisy-max provides a surprisingly good fit for as many as 50% of CPTs in two of the three Bayesian networks they tested.\n4 Tensorbox is freely available at http://www.bsp.brain.riken. jp/\u02dcphan/tensorbox.php.\nThe low rank approximation should be exploited not only in the model elicitation by using a compact parametrization of the internal structure of a CPT but also during the probabilistic inference. We conjecture that some low rank approximations of CPTs may actually correspond better to what a domain expert intended to model in the constructed CPT. One reason might be that when filling numerical values into a CPT with more than two parents it is quite demanding for the model creator to provide exact numerical values. Another reason might be that numerical values in the CPTs are often provided with a lower numerical precision, typically with two decimal points."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by the Czech Science Foundation through projects No. 13\u201320012S and No. 14\u201313713S."}], "references": [{"title": "Analysis of individual differences in multidimensional scaling via an n-way generalization of Eckart-Young decomposition", "author": ["J.D. Carroll", "J.J. Chang"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1970}, {"title": "Canonical probabilistic models for knowledge engineering", "author": ["F.J. D\u0131\u0301ez", "M.J. Druzdzel"], "venue": "Technical Report CISIAD-06-01,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "An efficient factorization for the noisy MAX", "author": ["F.J. D\u0131\u0301ez", "S.F. Gal\u00e1n"], "venue": "International Journal of Intelligent Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Foundations of the PARAFAC procedure: Models and conditions for an \u201dexplanatory\u201d multi-mode factor analysis", "author": ["R.A. Harshman"], "venue": "UCLA Working Papers in Phonetics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1970}, {"title": "A MUNIN network for the median nerve \u2014 a case study on loops", "author": ["K.G. Olesen", "U. Kj\u00e6rulff", "F. Jensen", "F.V. Jensen", "B. Falck", "S. Andreassen", "S.K. Andersen"], "venue": "Applied Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "Low complexity damped Gauss-Newton algorithms for CANDECOMP/PARAFAC", "author": ["A.-H. Phan", "P. Tichavsk\u00fd", "A. Cichocki"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Exploiting tensor rank-one decomposition in probabilistic inference", "author": ["P. Savicky", "J. Vomlel"], "venue": "Kybernetika, 43(5),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Probabilistic inference with noisy-threshold models based on a CP tensor decomposition", "author": ["J. Vomlel", "P. Tichavsk\u00fd"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Knowledge engineering for Bayesian networks: How common are noisy-MAX distributions in practice?", "author": ["A. Zagorecki", "M.J. Druzdzel"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "They include, so called, canonical models [2] whose typical examples are noisy-or, noisy-max and noisy-threshold, etc.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "Furthermore, it is possible to perform more efficient computations with them [5, 3, 7, 8].", "startOffset": 77, "endOffset": 89}, {"referenceID": 2, "context": "Furthermore, it is possible to perform more efficient computations with them [5, 3, 7, 8].", "startOffset": 77, "endOffset": 89}, {"referenceID": 6, "context": "Furthermore, it is possible to perform more efficient computations with them [5, 3, 7, 8].", "startOffset": 77, "endOffset": 89}, {"referenceID": 7, "context": "Furthermore, it is possible to perform more efficient computations with them [5, 3, 7, 8].", "startOffset": 77, "endOffset": 89}, {"referenceID": 3, "context": "We will apply the CP-tensor decomposition [4, 1] for CPTs [7, 8] of models from a BN repository.", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "We will apply the CP-tensor decomposition [4, 1] for CPTs [7, 8] of models from a BN repository.", "startOffset": 42, "endOffset": 48}, {"referenceID": 6, "context": "We will apply the CP-tensor decomposition [4, 1] for CPTs [7, 8] of models from a BN repository.", "startOffset": 58, "endOffset": 64}, {"referenceID": 7, "context": "We will apply the CP-tensor decomposition [4, 1] for CPTs [7, 8] of models from a BN repository.", "startOffset": 58, "endOffset": 64}, {"referenceID": 3, "context": "CP-tensor decomposition [4, 1] is decomposition of a tensor into a sum of r tensors of rank one.", "startOffset": 24, "endOffset": 30}, {"referenceID": 0, "context": "CP-tensor decomposition [4, 1] is decomposition of a tensor into a sum of r tensors of rank one.", "startOffset": 24, "endOffset": 30}, {"referenceID": 6, "context": "In [7] the CP tensor decomposition (called tensor rank-one decomposition there) was applied to CPTs of canonical models.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "See [7] or [8] for a more detailed explanation, which is out of scope of this paper.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "See [7] or [8] for a more detailed explanation, which is out of scope of this paper.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "composition we used the Fast Damped Gauss-Newton (LevenbergMarquard) algorithm [6] implemented in the Matlab package TENSORBOX .", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "Our results are in line with results presented in [9] where the authors experimentally verified that noisy-max provides a surprisingly good fit for as many as 50% of CPTs in two of the three Bayesian networks they tested.", "startOffset": 50, "endOffset": 53}], "year": 2016, "abstractText": "A difficult task in modeling with Bayesian networks is the elicitation of numerical parameters of Bayesian networks. A large number of parameters is needed to specify a conditional probability table (CPT) that has a larger parent set. In this paper we show that, most CPTs from real applications of Bayesian networks can actually be very well approximated by tables that require substantially less parameters. This observation has practical consequence not only for model elicitation but also for efficient probabilistic reasoning with these networks.", "creator": "LaTeX with hyperref package"}}}