{"id": "1409.7963", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2014", "title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation", "abstract": "in this work, we propose both digital and advanced texture - articulated human pose cues in videos using extended convolutional network architecture, which incorporates both facial and perception features. we propose each new human features tracking dataset, flic - motion, incorporating mixes the rendering dataset plus additional motion symbols. we apply our architecture to this dataset making report adopting evolving standards implementing classic way - of - the - art forensic judgment systems.", "histories": [["v1", "Sun, 28 Sep 2014 21:32:15 GMT  (7934kb,D)", "http://arxiv.org/abs/1409.7963v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["arjun jain", "jonathan tompson", "yann lecun", "christoph bregler"], "accepted": false, "id": "1409.7963"}, "pdf": {"name": "1409.7963.pdf", "metadata": {"source": "CRF", "title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation", "authors": ["Arjun Jain", "Jonathan Tompson", "Yann LeCun", "Christoph Bregler"], "emails": ["bregler}@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Human body pose recognition in video is a long-standing problem in computer vision with a wide range of applications. However, body pose recognition remains a challenging problem due to the high dimensionality of the input data and the high variability of possible body poses. Traditionally, computer vision-based approaches tend to rely on appearance cues such as texture patches, edges, color histograms, foreground silhouettes or hand-crafted local features (such as histogram of gradients (HoG) [2]) rather than motion-based features. Alternatively, psychophysical experiments [3] have shown that motion is a powerful visual cue that alone can be used to extract high-level information, including articulated pose.\nPrevious work [4, 5] has reported that using motion features to aid pose inference has had little or no impact on performance. Simply adding high-order temporal connectivity to traditional models would most often lead to intractable inference. In this work we show that deep learning is able to successfully incorporate motion features and is able to out-perform existing state-of-the-art techniques. Further, we show that by using motion features alone our method outperforms [6, 7, 8] (see Fig 9(a) and (b)), which further strengthens our claim that information coded in motion features is valuable and should be used when available.\nThis paper makes the following contributions: \u2013 A system that successfully incorporates motion-features to enhance the per-\nformance of pose-detection \u2018in-the-wild\u2019 compared to existing techniques. \u2013 An efficient and tractable algorithm that achieves close to real-time frame\nrates, making our method suitable for wide variety of applications.\n1 This dataset can be downloaded from http://cs.nyu.edu/~ajain/accv2014/\nar X\niv :1\n40 9.\n79 63\nv1 [\ncs .C\nV ]\n2 8\nSe p\n20 14\n\u2013 A new dataset called FLIC-motion, which is the FLIC dataset [1] augmented with \u2018motion-features\u2019 for each of the 5003 images collected from Hollywood movies."}, {"heading": "2 Prior Work", "text": "Geometric Model Based Tracking: One of the earliest works on articulated tracking in video was Hogg [9] in 1983 using edge features and a simple cylinder based body model. Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16]. The models used in these systems were explicit 2D or 3D jointed geometric models. Most systems had to be hand-initialized (except [12]), and focused on incrementally updating pose parameters from one frame to the next. More complex examples come from the HumanEva dataset competitions [17] that use video or higher-resolution shape models such as SCAPE [18] and extensions. We refer the reader to [19] for a complete survey of this era. Most recently such techniques have been shown to create very high-resolution animations of detailed body and cloth deformations [20, 21, 22]. Our approach differs, since we are dealing with single view videos in unconstrained environments.\nStatistical Based Recognition: One of the earliest systems that used no explicit geometric model was reported by Freeman et al. in 1995 [23] using oriented angle histograms to recognize hand configurations. This was the precursor for the bag-of-features, SIFT [24], STIP [25], HoG, and Histogram of Flow (HoF) [26] approaches that boomed a decade later, most notably including the work by Dalal and Triggs in 2005 [27]. Different architectures have since been proposed, including \u201cshape-context\u201d edge-based histograms from the human body [28, 29] or just silhouette features [30]. Shakhnarovich et al. [31] learn a parameter sensitive hash function to perform example-based pose estimation. Many techniques have been proposed that extract, learn, or reason over entire body features, using a combination of local detectors and structural reasoning (see [32] for coarse tracking and [33] for person-dependent tracking).\nThough the idea of using \u201cPictorial Structures\u201d by Fischler and Elschlager [34] has been around since the 1970s, matching them efficiently to images has only been possible since the famous work on \u2018Deformable Part Models\u2019 (DPM) by Felzenszwalb et al. [35] in 2008. Many algorithms that use DPM for creating the body part unary distribution [36, 6, 7, 37] with spatial-models incorporating body-part relationship priors have since then been developed. Johnson and Everingham [38], who also proposed the \u2018Leeds Sports Database\u2019, employ a cascade of body part detectors to obtain more discriminative templates. Almost all best performing algorithms since have solely built on HoG and DPM for local evidence, and yet more sophisticated spatial models. Pishchulin [39] proposes a model that augments the DPM unaries with Poselet conditioned [40] priors. Sapp and Taskar [1] propose a model where they cluster images in the posespace and then find the mode which best describes the input image. The pose\nof this mode then acts as a strong spatial prior, whereas the local evidence is again based on HoG and gradient features. Following the Poselets approach [40], the Armlets approach by Gkioxari et al. [41] incorporates edges, contours, and color histograms in addition to the HoG features. They employ a semi-global classifier for part configuration and show good performance on real-world data. However, they only show their results on arms. The major drawback of all these approaches is that both the local evidence and the global structure is hand crafted, whereas we jointly learn both the local features and the global structure using a multi-resolution convolutional network.\nShotton et al. [42] use an ensemble of random trees to perform per-pixel labeling of body parts in depth images. As a means of reducing overall system latency and avoiding repeated false detections, their work focuses on pose inference using only a single depth image. By contrast, we extend the single frame requirement to at least 2 frames (which we show considerably improves pose inference), and our input domain is unconstrained RGB images rather than depth.\nPose Detection Using Image Sequences:\nDeep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48]. [49, 50, 51] also apply neural networks for pose recognition, specifically Toshev et al. [49] show better than state-of-the-art performance on the \u2018FLIC\u2019 and \u2018LSP\u2019 [52] datasets. In contrast to Toshev et al., in our work we propose a translation invariant model which improves upon their method, especially in the high-precision region."}, {"heading": "3 Body-Part Detection Model", "text": "We propose a Convolutional Network (ConvNet) architecture for the task of estimating the 2D location of human joints in video (section 3.2). The input to the network is an RGB image and a set of motion features. We investigate a wide variety of motion feature formulations (section 3.1). Finally, we will also introduce a simple Spatial-Model to solve a specific sub-problem associated with evaluation of our model on the FLIC-motion dataset (section 3.3)."}, {"heading": "3.1 Motion Features", "text": "The aim of this section is to incorporate features that are representative of the true motion-field (the perspective projection of the 3D velocity-field of moving surfaces) as input to our detection network so that it can exploit motion as a cue for body part localization. To this end, we evaluate and analyze four motion features which fall under two broad categories: those using simple derivatives of the RGB video frames and those using optical flow features. For each RGB image pair fi and fi+\u03b4, we propose the following features:\n\u2013 RGB image pair - {fi, fi+\u03b4}\n\u2013 RGB image and an RGB difference image - {fi, fi+\u03b4 \u2212 fi} \u2013 Optical-flow2 vectors - {fi,FLOW(fi, fi+\u03b4)} \u2013 Optical-flow magnitude - {fi, ||FLOW(fi, fi+\u03b4)||2}\nThe RGB image pair is by far the simplest way of incorporating the relative motion information between the two frames. However, this representation clearly suffers from a lot of redundancy (i.e. if there is no camera movement) and is extremely high dimensional. Furthermore, it is not obvious to the deep network what changes in this high dimensional input space are relevant temporal information and what changes are due to noise or camera motion. A simple modification to this representation is to use a difference image, which reformulates the RGB input so that the algorithm sees directly the pixel locations where high energy corresponds to motion (alternatively the network would have to do this implicitly on the image pair). A more sophisticated representation is optical-flow, which is considered to be a high-quality approximation of the true motion-field. Implicitly learning to infer optical-flow from the raw RGB input would be nontrivial for the network to estimate, so we perform optical-flow calculation as a pre-processing step (at the cost of greater computational complexity).\nFLIC-motion dataset: We propose a new dataset which we call FLIC-motion3. It is comprised of the original FLIC dataset of 5003 labeled RGB images collected from 30 Hollywood movies, of which 1016 images are held out as a test set, augmented with the aforementioned motion features.\nWe experimented with different values for \u03b4 and investigated the above features with and without camera motion compensation; we use a simple 2D projective motion model between fi and fi+\u03b4, and warp fi+\u03b4 onto fi using the inverse of this best fitting projection to approximately remove camera motion. A comparison between image pairs with and without warping can be seen in Fig 1.\n2 We use the algorithm proposed by Weinzaepfel et al. [48] to compute optical-flow. 3 This dataset can be downloaded from http://cs.nyu.edu/~ajain/accv2014/\nTo obtain fi+\u03b4, we must know where the frames fi occur in each movie. Unfortunately, this was non-trivial as the authors Sapp et al. [1] could not provide us with the exact version of the movie that was used for creating the original dataset. Corresponding frames can be very different in multiple versions of the same movie (4:3 vs wide-screen, director\u2019s cut, special editions, etc.). We estimate the best similarity transform S between fi and each frame f m j from the movie m, and if the distance |fi\u2212Sfmj | is below a certain threshold (10 pixels), we conclude that we found the correct frame. We visually confirm the resulting matches and manually pick frames for which the automatic matching was unsuccessful (e.g. when enough feature points were not found)."}, {"heading": "3.2 Convolutional Network", "text": "Recent work [49, 50] has shown ConvNet architectures are well suited for the task of human body pose detection, and due to the availability of modern Graphics Processing Units (GPUs), we can perform Forward Propagation (FPROP) of deep ConvNet architectures at interactive frame-rates. Similarly, we realize our detection model as a deep ConvNet architecture. The input is a 3D tensor containing an RGB image and its corresponding motion features, and the output is a 3D tensor containing response-maps, with one response-map for each joint. Each response-map describes the per-pixel energy for the presence of the corresponding joint at that pixel location.\nOur ConvNet is based on a sliding-window architecture. A simplified version of this architecture is shown in Fig 2. The input patches are first normalized using Local Contrast Normalization (LCN [53]) for the RGB channels and a new normalization method for the motion features we call Local Motion Normalization (LMN). We formulate LMN as the local subtraction with the response from a Gaussian kernel with large standard deviation followed by a divisive normalization. The result is that it removes some unwanted background camera motion as\nwell as normalizing the local intensity of motion (which helps improve network generalization for motions of varying velocity but with similar pose). Prior to processing through the convolution stages, the normalized motion channels are concatenated along the feature dimension with the normalized RGB channels, and the resulting tensor is processed though 3 stages of convolution.\nThe first two convolution stages use rectified linear units (ReLU) and Maxpooling, and the last stage incorporates a single ReLU layer. The output of the last convolution stage is then passed to a three stage fully-connected neuralnetwork. The network is then applied to all 64 \u00d7 64 sub-windows of the image, stepped every 4 pixels horizontally and vertically. This produces a dense response-map output, one for each joint. The major advantage of this model is that the learned detector is translation invariant by construction.\nBecause the layers are convolutional, applying two instances of the network in Fig 2 to two overlapping input windows leads to a considerable amount of redundant computation. Recent work [54, 55] eliminates this redundancy and thus yields a dramatic speed up. This is achieved by applying each layer of the convolutional network to the entire input image. The fully connected layers for each window are also replicated for all sub-windows of the input. This formulation allows us to back-propagate though this network for all windows simultaneously. Due to the two 2 \u00d7 2 subsampling layers, we obtain one output vector every 4\u00d7 4 input pixels. An equivalent efficient version of the sliding window model is shown in Fig 3.\nNote that an alternative model (such as in Tompson et al. [51]) would replace the last 3 convolutional layers with a fully-connected neural network whose input context is the feature activations for the entire input image. Such a model would be appropriate if we knew a priori that there existed a strong correlation between skeletal pose and the position of the person in the input frame since this alternative model is not invariant with respect to the translation of the person within the image. However, the FLIC dataset has no such strong pose-location bias (i.e. a subject\u2019s torso is not always in the same location in the image), and therefore a sliding-window based architecture is more appropriate for our task.\nWe extend the single resolution ConvNet architecture of Fig 3 by incorporating a multi-resolution input. We do so by down-sampling the input (using appropriate anti-aliasing), and then each resolution image is processed through either\na LCN or LMN layer using the same normalization kernels for each bank producing an approximate Laplacian pyramid. The role of the Laplacian Pyramid is to provide each bank with non-overlapping spectral content which minimizes network redundancy. Our final, multi-resolution network is shown in Fig 4. The outputs of the convolution banks are concatenated (along the feature dimension) by point-wise up-scaling of the lower resolution bank to bring the feature maps into canonical resolution. Note that in our final implementation we use 3 resolution banks.\nWe train the Part-Detector network using supervised learning via Back Propagation and Stochastic Gradient Descent. We minimize a mean squared error criterion for the distance between the inferred response-map activation and a ground truth response-map, which is a 2D Gaussian distribution centered at the target joint location and with small standard deviation (1px). We use Nesterov momentum to reduce training time [56] and we randomly perturb the input images each epoch by randomly flipping and scaling the images to prevent network overtraining and improve generalization performance."}, {"heading": "3.3 Simple Spatial Model", "text": "Our model is evaluated on our new FLIC-motion dataset (section 3.1). As per the original FLIC dataset, the test images in FLIC-motion may contain multiple people, however, only a single actor per frame is labeled in the test set. As such, a rough torso location of the labeled person is provided at test time to help locate the \u201ccorrect\u201d person. We incorporate this information by means of a simple and efficient Spatial-Model.\nThe inclusion of this stage has two major advantages. Firstly, the correct feature activation from the Part-Detector output is selected for the person for whom a ground-truth label was annotated. An example of this is shown in Fig 5. Secondly, since the joint locations of each part are constrained in proximity to the single ground-truth torso location, then (indirectly) the connectivity between joints is also constrained, enforcing that inferred poses are anatomically viable\n(i.e. the elbow joint and the shoulder joint cannot be to far away from the torso, which in turn enforces spatial locality between the elbow and shoulder joints).\nThe core of our Spatial-Model is an empirically calculated joint-mask, shown in Fig 5(b). The joint-mask layer describes the possible joint locations, given that the supplied torso position is in the center of the mask. To create a mask layer for body part A, we first calculate the empirical histogram of the part A location, xA, relative to the torso position xT for the training set examples; i.e. xhist = xA\u2212xT . We then turn this histogram into a Boolean mask by setting the mask amplitude to 1 for pixels for which p (xhist) > 0. Finally, we blur the mask using a wide Gaussian low-pass filter which accounts for body part locations not represented in the training set (but which might be present in the test set).\nDuring test time, this joint-mask is shifted to the ground-truth torso location and the per-pixel energy from the Part-Model (section 3.2) is then multiplied with the mask to produce a filtered output. This process is carried out for each body part independently.\nIt should be noted that while this Spatial-Model does enforce some anatomic consistency, it does have limitations. Notably, we expect it to fail for datasets where the range of poses is not as constrained as the FLIC dataset (which is primarily front facing and standing up poses)."}, {"heading": "4 Results", "text": "Training time for our model on the FLIC-motion dataset (3957 training set images, 1016 test set images) is approximately 12 hours, and FPROP of a single image takes approximately 50ms4. For our models that use optical flow as a motion feature input, the most expensive part of our pipeline is the optical flow calculation, which takes approximately 1.89s per image pair. (We plan to investigate real-time flow estimations in the future).\n4 Analysis of our system was on a 12 core workstation with an NVIDIA Titan GPU\nSection 4.1 compares the performance of the motion features from section 3.1. Section 4.2 compares our architecture with other techniques and shows that our system significantly outperforms existing state-of-the-art techniques. Note that for all experiments in Section 4.1 we use a smaller model with 16 convolutional features in the first 3 layers. A model with 128 instead of 16 features for the first 3 convolutional layers is used for results in Section 4.2."}, {"heading": "4.1 Comparison and Analysis of Proposed Motion Features", "text": "Fig 6 shows a selection of example images from the FLIC test set which highlights the importance of using motion features for body pose detection. In Fig 6(a), the elbow position is occluded by the actor\u2019s sling, and no such examples exist in the training set; however, the presence of body motion provides a strong cue for elbow location. Figs 6(b) and (d) have extremely cluttered backgrounds and the correct joint location is locally similar to the surrounding region (especially for the camouflaged clothing in Fig 6(d)). For these images, motion features are essential in correct joint localization. Finally, Fig 6(c) is an example where motion blur (due to fast joint motion) reduces the fidelity of RGB edge features, which results in incorrect localization when motion features are not used.\nFigs 7(a) and (b) show the performance of the motion features of section 3.1 on the FLIC-motion dataset for the Elbow and Wrist joints respectively. For evaluating our test-set performance, we use the criterion proposed by Sapp et al. [1]. We count the percentage of the test-set images where joint predictions are within a given radius that is normalized to a 100 pixel torso size. Surprisingly, even the simple frame-difference temporal feature improves upon the baseline\nresult (which we define as a single RGB frame input) and even outperforms the 2D optical flow input (see 6(b) inset).\nNote that stable and accurate calculation of optical-flow from arbitrary RGB videos is a very challenging problem. Therefore, incorporating motion flow features as input to the network adds non-trivial localization cues that would be very difficult for the network to learn internally with limited learning capacity. Therefore, it is expected that the best performing networks in Fig 7 are those that incorporate motion flow features. However, it is surprising that using the magnitude of the flow vectors performs as well as - and in some cases outperforms - the full 2D motion flow. Even though the input data is richer, we hypothesize that when using 2D flow vectors the network must learn invariance to the direction of joint movement; for instance, the network should predict the same head position whether a person is turning his/her head to the left or right on the next frame. On the other hand, when the L2 magnitude of the flow vector is used, the network sees the high velocity motion cue but cannot over-train to the direction of the movement.\nFig 8(a) shows that the performance of our network is relatively agnostic to the frame separation (\u03b4) between the samples for which we calculate motion flow; the average precision between 0 and 20 pixel radii degrades 3.9% from -10 pixels offset to -1 pixel offset. A frame difference of 10 corresponds to approximately 0.42sec (at 24fps), and so we expect that large motions over this time period would result in complex non-linear trajectories in input space for which a single finite difference approximation of the pixel velocity would be inaccurate. Accordingly, our results show that performance indeed degrades as a larger frame step is used.\nSimilarly, we were surprised that our camera motion compensation technique (described in section 3.1) does not help to the extent that we expected,\nas shown in Fig 8(b). Likely this is because either LMN removes a lot of constant background motion or the network is able to learn to ignore the remaining foreground-background parallax motion due to camera movement."}, {"heading": "4.2 Comparison with Other Techniques", "text": "Fig 9(a) and 9(b) compares the performance of our system with other stateof-the-art models on the FLIC dataset for the elbow and wrist joints respectively. Our detector is able to significantly outperform all prior techniques on this challenging dataset. Note that using only motion features already outperforms [6, 7, 8]. Also note that using only motion features is less accurate than using a combination of motion features and RGB images, especially in the high accuracy region. This is because fine details such as eyes and noses are missing in motion features. Toshev et al. [49] suffers from inaccuracy in the high-precision region, which we attribute to inefficient direct regression of pose vectors from images. MODEC [1], Eichner et al. [6] and Sapp et al. [8] build on hand crafted HoG features. They all suffer from the limitations of HoG (i.e. they all discard color information, etc). Jain et al. [50] do not use multi-scale information and evaluate their model in a sliding window fashion, whereas we use the \u2018one-shot\u2019 approach. Finally, we believe that increasing the complexity of our simple spatial model will improve performance of our model, specifically for large radii."}, {"heading": "5 Conclusion", "text": "We have shown that when incorporating both RGB and motion features in our deep ConvNet architecture, our network is able to outperform existing state-ofthe-art techniques for the task of human body pose detection in video. We have\nalso shown that using motion features alone can outperform some traditional algorithms [6, 7, 8]. Our findings suggest that even very simple temporal cues can greatly improve performance with a very minor increase in model complexity. As such, we suggest that future work should place more emphasis on the correct use of motion features. We would also like to further explore higher level temporal features, potentially via learned spatiotemporal convolution stages and we hope that using a more expressive temporal-spatial model (using motion constraints) will help improve performance significantly."}, {"heading": "6 Acknowledgments", "text": "The authors would like to thank Tyler Zhu for his help with the data-set creation. This research was funded in part by the Office of Naval Research ONR Award N000141210327."}], "references": [{"title": "Modec: Multimodal decomposable models for human pose estimation", "author": ["B. Sapp", "B. Taskar"], "venue": "CVPR.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Visual perception of biological motion and a model for its analysis", "author": ["G. Johansson"], "venue": "Perception and Psychophysics", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1973}, {"title": "Progressive search space reduction for human pose estimation", "author": ["V. Ferrari", "M. Marin-Jimenez", "A. Zisserman"], "venue": "CVPR.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Sidestepping intractable inference with structured ensemble cascades", "author": ["D. Weiss", "B. Sapp", "B. Taskar"], "venue": "NIPS.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Better appearance models for pictorial structures", "author": ["M. Eichner", "V. Ferrari"], "venue": "BMVC.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Articulated pose estimation with flexible mixtures-ofparts", "author": ["Y. Yang", "D. Ramanan"], "venue": "CVPR.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Cascaded models for articulated pose estimation", "author": ["B. Sapp", "A. Toshev", "B. Taskar"], "venue": "ECCV.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Model-based vision: a program to see a walking person", "author": ["D. Hogg"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1983}, {"title": "Model-based tracking of self-occluding articulated objects", "author": ["J.M. Rehg", "T. Kanade"], "venue": "Computer Vision.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Model-based estimation of 3d human motion with occlusion based on active multi-viewpoint selection", "author": ["I.A. Kakadiaris", "D. Metaxas"], "venue": "CVPR.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Tracking people with twists and exponential maps", "author": ["C. Bregler", "J. Malik"], "venue": "CVPR.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Articulated body motion capture by annealed particle filtering", "author": ["J. Deutscher", "A. Blake", "I. Reid"], "venue": "CVPR.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Stochastic tracking of 3d human figures using 2d image motion", "author": ["H. Sidenbladh", "M.J. Black", "D.J. Fleet"], "venue": "ECCV.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Covariance scaled sampling for monocular 3d body tracking", "author": ["C. Sminchisescu", "B. Triggs"], "venue": "CVPR.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion", "author": ["L. Sigal", "A. Balan", "B.M. J"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Scape: shape completion and animation of people", "author": ["D. Anguelov", "P. Srinivasan", "D. Koller", "S. Thrun", "J. Rodgers", "J. Davis"], "venue": "TOG.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Vision-based human motion analysis: An overview", "author": ["R. Poppe"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Performance capture from sparse multi-view video", "author": ["E. De Aguiar", "C. Stoll", "C. Theobalt", "N. Ahmed", "H.P. Seidel", "S. Thrun"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Moviereshape: Tracking and reshaping of humans in videos", "author": ["A. Jain", "T. Thorm\u00e4hlen", "H.P. Seidel", "C. Theobalt"], "venue": "TOG.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast articulated motion tracking using a sums of gaussians body model", "author": ["C. Stoll", "N. Hasler", "J. Gall", "H. Seidel", "C. Theobalt"], "venue": "ICCV.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Orientation histograms for hand gesture recognition", "author": ["W.T. Freeman", "M. Roth"], "venue": "International Workshop on Automatic Face and Gesture Recognition.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1995}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "On space-time interest points", "author": ["I. Laptev"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Human detection using oriented histograms of flow and appearance", "author": ["N. Dalal", "B. Triggs", "C. Schmid"], "venue": "ECCV.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimating human body configurations using shape context matching", "author": ["G. Mori", "J. Malik"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "Recovering 3D human pose from monocular images", "author": ["A. Agarwal", "B. Triggs", "I. Rhone-Alpes", "F. Montbonnot"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Inferring 3d structure with a statistical image-based shape model", "author": ["K. Grauman", "G. Shakhnarovich", "T. Darrell"], "venue": "ICCV.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Fast pose estimation with parametersensitive hashing", "author": ["G. Shakhnarovich", "P. Viola", "T. Darrell"], "venue": "ICCV.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Strike a pose: Tracking people by finding stylized poses", "author": ["D. Ramanan", "D. Forsyth", "A. Zisserman"], "venue": "CVPR.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning sign language by watching TV (using weakly aligned subtitles)", "author": ["P. Buehler", "A. Zisserman", "M. Everingham"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "The representation and matching of pictorial structures", "author": ["M.A. Fischler", "R. Elschlager"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1973}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["P. Felzenszwalb", "D. McAllester", "D. Ramanan"], "venue": "CVPR.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Pictorial structures revisited: People detection and articulated pose estimation", "author": ["M. Andriluka", "S. Roth", "B. Schiele"], "venue": "CVPR.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Human pose estimation using body parts dependent joint regressors", "author": ["M. Dantone", "J. Gall", "C. Leistner", "L.V. Gool."], "venue": "CVPR.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning Effective Human Pose Estimation from Inaccurate Annotation", "author": ["S. Johnson", "M. Everingham"], "venue": "CVPR.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Poselet conditioned pictorial structures", "author": ["L. Pishchulin", "M. Andriluka", "P. Gehler", "B. Schiele"], "venue": "CVPR.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Poselets: Body part detectors trained using 3d human pose annotations", "author": ["L. Bourdev", "J. Malik"], "venue": "ICCV.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Articulated pose estimation using discriminative armlet classifiers", "author": ["G. Gkioxari", "P. Arbelaez", "L. Bourdev", "J. Malik"], "venue": "CVPR.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "T. Sharp", "A. Kipman", "A. Fitzgibbon", "M. Finocchio", "A. Blake", "M. Cook", "R. Moore"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M. Zeiler", "F.R."], "venue": "arXiv preprint arXiv:1311.2901.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Yaniv Taigman", "M.R. Ming Yang", "L. Wolf"], "venue": "CVPR.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion", "author": ["L. Deng", "O. Abdel-Hamid", "D. Yu"], "venue": "ICASSP.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Pedestrian detection with unsupervised multi-stage feature learning", "author": ["P. Sermanet", "K. Kavukcuoglu", "S. Chintala", "Y. LeCun"], "venue": "CVPR.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Deepflow: Large displacement optical flow with deep matching", "author": ["P. Weinzaepfel", "J. Revaud", "Z. Harchaoui", "C. Schmid"], "venue": "ICCV.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "CVPR.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning human pose estimation features with convolutional networks", "author": ["A. Jain", "J. Tompson", "M. Andriluka", "G. Taylor", "C. Bregler"], "venue": "ICLR.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time continuous pose recovery of human hands using convolutional networks", "author": ["J. Tompson", "M. Stein", "Y. LeCun", "K. Perlin"], "venue": "TOG.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustered pose and nonlinear appearance models for human pose estimation", "author": ["S. Johnson", "M. Everingham"], "venue": "BMVC.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast image scanning with deep max-pooling convolutional neural networks", "author": ["A. Giusti", "D.C. Ciresan", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "CoRR.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "ICLR.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "ICML.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "We propose a new human body pose dataset, FLIC-motion, that extends the FLIC dataset [1] with additional motion features.", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "Traditionally, computer vision-based approaches tend to rely on appearance cues such as texture patches, edges, color histograms, foreground silhouettes or hand-crafted local features (such as histogram of gradients (HoG) [2]) rather than motion-based features.", "startOffset": 222, "endOffset": 225}, {"referenceID": 2, "context": "Alternatively, psychophysical experiments [3] have shown that motion is a powerful visual cue that alone can be used to extract high-level information, including articulated pose.", "startOffset": 42, "endOffset": 45}, {"referenceID": 3, "context": "Previous work [4, 5] has reported that using motion features to aid pose inference has had little or no impact on performance.", "startOffset": 14, "endOffset": 20}, {"referenceID": 4, "context": "Previous work [4, 5] has reported that using motion features to aid pose inference has had little or no impact on performance.", "startOffset": 14, "endOffset": 20}, {"referenceID": 5, "context": "Further, we show that by using motion features alone our method outperforms [6, 7, 8] (see Fig 9(a) and (b)), which further strengthens our claim that information coded in motion features is valuable and should be used when available.", "startOffset": 76, "endOffset": 85}, {"referenceID": 6, "context": "Further, we show that by using motion features alone our method outperforms [6, 7, 8] (see Fig 9(a) and (b)), which further strengthens our claim that information coded in motion features is valuable and should be used when available.", "startOffset": 76, "endOffset": 85}, {"referenceID": 7, "context": "Further, we show that by using motion features alone our method outperforms [6, 7, 8] (see Fig 9(a) and (b)), which further strengthens our claim that information coded in motion features is valuable and should be used when available.", "startOffset": 76, "endOffset": 85}, {"referenceID": 0, "context": "\u2013 A new dataset called FLIC-motion, which is the FLIC dataset [1] augmented with \u2018motion-features\u2019 for each of the 5003 images collected from Hollywood movies.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "Geometric Model Based Tracking: One of the earliest works on articulated tracking in video was Hogg [9] in 1983 using edge features and a simple cylinder based body model.", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 10, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 11, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 12, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 13, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 14, "context": "Several other model based articulated tracking systems have been reported over the past two decades, most notably [10, 11, 12, 13, 14, 15, 16].", "startOffset": 114, "endOffset": 142}, {"referenceID": 15, "context": "More complex examples come from the HumanEva dataset competitions [17] that use video or higher-resolution shape models such as SCAPE [18] and extensions.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "More complex examples come from the HumanEva dataset competitions [17] that use video or higher-resolution shape models such as SCAPE [18] and extensions.", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "We refer the reader to [19] for a complete survey of this era.", "startOffset": 23, "endOffset": 27}, {"referenceID": 18, "context": "Most recently such techniques have been shown to create very high-resolution animations of detailed body and cloth deformations [20, 21, 22].", "startOffset": 128, "endOffset": 140}, {"referenceID": 19, "context": "Most recently such techniques have been shown to create very high-resolution animations of detailed body and cloth deformations [20, 21, 22].", "startOffset": 128, "endOffset": 140}, {"referenceID": 20, "context": "Most recently such techniques have been shown to create very high-resolution animations of detailed body and cloth deformations [20, 21, 22].", "startOffset": 128, "endOffset": 140}, {"referenceID": 21, "context": "in 1995 [23] using oriented angle histograms to recognize hand configurations.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "This was the precursor for the bag-of-features, SIFT [24], STIP [25], HoG, and Histogram of Flow (HoF) [26] approaches that boomed a decade later, most notably including the work by Dalal and Triggs in 2005 [27].", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "This was the precursor for the bag-of-features, SIFT [24], STIP [25], HoG, and Histogram of Flow (HoF) [26] approaches that boomed a decade later, most notably including the work by Dalal and Triggs in 2005 [27].", "startOffset": 64, "endOffset": 68}, {"referenceID": 24, "context": "This was the precursor for the bag-of-features, SIFT [24], STIP [25], HoG, and Histogram of Flow (HoF) [26] approaches that boomed a decade later, most notably including the work by Dalal and Triggs in 2005 [27].", "startOffset": 103, "endOffset": 107}, {"referenceID": 25, "context": "This was the precursor for the bag-of-features, SIFT [24], STIP [25], HoG, and Histogram of Flow (HoF) [26] approaches that boomed a decade later, most notably including the work by Dalal and Triggs in 2005 [27].", "startOffset": 207, "endOffset": 211}, {"referenceID": 26, "context": "Different architectures have since been proposed, including \u201cshape-context\u201d edge-based histograms from the human body [28, 29] or just silhouette features [30].", "startOffset": 118, "endOffset": 126}, {"referenceID": 27, "context": "Different architectures have since been proposed, including \u201cshape-context\u201d edge-based histograms from the human body [28, 29] or just silhouette features [30].", "startOffset": 118, "endOffset": 126}, {"referenceID": 28, "context": "Different architectures have since been proposed, including \u201cshape-context\u201d edge-based histograms from the human body [28, 29] or just silhouette features [30].", "startOffset": 155, "endOffset": 159}, {"referenceID": 29, "context": "[31] learn a parameter sensitive hash function to perform example-based pose estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Many techniques have been proposed that extract, learn, or reason over entire body features, using a combination of local detectors and structural reasoning (see [32] for coarse tracking and [33] for person-dependent tracking).", "startOffset": 162, "endOffset": 166}, {"referenceID": 31, "context": "Many techniques have been proposed that extract, learn, or reason over entire body features, using a combination of local detectors and structural reasoning (see [32] for coarse tracking and [33] for person-dependent tracking).", "startOffset": 191, "endOffset": 195}, {"referenceID": 32, "context": "Though the idea of using \u201cPictorial Structures\u201d by Fischler and Elschlager [34] has been around since the 1970s, matching them efficiently to images has only been possible since the famous work on \u2018Deformable Part Models\u2019 (DPM) by Felzenszwalb et al.", "startOffset": 75, "endOffset": 79}, {"referenceID": 33, "context": "[35] in 2008.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Many algorithms that use DPM for creating the body part unary distribution [36, 6, 7, 37] with spatial-models incorporating body-part relationship priors have since then been developed.", "startOffset": 75, "endOffset": 89}, {"referenceID": 5, "context": "Many algorithms that use DPM for creating the body part unary distribution [36, 6, 7, 37] with spatial-models incorporating body-part relationship priors have since then been developed.", "startOffset": 75, "endOffset": 89}, {"referenceID": 6, "context": "Many algorithms that use DPM for creating the body part unary distribution [36, 6, 7, 37] with spatial-models incorporating body-part relationship priors have since then been developed.", "startOffset": 75, "endOffset": 89}, {"referenceID": 35, "context": "Many algorithms that use DPM for creating the body part unary distribution [36, 6, 7, 37] with spatial-models incorporating body-part relationship priors have since then been developed.", "startOffset": 75, "endOffset": 89}, {"referenceID": 36, "context": "Johnson and Everingham [38], who also proposed the \u2018Leeds Sports Database\u2019, employ a cascade of body part detectors to obtain more discriminative templates.", "startOffset": 23, "endOffset": 27}, {"referenceID": 37, "context": "Pishchulin [39] proposes a model that augments the DPM unaries with Poselet conditioned [40] priors.", "startOffset": 11, "endOffset": 15}, {"referenceID": 38, "context": "Pishchulin [39] proposes a model that augments the DPM unaries with Poselet conditioned [40] priors.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "Sapp and Taskar [1] propose a model where they cluster images in the posespace and then find the mode which best describes the input image.", "startOffset": 16, "endOffset": 19}, {"referenceID": 38, "context": "Following the Poselets approach [40], the Armlets approach by Gkioxari et al.", "startOffset": 32, "endOffset": 36}, {"referenceID": 39, "context": "[41] incorporates edges, contours, and color histograms in addition to the HoG features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[42] use an ensemble of random trees to perform per-pixel labeling of body parts in depth images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 42, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 43, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 44, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 45, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 46, "context": "Deep Learning based Techniques: Recently, state-of-the-art performance has been reported on many vision tasks using deep learning algorithms [43, 44, 45, 46, 47, 48].", "startOffset": 141, "endOffset": 165}, {"referenceID": 47, "context": "[49, 50, 51] also apply neural networks for pose recognition, specifically Toshev et al.", "startOffset": 0, "endOffset": 12}, {"referenceID": 48, "context": "[49, 50, 51] also apply neural networks for pose recognition, specifically Toshev et al.", "startOffset": 0, "endOffset": 12}, {"referenceID": 49, "context": "[49, 50, 51] also apply neural networks for pose recognition, specifically Toshev et al.", "startOffset": 0, "endOffset": 12}, {"referenceID": 47, "context": "[49] show better than state-of-the-art performance on the \u2018FLIC\u2019 and \u2018LSP\u2019 [52] datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[49] show better than state-of-the-art performance on the \u2018FLIC\u2019 and \u2018LSP\u2019 [52] datasets.", "startOffset": 75, "endOffset": 79}, {"referenceID": 46, "context": "[48] to compute optical-flow.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] could not provide us with the exact version of the movie that was used for creating the original dataset.", "startOffset": 0, "endOffset": 3}, {"referenceID": 47, "context": "Recent work [49, 50] has shown ConvNet architectures are well suited for the task of human body pose detection, and due to the availability of modern Graphics Processing Units (GPUs), we can perform Forward Propagation (FPROP) of deep ConvNet architectures at interactive frame-rates.", "startOffset": 12, "endOffset": 20}, {"referenceID": 48, "context": "Recent work [49, 50] has shown ConvNet architectures are well suited for the task of human body pose detection, and due to the availability of modern Graphics Processing Units (GPUs), we can perform Forward Propagation (FPROP) of deep ConvNet architectures at interactive frame-rates.", "startOffset": 12, "endOffset": 20}, {"referenceID": 51, "context": "The input patches are first normalized using Local Contrast Normalization (LCN [53]) for the RGB channels and a new normalization method for the motion features we call Local Motion Normalization (LMN).", "startOffset": 79, "endOffset": 83}, {"referenceID": 52, "context": "Recent work [54, 55] eliminates this redundancy and thus yields a dramatic speed up.", "startOffset": 12, "endOffset": 20}, {"referenceID": 53, "context": "Recent work [54, 55] eliminates this redundancy and thus yields a dramatic speed up.", "startOffset": 12, "endOffset": 20}, {"referenceID": 49, "context": "[51]) would replace the last 3 convolutional layers with a fully-connected neural network whose input context is the feature activations for the entire input image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "We use Nesterov momentum to reduce training time [56] and we randomly perturb the input images each epoch by randomly flipping and scaling the images to prevent network overtraining and improve generalization performance.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Note that using only motion features already outperforms [6, 7, 8].", "startOffset": 57, "endOffset": 66}, {"referenceID": 6, "context": "Note that using only motion features already outperforms [6, 7, 8].", "startOffset": 57, "endOffset": 66}, {"referenceID": 7, "context": "Note that using only motion features already outperforms [6, 7, 8].", "startOffset": 57, "endOffset": 66}, {"referenceID": 47, "context": "[49] suffers from inaccuracy in the high-precision region, which we attribute to inefficient direct regression of pose vectors from images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "MODEC [1], Eichner et al.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "[6] and Sapp et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] build on hand crafted HoG features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 48, "context": "[50] do not use multi-scale information and evaluate their model in a sliding window fashion, whereas we use the \u2018one-shot\u2019 approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[49], Jain et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[50], MODEC [1], Eichner et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[50], MODEC [1], Eichner et al.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "[6], Yang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] and Sapp et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "also shown that using motion features alone can outperform some traditional algorithms [6, 7, 8].", "startOffset": 87, "endOffset": 96}, {"referenceID": 6, "context": "also shown that using motion features alone can outperform some traditional algorithms [6, 7, 8].", "startOffset": 87, "endOffset": 96}, {"referenceID": 7, "context": "also shown that using motion features alone can outperform some traditional algorithms [6, 7, 8].", "startOffset": 87, "endOffset": 96}], "year": 2014, "abstractText": "In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion, that extends the FLIC dataset [1] with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems.", "creator": "LaTeX with hyperref package"}}}