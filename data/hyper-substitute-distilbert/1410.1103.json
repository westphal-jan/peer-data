{"id": "1410.1103", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2014", "title": "Online Ranking with Top-1 Feedback", "abstract": "we consider difficulty setting where a system learns to avoid a fixed set along preference items. the goal would produce dynamic good ranking for users with diverse capabilities since interact thus suitable criteria for performance. creating an online fashion. we begin choosing novel top - 1 feedback model using this problem : considering most end of horizontal competition, the average score for choosing the top ranked components never revealed concerning previous contestants. fundamentally, the performance of retrieval system is determined on losing entire retrieval apparatus. we view a comprehensive database of constraints regarding learnability under this challenging view. for popular ranking measures that quantum win and dcg, help prove therefore the minimax regret of polynomial course t ^ { 2 / you }. luckily, the minimax cost is decided using an efficient algorithmic strategy that primarily spends o ( s \u00f7 4 ) time per implementation. the same algorithmic strategy : o ( m2 ^ { 2 / v } ) search for search @ recovery. surprisingly, we show bias for normalized versions under these ranking measures, namely auc, ndcg and map, thus online management algorithm can have sub - efficient regret.", "histories": [["v1", "Sun, 5 Oct 2014 00:51:59 GMT  (180kb,D)", "http://arxiv.org/abs/1410.1103v1", null], ["v2", "Tue, 4 Aug 2015 18:10:14 GMT  (864kb,D)", "http://arxiv.org/abs/1410.1103v2", "The previous version is being replaced by the conference version, which appeared in 18th International Conference on Artificial Intelligence and Statistics (AISTATS 2015). in Volume 38 of JMLR Workshop and Conference Proceedings (2015)"], ["v3", "Sun, 6 Mar 2016 20:42:02 GMT  (121kb,D)", "http://arxiv.org/abs/1410.1103v3", "Previous version being replaced by conference version. Appeared in AISTATS 2015"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sougata chaudhuri", "ambuj tewari"], "accepted": false, "id": "1410.1103"}, "pdf": {"name": "1410.1103.pdf", "metadata": {"source": "CRF", "title": "Online Ranking with Top-1 Feedback", "authors": ["Sougata Chaudhuri", "Ambuj Tewari"], "emails": ["sougata@umich.edu", "tewaria@umich.edu", "Precision@k."], "sections": [{"heading": "1 Introduction", "text": "Consider a system that is learning to rank a fixed set of objects for presentation to users, when different users have varied preference for the objects. Learning occurs in an online setting: at each round, the system outputs a ranked list of the objects and the quality of ranking is measured by one of several popular ranking measures (like DCG [12] or MAP [3]), taking into account the users preferences encoded as relevance vectors. We work in a game-theoretic setting and do not make any stochastic assumptions on how the relevance vectors are generated. Thus, it is assumed that the relevance vectors are generated by an oblivious adversary. The objective of the learner is to have a sub-linear (in the number of rounds) regret against the best ranking in hindsight. The idea of ranking for diverse preferences has been motivated from a branch of work, sometimes called \u201cranking with diversity\u201d [18, 17, 1] . In \u201cranking with diversity\u201d work in literature, the focus has been on learning an optimal ranking of a fixed set of objects where the loss in a round is simply 0\u2212 1, depending on whether the user finds any relevant object in top-k (of m) ranked items.\nIn addition to considering more practical losses associated with ranking like DCG and MAP, we consider a novel and challenging feedback model in this paper: the learner only gets to see the relevance of the object placed at the top (rank 1), whereas the ranking performance measure, and hence the regret, depends on the full relevance vector. We highlight two practical scenarios motivating the feedback model.\nEconomic Reason: A company wants to produce a ranked order of a fixed set of products related to a query. Different products are likely to have varying relevance to different users, depending on user characteristics such as age, gender etc. In principle, a user can browse through the\nar X\niv :1\n41 0.\n11 03\nv1 [\ncs .L\nG ]\n5 O\nct 2\n01 4\nentire list giving carefully considered ratings, say on a 5 point scale, to each product. In practice, however, it is quite likely that user will scan through all the products and have a rough idea about how relevant each product is to her. But she will likely be reluctant to give a thorough feedback on each product, unless the company provides some economic incentives to do so. Though the company needs high quality feedback on each product, to keep refining the ranking strategy, it cannot afford to give incentives due to budget constraints. Hence, they require the user to give feedback only on top placed product. This allows the user to look at all products but does not burden her with task of providing feedback beyond the top-ranked product. In this scenario, a full relevance vector is implicit in the user\u2019s mind but the system (company) gets to see the relevance of only the top placed product.\nPrivacy Reason: A medical company wants to advertise multiple new drugs for a particular medical condition. Not all drugs are suitable for everyone since the effects of the drugs vary depending on the user attributes like age, gender etc. To satisfy most users, the company wants to produce a useful ordering of the drugs, but users are unlikely to give feedback on the usefulness (relevance) of every drug due to privacy concerns. So the company can ask for feedback about just the top ranked drug while, as above, every drug has an implicit relevance score for each user. However, the system will only get to see the relevance of the top ranked drug.\nTheoretically, the top-1 feedback model is neither full-feedback nor bandit-feedback since not even the loss (quantified by some ranking measure) at each round is revealed to the learner. The appropriate framework to study the problem is that of partial monitoring [7]. A very recent paper shows another practical application of the idea where feedback is neither full information nor bandit [14]. Recent advances in the classification of partial monitoring games tell us that the minimax regret, in an adversarial setting, is governed by a property of the loss and feedback functions called observability [4, 10]. Observability is of two kinds: local and global. We instantiate these general observability notions for the top-1 feedback case and prove that, for some ranking measures, namely PairwiseLoss [9], DCG and Precision@k [15], global observability holds. This immediately shows that the upper bound on regret scales as O(T 2/3) (ignoring log T factors). Specifically for PairwiseLoss and DCG, we further prove that local observability fails, which shows that their minimax regret scales as \u0398(T 2/3). However, the generic algorithm that enjoys O(T 2/3(log T )1/3) regret for globally observable games maintains an explicit distribution over learner actions. For us, the action set is the exponentially large set of m! rankings over m objects. We therefore provide an efficient algorithm that exploits the structure of rankings. It runs in time O(m logm) per step and achieves a slightly improved (over generic algorithm) O(T 2/3) regret bound for PairwiseLoss, DCG and Precision@k.\nFor several measures, their normalized versions are considered. For example, the normalized versions of PairwiseLoss, DCG and Precision@k are called AUC [8], NDCG [11] and MAP respectively. We show an unexpected result for the normalized versions: they do not admit sub-linear regret algorithms under top-1 feedback. This is despite the fact that the opposite is true for their unnormalized counterparts! Intuitively, the normalization makes it hard to construct an unbiased estimator of the (unobserved) relevance vector. Surprisingly, we are able to translate this intuitive hurdle into a provable impossibility. Finally, we present some preliminary experiments to explore the performance of our efficient algorithm and compare its regret to its full information counterpart."}, {"heading": "2 Notation and Preliminaries", "text": "We have a fixed set of m objects numbered {1, 2, . . . ,m}. A permutation \u03c3 gives a mapping from objects to their ranks and its inverse \u03c3\u22121 gives a mapping from ranks back to objects. Thus, \u03c3(i) = j means object i is placed at position j while \u03c3\u22121(i) = j means object j is placed at position i. For a binary relevance vector r \u2208 {0, 1}m, r(i) indicates relevance level of object i. We denote {1, . . . , n} by [n]. The learner can choose from m! actions (permutations) whereas nature/adversary can choose from 2m outcomes (relevance vectors, when relevance levels are restricted to binary). The learner\u2019s (resp. adversary\u2019s) actions are denoted by {\u03c3i : i \u2208 [m!]} (resp. {ri : i \u2208 [2n]}). We sometimes interchangeably denote action \u03c3i as i (resp. ri as i). With this convention, \u03c3(i) is a number but \u03c3i is a permutation. Also, a vector can be row or column vector depending on context. At round t, the learner outputs a permutation (ranking) \u03c3t of the objects (possibly using some internal randomization, based on feedback history so far). The user looks at the list and generates a relevance vector rt which indicates relevance level of each object. The quality of \u03c3t is judged against rt by a ranking loss RL. Crucially, only the relevance of the top ranked object (i.e., rt(\u03c3 \u22121 t (1))) is revealed to the learner at end of round t. Thus, the learner gets to know neither rt (full information problem) nor RL(\u03c3t, rt) (bandit problem). The objective of the learner is to minimize the expected regret with respect to best permutation in hindsight (when RL is a gain, not loss, we need to negate the quantity below):\nE\u03c31,...,\u03c3T [ T\u2211 t=1 RL(\u03c3t, rt) ] \u2212min \u03c3 T\u2211 t=1 RL(\u03c3, rt). (1)\nWe consider binary relevance but many of our techniques should easily extend to multi-graded relevance provided the performance measure has the right form.\nPrecisely define minimax regret here: What is the precise definition? What is the precise technical formula?"}, {"heading": "3 Ranking Measures", "text": "We consider ranking measures which can be expressed in the form f(\u03c3) \u00b7 r, where the function f : Rm \u2192 Rm is composed of m copies of a univariate, monotonic, scalar valued function. Thus, f(\u03c3) = [fs(\u03c3(1)), fs(\u03c3(2)), . . . , f s(\u03c3(m))], where fs : R \u2192 R. Monotonic (increasing) means fs(\u03c3(i)) \u2265 fs(\u03c3(j)), whenever \u03c3(i) > \u03c3(j), \u2200 i, j. Monotonic (decreasing) is defined similarly. The following popular ranking measures can be expressed in the form f(\u03c3) \u00b7 r.\nPairwiseLoss & SumLoss: PairwiseLoss is defined as: PL(\u03c3, r) = \u2211m\ni=1 \u2211m j=1 1(\u03c3(i) <\n\u03c3(j))1(r(i) < r(j)). PairwiseLoss cannot be directly expressed in the form of f(\u03c3) \u00b7 r. Instead, we consider SumLoss, defined as: SumLoss(\u03c3, r) = \u2211m i=1 \u03c3(i) r(i). SumLoss has the form f(\u03c3) \u00b7 r, where f(\u03c3) = \u03c3. It has been shown in [2] that SumLoss differs from PairwiseLoss only by an r-dependent constant and hence the regret under the two measures are equal:\nT\u2211 t=1 PL(\u03c3t, rt)\u2212 T\u2211 t=1 PL(\u03c3, rt) =\nT\u2211 t=1 SumLoss(\u03c3t, rt)\u2212 T\u2211 t=1 SumLoss(\u03c3, rt).\n(2)\nDiscounted Cumulative Gain: DCG, which admits non-binary relevance vectors, is defined as: DCG(\u03c3, r) = \u2211m\ni=1 2r(i)\u22121\nlog2(1+\u03c3(i)) and becomes\n\u2211m i=1 r(i) log2(1+\u03c3(i))\nfor r(i) \u2208 {0, 1}. Thus, for binary relevance, DCG(\u03c3, r) has the form f(\u03c3) \u00b7 r, where f(\u03c3) = [ 1log2(1+\u03c3(1)) , 1 log2(1+\u03c3(2)) , . . . , 1log2(1+\u03c3(m)) ].\nPrecision@k Gain: Precision@k is defined as Prec@k(\u03c3, r) = \u2211m\ni=1 1(\u03c3(i) \u2264 k) r(i). Precision@k can be written as f(\u03c3) \u00b7 r where f(\u03c3) = [1(\u03c3(1) < k), . . . ,1(\u03c3(m) < k)]. Our focus is on k \u2265 2, since for k = 1, top-1 feedback is actually the same as full information feedback, for which efficient algorithms exist.\nNormalized measures are not of the form f(\u03c3) \u00b7 r: PairwiseLoss, DCG and Precion@k are unnormalized versions of popular ranking measures, namely, Area Under Curve (AUC), Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP) respectively. None of the latter can be expressed in the form f(\u03c3) \u00b7 r.\nNDCG: NDCG(\u03c3, r) = 1Z(r) \u2211m i=1 2r(i)\u22121 log2(1+\u03c3(i)) and becomes 1Z(r) \u2211m i=1 r(i) log2(1+\u03c3(i))\nfor r(i) \u2208 {0, 1}. Here Z(r) = max\n\u03c3\n\u2211m i=1 2r(i)\u22121 log2(1+\u03c3(i)) is the normalizing factor. It can be clearly seen that\nNDCG(\u03c3, r) = f(\u03c3) \u00b7 g(r), where f(\u03c3) is same as DCG but g(r) = rZ(r) is non-linear in r. MAP: MAP is a gain function and is defined as: MAP (\u03c3, r) = 1\u2016r\u20161 \u2211m i=1 \u2211m j=1 1(r(i) < r(j))1(\u03c3(i) < \u03c3(j)). It can be clearly seen that MAP cannot be expressed in the form f(\u03c3) \u00b7 r. MAP: AUC is a loss function and is defined as: AUC(\u03c3, r) = 1N(r) \u2211m i=1 \u2211m j=1 1(\u03c3(i) <\n\u03c3(j))1(r(i) < r(j)), where N(r) = ( \u2211m i=1 1(r(i) = 1)) \u2217 (m \u2212 \u2211m\ni=1 1(r(i) = 1)). It can be clearly seen that AUC cannot be expressed in the form f(\u03c3) \u00b7 r.\nAll subsequent results will be for binary valued relevance vectors, unless stated otherwise."}, {"heading": "4 Summary of Results", "text": "With notations fixed, we now summarize our main results here before delving into technical details. The regret bounds are over time horizon T , with learner playing against an oblivious adversary. Unless otherwise stated, all proofs and extensions are given in the appendix.\nResult 1: The minimax regret under DCG and PairwiseLoss (and hence SumLoss) is \u0398(T 2/3). Result 2: An efficient algorithm, with running time O(m logm) per step, achieves the minimax regret under DCG and PairwiseLoss and also has a regret of O(T 2/3) for Precision@k. The precise minimax regret under Precision@k, k \u2265 2, remains an open issue.\nResult 3: The minimax regret for any of the normalized versions \u2013 NDCG, MAP and AUC \u2013 is \u0398(T ). Thus, there is no algorithm that achieves sublinear regret for the normalized measures.\nResult 4: The minimax regret, both for DCG and NDCG, does not change (i.e., remains \u0398(T 2/3) and \u0398(T ) respectively) when we consider non-binary, multi-graded relevance vectors.\nWe re-emphasize that the relevance vectors are generated adversarially, without any stochastic assumption. Had the relevance vectors been stochastic in nature, the results would have held; however, efficient algorithm already exists in stochastic case [5] ."}, {"heading": "5 Relevant Definitions from Partial Monitoring", "text": "We develop all results in context of SumLoss. We then extend the results to other ranking measures. Our main results on regret bounds build on some of the theory for abstract partial monitoring games\ndeveloped in [4, 10]. For ease of understanding, we reproduce the relevant notations and definitions in context of SumLoss.\nLoss and Feedback Matrices: The online learning game with the SumLoss measure and feedback being relevance of top ranked object, can be expressed in form of a pair of loss matrix and feedback matrix. The loss matrix L is an m!\u00d72m dimensional matrix, with rows indicating the learner\u2019s actions (permutations) and columns representing adversary\u2019s actions (relevance vectors). The entry in cell (i, j) of L indicates loss suffered when learner plays action i (i.e., \u03c3i) and adversary plays action j (i.e., rj), that is, Li,j = \u03c3i \u00b7 rj = \u2211m k=1 \u03c3i(k)rj(k). The feedback matrix H has same dimension as loss matrix, with entry in cell {i, j} being the relevance of top ranked object, i.e., Hi,j = rj(\u03c3 \u22121 i (1)). When the learner plays action \u03c3i and adversary plays action rj , the true loss is Li,j , while the feedback received is Hi,j . Table 1 and 2 illustrate the matrices, with number of objects m = 3. In both the tables, the permutations indicate rank of each object and relevance vector indicates relevance of each object. For example, \u03c35 = 312 means object 1 is ranked 3, object 2 is ranked 1 and object 3 is ranked 2. r5 = 100 means object 1 has relevance level 1 and other two objects have relevance level 0. Also, L3,4 = \u03c33 \u00b7 r4 = \u22113 i=1 \u03c3(i)r(i) = 2 \u00b70 + 1 \u00b71 + 3 \u00b71 = 4; H3,4 = r4(\u03c3 \u22121 3 (1)) = r4(2) = 1. Other entries are computed similarly. Let `i \u2208 R2 m denote row i of L. Let \u2206 be the probability simplex in R2 m , i.e., \u2206 = {p \u2208 R2m :\n\u2200 1 \u2264 i \u2264 2m, pi \u2265 0, \u2211 pi = 1}. The following definitions, given for abstract problems in [4], has been refined to fit our problem context.\nDefinition 1: Pareto-optimality Learner action i is called optimal under distribution p \u2208 \u2206, if `i \u00b7p \u2264 `j \u00b7p, for all other learner actions 1 \u2264 j \u2264 m!, j 6= i. For every action i \u2208 [m!], probability cell of i is defined as Ci = {p \u2208 \u2206 : action i is optimal under p}. If a non-empty cell Ci is 2m \u2212 1 dimensional, then associated action i is called Pareto-optimal.\nNote that since entries in H are relevance levels of objects, there can be maximum of 2 distinct elements in each row of H, i.e., 0 or 1 (Assuming binary relevance).\nDefinition 2: Signal-matrices Signal matrix Si, associated with learner\u2019s action \u03c3i, is a matrix with 2 rows and 2m columns, with each entry 0 or 1, i.e., Si \u2208 {0, 1}2\u00d72 m . The entries of ` th column of row 1 and 2 of Si are respectively: (Si)1,` = I(Hi,` = 0) and (Si)2,` = I(Hi,` = 1), where I is the indicator function and H is the feedback matrix.\nNote that by definitions of signal and feedback matrices, the 2nd row of Si (or 2nd column of S>i )) is precisely the ith row of H. The 1st row of Si (or 1st column of S > i )) is the (boolean) complement of ith row of H."}, {"heading": "6 Minimax Regret for SumLoss", "text": "The minimax regret of SumLoss will be established by showing that: a) SumLoss satisfies global observability, and b) it does not satisfy local observability."}, {"heading": "6.1 Global Observability", "text": "Definition 3: The condition of global observability holds, w.r.t. loss matrix L and feedback matrix H, if for every pair of learner\u2019s actions {\u03c3i, \u03c3j}, it is true that `i\u2212 `j \u2208 \u2295k\u2208[m!]Col(S>k ) (where Col refers to column space).\nThe global observability condition basically states that the (vector) loss difference between any pair of learner\u2019s actions has to belong to the vector space spanned by columns of (transposed) signal matrices corresponding to all possible learner\u2019s actions. We derive the following theorem on global observability for SumLoss (Proof in appendix).\nTheorem 1. The global observability condition, as per Definition 3., holds w.r.t. loss matrix L and feedback matrix H defined for SumLoss, for any m \u2265 1.\nProof. For some \u03c3a (learner\u2019s action) and rb (adversary\u2019s action), we have\nLa,b = \u03c3a \u00b7 rb = m\u2211 i=1 \u03c3a(i)rb(i) 1 = m\u2211 j=1 j rb(\u03c3 \u22121 a (j)) 2 =\nm\u2211 j=1 j rb(\u03c3\u0303 \u22121 j(a)(1)) 3 = m\u2211 j=1 j (S>\u03c3\u0303j(a))rb,2.\nThus, we have `a =\n[La,1, La,2, . . . , La,2m ] = [L\u03c3a,r1 , L\u03c3a,r2 , . . . , L\u03c3a,r2m ] = [ m\u2211 j=1 j (S>\u03c3\u0303j(a))r1,2, m\u2211 j=1 j (S>\u03c3\u0303j(a))r2,2, .., m\u2211 j=1 j (S>\u03c3\u0303j(a))r2m ,2]\n4 = m\u2211 j=1 j (S>\u03c3\u0303j(a)):,2\nEquality 4 shows that `a is in the column span of m of the m! possible (transposed) signal matrices, specifically in the span of the 2nd columns of those (transposed) m matrices. Hence, for all actions \u03c3a, it is true that `a \u2208 \u2295k\u2208[m!]Col(S>k ), =\u21d2 la\u2212 lb \u2208 \u2295k\u2208[m!]Col(S>k ), \u2200 actions \u03c3a, \u03c3b.\n1: Equality 1 holds by the relation \u03c3a(i) = j \u21d2 i = \u03c3\u22121a (j). 2. Equality 2 holds because permutations are bijections. Thus, for a permutation \u03c3a and for every j \u2208 [m], \u2203 a permutation \u03c3\u0303j(a), s.t. the object which is assigned rank j by \u03c3a is the same object assigned rank 1 by \u03c3\u0303j(a), i.e, \u03c3 \u22121 a (j) = \u03c3\u0303 \u22121 j(a)(1).\n3. In equality 3, (S>\u03c3\u0303j(a))rb,2 indicates the rb th row and 2nd column of (transposed) signal matrix S\u03c3\u0303j(a) , corresponding to learner action \u03c3\u0303j(a). Equality 3 holds because rb(\u03c3\u0303 \u22121 j(a)(1)) is the entry in the row corresponding to action \u03c3\u0303j(a) and column corresponding to action rb of H and from Definition 2.\n4. Equality 4 holds from the observation that for a particular j, [(S>\u03c3\u0303j(a))r1,2, (S > \u03c3\u0303j(a) )r2,2, . . . , (S > \u03c3\u0303j(a) )r2m ,2]\nforms the 2nd column of (S>\u03c3\u0303j(a)), i.e, (S > \u03c3\u0303j(a) ):,2."}, {"heading": "6.2 Local Observability", "text": "Definition 4: Two Pareto-optimal (learner\u2019s) actions i and j are called neighboring actions if Ci\u2229 Cj is a (2\nm\u22122) dimensional polytope (where Ci is probability cell of action \u03c3i). The neighborhood action set of two neighboring (learner\u2019s) actions i and j is defined asN+i,j = {k \u2208 [m!] : Ci\u2229Cj \u2286 Ck}.\nDefinition 5: A pair of neighboring (learner\u2019s) actions i and j is said to be locally observable if `i \u2212 `j \u2208 \u2295k\u2208N+i,jCol(S > k ). The condition of local observability holds if every pair of neighboring (learner\u2019s) actions is locally observable. We now show that local observability condition fails for L,H under SumLoss. First, we propose the following lemmas characterizing Pareto-optimal actions and neighboring actions for SumLoss.\nLemma 2. For SumLoss, each learner\u2019s action i is Pareto-optimal, where Pareto-optimality has been defined in Definition 1.\nLemma 3. A pair of learner\u2019s actions {\u03c3i, \u03c3j} is a neighboring pair, if there is exactly one pair of objects, numbered {a, b}, whose positions differ in \u03c3i and \u03c3j. Moreover, a needs to be placed just before b in \u03c3i and b needs to placed just before a in \u03c3j.\nLemma 2 and 3 lead to following result.\nTheorem 4. The local observability condition, as per Definition 5, fails w.r.t. loss matrix L and feedback matrix H defined for SumLoss, already at m = 3."}, {"heading": "6.3 Minimax Regret Bound", "text": "We establish the minimax regret for SumLoss by combining results on global and local observability. First, we get a lower bound by combining our Theorem 4 with Theorem 4 in [4].\nCorollary 5. There exists an online game for SumLoss with top-1 feedback, such that for every online algorithm, there is an adversary strategy generating relevance vectors, that guarantees the following\nE [ T\u2211 t=1 SumLoss(\u03c3t, rt) ] \u2212min \u03c3 T\u2211 t=1 SumLoss(\u03c3, rt)\n= \u2126(T 2/3)\n(3)\nwhere the expectation is taken w.r.t. randomized learner\u2019s actions.\nAn immediate corollary of Theorem 1 and Theorem 3.1 in [7] gives an in-efficient algorithm (inspired by the algorithm originally given in [16]) obtaining O(T 2/3(log T )1/3) regret.\nCorollary 6. The algorithm in Figure 1 of [7] achieves O(T 2/3(log T )1/3) regret bound for SumLoss.\nThe results above establish that the minimax regret for SumLoss, under top-1 feedback model, is \u0398\u0303(T 2/3) where \u0398\u0303 hides log(T ) factors. However, the algorithm in [7] is intractable in our setting since the number of learner\u2019s actions is exponential in number of objects m. The next section tackles the efficiency issue."}, {"heading": "7 Efficient Algorithm for Obtaining Minimax Regret under Sum-", "text": "Loss\nWe provide an efficient algorithm for getting an O(poly(m)T 2/3) regret bound for SumLoss. The per round running time of the algorithm is O(m logm).\nThe key idea that we use in our algorithm is to divide time into phases. Within each phase, we allot a small number of rounds for pure exploration (this lets us estimate the average relevance vector for that phase). Using the average vector as a full information vector for next phase, rest of the rounds in next phase follow the actions according to the distribution suggested by Follow the Perturbed Leader (FTPL) [13] (this is exploitation of previous experience). One of the key reasons for exploiting FTPL in our algorithm, instead of exponential weighing schemes, is that the structure of our problem allows distribution to be maintained implicitly over m objects, instead of explicitly maintaining distribution over m! arms, a prohibitively expensive step.\nOur algorithm is motivated by the reduction from bandit-feedback to full feedback given in [6]. However, the algorithm in [6] cannot be directly applied to our problem, because we are not in the bandit setting and hence do not know loss of any action. Further, the algorithm spends N rounds per phase to try out each of the N available actions \u2013 this is infeasible in our setting since N = m!.\nOur main theorem on regret of Algorithm 1 is as follows.\nAlgorithm 1 RankingwithTop-1Feedback (RTop-1F)\nT = Time horizon, K = No. of (equal sized) blocks, Time horizon divided into blocks {B1, . . . , BK}, randomization parameter . Here, Bi = {(i\u2212 1)(T/K) + 1, (i\u2212 1)(T/K) + 2, . . . , i(T/K)}. Initialize s\u03020 = 0, r\u03020 = 0. For i = 1, . . . ,K\nUpdate s\u0302i = s\u0302i\u22121 + r\u0302i\u22121. Select m time points {i1, . . . , im} from block Bi, uniformly at random, without replacement. For t \u2208 Bi\nIf t = ij \u2208 {i1, . . . , im} Output any permutation \u03c3t which places jth object on top. Receive feedback on the jth object as rij (j). Else Sample pt \u2208 [0, 1/ ]m. (Product of uniform distribution in each dimension). Output permutation \u03c3t = M(s\u0302i + pt) where M(y) = argmin\n\u03c3 \u03c3 \u00b7 y.\nend for Set r\u0302i = [ri1(1), . . . , rim(m)].\nend for\nTheorem 7. The expected regret of SumLoss, obtained by applying Algorithm 1, with K = m\u22121/3T 2/3 and = \u221a\n1 mK , and the expectation being taken over random learner\u2019s actions \u03c3t, is\nE [ T\u2211 t=1 SumLoss(\u03c3t, rt) ] \u2264min \u03c3 T\u2211 t=1 SumLoss(\u03c3t, rt)+\nO(m8/3T 2/3).\n(4)\nThe following simple but useful lemma is required to prove Theorem 7.\nLemma 8. Let the average of full relevance vectors over the time period {1, 2, . . . , t} be denoted as ravg1:t , that is, r avg 1:t = \u2211t k=1\nrk t\n. Let {i1, i2, . . . , im} be m arbitrary time points, chosen uniformly at random, without replacement, from {1, . . . , t}. At time point ij, only the jth component of vector rij , i.e rij (j), becomes known, \u2200j \u2208 {1, . . . ,m}. Then the relevance vector r\u0302t = [ri1(1), . . . , rim(m)] is an unbiased estimator of ravg1:t ."}, {"heading": "8 Regret Bounds for PairwiseLoss, DCG and Prec@k", "text": "As shown in Eq. 2, the regret of SumLoss is same as regret of PairwiseLoss. Thus, SumLoss in Cor. 5 and Thm. 7 can be replaced by PairwiseLoss to get exactly same results on regret.\nAll the results of SumLoss can be extended to DCG. Moreover, the results can be extended even for discrete, non-binary relevance vectors. Thus, the minimax regret of DCG, when the\nadversary can take any discrete valued, non-negative relevance vector is \u0398(T 2/3), which can be achieved by the efficient algorithm of Sec. 7 . The main differences between SumLoss and DCG are the following. The former is a loss function, the latter is a gain function. Also, f(\u03c3) 6= \u03c3 in DCG (Def. in Sec.2 ) and when r \u2208 {0, 1, . . . , n}m, DCG cannot be expressed as f(\u03c3) \u00b7 r, as is clear from definition in Sec. 3 . Nevertheless, DCG can be expressed as f(\u03c3) \u00b7 g(r), , where g(r) = [gs(r(1)), gs(r(2)), . . . , gs(r(m))], gs(i) = 2i \u2212 1 is constructed from univariate, monotonic, scalar valued functions. Thus, there are minor differences in definitions and proofs of theorems for SumLoss and DCG. The structural properties of f(\u00b7), g(\u00b7) are key in extending results. For binary valued relevance vectors, Algorithm1 can be applied to DCG as is. For multi-graded relevance vector, the only thing that changes is that the relevance feedback is transformed via component functions of g(\u00b7).\nWe provide the extension of Theorem 7 for DCG. Let relevance vectors chosen by adversary be of level n + 1, i.e., r \u2208 {0, 1, . . . , n}m. In practice, n is always less than 5; hence exp(n) is also small.\nTheorem 9. The expected regret of DCG, obtained by applying Algorithm 1 , with K = m\u22121/3T 2/3 and = \u221a\n1 (2n\u22121)2mK , and the expectation being taken over random learner\u2019s actions \u03c3t, is\nE [ T\u2211 t=1 DCG(\u03c3t, rt) ] \u2265max \u03c3 T\u2211 t=1 DCG(\u03c3, rt)\u2212\nO((2n \u2212 1)m5/3T 2/3)\n(5)\nIn case of binary relevance vector, the regret term is O(m5/3T 2/3). Moreover, since local observability fails, there is a matching \u2126(T 2/3) lower bound.\nThe regret upper bounds we proved for SumLoss also easily extend to Precision@k. We have the following extension of Theorem 7.\nTheorem 10. The expected regret of Prec@k, obtained by applying algorithm 1, with K = m\u22121/3T 2/3 and = \u221a\n1 mK , and the expectation being taken over random learner\u2019s actions \u03c3t, is\nE [ T\u2211 t=1 Prec@k(\u03c3t, rt) ] \u2265max \u03c3 T\u2211 t=1 Prec@k(\u03c3, rt)\u2212\nO(km2/3T 2/3)\n(6)\nHowever, Prec@k is independent of the order of objects in top k positions of ranked list. This changes the neighboring action claims. Therefore, the minimax regret of Prec@k remains an open question, since the local observability results do not directly apply to Prec@k."}, {"heading": "9 Non-Existence of Sublinear Regret Bounds for NDCG, MAP", "text": "and AUC\nAs stated in Sec. 3 , NDCG, MAP and AUC are normalized versions of measures DCG, Precision@k and PairwiseLoss. We have the following lemma for all these normalized ranking measures.\nLemma 11. The global observability condition, as per Definition 1, fails for NDCG, MAP and AUC.\nCombining the above lemma with Theorem 2 of [4], we conclude that there cannot exist any algorithm which has sublinear regret for any of the following measures: NDCG, MAP or AUC, with top-1 feedback.\nTheorem 12. There exists an online game for NDCG with top-1 feedback, such that for every online algorithm, there is an adversary strategy that guarantees the following\nmax \u03c3 T\u2211 t=1 NDCG(\u03c3, rt)\u2212 E [ T\u2211 t=1 NDCG(\u03c3t, rt) ] = \u2126(T ). (7)\nFurthermore, the same lower bound holds if NDCG is replaced by MAP or AUC.\nNote: In the NDCG case, allowing the adversary to play multigraded, and not just binary, relevance vectors only makes the adversary more powerful. So the lower bound above continues to apply."}, {"heading": "10 Empirical Results", "text": "We conducted simulation studies to compare regret rates of SumLoss and DCG, when feedback is received only for top ranked object (by applying Algorithm1 ) and full relevance vector is revealed at end of each round (by applying FPL of [13]). Relevance vectors were restricted to take binary values.\nWe simulated relevance vectors for a fixed set of 10 objects (m = 10). We initially fixed half of the objects to be relevant and other half irrelevant, as the true relevance vector. Then, binary valued relevance vectors for adversary were simulated by adding small Gaussian noise to the true relevance vector. Thus, there was mostly small variances among the relevance vectors, simulating the fact that, in real world, majority of users will agree on the relevance of most objects, with small differences. A total of T = 10000 relevance vectors were generated (simulating number of rounds).\nIn Algorithm 1 , since the average of the relevance vectors per block was estimated by uniform sampling according to Lemma 8, the code for the algorithm was run 10 times, with the same set of relevance vectors, for averaging under the algorithm\u2019s randomization. Fig.1 shows (averaged over time) regret with top-1 feedback for SumLoss. Averaged over time means the cumulative regret upto time t was divided by t, for 1 \u2264 t \u2264 T . The figure clearly indicates that after the learning phase of the initial few iterations, the learner outputs mostly correct rankings, with the average regret going down to 0 at rate O(T\u22121/3). Fig.3 shows the same graph for DCG. It corroborates the fact that Algorithm1 can be applied to DCG to get same order of regret.\nFig.4 compares (averaged over time) regret, with top-1 and full information feedback respectively, for SumLoss. The comparison was done from 1000 iterations onwards, i.e., roughly after the learning phase of the learner. It can be clearly seen that average regret with full information goes down at rate faster (\u0398(T\u22121/2)) than average regret with top-1 feedback (\u0398(T\u22121/3)). Fig.4 shows the same comparison for DCG."}, {"heading": "11 Conclusion", "text": "We introduced a novel, interesting feedback model for online ranking of a fixed set of objects with diverse preferences. Our results are quite comprehensive as far as the T dependence is concerned. The only exception is Precision@k where the possibility of an O(T 1/2) regret algorithm remains open. Note that Precision@k is really peculiar since top-1 feedback is actually full feedback for k = 1.\nThe most interesting future extension of this work is to move beyond ranking fixed set of objects and considering different document lists associated with queries. This falls under the category of partial monitoring with side information. Very little relevant work has been done in the general setting and our current work can lay the foundations for interesting application in this field. Another extension is investigating whether an algorithm with sublinear regret can be defined for NDCG, MAP or AUC, when the regret is defined relative to some constant factor (larger than 1) times the best performance in hindsight."}, {"heading": "12 Regret for SumLoss", "text": ""}, {"heading": "12.1 Proof of Lemma 2", "text": "Proof. For any p \u2208 \u2206, we have `i \u00b7 p = \u22112m j=1 pj (\u03c3i \u00b7 rj) = \u03c3i \u00b7 ( \u22112m\nj=1 pjrj) = \u03c3i \u00b7 Er[r], where the expectation is taken w.r.t p (pj is the j th component of p). By dot product rule between 2 vectors, li \u00b7p is minimized when ranking of objects according to \u03c3i and expected relevance of objects are in opposite order. That is, the object with highest expected relevance is ranked 1 and so on. Formally, li \u00b7 p is minimized when Er[r(\u03c3\u22121i (1)] \u2265 Er[r(\u03c3 \u22121 i (2)] \u2265 . . . \u2265 Er[r(\u03c3 \u22121 i (m)].\nThus, for action i, probability cell is defined as Ci = {p \u2208 \u2206 : \u22112m j=1 pj = 1, Er[r(\u03c3 \u22121 i (1)] \u2265\nEr[r(\u03c3 \u22121 i (2)] \u2265 . . . \u2265 Er[r(\u03c3 \u22121 i (m)]}. That is, for any p \u2208 Ci, action i is optimal w.r.t p. Since Ci is obviously non-empty and it has only 1 equality constraint (hence 2m \u2212 1 dimensional), action i is Pareto optimal.\nThe above holds true for all learner\u2019s actions i."}, {"heading": "12.2 Proof of Lemma 3", "text": "Proof. From Lemma 2, we know that every one of learner\u2019s actions are pareto-optimal and Ci, associated with action \u03c3i, has structure Ci = {p \u2208 \u2206 : \u22112m j=1 pj = 1, Er[r(\u03c3 \u22121 i (1)] \u2265 Er[r(\u03c3 \u22121 i (2)] \u2265 . . . > Er[r(\u03c3 \u22121 i (m)]}.\nLet \u03c3\u22121i (k) = a, \u03c3 \u22121 i (k + 1) = b. Let it also be true that \u03c3 \u22121 j (k) = b, \u03c3 \u22121 j (k + 1) = a and \u03c3\u22121i (n) = \u03c3 \u22121 j (n), \u2200n 6= {k, k + 1}. This indicates the sufficient condition stated in Lemma. 3 for {\u03c3i, \u03c3j} to be neighbors. Then, Ci\u2229Cj = {p \u2208 \u2206 : \u22112m j=1 pj = 1, Er[r(\u03c3 \u22121 i (1)] \u2265 . . . \u2265 Er[r(\u03c3 \u22121 i (k)] = Er[r(\u03c3 \u22121 i (k+1)] \u2265 . . . \u2265 Er[r(\u03c3\u22121i (m)]}. Hence, there are two equalities in the non-empty set Ci \u2229 Cj and it is an (2m \u2212 2) dimensional polytope. Hence condition of Definition 4. holds true and {\u03c3i, \u03c3j} are neighbors."}, {"heading": "12.3 Proof of Theorem 4", "text": "Proof. We will explicitly show that local observability condition fails by considering the case when number of objects is m = 3. Specifically, action pair {\u03c31, \u03c32}, in Table 1 and 2, are neighboring actions, using Lemma 3 . Moreover, from Lemma 2 , since every action \u03c3k is Pareto-optimal, every Ck is 2\nm \u2212 1 dimensional. Since C1 \u2229 C2 is 2m \u2212 2 dimensional (from Def. 4), the neighborhood action set of actions {\u03c31, \u03c32} is precisely \u03c31 and \u03c32 and contains no other actions. By definition of signal matrices S\u03c31 , S\u03c32 and entries `1, `2 in table 1 and 2 , we have\nS\u03c31 = S\u03c32 = [ 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ] `1 \u2212 `2 = [ 0 1 \u22121 0 0 1 \u22121 0\n] (8) It is clear that `1 \u2212 `2 /\u2208 Col(S>\u03c31). Hence, Definition 5 fails to hold."}, {"heading": "13 Efficient Algorithm for Obtaining Regret", "text": ""}, {"heading": "13.1 Proof of Lemma 8", "text": "Proof. We can write r\u0302t = \u2211m j=1 rij (j)ej , where ej is the standard basis vector along coordinate j.\nThen Ei1,...,im(r\u0302t) = \u2211m j=1Eij (rij (j)ej) = \u2211m j=1 \u2211t k=1\nrk(j)ej t = ravg1:t ."}, {"heading": "13.2 Proof of Theorem 7", "text": "Proof. Instead of top-1 feedback, assume that at each round, after learner reveals his action, the full relevance vector is revealed to the learner. Then an O( \u221a T ) expected regret for SumLoss can be obtained by applying FTPL (Follow the perturbed leader), in the following manner:\nAt end of every round t, the full relevance vector generated by the adversary is revealed. The relevance vectors are accumulated as r1:t = r1:t\u22121 + rt, where r1:s = \u2211s i=1 ri. A learner\u2019s action (permutation) for round t + 1 is generated by solving M(r1:t + pt), where pt \u2208 [0, \u03b7]m and \u03b7 is algorithmic (randomization) parameter. It should be noted that M(y) = argmin\n\u03c3 \u03c3 \u00b7 y is simply\nsorting of y since f(\u03c3) = \u03c3 is a monotone function as defined in Sec.3 . The key idea is that FTPL implicitly maintains a distribution over m! actions (permutations) at beginning of each round, by randomly perturbing the scores of only m objects: score of each object is sum of (deterministic) accumulated relevance so far and (random) uniform value from [0, \u03b7]. Thus, it bypasses having to maintain explicit weight on each of m! arms, which is computationally prohibitive. This key property which introduces efficiency in our algorithm is in contrast to the general algorithms based on exponential weights, which have to maintain explicit weights, based on accumulated history, on each action and randomly select an action based on weights.\nNow let us look at a variant of the full information problem. The (known) time horizon T is divided into K blocks, i.e, {B1, . . . , BK}, of equal size T/K. Here, Bi = {(i \u2212 1)(T/K) + 1, (i \u2212 1)(T/K)+2, (i\u22121)T/K+3, . . . , i(T/K)}. While operating in a block, the relevance vectors within the block are accumulated, but not used to generate learner\u2019s actions like in the full information version. Assume at the start of block Bi, there was some relevance vector r\ni. Then at each time point in the block, a fresh p \u2208 [0, \u03b7]m is sampled and M(ri+p) is solved to generate permutation for next time point. At the end of a block, the average of the accumulated relevance vectors (ravg) for the block is used for updation, as ri + ravg, to get ri+1 for the next block. The process is repeated for each block. At the beginning of the first block, r1 = {0}m.\nFormally, let the FTPL have an implicit distribution \u03c1i (over the permutations) at the beginning of block Bi. That is \u03c1i \u2208 \u2206, where \u2206 is the probability simplex over m! actions. Sampling a permutation using \u03c1i at each time point of the block Bi means sampling a fresh p \u2208 [0, \u03b7]m at every time point t and solving M(s1:(i\u22121) +p), where s1:(i\u22121) = \u2211i\u22121 j=1 sj and sj is the average of relevance vectors of block Bj . Since action \u03c3 is generated according to distribution \u03c1, and in block k, distribution \u03c1k is fixed,\nwe have E\u03c1k \u2211 t\u2208[Bk] SumLoss(\u03c3t, rt) = \u2211 t\u2208[Bk] \u03c1k \u00b7 [SumLoss(\u03c31, rt), . . . , SumLoss(\u03c3m!, rt)] Thus,\nthe total expected loss of this variant of the full information problem is:\nE\u03c11,...,\u03c1K T\u2211 t=1 SumLoss(\u03c3t, rt) =\nE\u03c11,...,\u03c1K K\u2211 k=1 \u2211 t\u2208[Bk] SumLoss(\u03c3t, rt) (9)\n= K\u2211 k=1 \u2211 t\u2208[Bk] \u03c1k \u00b7 [SumLoss(\u03c31, rt), . . . , SumLoss(\u03c3m!, rt)]\n= K\u2211 k=1 \u2211 t\u2208[Bk] \u03c1k \u00b7 [\u03c31 \u00b7 rt, . . . , \u03c3m! \u00b7 rt)]\n= T\nK K\u2211 k=1 \u03c1k \u00b7 [\u03c31 \u00b7 sk, . . . , \u03c3m! \u00b7 sk]\n= T\nK K\u2211 k=1 E\u03c1kSumLoss(\u03c3k, sk)\n= T\nK E\u03c11,...,\u03c1k K\u2211 k=1 SumLoss(\u03c3k, sk) (10)\nwhere sk = \u2211 t\u2208[Bk] rt\nT/K . This clearly implies that the variant of the full information problem over\nT rounds reduces to full information problem in K rounds, where at end of every round k \u2208 [K], \u03c1k is being updated to \u03c1k+1 by updating s1:(k\u22121) to s1:k.\nBy the regret bound of FTPL, for K rounds of full information problem, with = \u221a D/RAK,\nwe have:\nE\u03c11,...,\u03c1k K\u2211 k=1 SumLoss(\u03c3k, sk)\n\u2264 min \u03c3 K\u2211 k=1 SumLoss(\u03c3, sk) + 2 \u221a DRAK\n= min \u03c3 T\u2211 t=1 \u03c3 \u00b7 sk + 2 \u221a DRAK\n= min \u03c3 T\u2211 t=1 \u03c3 \u00b7 rt T/K + 2 \u221a DRAK\n(11)\nwhere D,R,A are parameters implicit to the loss under consideration(To be detailed later).\nNow, since min \u03c3\n\u2211T t=1 \u03c3 \u00b7\nrt T/K = min \u03c3 1 T/K\n\u2211T t=1 SumLoss(\u03c3, rt), combining Eq. 10 and Eq.\n11, we get:\nE\u03c11,...,\u03c1K T\u2211 t=1 SumLoss(\u03c3t, rt)\n\u2264 min \u03c3 T\u2211 t=1 SumLoss(\u03c3, rt) + 2 T K \u221a DRAK.\n(12)\nHowever, in our top-1 feedback model, the learner does not get to see the full relevance vector at each round. Thus, we form the unbiased estimator s\u0302i of si, using Lemma. 8 . That is, at start of each block, we choose m time points uniformly at random, and at those time points, we output a random permutation which places each object, in turn, at top. At the end of the block, we form the relevance vector s\u0302 which is the unbiased estimator of s. Note that using s\u0302i instead of true si makes the distributions \u03c1i themselves random. But significantly, \u03c1k is dependent only on information received upto the beginning of block k and is independent of the information collected in the block. Thus, for block k, we have:\nE\u03c1k/s\u03021,s\u03022,..,s\u0302k\u22121 \u2211 t\u2208[Bk] SumLoss(\u03c3t, rt)\n= T\nK E\u03c1k/s\u03021,s\u03022,..,s\u0302k\u22121SumLoss(\u03c3k, sk) (From Eq.10)\n= T\nK E\u03c1k/s\u03021,s\u03022,..,s\u0302k\u22121Es\u0302kSumLoss(\u03c3k, s\u0302k) (Linearity property)\n(13)\nCrucially, since random distribution \u03c1k is independent of s\u0302k, the expectations are exchangeable and hence we have the following equation\nEs\u0302E\u03c1/s\u0302[ T\u2211 t=1 SumLoss(\u03c3t, rt)]\n= T\nK Es\u0302E\u03c1/s\u0302[ K\u2211 k=1 SumLoss(\u03c3k, s\u0302k)] (From Eq.10)\n\u2264 T K {Es\u0302[min \u03c3 K\u2211 k=1 \u03c3i \u00b7 s\u0302k] + 2 \u221a DRAK} (From Eq.11)\n\u2264 T K {min \u03c3 K\u2211 k=1 \u03c3i \u00b7 sk + 2 \u221a DRAK} (Jensen\u2019s Inequality)\n\u2264 min \u03c3 T\u2211 t=1 \u03c3i \u00b7 rt + 2 T K \u221a DRAK\n= min \u03c3 T\u2211 t=1 SumLoss(\u03c3, rt) + 2 T K \u221a DRAK\n(14)\nHowever, since in each block, m rounds are reserved for exploration, which do not follow from distribution \u03c1, the total expected loss is the sum of expected loss from exploitation and exploration.\nExploration leads to an extra regret of RmK, where R, as has been stated before, is an implicit parameter depending on the loss under consideration. The extra regret is because loss in each of the exploration rounds is \u2264 R (explained later), total of m rounds in each block and total of K blocks. Thus, overall regret is:\nE[ T\u2211 t=1 SumLoss(\u03c3t, rt)]\n\u2264 Es\u0302E\u03c1/s\u0302[ T\u2211 t=1 SumLoss(\u03c3t, rt)] +RmK\n\u21d2E[ T\u2211 t=1 SumLoss(\u03c3t, rt)]\u2212min \u03c3 T\u2211 t=1 SumLoss(\u03c3, rt)\n\u2264 RmK + 2 T K\n\u221a DRAK\n(15)\nNow, optimizing over K, we get K = (DA/R)1/3(T/m)2/3, and\nE[ T\u2211 t=1 SumLoss(\u03c3t, rt)] \u2264 min \u03c3 T\u2211 t=1 SumLoss(\u03c3, rt)\n+O(m1/3R2/3(DA)1/3T 2/3)\n(16)\nNow, by definition of D, R and A from [13], D is upper bound on `1 norm on vectors in learner\u2019s action space, R is upper bound on dot product of vectors in learner\u2019s and adversary\u2019s action space and A is upper bound on `1 norm on vectors in adversary\u2019s action space. Thus, for SumLoss, we have\nD = \u2211m\ni=1 \u03c3(i) = O(m 2), R = \u2211m i=1 \u03c3(i)r(i) = O(m 2), A = \u2211m\ni=1 r(i) = O(m). Plugging in these values gives us result of Theorem. 7 . R, as defined in [13], can be seen to be upper bound on loss \u03c3 \u00b7 r, for any \u03c3 and r."}, {"heading": "14 Regret Bounds for DCG and Prec@k", "text": ""}, {"heading": "14.1 Extension of Results of SumLoss to DCG", "text": "We give pointers in the direction of proving the following results: a) Local observability condition fails to hold for DCG which, in combination with Theorem 4 in [4], proves that the lower bound on regret of DCG is \u2126(T 2/3). b)The efficient algorithm of Sec.7 applies to DCG, with regret of O(T 2/3). Thus, the minimax regret of DCG is \u0398(T 2/3). All results are applicable to non-binary relevance vectors. This allows us to skip the proof of global observability, which is complicated for non-binary relevance vectors.\nLet adversary be able to choose r \u2208 {0, 1, . . . , n}m. Then, from definition of DCG in Sec.3 , it is clear DCG=f(\u03c3) \u00b7 g(r). f(\u03c3) and g(r) has already been defined for DCG. Both are composed of m copies of univariate, monotonic, scalar valued function, where for f(\u00b7), it is monotonically decreasing whereas for g(\u00b7), it is increasing.\nWith slight abuse of notations, the Loss matrix L implicitly means Gain matrix, where entry in cell {i, j} of L is f(\u03c3i) \u00b7 g(rj). The feedback matrix H remains the same. In Definition 1., learner action i is optimal if `i \u00b7 p \u2265 `j \u00b7 p, \u2200j 6= i.\nIn Definition 2., the maximum number of distinct elements that can be in a row of H is n+ 1. The signal matrix now becomes Si \u2208 {0, 1}(n+1)\u00d72 m , where (Si)k,` = 1(Hi,` = k \u2212 1).\nLocal Observability: In Lemma.2 , proposed for SumLoss, `i \u00b7 p equates to f(\u03c3) \u00b7 Er[g(r)]. From definition of DCG, and from the structure and properties of f(\u00b7), g(\u00b7) , it is clear that `i \u00b7 p is maximized under the same condition, i.e, Er[r(\u03c3 \u22121 i (1)] \u2265 Er[r(\u03c3 \u22121 i (2)] \u2265 . . . \u2265 Er[r(\u03c3 \u22121 i (m)]. Thus, all actions are pareto-optimal Careful observation of Lemma.3 shows that it is directly applicable to DCG, in light of extension of Lemma.2 to DCG. Finally, just like in SumLoss, simple calculations with m = 3 and n = 1, in light of Lemma.2 and 3, show that local observability condition fails to hold, Implementation of the Efficient Algorithm: The only change in Algorithm.1 which allows extension to DCG with non-binary relevance is that relevance values will enter into the algorithm via the transformation gs(\u00b7). That is, every component of relevance vector r, i.e. r(i), will become 2r(i) \u2212 1. Every operation of Algorithm.1 will happen on the transformed relevance vectors. It is very easy to see that every step in analysis of the algorithm will be valid by just considering the transformed relevance vectors to be some new relevance vectors with magnified relevance values. The fact that r was binary valued in SumLoss played no role in the analysis of the algorithm or Lemma.8 . The properties which allowed the extension was that g(\u00b7) is composed of univariate, monotonic, scalar valued functions andDCG(\u03c3, r) is a linear function of f(\u03c3) and g(r).\nIt is also interesting to note that M(y) = argmax \u03c3 f(\u03c3) \u00b7 y = argmin \u03c3 \u03c3 \u00b7 y. Thus, no changes in the algorithm is required, other than simple transformation of relevance values.\nProof of Theorem 9: Following the proof of Theorem 7 , modified for DCG, Eq.16 gives (for DCG):\nE[ T\u2211 t=1 DCG(\u03c3t, rt)] \u2265max \u03c3 T\u2211 t=1 DCG(\u03c3, rt)\n\u2212O(m1/3R2/3(DA)1/3T 2/3)\nFor DCG , D = \u2211m\ni=1 f s(\u03c3(i)) = O(m), R = \u2211m i=1 f\ns(\u03c3(i))gs(r(i)) = O(m(2n \u2212 1)), A =\u2211m i=1 g s(r(i)) = O(m(2n \u2212 1)) and hence the regret is O((2n \u2212 1)m5/3T 2/3)."}, {"heading": "14.2 Extension of Results of SumLoss to Prec@k", "text": "Since Prec@k= f(\u03c3) \u00b7 r, with f(\u00b7) having properties enlisted in Sec. 3 , all results of SumLoss trivially extend to Prec@k, except results on local observability. The reason is that while f(\u00b7) of SumLoss is strictly monotonic, f(\u00b7) of Prec@k is monotonic but not strict. The gain function depends only on the objects in the top-k position of the ranked list, irrespective of the order. A careful analysis shows that Lemma 3 fails to extend to the case of Prec@k. Thus, we cannot define the neighboring action set of the Pareto optimal action pairs, and hence cannot prove or disprove local observability. The structure of neighbors in Prec@k remains an open question.\nHowever, the non-strict monotonicity of Prec@k is required for solving M(y) = argmax \u03c3 f(\u03c3) \u00b7 y efficiently.\nProof of Corollary.10: Following the proof of Theorem.7 , modified for Prec@k, Eq.16 gives (for Prec@k):\nE[ T\u2211 t=1 Prec@k(\u03c3t, rt)] \u2265max \u03c3 T\u2211 t=1 Prec@k(\u03c3, rt)\n\u2212O(m1/3R2/3(DA)1/3T 2/3) for Prec@k, D = \u2211k\ni=1 f s(\u03c3(i)) = O(k), R = \u2211m i=1 f s(\u03c3(i))gs(r(i)) = O(k), A = \u2211m\ni=1 r(i) = O(m) and hence the regret is O(km2/3T 2/3)."}, {"heading": "15 Non-Existence of sublinear regret bounds for NDCG, MAP", "text": "and AUC- Extensions\nWe show via simple calculations that for the case m = 3, global observability condition fails to hold for NDCG, when relevance vectors are restricted to take binary values. The intuition behind failure to satisfy global observability condition is that the NDCG(\u03c3, r) = f(\u03c3) \u00b7 g(r), where where g(r) = r/Z(r) (See Sec.3 ). Thus, g(\u00b7) cannot be by univariate, scalar valued functions. This makes it impossible to write the difference between two rows as linear combination of columns of (transposed) signal matrices.\nProof. The following calculations can be easily done: The first and last row of Table 1 , when calculated for NDCG, is:\n`\u03c31 =[1, 1/2, log22/log23, (1 + log23/2)/(1 + log23),\n1, 3/(2(1 + 1/log23)), 1, 1]\n`\u03c36 =[1, 1, log22/log23, 1, 1/2, 3/(2(1 + 1/log23)),\n(1 + log23/2)/(1 + log23), 1]\nWe remind once again that NDCG is a gain function, as opposed to SumLoss. The difference between the two vectors is:\n`\u03c31 \u2212 `\u03c36 =[0, 1/2, 0, log23/(2(1 + log23)), \u2212 1/2, 0,\u2212log23/(2(1 + log23)), 0].\nThe signal matrices are same as SumLoss :\nS\u03c31 = S\u03c32 = [ 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ]\nS\u03c33 = S\u03c35 = [ 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 ]\nS\u03c34 = S\u03c36 = [ 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 ]\nIt can now be easily checked that `\u03c31 \u2212 `\u03c36 does not lie in the (combined) column span of the (transposed) signal matrices.\nWe show similar calculations for MAP and AUC: MAP: We once again take m = 3. The first and last row of Table 1 , when calculated for MAP, is:\n`\u03c31 = [1, 1/3, 1/2, 7/12, 1, 5/6, 1, 1] `\u03c36 = [1, 1, 1/2, 1, 1/3, 5/6, 7/12, 1]\nLike NDCG, MAP is also a gain function. The difference between the two vectors is:\n`\u03c31 \u2212 `\u03c36 = [0,\u22122/3, 0,\u22125/12, 2/3, 0, 5/12, 0].\nThe signal matrices are same SumLoss :\nS\u03c31 = S\u03c32 = [ 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ]\nS\u03c33 = S\u03c35 = [ 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 ]\nS\u03c34 = S\u03c36 = [ 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 ] It can now be easily checked that `\u03c31 \u2212 `\u03c36 does not lie in the (combined) column span of the\n(transposed) signal matrices.\nAUC: For AUC, we will show the calculations for m = 4. This is because global observability does hold with m = 3, as the normalizing factor for all relevance vectors with mixture of 0 and 1 is same. The normalizing factor changes from m = 4 onwards; hence global observability fails.\nTable.1 will be extended since m = 4. Instead of illustrating the full table, we point out the important facts about the loss matrix table with m = 4 for AUC.\nThe 24 relevance vectors heading the columns are: r1 = 0000, r2 = 0001, r3 = 0010, r4 = 0100, r5 = 10000, r6 = 0011, r7 = 0101, r8 = 1001, r9 = 0110, r10 = 1010, r11 = 1100, r12 = 0111, r13 = 1011, r14 = 1101, r15 = 1110, r16 = 1111.\nWe will calculate the losses of 1st and last (24th) action, where \u03c31 = 1234 and \u03c324 = 4321.\n`\u03c31 = [0, 1, 2/3, 1/3, 0, 1, 3/4, 1/2, 1/2, 1/4, 0, 1, 2/3, 1/3, 0, 0] `\u03c324 = [0, 0, 1/3, 2/3, 1, 0, 1/4, 1/2, 1/2, 3/4, 1, 0, 1/3, 2/3, 1, 0]\nAUC, like SumLoss, is a loss function. The difference between the two vectors is:\n`\u03c31 \u2212 `\u03c324 = [0, 1, 1/3,\u22121/3,\u22121, 1, 1/2, 0, 0,\u22121/2,\u22121, 1, 1/3,\u22121/3,\u22121, 0].\nThe signal matrices for AUC with m = 4 will be slightly different. This is becuase there are 24 signal matrices, corresponding to 24 actions. However, every 6 action will have same signal matrix. That is, all 6 permutations which places object 1 first will have same signal matrix, 6 permutations which places object 2 first will have same signal matrix and so on. For simplicity, we denote the signal matrices as {S1, S2, S3, S4}, where Si corresponds to signal matrix where object i is placed at top :\nS1 = [ 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 ]\nS2 = [ 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 1 ]\nS3 = [ 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 ]\nS4 = [ 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 ] It can now be easily checked that `\u03c31 \u2212 `\u03c324 does not lie in the (combined) column span of\nS1, S2, S3, S4 ."}], "references": [{"title": "Diversifying search results", "author": ["Rakesh Agrawal", "Sreenivas Gollapudi", "Alan Halverson", "Samuel Ieong"], "venue": "In Proceedings of the Second ACM International Conference on Web Search and Data Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Improved bounds for online learning over the permutahedron and other ranking polytopes", "author": ["Nir Ailon"], "venue": "In AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Modern information retrieval, volume 463", "author": ["R. Baeza-Yates", "B. Ribeiro-Neto"], "venue": "ACM press New York.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Partial monitoring\u2013classification, regret bounds, and algorithms, 2013", "author": ["G\u00e1bor Bart\u00f3k", "Dean Foster", "D\u00e1vid P\u00e1l", "Alexander Rakhlin", "Csaba Szepesv\u00e1ri"], "venue": "Best viewed in color. Code available on request", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Minimax regret of finite partial-monitoring games in stochastic environments", "author": ["G\u00e1bor Bart\u00f3k", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "From external to internal regret", "author": ["Avrim Blum", "Yishay Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Regret minimization under partial monitoring", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "Mathematics of Operations Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "AUC optimization vs. error rate minimization", "author": ["Corinna Cortes", "Mehryar Mohri"], "venue": "In NIPS, page 10,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "On the consistency of ranking algorithms", "author": ["John C Duchi", "Lester W Mackey", "Michael I Jordan"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML-", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "No internal regret via neighborhood watch", "author": ["Dean P Foster", "Alexander Rakhlin"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Cumulated gain-based evaluation of IR techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "IR evaluation methods for retrieving highly relevant documents", "author": ["Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen"], "venue": "In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Combinatorial partial monitoring game with linear feedback and its applications", "author": ["Tian Lin", "Bruno Abrahao", "Robert Kleinberg", "John Lui"], "venue": "In Proceedings of ICML 2014 Cycle", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Letor: Benchmark dataset for research on learning to rank for information retrieval", "author": ["Tie-Yan Liu", "Jun Xu", "Tao Qin", "Wenying Xiong", "Hang Li"], "venue": "In Proceedings of SIGIR 2007 workshop on learning to rank for information retrieval,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["Antonio Piccolboni", "Christian Schindelhauer"], "venue": "In Computational Learning Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Redundancy, diversity and interdependent document relevance", "author": ["Filip Radlinski", "Paul N Bennett", "Ben Carterette", "Thorsten Joachims"], "venue": "In ACM SIGIR Forum,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}], "referenceMentions": [{"referenceID": 11, "context": "Learning occurs in an online setting: at each round, the system outputs a ranked list of the objects and the quality of ranking is measured by one of several popular ranking measures (like DCG [12] or MAP [3]), taking into account the users preferences encoded as relevance vectors.", "startOffset": 193, "endOffset": 197}, {"referenceID": 2, "context": "Learning occurs in an online setting: at each round, the system outputs a ranked list of the objects and the quality of ranking is measured by one of several popular ranking measures (like DCG [12] or MAP [3]), taking into account the users preferences encoded as relevance vectors.", "startOffset": 205, "endOffset": 208}, {"referenceID": 16, "context": "The idea of ranking for diverse preferences has been motivated from a branch of work, sometimes called \u201cranking with diversity\u201d [18, 17, 1] .", "startOffset": 128, "endOffset": 139}, {"referenceID": 0, "context": "The idea of ranking for diverse preferences has been motivated from a branch of work, sometimes called \u201cranking with diversity\u201d [18, 17, 1] .", "startOffset": 128, "endOffset": 139}, {"referenceID": 6, "context": "The appropriate framework to study the problem is that of partial monitoring [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 13, "context": "A very recent paper shows another practical application of the idea where feedback is neither full information nor bandit [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 3, "context": "Recent advances in the classification of partial monitoring games tell us that the minimax regret, in an adversarial setting, is governed by a property of the loss and feedback functions called observability [4, 10].", "startOffset": 208, "endOffset": 215}, {"referenceID": 9, "context": "Recent advances in the classification of partial monitoring games tell us that the minimax regret, in an adversarial setting, is governed by a property of the loss and feedback functions called observability [4, 10].", "startOffset": 208, "endOffset": 215}, {"referenceID": 8, "context": "We instantiate these general observability notions for the top-1 feedback case and prove that, for some ranking measures, namely PairwiseLoss [9], DCG and Precision@k [15], global observability holds.", "startOffset": 142, "endOffset": 145}, {"referenceID": 14, "context": "We instantiate these general observability notions for the top-1 feedback case and prove that, for some ranking measures, namely PairwiseLoss [9], DCG and Precision@k [15], global observability holds.", "startOffset": 167, "endOffset": 171}, {"referenceID": 7, "context": "For example, the normalized versions of PairwiseLoss, DCG and Precision@k are called AUC [8], NDCG [11] and MAP respectively.", "startOffset": 89, "endOffset": 92}, {"referenceID": 10, "context": "For example, the normalized versions of PairwiseLoss, DCG and Precision@k are called AUC [8], NDCG [11] and MAP respectively.", "startOffset": 99, "endOffset": 103}, {"referenceID": 1, "context": "It has been shown in [2] that SumLoss differs from PairwiseLoss only by an r-dependent constant and hence the regret under the two measures are equal:", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "Had the relevance vectors been stochastic in nature, the results would have held; however, efficient algorithm already exists in stochastic case [5] .", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "developed in [4, 10].", "startOffset": 13, "endOffset": 20}, {"referenceID": 9, "context": "developed in [4, 10].", "startOffset": 13, "endOffset": 20}, {"referenceID": 3, "context": "The following definitions, given for abstract problems in [4], has been refined to fit our problem context.", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "First, we get a lower bound by combining our Theorem 4 with Theorem 4 in [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 6, "context": "1 in [7] gives an in-efficient algorithm (inspired by the algorithm originally given in [16]) obtaining O(T 2/3(log T )1/3) regret.", "startOffset": 5, "endOffset": 8}, {"referenceID": 15, "context": "1 in [7] gives an in-efficient algorithm (inspired by the algorithm originally given in [16]) obtaining O(T 2/3(log T )1/3) regret.", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "The algorithm in Figure 1 of [7] achieves O(T 2/3(log T )1/3) regret bound for SumLoss.", "startOffset": 29, "endOffset": 32}, {"referenceID": 6, "context": "However, the algorithm in [7] is intractable in our setting since the number of learner\u2019s actions is exponential in number of objects m.", "startOffset": 26, "endOffset": 29}, {"referenceID": 12, "context": "Using the average vector as a full information vector for next phase, rest of the rounds in next phase follow the actions according to the distribution suggested by Follow the Perturbed Leader (FTPL) [13] (this is exploitation of previous experience).", "startOffset": 200, "endOffset": 204}, {"referenceID": 5, "context": "Our algorithm is motivated by the reduction from bandit-feedback to full feedback given in [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "However, the algorithm in [6] cannot be directly applied to our problem, because we are not in the bandit setting and hence do not know loss of any action.", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "Combining the above lemma with Theorem 2 of [4], we conclude that there cannot exist any algorithm which has sublinear regret for any of the following measures: NDCG, MAP or AUC, with top-1 feedback.", "startOffset": 44, "endOffset": 47}, {"referenceID": 12, "context": "We conducted simulation studies to compare regret rates of SumLoss and DCG, when feedback is received only for top ranked object (by applying Algorithm1 ) and full relevance vector is revealed at end of each round (by applying FPL of [13]).", "startOffset": 234, "endOffset": 238}], "year": 2017, "abstractText": "We consider a setting where a system learns to rank a fixed set of m items. The goal is produce a good ranking for users with diverse interests who interact with the system for T rounds in an online fashion. We consider a novel top-1 feedback model for this problem: at the end of each round, the relevance score for only the top ranked object is revealed to the system. However, the performance of the system is judged on the entire ranked list. We provide a comprehensive set of results regarding learnability under this challenging setting. For popular ranking measures such as PairwiseLoss and DCG, we prove that the minimax regret is \u0398(T ). Moreover, the minimax regret is achievable using an efficient algorithmic strategy that only spends O(m logm) time per round. The same algorithmic strategy achieves O(T ) regret for Precision@k. Surprisingly, we show that for normalized versions of these ranking measures, namely AUC, NDCG and MAP, no online ranking algorithm can have sub-linear regret.", "creator": "LaTeX with hyperref package"}}}