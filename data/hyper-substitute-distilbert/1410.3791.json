{"id": "1410.3791", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2014", "title": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition", "abstract": "the increasing history of languages evolving on smart web introduces a new level of complexity effective ethnic retrieval ( ir ) systems. we cannot soon even assume semantic textual content is written in one language or representing the designated linguistic entity. addressing this paper, companies say how to build massive layered annotators with minimal physical expertise and bureaucracy. may describe a system helping builds named entity dictionary ( ner ) database for query tagged constituents using html code freebase. your algorithm does not require ner but annotated datasets or language specific resources like signatures, database corpora, parallel orthographic rules. the novelty of competing lies risk - creating extensive language learning entries, while praising competitive performance.", "histories": [["v1", "Tue, 14 Oct 2014 18:37:32 GMT  (781kb,D)", "http://arxiv.org/abs/1410.3791v1", "9 pages, 4 figures, 5 tables"]], "COMMENTS": "9 pages, 4 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["rami al-rfou", "vivek kulkarni", "bryan perozzi", "steven skiena"], "accepted": false, "id": "1410.3791"}, "pdf": {"name": "1410.3791.pdf", "metadata": {"source": "CRF", "title": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition", "authors": ["Rami Al-Rfou", "Vivek Kulkarni", "Bryan Perozzi", "Steven Skiena"], "emails": ["skiena}@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "In this paper, we demonstrate how to build massive multilingual annotators with minimal human expertise and intervention. We describe a system that builds Named Entity Recognition (NER) annotators for 40 major languages using Wikipedia and Freebase. Our approach does not require NER human annotated datasets or language specific resources like treebanks, parallel corpora, and orthographic rules. The novelty of approach lies therein - using only language agnostic techniques, while achieving competitive performance.\nOur method learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Then, we automatically generate datasets from Wikipedia link structure and Freebase attributes. Finally, we apply two preprocessing stages (oversampling and exact surface form matching) which do not require any linguistic expertise.\nOur evaluation is two fold: First, we demonstrate the system performance on human annotated datasets. Second, for languages where no gold-standard benchmarks are available, we propose a new method, distant evaluation, based on statistical machine translation."}, {"heading": "1 Introduction", "text": "The growth of the Internet is bringing new communities, cultures and languages online. However, not enough work has been proposed to deal with the increasing linguistic variety of web content. Natural Language Processing (NLP) tools are limited to a small number of languages, usually only English, which does not reflect the fast changing pace of the Internet. Correspondingly, current multilingual text-based Information Retrieval (IR) systems are restricted to simple processing stages that are based on word\u2019s surface forms and frequency based methods. We believe addressing the multilingual aspect of the problem is crucial for the future success of these systems.\nIn this work, we perform a case study on how to build a massively multilingual Named Entity Recognition (NER) system. The Named Entity Recognition task (also known as entity extraction or entity identification) extracts chunks of text as phrases and classifies them into pre-defined categories such as the names of persons, locations, and organizations. NER is an essential pre-processing stage\nin NLP and Information Retrieval (IR) systems, where it is used for a variety of purposes (e.g, event extraction or knowledge base population). Successful approaches to address NER rely on supervised learning [5, 12]. Applying these approaches to a massively multilingual setting exposes two major drawbacks; First, they require human annotated datasets which are scarce. Second, to design relevant features, sufficient linguistic proficiency is required for each language of interest. This makes building multilingual NER annotators a tedious and cumbersome process.\nOur work addresses these drawbacks by relying on language-independent techniques. We use neural word embeddings, Wikipedia link structure, and Freebase attributes to automatically construct NER annotators for 40 major languages. First, we learn neural word embeddings which encode semantic and syntactic features of words in each language. Second, we use the internal links embedded in Wikipedia articles to detect named entity mentions. When a link points to an article identified by Freebase as an entity article, we include the anchor text as a positive training example. However, not all entity mentions are linked in Wikipedia because of style guidelines. To address this problem, we propose oversampling and surface word matching to solve this positive-only label learning problem [10, 16\u201318] while avoiding any language-specific dependencies.\nLack of human annotated datasets not only limits quality of training but also system evaluation. We evaluate on standard NER datasets if they are available. For the remaining languages, we propose distant evaluation based on statistical machine translation (SMT) to generate testing datasets that provide insightful analysis of the system performance. In summary, our contributions are the following:\n\u2022 Language-independent extraction - for noisy datasets. Our proposed language-agnostic techniques address noise introduced by Wikipedia style guidelines, boosting the performance by at least 45% F1 on human annotated gold standards. \u2022 40 NER annotators1 - We are releasing the trained\n1Online demo is available at https://bit.ly/polyglot-ner.\nar X\niv :1\n41 0.\n37 91\nv1 [\ncs .C\nL ]\n1 4\nO ct\n2 01\nmodels as open source software. These annotators are invaluable, especially for resource scarce languages, like Serbian, Indonesian, Thai, Malay and Hebrew. \u2022 Distant Evaluation - We propose a technique based on statistical machine translation to scale our evaluation in the absence of human annotated datasets.\nOur paper is structured as follows: First, we review the related work in Section 2. In Section 3, we present our formulation of the NER problem and describe our semisupervised approach to build annotators (models) for 40 languages. Section 4 shows our procedure to generate training datasets using Wikipedia and Freebase. We discuss our results in Section 5. Section 6 shows how statistical machine translation is used to evaluate the performance of our system."}, {"heading": "2 Related Work", "text": "Wikipedia has been used as a resource for many tasks in NLP and IR [13, 19]. There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1. All previous work depends on language specific preprocessing stages such as as taggers and parallel corpora. The reliance on language specific processing poses a bottleneck to the scalability and diversity of the languages covered by the previous systems. In contrast, our work relies on only language agnostic techniques.\nThe closest related work is Nothman et al. [22]. Compared to their approach, we find that using only oversampling is a sufficient replacement for their entire proposed language dependent preprocessing pipeline (See Section 4.2.1)."}, {"heading": "3 Semi-supervised Learning", "text": "The goal of Named Entity Recognition (NER) is to identify sequences of tokens which are entities, and classify them into one of several categories. We follow the approach proposed by [7] to model NER as a word level classification problem. They observe that for most chunking tasks, including NER, the tag of a word depends mainly on its neighboring words. Considering only local context yields models with competitive performance to schemes which take into account the whole sentence structure. This word level approach ignores the dependencies between word tags and thus might not capture some constraints on tags\u2019 appearance order in the text. However, empirically, our evaluation does not indicate the manifestation of this problem. More importantly, this word based formulation allows us to use simpler oversampling and exact-matching mechanisms as we will see in Section 4.\n3.1 Word Embeddings capture semantic and syntactic characteristics of words through unsupervised learning [20]. They have been successfully used as features for several tasks including NER [7, 27] and proposed as a cornerstone for developing multilingual applications [1].\nWord embeddings are latent representations of words acquired by harnessing huge amounts of raw text through language modeling. These representations capture information about word co-occurrences and therefore their syntactic functionality and semantics. Given the abundance of unstructured text available online, we can automatically learn these embeddings for all languages and use them as features in an\nunsupervised manner. More specifically, given a language with vocabulary V , a word embedding is a mapping function \u03a6: w 7\u2192 Rd, where w \u2208 V and d is a constant value that ranges usually between 50 and 500. We use the Polyglot embeddings [1] as our sole features for each language under investigation2. The Polyglot embeddings are trained on Wikipedia without any labelled data, the vocabulary of each language consists of the most frequent 100K words and the word representation consist of 64 dimensions (d = 64). The Polyglot embeddings were trained using an objective function proposed by [6] which takes ordered sequences of words as its input. Therefore, the learned representations cluster words according to their part of speech tags. Given that most of named entities are proper nouns (part of speech), these representations are a natural fit to our task.\n3.2 Discriminative Learning We model NER as a word level classification problem. More formally, let Wni = (wi\u2212n \u00b7 \u00b7 \u00b7wi \u00b7 \u00b7 \u00b7wi+n) be a phrase centered around the word wi with a window of size 2n + 1. We seek to learn a target function F : Wni 7\u2192 Y , where Y is the set of tags. First, we map the phraseWni to its embedding representation\n\u03a6ni = [\u03a6(wi\u2212n); . . . ; \u03a6(wi); . . . ; \u03a6(wi+n)].\nNext, we learn a model \u03a8y to score tag y given \u03a6ni , i.e \u03a8y : \u03a6 n i 7\u2192 R, using a neural network with one hidden layer of size h\n(3.1) \u03a8y(\u03a6ni ) = s T (tanh(W\u03a6ni + b)),\nwhere W \u2208 Rh(2n+1)d and s \u2208 Rh are the first and second layer weights of the neural network, and b \u2208 Rh are the bias units of the hidden layer. Finally, we construct a one-vs-all classifier F and penalize it by the following hinge loss,\nJ = 1\nm\nm\u2211\ni=1\nmax (\n0, 1\u2212\u03a8ti(\u03a6ni ) + max y 6=ti y\u2208Y\n\u03a8y(\u03a6 n i ) ) ,\nwhere ti is the correct tag of the word wi, and m is the size of the training set.\n3.3 Optimization We learn the parameters \u03b8 = (s,W,b) via backpropagation with stochastic gradient descent. As the stochastic optimization performance is dependent on a good choice of the learning rate, we automate the learning rate selection through an adaptive update procedure [8]. This results in separate learning rates \u03b7i for each individual\n2Available: http://bit.ly/embeddings\nparameter, \u03b8i. More specifically the learning rate at step t for parameter i is given by the following:\n(3.2) \u03b7i(t) = 1.0\u221a\u2211t\ns=1 ( \u2202J(s) \u2202\u03b8i(s) )2 .\nFor the rest of paper, we train our annotators for 50 epochs with the adaptive learning values uniformly initialized with mean equal to zero.\nTo understand the upper bound of performance we can achieve through this specific modeling of NER. We trained new word embeddings with extended vocabulary (300K words) using English, Spanish and Dutch Wikipedia. Table 2 shows the performance of our annotators given CONLL training datasets [24, 25] and the word embeddings as features. Our results on English are similar to the ones reported by [28]. 3"}, {"heading": "4 Extracting Entity Mentions", "text": "Our procedure for creating a named entity training corpus from Wikipedia consists of two steps; First, we find which Wikipedia articles correspond to entities, using Freebase [4]. Second, we cope with the missing annotations problem through oversampling from the entity classes and extending the annotation coverage using an exact surface form matching rule.\n4.1 Article Categorization Freebase maintains several attributes for each Wikipedia article, covering around 40 different Wikipedia languages. We categorize the article topics into one of the following categories, Y = {PERSON, LOCATION, ORGANIZATION, NONENTITY}. For each entity category we specify the corresponding freebase attributes, as the following:\n\u2022 LOCATION /location/{citytown, country, region, continent, neighborhood, administrative_division} \u2022 ORGANIZATION /sports/sports_team, /book/newspaper, /organization/organization, \u2022 PERSON /people/person The result is a mapping of Wikipedia page titles and their redirects to Y . If an internal link points to any of these titles, we consider it an entity mention. Table 3 shows the\n3Notice, that we obtain higher results, here, than annotators trained on Wikipedia datasets (Table 6), because training and testing datasets belong to the same domain.\npercentage of pages that are covered by Freebase for some of the languages we consider. The entity coverage varies with each language, and this greatly biases the label distribution of the generated training data. We will overcome this bias in the distribution of entity examples using oversampling (See Section 4.2.1).\n4.2 Missing Links Unfortunately, generating a training dataset directly from the link structure results in very poor performance with DEVF1 < 10% in English, Spanish and Dutch due to missing annotations. This is a consequence of Wikipedia style guidelines4. Editors are instructed to link the first mention in the page, but not later ones. This results in leaving most entity mentions unmarked. Table 4 examines this effect by contrasting the percentage of words that are covered by entity phrases in both CONLL and Wikipedia.\nWe can view the generated examples as two sets; one that is truly positive and the other as a mix of negative and positive examples. Our task is to learn an annotator only from the positive examples. In such a setting, [10, 17, 18] show that considering the unlabeled set as negative examples while modifying the objective loss to accommodate different penalties for misclassifying each set of examples outperforms other heuristics and other iterative EM-based methods. Changing the label distribution by oversampling the positive labels will achieve a similar effect.\n4.2.1 Oversampling To overcome the effect of missing annotations, we correct the label distribution by oversampling from the entity classes. The intuition is that untagged words are not necessarily non-entities. Conversely, we have high confidence in the links which have been explicitly tagged by users. To reflect the difference in confidence levels, we categorize our labels into two subcategories: Y+ = {PERSON,\n4http://en.wikipedia.org/wiki/Wikipedia:Manual_ of_Style/Linking\nLOCATION, ORGANIZATION} and Y\u2212 ={NONENTITY}. A training example \u03a6ni is considered positive if F (\u03a6 n i ) \u2208 Y+ and negative otherwise. We define the oversampling ratio (\u03c1) to be\n\u03c1 =\n\u2211m i [F (\u03a6\nn i ) \u2208 Y+] m ,\nwhere [x] is the indicator function and m is the total number of training examples.\nOur goal, here, is to construct a subset of our training corpus where \u03c1 is higher in this subset than the original training dataset. We sample the positive class uniformly without replacement. This insures we do not change the conditional distribution of a specific entity class given it is a positive example.\nFigure 1 shows the effect of oversampling. The first point corresponds to the original distribution of positive labels in Wikipedia text, where \u03c1 \u223c= 2.5%. We observe that regardless of the chosen \u03c1, oversampling improves the results. This improvement is quite stable when 0.25 < \u03c1 < 0.75, and the Exact F1 score is increased by at least 40% for all languages we consider. We choose \u03c1 = 0.5 to be the value we use for our testing phase in Section 5 as it produces the maximum results across the three languages under investigation.\n4.2.2 Exact Surface Form Matching While oversampling mitigates the effect of the skewed label distribution, it does not address the stylistic bias with which Wikipedia editors create links. The first bias is to link only the first mention of a entity in an article. This canonical mention is usually the full name of an entity, and not the abbreviated form used throughout the remainder of the article. We found that in 200K examples tagged with PERSON, 45K examples belong to three terms mentions, 140K to two terms mentions and only 15K belonging to single term mentions. This bias\nagainst single term mentions in links does not reflect the true distribution of named entities in the text resulting in annotators which tag Noam Chomsky but not Chomsky. The second bias is that there are no self-referential links inside an entity\u2019s article (e.g. on Barack Obama\u2019s page, none of the sentences mentioning him are linked).\nIn order to extend our annotations coverage on each article, we apply a simple rule of surface form matching. If a word appeared in an entity mention, or in the title of the current article (to address the second bias), we consider all appearances of this word in the current article to be annotated with the same tag. In the case of having multiple tags for the same word, we use the most frequent tag. This process can be viewed as a first-order coreference resolution where we link mentions using exact string matching.\nFor example, after this procedure, every mention of \u2018Barack Obama\u2019, Barack, and Obama in the article on Barack_Obama will be considered a link referring to a PERSON entity. In order to avoid mislabeling functional words which appear in links (e.g. of, the, de) we exclude the most frequent 1000 words in our vocabulary.\nFigure 2 shows the improvement this stage adds when it is applied to Wikipedia before oversampling. F1 improvements that we observe in English, Spanish, and Dutch are significant, especially when \u03c1 \u2264 0.5. Most of this improvement is due to higher recall on the tag PERSON."}, {"heading": "5 Results", "text": "In this section, we will analyze the errors produced system for qualitative assessment. In addition, we evaluate the performance of POLYGLOT-NER on CONLL datasets to demonstrate the efficiency of the proposed solutions to deal with missing links in the Wikipedia markup.\nTable 5 shows annotated examples for 11 different languages. Table 5a shows correctly annotated examples\nand Table 5b a sample of the mistakes our annotators make. Analyzing our good examples shows that our system performs the best on the PERSON category, even for names that are transliterated from other languages (see Russian, Arabic and Korean examples). Moreover, our system is still able to identify entities in mixed languages scenario, for example, the appearance of \u2019Spain\u2019 in the Arabic example and \u2019Pe\u0161ek\u2019 in the Spanish example. This robustness stems from two factors; First, the embeddings vocabulary of a specific language includes frequent foreign words. Second, our annotators are able to capture sufficient local contextual clues.\nOur errors can be grouped into three categories (See Table 5b); First, common words {River, House} that appear in organizations are hard to identify. Second, our system does not consistently tag demonyms (nationalities), as Russian, French, Arabic, Greek examples show. Misclassification errors occur, common cases include confusion between LOCATION and ORGANIZATION tags in the case of nested entities (See the Chinese example) or between the PERSON and ORGANIZATION tags when company names are referred to in the same context as that of persons (See the Spanish example).\nIn addition to the qualitative analysis, we evaluate our models quantitatively on the CONLL 2002 Spanish and Dutch datasets, and the CONLL 2003 English dataset. We show results of our models trained on Wikipedia and evaluated on CONLL in Table 6. Observe that oversampling (POLYGLOT-NERS7) alone is able to get competitive results. With exact surface form matching applied to the data first (POLYGLOT-NERS6+S7), we outperform previous work on English and Spanish without applying any language-specific rules. Most of POLYGLOT-NER Dutch errors appear in the category of ORGANIZATION.\nTraining on Wikipedia results in lower scores on CONLL testing datasets compared to models trained on CONLL directly (See Table 2) due to two main factors. First this is an out-of-domain evaluation. Second, there are a variety of orthographic and contextual differences between Wikipedia and CONLL. Common differences between the datasets include: trailing periods, leading delimiters, and modifiers\nfalse ::::::: negative, and label misclassification. Translations are acquired through Google Translate\nand labels on translated phrases correspond to their annotations in the source language.\nand annotators\u2019 disagreements. Specifically, the English CONLL dataset has an over representation of upper case words and sports teams. In the Dutch dataset, country names were abbreviated after the journalist name. For example, Spain will be mapped to Spa and Italy to Ita. This leads to more out of vocabulary (OOV) terms for which Polyglot does not have embeddings. Since we do not rely on any CONLL-tailored preprocessing steps, such notational and stylistic differences affect our performance more than other approaches that tailor their systems to these differences at the cost of scalability. Such notational differences pose more harm to our performance than other approaches because we do not rely on CONLL-tailored preprocessing steps.\nHowever, [21] show that Wikipedia is better suited than human annotated datasets as a source of training for domain adaptation scenarios. We expect annotators trained on Wikipedia to be work better on heterogeneous content such as websites."}, {"heading": "6 Distant Evaluation", "text": "Scarcity of human annotated datasets has limited the scope of our evaluation so far. We seek to use Statistical Machine Translation (SMT) as a tool to generate automated evaluation datasets. However, translation is not a one-to-one term mapping between two languages; the generated sentences may not preserve the word count or order. This poses a challenge in mapping the annotations from the source language to the target one. Therefore, we do not use the generated datasets for training, but rather for evaluation. We rely on comparing aggregated statistics over sentence translation pairs as an indicator of the quality of our annotators. We call this approach Distant Evaluation to emphasize the indirect connection between our comparative measures and the annotations quality. To simplify our approach, we assume: \u2022 SMT is able to translate named entities from the source\nlanguage to their corresponding phrases in the target language. \u2022 SMT preserves the number of named entities mentioned. We will show later these assumptions hold with varying degree across languages.\nSpecifically, we define the set of entity phrases appearing in a sentence (S) to be P . Each phrase p \u2208 P belongs to a category T (p) \u2208 Y . For each category e \u2208 Y , we define\nCe(S) = \u2211\np\u2208P [T (p) = e],\nwhere [x] is the indicator function. We define the sets of sentences that belong to the source language and the target language to be L1, L2, respectively. We define two classes of error measures: omitting entities EM and adding entities EA,\nas the following\nZe = \u2211\nS\u2208L1 Ce(S),\nEM(e) = 1\nZe\n\u2211\n(S1,S2)\u2208(L1,L2)\n|Ce(S1)\u2212 Ce(S2)|+,\nEA(e) = 1\nZe\n\u2211\n(S1,S2)\u2208(L1,L2)\n|Ce(S2)\u2212 Ce(S1)|+,\nwhere (S1, S2) is the sentence translation pair and |x|+ = max(0, x).\nNext, we calculate EM and EA over all language pairs according the following steps: \u2022 Annotate English Wikipedia sentences using STANFORD\nNER5. \u2022 Set L1 to the above annotated sentences. \u2022 Randomly pick 1500 sentences that have at least one\nentity detected. \u2022 Translate these sentences using Google Translate to 40\nlanguages. \u2022 Calculate EM and EA for the language pairs for each\nentity type in Y+. Figure 3 shows the performance of our system compared to other annotators; OPENNLP {English, Spanish, Dutch} [2], NLTK English [3], and STANFORD German [11]. Our new metrics are consistent our CONLL evaluation, English outperforms both Spanish and Dutch. We outperform OPENNLP and NLTK, by a significant margin. POLYGLOTNER German annotator covers more PERSON entities than STANFORD without adding many false positives. We notice that languages with the large number of Wikipedia articles like English (en), French (fr), Spanish (es) and Portuguese (pt) show strong performance. Moreover, our performance vary across categories, the annotators performing the best on PERSON category followed by LOCATION and then ORGANIZATION.\nOur benchmark also highlights language specific issues. The poor performance in Japanese (ja) is due to the mismatch between the embedding vocabulary and the evaluation tokenizer. Vietnamese (vi) annotator aggressively annotates chunks as LOCATION (EA = 0.6) because Vietnamese Freebase distribution of attributes is skewed towards LOCATION (See Figure 3b).\nThe quality of our metric is directly correlated to the quality of translations; while in general the above mentioned assumptions hold true for most translation pairs, we found some exceptions. First, Google Translate does not translate the entities efficiently in some languages, for example, \u201c \u110b\u1175\u1105\u1165\u1112\u1161\u11ab\u110b\u1175\u1107\u1166\u11ab\u1110\u1173\u1102\u1173\u11ab Suriyothai\u110b\u1174\u110c\u1165\u11ab\u1109\u1165\u11af , Chatrichalerm Yukol\u1100\u1161\u11b7\u1103\u1169\u11a8 2001\u1110\u1162\u1100\u116e\u11a8\u110b\u1167\u11bc\u1112\u116a\u110b\u1166\u1106\u116d\u1109\u1161\u1103\u116c\u11ab\u1103\u1161.\u201d is the Korean translation of\n5http://nlp.stanford.edu/software/CRF-NER.shtml\nan English sentence \u201c These events are depicted in The Legend of Suriyothai, a 2001 Thai film directed by Chatrichalerm Yukol\u201d. This affects performance measure of Korean (ko), Greek (el) and Thai (th). Second, entity counts may not be preserved, for example, this Spanish translation \u201cYehuda Magidovitch (1886- 1961) fue uno de los arquitectos m\u00e1s prol\u00edficos de Israel.\u201d contains one location \u2018Israel\u2019 that does not appear in the original English sentence \u201cYehuda Magidovitch (1886\u20131961) was one of the most prolific Israeli architects.\u201d\nWe also investigate the effect of Wikipedia article counts on our performance. The size of Wikipedia of each language, affects our system in several aspects; Larger Wikipedia results in better word embeddings. Freebase has better attributes coverage for larger Wikipedias. More diverse set of training examples could be extracted from larger Wikipedia. Figure 4 shows the average error over all categories versus Wikipedia number of articles. We observe that larger Wikipedias result in many fewer false negatives with EM dropping by 0.6 (Figure 4a). On the other hand, larger Wikipedias annotates slightly more aggressively increasing EA by 0.15 (Figure 4b)."}, {"heading": "7 Conclusion & Future Work", "text": "We successfully built a multilingual NER system for 40 languages with no language specific knowledge or expertise. We use automatically learned features, and apply language agnostic data processing techniques. The system outperforms previous work in several languages and competitive in the rest on human annotated datasets. We demonstrate its performance on the rest of the languages, by a comparative analysis using machine translation. Our approach yields highly consistent performance across all languages. Wikipedia Cross-lingual links will be used in combination with Freebase to extend our approach to all languages as future work."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Nltk: the natural language toolkit. In 2 2 2 2 2 2 2 2 2 Wikipedia(Size", "author": ["Steven Bird"], "venue": "Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of SIGMOD,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Named entity extraction using adaboost", "author": ["Xavier Carreras", "Llu\u00eds M\u00e0rques", "Llu\u00eds Padr\u00f3"], "venue": "In Proceedings of CoNLL-2002,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Building a multilingual named entity-annotated corpus using annotation projection", "author": ["Maud Ehrmann", "Marco Turchi", "Ralf Steinberger"], "venue": "In RANLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["Charles Elkan", "Keith Noto"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Training and evaluating a german named entity recognizer with semantic generalization", "author": ["Manaal Faruqui", "Sebastian Pad\u00f3"], "venue": "In Proceedings of KONVENS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Named entity recognition through classifier combination", "author": ["Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "Proceedings of CoNLL-2003,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis", "author": ["Evgeniy Gabrilovich", "Shaul Markovitch"], "venue": "In IJCAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Exploiting Wikipedia as external knowledge for named entity recognition", "author": ["Jun\u2019ichi Kazama", "Kentaro Torisawa"], "venue": "In Proceedings of (EMNLP-CoNLL),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Multilingual named entity recognition using parallel data and metadata from wikipedia", "author": ["Sungchul Kim", "Kristina Toutanova", "Hwanjo Yu"], "venue": "In Proceedings of ACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Learning with positive and unlabeled examples using weighted logistic regression", "author": ["Wee Sun Lee", "Bing Liu"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Learning to classify texts using positive and unlabeled data", "author": ["Xiaoli Li", "Bing Liu"], "venue": "In IJCAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Building text classifiers using positive and unlabeled examples", "author": ["Bing Liu", "Yang Dai", "Xiaoli Li", "Wee Sun Lee", "Philip S Yu"], "venue": "In Data Mining,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Wikify!: linking documents to encyclopedic knowledge", "author": ["Rada Mihalcea", "Andras Csomai"], "venue": "In Proceedings of CIKM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Analysing Wikipedia and gold-standard corpora for NER training", "author": ["Joel Nothman", "Tara Murphy", "James R. Curran"], "venue": "In Proceedings of EACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R Curran"], "venue": "Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Mining wiki resources for multilingual named entity recognition", "author": ["Alexander E Richman", "Patrick Schone"], "venue": "In ACL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of CoNLL-2003,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Introduction to the conll- 2002 shared task: Language-independent named entity recognition", "author": ["Erik F. Tjong Kim Sang"], "venue": "In Proceedings of CoNLL-2002,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "A proposal to automatically build and maintain gazetteers for Named Entity Recognition by using Wikipedia", "author": ["A. Toral", "R. Munoz"], "venue": "Proceedings of the EACL-2006 Workshop on NEW TEXT-Wikis and blogs and other dynamic text sources", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Effect of non-linear deep architecture in sequence labeling", "author": ["Mengqiu Wang", "Christopher D. Manning"], "venue": "In Proceedings of IJCNLP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Successful approaches to address NER rely on supervised learning [5, 12].", "startOffset": 65, "endOffset": 72}, {"referenceID": 10, "context": "Successful approaches to address NER rely on supervised learning [5, 12].", "startOffset": 65, "endOffset": 72}, {"referenceID": 8, "context": "To address this problem, we propose oversampling and surface word matching to solve this positive-only label learning problem [10, 16\u201318] while avoiding any language-specific dependencies.", "startOffset": 126, "endOffset": 137}, {"referenceID": 14, "context": "To address this problem, we propose oversampling and surface word matching to solve this positive-only label learning problem [10, 16\u201318] while avoiding any language-specific dependencies.", "startOffset": 126, "endOffset": 137}, {"referenceID": 15, "context": "To address this problem, we propose oversampling and surface word matching to solve this positive-only label learning problem [10, 16\u201318] while avoiding any language-specific dependencies.", "startOffset": 126, "endOffset": 137}, {"referenceID": 16, "context": "To address this problem, we propose oversampling and surface word matching to solve this positive-only label learning problem [10, 16\u201318] while avoiding any language-specific dependencies.", "startOffset": 126, "endOffset": 137}, {"referenceID": 24, "context": "Language Specific Agnostic S1 S2 S3 S4 S5 S6 S7 # Language code Toral and Munoz [26] X X X 1 en Kazama and Torisawa [14] X X 1 en Richman and Schone [23] X X X 7 en, es, fr, uk, ru, pl, pt Ehrmann et al.", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "Language Specific Agnostic S1 S2 S3 S4 S5 S6 S7 # Language code Toral and Munoz [26] X X X 1 en Kazama and Torisawa [14] X X 1 en Richman and Schone [23] X X X 7 en, es, fr, uk, ru, pl, pt Ehrmann et al.", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "Language Specific Agnostic S1 S2 S3 S4 S5 S6 S7 # Language code Toral and Munoz [26] X X X 1 en Kazama and Torisawa [14] X X 1 en Richman and Schone [23] X X X 7 en, es, fr, uk, ru, pl, pt Ehrmann et al.", "startOffset": 149, "endOffset": 153}, {"referenceID": 7, "context": "[9] X X X X 6 en, es, fr, de, ru, cs Kim et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[15] X X X 3 en, ko, bg Nothman et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] X X X X 9 en, es, fr, de, ru, pl, pt, it, nl POLYGLOT-NER X X 40 en, es, fr, de, ru, pl, pt, it, nl ar, he, hi, zh, ko, ja, tl, ms, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "2 Related Work Wikipedia has been used as a resource for many tasks in NLP and IR [13, 19].", "startOffset": 82, "endOffset": 90}, {"referenceID": 17, "context": "2 Related Work Wikipedia has been used as a resource for many tasks in NLP and IR [13, 19].", "startOffset": 82, "endOffset": 90}, {"referenceID": 7, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 12, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 13, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 20, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 21, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 24, "context": "There is a body of literature regarding preprocessing Wikipedia for NER [9, 14, 15, 22, 23, 26] which is summarized in Table 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 20, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We follow the approach proposed by [7] to model NER as a word level classification problem.", "startOffset": 35, "endOffset": 38}, {"referenceID": 18, "context": "1 Word Embeddings capture semantic and syntactic characteristics of words through unsupervised learning [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 5, "context": "They have been successfully used as features for several tasks including NER [7, 27] and proposed as a cornerstone for developing multilingual applications [1].", "startOffset": 77, "endOffset": 84}, {"referenceID": 25, "context": "They have been successfully used as features for several tasks including NER [7, 27] and proposed as a cornerstone for developing multilingual applications [1].", "startOffset": 77, "endOffset": 84}, {"referenceID": 0, "context": "They have been successfully used as features for several tasks including NER [7, 27] and proposed as a cornerstone for developing multilingual applications [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "We use the Polyglot embeddings [1] as our sole features for each language under investigation2.", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "The Polyglot embeddings were trained using an objective function proposed by [6] which takes ordered sequences of words as its input.", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "As the stochastic optimization performance is dependent on a good choice of the learning rate, we automate the learning rate selection through an adaptive update procedure [8].", "startOffset": 172, "endOffset": 175}, {"referenceID": 22, "context": "Table 2 shows the performance of our annotators given CONLL training datasets [24, 25] and the word embeddings as features.", "startOffset": 78, "endOffset": 86}, {"referenceID": 23, "context": "Table 2 shows the performance of our annotators given CONLL training datasets [24, 25] and the word embeddings as features.", "startOffset": 78, "endOffset": 86}, {"referenceID": 26, "context": "Our results on English are similar to the ones reported by [28].", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "4 Extracting Entity Mentions Our procedure for creating a named entity training corpus from Wikipedia consists of two steps; First, we find which Wikipedia articles correspond to entities, using Freebase [4].", "startOffset": 204, "endOffset": 207}, {"referenceID": 8, "context": "In such a setting, [10, 17, 18] show that considering the unlabeled set as negative examples while modifying the objective loss to accommodate different penalties for misclassifying each set of examples outperforms other heuristics and other iterative EM-based methods.", "startOffset": 19, "endOffset": 31}, {"referenceID": 15, "context": "In such a setting, [10, 17, 18] show that considering the unlabeled set as negative examples while modifying the objective loss to accommodate different penalties for misclassifying each set of examples outperforms other heuristics and other iterative EM-based methods.", "startOffset": 19, "endOffset": 31}, {"referenceID": 16, "context": "In such a setting, [10, 17, 18] show that considering the unlabeled set as negative examples while modifying the objective loss to accommodate different penalties for misclassifying each set of examples outperforms other heuristics and other iterative EM-based methods.", "startOffset": 19, "endOffset": 31}, {"referenceID": 20, "context": "[22] 67.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] 61.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "However, [21] show that Wikipedia is better suited than human annotated datasets as a source of training for domain adaptation scenarios.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "Figure 3 shows the performance of our system compared to other annotators; OPENNLP {English, Spanish, Dutch} [2], NLTK English [3], and STANFORD German [11].", "startOffset": 127, "endOffset": 130}, {"referenceID": 9, "context": "Figure 3 shows the performance of our system compared to other annotators; OPENNLP {English, Spanish, Dutch} [2], NLTK English [3], and STANFORD German [11].", "startOffset": 152, "endOffset": 156}], "year": 2014, "abstractText": "The increasing diversity of languages used on the web introduces a new level of complexity to Information Retrieval (IR) systems. We can no longer assume that textual content is written in one language or even the same language family. In this paper, we demonstrate how to build massive multilingual annotators with minimal human expertise and intervention. We describe a system that builds Named Entity Recognition (NER) annotators for 40 major languages using Wikipedia and Freebase. Our approach does not require NER human annotated datasets or language specific resources like treebanks, parallel corpora, and orthographic rules. The novelty of approach lies therein using only language agnostic techniques, while achieving competitive performance. Our method learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language. Then, we automatically generate datasets from Wikipedia link structure and Freebase attributes. Finally, we apply two preprocessing stages (oversampling and exact surface form matching) which do not require any linguistic expertise. Our evaluation is two fold: First, we demonstrate the system performance on human annotated datasets. Second, for languages where no gold-standard benchmarks are available, we propose a new method, distant evaluation, based on statistical machine translation.", "creator": "LaTeX with hyperref package"}}}