{"id": "1410.7660", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2014", "title": "Non-convex Robust PCA", "abstract": "results propose a programming method stating performance ex - - convex procedure versus recovering a low - rank line from more parameter assume are of unknown parts than not. our method involves alternating rounds projecting appropriate operations onto the interval, constant - rank characters, and the set of sparse parts ; each bit is { \\ em non - convex } but easy too compute. in terms considering this non - intervention, users reject variable recovery and the correlation - coefficient operator, rendering least optimal matrix essentially come required by constraint methods ( which now measured on finite coordinates ). for an $ m \\ times n $ x mn ( $ ex \\ where n ) $, their method has the running time whereby $ o ( ct ^ 6 ) $ 21 iteration, whereby needs $ mn ( \\ x ( 1 / \\ epsilon ) ) $ j do keep execution accuracy of $ \\ 11 $. this optimal close mirror the survival time of simple pca including the power method, which requires $ o ( f ) $ per position, introduced $ r ( \\ log ( l / \\ \u025b ) ) $ rd. no contrast, fast methods for robust pca, which are bringing down convex optimization, introduce $ s ( m ^ 2n ) $ complexity per iteration, and take $ o ( 1 / \\ epsilon ) $ iterations, i. e., relatively more reliable for not same object.", "histories": [["v1", "Tue, 28 Oct 2014 15:33:13 GMT  (553kb,D)", "http://arxiv.org/abs/1410.7660v1", "Extended abstract to appear in NIPS 2014"]], "COMMENTS": "Extended abstract to appear in NIPS 2014", "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT stat.ML", "authors": ["praneeth netrapalli", "u n niranjan", "sujay sanghavi", "animashree anandkumar", "prateek jain 0002"], "accepted": true, "id": "1410.7660"}, "pdf": {"name": "1410.7660.pdf", "metadata": {"source": "CRF", "title": "Non-convex Robust PCA", "authors": ["Praneeth Netrapalli", "U N Niranjan", "Sujay Sanghavi", "Animashree Anandkumar", "Prateek Jain"], "emails": [], "sections": [{"heading": null, "text": "( r2mn ) per iteration,\nand needs O (log(1/ )) iterations to reach an accuracy of . This is close to the running times of simple PCA via the power method, which requires O (rmn) per iteration, and O (log(1/ )) iterations. In contrast, the existing methods for robust PCA, which are based on convex optimization, have O ( m2n ) complexity per iteration, and take O (1/ ) iterations, i.e., exponentially more iterations for the same accuracy. Experiments on both synthetic and real data establishes the improved speed and accuracy of our method over existing convex implementations.\nKeywords: Robust PCA, matrix decomposition, non-convex methods, alternating projections."}, {"heading": "1 Introduction", "text": "Principal component analysis (PCA) is a common procedure for preprocessing and denoising, where a low rank approximation to the input matrix (such as the covariance matrix) is carried out. Although PCA is simple to implement via eigen-decomposition, it is sensitive to the presence of outliers, since it attempts to \u201cforce fit\u201d the outliers to the low rank approximation. To overcome this, the notion of robust PCA is employed, where the goal is to remove sparse corruptions from an input matrix and obtain a low rank approximation. Robust PCA has been employed in a wide range of applications, including background modeling [LHGT04], 3d reconstruction [MZYM11], robust topic modeling [Shi13], and community detection [CSX12], and so on.\nConcretely, robust PCA refers to the following problem: given an input matrix M = L\u2217 + S\u2217, the goal is to decompose it into sparse S\u2217 and low rank L\u2217 matrices. The seminal works of [CSPW11, CLMW11] showed that this problem can be provably solved via convex relaxation methods, under some natural conditions on the low rank and sparse components. While the theory is elegant, in practice, convex techniques are expensive to run on a large scale and have poor convergence rates. Concretely, for decomposing an m\u00d7 n matrix, say with m \u2264 n, the best specialized implementations (typically first-order methods) have a per-iteration complexity of O ( m2n ) , and require O(1/ ) number of iterations to achieve an error of . In contrast, the usual PCA, which carries out a rank-r approximation of the input matrix, has O(rmn) complexity per iteration \u2013 drastically smaller when r is\n\u2217Microsoft Research, Cambridge MA. Email: praneeth@microsoft.com. Part of the work done while interning at Microsoft Research, India. \u2020The University of California at Irvine, CA. Email: un.niranjan@uci.edu. Part of the work done while interning at Microsoft Research, India. \u2021The University of Texas at Austin, TX. Email: sanghavi@mail.utexas.edu \u00a7The University of California at Irvine, CA. Email: a.anandkumar@uci.edu \u00b6Microsoft Research, Bangalore, India. Email: prajain@microsoft.com\nar X\niv :1\n41 0.\n76 60\nv1 [\ncs .I\nT ]\n2 8\nO ct\n2 01\n4\nmuch smaller than m,n. Moreover, PCA requires exponentially fewer iterations for convergence: an accuracy is achieved with only O (log(1/ )) iterations (assuming constant gap in singular values).\nIn this paper, we design a non-convex algorithm which is \u201cbest of both the worlds\u201d and bridges the gap between (the usual) PCA and convex methods for robust PCA. Our method has low computational complexity similar to PCA (i.e. scaling costs and convergence rates), and at the same time, has provable global convergence guarantees, similar to the convex methods. Proving global convergence for non-convex methods is an exciting recent development in machine learning. Non-convex alternating minimization techniques have recently shown success in many settings such as matrix completion [Kes12, JNS13, Har13], phase retrieval [NJS13], dictionary learning [AAJ+13], tensor decompositions for unsupervised learning [AGH+12], and so on. Our current work on the analysis of non-convex methods for robust PCA is an important addition to this growing list."}, {"heading": "1.1 Summary of Contributions", "text": "We propose a simple intuitive algorithm for robust PCA with low per-iteration cost and a fast convergence rate. We prove tight guarantees for recovery of sparse and low rank components, which match those for the convex methods. In the process, we derive novel matrix perturbation bounds, when subject to sparse perturbations. Our experiments reveal significant gains in terms of speed-ups over the convex relaxation techniques, especially as we scale the size of the input matrices.\nOur method consists of simple alternating (non-convex) projections onto low-rank and sparse matrices. For an m \u00d7 n matrix, our method has a running time of O(r2mn log(1/ )), where r is the rank of the low rank component. Thus, our method has a linear convergence rate, i.e. it requires O(log(1/ )) iterations to achieve an error of , where r is the rank of the low rank component L\u2217. When the rank r is small, this nearly matches the complexity of PCA, (which is O(rmn log(1/ ))).\nWe prove recovery of the sparse and low rank components under a set of requirements which are tight and match those for the convex techniques (up to constant factors). In particular, under the deterministic sparsity model, where each row and each column of the sparse matrix S\u2217 has at most \u03b1 fraction of non-zeros, we require that \u03b1 = O ( 1/(\u00b52r) ) , where \u00b5 is the incoherence factor (see Section 3).\nIn addition to strong theoretical guarantees, in practice, our method enjoys significant advantages over the state-of-art solver for (1), viz., the inexact augmented Lagrange multiplier (IALM) method [CLMW11]. Our method outperforms IALM in all instances, as we vary the sparsity levels, incoherence, and rank, in terms of running time to achieve a fixed level of accuracy. In addition, on a real dataset involving the standard task of foreground-background separation [CLMW11], our method is significantly faster and provides visually better separation.\nOverview of our techniques: Our proof technique involves establishing error contraction with each projection onto the sets of low rank and sparse matrices. We first describe the proof ideas when L\u2217 is rank one. The first projection step is a hard thresholding procedure on the input matrix M to remove large entries and then we perform rank-1 projection of the residual to obtain L(1). Standard matrix perturbation results (such as Davis-Kahan) provide `2 error bounds between the singular vectors of L\n(1) and L\u2217. However, these bounds do not suffice for establishing the correctness of our method. Since the next step in our method involves hard thresholding of the residual M \u2212L(1), we require element-wise error bounds on our low rank estimate. Inspired by the approach of Erdo\u030bs et al. [EKYY13], where they obtain similar element-wise bounds for the eigenvectors of sparse Erdo\u030bs\u2013Re\u0301nyi graphs, we derive these bounds by exploiting the fixed point characterization of the eigenvectors1. A Taylor\u2019s series expansion reveals that the perturbation between the estimated and the true eigenvectors consists of bounding the walks in a graph whose adjacency matrix corresponds to (a subgraph of) the sparse component S\u2217. We then show that if the graph is sparse enough, then this perturbation can be controlled, and thus, the next thresholding step results in further error contraction. We use an induction argument to show that the sparse estimate is always contained in the true support of S\u2217, and that there is an error contraction in each step. For the case, where L\u2217 has rank r > 1, our algorithm proceeds in several stages,\n1If the input matrix M is not symmetric, we embed it in a symmetric matrix and consider the eigenvectors of the corresponding matrix.\nwhere we progressively compute higher rank projections which alternate with the hard thresholding steps. In stage k = [1, 2, . . . , r], we compute rank-k projections, and show that after a sufficient number of alternating projections, we reduce the error to the level of (k + 1)th singular value of L\u2217, using similar arguments as in the rank-1 case. We then proceed to performing rank-(k + 1) projections which alternate with hard thresholding. This stage-wise procedure is needed for ill-conditioned matrices, since we cannot hope to recover lower eigenvectors in the beginning when there are large perturbations. Thus, we establish global convergence guarantees for our proposed non-convex robust PCA method."}, {"heading": "1.2 Related Work", "text": "Guaranteed methods for robust PCA have received a lot of attention in the past few years, starting from the seminal works of [CSPW11, CLMW11], where they showed recovery of an incoherent low rank matrix L\u2217 through the following convex relaxation method:\nConv-RPCA : min L,S \u2016L\u2016\u2217 + \u03bb\u2016S\u20161, s.t., M = L+ S, (1)\nwhere \u2016L\u2016\u2217 denotes the nuclear norm of L (nuclear norm is the sum of singular values). A typical solver for this convex program involves projection on to `1 and nuclear norm balls (which are convex sets). Note that the convex method can be viewed as \u201csoft\u201d thresholding in the standard and spectral domains, while our method involves hard thresholding in these domains.\n[CSPW11] and [CLMW11] consider two different models of sparsity for S\u2217. Chandrasekaran et al. [CSPW11] consider a deterministic sparsity model, where each row and column of the m \u00d7 n matrix, S, has at most \u03b1 fraction of non-zero entries. For guaranteed recovery, they require \u03b1 = O ( 1/(\u00b52r \u221a n) ) , where \u00b5 is the incoherence level of L\u2217, and r is its rank. Hsu et al. [HKZ11] improve upon this result to obtain guarantees for an optimal sparsity level of \u03b1 = O ( 1/(\u00b52r) ) . This matches the requirements of our non-convex method for exact recovery. Note that when the rank r = O(1), this allows for a constant fraction of corrupted entries. Cande\u0300s et al. [CLMW11] consider a different model with random sparsity and additional incoherence constraints, viz., they require \u2016UV >\u2016\u221e < \u00b5 \u221a r/n. Note that our assumption of incoherence, viz., \u2016U (i)\u2016 < \u00b5 \u221a r/n, only yields \u2016UV >\u2016\u221e < \u00b52r/n. The additional assumption enables [CLMW11] to prove exact recovery with a constant fraction of corrupted entries, even when L\u2217 is nearly full-rank. We note that removing the \u2016UV >\u2016\u221e condition for robust PCA would imply solving the planted clique problem when the clique size is less than \u221a n [Che13]. Thus, our recovery guarantees are tight upto constants without these additional assumptions.\nA number of works have considered modified models under the robust PCA framework, e.g. [ANW12, XCS12]. For instance, Agarwal et al. [ANW12] relax the incoherence assumption to a weaker \u201cdiffusivity\u201d assumption, which bounds the magnitude of the entries in the low rank part, but incurs an additional approximation error. Xu et al.[XCS12] impose special sparsity structure where a column can either be non-zero or fully zero.\nIn terms of state-of-art specialized solvers, [CLMW11] implements the in-exact augmented Lagrangian multipliers (IALM) method and provides guidelines for parameter tuning. Other related methods such as multi-block alternating directions method of multipliers (ADMM) have also been considered for robust PCA, e.g. [WHML13]. Recently, a multi-step multi-block stochastic ADMM method was analyzed for this problem [SAJ14], and this requires 1/ iterations to achieve an error of . In addition, the convergence rate is tight in terms of scaling with respect to problem size (m,n) and sparsity and rank parameters, under random noise models.\nThere is only one other work which considers a non-convex method for robust PCA [KC12]. However, their result holds only for significantly more restrictive settings and does not cover the deterministic sparsity assumption that we study. Moreover, the projection step in their method can have an arbitrarily large rank, so the running time is still O(m2n), which is the same as the convex methods. In contrast, we have an improved running time of O(r2mn)."}, {"heading": "2 Algorithm", "text": "In this section, we present our algorithm for the robust PCA problem. The robust PCA problem can be formulated as the following optimization problem: find L, S s.t. \u2016M \u2212 L\u2212 S\u2016F \u2264 2 and\n1. L lies in the set of low-rank matrices,\n2. S lies in the set of sparse matrices.\nA natural algorithm for the above problem is to iteratively project M \u2212 L onto the set of sparse matrices to update S, and then to project M \u2212S onto the set of low-rank matrices to update L. Alternatively, one can view the problem as that of finding a matrix L in the intersection of the following two sets: a) L = { set of rank-r matrices}, b) SM = {M \u2212 S, where S is a sparse matrix}. Note that these projections can be done efficiently, even though the sets are non-convex. Hard thresholding (HT) is employed for projections on to sparse matrices, and singular value decomposition (SVD) is used for projections on to low rank matrices. Rank-1 case: We first describe our algorithm for the special case when L\u2217 is rank 1. Our algorithm performs an initial hard thresholding to remove very large entries from input M . Note that if we performed the projection on to rank-1 matrices without the initial hard thresholding, we would not make any progress since it is subject to large perturbations. We alternate between computing the rank-1 projection of M \u2212 S, and performing hard thresholding on M \u2212 L to remove entries exceeding a certain threshold. This threshold is gradually decreased as the iterations proceed, and the algorithm is run for a certain number of iterations (which depends on the desired reconstruction error). General rank case: When L\u2217 has rank r > 1, a naive extension of our algorithm consists of alternating projections on to rank-r matrices and sparse matrices. However, such a method has poor performance on illconditioned matrices. This is because after the initial thresholding of the input matrix M , the sparse corruptions in the residual are of the order of the top singular value (with the choice of threshold as specified in the algorithm). When the lower singular values are much smaller, the corresponding singular vectors are subject to relatively large perturbations and thus, we cannot make progress in improving the reconstruction error. To alleviate the dependence on the condition number, we propose an algorithm that proceeds in stages. In the kth stage, the algorithm alternates between rank-k projections and hard thresholding for a certain number of iterations. We run the algorithm for r stages, where r is the rank of L\u2217. Intuitively, through this procedure, we recover the lower singular values only after the input matrix is sufficiently denoised, i.e. sparse corruptions at the desired level have been removed. Figure 1 shows a pictorial representation of the alternating projections in different stages.\nParameters: As can be seen, the only real parameter to the algorithm is \u03b2, used in thresholding, which represents \u201cspikiness\u201d of L\u2217. That is if the user expects L\u2217 to be \u201cspiky\u201d and the sparse part to be heavily diffused, then higher value of \u03b2 can be provided. In our implementation, we found that selecting \u03b2 aggressively helped speed up recovery of our algorithm. In particular, we selected \u03b2 = 1/ \u221a n.\n2 is the desired reconstruction error\nAlgorithm 1 (L\u0302, S\u0302) = AltProj(M, , r, \u03b2): Non-convex Alternating Projections based Robust PCA\n1: Input: Matrix M \u2208 Rm\u00d7n, convergence criterion , target rank r, thresholding parameter \u03b2. 2: Pk(A) denotes the best rank-k approximation of matrix A. HT\u03b6(A) denotes hard-thresholding, i.e.\n(HT\u03b6(A))ij = Aij if |Aij | \u2265 \u03b6 and 0 otherwise. 3: Set initial threshold \u03b60 \u2190 \u03b2\u03c31(M). 4: L(0) = 0, S(0) = HT\u03b60(M \u2212 L(0)) 5: for Stage k = 1 to r do 6: for Iteration t = 0 to T = 10 log ( n\u03b2 \u2225\u2225M \u2212 S(0)\u2225\u2225 2 / ) do 7: Set threshold \u03b6 as\n\u03b6 = \u03b2 ( \u03c3k+1(M \u2212 S(t)) + ( 1\n2\n)t \u03c3k(M \u2212 S(t)) ) (2)\n8: L(t+1) = Pk(M \u2212 S(t)) 9: S(t+1) = HT\u03b6(M \u2212 L(t+1))\n10: end for 11: if \u03b2\u03c3k+1(L (t+1)) < 2n then 12: Return: L(T ), S(T ) /* Return rank-k estimate if remaining part has small norm */ 13: else 14: S(0) = S(T ) /* Continue to the next stage */ 15: end if 16: end for 17: Return: L(T ), S(T )\nComplexity: The complexity of each iteration within a single stage is O(kmn), since it involves calculating the rank-k approximation3 of an m \u00d7 n matrix (done e.g. via vanilla PCA). The number of iterations in each stage is O (log (1/ )) and there are at most r stages. Thus the overall complexity of the entire algorithm is then O(r2mn log(1/ )). This is drastically lower than the best known bound of O ( m2n/ ) on the number of iterations required by convex methods, and just a factor r away from the complexity of vanilla PCA."}, {"heading": "3 Analysis", "text": "In this section, we present our main result on the correctness of AltProj. We assume the following conditions:\n(L1) Rank of L\u2217 is at most r. (L2) L\u2217 is \u00b5-incoherent, i.e., if L\u2217 = U\u2217\u03a3\u2217(V \u2217)> is the SVD of L\u2217, then \u2016(U\u2217)i\u20162 \u2264 \u00b5 \u221a r\u221a m , \u22001 \u2264 i \u2264 m and\n\u2016(V \u2217)i\u20162 \u2264 \u00b5 \u221a r\u221a n , \u22001 \u2264 i \u2264 n, where (U\u2217)i and (V \u2217)i denote the ith rows of U\u2217 and V \u2217 respectively.\n(S1) Each row and column of S have at most \u03b1 fraction of non-zero entries such that \u03b1 \u2264 1512\u00b52r .\nNote that in general, it is not possible to have a unique recovery of low-rank and sparse components. For example, if the input matrix M is both sparse and low rank, then there is no unique decomposition (e.g. M = e1e > 1 ). The above conditions ensure uniqueness of the matrix decomposition problem.\nAdditionally, we set the parameter \u03b2 in Algorithm 1 be set as \u03b2 = 4\u00b5 2r\u221a mn .\nWe now establish that our proposed algorithm recovers the low rank and sparse components under the above conditions.\n3Note that we only require a rank-k approximation of the matrix rather than the actual singular vectors. Thus, the computational complexity has no dependence on the gap between the singular values.\nTheorem 1 (Noiseless Recovery). Under conditions (L1), (L2) and S\u2217, and choice of \u03b2 as above, the outputs"}, {"heading": "L\u0302 and S\u0302 of Algorithm 1 satisfy:\u2225\u2225\u2225L\u0302\u2212 L\u2217\u2225\u2225\u2225", "text": "F \u2264 , \u2225\u2225\u2225S\u0302 \u2212 S\u2217\u2225\u2225\u2225 \u221e \u2264 \u221a mn , and Supp ( S\u0302 ) \u2286 Supp (S\u2217) .\nRemark (tight recovery conditions): Our result is tight up to constants, in terms of allowable sparsity level under the deterministic sparsity model. In other words, if we exceed the sparsity limit imposed in S1, it is possible to construct instances where there is no unique decomposition4. Our conditions L1, L2 and S1 also match the conditions required by the convex method for recovery, as established in [HKZ11]. Remark (convergence rate): Our method has a linear rate of convergence, i.e. O(log(1/ )) to achieve an error of , and hence we provide a strongly polynomial method for robust PCA. In contrast, the best known bound for convex methods for robust PCA is O(1/ ) iterations to converge to an -approximate solution.\nTheorem 1 provides recovery guarantees assuming that L\u2217 is exactly rank-r. However, in several real-world scenarios, L\u2217 can be nearly rank-r. Our algorithm can handle such situations, where M = L\u2217 + N\u2217 + S\u2217, with N\u2217 being an additive noise. Theorem 1 is a special case of the following theorem which provides recovery guarantees when N\u2217 has small `\u221e norm. Theorem 2 (Noisy Recovery). Under conditions (L1), (L2) and S\u2217, and choice of \u03b2 as in Theorem 1, when the noise \u2016N\u2217\u2016\u221e \u2264 \u03c3r(L\n\u2217) 100n ,the outputs L\u0302, S\u0302 of Algorithm 1 satisfy:\u2225\u2225\u2225L\u0302\u2212 L\u2217\u2225\u2225\u2225 F \u2264 + 2\u00b52r ( 7 \u2016N\u2217\u20162 + 8 \u221a mn\u221a r \u2016N\u2217\u2016\u221e ) ,\u2225\u2225\u2225S\u0302 \u2212 S\u2217\u2225\u2225\u2225\n\u221e \u2264 \u221a mn + 2\u00b52r\u221a mn ( 7 \u2016N\u2217\u20162 + 8 \u221a mn\u221a r \u2016N\u2217\u2016\u221e ) , and Supp ( S\u0302 ) \u2286 Supp (S\u2217) ."}, {"heading": "3.1 Proof Sketch", "text": "We now present the key steps in the proof of Theorem 1. A detailed proof is provided in the appendix.\nStep I: Reduce to the symmetric case, while maintaining incoherence of L\u2217 and sparsity of S\u2217. Using standard symmetrization arguments, we can reduce the problem to the symmetric case, where all the matrices involved are symmetric. See appendix for details on this step.\nStep II: Show decay in \u2016L\u2212L\u2217\u2016\u221e after projection onto the set of rank-k matrices. The t-th iterate L(t+1) of the k-th stage is given by L(t+1) = Pk(L\n\u2217 + S\u2217 \u2212 S(t)). Hence, L(t+1) is obtained by using the top principal components of a perturbation of L\u2217 given by L\u2217+ (S\u2217\u2212S(t)). The key step in our analysis is to show that when an incoherent and low-rank L\u2217 is perturbed by a sparse matrix S\u2217 \u2212 S(t), then \u2016L(t+1) \u2212 L\u2217\u2016\u221e is small and is much smaller than |S\u2217 \u2212 S(t)|\u221e. The following lemma formalizes the intuition; see the appendix for a detailed proof. Lemma 1. Let L\u2217, S\u2217 be symmetric and satisfy the assumptions of Theorem 1 and let S(t) and L(t) be the tth iterates of the kth stage of Algorithm 1. Let \u03c3\u22171 , . . . , \u03c3 \u2217 n be the eigenvalues of L\n\u2217, s.t., |\u03c3\u22171 | \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03c3\u2217r |. Then, the following holds:\n\u2225\u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r n (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ (12 )t |\u03c3\u2217k| ) , \u2225\u2225\u2225S\u2217 \u2212 S(t+1)\u2225\u2225\u2225 \u221e \u2264 8\u00b5 2r n (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ (12 )t |\u03c3\u2217k| ) , and Supp ( S(t+1) ) \u2286 Supp (S\u2217) .\n4For instance, consider the n\u00d7n matrix which has r copies of the all ones matrix, each of size n r\n, placed across the diagonal. We see that this matrix has rank r and is incoherent with parameter \u00b5 = 1. Note that a fraction of \u03b1 = O (1/r) sparse perturbations suffice to erase one of these blocks making it impossible to recover the matrix.\nMoreover, the outputs L\u0302 and S\u0302 of Algorithm 1 satisfy:\u2225\u2225\u2225L\u0302\u2212 L\u2217\u2225\u2225\u2225 F \u2264 , \u2225\u2225\u2225S\u0302 \u2212 S\u2217\u2225\u2225\u2225 \u221e \u2264 n , and Supp ( S\u0302 ) \u2286 Supp (S\u2217) .\nStep III: Show decay in \u2016S \u2212 S\u2217\u2016\u221e after projection onto the set of sparse matrices. We next show that if \u2016L(t+1)\u2212L\u2217\u2016\u221e is much smaller than \u2016S(t)\u2212S\u2217\u2016\u221e then the iterate S(t+1) also has a much smaller error (w.r.t. S\u2217) than S(t). The above given lemma formally provides the error bound.\nStep IV: Recurse the argument. We have now reduced the `\u221e norm of the sparse part by a factor of half, while maintaining its sparsity. We can now go back to steps II and III and repeat the arguments for subsequent iterations."}, {"heading": "4 Experiments", "text": "We now present an empirical study of our AltProj method. The goal of this study is two-fold: a) establish that our method indeed recovers the low-rank and sparse part exactly, without significant parameter tuning, b) demonstrate that AltProj is significantly faster than Conv-RPCA (see (1)); we solve Conv-RPCA using the IALM method [CLMW11], a state-of-the-art solver [LCM10]. We implemented our method in Matlab and used a Matlab implementation of the IALM method by [LCM10].\nWe consider both synthetic experiments and experiments on real data involving the problem of foregroundbackground separation in a video. Each of our results for synthetic datasets is averaged over 5 runs.\nParameter Setting: Our pseudo-code (Algorithm 1) prescribes the threshold \u03b6 in Step 4, which depends on the knowledge of the singular values of the low rank component L\u2217. Instead, in the experiments, we set the threshold at the (t + 1)-th step of k-th stage as \u03b6 = \u00b5\u03c3k+1(M\u2212S (t))\u221a\nn . For synthetic experiments, we employ the\n\u00b5 used for data generation, and for real-world datasets, we tune \u00b5 through cross-validation. We found that the above thresholding provides exact recovery while speeding up the computation significantly. We would also like to note that [CLMW11] sets the regularization parameter \u03bb in Conv-RPCA (1) as 1/ \u221a n (assuming m \u2264 n). However, we found that for problems with large incoherence such a parameter setting does not provide exact recovery. Instead, we set \u03bb = \u00b5/ \u221a n in our experiments.\nSynthetic datasets: Following the experimental setup of [CLMW11], the low-rank part L\u2217 = UV T is generated using normally distributed U \u2208 Rm\u00d7r, V \u2208 Rn\u00d7r. Similarly, supp(S\u2217) is generated by sampling a uniformly random subset of [m]\u00d7 [n] with size \u2016S\u2217\u20160 and each non-zero S\u2217ij is drawn i.i.d. from the uniform distribution over [r/(2 \u221a mn), r/ \u221a mn]. For increasing incoherence of L\u2217, we randomly zero-out rows of U, V and then renormalize them.\nThere are three key problem parameters for RPCA with a fixed matrix size: a) sparsity of S\u2217, b) incoherence of L\u2217, c) rank of L\u2217. We investigate performance of both AltProj and IALM by varying each of the three parameters while fixing the others. In our plots (see Figure 2), we report computational time required by each of the two methods for decomposing M into L + S up to a relative error (\u2016M \u2212 L \u2212 S\u2016F /\u2016M\u2016F ) of 10\u22123. Figure 2 shows that AltProj scales significantly better than IALM for increasingly dense S\u2217. We attribute this observation to the fact that as \u2016S\u2217\u20160 increases, the problem is \u201charder\u201d and the intermediate iterates of\nIALM have ranks significantly larger than r. Our intuition is confirmed by Figure 2 (b), which shows that when density (\u03b1) of S\u2217 is 0.4 then the intermediate iterates of IALM can be of rank over 500 while the rank of L\u2217 is only 5. We observe a similar trend for the other parameters, i.e., AltProj scales significantly better than IALM with increasing incoherence parameter \u00b5 (Figure 2 (c)) and increasing rank (Figure 2 (d)). See Appendix C for additional plots.\nReal-world datasets: Next, we apply our method to the problem of foreground-background (F-B) separation in a video [LHGT04]. The observed matrix M is formed by vectorizing each frame and stacking them columnwise. Intuitively, the background in a video is the static part and hence forms a low-rank component while the foreground is a dynamic but sparse perturbation.\nHere, we used two benchmark datasets named Escalator and Restaurant dataset. The Escalator dataset has 3417 frames at a resolution of 160 \u00d7 130. We first applied the standard PCA method for extracting low-rank part. Figure 3 (b) shows the extracted background from the video. There are several artifacts (shadows of people near the escalator) that are not desirable. In contrast, both IALM and AltProj obtain significantly better F-B separation (see Figure 3(c), (d)). Interestingly, AltProj removes the steps of the escalator which are moving and arguably are part of the dynamic foreground, while IALM keeps the steps in the background part. Also, our method is significantly faster, i.e., our method, which takes 63.2s is about 26 times faster than IALM, which takes 1688.9s.\nRestaurant dataset: Figure 4 shows the comparison of AltProj and IALM on a subset of the \u201cRestaurant\u201d dataset where we consider the last 2055 frames at a resolution of 120\u00d7 160. AltProj was around 19 times faster than IALM. Moreover, visually, the background extraction seems to be of better quality (for example, notice the blur near top corner counter in the IALM solution). Plot(b) shows the PCA solution and that also suffers from a similar blur at the top corner of the image, while the background frame extracted by AltProj does not have any noticeable artifacts."}, {"heading": "5 Conclusion", "text": "In this work, we proposed a non-convex method for robust PCA, which consists of alternating projections on to low rank and sparse matrices. We established global convergence of our method under conditions which match those for convex methods. At the same time, our method has much faster running times, and has superior experimental performance. This work opens up a number of interesting questions for future investigation. While we match the convex methods, under the deterministic sparsity model, studying the random sparsity model is of interest. Our noisy recovery results assume deterministic noise; improving the results under random noise needs to be investigated. There are many decomposition problems beyond the robust PCA setting, e.g. structured sparsity models, robust tensor PCA problem, and so on. It is interesting to see if we can establish global convergence for non-convex methods in these settings."}, {"heading": "Acknowledgements", "text": "AA and UN would like to acknowledge NSF grant CCF-1219234, ONR N00014-14-1-0665, and Microsoft faculty fellowship. SS would like to acknowledge NSF grants 1302435, 0954059, 1017525 and DTRA grant HDTRA113-1-0024. PJ would like to acknowledge Nikhil Srivastava and Deeparnab Chakrabarty for several insightful discussions during the course of the project."}, {"heading": "A Proof of Theorem 1", "text": "We will start with some preliminary lemmas. The first lemma is the well known Weyl\u2019s inequality in the matrix setting[Bha97]. Lemma 2. Suppose B = A+E be an n\u00d7n matrix. Let \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbn and \u03c31, \u00b7 \u00b7 \u00b7 , \u03c3n be the eigenvalues of B and A respectively such that \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbn and \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3n. Then we have:\n|\u03bbi \u2212 \u03c3i| \u2264 \u2016E\u20162 \u2200 i \u2208 [n].\nThe following lemma is the Davis-Kahan theorem[Bha97], specialized for rank-1 matrices. Lemma 3. Suppose B = A+E. Let A = u\u2217(u\u2217) >\nbe a rank-1 matrix with unit spectral norm. Suppose further that \u2016E\u20162 < 1 2 . Then, we have: |\u03bb\u2212 1| < \u2016E\u20162 , and\u2223\u2223\u2223\u3008u,u\u2217\u30092 \u2212 1\u2223\u2223\u2223 < 4 \u2016E\u20162 , where \u03bb and u are the top eigenvalue eigenvector pair of B.\nAs outlined in Section 3.1 (and formalized in the proof of Theorem 1), it is sufficient to prove the correctness of Algorithm 1 for the case of symmetric matrices. So, most of the lemmas we prove in this section assume that the matrices are symmetric. Lemma 4. Let S \u2208 Rn\u00d7n satisfy assumption (S1). Then, \u2016S\u20162 \u2264 \u03b1n \u2016S\u2016\u221e.\nProof of Lemma 4. Let x, y be unit vectors such that \u2016S\u20162 = xTSy = \u2211 ij xiyjSij . Then, using a\u00b7b \u2264 (a2+b2)/2, we have:\n\u2016S\u20162 \u2264 1\n2 \u2211 ij (x2i + y 2 j )Sij \u2264 1 2 (\u03b1n\u2016S\u2016\u221e + \u03b1n\u2016S\u2016\u221e), (3)\nwhere the last inequality follows from the fact that S has at most \u03b1n non-zeros per row and per column.\nLemma 5. Let S \u2208 Rn\u00d7n satisfy assumption (S1). Also, let U \u2208 Rn\u00d7r be a \u00b5-incoherent orthogonal matrix, i.e., maxi \u2225\u2225ei>U\u2225\u22252 \u2264 \u00b5\u221ar\u221an , where ei stands for the ith standard basis vector. Then, \u2200p \u2265 0, the following holds: max i\n\u2225\u2225ei>SpU\u2225\u22252 \u2264 \u00b5\u221ar\u221an (\u03b1 \u00b7 n \u00b7 \u2016S\u2016\u221e)p. Proof of Lemma 5. We prove the lemma using mathematical induction.\nBase Case (p = 0): This is just a restatement of the incoherence of U .\nInduction step: We have:\u2225\u2225ei>(S)p+1U\u2225\u222522 = \u2016ei>S(SpU)\u201622 = \u2211 ` (ei >S(SpU)e`) 2 = \u2211 ` ( \u2211 j Sijej >(SpU)e`) 2\n= \u2211 j1j2 Sij1Sij2 \u2211 ` (ej1 >(SpU)e`)(e` >(SpU) > ej2)\n\u03b61 \u2264 \u2211 j1j2 Sij1Sij2(ej1 >(SpU)(SpU) > ej2) \u2264 \u2211 j1j2 Sij1Sij2\u2016eTj1(S pU)\u20162\u2016ej2>(SpU)\u20162\n\u03b62 \u2264 \u00b5\n2r n (\u03b1 \u00b7 n \u00b7 \u2016S\u2016\u221e)2p,\nwhere \u03b61 follows by \u2211t `=1 e`e`\n> = I, and \u03b62 follows from assumption (S1) on S and from the inductive hypothesis on \u2225\u2225ei>SpU\u2225\u22252.\nIn what follows, we prove a number of lemmas concerning the structure of L(t) and E(t) := S\u2217 \u2212 S(t). The following lemma shows that the threshold in (2) is close to that with M \u2212 S(t) replaced by L\u2217. Lemma 6. Let L\u2217, S\u2217 be symmetric and satisfy the assumptions of Theorem 1 and let S(t) be the tth iterate of the kth stage of Algorithm 1. Let \u03c3\u22171 , . . . , \u03c3 \u2217 r be the eigenvalues of L\n\u2217, such that |\u03c3\u22171 | \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03c3\u2217r | and \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbn be the eigenvalues of M \u2212 S(t) such that |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbn|. Recall that E(t) := S\u2217 \u2212 S(t). Suppose further that\n1. \u2225\u2225E(t)\u2225\u2225\u221e \u2264 8\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t\u22121 |\u03c3\u2217k|), and\n2. Supp ( E(t) ) \u2286 Supp (S\u2217).\nThen,\n7\n8\n( |\u03c3\u2217k+1|+ ( 1\n2\n)t |\u03c3\u2217k| ) \u2264 ( |\u03bbk+1|+ ( 1\n2\n)t |\u03bbk| ) \u2264 9\n8\n( |\u03c3\u2217k+1|+ ( 1\n2\n)t |\u03c3\u2217k| ) . (4)\nProof. Note that M \u2212 S(t) = L\u2217 + E(t). Now, using Lemmas 2 and 4, we have:\u2223\u2223\u03bbk+1 \u2212 \u03c3\u2217k+1\u2223\u2223 \u2264 \u2225\u2225\u2225E(t)\u2225\u2225\u2225 2 \u2264 \u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225 \u221e \u2264 8\u00b52r\u03b1\u03b3t,\nwhere \u03b3t := ( |\u03c3\u2217k+1|+ ( 1 2 )t\u22121 |\u03c3\u2217k|). That is, \u2223\u2223|\u03bbk+1| \u2212 |\u03c3\u2217k+1|\u2223\u2223 \u2264 8\u00b52r\u03b1\u03b3t. Similarly, ||\u03bbk| \u2212 |\u03c3\u2217k|| \u2264 8\u00b52r\u03b1\u03b3t. So we have: \u2223\u2223\u2223\u2223\u2223 ( |\u03bbk+1|+ ( 1 2 )t |\u03bbk| ) \u2212 ( |\u03c3\u2217k+1|+ ( 1 2 )t |\u03c3\u2217k| )\u2223\u2223\u2223\u2223\u2223 \u2264 8\u00b52r\u03b1\u03b3t ( 1 + ( 1 2\n)t) \u2264 16\u00b52r\u03b1\u03b3t\n\u2264 1 8\n( |\u03c3\u2217k+1|+ ( 1\n2\n)t |\u03c3\u2217k| ) ,\nwhere the last inequality follows from the bound \u03b1 \u2264 1512\u00b52r .\nThe following lemma shows that under the same assumptions as in Lemma 6, we can obtain a bound on the `\u221e norm of L(t+1) \u2212 L\u2217. This is the most crucial step in our analysis since we bound `\u221e norm of errors which are quite hard to obtain. Lemma 7. Assume the notation of Lemma 6. Also, let L(t), S(t) be the tth iterates of kth stage of Algorithm 1 and L(t+1), S(t+1) be the (t + 1)th iterates of the same stage. Also, recall that E(t) := S\u2217 \u2212 S(t) and E(t+1) := S\u2217 \u2212 S(t+1). Suppose further that\n1. \u2225\u2225E(t)\u2225\u2225\u221e \u2264 8\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t\u22121 |\u03c3\u2217k|), and\n2. Supp ( E(t) ) \u2286 Supp (S\u2217).\nThen, we have: \u2225\u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r n ( |\u03c3\u2217k+1|+ ( 1 2 )t |\u03c3\u2217k| ) .\nProof. Let L(t+1) = Pk(M \u2212 S(t)) = U\u039bU> be the eigenvalue decomposition of L(t+1). Also, recall that\nM \u2212 S(t) = L\u2217 + E(t). Then, for every eigenvector ui of L(t+1), we have( L\u2217 + E(t) ) ui = \u03bbiui,(\nI \u2212 E (t)\n\u03bbi\n) ui = 1\n\u03bbi L\u2217ui,\nui =\n( I \u2212 E (t)\n\u03bbi )\u22121 L\u2217ui \u03bbi\n= ( I + E(t)\n\u03bbi +\n( E(t)\n\u03bbi\n)2 + . . . ) L\u2217ui \u03bbi . (5)\nNote that we used Lemmas 2 and 4 to guarantee the existence of ( I \u2212 E (t)\n\u03bbi\n)\u22121 . Hence,\nU\u039bU> \u2212 L\u2217 = ( L\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217 ) + \u2211 p+q\u22651 ( E(t) )p L\u2217U\u039b\u2212(p+q+1)U>L\u2217 ( E(t) )q .\nBy triangle inequality, we have\u2225\u2225U\u039bU> \u2212 L\u2217\u2225\u2225\u221e \u2264 \u2225\u2225L\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217\u2225\u2225\u221e + \u2211 p+q\u22651 \u2225\u2225\u2225(E(t))p L\u2217U\u039b\u2212(p+q+1)U>L\u2217 (E(t))q\u2225\u2225\u2225 \u221e . (6)\nWe now bound the two terms on the right hand side above.\nWe note that, \u2225\u2225L\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217\u2225\u2225\u221e = max ij ei > ( U\u2217\u03a3\u2217(U\u2217) > U\u039b\u22121U>U\u2217\u03a3\u2217(U\u2217) > \u2212 U\u2217\u03a3\u2217(U\u2217)> ) ej\n= max ij\nei >U\u2217 ( \u03a3\u2217(U\u2217) > U\u039b\u22121U>U\u2217\u03a3\u2217 \u2212 \u03a3\u2217 ) (U\u2217) > ej\n\u2264 max ij \u2016ei>U\u2217\u2016 \u00b7 \u2016ej>U\u2217\u2016 \u00b7 \u2016U\u2217\u03a3\u2217(U\u2217)>U\u039b\u22121U>U\u2217\u03a3\u2217(U\u2217)> \u2212 U\u2217\u03a3\u2217(U\u2217)>\u20162 \u2264 \u00b5 2r\nn \u2016L\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217\u20162, (7)\nwhere we denote U\u2217\u03a3\u2217(U\u2217) > to be the SVD of L\u2217. Let L\u2217 + E(t) = U\u039bU> + U\u0303 \u039b\u0303U\u0303> be the eigenvalue decomposition of L\u2217+E(t). Note that U\u0303>U = 0. Recall that, U\u039bU> = Pk(M \u2212S(t)) = Pk(L\u2217+E(t)) = L(t+1). Also note that,\nL\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217 = ( U\u039bU> + U\u0303 \u039b\u0303U\u0303> \u2212 E(t) ) U\u039b\u22121U> ( U\u039bU> + U\u0303 \u039b\u0303U\u0303T \u2212 E(t) ) \u2212 L\u2217,\n= ( UU> \u2212 ( E(t) ) U\u039b\u22121U> )( U\u039bU> + U\u0303 \u039b\u0303U\u0303T \u2212 E(t) ) \u2212 L\u2217,\n= \u2212UU>E(t) \u2212 E(t)UU> \u2212 E(t)U\u039b\u22121U>E(t) > \u2212 U\u0303 \u039b\u0303U\u0303> + E(t). (8)\nHence, using Lemma 8, we have:\n\u2016L\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217\u20162 \u2264 3\u2016E(t)\u20162 + \u2016E(t)\u201622 |\u03bbk| + |\u03bbk+1|\n\u2264 \u2223\u2223\u03c3\u2217k+1\u2223\u2223+ 5 \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n2 . (9)\nCombining (7) and (9), we have:\u2225\u2225L\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217\u2225\u2225\u221e \u2264 \u00b52rn (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ 5 \u2225\u2225\u2225E(t)\u2225\u2225\u22252) (10) Now, we will bound the (p, q)th term of \u2211 p+q\u22651 \u2225\u2225\u2225(E(t))p L\u2217U\u039b\u2212(p+q+1)U>L\u2217 (E(t))q\u2225\u2225\u2225 \u221e\n:\u2225\u2225\u2225(E(t))pL\u2217U\u039b\u2212(p+q+1)U>L\u2217(E(t))q\u2225\u2225\u2225 \u221e\n= max ij\nei > ( (E(t))pL\u2217U\u039b\u2212(p+q+1)U>L\u2217(E(t))q ) ej ,\n\u2264 max ij \u2225\u2225\u2225ei>(E(t))pU\u2217\u2225\u2225\u2225 2 \u2225\u2225\u2225ej>(E(t))qU\u2217\u2225\u2225\u2225 2 \u2225\u2225\u2225L\u2217U\u039b\u2212(p+q+1)U>L\u2217\u2225\u2225\u2225 2 ,\n\u03b61 \u2264 \u00b5\n2r\nn\n( \u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e\n)p ( \u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e )q \u2225\u2225\u2225L\u2217U\u039b\u2212(p+q+1)U>L\u2217\u2225\u2225\u2225 2 , (11)\nwhere \u03b61 follows from Lemma 5 and the incoherence of L \u2217. Now, similar to (8), we have:\u2225\u2225\u2225L\u2217U\u039b\u2212(p+q+1)U>L\u2217\u2225\u2225\u2225\n2 = \u2225\u2225\u2225U\u039b\u2212(p+q\u22121)U> \u2212 E(t)U\u039b\u2212(p+q)U> \u2212 U\u039b\u2212(p+q)U>E(t) + E(t)U\u039b\u2212(p+q+1)U>E(t)\u2225\u2225\u2225\n2 ,\n\u2264 \u2016\u039b\u2212(p+q\u22121)\u20162 + 2\u2016E(t)\u20162\u2016\u039b\u2212(p+q)\u20162 + \u2016E(t)\u201622\u2016\u039b\u2212(p+q+1)\u20162, \u2264 |\u03bbk|\u2212(p+q\u22121) (\n1 + 2 \u2016E(t)\u20162 |\u03bbk| + \u2016E(t)\u201622 \u03bb2k\n) = |\u03bbk|\u2212(p+q\u22121) ( 1 + \u2016E(t)\u20162 |\u03bbk| )2 ,\n\u2264 |\u03bbk|\u2212(p+q\u22121) ( 1 + \u2225\u2225E(t)\u2225\u2225 2\n|\u03bbk|\n)2 ,\n\u03b61 \u2264 |\u03bbk|\u2212(p+q\u22121) ( 1 +\n17\u00b52r\u03b1 |\u03c3\u2217k| (1\u2212 17\u00b52r\u03b1) |\u03c3\u2217k|\n)2 \u22642 |\u03bbk|\u2212(p+q\u22121) , (12)\nwhere \u03b61 follows from Lemma 8.\nUsing (11), (12), we have:\u2225\u2225\u2225(E(t))pL\u2217U\u039b\u2212(p+q+1)U>L\u2217(E(t))q\u2225\u2225\u2225 \u221e \u2264 2\u03b1\u00b52r \u2225\u2225\u2225E(t)\u2225\u2225\u2225 \u221e ( \u03b1n \u2225\u2225E(t)\u2225\u2225\u221e |\u03bbk| )p+q\u22121 . (13) Using the above bound, and the assumption on \u2225\u2225E(t)\u2225\u2225\u221e:\u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e \u2264 8\u00b5\n2r\nn (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ (12 )t\u22121 |\u03c3\u2217k| ) \u2264 17\u00b5 2r n |\u03c3\u2217k| ,\nwe have: \u2211 p+q\u22651 \u2225\u2225\u2225(E(t))p L\u2217U\u039b\u2212(p+q+1)U>L\u2217 (E(t))q\u2225\u2225\u2225 \u221e\n\u2264 2\u00b52r\u03b1 \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e \u2211 p+q\u22651 ( \u03b1n \u2225\u2225E(t)\u2225\u2225\u221e |\u03bbk| )p+q\u22121\n\u2264 2\u00b52r\u03b1 \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e\n( 1\n1\u2212 17\u00b52\u03b1r1\u221217\u00b52\u03b1\u00b7r\n)2\n\u2264 2\u00b52r\u03b1 \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e\n( 1\n1\u2212 34\u00b52r\u03b1 )2 \u2264 4\u00b52r\u03b1 \u2225\u2225\u2225E(t)\u2225\u2225\u2225 \u221e . (14)\nCombining (6), (10), (14), we have:\n\u2016U\u039bU> \u2212 L\u2217\u2016\u221e \u2264 \u00b52r\nn (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ 5 \u2225\u2225\u2225E(t)\u2225\u2225\u2225 2 + 4\u00b52r\u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225 \u221e ) \u2264 2\u00b5 2r\nn (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ (12 )t |\u03c3\u2217k| ) ,\nwhere we used Lemma 4 and the assumption on \u2225\u2225E(t)\u2225\u2225\u221e.\nWe used the following technical lemma in the proof of Lemma 7. Lemma 8. Assume the notation of Lemma 7. Suppose further that\n1. \u2225\u2225E(t)\u2225\u2225\u221e \u2264 8\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t\u22121 |\u03c3\u2217k|), and\n2. Supp ( E(t) ) \u2286 Supp (S\u2217).\nThen we have:\u2225\u2225\u2225E(t)\u2225\u2225\u2225 2 \u2264 17\u00b52r\u03b1|\u03c3\u2217k|, |\u03bbk| \u2265 |\u03c3\u2217k| (1\u2212 17\u00b52r\u03b1), and |\u03bbk+1| \u2264 |\u03c3\u2217k+1|+ \u2225\u2225\u2225E(t)\u2225\u2225\u2225 2 .\nProof. Using Lemmas 4 and 2, we have: |\u03bbi \u2212 \u03c3\u2217i | \u2264 \u2016E(t)\u20162 \u2264 \u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e .\nThe result follows by using the bound on \u2225\u2225E(t)\u2225\u2225\u221e.\nThe following lemma bounds the support of E(t+1) and \u2225\u2225E(t+1)\u2225\u2225\u221e, using an assumption on \u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u221e. Lemma 9. Assume the notation of Lemma 7. Suppose\n\u2225\u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r n ( |\u03c3\u2217k+1|+ ( 1 2 )t |\u03c3\u2217k| ) .\nThen, we have: 1. Supp ( E(t+1) ) \u2286 Supp (S\u2217).\n2. \u2225\u2225E(t+1)\u2225\u2225\u221e \u2264 7\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t |\u03c3\u2217k|), and\nProof. We first prove the first conclusion. Recall that,\nS(t+1) = H\u03b6(M \u2212 L(t+1)) = H\u03b6(L\u2217 \u2212 L(t+1) + S\u2217),\nwhere \u03b6 = 4\u00b5 2r n ( |\u03bbk+1|+ ( 1 2 )t |\u03bbk|) is as defined in Algorithm 1 and \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbn are the eigenvalues of M \u2212S(t) such that |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbn|.\nIf S\u2217ij = 0 then E (t+1) ij = 1 {\u2223\u2223\u2223L\u2217ij\u2212L(t+1)ij \u2223\u2223\u2223>\u03b6} \u00b7 (L\u2217ij \u2212 L(t+1)ij ). The first part of the lemma now follows by using the assumption that\n\u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u221e \u2264 2\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t |\u03c3\u2217k|) (\u03b61)\u2264 4\u00b52rn (|\u03bb\u2217k+1|+ ( 12)t |\u03bb\u2217k|) = \u03b6, where (\u03b61) follows from Lemma 6.\nWe now prove the second conclusion. We consider the following two cases: 1. \u2223\u2223\u2223Mij \u2212 L(t+1)ij \u2223\u2223\u2223 > \u03b6: Here, S(t+1)ij = S\u2217ij + L\u2217ij \u2212 L(t+1)ij . Hence, |S(t+1)ij \u2212 S\u2217ij | \u2264 |L\u2217ij \u2212 L(t+1)ij | \u2264 2\u00b52r n ( |\u03c3\u2217k+1|+ ( 1 2 )t |\u03c3\u2217k|).\n2. \u2223\u2223\u2223Mij \u2212 L(t+1)ij \u2223\u2223\u2223 \u2264 \u03b6: In this case, S(t+1)ij = 0 and \u2223\u2223\u2223S\u2217ij + L\u2217ij \u2212 L(t+1)ij \u2223\u2223\u2223 \u2264 \u03b6. So we have, \u2223\u2223\u2223E(t+1)ij \u2223\u2223\u2223 = \u2223\u2223S\u2217ij\u2223\u2223 \u2264 \u03b6 +\n\u2223\u2223\u2223L\u2217ij \u2212 L(t+1)ij \u2223\u2223\u2223 \u2264 7\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t |\u03c3\u2217k|). The last inequality above follows from Lemma 6. This proves the lemma.\nWe are now ready to prove Lemma 1. In fact, we prove the following stronger version.\nProof of Lemma 1. Recall that in the kth stage, the update L(t+1) is given by: L(t+1) = Pk(M \u2212 S(t)) and S(t+1) is given by: S(t+1) = H\u03b6(M \u2212 L(t+1)). Also, recall that E(t) := S\u2217 \u2212 S(t) and E(t+1) := S\u2217 \u2212 S(t+1).\nWe prove the lemma by induction on both k and t. For the base case (k = 1 and t = \u22121), we first note that the first inequality on \u2225\u2225L(0) \u2212 L\u2217\u2225\u2225\u221e is trivially satisfied. Due to the thresholding step (step 3 in Algorithm 1) and the incoherence assumption on L\u2217, we have:\u2225\u2225\u2225E(0)\u2225\u2225\u2225\n\u221e \u2264 8\u00b5\n2r\nn (\u03c3\u22172 + 2\u03c3 \u2217 1) , and Supp ( E(0) ) \u2286 Supp (S\u2217) .\nSo the base case of induction is satisfied. We first do the inductive step over t (for a fixed k). By inductive hypothesis we assume that: a) \u2225\u2225E(t)\u2225\u2225\u221e \u2264 8\u00b52r n ( |\u03c3\u2217k+1|+ ( 1 2\n)t\u22121 |\u03c3\u2217k|), b) Supp (E(t)) \u2286 Supp (S\u2217). Then by Lemma 7, we have: \u2225\u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u2225\n\u221e \u2264 2\u00b5\n2r\nn\n( |\u03c3\u2217k+1|+ ( 1\n2\n)t+1 |\u03c3\u2217k| ) .\nLemma 9 now tells us that 1. \u2225\u2225E(t+1)\u2225\u2225\u221e \u2264 8\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t |\u03c3\u2217k|), and\n2. Supp ( E(t+1) ) \u2286 Supp (S\u2217).\nThis finishes the induction over t. Note that we show a stronger bound than necessary on \u2225\u2225E(t+1)\u2225\u2225\u221e.\nWe now do the induction over k. Suppose the hypothesis holds for stage k. Let T denote the number of iterations in each stage. We first obtain a lower bound on T . Since\u2225\u2225\u2225M \u2212 S(0)\u2225\u2225\u2225\n2 \u2265 \u2016L\u2217\u20162 \u2212 \u2225\u2225\u2225E(0)\u2225\u2225\u2225 2 \u2265 |\u03c3\u22171 | \u2212 \u03b1n \u2225\u2225\u2225E(0)\u2225\u2225\u2225 \u221e \u2265 3 4 |\u03c3\u22171 | ,\nwe see that T \u2265 10 log ( 3\u00b52r |\u03c3\u22171 | / ) . So, at the end of stage k, we have:\n1. \u2225\u2225E(T )\u2225\u2225\u221e \u2264 7\u00b52rn (|\u03c3\u2217k+1|+ ( 12)T |\u03c3\u2217k|) \u2264 7\u00b52r|\u03c3\u2217k+1|n + 10n , and\n2. Supp ( E(T ) ) \u2286 Supp (S\u2217).\nLemmas 4 and 2 tell us that \u2223\u2223\u03c3k+1 (M \u2212 S(T ))\u2212 \u2223\u2223\u03c3\u2217k+1\u2223\u2223\u2223\u2223 \u2264 \u2225\u2225E(T )\u2225\u22252 \u2264 \u03b1 (7\u00b52r \u2223\u2223\u03c3\u2217k+1\u2223\u2223+ ). We will now consider two cases:\n1. Algorithm 1 terminates: This means that \u03b2\u03c3k+1 ( M \u2212 S(T ) ) < 2n which then implies that \u2223\u2223\u03c3\u2217k+1\u2223\u2223 <\n6\u00b52r . So we have: \u2225\u2225\u2225L\u0302\u2212 L\u2217\u2225\u2225\u2225 \u221e = \u2225\u2225\u2225L(T ) \u2212 L\u2217\u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r n ( |\u03c3\u2217k+1|+ ( 1 2 )T |\u03c3\u2217k| ) \u2264 5n . This proves the statement about L\u0302. A similar argument proves the claim on \u2225\u2225\u2225S\u0302 \u2212 S\u2217\u2225\u2225\u2225\n\u221e . The claim on Supp ( S\u0302 ) follows since Supp ( E(T ) ) \u2286 Supp (S\u2217).\n2. Algorithm 1 continues to stage (k+ 1): This means that \u03b2\u03c3k+1 ( L(T ) ) \u2265 2n which then implies that\u2223\u2223\u03c3\u2217k+1\u2223\u2223 > 8\u00b52r . So we have: \u2225\u2225\u2225E(T )\u2225\u2225\u2225\n\u221e \u2264 8\u00b5\n2r\nn\n( |\u03c3\u2217k+1|+ ( 1\n2\n)T |\u03c3\u2217k| )\n\u2264 8\u00b5 2r\nn\n( |\u03c3\u2217k+1|+\n10\u00b52rn ) \u2264 8\u00b5 2r\nn\n( |\u03c3\u2217k+1|+ 8 \u2223\u2223\u03c3\u2217k+1\u2223\u2223 10n )\n\u2264 8\u00b5 2r\nn\n( |\u03c3\u2217k+2|+ 2 \u2223\u2223\u03c3\u2217k+1\u2223\u2223) . Similarly for\n\u2225\u2225L(T ) \u2212 L\u2217\u2225\u2225\u221e. This finishes the proof.\nProof of Theorem 1. Using Lemma 1, it suffices to show that the general case can be reduced to the case of symmetric matrices. We will now outline this reduction.\nRecall that we are given an m \u00d7 n matrix M = L\u2217 + S\u2217 where L\u2217 is the true low-rank matrix and S\u2217 the sparse error matrix. Wlog, let m \u2264 n and suppose \u03b2m \u2264 n < (\u03b2 + 1)m, for some \u03b2 \u2265 1. We then consider the symmetric matrices\nM\u0303 =  0 ... 0 \u00b7 \u00b7 \u00b7 0 ... 0\nM> \u00b7 \u00b7 \u00b7M>\ufe38 \ufe37\ufe37 \ufe38 \u03b2 times\nM ... M 0\n , L\u0303 = \n0 ... 0 \u00b7 \u00b7 \u00b7 0 ... 0\n(L\u2217) > \u00b7 \u00b7 \u00b7 (L\u2217)>\ufe38 \ufe37\ufe37 \ufe38 \u03b2 times\nL\u2217\n... L\u2217\n0\n , (15)\nand S\u0303 = M\u0303\u2212L\u0303. A simple calculation shows that L\u0303 is incoherent with parameter \u221a\n3\u00b5 and S\u0303 satisfies the sparsity condition (S1) with parameter \u03b1\u221a\n2 . Moreover the iterates of AltProj with input M\u0303 have similar expressions\nas in (15) in terms of the corresponding iterates with input M . This means that it suffices to obtain the same guarantees for Algorithm 1 for the symmetric case. Lemma 1 does precisely this, proving the theorem."}, {"heading": "B Proof of Theorem 2", "text": "In this section, we prove Theorem 2. The roadmap of the proofs in this section is essentially the same as that in Appendix A.\nIn what follows, we prove a number of lemmas concerning the structure of L(t) and E(t) := S\u2217 \u2212 S(t). The first lemma is a generalization of Lemma 6 and shows that the threshold in (2) is close to that with M (t) replaced by L\u2217. Lemma 10. Let L\u2217, S\u2217, N\u2217 be symmetric and satisfy the assumptions of Theorem 2 and let S(t) be the tth iterate of the kth stage of Algorithm 1. Let \u03c3\u22171 , . . . , \u03c3 \u2217 r be the eigenvalues of L\n\u2217, such that |\u03c3\u22171 | \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03c3\u2217r | and \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbn be the eigenvalues of M \u2212 S(t) such that |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbn|. Recall that E(t) := S\u2217 \u2212 S(t). Suppose further that\n1. \u2225\u2225E(t)\u2225\u2225\u221e \u2264 8\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t\u22121 |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221ar \u2016N\u2217\u2016\u221e), and\n2. Supp ( S(t) ) \u2286 Supp (S\u2217).\nThen,\n7\n8\n( |\u03c3\u2217k+1|+ ( 1\n2\n)t |\u03c3\u2217k| ) \u2264 ( |\u03bbk+1|+ ( 1\n2\n)t |\u03bbk| ) \u2264 9\n8\n( |\u03c3\u2217k+1|+ ( 1\n2\n)t |\u03c3\u2217k| ) . (16)\nProof. Note that M \u2212 S(t) = L\u2217 +N\u2217 + E(t). Now, using Lemmas 2 and 4, we have:\u2223\u2223\u03bbk+1 \u2212 \u03c3\u2217k+1\u2223\u2223 \u2264 \u2225\u2225\u2225E(t)\u2225\u2225\u2225 2 \u2264 \u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225 \u221e \u2264 8\u00b52r\u03b1\u03b3t,\nwhere \u03b3t = ( |\u03c3\u2217k+1|+ ( 1 2 )t\u22121 |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221ar \u2016N\u2217\u2016\u221e). That is, \u2223\u2223|\u03bbk+1| \u2212 |\u03c3\u2217k+1|\u2223\u2223 \u2264 8\u00b52r\u03b1\u03b3t. Similarly, ||\u03bbk| \u2212 |\u03c3\u2217k|| \u2264 8\u00b52r\u03b1\u03b3t. So we have:\u2223\u2223\u2223\u2223\u2223 ( |\u03bbk+1|+ ( 1 2 )t |\u03bbk| ) \u2212 ( |\u03c3\u2217k+1|+ ( 1 2 )t |\u03c3\u2217k| )\u2223\u2223\u2223\u2223\u2223 \u2264 8\u00b52r\u03b1\u03b3t ( 1 + ( 1 2\n)t) \u2264 16\u00b52r\u03b1\u03b3t\n\u2264 1 8\n( |\u03c3\u2217k+1|+ ( 1\n2\n)t |\u03c3\u2217k| ) ,\nwhere the last inequality follows from the bound \u03b1 \u2264 1512\u00b52r and the assumption on \u2016N \u2217\u2016\u221e.\nThe following lemma shows that under the same assumptions as in Lemma 6, we can obtain a bound on the `\u221e norm of L(t+1) \u2212 L\u2217. This is the most crucial step in our analysis since we bound `\u221e norm of errors which are quite hard to obtain. Lemma 11. Assume the notation of Lemma 6. Also, let L(t), S(t) be the tth iterates of kth stage of Algorithm 1 and L(t+1), S(t+1) be the (t + 1)th iterates of the same stage. Also, recall that E(t) := S\u2217 \u2212 S(t) and E(t+1) := S\u2217 \u2212 S(t+1). Suppose further that\n1. \u2225\u2225E(t)\u2225\u2225\u221e \u2264 8\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t\u22121 |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221ar \u2016N\u2217\u2016\u221e), and\n2. Supp ( E(t) ) \u2286 Supp (S\u2217).\nThen, we have: \u2225\u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r n ( |\u03c3\u2217k+1|+ ( 1 2 )t |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221a r \u2016N\u2217\u2016\u221e ) .\nProof. Let L(t+1) = Pk(M \u2212 S(t)) = U\u039bU> be the eigenvalue decomposition of L(t+1). Also, recall that M \u2212 S(t) = L\u2217 +N\u2217 + E(t). Then, for every eigenvector ui of L(t+1), we have(\nL\u2217 +N\u2217 + E(t) ) ui = \u03bbiui,(\nI \u2212 E (t)\n\u03bbi\n) ui = 1\n\u03bbi (L\u2217 +N\u2217)ui,\nui =\n( I \u2212 E (t)\n\u03bbi\n)\u22121 (L\u2217 +N\u2217)ui\n\u03bbi\n= ( I + E(t)\n\u03bbi +\n( E(t)\n\u03bbi\n)2 + . . . ) (L\u2217 +N\u2217)ui\n\u03bbi .\nNote that we used Lemmas 2 and 4 to guarantee the existence of ( I \u2212 E (t)\n\u03bbi\n)\u22121 . Hence,\nU\u039bU> \u2212 L\u2217 = ((L\u2217 +N\u2217)U\u039b\u22121U> (L\u2217 +N\u2217)\u2212 L\u2217) + \u2211 p+q\u22651 (S(t))p (L\u2217 +N\u2217)U\u039b\u2212(p+q+1)U> (L\u2217 +N\u2217) (S(t))q.\nBy triangle inequality, we have\u2225\u2225U\u039bU> \u2212 L\u2217\u2225\u2225\u221e \u2264 \u2225\u2225(L\u2217 +N\u2217)U\u039b\u22121U> (L\u2217 +N\u2217)\u2212 L\u2217\u2225\u2225\u221e + \u2211 p+q\u22651 \u2225\u2225\u2225(S(t))p (L\u2217 +N\u2217)U\u039b\u2212(p+q+1)U> (L\u2217 +N\u2217) (S(t))q\u2225\u2225\u2225 \u221e . (17)\nWe now bound the two terms on the right hand side above.\nFor the first term, we again use triangle inequality to obtain\u2225\u2225(L\u2217 +N\u2217)U\u039b\u22121U> (L\u2217 +N\u2217)\u2212 L\u2217\u2225\u2225\u221e \u2264 \u2225\u2225L\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217\u2225\u2225\u221e + \u2225\u2225N\u2217U\u039b\u22121U>L\u2217\u2225\u2225\u221e + \u2225\u2225L\u2217U\u039b\u22121U>N\u2217\u2225\u2225\u221e + \u2225\u2225N\u2217U\u039b\u22121U>N\u2217\u2225\u2225\u221e . (18) We note that, \u2225\u2225L\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217\u2225\u2225\u221e = max ij ei > ( U\u2217\u03a3\u2217(U\u2217) > U\u039b\u22121U>U\u2217\u03a3\u2217(U\u2217) > \u2212 U\u2217\u03a3\u2217(U\u2217)> ) ej\n= max ij\nei >U\u2217 ( \u03a3\u2217(U\u2217) > U\u039b\u22121U>U\u2217\u03a3\u2217 \u2212 \u03a3\u2217 ) (U\u2217) > ej\n\u2264 max ij \u2016ei>U\u2217\u2016 \u00b7 \u2016ej>U\u2217\u2016 \u00b7 \u2016U\u2217\u03a3\u2217(U\u2217)>U\u039b\u22121U>U\u2217\u03a3\u2217(U\u2217)> \u2212 U\u2217\u03a3\u2217(U\u2217)>\u20162 \u2264 \u00b5 2r\nn \u2016L\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217\u20162, (19)\nwhere we denote U\u2217\u03a3\u2217(U\u2217) > to be the SVD of L\u2217. Let L\u2217 + N\u2217 + E(t) = U\u039bU> + U\u0303 \u039b\u0303U\u0303> be the eigenvalue decomposition of L\u2217 +N\u2217 +E(t). Note that U\u0303>U = 0. Recall that, U\u039bU> = Pk(M (t)) = L(t). Also note that,\nL\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217\n= (U\u039bU> + U\u0303 \u039b\u0303U\u0303> \u2212N\u2217 \u2212 E(t))U\u039b\u22121U>(U\u039bU> + U\u0303 \u039b\u0303U\u0303> \u2212N\u2217 \u2212 E(t))\u2212 L\u2217, = (UU> \u2212 ( N\u2217 + E(t) ) U\u039b\u22121U>)(U\u039bU> + U\u0303 \u039b\u0303U\u0303> \u2212N\u2217 \u2212 E(t))\u2212 L\u2217,\n= U\u039bU> \u2212 UU> ( N\u2217 + E(t) ) \u2212 ( N\u2217 + E(t) ) UU>\n\u2212 ( N\u2217 + E(t) ) U\u039b\u22121U> ( N\u2217 + E(t) )> \u2212 U\u039bU> \u2212 U\u0303 \u039b\u0303U\u0303> +N\u2217 + E(t). (20)\nHence, using Lemma 12, we have:\n\u2225\u2225L\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217\u2225\u2225 2 \u2264 3 \u2225\u2225\u2225N\u2217 + E(t)\u2225\u2225\u2225 2 + \u2225\u2225N\u2217 + E(t)\u2225\u22252 2 |\u03bbk| + |\u03bbk+1|\n\u2264 \u2223\u2223\u03c3\u2217k+1\u2223\u2223+ 4 \u2225\u2225\u2225N\u2217 + E(t)\u2225\u2225\u2225\n2 +\n\u2225\u2225N\u2217 + E(t)\u2225\u22252 2\n(1\u2212 17\u00b52r\u03b1) |\u03c3\u2217k| . (21)\nUsing (19), (21), and Lemma 12:\u2225\u2225L\u2217U\u039b\u22121U>L\u2217 \u2212 L\u2217\u2225\u2225\u221e \u2264 \u00b52rn (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ 7 \u2016N\u2217\u20162 + 5 \u2225\u2225\u2225E(t)\u2225\u2225\u22252) (22) Coming to the second term of (18), we have:\u2225\u2225N\u2217U\u039b\u22121U>L\u2217\u2225\u2225\u221e\n= max i,j\nei >N\u2217U\u039b\u22121U>L\u2217ej\n\u2264 max i \u2225\u2225ei>N\u2217U\u2225\u22252 \u2225\u2225\u039b\u22121U>U\u2217\u03a3\u2217\u2225\u22252 \u2225\u2225\u2225(U\u2217)>ej\u2225\u2225\u22252 \u2264 \u221a n \u2016N\u2217\u2016\u221e \u2225\u2225\u039b\u22121U>U\u2217\u03a3\u2217\u2225\u2225 2 \u00b5 \u221a r\u221a n = \u00b5 \u221a r \u2016N\u2217\u2016\u221e \u2225\u2225\u2225U\u039b\u22121U>U\u2217\u03a3\u2217(U\u2217)>\u2225\u2225\u2225 2 . (23)\nUsing an expansion along the lines of (20), we see that\n\u2225\u2225\u2225U\u039b\u22121U>U\u2217\u03a3\u2217(U\u2217)>\u2225\u2225\u2225 2 \u2264 1 + \u2225\u2225N\u2217 + E(t)\u2225\u2225 2 |\u03bbk| \u2264 1 + \u2016N\u2217\u20162 + \u2225\u2225E(t)\u2225\u2225 2 (1\u2212 17\u00b52r \u00b7 \u03b1) |\u03c3\u2217k|\n\u2264 2 + \u2225\u2225E(t)\u2225\u2225 2\n(1\u2212 17\u00b52r\u03b1) |\u03c3\u2217k| .\nPlugging this in (23) gives us \u2225\u2225N\u2217U\u039b\u22121U>L\u2217\u2225\u2225\u221e \u2264 3\u00b5\u221ar \u2016N\u2217\u2016\u221e . (24) A similar argument as in (23) gives us the following bound on the last term in (18): \u2225\u2225N\u2217U\u039b\u22121U>N\u2217\u2225\u2225\u221e \u2264 n \u2016N\u2217\u20162\u221e|\u03bbk| \u2264 \u2016N\u2217\u2016\u221e . (25) Plugging (22), (24) and (25), we obtain:\u2225\u2225(L\u2217 +N\u2217)U\u039b\u22121U> (L\u2217 +N\u2217)\u2212 L\u2217\u2225\u2225\u221e\n\u2264 \u00b5 2r\nn (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ 7 \u2016N\u2217\u20162 + 7 \u2225\u2225\u2225E(t)\u2225\u2225\u2225 2 + 7n\u221a r \u2016N\u2217\u2016\u221e ) . (26)\nNext, we analyze \u2211 p+q\u22651 \u2225\u2225(E(t))p(L\u2217 +N\u2217)U\u039b\u2212(p+q+1)U>(L\u2217 +N\u2217)(E(t))q\u2225\u2225\u221e. This can again be bounded by four quantities:\u2225\u2225\u2225(E(t))p(L\u2217 +N\u2217)U\u039b\u2212(p+q+1)U>(L\u2217 +N\u2217)(E(t))q\u2225\u2225\u2225\n\u221e \u2264 \u2225\u2225\u2225(E(t))pL\u2217U\u039b\u2212(p+q+1)U>L\u2217(E(t))q\u2225\u2225\u2225 \u221e + \u2225\u2225\u2225(E(t))pN\u2217U\u039b\u2212(p+q+1)U>L\u2217(E(t))q\u2225\u2225\u2225 \u221e (27)\n+ \u2225\u2225\u2225(E(t))pL\u2217U\u039b\u2212(p+q+1)U>N\u2217(E(t))q\u2225\u2225\u2225 \u221e + \u2225\u2225\u2225(E(t))pN\u2217U\u039b\u2212(p+q+1)U>N\u2217(E(t))q\u2225\u2225\u2225 \u221e . (28)\nWe bound the first term above:\u2225\u2225\u2225(E(t))pL\u2217U\u039b\u2212(p+q+1)U>L\u2217(E(t))q\u2225\u2225\u2225 \u221e\n= max ij\nei > ( (E(t))pL\u2217U\u039b\u2212(p+q+1)U>L\u2217(E(t))q ) ej ,\n\u2264 max ij \u2225\u2225\u2225ei>(E(t))pU\u2217\u2225\u2225\u2225 2 \u2225\u2225\u2225ej>(E(t))qU\u2217\u2225\u2225\u2225 2 \u2225\u2225\u2225L\u2217U\u039b\u2212(p+q+1)U>L\u2217\u2225\u2225\u2225 2 ,\n(\u03b61) \u2264 \u00b5 2r\nn\n( \u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e\n)p ( \u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e )q \u2225\u2225\u2225L\u2217U\u039b\u2212(p+q+1)U>L\u2217\u2225\u2225\u2225 2 , (29)\nwhere (\u03b61) follows from Lemma 5 and the incoherence of L \u2217. Now, similar to (20), we have:\u2225\u2225\u2225L\u2217U\u039b\u2212(p+q+1)U>L\u2217\u2225\u2225\u2225\n2 = \u2225\u2225\u2225U\u039b\u2212(p+q\u22121)U> \u2212 (N\u2217 + E(t))U\u039b\u2212(p+q)U> \u2212 U\u039b\u2212(p+q)U> (N\u2217 + E(t))\n+ ( N\u2217 + E(t) ) U\u039b\u2212(p+q+1)U> ( N\u2217 + E(t) )\u2225\u2225\u2225 2 ,\n\u2264 \u2016\u039b\u2212(p+q\u22121)\u20162 + 2\u2016N\u2217 + E(t)\u20162\u2016\u039b\u2212(p+q)\u20162 + \u2016N\u2217 + E(t)\u201622\u2016\u039b\u2212(p+q+1)\u20162, \u2264 |\u03bbk|\u2212(p+q\u22121) ( 1 + 2 \u2016N\u2217 + E(t)\u20162\n|\u03bbk| + \u2016N\u2217 + E(t)\u201622 \u03bb2k\n) ,\n= |\u03bbk|\u2212(p+q\u22121) ( 1 + \u2016N\u2217 + E(t)\u20162\n|\u03bbk|\n)2 ,\n\u2264 |\u03bbk|\u2212(p+q\u22121) ( 1 + \u2016N\u2217\u20162 + \u2225\u2225E(t)\u2225\u2225 2\n|\u03bbk|\n)2 ,\n(\u03b61) \u2264 |\u03bbk|\u2212(p+q\u22121) ( 1 + \u2016N\u2217\u20162 + 17\u00b52r\u03b1 |\u03c3\u2217k|\n(1\u2212 17\u00b52r\u03b1) |\u03c3\u2217k|\n)2 \u22642 |\u03bbk|\u2212(p+q\u22121) , (30)\nwhere (\u03b61) follows from Lemma 12 and the bound on \u2016N\u2217\u2016\u221e.\nUsing (29), (30), we have:\n\u2225\u2225\u2225(E(t))pL\u2217U\u039b\u2212(p+q+1)U>L\u2217(E(t))q\u2225\u2225\u2225 \u221e \u2264 2\u03b1\u00b52r \u2225\u2225\u2225E(t)\u2225\u2225\u2225 \u221e ( \u03b1n \u2225\u2225E(t)\u2225\u2225\u221e |\u03bbk| )p+q\u22121 . (31)\nComing to the second term of (28), we have\u2225\u2225\u2225(E(t))pN\u2217U\u039b\u2212(p+q+1)U>L\u2217(E(t))q\u2225\u2225\u2225 \u221e\n= max i,j\nei > ( (E(t))pN\u2217U\u039b\u2212(p+q+1)U>L\u2217(E(t))q ) ej ,\n\u2264 max ij \u2225\u2225\u2225ei>(E(t))pN\u2217U\u2225\u2225\u2225 2 \u2225\u2225\u2225ej>(E(t))qU\u2217\u2225\u2225\u2225 2 \u2225\u2225\u2225\u039b\u2212(p+q+1)U>L\u2217\u2225\u2225\u2225 2\n(\u03b61) \u2264 \u00b5 \u221a r\u221a n \u2016N\u2217U\u2016\u221e\n( \u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e\n)p ( \u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e )q \u2225\u2225\u2225U\u039b\u2212(p+q+1)U>L\u2217\u2225\u2225\u2225 2\n\u2264 \u00b5 \u221a r \u2016N\u2217\u2016\u221e ( \u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e )p+q \u2225\u2225\u2225U\u039b\u2212(p+q+1)U>L\u2217\u2225\u2225\u2225 2 , (32)\nwhere (\u03b61) follows from Lemma 5 and incoherence of U \u2217. Proceeding along the lines of (30), we obtain:\n\u2225\u2225\u2225U\u039b\u2212(p+q+1)U>L\u2217\u2225\u2225\u2225 2 \u2264 |\u03bbk|\u2212(p+q) ( 1 + \u2016N\u2217\u20162 + \u2225\u2225E(t)\u2225\u2225 2 |\u03bbk| ) \u22642 |\u03bbk|\u2212(p+q) .\nPlugging the above in (32) gives us\n\u2225\u2225\u2225(E(t))pN\u2217U\u039b\u2212(p+q+1)U>L\u2217(E(t))q\u2225\u2225\u2225 \u221e \u2264 2\u00b5 \u221a r ( \u03b1n \u2225\u2225E(t)\u2225\u2225\u221e |\u03bbk| )p+q \u2016N\u2217\u2016\u221e . (33)\nA similar argument as in (32) gives us\n\u2225\u2225\u2225(E(t))pN\u2217U\u039b\u2212(p+q+1)U>N\u2217(E(t))q\u2225\u2225\u2225 \u221e \u2264 n \u2016N\u2217\u2016\u221e |\u03bbk| ( \u03b1n \u2225\u2225E(t)\u2225\u2225\u221e |\u03bbk| )p+q \u2016N\u2217\u2016\u221e .\nPlugging the above inequality along with (31) and (33) into (28) gives us:\u2225\u2225\u2225(E(t))p(L\u2217 +N\u2217)U\u039b\u2212(p+q+1)U>(L\u2217 +N\u2217)(E(t))q\u2225\u2225\u2225 \u221e\n\u2264 2\u00b52r ( \u03b1 \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e + \u2016N\u2217\u2016\u221e\u221a r )( \u03b1n \u2225\u2225E(t)\u2225\u2225\u221e |\u03bbk| )p+q\u22121 .\nUsing the above bound, and the assumption on \u2225\u2225E(t)\u2225\u2225\u221e:\u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e \u2264 8\u00b5\n2r\nn (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ (12 )t\u22121 |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221a r \u2016N\u2217\u2016\u221e ) \u2264 17\u00b5 2r n |\u03c3\u2217k| ,\nwe have: \u2211 p+q\u22651 \u2225\u2225\u2225(E(t))p (L\u2217 +N\u2217)U\u039b\u2212(p+q+1)U> (L\u2217 +N\u2217) (E(t))q\u2225\u2225\u2225 \u221e\n(34)\n\u2264 2\u00b52r ( \u03b1 \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e + \u2016N\u2217\u2016\u221e\u221a r ) \u2211 p+q\u22651 ( \u03b1n \u2225\u2225E(t)\u2225\u2225\u221e |\u03bbk| )p+q\u22121\n\u2264 2\u00b52r ( \u03b1 \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e + \u2016N\u2217\u2016\u221e\u221a r\n)( 1\n1\u2212 17\u00b52\u03b1r1\u221217\u00b52\u03b1\u00b7r\n)2\n\u2264 2\u00b52r ( \u03b1 \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e + \u2016N\u2217\u2016\u221e\u221a r\n)( 1\n1\u2212 34\u00b52r\u03b1 )2 \u2264 4\u00b52r ( \u03b1 \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n\u221e + \u2016N\u2217\u2016\u221e\u221a r\n) . (35)\nCombining (17), (26), (35), we have:\u2225\u2225U\u039bU> \u2212 L\u2217\u2225\u2225\u221e (\u03b61)\u2264 \u00b52rn (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ 7 \u2016N\u2217\u20162 + 11n\u03b1 \u2225\u2225\u2225E(t)\u2225\u2225\u2225\u221e + 11n\u221ar \u2016N\u2217\u2016\u221e ) (\u03b62)\n\u2264 2\u00b5 2r\nn (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ (12 )t |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221a r \u2016N\u2217\u2016\u221e ) ,\nwhere (\u03b61) follows from Lemma 4, and (\u03b62) follows from the assumption on \u2225\u2225E(t)\u2225\u2225\u221e.\nWe used the following technical lemma in the proof of Lemma 11. Lemma 12. Assume the notation of Lemma 11. Suppose further that\n1. \u2225\u2225E(t)\u2225\u2225\u221e \u2264 8\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t\u22121 |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221ar \u2016N\u2217\u2016\u221e), and\n2. Supp ( E(t) ) \u2286 Supp (S\u2217).\nThen we have:\u2225\u2225\u2225E(t)\u2225\u2225\u2225 2 \u2264 17\u00b52r\u03b1|\u03c3\u2217k|, |\u03bbk| \u2265 |\u03c3\u2217k| (1\u2212 17\u00b52r\u03b1), and |\u03bbk+1| \u2264 |\u03c3\u2217k+1|+ \u2225\u2225\u2225E(t)\u2225\u2225\u2225 2 .\nProof. Using Lemmas 4 and 2, we have: |\u03bbi \u2212 \u03c3\u2217i | \u2264 \u2225\u2225\u2225E(t)\u2225\u2225\u2225\n2 \u2264 \u03b1n \u2225\u2225\u2225E(t)\u2225\u2225\u2225 \u221e .\nUsing the bound on \u2225\u2225E(t)\u2225\u2225\u221e and recalling the assumption that\n\u2016N\u2217\u2016\u221e \u2264 |\u03c3\u2217r | 100\nfinishes the proof.\nThe following lemma bounds the support of E(t+1) and \u2225\u2225E(t+1)\u2225\u2225\u221e, using an assumption on \u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u221e. Lemma 13. Assume the notation of Lemma 11. Suppose\u2225\u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r n ( |\u03c3\u2217k+1|+ ( 1 2 )t |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221a r \u2016N\u2217\u2016\u221e ) .\nThen, we have: 1. Supp ( E(t+1) ) \u2286 Supp (S\u2217).\n2. \u2225\u2225E(t+1)\u2225\u2225\u221e \u2264 7\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221ar \u2016N\u2217\u2016\u221e), and\nProof. We first prove the first conclusion. Recall that,\nS(t+1) = H\u03b6(M \u2212 L(t+1)) = H\u03b6(L\u2217 \u2212 L(t+1) +N\u2217 + S\u2217),\nwhere \u03b6 = 4\u00b5 2r n ( |\u03bbk+1|+ ( 1 2 )t |\u03bbk|) is as defined in Algorithm 1 and \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbn are the eigenvalues of M \u2212S(t) such that |\u03bb1| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03bbn|.\nIf S\u2217ij = 0 then E (t+1) ij = 1 {\u2223\u2223\u2223L\u2217ij\u2212L(t+1)ij +N\u2217ij\u2223\u2223\u2223>\u03b6} \u00b7 (L\u2217ij \u2212 L(t+1)ij + N\u2217ij). The first part of the lemma now follows by using the assumption that\n\u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u221e \u2264 2\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t |\u03c3\u2217k|) (\u03b61)\u2264 9\u00b52r4n (|\u03bb\u2217k+1|+ ( 12)t |\u03bb\u2217k|) = \u03b6, where (\u03b61) follows from Lemma 6, and the bound on \u2016N\u2217\u2016\u221e.\nWe now prove the second conclusion. We consider the following two cases: 1. \u2223\u2223\u2223Mij \u2212 L(t+1)ij \u2223\u2223\u2223 > \u03b6: Here, S(t+1)ij = S\u2217ij +L\u2217ij\u2212L(t+1)ij +N\u2217ij . Hence, |S(t+1)ij \u2212S\u2217ij | \u2264 |L\u2217ij\u2212L(t+1)ij |+ \u2223\u2223N\u2217ij\u2223\u2223 \u2264 2\u00b52r n ( |\u03c3\u2217k+1|+ ( 1 2\n)t |\u03c3\u2217k|)+ \u2016N\u2217\u2016\u221e. 2. \u2223\u2223\u2223Mij \u2212 L(t+1)ij \u2223\u2223\u2223 \u2264 \u03b6: In this case, S(t+1)ij = 0 and \u2223\u2223\u2223S\u2217ij + L\u2217ij \u2212 L(t+1)ij +N\u2217ij\u2223\u2223\u2223 \u2264 \u03b6. So we have, \u2223\u2223\u2223E(t+1)ij \u2223\u2223\u2223 =\u2223\u2223S\u2217ij\u2223\u2223 \u2264 \u03b6 + \u2223\u2223\u2223L\u2217ij \u2212 L(t+1)ij \u2223\u2223\u2223+ \u2223\u2223N\u2217ij\u2223\u2223 \u2264 7\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t |\u03c3\u2217k|)+ \u2016N\u2217\u2016\u221e. The last inequality above follows from Lemma 6.\nThis proves the lemma.\nThe following lemma is a generalization of Lemma 1. Lemma 14. Let L\u2217, S\u2217, N\u2217 be symmetric and satisfy the assumptions of Theorem 2 and let M (t) and L(t) be the tth iterates of the kth stage of Algorithm 1. Let \u03c3\u22171 , . . . , \u03c3 \u2217 n be the eigenvalues of L\n\u2217, s.t., |\u03c3\u22171 | \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03c3\u2217r |. Then, the following holds:\u2225\u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u2225\n\u221e \u2264 2\u00b5\n2r\nn (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ (12 )t |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221a r \u2016N\u2217\u2016\u221e ) ,\n\u2225\u2225\u2225E(t+1)\u2225\u2225\u2225 \u221e = \u2225\u2225\u2225S\u2217 \u2212 S(t+1)\u2225\u2225\u2225 \u221e \u2264 8\u00b5 2r n (\u2223\u2223\u03c3\u2217k+1\u2223\u2223+ (12 )t\u22121 |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221a r \u2016N\u2217\u2016\u221e ) , and\nSupp ( E(t+1) ) \u2286 Supp (S\u2217) .\nMoreover, the outputs L\u0302 and S\u0302 of Algorithm 1 satisfy:\u2225\u2225\u2225L\u0302\u2212 L\u2217\u2225\u2225\u2225 F \u2264 + 2\u00b52r ( 7 \u2016N\u2217\u20162 + 8n\u221a r \u2016N\u2217\u2016\u221e ) ,\u2225\u2225\u2225S\u0302 \u2212 S\u2217\u2225\u2225\u2225\n\u221e \u2264 n\n+ 8\u00b52r\nn\n( 7 \u2016N\u2217\u20162 +\n8n\u221a r \u2016N\u2217\u2016\u221e\n) , and\nSupp ( S\u0302 ) \u2286 Supp (S\u2217) .\nProof. Recall that in the kth stage, the update L(t+1) is given by: L(t+1) = Pk(M \u2212 S(t)) and S(t+1) is given by: S(t+1) = H\u03b6(M \u2212 L(t+1)). Also, recall that E(t) := S\u2217 \u2212 S(t) and E(t+1) := S\u2217 \u2212 S(t+1).\nWe prove the lemma by induction on both k and t. For the base case (k = 1 and t = \u22121), we first note that the first inequality on \u2225\u2225L(0) \u2212 L\u2217\u2225\u2225\u221e is trivially satisfied. Due to the thresholding step (step 3 in Algorithm 1) and the incoherence assumption on L\u2217, we have:\u2225\u2225\u2225E(0)\u2225\u2225\u2225\n\u221e \u2264 8\u00b5\n2r\nn (\u03c3\u22172 + 2\u03c3 \u2217 1) , and Supp ( E(0) ) \u2286 Supp (S\u2217) .\nSo the base case of induction is satisfied. We first do the inductive step over t (for a fixed k). By inductive hypothesis we assume that: a) \u2225\u2225E(t)\u2225\u2225\u221e \u2264 8\u00b52r n ( |\u03c3\u2217k+1|+ ( 1 2\n)t\u22121 |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221ar \u2016N\u2217\u2016\u221e), b) Supp (E(t)) \u2286 Supp (S\u2217). Then by Lemma 11, we have: \u2225\u2225\u2225L(t+1) \u2212 L\u2217\u2225\u2225\u2225\n\u221e \u2264 2\u00b5\n2r\nn\n( |\u03c3\u2217k+1|+ ( 1\n2\n)t |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 +\n8n\u221a r \u2016N\u2217\u2016\u221e\n) .\nLemma 13 now tells us that 1. \u2225\u2225E(t+1)\u2225\u2225\u221e \u2264 7\u00b52rn (|\u03c3\u2217k+1|+ ( 12)t |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221ar \u2016N\u2217\u2016\u221e), and\n2. Supp ( E(t+1) ) \u2286 Supp (S\u2217).\nThis finishes the induction over t. Note that we show a stronger bound than necessary on \u2225\u2225E(t+1)\u2225\u2225\u221e.\nWe now do the induction over k. Suppose the hypothesis holds for stage k. Let T denote the number of iterations in each stage. We first obtain a lower bound on T . Since\u2225\u2225\u2225M \u2212 S(0)\u2225\u2225\u2225\n2 \u2265 \u2016L\u2217 +N\u2217\u20162 \u2212 \u2225\u2225\u2225E(0)\u2225\u2225\u2225 2 \u2265 |\u03c3\u22171 | \u2212 \u03b1n \u2225\u2225\u2225E(0)\u2225\u2225\u2225 \u221e \u2265 3 4 |\u03c3\u22171 | ,\nwe see that T \u2265 10 log ( 3\u00b52r |\u03c3\u22171 | / ) . So, at the end of stage k, we have:\n1. \u2225\u2225E(T )\u2225\u2225\u221e \u2264 7\u00b52rn (|\u03c3\u2217k+1|+ ( 12)T |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221ar \u2016N\u2217\u2016\u221e) \u2264 7\u00b52r|\u03c3\u2217k+1|n + 10n , and\n2. Supp ( E(T ) ) \u2286 Supp (S\u2217).\nLemmas 4 and 2 tell us that \u2223\u2223\u03c3k+1 (M \u2212 S(T ))\u2212 \u2223\u2223\u03c3\u2217k+1\u2223\u2223\u2223\u2223 \u2264 \u2225\u2225E(T )\u2225\u22252 \u2264 \u03b1 (7\u00b52r \u2223\u2223\u03c3\u2217k+1\u2223\u2223+ ). We will now consider two cases:\n1. Algorithm 1 terminates: This means that \u03b2\u03c3k+1 ( M \u2212 S(T ) ) < 2n which then implies that \u2223\u2223\u03c3\u2217k+1\u2223\u2223 <\n6\u00b52r . So we have:\u2225\u2225\u2225L\u0302\u2212 L\u2217\u2225\u2225\u2225 \u221e = \u2225\u2225\u2225L(T ) \u2212 L\u2217\u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r n ( |\u03c3\u2217k+1|+ ( 1 2 )T |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 + 8n\u221a r \u2016N\u2217\u2016\u221e )\n\u2264 5n\n+ 2\u00b52r\nn\n( 7 \u2016N\u2217\u20162 +\n8n\u221a r \u2016N\u2217\u2016\u221e\n) .\nThis proves the statement about L\u0302. A similar argument proves the claim on \u2225\u2225\u2225S\u0302 \u2212 S\u2217\u2225\u2225\u2225\n\u221e . The claim on Supp ( S\u0302 ) follows since Supp ( E(T ) ) \u2286 Supp (S\u2217).\n2. Algorithm 1 continues to stage (k+ 1): This means that \u03b2\u03c3k+1 ( L(T ) ) \u2265 2n which then implies that\u2223\u2223\u03c3\u2217k+1\u2223\u2223 > 8\u00b52r . So we have:\u2225\u2225\u2225E(T )\u2225\u2225\u2225\n\u221e \u2264 7\u00b5\n2r\nn\n( |\u03c3\u2217k+1|+ ( 1\n2\n)T |\u03c3\u2217k|+ 7 \u2016N\u2217\u20162 +\n8n\u221a r \u2016N\u2217\u2016\u221e\n)\n\u2264 7\u00b5 2r\nn\n( |\u03c3\u2217k+1|+\n10\u00b52rn + 7 \u2016N\u2217\u20162 + 8n\u221a r \u2016N\u2217\u2016\u221e ) \u2264 7\u00b5 2r\nn\n( |\u03c3\u2217k+1|+ 8 \u2223\u2223\u03c3\u2217k+1\u2223\u2223 10n + 7 \u2016N\u2217\u20162 + 8n\u221a r \u2016N\u2217\u2016\u221e )\n\u2264 8\u00b5 2r\nn\n( |\u03c3\u2217k+2|+ 2 \u2223\u2223\u03c3\u2217k+1\u2223\u2223+ 7 \u2016N\u2217\u20162 + 8n\u221ar \u2016N\u2217\u2016\u221e ) .\nSimilarly for \u2225\u2225L(T ) \u2212 L\u2217\u2225\u2225\u221e.\nThis finishes the proof.\nProof of Theorem 2. Using Lemma 14, it suffices to show that the general case can be reduced to the case of symmetric matrices. We will now outline this reduction.\nRecall that we are given an m\u00d7 n matrix M = L\u2217 +N\u2217 + S\u2217 where L\u2217 is the true low-rank matrix, N\u2217 dense corruption matrix and S\u2217 the sparse error matrix. Wlog, let m \u2264 n and suppose \u03b2m \u2264 n < (\u03b2 + 1)m, for some \u03b2 \u2265 1. We then consider the symmetric matrices\nM\u0303 =  0 ... 0 \u00b7 \u00b7 \u00b7 0 ... 0\nM> \u00b7 \u00b7 \u00b7M>\ufe38 \ufe37\ufe37 \ufe38 \u03b2 times\nM ... M 0\n , L\u0303 = \n0 ... 0 \u00b7 \u00b7 \u00b7 0 ... 0\n(L\u2217) > \u00b7 \u00b7 \u00b7 (L\u2217)>\ufe38 \ufe37\ufe37 \ufe38 \u03b2 times\nL\u2217\n... L\u2217\n0\n ,\nN\u0303 =  0 ... 0 \u00b7 \u00b7 \u00b7 0 ... 0\n(N\u2217) > \u00b7 \u00b7 \u00b7 (N\u2217)>\ufe38 \ufe37\ufe37 \ufe38 \u03b2 times\nL\u2217\n... L\u2217\n0\n , (36)\nand S\u0303 = M\u0303 \u2212 L\u0303. A simple calculation shows that L\u0303 is incoherent with parameter \u221a\n3\u00b5, N\u0303 satisfies the assumption of Theorem 2 and S\u0303 satisfies the sparsity condition (S1) with parameter \u03b1\u221a\n2 . Moreover the iterates\nof AltProj with input M\u0303 have similar expressions as in (36) in terms of the corresponding iterates with input M . This means that it suffices to obtain the same guarantees for Algorithm 1 for the symmetric case. Lemma 14 does precisely this, proving the theorem."}, {"heading": "C Additional experimental results", "text": "Synthetic datasets: Extending Figure 2, the plots in Figure 5 illustrate the point that soft thresholding, i.e., the convex relaxation approach, leads to intermediate solutions with high ranks. Figures 5 (a)-(b) show the variation of the maximum rank of the intermediate low-rank solutions of IALM with rank and incoherence respectively; the results are averaged over 5 runs of the algorithm; we note that as the problem becomes harder, the maximum intermediate rank via soft thresholding (convex approach) increases, and this leads to higher running times. As an example of this phenomenon, Figure 5 (c) shows the rank of the intermediate iterates of IALM for a particular run with n = 2000, r = 10, \u03b1 = 100/n, \u00b5 = 3; here, while the rank of the final output is 10, intermediate iterates have a rank as high as 800. We run our synthetic simulations on a machine with Intel Dual 8-core Xeon (E5-2650) 2.0GHz CPU with 192GB RAM.\nReal-world datasets: We provide some additional results concerning foreground-background separation in videos 5. We compare NcRPCA with IALM, and also with the low-rank solution obtained using vanilla PCA; we report the solutions obtained by NcRPCA and IALM methods for decomposing M into L+S up to a relative error (\u2016M \u2212 L \u2212 S\u2016F /\u2016M\u2016F ) of 10\u22123. We report the rank and the sparsity of the solutions obtained by the two methods along with the computational time. As mentioned before, the observed matrix M is formed by\n5The datasets are available at http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html\nvectorizing each frame and stacking them column-wise. For illustration purposes, we arbitrarily select one of the original frames in the sequence of image frames obtained from the video, i.e., one of the columns of M , and the corresponding columns in L and S obtained using NcRPCA and IALM. We run our real data experiments on a machine with Intel Dual 8-core Xeon (E5-2650) 2.0GHz CPU with 192GB RAM.\nShopping Mall dataset: Figure 6 shows the comparison of NcRPCA and IALM on the \u201cShopping Mall\u201d dataset which has 1286 frames at a resolution of 256 \u00d7 320. NcRPCA achieves a solution of better visual quality (for example, unlike NcRPCA, notice the artifact of the low-rank solution from IALM in the top right corner of the image where the person is walking over the reflection of a light source; also notice the shadows of people in the low-rank part obtained by IALM which are not present in the low-rank solution obtained by NcRPCA), in 292.1s, compared to IALM, which takes 783.4s until convergence. NcRPCA obtains a rank 20 solution for L with \u2016S\u20160 = 95411896 whereas IALM obtains a rank 286 solution for L with \u2016S\u20160 = 86253965.\nCurtain dataset: We illustrate our recovery on one of the frames (frame 2773) wherein a person enters a room with a curtain on the background. Figure 7 shows the comparison of NcRPCA and IALM on the \u201cCurtain\u201d dataset which has 2964 frames at a resolution of 160\u00d7 128. NcRPCA achieves a solution, in 39.5s, which is of similar visual quality to that of IALM, which takes 989.0s until convergence. NcRPCA obtains a rank 1 solution for L with \u2016S\u20160 = 53897769 whereas IALM obtains a rank 701 solution for L with \u2016S\u20160 = 42310582."}], "references": [{"title": "Learning Sparsely Used Overcomplete Dictionaries via Alternating Minimization", "author": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon"], "venue": "Available on arXiv:1310.7991,", "citeRegEx": "Agarwal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2013}, {"title": "Tensor Methods for Learning Latent Variable Models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "Available at arXiv:1210.7559,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "author": ["A. Agarwal", "S. Negahban", "M. Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Incoherence-Optimal Matrix Completion", "author": ["Y. Chen"], "venue": "ArXiv e-prints,", "citeRegEx": "Chen.,? \\Q2013\\E", "shortCiteRegEx": "Chen.", "year": 2013}, {"title": "Robust principal component analysis", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "J. ACM,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["Venkat Chandrasekaran", "Sujay Sanghavi", "Pablo A. Parrilo", "Alan S. Willsky"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2011}, {"title": "Clustering sparse graphs", "author": ["Yudong Chen", "Sujay Sanghavi", "Huan Xu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Spectral statistics of Erd\u0151s\u2013R\u00e9nyi graphs I: Local semicircle law", "author": ["L\u00e1szl\u00f3 Erd\u0151s", "Antti Knowles", "Horng-Tzer Yau", "Jun Yin"], "venue": "The Annals of Probability,", "citeRegEx": "Erd\u0151s et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Erd\u0151s et al\\.", "year": 2013}, {"title": "On the provable convergence of alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": null, "citeRegEx": "Hardt.,? \\Q2013\\E", "shortCiteRegEx": "Hardt.", "year": 2013}, {"title": "Robust matrix decomposition with sparse corruptions", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "ITIT,", "citeRegEx": "Hsu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2011}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In STOC,", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Matrix alps: Accelerated low rank and sparse matrix reconstruction", "author": ["Anastasios Kyrillidis", "Volkan Cevher"], "venue": "In SSP Workshop,", "citeRegEx": "Kyrillidis and Cevher.,? \\Q2012\\E", "shortCiteRegEx": "Kyrillidis and Cevher.", "year": 2012}, {"title": "Efficient algorithms for collaborative filtering", "author": ["Raghunandan H. Keshavan"], "venue": "Phd Thesis, Stanford University,", "citeRegEx": "Keshavan.,? \\Q2012\\E", "shortCiteRegEx": "Keshavan.", "year": 2012}, {"title": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Zhouchen Lin", "Minming Chen", "Yi Ma"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Statistical modeling of complex backgrounds for foreground object detection", "author": ["Liyuan Li", "Weimin Huang", "IY-H Gu", "Qi Tian"], "venue": "ITIP,", "citeRegEx": "Li et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Li et al\\.", "year": 2004}, {"title": "Holistic 3d reconstruction of urban structures from low-rank textures", "author": ["Hossein Mobahi", "Zihan Zhou", "Allen Y. Yang", "Yi Ma"], "venue": "In ICCV Workshops,", "citeRegEx": "Mobahi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mobahi et al\\.", "year": 2011}, {"title": "Phase retrieval using alternating minimization", "author": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "In NIPS,", "citeRegEx": "Netrapalli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Netrapalli et al\\.", "year": 2013}, {"title": "Guarantees for Stochastic ADMM in High Dimensions", "author": ["H. Sedghi", "A. Anandkumar", "E. Jonckheere"], "venue": null, "citeRegEx": "Sedghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sedghi et al\\.", "year": 2014}, {"title": "Sparse additive text models with low rank background", "author": ["Lei Shi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shi.,? \\Q2013\\E", "shortCiteRegEx": "Shi.", "year": 2013}, {"title": "Solving multiple-block separable convex minimization problems using two-block alternating direction method of multipliers", "author": ["X. Wang", "M. Hong", "S. Ma", "Z. Luo"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Robust pca via outlier pursuit", "author": ["Huan Xu", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}], "referenceMentions": [], "year": 2014, "abstractText": "We propose a new method for robust PCA \u2013 the task of recovering a low-rank matrix from sparse corruptions that are of unknown value and support. Our method involves alternating between projecting appropriate residuals onto the set of low-rank matrices, and the set of sparse matrices; each projection is non-convex but easy to compute. In spite of this non-convexity, we establish exact recovery of the lowrank matrix, under the same conditions that are required by existing methods (which are based on convex optimization). For anm\u00d7n input matrix (m \u2264 n), our method has a running time of O ( rmn ) per iteration, and needs O (log(1/ )) iterations to reach an accuracy of . This is close to the running times of simple PCA via the power method, which requires O (rmn) per iteration, and O (log(1/ )) iterations. In contrast, the existing methods for robust PCA, which are based on convex optimization, have O ( mn ) complexity per iteration, and take O (1/ ) iterations, i.e., exponentially more iterations for the same accuracy. Experiments on both synthetic and real data establishes the improved speed and accuracy of our method over existing convex implementations.", "creator": "LaTeX with hyperref package"}}}