{"id": "1411.0860", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2014", "title": "CUR Algorithm for Partially Observed Matrices", "abstract": "regression image consistently computes the low rank approximation of a given function before using any actual rows stacked vertical of the matrix. it got proved our very useful tool for recovering large sizes. in improvement enabling simplified performance models for cur parameter matrices is often they need an access across the { \\ it full } matrix, a tool that can be difficult even find in many different world simulation. in this work, we alleviate null criterion by developing a cur decomposition when modeling partially observed matrices. in article, : proposed algorithm computes the low probability approximation of the target matrix based on ( i ) the densely sampled rows np }, and ( sc ) dense subset of observed entries that preserve randomly sampled sample cache matrix. model regression shows perfect relative error bound, generated through numerical norm, for the proposed algorithm initially predicting missing column encounters approximately full rank. we fully check that only $ o ( n ] \\ ln r ) $ observed entries citation needed thus the proposed algorithm to perfectly recover a compute $ r $ matrix of size $ n \\ times np $, which improves the sample complexity of his existing reconstruction for matrix completion. empirical studies on both horizontal and closed - world versions verify some complexity complexity and proven proved effectiveness of the proposed algorithm.", "histories": [["v1", "Tue, 4 Nov 2014 11:03:50 GMT  (4262kb)", "http://arxiv.org/abs/1411.0860v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["miao xu", "rong jin", "zhi-hua zhou"], "accepted": true, "id": "1411.0860"}, "pdf": {"name": "1411.0860.pdf", "metadata": {"source": "CRF", "title": "CUR Algorithm for Partially Observed Matrices", "authors": ["Miao Xu", "Rong Jin", "Zhi-Hua Zhou"], "emails": ["zhouzh@nju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n08 60\nv1 [\ncs .L\nG ]\n4 N\nov 2\nCUR matrix decomposition computes the low rank approximation of a given matrix by using the actual rows and columns of the matrix. It has been a very useful tool for handling large matrices. One limitation with the existing algorithms for CUR matrix decomposition is that they need an access to the full matrix, a requirement that can be difficult to fulfill in many real world applications. In this work, we alleviate this limitation by developing a CUR decomposition algorithm for partially observed matrices. In particular, the proposed algorithm computes the low rank approximation of the target matrix based on (i) the randomly sampled rows and columns, and (ii) a subset of observed entries that are randomly sampled from the matrix. Our analysis shows the relative error bound, measured by spectral norm, for the proposed algorithm when the target matrix is of full rank. We also show that only O(nr ln r) observed entries are needed by the proposed algorithm to perfectly recover a rank r matrix of size n \u00d7 n, which improves the sample complexity of the existing algorithms for matrix completion. Empirical studies on both synthetic and real-world datasets verify our theoretical claims and demonstrate the effectiveness of the proposed algorithm. Key words: Matrix approximation, CUR algorithm, matrix completion"}, {"heading": "1. Introduction", "text": "In many machine learning applications, it is convenient to represent data by matrix. Examples include user-item rating matrix in recommender system [SRJ04], gene expression matrix in bioin-\n\u2217Corresponding author. Email: zhouzh@nju.edu.cn\nPreprint submitted for review November 5, 2014\nformatics [MD08], kernel matrix in kernel learning [WS00], document-term matrix in document retrieval [MD08], and instance-label matrix in multi-label learning [GZR+10]. An effective approach for handling big matrices is to approximate them by their low rank counterparts which can be computed and stored efficiently. Various methods have been developed for low rank matrix approximation, including truncated singular value decomposition, matrix factorization [SRJ04], matrix regression [Kol11], column subset selection [BDMI11], the Nystro\u0308m method [WS00].\nIn this work, we will focus on the CUR algorithm for low rank matrix approximation [MD09]. It is a randomized algorithm that computes the low rank approximation for a given rectangle matrix by randomly sampled columns and rows of the matrix. Compared to other low rank approximation algorithms, CUR is advantageous in that it has (i) an easy interpretation of the approximation result because the subspace is constructed by the actual columns and rows of the target matrix [MD09], and (ii) strong (near-optimal) theoretical guarantee [BXM10, DKM06, MD08, MD09, WZ12, WZ13]. The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08]. In the past decade, many variants of the CUR algorithm have been developed [BXM10, DKM06, MTJ11, MMD06, MD08, MD09, WZ12, WZ13].\nDespite the success, one limitation with the existing CUR algorithms is that to effectively compute the low rank approximation, they require an access to the full matrix, a requirement that can be difficult to fulfill. For instance, in bioinformatics, it is usually too expensive to acquire the full expression information for hundreds of genes and thousands of individuals; in crowdsourcing, when both the number of workers and instances are large, it becomes impractical to request every worker to label all the instances in study ; in social network analysis, it is often the case that only part of the links between individuals can be accurately detected. In all the above cases, due to the physical or financial constraints, we only have a partial observation of the target matrix, making it difficult to apply the existing CUR algorithm.\nOne way to deal with the missing entries is to first compute an unbiased estimation of the target matrix based on the observed entries, and then apply the CUR algorithm to the estimated matrix. The main shortcoming of this simple method is that the unbiased estimate can be far from the target matrix when the number of observation is small, as we will show in the empirical study.\n2\nAnother approach is to recover the target matrix from the observed entries by using the matrix completion technique [CCS10, CR12]. Since most matrix completion algorithms are developed only for matrices of exactly low rank, they usually work poorly for matrices of full rank [EBN11]. We note that although an adaptive sampling approach is developed in [KS13] that does apply to matrices of full rank, they use a different sampling strategy, and their bound has a poor dependence on failure probability \u03b4 (i.e. O(1/\u03b4)), which significantly limit its application when both rows and columns are randomly sampled.\nIn this work, we address the challenge by developing a novel CUR algorithm, named CUR+, for partially observed matrix. More specifically, the proposed algorithm computes a low rank approximation of matrix M based on (i) randomly sampled rows and columns from M , and (ii) randomly sampled entries fromM . Unlike most matrix completion algorithms that require solving an optimization problem involving trace norm regularization [Bac08, CCS10, JY09, MHT10, TS10], the proposed algorithm only needs to solve a standard regression problem and therefore is computationally efficient. In addition, we develop a relative error bound for the proposed CUR+ that works for both low-rank and full-rank matrices. In particular, to perfectly recover a rank-r matrix of size n \u00d7 n, only O(nr ln r) observed entries are needed, significantly lower than O(nr ln2 n) for standard matrix completion theory [CR12, CT10, Gro11, KMO10, Rec11] and lower than O(nr3/2 ln r) for adaptive algorithm for matrix recovery [KS13]. We verify our theoretical claims by empirical studies of low rank matrix approximation.\nThe rest of the paper is organized as follows: Section 2 briefly reviews the related work on the CUR algorithms and matrix completion; Section 3 presents the proposed algorithm and its theoretical properties. Section 4 gives our empirical study. Section 5 concludes our work with future directions."}, {"heading": "2. Related Work", "text": "CUR matrix decomposition. CUR algorithms compute a low rank approximation of the target matrix using the actual rows and columns of the matrix [BXM10, DKM06, GZT97, GTZ97, MD08, MD09, Ste99, Tyr00, WZ12, WZ13]. More specially, let M \u2208 Rn\u00d7m be the given matrix and r be the target rank for approximation. A classical CUR decomposition algorithm [MD08,\n3\nMD09] randomly samples d1 columns and d2 rows from M , according to their leverage scores, to form matrices C and R, respectively. The approximated matrix M\u0302 is then computed as M\u0302 = C(C\u2020MR\u2020)R, where \u2020 is the pseudoinverse. [DKM06] gives an additive error bound for the CUR decomposition, and a relative error bound, a significantly stronger result, is given in [MD08]. It stated that, with a high probability,\n\u2016M \u2212 M\u0302\u2016F \u2264 (1 + \u01eb)\u2016M \u2212Mr\u2016F (1)\nwhere Mr is the best rank-r approximation to M , and \u2016 \u00b7 \u2016F is the Frobenius norm of a matrix.\nVarious improved versions of CUR have been developed. [MTJ11] proposes a divide-and-conquer method to compute the CUR decomposition in parallel. [WZ13] proposes an adaptive CUR algorithm with much tighter error bound and much lower time complexity. In [DKM06], the authors suggest a simple uniform sampling of columns and rows for the CUR decomposition when the maximum statistical leverage scores, also referred to as incoherence measure [CR12, CT10, Rec11], is limited. In [MDMIW12], algorithms have been developed to efficiently compute the approximated values of statistical leverage scores without having to calculate the SVD decomposition of a large matrix. As we claimed in the introduction section, all the existing CUR algorithms require the knowledge of every entry in the target matrix and therefore cannot be applied directly to partially observed matrices. More complete list of related work on CUR can be found in [MD08, WZ13].\nCUR decomposition is closely related to column subset selection problem [BDMI11, DR10, MD08], which has been studied extensively in theoretical computer science and numerical analysis communities [MD08, MD09, WZ13]. It samples multiple columns from the target matrix M and use them as the basis to approximate M , and is often viewed as special case of the CUR algorithm. A special case of column subset selection is Nystro\u0308m methods, which is usually used to approximate Positive Semi-Definitive (PSD) matrix in kernel learning [WS00]. A more complete list of related Nystro\u0308m methods can be found in [JYM+13].\nMatrix Completion. The objective of matrix completion is to fill out the missing entries of a lowrank matrix based on the observed ones. In the standard matrix completion theory, when entries are missing uniformly at random, it requires O(nr ln2 n) observed entries to perfectly recover the target matrix under the incoherence condition [CR12, CT10, Gro11, KMO10, Rec11]. Multiple\n4\nimprovements have been developed for matrix completion, either to deal with nonuniform missing entries or to develop tighter bounds under more strict coherence conditions. [KS13] developed an adaptive sensing strategy for matrix completion that removes an lnn factor from the sample complexity. In [BJ14, CBSW14], the authors study matrix completion when observed entries are not sampled uniformly at random. [NW10, RT11] generalize matrix completion to matrix regression. In [XJZ13], the authors show that the sample complexity for perfect matrix recovery can be reduced dramatically with appropriate side information.\nAlthough it is appealing to directly combine the CUR algorithm with matrix completion to estimate a low rank approximation of a partially observed matrix, it may not work well in practice. One issue is that most matrix completion algorithms are developed for matrix of exactly low rank, significantly limiting its application to low rank matrix application. Although a few studies develop recovery bounds for matrix of full rank [EBN11, KS13], recovery errors usually deteriorate dramatically when applied to a matrix with a long tail spectrum. In addition, most matrix completion algorithms are computationally expensive, especially for large matrices, since they require, at each iteration of optimization, computing the SVD decomposition of the approximate matrix [Bac08, CCS10, JY09, MHT10, TS10]. In contrast, the proposed CUR algorithm scales to large matrix and works well for matrix of full rank."}, {"heading": "3. CUR+ for Partially Observed Matrices", "text": "We describe the proposed CUR+ algorithm, and then present the key theoretical results for it. Due to space limitation, we postpone all the detailed analysis to the supplementary document."}, {"heading": "3.1. CUR+ Algorithm", "text": "Let M \u2208 Rn\u00d7m be the matrix to be approximated, where n \u2265 m. To approximate M , we first sample uniformly at random d1 columns and d2 rows from M , denoted by A = (a1, . . . ,ad1) \u2208 R n\u00d7d1 , and B = (b1, . . . ,bd2) \u2208 Rm\u00d7d2 , respectively, where each ai \u2208 Rn and bj \u2208 Rm is the ith row and the jth column of M respectively. We noticed that uniform sampling of rows and columns may not be the best strategy as it does not take into account the difference between individual rows and columns. Other sampling strategies, such as sampling rows/columns based\n5\non their statistical leverage scores [MD08] and adaptive sampling [KS13, WZ12], can be more effective. We do not choose these sampling methods because they either require an access to the full matrix [MD08], introduce serious overhead in computation [WZ12], or result in significantly worse bound when matrix is of full rank [KS13]. Finally, for simplicity of discussion, we will assume d1 = d2 = d throughout the draft even though our algorithm and analysis can easily be extended to the case when d1 6= d2.\nLet r be the target rank for approximation, with r \u2264 d. U\u0302 = (u\u03021, . . . , u\u0302r) \u2208 Rn\u00d7r and V\u0302 = (v\u03021, . . . , v\u0302r) \u2208 Rm\u00d7r are the first r eigenvectors of AA\u22a4 and BB\u22a4, respectively. Besides A and B, we furthermore sample, uniformly at random, entries from matrix M . Let \u2126 include the indices of randomly sampled entries. Our goal is to estimate a low rank approximation of matrix M using A, B, and randomly sampled entries in \u2126. To this end, we need to solve the following optimization\nmin Z\u2208Rr\u00d7r\n1 2 \u2016R\u2126(M)\u2212R\u2126(U\u0302ZV\u0302 \u22a4)\u20162F (2)\nwhere given \u2126, we define a linear operator R\u2126(M) : Rn\u00d7m 7\u2192 Rn\u00d7m as\n[R\u2126(M)]i,j =    Mi,j (i, j) \u2208 \u2126 0 (i, j) /\u2208 \u2126\nLet Z\u2217 be an optimal solution to (2). The estimated low rank approximation is given by M\u0302 = U\u0302Z\u2217V\u0302 \u22a4. M\u0302 can also be expressed using standard C \u00d7 U \u00d7R formulation by solving a group of linear equations. We note that (2) is a standard regression problem and therefore can be solved efficiently using the standard regression method (e.g. accelerated gradient descent [Nes03]). We refer to the proposed algorithm as CUR+."}, {"heading": "3.2. Guarantee for CUR+", "text": "Before presenting the theoretical results, we first describe the notations that will be used throughout the analysis. Let \u03c3i, i = 1, . . . ,m be the singular values of M ranked in descending order, and let ui and vi be the corresponding left and right singular vectors. Define U = (u1, . . . ,um) and V = (v1, . . . ,vm). Given r \u2208 [m], partitioning the SVD decomposition of M as\nM = U\u03a3V \u22a4 = r m\u2212 r [U1 U2]\n  \u03a31\n\u03a32\n    V \u22a4 1\nV \u22a42\n  (3)\n6\nLet u\u0303i, i \u2208 [n] be the ith row of U1 and v\u0303i, i \u2208 [m] be the ith row of V1. The incoherence measure for U1 and V1 is defined as\n\u00b5(r) = max ( max i\u2208[n] n r |u\u0303i|2,max i\u2208[m] m r |v\u0303i|2 ) (4)\nSimilarly, we can have the incoherence measure for matrices U\u0302 and V\u0302 that include the first r eigenvectors of AA\u22a4 and BB\u22a4, respectively. Let u\u0302\u2032i, i \u2208 [n] be the ith row of U\u0302 and v\u0302\u2032i, i \u2208 [m] be the ith row of V\u0302 . Define the incoherence measure for U\u0302 and V\u0302 as\n\u00b5\u0302(r) = max ( max i\u2208[n] n r |u\u0302\u2032i|2,max i\u2208[m] m r |v\u0302\u2032i|2 ) (5)\nDefine projection operators PU = UU \u22a4, PV = V V \u22a4, P U\u0302 = U\u0302 U\u0302\u22a4, and P V\u0302 = V\u0302 V\u0302 \u22a4. We will use \u2016 \u00b7 \u20162 and \u2016 \u00b7 \u2016F respectively for the spectral norm and Frobenius norm of a matrix.\nWe first present the theoretical guarantee for the CUR+ algorithm when the rank of the target matrix M is no greater than r.\nTheorem 1 (Low-Rank Matrix Approximation) Assume rank(M) \u2264 r, d \u2265 7\u00b5(r)r(t+ln r), and |\u2126| \u2265 7\u00b52(r)r2(t+2 ln r). Then, with a probability at least 1\u2212 5e\u2212t, we have M = M\u0302 , where M\u0302 is a low rank approximation estimated by the CUR+ algorithm.\nRemark. Theorem 1 shows that a rank-r matrix can be perfectly recovered from 2dn + |\u2126| = O(nr ln r) observed entries if we set t = \u2126(ln r). In Table 1, we compare the sample complexity of the CUR+ algorithm with the sample complexity of the other matrix completion algorithms. We observe that our result significantly improves the sample complexity from previous work. We should note that unlike [KS13] where the incoherence measure is only assumed for column vectors, we assume a small incoherence measure for both row and column vectors here. It is this\n7\nstronger assumption that allows us to sample both rows and columns, leading to the improvement in the sample complexity from O(nr3/2 ln r) in [KS13] to O(nr ln r).\nWe now consider a more general case where matrix M is of full rank. Theorem 7 bounds the difference between M and M\u0302 , measured in spectral norm,\nTheorem 2 Let r \u2264 m be an integer that is no larger than m. Assume (i) d \u2265 7\u00b5(r)r(t+ ln r) , and (ii) |\u2126| \u2265 7\u00b5\u03022(r)r2(t+ 2 ln r). Then with a probability at least 1\u2212 3e\u2212t\n\u2016M \u2212 M\u0302\u201622 \u2264 8\u03c32r+1 (1 + 2mn) ( 1 + m+ n\nd\n) .\nAs indicated by Theorem 7, when both \u00b5(r) and \u00b5\u0302(r), the incoherence measure for the first r singular/eigen vectors of M and the sampled columns/rows, are small, we have\n\u2016M \u2212 M\u0302\u20162 \u2264 O (\u221a mn \u221a n\nd \u2016M \u2212Mr\u20162\n)\nprovided that d \u2265 O(r ln r) and |\u2126| \u2265 O(r2 ln r).\nOne limitation with Theorem 7 is that \u00b5\u0302(r) is a random variable depending on the sampled columns and rows. Since \u00b5\u0302(r) can be as high as n/r, |\u2126|, the number of observed entries required by Theorem 7, can be as large as O(n2), making it practically meaningless. Below, we develop a result that explicitly bounds \u00b5\u0302 with a high probability. Using the high probability bound for \u00b5\u0302, we are able to show that under appropriate conditions, we need at most O(n2/d2) observed entries in order to establish a relative error bound for \u2016M \u2212 M\u0302\u2016.\nTo make our analysis simple, we focus on the case whenM is of full rank but with skewed singular value distribution. In particular, we assume \u03c3r \u2265 \u221a 2\u03c3r+1. In order to effectively capture the skewed singular value distribution, we introduce the concept of numerical rank r(M,\u03b7) [GL96] with respect to non-negative constant \u03b7 > 0\nr(M,\u03b7) = m\u2211\ni=1\n\u03c32i \u03c32i +mn\u03b7\nNote that when \u03b7 = 0, the numerical rank is equivalent to the true rank of the matrix. The larger \u03b7 is , the smaller it compared to the true rank. In the following analysis, we will replace rank r with numerical rank r(M,\u03b7).\n8\nWe furthermore generalize the definition of incoherence measure to matrix with numerical rank, that is, we further define incoherence measure \u00b5(\u03b7) as\n\u00b5(\u03b7) = max ( max 1\u2264i\u2264m\nm\nr(M,\u03b7) |Vi,\u2217\u03a3|2, max 1\u2264i\u2264n\nn\nr(M,\u03b7) |Ui,\u2217\u03a3|2\n) (6)\nIt is easy to verify that \u00b5(\u03b7) \u2265 1. Compared to the standard incoherence measure defined in (4), the key difference is that (6) introduces singular values \u03a3 into the definition of incoherence measure, making it appropriate for matrix of full rank.\nThe following two lemmas relate r\u00b5(r) and r\u00b5\u0302(r), respectively, with r(M,\u03b7)\u00b5(\u03b7),\nLemma 1 If we choose \u03b7 = \u03c32r/mn, we have\nr\u00b5(r) \u2264 2r(M,\u03b7)\u00b5(\u03b7)\nLemma 2 Assume that d \u2265 16(\u00b5(\u03b7)r(M,\u03b7) + 1)(t + lnn), and \u03c3r \u2265 \u221a 2\u03c3r+1. Set \u03b7 = \u03c3 2 r/mn. With a probability 1\u2212 4e\u2212t, we have\nr\u00b5\u0302(r) \u2264 2r(M,\u03b7)\u00b5(\u03b7) + 18n\u03b42/r where \u03b42 = 4 d (\u00b5(\u03b7)r(M,\u03b7) + 1)(t+ lnn)\nUsing Theorem 7, Lemma 1 and 2, we have the result for full-rank matrix with skewed singular value distribution,\nTheorem 3 (Full Rank Matrix Approximation) Assume d \u2265 16(\u00b5(\u03b7)r(M,\u03b7) + 1)(t+ lnn) and \u03c3r \u2265 \u221a 2\u03c3r+1. Set \u03b7 = \u03c3 2 r/mn. We have, with a probability 1\u2212 7e\u2212t,\n\u2016M \u2212 M\u0302\u201622 \u2264 8\u03c32r+1 (1 + 2mn) ( 1 + m+ n\nd\n) .\nif |\u2126| \u2265 7 ( 2\u00b5(\u03b7)r(M,\u03b7) + 72 n\nd (\u00b5(\u03b7)r(M,\u03b7) + 1)(t+ lnn)\n)2 (t+ 2 ln r) = O ( n2\nd2\n)\nAs indicated by Theorem 3, we will have a bound similar to that of Theorem 7 if |\u2126| \u2265 O(n2/d2). The key difference between Theorem 7 and 3 is that in Theorem 7, the requirement for |\u2126| depends on \u00b5\u0302(r), a random variable depending on the sampled rows and columns. In contrast, in Theorem 3, we remove \u00b5\u0302 and bound |\u2126| directly. We finally note that the result |\u2126| \u2265 O(n2/d2) requires a large number of sampled entries for accurately estimating the low rank approximation of the target matrix. This is mostly due to the potentially loose bound for \u00b5\u0302. It remains an open question whether it is possible to reduce the number of observed entries for CUR-type low rank approximation.\n9"}, {"heading": "4. Experiments", "text": "We first verify the theoretical result in Theorem 1, i.e. the dependence of sample complexity on r and n, using synthetic data. We then evaluate the performance of the proposed CUR+ algorithm by comparing it to the state-of-the-art algorithms for low rank matrix approximation. We implement the proposed algorithm using Matlab, and all the experiments were run on a Linux server with CPU 2.53GHz and 48GB memory."}, {"heading": "4.1. Experiment (I): Verifying the Dependence on r and n", "text": "We will verify the sample complexity result in Theorem 1, i.e. d \u2265 O(r ln r) and |\u2126| \u2265 O(r2 ln r). We note both the requirements on d and |\u2126| are independent from matrix size.\nSettings. Here we study square matrices of different sizes and ranks, with n varied in {1, 000; 2, 000; 4, 000; 8, 000; 10, 000}, and r varied in {10, 20, 30, 50}. For each special n and r, we search for the smallest d and |\u2126| that can lead to almost perfect recovery of the target matrix (i.e. \u2016M \u2212 M\u0302\u2016F /\u2016M\u2016F \u2264 2 \u00d7 10\u22124) in all 10 independent trials. To create the rank-r matrix M \u2208 Rn\u00d7n, we first randomly generate matrix ML \u2208 Rn\u00d7r and MR \u2208 Rr\u00d7n with each entry of ML and MR drawn independently at random from N (0, 1), and M is given by M = ML\u00d7MR. To create A and B, we sample uniformly at random d rows and columns. We further sample |\u2126| entries from M to be partially observed. Under this construction scheme, the difference between the incoherence \u00b5(r) for different sized matrices are relatively small (from minimum 1.4127 to maximum 2.4885). Although we will plot d and |\u2126|\u2019s dependence on \u00b5(r), we will ignore their impact in discussion of the results.\n10\nResults. The dependence of minimal d on r and n is given in Figure 1(a) and (b), where (a) plots d against r ln r and (b) shows d versus r2 ln r. We can see clearly that d has a linear dependence on r ln r. We also observed from Figure 1(a) that d is almost independent from n, the matrix size. Figure 1(c) and (d) plot the |\u2126|, the minimum number of observed entries, against r ln r and r2 ln r. The result in Figure 1 (d) confirms our theoretical finding, i.e. |\u2126| \u221d r2 ln r."}, {"heading": "4.2. Experiment(II): Comparison with Baseline Methods for Low Rank Approximation", "text": "We evaluate the performance of the proposed CUR+ algorithm on several benchmark data sets that have been used in the recent studies of the CUR matrix decomposition algorithm, including Enron emails (39, 861\u00d728, 102), Dexter (20, 000\u00d72, 600), Farm Ads (54, 877\u00d74, 143) and Gisette (13, 500 \u00d7 5, 000), where each row of the matrix corresponds to a document and each column corresponds to a term/word. Detailed information of these data sets can be found in [WZ13]. All four matrices are of full rank and have skewed singular value distribution, as shown in Figure 2\nBaselines. Since both the rows/columns and entries observed in the proposed algorithm are sampled uniformly at random, we only compare our approach to the standard CUR algorithm using uniformly sampled rows and columns. Although the adaptive sampling based approaches [KS13] usually yield lower errors than the standard CUR algorithm, they do not choose observed entries randomly and therefore are not included in the comparison. Let C be a set of d1 sampled columns and R be the set of d2 sampled rows. The low rank approximation by the CUR algorithm is given by M\u0302 = CZR, where Z \u2208 Rd1\u00d7d2 . Two methods are adopted to estimate Z. We first estimated Z by Z = C\u2020MR\u2020. Since this estimation requires an access to the full matrix, we refer to it as CUR-F. In the second method, we first construct an unbiased estimator Me by using\n11\nthe randomly observed entries in \u2126, and then estimate matrix Z by Z = C\u2020MeR \u2020. Here, the unbiased estimation Me is given by\n[Me]i,j =    mn |\u2126|Mi,j (i, j) \u2208 \u2126\n0 (i, j) /\u2208 \u2126\nWe call this algorithm CUR-E. Evidently, CUR-F is expected to work better than our proposal and will provide a lower bound for the CUR algorithm for partially observed matrices.\nSettings. To make our result comparable to the previous studies, we adapted the same experiment strategy as in [WZ12, WZ13]. More specially, for each data set, we set d1 = \u03b1r and d2 = \u03b1d1, with rank r varied in the range of (10, 20, 50) and \u03b1 varied from 1 to 5. To create partial observations, we randomly sample |\u2126| = \u21260 = nmr2/nnz(M) entries from the target matrix M , where nnz(M) is the number of non-zero entries of M . We measure the performance of low rank matrix approximation by the relative spectral-norm difference \u2113s = \u2016M \u2212 M\u0302\u2016/\u2016M \u2212Mr\u2016 which has solid theoretical guarantee according to Theorem 3. We noticed that most previous work report their results in the form of relative Frobenius norm, thus we will also show the results compared to the state-of-the-art algorithms for low rank matrix approximation measured by the relative Frobenius norm \u2113F = \u2016M \u2212 M\u0302\u2016F /\u2016M \u2212 Mr\u2016F . Finally, we follow the experimental protocol specified in [WZ12] by repeating every experiment 10 times and reporting the mean value.\nResults. Figure 3 shows the results of low rank matrix approximation for r = 10, 20, 50. We observe that the CUR+ works significantly better than the CUR-E method, and yields a similar performance as the CUR-F that has an access to the full target matrix M .\nWe observe that with larger \u03b1 (i.e. increasing numbers of rows and columns), the approximation errors for CUR+ and CUR-F decrease while, to our surprise, the error of CUR-E increases significantly. This counter-intuitive result can be explained by the fact that CUR-E estimates matrix Z based on the observed entries in \u2126. Since the size of Z is d1 \u00d7 d2, which increases at the rate of \u03b13. But on the other hand, |\u2126|, the number of observed entries based on which Z is estimated, remains unchanged. As a result, with increasing values of \u03b1, it becomes more and more difficult to come up with an accurate estimation of Z and consequentially a worse and worse approximation of M . We have verified this explanation in Fig 4 by simultaneously increasing the\n12\nnumber of observed entries in \u2126 and observing that the approximation error of CUR-E decreases with increasing \u03b1, although with perturbation. It is also to our surprise that when r is increasing, the relative spectral-norm difference \u2113s is increasing. This may due to the fact that we normalize the spectral-norm, dividing it by \u2016M \u2212Mr\u2016 which decreases fast. And we observe that \u2016M \u2212M\u0302\u2016 decreases when r becomes larger and larger.\nIn the second experiment, we fix the number of sampled rows and columns and vary the number of observed entries from \u21260 to 5\u21260. Figure 5 shows the results for r = 10, 20 and 50. Again, we found that CUR+ yields similar performance as CUR-F, and performs significantly better than CUR-E, although the gap between CUR+ and CUR-E does decline with increasing number of observed entries. It is also to our surprise that for datasets Enron and Farm Ads, the approximation error of CUR+ remains almost unchanged with increasing number of observed entries. We plan to examine this unusual phenomenon in the future.\n13\n14\nResults Measured by Frobenius Norm. Similar results on relative Frobenius norm are also reported. The results are plotted in Figure 6 when |\u2126| is fixed and we vary \u03b1, and in Figure 7 when \u03b1 is fixed and we vary |\u2126|. We can see that similar as the results measured by spectral norm, the proposed CUR+ works significantly better than the CUR-E method, and yields a similar performance as the CUR-F algorithm that has an access to the full target matrix M ."}, {"heading": "5. Conclusion", "text": "In this paper, we propose a CUR-style low rank approximation algorithm for partially observed matrix. Our analysis shows that the proposed algorithm only needs O(nr ln r) number of observed entries to perfectly recover a low-rank matrix, improving the results of the existing algorithms for matrix completion (of course under a slightly stronger condition). We also show the the spectral error bound for the proposed algorithm when the target matrix is of full rank. Empirical studies on both synthetic data and real datasets verify our theoretical claims and furthermore,\n15\ndemonstrate that the proposed algorithm is more effective in handling partially observed matrix than the existing CUR algorithms. Since adaptive sampling has shown promising results for low rank matrix approximation [KS13], in the future, we plan to combine the proposed algorithm with adaptive sampling strategy to further reduce the error bound. We also plan to exploit the recent studies on matrix approximation/completion with non-uniform sampling and extend the CUR algorithm to the case when observed entries are non-uniform sampled."}, {"heading": "A. Appendix", "text": "We will first give the supporting theorems we will use in the analysis. Then we will give the detailed proof of the three theorems in the paper."}, {"heading": "A.1. Supporting Theorems", "text": "The following results are used throughout the analysis.\n16\nTheorem 4 (Theorem 9.1 in [HMT11]) Let M be an n\u00d7m matrix with singular value decomposition M = U\u03a3V \u22a4. There is a fixed r > 0. Choose a test matrix \u03a8 \u2208 Rm\u00d7d and construct sample matrix Y = M\u03a8. Partition M as in (7)\nM = U\u03a3V \u22a4 = r m\u2212 r [U1 U2]\n  \u03a31\n\u03a32\n    V \u22a4 1\nV \u22a42\n  (7)\nand define \u03a81 = V \u22a4 1 \u03a8 and \u03a82 = V \u22a4 2 \u03a8. Assuming \u03a81 has full row rank, the approximation error satisfies\n\u2016M \u2212 PY (M)\u201622 \u2264 \u2016\u03a32\u201622 + \u2016\u03a32\u03a82\u03a8\u20201\u201622\nwhere PY (M) projects column vectors in M in the subspace spanned by the column vectors in Y and \u2020 denotes the pseudoinverse.\nTheorem 5 (Derived From Theorem 2.2 of [Tro11]) Let X be a finite set of PSD matrices with dimension k (means the size of the square matrix is k \u00d7 k). \u03bbmax(\u00b7) and \u03bbmin(\u00b7) calculate the maximum and minimum eigen value respectively.\nSuppose that\nmax X\u2208X\n\u03bbmax(X) \u2264 B.\nSample {X1, . . . ,X\u2113} uniformly at random from X without replacement. Compute\n\u00b5max = \u2113\u03bbmax(E[X1]), \u00b5min = \u2113\u03bbmin(E[X1])"}, {"heading": "Then", "text": "Pr { \u03bbmax ( \u2113\u2211\ni=1\nXi ) \u2265 (1 + \u03c1)\u00b5max } \u2264 k exp \u2212\u00b5max\nB [(1 + \u03c1) ln(1 + \u03c1)\u2212 \u03c1] for \u03c1 \u2208 [0, 1)\nPr { \u03bbmin ( \u2113\u2211\ni=1\nXi ) \u2264 (1\u2212 \u03c1)\u00b5min } \u2264 k exp \u2212\u00b5min\nB [(1\u2212 \u03c1) ln(1\u2212 \u03c1) + \u03c1] for \u03c1 \u2265 0\nTheorem 6 Let A = S\u22a4HS and A\u0303 = S\u22a4H\u0303S be two symmetric matrices of size n \u00d7 n. Let \u03bbi, i \u2208 [n] and \u03bb\u0303i, i \u2208 [n] be the eigenvalues of A and A\u0303, respectively, ranked in descending order. Let UA, U\u0303A \u2208 Rn\u00d7r include the first r eigenvectors of A and A\u0303, respectively. Let \u2016 \u00b7 \u2016 be any\n17\ninvariant norm. Define\n\u2206\u03bb = min\n(\u221a 2 ( 1\u2212 \u03bbr+1\n\u03bbr ) , 1\u221a 2 ) \u2264 1\u221a 2\n\u2206H = \u2016H\u22121\u2016\u2016H \u2212 H\u0303\u2016\u221a 1\u2212 \u2016H\u22121\u2016\u2016H \u2212 H\u0303\u2016\nIf \u2206\u03bb \u2265 \u2206H/2, we have\n\u2016 sin\u0398(UA, U\u0303A)\u2016 \u2264 \u2206H\n\u2206\u03bb \u2212\u2206H/2\n( 1 +\n\u2206H\u2206\u03bb 16\n)\nwhere\n\u0398(X, X\u0303) = arccos((X\u2217X)\u22121/2X\u2217X\u0303(X\u0303\u2217X\u0303)\u22121X\u0303\u2217X(X\u2217X)\u22121/2)1/2\ndefines the angle matrix between X and X\u0303.\nNote that the above Theorem 6 follows directly from Theorem 4.4 and discussion in Section 5 from [Li99]."}, {"heading": "A.2. Proof of Theorem 2", "text": "We will first provide the key result for our analysis, and then bound each component of the key result, that is, first, we will show that \u2016M \u2212 PU\u0302MPV\u0302 \u201622 is small; then, we will bound the strong convexity of the objective function.\nThe following theorem shows that the difference between M and M\u0302 is well bounded if both \u2016M \u2212 P U\u0302 MP V\u0302 \u201622 and the strong convexity of Eq.2 are well bounded,\nTheorem 7 Assume (i) \u2016M \u2212 P U\u0302 MP V\u0302 \u201622 \u2264 \u2206, and (ii) the strong convexity of the objective function is no less than |\u2126|\u03b3. Then\n\u2016M \u2212 M\u0302\u201622 \u2264 2 ( \u2206+ \u2206\n\u03b3\n) .\nwhere strongly convexity is defined as,\n18\nDefinition 8 A function f : D \u2192 R is \u03be-strongly convex w.r.t. norm \u2016 \u00b7 \u2016 if f is everywhere differentiable and\nf(w) \u2265 f(w\u2032) +\u2207f(w\u2032)(w \u2212 w\u2032) + \u03be 2 \u2016w \u2212 w\u2032\u20162."}, {"heading": "Then \u03be is the strongly convexity of f .", "text": "Proof: Set Z = U\u0302\u22a4MV\u0302 . Since \u2016M \u2212 PU\u0302MPV\u0302 \u201622 \u2264 \u2206, we have\n\u2016M \u2212 U\u0302ZV\u0302 \u22a4\u201622 \u2264 \u2206,\nimplying\n\u2016R\u2126(M)\u2212R\u2126(U\u0302ZV\u0302 \u22a4)\u20162F \u2264 \u2206\nLet Z\u2217 be the optimal solution to Eq.2. Using the strongly convexity of Eq.2, we have\n1 2 \u03b3|\u2126|\u2016Z \u2212 Z\u2217\u20162F \u2264 1 2 |\u2126|\u2206,\ni.e. \u2016Z \u2212 Z\u2217\u20162F \u2264 \u2206/(\u03b3).\nThis is because f(Z) = 12\u2016R\u2126(M) \u2212 R\u2126(U\u0302ZV\u0302 \u22a4)\u20162F , such that \u2207f(Z) = U\u0302T [R\u2126(U\u0302ZV\u0302 T ) \u2212 R\u2126(M)]V\u0302 , and \u2207f(Z\u2217) = 0\n|\u2126|\u03b3 2 \u2016Z \u2212 Z\u2217\u20162F \u2264 1 2 \u2016R\u2126(M)\u2212R\u2126(U\u0302ZV\u0302 \u22a4)\u20162F \u2212 1 2 \u2016R\u2126(M)\u2212R\u2126(U\u0302Z\u2217V\u0302 \u22a4)\u20162F\n\u2264 1 2 \u2016R\u2126(M)\u2212R\u2126(U\u0302ZV\u0302 \u22a4)\u20162F \u2264 |\u2126|\u2206 2\nWe thus have,\n\u2016M \u2212 M\u0302\u201622 \u2264 2\u2016M \u2212 PU\u0302MPV\u0302 \u2016 2 2 + 2\u2016PU\u0302MPV\u0302 \u2212 U\u0302Z\u2217V\u0302 \u22a4\u201622 \u2264 2\u2016M \u2212 PU\u0302MPV\u0302 \u2016 2 2 + 2\u2016PU\u0302MPV\u0302 \u2212 U\u0302Z\u2217V\u0302\n\u22a4\u20162F \u2264 2\u2016M \u2212 PU\u0302MPV\u0302 \u2016 2 2 + 2\u2016Z \u2212 Z\u2217\u20162F \u2264 2 ( \u2206+ \u2206\n\u03b3|\n)\nIn order to bound \u2206, we need the following theorem,\n19\nTheorem 9 With a probability 1\u2212 2e\u2212t, we have,\n\u2016M \u2212MP V\u0302 \u201622 \u2264 \u03c32r+1\n( 1 + 2 m\nd\n)\nand\n\u2016M \u2212 PU\u0302M\u20162 \u2264 \u03c3 2 r+1\n( 1 + 2 n\nd\n)\nprovided that d \u2265 7\u00b5(r)r(t+ ln r).\nProof: Let i1, . . . , id are the d selected columns. Define \u03a8 = (ei1 , . . . , eid) \u2208 Rm\u00d7d, where ei is the ith canonical basis. Such that we have A = M \u00d7\u03a8, that is, A is composed of the d selected columns of M . To utilize Theorem 4, we need to bound the minimum eigenvalue of \u03a81\u03a8 \u22a4 1 , where \u03a81 = V T 1 \u03a8 \u2208 Rr\u00d7d is full rank. We have\n\u03a81\u03a8 \u22a4 1 = V \u22a4 1 \u03a8\u03a8 \u22a4V1\nLet v\u0303\u22a4i , i \u2208 [d] be the ith row vector of V1. We have,\n\u03a81\u03a8 \u22a4 1 =\nd\u2211\nj=1\nv\u0303ij v\u0303 \u22a4 ij\nIt is straightforward to show that\nE [ \u03a81\u03a8 \u22a4 1 ] = d\nm Ir\nand\nE [ v\u0303ij v\u0303 \u22a4 ij ] = 1\nm Ir.\nTo bound the minimum eigenvalue of \u03a81\u03a8 \u22a4 1 , we need Theorem 5, where we first need to bound the maximum eigen value of v\u0303ij v\u0303 \u22a4 ij , which is a rank-1 matrix, whose eigen value\nmax 1\u2264i\u2264m \u03bbmax(v\u0303ij v\u0303 \u22a4 ij ) = max1\u2264i\u2264m\n|v\u0303i|2 \u2264 \u00b5(r) r\nm ,\nand\n\u03bbmax(E [ v\u0303ij v\u0303 \u22a4 ij ] ) = \u03bbmin(E [ v\u0303ij v\u0303 \u22a4 ij ] ) = 1\nm\n20\nThus, we have,\nPr { \u03bbmin(\u03a81\u03a8 \u22a4 1 ) \u2264 (1\u2212 \u03b4) d\nm\n} \u2264 r exp \u2212d/m\nr\u00b5(r)/m [(1\u2212 \u03c1) ln(1\u2212 \u03c1) + \u03c1]\n= r exp \u2212d\nr\u00b5(r) [(1\u2212 \u03c1) ln(1\u2212 \u03c1) + \u03c1]\nBy setting \u03b4 = 1/2, we have,\nPr { \u03bbmin(\u03a81\u03a8 \u22a4 1 ) \u2264 d\n2m\n} \u2264 r exp \u2212d\n7r\u00b5(r) = re\u2212d/[7\u00b5(r)r]\nwhere with d \u2265 7\u00b5(r)r(t+ ln r), we have r exp\u2212d/[7\u00b5(r)r] \u2264 e\u2212t, that is,\nPr { \u03bbmin(\u03a81\u03a8 \u22a4 1 ) \u2265 d\n2m\n} \u2265 1\u2212 e\u2212t\nWith\n\u03bbmin(\u03a81\u03a8 \u22a4 1 ) \u2265\nd\n2m\naccording to Theorem 4, we have\n\u2016M \u2212MPV\u0302 \u2016 2 2 \u2264 \u2016\u03a32\u201622 + \u2016\u03a32\u03a82\u03a8\u20201\u201622\n\u2264 \u03c32r+1 + \u2225\u2225\u2225\u03a32\u03a82\u03a8\u20201 \u2225\u2225\u2225 2\n2\n\u2264 \u03c32r+1 + \u2016\u03a8\u20201\u201622\u2016\u03a32\u03a82\u201622 \u2264 \u03c32r+1 + 2m\nd \u2016\u03a32\u03a82\u201622\n\u2264 \u03c32r+1 + 2m\nd \u2016\u03a32\u201622\u2016\u03a82\u201622\n\u2264 \u03c32r+1 + 2m\nd \u03c32r+1\n\u2264 \u03c32r+1 ( 1 + 2m\nd\n)\n\u2022 The 1st inequality is according to Theorem 4.\n\u2022 The 3rd inequality is because the two facts, \u2016M1M2\u20162 \u2264 \u2016M1\u20162 \u00d7 \u2016M2\u20162 \u2022 The 4th inequality is becuase \u2016\u03a8\u20201\u20162 = 1/\u03c3min(\u03a81) = \u221a 1/\u03bbmin(\u03a81\u03a8\u22a41 ) \u2264 \u221a 2m/d\n\u2022 The 6th inequality is because \u2016\u03a32\u20162 = \u03c3r+1 and \u2016\u03a82\u20162 \u2264 \u2016V2\u20162\u2016\u03a8\u20162 = 1\n21\nWe then bound \u2206,\nTheorem 10 With a probability 1\u2212 2e\u2212t, we have,\n\u2206 := \u2016M \u2212 PU\u0302MPV\u0302 \u2016 2 2 \u2264 4\u03c32r+1\n( 1 + m+ n\nd\n)\nif d \u2265 7\u00b5(r)r(t+ ln r).\nProof: Using Theorem 9, we have, with a probability 1\u2212 2e\u2212t\n\u2016M \u2212 P U\u0302 MP V\u0302 \u201622 \u2264 2\u2016M \u2212MPV\u0302 \u2016 2 2 + 2\u2016(M \u2212 PU\u0302M)PV\u0302 \u2016 2 2\n\u2264 2\u2016M \u2212MPV\u0302 \u2016 2 2 + 2\u2016M \u2212 PU\u0302M\u2016 2 2\n\u2264 4\u03c32r+1 ( 1 + n+m\nd\n)\nWe will then bound the strong convexity of the objective function,\nTheorem 11 With a probability 1\u2212e\u2212t, we have that \u03b3|\u2126|, the strongly convexity for the objective function in (2), is bounded from below by |\u2126|/[2mn] (that is, \u03b3 \u2265 1/(2mn)), provided that\n|\u2126| \u2265 7\u00b5\u03022(r)r2(t+ 2 ln r)\nProof: To bound the strong convexity, we could instead bound the smallest eigen value of the Hessian matrix. The Hessian matrix is an r2 \u00d7 r2 matrix. Assuming the second-order derivative of the (i1, j1)th and (i2, j2)th entry of Z is the (r(i1\u22121)+ j1, r(i2\u22121)+ j2)th entry of the Hessian matrix, the Hessian matrix could be written as,\nH = \u2211\n(i,j)\u2208\u2126\n[vec(u\u0303\u22a4i v\u0303j)][vec(u\u0303 \u22a4 i v\u0303j)] T\nTo bound the minimum eigenvalue of H, we will use Lemma 5. Thus first we need to bound\nmax i,j\n\u03bbmax([vec(u\u0303 \u22a4 i v\u0303j)][vec(u\u0303 \u22a4 i v\u0303j)] T ) = max i,j\n|vec(u\u0303\u22a4i v\u0303j)|2 \u2264 max \u2016u\u0303\u22a4i v\u0303j\u20162F \u2264 \u00b5\u03022(r)r2\nmn\n22\nand\n\u03bbmin ( E([vec(u\u0303\u22a4i v\u0303j)][vec(u\u0303 \u22a4 i v\u0303j)] T ) ) = 1\nmn \u03bbmin\n( (U \u2297 V )T \u00d7 (U \u2297 V ) )\n= 1\nmn\nwhere \u2297 is the Kronecker product.\nBased on Theorem 5, we have\nPr { \u03bbmin(H) \u2264\n|\u2126| 2mn\n} \u2264 r2e \u2212|\u2126| 7\u00b5\u03022(r)r2\nHence, with a probability 1\u2212 e\u2212t, we have\n\u03bbmin(H) \u2265 |\u2126| 2mn\nprovided that\n|\u2126| \u2265 7\u00b5\u03022(r)r2(t+ 2 ln r)\nTheorem 2 can be easily proved combining Theorems 7, 10 and 11."}, {"heading": "A.3. Proof of Theorem 1", "text": "The following theorem allows us to replace \u00b5\u0302(r) in Theorem 11 with \u00b5(r) when the rank of M is less than or equal to r.\nTheorem 12 With a probability 1\u2212 2e\u2212t, we have \u00b5\u0302(r) = \u00b5(r), if d \u2265 7\u00b5(r)r(t+ ln r).\nProof: According to Theorem 10, with a probability 1 \u2212 2e\u2212t, we have M = P U\u0302 MP V\u0302 , provided that d \u2265 7\u00b5(r)r(t+ ln r). Hence PU1 = PU\u0302 and PV1 = PV\u0302 , which directly implies that \u00b5(r) = \u00b5\u0302(r).\nTheorem 1 can be proved directly from Theorem 2 and Theorem 12.\n23"}, {"heading": "A.4. Proof of Theorem 3", "text": "Define\nHA = \u03b7I + 1\nmn MM\u22a4, H\u0302A = \u03b7I +\n1\ndn AA\u22a4\nand\nHB = \u03b7I + 1\nmn M\u22a4M, H\u0302B = \u03b7I +\n1\ndm BB\u22a4\nWe can have the first r eigen vector of would be HA, because\nHA = \u03b7I + 1\nmn MM\u22a4\n= \u03b7UUT + 1\nmn U(\u03a3\u03a3T )UT\n= U(\u03b7I + 1\nmn \u03a3\u03a3T )UT\nand\nH \u22121/2 A = Udiag(\n\u221a mn\n\u03c321 +mn\u03b7 , . . . ,\n\u221a mn\n\u03c32m +mn\u03b7 ) =\n\u221a mnUTUT\nwhere\nT = diag(\n\u221a 1\n\u03c321 +mn\u03b7 , . . . ,\n\u221a 1\n\u03c32m +mn\u03b7 )"}, {"heading": "A.4.1. Proof of Lemma 1", "text": "Proof: Just consider the maximization of the norm of rows of U , then we will have\n\u00b5(\u03b7) = max i=1,...,n\nm\u2211\nj=1\nn\nr(M,\u03b7) \u03c32j \u03c32j +mn\u03b7 U2i,j\n= max i=1,...,n\nn r\nm\u2211\nj=1\nr \u03c32j\nr(M,\u03b7)(\u03c32j +mn\u03b7) U2i,j\n\u2265 max i=1,...,n\nn r\nm\u2211\nj=1\nr a\nr U2i,j\n= a max i=1,...,n\nn r\nm\u2211\nj=1\nU2i,j\n= a\u00b5(r)\n24\nwhen \u03b7 = \u03c32r/mn, then a \u2264 r/2r(M,\u03b7), then\n\u00b5(r) \u2264 1 a \u00b5(\u03b4) \u2264 2r(M,\u03b7) r \u00b5(\u03b7)\ncompletes our proof."}, {"heading": "A.4.2. Proof of Lemma 2", "text": "To this end, we need the following theorem.\nTheorem 13 With a probability 1\u2212 4e\u2212t, we have\n1\u2212 \u03b4 \u2264 \u03bbk(H\u22121/2A H\u0302AH \u22121/2 A ) \u2264 1 + \u03b4, 1\u2212 \u03b4 \u2264 \u03bbk(H \u22121/2 B H\u0302BH \u22121/2 B ) \u2264 1 + \u03b4, \u2200k \u2208 [n]\nif\nd \u2265 4 \u03b42 (\u00b5(\u03b7)r(M,\u03b7) + 1)(t+ lnn)\nProof: It is sufficient to show the result for H\u0302A.\nDefine\nX = { Mi = (H \u22121/2 A ) T ( 1\nn M\u2217,iM\n\u22a4 \u2217,i + \u03b7I\n) H\n\u22121/2 A , i = 1, . . . ,m\n}\nNote that if ai is the jth column of matrix M , then,\nM\u2217,i = U\u03a3(Vi,\u2217) \u22a4\nThus we have\nMi = mnUTU \u22a4(\n1 n U\u03a3V \u22a4i,\u2217Vi,\u2217\u03a3U \u22a4 + \u03b7I)UTU\u22a4\n= U ( mT\u03a3V \u22a4i,\u2217Vi,\u2217\u03a3T +mn\u03b7T 2 ) U\u22a4\n25\nIn this way\n\u03bbmax(Mi) \u2264 \u03bbmax(mUT\u03a3V \u22a4i,\u2217Vi,\u2217\u03a3TU\u22a4) + \u03bbmax(mn\u03b7UT 2U\u22a4)\n= m|UT\u03a3V \u22a4i,\u2217|22 + mn\u03b7\n\u03c32m +mn\u03b7\n\u2264 \u00b5(\u03b7)r(M,\u03b7) + 1\n(this is because |Ax|22 \u2264 \u2016A\u201622|x|22 \u2264 \u2016A\u20162F |x|22) and\n\u03bbmax(E[Mi]) = \u03bbmax(U ( T\u03a3V \u22a4V \u03a3T +mn\u03b7T 2 ) U\u22a4)\n= \u03bbmax(U ( T\u03a3\u03a3T +mn\u03b7T 2 ) U\u22a4) = \u03c321\nmn\u03b7 + \u03c321 +\nmn\u03b7\nmn\u03b7 + \u03c321 = 1\nSo\n\u00b5max = d\u03bb1(E[Mi]) = d\nwe have (using Lemma 5),\nPr { \u03bbmax ( H \u22121/2 A H\u0302AH \u22121/2 A ) \u2265 1 + \u03b4 } \u2264 n exp ( \u2212 d \u00b5(\u03b7)r(M,\u03b7) + 1 [(1 + \u03b4) ln(1 + \u03b4)\u2212 \u03b4] )\nUsing the fact that (at 0 they are the same, but the left increase faster than the right)\n(1 + \u03b4) ln(1 + \u03b4) \u2265 \u03b4 + 1 4 \u03b42,\u2200\u03b4 \u2208 [0, 1],\nwe have\nPr { \u03bbmax ( H \u22121/2 A H\u0302AH \u22121/2 A ) \u2265 1 + \u03b4 } \u2264 n exp ( \u2212 d\u03b4 2\n4(\u00b5r(M,\u03b7) + 1)\n)\nWe have the result by setting d \u2265 4(\u00b5(\u03b7)r(M,\u03b7)+1)(ln n+ t)/\u03b42. Similarly, for the lower bound, we have (using Lemma 5)\nPr { \u03bbmin ( H \u22121/2 A H\u0302AH \u22121/2 A ) \u2264 1\u2212 \u03b4 } \u2264 n exp ( \u2212 d \u00b5(\u03b7)r(M,\u03b7) + 1 [(1\u2212 \u03b4) ln(1\u2212 \u03b4) + \u03b4] )\nUsing the fact that (by Taylor Expansion of ln(1\u2212 \u03b4))\n(1\u2212 \u03b4) ln(1\u2212 \u03b4) \u2265 \u2212\u03b4 + \u03b4 2\n2\n26\nWe have the result by setting d \u2265 2(\u00b5(\u03b7)r(M,\u03b7) + 1)(ln n+ t)/\u03b42.\nUsing Theorem 13, we will prove Lemma 2, Proof: To utilize Theorem 6, we rewrite HA and H\u0302A, as\nHA = H 1/2 A IHA, H\u0302A = H 1/2 A DH 1/2 A\nwhere D = H \u22121/2 A H\u0302AH \u22121/2 A . According to Theorem 13, with a probability 1 \u2212 2e\u2212t, we have \u2016D \u2212 I\u20162 \u2264 \u03b4, provided that\nd = 4\n\u03b42 (\u00b5(\u03b7)r(M,\u03b7) + 1)(t+ lnn)\nWe then compute \u2206H defined in Theorem 6 as\n\u2206H \u2264 \u03b4\u221a 1\u2212 \u03b4\nBecause d \u2265 16(\u00b5(\u03b7)r(M,\u03b7) + 1)(t+ lnn), we have 4\n\u03b42 (\u00b5(\u03b7)r(M,\u03b7) + 1)(t+ lnn) \u2265 16(\u00b5(\u03b7)r(M,\u03b7) + 1)(t+ lnn)\nthat is \u03b4 \u2264 1/2. Because \u03c3r \u2265 \u221a 2\u03c3r+1, we have 1/2 \u2264 1 \u2212 \u03c32r+1/\u03c32r . Since \u03b4 \u2264 1/2 \u2264 1 \u2212 \u03c32r+1/\u03c32r , we have \u2206H \u2264 \u221a 2\u03b4.\nThen according to Theorem 6, we have,\n\u2016 sin\u0398(U1, U\u0302)\u20162 \u2264 \u221a 2\u03b4\n\u2206\u03bb \u2212 \u221a 2\u03b4/2 (1 + \u221a 2\u03b4\u2206\u03bb 16 )\n\u2264 \u221a 2\u03b4\n\u2206\u03bb \u2212 \u221a 2\u03b4/2\n(1 + 1\n32 ) < 3\n\u221a 2\u03b4\nSimilarly, we have,\n\u2016 sin\u0398(V1, V\u0302 )\u20162 < 3 \u221a 2\u03b4\nThus, with a probability 1\u2212 4e\u2212t, we have\n\u00b5\u0302(r) \u2264 2r(M,\u03b7) r \u00b5(\u03b7) + n r \u2016 sin\u0398(V1, V\u0302 )\u201622 \u2264 2r(M,\u03b7) r \u00b5(\u03b7) +\n18n\u03b42\nr\nTheorem 3 can be proved by combining the results of Theorems 7, 11, Lemma 1 and Lemma 2.\n27"}], "references": [{"title": "Consistency of trace norm minimization", "author": ["F. Bach"], "venue": "JMLR, 9:1019\u20131048", "citeRegEx": "Bac08", "shortCiteRegEx": null, "year": 2008}, {"title": "Near optimal column-based matrix reconstruction", "author": ["C. Boutsidis", "P. Drineas", "M. Magdon-Ismail"], "venue": "FOCS", "citeRegEx": "BDMI11", "shortCiteRegEx": null, "year": 2011}, {"title": "Universal matrix completion", "author": ["S. Bhojanapalli", "P. Jain"], "venue": "ICML", "citeRegEx": "BJ14", "shortCiteRegEx": null, "year": 2014}, {"title": "Cur from a sparse optimization viewpoint", "author": ["J. Bien", "Y. Xu", "M. Mahoney"], "venue": "NIPS", "citeRegEx": "BXM10", "shortCiteRegEx": null, "year": 2010}, {"title": "Coherent matrix completion", "author": ["Y. Chen", "S. Bhojanapalli", "S. Sanghavi", "R. Ward"], "venue": "ICML", "citeRegEx": "CBSW14", "shortCiteRegEx": null, "year": 2014}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.-F. Cai", "E. Cand\u00e8s", "Z. Shen"], "venue": "SIAM J. Opti., 20(4):1956\u20131982", "citeRegEx": "CCS10", "shortCiteRegEx": null, "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["E. Cand\u00e8s", "B. Recht"], "venue": "Commun. ACM", "citeRegEx": "CR12", "shortCiteRegEx": null, "year": 2012}, {"title": "The power of convex relaxation: near-optimal matrix completion", "author": ["E. Cand\u00e8s", "T. Tao"], "venue": "TIT", "citeRegEx": "CT10", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast Monte Carlo algorithms for matrices III: Computing a compressed approximate matrix decomposition", "author": ["P. Drineas", "R. Kannan", "M.W. Mahoney"], "venue": "SIAM J. Comput., 36:184\u2013206", "citeRegEx": "DKM06", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["A. Deshpande", "L. Rademacher"], "venue": "FOCS", "citeRegEx": "DR10", "shortCiteRegEx": null, "year": 2010}, {"title": "High-rank matrix completion and subspace clustering with missing data", "author": ["B. Eriksson", "L. Balzano", "R. Nowak"], "venue": "CoRR", "citeRegEx": "EBN11", "shortCiteRegEx": null, "year": 2011}, {"title": "Loan", "author": ["C G. Golub"], "venue": "Matrix computations (3rd ed.). Johns Hopkins University Press,", "citeRegEx": "GL96", "shortCiteRegEx": null, "year": 1996}, {"title": "TIT", "author": ["D. Gross. Recovering low-rank matrices from few coefficients in any basis"], "venue": "57(3):1548\u20131566,", "citeRegEx": "Gro11", "shortCiteRegEx": null, "year": 2011}, {"title": "A theory of pseudoskeleton approximations", "author": ["S.A. Goreinov", "E.E. Tyrtyshnikov", "N.L. Zamarashkin"], "venue": "Linear Algebra and Its Applications, 261(1-3):1\u201321", "citeRegEx": "GTZ97", "shortCiteRegEx": null, "year": 1997}, {"title": "Transduction with matrix completion: Three birds with one stone", "author": ["A. Goldberg", "X. Zhu", "B. Recht", "J.-M. Xu", "R. Nowak"], "venue": "NIPS", "citeRegEx": "GZR+10", "shortCiteRegEx": null, "year": 2010}, {"title": "seudo-skeleton approximations by matrices of maximal volume", "author": ["S. Goreinov", "N. Zamarashkin", "E. Tyrtyshnikov"], "venue": "Mathematical Notes, 62(4):515\u2013519", "citeRegEx": "GZT97", "shortCiteRegEx": null, "year": 1997}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.-G. Martinsson", "J. Tropp"], "venue": "SIAM Review, 53(2):217\u2013288", "citeRegEx": "HMT11", "shortCiteRegEx": null, "year": 2011}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["P. Jain", "P. Netrapalli", "S. Sanghavi"], "venue": "STOC", "citeRegEx": "JNS13", "shortCiteRegEx": null, "year": 2013}, {"title": "An accelerated gradient method for trace norm minimization", "author": ["S. Ji", "J. Ye"], "venue": "ICML", "citeRegEx": "JY09", "shortCiteRegEx": null, "year": 2009}, {"title": "Improved bounds for the nystr\u00f6m method with application to kernel classification", "author": ["R. Jin", "T. Yang", "M. Mahdavi", "Y.-F. Li", "Z.-H. Zhou"], "venue": "TIT, 59(10):6939\u20136949", "citeRegEx": "JYM+13", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix completion from a few entries", "author": ["R. Keshavan", "A. Montanari", "S. Oh"], "venue": "TIT", "citeRegEx": "KMO10", "shortCiteRegEx": null, "year": 2010}, {"title": "Low rank matrix recovery: nuclear norm penalization", "author": ["V. Koltchinskii"], "venue": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems. Springer", "citeRegEx": "Kol11", "shortCiteRegEx": null, "year": 2011}, {"title": "Low-rank matrix and tensor completion via adaptive sampling", "author": ["A. Krishnamurthy", "A. Singh"], "venue": "NIPS", "citeRegEx": "KS13", "shortCiteRegEx": null, "year": 2013}, {"title": "Relative perturbation theory: (II) eigenspace and singular subspace variations", "author": ["R.-C. Li"], "venue": "SIAM J. Matrix Anal. Appl., 20:471\u2013492,", "citeRegEx": "Li99", "shortCiteRegEx": null, "year": 1999}, {"title": "Relative-error CUR matrix decompositions", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "SIAM J. Matrix Anal. Appl., 30:844\u2013881", "citeRegEx": "MD08", "shortCiteRegEx": null, "year": 2008}, {"title": "CUR matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proc. Natl. Acad. Sci. USA, 106:697\u2013702", "citeRegEx": "MD09", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["M. Mahoney", "P. Drineas", "M. Magdon-Ismail", "D. Woodruff"], "venue": "ICML", "citeRegEx": "MDMIW12", "shortCiteRegEx": null, "year": 2012}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["R. Mazumder", "T. Hastie", "R. Tibshirani"], "venue": "JMLR, 11:2287\u20132322", "citeRegEx": "MHT10", "shortCiteRegEx": null, "year": 2010}, {"title": "Tensor-cur decompositions for tensorbased data", "author": ["M. Mahoney", "M. Maggioni", "P. Drineas"], "venue": "KDD", "citeRegEx": "MMD06", "shortCiteRegEx": null, "year": 2006}, {"title": "Divide-and-conquer matrix factorization", "author": ["L. Mackey", "A. Talwalkar", "M. Jordan"], "venue": "NIPS", "citeRegEx": "MTJ11", "shortCiteRegEx": null, "year": 2011}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course", "author": ["Y. Nesterov"], "venue": "Springer", "citeRegEx": "Nes03", "shortCiteRegEx": null, "year": 2003}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "author": ["S. Negahban", "M. Wainwright"], "venue": "ICML", "citeRegEx": "NW10", "shortCiteRegEx": null, "year": 2010}, {"title": "A simpler approach to matrix completion", "author": ["B. Recht"], "venue": "JMLR, 12:3413\u20133430", "citeRegEx": "Rec11", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimation of high dimensional low rank matrices", "author": ["A. Rhode", "A. Tsybakov"], "venue": "Annual of Statistics, 39(2):887\u2013930", "citeRegEx": "RT11", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": "NIPS", "citeRegEx": "SRJ04", "shortCiteRegEx": null, "year": 2004}, {"title": "Four algorithms for the the efficient computation of truncated pivoted qr approximations to a sparse matrix", "author": ["G. Stewart"], "venue": "Numerische Mathematik", "citeRegEx": "Ste99", "shortCiteRegEx": null, "year": 1999}, {"title": "Data Anal", "author": ["J. Tropp. Improved analysis of the subsampled randomized hadamard transform. Adv. Adapt"], "venue": "3:115\u2013126,", "citeRegEx": "Tro11", "shortCiteRegEx": null, "year": 2011}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["K.-C. Toh", "Y. Sangwoon"], "venue": "Pacific Journal of Optimization", "citeRegEx": "TS10", "shortCiteRegEx": null, "year": 2010}, {"title": "Incomplete cross approximation in the mosaic-skeleton method", "author": ["E. Tyrtyshnikov"], "venue": "Computing", "citeRegEx": "Tyr00", "shortCiteRegEx": null, "year": 2000}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "NIPS", "citeRegEx": "WS00", "shortCiteRegEx": null, "year": 2000}, {"title": "A scalable cur matrix decomposition algorithm: Lower time complexity and tighter bound", "author": ["S. Wang", "Z. Zhang"], "venue": "NIPS", "citeRegEx": "WZ12", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving cur matrix decomposition and the nystr\u00f6m approximation via adaptive sampling", "author": ["S. Wang", "Z. Zhang"], "venue": "JMLR, 14(1):2729\u20132769", "citeRegEx": "WZ13", "shortCiteRegEx": null, "year": 2013}, {"title": "Speedup matrix completion with side information: Application to multi-label learning", "author": ["M. Xu", "R. Jin", "Z.-H. Zhou"], "venue": "NIPS,", "citeRegEx": "XJZ13", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 34, "context": "Examples include user-item rating matrix in recommender system [SRJ04], gene expression matrix in bioinCorresponding author.", "startOffset": 63, "endOffset": 70}, {"referenceID": 24, "context": "formatics [MD08], kernel matrix in kernel learning [WS00], document-term matrix in document retrieval [MD08], and instance-label matrix in multi-label learning [GZR+10].", "startOffset": 10, "endOffset": 16}, {"referenceID": 39, "context": "formatics [MD08], kernel matrix in kernel learning [WS00], document-term matrix in document retrieval [MD08], and instance-label matrix in multi-label learning [GZR+10].", "startOffset": 51, "endOffset": 57}, {"referenceID": 24, "context": "formatics [MD08], kernel matrix in kernel learning [WS00], document-term matrix in document retrieval [MD08], and instance-label matrix in multi-label learning [GZR+10].", "startOffset": 102, "endOffset": 108}, {"referenceID": 34, "context": "Various methods have been developed for low rank matrix approximation, including truncated singular value decomposition, matrix factorization [SRJ04], matrix regression [Kol11], column subset selection [BDMI11], the Nystr\u00f6m method [WS00].", "startOffset": 142, "endOffset": 149}, {"referenceID": 21, "context": "Various methods have been developed for low rank matrix approximation, including truncated singular value decomposition, matrix factorization [SRJ04], matrix regression [Kol11], column subset selection [BDMI11], the Nystr\u00f6m method [WS00].", "startOffset": 169, "endOffset": 176}, {"referenceID": 1, "context": "Various methods have been developed for low rank matrix approximation, including truncated singular value decomposition, matrix factorization [SRJ04], matrix regression [Kol11], column subset selection [BDMI11], the Nystr\u00f6m method [WS00].", "startOffset": 202, "endOffset": 210}, {"referenceID": 39, "context": "Various methods have been developed for low rank matrix approximation, including truncated singular value decomposition, matrix factorization [SRJ04], matrix regression [Kol11], column subset selection [BDMI11], the Nystr\u00f6m method [WS00].", "startOffset": 231, "endOffset": 237}, {"referenceID": 25, "context": "In this work, we will focus on the CUR algorithm for low rank matrix approximation [MD09].", "startOffset": 83, "endOffset": 89}, {"referenceID": 25, "context": "Compared to other low rank approximation algorithms, CUR is advantageous in that it has (i) an easy interpretation of the approximation result because the subspace is constructed by the actual columns and rows of the target matrix [MD09], and (ii) strong (near-optimal) theoretical guarantee [BXM10, DKM06, MD08, MD09, WZ12, WZ13].", "startOffset": 231, "endOffset": 237}, {"referenceID": 25, "context": "The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08].", "startOffset": 111, "endOffset": 117}, {"referenceID": 29, "context": "The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08].", "startOffset": 143, "endOffset": 150}, {"referenceID": 29, "context": "The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08].", "startOffset": 178, "endOffset": 185}, {"referenceID": 28, "context": "The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08].", "startOffset": 224, "endOffset": 231}, {"referenceID": 24, "context": "The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08].", "startOffset": 252, "endOffset": 258}, {"referenceID": 10, "context": "Since most matrix completion algorithms are developed only for matrices of exactly low rank, they usually work poorly for matrices of full rank [EBN11].", "startOffset": 144, "endOffset": 151}, {"referenceID": 22, "context": "We note that although an adaptive sampling approach is developed in [KS13] that does apply to matrices of full rank, they use a different sampling strategy, and their bound has a poor dependence on failure probability \u03b4 (i.", "startOffset": 68, "endOffset": 74}, {"referenceID": 22, "context": "In particular, to perfectly recover a rank-r matrix of size n \u00d7 n, only O(nr ln r) observed entries are needed, significantly lower than O(nr ln n) for standard matrix completion theory [CR12, CT10, Gro11, KMO10, Rec11] and lower than O(nr3/2 ln r) for adaptive algorithm for matrix recovery [KS13].", "startOffset": 292, "endOffset": 298}, {"referenceID": 8, "context": "[DKM06] gives an additive error bound for the CUR decomposition, and a relative error bound, a significantly stronger result, is given in [MD08].", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "[DKM06] gives an additive error bound for the CUR decomposition, and a relative error bound, a significantly stronger result, is given in [MD08].", "startOffset": 138, "endOffset": 144}, {"referenceID": 29, "context": "[MTJ11] proposes a divide-and-conquer method to compute the CUR decomposition in parallel.", "startOffset": 0, "endOffset": 7}, {"referenceID": 41, "context": "[WZ13] proposes an adaptive CUR algorithm with much tighter error bound and much lower time complexity.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "In [DKM06], the authors suggest a simple uniform sampling of columns and rows for the CUR decomposition when the maximum statistical leverage scores, also referred to as incoherence measure [CR12, CT10, Rec11], is limited.", "startOffset": 3, "endOffset": 10}, {"referenceID": 26, "context": "In [MDMIW12], algorithms have been developed to efficiently compute the approximated values of statistical leverage scores without having to calculate the SVD decomposition of a large matrix.", "startOffset": 3, "endOffset": 12}, {"referenceID": 39, "context": "A special case of column subset selection is Nystr\u00f6m methods, which is usually used to approximate Positive Semi-Definitive (PSD) matrix in kernel learning [WS00].", "startOffset": 156, "endOffset": 162}, {"referenceID": 22, "context": "[KS13] developed an adaptive sensing strategy for matrix completion that removes an lnn factor from the sample complexity.", "startOffset": 0, "endOffset": 6}, {"referenceID": 42, "context": "In [XJZ13], the authors show that the sample complexity for perfect matrix recovery can be reduced dramatically with appropriate side information.", "startOffset": 3, "endOffset": 10}, {"referenceID": 24, "context": "on their statistical leverage scores [MD08] and adaptive sampling [KS13, WZ12], can be more effective.", "startOffset": 37, "endOffset": 43}, {"referenceID": 24, "context": "We do not choose these sampling methods because they either require an access to the full matrix [MD08], introduce serious overhead in computation [WZ12], or result in significantly worse bound when matrix is of full rank [KS13].", "startOffset": 97, "endOffset": 103}, {"referenceID": 40, "context": "We do not choose these sampling methods because they either require an access to the full matrix [MD08], introduce serious overhead in computation [WZ12], or result in significantly worse bound when matrix is of full rank [KS13].", "startOffset": 147, "endOffset": 153}, {"referenceID": 22, "context": "We do not choose these sampling methods because they either require an access to the full matrix [MD08], introduce serious overhead in computation [WZ12], or result in significantly worse bound when matrix is of full rank [KS13].", "startOffset": 222, "endOffset": 228}, {"referenceID": 30, "context": "accelerated gradient descent [Nes03]).", "startOffset": 29, "endOffset": 36}, {"referenceID": 22, "context": "Method CUR+ [KS13] [BJ14] [JNS13] [CR12, CT10, CBSW14, KMO10, Rec11] # Observation nr ln r nr3/2 ln r nr2 nr4.", "startOffset": 12, "endOffset": 18}, {"referenceID": 2, "context": "Method CUR+ [KS13] [BJ14] [JNS13] [CR12, CT10, CBSW14, KMO10, Rec11] # Observation nr ln r nr3/2 ln r nr2 nr4.", "startOffset": 19, "endOffset": 25}, {"referenceID": 17, "context": "Method CUR+ [KS13] [BJ14] [JNS13] [CR12, CT10, CBSW14, KMO10, Rec11] # Observation nr ln r nr3/2 ln r nr2 nr4.", "startOffset": 26, "endOffset": 33}, {"referenceID": 22, "context": "We should note that unlike [KS13] where the incoherence measure is only assumed for column vectors, we assume a small incoherence measure for both row and column vectors here.", "startOffset": 27, "endOffset": 33}, {"referenceID": 22, "context": "stronger assumption that allows us to sample both rows and columns, leading to the improvement in the sample complexity from O(nr3/2 ln r) in [KS13] to O(nr ln r).", "startOffset": 142, "endOffset": 148}, {"referenceID": 11, "context": "In order to effectively capture the skewed singular value distribution, we introduce the concept of numerical rank r(M,\u03b7) [GL96] with respect to non-negative constant \u03b7 > 0 r(M,\u03b7) = m \u2211 i=1 \u03c32 i \u03c32 i +mn\u03b7 Note that when \u03b7 = 0, the numerical rank is equivalent to the true rank of the matrix.", "startOffset": 122, "endOffset": 128}, {"referenceID": 41, "context": "Detailed information of these data sets can be found in [WZ13].", "startOffset": 56, "endOffset": 62}, {"referenceID": 22, "context": "Although the adaptive sampling based approaches [KS13] usually yield lower errors than the standard CUR algorithm, they do not choose observed entries randomly and therefore are not included in the comparison.", "startOffset": 48, "endOffset": 54}, {"referenceID": 40, "context": "Finally, we follow the experimental protocol specified in [WZ12] by repeating every experiment 10 times and reporting the mean value.", "startOffset": 58, "endOffset": 64}, {"referenceID": 22, "context": "Since adaptive sampling has shown promising results for low rank matrix approximation [KS13], in the future, we plan to combine the proposed algorithm with adaptive sampling strategy to further reduce the error bound.", "startOffset": 86, "endOffset": 92}, {"referenceID": 16, "context": "1 in [HMT11]) Let M be an n\u00d7m matrix with singular value decomposition M = U\u03a3V \u22a4.", "startOffset": 5, "endOffset": 12}, {"referenceID": 36, "context": "2 of [Tro11]) Let X be a finite set of PSD matrices with dimension k (means the size of the square matrix is k \u00d7 k).", "startOffset": 5, "endOffset": 12}, {"referenceID": 23, "context": "4 and discussion in Section 5 from [Li99].", "startOffset": 35, "endOffset": 41}], "year": 2014, "abstractText": "CUR matrix decomposition computes the low rank approximation of a given matrix by using the actual rows and columns of the matrix. It has been a very useful tool for handling large matrices. One limitation with the existing algorithms for CUR matrix decomposition is that they need an access to the full matrix, a requirement that can be difficult to fulfill in many real world applications. In this work, we alleviate this limitation by developing a CUR decomposition algorithm for partially observed matrices. In particular, the proposed algorithm computes the low rank approximation of the target matrix based on (i) the randomly sampled rows and columns, and (ii) a subset of observed entries that are randomly sampled from the matrix. Our analysis shows the relative error bound, measured by spectral norm, for the proposed algorithm when the target matrix is of full rank. We also show that only O(nr ln r) observed entries are needed by the proposed algorithm to perfectly recover a rank r matrix of size n \u00d7 n, which improves the sample complexity of the existing algorithms for matrix completion. Empirical studies on both synthetic and real-world datasets verify our theoretical claims and demonstrate the effectiveness of the proposed algorithm.", "creator": "LaTeX with hyperref package"}}}