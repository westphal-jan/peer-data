{"id": "1411.2679", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2014", "title": "Inferring User Preferences by Probabilistic Logical Reasoning over Social Networks", "abstract": "we propose a recipe for inferring those psychological beliefs into dislike of users actively generating probabilistic first - rule biological reasoning concepts which present behavioral world. her platform answers questions about twitter moments like { \\ com does @ user produce sushi? } if { \\ ev is we clicking a hello ki knicks connection? } by implementing unique probabilistic model documenting reasons frequent user interactions ( the spouse's location ~ gender ) illustrate relevant social relations ( another member's friends and feelings ), via inferences possibly homophily ( actors would equally frequently to help society if spouse benefits friends like sushi, i am more likely to like the self if i guess in new york ). ` trick uses distant supervision, semi - supervised data harvesting while analytic space models to extracted choice phrases ( e. u. spouse, education, location ) and preferences ( likes and losers ) from text. hypothetical individual propositions are currently accumulated into a probabilistic reasoner ( we investigate both markov logic and probabilistic logic logic ). our experiments argue that probabilistic logical reasoning significantly indicates reasoning sensitivity on attribute and relation extraction, thereby also achieves an f - 105 accuracy 0. 791 accurately predicting the party buying your con, significantly high than two strong baselines.", "histories": [["v1", "Tue, 11 Nov 2014 01:53:21 GMT  (106kb,D)", "http://arxiv.org/abs/1411.2679v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.AI cs.CL cs.LG", "authors": ["jiwei li", "alan ritter", "dan jurafsky"], "accepted": false, "id": "1411.2679"}, "pdf": {"name": "1411.2679.pdf", "metadata": {"source": "CRF", "title": "Inferring User Preferences by Probabilistic Logical Reasoning over Social Networks", "authors": ["Jiwei Li", "Alan Ritter", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu", "Ritter.1492@osu.edu", "jurafsky@stanford.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "Categories and Subject Descriptors H.0 [Information Systems]: General\nKeywords Logical Reasoning, User Attribute Inference, Social Networks"}, {"heading": "1. INTRODUCTION", "text": "Extracting the latent attitudes or preferences of users on the web is an important goal, both for practical applications like product recommendation, targeted online advertising, friend recommendation, or for helping social scientists and political analysts gain insights into public opinion and user behavior.\nEvidence for latent preferences can come both from attributes of a user or from preferences of other people in their social network. For example people from Illinois may be more likely to like the\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nChicago bears, while people whose friends like sushi may be more likely to like sushi.\nA popular approach to draw on such knowledge to help extract user preferences is to make use of collaborative filtering, typically applied on structured data describing explicitly provided user preferences (e.g. movie ratings), and often enriched by information from a social network [18, 25, 35, 38, 52]. These methods can thus combine information from shared preferences and attributes with information about social relations.\nIn many domains, however, these user preferences and user attributes are not explictly provided, and we may not even have explicit knowledge of relations in the social network. In such cases, we may first need to estimate these latent attributes or preferences, resulting in only probabilistic estimates of each of these sources of knowledge. How can we reason about user preferences given only weak probabilistic sources of knowledge about users\u2019 attributes, preferences, and social ties? This problem occurs in domains like Twitter, where knowledge about users\u2019 attitudes, attributes, and social relations must be inferred.\nWe propose to infer user preferences on domains like Twitter without explicit information by applying relational reasoning frameworks like Markov Logic Networks (MLN) [71] and Probabilistic Soft Logic (PSL) [26] to help infer these relational rules. Such probabilistic logical systems are able to combine evidence probabilistically to draw logical inference.\nFor example, such systems could learn individual probabilistic inference rules expressing facts like \u201cpeople who work in IT companies like electronic devices\" with an associated probability (in this case 0.242):\nWORK-IN-IT-COMPANY(A)\u21d2 LIKE-ELECTRONIC-DEVICE (0.242)\nSuch systems are also able to perform global inference over an entire network of such rules, combining probabilistic information about user attributes, preference, and relations to predict preferences of other users.\nOur algorithm has two stages. In the first stage we extract user attributes. Unlike structured knowledge bases such as Freebase and Wikipedia, propositional knowledge describing the attributes of entities in social networks is very sparse. Although some social media websites (such as Facebook or LinkedIn) do support structured data format of personal attributes, these attributes may still be sparse, since only a small proportion of users fill out any particular profile fact, and many sites (such as Twitter) do not provide them at all. On the other hand, users of online social media frequently publish messages describing their preferences and activities, often explicitly mentioning attributes such as their JOB, RELIGION, or EDUCATION [48]. We propose a text extraction system\nar X\niv :1\n41 1.\n26 79\nv1 [\ncs .S\nI] 1\n1 N\nov 2\n01 4\nfor Twitter that combines supervision [15], semi-supervised data harvesting (e.g., [40, 41]) and vector space models [5, 55] to automatically extract structured profiles from the text of users\u2019 messages. Based on this approach, we are able to construct a comprehensive list of personal attributes which are explicitly mentioned in text (e.g., LIKE/DISLIKE, LIVE-IN, WORK-FOR) and user relations (e.g., FRIEND,COUPLE).\nWhile coverage of user profile information can dramatically be increased by extracting information from text, not all users explicitly mention all of their attributes. To address this, in the second stage of our work we further investigate whether it is possible to extend the coverage of extracted user profiles by inferring attributes not explicitly mentioned in text through logical inference.\nFinally, we feed the extracted attributes and relations into relational reasoning frameworks, including Markov Logic Networks (MLN) [71] and Probabilistic Soft Logic (PSL) [26], to infer the relational rules among users, attributes and user relations that allow us to predict user preferences.\nWe evaluate the system on a range of prediction tasks including preference prediction (liking or disliking) but also attributes like location or relations like friend-of.\nThe system described in this paper provides new perspectives for understanding, predicting interests, tendencies and behaviors of social media users in everyday life. While our experiments are limited to one dataset, Twitter, the techniques are general and can be easily adapted with minor adjustments. The major contributions of this paper can be summarized as follows: \u2022 We present an attempt to perform probabilistic logical rea-\nsoning on social networks. \u2022 Our framework estimates the attributes of online social me-\ndia users without requiring explicit mentions. \u2022 Our framework combines probabilistic information about user\nattributes, preferences, and relations to predict latent relations and preferences. \u2022 We present a large-scale user dataset specific for this task. The next sections show how user attributes, relations, and preferences are extracted from text and introduce the probabilistic logical frameworks. Our algorithm and results are illustrated in Section 4 and 5."}, {"heading": "2. EXTRACTING PROBABILISTIC LOGICAL PREDICATES", "text": "Given the message streams from Twitter users, our first task is to extract information about user attributes, relations, and preferences in logical form; these will then be input to our global logical inference network.\nWe represent these facts by two kinds of propositional logic objects: predicates and functions. Functions represent mappings from an object to another object, returning an object, as in CAPITALOF(FRANCE)=PARIS. Predicates represent whether a relation holds among two objects and return a boolean value. For example, if usrA and usrB are friends on Twitter, the predicate ISFRIEND (USRA,USRB)= TRUE. Predicates and functions can be transformed to each other. Given the function WIFEOF(USRA)=USRB, the predicate ISCOUPLE(USRA,USRB)=TRUE will naturally hold. As we will demonstrate later, all functions will be transformed to predicates in graph construction procedure."}, {"heading": "2.1 Dataset", "text": "We use a random sample of Twitter users\u2014after discarding users with less than 10 tweets\u2013 consisting of 0.5 million Twitter users. We crawled their published tweets and their network using the Twit-\nter API1, resulting in a dataset of roughly 75 million tweets."}, {"heading": "2.2 User Attributes", "text": "In the next sections we first briefly describe how we extract predicates for user attributes (location, education, gender) and user relations (friend, spouse), and then focus in detail on the extraction of user preferences (like/dislike).\n2.2.1 Location Our goal is to associate one of the 50 states of the United States\nwith each user. While there has been a significant amount of work on inferring the location of a given published tweet (e.g., [10, 17, 78]), there is less focus on user-level inference. In this paper, we employ a rule-based approach for user-location identification. We selected out all geo-tagged tweets from a specific user, and say an entity e corresponds to the location of current user i if it satisfies the following criteria, designed to ensure high-precision (although with a natural corresponding drop in recall):\n1. user i published more than 10 tweets from location e 2. user i published from location e in at least three different\nmonths of a year. We only consider locations within the United States and entities are matched to state names via Google Geocoding. In the end, we are able to extract locations for 1.1% of the users from our dataset.\n2.2.2 Education/Job Job and education attributes are extracted by combining a rule\nbased approach with an existing probabilistic system described in [48].\nFirst, for each user, we obtained his or her full name and fed it into a Google+ API2. Many Google+ profiles are publicly accessible and many users explicitly list attributes such as their education and job. The major challenge involved here is name disambiguation, to match users\u2019 Twitter accounts to Google+ accounts.3 We adopted the friend \u2212 shared strategy taken in [48] that if more than 10 percent of and at least 20 friends are shared by Google+ circles and Twitter followers, we assume that the two accounts point to the same person. 0.8 percent of users\u2019 job or education attributes are finalized based on their Google+ accounts.\nFor cases where user names can not be disambiguated or no relevant information is obtained from Google+, we turn to the system provided by Li et al. [48] (for details about algorithms in [48], see Section 6). This system extracts education or job entities from the published Twitter content of the user, For each Twitter user, the system returns the education or job entity mentioned in the users Tweets, associated with a corresponding probability, for example,\nWORK-IN-GOOGLE (USER) = 0.6\nSince the Li et al. system system requires the user to explicitly mention their education or job entities in their published content, it is again low-recall: another 0.5 percent of users\u2019 job or education attributes are inferred from the system.\n2.2.3 Gender Many frameworks have been devoted to gender prediction from\nTwitter posts (e.g., [9, 12, 66, 84]) studying whether high level tweet features (e.g., link, mention, hashtag frequency) can help in the absence of highly-predictive user name information. Since our 1Due to API limitations, we can crawl at most 2,000 tweets for each user. 2https://developers.google.com/+/api/ 3A small property of Google+ accounts contain direct Twitter links. In those cases, accounts can be directly matched.\ngoal is not guessing the gender without names but rather studying the extent to which global probabilistic logical inference over the social network can improve the accuracy of local predictors, we implement a high-precision rule based approach that uses the national Social Security Gender Database (SSGD)4. SSGD contains firstname records annotated for gender for every US birth since 1880 A.D. Many names are highly gender-specific, while others are ambiguous. We assign a user a gender if his/her first name appears in the dataset for one gender at least 20 times as often as for the other. Using this rule we assign gender to 78% of the users in our dataset."}, {"heading": "2.3 User Relations", "text": "The user-user relations we consider in this work include FRIEND (USRA, USRB), SPOUSE (USRA, USRB) and LIVEINSAMEPLACE (USRA, USRB). Friend: Twitter supports two types of following patterns, FOLLOWING and FOLLOWED. We consider two people as friends if they both follow each other (i.e. bidirectional following). Thus if relation FRIEND(USRA,USRB) holds, usrA has to be both following and followed by usrB. The friend relation is extracted straightforwardly from the Twitter network. Spouse/Boyfriend/Girlfriend: For the spouse relation, we again turn to Li et al.\u2019s system [48]. For any two given Twitter users and their published contents, the system returns a score Sspouse in the range of [0,1] indicating how likely SPOUSE(USR1,USR2) relation is to hold. We use a threshold of 0.5 and then for any pair of users with a higher score than 0.5, we use a continuous variable to denote the confidence, the value of which is computed by linearly projecting Sspouse into [0,1] space. LiveInSamePlace: Straightforwardly inferred from the location extraction approach described in 2.2.1."}, {"heading": "2.4 User Preferences: Like and Dislike", "text": "We now turn to user preferences and attitudes\u2014a central focus of our work\u2014and specifically the predicates LIKE(USR,ENTITY) and DISLIKE(USR,ENTITY). Like the large literature on sentiment analysis from social media (e.g., [1, 39, 65, 79]). our goal is to extract sentiment, but in addition to extract the target or object of the sentiment. Our work thus resembles other work on sentiment target extraction ([11, 36, 91]) using supervised classifiers or sequence models based on manually-labeled datasets. Unfortunately, manually collecting training data in this task is problematic because (1) tweets talking about what the user LIKES/DISLIKES are very sparsely distributed among the massive number of topics people discuss on Twitter and (2) tweets expressing what the user LIKES exist in a great variety of scenarios and forms.\nTo deal with data sparsity issues, we collect training data by combining semi-supervised information harvesting techniques [16, 40, 41, 47] and the concept of distant supervision [15, 24, 57] as follows:\nSemi-supervised information harvesting: We applied the standard seed-based information-extraction method of obtaining training data recursively by using seed examples to extract patterns, which are used to harvest new examples, which are further used as new seeds to train new patterns. We begin with pattern seeds including \u201cI _ _ like/love/enjoy (entity)\", \u201cI _ _ hate/dislike (entity)\", \u201c(I think) (entity) is good/ terrific/ cool/ awesome/ fantastic\", \u201c(I think) (entity) is bad/terrible/awful suck/sucks\". Entities extracted here should be nouns, which is determined by a Twitter-tuned POS package [64].\nBased on the harvested examples from each iteration, we train 3 machine learning classifiers: 4http://www.ssa.gov/oact/babynames/names.zip\n\u2022 A tweet-level SVM classifier (tweet-model 1) to distinguish between tweets that intend to express like/dislike properties and tweets for all other purposes. \u2022 A tweet-level SVM classifier (tweet-model 2) to distinguish\nbetween like and dislike5. \u2022 A token-level CRF sequence model (entity-model) to iden-\ntify entities that are the target of the users like/dislike. The SVM classifiers are trained using the SVMlight package [34] with the following features: \u2022 Unigram, bigram features with corresponding part-of-speech\ntags and NER labels. \u2022 Dictionary-derived features based on a subjectivity lexicon\n[90]. The CRF model [43] is trained using the CRF++ package6 based on the following features: \u2022 Current word, context words within a window of 3 words and\ntheir part-of-speech tags. \u2022 Name entity tags and corresponding POS tags. \u2022 Capitalization and word shape.\nTrained models are used to harvest more examples, which are further used to train updated models. We do this iteratively until the stopping condition is satisfied.\nDistant Supervision: The main idea of distant supervision is to obtain labeled data by drawing on some external sort of evidence. The evidence may come from a database7 or common-sense knowledge8. In this work, we assume that if a relation LIKE(USR, ENTITY) holds for a specific user, then many of their published tweets mentioning the ENTITY also express the LIKE relationship and are therefore treated as positive training data. Since semi-supervised approaches heavily rely on seed quality [41] and the patterns derived by the recursive framework may be strongly influenced by the starting seeds, adding in examples from distant supervision helps increase the diversity of positive training examples.\nAn overview of our algorithm showing how the semi-supervised approach is combined with distant supervision is illustrated in Figure 1.\nBegin Train tweet classification model (SVM) and entity labeling model (CRF) based on positive/negative data harvested from starting seeds. While stopping condition not satisfied:\n1. Run classification model and labeling model on raw tweets. Add newly harvested positive tweets and entities to the positive dataset. 2. For any user usr and entity entity, if relation LIKE(USR,ENTITY) holds, add all posts published by usr mentioning entity to positive training data.\nEnd\nFigure 1: Algorithm for training data harvesting for extraction user LIKE/DISLIKE preferences.\n5We also investigated a 3-class classifier for like, dislike and notrelated, but found the performance constantly underperforms using separate classifiers. 6https://code.google.com/p/crfpp/ 7For example, if datasets says relation ISCAPITAL holds between Britain and London, then all sentences with mention of \u201cBritain\" and \u201cLondon\" are treated as expressing ISCAPITAL relation [57, 75]. 8Tweets with happy emoticons such as :-) : ) are of positive sentiment [24].\nStopping Condition: To decide the optimum number of steps for the algorithm to stop, we manually labeled a dataset which contains 200 positive tweets (100 like and 100 dislike) with entities., selected from the original raw tweet dataset rather than the automatically harvested data. The positive dataset is matched with 800 negative tweets. For each iteration of data harvesting, we evaluate the performance of the classification models and labeling model on this human-labeled dataset, which can be viewed as a development set for parameter tuning. Results are reported in Table 1. As can be seen, the precision score decreases as the algorithm iterates, but the recall rises. The best F1 score is obtained at the end of third round of iteration.\nFor evaluation purposes, data harvesting without distant supervision (NO-DISTANT) naturally constitutes a baseline. Another baseline we employ is to train a one-step CRF model which directly decide whether a specific token corresponds to a LIKE/DISLIKE entity rather than making tweet-level decision first. Both (NO-DISTANT) and ONE-STEP-CRF rely on our recursive framework and tune the number of iterations on the aforementioned gold standards. Our testing dataset is comprised of additional 100 like/dislike property related tweets (50 like and 50 dislike) with entity labels, which are then matched with 400 negative tweets. The last baseline we employ is the rule based extraction approach by using the seed patterns. We report the best performance model on the end-to-end entity extraction precision and recall. To note, end-to-end evaluation setting here is different from what it is in Table 1, as if tweet level models make erroneous decision, labels assigned at entity level would be treated as wrong,\nAs can be seen from Table 2, about three points of performance boost are obtained by incorporating user-entity information from distant supervision. Modeling tweet-level and entity-level information separately yields better performance than incorporating them in a unified model (ONE-STEP-CRF).\nWe apply the model trained in this subsection to our tweet corpora. We filter out entities that appear less than 20 times, resulting\nin roughly 40,000 different entities9 in the dataset. Entity Clustering: We further cluster the extracted entities into different groups, with an goal of answering questions like \u2018if usr1 likes films, how likely would she like the film Titanic?\u2019\nTowards this goal, we train a skip-gram neural language model [55, 56] based on the tweet dataset using word2vec where each word is represented as a real-valued, low-dimensional vector10. Skipgram language models draw on local context in order to learn similar embeddings for semantically similar words. Next we run a k-means clustering algorithm (k=20) on the extracted entities, using L2 distance. From the learned clusters, we manually selected out 12 sensible ones, including food, sports, TV and movies, politics, electronic products, albums/concerts/songs, travels, books, fashions, financial stuff, and pets/animals. Each of the identified clusters is then matched with a human label.\nLike Attribute Extracted from Network: We extract more LIKE/DISLIKE preferences by using the FOLLOWING network of Twitter. If a twitter user ei is followed by current user ej , but not bidirectionally, and that ei contains more than 100,000 followers, we treat ei as a public figure/celebrity that is liked by current user ej ."}, {"heading": "3. LOGIC NETWORKS", "text": "In this section, we describe MLN and PSL, which have been widely applied in relational learning and logic reasoning."}, {"heading": "3.1 Markov Logic", "text": "Markov Logic [71] is a probabilistic logic framework which encodes weighted first-order logic formulas in a Markov network. By translating to logic, the expression people from Illinois like the NFL football team Chicago Bears can be expressed as:\n\u2200xLIVE-IN(x, Illinois)\u21d2 LIKE(x,ChicagoBears) (1)\nReal world predicates are first converted to symbols using logical connectives and quantifiers. In MLN, each of the predicates (e.g., LIVEIN and LIKE) corresponds to a node and each formula is associated with a weighted value wi. The frameworks optimizes the following probability:\nP (X) = 1\nZ \u220f i \u03c6(xi) ni(x) (2)\nwhere \u03c6(xi) = exp(wi). Z denotes the normalization factor and xi denotes the states of nodes in the network. In our early example, x could take the following 4 values, i.e., (l1, l2), (\u00acl1, l2), (l1,\u00acl2) and (\u00acl1,\u00acl2). ni(x) is the number of true groundings for state xi. Consider the simple logic network shown in Equ. 1 with weight w, given the logic rule that f1 \u21d2 f2 is true iff f1 is false or f2 is true, we have P (l1,\u00acl2) = 1/(1 + 3 exp(w)) and the probability of each of the other three exp(w)/(1 + 3 exp(w)).\nFor inference, the probability of predicate li given the rest of the predicates is written as:\nP (li|lrest) = P (li \u2227 lrest) P (lrest)\n= \u2211 x\u2208li\u222alrest P (x|\u00b7)\u2211 x\u2208lrest P (x|\u00b7)\n(3)\nMany approaches have been proposed for fast and effective learning for MLNs [53, 63, 82]. In this work, we use the discriminative training approach [82], as will be demonstrated in Section 4.1. 9Consecutive entities with same type of NER labels are merged. 10Word embedding dimension is set to 200"}, {"heading": "3.2 Probabilistic Soft Logic", "text": "PSL [4, 37] is another sort of logic reasoning architecture. It first associates each predicate l with a soft truth value I(l). Based on such soft truth values, PSL performs logical conjunction and disjunction in the following ways:\nI(l1 \u2228 l2) = max{0, I(l1) + I(l2)\u2212 1} I(l1 \u2227 l2) = min{1, I(l1) + I(l2)}\n(4)\nNext a given formula l1 \u21d2 l2 is said to be satisfied if I(l1) \u2264 I(l2). PSL defines a variable d(r), the \u2018distance to satisfaction\u2019, to capture how far rule r is from being true. d(r) is given by max{0, I(l2)\u2212 I(l1)}. For example, if\nI(SPOUSE(USR1,USR2)) = 1 (5)\nand\nI(LIKE(USR1,ENTITY1) = 0.6 (6)\nthen\nI(SPOUSE(USR1,USR2) \u2227 LIKE(USR1,ENTITY1)) = max(0, 1 + 0.6\u2212 1) = 0.6 (7)\nPSL is optimized through maximizing observed rules in terms of distant d(r):\nP (I) = 1\nZ exp[\u2212 \u2211 r \u03bbr(d(I))]\nZ = \u222b I 1 Z exp[\u2212 \u2211 r \u03bbr(d(I))]\n(8)\nwhereZ denotes the normalization factor, and \u03bbr denotes the weight for formula. Inference can be straightforwardly performed by calculating the distance d between the predicates. Compared with MLN, the PSL framework can be efficiently optimized based on a linear program. Another key distinguishing feature for PSL is that it uses continuous variables (soft truth values) rather than binary ones in MLN."}, {"heading": "4. LOGIC REASONING ON SOCIAL NETWORKS", "text": "Based on our extraction algorithm in Section 2, each user i, is associated with a list of attributes and preferences, and is related by various relations to other users in a network. Function symbols are transformed to predicates for graph construction, where all the nodes in the graph take on binary values (i.e., true or false)."}, {"heading": "4.1 Assumptions and Simplifications", "text": "As existing algorithms might be difficult to scale up to the size of users and attributes we consider, we make some assumptions to enable faster learning and inference:\nCut off Edges: If relations LIKE (USRA, ENTITY1), LIKE (USRB, ENTITY2) and FRIEND (USRA, USRB) hold, but ENTITIY1 and ENTITY2 are from different like-entity categories, we would say LIKE (USR1, ENTITY1) and FRIEND (USR1, USR2) are independent, which means there would be no edge connecting nodes LIKE (ENTITY1) and LIKE (ENTITY2) in the Markov network. As an example, if usrA likes fish and usrB likes football, as fish and football belong to different entity categories, we would treat these two predicates as independent.\nDiscriminative Training for MLN: We use the approach described in [82] where we assume that we have a priori knowledge about which predicates will be evidence and which ones will be\nqueried. Instead of optimizing over all nodes along the graph, the system optimizes the probability of predicting the queried nodes given evidence nodes. This prunes a large number of branches. Let Y be the set of queried models and X be evidence nodes, the system optimizes the conditional probability as follows:\np(Y |X) = 1 Z exp( \u2211 i\u2208FY wini(x, y)) (9)\nwhere FY denotes all cliques with at least one node involving a query node."}, {"heading": "4.2 Modeling Missing Values", "text": "A major challenge is missing values, for example in situations where users do not mention an entity; a user not mentioning one entity does not necessarily mean they do not like it. Consider the following situation:\nFRIEND(A,B)\u2227LIKE(A,SOCCER) \u21d2 LIKE(B,SOCCER)\n(10)\nDrawing this inference in this way is requires that (1) usrB indeed likes soccer (2) usrB explicitly mentions soccer in his or her posts. Satisfying both premises (especially the latter one) is a luxury. Inspired by common existing approaches to deal with missing data [44, 51], we treat users\u2019 LIKE/DISLIKE preferences as latent variables, while what is observed is whether users explicitly mention their preferences in their posts. The latent variables and observed variables are connected via a binary distribution parameterized by a [0,1] variable Sentity , indicating how likely a user would be to report the correspondent entity in their posts.\nFor MLN, a brief illustration is shown in Figure 2. The conditional probability can be expressed by summing over latent variables. The system can be optimized by incorporating a form of EM algorithm into MLN [83].\nFor PSI, each entity is associated with an additional predicate MENTION (USR, ENTITY), denoting the situation where any given user publishes posts about one specific entity. Predicate PUBLISHENTITY(USR) comes with the following constraints : \u2022 \u00ac LIKE-ENTITY(USR)\u2227PUBLISH-ENTITY(USR)=0 \u2022 \u00ac DISLIKE-ENTITY(USR)\u2227PUBLISH-ENTITY(USR)=0 which can be interpreted as saying that a user would mention his like or dislike towards an entity only if he likes or dislikes it.\n4.3 Inference\nInference is performed on two settings: FRIEND-OBSERVED and NEIGH-LATENT11. FRIEND-OBSERVED addresses the leave-oneout testing to infer one specific attribute or relation given all the rest. FRIEND-LATENT refers to a another scenario where some of the attributes (or other information) for multiple users along the network are missing and require joint inference over multiple values along the graph. Real world applications, where network information can be partly retrieved, likely fall in between.\nInference for the FRIEND-OBSERVED setting is performed directly from the standard MLN and PSL inference framework, which is implemented using MCMC for MLN and MPE (Most Probable Explanation) for PSL. For the FRIEND-LATENT setting, we need to jointly infer location attributes along the users. As the objective function for joint inference would be difficult to optimize (especially since inference on MLN is hard) and existing algorithms may not able to scale up to the size of network we consider, we turn to a greedy approach inspired by recent work [48, 68]: attributes are initialized from the logic network based on given attributes where missing values are not considered. Then for each user along the network, we iteratively re-estimate their attributes given the evidence both from her own attribute values and her friends by performing standard inference in MLN or PSL. In this way, highly confident predictions will be made based on individual features in the first round, then user-user relations would either support or contradict these decisions. We run 3 rounds of iterations. We expect FRIENDOBSERVED to yield better results than the FRIEND-LATENT setting since the former benefits from gold network information [47]."}, {"heading": "5. EXPERIMENTS", "text": "We now turn to our experiments on using global inference across the logic networks to augment the individual local detectors to infer user attributes, user relations and finally user preferences. These results are based on the datasets extracted in the previous section, where each user is represented with a series of extracted attribute values (e.g., like/dislike, location, gender) and users are connected along the social network. We use 90% of the data as training corpus, reserving 10% for testing, from which we respectively extract testing data for each relations, attribute, or preference, as described below.\nIn each case, our goal is to understand whether global probabilistic logical inference over the entire social network graph improves over baseline classifiers like SVNs that use only local features."}, {"heading": "5.1 User Attributes: Location", "text": "The goal of location inference is to identify the US state the user tweets from, out of the 50 states. Evaluation is performed on the subset of users for which our rule-based approach in Section 2 identified a gold-standard location with high precision. We report on two settings. The FRIEND-LATENT setting makes joint predictions for user locations across the network while the more precise FRIEND-OBSERVED setting predicts the locations of each user given all other attributes, relations, and preferences. Baselines we employ include: \u2022 Random: Assign location attributed from distribution based\non population12. \u2022 Unified: Assign the most populated state in USA (Califor-\nnia) to each user. \u2022 SVM and Naive Bayes: Train multi-class classifiers where\nfeatures are the predicted extracted attributes and network\n11We draw on a similar idea in [47]. 12http://en.wikipedia.org/wiki/List_of_U.S. _states_and_territories_by_population\nThe performances of the different models are illustrated in Table 314. As expected, FRIEND-OBSERVED outperforms FRIENDLATENT, detecting locations with an accuracy of about 0.35. onlynetwork and only-like models, where evidence is partially considered, consistently underperform settings where evidence is fully considered. Logic networks, which are capable of capturing the complicated interaction between factors and features, yield better performance than traditional SVM and Naive Bayes classifiers.\nTable 4 gives some examples based on conditional probability calculated from MLN, respectively correspond: (1) people from Illinois like Chicago Bears (2) People from Alabama like barbecue (3) People from hockey like hockey (4) People from North Dakota like Krumkake."}, {"heading": "5.2 User Attributes: Gender", "text": "We evaluate gender based on a dataset of 10,000 users (half male, half female) drawn from the users whose gold standard gender was assigned with sufficiently high precision by the social-security informed system in Section 2. We only focus on NEIGH-OBSERVE\n13For probabilistic attributes (e.g., education), values are leveraged by weights. 14This is a 50-class classification problem;accuracy for random assignment without prior knowledge is 0.02%.\nsetting. SVM baseline takes individual and network features as described in Section 5.1. Table 5 shows the results. Using the logic networks across all attributes, relations, and preferences, the accuracy of our algorithm is 0.772.\nOf course the performance of the algorithm could very likely be even higher if we were to additionally incorporating features designed directly for the gender ID task (such as entities mentioned, links, and especially the wide variety of writing style features used in work such as [12], which achieves gender ID accuracies of 0.85 on a different dataset). Nonetheless, the fact that global probabilistic inference over the network of attributes and relations achieves such high accuracies without any such features points to the strength of the network approach.\nTable 6 gives some examples about gender preference inferred from MLN. As can be seen, males prefer sports while females prefer fashions (as expected). Females emphasize more on food and movies than males, but not significantly."}, {"heading": "5.3 Predicting Relations Between Users", "text": "We tested relation prediction on the detection of the three relations defined in section 2: FRIEND, SPOUSE and LIVEINSAMELOCATION. Positive training data is selected from pairs of users among whom one specific type of relation holds while random user pairs are used as negative examples. We weighted toward negative examples to match the natural distribution Statistics are shown in Table 10.\nFor relation evaluation, we only focus on the NEIGH-OBSERVE setting. Decisions are made by comparing the conditional probability that a specific relation holds given other types of information, for example Pr(SPOUSE(A,B)|\u00b7) and 1-Pr(SPOUSE(A,B)|\u00b7). Baselines we employ include: \u2022 SVM: We use co-occurrence of attributes as features:\nLIKE-ENTITY1(A) \u2227 LIKE-ENTITY2(B)\nFor LIVEINSAMELOCATION prediction, the location identification classifier without any global information naturally constitutes a baseline, where two users are viewed as living in the same location if classifiers trained in 5.2 assigned them the same location labels. \u2022 Random: Assign labels randomly based on the proportion of\npositive examples. We report the theoretical values of preci-\nsion and recall, which are given by:\nPre, Rec = # positive-examples #total-examples\nThe performance of the different approaches are reported in Table 10. As can be seen, the logic models consistently yield better performances than SVMs on relation prediction tasks due to their power in leveraging the global interactions between different sources of evidence."}, {"heading": "5.4 Predicing Preference: Likes or Dislikes", "text": "Evaluating our ability to detect user preferences is complex since, as mentioned in Section 4.2, we don\u2019t gold-standard labels of Twitter users\u2019 true attitudes towards different entities. That is, we don\u2019t actually know what users actually like: only what they say they like. We therefore evaluate our ability to detect what users say about an entity.\nWe evaluate two distinct tasks, beginning with the simpler: given that the user talked about an entity, was their opinion positive or negative.\nWe then proceed to the much more difficult task of predicting whether a user will talk about an entity at all, and if so whether her opinion will be positive or negative.\nIn both tasks our task is to estimate without using the text of the message. This is because our goal is to understand how useful the social network structure is alone in solving the problem. This information could then easily be combined with standard sentimentanalysis techniques in future work.\nEvaluations are performed under both the FRIEND-OBSERVED setting and the FRIEND-LATENT setting.\n5.4.1 Predicting Like/Dislike We begin with the scenario in which we know that an entity e is\nalready mentioned by user i and we try to predict a user\u2019s attribute towards e without looking at text-level evidence. The goal of this experiment is to predict sentiment (e.g., whether one likes Barack Obama) given other types of attributes of the user himself (e.g.,\nwhere he lives) or his network (e.g., whether his friends hate Mitt Romney) but without using sentiment-analysis features of the text itself.\nWe created a test set in which the like/dislike preferences are expressed toward entities (extracted in Section 2) that are frequent from our database, which has a total of 92 distinguished entities (e.g., BarackObama, New York Knicks). We extracted 1000 like examples and 1000 dislike examples (e.g., LIKE-BARACKOBAMA(USER), DISLIKE-BARACKOBAMA(USER)).\nPredictions are make by comparing Pr( LIKE-ENTITY (USER,ENTITY)|\u00b7) and Pr( DISLIKE-ENTITY (USER,ENTITY)| \u00b7). We extracted gold standards for each data point, with 0.5 random guess accuracy. Evaluations are performed in terms of prediction and recall. We only consider the NEIGH-LATENT setting.\nThe Baselines we employ include: \u2022 SVM: We train binary SVM classifiers to decide, for a spe-\ncific entity e, whether a user likes/dislikes e. Features include individual attributes values (e.g., like/dislike, location, gender, etc) and network information (attributes from his friends along the network) \u2022 Collaborative Filtering (CF): CF [18, 25, 33, 35] accounts\nfor a popular approach in recommendation system, which utilizes the information of the user-item matrix for recommendations. The key idea of CF is to recommend similar items to similar users. We view the like/dislike entity prediction as entity recommendation problem and adopt the approach described in [80] by constructing user-user similarity matrix from weighted cosine similarity calculated from shared attributes and network information. Entity-entity similarity is computed based on entity embedding (described in Section 2). As in [80], a regression model is trained to fill out {0, 1} value in user-entity matrix indicating whether a specific user likes/hates one specific entity. Prediction is then made based on a weighted nearest neighbor algorithm. Results are reported in Table 9. As can be seen, MLN and PSL outperform other baselines.\n5.4.2 Predicting Mentions of Likes/Dislikes We are still confronted with the missing value problem of Sec-\ntion 4.2, where we don\u2019t know what users actually believe, so we can only try to predict what they will say they believe. But where the previous section assumed we knew that the user talked about an entity and just predicted the sentiment, we now turn to the much more difficult task of predicting both whether a user will mention an entity and what the users attitude toward the entity is. Evaluations are again performed under the FRIEND-OBSERVED setting and the FRIEND-LATENT setting.\nWe construct the testing dataset using a random sample of 2,000 users with an average number of 3.2 like/dislike entities mentioned per user (a total number of 4,300 distinct entities). Baselines we employ include: \u2022 Random: Estimate the overall popularity of a specific entity\nbeing liked by the whole population. The probability pentity\nis given by:\npentity =\n\u2211 usr I(LIKE-ENTITY(usr) = true)\u2211\nusr I\nThe decision is made by sampling a {0, 1} variable from a binary distribution parameterized by pentity . \u2022 SVM and Naive Bayes: We train SVM and Naive Bayes\nclassifiers to decide whether a specific user would express his like/dislike attitude towards a specific entity. Features include individual attributes values and network information (For feature details, see Section 5.1). \u2022 Collaborative Filtering (CF): As described in the previous\nsection. Performances are evaluated in terms of precision and recall, reported in Table 10. Note that predicting like/dislike mention is an extremely difficult task, since users tweet about only a very very small percentage of all the entities they like and dislike in the world. Predicting which ones they will decide to talk about is a difficult task requiring much more kinds of evidence about the individual and the network that our system has access to.\nNonetheless, given the limited information we have at hand, and considering the great number of entities, our proposed model does surprisingly well, with about 7% precision and 12% recall, significantly outperforming Collaborative Filtering, SVM and Naive Bayes."}, {"heading": "6. RELATED WORK", "text": "This work is related to four different research areas. Information Extraction on Social Media : Much work has been devoted to automatic extraction of well-structured information profiles from online social media, which mainly fall into two major levels: at public level [49, 50, 90] or at user level. The former includes public event identification [19], event tracking [67] or event-referring expression extraction [74]. The latter focus on user studies, examining users\u2019 interests [3], timeline [46], personal events [47] or individual attributes such as age [70, 69], gender [12], political polarity [13], locations [78], jobs and educations [48], student information (e.g., major, year of matriculation) [58].\nThe first step of proposed approach highly relies on attribute extraction algorithm described in [48] which extracts three categories of user attributes (i.g., education, job and spouse) for a given user based on their posts. [48] gathers training data based on the concept of distant supervision where Google+ treated used as \u201cknowledge base\" to provide supervision. The algorithm returns the probability of whether the following predicates hold: WORKIN(USR,ENTITY) (job), STUDY-AT(USR,ENTITY) (education) and SPOUSE(USR1,USR2) (spouse).\nHomophily: Our work is based on the fundamental homophily property of online users [54], which assumes that people sharing\nmore attributes or background have a higher chance of becoming friends in social media15, and that friends (or couples, or people living in the same location) tend to share more attributes. Such properties have been harnessed for applications like community detection [92] or friend recommendation [27].\nData Harvesting: The techniques adopted in like/dislike attribute extraction are related to a strand of work in data harvesting/information extraction, the point of which is to use some seeds to harvest some data, which is used to learn additional rules or patterns to harvest more data [16, 31, 40, 41, 73]. Distant supervision is another methodology for data harvesting [15, 28, 57] that relies on structured data sources as a source of supervision for data harvesting from raw text.\nLogic/Relational Reasoning: Logic reasoning, usually based on first-order logic representations, can be tracked back to the early days of AI [59, 76], and has been adequately explored since then (e.g., [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]). A variety of reasoning models have been proposed, based on ideas or concepts from the fields of graphical models, relational logic, or programming languages [7, 8, 60], each of which has it own generalization capabilities in terms of different types of data. Frameworks include Stochastic Logic Programs [61] which combines logic programming and log-linear models, Probabilistic Relational Networks [23] which incorporates Bayesian networks for reasoning, Relational Markov Networks [85] that uses dataset queries as cliques and model the state of clique in a Markov network, Relational Dependency Networks [62] which combines Bayes networks and Markov networks, and probabilistic similarity logic [7] which jointly considers probabilistic reasoning about similarities and relational structure.\nA great number of applications benefit from logical reasoning, including natural language understanding (e.g., [6]), health modeling [21], group modeling [29], web link based clustering [22], object identification [20], trust analysis [30], and many more."}, {"heading": "7. CONCLUSION AND DISCUSSION", "text": "In this work, we propose a framework for applying probabilistic logical reasoning to inference problems on on social networks. Our two-step procedure first extracts logical predicates, each associated with a probability, from social networks, and then performs logical reasoning. We evaluated our system on predicting user attributes (gender, education, location), user relations (friend, spouse, samelocation), and user preferences (liking or disliking different entities). Our results show that using probabilistic logical reasoning\n15summarized by the proverb \u201cbirds of a feather flock together\" [2].\nover the network improves the accuracy of the resulting predictings, demonstrating the effectiveness of the proposed framework.\nOf course the current system is particularly weak in recall, since many true user attributes or relations are simply never explicitly expressed on platforms like Twitter. Also, the \u201cgold-standard\" firstorder logics extracted are not really gold-standard. One promising perspective is to integrate user information from different sorts of online social media. Many websites directly offer gold-standard attributes; Facebook contains user preference for movies, books, religions, musics or locations; LinkedIn offers comprehensive professional information. Combining these different types of information will offer more evidence for decision making."}, {"heading": "8. ADDITIONAL AUTHORS", "text": ""}, {"heading": "9. REFERENCES", "text": "[1] A. Agarwal, B. Xie, I. Vovsha, O. Rambow, and\nR. Passonneau. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media, pages 30\u201338. Association for Computational Linguistics, 2011.\n[2] F. Al Zamal, W. Liu, and D. Ruths. Homophily and latent attribute inference: Inferring latent attributes of twitter users from neighbors. In ICWSM, 2012.\n[3] N. Banerjee, D. Chakraborty, K. Dasgupta, S. Mittal, A. Joshi, S. Nagar, A. Rai, and S. Madan. User interests in social media sites: an exploration with micro-blogs. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1823\u20131826. ACM, 2009.\n[4] I. Beltagy, K. Erk, and R. Mooney. Probabilistic soft logic for semantic textual similarity. Proceedings of Association for Computational Linguistics (ACL-14), 2014.\n[5] Y. Bengio, H. Schwenk, J.-S. Sen\u00e9cal, F. Morin, and J.-L. Gauvain. Neural probabilistic language models. In Innovations in Machine Learning, pages 137\u2013186. Springer, 2006.\n[6] J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on freebase from question-answer pairs. In EMNLP, pages 1533\u20131544, 2013.\n[7] M. Brocheler, L. Mihalkova, and L. Getoor. Probabilistic similarity logic. arXiv preprint arXiv:1203.3469, 2012.\n[8] M. Broecheler and L. Getoor. Computing marginal distributions over continuous markov networks for statistical relational learning. In Advances in Neural Information Processing Systems, pages 316\u2013324, 2010.\n[9] J. D. Burger, J. Henderson, G. Kim, and G. Zarrella. Discriminating gender on twitter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1301\u20131309. Association for Computational Linguistics, 2011.\n[10] Z. Cheng, J. Caverlee, and K. Lee. You are where you tweet: a content-based approach to geo-locating twitter users. In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 759\u2013768. ACM, 2010.\n[11] Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. Identifying sources of opinions with conditional random fields and extraction patterns. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 355\u2013362. Association for Computational Linguistics, 2005.\n[12] M. Ciot, M. Sonderegger, and D. Ruths. Gender inference of twitter users in non-english contexts. In EMNLP, pages 1136\u20131145, 2013.\n[13] M. Conover, J. Ratkiewicz, M. Francisco, B. Gon\u00e7alves, F. Menczer, and A. Flammini. Political polarization on twitter. In ICWSM, 2011.\n[14] V. S. Costa, D. Page, M. Qazi, and J. Cussens. Clp (bn): Constraint logic programming for probabilistic knowledge. In Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence, pages 517\u2013524. Morgan Kaufmann Publishers Inc., 2002.\n[15] M. Craven, J. Kumlien, et al. Constructing biological knowledge bases by extracting information from text sources. In ISMB, volume 1999, pages 77\u201386, 1999.\n[16] D. Davidov, A. Rappoport, and M. Koppel. Fully unsupervised discovery of concept-specific relationships by web mining. 2007.\n[17] C. A. Davis Jr, G. L. Pappa, D. R. R. de Oliveira, and F. de L Arcanjo. Inferring the location of twitter messages based on user relationships. Transactions in GIS, 15(6):735\u2013751, 2011.\n[18] S. Debnath, N. Ganguly, and P. Mitra. Feature weighting in content based recommendation system using social network analysis. In Proceedings of the 17th international conference on World Wide Web, pages 1041\u20131042. ACM, 2008.\n[19] Q. Diao, J. Jiang, F. Zhu, and E.-P. Lim. Finding bursty topics from microblogs. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 536\u2013544. Association for Computational Linguistics, 2012.\n[20] P. Domingos. Multi-relational record linkage. In In Proceedings of the KDD-2004 Workshop on Multi-Relational Data Mining. Citeseer, 2004.\n[21] S. Fakhraei, B. Huang, L. Raschid, and L. Getoor. Network-based drug-target interaction prediction with probabilistic soft logic.\n[22] G. W. Flake, S. Lawrence, and C. L. Giles. Efficient identification of web communities. In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 150\u2013160. ACM, 2000.\n[23] N. Friedman, L. Getoor, D. Koller, and A. Pfeffer. Learning probabilistic relational models. In IJCAI, volume 99, pages 1300\u20131309, 1999.\n[24] A. Go, R. Bhayani, and L. Huang. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, pages 1\u201312, 2009.\n[25] M. Goeksel and C. P. Lam. System and method for utilizing social networks for collaborative filtering, Mar. 30 2010. US Patent 7,689,452.\n[26] B. Goertzel, C. Pennachin, and N. Geisweiller. Probabilistic logic networks. In Engineering General Intelligence, Part 2, pages 275\u2013291. Springer, 2014.\n[27] I. Guy, N. Zwerdling, I. Ronen, D. Carmel, and E. Uziel. Social media recommendation based on people and tags. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 194\u2013201. ACM, 2010.\n[28] R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and D. S. Weld. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 541\u2013550. Association for Computational Linguistics, 2011.\n[29] B. Huang, S. H. Bach, E. Norris, J. Pujara, and L. Getoor. Social group modeling with probabilistic soft logic. In NIPS Workshop on Social Network and Social Media Analysis: Methods, Models, and Applications, 2012.\n[30] B. Huang, A. Kimmig, L. Getoor, and J. Golbeck. Probabilistic soft logic for trust analysis in social networks. In International Workshop on Statistical Relational AI, pages 1\u20138, 2012.\n[31] S. P. Igo and E. Riloff. Corpus-based semantic lexicon induction with web-based corroboration. In Proceedings of the Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 18\u201326. Association for Computational Linguistics, 2009.\n[32] M. Jaeger et al. Probabilistic reasoning in terminological logics. KR, 94:305\u2013316, 1994.\n[33] M. Jamali and M. Ester. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the fourth ACM conference on Recommender systems, pages 135\u2013142. ACM, 2010.\n[34] T. Joachims. Making large scale svm learning practical. 1999.\n[35] H. Kautz, B. Selman, and M. Shah. Referral web: combining social networks and collaborative filtering. Communications of the ACM, 40(3):63\u201365, 1997.\n[36] S.-M. Kim and E. Hovy. Extracting opinions, opinion holders, and topics expressed in online news media text. In Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1\u20138. Association for Computational Linguistics, 2006.\n[37] A. Kimmig, S. Bach, M. Broecheler, B. Huang, and L. Getoor. A short introduction to probabilistic soft logic. In Proceedings of the NIPS Workshop on Probabilistic Programming: Foundations and Applications, pages 1\u20134, 2012.\n[38] I. Konstas, V. Stathopoulos, and J. M. Jose. On social networks and collaborative recommendation. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 195\u2013202. ACM, 2009.\n[39] E. Kouloumpis, T. Wilson, and J. Moore. Twitter sentiment analysis: The good the bad and the omg! ICWSM, 11:538\u2013541, 2011.\n[40] Z. Kozareva and E. Hovy. Learning arguments and supertypes of semantic relations using recursive patterns. In Proceedings of the 48th Annual Meeting of the Association\nfor Computational Linguistics, pages 1482\u20131491. Association for Computational Linguistics, 2010.\n[41] Z. Kozareva and E. Hovy. Not all seeds are equal: Measuring the quality of text mining seeds. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 618\u2013626. Association for Computational Linguistics, 2010.\n[42] T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer. Scaling semantic parsers with on-the-fly ontology matching. 2013.\n[43] J. Lafferty, A. McCallum, and F. C. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. 2001.\n[44] S. L. Lauritzen. The em algorithm for graphical association models with missing data. Computational Statistics & Data Analysis, 19(2):191\u2013201, 1995.\n[45] M. Lewis and M. Steedman. Combined distributional and logical semantics. TACL, 1:179\u2013192, 2013.\n[46] J. Li and C. Cardie. Timeline generation: tracking individuals on twitter. In Proceedings of the 23rd international conference on World wide web, pages 643\u2013652. International World Wide Web Conferences Steering Committee, 2014.\n[47] J. Li, A. Ritter, C. Cardie, and E. Hovy. Major life event extraction from twitter based on congratulations/condolences speech acts. In Proceedings of Empirical Methods in Natural Language Processing, 2014.\n[48] J. Li, A. Ritter, and E. Hovy. Weakly supervised user profile extraction from twitter. ACL, 2014.\n[49] C. X. Lin, B. Zhao, Q. Mei, and J. Han. Pet: a statistical model for popular events tracking in social communities. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 929\u2013938. ACM, 2010.\n[50] J. Lin, R. Snow, and W. Morgan. Smoothing techniques for adaptive online language models: topic tracking in tweet streams. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 422\u2013429. ACM, 2011.\n[51] R. J. Little and D. B. Rubin. Statistical analysis with missing data. 2002.\n[52] F. Liu and H. J. Lee. Use of social network information to enhance collaborative filtering performance. Expert Systems with Applications, 37(7):4772\u20134778, 2010.\n[53] D. Lowd and P. Domingos. Efficient weight learning for markov logic networks. In Knowledge Discovery in Databases: PKDD 2007, pages 200\u2013211. Springer, 2007.\n[54] M. McPherson, L. Smith-Lovin, and J. M. Cook. Birds of a feather: Homophily in social networks. Annual review of sociology, pages 415\u2013444, 2001.\n[55] T. Mikolov, M. Karafi\u00e1t, L. Burget, J. Cernocky\u0300, and S. Khudanpur. Recurrent neural network based language model. In INTERSPEECH, pages 1045\u20131048, 2010.\n[56] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur. Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5528\u20135531. IEEE, 2011.\n[57] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual\nMeeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003\u20131011. Association for Computational Linguistics, 2009.\n[58] A. Mislove, B. Viswanath, K. P. Gummadi, and P. Druschel. You are who you know: inferring user profiles in online social networks. In Proceedings of the third ACM international conference on Web search and data mining, pages 251\u2013260. ACM, 2010.\n[59] R. Montague. Universal grammar. Theoria, 36(3):373\u2013398, 1970.\n[60] S. Muggleton and L. De Raedt. Inductive logic programming: Theory and methods. The Journal of Logic Programming, 19:629\u2013679, 1994.\n[61] S. Muggleton et al. Stochastic logic programs. Advances in inductive logic programming, 32:254\u2013264, 1996.\n[62] J. Neville and D. Jensen. Relational dependency networks. The Journal of Machine Learning Research, 8:653\u2013692, 2007.\n[63] F. Niu, C. R\u00e9, A. Doan, and J. Shavlik. Tuffy: Scaling up statistical inference in markov logic networks using an rdbms. Proceedings of the VLDB Endowment, 4(6):373\u2013384, 2011.\n[64] O. Owoputi, B. O\u2019Connor, C. Dyer, K. Gimpel, N. Schneider, and N. A. Smith. Improved part-of-speech tagging for online conversational text with word clusters. In HLT-NAACL, pages 380\u2013390, 2013.\n[65] A. Pak and P. Paroubek. Twitter as a corpus for sentiment analysis and opinion mining. In LREC, 2010.\n[66] M. Pennacchiotti and A.-M. Popescu. A machine learning approach to twitter user classification. ICWSM, 11:281\u2013288, 2011.\n[67] A.-M. Popescu, M. Pennacchiotti, and D. Paranjpe. Extracting events and event descriptions from twitter. In Proceedings of the 20th international conference companion on World wide web, pages 105\u2013106. ACM, 2011.\n[68] K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers, M. Surdeanu, D. Jurafsky, and C. Manning. A multi-pass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 492\u2013501. Association for Computational Linguistics, 2010.\n[69] D. Rao and D. Yarowsky. Detecting latent user properties in social media. In Proc. of the NIPS MLSN Workshop, 2010.\n[70] D. Rao, D. Yarowsky, A. Shreevats, and M. Gupta. Classifying latent user attributes in twitter. In Proceedings of the 2nd international workshop on Search and mining user-generated contents, pages 37\u201344. ACM, 2010.\n[71] M. Richardson and P. Domingos. Markov logic networks. Machine learning, 62(1-2):107\u2013136, 2006.\n[72] S. Riedel, L. Yao, A. McCallum, and B. M. Marlin. Relation extraction with matrix factorization and universal schemas. 2013.\n[73] E. Riloff, R. Jones, et al. Learning dictionaries for information extraction by multi-level bootstrapping. In AAAI/IAAI, pages 474\u2013479, 1999.\n[74] A. Ritter, O. Etzioni, S. Clark, et al. Open domain event extraction from twitter. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1104\u20131112. ACM, 2012.\n[75] A. Ritter, L. Zettlemoyer, Mausam, and O. Etzioni. Modeling\nmissing data in distant supervision for information extraction. TACL, 1:367\u2013378, 2013.\n[76] J. A. Robinson. A machine-oriented logic based on the resolution principle. Journal of the ACM (JACM), 12(1):23\u201341, 1965.\n[77] C. C. D. Roth. Feature extraction languages for propositionalized relational learning.\n[78] A. Sadilek, H. Kautz, and J. P. Bigham. Finding your friends and following them to where you are. In Proceedings of the fifth ACM international conference on Web search and data mining, pages 723\u2013732. ACM, 2012.\n[79] H. Saif, Y. He, and H. Alani. Semantic sentiment analysis of twitter. In The Semantic Web\u2013ISWC 2012, pages 508\u2013524. Springer, 2012.\n[80] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on World Wide Web, pages 285\u2013295. ACM, 2001.\n[81] S. Schoenmackers, O. Etzioni, D. S. Weld, and J. Davis. Learning first-order horn clauses from web text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1088\u20131098. Association for Computational Linguistics, 2010.\n[82] P. Singla and P. Domingos. Discriminative training of markov logic networks. In AAAI, volume 5, pages 868\u2013873, 2005.\n[83] P. Singla and P. Domingos. Entity resolution with markov logic. In Data Mining, 2006. ICDM\u201906. Sixth International Conference on, pages 572\u2013582. IEEE, 2006.\n[84] C. Tang, K. Ross, N. Saxena, and R. Chen. What\u00e2A\u0306Z\u0301s in a name: A study of names, gender inference, and gender behavior in facebook. In Database Systems for Adanced Applications, pages 344\u2013356. Springer, 2011.\n[85] B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilistic models for relational data. In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence, pages 485\u2013492. Morgan Kaufmann Publishers Inc., 2002.\n[86] W. Y. Wang, K. Mazaitis, and W. W. Cohen. Programming with personalized pagerank: A locally groundable first-order probabilistic logic. Proceedings of the 22nd ACM International Conference on Information and Knowledge Management (CIKM 2013), 2013.\n[87] W. Y. Wang, K. Mazaitis, and W. W. Cohen. Proppr: Efficient first-order probabilistic logic programming for structure discovery, parameter learning, and scalable inference. Proceedings of the AAAI 2014 Workshop on Statistical Relational AI (StarAI 2014), 2014.\n[88] W. Y. Wang, K. Mazaitis, and W. W. Cohen. Structure learning via parameter learning. Proceedings of the 23rd ACM International Conference on Information and Knowledge Management (CIKM 2014), 2014.\n[89] W. Y. Wang, K. Mazaitis, N. Lao, T. Mitchell, and W. W. Cohen. Efficient inference and learning in a large knowledge base: Reasoning with extracted information using a locally groundable first-order probabilistic logic. in progress, 2014.\n[90] J. Wiebe, T. Wilson, and C. Cardie. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165\u2013210, 2005.\n[91] B. Yang and C. Cardie. Joint inference for fine-grained opinion extraction. In ACL (1), pages 1640\u20131649, 2013.\n[92] J. Yang and J. Leskovec. Overlapping community detection at scale: a nonnegative matrix factorization approach. In Proceedings of the sixth ACM international conference on Web search and data mining, pages 587\u2013596. ACM, 2013."}], "references": [{"title": "Sentiment analysis of twitter data", "author": ["A. Agarwal", "B. Xie", "I. Vovsha", "O. Rambow", "R. Passonneau"], "venue": "Proceedings of the Workshop on Languages in Social Media, pages 30\u201338. Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Homophily and latent attribute inference: Inferring latent attributes of twitter users from neighbors", "author": ["F. Al Zamal", "W. Liu", "D. Ruths"], "venue": "ICWSM,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "User interests in social media sites: an exploration with micro-blogs", "author": ["N. Banerjee", "D. Chakraborty", "K. Dasgupta", "S. Mittal", "A. Joshi", "S. Nagar", "A. Rai", "S. Madan"], "venue": "Proceedings of the 18th ACM conference on Information and knowledge management, pages 1823\u20131826. ACM,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic soft logic for semantic textual similarity", "author": ["I. Beltagy", "K. Erk", "R. Mooney"], "venue": "Proceedings of Association for Computational Linguistics (ACL-14),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "EMNLP, pages 1533\u20131544,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic similarity logic", "author": ["M. Brocheler", "L. Mihalkova", "L. Getoor"], "venue": "arXiv preprint arXiv:1203.3469,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Computing marginal distributions over continuous markov networks for statistical relational learning", "author": ["M. Broecheler", "L. Getoor"], "venue": "Advances in Neural Information Processing Systems, pages 316\u2013324,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Discriminating gender on twitter", "author": ["J.D. Burger", "J. Henderson", "G. Kim", "G. Zarrella"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1301\u20131309. Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "You are where you tweet: a content-based approach to geo-locating twitter users", "author": ["Z. Cheng", "J. Caverlee", "K. Lee"], "venue": "Proceedings of the 19th ACM international conference on Information and knowledge management, pages 759\u2013768. ACM,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Identifying sources of opinions with conditional random fields and extraction patterns", "author": ["Y. Choi", "C. Cardie", "E. Riloff", "S. Patwardhan"], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 355\u2013362. Association for Computational Linguistics,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Gender inference of twitter users in non-english contexts", "author": ["M. Ciot", "M. Sonderegger", "D. Ruths"], "venue": "EMNLP, pages 1136\u20131145,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Political polarization on twitter", "author": ["M. Conover", "J. Ratkiewicz", "M. Francisco", "B. Gon\u00e7alves", "F. Menczer", "A. Flammini"], "venue": "ICWSM,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Clp (bn): Constraint logic programming for probabilistic knowledge", "author": ["V.S. Costa", "D. Page", "M. Qazi", "J. Cussens"], "venue": "Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence, pages 517\u2013524. Morgan Kaufmann Publishers Inc.,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["M. Craven", "J. Kumlien"], "venue": "In ISMB,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Fully unsupervised discovery of concept-specific relationships by web mining", "author": ["D. Davidov", "A. Rappoport", "M. Koppel"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Inferring the location of twitter messages based on user relationships", "author": ["C.A. Davis Jr.", "G.L. Pappa", "D.R.R. de Oliveira", "F. de L Arcanjo"], "venue": "Transactions in GIS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Feature weighting in content based recommendation system using social network analysis", "author": ["S. Debnath", "N. Ganguly", "P. Mitra"], "venue": "Proceedings of the 17th international conference on World Wide Web, pages 1041\u20131042. ACM,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Finding bursty topics from microblogs", "author": ["Q. Diao", "J. Jiang", "F. Zhu", "E.-P. Lim"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 536\u2013544. Association for Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-relational record linkage", "author": ["P. Domingos"], "venue": "In Proceedings of the KDD-2004 Workshop on Multi-Relational Data Mining. Citeseer,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient identification of web communities", "author": ["G.W. Flake", "S. Lawrence", "C.L. Giles"], "venue": "Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 150\u2013160. ACM,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning probabilistic relational models", "author": ["N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer"], "venue": "IJCAI, volume 99, pages 1300\u20131309,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Twitter sentiment classification using distant supervision", "author": ["A. Go", "R. Bhayani", "L. Huang"], "venue": "CS224N Project Report, Stanford, pages 1\u201312,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "System and method for utilizing social networks for collaborative filtering, Mar", "author": ["M. Goeksel", "C.P. Lam"], "venue": "30", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic logic networks", "author": ["B. Goertzel", "C. Pennachin", "N. Geisweiller"], "venue": "Engineering General Intelligence, Part 2, pages 275\u2013291. Springer,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Social media recommendation based on people and tags", "author": ["I. Guy", "N. Zwerdling", "I. Ronen", "D. Carmel", "E. Uziel"], "venue": "Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 194\u2013201. ACM,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["R. Hoffmann", "C. Zhang", "X. Ling", "L. Zettlemoyer", "D.S. Weld"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 541\u2013550. Association for Computational Linguistics,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Social group modeling with probabilistic soft logic", "author": ["B. Huang", "S.H. Bach", "E. Norris", "J. Pujara", "L. Getoor"], "venue": "NIPS Workshop on Social Network and Social Media Analysis: Methods, Models, and Applications,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic soft logic for trust analysis in social networks", "author": ["B. Huang", "A. Kimmig", "L. Getoor", "J. Golbeck"], "venue": "International Workshop on Statistical Relational AI, pages 1\u20138,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Corpus-based semantic lexicon induction with web-based corroboration", "author": ["S.P. Igo", "E. Riloff"], "venue": "Proceedings of the Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 18\u201326. Association for Computational Linguistics,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic reasoning in terminological logics", "author": ["M. Jaeger"], "venue": "KR, 94:305\u2013316,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1994}, {"title": "A matrix factorization technique with trust propagation for recommendation in social networks", "author": ["M. Jamali", "M. Ester"], "venue": "Proceedings of the fourth ACM conference on Recommender systems, pages 135\u2013142. ACM,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Making large scale svm learning practical", "author": ["T. Joachims"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1999}, {"title": "Referral web: combining social networks and collaborative filtering", "author": ["H. Kautz", "B. Selman", "M. Shah"], "venue": "Communications of the ACM, 40(3):63\u201365,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1997}, {"title": "Extracting opinions, opinion holders, and topics expressed in online news media text", "author": ["S.-M. Kim", "E. Hovy"], "venue": "Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1\u20138. Association for Computational Linguistics,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "A short introduction to probabilistic soft logic", "author": ["A. Kimmig", "S. Bach", "M. Broecheler", "B. Huang", "L. Getoor"], "venue": "Proceedings of the NIPS Workshop on Probabilistic Programming: Foundations and Applications, pages 1\u20134,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "On social networks and collaborative recommendation", "author": ["I. Konstas", "V. Stathopoulos", "J.M. Jose"], "venue": "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 195\u2013202. ACM,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Twitter sentiment analysis: The good the bad and the omg", "author": ["E. Kouloumpis", "T. Wilson", "J. Moore"], "venue": "ICWSM, 11:538\u2013541,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "Learning arguments and supertypes of semantic relations using recursive patterns", "author": ["Z. Kozareva", "E. Hovy"], "venue": "Proceedings of the 48th Annual Meeting of the Association  for Computational Linguistics, pages 1482\u20131491. Association for Computational Linguistics,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Not all seeds are equal: Measuring the quality of text mining seeds", "author": ["Z. Kozareva", "E. Hovy"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 618\u2013626. Association for Computational Linguistics,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["T. Kwiatkowski", "E. Choi", "Y. Artzi", "L. Zettlemoyer"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2001}, {"title": "The em algorithm for graphical association models with missing data", "author": ["S.L. Lauritzen"], "venue": "Computational Statistics & Data Analysis, 19(2):191\u2013201,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1995}, {"title": "Combined distributional and logical semantics", "author": ["M. Lewis", "M. Steedman"], "venue": "TACL, 1:179\u2013192,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Timeline generation: tracking individuals on twitter", "author": ["J. Li", "C. Cardie"], "venue": "Proceedings of the 23rd international conference on World wide web, pages 643\u2013652. International World Wide Web Conferences Steering Committee,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Major life event extraction from twitter based on congratulations/condolences speech acts", "author": ["J. Li", "A. Ritter", "C. Cardie", "E. Hovy"], "venue": "Proceedings of Empirical Methods in Natural Language Processing,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Weakly supervised user profile extraction from twitter", "author": ["J. Li", "A. Ritter", "E. Hovy"], "venue": "ACL,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Pet: a statistical model for popular events tracking in social communities", "author": ["C.X. Lin", "B. Zhao", "Q. Mei", "J. Han"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 929\u2013938. ACM,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "Smoothing techniques for adaptive online language models: topic tracking in tweet streams", "author": ["J. Lin", "R. Snow", "W. Morgan"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 422\u2013429. ACM,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Statistical analysis with missing data", "author": ["R.J. Little", "D.B. Rubin"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2002}, {"title": "Use of social network information to enhance collaborative filtering performance", "author": ["F. Liu", "H.J. Lee"], "venue": "Expert Systems with Applications, 37(7):4772\u20134778,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient weight learning for markov logic networks", "author": ["D. Lowd", "P. Domingos"], "venue": "Knowledge Discovery in Databases: PKDD 2007, pages 200\u2013211. Springer,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2007}, {"title": "Birds of a feather: Homophily in social networks", "author": ["M. McPherson", "L. Smith-Lovin", "J.M. Cook"], "venue": "Annual review of sociology, pages 415\u2013444,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2001}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, pages 1045\u20131048,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5528\u20135531. IEEE,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2011}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "Proceedings of the Joint Conference of the 47th Annual  Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003\u20131011. Association for Computational Linguistics,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2009}, {"title": "You are who you know: inferring user profiles in online social networks", "author": ["A. Mislove", "B. Viswanath", "K.P. Gummadi", "P. Druschel"], "venue": "Proceedings of the third ACM international conference on Web search and data mining, pages 251\u2013260. ACM,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2010}, {"title": "Universal grammar", "author": ["R. Montague"], "venue": "Theoria, 36(3):373\u2013398,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1970}, {"title": "Inductive logic programming: Theory and methods", "author": ["S. Muggleton", "L. De Raedt"], "venue": "The Journal of Logic Programming, 19:629\u2013679,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1994}, {"title": "Stochastic logic programs", "author": ["S. Muggleton"], "venue": "Advances in inductive logic programming,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1996}, {"title": "Relational dependency networks", "author": ["J. Neville", "D. Jensen"], "venue": "The Journal of Machine Learning Research, 8:653\u2013692,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2007}, {"title": "Tuffy: Scaling up statistical inference in markov logic networks using an rdbms", "author": ["F. Niu", "C. R\u00e9", "A. Doan", "J. Shavlik"], "venue": "Proceedings of the VLDB Endowment, 4(6):373\u2013384,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["O. Owoputi", "B. O\u2019Connor", "C. Dyer", "K. Gimpel", "N. Schneider", "N.A. Smith"], "venue": "In HLT-NAACL,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2013}, {"title": "Twitter as a corpus for sentiment analysis and opinion mining", "author": ["A. Pak", "P. Paroubek"], "venue": "LREC,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2010}, {"title": "A machine learning approach to twitter user classification", "author": ["M. Pennacchiotti", "A.-M. Popescu"], "venue": "ICWSM, 11:281\u2013288,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2011}, {"title": "Extracting events and event descriptions from twitter", "author": ["A.-M. Popescu", "M. Pennacchiotti", "D. Paranjpe"], "venue": "Proceedings of the 20th international conference companion on World wide web, pages 105\u2013106. ACM,", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2011}, {"title": "A multi-pass sieve for coreference resolution", "author": ["K. Raghunathan", "H. Lee", "S. Rangarajan", "N. Chambers", "M. Surdeanu", "D. Jurafsky", "C. Manning"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 492\u2013501. Association for Computational Linguistics,", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2010}, {"title": "Detecting latent user properties in social media", "author": ["D. Rao", "D. Yarowsky"], "venue": "Proc. of the NIPS MLSN Workshop,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2010}, {"title": "Classifying latent user attributes in twitter", "author": ["D. Rao", "D. Yarowsky", "A. Shreevats", "M. Gupta"], "venue": "Proceedings of the 2nd international workshop on Search and mining user-generated contents, pages 37\u201344. ACM,", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2010}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning, 62(1-2):107\u2013136,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2006}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["S. Riedel", "L. Yao", "A. McCallum", "B.M. Marlin"], "venue": null, "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2013}, {"title": "Learning dictionaries for information extraction by multi-level bootstrapping", "author": ["E. Riloff", "R. Jones"], "venue": "In AAAI/IAAI,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1999}, {"title": "Open domain event extraction from twitter", "author": ["A. Ritter", "O. Etzioni", "S. Clark"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2012}, {"title": "Modeling  missing data in distant supervision for information", "author": ["A. Ritter", "L. Zettlemoyer", "Mausam", "O. Etzioni"], "venue": "extraction. TACL,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2013}, {"title": "A machine-oriented logic based on the resolution principle", "author": ["J.A. Robinson"], "venue": "Journal of the ACM (JACM), 12(1):23\u201341,", "citeRegEx": "76", "shortCiteRegEx": null, "year": 1965}, {"title": "Finding your friends and following them to where you are", "author": ["A. Sadilek", "H. Kautz", "J.P. Bigham"], "venue": "Proceedings of the fifth ACM international conference on Web search and data mining, pages 723\u2013732. ACM,", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2012}, {"title": "Semantic sentiment analysis of twitter", "author": ["H. Saif", "Y. He", "H. Alani"], "venue": "The Semantic Web\u2013ISWC 2012, pages 508\u2013524. Springer,", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2012}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["B. Sarwar", "G. Karypis", "J. Konstan", "J. Riedl"], "venue": "Proceedings of the 10th international conference on World Wide Web, pages 285\u2013295. ACM,", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning first-order horn clauses from web text", "author": ["S. Schoenmackers", "O. Etzioni", "D.S. Weld", "J. Davis"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1088\u20131098. Association for Computational Linguistics,", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2010}, {"title": "Discriminative training of markov logic networks", "author": ["P. Singla", "P. Domingos"], "venue": "AAAI, volume 5, pages 868\u2013873,", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2005}, {"title": "Entity resolution with markov logic", "author": ["P. Singla", "P. Domingos"], "venue": "Data Mining, 2006. ICDM\u201906. Sixth International Conference on, pages 572\u2013582. IEEE,", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2006}, {"title": "What\u00e2\u0102\u0179s in a name: A study of names, gender inference, and gender behavior in facebook", "author": ["C. Tang", "K. Ross", "N. Saxena", "R. Chen"], "venue": "Database Systems for Adanced Applications, pages 344\u2013356. Springer,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2011}, {"title": "Discriminative probabilistic models for relational data", "author": ["B. Taskar", "P. Abbeel", "D. Koller"], "venue": "Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence, pages 485\u2013492. Morgan Kaufmann Publishers Inc.,", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2002}, {"title": "Programming with personalized pagerank: A locally groundable first-order probabilistic logic", "author": ["W.Y. Wang", "K. Mazaitis", "W.W. Cohen"], "venue": "Proceedings of the 22nd ACM International Conference on Information and Knowledge Management (CIKM 2013),", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2013}, {"title": "Proppr: Efficient first-order probabilistic logic programming for structure discovery, parameter learning, and scalable inference", "author": ["W.Y. Wang", "K. Mazaitis", "W.W. Cohen"], "venue": "Proceedings of the AAAI 2014 Workshop on Statistical Relational AI (StarAI 2014),", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2014}, {"title": "Structure learning via parameter learning", "author": ["W.Y. Wang", "K. Mazaitis", "W.W. Cohen"], "venue": "Proceedings of the 23rd ACM International Conference on Information and Knowledge Management (CIKM 2014),", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient inference and learning in a large knowledge base: Reasoning with extracted information using a locally groundable first-order probabilistic logic", "author": ["W.Y. Wang", "K. Mazaitis", "N. Lao", "T. Mitchell", "W.W. Cohen"], "venue": "progress,", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2014}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["J. Wiebe", "T. Wilson", "C. Cardie"], "venue": "Language resources and evaluation, 39(2-3):165\u2013210,", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2005}, {"title": "Joint inference for fine-grained opinion extraction", "author": ["B. Yang", "C. Cardie"], "venue": "ACL (1), pages 1640\u20131649,", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2013}, {"title": "Overlapping community detection at scale: a nonnegative matrix factorization approach", "author": ["J. Yang", "J. Leskovec"], "venue": "Proceedings of the sixth ACM international conference on Web search and data mining, pages 587\u2013596. ACM,", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": "movie ratings), and often enriched by information from a social network [18, 25, 35, 38, 52].", "startOffset": 72, "endOffset": 92}, {"referenceID": 23, "context": "movie ratings), and often enriched by information from a social network [18, 25, 35, 38, 52].", "startOffset": 72, "endOffset": 92}, {"referenceID": 33, "context": "movie ratings), and often enriched by information from a social network [18, 25, 35, 38, 52].", "startOffset": 72, "endOffset": 92}, {"referenceID": 36, "context": "movie ratings), and often enriched by information from a social network [18, 25, 35, 38, 52].", "startOffset": 72, "endOffset": 92}, {"referenceID": 50, "context": "movie ratings), and often enriched by information from a social network [18, 25, 35, 38, 52].", "startOffset": 72, "endOffset": 92}, {"referenceID": 69, "context": "We propose to infer user preferences on domains like Twitter without explicit information by applying relational reasoning frameworks like Markov Logic Networks (MLN) [71] and Probabilistic Soft Logic (PSL) [26] to help infer these relational rules.", "startOffset": 167, "endOffset": 171}, {"referenceID": 24, "context": "We propose to infer user preferences on domains like Twitter without explicit information by applying relational reasoning frameworks like Markov Logic Networks (MLN) [71] and Probabilistic Soft Logic (PSL) [26] to help infer these relational rules.", "startOffset": 207, "endOffset": 211}, {"referenceID": 46, "context": "On the other hand, users of online social media frequently publish messages describing their preferences and activities, often explicitly mentioning attributes such as their JOB, RELIGION, or EDUCATION [48].", "startOffset": 202, "endOffset": 206}, {"referenceID": 14, "context": "for Twitter that combines supervision [15], semi-supervised data harvesting (e.", "startOffset": 38, "endOffset": 42}, {"referenceID": 38, "context": ", [40, 41]) and vector space models [5, 55] to automatically extract structured profiles from the text of users\u2019 messages.", "startOffset": 2, "endOffset": 10}, {"referenceID": 39, "context": ", [40, 41]) and vector space models [5, 55] to automatically extract structured profiles from the text of users\u2019 messages.", "startOffset": 2, "endOffset": 10}, {"referenceID": 4, "context": ", [40, 41]) and vector space models [5, 55] to automatically extract structured profiles from the text of users\u2019 messages.", "startOffset": 36, "endOffset": 43}, {"referenceID": 53, "context": ", [40, 41]) and vector space models [5, 55] to automatically extract structured profiles from the text of users\u2019 messages.", "startOffset": 36, "endOffset": 43}, {"referenceID": 69, "context": "Finally, we feed the extracted attributes and relations into relational reasoning frameworks, including Markov Logic Networks (MLN) [71] and Probabilistic Soft Logic (PSL) [26], to infer the relational rules among users, attributes and user relations that allow us to predict user preferences.", "startOffset": 132, "endOffset": 136}, {"referenceID": 24, "context": "Finally, we feed the extracted attributes and relations into relational reasoning frameworks, including Markov Logic Networks (MLN) [71] and Probabilistic Soft Logic (PSL) [26], to infer the relational rules among users, attributes and user relations that allow us to predict user preferences.", "startOffset": 172, "endOffset": 176}, {"referenceID": 9, "context": ", [10, 17, 78]), there is less focus on user-level inference.", "startOffset": 2, "endOffset": 14}, {"referenceID": 16, "context": ", [10, 17, 78]), there is less focus on user-level inference.", "startOffset": 2, "endOffset": 14}, {"referenceID": 75, "context": ", [10, 17, 78]), there is less focus on user-level inference.", "startOffset": 2, "endOffset": 14}, {"referenceID": 46, "context": "Job and education attributes are extracted by combining a rule based approach with an existing probabilistic system described in [48].", "startOffset": 129, "endOffset": 133}, {"referenceID": 46, "context": "We adopted the friend \u2212 shared strategy taken in [48] that if more than 10 percent of and at least 20 friends are shared by Google+ circles and Twitter followers, we assume that the two accounts point to the same person.", "startOffset": 49, "endOffset": 53}, {"referenceID": 46, "context": "[48] (for details about algorithms in [48], see Section 6).", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[48] (for details about algorithms in [48], see Section 6).", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": ", [9, 12, 66, 84]) studying whether high level tweet features (e.", "startOffset": 2, "endOffset": 17}, {"referenceID": 11, "context": ", [9, 12, 66, 84]) studying whether high level tweet features (e.", "startOffset": 2, "endOffset": 17}, {"referenceID": 64, "context": ", [9, 12, 66, 84]) studying whether high level tweet features (e.", "startOffset": 2, "endOffset": 17}, {"referenceID": 81, "context": ", [9, 12, 66, 84]) studying whether high level tweet features (e.", "startOffset": 2, "endOffset": 17}, {"referenceID": 46, "context": "\u2019s system [48].", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "For any two given Twitter users and their published contents, the system returns a score Sspouse in the range of [0,1] indicating how likely SPOUSE(USR1,USR2) relation is to hold.", "startOffset": 113, "endOffset": 118}, {"referenceID": 0, "context": "5, we use a continuous variable to denote the confidence, the value of which is computed by linearly projecting Sspouse into [0,1] space.", "startOffset": 125, "endOffset": 130}, {"referenceID": 0, "context": ", [1, 39, 65, 79]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 37, "context": ", [1, 39, 65, 79]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 63, "context": ", [1, 39, 65, 79]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 76, "context": ", [1, 39, 65, 79]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 10, "context": "Our work thus resembles other work on sentiment target extraction ([11, 36, 91]) using supervised classifiers or sequence models based on manually-labeled datasets.", "startOffset": 67, "endOffset": 79}, {"referenceID": 34, "context": "Our work thus resembles other work on sentiment target extraction ([11, 36, 91]) using supervised classifiers or sequence models based on manually-labeled datasets.", "startOffset": 67, "endOffset": 79}, {"referenceID": 88, "context": "Our work thus resembles other work on sentiment target extraction ([11, 36, 91]) using supervised classifiers or sequence models based on manually-labeled datasets.", "startOffset": 67, "endOffset": 79}, {"referenceID": 15, "context": "To deal with data sparsity issues, we collect training data by combining semi-supervised information harvesting techniques [16, 40, 41, 47] and the concept of distant supervision [15, 24, 57] as follows: Semi-supervised information harvesting: We applied the standard seed-based information-extraction method of obtaining training data recursively by using seed examples to extract patterns, which are used to harvest new examples, which are further used as new seeds to train new patterns.", "startOffset": 123, "endOffset": 139}, {"referenceID": 38, "context": "To deal with data sparsity issues, we collect training data by combining semi-supervised information harvesting techniques [16, 40, 41, 47] and the concept of distant supervision [15, 24, 57] as follows: Semi-supervised information harvesting: We applied the standard seed-based information-extraction method of obtaining training data recursively by using seed examples to extract patterns, which are used to harvest new examples, which are further used as new seeds to train new patterns.", "startOffset": 123, "endOffset": 139}, {"referenceID": 39, "context": "To deal with data sparsity issues, we collect training data by combining semi-supervised information harvesting techniques [16, 40, 41, 47] and the concept of distant supervision [15, 24, 57] as follows: Semi-supervised information harvesting: We applied the standard seed-based information-extraction method of obtaining training data recursively by using seed examples to extract patterns, which are used to harvest new examples, which are further used as new seeds to train new patterns.", "startOffset": 123, "endOffset": 139}, {"referenceID": 45, "context": "To deal with data sparsity issues, we collect training data by combining semi-supervised information harvesting techniques [16, 40, 41, 47] and the concept of distant supervision [15, 24, 57] as follows: Semi-supervised information harvesting: We applied the standard seed-based information-extraction method of obtaining training data recursively by using seed examples to extract patterns, which are used to harvest new examples, which are further used as new seeds to train new patterns.", "startOffset": 123, "endOffset": 139}, {"referenceID": 14, "context": "To deal with data sparsity issues, we collect training data by combining semi-supervised information harvesting techniques [16, 40, 41, 47] and the concept of distant supervision [15, 24, 57] as follows: Semi-supervised information harvesting: We applied the standard seed-based information-extraction method of obtaining training data recursively by using seed examples to extract patterns, which are used to harvest new examples, which are further used as new seeds to train new patterns.", "startOffset": 179, "endOffset": 191}, {"referenceID": 22, "context": "To deal with data sparsity issues, we collect training data by combining semi-supervised information harvesting techniques [16, 40, 41, 47] and the concept of distant supervision [15, 24, 57] as follows: Semi-supervised information harvesting: We applied the standard seed-based information-extraction method of obtaining training data recursively by using seed examples to extract patterns, which are used to harvest new examples, which are further used as new seeds to train new patterns.", "startOffset": 179, "endOffset": 191}, {"referenceID": 55, "context": "To deal with data sparsity issues, we collect training data by combining semi-supervised information harvesting techniques [16, 40, 41, 47] and the concept of distant supervision [15, 24, 57] as follows: Semi-supervised information harvesting: We applied the standard seed-based information-extraction method of obtaining training data recursively by using seed examples to extract patterns, which are used to harvest new examples, which are further used as new seeds to train new patterns.", "startOffset": 179, "endOffset": 191}, {"referenceID": 62, "context": "Entities extracted here should be nouns, which is determined by a Twitter-tuned POS package [64].", "startOffset": 92, "endOffset": 96}, {"referenceID": 32, "context": "The SVM classifiers are trained using the SVMlight package [34] with the following features: \u2022 Unigram, bigram features with corresponding part-of-speech tags and NER labels.", "startOffset": 59, "endOffset": 63}, {"referenceID": 87, "context": "\u2022 Dictionary-derived features based on a subjectivity lexicon [90].", "startOffset": 62, "endOffset": 66}, {"referenceID": 41, "context": "The CRF model [43] is trained using the CRF++ package based on the following features: \u2022 Current word, context words within a window of 3 words and their part-of-speech tags.", "startOffset": 14, "endOffset": 18}, {"referenceID": 39, "context": "Since semi-supervised approaches heavily rely on seed quality [41] and the patterns derived by the recursive framework may be strongly influenced by the starting seeds, adding in examples from distant supervision helps increase the diversity of positive training examples.", "startOffset": 62, "endOffset": 66}, {"referenceID": 55, "context": "com/p/crfpp/ For example, if datasets says relation ISCAPITAL holds between Britain and London, then all sentences with mention of \u201cBritain\" and \u201cLondon\" are treated as expressing ISCAPITAL relation [57, 75].", "startOffset": 199, "endOffset": 207}, {"referenceID": 73, "context": "com/p/crfpp/ For example, if datasets says relation ISCAPITAL holds between Britain and London, then all sentences with mention of \u201cBritain\" and \u201cLondon\" are treated as expressing ISCAPITAL relation [57, 75].", "startOffset": 199, "endOffset": 207}, {"referenceID": 22, "context": "Tweets with happy emoticons such as :-) : ) are of positive sentiment [24].", "startOffset": 70, "endOffset": 74}, {"referenceID": 53, "context": "Entity Clustering: We further cluster the extracted entities into different groups, with an goal of answering questions like \u2018if usr1 likes films, how likely would she like the film Titanic?\u2019 Towards this goal, we train a skip-gram neural language model [55, 56] based on the tweet dataset using word2vec where each word is represented as a real-valued, low-dimensional vector.", "startOffset": 254, "endOffset": 262}, {"referenceID": 54, "context": "Entity Clustering: We further cluster the extracted entities into different groups, with an goal of answering questions like \u2018if usr1 likes films, how likely would she like the film Titanic?\u2019 Towards this goal, we train a skip-gram neural language model [55, 56] based on the tweet dataset using word2vec where each word is represented as a real-valued, low-dimensional vector.", "startOffset": 254, "endOffset": 262}, {"referenceID": 69, "context": "Markov Logic [71] is a probabilistic logic framework which encodes weighted first-order logic formulas in a Markov network.", "startOffset": 13, "endOffset": 17}, {"referenceID": 51, "context": "Many approaches have been proposed for fast and effective learning for MLNs [53, 63, 82].", "startOffset": 76, "endOffset": 88}, {"referenceID": 61, "context": "Many approaches have been proposed for fast and effective learning for MLNs [53, 63, 82].", "startOffset": 76, "endOffset": 88}, {"referenceID": 79, "context": "Many approaches have been proposed for fast and effective learning for MLNs [53, 63, 82].", "startOffset": 76, "endOffset": 88}, {"referenceID": 79, "context": "In this work, we use the discriminative training approach [82], as will be demonstrated in Section 4.", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "PSL [4, 37] is another sort of logic reasoning architecture.", "startOffset": 4, "endOffset": 11}, {"referenceID": 35, "context": "PSL [4, 37] is another sort of logic reasoning architecture.", "startOffset": 4, "endOffset": 11}, {"referenceID": 79, "context": "Discriminative Training for MLN: We use the approach described in [82] where we assume that we have a priori knowledge about which predicates will be evidence and which ones will be Figure 2: (a) Standard Approach (b) Revised version with missing values in MLN.", "startOffset": 66, "endOffset": 70}, {"referenceID": 42, "context": "Inspired by common existing approaches to deal with missing data [44, 51], we treat users\u2019 LIKE/DISLIKE preferences as latent variables, while what is observed is whether users explicitly mention their preferences in their posts.", "startOffset": 65, "endOffset": 73}, {"referenceID": 49, "context": "Inspired by common existing approaches to deal with missing data [44, 51], we treat users\u2019 LIKE/DISLIKE preferences as latent variables, while what is observed is whether users explicitly mention their preferences in their posts.", "startOffset": 65, "endOffset": 73}, {"referenceID": 0, "context": "The latent variables and observed variables are connected via a binary distribution parameterized by a [0,1] variable Sentity , indicating how likely a user would be to report the correspondent entity in their posts.", "startOffset": 103, "endOffset": 108}, {"referenceID": 80, "context": "The system can be optimized by incorporating a form of EM algorithm into MLN [83].", "startOffset": 77, "endOffset": 81}, {"referenceID": 46, "context": "As the objective function for joint inference would be difficult to optimize (especially since inference on MLN is hard) and existing algorithms may not able to scale up to the size of network we consider, we turn to a greedy approach inspired by recent work [48, 68]: attributes are initialized from the logic network based on given attributes where missing values are not considered.", "startOffset": 259, "endOffset": 267}, {"referenceID": 66, "context": "As the objective function for joint inference would be difficult to optimize (especially since inference on MLN is hard) and existing algorithms may not able to scale up to the size of network we consider, we turn to a greedy approach inspired by recent work [48, 68]: attributes are initialized from the logic network based on given attributes where missing values are not considered.", "startOffset": 259, "endOffset": 267}, {"referenceID": 45, "context": "We expect FRIENDOBSERVED to yield better results than the FRIEND-LATENT setting since the former benefits from gold network information [47].", "startOffset": 136, "endOffset": 140}, {"referenceID": 45, "context": "We draw on a similar idea in [47].", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "Of course the performance of the algorithm could very likely be even higher if we were to additionally incorporating features designed directly for the gender ID task (such as entities mentioned, links, and especially the wide variety of writing style features used in work such as [12], which achieves gender ID accuracies of 0.", "startOffset": 282, "endOffset": 286}, {"referenceID": 17, "context": ", like/dislike, location, gender, etc) and network information (attributes from his friends along the network) \u2022 Collaborative Filtering (CF): CF [18, 25, 33, 35] accounts for a popular approach in recommendation system, which utilizes the information of the user-item matrix for recommendations.", "startOffset": 146, "endOffset": 162}, {"referenceID": 23, "context": ", like/dislike, location, gender, etc) and network information (attributes from his friends along the network) \u2022 Collaborative Filtering (CF): CF [18, 25, 33, 35] accounts for a popular approach in recommendation system, which utilizes the information of the user-item matrix for recommendations.", "startOffset": 146, "endOffset": 162}, {"referenceID": 31, "context": ", like/dislike, location, gender, etc) and network information (attributes from his friends along the network) \u2022 Collaborative Filtering (CF): CF [18, 25, 33, 35] accounts for a popular approach in recommendation system, which utilizes the information of the user-item matrix for recommendations.", "startOffset": 146, "endOffset": 162}, {"referenceID": 33, "context": ", like/dislike, location, gender, etc) and network information (attributes from his friends along the network) \u2022 Collaborative Filtering (CF): CF [18, 25, 33, 35] accounts for a popular approach in recommendation system, which utilizes the information of the user-item matrix for recommendations.", "startOffset": 146, "endOffset": 162}, {"referenceID": 77, "context": "We view the like/dislike entity prediction as entity recommendation problem and adopt the approach described in [80] by constructing user-user similarity matrix from weighted cosine similarity calculated from shared attributes and network information.", "startOffset": 112, "endOffset": 116}, {"referenceID": 77, "context": "As in [80], a regression model is trained to fill out {0, 1} value in user-entity matrix indicating whether a specific user likes/hates one specific entity.", "startOffset": 6, "endOffset": 10}, {"referenceID": 47, "context": "Information Extraction on Social Media : Much work has been devoted to automatic extraction of well-structured information profiles from online social media, which mainly fall into two major levels: at public level [49, 50, 90] or at user level.", "startOffset": 215, "endOffset": 227}, {"referenceID": 48, "context": "Information Extraction on Social Media : Much work has been devoted to automatic extraction of well-structured information profiles from online social media, which mainly fall into two major levels: at public level [49, 50, 90] or at user level.", "startOffset": 215, "endOffset": 227}, {"referenceID": 87, "context": "Information Extraction on Social Media : Much work has been devoted to automatic extraction of well-structured information profiles from online social media, which mainly fall into two major levels: at public level [49, 50, 90] or at user level.", "startOffset": 215, "endOffset": 227}, {"referenceID": 18, "context": "The former includes public event identification [19], event tracking [67] or event-referring expression extraction [74].", "startOffset": 48, "endOffset": 52}, {"referenceID": 65, "context": "The former includes public event identification [19], event tracking [67] or event-referring expression extraction [74].", "startOffset": 69, "endOffset": 73}, {"referenceID": 72, "context": "The former includes public event identification [19], event tracking [67] or event-referring expression extraction [74].", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "The latter focus on user studies, examining users\u2019 interests [3], timeline [46], personal events [47] or individual attributes such as age [70, 69], gender [12], political polarity [13], locations [78], jobs and educations [48], student information (e.", "startOffset": 61, "endOffset": 64}, {"referenceID": 44, "context": "The latter focus on user studies, examining users\u2019 interests [3], timeline [46], personal events [47] or individual attributes such as age [70, 69], gender [12], political polarity [13], locations [78], jobs and educations [48], student information (e.", "startOffset": 75, "endOffset": 79}, {"referenceID": 45, "context": "The latter focus on user studies, examining users\u2019 interests [3], timeline [46], personal events [47] or individual attributes such as age [70, 69], gender [12], political polarity [13], locations [78], jobs and educations [48], student information (e.", "startOffset": 97, "endOffset": 101}, {"referenceID": 68, "context": "The latter focus on user studies, examining users\u2019 interests [3], timeline [46], personal events [47] or individual attributes such as age [70, 69], gender [12], political polarity [13], locations [78], jobs and educations [48], student information (e.", "startOffset": 139, "endOffset": 147}, {"referenceID": 67, "context": "The latter focus on user studies, examining users\u2019 interests [3], timeline [46], personal events [47] or individual attributes such as age [70, 69], gender [12], political polarity [13], locations [78], jobs and educations [48], student information (e.", "startOffset": 139, "endOffset": 147}, {"referenceID": 11, "context": "The latter focus on user studies, examining users\u2019 interests [3], timeline [46], personal events [47] or individual attributes such as age [70, 69], gender [12], political polarity [13], locations [78], jobs and educations [48], student information (e.", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "The latter focus on user studies, examining users\u2019 interests [3], timeline [46], personal events [47] or individual attributes such as age [70, 69], gender [12], political polarity [13], locations [78], jobs and educations [48], student information (e.", "startOffset": 181, "endOffset": 185}, {"referenceID": 75, "context": "The latter focus on user studies, examining users\u2019 interests [3], timeline [46], personal events [47] or individual attributes such as age [70, 69], gender [12], political polarity [13], locations [78], jobs and educations [48], student information (e.", "startOffset": 197, "endOffset": 201}, {"referenceID": 46, "context": "The latter focus on user studies, examining users\u2019 interests [3], timeline [46], personal events [47] or individual attributes such as age [70, 69], gender [12], political polarity [13], locations [78], jobs and educations [48], student information (e.", "startOffset": 223, "endOffset": 227}, {"referenceID": 56, "context": ", major, year of matriculation) [58].", "startOffset": 32, "endOffset": 36}, {"referenceID": 46, "context": "The first step of proposed approach highly relies on attribute extraction algorithm described in [48] which extracts three categories of user attributes (i.", "startOffset": 97, "endOffset": 101}, {"referenceID": 46, "context": "[48] gathers training data based on the concept of distant supervision where Google+ treated used as \u201cknowledge base\" to provide supervision.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "Homophily: Our work is based on the fundamental homophily property of online users [54], which assumes that people sharing", "startOffset": 83, "endOffset": 87}, {"referenceID": 89, "context": "Such properties have been harnessed for applications like community detection [92] or friend recommendation [27].", "startOffset": 78, "endOffset": 82}, {"referenceID": 25, "context": "Such properties have been harnessed for applications like community detection [92] or friend recommendation [27].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "Data Harvesting: The techniques adopted in like/dislike attribute extraction are related to a strand of work in data harvesting/information extraction, the point of which is to use some seeds to harvest some data, which is used to learn additional rules or patterns to harvest more data [16, 31, 40, 41, 73].", "startOffset": 287, "endOffset": 307}, {"referenceID": 29, "context": "Data Harvesting: The techniques adopted in like/dislike attribute extraction are related to a strand of work in data harvesting/information extraction, the point of which is to use some seeds to harvest some data, which is used to learn additional rules or patterns to harvest more data [16, 31, 40, 41, 73].", "startOffset": 287, "endOffset": 307}, {"referenceID": 38, "context": "Data Harvesting: The techniques adopted in like/dislike attribute extraction are related to a strand of work in data harvesting/information extraction, the point of which is to use some seeds to harvest some data, which is used to learn additional rules or patterns to harvest more data [16, 31, 40, 41, 73].", "startOffset": 287, "endOffset": 307}, {"referenceID": 39, "context": "Data Harvesting: The techniques adopted in like/dislike attribute extraction are related to a strand of work in data harvesting/information extraction, the point of which is to use some seeds to harvest some data, which is used to learn additional rules or patterns to harvest more data [16, 31, 40, 41, 73].", "startOffset": 287, "endOffset": 307}, {"referenceID": 71, "context": "Data Harvesting: The techniques adopted in like/dislike attribute extraction are related to a strand of work in data harvesting/information extraction, the point of which is to use some seeds to harvest some data, which is used to learn additional rules or patterns to harvest more data [16, 31, 40, 41, 73].", "startOffset": 287, "endOffset": 307}, {"referenceID": 14, "context": "Distant supervision is another methodology for data harvesting [15, 28, 57] that relies on structured data sources as a source of supervision for data harvesting from raw text.", "startOffset": 63, "endOffset": 75}, {"referenceID": 26, "context": "Distant supervision is another methodology for data harvesting [15, 28, 57] that relies on structured data sources as a source of supervision for data harvesting from raw text.", "startOffset": 63, "endOffset": 75}, {"referenceID": 55, "context": "Distant supervision is another methodology for data harvesting [15, 28, 57] that relies on structured data sources as a source of supervision for data harvesting from raw text.", "startOffset": 63, "endOffset": 75}, {"referenceID": 57, "context": "Logic/Relational Reasoning: Logic reasoning, usually based on first-order logic representations, can be tracked back to the early days of AI [59, 76], and has been adequately explored since then (e.", "startOffset": 141, "endOffset": 149}, {"referenceID": 74, "context": "Logic/Relational Reasoning: Logic reasoning, usually based on first-order logic representations, can be tracked back to the early days of AI [59, 76], and has been adequately explored since then (e.", "startOffset": 141, "endOffset": 149}, {"referenceID": 5, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 13, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 24, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 30, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 40, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 43, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 69, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 70, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 78, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 83, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 84, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 85, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 86, "context": ", [6, 14, 26, 32, 42, 45, 71, 72, 77, 81, 86, 87, 88, 89]).", "startOffset": 2, "endOffset": 57}, {"referenceID": 6, "context": "A variety of reasoning models have been proposed, based on ideas or concepts from the fields of graphical models, relational logic, or programming languages [7, 8, 60], each of which has it own generalization capabilities in terms of different types of data.", "startOffset": 157, "endOffset": 167}, {"referenceID": 7, "context": "A variety of reasoning models have been proposed, based on ideas or concepts from the fields of graphical models, relational logic, or programming languages [7, 8, 60], each of which has it own generalization capabilities in terms of different types of data.", "startOffset": 157, "endOffset": 167}, {"referenceID": 58, "context": "A variety of reasoning models have been proposed, based on ideas or concepts from the fields of graphical models, relational logic, or programming languages [7, 8, 60], each of which has it own generalization capabilities in terms of different types of data.", "startOffset": 157, "endOffset": 167}, {"referenceID": 59, "context": "Frameworks include Stochastic Logic Programs [61] which combines logic programming and log-linear models, Probabilistic Relational Networks [23] which incorporates Bayesian networks for reasoning, Relational Markov Networks [85] that uses dataset queries as cliques and model the state of clique in a Markov network, Relational Dependency Networks [62] which combines Bayes networks and Markov networks, and probabilistic similarity logic [7] which jointly considers probabilistic reasoning about similarities and relational structure.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Frameworks include Stochastic Logic Programs [61] which combines logic programming and log-linear models, Probabilistic Relational Networks [23] which incorporates Bayesian networks for reasoning, Relational Markov Networks [85] that uses dataset queries as cliques and model the state of clique in a Markov network, Relational Dependency Networks [62] which combines Bayes networks and Markov networks, and probabilistic similarity logic [7] which jointly considers probabilistic reasoning about similarities and relational structure.", "startOffset": 140, "endOffset": 144}, {"referenceID": 82, "context": "Frameworks include Stochastic Logic Programs [61] which combines logic programming and log-linear models, Probabilistic Relational Networks [23] which incorporates Bayesian networks for reasoning, Relational Markov Networks [85] that uses dataset queries as cliques and model the state of clique in a Markov network, Relational Dependency Networks [62] which combines Bayes networks and Markov networks, and probabilistic similarity logic [7] which jointly considers probabilistic reasoning about similarities and relational structure.", "startOffset": 224, "endOffset": 228}, {"referenceID": 60, "context": "Frameworks include Stochastic Logic Programs [61] which combines logic programming and log-linear models, Probabilistic Relational Networks [23] which incorporates Bayesian networks for reasoning, Relational Markov Networks [85] that uses dataset queries as cliques and model the state of clique in a Markov network, Relational Dependency Networks [62] which combines Bayes networks and Markov networks, and probabilistic similarity logic [7] which jointly considers probabilistic reasoning about similarities and relational structure.", "startOffset": 348, "endOffset": 352}, {"referenceID": 6, "context": "Frameworks include Stochastic Logic Programs [61] which combines logic programming and log-linear models, Probabilistic Relational Networks [23] which incorporates Bayesian networks for reasoning, Relational Markov Networks [85] that uses dataset queries as cliques and model the state of clique in a Markov network, Relational Dependency Networks [62] which combines Bayes networks and Markov networks, and probabilistic similarity logic [7] which jointly considers probabilistic reasoning about similarities and relational structure.", "startOffset": 439, "endOffset": 442}, {"referenceID": 5, "context": ", [6]), health modeling [21], group modeling [29], web link based clustering [22], object identification [20], trust analysis [30], and many more.", "startOffset": 2, "endOffset": 5}, {"referenceID": 27, "context": ", [6]), health modeling [21], group modeling [29], web link based clustering [22], object identification [20], trust analysis [30], and many more.", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": ", [6]), health modeling [21], group modeling [29], web link based clustering [22], object identification [20], trust analysis [30], and many more.", "startOffset": 77, "endOffset": 81}, {"referenceID": 19, "context": ", [6]), health modeling [21], group modeling [29], web link based clustering [22], object identification [20], trust analysis [30], and many more.", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": ", [6]), health modeling [21], group modeling [29], web link based clustering [22], object identification [20], trust analysis [30], and many more.", "startOffset": 126, "endOffset": 130}, {"referenceID": 1, "context": "summarized by the proverb \u201cbirds of a feather flock together\" [2].", "startOffset": 62, "endOffset": 65}], "year": 2014, "abstractText": "We propose a framework for inferring the latent attitudes or preferences of users by performing probabilistic first-order logical reasoning over the social network graph. Our method answers questions about Twitter users like Does this user like sushi? or Is this user a New York Knicks fan? by building a probabilistic model that reasons over user attributes (the user\u2019s location or gender) and the social network (the user\u2019s friends and spouse), via inferences like homophily (I am more likely to like sushi if spouse or friends like sushi, I am more likely to like the Knicks if I live in New York). The algorithm uses distant supervision, semi-supervised data harvesting and vector space models to extract user attributes (e.g. spouse, education, location) and preferences (likes and dislikes) from text. The extracted propositions are then fed into a probabilistic reasoner (we investigate both Markov Logic and Probabilistic Soft Logic). Our experiments show that probabilistic logical reasoning significantly improves the performance on attribute and relation extraction, and also achieves an F-score of 0.791 at predicting a users likes or dislikes, significantly better than two strong baselines.", "creator": "LaTeX with hyperref package"}}}