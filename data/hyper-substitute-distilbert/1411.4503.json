{"id": "1411.4503", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2014", "title": "Outlier-Robust Convex Segmentation", "abstract": "we achieved a convex hull trajectory concerning experimental concept of segmenting simpler algorithms, yet explicitly reduced value to rows. we employed two algorithms for programming this problem, one exact and one a top - down novel iteration, having we devised a consistency law around the case of numerical variables and no outliers. robustness to outliers is evaluated on two bug - world tasks related to vector segmentation. our algorithms outperform individual behavioral goals.", "histories": [["v1", "Mon, 17 Nov 2014 14:59:25 GMT  (127kb,D)", "https://arxiv.org/abs/1411.4503v1", "Accepted to AAAI-15, this version includes the appendix/supplementary material referenced in the AAAI-15 submission"], ["v2", "Tue, 18 Nov 2014 07:59:33 GMT  (127kb,D)", "http://arxiv.org/abs/1411.4503v2", "* Accepted to AAAI-15, this version includes the appendix/supplementary material referenced in the AAAI-15 submission, as well as color figures * This version include some minor typos correction"]], "COMMENTS": "Accepted to AAAI-15, this version includes the appendix/supplementary material referenced in the AAAI-15 submission", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["itamar katz", "koby crammer"], "accepted": true, "id": "1411.4503"}, "pdf": {"name": "1411.4503.pdf", "metadata": {"source": "CRF", "title": "Outlier-Robust Convex Segmentation", "authors": ["Itamar Katz", "Koby Crammer"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Segmentation of sequential data, also known as changepoint detection, is a fundamental problem in the field of unsupervised learning, and has applications in diverse fields such as speech processing (Brent 1999; Qiao, Shimomura, and Minematsu 2008; Shriberg et al. 2000), text processing (Beeferman, Berger, and Lafferty 1999), bioinformatics (Olshen et al. 2004) and network anomaly detection (Le\u0301vyLeduc and Roueff 2009), to name a few. We are interested in formulating the segmentation task as a convex optimization problem that avoids issues such as local-minima or sensitivity to initializations. In addition, we want to explicitly incorporate robustness to outliers. Given a sequence of samples {xi}ni=1, for xi \u2208 Rd, our goal is to segment it into a few subsequences, where each subsequence is homogeneous under some criterion. Our starting point is a convex objective that minimizes the sum of squared distances of samples xi from each sample\u2019s associated \u2018centroid\u2018, \u00b5i. Identical adjacent \u00b5is identify their corresponding samples as belonging to the same segment. In addition, some of the samples are allowed to be identified as outliers, allowing reduced loss on these samples. Two regularization terms are added to the objective, in order to constrain the number of detected segments and outliers, respectively.\nWe propose two algorithms based on this formulation, both alternate between detecting outliers, which is solved analytically, and solving the problem with modified samples , which can be solved iteratively. The first algorithm, denoted by Outlier-Robust Convex Sequential (ORCS) segmentation, solves the optimization problem exactly, while\nCopyright \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nthe second is a top-down hierarchical version of the algorithm, called TD-ORCS. We also derive a weighted version of this algorithm, denoted by WTD-ORCS. We show that for the case of K = 2 segments and no outliers, a specific choice of the weights leads to a solution which recovers the exact solution of an un-relaxed optimization problem.\nWe evaluate the performance of the proposed algorithms on two speech segmentation tasks, for both clean sources and sources contaminated with added non-stationary noise. Our algorithms outperform other algorithms in both the clean and outlier-contaminated setting. Finally, based on the empirical results, we propose a heuristic approach for approximating the number of outliers.\nNotation: The samples to be segmented are denoted by {xi \u2208 Rd}ni=1, and their associated quantities (both variables and solutions) are \u00b5i, zi \u2208 Rd. The same notation with no subscript, \u00b5, denotes the collection of all \u00b5i\u2019s. The same holds for x, z. We abuse notation and use\u00b5i to refer to both the \u2018centroid\u2019 vector of a segment (these are not center of mass, due to the regularization term), and to the indexes of measurements assigned to that segment."}, {"heading": "2 Outlier-Robust Convex Segmentation", "text": "Segmentation is the task of dividing a sequence of n data samples {xi}ni=1, into K groups of consecutive samples, or segments, such that each group is homogeneous with respect to some criterion. A common choice of such a criterion often involves minimizing the squared Euclidean distance of a sample to some representative sample \u00b5i. This criterion is highly sensitive to outliers and indeed, as we show empirically below, the performance of segmentation algorithms degrades drastically when the data is contaminated with outliers. It is therefore desirable to incorporate robustness to outliers into the model. We achieve this by allowing some of the input samples xi to be identified as outliers, in which case we do not require \u00b5i to be close to these samples. To\nar X\niv :1\n41 1.\n45 03\nv2 [\ncs .L\nG ]\n1 8\nN ov\n2 01\n4\nthis end we propose to minimize:\nmin \u00b5,z\n{ 1\n2 n\u2211 i=1\n\u2016xi \u2212 zi \u2212 \u00b5i\u20162 }\ns.t. n\u22121\u2211 i=1 1 { \u2016\u00b5i+1 \u2212 \u00b5i\u2016p > 0 } \u2264 K \u2212 1 ,\nn\u2211 i=1 1 { \u2016zi\u2016q > 0 } \u2264M ,\nwhere p, q \u2265 1. Considering samples xi for which zi = 0, the objective measures the loss of replacing a point xi with some shared point \u00b5i, and can be thought of as minus the log-likelihood under Gaussian noise. Samples i with zi 6= 0 are intuitively identified as outliers. The first constraint bounds the number of segments by K, while the second constraint bounds the number of outliers by M . The optimal value for a nonzero zi is to set zi = xi \u2212 \u00b5i, making the contribution to the objective zero, and thus in practice ignoring this sample, treating it as an outlier. We note that a similar approach to robustness was employed by (Forero, Kekatos, and Giannakis 2011) in the context of robust clustering, and by (Mateos and Giannakis 2012) in the context of robust PCA. Since the `0 constraints results in a non convex problem, we use a common practice and replace it with a convex surrogate `1 norm which induces sparsity. For the \u00b5i variables it means that for most samples we will have \u00b5i+1 \u2212 \u00b5i = 0, allowing the identification of the corresponding samples as belonging to the same segment. For the zi variables, it means that most will satisfy zi = 0 and for some of them, the outliers, otherwise. We now incorporate the relaxed constraints into the objective, and in addition consider a slightly more general formulation in which we allow weighting of the summands in the first constraint. We get the following optimization problem:\nmin \u00b5,z\n{ 1\n2 n\u2211 i=1 \u2016xi \u2212 zi \u2212 \u00b5i\u20162 + \u03bb n\u22121\u2211 i=1 wi \u2016\u00b5i+1 \u2212 \u00b5i\u2016p\n(1)\n+ \u03b3 n\u2211 i=1 \u2016zi\u2016q\n} ,\nwhere wi are weights to be determined. The parameter \u03bb > 0 can be thought of as a tradeoff parameter between the first term which is minimized with n segments, and the second term which is minimized with a single segment. As \u03bb is decreased, it crosses values at which there is a transition from K segments to K + 1 segments, in a phase-transition like manner where 1/\u03bb is the analog of temperature. The parameter \u03b3 > 0 controls the amount of outliers, where for \u03b3 =\u221e we enforce zi = 0 for all samples, and for \u03b3 = 0 the objective is optimal for zi = xi \u2212 \u00b5i, and thus all samples are in-fact outliers. Alternatively, one can think of \u03bb, \u03b3 as the Lagrange multipliers of a constrained optimization problem. In what follows we consider p = 2 and q = 1, 2, focusing empirically on q = 2. Note that q = 1 encourages sparsity of coordinates of zi, and not of the vector as a whole. This\namounts to outliers being modeled as noise in few features or samples, respectively."}, {"heading": "2.1 Algorithms", "text": "The decoupling between \u00b5 and z allows us to optimize Eq. (1) in an alternating manner, and we call this algorithm Outlier-Robust Convex Sequential (ORCS) segmentation. Holding \u00b5 constant, optimizing over z is done analytically by noting that Eq. (1) becomes the definition of the proximal operator evaluated at xi \u2212 \u00b5i, for which a closedform solution exists. For q = 1 the objective as a function of z is separable both over coordinates and over data samples, and the proximal operator is the shrinkage-and-threshold operator evaluated at each coordinate k:\nprox\u03b3 (vk) = sign (vk) \u00b7max {0, |vk| \u2212 \u03b3} .\nHowever, we are interested in zeroing some of the zi\u2019s as a whole, so we set q = 2. In this case, the objective is separable over data samples, and the proximal operator is calculated to be:\nprox\u03b3 (v) = v \u00b7max {\n0, 1\u2212 \u03b3 \u2016v\u20162\n} . (2)\nHolding z constant, optimizing over \u00b5 is done by defining x\u0302i , xi \u2212 zi, which results in the following optimization problem:\nmin \u00b5 n\u2211 i=1 \u2016x\u0302i \u2212 \u00b5i\u20162 + \u03bb n\u22121\u2211 i=1 wi \u2016\u00b5i+1 \u2212 \u00b5i\u20162 . (3)\nNote that Eq. (3) is equivalent to Eq. (1) with no outliers present. We also note that if we plug the analytical solution for the zis into Eq. (3) (via the x\u0302is), the loss term turns out to be the multidimensional equivalent of the Huber loss of robust regression. We now discuss two approaches for solving Eq. (3), either exactly or approximately.\nExact solution of Eq. (3): The common proximalgradient approach (Bach et al. 2011; Beck and Teboulle 2009b) for solving non-smooth convex problems has in this case the disadvantage of convergence time which grows linearly with the number of samples n. The reason is that the Lipschitz constant of the gradient of the first term in Eq. (3) grows linearly with n, which results in a decreasing step size. An alternative approach is to derive the dual optimization problem to Eq. (3), analogously to the derivation of (Beck and Teboulle 2009a) in the context of image denoising. The resulting objective is smooth and has a bounded Lipschitz constant independent of n. Yet another approach was proposed by (Bleakley and Vert 2011) for the task of change-point detection, who showed that under a suitable change of variables Eq. (3) can be formulated as a group-LASSO regression (Tibshirani 1996; Yuan and Lin 2006).\nApproximate solution of Eq. (3): Two reasons suggest that deriving an alternative algorithm for solving Eq. (3)\nmight have an advantage. First, the parameter \u03bb does not allow direct control of the resulting number of segments, and in many use-cases such a control is a desired property. Second, as mentioned above, (Bleakley and Vert 2011) showed that Eq. (3) is equivalent to group-LASSO regression, under a suitable change of variables. It is known from the theory of LASSO regression that certain conditions on the design matrix must hold in order for perfect detection of segment boundaries to be possible. Unfortunately, these conditions are violated for the objective in Eq. (3) ; see (Levy-leduc and others 2007) and references therein. Therefore a non-exact solution has a potential of performing better, at least in some situations. We indeed encountered this phenomenon empirically, as is demonstrated in Sec. 3. Therefore we also derive an alternative top-down, greedy algorithm, which finds a segmentation intoK segments, whereK is a user-controlled parameter. The algorithm works in rounds. On each round it picks a segment of a current segmentation, and finds the optimal segmentation of it into two subsequences. We start with the following lemma, which gives an analytical rule which solves Eq. (3) for the case of K = 2 segments.\nLemma 1 Consider the optimal solution of Eq. (3) for the largest parameter \u03bb for which there are K = 2 segments, and denote this value of the parameter by \u03bb\u2217. Denote by i\u2217 the associated splitting point into 2 segments, i.e. samples x\u0302i with i \u2264 i\u2217 belong to the first segment, and otherwise belong to the second segment. Then i\u2217 (x) = argmax\n1\u2264i\u2264n\u22121 g (i,x),\nwhere:\ng (i,x) = { i(n\u2212 i) win \u2016x\u03042(i)\u2212 x\u03041(i)\u20162 } , (4)\nand x\u03041,2(i) are the means of the first and second segments, respectively, given that the split occurs after the ith sample. In addition, \u03bb\u2217 (x) = g (i\u2217,x).\nThe proof is given in the supplementary material. This result motivates a top-down hierarchical segmentation algorithm, which chooses at each iteration to split the segment which results in the maximal decrease of the sum-of-squared-errors criterion. Note that we cannot use the criterion of minimal increment to the objective in Eq. (3), since by continuity of the solution path, there is no change in the objective at the splitting from K = 1 to K = 2 segments. The top-down algorithm can be implemented in O (nK). It has the advantage that no search in the solution path is needed in case K is known, and that this search can be made efficiently in case where K is not known. The top-down approach is used in the algorithm presented in Sec. 2.1.\nFrom the functional form of g (i,x) in Eq. (4) it is clear that in the unweighted case (wi = 1 for all i), the solution is biased towards segments of approximately the same length, because of the i (n\u2212 i) factor. We now show that a specific choice of wi exactly recovers the solution to the unrelaxed optimization problem, where the regularization term in Eq. (3) is replaced with the `0 constraint, that is\u2211n\u22121 i=1 1 {\u2016\u00b5i+1 \u2212 \u00b5i\u20162 > 0} = 1. This is formulated by the following lemma:\nLemma 2 Consider the case of two segments, K = 2, and denote by n\u2217w the minimizer of Eq. (4) withwi = \u221a i (n\u2212 i). Then the split into two segments found by solving the following:\nargmin \u00b5 { n\u2211 i=1 \u2016xi \u2212 \u00b5i\u20162 } , (5)\ns.t. n\u22121\u2211 i=1 1 {\u2016\u00b5i+1 \u2212 \u00b5i\u20162 > 0} = 1 ,\nis also given by n\u2217w. The proof appears in the supplementary material. We note that the same choice for wi was derived by (Bleakley and Vert 2011) from different considerations based on a specific noise model for the stochastic process generating the data. In this sense our derivation is more general, as it does not make any assumptions about the data.\nRobust top-down algorithm We now propose a robust top-down algorithm for approximately optimizing Eq. (1). For a fixed value of \u00b5, using Eq. (2) we can calculate analytically which of the zis represent a detected outlier. These are zis which satisfy \u2016zi\u20162 > \u03b3. This allows us to calculate the value \u03b3\u2217 for which the first outlier is detected as having a non-zero norm. Furthermore, for \u03bb = \u03bb\u2217, \u03b3 = \u03b3\u2217 we know that zi = 0 for all i = 1, . . . , n, and therefore \u00b5i = mean (x\u2212 z) = x\u0304 for all i = 1, . . . , n, and we can find \u03b3\u2217 analytically:\n\u03b3\u2217 = max i {\u2016xi \u2212 x\u0304\u20162} , (6)\nwhere the index i\u2217 at which the maximum is attained is the index to the first detected outlier. The value of \u03bb\u2217 is found as given in Lemma 1, with the replacement of each xi with x\u0302i as defined above. We note that the values \u03bb\u2217, \u03b3\u2217 are helpful for finding a solution path, since they allow to exclude parameters which result in trivial solutions.\nIn the case where \u03bb = \u03bb\u2217 we can extend Eq. (6) for any number M > 1 of outliers, by simply looking for the first M vectors xi \u2212 x\u0304 with the largest norm. In this case it no longer holds true that zi = 0 for all i = 1, . . . , n, so we have to use the alternating optimization in order to find a solution. However, each iteration is now solved analytically and convergence is fast compared to the case \u03bb < \u03bb\u2217 where we do not have an analytical solution for the optimization over \u00b5. This result motivates the top-down version of the ORCS algorithm. We denote the algorithm by TD-ORCS for the unweighted case (wi = 1, i = 1, . . . , n), and by WTD-ORCS when using the weights given in Lemma 2. The number of required segments K and number of required outliers M is set by the user. In each iteration the algorithm chooses the segment-split which results in the maximal decrease in the squared loss. Whenever a segment is split, the number of outliers belonging to each sub-segment is kept and used in the next iteration, so the overall number of outliers equals M at all iterations. The algorithm is summarized in Alg. 1."}, {"heading": "2.2 Analysis of Lemma 1 for K=2", "text": "We now bound the probability that the solution as given by Lemma 1 fails to detect the correct boundary. We use the\nAlgorithm 1 Top-down outlier-robust hierarchical segmentation\nInput: Data samples {xi}ni=1. Parameters: Number of required segments K, number of required outliers M , weights {wi}n\u22121i=1 . Initialize: Segments boundaries B = {1, n}, current number of segments k = |B| \u2212 1, number of outliers for each segment Mj = M (for j = 1). while k < K do\nfor j = 1, ..., k do Set Sj = { xBj , ..,xBj+1\u22121 } .\nwhile not converged do Split segment Sj into sub-sequences S1,2, using the solution to Eq. (4) at x\u2212 z, with weights {wi}. Find \u03b3 for Mj outliers, using the extension of Eq. (6) to Mj outliers. Set zi = prox\u03b3 (xi \u2212 x\u0304), for i = 1, . . . , n. end while Calculate the mean x\u0304j of segment Sj , and the means of S1,2, denoted by x\u03041 and x\u03042. Set L(j) =\n\u2211 i\u2208S1 \u2016xi \u2212 x\u03041\u20162 + \u2211 i\u2208S2 \u2016xi \u2212 x\u03042\u20162 \u2212\u2211\ni\u2208Sj \u2016xi \u2212 x\u0304j\u20162.\nend for Choose segment j\u2217 = arg maxj L(j) and calculate the splitting point nj\u2217 Add the new boundary xBj\u2217 +nj\u2217 +1 to the (sorted) boundary list B. Set Mj and Mj+1 to the number of zis with non-zero norm in segment Sj and Sj+1, respectively.\nend while\nderived bound to show that the weights given in Lemma 2 are optimal in a sense explained below. For simplicity we analyze the one dimensional case xi \u2208 R, and we show later how the results generalize to multidimensional data.\nWe assume now that the data sequence is composed of two subsequences of lengths n1 and n2, each composed of samples taken iid from some probability distributions with means \u00b51 and \u00b52 respectively, and define \u2206\u00b5 , \u00b52 \u2212 \u00b51. We further assume that the samples are bounded, i.e. |xi| \u2264 M/2, i = 1, . . . , n, for some positive constant M . We set wi = 1 for all i = 1, . . . , n \u2212 1, and quote results for the weighted case where relevant. We note that n1 and n2 represent the ground-truth, and not a variable we have to optimize. We parameterize the sample-index argument of g (\u00b7) in Eq. (4) as i = n1 +m (and similarly i\u2217 = n1 +m\u2217), that is we measure it relatively to the true splitting point n1. For ease of notation, in what follows we substitute g (m) for g (n1 +m). Without loss of generality, we treat the case where m \u2265 0. Note that m\u2217 6= 0 if g(0) < g(m) for some m > 0. The probability of this event is bounded:\nP (g(0) < g(m)) \u2264 2 exp (\u2212Cm) , (7) for C = ( 2\u2206\u00b52n21 ) / ( M2n2 ) . The proof is given in the supplementary material. Note that in order for the bound to be useful, the true segments should not be too long or too short, in agreement with the motivation for using weights given before Lemma 2. We now use Eq. (7) to prove the following theorem:\nTheorem 3 Consider a sequence of n variables as described above. Given \u03b4 \u2208 (0, 1) set m0 = log(2n2/\u03b4)C . Then, the probability that the solution i\u2217 = n1 + m\u2217 as given in Lemma 1 is no less than m0 samples away from the true boundary is bounded, P (m\u2217 \u2265 m0) \u2264 \u03b4 . The proof appears in the supplementary material.\nConsidering the weighted case with arbitrary wi, we repeated the calculation for the bound on P (g(0) < g(m)). To illustrate the influence of the weights on the bound, we heuristically parameterize wj = (j (n\u2212 j))\u03b1 for some \u03b1 \u2208 [0, 1]1. The bound as a function of m is illustrated in Fig. 1 for several values of \u03b1. It is evident that indeed \u03b1 > 0 achieves a faster decaying bound for small n1, and that \u03b1 = 0.5 is optimal in the sense that for \u03b1 > 0.5 the bound is no longer a monotonous function ofm. This agrees with the weights given by Lemma 2. We note that this specific choice of the weights was derived as well by (Bleakley and Vert 2011) by assuming a Gaussian noise model, while our derivation is more general. Finally, we note that generalizing the results for multidimensional data is done by using the fact that for any two vectors a, b \u2208 Rd, it holds true that P (\u2016a\u20162 \u2264 \u2016b\u20162) \u2264 \u2211 i P (|ai| \u2264 |bi|). Thus generalizing Eq. (7) for d > 1 is straightforward. While the bound derived in this way will have a multiplicative factor of the\n1This parametrization is motivated by Eq. (4)\ndimension of the data d, it is still exponential in the number of samples n."}, {"heading": "3 Empirical Study", "text": "We compared the unweighted (TD-ORCS) and weighted (WTD-ORCS) versions of our top-down algorithm to LASSO and group fused LARS 2 of (Bleakley and Vert 2011), which are based on reformulating Eq. (3) as group LASSO regression, and solving the optimization problem either exactly or approximately. Both the TD-ORCS and LARS algorithms have complexity of O (nK). We also report results for a Bayesian change-point detection algorithm (BCP), as formulated by (Erdman and Emerson 2008). We note that we experimented with a left-to-right Hidden Markov Model (HMM) for segmentation. We do not report results for this model, as its performance was inferior to the other baselines."}, {"heading": "3.1 Biphones subsequences segmentation", "text": "In this experiment we used the TIMIT corpus (Garofolo and others 1988). Data include 4, 620 utterances with annotated phoneme boundaries, amounting to more than 170, 000 boundaries. Audio was divided into frames of 16ms duration and 1 ms hop-length, each represented with 13 MFCC coefficients. The task is to find the boundary between two consecutive phonemes (biphones), and performance is evaluated as the mean absolute distance between the detected and ground-truth boundaries. Since the number of segments isK = 2 the ORCS and the TD-ORCS algorithms are essentially identical, and the same holds for LASSO and LARS. Outliers were incorporated by adding short (0.25 framelength) synthetic transients to the audio source. The percentage of outliers reflects the percentage of contaminated frames. Results are shown in Fig. 2 as the mean error as a function of outlier percentage. For low fraction of outliers, all algorithms perform the same, except WTD-ORCS, which is slightly worse. For about 15% outliers, the performance of W-LARS degrades to ~27ms mean error vs ~22ms for the rest. For 30% outliers both TD-ORCS algorithms outperform all other algorithms. The counter-intuitive drop of error at high outliers rate for the TD-ORCS algorithms might be the result of over-estimating the number of outliers. We plan to further investigate this phenomenon in future work.\nWe also compared our algorithm to RD, which (Qiao, Shimomura, and Minematsu 2008) found to be the best among five different objective functions, and was not designed for treating outliers. In this setting (no outliers) the RD algorithm achieved 15.1ms mean error, while TDORCS achieved 13.4ms, with 95% confidence interval (not reported for the RD algorithm) of 0.1."}, {"heading": "3.2 Radio show segmentation", "text": "In this experiment we used a 35 minutes, hand-annotated audio recording of a radio talk show, composed of different sections such as opening title, monologues, dialogs, and songs. A detected segment boundary is considered a\n2http://cbio.ensmp.fr/ jvert/svn/GFLseg/html/\ntrue positive if it falls within a tolerance window of two frames around a ground-truth boundary. Segmentation quality is commonly measured using the F measure, which is defined as 2pr/ (p+ r), where p is the precision and r is the recall. Instead, we used the R measure introduced by (Ra\u0308sa\u0308nen, Laine, and Altosaar 2009), which is more robust to over-segmentation. It is defined as R , 1 \u2212 0.5 (|s1|+ |s2|), where s1 , \u221a (1\u2212 r)2 + (r/p\u2212 1)2 and s2 , (r \u2212 r/p) / \u221a\n2. The R measure satisfies R \u2264 1, and R = 1 only if p = r = 1.\nSignal representation A common representation in speech analysis is the MFCC coefficients mentioned in Sec. 3.1. However, this representation is computed over time windows of tens of milliseconds, and therefore it is not designed to capture the characteristics of a segment with length in the order of seconds or minutes. We therefore apply postprocessing on the MFCC representation. First, the raw audio is divided into N non-overlapping blocks of 5 seconds duration, and the MFCC coefficients are computed for all blocks {Sj}Nj=1. We used 13 MFCC coefficient with 25ms window length and 10ms hop length. Then a Gaussian Mixture Model (GMM) Tj with 10 components and a diagonal covariance matrix is fitted to the jth block Sj . These parameters of the GMM were selected using the Bayesian Information Criterion (BIC). The log-likelihood matrix A is then defined by Aij = logP (Sj |Ti). The clean feature matrix (no outliers) is shown in Fig. 3(a), where different segments can be discerned. Since using the columns of A as features yields a dimension growing with N , we randomly choose a subset of d = 100 rows of A, and the columns of the resulting matrix X \u2208 Rd\u00d7N are the input to the segmentation algorithm. We repeat the experiment for different number of outliers, ranging between 0% and 16% with intervals of 2%. Outliers were added to the raw audio. A given percentage of outliers refers to the relative number of blocks randomly selected as outliers, to which we add a 5 seconds recording of repeated hammer strokes, normalized to a level of 0dB SNR.\nAlgorithms We consider the Outlier-Robust Convex Sequential (ORCS) segmentation, and its top-down versions (weighted and unweighted) which we denote by WTDORCS and TD-ORCS, respectively. We compare the performance to three other algorithms. The first is a greedy bottom-up (BU) segmentation algorithm, which minimizes the sum of squared errors on each iteration. The bottomup approach has been successfully used in tasks of speech segmentation (Qiao, Luo, and Minematsu 2012; Gracia and Binefa 2011). The second algorithm is the W-LARS algorithm of (Bleakley and Vert 2011). The third algorithm is a Bayesian change-point detection algorithm (BCP), as formulated by (Erdman and Emerson 2008). A solution path was found as follows. For the ORCS algorithm, a 35 \u00d7 35 parameter grid was used, where 0 < \u03b3 < \u03b3\u2217 was sampled uniformly, and for each \u03b3 value, 0 < \u03bb < \u03bb\u2217 (\u03b3) was sampled logarithmically, where \u03bb\u2217 (\u03b3) is the critical value for \u03bb\nfor a given choice of \u03b3 (see Sec. 2.1 for details). For the TD-ORCS, W-LARS, and BU algorithms, K = 2, . . . , 150 number of segments were used as an input to the algorithms. For the TD-ORCS algorithm, where the number of required outliers is an additional input parameter, the correct number of outliers was used. For the BCP algorithm, a range of thresholds on the posterior probability of change-points was used to detect a range of number of segments. As is evident from the empirical results below, the ORCS algorithm can achieve high detection rate of the outliers even without knowing their exact number a-priori. Furthermore, we suggest below a way of estimating the number of outliers. For each algorithm, the maximal R measure over all parameters range was used to compare all algorithms.\nResults Results are shown in Fig. 3(b) as the maximal R measure achieved versus the percentage of outliers, for each of the algorithms considered. It is evident that the performance of the BU and BCP algorithms decreases significantly as more outliers are added, while the outlierrobust ORCS algorithm keeps an approximately steady performance. Our unweighted and weighted TD-ORCS algorithms achieve the best performance for all levels of outliers. Results for LARS algorithm are omitted as it did not perform better than other algorithms. We verified the ability of our algorithms to correctly detect outliers by calculating the R measure of the outliers detection of the ORCS algorithm, with zero length tolerance window, i.e a detection is considered a true-positive only if it exactly pinpoints an outlier. The R measure of the detection was evaluated on the \u03b3, \u03bb parameter grid, as well as the corresponding numbers of detected outliers. Results for the representative case of p = 10% outliers are shown in Fig. 4(a). It is evident that a high R measure (> 0.9) is attained on a range of parameters that yield around the true number of outliers. We conclude that one does not need to know the exact number of outliers in order to use the ORCS algorithm, and a rough estimate is enough. Some preliminary results suggest that such an estimate can be approximated from the histogram of number of detected outliers (i.e. Fig. 4(b))."}, {"heading": "4 Related Work and Conclusion", "text": "There is a large amount of literature on change-point detection, see for example (Basseville and Nikiforov 1993; Brodsky and Darkhovsky 1993). Optimal segmentation can be found using dynamic programming (Lavielle and Teyssie\u0300re 2006); however, the complexity of this approach is quadratic in the number of samples n, and therefore might be infeasible for large data sets. Some approaches which achieve complexity linear in n (Levy-leduc and others 2007; Killick, Fearnhead, and Eckley 2012) treat only one dimensional data. Some related work is concerned with the objective Eq. (3) we presented in Sec. 2.1. In (Levy-leduc and others 2007) it was suggested to reformulate Eq. (3) for the one dimensional case as a LASSO regression problem (Tibshirani 1996; Yuan and Lin 2006), while (Bleakley and Vert 2011) extended this approach to multidimensional data, although not treating outliers directly. An-\nother common approach is deriving an objective from a maximum likelihood criterion of a generative model, and then either optimize the objective or use it as a criterion for a top-down or a bottom-up approach (Qiao, Luo, and Minematsu 2012; Qiao, Shimomura, and Minematsu 2008; Gracia and Binefa 2011; Olshen et al. 2004). We note that the two-dimensional version of Eq. (3) is used in image denoising applications, where it is known as the TotalVariation of the image (Rudin, Osher, and Fatemi 1992; Chambolle 2004; Beck and Teboulle 2009a). Finally, we note that all these approaches do not directly incorporate outliers into the model.\nWe formulated the task of segmenting sequential data and detecting outliers using convex optimization, which can be solved in an alternating manner. We showed that a specific choice of weighting can empirically enhance performance. We described how to calculate \u03bb\u2217 and \u03b3\u2217, the critical values for the split into two segments and the detection of the first outlier, respectively. These values are useful for finding a solution path in the two-dimensional parameter space. We also derived a top-down, outlier-robust hierarchical segmentation algorithm which minimizes the objective in a greedy manner. This algorithm allows for directly controlling both the number of desired segments K and number of outliers M . Experiments with real-world audio data with outliers added manually demonstrated the superiority of our algorithms.\nWe consider a few possible extensions to the current work. One is deriving algorithms that will work on-the-fly. Another direction is to investigate more involved noise models, such as noise which corrupts a single feature along all samples, or a consecutive set of samples. Yet another interesting question is how to identify that different segments come from the same source, e.g. that the same speaker is present at different locations in a recording. We plan to investigate these directions in future work."}, {"heading": "A Proofs", "text": ""}, {"heading": "A.1 Proof of Lemma 1", "text": "Proof: Our starting point is the following lemma, which makes further analysis easier.\nLemma 4 Assume an optimal solution \u00b5\u2217 of Eq. (3) is given, and therefore we also know to which segment each data sample belongs. If we replace all samples in a segment with the mean of these samples, the optimal solution \u00b5\u2217 will not change.\nThe proof appears in Sec. A.5. We now analyze the transition of the solution to Eq. (3) from K = 1 to K = 2 segments. During the analysis we use the fact that the solution path is continuous in \u03bb, as was shown previously in another context by (Chi and Lange 2013). We denote by \u03bb\u2217 the value of \u03bb at the splitting point, and we assume that for the two segments solution we have n1 samples in the first segment and n2 = n \u2212 n1 samples in the second segment. We denote the means of the two segments by x\u03041 and x\u03042. Lemma 4 allows us to replace samples in a segment with the mean of the segment, without changing the optimal solution \u00b5\u2217. This means that for K = 2 all analysis is taking place on the line connecting x\u03041 and x\u03042 and is therefore essentially one dimensional. For K = 1 the regularization term vanishes, so the solution which we denote by \u00b5 is simply the mean of the whole data set, \u00b5 = x\u03041 + \u03b10(x\u03042 \u2212 x\u03041), where \u03b10 = n2/n. For K = 2, we denote the solution by \u00b51 and \u00b52, and we parameterize \u00b51 by \u00b51 = x\u03041 +\u03b1 (x\u03042 \u2212 x\u03041), for some \u03b1. We note that \u03b1 \u2264 \u03b10, since we know that \u00b51 is closer to x\u03041 than \u00b5 is. The parametrization for \u00b52 is therefore \u00b52 = x1 + ( 1\u2212 n1n2\u03b1 ) (x\u03042 \u2212 x\u03041) 3. In order to find \u03b1, we look for a minimum of the objective h for K = 2:\nh = n1 2 \u2016x\u03041 \u2212 \u00b51\u20162 + n2 2 \u2016x\u03042 \u2212 \u00b52\u20162 (8)\n+ \u03bbwn1 \u2016\u00b52 \u2212 \u00b51\u20162 .\nPlugging \u00b51 and \u00b52 as parameterized by \u03b1 into Eq. (8) and looking for the minimum, we get\n\u03b1 = argmin \u03b1\u2032\n{ y2\u03b1\u20322nn1/2n2 \u2212 \u03bbwn1y\u03b1\u2032nn2 + \u03bbwn1y } = \u03bbwn1/n1y,\nwhere we define y , \u2016x\u03042 \u2212 x\u03041\u2016. In order to find \u03bb\u2217, we require that the objective forK = 1 andK = 2 has the same value at the splitting point, where \u03bb = \u03bb\u2217. This requirement is equivalent to the requirement that \u03b1 = \u03b10, since at the splitting point we have \u00b5 = \u00b51 = \u00b52. This leads to the solution for \u03bb\u2217, where we explicitly include the dependence on n1, to emphasize that this is the solution provided that n1 is known:\n\u03bb\u2217(n1) = 1\nwn1\nn1(n\u2212 n1) n \u2016x\u03042(n1)\u2212 x\u03041(n1)\u20162 . (9)\n3This can be derived either directly by requiring that the derivative of Eq. (3) forK = 2 equals zeros, or by noting that the \u2018center of mass\u2019 of \u00b51 and \u00b52 is just \u00b5. Both approaches gives the same equation, namely n1\u00b51 + n2\u00b52 = n\u00b5.\nIn order to find the actual splitting point n\u2217, we note that the split into two segments occurs as \u03bb is decreased from \u03bb > \u03bb\u2217 to \u03bb < \u03bb\u2217, so maximizing \u03bb\u2217(n1) over n1 gives the splitting point n\u2217:\nn\u2217 (x) = argmax n1 g (n1,x) , \u03bb\u2217 (x) = g (n\u2217,x) ,\nwhere:\ng (n1,x) =\n{ 1\nwn1\nn1(n\u2212 n1) n \u2016x\u03042(n1)\u2212 x\u03041(n1)\u20162\n} ."}, {"heading": "A.2 Proof of Lemma 2", "text": "Proof: For K = 2 the unrelaxed optimization problem Eq. (5) is equivalent to\nargmin n1,\u00b51,\u00b52 { n1\u2211 i=1 \u2016xi \u2212 \u00b51\u20162 + n\u2211\ni=n1+1\n\u2016xi \u2212 \u00b52\u20162 } .\nThe minimization on \u00b51,\u00b52 is immediate and is given by the means of the segments, so we get\nargmin n1 { n1\u2211 i=1 \u2016xi \u2212 x\u03041\u20162 + n\u2211\ni=n1+1\n\u2016xi \u2212 x\u03042\u20162 } , (10)\nwhere we defined\nx\u03041 = 1\nn1 n1\u2211 i=1 xi\nx\u03042 = 1\nn2 n\u2211 i=n1+1 xi\nand n2 , n \u2212 n1. Recall that the solution to the relaxed optimization problem is given by Lemma 1, as described in Sec. 2:\nargmax n1,n2=n\u2212n1 { n1n2 nwn1 \u2016x\u03042 \u2212 x\u03041\u20162 } , (11)\nwhere wi are the weights. We now argue that Eq. (10) and Eq. (11) have the same solution, for the specific choice of wi = \u221a i (n\u2212 1) /n. First note that Eq. (10) can be rewrit-\nten as argmax n1,n2=n\u2212n1\n{ n1 \u2016x\u03041\u20162 + n2 \u2016x\u03042\u20162 } . Next, we show\nthat this objective and the square of the (non-negative) objective Eq. (11) differ by a constantC, which depends on the data xi but not on n1:\nn1 \u2016x\u03041\u20162 + n2 \u2016x\u03042\u20162 = n1n2 n \u2016x\u03042 \u2212 x\u03041\u20162 + C\nn21 n \u2016x\u03041\u20162 + n22 n \u2016x\u03042\u20162 = C \u2212 2n1n2 n x\u0304T1 x\u03042\n1\nn \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 xi \u2225\u2225\u2225\u2225\u2225 2 = C,\nwhich proves that indeed Eq. (10) and Eq. (11) attain their optimal value at the same n\u22171"}, {"heading": "A.3 Proof of Eq. (7)", "text": "Proof: Define the random variable Ym which is the difference between the empirical means of two subsequences created by splitting after n1 +m samples:\nYm , 1\nn2 \u2212m n\u2211 i=n1+m+1 xi \u2212 1 n1 +m n1+m\u2211 i=1 xi.\nThis allows us to rewrite Eq. (4) as an optimization over m:\ng (m) , (n1 +m)(n2 \u2212m)\nn |Ym| ,\nm\u2217 = argmax m {g (m)} ,\nwhere m \u2208 [\u2212n1 + 1, n2 \u2212 1]. Without loss of generality, we treat the case where m \u2265 0. Note that m\u2217 6= 0 if g(0) < g(m) for some m > 0. The probability of this event is:\nP (g(0) < g(m)) = (12) P ( n1n2 n |Y0| < (n1 +m)(n2 \u2212m) n |Ym| ) .\nDefining the following random variables:\nW\u00b1m , n1n2 n Y0 \u00b1 (n1 +m)(n2 \u2212m) n Ym, (13)\nwe can rewrite P (g(0) < g(m)) \u2264 P ( W+m < 0 ) + P ( W\u2212m < 0 ) , (14)\nwhere we used the fact that for two random variables A and B, it holds true that\nP (|A| < |B|) \u2264 P (A < B) + P (A < \u2212B) .\nUsing the definition of Ym, we rewriteW\u00b1m as the (weighted) average of a sequence of the n random variables composing the data,\nW+m = 1\nn\n( (m\u2212 2n2)\nn1\u2211 i=1 xi + (n1 \u2212 n2 +m) n1+m\u2211 i=n1+1 xi\n+ (2n1 +m) n\u2211 i=n1+m+1 xi\n) ,\nand\nW\u2212m = 1\nn\n( \u2212m\nn1\u2211 i=1 xi + (n\u2212m) n1+m\u2211 i=n1+1 xi \u2212m n\u2211 i=n1+m+1 xi\n) .\nThe means of W\u00b1m are given by E [ W\u2212m ] = n1m\nn \u2206\u00b5,\nE [ W+m ] = n1 (2n2 \u2212m)\nn \u2206\u00b5,\nwhere we define \u2206\u00b5 , \u00b52 \u2212 \u00b51. From Eq. (12), Eq. (13), and Eq. (14) it follows that\nP ( n1n2 n Y0 < (n1 +m)(n2 \u2212m) n |Ym| ) \u2264P ( W\u2212m < 0 ) + P ( W+m < 0 ) .\nUsing Hoeffding\u2019s inequality to bound the probabilities that W\u00b1m are negative we get:\nP ( W\u2212m < 0 ) \u2264 exp ( \u2212 2n 2 1m\u2206\u00b5 2\nM2n (n\u2212m)\n) , B\u2212,\nand similarly: P ( W+m < 0 ) \u2264 exp ( \u2212 2n 2 1 (2n2 \u2212m) 2 \u2206\u00b52\nM2n (m (n\u2212m\u2212 4n1) + 4n1 (n\u2212 n1)) ) , B+.\nIt can be shown that B+ < B\u2212, provided that m is in its feasible range, i.e 0 < m < n2. We conclude that\nP ( n1n2 n |Y0| < (n1 +m)(n2 \u2212m) n |Ym| )\n\u2264 B\u2212 +B+ \u2264 2B\u2212,\nso we have that P ( n1n2 n |Y0| < (n1 +m)(n2 \u2212m) n |Ym| )\n\u2264 2 exp ( \u2212 2n 2 1m\u2206\u00b5 2\nM2n (n\u2212m)\n) \u2264 2 exp ( \u22122n 2 1\u2206\u00b5 2\nM2n2 m\n) ,\nwhich proves Eq. (7) for C = 2\u2206\u00b5 2n21\nM2n2 ."}, {"heading": "A.4 Proof of Theorem 3", "text": "Proof: Using Eq. (7) and the union bound, we get\nP (m\u2217 \u2265 m0) \u2264 P (\u2203m \u2265 m0 : g(0) < g(m))\n\u2264 n2\u2211\nm=m0\nP (g(0) < g(m))\n\u2264 n2P (g (0) < g (m0))\n\u2264 2n2 exp (\u2212Cm0) (?) \u2264 \u03b4 ,\nwhere (?) follows from the definition of m0."}, {"heading": "A.5 Proof of Lemma 4", "text": "Proof: Consider a segment of n0 data samples, and denote by \u00b50 the centroid of this segment. The mean of the segment is given by x\u03040 = 1n0 \u2211 xi\u2208\u00b50 xi. The contribution of this segment to the first term in the objective Eq. (3) is given by\n1\n2 \u2211 xi\u2208\u00b50 \u2016xi \u2212 \u00b50\u20162 = 1 2 \u2211 xi\u2208\u00b50 ( \u2016xi\u20162 \u2212 2xTi \u00b50 + \u2016\u00b50\u2016 2 ) . (15) If we now substitute x\u03040 for each sample xi in this segment, the contribution to the objective becomes n0 2 \u2016x\u03040 \u2212 \u00b50\u20162 = n0 2 ( \u2016x\u03040\u20162 \u2212 2x\u0304T0 \u00b50 + \u2016\u00b50\u2016 2 ) . (16)\nIt is straightforward to show that as a function of \u00b50, Eq. (15) and Eq. (16) differ by a constant which depends only on the data samples xi \u2208 \u00b50, not on \u00b50. Since this argument holds for all segments, we conclude that replacing each data sample with the mean of the segment to which it belongs, results in the same objective, up to a constant. Therefore the optimal solution \u00b5\u2217 does not change."}], "references": [{"title": "Convex optimization with sparsityinducing norms. Optimization for Machine Learning 19\u201353", "author": ["Bach"], "venue": null, "citeRegEx": "Bach,? \\Q2011\\E", "shortCiteRegEx": "Bach", "year": 2011}, {"title": "I", "author": ["M. Basseville", "Nikiforov"], "venue": "V.", "citeRegEx": "Basseville and Nikiforov 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems", "author": ["Beck", "A. Teboulle 2009a] Beck", "M. Teboulle"], "venue": "Image Processing, IEEE Transactions on", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Beck", "A. Teboulle 2009b] Beck", "M. Teboulle"], "venue": "SIAM J. Img. Sci", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "Statistical models for text segmentation. Machine learning 34(1-3):177\u2013210", "author": ["Berger Beeferman", "D. Lafferty 1999] Beeferman", "A. Berger", "J. Lafferty"], "venue": null, "citeRegEx": "Beeferman et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Beeferman et al\\.", "year": 1999}, {"title": "and Vert", "author": ["K. Bleakley"], "venue": "J.-P.", "citeRegEx": "Bleakley and Vert 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "M", "author": ["Brent"], "venue": "R.", "citeRegEx": "Brent 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "B", "author": ["B.E. Brodsky", "Darkhovsky"], "venue": "S.", "citeRegEx": "Brodsky and Darkhovsky 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "and Lange", "author": ["E.C. Chi"], "venue": "K.", "citeRegEx": "Chi and Lange 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "J", "author": ["C. Erdman", "Emerson"], "venue": "W.", "citeRegEx": "Erdman and Emerson 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "G", "author": ["P.A. Forero", "V. Kekatos", "Giannakis"], "venue": "B.", "citeRegEx": "Forero. Kekatos. and Giannakis 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "J", "author": ["Garofolo"], "venue": "S., et al.", "citeRegEx": "Garofolo and others 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "and Binefa", "author": ["C. Gracia"], "venue": "X.", "citeRegEx": "Gracia and Binefa 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimal detection of changepoints with a linear computational cost", "author": ["Fearnhead Killick", "R. Eckley 2012] Killick", "P. Fearnhead", "I. Eckley"], "venue": "Journal of the American Statistical Association", "citeRegEx": "Killick et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Killick et al\\.", "year": 2012}, {"title": "and Teyssi\u00e8re", "author": ["M. Lavielle"], "venue": "G.", "citeRegEx": "Lavielle and Teyssi\u00e8re 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Catching change-points with lasso", "author": ["Levy-leduc", "C others 2007] Levy-leduc"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy.leduc and Levy.leduc,? \\Q2007\\E", "shortCiteRegEx": "Levy.leduc and Levy.leduc", "year": 2007}, {"title": "and Roueff", "author": ["C. L\u00e9vy-Leduc"], "venue": "F.", "citeRegEx": "L\u00e9vy.Leduc and Roueff 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "G", "author": ["G. Mateos", "Giannakis"], "venue": "B.", "citeRegEx": "Mateos and Giannakis 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A", "author": ["Olshen"], "venue": "B.; Venkatraman, E.; Lucito, R.; and Wigler, M.", "citeRegEx": "Olshen et al. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "A study on unsupervised phoneme segmentation and its application to automatic evaluation of shadowed utterances", "author": ["Luo Qiao", "Y. Minematsu 2012] Qiao", "D. Luo", "N. Minematsu"], "venue": "Technical report", "citeRegEx": "Qiao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Qiao et al\\.", "year": 2012}, {"title": "Unsupervised optimal phoneme segmentation: Objectives, algorithm and comparisons", "author": ["Shimomura Qiao", "Y. Minematsu 2008] Qiao", "N. Shimomura", "N. Minematsu"], "venue": "In ICASSP", "citeRegEx": "Qiao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Qiao et al\\.", "year": 2008}, {"title": "U", "author": ["R\u00e4s\u00e4nen, O.J.", "Laine"], "venue": "K.; and Altosaar, T.", "citeRegEx": "R\u00e4s\u00e4nen. Laine. and Altosaar 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "L", "author": ["Rudin"], "venue": "I.; Osher, S.; and Fatemi, E.", "citeRegEx": "Rudin. Osher. and Fatemi 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "Prosody-based automatic segmentation of speech into sentences and topics. Speech Communication 32(1\u20132)", "author": ["Shriberg"], "venue": null, "citeRegEx": "Shriberg,? \\Q2000\\E", "shortCiteRegEx": "Shriberg", "year": 2000}, {"title": "and Lin", "author": ["M. Yuan"], "venue": "Y.", "citeRegEx": "Yuan and Lin 2006", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [], "year": 2014, "abstractText": "We derive a convex optimization problem for the task of segmenting sequential data, which explicitly treats presence of outliers. We describe two algorithms for solving this problem, one exact and one a top-down novel approach, and we derive a consistency results for the case of two segments and no outliers. Robustness to outliers is evaluated on two real-world tasks related to speech segmentation. Our algorithms outperform baseline segmentation algorithms.", "creator": "LaTeX with hyperref package"}}}