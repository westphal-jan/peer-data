{"id": "1411.6231", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Compound Rank-k Projections for Bilinear Analysis", "abstract": "in many close - world techniques, data are denoted by dynamic or high - contrast tensors. despite widespread empirical literature, currently standardized two - dimensional difference analysis algorithms operating a single projection model can exploit the discriminant information across 2d, positioning the model less flexible. in that paper, we propose tandem parallel compound beta - 3 projection ( crp ) algorithm for smoothing analysis. crp deals across situations directly avoiding transforming them into lines, and it simultaneously preserves the correlations following statistical estimation and decreases the computation footprint. different from the primitive two dimensional problem analysis algorithms, objective function values of coordinates changes somewhat. even second, computational utilizes multiple rank - k discount algorithms to enable thus larger search degree radius which the optimal functions can confidently found. however this way, the winning distance is decreasing.", "histories": [["v1", "Sun, 23 Nov 2014 12:50:20 GMT  (1468kb)", "https://arxiv.org/abs/1411.6231v1", "10 pages"], ["v2", "Sun, 24 May 2015 07:58:29 GMT  (1467kb)", "http://arxiv.org/abs/1411.6231v2", "10 pages"], ["v3", "Wed, 3 Jun 2015 04:26:25 GMT  (3259kb)", "http://arxiv.org/abs/1411.6231v3", "Accepted by IEEE Transactions on Neural Networks and Learning Systems (IEEE T-NNLS), 2015"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaojun chang", "feiping nie", "sen wang", "yi yang", "xiaofang zhou", "chengqi zhang"], "accepted": false, "id": "1411.6231"}, "pdf": {"name": "1411.6231.pdf", "metadata": {"source": "CRF", "title": "Compound Rank-k Projections for Bilinear Analysis", "authors": ["Xiaojun Chang", "Feiping Nie", "Sen Wang", "Yi Yang"], "emails": ["chengqi.zhang@uts.edu.au}", "nie@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n62 31\nv3 [\ncs .L\nG ]\n3 J\nun 2\n01 5\nIndex Terms\u2014Discriminant Analysis, Feature Extraction, Rank-k Projection, High-order Representation\n\u2726\n1 INTRODUCTION\nL INEAR discriminant analysis (LDA), also knownas Fisher\u2019s Linear Discriminant (FLD), is a classical method for data representation and feature extraction. FLD is commonly utilized in the fields of computer vision and pattern recognition [1], [2], [3], [4]. For example, Peter N. Belhumeur et al. [5] use LDA to represent facial expression images efficiently. The classical LDA aims to find a set of vectors so as to maximize the trace of between-class scatter matrix while minimizing the trace of within-class scatter matrix in the transformed feature space. Recent works have indicated that it is more natural and beneficial to represent an image with a matrix since exploiting the neighborhood information of a certain pixel is essential to the performance. The classical LDA, however, requires that an image is represented by a vector. This vectorization method has some inherent drawbacks. Firstly, it will erase the correlations within the matrix. Secondly, the data dimensionality increases when we transform the matrix representation into the vector\nThis work was partially supported by the National Program on Key Basic Research Project (973 Program) under Grant 2015CB352302, partially supported by the ARC DECRA project under Grant DE130101311 and partially supported by the National Natural Science Foundation of China under Grant 61303143. X. Chang, Y. Yang and C. Zhang are with Center for Quantum Computation and Intelligent Systems, University of Technology Sydney, Australia. Email: {uqxchan1@cs.cmu.edu,yi.yang@uts.edu.au, chengqi.zhang@uts.edu.au} F. Nie is with Center for OPTical Imagery Analysis and Learning, Northwestern Polytechnical University, Shaanxi, China. Email: feipingnie@gmail.com S. Wang and X. Zhou are with the school of Information Technology and Electrical Engineering, The University of Queensland, Queensland 4072. Email: {sen.wang}@itee.uq.edu.au\nrepresentation [6], [7]. Hence, the computational burden is increased dramatically. To address these problems, two-dimensional linear discriminant analysis and its variants have been studied over the last decade. Instead of vectorizing the matrices before dimension reduction, the twodimensional linear discriminant analysis works with data in a matrix representation, which can preserve the spatial correlation of the original data and reduce the computation complexity. Liu et al. [8] propose to use an optimal discriminant criterion to extract algebraic features, calculating a set of optimal discriminant projection vectors according to a generalized Fisher criterion function. The classical 2DLDA, proposed by Ye et al. [9], aims to learn a single set of projection matrices and introduces an iterative algorithm. In their experimental results, performance is stable when the number of iteration increases. Thus, only one iteration step is required in their experiments. Inoue et al. [10], however, have pointed out the iterative algorithm proposed in [9] does not necessarily guarantee the monotonicity of the objective function value, which is mainly caused by the singularity of the between-class scatter matrix. In [10], they present a simple method, namely Selective Algorithm for 2DLDA, to select transformation matrices with a higher discriminant ability. In addition to the proposed algorithm, they also propose a noniteratively parallel algorithm, which transforms rows and columns of the matrices independently. Although this method seems promising from the theoretical perspective, their experimental results show that its recognition rate is a little worse than the classical 2DLDA.\nAnother limitation of all the existing discriminant analysis algorithms [1], [2], [3], [5], [11], [12], [9], [13], [14] is that their performance suffer from the balance between the degree of freedom and the avoidance of the over-fitting problem. Specifically, LDA produces multiple full rank bilinear projections, which has the largest degree of freedom but induces the well-known over-fitting problem. To overcome this problem, the authors [15] has proposed multiple rank-1 projection method based on the principal component. Its experimental results demonstrate that this method is at its best when the number of training samples is very small. The classical 2DLDA produces multiple rank1 projections and has better performance in dealing with the over-fitting problem, but has a much smaller degree of freedom. Motivated by these observations, we intend to increase the degree of freedom while avoiding the over-fitting problem. In this paper, we aim to solve the above limitations of the existing discriminant analysis algorithms for high-order data and propose a compound rank-k projection algorithm for discriminant bilinear analysis. Different from [9], the convergence of our optimization approach is explicitly guaranteed. We adopt multiple orthogonal projection models to obtain more discriminant projection directions. In particular, we use h sets of projection matrices to find a lowdimensional representation of the original data. The h projection matrices are orthogonal to each other. By doing so, we can project the original data into different orthogonal basis and information from various perspectives can be obtained. The key novelty of our method is that it adopts multiple projection models, which are integrated and work collaboratively. In this way, a larger search space is provided to find the optimal solution, which will yield better classification performance. We name the proposed algorithm as Compound Rank-k Projection for Bilinear Analysis (CRP). It is worthwhile noting that the algorithm can be readily extended to high-order tensor discriminant analysis. The main contributions of our work can be summarized as follows:\n1) CRP can deal with matrix representations directly without converting them into vectors. Hence, spatial correlations within the original data can be preserved. Compared with the conventional algorithms, the computation complexity is reduced. 2) Compared to the classical 2-dimensional linear discriminant analysis methods [9], [13], [16], CRP benefits from the trade-off between the degree of freedom and the avoidance of the over-fitting problem. 3) Although the classical 2DLDA gains good performance, its iterative optimization algorithm may not converge due to the singularity of the between-class scatter matrix. Differently, the\nconvergence of our algorithm is explicitly guaranteed.\nThe rest of this paper is organized as follows: Section 2 summarizes an overview of the classical LDA as well as 2DLDA. A novel compound rank-k projection for bilinear analysis is proposed in section 3. We present our experimental results on five different datasets in section 4. The conclusion of our work is discussed in section 5.\n2 RELATED WORK 2.1 Classical LDA\nThe conventional LDA aims to project the original high-dimensional data to a lower dimensional subspace for better classification performance [17], [18], [19], [20]. The original dataset is denoted as X \u2208 Rl\u00d7n, which is grouped into c classes \u03c0 = {\u03c01, \u03c02, ..., \u03c0c}. \u03c0i contains ni data points from the i-th class. The transformation of the classical LDA to a lower dimensional subspace is yi = W Txi, where W \u2208 R l\u00d7c and xi is a vector representation of the original data. By finding the best transformation matrix W , data points from different classes become more separated while data points from the same class become more compact after the transformation [21], [22], [23], [24], [25]. In this way, better classification performance is achieved. More specifically, two scatter matrices in LDA, namely between-class matrix Sb and within-class matrix Sw, are defined as follows:\nSb =\nc\u2211\ni=1\nni(Mi \u2212M)(Mi \u2212M) T\nand\nSw =\nc\u2211\ni=1\n\u2211\nXj\u2208\u03c0i\n(Xj \u2212Mi)(Xj \u2212Mi) T ,\nwhere ni is the number of data samples in the i-th class and Xj is the j-th sample in the i-th class. Mi = 1 ni \u2211 Xj\u2208\u03c0i\nXj is the mean of the i-th class, and M = 1 n \u2211c i=1 \u2211 Xj\u2208\u03c0i\nXj is the global mean. In a lower dimensional subspace, the between-class scatter matrix and the within-class scatter matrix are transformed to S\u0303b = W TSbW and S\u0303w = W TSwW , respectively, according to [26]. The objective function is defined as follows:\nmax W\nTr((W TSwW ) \u22121(W TSbW )), (1)\nwhere Tr(\u00b7) denotes the matrix trace operation. The objective function aims to find the best W to maximize the trace of the transformed between-class scatter matrix S\u0303b and minimize the trace of the transformed within-class scatter matrices S\u0303w. This objective function can be solved by eigen-decomposition of (Sw)\n\u22121Sb in [27], [28], [29]. However, as we mentioned in section 1, the classical LDA has some inherent drawbacks. To tackle these problems, twodimensional LDA has been proposed.\n2.2 Classical 2DLDA\nDifferent from the classical LDA, 2DLDA uses the matrix representation instead of the vector representation. We denote a set of data as X = {X1, X2, ..., Xn}, Xi \u2208 R\nl1\u00d7l2 , which are grouped into c different classes \u03c01, ..., \u03c0c. The goal of 2DLDA is to seek a single set of transformation matrices, U and V , projecting the original data into a lower dimensional subspace [30]. In this subspace, the two newly transformed matrices can be computed as follows:\nS\u0303w =\nc\u2211\ni=1\n\u2211\nXj\u2208\u03c0i\nUT (Xj \u2212Mi)V V T (Xj \u2212Mi) TU\nS\u0303b =\nc\u2211\ni=1\nniU T (Mi \u2212M)V V T (Mi \u2212M) TU\nThe objective function is defined as follows:\nmax U,V f = max U,V\nTr((S\u0303w) \u22121S\u0303b) (2)\nAs mentioned before, the key idea of the classical 2DLDA is to find the optimal U and V which maximize the objective function value f in (2). Since it is difficult to compute the optimal U and V simultaneously, Ye et al. [9] propose an iterative algorithm. However, Inoue et al. [10] have pointed out that this iterative algorithm cannot guarantee the monotonicity of the objective function value f and it is hard to determine appropriate termination criteria. To promise the monotonicity, the authors adopt trace ratio and trace difference in [31].\n3 COMPOUND RANK-k PROJECTION In this section, we describe in detail our proposed algorithm. We define a set of data points as X = {X1, X2, ..., Xn}, Xi \u2208 R\nl1\u00d7l2 , which is grouped into c different classes {\u03c01, ..., \u03c0c}. In contrast to the classical 2DLDA, our proposed approach aims to seek h optimal sets of U = {U1, U2, ..., Uh}, Ui \u2208 R\nl1\u00d7k and V = {V1, V2, ..., Vh}, Vi \u2208 R\nl2\u00d7k to project the original data points into h-dimensional subspaces:\nX\u0303i = {Tr(U T 1 XiV1), T r(U T 2 XiV2), ..., T r(U T h XiVh)}\nThe primary goal of our approach is that it employs multiple models to provide a larger space to find the optimal solution. In this way, the degree of freedom is increased. To put it from another way, we enhance the discriminant ability. It is worth noticing that there is a trade-off between the degree of freedom and the avoidance of the overfitting problem. To be more specific, when we allow the rank of bilinear projection UV T to be full-rank, the proposed algorithm can be reduced to the classical LDA, which has the largest degree of freedom but induces the well-known over-fitting problem. The classical 2DLDA has better performance in dealing\nwith the over-fitting problem but has a much smaller degree of freedom. Compared with these algorithms, our proposed algorithm can benefit from this tradeoff. Suppose we have extracted the first p \u2212 1 dimensional features, now we begin to extract the p-th dimension and make it orthogonal to the first p \u2212 1 dimensions. First we perform an orthogonal transformation on the data:\nvec(X)\u2190 vec(X)\u2212vec(Up\u22121V T p\u22121)(vec(Up\u22121V T p\u22121)) T vec(X)\nThe key point of this orthogonal transformation is that we can project the original data into different orthogonal basis so as to get various information from differing perspectives. According to Lemma 1 in Appendix, we obtain:\nvec(X)\u2190 vec(X)\u2212 Tr(UTp\u22121XVp\u22121)vec(Up\u22121V T p\u22121) (3)\nThen we compute the optimal solution of the following objective function for the transformed data.\nmax h\u2211\np=1\nc\u2211 i=1 Tr(UTp (Xi \u2212X)Vp) 2\nc\u2211 i=1 \u2211 Xj\u2208\u03c0i Tr(UTp (Xj \u2212Xi)Vp)2 ,\ns.t. vec(UpV T p ) T vec(UpV T p ) = 1\nIn order to avoid over-fitting and singularity of the within-class scatter matrix, a regularization term is added to the objective function. We can rewrite the objective function as follows.\nmax h\u2211\np=1\nc\u2211 i=1 Tr(UTp (Xi \u2212X)Vp) 2\nc\u2211 i=1 \u2211 Xj\u2208\u03c0i Tr(UTp (Xj \u2212Xi)Vp)2 + \u03bbTr(UpV Tp VpUTp ) ,\n(4)\ns.t. vec(UpV T p ) T vec(UpV T p ) = 1\nAccording to Lemma 3 in the appendix, the equation can be transformed to\nmax h\u2211\np=1\nc\u2211 i=1 Tr(UTp (Xi \u2212X)Vp) 2\nc\u2211 i=1 \u2211 Xj\u2208\u03c0i Tr(UTp (Xj \u2212Xi)Vp) 2 + \u03bbTr(UpV Tp VpU T p )\n,\n(5)\ns.t. Tr(UTp UpV T p Vp) = 1\nThe optimal Up and Vp would maximize the objective function. Since it is difficult to compute the optimal Up and Vp simultaneously, we present an iterative algorithm. To be more specific, for a fixed Vp, we can obtain the optimal Up by solving the optimization problem that is quite similar to Eq. (2). Afterwards, Vp is similarly updated by using the obtained Up. Note that our algorithm can promise a monotonic increase of the objective function.\n4 OPTIMIZATION In this section, we propose an iterative approach to optimizing the objective function in (5). Specifically, for a fixed Vp, the objective function equals to:\nmax\nc\u2211\ni=1\nTr(UTp (Xi \u2212 X)Vp) 2\nc\u2211\ni=1\n\u2211\nXj\u2208\u03c0i\nTr(UTp (Xj \u2212 Xi)Vp) 2 + \u03bbTr(UpV Tp VpU T p )\ns.t. Tr(UTp UpV T p Vp) = 1\nSince there is no straightforward solution to this function, we aim to rewrite the objective function to a generalized eigen-decomposition problem. According to Lemma 5 and Lemma 6 in the appendix, we can rewrite the objective function as follows:\nmax\nc\u2211\ni=1\n(uTp (I \u2297 (Xi \u2212 X))vp) 2\nc\u2211\ni=1\n\u2211\nXj\u2208\u03c0i\n(uTp (I \u2297 (Xj \u2212 Xi))vp) 2 + \u03bbTr(UpV Tp VpU T p )\ns.t. u T p D v pup = 1\n\u21d4 max uTp M v pup\nuTp N v pup\n, (6)\ns.t. uTp D v pup = 1\nwhere up = vec(Up) (7)\nvp = vec(Vp) (8)\nDvp = (V T p Vp)\u2297 I (9)\nMvp =\nc\u2211\ni=1\n(I \u2297 (Xi \u2212X))vpv T p (I \u2297 (Xi \u2212X) T ) (10)\nN v p =\nc\u2211\ni=1\n\u2211\nXj\u2208\u03c0i\n(I \u2297 (Xj \u2212Xi))vpv T p (I \u2297 (Xj \u2212Xi) T )+\u03bbDvp\n(11)\nIt is noticed that the rewritten objective function in (6) has become similar to the optimization problem in (1). Therefore, we can compute the optimal Up by solving the optimization problem in (6) as follows:\nup = q\u221a\nqTDvpq , (12)\nwhere q is the largest eigenvector of (Nvp ) \u22121Mvp .\nNext, we compute the optimal Vp for a fixed Up.\nmax\nc\u2211\ni=1\nTr(UTp (Xi \u2212 X)Vp) 2\nc\u2211\ni=1\n\u2211\nXj\u2208\u03c0i\nTr(UTp (Xj \u2212 Xi)Vp) 2 + \u03bbTr(UpV Tp VpU T p )\ns.t. Tr(UTp UpV T p Vp) = 1\nWith similar procedures, we rewrite the objective function to an eigen-decomposition problem. According to Lemma 5 and Lemma 6 in Appendix, the objective function can be transformed into:\nmax\nc\u2211\ni=1\n(vTp (I \u2297 (Xi \u2212 X) T )up) 2\nc\u2211\ni=1\n\u2211\nXj\u2208\u03c0i\n(vTp (I \u2297 (Xj \u2212 Xi) T )up)2 + \u03bbTr(UpV Tp VpU T p )\ns.t. vTp D u pvp = 1\n\u21d4 max vTp M u p vp\nvTp N u p vp\n, (13)\ns.t. vTp D u pvp = 1\nwhere\nDup = (U T p Up)\u2297 I (14)\nMup =\nc\u2211\ni=1\n(I \u2297 (Xi \u2212X) T )upu T p (I \u2297 (Xi \u2212X)) (15)\nN u p =\nc\u2211\ni=1\n\u2211\nXj\u2208\u03c0i\n(I\u2297 (Xj\u2212Xi) T )upu T p (I\u2297 (Xj\u2212Xi))+\u03bbD u p\n(16)\nSimilarly, we can compute the optimal Vp by solving the optimization problem in Eq. (13). The solution is\nvp = q\u221a\nqTDupq , (17)\nwhere q is the largest eigenvector of (Nvp ) \u22121Mvp .\nThe optimizations of Up| h p=1 and Vp| h p=1 are iterated until convergence. Pseudo-code for our proposed CRP is given in Algorithm 1. We set k = 2 empirically. The most time-consuming steps are eigenvalue decomposition operations (line 10 and line 16 in Algorithm 1) and the total time complexity is O((kmax(l1, l2))\n3). For example, we use Coil20 dataset, in which the dimensionality of the original data is 32 \u00d7 32. The time complexity of CRP is O((2 \u00d7 32)3), which is significantly small compared to that of the classical LDA [9] O((1024)3).\n5 CONVERGENCE ANALYSIS In this section, we show that Algorithm 1 converges monotonically and thus we can obtain the local optima of Uj | h j=1 and Vj | h j=1. We prove that Algorithm 1 converges by the following theorem. Theorem 1: The value of objective function f of our proposed algorithm promises to increase monotonically until convergence.\nAlgorithm 1: Optimization Algorithm for CRP\nData: X = {X1, X2, ..., Xn}, Xi \u2208 R l1\u00d7l2 Result: Up| h p=1 and Vp| h p=1\n1 for p\u2190 1 to h do 2 Compute the mean Xi of the ith class for each i; 3 Compute the global mean X ; 4 Initialise Vp as I(c,k); 5 repeat 6 Update vp using Eq. (8); 7 Update Dvp using Eq. (9); 8 Update Mvp using Eq. (10); 9 Update Nvp using Eq. (11);\n10 p=the largest eigenvector of (Nvp ) \u22121Mvp ; 11 Update up using Eq. (12); 12 Up = reshape(up, r, k); 13 Update Dup using Eq. (14); 14 Update Mup using Eq. (15); 15 Update Nup using Eq. (16); 16 p=the largest eigenvector of (Nup )\n\u22121Mup ; 17 Update vp using Eq. (17); 18 Vp = reshape(vp, c, k); 19 until Convergence; 20 Update the training data according to Eq. (3) 21 end\nProof: Suppose we have finished the first r-th iteration, we have got U rj | h j=1 and V r j | h j=1. Now we continue the next iteration. For a fixed Vj | h j=1 as Vj\nr|hj=1 we solve the optimization problem for U r+1j | h j=1. For a fixed Vj | h j=1, the objective function in Eq. (5) is converted to the optimization problem in Eq. (6). We can easily tell that it is a convex optimization problem w.r.t Uj| h j=1. Hence, the optimal solution for Uj| h j=1 can be obtained by setting the derivative of Eq.(6) w.r.t Uj | h j=1 to zero respectively. Thus, we have:\nc\u2211\ni=1\nTr(Ur+1p T (Xi \u2212 X)V r p ) 2\n\u2211 c i=1 \u2211\nXj\u2208\u03c0i\nTr(Ur+1p T (Xj \u2212 Xi)V rp ) 2 + \u03bbTr(Ur+1p V rp TV rp U r+1 p T )\n\u2265\nc\u2211\ni=1\nTr(Urp T (Xi \u2212 X)V r p ) 2\nc\u2211\ni=1\n\u2211\nXj\u2208\u03c0i\nTr(Urp T (Xj \u2212 Xi)V rp ) 2 + \u03bbTr(UrpV r p TV rp U r p T )\n(18)\nSimilarly, we can obtain the following inequality for fixed Uj| h j=1 as U r j | h j=1.\n\u2211c i=1 Tr(Urp T (Xi \u2212 X)V r+1 p ) 2\n\u2211 c i=1\nni\u2211\nj=1\nTr(Urp T (xj \u2212 Xi)V r+1 p )2 + \u03bbTr(UrpV r+1 p T V r+1 p Urp T )\n\u2265\nc\u2211\ni=1\nTr(Urp T (Xi \u2212 X)V r p ) 2\nc\u2211\ni=1\n\u2211\nXj\u2208\u03c0i\nTr(Urp T (Xj \u2212 Xi)V rp ) 2 + \u03bbTr(UrpV r p TV rp U r p T )\n(19)\nWe have the following inequality by integrating Eq. (18) and Eq. (19).\nc\u2211\ni=1\nTr(Ur+1p T (Xi \u2212 X)V r+1 p ) 2\nc\u2211\ni=1\n\u2211\nXj\u2208\u03c0i\nTr(Ur+1p T (Xj \u2212 Xi)V r+1 p )2 + \u03bbTr(U r+1 p V r+1 r T V r+1 r U r+1 r T )\n\u2265\nc\u2211\ni=1\nTr(Urp T (Xi \u2212 X)V r p ) 2\n\u2211 c i=1 \u2211\nXj\u2208\u03c0i\nTr(Urp T (Xj \u2212 Xi)V rp ) 2 + \u03bbTr(UrpV r p TV rp U r p T )\n(20)\nFrom Eq. (20) we can see that the objective function value increases monotonically. Theorem 1 has been proved.\n6 EXPERIMENT In this section, we test the proposed CRP algorithm. We compare CRP with seven algorithms, including LDA [5], 2DPCA [32], 2DLDA [9], BilinearSVM [33], two non-iterative 2DLDA algorithms (S2DLDA and P2DLDA) [10] and Tensor LPP [34]. There are five parts in our experiments. We first validate how fast our algorithm converges over the five different datasets. Secondly, we evaluate how the classification performance varies w.r.t different ks. Thirdly, we conduct several different initializations and report the performance variance with different initialization manners. Then, we compare the results of classification in a variety of multimedia analysis, including face recognition, object recognition, facial expression recognition, head pose recognition and handwritten digit recognition. Accuracy is used as the evaluation metric for classification. Finally, comparisons have been also made under a two-class setting, in which gender recognition is performed over three different face datasets. Following [9], [16], [6], we use the gray pixel values of the images as the features. In all of our experiments, we randomly sample 3, 5, 10, and 20 data per class as the training data for all the experiments. The remaining samples are used as testing data. To evaluate the performance with sufficient number of training data, we further use 80% data as training data and the remaining as testing data. The regularization parameter, \u03bb, in the proposed algorithm is tuned in a range of {10\u22126, 10\u22124, . . . , 104, 106} and the best result is reported. We independently repeat the experiments five times and report the results of average accuracy with the stand deviations. Following the work in [5], [35], we project the original data into a (c \u2212 1)2 dimensional subspace for all the compared algorithms. LIBSVM is applied as the implementation of SVM. We learn the optimal regularization parameter of SVM through a tenfold cross-validation.\n6.1 Datasets Description\nUUIm: The UUIm Head Pose and Gaze database [36] is used to evaluate the performance of head pose and\ngaze. This database comprises 2,220 images from ten different people. In our experiment, we resize each image to 24\u00d7 32. In this database, all horizontal head poses are with a vertical orientation of 0 degree. CVL: The CVL dataset [37] is used to evaluate the performance of handwritten digit recognition. There are 21,780 handwritten digit images in this dataset. In our experiment, we resize each image to 32\u00d7 32. Pointing\u201904: The Pointing\u201904 dataset [38] is used for head pose estimation. Pointing\u201904 comprises 2,790 images from 15 people. In our experiment, we resize each image to 40\u00d730. Both the tilt and pan angles are used to determine the head pose. For tilt, there are nine poses. Whereas for pan, there are 13 poses. USPS: We use the USPS database to test the performance of our algorithm on handwritten digit recognition. There are 9,298 handwritten digit images in this database. We resize all the images to 16\u00d7 16. Coil20: Coil20 comprises 1,440 images of 20 objects. In our experiment, we resize each image to 32\u00d7 32. The detailed information of the dataset is summarized in Tab. 1, including the number of samples, the feature dimensions and the total number of classes.\n6.2 Experimental Results\n6.2.1 Convergence Demonstration We first conduct experiments to validate the convergence of our algorithm. Note that our algorithm learns multiple (i.e., k) projection models one by one via the same approach. We therefore randomly select one projection model and plot its objective function values. The convergence demonstration on five different datasets is shown in Fig. 1, where the vertical axis stands for the objective function value and the horizontal axis denotes the number of iterations. From Fig. 1, we observe that our proposed algorithm converges fast on all datasets. In most of the cases, the algorithm converges within ten iterations, demonstrating that the proposed optimization algorithm efficiently converges. For other projection models, we observe similar results.\n6.2.2 Performance Variance w.r.t k In this section, experiments are conducted to study how k affects the performance of the proposed algorithm. UUIm dataset is utilized in this experiment. Three data per class are utilized as training data.\nFig. 2 shows classification accuracy varies when changing different ks. Taking UUIm as an example, we have the following observations: 1) When k is set to 1, the classification accuracy is relatively low, at only 29.6%. 2) When we increase k to 2, the classification accuracy rises to about 32.5%. 3) When k is further increased, the classification accuracy is stable.\nBased on the above observations, we empirically set k to 2 in the remaining experiments, which can decrease computation complexity and obtain decent results as well.\n6.2.3 Performance with Different Initializations\nIn this section, experiments are conducted to evaluate how performance varies with different initializations. We select three samples per class for training and the rest as testing data. We conduct different initializations, including setting all the diagonal elements of V to 0.5, 1, 2 respectively and random values. The experimental results are shown in Tab. 2.\nFrom the experiment results, we can observe that the proposed algorithm can always get good local optima with different initializations.\nIn this experiment, we compare classification performance of our algorithm with other methods, including 2DPCA [32], 2DLDA [9], S2DLDA [10], P2DLDA [10], T-LPP [34], Bilinear SVM [33]. Accuracy is used as an evaluation metric. Note that once we obtain a new lower dimensional representation using CRP, any classification algorithm can be used for data classification. In order to show the discriminant capability of our algorithm, we use two classifiers in this experiment. The first one is SVM, which has been widely used. The second one is 1-Nearest-Neighbor (1NN), which is used in [9], [16] to evaluate the effectiveness of the classical 2DLDA and other related algorithms. Since bilinear SVM is a classifier, we directly compare the classification results obtained from bilinear SVM with other methods. The results of classification performance using SVM are reported from Tab. 3 to Tab. 7 and the results of classification performance using 1NN are presented from Tab. 8 to Tab. 12. When we use SVM as the classifier, we have the following observations.\nCRP outperforms the other seven algorithms, which demonstrates that the utilization of multiple projection models is useful to improve the classification performance. We also observe that when there are insufficient training data, CRP has more advantages over the other compared algorithms. For example, when we use three training data per class for CVL, the classification accuracy of CRP outperforms 2DLDA by 107%, relatively. It further indicates that CRP is more capable of capturing discriminant information when the training samples are quite limited. S2DLDA gets slightly better performance results than 2DLDA, on the whole, demonstrating that selecting a projection model with a higher discriminant capability can improve the classification performance. Nevertheless, CRP still outperforms S2DLDA. With the increase of training samples, the classification results of all the compared algorithms are improved. CRP consistently performs better than the other compared algorithms. Take UUIm for an example. When we increase the number of training data from 3 \u00d7 c to 20 \u00d7 c, the classification accuracy of Bilinear SVM, the second best algorithm, and CRP are improved from 28.2 to 61.2 and from 32.5 to 64.2 respectively. This observation indicates that the proposed algorithm adopts multiple projection models to enable more discriminant lower dimensional embeddings. We can get similar trends when using 1-NN as the classifier, again demonstrating the effectiveness of CRP. To step further, we also evaluate the performance when the training samples are sufficient. 80% samples are utilized for training. The experimental results are reported in Tab. 7 and Tab. 12. From the result, we can observe that when training samples are sufficient, the proposed algorithm still has the best performance with a relative improvement. 6.2.5 Performance Comparison in Two-class Setting In this section, we compare the proposed algorithm with the other related algorithms in a two-class setting. Three face datasets, including UMIST, yaleB, and ORL, are utilized to evaluate the performance of gender recognition, which is a binary class problem. Note that there are only two classes in this case. We project the original data into a 52 dimensional subspace to exploit enough discriminant information for all the compared algorithms empirically. In this experiment, we randomly sample 3 data per class as the training data. The experimental results are shown in Tab. 13. From the experimental results, we can observe that the proposed CRP can perform better than all the compared algorithms in the two-class setting. 7 CONCLUSION It is more natural to represent the real world application data as matrices since we can preserve the spatial correlations while avoiding the curse of dimensionality. In this paper, we propose a novel compound rank-k projection algorithm for bilinear analysis. Our approach directly deal with the matrices. In this way, the spatial correlations can be preserved, and computation complexity can be decreased. Our approach achieves better performance than the classical two-dimensional linear discriminant analysis because our method exploits the multiple projection models and promises a monotonic increase of the objective function value. Hence, the optimum can be obtained. The major novelty of our approach is that multiple projection models are used to provide a larger space in which the local optimal is obtained. Consequently, CRP can get better performances. In machine learning and statistics, ensemble methods use multiple learning algorithms to obtain better\npredictive performance than could be obtained from any of the constituent learning algorithms. In some sense, the proposed algorithm has a close relationship to ensemble learning. Our algorithm differs from ensemble learning in the following two aspects: First, in a typical ensemble learning scenario, the multiple learning algorithms usually adopt different criteria, e.g., to combine the SVM classifier and the least\nregression classifier for classification. Differently, the proposed algorithm is a single algorithm (as opposed multiple algorithms) designed in a single framework, i.e., the Linear Discriminant Analysis framework. Second, in ensemble learning, the output of a single constituent learning algorithm can be directly regarded as the final results. For example, the output of an SVM classifier can be used as the classification result,\neven though the performance could be improved if we combine SVM with least square regression. In contrast, the output of each projection model of our algorithm corresponds to only one dimension of the subspace, and thus we cannot use the output of a single model to represent the original data. We must use all the models to obtain a -dimensional representation. Our experimental results validate that our proposed algorithm outperform the other compared algorithms in terms of classification accuracy using two different classifiers.\nAPPENDIX LEMMAS USED IN THIS PAPER\nLemma 1:\nvec(UV T )(vec(UV T ))T vec(X) = Tr(UTXV )vec(UV T ),\nwhere vec(\u00b7) represents the vectorization of a matrix. Proof: By substituting Lemma 2, we can obtain\nvec(UV T )(vec(UV T ))T = \u2211\ni\nvi \u2297 ui( \u2211\ni\nvi \u2297 ui) T\n= \u2211\ni,j\n(vi \u2297 ui)(vj \u2297 uj) T\nAccording to the properties of Kronecker product, we can obtain\n\u2211\ni,j\n(vi \u2297 ui)(vj \u2297 uj) T =\n\u2211\ni,j\n(viv T j \u2297 uiu T j )\nBy multiplying vec(X), we can get\n\u2211\ni,j\n(viv T j \u2297 uiu T j )vec(X) = vec(\n\u2211\nj\nuTj xvj \u2211\ni\nuiv T i )\n= vec( \u2211\nj\nuTj xvj \u2211\ni\nuiv T i )\n= Tr(UTXV )vec(UV T )\nThe result follows. Lemma 2:\nvec(UV T ) = \u2211\ni\nvi \u2297 ui,\nwhere \u2297 is the Kronecker product. Proof: Referring to Page 26, Chapter 2 of \u201dMatrix Calculus and the Kronecker Product with Applications and C++ Programs\u201d [39].\nLemma 3:\nvec(UV T )T vec(UV T ) = Tr(UTUV TV )\nProof: Substituting Lemma 2, we can obtain\n(vec(UV T ))T vec(UV T ) = ( \u2211\ni\nvi \u2297 ui) T \u2211\ni\n(vi \u2297 ui)\n= \u2211\ni,j\n(vj \u2297 uj) T (vi \u2297 ui)\nFor any two vectors, u and v, we have properties of Kronecker product as follows:\n\u2211\ni,j\n(vj \u2297 uj) T (vi \u2297 ui) =\n\u2211\ni,j\n(vTj vi \u2297 u T j ui)\n= \u2211\ni,j\n(vTj viu T j ui)\nAccording to properties of dot product, we have\n\u2211\ni,j\nuTj ui = \u2211\ni,j\nuTi uj\nHence we can obtain\n\u2211\ni,j\n(vTj viu T i uj) =\n\u2211\ni,j\n(uTi (ujv T j )vi)\n= \u2211\ni\n(uTi \u2211\nj\n(ujv T j )vi)\n= Tr(UTUV TV )\nThe result follows. Lemma 4: Suppose A, B, X are matrices, we have vec(AXB) = (BT \u2297A)vec(X). Proof: As proved in [39], the one vector of order n obeys the relation e = \u2211\nei. Similarly,\n(AXB)k = \u2211\nj\n(BjkA)Xj\n= [B1kA B2kA...BnkA]   X1 X2 ...\nXn\n \n= [BTk \u2297A]vec(X)\nThe result follows. Lemma 5:\nTr(ATBC) = vec(A)T (I \u2297B)vec(C)\nProof: By the trace definition, we have\nTr(ATB) = vec(A)T vec(B)\nAccording to Lemma 4, we can obtain\nvec(BC) = vec(BCI)\n= (I \u2297B)vec(C)\nBy incorporating the above two equations, we can get\nTr(ATBC) = vec(A)T vec(BC)\n= vec(A)T (I \u2297B)vec(C)\nThe result follows. Lemma 6:\nTr(ATBC) = vec(A)T (CT \u2297 I)vec(B)\nProof: Similarly, according to the trace definition, we have\nTr(ATB) = vec(A)T vec(B),\nso\nTr(ATBC) = vec(A)T vec(BC)\n= vec(A)T (CT \u2297 I)vec(B)\n. The result follows.\nREFERENCES [1] J. Ye, R. Janardan, Q. Li, and H. Park, \u201cFeature reduction via\ngeneralized uncorrelated linear discriminant analysis,\u201d IEEE Trans. Knowl. Data Eng., vol. 18, no. 10, pp. 1312\u20131322, 2006. [2] B. Zou, L. Li, Z. Xu, T. Luo, and Y. Y. Tang, \u201cGeneralization performance of fisher linear discriminant based on markov sampling,\u201d IEEE Trans. Neural Netw. Learn. Syst., vol. 24, no. 2, pp. 288\u2013300, 2012. [3] W.-H. Yang, D.-Q. Dai, and H. Yan, \u201cFeature extraction and uncorrelated discriminant analysis for high-dimensional data,\u201d IEEE Trans. Knowl. Data Eng., vol. 20, no. 5, pp. 601\u2013614, 2008. [4] Y. Pang, S. Wang, and Y. Yuan, \u201cLearning regularized lda by clustering,\u201d IEEE Trans. Neural Netw. Learn. Syst., 2014. [5] P. N. Belhumeur, J. P. Hespanha, and D. Kriegman, \u201cEigenfaces vs. fisherfaces: Recognition using class specific linear projection,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 7, pp. 711\u2013720, 1997. [6] Z. Ma, Y. Yang, F. Nie, and N. Sebe, \u201cThinking of images as what they are: Compound matrix regression for image classification,\u201d in IJCAI, 2013. [7] C. Hou, F. Nie, D. Yi, and Y. Wu, \u201cEfficient image classification via multiple rank regression,\u201d IEEE Transactions on Image Processing, vol. 22, no. 1, pp. 340\u2013352, 2013. [8] K. Liu, Y.-Q. Cheng, and J.-Y. Yang, \u201cAlgebraic feature extraction for image recognition based on an optimal discriminant criterion,\u201d Pattern Recog., vol. 26, no. 6, pp. 903\u2013911, 1993. [9] J. Ye, R. Janardan, and Q. Li, \u201cTwo-dimensional linear discriminant analysis,\u201d in NIPS, 2004. [10] K. Inoue and K. Urahama, \u201cNon-iterative two-dimensional linear discriminant analysis,\u201d ICPR, 2006. [11] Z. Fan, Y. Xu, and D. Zhang, \u201cLocal linear discriminant analysis framework using sample neighbors,\u201d IEEE Trans. Neural Netw., vol. 22, no. 7, pp. 1119\u20131132, 2011. [12] F. Dufrenois and J. C. Noyer, \u201cFormulating robust linear regression estimation as a one-class lda criterion: Discriminative hat matrix,\u201d IEEE Trans. Neural Netw. Learn. Syst., vol. 24, no. 2, pp. 262\u2013273, 2012.\n[13] H. Xiong, M. N. S. Swamy, and M. O. Ahmad, \u201cTwodimensional fld for face recognition,\u201d Pattern Recog., vol. 38, no. 7, pp. 1121\u20131124, 2005. [14] Y. Yang, F. Nie, D. Xu, J. Luo, Y. Zhuang, and Y. Pan, \u201cA multimedia retrieval framework based on semi-supervised ranking and relevance feedback,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 4, pp. 723\u2013742, 2012. [15] A. Shashua and A. Levin, \u201cLinear image coding for regression and classification using the tensor-rank principle,\u201d in CVPR, 2001. [16] M. Li and B. Yuan, \u201c2d-lda: A statistical linear discriminant analysis for image matrix,\u201d Pattern Recognit. Letters, vol. 26, no. 5, pp. 527\u2013532, 2005. [17] D. Cai, X. He, and J. Han, \u201cSrda: An efficient algorithm for large-scale discriminant analysis,\u201d IEEE Trans. Kowl. Data Eng., vol. 20, no. 1, pp. 1\u201312, 2008. [18] B. Ni, S. Yan, and A. Kassim, \u201cLearning a propagable graph for semisupervised learning: classification and regression,\u201d IEEE Trans. Knowl. Data Eng., vol. 24, no. 1, pp. 114\u2013126, 2012. [19] Y. Yan, E. Ricci, S. Ramanathan, G. Liu, and N. Sebe, \u201cMultitask linear discriminant analysis for view invariant action recognition,\u201d IEEE Transactions on Image Processing, vol. 42, no. 1, pp. 105\u2013114, 2009. [20] Y. Yan, E. Ricci, G. Liu, R. Subramanian, and N. Sebe, \u201cClustered multi-task linear discriminant analysis for view invariant color-depth action recognition,\u201d in ICPR, 2014. [21] H. Kim, B. L. Drake, and H. Park, \u201cAdaptive nonlinear discriminant analysis by regularized minimum squared errors,\u201d IEEE Trans. Knowl. Data Eng., vol. 18, no. 5, pp. 603\u2013612, 2006. [22] T. V. Bandos, L. Bruzzone, and G. Camps-Valls, \u201cClassification of hyperspectral images with regularized linear discriminant analysis,\u201d IEEE Trans. Geos. Remo. Sens., vol. 47, no. 3, pp. 862\u2013 873, 2009. [23] F. Wu, Y. Han, Q. Tian, and Y. Zhuang, \u201cMulti-label boosting for image annotation by structural grouping sparsity,\u201d in ACM MM, 2010. [24] Y. Han, F. Wu, J. Jia, Y. Zhuang, and B. Yu, \u201cMulti-task sparse discriminant analysis (mtsda) with overlapping categories,\u201d in AAAI, 2010. [25] F. Nie, S. Xiang, Y. Song, and C. Zhang, \u201cExtracting the optimal dimensionality for local tensor discriminant analysis,\u201d Pattern Recognition, vol. 42, no. 1, pp. 105\u2013114, 2009. [26] Y. Hou, L. Song, H.-K. Min, and C. H. Park, \u201cComplexityreduced scheme for feature extraction with linear discriminant analysis,\u201d IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 6, pp. 1003\u20131009, 2012. [27] A. Stuhlsatz, J. Lippel, and T. Zielke, \u201cFeature extraction with deep neural networks by a generalized discriminant analysis,\u201d IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 4, pp. 596\u2013608, 2012. [28] S. Zafeiriou, G. Tzimiropoulos, M. Petrou, and T. Stathaki, \u201cRegularized kernel discriminant analysis with a robust kernel for face recognition and verification,\u201d IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 3, pp. 526\u2013534, 2012. [29] Y. Jia, F. Nie, and C. Zhang, \u201cTrace ratio problem revisited,\u201d IEEE Trans. Neural Netw., vol. 20, no. 4, pp. 729\u2013735, 2009. [30] Y. Yang, D. Xu, F. Nie, S. Yan, and Y. Zhuang, \u201cImage clustering using local discriminant models and global integration,\u201d IEEE Trans. Image Process., vol. 19, no. 10, pp. 2761\u20132773, 2010. [31] F. Nie, S. Xiang, Y. Song, and C. Zhang, \u201cExtracting the optimal dimensionality for local tensor discriminant analysis,\u201d Pattern Recognition, pp. 105\u2013114, 2009. [32] J. Yang, D. Zhang, A. F. Frangi, and J. yu Yang, \u201cTwodimensional pca: a new approach to appearance-based face representation and recognition,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 16, no. 1, pp. 131\u2013137, 2004. [33] H. Pirsiavash, D. Ramanan, and C. Fowlkes, \u201cBilinear classifiers for visual recognition,\u201d in NIPS, 2009. [34] X. He, D. Cai, and P. Niyogi, \u201cTensor subspace analysis,\u201d in NIPS, 2005. [35] C. Hou, F. Nie, C. Zhang, D. Yi, and Y. Wu, \u201cMultiple rank multi-linear SVM for matrix data classification,\u201d Pattern Recognition, vol. 47, no. 1, pp. 454\u2013469, 2014. [36] U. Weidenbacher, G. Layher, P.-M. Strauss, and H. Neumann, \u201cA comprehensive head pose and gaze database,\u201d in Intelligent Environments, 2007.\n[37] M. Diem, S. Fiel, A. Garz, M. Keglevic, F. Kleber, and R. Sablatnig, \u201cIcdar 2013 competition on handwritten digit recognition (hdrc 2013),\u201d in Proc. ICDAR, 2013. [38] N. Gourier, D. Hall, and J. L. Crowley, \u201cEstimating face orientation from robust detection of salient facial features,\u201d in Proc. ICPR Workshop on Visual Observation of Deictic Gestures, 2004. [39] W.-H. Steeb, Matrix Calculus and the Kronecker Product with Applications and C++ Programs. World Scientific, 1997.\nXiaojun Chang is a Ph.D. student at University of Technology Sydney, under the supervision of Dr. Yi Yang. His research interests include machine learning, data mining and computer vision. His publications appear in proceedings of prestigious international conference like ICML, AAAI, IJCAI and etc.\nFeiping Nie received the Ph.D. degree in computer science from Tsinghua University, Beijing, China in 2009. He is currently a Professor with Center for OPTical Imagery Analysis and Learning, Northwestern Polytechnical University, Shaanxi, China. His research interests are machine learning and its applications fields, such as pattern recognition, data mining,computer vision, image processing and information retrieval. He has published more than 100 papers in the pres-\ntigious journals and conferences like TPAMI, TKDE, ICML, NIPS, KDD, and etc. He is now serving as Associate Editor or PC member for several prestigious journals and conferences in the related fields.\nSen Wang received his Ph.D. in School of Information Technology and Electrical Engineering, The University of Queensland, in 2014. He is currently a Australian Research Council Post-doctoral Research Fellow in DKE Group supervised by A. Prof. Xue Li. His research interest includes data mining, pattern recognition, and relevant applications in medical data and social media analysis.\nYi Yang received the Ph.D. degree in computer science from Zhejiang University, Hangzhou, China, in 2010. He is currently a senior lecturer with University of Technology Sydney, Australia. He was a Post-Doctoral Research with the School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA. His current research interest include machine learning and its applications to multimedia content analysis and computer vision, such as multimedia indexing and re-\ntrieval, surveillance video analysis and video semantics understanding.\nXiaofang Zhou received the B.S. and M.S. degrees in computer science from Nanjing University, Nanjing, China, and the Ph.D. degree in computer science from The University of Queensland, Brisbane, QLD, Australia, in 1984, 1987 and 1994, respectively. He is a Professor of Computer Science with the University of Queensland. He is the Head of the Data and Knowledge Engineering Research Division, School of Information Technology and Electrical Engineering. His current re-\nsearch interests include spatial and multimedia databases, high performance query processing, web information systems, data mining, bioinformatics, and e-research.\nChengqi Zhang received the Ph.D. degree in computer science from The University of Queensland, Brisbane, Australia, in 1991, and the DrSc degree from Deakin University, Geelong, Australia, in 2002. He is currently with the University of Technology, Sydney (UTS), Sydney, Australia, where he is a research professor of information technology and the director of the UTS Priority Investment Research Center for Quantum Computation and Intelligent Systems. He has pub-\nlished more than 200 refereed research papers. His main research interests include data mining and its applications. He is a fellow of the Australian Computer Society."}], "references": [{"title": "Feature reduction via generalized uncorrelated linear discriminant analysis", "author": ["J. Ye", "R. Janardan", "Q. Li", "H. Park"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 18, no. 10, pp. 1312\u20131322, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Generalization performance of fisher linear discriminant based on markov sampling", "author": ["B. Zou", "L. Li", "Z. Xu", "T. Luo", "Y.Y. Tang"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 24, no. 2, pp. 288\u2013300, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature extraction and uncorrelated discriminant analysis for high-dimensional data", "author": ["W.-H. Yang", "D.-Q. Dai", "H. Yan"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 20, no. 5, pp. 601\u2013614, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning regularized lda by clustering", "author": ["Y. Pang", "S. Wang", "Y. Yuan"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Eigenfaces vs. fisherfaces: Recognition using class specific linear projection", "author": ["P.N. Belhumeur", "J.P. Hespanha", "D. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 7, pp. 711\u2013720, 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Thinking of images as what they are: Compound matrix regression for image classification", "author": ["Z. Ma", "Y. Yang", "F. Nie", "N. Sebe"], "venue": "IJCAI, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient image classification via multiple rank regression", "author": ["C. Hou", "F. Nie", "D. Yi", "Y. Wu"], "venue": "IEEE Transactions on Image Processing, vol. 22, no. 1, pp. 340\u2013352, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Algebraic feature extraction for image recognition based on an optimal discriminant criterion", "author": ["K. Liu", "Y.-Q. Cheng", "J.-Y. Yang"], "venue": "Pattern Recog., vol. 26, no. 6, pp. 903\u2013911, 1993.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "Two-dimensional linear discriminant analysis", "author": ["J. Ye", "R. Janardan", "Q. Li"], "venue": "NIPS, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Non-iterative two-dimensional linear discriminant analysis", "author": ["K. Inoue", "K. Urahama"], "venue": "ICPR, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Local linear discriminant analysis framework using sample neighbors", "author": ["Z. Fan", "Y. Xu", "D. Zhang"], "venue": "IEEE Trans. Neural Netw., vol. 22, no. 7, pp. 1119\u20131132, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Formulating robust linear regression estimation as a one-class lda criterion: Discriminative hat matrix", "author": ["F. Dufrenois", "J.C. Noyer"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 24, no. 2, pp. 262\u2013273, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Twodimensional fld for face recognition", "author": ["H. Xiong", "M.N.S. Swamy", "M.O. Ahmad"], "venue": "Pattern Recog., vol. 38, no. 7, pp. 1121\u20131124, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "A multimedia retrieval framework based on semi-supervised ranking and relevance feedback", "author": ["Y. Yang", "F. Nie", "D. Xu", "J. Luo", "Y. Zhuang", "Y. Pan"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 4, pp. 723\u2013742, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Linear image coding for regression and classification using the tensor-rank principle", "author": ["A. Shashua", "A. Levin"], "venue": "CVPR, 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "2d-lda: A statistical linear discriminant analysis for image matrix", "author": ["M. Li", "B. Yuan"], "venue": "Pattern Recognit. Letters, vol. 26, no. 5, pp. 527\u2013532, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Srda: An efficient algorithm for large-scale discriminant analysis", "author": ["D. Cai", "X. He", "J. Han"], "venue": "IEEE Trans. Kowl. Data Eng., vol. 20, no. 1, pp. 1\u201312, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning a propagable graph for semisupervised learning: classification and regression", "author": ["B. Ni", "S. Yan", "A. Kassim"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 24, no. 1, pp. 114\u2013126, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Multitask linear discriminant analysis for view invariant action recognition", "author": ["Y. Yan", "E. Ricci", "S. Ramanathan", "G. Liu", "N. Sebe"], "venue": "IEEE Transactions on Image Processing, vol. 42, no. 1, pp. 105\u2013114, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Clustered multi-task linear discriminant analysis for view invariant color-depth action recognition", "author": ["Y. Yan", "E. Ricci", "G. Liu", "R. Subramanian", "N. Sebe"], "venue": "ICPR, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive nonlinear discriminant analysis by regularized minimum squared errors", "author": ["H. Kim", "B.L. Drake", "H. Park"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 18, no. 5, pp. 603\u2013612, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Classification of hyperspectral images with regularized linear discriminant analysis", "author": ["T.V. Bandos", "L. Bruzzone", "G. Camps-Valls"], "venue": "IEEE Trans. Geos. Remo. Sens., vol. 47, no. 3, pp. 862\u2013 873, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-label boosting for image annotation by structural grouping sparsity", "author": ["F. Wu", "Y. Han", "Q. Tian", "Y. Zhuang"], "venue": "ACM MM, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-task sparse discriminant analysis (mtsda) with overlapping categories", "author": ["Y. Han", "F. Wu", "J. Jia", "Y. Zhuang", "B. Yu"], "venue": "AAAI, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Extracting the optimal dimensionality for local tensor discriminant analysis", "author": ["F. Nie", "S. Xiang", "Y. Song", "C. Zhang"], "venue": "Pattern Recognition, vol. 42, no. 1, pp. 105\u2013114, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Complexityreduced scheme for feature extraction with linear discriminant analysis", "author": ["Y. Hou", "L. Song", "H.-K. Min", "C.H. Park"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 6, pp. 1003\u20131009, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature extraction with deep neural networks by a generalized discriminant analysis", "author": ["A. Stuhlsatz", "J. Lippel", "T. Zielke"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 4, pp. 596\u2013608, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularized kernel discriminant analysis with a robust kernel for face recognition and verification", "author": ["S. Zafeiriou", "G. Tzimiropoulos", "M. Petrou", "T. Stathaki"], "venue": "IEEE Trans. Neural Netw. Learn. Syst., vol. 23, no. 3, pp. 526\u2013534, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Trace ratio problem revisited", "author": ["Y. Jia", "F. Nie", "C. Zhang"], "venue": "IEEE Trans. Neural Netw., vol. 20, no. 4, pp. 729\u2013735, 2009.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Y. Yang", "D. Xu", "F. Nie", "S. Yan", "Y. Zhuang"], "venue": "IEEE Trans. Image Process., vol. 19, no. 10, pp. 2761\u20132773, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Extracting the optimal dimensionality for local tensor discriminant analysis", "author": ["F. Nie", "S. Xiang", "Y. Song", "C. Zhang"], "venue": "Pattern Recognition, pp. 105\u2013114, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Twodimensional pca: a new approach to appearance-based face representation and recognition", "author": ["J. Yang", "D. Zhang", "A.F. Frangi", "J. yu Yang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 16, no. 1, pp. 131\u2013137, 2004.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Bilinear classifiers for visual recognition", "author": ["H. Pirsiavash", "D. Ramanan", "C. Fowlkes"], "venue": "NIPS, 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor subspace analysis", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "NIPS, 2005.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiple rank multi-linear SVM for matrix data classification", "author": ["C. Hou", "F. Nie", "C. Zhang", "D. Yi", "Y. Wu"], "venue": "Pattern Recognition, vol. 47, no. 1, pp. 454\u2013469, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "A comprehensive head pose and gaze database", "author": ["U. Weidenbacher", "G. Layher", "P.-M. Strauss", "H. Neumann"], "venue": "Intelligent Environments, 2007.  JOURNAL OF  LTEX CLASS FILES, VOL. X, NO. X, XXXXXXX 20XX  12", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Icdar 2013 competition on handwritten digit recognition (hdrc 2013)", "author": ["M. Diem", "S. Fiel", "A. Garz", "M. Keglevic", "F. Kleber", "R. Sablatnig"], "venue": "Proc. ICDAR, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Estimating face orientation from robust detection of salient facial features", "author": ["N. Gourier", "D. Hall", "J.L. Crowley"], "venue": "Proc. ICPR Workshop on Visual Observation of Deictic Gestures, 2004.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "FLD is commonly utilized in the fields of computer vision and pattern recognition [1], [2], [3], [4].", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "FLD is commonly utilized in the fields of computer vision and pattern recognition [1], [2], [3], [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "FLD is commonly utilized in the fields of computer vision and pattern recognition [1], [2], [3], [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "FLD is commonly utilized in the fields of computer vision and pattern recognition [1], [2], [3], [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "[5] use LDA to represent facial expression images efficiently.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "au representation [6], [7].", "startOffset": 18, "endOffset": 21}, {"referenceID": 6, "context": "au representation [6], [7].", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "[8] propose to use an optimal discriminant criterion to extract algebraic features, calculating a set of optimal discriminant projection vectors according to a generalized Fisher criterion function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9], aims to learn a single set of projection matrices and introduces an iterative algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10], however, have pointed out the iterative algorithm proposed in [9] does not necessarily guarantee the monotonicity of the objective function value, which is mainly caused by the singularity of the between-class scatter matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10], however, have pointed out the iterative algorithm proposed in [9] does not necessarily guarantee the monotonicity of the objective function value, which is mainly caused by the singularity of the between-class scatter matrix.", "startOffset": 68, "endOffset": 71}, {"referenceID": 9, "context": "In [10], they present a simple method, namely Selective Algorithm for 2DLDA, to select transformation matrices with a higher discriminant ability.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Another limitation of all the existing discriminant analysis algorithms [1], [2], [3], [5], [11], [12], [9], [13], [14] is that their performance suffer from the balance between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "Another limitation of all the existing discriminant analysis algorithms [1], [2], [3], [5], [11], [12], [9], [13], [14] is that their performance suffer from the balance between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "Another limitation of all the existing discriminant analysis algorithms [1], [2], [3], [5], [11], [12], [9], [13], [14] is that their performance suffer from the balance between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Another limitation of all the existing discriminant analysis algorithms [1], [2], [3], [5], [11], [12], [9], [13], [14] is that their performance suffer from the balance between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 87, "endOffset": 90}, {"referenceID": 10, "context": "Another limitation of all the existing discriminant analysis algorithms [1], [2], [3], [5], [11], [12], [9], [13], [14] is that their performance suffer from the balance between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "Another limitation of all the existing discriminant analysis algorithms [1], [2], [3], [5], [11], [12], [9], [13], [14] is that their performance suffer from the balance between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 98, "endOffset": 102}, {"referenceID": 8, "context": "Another limitation of all the existing discriminant analysis algorithms [1], [2], [3], [5], [11], [12], [9], [13], [14] is that their performance suffer from the balance between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "Another limitation of all the existing discriminant analysis algorithms [1], [2], [3], [5], [11], [12], [9], [13], [14] is that their performance suffer from the balance between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "Another limitation of all the existing discriminant analysis algorithms [1], [2], [3], [5], [11], [12], [9], [13], [14] is that their performance suffer from the balance between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "To overcome this problem, the authors [15] has proposed multiple rank-1 projection method based on the principal component.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "Different from [9], the convergence of our optimization approach is explicitly guaranteed.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "2) Compared to the classical 2-dimensional linear discriminant analysis methods [9], [13], [16], CRP benefits from the trade-off between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 80, "endOffset": 83}, {"referenceID": 12, "context": "2) Compared to the classical 2-dimensional linear discriminant analysis methods [9], [13], [16], CRP benefits from the trade-off between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "2) Compared to the classical 2-dimensional linear discriminant analysis methods [9], [13], [16], CRP benefits from the trade-off between the degree of freedom and the avoidance of the over-fitting problem.", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "1 Classical LDA The conventional LDA aims to project the original high-dimensional data to a lower dimensional subspace for better classification performance [17], [18], [19], [20].", "startOffset": 158, "endOffset": 162}, {"referenceID": 17, "context": "1 Classical LDA The conventional LDA aims to project the original high-dimensional data to a lower dimensional subspace for better classification performance [17], [18], [19], [20].", "startOffset": 164, "endOffset": 168}, {"referenceID": 18, "context": "1 Classical LDA The conventional LDA aims to project the original high-dimensional data to a lower dimensional subspace for better classification performance [17], [18], [19], [20].", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "1 Classical LDA The conventional LDA aims to project the original high-dimensional data to a lower dimensional subspace for better classification performance [17], [18], [19], [20].", "startOffset": 176, "endOffset": 180}, {"referenceID": 20, "context": "By finding the best transformation matrix W , data points from different classes become more separated while data points from the same class become more compact after the transformation [21], [22], [23], [24], [25].", "startOffset": 186, "endOffset": 190}, {"referenceID": 21, "context": "By finding the best transformation matrix W , data points from different classes become more separated while data points from the same class become more compact after the transformation [21], [22], [23], [24], [25].", "startOffset": 192, "endOffset": 196}, {"referenceID": 22, "context": "By finding the best transformation matrix W , data points from different classes become more separated while data points from the same class become more compact after the transformation [21], [22], [23], [24], [25].", "startOffset": 198, "endOffset": 202}, {"referenceID": 23, "context": "By finding the best transformation matrix W , data points from different classes become more separated while data points from the same class become more compact after the transformation [21], [22], [23], [24], [25].", "startOffset": 204, "endOffset": 208}, {"referenceID": 24, "context": "By finding the best transformation matrix W , data points from different classes become more separated while data points from the same class become more compact after the transformation [21], [22], [23], [24], [25].", "startOffset": 210, "endOffset": 214}, {"referenceID": 25, "context": "In a lower dimensional subspace, the between-class scatter matrix and the within-class scatter matrix are transformed to S\u0303b = W SbW and S\u0303w = W SwW , respectively, according to [26].", "startOffset": 178, "endOffset": 182}, {"referenceID": 26, "context": "This objective function can be solved by eigen-decomposition of (Sw) Sb in [27], [28], [29].", "startOffset": 75, "endOffset": 79}, {"referenceID": 27, "context": "This objective function can be solved by eigen-decomposition of (Sw) Sb in [27], [28], [29].", "startOffset": 81, "endOffset": 85}, {"referenceID": 28, "context": "This objective function can be solved by eigen-decomposition of (Sw) Sb in [27], [28], [29].", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "The goal of 2DLDA is to seek a single set of transformation matrices, U and V , projecting the original data into a lower dimensional subspace [30].", "startOffset": 143, "endOffset": 147}, {"referenceID": 8, "context": "[9] propose an iterative algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] have pointed out that this iterative algorithm cannot guarantee the monotonicity of the objective function value f and it is hard to determine appropriate termination criteria.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "To promise the monotonicity, the authors adopt trace ratio and trace difference in [31].", "startOffset": 83, "endOffset": 87}, {"referenceID": 8, "context": "The time complexity of CRP is O((2 \u00d7 32)), which is significantly small compared to that of the classical LDA [9] O((1024)).", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": "We compare CRP with seven algorithms, including LDA [5], 2DPCA [32], 2DLDA [9], BilinearSVM [33], two non-iterative 2DLDA algorithms (S2DLDA and P2DLDA) [10] and Tensor LPP [34].", "startOffset": 52, "endOffset": 55}, {"referenceID": 31, "context": "We compare CRP with seven algorithms, including LDA [5], 2DPCA [32], 2DLDA [9], BilinearSVM [33], two non-iterative 2DLDA algorithms (S2DLDA and P2DLDA) [10] and Tensor LPP [34].", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "We compare CRP with seven algorithms, including LDA [5], 2DPCA [32], 2DLDA [9], BilinearSVM [33], two non-iterative 2DLDA algorithms (S2DLDA and P2DLDA) [10] and Tensor LPP [34].", "startOffset": 75, "endOffset": 78}, {"referenceID": 32, "context": "We compare CRP with seven algorithms, including LDA [5], 2DPCA [32], 2DLDA [9], BilinearSVM [33], two non-iterative 2DLDA algorithms (S2DLDA and P2DLDA) [10] and Tensor LPP [34].", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "We compare CRP with seven algorithms, including LDA [5], 2DPCA [32], 2DLDA [9], BilinearSVM [33], two non-iterative 2DLDA algorithms (S2DLDA and P2DLDA) [10] and Tensor LPP [34].", "startOffset": 153, "endOffset": 157}, {"referenceID": 33, "context": "We compare CRP with seven algorithms, including LDA [5], 2DPCA [32], 2DLDA [9], BilinearSVM [33], two non-iterative 2DLDA algorithms (S2DLDA and P2DLDA) [10] and Tensor LPP [34].", "startOffset": 173, "endOffset": 177}, {"referenceID": 8, "context": "Following [9], [16], [6], we use the gray pixel values of the images as the features.", "startOffset": 10, "endOffset": 13}, {"referenceID": 15, "context": "Following [9], [16], [6], we use the gray pixel values of the images as the features.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "Following [9], [16], [6], we use the gray pixel values of the images as the features.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "Following the work in [5], [35], we project the original data into a (c \u2212 1) dimensional subspace for all the compared algorithms.", "startOffset": 22, "endOffset": 25}, {"referenceID": 34, "context": "Following the work in [5], [35], we project the original data into a (c \u2212 1) dimensional subspace for all the compared algorithms.", "startOffset": 27, "endOffset": 31}, {"referenceID": 35, "context": "1 Datasets Description UUIm: The UUIm Head Pose and Gaze database [36] is used to evaluate the performance of head pose and", "startOffset": 66, "endOffset": 70}, {"referenceID": 36, "context": "CVL: The CVL dataset [37] is used to evaluate the performance of handwritten digit recognition.", "startOffset": 21, "endOffset": 25}, {"referenceID": 37, "context": "Pointing\u201904: The Pointing\u201904 dataset [38] is used for head pose estimation.", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": "4 Classification Performance Comparison In this experiment, we compare classification performance of our algorithm with other methods, including 2DPCA [32], 2DLDA [9], S2DLDA [10], P2DLDA [10], T-LPP [34], Bilinear SVM [33].", "startOffset": 151, "endOffset": 155}, {"referenceID": 8, "context": "4 Classification Performance Comparison In this experiment, we compare classification performance of our algorithm with other methods, including 2DPCA [32], 2DLDA [9], S2DLDA [10], P2DLDA [10], T-LPP [34], Bilinear SVM [33].", "startOffset": 163, "endOffset": 166}, {"referenceID": 9, "context": "4 Classification Performance Comparison In this experiment, we compare classification performance of our algorithm with other methods, including 2DPCA [32], 2DLDA [9], S2DLDA [10], P2DLDA [10], T-LPP [34], Bilinear SVM [33].", "startOffset": 175, "endOffset": 179}, {"referenceID": 9, "context": "4 Classification Performance Comparison In this experiment, we compare classification performance of our algorithm with other methods, including 2DPCA [32], 2DLDA [9], S2DLDA [10], P2DLDA [10], T-LPP [34], Bilinear SVM [33].", "startOffset": 188, "endOffset": 192}, {"referenceID": 33, "context": "4 Classification Performance Comparison In this experiment, we compare classification performance of our algorithm with other methods, including 2DPCA [32], 2DLDA [9], S2DLDA [10], P2DLDA [10], T-LPP [34], Bilinear SVM [33].", "startOffset": 200, "endOffset": 204}, {"referenceID": 32, "context": "4 Classification Performance Comparison In this experiment, we compare classification performance of our algorithm with other methods, including 2DPCA [32], 2DLDA [9], S2DLDA [10], P2DLDA [10], T-LPP [34], Bilinear SVM [33].", "startOffset": 219, "endOffset": 223}, {"referenceID": 8, "context": "The second one is 1-Nearest-Neighbor (1NN), which is used in [9], [16] to evaluate the effectiveness of the classical 2DLDA and other related algorithms.", "startOffset": 61, "endOffset": 64}, {"referenceID": 15, "context": "The second one is 1-Nearest-Neighbor (1NN), which is used in [9], [16] to evaluate the effectiveness of the classical 2DLDA and other related algorithms.", "startOffset": 66, "endOffset": 70}, {"referenceID": 31, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 16.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 16.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 16.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 16.", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 16.", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 16.", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 36.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 36.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 36.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 36.", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 36.", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 36.", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 88.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 88.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 88.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 88.", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 88.", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 88.", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 24.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 24.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 24.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 24.", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 24.", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 24.", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 43.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 43.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 43.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 43.", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 43.", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 43.", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 89.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 89.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 89.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 89.", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 89.", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP UUIm 89.", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Dataset LDA 2DPCA [32] 2DLDA [9] S2DLDA [10] P2DLDA [10] T-LPP [34] B-SVM [33] CRP", "startOffset": 74, "endOffset": 78}], "year": 2015, "abstractText": "In many real-world applications, data are represented by matrices or high-order tensors. Despite the promising performance, the existing two-dimensional discriminant analysis algorithms employ a single projection model to exploit the discriminant information for projection, making the model less flexible. In this paper, we propose a novel Compound Rank-k Projection (CRP) algorithm for bilinear analysis. CRP deals with matrices directly without transforming them into vectors, and it, therefore, preserves the correlations within the matrix and decreases the computation complexity. Different from the existing twodimensional discriminant analysis algorithms, objective function values of CRP increase monotonically. In addition, CRP utilizes multiple rank-k projection models to enable a larger search space in which the optimal solution can be found. In this way, the discriminant ability is enhanced. We have tested our approach on five datasets, including UUIm, CVL, Pointing\u201904, USPS and Coil20. Experimental results show that the performance of our proposed CRP performs better than other algorithms in terms of classification accuracy.", "creator": "LaTeX with hyperref package"}}}