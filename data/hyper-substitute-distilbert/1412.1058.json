{"id": "1412.1058", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2014", "title": "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks", "abstract": "adaptive neural network ( cnn ) implemented dynamic neural simulator that researchers make use of redundant digital structure of data such cross natural 2d structure using morphological patterns. weak behavior studies cnn on text expressions whereas exploit the 1d structure ( namely, word order ). text presented for accurate prediction. we directly apply cnn to high - length text data, where use less - dimensional xml vectors testing is often done. two paths affect cnn analysis studied : a third treatment employing cnn is plain composite text, and a simple but detailed variation which drives bag - 1 - word conversion in the product transformation. now experiments demonstrate the effectiveness facing his approach in coordination with state - of - the - art methods, as well as previous cnn models against analysis, already presented wildly complex and expensive on train.", "histories": [["v1", "Mon, 1 Dec 2014 16:19:51 GMT  (235kb,D)", "http://arxiv.org/abs/1412.1058v1", null], ["v2", "Thu, 26 Mar 2015 12:59:35 GMT  (270kb,D)", "http://arxiv.org/abs/1412.1058v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["rie johnson", "tong zhang 0001"], "accepted": true, "id": "1412.1058"}, "pdf": {"name": "1412.1058.pdf", "metadata": {"source": "CRF", "title": "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks", "authors": ["Rie Johnson", "Tong Zhang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Text categorization is the task of automatically assigning pre-defined categories to documents written in natural languages. Several types of text categorization have been studied, each of which deals with different types of documents and categories, such as topic categorization to detect discussed topics (e.g., sports, politics), spam detection (Sahami et al., 1998), and sentiment classification (Pang et al., 2002; Pang and Lee, 2008; Maas et al., 2011) to determine the sentiment typically in product or movie reviews. A standard approach to text categorization is to represent documents by bag-of-word vectors, namely, vectors that indicate which words appear in the documents but do not preserve word order, and use classification models such as SVM.\nIt has been noted that loss of word order caused by bag-of-word vectors (bow vectors) is particularly problematic on sentiment classification. A simple remedy is to use word bi-grams in addition to unigrams (Blitzer et al., 2007; Glorot et al., 2011; Wang and Manning, 2012). However, use of word n-grams with n > 1 on text categorization in general is not always effective; e.g., Wang and Manning (2012) report that use of tri-grams on sentiment classification slightly hurt performance; on topic categorization, simply adding phrases or n-grams is not effective (see, e.g., references in (Tan et al., 2002)).\nTo benefit from word order on text categorization, we take a different approach, which employs convolutional neural networks (CNN) (LeCun et al., 1986). CNN is a neural network that can make use of the internal structure of data such as the 2D structure of image data through convolution layers, where each computation unit responds to a small region of input data (e.g., a small square of a large image). We apply CNN to text categorization to make use of the 1D structure (word order) of document data so that each unit in the convolution layer responds to a small region of a document (a sequence of words).\nCNN has been very successful on image classification; see e.g., the winning solutions of ImageNet Large Scale Visual Recognition Challenge (Krizhevsky et al., 2012; Zeiler and Fergus, 2013; Szegedy et al., 2014; Russakovsky et al., 2014).\nOn text, since the work on token-level applications (e.g., POS tagging) by Collobert et al. (2011), CNN has been used in systems for entity search, sentence modeling, word embedding learning, product feature mining, and so on (Gao et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Xu et al., 2014; Tang et al., 2014; Weston et al., 2014; Kim, 2014).\nar X\niv :1\n41 2.\n10 58\nv1 [\ncs .C\nL ]\n1 D\nec 2\n01 4\nNotably, in many of these CNN studies on text, the first layer of the network converts words in sentences to word vectors by table lookup. The word vectors are either trained as part of CNN training, or fixed to those learned by some other method (e.g., word2vec (Mikolov et al., 2013)) from an additional large corpus. The latter is a form of semi-supervised learning, which we study elsewhere. We are interested in the effectiveness of CNN itself without aid of additional resources; therefore, word vectors should be trained as part of network training if word vector lookup is to be done.\nA question arises, however, whether word vector lookup in a purely-supervised setting is really useful for text categorization. The essence of convolution layers is to convert text regions of a fixed size (e.g., \u201cam so happy\u201d with size 3) to feature vectors, as described later. In that sense, a word vector learning layer is a special (and unusual) case of convolution layer with region size one. Why is size one appropriate if bi-grams are more discriminating than uni-grams? Hence, we take a different approach. We directly apply CNN to high-dimensional one hot vectors. This approach is made possible by solving the computational issue1 through efficient handling of high-dimensional sparse data on GPU, and it turned out to have the merits of improving accuracy, simplifying the system (fewer hyper-parameters to tune), and speeding up training/prediction significantly. Speed up is due to the fact that convolution over high-dimensional one-hot vectors is much less expensive than convolution over dense word vectors, when sparse data is handled efficiently. This high-speed CNN code for text will be made publicly available.\nWe study the effectiveness of CNN on text categorization and explain why CNN is suitable for the task. Two types of CNN are tested: seq-CNN is a straightforward adaptation of CNN from image to text, and bow-CNN is a simple but new variation of CNN that employs bag-of-word conversion in the convolution layer. The experiments show that seqCNN outperforms bow-CNN on sentiment classification, vice versa on topic classification, and both\n1 CNN implemented for image would not handle sparse data efficiently, and without efficient handling of sparse data, convolution over high-dimensional one-hot vectors would be computationally infeasible.\noutperform the conventional bag-of-n-gram vectorbased methods, as well as previous CNN models for text, which are more complex and more expensive to train. In particular, to our knowledge, this is the first work that has successfully used word order to significantly improve topic classification performance. Through empirical analysis, we will show that CNN can make more effective use of high-order n-grams than the conventional methods."}, {"heading": "2 CNN for document classification", "text": "We first review CNN applied to image data and then discuss application of CNN to document classification tasks to introduce seq-CNN and bow-CNN."}, {"heading": "2.1 Preliminary: CNN for image", "text": "CNN is a feed-forward neural network with convolution layers interleaved with pooling layers, as illustrated in Figure 1, where the top layer performs classification using the features generated by the layers below. A convolution layer consists of several computation units, each of which takes as input a region vector that represents a small region of the input image and applies a non-linear function to it. Typically, the region vector is a concatenation of pixels in the region, which would be, for example, 75-dimensional if the region is 5\u00d75 and the number of channels is three (red, green, and blue). Concep-\ntually, computation units are placed over the input image so that the entire image is collectively covered, as illustrated in Figure 2. The region stride (distance between the region centers) is often set to a small value such as 1 so that regions overlap with each other, though the stride in Figure 2 is set larger than the region size for illustration.\nA distinguishing feature of convolution layers is weight sharing. Given input x, a unit associated with the `-th region computes \u03c3(W \u00b7 r`(x) + b), where r`(x) is a region vector representing the region of x at location `, and \u03c3 is a predefined component-wise non-linear activation function, (e.g., applying \u03c3(x) = max(x, 0) to each vector component). The matrix of weights W and the vector of biases b are learned through training, and they are shared by the computation units in the same layer. This weight sharing enables learning useful features irrespective of their location, while preserving the location where the useful features appeared.\nWe regard the output of a convolution layer as an \u2018image\u2019 so that the output of each computation unit is considered to be a \u2018pixel\u2019 of m channels where m is the number of weight vectors (i.e., the number of rows of W) or the number of neurons. In other words, a convolution layer converts image regions to m-dim vectors, and the locations of the regions are inherited through this conversion.\nThe output image of the convolution layer is passed to a pooling layer, which essentially shrinks the image by merging neighboring pixels, so that higher layers can deal with more abstract/global information. A pooling layer consists of pooling units, each of which is associated with a small region of the image. Commonly-used merging methods are average-pooling and max-pooling, which respectively compute the channel-wise average/maximum of each region."}, {"heading": "2.2 CNN for text", "text": "Now we consider application of CNN to text data. Suppose that we are given a document D = (w1, w2, . . .) with vocabulary V . CNN requires vector representation of data that preserves internal locations (word order in this case) as input. A straightforward representation would be to treat each word as a pixel, treat D as if it were an image of |D| \u00d7 1 pixels with |V | channels, and to represent each pixel\n(i.e., each word) as a |V |-dimensional one-hot vector2. As a running toy example, suppose that vocabulary V = { \u201cdon\u2019t\u201d, \u201chate\u201d, \u201cI\u201d, \u201cit\u201d, \u201clove\u201d } and we associate the words with dimensions of vector in alphabetical order (as shown), and that document D=\u201cI love it\u201d. Then, we have a document vector:\nx = [ 0 0 1 0 0 | 0 0 0 0 1 | 0 0 0 1 0 ]> ."}, {"heading": "2.2.1 seq-CNN for text", "text": "As in the convolution layer for image, we represent each region (which each computation unit responds to) by a concatenation of the pixels, which makes p|V |-dimensional region vectors where p is the region size fixed in advance. For example, on the example document vector x above, with p = 2 and stride 1, we would have two regions \u201cI love\u201d and \u201clove it\u201d represented by the following vectors:\nr0(x) =  0 0 1 0 0\n\u2014 0 0 0 0 1\n don\u2032t hate I it love don\u2032t hate I it\nlove\nr1(x) =  0 0 0 0 1 \u2014 0 0 0 1 0  don\u2032t hate I it love don\u2032t hate I it love\nThe rest is the same as image, i.e., the text regions are converted to feature vectors. We call a neural network with a convolution layer with this region representation seq-CNN (\u2018seq\u2019 for keeping sequences of words) to distinguish it from bow-CNN described next."}, {"heading": "2.2.2 bow-CNN for text", "text": "A potential problem of seq-CNN however, is that unlike image data with 3 RGB channels, the number of \u2018channels\u2019 |V | (size of vocabulary) may be very large (e.g., 100K), which could make each region vector r`(x) very high-dimensional if the region size p is large. Since the dimensionality of region vectors determines the dimensionality of weight vectors, having high-dimensional region vectors means more parameters to learn. If p|V | is too large, the\n2 Alternatively, one could use bag-of-letter-n-gram vectors as in (Shen et al., 2014; Gao et al., 2014) to cope with out-ofvocabulary words and typos.\nmodel becomes too complex (w.r.t. the amount of training data available) and/or training becomes unaffordably expensive even with efficient handling of sparse data; therefore, one has to lower the dimensionality by lowering the vocabulary size |V | and/or the region size p, which may or may not be desirable, depending on the nature of the task.\nAn alternative we provide is to perform bagof-word conversion to make region vectors |V |- dimensional instead of p|V |-dimensional; e.g., the example region vectors above would be converted to:\nr0(x) =  0 0 1 0 1  don\u2032t hate I it love r1(x) =  0 0 0 1 1  don\u2032t hate I it love\nWith this representation, we have fewer parameters to learn. Essentially, the expressiveness of bow-convolution (which loses word order only within small regions) is somewhere between seqconvolution and bow vectors."}, {"heading": "2.2.3 Pooling for text", "text": "Whereas the size of images is fixed in image applications, documents are naturally variable-sized, and therefore, with a fixed stride, the output of a convolution layer is also variable-sized as shown in Figure 3. Given the variable-sized output of the convolution layer, standard pooling for image (which uses a fixed pooling region size and a fixed stride) would produce variable-sized output, which can be passed to another convolution layer. To produce fixed-sized output, which is required by the fully-connected top layer3, we fix the number of pooling units and dynamically determine the pooling region size on each data point so that the entire data is covered without overlapping.\nIn the previous CNN work on text, pooling is typically max-pooling over the entire data (i.e., one pooling unit associated with the whole text). The dynamic k-max pooling of (Kalchbrenner et al., 2014) for sentence modeling extends it to take the k largest values where k is a function of the sentence length,\n3 In this work, the top layer is fully-connected (i.e., each neuron responds to the entire data) as in CNN for image. Alternatively, the top layer could be convolutional so that it can receive variable-sized input, but such CNN would be more complex.\nbut it is again over the entire data, and the operation is limited to max-pooling. Our pooling differs in that it is a natural extension of standard pooling for image, in which not only max-pooling but other types can be applied. With multiple pooling units associated with different regions, the top layer can receive locational information (e.g., if there are two pooling units, the features from the first half and last half of a document are distinguished). This turned out to be useful (along with average-pooling) on topic classification, as shown later.\n2.3 CNN vs. bag-of-n-grams\nTraditional methods represent each document entirely with one bag-of-n-gram vector and then apply a classifier model such as SVM. However, highorder n-grams are susceptible to the data sparsity problem, and to counteract it, it is necessary to include not only high-order n-grams but also lowerorder n-grams in the vocabulary set; otherwise, performance would be rather degraded. This implies that the discriminating power of high-order n-grams, which is obvious to humans, cannot be fully exploited by the conventional methods based on bag-of-n-gram vectors.\nBy contrast, CNN for text introduced above is more robust in this regard. This is because instead of learning how to weight n-grams, it learns how to weight individual words in the sequence of a fixed size, in order to produce useful features for the intended task. As a result, e.g., a neuron trained to assign a large value to \u201cI love\u201d (and a small value to \u201cI hate\u201d) is likely to assign a large value to \u201cwe love\u201d (and a small value to \u201cwe hate\u201d) as well, even though \u201cwe love\u201d was never seen during training. We will confirm this point empirically in Section 3.5."}, {"heading": "3 Experiments", "text": "We experimented with CNN on two tasks, topic classification and sentiment classification."}, {"heading": "3.1 Experimental framework", "text": "CNN To experiment with CNN, we fixed the activation function to rectifier \u03c3(x) = max(x, 0) and minimized square loss with L2 regularization by stochastic gradient descent (SGD). Our experiments focused on network architectures with one pair of convolution and pooling layers. However, note that it is possible to have more than one convolutionpooling layer and/or to have fully-connected hidden layers above the convolution-pooling layer. We tested several region sizes, pooling types, and the numbers of pooling units. Out-of-vocabulary words (e.g., stopwords) were represented by a zero vector. On bow-CNN, to speed up computation, we used variable region stride so that a larger stride was taken where repetition4 of the same region vectors can be avoided by doing so. Padding size was fixed to p\u2212 1 where p is the region size.\nBaseline methods For comparison, we tested SVM with the linear kernel and fully-connected neural networks (see e.g., Bishop (1995)) with bag-ofn-gram vectors as input. To experiment with fullyconnected neural nets, as in CNN, we minimized square loss with L2 regularization by SGD, and activation was fixed to rectifier. The bag-of-n-gram vectors were generated by first setting each component to log(x + 1) where x is the word frequency in the document and then scaling to unit vectors, which we found always significantly improved performance over raw frequency. We tested three types of bag-of-n-gram: bow1 with n \u2208 {1}, bow2 with n \u2208 {1, 2}, and bow3 with n \u2208 {1, 2, 3}; that is, bow1 is the traditional bow vectors, and with bow3, each component of the vectors corresponds to either uni-gram, bi-gram, or tri-gram of words. To test the fully-connected neural networks, we tried several configurations in terms of the number of hidden layers and the number of weight vectors in each layer and performed model selection.\nModel selection Importantly, for all the methods, the hyper-parameters such as net configurations and regularization parameters were chosen based on the performance on the development data (held-out por-\n4 For example, if we slide a window of size 3 over \u201c* * foo * *\u201d where \u201c*\u201d is out of vocabulary, a bag of \u201cfoo\u201d will be repeated three times with stride fixed to 1.\ntion of the training data), and using the chosen hyper-parameters, the models were re-trained using all the training data.\nImplementation We used SVM-light5 for the SVM experiments. Our CNN code on GPU and detailed information for reproducing the results will be available through the internet."}, {"heading": "3.2 Data, tasks, and data preprocessing", "text": "IMDB: movie reviews The IMDB dataset (Maas et al., 2011) is a benchmark dataset for sentiment classification. The task is to determine if the movie reviews are positive or negative. Both the training and test sets consist of 25K reviews. For preprocessing, we tokenized the text so that emoticons such as \u201c:-)\u201d are treated as tokens and converted all the characters to lower case. We used 30K words (and n-grams for bow2 or bow3) that appeared most frequently in the training set.\nElec: electronics product reviews Elec consists of electronic product reviews. It is part of a large Amazon review dataset (McAuley and Leskovec, 2013). We chose electronics as it seemed to be very different from movies. Following the generation of IMDB (Maas et al., 2011), we chose the training set and the test set so that one half of each set consists of positive reviews and the other half is negative, regarding rating 1 and 2 as positive and 4 and 5 as negative, and that the reviewed products are disjoint between the training set and test set. Note that to extract text from the original data, we only used the text section, and we did not use the summary section. This way, we obtained a test set of 25K reviews (same as IMDB) and training sets of various sizes. The training and test set information will be available through the internet. Data preprocessing was the same as IMDB.\nRCV1: topic categorization RCV1 is a corpus of Reuters news articles as described in LYRL04 (Lewis et al., 2004). RCV1 has 103 topic categories in a hierarchy, and one document may be associated with more than one topic. Performance on this task (multi-label categorization) is known to be sensitive to thresholding strategies, which are algorithms additional to the models we would like to test. There-\n5 http://svmlight.joachims.org/\nfore, we also experimented with single-label categorization to assign one of 55 second-level topics to each document to directly evaluate models. For this task, we used the documents from a one-month period as the test set and generated various sizes of training sets from the documents with earlier dates. Data sizes are shown in Table 1. Data preprocessing was the same as IMDB except that we used the stopword list provided by LYRL04 and regarded numbers as stopwords."}, {"heading": "3.3 Performance results", "text": "Table 2 shows the error rates of CNN in comparison with the baseline methods on all three datasets. Both types of CNN outperform the baseline methods on all the datasets, and seq-CNN outperforms bowCNN on sentiment classification whereas bow-CNN outperforms seq-CNN on topic classification.\nOn sentiment classification (IMDB and Elec), the configuration chosen by model selection (using the development set) was: region size 3, stride 1, 1000 weight vectors, and max-pooling with one pooling unit, for both types of CNN. Note that with a small region size and max-pooling, if a review contains a short phrase that conveys strong sentiment (e.g., \u201cA great movie!\u201d), the review could receive a high score irrespective of the rest of the review. It is sensible that this type of configuration is effective on senti-\nment classification. By contrast, on topic categorization (RCV1), the configuration chosen for bow-CNN by model selection was: region size 20, variable-stride\u22652, averagepooling with 10 pooling units, and 1000 weight vectors, which is very different from sentiment classification. This is presumably because on topic classification, a larger context would be more predictive than short fragments (\u2192 larger region size), the entire document matters (\u2192 the effectiveness of average-pooling), and the location of predictive text also matters (\u2192 multiple pooling units). The last point may be because news documents tend to have crucial sentences at the beginning. On this task, bow-CNN outperforms seq-CNN, which indicates that in this setting the merit of having fewer parameters is larger than the benefit of keeping word order in each region.\nComparing the baseline methods with each other, on sentiment classification, error rates were significantly reduced by addition of bi-grams but further adding tri-grams did not improve performance much. On topic categorization, bi-grams only slightly improved accuracy. These are consistent with the previous studies.\nComparison with state-of-the-art results The previous best supervised single-classifier result6 on IMDB is 10.77 achieved by word representation Restricted Boltzmann Machine (WRRBM) combined with bow vectors (Dahl et al., 2012), as shown in\n6 We exclude semi-supervised learning results (Le and Mikolov, 2014) and classifier combination results (Wang and Manning, 2012) as they are not directly comparable with our results.\nTable 3. Our CNN results outperform WRRBM by a relatively large margin.\nWe tested bow-CNN on the multi-label topic categorization task on RCV1 to compare with LYRL04. We used the same thresholding strategy as LYRL04. As shown in Table 4, bow-CNN outperforms LYRL04\u2019s best results even though our data preprocessing is much simpler (no stemming and no tf-idf weighting).\nPrevious CNN We focus on the sentence classification studies due to its relation to text categorization. Kim (2014) studied fine-tuning of pretrained word vectors to produce input to one-layer CNN with one-unit max-pooling. He reported that performance was poor when word vectors were trained as part of CNN training (i.e., no additional method/corpus). On our tasks, this type of model also underperformed the baseline while training was 3\u20135 times slower (using our code7) than our models. As mentioned earlier, the word vector learning layer in this setting can be viewed as a special case of convolution layer with region size one, and region size one is apparently not suitable for these tasks.\nKalchbrenner et al. (2014) proposed complex modifications of CNN for sentence modeling. Notably, given word vectors \u2208 Rd, their convolution with m feature maps produces for each region a matrix \u2208 Rd\u00d7m (instead of a vector \u2208 Rm as in standard CNN). Using the provided code, we found that their model is too resource-demanding for our tasks. On IMDB and Elec8 the best error rates we obtained by training with various configurations that fit in memory for 24 hours each on GPU (cf. Fig 5) were 10.13 and 9.37, respectively, which are only as good as SVM bow2. Since excellent performances were reported on short sentence classification, we presume that their model is optimized for short sentences, but not for text categorization in general.\nThus, on text categorization, the CNN models we propose have an advantage of higher accuracy, simplicity, and faster training.\n7 K14\u2019s code did not scale to our tasks. 8 We could not train adequate models on RCV1 on either\nTesla K20 or M2070 due to memory shortage."}, {"heading": "3.4 Performance dependency analysis", "text": "The results with training sets of various sizes are shown in Figure 4 (a) (Elec) and (b) (RCV1). On both, CNN consistently outperforms the baseline methods. The performance gains of CNN in comparison with the baseline methods tend to be larger when the size of training data is larger.\nFigure 4 (c) plots performance dependency on the number of weight vectors (or neurons) in the convolution layer on RCV1. The results indicate that it is important to have sufficient number of weight vectors. Since the task is 55-way classification, having fewer neurons than 55 degrades performance.\nFigure 5 shows error rates in relation to the time spent for training on Tesla K20. Error rates become better than the best-performing baseline within 3\u201315 minutes and reach nearly the best in 20\u201350 minutes."}, {"heading": "3.5 Why is CNN effective?", "text": "In this section we explain the effectiveness of CNN through looking into what it learns from training.\nFirst, for comparison, we show the n-grams that SVM with bow3 found to be the most predictive; i.e., the following n-grams were assigned the 10 largest weights by SVM on Elec (#train=25K), for the negative and positive class, respectively:\n\u2022 useless, poor, returned, worse, return, not worth, disappointing, horrible, terrible, disappointed \u2022 excellent, great, amazing, perfect, awesome, love,\nno problems, easy, perfectly, my only\nNote that, even though SVM was also given bi- and tri-grams, the top 10 features chosen by SVM are mostly uni-grams; furthermore, the top 100 features (50 for each class) include 26 bi-grams but only four tri-grams. This means that, with the given size of training data, SVM still heavily counts on unigrams, which could be ambiguous, and cannot fully take advantage of higher-order n-grams.\nIn Table 5, we show some of text regions learned by seq-CNN to be predictive on Elec. The net configuration is the one from Table 2, which has 1000 neurons (weight vectors) in the convolution layer. Recall that the output/activation of the 1000 neurons (after pooling) serve as features in the top layer, and the top layer assigns weights to the features. In the table, Ni/Pi indicates the neuron whose output received the i-th highest weight in the top layer for the negative/positive class, respectively. The table shows the text regions that appear in the training set and highly activate the corresponding neurons. Note that, e.g., \u201cis so bad\u201d cannot be shorter for detection of the negative sentiment since \u201cso bad\u201d could be a part of \u201cnot so bad\u201d; thus, tri-gram is indeed helpful for accurate prediction.\nAs is mentioned in Section 2.3, the methods that rely on bag of high-order n-grams tend to suffer from data sparsity. This is because with conventional methods, only the n-grams that appear in the training data can participate in prediction, and the vocabulary overlap between training data and test data rapidly decreases as n increases. By contrast, one strength of CNN is that n-grams (or text regions\nof size n) can contribute to accurate prediction even if they did not appear in the training data, as long as (some of) their constituent words did. This is because vector representation of each text region is based on their constituent words (see Section 2.2.1).\nTo see this point, in Table 6 we show the text regions from the test set, which did not appear in the training data, either entirely or partially as bi-grams, and yet highly activate heavily-weighted (predictive) neurons thus contributing to the prediction. There are many more of these, and we only show a small part of them that fit certain patterns. One noticeable pattern is (be-verb, adverb, sentiment adjective) such as \u201cam entirely satisfied\u201d and \u201c\u2019m overall impressed\u201d. These adjectives alone could be ambiguous as they may be negated. To know that the writer is indeed \u201csatisfied\u201d, we need to see the sequence \u201cam satisfied\u201d, but the insertion of adverb such as \u201centirely\u201d is very common. \u201cbest X ever\u2019 is another pattern that a discriminating pair of words are not adjacent to each other. These patterns require trigrams for disambiguation, and seq-CNN successfully makes use of them even though the exact tri-\ngrams were not seen during training, as a result of learning, e.g., \u201cam X satisfied\u201d with non-negative X (e.g., \u201cam very satisfied\u201d, \u201cam so satisfied\u201d) to be predictive of the positive class through training. That is, CNN can effectively use word order even when bag-of-n-gram-based approaches fail."}], "references": [{"title": "Neural networks for pattern recognition", "author": ["Christopher Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q1995\\E", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "Biographies, bollywood, boomboxes, and blenders: Domain adaptation for sentiment classification", "author": ["Blitzer et al.2007] John Blitzer", "Mark Dredze", "Fernando Pereira"], "venue": "In Proceedings of ACL", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Training restricted boltzmann machines on word observations", "author": ["Dahl et al.2012] George E. Dahl", "Ryan P. Adams", "Hugo Larochelle"], "venue": "In Proceedings of ICML", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Modeling interestingness with deep neural networks", "author": ["Gao et al.2014] Jianfeng Gao", "Patric Pantel", "Michael Gamon", "Xiaodong He", "Li dent"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of ICML", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "A convolutional neural network for modeling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of ACL,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Proceedings of NIPS", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of ICML", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al.1986] Yann LeCun", "Le\u00f3n Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1986\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1986}, {"title": "RCV1: A new benchmark collection for text categorization research", "author": ["Lewis et al.2004] David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": "Journal of Marchine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas et al.2011] Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "Proceedings of ACL", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Hidden factors and hidden topics: Understanding rating dimensions with review text. In RecSys", "author": ["McAuley", "Leskovec2013] Julian McAuley", "Jure Leskovec"], "venue": null, "citeRegEx": "McAuley et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Proceedings of NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Opinion mining and sentiment analysis", "author": ["Pang", "Lee2008] Bo Pang", "Lillian Lee"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "Thumbs up? sentiment classification using machine learning techniques", "author": ["Pang et al.2002] Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "ImageNet Large Scale Visual Recognition Chal", "author": ["Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "A bayesian approach to filtering junk e-mail", "author": ["Sahami et al.1998] Mehran Sahami", "Susan Dumais", "David Heckerman", "Eric Horvitz"], "venue": "In Proceedings of AAAI\u201998 Workshop on Learning for Text Categorization", "citeRegEx": "Sahami et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sahami et al\\.", "year": 1998}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Shen et al.2014] Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mensnil"], "venue": "Proceedings of CIKM", "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "Technical Report arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "The use of bigrams to enhance text categorization", "author": ["Tan et al.2002] Chade-Meng Tan", "Yuan-Fang Wang", "Chan-Do Lee"], "venue": "Information Processing and Management,", "citeRegEx": "Tan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2002}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["Tang et al.2014] Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin"], "venue": "In Proceedings of ACL,", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Wang", "Manning2012] Sida Wang", "Christopher D. Manning"], "venue": "In Proceedings of ACL (short paper)", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "tagspace: Semantic embeddings from hashtags", "author": ["Weston et al.2014] Jason Weston", "Sumit Chopra", "Keith Adams"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Product feature mining: Semantic clues versus syntactic constituents", "author": ["Xu et al.2014] Liheng Xu", "Kang Liu", "Siwei Lai", "Jun Zhao"], "venue": "In Proceedings of ACL,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Fergus2013] Matthew D. Zeiler", "Rob Fergus"], "venue": "Technical Report arXiv:1311.2901", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": ", sports, politics), spam detection (Sahami et al., 1998), and sentiment classification (Pang et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 16, "context": ", 1998), and sentiment classification (Pang et al., 2002; Pang and Lee, 2008; Maas et al., 2011) to determine the sentiment typically in product or movie reviews.", "startOffset": 38, "endOffset": 96}, {"referenceID": 12, "context": ", 1998), and sentiment classification (Pang et al., 2002; Pang and Lee, 2008; Maas et al., 2011) to determine the sentiment typically in product or movie reviews.", "startOffset": 38, "endOffset": 96}, {"referenceID": 1, "context": "A simple remedy is to use word bi-grams in addition to unigrams (Blitzer et al., 2007; Glorot et al., 2011; Wang and Manning, 2012).", "startOffset": 64, "endOffset": 131}, {"referenceID": 5, "context": "A simple remedy is to use word bi-grams in addition to unigrams (Blitzer et al., 2007; Glorot et al., 2011; Wang and Manning, 2012).", "startOffset": 64, "endOffset": 131}, {"referenceID": 21, "context": ", references in (Tan et al., 2002)).", "startOffset": 16, "endOffset": 34}, {"referenceID": 10, "context": "To benefit from word order on text categorization, we take a different approach, which employs convolutional neural networks (CNN) (LeCun et al., 1986).", "startOffset": 131, "endOffset": 151}, {"referenceID": 8, "context": ", the winning solutions of ImageNet Large Scale Visual Recognition Challenge (Krizhevsky et al., 2012; Zeiler and Fergus, 2013; Szegedy et al., 2014; Russakovsky et al., 2014).", "startOffset": 77, "endOffset": 175}, {"referenceID": 20, "context": ", the winning solutions of ImageNet Large Scale Visual Recognition Challenge (Krizhevsky et al., 2012; Zeiler and Fergus, 2013; Szegedy et al., 2014; Russakovsky et al., 2014).", "startOffset": 77, "endOffset": 175}, {"referenceID": 17, "context": ", the winning solutions of ImageNet Large Scale Visual Recognition Challenge (Krizhevsky et al., 2012; Zeiler and Fergus, 2013; Szegedy et al., 2014; Russakovsky et al., 2014).", "startOffset": 77, "endOffset": 175}, {"referenceID": 4, "context": "(2011), CNN has been used in systems for entity search, sentence modeling, word embedding learning, product feature mining, and so on (Gao et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Xu et al., 2014; Tang et al., 2014; Weston et al., 2014; Kim, 2014).", "startOffset": 134, "endOffset": 266}, {"referenceID": 19, "context": "(2011), CNN has been used in systems for entity search, sentence modeling, word embedding learning, product feature mining, and so on (Gao et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Xu et al., 2014; Tang et al., 2014; Weston et al., 2014; Kim, 2014).", "startOffset": 134, "endOffset": 266}, {"referenceID": 6, "context": "(2011), CNN has been used in systems for entity search, sentence modeling, word embedding learning, product feature mining, and so on (Gao et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Xu et al., 2014; Tang et al., 2014; Weston et al., 2014; Kim, 2014).", "startOffset": 134, "endOffset": 266}, {"referenceID": 25, "context": "(2011), CNN has been used in systems for entity search, sentence modeling, word embedding learning, product feature mining, and so on (Gao et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Xu et al., 2014; Tang et al., 2014; Weston et al., 2014; Kim, 2014).", "startOffset": 134, "endOffset": 266}, {"referenceID": 22, "context": "(2011), CNN has been used in systems for entity search, sentence modeling, word embedding learning, product feature mining, and so on (Gao et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Xu et al., 2014; Tang et al., 2014; Weston et al., 2014; Kim, 2014).", "startOffset": 134, "endOffset": 266}, {"referenceID": 24, "context": "(2011), CNN has been used in systems for entity search, sentence modeling, word embedding learning, product feature mining, and so on (Gao et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Xu et al., 2014; Tang et al., 2014; Weston et al., 2014; Kim, 2014).", "startOffset": 134, "endOffset": 266}, {"referenceID": 7, "context": "(2011), CNN has been used in systems for entity search, sentence modeling, word embedding learning, product feature mining, and so on (Gao et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Xu et al., 2014; Tang et al., 2014; Weston et al., 2014; Kim, 2014).", "startOffset": 134, "endOffset": 266}, {"referenceID": 1, "context": "A simple remedy is to use word bi-grams in addition to unigrams (Blitzer et al., 2007; Glorot et al., 2011; Wang and Manning, 2012). However, use of word n-grams with n > 1 on text categorization in general is not always effective; e.g., Wang and Manning (2012) report that use of tri-grams on sentiment classification slightly hurt performance; on topic categorization, simply adding phrases or n-grams is not effective (see, e.", "startOffset": 65, "endOffset": 262}, {"referenceID": 1, "context": "A simple remedy is to use word bi-grams in addition to unigrams (Blitzer et al., 2007; Glorot et al., 2011; Wang and Manning, 2012). However, use of word n-grams with n > 1 on text categorization in general is not always effective; e.g., Wang and Manning (2012) report that use of tri-grams on sentiment classification slightly hurt performance; on topic categorization, simply adding phrases or n-grams is not effective (see, e.g., references in (Tan et al., 2002)). To benefit from word order on text categorization, we take a different approach, which employs convolutional neural networks (CNN) (LeCun et al., 1986). CNN is a neural network that can make use of the internal structure of data such as the 2D structure of image data through convolution layers, where each computation unit responds to a small region of input data (e.g., a small square of a large image). We apply CNN to text categorization to make use of the 1D structure (word order) of document data so that each unit in the convolution layer responds to a small region of a document (a sequence of words). CNN has been very successful on image classification; see e.g., the winning solutions of ImageNet Large Scale Visual Recognition Challenge (Krizhevsky et al., 2012; Zeiler and Fergus, 2013; Szegedy et al., 2014; Russakovsky et al., 2014). On text, since the work on token-level applications (e.g., POS tagging) by Collobert et al. (2011), CNN has been used in systems for entity search, sentence modeling, word embedding learning, product feature mining, and so on (Gao et al.", "startOffset": 65, "endOffset": 1417}, {"referenceID": 14, "context": ", word2vec (Mikolov et al., 2013)) from an additional large corpus.", "startOffset": 11, "endOffset": 33}, {"referenceID": 19, "context": "2 Alternatively, one could use bag-of-letter-n-gram vectors as in (Shen et al., 2014; Gao et al., 2014) to cope with out-ofvocabulary words and typos.", "startOffset": 66, "endOffset": 103}, {"referenceID": 4, "context": "2 Alternatively, one could use bag-of-letter-n-gram vectors as in (Shen et al., 2014; Gao et al., 2014) to cope with out-ofvocabulary words and typos.", "startOffset": 66, "endOffset": 103}, {"referenceID": 6, "context": "The dynamic k-max pooling of (Kalchbrenner et al., 2014) for sentence modeling extends it to take the k largest values where k is a function of the sentence length,", "startOffset": 29, "endOffset": 56}, {"referenceID": 0, "context": ", Bishop (1995)) with bag-ofn-gram vectors as input.", "startOffset": 2, "endOffset": 16}, {"referenceID": 12, "context": "IMDB: movie reviews The IMDB dataset (Maas et al., 2011) is a benchmark dataset for sentiment classification.", "startOffset": 37, "endOffset": 56}, {"referenceID": 12, "context": "Following the generation of IMDB (Maas et al., 2011), we chose the training set and the test set so that one half of each set consists of positive reviews and the other half is negative, regarding rating 1 and 2 as positive and 4 and 5 as negative, and that the reviewed products are disjoint between the training set and test set.", "startOffset": 33, "endOffset": 52}, {"referenceID": 11, "context": "RCV1: topic categorization RCV1 is a corpus of Reuters news articles as described in LYRL04 (Lewis et al., 2004).", "startOffset": 92, "endOffset": 112}, {"referenceID": 3, "context": "77 achieved by word representation Restricted Boltzmann Machine (WRRBM) combined with bow vectors (Dahl et al., 2012), as shown in", "startOffset": 98, "endOffset": 117}, {"referenceID": 7, "context": "Kim (2014) studied fine-tuning of pretrained word vectors to produce input to one-layer CNN with one-unit max-pooling.", "startOffset": 0, "endOffset": 11}], "year": 2014, "abstractText": "Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. We directly apply CNN to high-dimensional text data, instead of lowdimensional word vectors as is often done. Two types of CNN are studied: a straightforward adaptation of CNN from image to text, and a simple but new variation which employs bag-of-word conversion in the convolution layer. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods, as well as previous CNN models for text, which are more complex and expensive to train.", "creator": "LaTeX with hyperref package"}}}