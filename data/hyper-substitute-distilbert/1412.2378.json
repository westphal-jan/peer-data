{"id": "1412.2378", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2014", "title": "Learning Word Representations from Relational Graphs", "abstract": "representation of words making signals between two words is central when numerous tasks addressing artificial intelligence such as knowledge representation, similarity formation, and analogy detection. often means two text share one semantic subsequent attributes in isolation, they are connected by closer semantic relations. on the two way, if there arose analogous semantic relations between two words, we inevitably expect some of what examples of one of if words together be inherited but the other. motivated by this close connection between attributes \u2019 relations, given a relational graph in which words are custom - copied via simpler semantic aspects, advocates propose a method shall learn a speech representation for the chosen traits. per proposed method considers not only the always - existing communication pairs as done under visual approaches under meaningful resource learning, albeit also the frequency matching in which two words co - occur. to evaluate the accuracy of the word representations found across the proposed method, designers analyze the learnt word definitions / solve semantic word analogy learning. numerous experimental results show that it is advised to learn better word meanings based choosing semantic semantics between words.", "histories": [["v1", "Sun, 7 Dec 2014 17:49:53 GMT  (211kb,D)", "http://arxiv.org/abs/1412.2378v1", "AAAI 2015"]], "COMMENTS": "AAAI 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["danushka bollegala", "takanori maehara", "yuichi yoshida", "ken-ichi kawarabayashi"], "accepted": true, "id": "1412.2378"}, "pdf": {"name": "1412.2378.pdf", "metadata": {"source": "CRF", "title": "Learning Word Representations from Relational Graphs", "authors": ["Danushka Bollegala", "Takanori Maehara", "Yuichi Yoshida", "Ken-ichi Kawarabayashi"], "emails": [], "sections": [{"heading": "Introduction", "text": "The notions of attributes and relations are central to Artificial Intelligence. In Knowledge Representation (KR) (Brachman and Levesque 2004), a concept is described using its attributes and the relations it has with other concepts in a domain. If we already know a particular concept such as pets, we can describe a new concept such as dogs by stating the semantic relations that the new concept shares with the existing concepts such as dogs belongs-to pets. Alternatively, we could describe a novel concept by listing all the attributes it shares with existing concepts. In our example, we can describe the concept dog by listing attributes such as mammal, carnivorous, and domestic animal that it shares with another concept such as the cat. Therefore, both attributes and relations can be considered as alternative descriptors of the same knowledge. This close connection between attributes and relations can be seen in knowledge representation schemes such as predicate logic, where attributes\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nare modelled by predicates with a single argument whereas relations are modelled by predicates with two or more arguments.\nLearning representations of words is an important task with numerous applications (Bengio et al. 2013). Better representations of words can improve the performance in numerous natural language processing tasks that require word representations such as language modelling (Collobert et al. 2011; Bengio et al. 2003), part-of-speech tagging (Zheng et al. 2013), sentiment classification (Socher et al. 2011b), and dependency parsing (Socher et al. 2013a; Socher et al. 2011a). For example, to classify a novel word into a set of existing categories one can measure the cosine similarity between the words in each category and the novel word to be classified. Next, the novel word can be assigned to the category of words that is most similar to (Huang et al. 2012). However, existing methods for learning word representations only consider the co-occurrences of two words within a short window of context, and ignore any semantic relations that exist between the two words.\nConsidering the close connection between attributes and relations, an obvious question is, can we learn better word representations by considering the semantic relations that exist among words? More importantly whether word representations learnt by considering the semantic relations among words could outperform methods that focus only on the co-occurrences of two words, ignoring the semantic relations. We study these problems in this paper and arrive at the conclusion that it is indeed possible to learn better word representations by considering the semantic relations between words. Specifically, given as input a relational graph, a directed labelled weighted graph where vertices represent words and edges represent numerous semantic relations that exist between the corresponding words, we consider the problem of learning a vector representation for each vertex (word) in the graph and a matrix representation for each label type (pattern). The learnt word representations are evaluated for their accuracy by using them to solve semantic word analogy questions on a benchmark dataset.\nOur task of learning word attributes using relations between words is challenging because of several reasons. First, there can be multiple semantic relations between two words. For example, consider the two words ostrich and bird. An ostrich is-a-large bird as well as is-a-flightless bird. In this\nar X\niv :1\n41 2.\n23 78\nv1 [\ncs .C\nL ]\n7 D\nec 2\n01 4\nregard, the relation between the two words ostrich and bird is similar to the relation between lion and cat (lion is a large cat) as well as to the relation between penguin and bird (penguin is a flightless bird). Second, a single semantic relation can be expressed using multiple lexical patterns. For example, the two lexical patterns X is a large Y and large Ys such as X represent the same semantic relation is-a-large. Beside lexical patterns, there are other representations of semantic relations between words such as POS patterns and dependency patterns. An attribute learning method that operates on semantic relations must be able to handle such complexities inherent in semantic relation representations. Third, the three-way co-occurrences between two words and a pattern describing a semantic relation is much sparser than the co-occurrences between two words (ignoring the semantic relations) or occurrences of individual words. This is particularly challenging for existing methods of representation learning where one must observe a sufficiently large number of co-occurrences to learn an accurate representation for words.\nGiven a relational graph, a directed labeled weighted graph describing various semantic relations that exist between words denoted by the vertices of the graph, we propose an unsupervised method to factorise the graph and assign latent attributional vectors x(u) for each vertex (word) u and a matrix G(l) to each label (pattern) l in the graph. Then, the co-occurrences between two words u and v in a pattern l that expresses some semantic relation is modelled as the scalar product x(u)>G(l)x(v). A regression model that minimises the squared loss between the predicted cooccurrences according to the proposed method and the actual co-occurrences in the corpus is learnt. In the relational graphs we construct, the edge between two vertices is labeled using the patterns that co-occur with the corresponding words, and the weight associated with an edge represents the strength of co-occurrence between the two words under the pattern.\nOur proposed method does not assume any particular pattern extraction method or a co-occurrence weighting measure. Consequently, the proposed method can be applied to a wide-range of relational graphs, both manually created ones such as ontologies, as well as automatically extracted ones from unstructured texts. For concreteness of the presentation, we consider relational graphs where the semantic relations between words are represented using lexical patterns, part-of-speech (POS) patterns, or dependency patterns. Moreover, by adjusting the dimensionality of the decomposition, it is possible to obtain word representations at different granularities. Our experimental results show that the proposed method obtain robust performances over a wide-range of relational graphs constructed using different pattern types and co-occurrence weighting measures. It learns compact and dense word representations with as low as 200 dimensions. Unlike most existing methods for word representation learning, our proposed method considers the semantic relations that exist between two words in their cooccurring contexts. To evaluate the proposed method, we use the learnt word representations to solve semantic analogy problems in a benchmark dataset (Mikolov et al. 2013a)."}, {"heading": "Related Work", "text": "Representing the semantics of a word is a fundamental step in many NLP tasks. Given word-level representations, numerous methods have been proposed in compositional semantics to construct phrase-level, sentence-level, or document-level representations (Grefenstette 2013; Socher et al. 2012). Existing methods for creating word representations can be broadly categorised into two groups: countingbased methods, and prediction-based methods.\nCounting-based approaches follow the distributional hypothesis (Firth 1957) which says that the meaning of a word can be represented using the co-occurrences it has with other words. By aggregating the words that occur within a pre-defined window of context surrounding all instances of a word from a corpus and by appropriately weighting the co-occurrences, it is possible to represent the semantics of a word. Numerous definitions of co-occurrence such as within a proximity window or involved in a particular dependency relation etc. and co-occurrence measures have been proposed in the literature (Baroni and Lenci 2010). This counting-based bottom-up approach often results in sparse word representations. Dimensionality reduction techniques such as the singular value decomposition (SVD) have been employed to overcome this problem in tasks such as measuring similarity between words using the learnt word representations (Turney and Pantel 2010).\nPrediction-based approaches for learning word representations model the problem of representation learning as a prediction problem where the objective is to predict the presence (or absence) of a particular word in the context of another word. Each word w is assigned a feature vector vw of fixed dimensionality such that the accuracy of predictions of other words made using vw is maximised. Different objective functions for measuring the prediction accuracy such as perplexity or classification accuracy, and different optimisation methods have been proposed. For example, Neural Network Language Model (NLMM) (Bengio et al. 2003) learns word representations to minimise perplexity in a corpus. The Continuous Bag-Of-Words model (CBOW) (Mikolov et al. 2013b) uses the representations of all the words c in the context of w to predict the existence of w, whereas the skip-gram model (Mikolov et al. 2013c; Mikolov et al. 2013a) learns the representation of a word w by predicting the words c that appear in the surrounding context of w. Noise contrastive estimation has been used to speed-up the training of word occurrence probability models to learn word representations (Mnih and Kavukcuoglu 2013).\nGiven two words u and v represented respectively by vectors x(u) and x(v) of equal dimensions, GloVe (Pennington et al. 2014) learns a linear regression model to minimise the squared loss between the inner-product x(u)>x(v) and the logarithm of the co-occurrence frequency of u and v. They show that this minimisation problem results in vector spaces that demonstrate linear relationships observed in word analogy questions. However, unlike our method, GloVe does not consider the semantic relations that exist between two words when they co-occur in a corpus. In particular, GloVe can be seen as a special case of our proposed method when\nwe replace all patterns by a single pattern that indicates cooccurrence, ignoring the semantic relations.\nOur work in this paper can be categorised as a predictionbased method for word representation learning. However, prior work in prediction-based word representation learning have been limited to considering the co-occurrences between two words, ignoring any semantic relations that exist between the two words in their co-occurring context. On the other hand, prior studies on counting-based approaches show that specific co-occurrences denoted by dependency relations are particularly useful for creating semantic representations for words (Baroni and Lenci 2010). Interestingly, prediction-based approaches have shown to outperform counting-based approaches in comparable settings (Baroni et al. 2014). Therefore, it is natural for us to consider the incorporation of semantic relations between words into prediction-based word representation learning. However, as explained in the previous section, three-way co-occurrences of two words and semantic relations expressed by contextual patterns are problematic due to data sparseness. Therefore, it is non-trivial to extend the existing prediction-based word representation methods to three-way co-occurrences.\nMethods that use matrices to represent adjectives (Baroni and Zamparelli 2010) or relations (Socher et al. 2013b) have been proposed. However, high dimensional representations are often difficult to learn because of their computational cost (Paperno et al. 2014). Although we learn matrix representations for patterns as a byproduct, our final goal is the vector representations for words. An interesting future research direction would be to investigate the possibilities of using the learnt matrix representations for related tasks such as relation clustering (Duc et al. 2010)."}, {"heading": "Learning Word Representations from Relational Graphs", "text": ""}, {"heading": "Relational Graphs", "text": "We define a relational graph G(V, E) as a directed labelled weighted graph where the set of vertices V denotes words in the vocabulary, and the set of edges E denotes the cooccurrences between word-pairs and patterns. A pattern is a predicate of two arguments and expresses a semantic relation between the two words. Formally, an edge e \u2208 E connecting two vertices u, v \u2208 V in the relational graph G is a tuple (u, v, l(e), w(e)), where l(e) denotes the label type corresponding to the pattern that co-occurs with the two words u and v in some context, and w(e) denotes the cooccurrence strength between l(e) and the word-pair (u, v). Each word in the vocabulary is represented by a unique vertex in the relational graph and each pattern is represented by a unique label type. Because of this one-to-one correspondence between words and vertices, and patterns and labels, we will interchange those terms in the subsequent discussions. The direction of an edge e is defined such that the first slot (ie. X) matches with u, and the second slot (ie. Y) matches with v in a pattern l(e). Note that multiple edges can exist between two vertices in a relational graph corresponding to different patterns. Most manually created as well as"}, {"heading": "X is a large Y [0.8]", "text": "automatically extracted ontologies can be represented as relational graphs.\nConsider the relational graph shown in Figure 1. For example, let us assume that we observed the context ostrich is a large bird that lives in Africa in a corpus. Then, we extract the lexical pattern X is a large Y between ostrich and bird from this context and include it in the relational graph by adding two vertices each for ostrich and bird, and an edge from ostrich to bird. Such lexical patterns have been used for related tasks such as measuring semantic similarity between words (Bollegala et al. 2007). The co-occurrence strength between a word-pair and a pattern can be computed using an association measure such as the positive pointwise mutual information (PPMI). Likewise, observation of the contexts both ostrich and penguin are flightless birds and penguin is a bird will result in the relational graph shown in Figure 1."}, {"heading": "Learning Word Representations", "text": "Given a relational graph as the input, we learn d dimensional vector representations for each vertex in the graph. The dimensionality d of the vector space is a pre-defined parameter of the method, and by adjusting it one can obtain word representations at different granularities. Let us consider two vertices u and v connected by an edge with label l and weight w. We represent the two words u and v respectively by two vectors x(u),x(v) \u2208 Rd, and the label l by a matrix G(l) \u2208 Rd\u00d7d. We model the problem of learning optimal word representations x\u0302(u) and pattern representations G\u0302(l) as the solution to the following squared loss minimisation problem\nargmin x(u)\u2208Rd,G(l)\u2208Rd\u00d7d\n1\n2 \u2211 (u,v,l,w)\u2208E (x(u)>G(l)x(v)\u2212 w)2. (1)\nThe objective function given by Eq. 1 is jointly nonconvex in both word representations x(u) (or alternatively x(v)) and pattern representations G(l). However, if G(l) is positive semidefinite, and one of the two variables is held fixed, then the objective function given by Eq. 1 becomes convex in the other variable. This enables us to use Alternating Least Squares (ALS) (Boyd et al. 2010) method to solve the optimisation problem. To derive the stochastic gradient descent (SGD) updates for the parameters in the model, let us denote the squared loss associated with a single edge e = (u, v, l, w) in the relational graph G by E(e), given\nby,\nE = 1\n2 \u2211 (u,v,l,w)\u2208E (x(u)>G(l)x(v)\u2212 w)2. (2)\nIn Eq. 2. The gradient of the error w.r.t. x(u) and G(l) are given by,\n\u2207x(u)E = \u2211\n(u,v,l,w)\u2208E\n(x(u)>G(l)x(v)\u2212 w)G(l)x(v) +\n\u2211 (v,u,l,w)\u2208E (x(v)>G(l)x(u)\u2212 w)G(l)>x(u) (3)\n\u2207G(l)E = \u2211\n(u,v,l,w)\u2208E\n(x(u)>G(l)x(v)\u2212 w)x(v)x(u)> (4)\nIn Eq. 4, x(v)x(u)> denotes the outer-product between the two vectors x(v) and x(u), which results in a d\u00d7 d matrix. Note that the summation in Eq. 3 is taken over the edges that contain either u as a start or an end point, and the summation in Eq. 4 is taken over the edges that contain the label l.\nThe SGD update for the i-th dimension of x(u) is given by,\nx(u) (t+1) (i) = x(u) (t) (i)\u2212 \u03b70\u221a\nt\u2211 t\u2032=1 ( \u2207 x(u)(t \u2032)E ) (i) 2\n( \u2207x(u)(t)E ) (i) .\n(5) Here, ( \u2207x(u)(t)E ) (i) denotes the i-th dimension of the gradient vector of E w.r.t. x(u)(t), and the superscripts (t) and (t + 1) denote respectively the current and the updated values. We use adaptive subgradient method (AdaGrad) (Duchi et al. 2011) to schedule the learning rate. The initial learning rate, \u03b70 is set to 0.0001 in our experiments.\nLikewise, the SGD update for the (i, j) element of G(l) is given by,\nG(l)(t+1)(i,j) = G(l) (t) (i,j)\u2212 \u03b70\u221a\nt\u2211 t\u2032=1 ( \u2207G(l)(t\u2032)E ) (i,j) 2\n( \u2207G(l)(t)E ) (i,j) .\n(6) Recall that the positive semidefiniteness of G(l) is a requirement for the convergence of the above procedure. For example, if G(l) is constrained to be diagonal then this requirement can be trivially met. However, doing so implies that x(u)>G(l)x(v) = x(v)>G(l)x(u), which means we can no longer capture asymmetric semantic relations in the model. Alternatively, without constraining G(l) to diagonal matrices, we numerically guarantee the positive semidefiniteness of G(l) by adding a small noise term \u03b4I after each update to G(l), where I is the d\u00d7d identity matrix and \u03b4 \u2208 R is a small perturbation coefficient, which we set to 0.001 in our experiments.\nPseudo code for the our word representation learning algorithm is shown in Algorithm 1. Algorithm 1 initialises word and pattern representations randomly by sampling from the zero-mean and unit variance Gaussian distribution. Next, SGD updates are performed alternating between updates for x(u) and G(l) until a pre-defined number of maximum epochs is reached. Finally, the final values of x(u) and G(l) are returned.\nAlgorithm 1 Learning word representations. Input: Relational graph G, dimensionality d of the word represen-\ntations, maximum epochs T , initial learning rate \u03b70. Output: Word representations x(u) for words u \u2208 V .\n1: Initialisation: For each vertex (word) u \u2208 V , randomly sample d dimensional real-valued vectors from the normal distribution. For each label (pattern) l \u2208 L, randomly sample d\u00d7 d dimensional real-valued matrices from the normal distribution.\n2: for t = 1, . . . , T do 3: for edge (u, v, l, w) \u2208 G do 4: Update x(u) according to Eq. 5 5: Update G(l) according to Eq. 6 6: end for 7: end for 8: return x(u),G(l)"}, {"heading": "Experiments", "text": ""}, {"heading": "Creating Relational Graphs", "text": "We use the English ukWaC1 corpus in our experiments. ukWaC is a 2 billion token corpus constructed from the Web limiting the crawl to .uk domain and medium-frequency words from the British National Corpus (BNC). The corpus is lemmatised and Part-Of-Speech tagged using the TreeTagger2. Moreover, MaltParser3 is used to create a dependency parsed version of the ukWaC corpus.\nTo create relational graphs, we first compute the cooccurrences of words in sentences in the ukWaC corpus. For two words u and v that co-occur in more than 100 sentences, we create two word-pairs (u, v) and (v, u). Considering the scale of the ukWaC corpus, low co-occurring words often represents misspellings or non-English terms.\nNext, for each generated word-pair, we retrieve the set of sentences in which the two words co-occur. For explanation purposes let us assume that the two words u and v co-occur in a sentence s. We replace the occurrences of u in s by a slot marker X and v by Y. If there are multiple occurrences of u or v in s, we select the closest occurrences u and v measured by the number of tokens that appear in between the occurrences in s. Finally, we generate lexical patterns by limiting prefix (the tokens that appears before X in s), midfix (the tokens that appear in between X and Y in s), and suffix (the tokens that appear after Y in s) each separately to a maximum length of 3 tokens. For example, given the sentence ostrich is a large bird that lives in Africa, we will extract the lexical patterns X is a large Y, X is a large Y that, X is a large Y that lives, and X is a large Y that lives in. We select lexical patterns that co-occur with at least two word pairs for creating a relational graph.\nIn addition to lexical patterns, we generate POS patterns by replacing each lemma in a lexical pattern by its POS tag. POS patterns can be considered as an abstraction of the lexical patterns. Both lexical and POS patterns are unable to capture semantic relations between two words if those words\n1http://wacky.sslmit.unibo.it/doku.php?id=corpora 2www.cis.uni-muenchen.de/\u223cschmid/tools/TreeTagger 3www.maltparser.org\nare located beyond the extraction window. One solution to this problem is to use the dependency path between the two words along the dependency tree for the sentence. We consider pairs of words that have a dependency relation between them in a sentence and extract dependency patterns such as X direct-object-of Y. Unlike lexical or POS patterns that are proximity-based, the dependency patterns are extracted from the entire sentence.\nWe create three types of relational graphs using 127, 402 lexical (LEX) patterns , 57, 494 POS (POS) patterns, and 6835 dependency (DEP) patterns as edge labels. Moreover, we consider several popular methods for weighting the cooccurrences between a word-pair and a pattern as follows. RAW: The total number of sentences in which a pattern l co-occurs with a word-pair (u, v) is considered as the co-occurrence strength w.\nPPMI: The positive pointwise mutual information between a pattern l and a word-pair (u, v) computed as,\nmax ( 0, log ( h(u, v, l)\u00d7 h(\u2217, \u2217, \u2217) h(u, v, \u2217)\u00d7 h(\u2217, \u2217, l) )) .\nHere, h(u, v, l) denotes the total number of sentences in which u, v, and l co-occurs. The operator \u2217 denotes the summation of h over the corresponding variables.\nLMI: The local mutual information between a pattern l and a word pair (u, v) is computed as,\nh(u, v, l) h(\u2217, \u2217, \u2217) log ( h(u, v, l)\u00d7 h(\u2217, \u2217, \u2217) h(u, v, \u2217)\u00d7 h(\u2217, \u2217, l) ) .\nLOG: This method considers the logarithm of the raw cooccurrence frequency as the co-occurrence strength. It has been shown to produce vector spaces that demonstrate vector substraction-based analogy representations (Pennington et al. 2014).\nENT: Patterns that co-occur with many word-pairs tend to be generic ones that does not express a specific semantic relation. Turney (2006) proposed the entropy of a pattern over word-pairs as a measure to down-weight the effect of such patterns. We use this entropy-based co-occurrence weighting method to weigh the edges in relational graphs."}, {"heading": "Evaluation", "text": "We use the semantic word analogy dataset first proposed by Mikolov et al. (2013a) and has been used in much previous work for evaluating word representation methods. Unlike syntactic word analogies such as the past tense or plural forms of verbs which can be accurately captured using rule-based methods (Lepage 2000), semantic analogies are more difficult to detect using surface-level transformations. Therefore, we consider it is appropriate to evaluate word representation methods using semantic word analogies. The dataset contains 8869 word-pairs that represent word analogies covering various semantic relations such as the capital of a country (e.g. Tokyo, Japan vs. Paris, France), and family (gender) relationships (e.g. boy, girl vs. king, queen).\nA word representation method is evaluated by its ability to correctly answer word analogy questions using the\nword representations created by that method. For example, the semantic analogy dataset contains word pairs such as (man, woman) and (king, queen), where the semantic relations between the two words in the first pair is similar to that in the second. Denoting the representation of a word w by a vector v(w), we rank all words w in the vocabulary according to their cosine similarities with the vector, v(king)\u2212v(man)+v(woman). The prediction is considered correct in this example only if the top most similar vector is v(queen). During evaluations, we limit the evaluation to analogy questions where word representations have been learnt for all the four words. Moreover, we remove three words that appear in the question from the set of candidate answers. The set of candidates for a question is therefore the set consisting of fourth words in all semantic analogy questions considered as valid (after the removal step described above), minus the three words in the question under consideration. The percentage of correctly answered semantic analogy questions out of the total number of questions in the dataset (ie. micro-averaged accuracy) is used as the evaluation measure."}, {"heading": "Results", "text": "To evaluate the performance of the proposed method on relational graphs created using different pattern types and cooccurrence measures, we train 200 dimensional word representations (d = 200) using Algorithm 1. 100 iterations (T = 100) was sufficient to obtain convergence in all our experiments. We then used the learnt word representations to obtain the accuracy values shown in Table 1. We see that the proposed method obtains similar results with all pattern types and co-occurrence measures. This result shows the robustness of our method against a wide-range of typical methods for constructing relational graphs from unstructured texts. For the remainder of the experiments described in the paper, we use the RAW co-occurrence frequencies as the co-occurrence strength due to its simplicity.\nTo study the effect of the dimensionality d of the representation that we learn on the accuracy of the semantic analogy task, we plot the accuracy obtained using LEX, POS, and DEP relational graphs against the dimensionality of the representation as shown in Figure 2. We see that the accuracy steadily increases with the dimensionality of the representation up to 200 dimensions and then becomes relatively stable. This result suggests that it is sufficient to use 200 dimensions for all three types of relational graphs that we constructed. Interestingly, among the three pattern types,\nTable 2: Comparison of the proposed method (denoted by Prop) against prior work on word representation learning.\nMethod capital-common capital-world city-in-state family (gender) currency Overall Accuracy SVD+LEX 11.43 5.43 0 9.52 0 3.84 SVD+POS 4.57 9.06 0 29.05 0 6.57 SVD+DEP 5.88 3.02 0 0 0 1.11 CBOW 8.49 5.26 4.95 47.82 2.37 10.58 skip-gram 9.15 9.34 5.97 67.98 5.29 14.86 GloVe 4.24 4.93 4.35 65.41 0 11.89 Prop+LEX 22.87 31.42 15.83 61.19 25.0 26.61 Prop+POS 22.55 30.82 14.98 60.48 20.0 25.35 Prop+DEP 20.92 31.40 15.27 56.19 20.0 24.68\nLEX stabilises with the least number of dimensions followed by POS and DEP. Note that LEX patterns have the greatest level of specificity compared to POS and DEP patterns, which abstract the surface-level lexical properties of the semantic relations. Moreover, relational graphs created with LEX patterns have the largest number of labels (patterns) followed by that with POS and DEP patterns. The ability of the proposed method to obtain better performances even with a highly specified, sparse and high-dimensional feature representations such as the LEX patterns is important when applying the proposed method to large relational graphs.\nWe compare the proposed method against several word representation methods in Table 2. All methods in Table 2 use 200 dimensional vectors to represent a word. A baseline method is created that shows the level of performance we can reach if we represent each word u as a vector of patterns l in which u occurs. First, we create a co-occurrence matrix between words u and patterns l, and use Singular Value Decomposition (SVD) to create 200 dimensional projections for the words. Because patterns represent contexts in which words appear in the corpus, this baseline can be seen as a version of the Latent Semantic Analysis (LSA), that has been widely used to represent words and documents in information retrieval. Moreover, SVD reduces the data sparseness in raw co-occurrences. We create three versions of this baseline denoted by SVD+LEX, SVD+POS, and SVD+DEP corresponding to relational graphs created using respectively LEX, POS, and DEP patterns. CBOW (Mikolov et al. 2013b), skip-gram (Mikolov et al. 2013c), and GloVe (Pen-\nnington et al. 2014) are previously proposed word representation learning methods. In particular, skip-gram and GloVe are considered the current state-of-the-art methods. We learn 200 dimensional word representations using their original implementations with the default settings. We used the same set of sentences as used by the proposed method to train these methods. Proposed method is trained using 200 dimensions and with three relational graphs (denoted by Prop+LEX, Prop+POS, and Prop+DEP), weighted by RAW co-occurrences.\nFrom Table 2, we see that Prop+LEX obtains the best overall results among all the methods compared. We note that the previously published results for skip-gram and CBOW methods are obtained using a 100B token news corpus, which is significantly large than the 2B token ukWaC corpus used in our experiments. However, the differences among the Prop+LEX, Prop+POS, and Prop+DEP methods are not significantly different according to the Binomial exact test. SVD-based baseline methods perform poorly indicating that de-coupling the 3-way co-occurrences between (u, v) and l into 2-way co-occurrences between u and l is inadequate to capture the semantic relations between words. Prop+LEX reports the best results for all semantic relations except for the family relation. Comparatively, higher accuracies are reported for the family relation by all methods, whereas relations that involve named-entities such as locations are difficult to process. Multiple relations can exist between two locations, which makes the analogy detection task hard."}, {"heading": "Conclusion", "text": "We proposed a method that considers not only the cooccurrences of two words but also the semantic relations in which they co-occur to learn word representations. It can be applied to manually created relational graphs such as ontologies, as well as automatically extracted relational graphs from text corpora. We used the proposed method to learn word representations from three types of relational graphs. We used the learnt word representations to answer semantic word analogy questions using a previously proposed dataset. Our experimental results show that lexical patterns are particularly useful for learning good word representations, outperforming several baseline methods. We hope our work will inspire future research in word representation learning to exploit the rich semantic relations that exist between words, extending beyond simple co-occurrences."}], "references": [{"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Marco Baroni", "Alessandro Lenci"], "venue": "Computational Linguistics, 36(4):673 \u2013 721,", "citeRegEx": "Baroni and Lenci 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli. Nouns are vectors"], "venue": "EMNLP, pages 1183 \u2013 1193,", "citeRegEx": "Baroni and Zamparelli 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Don\u2019t count", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL, pages 238\u2013247,", "citeRegEx": "Baroni et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Journal of Machine Learning Research", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin. A neural probabilistic language model"], "venue": "3:1137 \u2013 1155,", "citeRegEx": "Bengio et al. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798 \u2013 1828, March", "citeRegEx": "Bengio et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Websim: A web-based semantic similarity measure", "author": ["Danushka Bollegala", "Yutaka Matsuo", "Mitsuru Ishizuka"], "venue": "Proc. of 21st Annual Conference of the Japanese Society of Artitificial Intelligence, pages 757 \u2013 766,", "citeRegEx": "Bollegala et al. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Foundations and Trends in Machine Learning", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Exkstein. Distributed optimization", "statistical learning via the alternating direction method of multipliers"], "venue": "3(1):1 \u2013 122,", "citeRegEx": "Boyd et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "The Morgan Kaufmann Series in Artificial Intelligence", "author": ["Ronald Brachman", "Hector J. Levesque. Knowledge Representation", "Reasoning"], "venue": "Morgan Kaufmann Publishers Inc., June", "citeRegEx": "Brachman and Levesque 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuska"], "venue": "Journal of Machine Learning Research, 12:2493 \u2013 2537,", "citeRegEx": "Collobert et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Using relational similarity between word pairs for latent relational search on the web", "author": ["Duc"], "venue": "In IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,", "citeRegEx": "Duc,? \\Q2010\\E", "shortCiteRegEx": "Duc", "year": 2010}, {"title": "12:2121 \u2013 2159", "author": ["John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization. Journal of Machine Learning Research"], "venue": "July", "citeRegEx": "Duchi et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A synopsis of linguistic theory 1930-55", "author": ["John R. Firth"], "venue": "Studies in Linguistic Analysis, pages 1 \u2013 32,", "citeRegEx": "Firth 1957", "shortCiteRegEx": null, "year": 1957}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Edward Grefenstette"], "venue": "*SEM, pages 1 \u2013 10,", "citeRegEx": "Grefenstette 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In ACL", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng. Improving word representations via global context", "multiple word prototypes"], "venue": "pages 873 \u2013 882,", "citeRegEx": "Huang et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In ACL", "author": ["Yves Lepage. Languages of analogical strings"], "venue": "pages 488 \u2013 494,", "citeRegEx": "Lepage 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "CoRR", "author": ["Tomas Mikolov", "Kai Chen", "Jeffrey Dean. Efficient estimation of word representation in vector space"], "venue": "abs/1301.3781,", "citeRegEx": "Mikolov et al. 2013a", "shortCiteRegEx": null, "year": 2013}, {"title": "In NIPS", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111 \u2013 3119,", "citeRegEx": "Mikolov et al. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "In NAACL", "author": ["Tomas Mikolov", "Wen tau Yih", "Geoffrey Zweig. Linguistic regularities in continous space word representations"], "venue": "pages 746 \u2013 751,", "citeRegEx": "Mikolov et al. 2013c", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu"], "venue": "NIPS,", "citeRegEx": "Mnih and Kavukcuoglu 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In ACL", "author": ["Denis Paperno", "Nghia The Pham", "Marco Baroni. A practical", "linguistically-motivated approach to compositional distributional semantics"], "venue": "pages 90\u201399,", "citeRegEx": "Paperno et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Glove: global vectors for word representation", "author": ["Jeffery Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "EMNLP,", "citeRegEx": "Pennington et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Ng", "Chris Manning"], "venue": "ICML,", "citeRegEx": "Socher et al. 2011a", "shortCiteRegEx": null, "year": 2011}, {"title": "In EMNLP", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions"], "venue": "pages 151\u2013161,", "citeRegEx": "Socher et al. 2011b", "shortCiteRegEx": null, "year": 2011}, {"title": "In EMNLP", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng. Semantic compositionality through recursive matrix-vector spaces"], "venue": "pages 1201\u20131211,", "citeRegEx": "Socher et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In ACL", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y. Parsing with compositional vector grammars"], "venue": "pages 455\u2013465,", "citeRegEx": "Socher et al. 2013a", "shortCiteRegEx": null, "year": 2013}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "NIPS,", "citeRegEx": "Socher et al. 2013b", "shortCiteRegEx": null, "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Aritificial Intelligence Research, 37:141 \u2013 188,", "citeRegEx": "Turney and Pantel 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Similarity of semantic relations", "author": ["P.D. Turney"], "venue": "Computational Linguistics, 32(3):379\u2013416", "citeRegEx": "Turney 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "In EMNLP", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu. Deep learning for Chinese word segmentation", "POS tagging"], "venue": "pages 647\u2013657,", "citeRegEx": "Zheng et al. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2014, "abstractText": "Attributes of words and relations between two words are central to numerous tasks in Artificial Intelligence such as knowledge representation, similarity measurement, and analogy detection. Often when two words share one or more attributes in common, they are connected by some semantic relations. On the other hand, if there are numerous semantic relations between two words, we can expect some of the attributes of one of the words to be inherited by the other. Motivated by this close connection between attributes and relations, given a relational graph in which words are interconnected via numerous semantic relations, we propose a method to learn a latent representation for the individual words. The proposed method considers not only the co-occurrences of words as done by existing approaches for word representation learning, but also the semantic relations in which two words co-occur. To evaluate the accuracy of the word representations learnt using the proposed method, we use the learnt word representations to solve semantic word analogy problems. Our experimental results show that it is possible to learn better word representations by using semantic semantics be-", "creator": "LaTeX with hyperref package"}}}