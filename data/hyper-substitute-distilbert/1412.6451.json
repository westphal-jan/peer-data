{"id": "1412.6451", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Grounding Hierarchical Reinforcement Learning Models for Knowledge Transfer", "abstract": "criticisms of constraint machine learning enable agencies to analyze low - level modeling efficiently for generating underlying abstract grammar - level concepts. originally, abstract learning had been criticized passively ( so. g., for statistical purposes ). recently, it just been extended to estimate objective value fitting procedures for functional inference within the framework of database functions ( rl ). explicit models of information environment inevitably be expected simultaneously augment such a numerical function. other \" active \" connectionist methods have likewise got used using model - aided rl, only to once, only simulation - free variants across rl have been equipped with methods in deep learning. we require a variant of simply model - integrated rl however enables an agent to learn all abstract linguistic behavior near its environment. in this paper, audiences present research on until totally alternative methods can be grounded in dual fields between an entity and its environment.", "histories": [["v1", "Fri, 19 Dec 2014 17:41:59 GMT  (54kb,D)", "http://arxiv.org/abs/1412.6451v1", "14 pages, 4 figures"]], "COMMENTS": "14 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["mark wernsdorfer", "ute schmid"], "accepted": false, "id": "1412.6451"}, "pdf": {"name": "1412.6451.pdf", "metadata": {"source": "META", "title": "Grounding Hierarchical Reinforcement Learning Models for Knowledge Transfer", "authors": ["Mark Wernsdorfer", "Ute Schmid"], "emails": ["ute.schmid}@uni-bamberg.de"], "sections": [{"heading": "1 INTRODUCTION", "text": "Machine learning algorithms are derived from models of the problem-to-be-solved. In RL, problems are modeled in the form of Markov decision processes (MDPs). A MDP can be defined as the 4-tuple (S,A, T ,R). The first element S describes the set of wold states\u2014in most cases it is interpreted as the agent\u2019s sensory perception. The second element A describes the set of possible actions. The transition function determines transitions in state space. It has the general form T : S \u00d7A\u2192 S. A particular action in a particular state causes a particular successor state. Lastly, the reward function gives a scalar value which acts as reward. It indicates whether a particular action in a particular state is to be repeated or avoided. It has the general formR : S \u00d7A\u2192 R where usually R = Z. Although S might be chosen in a way that captures the whole state of the world, for realistic applications, it must be restricted to cover only a small subset of the statements that are currently true about the world. Consider the following examples.\n1. Observable world state\n\u2022 The agent perceives that it is in position (2, 13). \u2022 It moves north to enter sensor state (2, 12).\n2. Hidden world state\n\u2022 The agent perceives a wall in front of it. \u2022 It turns around to enter a sensor state that suggests free passing.\nTo model the second case, the agent\u2019s restricted perception must be taken into account. To cover this, MDPs can be generalized into partially observable MDPs (POMDPs). They can be defined as the 6-tuple (S,A, T ,R,\u2126,O). In addition to the four elements above, these models also comprise the set of possible observations \u2126\u2014possibly distinct from the set of world states\u2014and the observation\n\u2217http://www.uni-bamberg.de/kogsys/\nar X\niv :1\n41 2.\n64 51\nv1 [\ncs .L\nG ]\n1 9\nD ec\n2 01\nfunction which has the general form O : S \u00d7A\u2192 \u2126. A particular action in a particular world state causes a particular observation. (Kaelbling et al., 1998)\nPOMDPs can describe a plethora of tasks for RL agents. If the set of world states is accessible to the agent, it can infer a particular belief about what the current world state is. Indications for the current world state can be inferred from previous beliefs and those world states, that recently observed transitions match with. The result is a belief in the form of a probability distribution over all world states.\nAs a consequence, the resulting belief MDP describes transitions between continuous probability distributions instead of discrete states. To generate probability distributions over wold states, however, these world states need to be known beforehand. Providing the set of world states a priori implies two fatal consequences."}, {"heading": "1.1 SEMANTIC LOAD OF REPRESENTATIONS", "text": "The first problem concerns the semantic load of predefined representations. The set of world states, and therefore the beliefs inferred from them, can be regarded as representations of the environment. To provide an agent with such a fixed set of representations, however, means to inject considerable amounts of knowledge into the system.\nIt tacitly implies that these representations are appropriate for solving the POMDP at hand. Having appropriate representations implies knowledge about how those representations are to be used. To know how to use representations, on the other hand, means to already know how to handle the task at hand.\nBengio et al. (2013) mention the necessary trade-off between the \u201crichness\u201d of representations and the effort necessary to process them. Diuk et al. (2008) show that the selection of representations has immense influence on the performance of RL algorithms. More specifically, they show that providing semantically rich representations (e.g. representations of objects) can simplify a given task considerably compared to semantically sparse representations (e.g. representations of world states)."}, {"heading": "1.2 TRANSFER OF KNOWLEDGE", "text": "The second problem is that knowledge transfer with objective inputs requires elaborate methods for joining discrete data. Partial observability is frequently avoided by providing the world state as sensory input. In any (stochastically) determinate world, complete world states enable to learn an interaction policy which optimizes the cumulative reward received over time. The detachment of objective \u201csnapshots\u201d of the whole world, however, masks the actually relevant information in interaction data.\nThe first example of observable world states is such a case of objective interaction. Knowledge transfer with objective data is successfully achieved if the equivalence of seemingly different states is discovered. It might be discovered, for example, that column 2 in a grid world is always devoid of obstacles. Therefore, state transitions to the north in row 2 can be modeled by approximating the transition function T with T2((x, y), an) = (x, y \u2212 1). All states in column 2 can be pooled according to their successor state when the agent moves to the north. The data structure implementing T2 can be regarded as a high-level representation of column 2.\nThe second example of hidden world states describes a case of subjective interaction. The agent has no access to absolute information about its relation to the environment. Assuming the existence of an objective agent position, its perception must therefore be regarded as ambiguous. Every single location in column 2 might be perceived as identical, although, from an objective perspective, they differ quite obviously in their value of x, the absolute position of the agent on a vertical axis.1\nDepending on the interaction paradigm, the problem of knowledge transfer takes two contrastive forms. Objective interaction makes it necessary to analyze states which appear to the agent as if\n1This concerns not only perception. In relative interaction, actions also depend on hidden world states: moving north, east, south, or west is not part of the agent\u2019s motor capabilities. Instead, its absolute direction of movement depends on its current orientation, which is not directly observable. The objective direction of movement is not only for the agent to decide.\nthey were different and pool them to recognize their similarity. In return, the agent does not have ambiguous perceptions from an objective observer\u2019s perspective.\nSubjective interaction, on the other hand, makes it necessary to synthesize states which appear to the agent as if they were identical and differentiate them to recognize their distinctness. In return, knowledge transfer comes for free: although the agent might be in objectively different situations, as long as there is no reason for differentiation, knowledge which has been been acquired from identical perceptions and actions is applied effortlessly, in fact even unnoticed by the agent.\nFigure 1 shows the corridor environment for testing knowledge transfer in RL agents. Figure 1a illustrates the training environment and fig. 1b the environment where the acquired knowledge is tested. In each time step, the agent receives a reward of \u22121. If it reaches the goal position G, the current episode ends, its position is reset to the starting position S, and it receives a reward of +10.\nObjective interaction data consists of the agent\u2019s absolute position as sensor input. Objective motor output determines whether it enters the cell to its north, east, south, or west in the next time step in case it is not occupied by a wall.2 The new part of the corridor in the larger testing environment contains qualitatively new objective data for the agent (i.e., coordinates with an unknown horizontal component). Therefore, it cannot benefit from the knowledge acquired in the training environment. It learns a new policy by successively overwriting the old one, as if it were in a completely new environment.\nSubjective interaction data consists of the four surrounding cells as sensor input. Subjective motor output determines whether the agent turns 90 degrees to its right, to its left, or if it enters the cell in front of it (in case it is not occupied by a wall). Although the new part of the testing corridor is objectively different to the smaller training corridor, they subjectively appear identical to the agent. Subjective interaction data enables the agent to efficiently reapply its knowledge without even knowing so.\nWhen knowledge transfer is the issue, the drawback of objective data becomes obvious. The drawback of subjective data is that any sufficiently complex environment presents behavior optimizing algorithms with POMDPs. To avoid semantically loaded representations, however, the set of world states cannot be provided a priori. POMDPs without knowing the complete set of world states cannot be solved by any single policy. As our explicit goal is the reuse of representations in objectively different situations, however, it still seems reasonable to use subjective interaction data and cope with these problems as they occur.\nSemantic load and knowledge transfer are essentially two stages of coping with the same problem. The problem of semantic load shows that transferring information (i.e., semantics) might increase performance but it impairs autonomy at the same rate. Knowledge transfer, on the other hand, enables to investigate which information can be transferred and how to transfer it. The difference is, that the former is usually understood as pathological because the transferred knowledge originates in the designer, while the latter is understood as beneficial because knowledge is transferred within the same system.\nIn section 1 we have differentiated two perspectives on agent interaction that are of utmost importance for knowledge transfer. Section 2 presents related work from the complementary fields of deep representation learning and hierarchical RL. In section 3, we present the general components of our architecture and the type of information they exchange. Section 4 sketches an instantiation of the\n2The agent does not receive additional punishment for running into walls.\narchitecture and the concrete machine learning methods applied. In section 5 we present and discuss the results from testing our architecture in typical hierarchical RL gridworld environments. Section 6 outlines subsequent research."}, {"heading": "2 RELATED WORK", "text": "Usually, the contrast between objective and subjective interaction is reduced to the difference between MDPs and POMDPs. To develop a machine learning algorithm from an appropriate model of the problem-to-be-solved, however, we see it fit not to take an observer\u2019s perspective but the agent\u2019s perspective on the problem. From the agent\u2019s perspective, there are no world states to begin with.\nFirst and foremost, beliefs are identical to the world perceived and influenced by the agent. To refrain from one\u2019s believes is an accomplishment of highly developed animals and by no means something to take for granted. POMDPs already imply a differentiation between belief and world states in separately modeling a probability distribution (i.e., a belief) and its targets (i.e., world states).\nTherefore, POMDPs might be the perfect model for the problem of RL from an observer\u2019s perspective (i.e., for developing algorithms for objective interaction),3 to model learning as it is experienced, however, requires to refrain from maintaining representations of objective world states. Instead, those states need to be constructed by the agent itself in a process of deep representation learning."}, {"heading": "2.1 DEEP REPRESENTATION LEARNING", "text": "An unbiased learner cannot rely on representations that have been provided by its designer. This insight dates back at least to the philosophical criticism that has been brought forward towards artificial intelligence in the early 80ies in the form of the Chinese room argument (Searle, 1980), the epistemological frame problem (Dennett, 1981), and, more explicitly, the symbol grounding problem (Harnad, 1990). The semantic implications of representations have been recognized and are investigated in the field of representation learning.\nThe most famous application of representation learning at the time being is in deep learning. Deep learning is commonly performed with connectionist methods (e.g. restricted Boltzmann machines, auto-encoders, or artificial neural networks). Its eponymous feature, however, is not its class of methods but how they are employed.\nOne intuition behind deep learning is that data complexity can be reduced considerably by generating appropriate representations from the dynamics of interaction between a particular system and its environment.4 Representation learning methods can be \u201cstacked\u201d by providing the representations generated in one layer to a higher-level instance of a similar representation learner. Therefore, in each layer, data complexity can be reduced more and more by recognizing and exploiting patterns. Data complexity is transformed into structural complexity of representations. As recognized patterns can be applied repeatedly, however, structural complexity only grows logarithmically with input complexity.\nConnectionist variations of deep representation learners effectively treat the output of a layer as input for the layer above. The idea, however, can be generalized beyond connectionist implementations. Stacking any method that reduces data complexity by generating representations with less degrees of freedom (e.g. dimensionality reduction, principal component analysis, lossy, or loss-less compression) must eventually lead to a hierarchy of representations, with the most abstract at the top and the most concrete at the bottom.\nIf representations are generated not only from proto-features, sub-symbolic information, raw input, or however one may call the first data to enter the system, but also from previously learned representations, then the system effectively generates layers containing abstract representations. A hierarchical organization of cognition comes naturally if a learning system is able to reuse the representations in different layers. Benefiting from the same set of representations in objectively different situations is a case of knowledge transfer.\n3Although section 1.1 casts doubt on this. 4Interestingly, this insight corresponds to recent developments in cognitive sciences that have been labeled as\n\u201cembodied cognition\u201d (Shapiro, 2011).\nHierarchical representations of a passive data space, e.g. in image analysis (Schmidhuber, 2014) or loss-less sequence compression (Nevill-Manning & Witten, 1997), have been proposed in various fields of computer science and with various applications. Approaches to learn the structure of hierarchical representations of an interactive data space (i.e., a policy) go back at least to the early 90ies (Schmidhuber, 1990; Schmidhuber, 1991; Feldkamp & Puskorius, 1998), but only recently, RL methods have been presented that are efficiently able to generate deep policies from scratch (Mnih et al., 2013; Guo et al., 2014)."}, {"heading": "2.2 REINFORCEMENT LEARNING", "text": "RL is concerned with optimizing behavior in unknown environments. Optimality is quantified as reward function that provides an evaluation feedback for the agent, indicating whether an action should be considered as good or bad. To achieve optimal behavior over time, the agent optimizes not the immediate but the cumulative reward it expects to receive by following the current policy. This cumulative reward is discounted over future states because immediate rewards might matter more than those far in the future.\nIn general, such an evaluation can be estimated by remembering which action in which sensor state was followed by which discounted, cumulative reward. The future discounted, cumulative reward R of unknown perception-action tuple (s, a) \u2208 S \u00d7A can be predicted by estimating a value function V .\nV : S \u00d7A\u2192 R (1)\nChoosing a perception-action pair that maximizes the value function enables optimal behavior over a certain period of time.\nThe same action in the same state may not always lead to the same successor state. Non-stationary probability distributions can force the value function to adapt. More importantly, however, the same state-action pair might produce different states systematically. To avoid conflating the rewards of those states, the value function can be augmented by past experience about state transitions in the observed environment. Model-based RL explicitly models these state transitions.\nOptimal models are identical to the transition function of the environment T . Interpolating transition function T in eq. (2) enables predicting the outcome of perception-action pairs that have not been observed before.\nT : S \u00d7A\u2192 S (2)\nIn model-based RL, instead of associating cumulative reward with perception-action pairs, immediate rewards are associated with sensor states. Knowledge about state transitions in the specific environment enables to choose attractive successor states via their estimated immediate reward and the probability of reaching them with a particular action. Choosing an action that maximizes the estimated reward of the estimated successor states enables planned behavior.\nMore importantly, if a reliable model of the environment is available, the value of each perceptionaction tuple need not be tediously approximated by actually experiencing the future cumulative reward over all of its possible consequences. Instead, state transitions can be \u201csimulated\u201d arbitrarily far into the (estimated) future by exploiting the internal model.\nData structures which implement such a transition model are representations of parts of the environment. Value and model function eqs. (1) and (2) determine the current interaction policy \u03c0 = (V, T ) for the agent.\nHierarchical RL explores hierarchical models of the environment. Most of recent research uses objective interaction data (Sammut & Webb, 2011). First and foremost reason is that objective data produces MDPs. If the agent is in a determinate sensor state, and any given action has (stochastically) predictable outcome (i.e., successor state and reward), transition and value functions can be approximated precisely.\nIntuitively, transitions of abstract states in a high layer of the hierarchy occur less often than transitions in a low layer. Therefore, semi MDPs are frequently used to model such time-sensitivity (Thrun & Schwartz, 1995; McGovern et al., 1997; Dietterich, 1999; McGovern & Barto, 2001; Hengst, 2002).\nThis generalization of MDPs extends the model of state transitions by duration as proposed by Sutton et al. (1999).\nThe structure of hierarchical models, however, is mostly trained: stochastic information is integrated into a fixed hierarchical structure. It is desirable to enable a RL agent to generate hierarchical representations autonomously, on the one hand, to avoid the problem of semantically loaded representations and, on the other hand, to resolve the various ambiguities that result from subjective interaction in POMDPs. Deep representation learning methods can help to achieve this goal."}, {"heading": "3 OVERVIEW", "text": "In the following, we will present a rough outline of our approach to the grounding of deep models for RL. We present the general form of the functions we used. We implemented several agents according to this architecture. In this section, however, we present only the general structure such that the individual methods remain exchangeable.\nWe have shown that subjective interaction requires the disambiguation of seemingly identical states. To differentiate identical observations, additional information is necessary. This information is not directly accessible, it needs to be \u201cuncovered in,\u201d or \u201cgenerated from,\u201d observable data. This hidden information can be modeled as latent states. In belief MDPs, the current world state is such a latent state. To avoid semantic load, however, we have determined that representations (i.e., the set of latent states) cannot be predefined but must be inferred from the dynamics of interaction between agent and environment alone. These dynamics can be stored in trajectories or interaction histories.5\nA history h is a sequence of single experiences e. In the literature, experiences usually consist of the 4-tuple (st, at, st+1, rt+1) with perception and action at time step t, and perception and reward at time step t+ 1 (Lin, 1993). The perceived history h can be recognized as an instance of a latentstate-specific model Ml, where \u2200l \u2208 L. l\u2194Ml during latent state l. Inspired by Peircean semiotics, our definition of representation consists of shape (i.e., a latent state l), content (i.e., a policy \u03c0l = (Vl,Ml) of value and model function), and reference (i.e., history hl). A history is considered as the reference of a particular latent state, if the likelihood of the history being generated from the latent state\u2019s model exceeds a particular threshold.6 Latent states are the shapes of abstract representations for situations in which their transitions hold. These situations are interaction histories."}, {"heading": "3.1 LEARNING QUERY PROCESSES", "text": "Transitions between abstract representations can be described by the query function. The query function is analogue to the transition function T in MDPs. Notice, however, that MDPs assume a perception-action tuple to determine a single successor perception, whereas in an abstract representation, various queries from abstract representations might succeed and/or fail.\nQ : L\u00d7 L\u2192 S S = {>,\u22a5} (3)\nChoosing an abstract representation, and therefore its corresponding model, is not a MDP. The reason is that inducing a policy is not as straight-forward as performing an action. Whether the model function of a queried latent state matches the subsequent history is not fully under control of the agent. Among abstract representations, there is no unambiguous distribution of responsibilities between agent and environment as it is among perceptions and actions.\nIn general, transitions of representations follow query processes (QPs). A QP is defined by the 3-tuple (L,Q,R). It contains the open set of latent states, the query function in eq. (3), and the reward functionR : L\u2192 R, where R = Z. The reward function in QPs is analogue to the reward function in MDPs.\n5Although we regard both as equivalent (in contrast, see Singh et al., 2000), here, we will stick with the latter term, as it has already been established in connectionist RL (Lin, 1993). The former is more common in applications of dynamic systems theory (e.g. Guenter & Billard, 2007).\n6Note that only linear histories can be observed. Models, on the other hand contain nonlinear information about possible transitions, whereas histories only contain actual transitions.\nAn agent must be able to estimate whether a latent state can be induced from another latent state at all. To perform successful queries, therefore, the query function must be approximated by a model function. This model function is analogue to the model function in eq. (2). Optimal model functions are identical to the query function of the environment Q.\nM : L\u00d7 L\u2192 S S = {>,\u22a5} (4)\nAbstract representations also need to be chosen according to the expected change in discounted, cumulative reward caused by their policy. This can be estimated by the abstract value function. The abstract value function is analogue to the value function in eq. (1).\nVl : L\u2192 R (5)\nEquations (4) and (5) determine the policy \u03c0 = (Vl,M) for solving QPs, just like eqs. (1) and (2) determine the policy \u03c0 = (V, T ) for solving MDPs. The experiences in history h enable to iteratively approximate value and transition function.7"}, {"heading": "3.2 ABSTRACTING QUERY PROCESSES", "text": "Associating histories with models enables abstract representations of concrete observations in the form of latent states. These latent states model the agent\u2019s abstract perception of the environment: they represent the environment to the agent. A function to differentiate a perceived history according to the latent state whose model matches best has the general form\nA : H \u2192 L (6)\nwhere H = [et\u2212n, . . . , et], t is the current time-step, and n is the length of the history. This function realizes an abstract representation of a history referenced by a the shape of a latent state via the model it contains. Therefore, we call it \u201cabstraction function.\u201d\nThe environment responds to queries by either accepting or rejecting the queried state, depending on whether the interaction history that has been observed after querying a representation actually matches the according model function. If it does, the query is successful. Such a measure of \u201cmatching\u201d must be integrated in the abstraction function, such that only latent states are returned that are associated with models compatible to the observed history.\nEach abstract representation provides an interaction policy that consists of value and model function. To effectively reuse policies, they must be efficiently retrievable. The general form of a retrieval function must be inverse toA: not from observed histories to representations, but from representations to concrete interaction policies. This application function has the following form:\nA\u2032 : L\u2192 \u03a0 (7)\nEnvironment and agent determine together where the transition from the current representation leads to. Consequentially, the intension to induce a particular successor representation lq may fail, dependent on whether the current history which has been evoked by the queried policy h\u03c0qt is actually compatible to this policy. Whether a query succeeds or fails is determined in eq. (8).\nh \u03c0q t = [et\u2212n, . . . , et]\nlt = A(h \u03c0q t )\nQ(lt\u2212n\u22121, lq)\u2194 lt = lq (8)"}, {"heading": "3.3 STACKING QUERY PROCESSES", "text": "There are obvious parallels between observable and latent states. Value functions are applicable straight-forwardly in both. In both cases, choices influence the future reward. The forms of the\n7At this stage, the overall architecture resembles a hidden Markov model. As more and more layers are added, however, the binary differentiation between latent and observable layers makes way for an unbounded hierarchy of stacked QPs. Also, notice that transitions between latent states are not determined by incoming information alone but they are also a means to inject information into the environment via actions.\nmodel for observable state transitions in eq. (2) and the model for representation transitions in eq. (4), however, differ. MDPs regard observable states as conditional on a perception-action tuple, whereas QPs regard the success of a query as conditional on a tuple of two consecutive latent states. As we have seen, latent states cannot be differentiated into perception and action. The general form of the model function of MDPs in eq. (2) can therefore not be applied to them.\nTo enable an unbounded hierarchy, it is desirable to unify the model functions in each layer. QPs can be stacked. Abstract model functions can be generated which describe the transitions of latent states. Latent states, on the other hand, contain low-level model functions themselves. To be grounded in observable states, however, concrete sensorimotor interaction must also be described by QPs.\nIf we perform some modifications to what we understand as an observable state, the model function of QPs in eq. (4) can be applied to observable states. We have shown that observable states in RL are commonly associated with the sensor states of an agent. Its actions, on the other hand, are also observable, but essentially different. Action at and perception st are separate. To translate MDPs into QPs, action and perception need to be integrated. We introduce sensorimotor states by simple concatenation of the last action and the current observation: x = (at\u22121, st) and x \u2208 SM where SM = A\u00d7 S. Sensorimotor states can be queried like latent states. A sensorimotor state, however, does not imply an interaction policy, because sensorimotor states are the lowest layer of interaction. Instead, querying a sensorimotor state implies the execution of the part of the state the agent has control over: the motor component. The agent has only partial influence on the success of a query. The sensor component is controlled by the environment. If the sensorimotor state in the query contains another sensor component than the agent\u2019s perception in the next time step, the query will fail.\nModifying observable states also implies modifying experiences. For QPs we differentiate two types of experiences. One contributing to the value function and one contributing to the model function. Value-related experiences consist of a sensorimotor state and the simultaneously received reward ev = (xt, rt). Model-related experiences consist of an observed sensorimotor state, a queried sensorimotor state, and whether the query was successful em = (x, q, s) where s \u2208 {>,\u22a5} and x, q \u2208 SM ."}, {"heading": "4 IMPLEMENTATION", "text": "In the following, we describe how we implemented the above functions and related methods from conventional RL. The value function estimates the discounted, cumulative reward. The model function estimates which sensorimotor states can be queried. The query function Q in QPs replaces the transition function T in MDPs. Finally, the abstraction function generalizes histories of experiences of sensorimotor states into high-level representations. The application function merely remembers which value and model functions are associated with which abstract state. Therefore, we will not cover it in more detail. We will present practical methods to approximate these functions.8"}, {"heading": "4.1 VALUE FUNCTION", "text": "RL literature provides numerous methods for approximating a value function. Two of the most prominent ones are Q-learning (originally Watkins, 1989) and SARSA-learning (interestingly, originally in a connectionnist context Rummery & Niranjan, 1994). The former generates a value function which describes the optimal interaction policy (i.e., offline learning), the latter generates a value function which describes the current interaction policy (i.e., online learning).\nConsider an agent that performs its (arbitrarily chosen) actions only with a certain probability. In some cases it performs a random action instead. The task is to learn to walk the shortest possible path along a straight cliff. Q-agents learn to walk close to the edge, because it is the shortest path with the most cumulative reward. If a random action occurs, however, they might drop, suffering large amounts of negative reward. Q-learning performs offline learning.\n8In the present research, we evaluate QPs as a means for grounding a hierarchical model for RL. Our ongoing research investigates the possibility to effectively generate such a model.\nOnline policy learners, on the other hand, learn from actually performed actions. In the cliff walking example, they learn to follow an arc that keeps a safe distance to the edge. Any autonomously learning agent needs to perform random actions to explore unknown environments. For life-long learning, therefore, online learning seems preferable.\nFor this reason we adopted SARSA-learning. The original update function for separate sensor and motor states in SARSA-learning is as follows.\nV (st\u22121, at\u22121)\u2190 V (st\u22121, at\u22121) + \u03b1[rt + \u03b3V (st, at)\u2212 V (st\u22121, at\u22121)] \u03b1, \u03b3 \u2208 [0, 1] (9)\nThis iterative process is parametrized with learning factor \u03b1 and discount factor \u03b3. The learning factor determines the readiness to overwrite previous experiences with new ones. The bigger \u03b1 is, the more weight new value-related experiences have in comparison to older ones. The discount factor \u03b3 determines the importance of future rewards.\nQ- and SARSA-learning use MDPs as a model. Therefore, both separate sensor and motor states. They are, however, easily adaptable to accommodate for an element of the set of all sensorimotor states x \u2208 SM . We modified the SARSA-value update in the following way. Notice that in our modification, the simultaneously received reward is used to update the value of a state, instead of the next reward as in eq. (9).\nVl(xt\u22121)\u2190 Vl(xt\u22121) + \u03b1[rt\u22121 + \u03b3Vl(xt)\u2212 Vl(xt\u22121)] \u03b1, \u03b3 \u2208 [0, 1] (10)\nIf the task is segmented into episodes, the expected cumulative reward has an upper bound (i.e., the cumulative reward to be expected during the remainder of the current episode). If the task is ongoing, however, the value function must be bounded by normalization.\nVl(xt\u22121)\u2190 Vl(xt\u22121) + \u03b1[rt\u22121 + \u03b3Vl(xt)\u2212 Vl(xt\u22121)]\n1 + \u03b1 \u03b1, \u03b3 \u2208 [0, 1] (11)\nQ- and SARSA-learning are used in model-free RL. The interaction policy followed by a Q- or SARSA-agent is determined by the value function: the agent performs the one action that maximizes the value function. In model-based RL, however, the reward estimate is only one component responsible for a particular policy."}, {"heading": "4.2 MODEL FUNCTION", "text": "The environment can be modeled by approximating the transition function for observable states. Transition probabilities are increased for actually observed transitions, see eq. (12), and decreased for all other transitions, see eq. (13).\nT (st\u22121, at\u22121, st)\u2190 T (st\u22121, at\u22121, st) + \u03b1(1\u2212 T (st\u22121, at\u22121, st)) (12) \u2200s\u2032 \u2208 S \u2227 s\u2032 6= st.T (st\u22121, at\u22121, s\u2032)\u2190 T (st\u22121, at\u22121, s\u2032) + \u03b1(0\u2212 T (st\u22121, at\u22121, s\u2032)) (13)\nEstimates of state transitions enable to plan behavior. Traditionally, transition functions realize models that enable to estimate transition probabilities from one sensor state via action to another sensor state. They are usually not combined with value functions that provide estimates of cumulative reward, because the expected cumulative reward can be \u201csimulated\u201d with an appropriate model. Equation (14) shows how model-based approaches rather estimate the immediate reward of perception-action pairs.\nR(st, at)\u2190 R(st, at) + \u03b1(rt \u2212R(st, at)) (14)\nGiven a comprehensive model of state transitions and expected rewards, model-based reinforcement agents are able to chose the action that maximizes the sum of expected rewards weighted by the probability of expected future states. The probability of future states is estimated by the model function. By recursively retrieving the estimated value V of potential successor states, model-based agents are able to determine values that take states into account which are arbitrarily far in the future.\nV (s, a) = R(s, a) + \u03b3 S\u2211 s\u2032 T (s, a, s\u2032) A max a\u2032 V (s\u2032, a\u2032) (15)\nThe recursive call in eq. (15) enables to use an existing model for estimating future cumulative reward, instead of visiting each state several times to cover all of its possible consequences like in in eq. (9) of model-free RL.\nThe downside is that a recursion necessitates the definition of a termination condition. Introducing conditions always comes at the risk of corrupting universality. Secondly, the value function has to be evaluated several times in each iteration and, lastly but most importantly, an approximately correct model of the environment needs to be already available to estimate the value of states.\nBy incorporating the iterative value update in eq. (11), we are able to avoid a potentially expensive recursion. Our architecture tries to exploit both: a model of the environment and an estimate of the long-term attractiveness of a state. With each query it considers only those sensorimotor states that the model deems possible. Among those states, the value function enables to choose the most promising ones.\nOur approximation of the model function can be seen in the following. The value of s indicates whether the environment accepted the last query qt\u22121 \u2208 SM or not (i.e., whether qt\u22121 is identical to the current state xt). It follows that the \u201cinducibility\u201d of query q from sensorimotor state x can be determined by eq. (16).\nIl(xt\u22121, qt\u22121)\u2190 Il(xt\u22121, qt\u22121) + \u03b1(s\u2212 Il(xt\u22121, qt\u22121)) s = {\n1, if qt\u22121 = xt 0, otherwise\n(16)"}, {"heading": "4.3 ACTION SELECTION", "text": "If the value function enables to evaluate perception-action tuples, then action selection simply returns the action a from the a tuple that maximizes the estimated value among all perception-action tuples. Drawbacks are costs linear to the sum of the number of perceptions and the number of actions. Equation (17) shows the decision function in conventional model-based and model-free RL during perception st.\nD(st) = a V (st, a) = S\u00d7A max st,a\u2032 V (st, a \u2032) (17)\nInstead of the probability of successor states, query selection must consider the \u201cinducibility\u201d of potential successor states. The approach also performs a simple maximization. Its specific form, however, is quite different from conventional action selection in RL. The decision function realizes query selection determined by the value function and the approximation of the model function.\nD(xt) = x Vl(x) = Xi\nmax x\u2032\nVl(x \u2032), \u2200xi \u2282 Xi. I(xt, xi) \u2265 c c \u2208 [0, 1] (18)\nEquation (18) enables to select sensorimotor and latent states alike. The state x can either be a sensorimotor or a latent state. For each query, first, the set of most inducible states with a certainty above c is selected. From this set, a query is selected that maximizes the value function. Both parts of the current policy \u03c0, value function V and model function M , are determined by the currently active latent state l. This latent state, on the other hand, is subject to simple QPs, just as low-level sensorimotor states are. From the explications above the architecture in fig. 2 emerges."}, {"heading": "5 RESULTS", "text": "The more abstract representations get, the more blurry the line between action and perception becomes. In contrast to MDPs, QPs can be applied to abstract state transitions. By continuously performing queries for desirable states, an agent can learn transition probabilities between these states. Once the query function Q has been sufficiently approximated, the agent can choose those successor states that promise to maximize a normalized variant of expected future cumulative reward. This behavior effectively realizes a value optimizing policy in a particular QP.\nAlthough both, Markovian policy and query policy, converge in the small corridor environment from fig. 1a after 14 episodes, QPs take much longer to complete an episode.9 The performance of the\n9In all experiments we set \u03b1 = .5 and \u03b3 = .5 with an initial optimistic value of 5.\nabstract layer\nsmt\nht = [smt\u2212n, . . . , smt]\nlt lq\n\u03c0 = (Vl,M)\nsmq\nD1\nA\nD2 D2\nA\u2032\nD1\nbase layer\nFigure 2: Overview of base layer of the architecture and the recursive integration of further layers.\nS\nG\nFigure 3: Labyrinth environment.\n0 20 40 60 \u22123,000\n\u22122,000\n\u22121,000\n0\nobjective Markov subjective Markov\n(a) Performance of Markov policies.\n0 10 20 30\n\u2212400\n\u2212200\n0\nsubjective query optimal policy\n(b) Performance subjective query policy.\nFigure 4: Cumulative reward per episode in the labyrinth environment.\nquery policy suffers from condensing sensor and motor states. The decline in performance can be explained by the increased number of sensorimotor states. Whereas a Markovian model function needs to keep track of |S \u00d7 A \u00d7 S| entries (432 values in the small corridor environment with subjective interaction), the query model function needs to track 2 \u2217 |S \u00d7 A|2 entries (2592 values, respectively). The value function V in model-based Markovian policies contains |S| values (12 in our example), the value function in query policies contains |S \u00d7A| entries (36, respectively). Consider, however, that subjective policies do not grow with new objective input. As long as objectively unknown input can be interpreted as subjectively known, the policy size remains unchanged. Of course, this advantage of subjective interaction also holds for Markovian policies. As we have shown, however, the distinction between perception and action effectively prevents any straight-forward application of Markov policies to latent states. Any ambitions towards a hierarchy of representations with MDPs will first have to solve the problem of how to integrate perception and action.\nFigure 3 illustrates the labyrinth environment. Reward and interaction conditions are identical to the corridor setting. In contrast to the small corridor environment, however, the labyrinth cannot be solved by a Markovian policy given subjective interaction data. To reach the goal, an agent has to go straight at some crossroads but turn at others. Without further means of differentiation, subjective Markov policies can only perform one of both each time. Figure 4a shows that the labyrinth cannot be solved by MDPs with subjective interaction data.\nThe policy of the objective Markov agent converges at the optimum after roughly 140 episodes. Figure 4b shows that the policy of the subjective query agent converges at sub-optimal performance\nafter only 18 episodes. A query policy is able to differentiate the identical perceptions at crossroads according to the last performed action: it develops a strategy to solve the labyrinth.\nThe following policy is one of the successful, sub-optimal policies developed by a query agent. The policy has been condensed, such that the underlying search strategy becomes obvious. The conditionals to the left are sensorimotor states, consisting of the last action and the current perception.\n\u2022 (turn, a wall to the right): turn left\n\u2022 (turn, at a crossroad): move forward until one of the following holds.\n\u2022 (forward, at a crossroad): turn left\n\u2022 (forward, facing a wall): turn left\nBy storing the last motor activation, query policies with sensorimotor states realize a sort of short-term memory. Subjective single-layer query policies can solve tasks that cannot be solved by subjective single-layer Markovian policies. Such a \u201cgraceful decent\u201d of performance is highly desirable if the relevant states of the task are not known in advance. Even if the agent is not able to develop a policy that is optimal from an observer\u2019s point of view, it can still produce solutions that take its own limitations into account."}, {"heading": "6 FUTURE WORK", "text": "The greatest challenge for extruding a grounded query policy into a hierarchical architecture is to find an appropriate abstraction function A that reliably and repeatedly provides the same models given only linear histories of them. The fact that the dimensionality of the history is always smaller or equal to the dimensionality of the model complicates reliable comparisons. The history might not contain the information relevant for deciding whether is corresponds to one model or another.\nAdditional information needs to be acquired to resolve ambiguities. It is clear, however, that this information cannot be extracted from the structure of history or model. Representation learning might be able to generalize histories, but any generalization that considers only the structure of history and model is confronted with underdetermined exemplars: it cannot be decided which one of a structurally equally similar set of models a history belongs to. In cognitive science, a similar situation presents a problem in explaining the universality of human object recognition. It is known as the problem of vanishing intersections (Harnad, 1990).\nA hierarchical architecture like in fig. 2, however, can estimate the model of an observed history from information beyond sheer structure. The transitions of policies, and therefore the transitions of models, that are motivated by observing a particular history are at the same time abstract state transitions in the layer above. To be able to model and predict abstract state transitions (e.g. as QPs) implies the ability to model and predict lower policy transitions.\nThe top layer effectively provides prior probabilities for policy transitions. Even without any useful structural information (i.e., no significant intersection of history and model), the likelihood for a particular history for having been generated by a particular model can be acquired by previously experienced abstract state transitions. This realizes an inductive expectation bias.\nQPs provide the formal grounds for augmenting base layer query selection D1 with an unbounded number of more abstract and less frequent query selections Dm where m = [1,\u221e]. Layers grow as the acquired knowledge is no longer applicable in every situation. The general case, however, is to assume that every environment can be described in terms of a QP. No sooner than when this assumption is violated, new data structures (i.e., representations) are generated. Until then, available knowledge is transfered onto every objectively new situation per se."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Brainstorms: Philosophical essays on mind and psychology", "author": ["Dennett", "Daniel Clement"], "venue": "Number 8. MIT press,", "citeRegEx": "Dennett and Clement.,? \\Q1981\\E", "shortCiteRegEx": "Dennett and Clement.", "year": 1981}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Dietterich", "Thomas G"], "venue": "CoRR, cs.LG/9905014,", "citeRegEx": "Dietterich and G.,? \\Q1999\\E", "shortCiteRegEx": "Dietterich and G.", "year": 1999}, {"title": "An object-oriented representation for efficient reinforcement learning", "author": ["Diuk", "Carlos", "Cohen", "Andre", "Littman", "Michael L"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Diuk et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Diuk et al\\.", "year": 2008}, {"title": "A signal processing framework based on dynamic neural networks with application to problems in adaptation, filtering, and classification", "author": ["Feldkamp", "Lee A", "Puskorius", "Gintaras V"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Feldkamp et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Feldkamp et al\\.", "year": 1998}, {"title": "Using reinforcement learning to adapt an imitation task", "author": ["Guenter", "Florent", "Billard", "Aude G"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "Guenter et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Guenter et al\\.", "year": 2007}, {"title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning", "author": ["Guo", "Xiaoxiao", "Singh", "Satinder", "Lee", "Honglak", "Lewis", "Richard L", "Wang", "Xiaoshi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "The symbol grounding problem", "author": ["Harnad", "Stevan"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad and Stevan.,? \\Q1990\\E", "shortCiteRegEx": "Harnad and Stevan.", "year": 1990}, {"title": "Discovering hierarchy in reinforcement learning with hexq", "author": ["Hengst", "Bernhard"], "venue": "In ICML,", "citeRegEx": "Hengst and Bernhard.,? \\Q2002\\E", "shortCiteRegEx": "Hengst and Bernhard.", "year": 2002}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Kaelbling", "Leslie Pack", "Littman", "Michael L", "Cassandra", "Anthony R"], "venue": "Artificial intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Reinforcement learning for robots using neural networks", "author": ["Lin", "Long-Ji"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Lin and Long.Ji.,? \\Q1993\\E", "shortCiteRegEx": "Lin and Long.Ji.", "year": 1993}, {"title": "Automatic discovery of subgoals in reinforcement learning using diverse density", "author": ["McGovern", "Amy", "Barto", "Andrew G"], "venue": "Computer Science Department Faculty Publication Series, pp", "citeRegEx": "McGovern et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 2001}, {"title": "Roles of macro-actions in accelerating reinforcement learning", "author": ["McGovern", "Amy", "Sutton", "Richard S", "Fagg", "Andrew H"], "venue": "In Grace Hopper celebration of women in computing,", "citeRegEx": "McGovern et al\\.,? \\Q1997\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 1997}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Graves", "Alex", "Antonoglou", "Ioannis", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Identifying hierarchical structure in sequences: A linear-time algorithm", "author": ["Nevill-Manning", "Craig G", "Witten", "Ian H"], "venue": "CoRR, cs.AI/9709102,", "citeRegEx": "Nevill.Manning et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Nevill.Manning et al\\.", "year": 1997}, {"title": "On-line Q-learning using connectionist systems", "author": ["Rummery", "Gavin A", "Niranjan", "Mahesan"], "venue": "University of Cambridge, Department of Engineering,", "citeRegEx": "Rummery et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Rummery et al\\.", "year": 1994}, {"title": "An on-line algorithm for dynamic reinforcement learning and planning in reactive environments", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1990\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1990}, {"title": "Deep learning in neural networks: An overview", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1404.7828,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2014\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2014}, {"title": "Minds, brains, and programs", "author": ["Searle", "John R"], "venue": "Behavioral and brain sciences,", "citeRegEx": "Searle and R.,? \\Q1980\\E", "shortCiteRegEx": "Searle and R.", "year": 1980}, {"title": "Convergence results for single-step on-policy reinforcement-learning algorithms", "author": ["Singh", "Satinder", "Jaakkola", "Tommi", "Littman", "Michael L", "Szepesv\u00e1ri", "Csaba"], "venue": "Machine Learning,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Finding structure in reinforcement learning", "author": ["Thrun", "Sebastian", "Schwartz", "Anton"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Thrun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1995}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "Christopher John Cornish Hellaby"], "venue": "PhD thesis, King\u2019s College,", "citeRegEx": "Watkins and Hellaby.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and Hellaby.", "year": 1989}], "referenceMentions": [{"referenceID": 9, "context": "(Kaelbling et al., 1998) POMDPs can describe a plethora of tasks for RL agents.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Bengio et al. (2013) mention the necessary trade-off between the \u201crichness\u201d of representations and the effort necessary to process them.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2013) mention the necessary trade-off between the \u201crichness\u201d of representations and the effort necessary to process them. Diuk et al. (2008) show that the selection of representations has immense influence on the performance of RL algorithms.", "startOffset": 0, "endOffset": 156}, {"referenceID": 13, "context": ", a policy) go back at least to the early 90ies (Schmidhuber, 1990; Schmidhuber, 1991; Feldkamp & Puskorius, 1998), but only recently, RL methods have been presented that are efficiently able to generate deep policies from scratch (Mnih et al., 2013; Guo et al., 2014).", "startOffset": 231, "endOffset": 268}, {"referenceID": 6, "context": ", a policy) go back at least to the early 90ies (Schmidhuber, 1990; Schmidhuber, 1991; Feldkamp & Puskorius, 1998), but only recently, RL methods have been presented that are efficiently able to generate deep policies from scratch (Mnih et al., 2013; Guo et al., 2014).", "startOffset": 231, "endOffset": 268}, {"referenceID": 12, "context": "Therefore, semi MDPs are frequently used to model such time-sensitivity (Thrun & Schwartz, 1995; McGovern et al., 1997; Dietterich, 1999; McGovern & Barto, 2001; Hengst, 2002).", "startOffset": 72, "endOffset": 175}, {"referenceID": 20, "context": "This generalization of MDPs extends the model of state transitions by duration as proposed by Sutton et al. (1999). The structure of hierarchical models, however, is mostly trained: stochastic information is integrated into a fixed hierarchical structure.", "startOffset": 94, "endOffset": 115}], "year": 2014, "abstractText": "Methods of deep machine learning enable to to reuse low-level representations efficiently for generating more abstract high-level representations. Originally, deep learning has been applied passively (e.g., for classification purposes). Recently, it has been extended to estimate the value of actions for autonomous agents within the framework of reinforcement learning (RL). Explicit models of the environment can be learned to augment such a value function. Although \u201cflat\u201d connectionist methods have already been used for model-based RL, up to now, only modelfree variants of RL have been equipped with methods from deep learning. We propose a variant of deep model-based RL that enables an agent to learn arbitrarily abstract hierarchical representations of its environment. In this paper, we present research on how such hierarchical representations can be grounded in sensorimotor interaction between an agent and its environment.", "creator": "LaTeX with hyperref package"}}}