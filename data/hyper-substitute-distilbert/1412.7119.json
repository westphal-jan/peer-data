{"id": "1412.7119", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Pragmatic Neural Language Modelling in Machine Translation", "abstract": "this seminar exhibits explicit in - mind investigation on several neural language models past translation systems. involving neural language models is a difficult problem, finding powerful given real - world applications. this part evaluates the outlook on end - meet - end rehearsal quality representing both present and existing scaling contexts. we show when explicitly assuming neural models is necessary and what optimisation values one should use in such scenarios. we subsequently understand between scalable training algorithms and eliminating noise contrastive criteria and other contexts affecting areas requiring further data improvements. we explore the break - offs between neural problems and jump - off n - gram analyses and realize that neural models make strong connections to natural design explanations in memory constrained environments, these uniquely identify that traditional models in raw translation model. we conclude gives a set of recommendations one should recover from reach a scalable neural language equation below mt.", "histories": [["v1", "Mon, 22 Dec 2014 20:08:06 GMT  (448kb,D)", "https://arxiv.org/abs/1412.7119v1", null], ["v2", "Tue, 23 Dec 2014 02:17:28 GMT  (449kb,D)", "http://arxiv.org/abs/1412.7119v2", null], ["v3", "Fri, 20 Mar 2015 17:20:03 GMT  (218kb,D)", "http://arxiv.org/abs/1412.7119v3", "NAACL 2015"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["paul baltescu", "phil blunsom"], "accepted": true, "id": "1412.7119"}, "pdf": {"name": "1412.7119.pdf", "metadata": {"source": "CRF", "title": "Pragmatic Neural Language Modelling in Machine Translation", "authors": ["Paul Baltescu", "Phil Blunsom"], "emails": ["paul.baltescu@cs.ox.ac.uk", "phil.blunsom@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Language models are used in translation systems to improve the fluency of the output translations. The most popular language model implementation is a back-off n-gram model with Kneser-Ney smoothing (Chen and Goodman, 1999). Back-off n-gram models are conceptually simple, very efficient to construct and query, and are regarded as being extremely effective in translation systems.\nNeural language models are a more recent class of language models (Bengio et al., 2003) that have been\nshown to outperform back-off n-gram models using intrinsic evaluations of heldout perplexity (Chelba et al., 2013; Bengio et al., 2003), or when used in addition to traditional models in natural language systems such as speech recognizers (Mikolov et al., 2011a; Schwenk, 2007). Neural language models combat the problem of data sparsity inherent to traditional n-gram models by learning distributed representations for words in a continuous vector space.\nIt has been shown that neural language models can improve translation quality when used as additional features in a decoder (Vaswani et al., 2013; Botha and Blunsom, 2014; Baltescu et al., 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al., 2013). These results show great promise and in this paper we continue this line of research by investigating the tradeoff between speed and accuracy when integrating neural language models in a decoder. We also focus on how effective these models are when used as the sole language model in a translation system. This is important because our hypothesis is that most of the language modelling is done by the n-gram model, with the neural model only acting as a differentiating factor when the n-gram model cannot provide a decisive probability. Furthermore, neural language models are considerably more compact and represent strong candidates for modelling language in memory constrained environments (e.g. mobile devices, commodity machines, etc.), where back-off n-gram models trained on large amounts of data do not fit into memory.\nOur results show that a novel combination of noise contrastive estimation (Mnih and Teh, 2012)\nar X\niv :1\n41 2.\n71 19\nv3 [\ncs .C\nL ]\n2 0\nM ar\nand factoring the softmax layer using Brown clusters (Brown et al., 1992) provides the most pragmatic solution for fast training and decoding. Further, we confirm that when evaluated purely on BLEU score, neural models are unable to match the benchmark Kneser-Ney models, even if trained with large hidden layers. However, when the evaluation is restricted to models that match a certain memory footprint, neural models clearly outperform the n-gram benchmarks, confirming that they represent a practical solution for memory constrained environments."}, {"heading": "2 Model Description", "text": "As a basis for our investigation, we implement a probabilistic neural language model as defined in Bengio et al. (2003).1 For every word w in the vocabulary V , we learn two distributed representations qw and rw in RD. The vector qw captures the syntactic and semantic role of the word w when w is part of a conditioning context, while rw captures its role as a prediction. For some word wi in a given corpus, let hi denote the conditioning context wi\u22121, . . . , wi\u2212n+1. To find the conditional probability P (wi|hi), our model first computes a context projection vector:\np = f n\u22121\u2211 j=1 Cjqhij  , where Cj \u2208 RD\u00d7D are context specific transformation matrices and f is a component-wise rectified\n1Our goal is to release a scalable neural language modelling toolkit at the following URL: http://www.example.com.\nlinear activation. The model computes a set of similarity scores measuring how well each word w \u2208 V matches the context projection of hi. The similarity score is defined as \u03c6(w, hi) = rTwp + bw, where bw is a bias term incorporating the prior probability of the word w. The similarity scores are transformed into probabilities using the softmax function:\nP (wi|hi) = exp(\u03c6(wi, hi))\u2211 w\u2208V exp(\u03c6(w, hi)) ,\nThe model architecture is illustrated in Figure 1. The parameters are learned with gradient descent to maximize log-likelihood with L2 regularization.\nScaling neural language models is hard because any forward pass through the underlying neural network computes an expensive softmax activation in the output layer. This operation is performed during training and testing for all contexts presented as input to the network. Several methods have been proposed to alleviate this problem: some applicable only during training (Mnih and Teh, 2012; Bengio and Senecal, 2008), while others may also speed up arbitrary queries to the language model (Morin and Bengio, 2005; Mnih and Hinton, 2009).\nIn the following subsections, we present several extensions to this model, all sharing the goal of reducing the computational cost of the softmax step. Table 1 summarizes the complexities of these methods during training and decoding."}, {"heading": "2.1 Class Based Factorisation", "text": "The time complexity of the softmax step is O(|V | \u00d7 D). One option for reducing this excessive amount of computation is to rely on a class based factorisation trick (Goodman, 2001). We partition the vocabulary into K classes {C1, . . . , CK} such that V = \u22c3K i=1 Ci and Ci \u2229 Cj = \u2205, \u22001 \u2264 i < j \u2264 K.\nWe define the conditional probabilities as:\nP (wi|hi) = P (ci|hi)P (wi|ci, hi),\nwhere ci is the class the word wi belongs to, i.e. wi \u2208 Cci . We adjust the model definition to also account for the class probabilities P (ci|hi). We associate a distributed representation sc and a bias term tc to every class c. The class conditional probabilities are computed reusing the projection vector p with a new scoring function \u03c8(c, hi) = sTc p + tc. The probabilities are normalised separately:\nP (ci|hi) = exp(\u03c8(ci, hi))\u2211K j=1 exp(\u03c8(cj , hi))\nP (wi|ci, hi) = exp(\u03c6(wi, hi))\u2211\nw\u2208Cci exp(\u03c6(w, hi))\nWhenK \u2248 \u221a |V | and the word classes have roughly equal sizes, the softmax step has a more manageable time complexity of O( \u221a |V | \u00d7D) for both training and testing."}, {"heading": "2.2 Tree Factored Models", "text": "One can take the idea presented in the previous section one step further and construct a tree over the vocabulary V . The words in the vocabulary are used to label the leaves of the tree. Let n1, . . . , nk be the nodes on the path descending from the root (n1) to the leaf labelled with wi (nk). The probability of the word wi to follow the context hi is defined as:\nP (wi|hi) = k\u220f\nj=2\nP (nj |n1, . . . , nj\u22121, hi).\nWe associate a distributed representation sn and bias term tn to each node in the tree. The conditional probabilities are obtained reusing the scoring function \u03c8(nj , hi): P (nj |n1, . . . , nj\u22121, hi) = exp(\u03c8(nj , hi))\u2211\nn\u2208S(nj) exp(\u03c8(n, hi)) ,\nwhere S(nj) is the set containing the siblings of nj and the node itself. Note that the class decomposition trick described earlier can be understood as a tree factored model with two layers, where the first layer contains the word classes and the second layer contains the words in the vocabulary.\nThe optimal time complexity is obtained by using balanced binary trees. The overall complexity of the normalisation step becomesO(log |V |\u00d7D) because the length of any path is bounded by O(log |V |) and because exactly two terms are present in the denominator of every normalisation operation.\nInducing high quality binary trees is a difficult problem which has received some attention in the research literature (Mnih and Hinton, 2009; Morin and Bengio, 2005). Results have been somewhat unsatisfactory, with the exception of Mnih and Hinton (2009), who did not release the code they used to construct their trees. In our experiments, we use Huffman trees (Huffman, 1952) which do not have any linguistic motivation, but guarantee that a minimum number of nodes are accessed during training. Huffman trees have depths that are close to log |V |."}, {"heading": "2.3 Noise Contrastive Estimation", "text": "Training neural language models to maximise data likelihood involves several iterations over the entire training corpus and applying the backpropagation algorithm for every training sample. Even with the previous factorisation tricks, training neural models is slow. We investigate an alternative approach for training language models based on noise contrastive estimation, a technique which does not require normalised probabilities when computing gradients (Mnih and Teh, 2012). This method has already been used for training neural language models for machine translation by Vaswani et al. (2013).\nThe idea behind noise contrastive training is to transform a density estimation problem into a classification problem, by learning a classifier to discriminate between samples drawn from the data distribution and samples drawn for a known noise distribution. Following Mnih and Teh (2012), we set the unigram distribution Pn(w) as the noise distribution and use k times more noise samples than data samples to train our models. The new objective is:\nJ(\u03b8) = m\u2211 i=1 logP (C = 1|\u03b8, wi, hi)\n+ m\u2211 i=1 k\u2211 j=1 logP (C = 0|\u03b8, nij , hi),\nwhere nij are the noise samples drawn from Pn(w). The posterior probability that a word is generated\nLanguage pairs # tokens # sentences\nfrom the data distribution given its context is:\nP (C = 1|\u03b8, wi, hi) = P (wi|\u03b8, hi)\nP (wi|\u03b8, hi) + kPn(wi) .\nMnih and Teh (2012) show that the gradient of J(\u03b8) converges to the gradient of the log-likelihood objective when k \u2192\u221e.\nWhen using noise contrastive estimation, additional parameters can be used to capture the normalisation terms. Mnih and Teh (2012) fix these parameters to 1 and obtain the same perplexities, thereby circumventing the need for explicit normalisation. However, this method does not provide any guarantees that the models are normalised at test time. In fact, the outputs may sum up to arbitrary values, unless the model is explicitly normalised.\nNoise contrastive estimation is more efficient than the factorisation tricks at training time, but at test time one still has to normalise the model to obtain valid probabilities. We propose combining this approach with the class decomposition trick resulting in a fast algorithm for both training and testing. In the new training algorithm, when we account for the class conditional probabilities P (ci|hi), we draw noise samples from the class unigram distribution, and when we account for P (wi|ci, hi), we sample from the unigram distribution of only the words in the class Cci ."}, {"heading": "3 Experimental Setup", "text": "In our experiments, we use data from the 2014 ACL Workshop in Machine Translation.2 We train standard phrase-based translation systems for French\u2192 English, English \u2192 Czech and English \u2192 German using the Moses toolkit (Koehn et al., 2007).\nWe used the europarl and the news commentary corpora as parallel data for training\n2The data is available here: http://www.statmt. org/wmt14/translation-task.html.\nthe translation systems. The parallel corpora were tokenized, lowercased and sentences longer than 80 words were removed using standard text processing tools.3 Table 2 contains statistics about the training corpora after the preprocessing step. We tuned the translation systems on the newstest2013 data using minimum error rate training (Och, 2003) and we used the newstest2014 corpora to report uncased BLEU scores averaged over 3 runs.\nThe monolingual training data used for training language models consists of the europarl, news commentary and the news crawl 2007-2013 corpora. The corpora were tokenized and lowercased using the same text processing scripts and the words not occuring the in the target side of the parallel data were replaced with a special <unk> token. Statistics for the monolingual data after the preprocessing step are reported in Table 3.\nThroughout this paper we report results for 5- gram language models, regardless of whether they are back-off n-gram models or neural models. To construct the back-off n-gram models, we used a compact trie-based implementation available in KenLM (Heafield, 2011), because otherwise we would have had difficulties with fitting these models in the main memory of our machines. When training neural language models, we set the size of the distributed representations to 500, we used diagonal context matrices and we used 10 negative samples for noise contrastive estimation, unless otherwise indicated. In cases where we perform experiments on only one language pair, the reader should assume we used French\u2192English data."}, {"heading": "4 Normalisation", "text": "The key challenge with neural language models is scaling the softmax step in the output layer of the\n3We followed the first two steps from http://www. cdec-decoder.org/guide/tutorial.html.\nnetwork. This operation is especially problematic when the neural language model is incorporated as a feature in the decoder, as the language model is queried several hundred thousand times for any sentence of average length.\nPrevious publications on neural language models in machine translation have approached this problem in two different ways. Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalisation when decoding, albeit Devlin et al. (2014) alter their training objective to learn self-normalised models, i.e. models where the sum of the values in the output layer is (hopefully) close to 1. Vaswani et al. (2013) use noise contrastive estimation to speed up training, while Devlin et al. (2014) train their models with standard gradient descent on a GPU.\nThe second approach is to explicitly normalise the models, but to limit the set of words over which the normalisation is performed, either via class-based factorisation (Botha and Blunsom, 2014; Baltescu et al., 2014) or using a shortlist containing only the most frequent words in the vocabulary and scoring the remaining words with a back-off n-gram model (Schwenk, 2010). Tree factored models follow the same general approach, but to our knowledge, they have never been investigated in a translation system before. These normalisation techniques can be successfully applied both when training the models and when using them in a decoder.\nTable 4 shows a side by side comparison of out of the box neural language models and back-off n-gram models. We note a significant drop in quality when neural language models are used (roughly 1.5 BLEU for fr\u2192en and en\u2192de and 0.5 BLEU for en\u2192 cs). This result is in line with Zhao et al. (2014) and shows that by default back-off n-gram models are much more effective in MT. An interesting observation is that the neural models have lower perplexities than the n-gram models, implying that BLEU scores\nand perplexities are only loosely correlated. Table 5 and Table 6 show the impact on translation quality for the proposed normalisation schemes with and without an additional n-gram model. We note that when KenLM is used, no significant differences are observed between normalised and unnormalised models, which is again in accordance with the results of Zhao et al. (2014). However, when the n-gram model is removed, class factored models perform better (at least for fr\u2192en and en\u2192de), despite being only an approximation of the fully normalised models. We believe this difference in not observed in the first case because most of the language modelling is done by the n-gram model (as indicated by the results in Table 4) and that the neural models only act as a differentiating feature when the n-gram models do not provide accurate probabilities. We conclude that some form of normalisation is likely to be necessary whenever neural models are used alone. This result may also explain why Zhao et al. (2014) show, perhaps surprisingly, that normalisation is important when reranking n-best lists with recurrent neural language models, but not in other cases. (This is the only scenario where they use neural models without supporting n-gram models.)\nTable 5 and Table 6 also show that tree factored models perform poorly compared to the other candidates. We believe this is likely to be a result of the artificial hierarchy imposed by the tree over the vocabulary.\nTable 7 compares two popular techniques for obtaining word classes: Brown clustering (Brown et al., 1992; Liang, 2005) and frequency binning (Mikolov et al., 2011b). From these results, we learn that the clustering technique employed to partition the vocabulary into classes can have a huge impact on translation quality and that Brown clustering is clearly superior to frequency binning.\nAnother thing to note is that frequency binning partitions the vocabulary in a similar way to Huffman encoding. This observation implies that the BLEU scores we report for tree factored models are not optimal, but we can get an insight on how much we expect to lose in general by imposing a tree structure over the vocabulary (on the fr\u2192en setup, we lose roughly 0.7 BLEU points). Unfortunately, we are not able to report BLEU scores for factored models using Brown trees because the time complexity for constructing such trees is O(|V |3).\nWe report the average time needed to decode a sentence for each of the models described in this paper in Table 8. We note that factored models are slow compared to unnormalised models. One option for speeding up factored models is using a GPU to perform the vector-matrix operations. However, GPU integration is architecture specific and thus against our goal of making our language modelling toolkit usable by everyone."}, {"heading": "5 Training", "text": "In this section, we are concerned with finding scalable training algorithms for neural language models. We investigate noise contrastive estimation as a much more efficient alternative to standard maximum likelihood training via stochastic gradient descent. Class factored models enable us to conduct this investigation at a much larger scale than previous results (e.g. the WSJ corpus used by Mnih and Teh (2012) has slightly over 1M tokens), thereby gaining useful insights on how this method truly performs at scale. (In our experiments, we use a 2B words corpus and a 100k vocabulary.) Table 9 summarizes our findings. We obtain a slightly better BLEU score with stochastic gradient descent, but this is likely to be just noise from tuning the translation system with MERT. On the other hand, noise contrastive training reduces training time by a factor of 7.\nTable 10 reviews the neural models described in this paper and shows the time needed to train each one. We note that noise contrastive training requires roughly the same amount of time regardless of the structure of the model. Also, we note that this method is at least as fast as maximum likelihood training even when the latter is applied to tree factored models. Since tree factored models have lower quality, take longer to query and do not yield any substantial benefits at training time when compared to unnormalised models, we conclude they represent a suboptimal language modelling choice\nfor machine translation."}, {"heading": "6 Diagonal Context Matrices", "text": "In this section, we investigate diagonal context matrices as a source for reducing the computational cost of calculating the projection vector. In the standard definition of a neural language model, this cost is dominated by the softmax step, but as soon as tricks like noise contrastive estimation or tree or class factorisations are used, this operation becomes the main bottleneck for training and querying the model. Using diagonal context matrices when computing the projection layer reduces the time complexity from O(D2) to O(D). A similar optimization is achieved in the backpropagation algorithm, as only O(D) context parameters need to be updated for every training instance.\nDevlin et al. (2014) also identified the need for finding a scalable solution for computing the projection vector. Their approach is to cache the product between every word embedding and every context matrix and to look up these terms in a table as needed. Devlin et al. (2014)\u2019s approach works well when decoding, but it requires additional memory and is not applicable during training.\nTable 11 compares diagonal and full context matrices for class factored models. Both models have similar BLEU scores, but the training time is reduced by a factor of 3 when diagonal context matrices are used. We obtain similar improvements when decoding with class factored models, but the speed up for unnormalised models is over 100x!"}, {"heading": "7 Quality vs. Memory Trade-off", "text": "Neural language models are a very appealing option for natural language applications that are expected to run on mobile phones and commodity computers, where the typical amount of memory available is limited to 1-2 GB. Nowadays, it is becom-\ning more and more common for these devices to include reasonably powerful GPUs, supporting the idea that further scaling is possible if necessary. On the other hand, fitting back-off n-gram models on such devices is difficult because these models store the probability of every n-gram in the training data. In this section, we seek to gain further understanding on how these models perform under such conditions.\nIn this analysis, we used Heafield (2011)\u2019s triebased implementation with quantization for constructing memory efficient back-off n-gram models. A 5-gram model trained on the English monolingual data introduced in section 3 requires 12 GB of memory. We randomly sampled sentences with an acceptance ratio ranging between 0.01 and 1 to construct smaller models and observe their performance on a larger spectrum. The BLEU scores obtained using these models are reported in Figure 2. We note that the translation quality improves as the amount of training data increases, but the improvements are less significant when most of the data is used.\nThe neural language models we used to report results throughout this paper are roughly 400 MB in size. Note that we do not use any compression techniques to obtain smaller models, although this is technically possible (e.g. quantization). We are interested to see how these models perform for various memory thresholds and we experiment with setting the size of the word embeddings between 100\nand 5000. More importantly, these experiments are meant to give us an insight on whether very large neural language models have any chance of achieving the same performance as back-off n-gram models in translation tasks. A positive result would imply that significant gains can be obtained by scaling these models further, while a negative result signals a possible inherent inefficiency of neural language models in MT. The results are shown in Figure 2.\nFrom Figure 2, we learn that neural models perform significantly better (over 1 BLEU point) when there is under 1 GB of memory available. This is exactly the amount of memory generally available on mobile phones and ordinary computers, confirming the potential of neural language models for applications designed to run on such devices. However, at the other end of the scale, we can see that back-off models outperform even the largest neural language models by a decent margin and we can expect only modest gains if we scale these models further."}, {"heading": "8 Conclusion", "text": "This paper presents an empirical analysis of neural language models in machine translation. The experiments presented in this paper help us draw several useful conclusions about the ideal usage of these language models in MT systems.\nThe first problem we investigate is whether normalisation has any impact on translation quality and we survey the effects of some of the most frequently used techniques for scaling neural language models. We conclude that normalisation is not necessary when neural models are used in addition to back-off n-gram models. This result is due to the fact that most of the language modelling is done by the ngram model. (Experiments show that out of the box n-gram models clearly outperform their neural counterparts.) The MT system learns a smaller weight for neural models and we believe their main use is to correct the inaccuracies of the n-gram models.\nOn the other hand, when neural language models are used in isolation, we observe that normalisation does matter. We believe this result generalizes to other neural architectures such as neural translation models (Sutskever et al., 2014; Cho et al., 2014). We observe that the most effective normalisation strategy in terms of translation quality is the class-based\ndecomposition trick. We learn that the algorithm used for partitioning the vocabulary into classes has a strong impact on the overall quality and that Brown clustering (Brown et al., 1992) is a good choice. Decoding with class factored models can be slow, but this issue can be corrected using GPUs, or if a comprise in quality is acceptable, unnormalised models represent a much faster alternative. We also conclude that tree factored models are not a strong candidate for translation since they are outperformed by unnormalised models in every aspect.\nWe introduce noise contrastive estimation for class factored models and show that it performs almost as well as maximum likelihood training with stochastic gradient descent. To our knowledge, this is the first side by side comparison of these two techniques on a dataset consisting of a few billions of training examples and a vocabulary with over 100k tokens. On this setup, noise contrastive estimation can be used to train standard or class factored models in a little over 1 day.\nWe explore diagonal context matrices as an optimization for computing the projection layer in the neural network. The trick effectively reduces the time complexity of this operation from O(D2) to O(D). Compared to Devlin et al. (2014)\u2019s approach of caching vector-matrix products, diagonal context matrices are also useful for speeding up training and do not require additional memory. Our experiments show that diagonal context matrices perform just as well as full matrices in terms of translation quality.\nWe also explore the trade-off between neural language models and back-off n-gram models. We observe that in the memory range that is typically available on a mobile phone or a commodity computer, neural models outperform n-gram models with more than 1 BLEU point. On the other hand, when memory is not a limitation, traditional n-gram models outperform even the largest neural models by a sizable margin (over 0.5 BLEU in our experiments).\nOur work is important because it reviews the most important scaling techniques used in neural language modelling for MT. We show how these methods compare to each other and we combine them to obtain neural models that are fast to both train and test. We conclude by exploring the strengths and weaknesses of these models into greater detail."}, {"heading": "Acknowledgments", "text": "This work was supported by a Xerox Foundation Award and EPSRC grant number EP/K036580/1."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Oxlm: A neural language modelling framework for machine translation", "author": ["Paul Baltescu", "Phil Blunsom", "Hieu Hoang"], "venue": "The Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Baltescu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baltescu et al\\.", "year": 2014}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Yoshua Bengio", "Jean-Sbastien Senecal"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio and Senecal.,? \\Q2008\\E", "shortCiteRegEx": "Bengio and Senecal.", "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A. Botha", "Phil Blunsom"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML", "citeRegEx": "Botha and Blunsom.,? \\Q2014\\E", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "Class-based n-gram models of natural language", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn"], "venue": null, "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "Chen and Goodman.,? \\Q1999\\E", "shortCiteRegEx": "Chen and Goodman.", "year": 1999}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Classes for fast maximum entropy training", "author": ["Joshua Goodman"], "venue": null, "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Kenlm: Faster and smaller language model queries", "author": ["Kenneth Heafield"], "venue": "In Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT", "citeRegEx": "Heafield.,? \\Q2011\\E", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "A method for the construction of minimum-redundancy codes", "author": ["David A. Huffman"], "venue": "Proceedings of the Institute of Radio Engineers,", "citeRegEx": "Huffman.,? \\Q1952\\E", "shortCiteRegEx": "Huffman.", "year": 1952}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology,", "citeRegEx": "Liang.,? \\Q2005\\E", "shortCiteRegEx": "Liang.", "year": 2005}, {"title": "Strategies for training large scale neural network language models", "author": ["Tomas Mikolov", "Anoop Deoras", "Daniel Povey", "Lukas Burget", "Jan Cernocky"], "venue": "In Proceedings of the 2011 Automatic Speech", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom Mikolov", "Stefan Kombrink", "Luk Burget", "Jan ernock", "Sanjeev Khudanpur"], "venue": "In Proceedings of the 2011 IEEE International Conference on Acoustics,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Computer Speech & Language,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "Continuous-space language models for statistical machine translation", "author": ["Holger Schwenk"], "venue": "Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Schwenk.,? \\Q2010\\E", "shortCiteRegEx": "Schwenk.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "The most popular language model implementation is a back-off n-gram model with Kneser-Ney smoothing (Chen and Goodman, 1999).", "startOffset": 100, "endOffset": 124}, {"referenceID": 3, "context": "Neural language models are a more recent class of language models (Bengio et al., 2003) that have been shown to outperform back-off n-gram models using intrinsic evaluations of heldout perplexity (Chelba et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 6, "context": ", 2003) that have been shown to outperform back-off n-gram models using intrinsic evaluations of heldout perplexity (Chelba et al., 2013; Bengio et al., 2003), or when used in addition to traditional models in natural language systems such as speech recognizers (Mikolov et al.", "startOffset": 116, "endOffset": 158}, {"referenceID": 3, "context": ", 2003) that have been shown to outperform back-off n-gram models using intrinsic evaluations of heldout perplexity (Chelba et al., 2013; Bengio et al., 2003), or when used in addition to traditional models in natural language systems such as speech recognizers (Mikolov et al.", "startOffset": 116, "endOffset": 158}, {"referenceID": 17, "context": ", 2003), or when used in addition to traditional models in natural language systems such as speech recognizers (Mikolov et al., 2011a; Schwenk, 2007).", "startOffset": 111, "endOffset": 149}, {"referenceID": 20, "context": "It has been shown that neural language models can improve translation quality when used as additional features in a decoder (Vaswani et al., 2013; Botha and Blunsom, 2014; Baltescu et al., 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al.", "startOffset": 124, "endOffset": 214}, {"referenceID": 4, "context": "It has been shown that neural language models can improve translation quality when used as additional features in a decoder (Vaswani et al., 2013; Botha and Blunsom, 2014; Baltescu et al., 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al.", "startOffset": 124, "endOffset": 214}, {"referenceID": 1, "context": "It has been shown that neural language models can improve translation quality when used as additional features in a decoder (Vaswani et al., 2013; Botha and Blunsom, 2014; Baltescu et al., 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al.", "startOffset": 124, "endOffset": 214}, {"referenceID": 18, "context": ", 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al., 2013).", "startOffset": 65, "endOffset": 99}, {"referenceID": 0, "context": ", 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al., 2013).", "startOffset": 65, "endOffset": 99}, {"referenceID": 5, "context": "and factoring the softmax layer using Brown clusters (Brown et al., 1992) provides the most pragmatic solution for fast training and decoding.", "startOffset": 53, "endOffset": 73}, {"referenceID": 3, "context": "As a basis for our investigation, we implement a probabilistic neural language model as defined in Bengio et al. (2003).1 For every word w in the vocabulary V , we learn two distributed representations qw and rw in RD.", "startOffset": 99, "endOffset": 120}, {"referenceID": 2, "context": "Several methods have been proposed to alleviate this problem: some applicable only during training (Mnih and Teh, 2012; Bengio and Senecal, 2008), while others may also speed up arbitrary queries to the language model (Morin and Bengio, 2005; Mnih and Hinton, 2009).", "startOffset": 99, "endOffset": 145}, {"referenceID": 15, "context": "Several methods have been proposed to alleviate this problem: some applicable only during training (Mnih and Teh, 2012; Bengio and Senecal, 2008), while others may also speed up arbitrary queries to the language model (Morin and Bengio, 2005; Mnih and Hinton, 2009).", "startOffset": 218, "endOffset": 265}, {"referenceID": 9, "context": "One option for reducing this excessive amount of computation is to rely on a class based factorisation trick (Goodman, 2001).", "startOffset": 109, "endOffset": 124}, {"referenceID": 15, "context": "Inducing high quality binary trees is a difficult problem which has received some attention in the research literature (Mnih and Hinton, 2009; Morin and Bengio, 2005).", "startOffset": 119, "endOffset": 166}, {"referenceID": 11, "context": "In our experiments, we use Huffman trees (Huffman, 1952) which do not have any linguistic motivation, but guarantee that a minimum number of nodes are accessed during training.", "startOffset": 41, "endOffset": 56}, {"referenceID": 14, "context": "Inducing high quality binary trees is a difficult problem which has received some attention in the research literature (Mnih and Hinton, 2009; Morin and Bengio, 2005). Results have been somewhat unsatisfactory, with the exception of Mnih and Hinton (2009), who did not release the code they used to construct their trees.", "startOffset": 120, "endOffset": 256}, {"referenceID": 20, "context": "This method has already been used for training neural language models for machine translation by Vaswani et al. (2013). The idea behind noise contrastive training is to transform a density estimation problem into a classification problem, by learning a classifier to discriminate between samples drawn from the data distribution and samples drawn for a known noise distribution.", "startOffset": 97, "endOffset": 119}, {"referenceID": 20, "context": "This method has already been used for training neural language models for machine translation by Vaswani et al. (2013). The idea behind noise contrastive training is to transform a density estimation problem into a classification problem, by learning a classifier to discriminate between samples drawn from the data distribution and samples drawn for a known noise distribution. Following Mnih and Teh (2012), we set the unigram distribution Pn(w) as the noise distribution and use k times more noise samples than data samples to train our models.", "startOffset": 97, "endOffset": 409}, {"referenceID": 16, "context": "We tuned the translation systems on the newstest2013 data using minimum error rate training (Och, 2003) and we used the newstest2014 corpora to report uncased BLEU scores averaged over 3 runs.", "startOffset": 92, "endOffset": 103}, {"referenceID": 10, "context": "To construct the back-off n-gram models, we used a compact trie-based implementation available in KenLM (Heafield, 2011), because otherwise we would have had difficulties with fitting these models in the main memory of our machines.", "startOffset": 104, "endOffset": 120}, {"referenceID": 20, "context": "Vaswani et al. (2013) and Devlin et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 20, "context": "Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalisation when decoding, albeit Devlin et al.", "startOffset": 0, "endOffset": 47}, {"referenceID": 20, "context": "Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalisation when decoding, albeit Devlin et al. (2014) alter their training objective to learn self-normalised models, i.", "startOffset": 0, "endOffset": 118}, {"referenceID": 20, "context": "Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalisation when decoding, albeit Devlin et al. (2014) alter their training objective to learn self-normalised models, i.e. models where the sum of the values in the output layer is (hopefully) close to 1. Vaswani et al. (2013) use noise contrastive estimation to speed up training, while Devlin et al.", "startOffset": 0, "endOffset": 291}, {"referenceID": 20, "context": "Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalisation when decoding, albeit Devlin et al. (2014) alter their training objective to learn self-normalised models, i.e. models where the sum of the values in the output layer is (hopefully) close to 1. Vaswani et al. (2013) use noise contrastive estimation to speed up training, while Devlin et al. (2014) train their models with standard gradient descent on a GPU.", "startOffset": 0, "endOffset": 373}, {"referenceID": 4, "context": "The second approach is to explicitly normalise the models, but to limit the set of words over which the normalisation is performed, either via class-based factorisation (Botha and Blunsom, 2014; Baltescu et al., 2014) or using a shortlist containing only the most frequent words in the vocabulary and scoring the remaining words with a back-off n-gram model (Schwenk, 2010).", "startOffset": 169, "endOffset": 217}, {"referenceID": 1, "context": "The second approach is to explicitly normalise the models, but to limit the set of words over which the normalisation is performed, either via class-based factorisation (Botha and Blunsom, 2014; Baltescu et al., 2014) or using a shortlist containing only the most frequent words in the vocabulary and scoring the remaining words with a back-off n-gram model (Schwenk, 2010).", "startOffset": 169, "endOffset": 217}, {"referenceID": 18, "context": ", 2014) or using a shortlist containing only the most frequent words in the vocabulary and scoring the remaining words with a back-off n-gram model (Schwenk, 2010).", "startOffset": 148, "endOffset": 163}, {"referenceID": 5, "context": "Table 7 compares two popular techniques for obtaining word classes: Brown clustering (Brown et al., 1992; Liang, 2005) and frequency binning (Mikolov et al.", "startOffset": 85, "endOffset": 118}, {"referenceID": 12, "context": "Table 7 compares two popular techniques for obtaining word classes: Brown clustering (Brown et al., 1992; Liang, 2005) and frequency binning (Mikolov et al.", "startOffset": 85, "endOffset": 118}, {"referenceID": 10, "context": "In this analysis, we used Heafield (2011)\u2019s triebased implementation with quantization for constructing memory efficient back-off n-gram models.", "startOffset": 26, "endOffset": 42}, {"referenceID": 19, "context": "We believe this result generalizes to other neural architectures such as neural translation models (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 99, "endOffset": 141}, {"referenceID": 8, "context": "We believe this result generalizes to other neural architectures such as neural translation models (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 99, "endOffset": 141}, {"referenceID": 5, "context": "We learn that the algorithm used for partitioning the vocabulary into classes has a strong impact on the overall quality and that Brown clustering (Brown et al., 1992) is a good choice.", "startOffset": 147, "endOffset": 167}], "year": 2015, "abstractText": "This paper presents an in-depth investigation on integrating neural language models in translation systems. Scaling neural language models is a difficult task, but crucial for real-world applications. This paper evaluates the impact on end-to-end MT quality of both new and existing scaling techniques. We show when explicitly normalising neural models is necessary and what optimisation tricks one should use in such scenarios. We also focus on scalable training algorithms and investigate noise contrastive estimation and diagonal contexts as sources for further speed improvements. We explore the tradeoffs between neural models and back-off ngram models and find that neural models make strong candidates for natural language applications in memory constrained environments, yet still lag behind traditional models in raw translation quality. We conclude with a set of recommendations one should follow to build a scalable neural language model for MT.", "creator": "LaTeX with hyperref package"}}}