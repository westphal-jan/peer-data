{"id": "1412.7156", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Representation Learning for cold-start recommendation", "abstract": "a graphical approach to context filtering ( cf ), i. e. prediction of user ratings the scale, deals without additive factorization techniques. results for candidate responses and items are sourced from specific observed ratings once scaled for evaluation. unfortunatly, these graphical approaches deliberately handle the quantity of new users arriving in the system, or yet known rating, a problem formulated as user frost - start. ultimate common hazard in supporting one is to reach these incoming users satisfying a high initialization ratings. although example presents a protocol must tackle this scaling deficiency including ( i ) finding attractive questions to ask, ( d ) building complex representations onto that small chosen given information. newer solution can occasionally provide used in a more standard ( warm ) overview. our approach will evaluated on some classical cf problem and on the cold - start problem yielding four different datasets showing limited ability to improve baseline performance approaching both settings.", "histories": [["v1", "Mon, 22 Dec 2014 21:58:06 GMT  (80kb,D)", "https://arxiv.org/abs/1412.7156v1", null], ["v2", "Fri, 27 Feb 2015 18:56:23 GMT  (392kb,D)", "http://arxiv.org/abs/1412.7156v2", null], ["v3", "Fri, 27 Mar 2015 09:59:25 GMT  (392kb,D)", "http://arxiv.org/abs/1412.7156v3", "Accepted as workshop contribution at ICLR 2015"], ["v4", "Wed, 8 Apr 2015 15:37:19 GMT  (392kb,D)", "http://arxiv.org/abs/1412.7156v4", "Accepted as workshop contribution at ICLR 2015"], ["v5", "Mon, 22 Jun 2015 14:01:33 GMT  (392kb,D)", "http://arxiv.org/abs/1412.7156v5", "Accepted as workshop contribution at ICLR 2015"]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["gabriella contardo", "ludovic denoyer", "thierry artieres"], "accepted": true, "id": "1412.7156"}, "pdf": {"name": "1412.7156.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Gabriella Contardo", "Ludovic Denoyer", "Thierry Arti\u00e8res"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "Most of the successful machine learning algorithms rely on data representation, i.e a way to disentangle and extract useful information from the data, which will help the model in its objective task. As highlighted by Bengio et al. (2013), designing models able to learn these representations from (raw) data instead of manual pre-processing seems crucial to go further in Artificial Intelligence, and representation learning has gain a surge of interest in machine learning. In parallel, recommender systems have became an active field of research and are now used in an increasing variety of applications, such as e-commerce, social networks or participative platforms. They aim to suggest the most relevant items (e.g products) to each user, in order to facilitate their experience. To recommend such relevant items, recommender systems can rely on different types of data, such as users\u2019 explicit and/or implicit feedbacks (e.g rating a movie on a scale of stars, buying an item or listening to a song), or informative features about users (age, post code) or items (type of movie, actors). One of the most common approach to recommendation is Collaborative Filtering (CF) which consists in making recommendation only based on the ratings provided by users over a set of items (i.e without using any additional features).\nWithin CF context, a popular and efficient family of methods are Latent Factor Models, which rely on matrix factorization-based techniques 1. These approaches treat the recommender problem as a representation learning one, by computing representations for users and items in a common latent space. More formally, let us consider a set U of U known users and a set I of I items. Let ru,i denote the rating of user u \u2208 U for item i \u2208 I. A rating is usually a discrete value between 1 and 5, that can be binarized (-1/1) with a proper threshold (often 3). The U \u00d7 I matrix R = {ru,i} is the rating matrix which is incomplete since all ratings are not known. We will denote O the set of observed pairs (u, i) such that a rating on item i has been made by user u. Let us denote N the dimension of the latent representation space of users and items, pu \u2208 RN being the (learned) representation of user u and qi \u2208 RN denoting the (learned) representation of item i. Given these representations, classical approaches are able to compute missing ratings made by a user u over an item i as the dot product between pu and qi. In other words, the more similar the user and the item representations are, the higher the predicted rating will be. Let us denote r\u0303u,i this predicted rating, we have:\nr\u0303u,i = q T i pu (1)\n1Other families of approaches are detailed in Section 4.\nar X\niv :1\n41 2.\n71 56\nv5 [\ncs .I\nR ]\n2 2\nJu n\n20 15\nThe representation pu and qi are usually learned on the sparse input rating matrixR by minimizing an objective loss function over L(p,q) which measures the difference between observed ratings ru,i and predicted ones. p is the set of users representations, q being the representation of items. The loss is usually defined as a L2 objective:\nL(p,q) = \u2211\n(u,i)\u2208O (ru,i \u2212 qTi pu)2 + \u03bb( \u2211 i ||qi||2 + \u2211 u ||pu||2) (2)\nThe coefficient \u03bb is a manually defined regularization coefficient. This loss corresponds to a matrix decomposition in latent factors and different optimization algorithms have been proposed as alternated least squares or stochastic gradient descent (Koren et al. (2009)). Note that this models is a transductive model since it allows one to compute representations over a set of a priori known users and items.\nThe transductive nature of Matrix Factorization approaches makes them well adapted when the sets of users and of items are fixed. Yet in practical applications, new items and new users regularly appear in the system. This requires often retraining the whole system which is time consuming and also makes the system behavior unstable. Furthermore, one main limitation of transductive Matrix Factorization approaches is that they strongly rely on a certain amount of data to build relevant representations, e.g. one must have enough ratings from a new user to construct an accurate representation. Indeed, facing new users, MF methods (and more generally CF-based approaches) have to wait for this user to interact with the system and to provide ratings before being able to make recommendations for this user. These methods are thus not well-suited to propose recommendation at the beginning of the process.\nWe propose to focus on the user cold-start problem2 by interview method which consists in building a set of items on which ratings are asked to any new user. Then, recommendations are made based on this list of (incomplete) ratings. We consider a representation-learning approach which is an original approach in this context and which simultaneously learns which items to use in the interview, but also how to use these ratings for building relevant user representations. Our method is based on an inductive model whose principle is to code ratings on items as translations in the latent representation space, allowing to easily integrate different opinions at a low computational cost. The contributions of this paper are thus the following: (i) We propose a generic representation-learning formalism for user cold-start recommendation. This formalism integrates the representation building function as part of the objective loss, and restriction over the number of items to consider in the interview process. (ii) We present a particular representation-learning model called Inductive Additive Model (IAM) which is based on simple assumptions about the nature of users\u2019 representations to build and that we are able to optimize using classical gradient-descent algorithms. (iii) We perform experiments on four datasets in the classical CF context as well as in the user cold-start context. Quantitative results show the effectiveness of our approach in both contexts while qualitative results show the relevancy of learned representations.\nThe paper is organized as follow: in Section 2, we propose the generic formulation of the representation learning problem for user cold-start, and the particular instance of model we propose. The Section 3 presents the experiments and Section 4 discusses the related work in the collaborative filtering domain. Section 5 proposes perspectives to this contribution."}, {"heading": "2 PROPOSED APPROACH", "text": "We now rewrite the objective function detailed in Equation 2 in a more general form that will allow us to integrate the user cold-start problem as a representation-learning problem. As seen above, we still consider that each item will have its own learned representation denoted qi \u2208 RN and focus on building a user representation. When facing any new user, our model will first collect a set of ratings by asking a set of queries during an interview process. This process is composed by a set of items3 that are selected during the training phase. For each item in the interview, the new user\n2The integration of new items which is less critical in practical applications is not the focus of this paper but is discussed in the conclusion.\n3The article focuses on a static interview process i.e interview where the set of items is the same for all incoming users. A discussion on that point is provided in Section 4\ncan provide a rating, but can also choose not to provide this rating when he has no opinion. This is typically the case for example with recommendation of movies, where users are only able to provide ratings on movies they have seen. The model will thus have to both select relevant items to include in the interview, but also to learn how (incomplete) collected ratings will be used to build a user representation. Let us denote Q \u2282 I the subset of items that will be used in the interview. The representation of a new incoming user uwill thus depend on the ratings of u overQ that we will note Q(u). This representation will be given by a function f\u03a8(Q(u)) whose parameters, to be optimized, are denoted \u03a8. These \u03a8 parameters are global, i.e shared by all users. The objective function of the cold-start problem (finding the parameters \u03a8, the items\u2019 representations and the interview questions conjointly) can then be written as:\nLcold(q,\u03a8,Q) = \u2211\n(u,i)\u2208O (ru,i\u2212qTi f\u03a8(Q(u)))2 +\u03bb1( \u2211 i ||qi||2 + \u2211 u ||f\u03a8(Q(u))||2)+\u03bb2#Q (3)\nThe difference between this loss and the classical CF loss is twofold: (i) first, the learned representations pu are not free parameters, but computed by using a parametric function f\u03a8(Q(u)), whose parameters \u03a8 are learned; (ii) the loss includes an additional term \u03bb2#Q which measures the balance between the quality of the prediction, and the size of the interview, #Q denoting the number of items of the interview; \u03bb1 and \u03bb2 are manually chosen hyper-parameters - by changing their values, the user can obtain more robust models, and models with more or less interview questions. Note that solving this problem aims at simultaneously learning the items representations, the set of items in the interview, and the parameters of the representation building function."}, {"heading": "2.1 INDUCTIVE ADDITIVE MODEL (IAM)", "text": "The generic formulation presented above cannot easily be optimized with any representation function. Particularly, the use of a transductive model in this context is not trivial and, when using MF-based approaches in that case, we only obtained very complex solutions with a high computation complexity. We thus need to use a more appropriate representation-learning function f\u03a8 that is described below. The Inductive Additive Model (IAM) is based on two simple ideas concerning the representation of users we want to build: (i) First, one has to be able to provide good recommendation to any user that does not provide ratings during the interview process Q. (ii) Second we want the user representation to be easily enriched as new ratings are available. This feature makes our approach suitable for the particular cold-start setting but also for the standard CF setting as well.\nBased on the first idea, IAM considers that any user without answers will be mapped to a representation denoted \u03a80 \u2208 RN . Moreover, the second idea naturally led us to build an additive model where a user representation is defined as a sum of the particular items representations. This means that providing a rating will yield a translation of the user representation in the latent space. This translation will depend on the item i but also on the rating value. This translation will be learned for each possible rating value and item and denoted \u03a8ri where r is the value of the rating. More precisely, in case of binary ratings like and dislike, the like over a particular item will correspond to a particular translation \u03a8+1i , and a dislike to the translation \u03a8 \u22121 i . The fact that the two rating values correspond to two different unrelated translations is interesting since, for some items, the dislike rating can provide no additional information represented by a null translation, while the like rating can be very informative, modifying the user representation - see Section 3 for a qualitative study over \u03a8. The resulting model f\u03a8 can thus be written as:\nf\u03a8(u,Q) = \u03a80 + \u2211\n(u,i)\u2208O/i\u2208Q\n\u03a8 ru,i i (4)\nwhere the set {(u, i) \u2208 O/i \u2208 Q} is the set of items selected in the interview on which user u has provided a rating."}, {"heading": "2.1.1 CONTINUOUS LEARNING PROBLEM", "text": "Now, let us describe how the objective function described in Equation 3 with IAM model described in Equation 4 can be optimized. The optimization problem consisting in minimizing Lcold(q,\u03a8,Q) over q,\u03a8 and Q is a combinatorial problem since Q is a subset of the items. This combinatorial nature prevents us from using classical optimization methods such as gradient-descent methods and\ninvolves an intractable number of possible combinations of items. We propose to use a L1 relaxation in order to transform this problem in a continuous one. Let us denote \u03b1 \u2208 RI a weight vector, one weight per item, such that if \u03b1i = 0 then item i will not be in the interview. The cold-start loss can be rewritten with \u03b1\u2019s as:\nLcold(q,\u03a8, \u03b1) = \u2211\n(u,i)\u2208O\n(ru,i \u2212 qTi f\u03a8(u, \u03b1))2 + \u03bb|\u03b1| (5)\nNote that the L2 regularization term over the computed representation of users and items is removed here for sake of clarity. The representation of a user thus depends on the ratings made by this user for items i that have a non-null weight \u03b1i, restricting our model to compute its prediction on a subset of items which compose the interview. If we rewrite the proposed model as:\nf\u03a8(u, \u03b1) = \u03a80 + \u2211\n(u,i)\u2208O\n\u03b1i\u03a8 ru,i i (6)\nthen we obtain the following loss function: Lcold(q,\u03a8, \u03b1) = \u2211\n(u,i)\u2208O\n(ru,i \u2212 qTi (\u03a80 + \u2211\n(u,i)\u2208O\n\u03b1i\u03a8 ru,i i )) 2 + \u03bb|\u03b1| (7)\nwhich is now continuous. Note that, in that case, the translation resulting from a rating over an item corresponds to \u03b1i\u03a8 ru,i i rather than to \u03a8 ru,i i .\nThis objective loss can be optimized by using stochastic gradient-descent methods. Since it contains a L1 term which is not derivable on all the points, we propose to use the same idea than proposed in Carpenter (2008) which consists in first making a gradient step without considering the L1 term, and then applying the L1 penalty to the weight to the extent that it does not change its sign. In other words, a weight \u03b1i is clipped when it crosses zero."}, {"heading": "2.2 IAM AND CLASSICAL COLLABORATIVE FILTERING", "text": "The IAM, which is particularly well-fitted for user cold-start recommendation, can also be used in the classical collaborative filtering problem, without constraining the set of items. In that case, the objective function can be written as:\nLwarm(q,\u03a8) = \u2211\n(u,i)\u2208O\n(ru,i \u2212 qTi (\u03a80 + \u2211\n(u,i)\u2208O\n\u03a8 ru,i i )) 2 (8)\nwhich can be easily optimized through gradient descent. This model is a simple alternative to matrix factorization-based approaches, which is also evaluated in the experimental section. This model have some nice properties in comparison to transductive techniques, mainly it can easily update users\u2019 representations when faced with new incoming ratings, but this is not the topic of this article."}, {"heading": "3 EXPERIMENTS", "text": "We evaluate our models on four benchmark datasets - Table 1a - of various size in terms of number of users, of items or regarding the sparsity of ratings. The datasets are classical datasets used in the literature (Zhou et al. (2011); Golbandi et al. (2010)). ML1M corresponds to the MovieLens 1 millon dataset and Yahoo corresponds to the Yahoo! Music benchmark. Flixter and Jester are\nclassical datasets. As our main goal is mainly to evaluate the quality of our approach in the context of new users arriving in the system, we define the following protocol in order to simulate a realistic interview process on incoming users, and to evaluate different models. We proceed as follow: (i) We randomly divide each dataset along users, to have a pool of training users denoted U train, composed of 50% of the users of the complete dataset, on which we learn our model. The remaining users are split in two sets ( representing each 25% of initial users) for validation and testing. The interview process will be applied on each of these two subsets. (ii) The U test and Uvalid sets are then randomly split in two subsets of ratings to simulate the possible known answers : 50% of the ratings of a set are used as the possible answers to the interview questions (Answer Set). The 50% of ratings left will be used for evaluating our models (Evaluation Set). Ratings have been binarized for each datasets, a rating of -1 (resp. 1) being considered a dislike, (resp. like).\nThe quality of the different models is evaluated by two different measures. The root mean squared error (RMSE) measures the average ratings\u2019 prediction precision measured as the difference between predicted and actual ratings (r\u0302u,i \u2212 ru,i)2. As we work with binary ratings, we also use the accuracy as a performance evaluation. In this context, it means that we focus on the overall prediction, i.e on the fact that the system has rightly predicted like or dislike, rather than on its precision regarding the \u201dtrue\u201d rating. The accuracy is calculated as the average \u201dlocal\u201d accuracy along users. These measures are computed over the set of missing ratings i.e the Evaluation Set.\nWe explore the quality of our approach on both the classical CF context using the IAM Model (Equation 8) and on the cold-start problem using the CS-IAM model defined in Equation 7. We compare our models with two baseline collaborative filtering methods: Matrix Factorization (MF) that we presented earlier, and the Item-KNN with Pearson correlation measure (Koren (2010)) which does not compute representations for users nor items but is a state-of-the-art CF method. Note that the inductive models (IAM and CS-IAM) are trained using only the set of training users U train. The ratings in the Answer Set are only used as inputs during the testing phase, but not during training. Transductive models are trained using both the training users U train, but also the Answer set of ratings defined over the testing users. It is a crucial difference as our model has significantly less information during training.\nEach model has its own hyper-parameters to be tuned: the learning-rate of the gradient descent procedure, the sizeN of the latent space, the different regularization coefficients... The evaluation is thus made as follows: models are evaluated for several hyper-parameters values using a grid-search procedure, the performance being averaged over 3 different randomly initialized runs. The models with the best average performance are presented in the next figures and tables. All models have been evaluated over the same datasets splits."}, {"heading": "3.1 COLLABORATIVE FILTERING", "text": "First, we evaluate the ability of our model to learn relevant representations in a classical CF context. In that case, the IAM model directly predicts ratings based on the ratings provided by a user. Results for the four different datasets are presented in Table 1b. We can observe that, despite having much\nless information during the learning phase, IAM obtains competitive results, attesting the ability of the additive model to generalize to new users. More precisely, IAM is better than MF on three out of four datasets. For example, on the MovieLens-1M dataset, IAM obtains 72.7% in terms of accuracy while MF\u2019s accuracy is only 68.9%. Similar scores are observed for Jester and Yahoo. Although Item-KNN model gives slightly better results for two datasets, one should note that this method do not rely on nor provide any representations for users or items and belongs to a different family of approach. Moreover, ItemKNN - which is based on a KNN-based method - has a high complexity, and is thus very slow to use, and unable to deal with large scale datasets like Flixter on which many days are needed in order to compute performance. Beyond its nice performance IAM is able to predict over a new user in a very short-time, on the contrary to MF and ItemKNN."}, {"heading": "3.2 COLD-START SETTING", "text": "We now study the ability of our approach to predict ratings in a realistic cold-start situation. As MF and ItemKNN do not provide a way to select a set of items for the interview, we use two benchmark selection methods used in the literature (Rashid et al. (2002)). The POP method select the most popular items - i.e the items with the highest number of ratings in the training set - and the HELF (Harmonic mean of Entropy and Logarithm of rating Frequency) method which select items based on both their popularity but also using an entropy criterion, which focus on the informativeness of items (e.g a controversial movie can be more informative than a movie liked by everyone) (Rashid et al. (2008)). Our model is learned solely on the U train set. Baselines are computed on a dataset composed of the original U train ratings with the additional ratings of the AnswerSet of U test that lie into the set of items selected by the POP or the HELF approach. Transductive approaches use more information during training that our inductive model.\nThe number of items selected by the CS-IAM model directly depends on the value of the L1 regularization coefficient and several values have been evaluated. In CS-IAM, the number of selected items correspond to the number of non-null \u03b1i parameters. The number of items selected by POP and HELF is manually chosen.\nFigure 1 shows accuracy and RMSE results for all models on the Yahoo dataset as a function of the interview size. It first illustrates that ItemKNN approach does not provide good results for RMSEevaluation, as it is not a regression-based method, but is better than MF in terms of accuracy. It also shows that HELF criterion does not seem to be specifically better on this dataset than the POP criterion. For both evaluations, CS-IAM gives better results, for all sizes of interview. It can also be noted that CS-IAM also gives good results when no item is selected due to the \u03a80 parameters that correspond to the learned default representation. The model with 0 items also expresses the base performance obtained on users unable to provide ratings during the interview.\nDetailed accuracy results for the four datasets are summarized in Table 2, for different reasonable sizes of interview. Similar observations can be made on the results, where CS-IAM managed to have the best or competitive accuracy for all datasets and all number of questions allowed, while using less information in train.\nAt last, when comparing the performance of CS-IAM with a version of IAM where items have been selected by the POP criterion -IAM-Pop, Figure 2a - one can see that the CS-IAM outperforms the\nother approaches. It interestingly shows that (i) IAM managed to give better results than MF with the same information selection strategy (POP) (ii) CS-IAM with all its parameters learned, managed to select more useful items for the interview process, illustrating that the performance of this model is due to both, its expressive power, but also on its ability to simultaneously learn representations, and select relevant items.\nWe have shown that our approach gives significantly good quantitative results. We now focus our interest on a qualitative analysis of the results performed over the MovieLens dataset. First, we compare the items selected by the three selection methods (CS-IAM, POP and HELF). These items are presented in Table 3. First, when using the POP criterion, one can see that many redundant movies are selected - i.e the three last episodes of Star Wars on which the ratings are highly correlated: a user likes or dislikes Star Wars, not only some episodes. The same effect seems to appear also with CS-IAM which selects Back to the future I and Back to the future III. But, in fact, the situation is different since the ratings on these two movies have less correlations. Half of the users that like Back to the future I dislike Back to the future III.\nFigure 2b shows the translations \u03b1i\u03a8i after having performed a PCA in order to obtain 2D representations. What we can see is that depending on the movie, the fact of having a positive rating or a negative rating does not have the same consequences in term of representation: For example, liking or disliking Saving Private Ryan is different than liking or disliking Star Wars; the translation concerning these two movies are almost perpendicular and thus result in a very different modification of the representation of the user. Schindler\u2019s List has less consequences concerning the user representation i.e the norm of \u03b1i\u03a8ri is lower than the others."}, {"heading": "3.3 MIXING COLD-START AND WARM RECOMMENDATION", "text": "Our model can also allow one to smoothly move from a cold-start to a warm context : after having answered the interview, the user will start interacting with the system, providing new ratings, which will be easily integrated with our inductive translation model to update his representation and thus, the resulting recommendations. To do so, we simply change the learning strategy: (i) The model is learned in the warm setting described in Equation (8), i.e we learn each item\u2019s representation qi and the translations on representations (the \u03a8ri parameters). (ii) We select the most relevant items for the interview process by learning the \u03b1\u2019s weights using a L1 regularization as explained in Equation (7). In this phase, we only learn the \u03b1-values which will allow us to choose which items to use during the interview, following Equation (6). After the interview, each new incoming rating modifies the user representation as explained in Equation (4), resulting in a system that is naturally able to take into account new information. Note that, in this setting, the use of an hyperbolic tangent function on the representation, which will limit its norm, improves the quality of the system.\nThis model has been evaluated on the Yahoo dataset with the following experimental protocol: First the model is evaluated in its cold-start setting using the item with non-null \u03b1\u2019s values. Then, we evaluate the performance of this model when adding different amount of \u201dnew\u201d ratings sampled uniformly from the set of items. The results are illustrated in Figure 3 which shows that the performance of this strategy increases as new ratings are added and almost reaches the one obtain for the\nclassical warm setting (see Table 1b). Curves for different sizes of initial interviews are shown. We think that this extension of our approach which makes the link between the cold-start and the warm settings is an original and promising feature."}, {"heading": "4 RELATED WORK", "text": "The recommendation problem has been studied under various assumptions. We focus on Collaborative Filtering (CF) methods, which only use the past ratings observed on the users and items, but other families of approaches exists, as Content-Based methods, which use informative features on users and items (Pazzani & Billsus (2007)), and hybrid methods that mix ratings and informative features (Basilico & Hofmann (2004)). CF techniques can be distinguished into two categories. Memory-based methods, such as Neighborbased CF Resnick et al. (1994), calculate weights between pairs of items (Sarwar et al. (2001)) or users (Herlocker et al. (1999)), based on similarities or correlations between them. Model-based methods, such as Latent Factor Models, have rather a representation learning approach, where representations vectors for each user and item are inferred from the matrix of ratings with matrix factorization techniques (Koren et al. (2009)). Collaborative filtering models have a major limitation when there is no history for a user or an item. A classical approach in this case is to use an interview process with a few questions asked to the new user as it is done in this paper. Several papers have proposed different methods to choose which questions to select. Static approaches (see Rashid et al. (2002) for a comparative study), construct a static seed set of questions (fixed for all users) following a selection criterion like measures of popularity, entropy or coverage while Golbandi et al. (2010) also proposed a greedy algorithm that aims to minimize the prediction error performed with the seed set. Adaptive approaches have also been proposed, where the interview process considers the user\u2019s answers to choose the next question. For example, Rashid et al. (2008) fits a decision tree to find a set of clusters of users, while Golbandi et al. (2011) uses a ternary tree where each node is an item and branch corresponds to eventual answers (like,dislike,unknown). Zhou et al. (2011) presents functional matrix factorization, a decision tree based method which also associate a latent profile to each nodes of the tree. The closest model to our approach is Sun et al. (2013), who learn a ternary tree allowing multiple questions at each node, each node containing a (learned) regressor and translations functions on selected items. Our model can be seen as one node of their tree. However, their approach does not seem to allow a bridge between cold start and warm context as ours does. It is also interesting to note that while usually more efficient, one drawback of such adaptive approaches is that users usually dislike having to rate item one by one and prefer rating several items in one shot (Golbandi et al. (2011); Rashid et al. (2002))."}, {"heading": "5 CONCLUSION AND PERSPECTIVES", "text": "We have proposed a new representation-based model for collaborative filtering. This inductive model (IAM) directly computes the representation of a user by cumulative translations in the la-\ntent space, each translation depending on a rating value on a particular item. We have also proposed a generic formulation of the user cold-start problem as a representation learning problem and shown that the IAM method can be instantiated in this framework allowing one to learn both which items to use in order to build a preliminary interview for incoming users, but also how to use these ratings for recommendation. The results obtained over four datasets show the ability of our approach to outperform baseline methods. Different research directions are opened by this work: (i) first, the model can certainly be extended to deal with both incoming users, but also new items. In that last case, the interview process would consist in asking reviews for any new item to a particular subset of relevant users. (ii) While we have studied the problem of building a static interview - i.e the opinions on a fixed set of items is asked to any new user - we are currently investigating how to produce personalized interviews by using sequential learning models i.e reinforcement learning techniques."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This article has been supported within the Labex SMART supported by French state funds managed by the ANR within the Investissements d\u2019Avenir programme under reference ANR-11-LABX-65. Part of this work has benefited from a grant from program DGA-RAPID, project LuxidX."}], "references": [{"title": "Unifying collaborative and content-based filtering", "author": ["Basilico", "Justin", "Hofmann", "Thomas"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Basilico et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Basilico et al\\.", "year": 2004}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Lazy sparse stochastic gradient descent for regularized multinomial logistic regression", "author": ["Carpenter", "Bob"], "venue": "Alias-i, Inc., Tech. Rep, pp", "citeRegEx": "Carpenter and Bob.,? \\Q2008\\E", "shortCiteRegEx": "Carpenter and Bob.", "year": 2008}, {"title": "On bootstrapping recommender systems", "author": ["Golbandi", "Nadav", "Koren", "Yehuda", "Lempel", "Ronny"], "venue": "In Proceedings of the 19th ACM CIKM,", "citeRegEx": "Golbandi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Golbandi et al\\.", "year": 2010}, {"title": "Adaptive bootstrapping of recommender systems using decision trees", "author": ["Golbandi", "Nadav", "Koren", "Yehuda", "Lempel", "Ronny"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "Golbandi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Golbandi et al\\.", "year": 2011}, {"title": "An algorithmic framework for performing collaborative filtering", "author": ["Herlocker", "Jonathan L", "Konstan", "Joseph A", "Borchers", "Al", "Riedl", "John"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR,", "citeRegEx": "Herlocker et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Herlocker et al\\.", "year": 1999}, {"title": "Factor in the neighbors: Scalable and accurate collaborative filtering", "author": ["Koren", "Yehuda"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),", "citeRegEx": "Koren and Yehuda.,? \\Q2010\\E", "shortCiteRegEx": "Koren and Yehuda.", "year": 2010}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert", "Volinsky", "Chris"], "venue": null, "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Content-based recommendation systems", "author": ["Pazzani", "Michael J", "Billsus", "Daniel"], "venue": "In The adaptive web,", "citeRegEx": "Pazzani et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pazzani et al\\.", "year": 2007}, {"title": "Getting to know you: learning new user preferences in recommender systems", "author": ["Rashid", "Al Mamunur", "Albert", "Istvan", "Cosley", "Dan", "Lam", "Shyong K", "McNee", "Sean M", "Konstan", "Joseph A", "Riedl", "John"], "venue": "In Proceedings of the 7th IUI,", "citeRegEx": "Rashid et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Rashid et al\\.", "year": 2002}, {"title": "Learning preferences of new users in recommender systems: an information theoretic approach", "author": ["Rashid", "Al Mamunur", "Karypis", "George", "Riedl", "John"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Rashid et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rashid et al\\.", "year": 2008}, {"title": "Grouplens: an open architecture for collaborative filtering of netnews", "author": ["Resnick", "Paul", "Iacovou", "Neophytos", "Suchak", "Mitesh", "Bergstrom", "Peter", "Riedl", "John"], "venue": "In Proceedings of the 1994 ACM conference on Computer supported cooperative work,", "citeRegEx": "Resnick et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Resnick et al\\.", "year": 1994}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["Sarwar", "Badrul", "Karypis", "George", "Konstan", "Joseph", "Riedl", "John"], "venue": "In Proceedings of WWW,", "citeRegEx": "Sarwar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sarwar et al\\.", "year": 2001}, {"title": "Learning multiple-question decision trees for cold-start recommendation", "author": ["Sun", "Mingxuan", "Li", "Fuxin", "Lee", "Joonseok", "Zhou", "Ke", "Lebanon", "Guy", "Zha", "Hongyuan"], "venue": "In Proceedings of the sixth ACM international conference on Web search and data mining,", "citeRegEx": "Sun et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "Functional matrix factorizations for cold-start recommendation", "author": ["Zhou", "Ke", "Yang", "Shuang-Hong", "Zha", "Hongyuan"], "venue": "In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,", "citeRegEx": "Zhou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "As highlighted by Bengio et al. (2013), designing models able to learn these representations from (raw) data instead of manual pre-processing seems crucial to go further in Artificial Intelligence, and representation learning has gain a surge of interest in machine learning.", "startOffset": 18, "endOffset": 39}, {"referenceID": 7, "context": "This loss corresponds to a matrix decomposition in latent factors and different optimization algorithms have been proposed as alternated least squares or stochastic gradient descent (Koren et al. (2009)).", "startOffset": 183, "endOffset": 203}, {"referenceID": 12, "context": "The datasets are classical datasets used in the literature (Zhou et al. (2011); Golbandi et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 3, "context": "(2011); Golbandi et al. (2010)).", "startOffset": 8, "endOffset": 31}, {"referenceID": 9, "context": "As MF and ItemKNN do not provide a way to select a set of items for the interview, we use two benchmark selection methods used in the literature (Rashid et al. (2002)).", "startOffset": 146, "endOffset": 167}, {"referenceID": 9, "context": "As MF and ItemKNN do not provide a way to select a set of items for the interview, we use two benchmark selection methods used in the literature (Rashid et al. (2002)). The POP method select the most popular items - i.e the items with the highest number of ratings in the training set - and the HELF (Harmonic mean of Entropy and Logarithm of rating Frequency) method which select items based on both their popularity but also using an entropy criterion, which focus on the informativeness of items (e.g a controversial movie can be more informative than a movie liked by everyone) (Rashid et al. (2008)).", "startOffset": 146, "endOffset": 604}, {"referenceID": 5, "context": "Memory-based methods, such as Neighborbased CF Resnick et al. (1994), calculate weights between pairs of items (Sarwar et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 5, "context": "Memory-based methods, such as Neighborbased CF Resnick et al. (1994), calculate weights between pairs of items (Sarwar et al. (2001)) or users (Herlocker et al.", "startOffset": 47, "endOffset": 133}, {"referenceID": 3, "context": "(2001)) or users (Herlocker et al. (1999)), based on similarities or correlations between them.", "startOffset": 18, "endOffset": 42}, {"referenceID": 3, "context": "(2001)) or users (Herlocker et al. (1999)), based on similarities or correlations between them. Model-based methods, such as Latent Factor Models, have rather a representation learning approach, where representations vectors for each user and item are inferred from the matrix of ratings with matrix factorization techniques (Koren et al. (2009)).", "startOffset": 18, "endOffset": 346}, {"referenceID": 3, "context": "(2001)) or users (Herlocker et al. (1999)), based on similarities or correlations between them. Model-based methods, such as Latent Factor Models, have rather a representation learning approach, where representations vectors for each user and item are inferred from the matrix of ratings with matrix factorization techniques (Koren et al. (2009)). Collaborative filtering models have a major limitation when there is no history for a user or an item. A classical approach in this case is to use an interview process with a few questions asked to the new user as it is done in this paper. Several papers have proposed different methods to choose which questions to select. Static approaches (see Rashid et al. (2002) for a comparative study), construct a static seed set of questions (fixed for all users) following a selection criterion like measures of popularity, entropy or coverage while Golbandi et al.", "startOffset": 18, "endOffset": 716}, {"referenceID": 3, "context": "(2002) for a comparative study), construct a static seed set of questions (fixed for all users) following a selection criterion like measures of popularity, entropy or coverage while Golbandi et al. (2010) also proposed a greedy algorithm that aims to minimize the prediction error performed with the seed set.", "startOffset": 183, "endOffset": 206}, {"referenceID": 3, "context": "(2002) for a comparative study), construct a static seed set of questions (fixed for all users) following a selection criterion like measures of popularity, entropy or coverage while Golbandi et al. (2010) also proposed a greedy algorithm that aims to minimize the prediction error performed with the seed set. Adaptive approaches have also been proposed, where the interview process considers the user\u2019s answers to choose the next question. For example, Rashid et al. (2008) fits a decision tree to find a set of clusters of users, while Golbandi et al.", "startOffset": 183, "endOffset": 476}, {"referenceID": 3, "context": "(2002) for a comparative study), construct a static seed set of questions (fixed for all users) following a selection criterion like measures of popularity, entropy or coverage while Golbandi et al. (2010) also proposed a greedy algorithm that aims to minimize the prediction error performed with the seed set. Adaptive approaches have also been proposed, where the interview process considers the user\u2019s answers to choose the next question. For example, Rashid et al. (2008) fits a decision tree to find a set of clusters of users, while Golbandi et al. (2011) uses a ternary tree where each node is an item and branch corresponds to eventual answers (like,dislike,unknown).", "startOffset": 183, "endOffset": 562}, {"referenceID": 3, "context": "(2002) for a comparative study), construct a static seed set of questions (fixed for all users) following a selection criterion like measures of popularity, entropy or coverage while Golbandi et al. (2010) also proposed a greedy algorithm that aims to minimize the prediction error performed with the seed set. Adaptive approaches have also been proposed, where the interview process considers the user\u2019s answers to choose the next question. For example, Rashid et al. (2008) fits a decision tree to find a set of clusters of users, while Golbandi et al. (2011) uses a ternary tree where each node is an item and branch corresponds to eventual answers (like,dislike,unknown). Zhou et al. (2011) presents functional matrix factorization, a decision tree based method which also associate a latent profile to each nodes of the tree.", "startOffset": 183, "endOffset": 695}, {"referenceID": 3, "context": "(2002) for a comparative study), construct a static seed set of questions (fixed for all users) following a selection criterion like measures of popularity, entropy or coverage while Golbandi et al. (2010) also proposed a greedy algorithm that aims to minimize the prediction error performed with the seed set. Adaptive approaches have also been proposed, where the interview process considers the user\u2019s answers to choose the next question. For example, Rashid et al. (2008) fits a decision tree to find a set of clusters of users, while Golbandi et al. (2011) uses a ternary tree where each node is an item and branch corresponds to eventual answers (like,dislike,unknown). Zhou et al. (2011) presents functional matrix factorization, a decision tree based method which also associate a latent profile to each nodes of the tree. The closest model to our approach is Sun et al. (2013), who learn a ternary tree allowing multiple questions at each node, each node containing a (learned) regressor and translations functions on selected items.", "startOffset": 183, "endOffset": 886}, {"referenceID": 3, "context": "(2002) for a comparative study), construct a static seed set of questions (fixed for all users) following a selection criterion like measures of popularity, entropy or coverage while Golbandi et al. (2010) also proposed a greedy algorithm that aims to minimize the prediction error performed with the seed set. Adaptive approaches have also been proposed, where the interview process considers the user\u2019s answers to choose the next question. For example, Rashid et al. (2008) fits a decision tree to find a set of clusters of users, while Golbandi et al. (2011) uses a ternary tree where each node is an item and branch corresponds to eventual answers (like,dislike,unknown). Zhou et al. (2011) presents functional matrix factorization, a decision tree based method which also associate a latent profile to each nodes of the tree. The closest model to our approach is Sun et al. (2013), who learn a ternary tree allowing multiple questions at each node, each node containing a (learned) regressor and translations functions on selected items. Our model can be seen as one node of their tree. However, their approach does not seem to allow a bridge between cold start and warm context as ours does. It is also interesting to note that while usually more efficient, one drawback of such adaptive approaches is that users usually dislike having to rate item one by one and prefer rating several items in one shot (Golbandi et al. (2011); Rashid et al.", "startOffset": 183, "endOffset": 1434}, {"referenceID": 3, "context": "(2002) for a comparative study), construct a static seed set of questions (fixed for all users) following a selection criterion like measures of popularity, entropy or coverage while Golbandi et al. (2010) also proposed a greedy algorithm that aims to minimize the prediction error performed with the seed set. Adaptive approaches have also been proposed, where the interview process considers the user\u2019s answers to choose the next question. For example, Rashid et al. (2008) fits a decision tree to find a set of clusters of users, while Golbandi et al. (2011) uses a ternary tree where each node is an item and branch corresponds to eventual answers (like,dislike,unknown). Zhou et al. (2011) presents functional matrix factorization, a decision tree based method which also associate a latent profile to each nodes of the tree. The closest model to our approach is Sun et al. (2013), who learn a ternary tree allowing multiple questions at each node, each node containing a (learned) regressor and translations functions on selected items. Our model can be seen as one node of their tree. However, their approach does not seem to allow a bridge between cold start and warm context as ours does. It is also interesting to note that while usually more efficient, one drawback of such adaptive approaches is that users usually dislike having to rate item one by one and prefer rating several items in one shot (Golbandi et al. (2011); Rashid et al. (2002)).", "startOffset": 183, "endOffset": 1456}], "year": 2015, "abstractText": "A standard approach to Collaborative Filtering (CF), i.e. prediction of user ratings on items, relies on Matrix Factorization techniques. Representations for both users and items are computed from the observed ratings and used for prediction. Unfortunatly, these transductive approaches cannot handle the case of new users arriving in the system, with no known rating, a problem known as user cold-start. A common approach in this context is to ask these incoming users for a few initialization ratings. This paper presents a model to tackle this twofold problem of (i) finding good questions to ask, (ii) building efficient representations from this small amount of information. The model can also be used in a more standard (warm) context. Our approach is evaluated on the classical CF problem and on the cold-start problem on four different datasets showing its ability to improve baseline performance in both cases.", "creator": "LaTeX with hyperref package"}}}