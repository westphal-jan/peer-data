{"id": "1412.7584", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2014", "title": "Differential Privacy and Machine Learning: a Survey and Review", "abstract": "simple objective of machine learning is to publish valid information from laboratories, while privacy is required constantly concealing information. traditionally it seems hard to reconcile these competing interests. however, they become more be balanced after mining rich material. biological science, longitudinal research represents an important application where it demonstrates necessary both to define useful information and examine patient responses. one way to minimize systemic conflict is to extract general concepts of occurring phenomena without disclosing the common information of everybody.", "histories": [["v1", "Wed, 24 Dec 2014 01:51:06 GMT  (31kb)", "http://arxiv.org/abs/1412.7584v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.DB", "authors": ["zhanglong ji", "zachary c lipton", "charles elkan"], "accepted": false, "id": "1412.7584"}, "pdf": {"name": "1412.7584.pdf", "metadata": {"source": "CRF", "title": "Differential Privacy and Machine Learning: a Survey and Review", "authors": ["Zhanglong Ji", "Zachary C. Lipton", "Charles Elkan"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n75 84\nv1 [\ncs .L\nG ]\n2 4\nThe objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conflict is to extract general characteristics of whole populations without disclosing the private information of individuals.\nIn this paper, we consider differential privacy, one of the most popular and powerful definitions of privacy. We explore the interplay between machine learning and differential privacy, namely privacy-preserving machine learning algorithms and learning-based data release mechanisms. We also describe some theoretical results that address what can be learned differentially privately and upper bounds of loss functions for differentially private algorithms.\nFinally, we present some open questions, including how to incorporate public data, how to deal with missing data in private datasets, and whether, as the number of observed samples grows arbitrarily large, differentially private machine learning algorithms can be achieved at no cost to utility as compared to corresponding non-differentially private algorithms.\nThe objective of machine learning is to extract useful information from data, such as how to classify data, how to predict a quantity, or how to find clusters of similar samples. Given a family of learning models, machine learning algorithms select and output the best one based on some given data. The output model can be used in either dealing with future data or interpreting the distribution of data. Although the output model typically far more compact than the underlying dataset, it must capture some information describing the dataset. Privacy, on the other hand, concerns the protection of private data from leakage, especially the information of individuals.1\n1 [9] uses privacy to mean both population privacy and individual privacy. For example, disclosing that members of some family are highly susceptible to a given genetic condition might violate population privacy, while disclosing that some specific patient suffers from this condition would violate their individual privacy. However, even if one conceals all his/her\nIt would be reasonable to ask, \u201cwhy is it insufficient to anonymize data?\u201d One could remove names and other obviously identifiable information from a database. It might seem difficult then, for an attacker to identify an individual. Current HIPAA guidelines promotes such an approach, listing 18 categories of personally identifiable information which must be redacted in the case of research data for publication. Unfortunately, this method can leak information when the attacker already has some information about the individuals in question. In a well-known case, the personal health information of Massachusetts governor William Weld was discovered in a supposedly anonymized public database [51]. By merging overlapping records between the health database and a voter registry, researchers were able to identify the personal health records of the governor, among others.\nTo combat such background attacks, some more robust definitions of privacy (such as k-anonymity[51], l-diversity[36] and t-closeness[34]) have been proposed. In these approaches, samples are grouped if their sensitive features are the same, and a group is published if the number of samples in that group is large enough. Intuitively, it should be difficult for an attacker to distinguish individual samples. However, even these definitions cannot prevent background attacks, in which the attackers already know something about the information contained in the dataset. In an extreme case, the attacker might know the contents of all but one of the rows in the set.\nConsider a database which holds the address and income of four people and publishes private data using 3-anonymity. According to 3-anonymity, if any three people live in the same city, the city and average income of the three people is released. Now suppose an attacker knows that two people in the database live in Los Angeles and that a third lives in New York. If no data is published, the attacker can easily infer that the fourth person does not live in Los Angeles.\nEven if one released aggregated statistics, they might risk compromising private information. Recently [23], researchers demonstrated that an attacker could infer whether an individual had participated in a genome study using only publicly available aggregated genetic data. In such cases, aggregation is no longer safe.\nDifferential privacy [12, 13], which will be introduced in the next section, uses random noise to ensure that the publicly visible information doesn\u2019t change much if one individual in the dataset changes.\nAs no individual sample can significantly affect the output, attackers cannot infer the private information corresponding to any individual sample confidently. This paper addresses the interplay between machine learning and differential privacy.\nAlthough it seems that machine learning and privacy protection are in opposition, it is often possible to reconcile them. Researchers have designed many mechanisms to build models that can capture the distributions corresponding to\ninformation, it\u2019s still possible to breach population privacy by collecting information from other members of the subpopulation. As one cannot easily protect oneself from the disclosure of this information, we will use privacy to refer only to individual privacy in this paper.\nlarge datasets while guaranteeing differential privacy with respect to individual examples. In order to achieve generalizability, machine learning models should not depend heavily on any single sample. Therefore, it is possible to hide the effects of individual samples, simultaneously preserving privacy and providing utility.\n0.1 Prior Work\nSeveral recent surveys address differential privacy and data science [17, 47, 22]. Some others ([17, 22]) mainly focus on statistical estimators, while [47] discusses the high level interactions between differential privacy and machine learning.\nOur survey focuses specifically on methods by which machine learning algorithms can be made differentially private. We study current differentially private machine learning algorithms and organize them according to the fundamental machine learning tasks they address, including classification, regression, clustering, and dimensionality reduction. We also describe some differentially private data release mechanisms, both because their mechanisms involves differential privacy, and because their output can be used to learn differentially private machine learning models. We explain how all of these mechanisms work and compare their theoretical guarantees. Some general theoretical results and discussion follow."}, {"heading": "1 Differential Privacy", "text": "Differential privacy is one of the most popular definitions of privacy today. Intuitively, it requires that the mechanism outputting information about an underlying dataset is robust to any change of one sample, thus protecting privacy.\nThe following subsections mathematically define differential privacy and introduce some commonly used methods in differential privacy."}, {"heading": "1.1 Definition of Differential Privacy", "text": "Definition 1: a mechanism f\u0303 is a random function that takes a dataset D as input, and outputs a random variable f\u0303(D).\nFor example, suppose D is a medical dataset, then the function that outputs the number of patients in D plus noise from the standard normal distribution is a mechanism.\nDefinition 2: the distance of two datasets, d(D,D\u2032), denotes the minimum number of sample changes that are required to change D into D\u2032.\nFor example, ifD andD\u2032 differ on at most one individual, there is d(D,D\u2032) = 1. We also call such a pair of datasets neighbors.\nThe original definition of differential privacy defines as neighbors datasets which \u2018differ on at most one individual\u2019. This phrasing has given rise to two different understandings. Some interpret this as the replacement of a sample,\nwhile others also consider addition and deletion. Although the second interpretation is stronger than the first one, most of the mechanisms discussed in this paper work for both definitions if slightly modified. Different definitions of distance usually lead to different values of sensitivity, while both are bounded. In order to make a mechanism designed for one definition of distance work for another definition of distance, we only need to make slight changes to the scale of noise. Therefore we won\u2019t distinguish them.\nDefinition 3: a mechanism f\u0303 satisfies (\u01eb, \u03b4)-differential privacy [12, 13] for two non-negative numbers \u01eb and \u03b4 iff for all neighbors d(D,D\u2032) = 1, and all subset S of f\u0303 \u2019s range, as long as the following probabilities are well-defined, there holds P (f\u0303(D) \u2208 S) \u2264 \u03b4 + e\u01ebP (f\u0303(D\u2032) \u2208 S) Intuitively speaking, the number \u03b4 represents the probability that a mechanism\u2019s output varies by more than a factor of e\u01eb when applied to a dataset and any one of its neighbors. A lower value of \u03b4 signifies greater confidence and a smaller value of \u01eb tightens the standard for privacy protection. The smaller \u01eb and \u03b4 are, the closer P (f\u0303(D) \u2208 S) and P (f\u0303(D\u2032) \u2208 S) are, and the stronger protection is.\nThere is also a commonly used heuristic to choose \u03b4[20]: when there are n samples in the dataset, \u03b4 \u2208 o(1/n). This is because a mechanism can satisfy (0, \u03b4)-differential privacy but breach privacy with high probability when \u03b4 is large. For each sample in the dataset, the mechanism releases it with probability \u03b4, and the release of different samples are independently. It\u2019s easy to prove the mechanism is differentially private. However by expectation, the mechanism release n\u03b4 samples from the dataset. To prevent such leakage, \u03b4 must be smaller than 1/n.\nTypically, (\u01eb, 0)-differential privacy is simplified to \u01eb-differential privacy. With (\u01eb, \u03b4)-differential privacy, when \u03b4 > 0, there is still a small chance that some information is leaked. When \u03b4 = 0, the guarantee is not probabilistic. [11] shows that in terms of mtutal information, \u01eb-differential privacy is much stronger than (\u01eb, \u03b4)-differential privacy.\nIn differential privacy, the number \u01eb is also called the privacy budget."}, {"heading": "1.2 Query", "text": "Usually, we want the output of a mechanism to be both differentially private and useful. By \u2018useful\u2019, we mean the output accurately answers some queries on the dataset. Definition 4 defines query below and in the following subsections, some mechanisms that guarantee differential privacy will be introduced.\nDefinition 4: a query f is a function that takes a dataset as input. The answer to the query f is denoted f(D).\nFor example, if D is a medical dataset, then \u2018how many patients were successfully cured?\u2019 is a query since it takes D as input and outputs a number. The output of a query is not necessarily a number. However, some mechanisms, notably the Laplacian mechanism, assume that answers to queries are numerical, or vectors f(D) \u2208 Rp but not categorical. A more sophisticated query\ncan be \u2018a logistic regression model trained from the dataset\u2019, which outputs a classification model."}, {"heading": "1.3 The Laplacian Mechanism", "text": "The Laplacian mechanism[15] is a popular \u01eb-differentially private mechanism for queries f with answers f(D) \u2208 Rp, in which sensitivity (Definition 5) plays an important role.\nDefinition 5: given a query f and a norm function \u2016.\u2016 over the range of f , the sensitivity s(f, \u2016.\u2016) is defined as\ns(f, \u2016.\u2016) = max d(D,D\u2032)=1 \u2016f(D)\u2212 f(D\u2032)\u2016\nUsually, the norm function \u2016.\u2016 is either L1 or L2 norm. The Laplacian mechanism[15]: given a query f and a norm function over the range of f , the random function f\u0303(D) = f(D) + \u03b7 satisfies \u01eb-differential privacy. Here \u03b7 is a random variable whose probability density function is p(\u03b7) \u221d e\u2212\u01eb\u2016\u03b7\u2016/s(f,\u2016.\u2016).\nThere is a variation of the Laplacian mechanism, which replaces Laplacian noise with Gaussian noise. On one side, this replacement greatly reduces the probability of very large noise; on the other side, it only preserves (\u01eb, \u03b4)differential privacy for some \u03b4 > 0, which is weaker than \u01eb-differential privacy.\nVariation of the Laplacian mechanism: given a query f and a distance function over the range of f , the random function f\u0303(D) = f(D) + \u03b7 satisfies (\u01eb, \u03b4)-differential privacy. Here \u03b7 is a random variable from distribution N(0, 2\u01eb2 (s(f, \u2016.\u2016))2 log 2\u03b4 ) [2]."}, {"heading": "1.4 The Exponential Mechanism", "text": "The exponential mechanism[38] is an \u01eb-differentially private method to select one element from a set. Suppose the set to select from is A, and there exists a score function H whose input is a dataset D and a potential answer a \u2208 A, and whose output is a real number. Given a dataset D, the exponential mechanism selects the element a \u2208 A that has a large score H(D, a).\nDefinition 6: the sensitivity of score function H is defined as\ns(H, \u2016.\u2016) = max d(D,D\u2032)=1,a\u2208A \u2016H(D, a)\u2212H(D\u2032, a)\u2016\nThe exponential mechanism: given a dataset D and a set of possible answers A, if a random mechanism selects an answer based on the following probability, then the mechanism is \u01eb-differentially private:"}, {"heading": "P (a \u2208 A is selected) \u221d e\u01ebH(D,a)/2s(H,\u2016.\u2016)", "text": "The Laplacian mechanism is related to the exponential mechanism. If f(D) is a vector in Rp, and \u2200a \u2208 Rp H(D, a) = \u2016a \u2212 f(D)\u2016, then the output has exactly the same distribution as f\u0303(D) in the Laplacian mechanism with half privacy budget."}, {"heading": "1.5 The Smooth Sensitivity Framework and the Sample and Aggregate Framework", "text": "Smooth sensitivity [42] is a framework which allows one to publish an (\u01eb, \u03b4)differentially private numerical answer to a query. The noise it adds is determined not only by the query but also by the database itself. By avoiding using the worst-case sensitivity, this framework can enjoy much smaller noise, though the definition of privacy is weaker in this framework compared to the Laplacian mechanism. Two concepts, local and smooth sensitivities, are introduced.\nDefinition 7: given a query function f , a norm function \u2016.\u2016 and a dataset D, the local sensitivity of f is defined as:\nLS(f, \u2016.\u2016, D) = max D\u2032:d(D,D\u2032)=1 \u2016f(D)\u2212 f(D\u2032)\u2016\nOne intuitive mechanism would be to add noise to the answer f(D) proportional to the local sensitivity given (f,D). However such a mechanism may leak information. For example, assume d(D,D\u2032) = 1, If D has very small local sensitivity and D\u2032 has large local sensitivity w.r.t. some query f , the answer given by the mechanism on dataset D is very close to f(D). However, the answer given D\u2032 might be far away from f(D). In that case, attackers can infer whether the dataset is D or D\u2032 according to the distance between f(D) and the output. To overcome this problem, the smooth sensitivity framework smooths the scale of noise across neighboring datasets.\nDefinition 8: Given a query function f , a norm function \u2016.\u2016, a dataset D and a number \u03b2, the \u03b2 smooth sensitivity of f is defined as\ns(f, \u2016.\u2016, D, \u03b2) = max Any dataset D\u2217 (e\u2212\u03b2d(D,D \u2217)LS(f, \u2016.\u2016, D\u2217))\nSmooth sensitivity framework: Given a query function f , the dimension d of the sample space, and Gaussian noise Z \u223c N(0, 1), the output f\u0303(D) = f(D) + s(f,\u2016.\u2016,D\u2217,\u03b2)\n\u03b1 Z is (\u01eb, \u03b4)-differentially private, provided that \u03b1 = \u01eb/ \u221a ln(1/\u03b4) and\n\u03b2 = \u2126(\u01eb/ \u221a\nd ln(1/\u03b4)). The sample and aggregate framework [42] is a mechanism to respond to queries whose answers can be approximated well with a small number of samples, while ensuring (\u01eb, \u03b4)-differential privacy. The algorithm consists of a sampling step and an aggregating step. In the sampling step, the framework partitions the private data set D into many subsets {D1, ..., Dk}, and the answer f(Di) is estimated on each subset. Given the assumption that f can be measured well with small subsets, f(D1), ..., f(Dk) are fairly accurate. However, we haven\u2019t yet placed a privacy constraint on the estimation, thus the estimates cannot be released.\nIn the aggregating step, the framework first defines a quantity r(i) to denote the distance between f(Di) and f(Di)\u2019s t-th nearest neighbor among f(D1), ..., f(Dk) while t \u2248 k/2. Then the framework defines a function g(f(D1), ..., f(Dk)) which outputs the f(Di) with the smallest r(i). Then the smooth sensitivity\nframework is applied to the function g(f(D1), ..., f(Dk)) to ensure differential privacy.\nAs changing one sample affects only one estimate, the function g(f(D1), ..., f(Dk)) has small local sensitivity. Therefore the noise required is small. Furthermore, as most of the estimates f(D1), ..., f(Dk) are close to the true answer, g(f(D1), ..., f(Dk)) is accurate. Together, these two properties ensure that the output is accurate.\nAn efficient aggregation function is provided in the paper [42]. Given m answers {f1, f2, ..., fm} and a constant t0, the function first computes a quantity ri for each fi. The quantity ri is radius of the smallest ball that is centred at fi and covers at least t0 answers in {f1, f2, ..., fm}. Then the function outputs the answer fi which has the smallest ri."}, {"heading": "1.6 Combination of Differentially Private Mechanisms", "text": "Sometimes we need to combine several differentially private mechanisms in data processing, thus we need to know how the combination affects the privacy protection. In this subsection, f\u0303i represents differentially private algorithms, D is the dataset, and {Di} is a partition of D. Notation g() represents any function. [37] provides the following two theorems.\nSequential Theorem[15, 13, ?]: if f\u0303i is (\u01ebi, \u03b4i)-differentially private, then f\u0303(D) = g(f\u03031(D), f\u03032(D, f\u03031(D)), ..., f\u0303n(D, f\u03031(D), f\u03032(D), ..., f\u0303n\u22121(D))) is ( \u2211n i=1 \u01ebi, \u2211n i=1 \u03b4i)differentially private. Intuitively, it means that we can split \u01eb among a sequence of differentially private mechanisms and allow a mechanism in the sequence to use both the dataset and the outputs of previous mechanisms, while the final output is still differentially private. Some more sophisticated forms of this theorem can be find in [16, 43].\nParallel Theorem: if each f\u0303i is \u01eb-differentially private, given a partition {Di} of the dataset D, then f\u0303(D = \u222aDi) = g(f\u03031(D1), f\u03032(D2), ..., f\u0303n(Dn)) is \u01eb-differentially private.\nIf we apply \u01eb-differentially private mechanisms to each partition of the dataset, the combined output is still \u01eb-differentially private. The partitioning here can be either independent of private data or based on output of some other differentially private mechanism.\nBoth the sequential method and the parallel method have multiple outputs. A natural question is whether it is always beneficial to the utility of such privacypreserving mechanisms to average those outputs. The answer is no. For the sequential method, the privacy budget has to be split among several steps; for the parallel method, each partition has less samples than D. In both cases, the ratio between the amount of noise applied and the accurate answer is larger than the corresponding ratio for the original mechanism. Therefore, simply averaging them doesn\u2019t necessarily lead to better performance."}, {"heading": "2 Machine Learning", "text": "Machine learning algorithms extract information about the distribution of data. Informally, a learning algorithm takes as input a set of samples called a training set and outputs a model that captures some knowledge about the underlying distribution. Samples are also called examples. Typically an individual as discussed in the context of differential privacy will correspond to a single sample in the machine learning context. The set of all possible samples is called a sample space, and all samples in the sample space have the same set of variables. These variables can be either categorical or numerical. In the following sections, if there are variables whose values we would like to predict, that are known in training, but unknown for future examples, that variable is denoted Y . All the other variables are denoted X. When we want to predict labels for new examples, this task is called supervised learning. The task in which there are no labels and we want to identify structure in the dataset is called unsupervised learning.\nThe information extracted is represented by machine learning models, and different models are used for different tasks. Regression models predict a numerical variable Y given a set of variables X. Classification models predict a categorical variable Y given a set of variables X. Clustering models group unlabelled samples into several groups based on similarity. Dimension reduction models find a projection from the original sample space to a low-dimensional space, which preserves the most useful information for further machine learning. Feature selection techniques select the variables that are most informative for further research. According to whether the learning task is supervised or unsupervised, the training set is denoted either {(xi)}ni=1 or {(xi, yi)}ni=1, while n is the number of training samples.\nGiven a family of possible models and a dataset, a machine learning algorithm selects one model that fits the data best. The process of selection is called training.\nIn the following section, we assume that there is only one variable to predict in regression or classification tasks. For binary classifications, Y \u2208 {\u22121, 1}.\nIn all the following sections, we use the same notation for machine learning tasks. Usual capital letters X and Y mean random variables while bold capital letters X and Y mean data matrices. The j-th component of X is denoted Xj. There are n samples in the dataset and the i-th sample is denoted xi for unsupervised learning tasks or (xi, yi) for supervised ones. The j-th component of xi is denoted xij . All constants are denoted by capital letter C."}, {"heading": "2.1 Performance Measurement", "text": "Many papers have analyzed the performance of their mechanisms and proven that the private models they output are very close to the true models. However, the analyses in these papers differ in how they define the true model, how they define the distance between two models, and given such a distance metric, how they define closeness. These differences can impede our efforts to compare\ndifferent mechanisms. To assess the performance of a differentially private algorithm, it is necessary to have some notion of a \u2018true model\u2019 against which comparisons can be made. Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data. However, others [14, 33, 40] consider the \u2018true model\u2019 to mean the optimal model if the true distribution were known.\nThey also differ on how to define the distance between two models. Some papers [6, 25, 53] use the difference of values of the target function. Thus the distance between the private model and the true model is the difference between the values taken by the target functions corresponding to each of the two models. Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters. Still others [45, 26] use the distance between the predictions made by private and non-private models at certain points in the sample space.\nFinally, they differ on the definition of \u2018closeness\u2019. Given a measure of distance between two models, some papers [14, 40] prove that as the number of training examples grows large, the output converges to the true model. However they do not provide a guaranteed rate of convergence. Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models. For those which prove bounds on the speed of convergence, the convergence is usually measured by (\u03b1, \u03b2)-usefulness [3]. If the mechanism output f\u0303(D) is an (\u03b1, \u03b2)-useful answer to f on dataset D, then with probability 1 \u2212 \u03b2, the difference between f\u0303(D) and f(D) is less than \u03b1. Such mechanisms usually provide a relationship between data size, model settings, \u03b1 and \u03b2. A few papers [33] provide worst case guarantees on the distance, which is equivalent to (\u03b1, 0)-usefulness. Yet another paper [25] uses the expectation of difference.\nBelow, we will describe the utility analysis of various algorithms, but we cannot always compare two mechanisms that differ on some of these aspects. Furthermore, even if we can compare the utility of two mechanisms, one might outperform in some situations while the second outperforms in others. Suppose one is (\u01eb, \u03b4)-differentially private and the other is \u01eb-differentially private. If we can tolerate a very small probability that information is leaked, then the first may be better; if we are opposed to taking any risk, the second may be better. Therefore, choice of mechanism can depend on specific applications."}, {"heading": "2.2 General Ideas of Differentially Private Machine Learning Algorithms", "text": "Many differentially private machine learning algorithms can be grouped according to the basic approaches they use to compute a privacy-preserving model. This applies both for supervised and unsupervised learning.\nSome approaches first learn a model on clean data, and then use either the exponential mechanism or the Laplacian mechanism to generate a noisy model. For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while\n[7, 53] use the exponential mechanism. For some other approaches that have many iterations or multiple steps, the Laplacian mechanism and the exponential mechanism are applied to output parameters of each iteration/step. Such approaches include [21, 31, 24, 19, 25, 41, 55].\nSome mechanisms add noise to the target function and use the minimum/maximum of the noisy function as the output model. These technique is called objective perturbation. Some examples include [56, 5, 6, 45].\nSome mechanisms use the idea of the sample and aggregate framework. They are specially designed for queries that can be measured with a small number of samples. First, they split the dataset into many small subsets. Next, they combine the results from all subsets to estimate a model, adding noise in this aggregation step. Mechanisms that employ this idea include [42, 27]. The linear regression in [14] is partially based on this idea.\nSome mechanisms explore other ideas. For example, [33] partitions the sample space and uses counts in each partition to estimate the density function. [26] interprets a model as a function and uses another function to approximate it by iteratively minimizing the largest distance.\nMost output perturbation and objective perturbation mechanisms require a bounded sample space. This is because unbounded sample space usually leads to unbounded sensitivity. Mechanisms based on the sample and aggregate framework don\u2019t have this limitation. However most of them use (\u01eb, \u03b4)-differential privacy. In practice, if the sample space is unbounded and we want to use \u01eb-differential privacy, we can simply truncate the values in pre-processing. If the rule to truncate is independent of the private data, then the truncation is privacy-safe.\nIn the next sections, some differentially private machine learning mechanisms are introduced. We will briefly introduce the learning models, describe additional conditions assumed by various authors, explain how they design the mechanisms, and provide some utility analysis. However, the computation of sensitivity, the proof of differential privacy and some mechanism details won\u2019t be discussed here."}, {"heading": "3 Differentially Private Supervised Learning", "text": "Supervised machine learning describes the setting when labels are known for training data, and the task is to train a model to predict accurate labels given a new example. In this section we will describe differentially private versions of commonly used supervised machine learning algorithms."}, {"heading": "3.1 Naive Bayes Model", "text": "The naive Bayes model is a classifier which predicts label Y according to features in X . Given features X and a model, one can compute the conditional probability P (Y |X) for all labels Y and predict the label with largest conditional probability. The naive Bayes model is based on two assumptions.\nThe first assumption is that Xj are conditionally independent given Y , i.e., P (Xj |Y,X1, ..., Xj\u22121) = P (Xj |Y ). This enables us to compute the coefficient of each feature independently. The second assumption is that for all numerical features in X , P (X |Y ) is a normal distribution.\nBased on the first assumption and Bayes\u2019 theorem, the conditional probability is as follows:\nP (Y |X1, ..., Xp) \u221d P (Y ) p \u220f\nj=1\nP (Xj |Y )\nTo train the model, we need to estimate all the P (Y ) and P (Xj |Y ). The probabilities P (Y ) can be estimated by the frequencies of samples with label Y in the training set. For conditional probabilities P (Xj |Y ), the training is based on whether Xj is categorical or numerical. If Xj is a categorical feature, for all values x and y, we have P (Xj = x|Y = y) = P (Xj = x, Y = y)/P (Y = y) = \u2211\ni I[xij = x]I[yi = y]/ \u2211 i I[yi = y]. Thus we need counts \u2211\ni I[yi = y] and \u2211\ni I[xij = x]I[yi = y] to compute the conditional probabilities. If Xj is numeric, then based on the second assumption, the normal distribution p(Xj |Y ) is decided by E[Xj |Y ] and V ar[Xj |Y ]. Thus to compute the model we only need the following information: \u2211\ni I[yi = y], all \u2211\ni I[xij = x]I[yi = y] for categorical variables and all E[Xj |Y ] and V ar[Xj |Y ] for numerical variables.\nAn \u01eb-differentially private naive Bayes model mechanism is introduced in [52]. This mechanism relies on one additional assumption: all values for all features in the dataset are bounded by some known number. If the bound covers most of the Gaussian distribution, then both the bound assumption and Gaussian assumption hold approximately. Therefore the sensitivity of the information that is needed to compute the model can be calculated. The mechanism then adds noise to this information according to the Laplacian mechanism and computes the model. Although no analysis on utility is provided, it is easy to see that the noise on the parameters is O(1/n\u01eb).\nSometimes the (non-private) naive Bayes model is more accurate if we model the continuous features with histograms instead of a Gaussian distribution. However, in this case, many histograms may lead to high sensitivity. Thus as long as the Gaussian assumption is not far from the truth, there is no need to use histograms. A good assumption about a distribution can result in good performance. If in extreme cases the assumption is too far from the truth, we can represent those features with histograms in preprocessing. If the rule in preprocessing is independent of the private dataset, the preprocessing is privacyfree.\nAnother question is whether we can use the logarithms of counts \u2211\ni I[xij = x]I[yi = y] here, as we sometimes do in using the standard naive Bayes model. Clearly, we can apply the Laplacian mechanism to logarithms, however this change is useless. For example, suppose the true count is c, the noisy count is c\u0303 and we add noise to log(c + 1). According to the Laplacian mechanism, the sensitivity of log(c) is log 2. Thus the probability P ( \u02dc(c) = 1|c = 0) stays the same, while probabilities such as P ( \u02dc(c) = 9|c = 4) increases a lot. Such\ntransformation cannot reduce noise when the count is small, however it increases noise a lot when the count is large. Therefore, it is better to add noise to the counts directly."}, {"heading": "3.2 Linear Regression", "text": "Linear regression is a technique for predicting numerical values Y in which the value is modelled as a linear combination wTX of features X . Here, the vector w contains the weights corresponding to each feature and constitutes the set of parameters which must be optimized during training. To train the model, w is computed by minimizing square loss \u2211\ni(yi \u2212wTxi)2 over the training set. [56] assumes bounded sample space and proposes a differentially private mechanism for linear regression. As the loss function is analytic, the mechanism expands the function with Taylor expansion, approximates it with a low order approximation, and adds noise to the coefficients of the terms. The mechanism then finds the w that minimizes the approximate loss function. As the sensitivities of the coefficients are easy to compute, the Laplacian mechanism can ensure the differential privacy of the noisy approximation. Since no private information is used after adding noise, the output vector w here is also \u01eb-differentially private. Furthermore, the model that is decided by w is also differentially private."}, {"heading": "3.3 Linear SVM", "text": "Linear SVM is a linear classifier in which a vector w captures the model parameters. A linear SVM model outputs a score wTX for features X in a sample, and usually uses sign(wTX) as the label Y . The parameter w is computed by minimizing C \u2211\nimax(0, 1 \u2212 yi(wTxi)) + wTw/2, while C > 0 is an input parameter which sets the strength of prediction error.\nUnder the assumption that the sample space is bounded, the linear SVM model satisfies the following two conditions. First, it computes w by minimizing a strongly convex and differentiable loss function L(w). Second, a change of one sample results in bounded change in L\u2032(w). For linear SVM and all other models satisfying the two conditions, [5, 6] provide an output perturbation mechanism and an objective perturbation mechanism. The output perturbation mechanism first trains the model and then adds noise to it. The objective perturbation mechanism introduces noise by adding a carefully designed linear perturbation item to the original loss function. The w computed from the perturbed loss function is \u01eb-differentially private.\n[6] also provides a performance analysis of the objective perturbation mechanism. To achieve (\u03b1, \u03b2)-usefulness and \u01eb-differential privacy, the mechanism needs O( log(1/\u03b2)\u03b12 + 1 \u01eb\u03b1 + log(1/\u03b2) \u03b1\u01eb ) samples. As defined earlier, (\u03b1, \u03b2) usefulness provides a guarantee that with respect to the true loss function, the performance of the private model will be within a distance \u03b1 of that achieved by the true model with probability greater than 1\u2212 \u03b2."}, {"heading": "3.4 Logistic Regression", "text": "Logistic regression is model for binary classification. It makes prediction P (Y = 1|X) = 1/(1 + e\u2212wTX) given features X in a sample. The parameters w are trained by minimizing negative log-likelihood \u2211\ni log(1 + exp(\u2212yiwTxi)) over the training set.\nRegularized logistic regression differs from standard logistic regression in that the loss function includes a regularization term. Its w is computed by minimizing \u2211\ni log(1 + exp(\u2212yiwTxi)) + \u03bbwTw over the training set {(xi, yi)} while \u03bb > 0 is a hyperparameter which sets the strength of regularization.\nAssuming that the sample space is bounded, the mechanism in [56] (see Section 3.2) can be applied to make both models \u01eb-differentially private. Furthermore, the output perturbation and objective perturbation mechanism in [5, 6] (see Section 3.3) can ensure \u01eb-differential privacy for regularized logistic regression."}, {"heading": "3.5 Kernel SVM", "text": "Kernel SVM is a machine learning model that uses a kernel function K(, ), which takes two samples as input and outputs a real number. Different kernel functions lead to different SVM models. Kernel SVM can be used both for classification and regression. When used to classify a sample with features X , kernel SVM predicts the label Y = sign( \u2211\niwiK(X,xi)); when used for regression, kernel SVM predicts the quantity Y = \u2211\ni wiK(X,xi). In both cases {(xi, yi)} are training samples and wi are weights in the model to compute. Note the model includes the kernel function K(, ), all training data and a vector of weights {wi}ni=1. Although there exist many algorithms to train kernel SVM, I will only address those relevant to current differentially private versions.\nUnlike previous models, kernel SVMs contain all the training data. Therefore the techniques required to make differentially private kernel SVM mechanisms are different from those we have already described. In [6, 45], an idea for private kernel SVM is proposed. It works for all translation-invariant kernels, where there exists some function g(x) such that K(x1,x2) = g(x1 \u2212 x2)\u2200x1,x2. For example, radial basis function kernel is translation-invariant. The basic idea is to approximate kernel functions in the original sample space with a linear kernel in another space, so as to avoid publishing training data. It first constructs a space independent of private training data and then projects data from the original sample space to that space. According to [44], the kernel function of two samples in the original sample space can be approximated by the inner product of their projections in the new space. Thus the kernel SVM model turns out to be a linear SVM model in the new space and we can use private linear SVM mechanisms mentioned above. Furthermore, the non-private projection can be published, thus future data can be projected to the same space and then use the parameters from private linear SVM to predict. In this way, the mechanism transforms a kernel SVM model training problem into a linear SVM training problem. To achieve both (\u03b1, \u03b2)-usefulness w.r.t. to predictions on any sample\nand \u01eb-differential privacy, this mechanism needs n = O( log 1.5(1/\u03b1\u03b2) \u01eb\u03b13 ) samples.\nThe previous mechanism can not be applied to kernel functions that are not translation-invariant, such as polynomial kernel or sigmoid kernel. Therefore [26] proposes another private kernel SVM algorithm for all RKHS kernels. An RKHS kernel means that there is some function \u03c6(x) that projects x onto another space such that K(x1,x2) equals the inner product of \u03c6(x1) and \u03c6(x2). This mechanism seems similar to the one previously described (where the projection can be seen as an approximate to \u03c6(x)), however here projection doesn\u2019t need to be explicit.\nThe Test Data-independent Learner (TTDP) mechanism in [26] publishes a private kernel SVM model satisfying (\u01eb, \u03b4)-differential privacy as follows. Intuitively, it trains a non-private kernel SVM model f(x) from the private data and then approximates it in a differentially private way. The private model g(x) is trained iteratively. First, the mechanism sets g(x) = 0. Then it computes a non-private model f(x). Next, it constructs a finite set Z from the unit sphere that represents the sample space. Each iteration consists of three steps. In the first step, the mechanism selects a point z \u2208 Z where g(z) and f(z) disagree the most. This selection is based on the exponential mechanism, and |g(z)\u2212f(z)| is used as the score function. In the second step, a noisy difference |g(z)\u2212f(z)|+\u03b7 is computed while \u03b7 is Laplacian noise. In the third step, the mechanism tests whether the noisy difference exceeds some threshold. If not, the mechanism proceeds to the next iteration; if it does exceed the threshold, the mechanism updates g(x) to be closer to f(x) at point z. After many iterations, g(x) may approximate f(x). To achieve both (\u03b1, \u03b2)-usefulness w.r.t. to predictions on any sample in the unit ball and (\u01eb, \u03b4)-differential privacy, this mechanism needs n = O( log3 1 \u03b4 log1.5 1 \u03b2\n\u03b11.5\u01eb0.75 ) samples."}, {"heading": "3.6 Decision Tree Learning", "text": "Learning a decision tree classifier involves partitioning the sample space and assigning labels to each partition. The training algorithm for a decision tree classifier consists of a tree building process and a pruning process. In the tree building process, first the entire sample space and all samples are put in the root partition. Then the algorithm iteratively selects an existing partition, selects a variable based on the samples in that partition and a score function such as information gain or Gini index, and partitions the sample space (and the samples corresponding to that partition) according to the variable selected. If the selected variable is categorical, usually each value of that variable corresponds to a partition; if the variable is numerical, then some thresholds will be selected and the partitioning is based on those thresholds. The partitioning process ends when the spaces corresponding to all partitions are small enough or the numbers of samples in each partition are too small. After building the tree, the pruning process removes unnecessary partitions from the tree and merges their spaces and samples to their parents.\n[24] proposes an \u01eb-differentially private mechanism. The mechanism con-\nstructs N decision trees and uses the ensemble to make classification. When constructing a decision tree Ti, it randomly partitions the sample space into partitions Pi1, ..., Pimi without using private data and computes Countijy , noisy counts of samples with each label y in partition Pij . When predicting the label of a sample X , it looks for all the partitions P1a1 , ..., PNaN from all trees T1, ..., TN that include X , sums the counts of samples with each label from all those partitions Sy = \u2211n i=1 Countiaiy, and computes the probabilities of label y\u2032 by P (Y = y\u2032|X) = Sy\u2032/ \u2211\ny Sy. [19] proposes another \u01eb-differentially private decision tree algorithm. The mechanism is based on the assumption that all features are categorical, in order to avoid selecting partition points for any feature. In the partitioning process, the mechanism uses the exponential mechanism to select the variable with largest score (for example, information gain or Gini index) differentially privately. Each time a partition reaches a pre-determined depth, or the number of samples in that partition is about the same scale as random noise, or the sample space corresponding to that partition is too small, the mechanism stops operating on that partition. It then assigns to that partition a noisy count of samples with each label. After the partitioning process has completed altogether, these noisy counts are used to decide whether to remove those nodes without having to consider privacy."}, {"heading": "3.7 Online Convex Programming", "text": "Many machine learning techniques, such as logistic regression and SVM, specify optimization problems which must then be solved to find the optimal parameters. Online algorithms, such as gradient descent, which consider examples one at a time, are widely used for this purpose. To that a machine learning algorithm is differentially private, it is therefore important to demonstrate that the optimization algorithm doesn\u2019t leak information.\nOnline convex programming (OCP) solves convex programming problems in an online manner. The input to an OCP algorithm is a sequence of functions (f1, ..., fT ) and the output is a sequence of points (w1, ..., wT ) from a convex set C. The algorithm is iterative and starts from a random point w0. In the t-th iteration, the algorithm receives the function ft and outputs a point wt+1 \u2208 C according to (f1, ..., ft) and (w1, ..., wt). The target of the OCP algorithm is to minimize regret, which is defined as\nR = T \u2211\nt=1\nft(wt)\u2212 min w\u2208C\nT \u2211\nt=1\nft(w).\nNote that the input here is a sequence of functions instead of samples. There are many methods to find wt+1 \u2208 C according to (f1, ..., ft) and (w1, ..., wt). [25] provides (\u01eb, \u03b4)-differentially private versions for two of them: the Implicit Gradient Descent (IGD) and the Generalized Infinitesimal Gradient Ascent (GIGA) given all the functions are L-Lipschitz continuous for some\nconstant L and \u03b7-strongly convex. IGD first computes\nw\u0302t+1 = arg min w\u2208C\n(\n1 2 \u2016w \u2212 wt\u20162 + 1 \u03b7t ft(w)\n)\nand projects w\u0302t+1 onto C to get the output wt+1. GIGA first computes\nw\u0302t+1 = wt \u2212 1\n\u03b7t \u2207ft(wt)\nand then does the projection. Both algorithms ensure bounded sensitivity of wt+1 given wt. The private mechanism in [25] adds Gaussian noise to every w\u0302t before it is projected to wt to preserve privacy, and then use the noisy wt for the future computation. Given T functions(samples), the expected regret by this (\u01eb, \u03b4)-differentially private mechanism is O ( \u221a\nT \u01eb ln 2 T \u03b4\n)\n."}, {"heading": "4 Differentially Private Unsupervised Learning", "text": "Unsupervised learning describes the setting when there are no labels associated with each training example. In the absence of labels, unsupervised machine learning algorithms find structure in the dataset. In clustering, for example, seeks to find distinct groups to which each datapoint belongs. It can be useful in many contexts such as medical diagnosis, to know of an individuals member ship in a group which shares certain specific characteristics. However, releasing the high-level information about a group may inadvertently leak information about the individuals in the dataset. Therefore it is important to develop differentially private unsupervised machine learning algorithms."}, {"heading": "4.1 K-means clustering", "text": "K-means is a commonly used model in clustering. To train the model, the algorithm starts with k randomly selected points which represent the k groups, then iteratively clusters samples to the nearest point and updates the points by the mean of the samples that are clustered to the points.\n[42] proposes an (\u01eb, \u03b4)-differentially private k-means clustering algorithm using the sample and aggregate framework. The mechanism is based on the assumption that the data are well-separated. \u2018Well separated\u2019 means that the clusters can be estimated easily with a small number of samples. This is a prerequisite of the sample and aggregate framework. The mechanism randomly splits the training set into many subsets, runs the non-private k-means algorithm on each subset to get many outputs, and then uses the smooth sensitivity framework to publish the output from a dense region differentially privately. This step preserves privacy while the underlying k-means algorithm is unchanged.\nAny modifications on the k-means clustering algorithm (such as k-means++) can be used in the sample step, with the sole restriction that such modifications leave intact the property that the algorithm can be estimated with a small\nnumber of samples. Additionally, if the sample space is bounded and the number of samples surpasses a threshold, there is a bound on the noise added. However this bound is not directly related to the number of samples in the dataset."}, {"heading": "5 Differentially Private Dimensionality Reduc-", "text": "tion\nIn machine learning contexts, when data is high dimensional, it is often desirable learn a low-dimensional representation. Lower dimensional datasets yield models with less degrees of freedom and tend to be less prone to overfitting. From a differential privacy perspective, lower dimensional representations are desirable because they tend to have lower sensitivity.\nFeature selection is one technique for dimensionality reduction, in which a subset of features is kept from an original feature space. Principal component analysis (PCA), on the other hand is a matrix factorization technique in which a linear projection of the original dataset into a low dimensional space is learned such that the new representation explains as much of the variance in the original dataset as possible."}, {"heading": "5.1 Feature Selection", "text": "[53] proposes an \u01eb-differentially private feature selection, PrivateKD, for classification. PrivateKD is based on the assumption that all features are categorical and each feature has finite possible values. For any set of features S, it defines a function F (S) which tells how many pairs of samples from different classes can features in S distinguish. The set of selected features S\u2032 is initialized to \u2205. Then a greedy algorithm adds new features one by one to S\u2032. When selecting a feature to add, the algorithm uses the exponential mechanism to select the feature that can lead to the largest increase of F (S\u2032). The paper provides a utility guarantee for the special case where the cardinality of sample space m and the number of features d have the relation m = d \u2212 1. In that case, except probability O(1/poly(m)) (poly(m) means a polynomial expression of m), F (S\u2032) \u2265 (1\u2212 1/e)F (Soptimal)\u2212O(logm/\u01eb).\n[27] proposes an (\u01eb, \u03b4)-differentially private algorithm for feature selection when the target function is stable. Unlike the previous paper, this paper doesn\u2019t explicitly state the algorithm for feature selection. Instead, it only requires the selection algorithm to be stable. By \u2018stable\u2019, we mean that either the value of function as calculated on the input dataset doesn\u2019t change when some samples in the set change, or that the function can output the same result on a random subset from the input dataset with high probability. For the first kind of functions, the mechanism uses the smooth sensitivity framework in [42] to select features. If adding or removing any log(1/\u03b4\u03b2)\u01eb samples from the input dataset doesn\u2019t change the selection result, then the algorithm can output the correct selection result with probability 1\u2212 \u03b2.\nFor the second kind of functions, the mechanism uses an idea similar to the sample and aggregate framework in [42]: it creates some bootstrap sets from the private dataset, selects features non-privately on each set, and counts the frequencies of feature sets output by the algorithm. Intuitively, if the number of samples is large and the features set is not too large, there is high probability that the correct output is far more frequent than any other one. Thus, the mechanism can release the most frequently selected set of features. If a random subset of the input dataset with \u01eb/(32 log(1/\u03b4)) samples outputs the same selection result with probability at least 3/4, then the mechanism outputs the correct solution with probability at least 1\u2212 \u03b4."}, {"heading": "5.2 Principal Component Analysis", "text": "Principal Component Analysis (PCA) is a popular method in dimension reduction. It finds k orthogonal directions on which the projections of data have largest variance. The original data can then be represented by its projection onto those k directions. Usually k is much smaller than the dimension of sample space. Thus the projection greatly reduces the dimensionality of the data. It is well-known that this analysis is closely related to eigen-decomposition: if we rank the eigenvectors of matrix A = V ar[X ] according to the corresponding eigenvalues \u03bb1 \u2265 \u03bb2 \u2265 ... \u2265 \u03bbp, then the first k eigenvectors are the k directions.\nThere are two differentially private mechanisms to select eigenvectors iteratively. The iterative methods are based on the spectral decomposition, which ensures that if the components corresponding to the first i \u2212 1 eigenvectors of A are subtracted from A, then the i-th largest eigenvector becomes the largest eigenvector of what remains. Therefore the process of selecting the largest k eigenvectors can be replaced by repeatedly finding the first eigenvector and removing the component corresponding to the selected eigenvector. The following two mechanisms both make use of this idea but differ on how to select the first eigenvector.\nAn (\u01eb, \u03b4)-differentially private mechanism is proposed in [21]. The mechanism uses the power method: Anv/\u2016Anv\u2016 converges to the first eigenvector of A if v is not orthogonal to the first eigenvector. It randomly starts with a unitlength vector v, then iteratively updates v with (Av + \u03b7i)/\u2016Av + \u03b7i\u2016 while \u03b7i is Gaussian noise in the i-th iteration. Since it is exceedingly improbable that a random vector is orthogonal to the first eigenvector, the vector v will get close to the first eigenvector. However due to the noise, v cannot converge with arbitrary accuracy. Thus it outputs v after a fixed number of iterations and proceed to find the next largest eigenvector. A utility guarantee is provided on the power method, which outputs the first eigenvector. However there is no direct guarantee on the k eigenvectors. For each eigenvector a output by running the power method on matrix A, the distance from the first eigenvector (\u2016Aa\u2016/\u2016a\u2016 \u2212 \u03bb1 while \u03bb1 is the first eigenvalue) is O(( \u221a\nlog(1/\u03b4) logn)/\u01eb). [31] provides an \u01eb-differentially private mechanism for principal component analysis. According to the property that the first eigenvector v of A is the unitlength vector that maximizes vTAv, the mechanism uses H(X,v) = vTAv as\nthe score function in the exponential mechanism to select the first eigenvector from the set {v : vTv = 1} differentially privately. The selection algorithm is specially designed to be computable in reasonable time. This paper also provides two proofs on utility of this mechanism. For any 0 < \u03b4 < 1 and privacy budget \u01eb, if the first eigenvalue of matrix A, \u03bb1 > O(ln(1/\u03b4)/(n\u01eb\u03b4)), then the first eigenvector v has the property E[vTAv] \u2265 (1 \u2212 \u03b4)\u03bb1. For any 0 < \u03b4 < 1 and privacy budget \u01eb, if the first eigenvalue \u03bb1 > O(1/(n\u01eb\u03b4\n6)), the k+1-th eigenvalue is denoted \u03bbk+1, and the k-rank approximation matrix output is denoted Ak, then the largest eigenvalue of A \u2212 Ak is smaller than \u03bbk+1 + \u03b4\u03bb1 with large probability.\nNot all differentially private approaches to PCA rely on iterative algorithms. [7] proposes an \u01eb-differentially private mechanism, PPCA, to compute k largest eigenvectors at the same time. The mechanism uses the property that the first k eigenvectors ofA are the columns of the p\u00d7k matrix V = argmaxV :V TV =Ik tr(V TAV ). Therefore, it uses H(X,V ) = tr(V TAV ) as the score function and selects V from the set of matrices {V : V TV = Ik} according to the exponential mechanism. A Gibbs sampler method is used here to select the matrix V . This paper provides a guarantee on the first eigenvector. For any 0 < \u03c1, \u03b7 < 1, if the sample size n = O (\n1 \u01eb(1\u2212\u03c1)\n(\nlog 1\u03b7 + log 1 1\u2212\u03c12\n))\n, then the inner product of output first\neigenvector and true eigenvector is larger than \u03c1 with probability 1\u2212 \u03b7."}, {"heading": "6 Statistical Estimators", "text": "Statistical estimators calculate approximations of quantities of interest based upon the evidence in a given dataset. Simple examples include the population mean and variance. While estimators are clearly useful, they may potentially leak information about the individuals contained in the dataset, especially when the dataset is small or features are rare. Therefore, to protect privacy, it is necessary to develop differentially private estimators."}, {"heading": "6.1 Robust Statistics Estimator", "text": "[14] proposes an (\u01eb, \u03b4)-differentially private mechanism for robust statistical estimators. Roughly speaking, a statistical estimator produces an estimate of a vector (such as the mean and variance of a Gaussian distribution) based on the input dataset. The estimator T can be seen as a function that maps a dataset D to the output vector T (D). Most statistical estimators converge when the number of samples tends to infinity and the samples are i.i.d. drawn from some distribution P . When the estimator does converge, the limit lim|D|\u2192+\u221e T (D) is denoted T (P ). The definition of robust estimator is based on the stability of estimates. An estimator is robust if for any element x in the sample space, the following limit exists limt\u21920(T ((1\u2212 t)P + t\u03b4x)\u2212 T (P ))/t. The distribution (1\u2212 t)P + t\u03b4x means that with probability 1\u2212 t the sample is from P and with probability t the sample is x.\nThe output of a robust estimator doesn\u2019t change much if a small number of samples change. Based on the property, [14] comes up with a ProposeTest-Release framework. The framework is based on the assumption that the statistics are in Rp. It divides Rp into small cubes, then computes the statistics T (D) from a dataset D and the number of sample changes needed to make T (D) fall into another cube. If the number is large, the statistics are stable and thus the mechanism can add Laplacian noise to the statistics to make it private; if the number is small, then the mechanism outputs \u22a5, which means it fails. When the number of samples tends to infinity, and the samples are i.i.d. drawn, the framework output is asymptotically equivalent to a non-private robust estimator.\nBased on this framework, [14] proposes three mechanisms for interquartile range estimation, trimmed mean and median, and linear regression, respectively. When applying the framework to linear regression, the framework uses a robust estimator to learn a model from the training set {(xi, yi)}\nw\u0302 = argmin w\n\u2211\ni\n|yi \u2212wTxi| \u2016xi\u2016\ninstead of minimizing the mean square error. Given n samples, the linear regression estimator can successfully output a model with probability 1\u2212O(n\u2212c lnn) for some constant c. Additionally, its output converges to the true linear regression parameter when n tends to infinite.\n[4] explores robust estimators in another way. They prove that if effect of one sample is bounded by O(1/n), and the range of T (P ) is bounded, then the smooth sensitivity framework provides bounded error.\nHowever, if T (P ) is not bounded, and if for any value \u03c4 in an infinite range, there exists some P such that T (P ) = \u03c4 , then the error of any \u01eb-differentially private mechanism cannot be upper bounded.\nHere, a bound on the effect of one sample means that there exists a uniform upper bound M(P ) for distribution P , such that for all distributions P \u2032 satisfying |P \u2212 P \u2032| \u2264 O( \u221a\n1/n), all x in the sample space, T ((1\u2212 1/n)P + \u03b4x/n)\u2212 T (P )) \u2264 M(P )/n.\nTo achieve both (\u01eb, \u03b4)-differential privacy and (\u03b1, \u03b2)-usefulness, the smooth\nsensitivity framework requires O (\nln(1/\u03b2) \u01eb\u03b1 + ln2(1/\u03b4) ln2(\u03b1\u01eb/ ln(1/\u03b2)) \u01eb2 ln(1/\u03b2)\n)\nsamples."}, {"heading": "6.2 Point Estimator", "text": "[49, 50] give a differentially private mechanism for point estimation. Using the notation in 6.1, the mechanism splitsD into k subsets with equal size {D1..., Dk} randomly and estimates parameters {T (D1), T (D2), ..., T (Dk)} on each subset. Then it uses a differentially private mean of all T (Di) to approximate T (D). The mean is computed in two steps. First, if the space of parameters is unbounded, it computes two quantiles and truncates all estimates according to the quantiles and the sample size. Second, the mechanism adds Laplacian noise to the mean of truncated values and publishes the noisy mean. When the space of possible\nparameters are bounded, then the mechanism only executes the second step and becomes \u01eb-differentially private. When the space of possible parameters is not bounded, then both steps have to be executed and the mechanism is (\u01eb, \u03b4)-differentially private.\nThe papers give sufficient conditions under which the mechanism is as accurate as non-private estimators asymptotically. The conditions are listed below. n is the number of samples in D and \u03c3P is a real number. For models that don\u2019t converge to a fixed point (for example, some EM algorithms) or models with varying numbers of features (such as Kernel SVM), there is no such guarantee.\nT (D)\u2212 T (P ) \u03c3P / \u221a n \u2192 N(0, 1) when n \u2192 +\u221e\nE[T (D)]\u2212 T (P ) = O(1/n)\nE\n[\n( |T (D)\u2212 T (P )| \u03c3P / \u221a n\n)3 ]\n= O(1)"}, {"heading": "6.3 M-estimator", "text": "[33] proposes an \u01eb-differentially private mechanism for M-estimator. Unlike the robust estimator above, the definition of an M-estimator depends on the function from which the estimates come. M-estimation relies on a function m(, ), which takes a sample and a parameter \u03b8 as input and outputs a real number. An M-estimator estimates the parameter \u03b8 by computing\n\u03b8\u0302 = argmin \u03b8\n1\nn\n\u2211\ni\nm(xi, \u03b8)\nThe mechanism in [33] first divides the sample space ([0, 1]d) into many small cubes without using private data. Then it adds Laplacian noise to the counts of samples in the cubes and computes the density function in each cube by dividing the noisy count corresponding to that cube by the volume of the cube. The noisy density function leads to a noisy target function in the minimization problem above, and the noisy target function leads to a noisy minimum \u03b8\u0303. The noisy minimum can be released as an estimate.\nThe output parameter will converge to the true parameter based on the distribution of training data with speed O(n\u22121/2 + ( \u221a logn/n)2/(d+2)) under some regularity conditions. Here d is the number of features and n is the number of samples."}, {"heading": "7 Learning in Private Data Release", "text": "Many differentially private data release mechanisms have been described, such as [3, 1, 18, 10]. In this section, we focus on data release mechanisms that are either useful for machine learning or based on machine learning algorithms.\nMany papers use partition based algorithms to release data. [55] assumes that the density function is smooth, [8, 57] assume that the data\u2019s format permits it to be organized in a tree, and [41] assumes that partitions can preserve most important information for further data mining. All these assumptions motivate partitioning the sample space and publishing counts in each partition. With respect to the mechanism design, the mechanisms can be divided into two groups. [41] first partitions the sample space using the exponential mechanism and then adds noise to the counts using the Laplacian mechanism. Some others ([8, 57, 55]) generate noisy counts with the Laplacian mechanism for each cell and then partition according to the noisy counts.\nSome data release mechanisms don\u2019t depend on partitioning. Some of them assume the private data is fit well by some family of models. They select an optimal model from that family privately and then generate new data according to the selected model. Some others assume some property (like sparsity) of the data, and propose mechanisms that can make use of that property.\n[40, 46] publish a graph generator model based on the assumption that the private data is fit well by some parametrized generative model. Though the two mechanisms use different generative models, both train the model first, add Laplacian noise to the parameters of the model, and then use the noisy model to generate a new graph.\n[54] represents the network structure by using a statistical hierarchical random graph model. Unlike the two models above, the number of parameters in this model is proportional to the number of nodes in the graph. Thus we will introduce too much noise if we use the Laplacian mechanism to publish the model. As the parameter space is very large and no score function exists, which is both simple and meaningful, it is not easy to use the exponential mechanism directly. To overcome this difficulty, the authors propose a mechanism based on the Markov Chain Monte Carlo procedure. It uses the MetropolisHastings algorithm to draw a model from the distribution in the exponential mechanism given the score function is the likelihood function. Though the likelihood function is complicated w.r.t. the whole space of parameters, it is simple w.r.t. one parameter if all the others are fixed. Thus the Markov Chain Monte Carlo procedure is possible. The graph can be reconstructed from the output model after many iterations.\nSometimes corresponding to a large private dataset of interest, there exists a similarly structured but smaller publicly available dataset. [28] makes use of a public dataset, assigning weights to its examples to embed information contained in the private data. The mechanism assumes the existence of such a public dataset, but does not require that it is drawn from the same distribution as the private data. First they use public/private datasets as positive/negative samples in a logistic regression model. They train a noisy logistic regression model and assign weights based on the noisy model to all public samples. The weighted public dataset can replace the private dataset in future data mining. If the weighted set is used in measuring the expectation E[f(X)] for some function f(X) while X is from the distribution of private data, the standard deviation of the estimate is O(1/ \u221a n). If the assumption holds exactly, then the estimate\nis asymptotically unbiased. [30] assumes that samples have binary labels and that they are fit by a linear discriminant analysis (LDA) model. The idea is similar to the exponential mechanism: The exponential mechanism computes the non-private LDA model from the private dataset D. Then, it uses the distance between the non-private model parameters and the LDA parameters trained from another dataset D\u2032 as the score function, and draws a dataset differentially privately.\nThe mechanism in this paper, however, first computes a private LDA model by the Laplacian mechanism, and then draws a dataset that minimizes the distance. Such an output dataset can preserve the classification information from the private data.\n[35] assumes that the data matrix is sparse, thus the mechanism can make use of results from compressive sensing research. Informally speaking, if we randomly project the high-dimensional data matrix to a low-dimensional space, and then attempt to recover the high-dimensional matrix using the low-dimensional embedding and the constraint that the recovered matrix is sparse, there is high probability that the recovered matrix exactly matches the original one. [35] first randomly projects the data matrix, then adds noise to the compressed information, and reconstructs data from the noisy compressed information. As the dimension of compressed information is much smaller than that of the original data matrix, the scale of Laplacian noise needed to preserve \u01eb-differential privacy can be reduced from O( \u221a n/\u01eb) to O(log n/\u01eb) given n samples."}, {"heading": "8 Theoretical Results", "text": "[32] studies the general properties of private classifiers, instead of individual learning models. They first define a problem to be learnable if there is an algorithm that can output a highly accurate model with a large probability given enough data. The accuracy here is measured as the percentage of samples that are correctly classified. They further claim that if a problem is learnable, then it can be learned differentially privately. The mechanism they constructs takes the number of correctly classified samples as a score function and uses the exponential mechanism to draw the best model, which it calls the best hypothesis in a class.\n[39] formulates differentially private learning in an information theoretic framework. The paper uses a concept in information theory, PAC-Bayesian bound. This concept is for parametrized models that have bounded loss functions and it uses the Bayesian learning framework. PAC-Bayesian bound PAC(\u03c0\u0303, \u03c0, \u01eb, \u03bb) is a function of the posterior distribution \u03c0\u0303 of model parameters, the prior distribution \u03c0 of parameters, a positive number \u01eb representing the privacy budget, and another number \u03bb \u2208 (0, 1), which is related to the strength of the bound. With probability 1\u2212 \u03bb and prior distribution \u03c0, PAC(\u03c0\u0303, \u03c0, \u01eb, \u03bb) upper bounds the expected loss on the true distribution if the model parameter is from the distribution \u03c0\u0303. This bound varies given different \u01eb. According to [39], the output of the exponential mechanism follows the posterior distribution that minimizes\nPAC-Bayesian bound. Note that the PAC-Bayesian bound upper bounds the loss function. It is possible that other mechanisms have loss functions that achieve a better bound. Therefore the conclusion in [39] doesn\u2019t necessarily mean that the exponential mechanism is the best."}, {"heading": "9 Discussion", "text": "The papers reviewed by this survey address the question of how to train a differentially private model with as little noise as possible. To summarize, there are generally four guiding principles for reducing the scale of noise. First, adding noise only one time is usually better than adding noise many times. This is because if we add noise many times, we have to split the privacy budget into many smaller portions and let each noise addition procedure use one portion. Because the budget allocated to each procedure is small and the scale of noise is inversely related to the privacy budget, the amount of noise added in each procedure is large. Furthermore, when we aggregate the outputs, the noise can grow even larger. Therefore one-time noise addition is usually better.\nFor example, when we train a logistic regression model, we can add noise to the training process, the target function or the final model. Adding noise to the target function is a one-time procedure. This is also true for the final model. However, as the training process is iterative, adding noise during training requires adding noise many times. According to our experience, noise addition in training process leads to significantly worse performance.\nSecond, lower global sensitivity (compared to the result) leads to smaller noise. In one strategy to lower global sensitivity, some queries be approximated by combining the results of other queries, each of which have far lower global sensitivities than the original query. For example, [52] adds noise to the counts that generate the naive Bayes model instead of conditional probabilities of the model directly. The global sensitivity of each conditional probability is 1, which is too high to be useful. The global sensitivity of each underlying count is 1, which is much lower compared to the counts. By adding noise to those counts, we encounter lower global sensitivity.\nAnother approach is to modify the model. For example, [45] transforms kernel SVM to linear SVM, and [14] uses a robust linear regression model to replace the commonly used model.\nThird, use of public data, when available, can reduce the noise in some cases. For a private dataset, there is often a smaller public dataset drawn from a similar population. This public dataset can be from a previous leak or by consent of data owners. Because differentially private mechanisms distort private data, the smaller public dataset sometimes provides similar or better utility. According to [28, 29], such a public dataset can enhance the performance of differentially private mechanisms.\nFourth, for some models, iterative noise addition may be reasonable. There are some times when the sensitivity of output model parameters is very large\nbut the iterative algorithm has smaller sensitivity. This statement may seem counterintuitive, as the sum of sensitivities of all iterations should be similar to the sensitivity of the model parameters. However, in some cases, the sensitivity of each iteration is determined by the parameter before that iteration. Thus the sum of sensitivities of those iterations in fact relies on the training path. Excepting some extreme cases, the sum can be much smaller than the sensitivity of the model parameters. In this case, it seems necessary to add noise in the iterations.\nFor those models, one can consider trying the MCMC-based algorithm as in [48]. Likelihood function or loss function can be used as score functions and the Metropolis Hastings algorithm ensures that the output is from the same distribution as that in the exponential mechanism. This idea is still not widely used, however it seems possible that it improves learning performance.\nIn addition to these four ideas, some other issues warrant attention. For example, most differentially private mechanisms use clean and complete data as input, which is not always available in practice. Furthermore, traditional methods for missing data or pre-processing may not satisfy differential privacy. Thus mechanisms that can deal with incomplete data are desired. Such mechanisms can either release data, or be combined with other differentially private learning mechanisms.\nWhen private data is discussed, medical data is typically offered as an example application. However, medical datasets are often not relational. They may be temporal, and sometimes structural. Although we can transform such data, the transformation may lose some important information and increase sensitivity. Therefore, mechanisms specially designed for such data are required.\nAnother important question is whether privacy can be free, i.e., achieved at no cost to utility in differentially private learning. For privacy to be free, the noise required to preserve privacy might need to be smaller than noise from sample randomness. In that case, it wouldn\u2019t change the magnitude of noise to take privacy into account. For example, [50] proves that (\u01eb, \u03b4)-differential privacy is free for learning models satisfying a certain set of conditions. The mechanism in [6] ensures free \u01eb-differential privacy for regularized logistic regression models and linear SVM models, where noise from sample randomness is O(1/ \u221a n) and the noise to preserve privacy is O(1/n). The mechanism in [28] also proves that the effect of noise brought by differential privacy is O(1/n), while the effect from sample randomness is O(1/ \u221a n).\nWe should also consider the extent to which privacy is compatible with and related to the idea of generalization in machine learning. Intuitively, machine learning algorithms seek to generalize patterns gleaned from a training set avoiding the effects of sample randomness. Ideally, these algorithms should be robust to small changes in the empirical distribution of training data. A model which fits too heavily to individual examples loses generalizability and is said to overfit. Perhaps the goals of differential privacy and generalization are compatible."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Kamalika Chaudhuri for her comments. The authors are in part funded by NLM(R00LM011392)."}], "references": [{"title": "Privacy, accuracy, and consistency too: a holistic solution to contingency table release", "author": ["Boaz Barak", "Kamalika Chaudhuri", "Cynthia Dwork", "Satyen Kale", "Frank McSherry", "Kunal Talwar"], "venue": "In ACM SIGACT-SIGMOD- SIGART Symposium on Principles of Database Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Practical privacy: the SuLQ framework", "author": ["Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim"], "venue": "In ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "A learning theory approach to non-interactive database privacy", "author": ["Avrim Blum", "Katrina Ligett", "Aaron Roth"], "venue": "In ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Convergence rates for differentially private statistical estimation", "author": ["Kamalika Chaudhuri", "Daniel Hsu"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Privacy-preserving logistic regression", "author": ["Kamalika Chaudhuri", "Claire Monteleoni"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Differentially private empirical risk minimization", "author": ["Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Near-optimal differentially private principal components", "author": ["Kamalika Chaudhuri", "Anand D. Sarwate", "Kaushik Sinha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Publishing set-valued data via differential privacy", "author": ["Rui Chen", "Noman Mohammed", "Benjamin C.M. Fung", "Bipin C. Desai", "Li Xiong"], "venue": "In International Conference on Very Large Data Bases,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Personal privacy vs population privacy: learning to attack anonymization", "author": ["Graham Cormode"], "venue": "In International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Differentially private summaries for sparse data", "author": ["Graham Cormode", "Cecilia M. Procopiuc", "Divesh Srivastava", "Thanh T.L. Tran"], "venue": "In International Conference on Database Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Lower bounds in differential privacy", "author": ["Anindya De"], "venue": "In Theory of Cryptography,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Differential privacy. In Encyclopedia of Cryptography and Security (2nd Ed.)", "author": ["Cynthia Dwork"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Our data, ourselves: Privacy via distributed noise generation", "author": ["Cynthia Dwork", "Krishnaram Kenthapadi", "Frank McSherry", "Ilya Mironov", "Moni Naor"], "venue": "In International Conference on the Theory and Applications of Cryptographic Techniques,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Differential privacy and robust statistics", "author": ["Cynthia Dwork", "Jing Lei"], "venue": "In ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In Theory of Cryptography Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Boosting and differential privacy", "author": ["Cynthia Dwork", "Guy N. Rothblum", "Salil P. Vadhan"], "venue": "In FOCS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Differential privacy for statistics: What we know and what we want to learn", "author": ["Cynthia Dwork", "Adam Smith"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Adaptive differentially private histogram of low-dimensional data", "author": ["Chengfang Fang", "Ee-Chien Chang"], "venue": "In Privacy Enhancing Technologies,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Data mining with differential privacy", "author": ["Arik Friedman", "Assaf Schuster"], "venue": "In International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Composition attacks and auxiliary information in data privacy", "author": ["Srivatsava Ranjit Ganta", "Shiva Prasad Kasiviswanathan", "Adam Smith"], "venue": "In KDD,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Beyond worst-case analysis in private singular vector computation", "author": ["Moritz Hardt", "Aaron Roth"], "venue": "In Computing Research Repository,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Privacy and data-based research", "author": ["Ori Heffetz", "Katrina Ligett"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Resolving individuals contributing trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays", "author": ["Nils Homer", "Szabolcs Szelinger", "Margot Redman", "David Duggan", "Waibhav Tembe", "Jill Muehling", "John Pearson", "Dietrich Stephan", "Stanley Nelson", "David Craig"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "A practical differentially private random decision tree classifier", "author": ["Geetha Jagannathan", "Krishnan Pillaipakkamnatt", "Rebecca N. Wright"], "venue": "In International Conference on Data Mining Workshops,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Differentially private online learning", "author": ["Prateek Jain", "Pravesh Kothari", "Abhradeep Thakurta"], "venue": "In Conference on Learning Theory,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Differentially private learning with kernels", "author": ["Prateek Jain", "Abhradeep Thakurta"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Application of rough sets in the presumptive diagnosis of urinary system diseases", "author": ["J.Czerniak", "H.Zarzycki"], "venue": "In Artifical Inteligence and Security in Computing Systems, ACS\u20192002 9th International Conference Proceedings,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Differential privacy based on importance weighting", "author": ["Zhanglong Ji", "Charles Elkan"], "venue": "In Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Differentially private distributed logistic regression using private and public data", "author": ["Zhanglong Ji", "Xiaoqian Jiang", "Shuang Wang", "Li Xiong", "Lucila Ohno- Machado"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Differential-private data publishing through component analysis", "author": ["Xiaoqian Jiang", "Zhanglong Ji", "Shuang Wang", "Noman Mohammed", "Samuel Cheng", "Lucila Ohno-Machado"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "On differentially private low rank approximation", "author": ["Michael Kapralov", "Kunal Talwar"], "venue": "In ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "What can we learn privately", "author": ["Shiva Prasad Kasiviswanathan", "Homin K. Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "venue": "In IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Differentially private M-estimators", "author": ["Jing Lei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "t-closeness: Privacy beyond k-anonymity and l-diversity", "author": ["Ninghui Li", "Tiancheng Li", "Suresh Venkatasubramanian"], "venue": "In International Conference on Data Engineering,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Compressive mechanism: utilizing sparse representation in differential privacy", "author": ["Yang D. Li", "Zhenjie Zhang", "Marianne Winslett", "Yin Yang"], "venue": "In Workshop on Privacy in the Electronic Society,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "l-diversity: Privacy beyond kanonymity", "author": ["Ashwin Machanavajjhala", "Johannes Gehrke", "Daniel Kifer"], "venue": "In International Conference on Data Engineering,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "Privacy integrated queries: an extensible platform for privacy-preserving data analysis", "author": ["Frank McSherry"], "venue": "In SIGMOD Conference,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Mechanism design via differential privacy", "author": ["Frank McSherry", "Kunal Talwar"], "venue": "In FOCS, pages 94\u2013103,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Differentially-private learning and information theory", "author": ["Darakhshan J. Mir"], "venue": "In International Conference on Extending Database Technology Workshops,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "A differentially private graph estimator", "author": ["Darakhshan J. Mir", "Rebecca N. Wright"], "venue": "In International Conference on Data Mining Workshops,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Differentially private data release for data mining", "author": ["Noman Mohammed", "Rui Chen", "Benjamin C.M. Fung", "Philip S. Yu"], "venue": "In International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Smooth sensitivity and sampling in private data analysis", "author": ["Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "venue": "In ACM SIGACT-SIGMOD- SIGART Symposium on Principles of Database Systems,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "The composition theorem for differential privacy", "author": ["Sewoong Oh", "Pramod Viswanath"], "venue": "In Computing Research Repository,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "Learning in a large function space: Privacy-preservingmechanisms for SVM learning", "author": ["Benjamin I.P. Rubinstein", "Peter L. Bartlett", "Ling Huang", "Nina Taft"], "venue": "In Computing Research Repository,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "Sharing graphs using differentially private graph models", "author": ["Alessandra Sala", "Xiaohan Zhao", "Christo Wilson", "Haitao Zheng", "Ben Y. Zhao"], "venue": "In Internet Measurement Conference,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}, {"title": "Signal processing and machine learning with differential privacy: Algorithms and challenges for continuous data", "author": ["Anand D. Sarwate", "Kamalika Chaudhuri"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Mining frequent graph patterns with differential privacy", "author": ["Entong Shen", "Ting Yu"], "venue": "In KDD,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Efficient, differentially private point estimators", "author": ["Adam Smith"], "venue": "In Computing Research Repository,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2008}, {"title": "Privacy-preserving statistical estimation with optimal convergence rates", "author": ["Adam Smith"], "venue": "In ACM Symposium on Theory of Computing,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2011}, {"title": "k-anonymity: A model for protecting privacy", "author": ["Latanya Sweeney"], "venue": "In International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2002}, {"title": "Differentially private naive Bayes classification", "author": ["Jaideep Vaidya", "Basit Shafiq", "Anirban Basu", "Yuan Hong"], "venue": "InWeb Intelligence,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "Differentially private projected histograms: Construction and use for prediction", "author": ["Staal A. Vinterbo"], "venue": "In European Conference on Machine Learning (ECML) and Conference on Principles and Practice of Knowledge Discovery in Databases,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2012}, {"title": "Differentially private network data release via structural inference", "author": ["Qian Xiao", "Rui Chen", "Kian-Lee Tan"], "venue": "In KDD,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "Differentially private data release through multidimensional partitioning", "author": ["Yonghui Xiao", "Li Xiong", "Chun Yuan"], "venue": "In Secure Data Management,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2010}, {"title": "Functional mechanism: Regression analysis under differential privacy", "author": ["Jun Zhang", "Zhenjie Zhang", "Xiaokui Xiao", "Yin Yang", "Marianne Winslett"], "venue": "In International Conference on Very Large Data Bases,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2012}, {"title": "Differentially private setvalued data release against incremental updates", "author": ["Xiaojian Zhang", "Xiaofeng Meng", "Rui Chen"], "venue": "In International Conference on Database Systems for Advanced Applications,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "1 [9] uses privacy to mean both population privacy and individual privacy.", "startOffset": 2, "endOffset": 5}, {"referenceID": 50, "context": "In a well-known case, the personal health information of Massachusetts governor William Weld was discovered in a supposedly anonymized public database [51].", "startOffset": 151, "endOffset": 155}, {"referenceID": 50, "context": "To combat such background attacks, some more robust definitions of privacy (such as k-anonymity[51], l-diversity[36] and t-closeness[34]) have been proposed.", "startOffset": 95, "endOffset": 99}, {"referenceID": 35, "context": "To combat such background attacks, some more robust definitions of privacy (such as k-anonymity[51], l-diversity[36] and t-closeness[34]) have been proposed.", "startOffset": 112, "endOffset": 116}, {"referenceID": 33, "context": "To combat such background attacks, some more robust definitions of privacy (such as k-anonymity[51], l-diversity[36] and t-closeness[34]) have been proposed.", "startOffset": 132, "endOffset": 136}, {"referenceID": 22, "context": "Recently [23], researchers demonstrated that an attacker could infer whether an individual had participated in a genome study using only publicly available aggregated genetic data.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "Differential privacy [12, 13], which will be introduced in the next section, uses random noise to ensure that the publicly visible information doesn\u2019t change much if one individual in the dataset changes.", "startOffset": 21, "endOffset": 29}, {"referenceID": 12, "context": "Differential privacy [12, 13], which will be introduced in the next section, uses random noise to ensure that the publicly visible information doesn\u2019t change much if one individual in the dataset changes.", "startOffset": 21, "endOffset": 29}, {"referenceID": 16, "context": "Several recent surveys address differential privacy and data science [17, 47, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 46, "context": "Several recent surveys address differential privacy and data science [17, 47, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 21, "context": "Several recent surveys address differential privacy and data science [17, 47, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 16, "context": "Some others ([17, 22]) mainly focus on statistical estimators, while [47] discusses the high level interactions between differential privacy and machine learning.", "startOffset": 13, "endOffset": 21}, {"referenceID": 21, "context": "Some others ([17, 22]) mainly focus on statistical estimators, while [47] discusses the high level interactions between differential privacy and machine learning.", "startOffset": 13, "endOffset": 21}, {"referenceID": 46, "context": "Some others ([17, 22]) mainly focus on statistical estimators, while [47] discusses the high level interactions between differential privacy and machine learning.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "Definition 3: a mechanism f\u0303 satisfies (\u01eb, \u03b4)-differential privacy [12, 13] for two non-negative numbers \u01eb and \u03b4 iff for all neighbors d(D,D) = 1, and all subset S of f\u0303 \u2019s range, as long as the following probabilities are well-defined, there holds P (f\u0303(D) \u2208 S) \u2264 \u03b4 + eP (f\u0303(D) \u2208 S)", "startOffset": 67, "endOffset": 75}, {"referenceID": 12, "context": "Definition 3: a mechanism f\u0303 satisfies (\u01eb, \u03b4)-differential privacy [12, 13] for two non-negative numbers \u01eb and \u03b4 iff for all neighbors d(D,D) = 1, and all subset S of f\u0303 \u2019s range, as long as the following probabilities are well-defined, there holds P (f\u0303(D) \u2208 S) \u2264 \u03b4 + eP (f\u0303(D) \u2208 S)", "startOffset": 67, "endOffset": 75}, {"referenceID": 19, "context": "There is also a commonly used heuristic to choose \u03b4[20]: when there are n samples in the dataset, \u03b4 \u2208 o(1/n).", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "[11] shows that in terms of mtutal information, \u01eb-differential privacy is much stronger than (\u01eb, \u03b4)-differential privacy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The Laplacian mechanism[15] is a popular \u01eb-differentially private mechanism for queries f with answers f(D) \u2208 R, in which sensitivity (Definition 5) plays an important role.", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "The Laplacian mechanism[15]: given a query f and a norm function over the range of f , the random function f\u0303(D) = f(D) + \u03b7 satisfies \u01eb-differential privacy.", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "\u2016))2 log 2 \u03b4 ) [2].", "startOffset": 15, "endOffset": 18}, {"referenceID": 37, "context": "The exponential mechanism[38] is an \u01eb-differentially private method to select one element from a set.", "startOffset": 25, "endOffset": 29}, {"referenceID": 41, "context": "Smooth sensitivity [42] is a framework which allows one to publish an (\u01eb, \u03b4)differentially private numerical answer to a query.", "startOffset": 19, "endOffset": 23}, {"referenceID": 41, "context": "The sample and aggregate framework [42] is a mechanism to respond to queries whose answers can be approximated well with a small number of samples, while ensuring (\u01eb, \u03b4)-differential privacy.", "startOffset": 35, "endOffset": 39}, {"referenceID": 41, "context": "An efficient aggregation function is provided in the paper [42].", "startOffset": 59, "endOffset": 63}, {"referenceID": 36, "context": "[37] provides the following two theorems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Some more sophisticated forms of this theorem can be find in [16, 43].", "startOffset": 61, "endOffset": 69}, {"referenceID": 42, "context": "Some more sophisticated forms of this theorem can be find in [16, 43].", "startOffset": 61, "endOffset": 69}, {"referenceID": 49, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 44, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 5, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 24, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 25, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 20, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 30, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 6, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 52, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 26, "context": "Some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the \u2018true model\u2019 to be the output of a noiseless algorithm on training data.", "startOffset": 12, "endOffset": 50}, {"referenceID": 13, "context": "However, others [14, 33, 40] consider the \u2018true model\u2019 to mean the optimal model if the true distribution were known.", "startOffset": 16, "endOffset": 28}, {"referenceID": 32, "context": "However, others [14, 33, 40] consider the \u2018true model\u2019 to mean the optimal model if the true distribution were known.", "startOffset": 16, "endOffset": 28}, {"referenceID": 39, "context": "However, others [14, 33, 40] consider the \u2018true model\u2019 to mean the optimal model if the true distribution were known.", "startOffset": 16, "endOffset": 28}, {"referenceID": 5, "context": "Some papers [6, 25, 53] use the difference of values of the target function.", "startOffset": 12, "endOffset": 23}, {"referenceID": 24, "context": "Some papers [6, 25, 53] use the difference of values of the target function.", "startOffset": 12, "endOffset": 23}, {"referenceID": 52, "context": "Some papers [6, 25, 53] use the difference of values of the target function.", "startOffset": 12, "endOffset": 23}, {"referenceID": 13, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 49, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 32, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 39, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 20, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 30, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 6, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 26, "context": "Some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters in private and non-private models when the models are parametric and have the same parameters.", "startOffset": 18, "endOffset": 49}, {"referenceID": 44, "context": "Still others [45, 26] use the distance between the predictions made by private and non-private models at certain points in the sample space.", "startOffset": 13, "endOffset": 21}, {"referenceID": 25, "context": "Still others [45, 26] use the distance between the predictions made by private and non-private models at certain points in the sample space.", "startOffset": 13, "endOffset": 21}, {"referenceID": 13, "context": "Given a measure of distance between two models, some papers [14, 40] prove that as the number of training examples grows large, the output converges to the true model.", "startOffset": 60, "endOffset": 68}, {"referenceID": 39, "context": "Given a measure of distance between two models, some papers [14, 40] prove that as the number of training examples grows large, the output converges to the true model.", "startOffset": 60, "endOffset": 68}, {"referenceID": 44, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 49, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 5, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 25, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 20, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 30, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 6, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 52, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 26, "context": "Other papers [45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models converge to true models.", "startOffset": 13, "endOffset": 47}, {"referenceID": 2, "context": "For those which prove bounds on the speed of convergence, the convergence is usually measured by (\u03b1, \u03b2)-usefulness [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 32, "context": "A few papers [33] provide worst case guarantees on the distance, which is equivalent to (\u03b1, 0)-usefulness.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "Yet another paper [25] uses the expectation of difference.", "startOffset": 18, "endOffset": 22}, {"referenceID": 51, "context": "For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while", "startOffset": 13, "endOffset": 33}, {"referenceID": 39, "context": "For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while", "startOffset": 13, "endOffset": 33}, {"referenceID": 45, "context": "For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while", "startOffset": 13, "endOffset": 33}, {"referenceID": 27, "context": "For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while", "startOffset": 13, "endOffset": 33}, {"referenceID": 29, "context": "For example, [52, 40, 46, 28, 30] use the Laplacian mechanism, while", "startOffset": 13, "endOffset": 33}, {"referenceID": 6, "context": "[7, 53] use the exponential mechanism.", "startOffset": 0, "endOffset": 7}, {"referenceID": 52, "context": "[7, 53] use the exponential mechanism.", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 30, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 23, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 18, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 24, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 40, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 54, "context": "Such approaches include [21, 31, 24, 19, 25, 41, 55].", "startOffset": 24, "endOffset": 52}, {"referenceID": 55, "context": "Some examples include [56, 5, 6, 45].", "startOffset": 22, "endOffset": 36}, {"referenceID": 4, "context": "Some examples include [56, 5, 6, 45].", "startOffset": 22, "endOffset": 36}, {"referenceID": 5, "context": "Some examples include [56, 5, 6, 45].", "startOffset": 22, "endOffset": 36}, {"referenceID": 44, "context": "Some examples include [56, 5, 6, 45].", "startOffset": 22, "endOffset": 36}, {"referenceID": 41, "context": "Mechanisms that employ this idea include [42, 27].", "startOffset": 41, "endOffset": 49}, {"referenceID": 26, "context": "Mechanisms that employ this idea include [42, 27].", "startOffset": 41, "endOffset": 49}, {"referenceID": 13, "context": "The linear regression in [14] is partially based on this idea.", "startOffset": 25, "endOffset": 29}, {"referenceID": 32, "context": "For example, [33] partitions the sample space and uses counts in each partition to estimate the density function.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "[26] interprets a model as a function and uses another function to approximate it by iteratively minimizing the largest distance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "An \u01eb-differentially private naive Bayes model mechanism is introduced in [52].", "startOffset": 73, "endOffset": 77}, {"referenceID": 55, "context": "[56] assumes bounded sample space and proposes a differentially private mechanism for linear regression.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "For linear SVM and all other models satisfying the two conditions, [5, 6] provide an output perturbation mechanism and an objective perturbation mechanism.", "startOffset": 67, "endOffset": 73}, {"referenceID": 5, "context": "For linear SVM and all other models satisfying the two conditions, [5, 6] provide an output perturbation mechanism and an objective perturbation mechanism.", "startOffset": 67, "endOffset": 73}, {"referenceID": 5, "context": "[6] also provides a performance analysis of the objective perturbation mechanism.", "startOffset": 0, "endOffset": 3}, {"referenceID": 55, "context": "Assuming that the sample space is bounded, the mechanism in [56] (see Section 3.", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "Furthermore, the output perturbation and objective perturbation mechanism in [5, 6] (see Section 3.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "Furthermore, the output perturbation and objective perturbation mechanism in [5, 6] (see Section 3.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "In [6, 45], an idea for private kernel SVM is proposed.", "startOffset": 3, "endOffset": 10}, {"referenceID": 44, "context": "In [6, 45], an idea for private kernel SVM is proposed.", "startOffset": 3, "endOffset": 10}, {"referenceID": 43, "context": "According to [44], the kernel function of two samples in the original sample space can be approximated by the inner product of their projections in the new space.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "Therefore [26] proposes another private kernel SVM algorithm for all RKHS kernels.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "The Test Data-independent Learner (TTDP) mechanism in [26] publishes a private kernel SVM model satisfying (\u01eb, \u03b4)-differential privacy as follows.", "startOffset": 54, "endOffset": 58}, {"referenceID": 23, "context": "[24] proposes an \u01eb-differentially private mechanism.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] proposes another \u01eb-differentially private decision tree algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] provides (\u01eb, \u03b4)-differentially private versions for two of them: the Implicit Gradient Descent (IGD) and the Generalized Infinitesimal Gradient Ascent (GIGA) given all the functions are L-Lipschitz continuous for some", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "The private mechanism in [25] adds Gaussian noise to every \u0175t before it is projected to wt to preserve privacy, and then use the noisy wt for the future computation.", "startOffset": 25, "endOffset": 29}, {"referenceID": 41, "context": "[42] proposes an (\u01eb, \u03b4)-differentially private k-means clustering algorithm using the sample and aggregate framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[53] proposes an \u01eb-differentially private feature selection, PrivateKD, for classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] proposes an (\u01eb, \u03b4)-differentially private algorithm for feature selection when the target function is stable.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "For the first kind of functions, the mechanism uses the smooth sensitivity framework in [42] to select features.", "startOffset": 88, "endOffset": 92}, {"referenceID": 41, "context": "For the second kind of functions, the mechanism uses an idea similar to the sample and aggregate framework in [42]: it creates some bootstrap sets from the private dataset, selects features non-privately on each set, and counts the frequencies of feature sets output by the algorithm.", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "An (\u01eb, \u03b4)-differentially private mechanism is proposed in [21].", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "[31] provides an \u01eb-differentially private mechanism for principal component analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] proposes an \u01eb-differentially private mechanism, PPCA, to compute k largest eigenvectors at the same time.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] proposes an (\u01eb, \u03b4)-differentially private mechanism for robust statistical estimators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Based on the property, [14] comes up with a ProposeTest-Release framework.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "Based on this framework, [14] proposes three mechanisms for interquartile range estimation, trimmed mean and median, and linear regression, respectively.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "[4] explores robust estimators in another way.", "startOffset": 0, "endOffset": 3}, {"referenceID": 48, "context": "[49, 50] give a differentially private mechanism for point estimation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 49, "context": "[49, 50] give a differentially private mechanism for point estimation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[33] proposes an \u01eb-differentially private mechanism for M-estimator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "The mechanism in [33] first divides the sample space ([0, 1]) into many small cubes without using private data.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "The mechanism in [33] first divides the sample space ([0, 1]) into many small cubes without using private data.", "startOffset": 54, "endOffset": 60}, {"referenceID": 2, "context": "Many differentially private data release mechanisms have been described, such as [3, 1, 18, 10].", "startOffset": 81, "endOffset": 95}, {"referenceID": 0, "context": "Many differentially private data release mechanisms have been described, such as [3, 1, 18, 10].", "startOffset": 81, "endOffset": 95}, {"referenceID": 17, "context": "Many differentially private data release mechanisms have been described, such as [3, 1, 18, 10].", "startOffset": 81, "endOffset": 95}, {"referenceID": 9, "context": "Many differentially private data release mechanisms have been described, such as [3, 1, 18, 10].", "startOffset": 81, "endOffset": 95}, {"referenceID": 54, "context": "[55] assumes that the density function is smooth, [8, 57] assume that the data\u2019s format permits it to be organized in a tree, and [41] assumes that partitions can preserve most important information for further data mining.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[55] assumes that the density function is smooth, [8, 57] assume that the data\u2019s format permits it to be organized in a tree, and [41] assumes that partitions can preserve most important information for further data mining.", "startOffset": 50, "endOffset": 57}, {"referenceID": 56, "context": "[55] assumes that the density function is smooth, [8, 57] assume that the data\u2019s format permits it to be organized in a tree, and [41] assumes that partitions can preserve most important information for further data mining.", "startOffset": 50, "endOffset": 57}, {"referenceID": 40, "context": "[55] assumes that the density function is smooth, [8, 57] assume that the data\u2019s format permits it to be organized in a tree, and [41] assumes that partitions can preserve most important information for further data mining.", "startOffset": 130, "endOffset": 134}, {"referenceID": 40, "context": "[41] first partitions the sample space using the exponential mechanism and then adds noise to the counts using the Laplacian mechanism.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Some others ([8, 57, 55]) generate noisy counts with the Laplacian mechanism for each cell and then partition according to the noisy counts.", "startOffset": 13, "endOffset": 24}, {"referenceID": 56, "context": "Some others ([8, 57, 55]) generate noisy counts with the Laplacian mechanism for each cell and then partition according to the noisy counts.", "startOffset": 13, "endOffset": 24}, {"referenceID": 54, "context": "Some others ([8, 57, 55]) generate noisy counts with the Laplacian mechanism for each cell and then partition according to the noisy counts.", "startOffset": 13, "endOffset": 24}, {"referenceID": 39, "context": "[40, 46] publish a graph generator model based on the assumption that the private data is fit well by some parametrized generative model.", "startOffset": 0, "endOffset": 8}, {"referenceID": 45, "context": "[40, 46] publish a graph generator model based on the assumption that the private data is fit well by some parametrized generative model.", "startOffset": 0, "endOffset": 8}, {"referenceID": 53, "context": "[54] represents the network structure by using a statistical hierarchical random graph model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] makes use of a public dataset, assigning weights to its examples to embed information contained in the private data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] assumes that samples have binary labels and that they are fit by a linear discriminant analysis (LDA) model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] assumes that the data matrix is sparse, thus the mechanism can make use of results from compressive sensing research.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] first randomly projects the data matrix, then adds noise to the compressed information, and reconstructs data from the noisy compressed information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] studies the general properties of private classifiers, instead of individual learning models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] formulates differentially private learning in an information theoretic framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "According to [39], the output of the exponential mechanism follows the posterior distribution that minimizes", "startOffset": 13, "endOffset": 17}, {"referenceID": 38, "context": "Therefore the conclusion in [39] doesn\u2019t necessarily mean that the exponential mechanism is the best.", "startOffset": 28, "endOffset": 32}, {"referenceID": 51, "context": "For example, [52] adds noise to the counts that generate the naive Bayes model instead of conditional probabilities of the model directly.", "startOffset": 13, "endOffset": 17}, {"referenceID": 44, "context": "For example, [45] transforms kernel SVM to linear SVM, and [14] uses a robust linear regression model to replace the commonly used model.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "For example, [45] transforms kernel SVM to linear SVM, and [14] uses a robust linear regression model to replace the commonly used model.", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "According to [28, 29], such a public dataset can enhance the performance of differentially private mechanisms.", "startOffset": 13, "endOffset": 21}, {"referenceID": 28, "context": "According to [28, 29], such a public dataset can enhance the performance of differentially private mechanisms.", "startOffset": 13, "endOffset": 21}, {"referenceID": 47, "context": "For those models, one can consider trying the MCMC-based algorithm as in [48].", "startOffset": 73, "endOffset": 77}, {"referenceID": 49, "context": "For example, [50] proves that (\u01eb, \u03b4)-differential privacy is free for learning models satisfying a certain set of conditions.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "The mechanism in [6] ensures free \u01eb-differential privacy for regularized logistic regression models and linear SVM models, where noise from sample randomness is O(1/ \u221a n) and the noise to preserve privacy is O(1/n).", "startOffset": 17, "endOffset": 20}, {"referenceID": 27, "context": "The mechanism in [28] also proves that the effect of noise brought by differential privacy is O(1/n), while the effect from sample randomness is O(1/ \u221a n).", "startOffset": 17, "endOffset": 21}], "year": 2014, "abstractText": "The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conflict is to extract general characteristics of whole populations without disclosing the private information of individuals. In this paper, we consider differential privacy, one of the most popular and powerful definitions of privacy. We explore the interplay between machine learning and differential privacy, namely privacy-preserving machine learning algorithms and learning-based data release mechanisms. We also describe some theoretical results that address what can be learned differentially privately and upper bounds of loss functions for differentially", "creator": "LaTeX with hyperref package"}}}