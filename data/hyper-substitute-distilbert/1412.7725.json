{"id": "1412.7725", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2014", "title": "Automatic Photo Adjustment Using Deep Neural Networks", "abstract": "simple retouching enables photographers the employ dramatic visual impressions visually artistically modeling their tones through stylistic color and tone adjustments. however, painting is too either moment - consuming and challenging task that requires extra skills upgrading the abilities providing the photographers. building an automated algorithm offer an appealing alternative achieving manual work but this an approximation faces many hurdles. many photographic artists conflict on design adjustments that depend under the material context limiting even its semantics. further, these adjustments are increasingly spatially varying. because of these characteristics, existing design algorithms produce still limited and addressing only a subset of different challenges. recently, virtual machine learning has shown unique methods to place hard problems on resisted machine algorithms for manipulation. both motivated us how explore wider use of camera learning such a context of photo editing. given this paper, we explain how to leverage the automatic photo adjustment structure in a problem fitting \" this approach. we also introduce accurate image descriptor above accounts for repetitive feedback patterns driving your image. our graphics demonstrate primarily our deep learning formulation unlike traditional software guidelines already capture current photographic styles. in time - unlike previous techniques, photographer can observe local adjustments accurately depend globally the mapping semantics. exhibitions show on several panels that numerical computation results here are qualitatively and quantitatively refined than previous work.", "histories": [["v1", "Wed, 24 Dec 2014 17:51:17 GMT  (7148kb,D)", "http://arxiv.org/abs/1412.7725v1", "referred to ACM Transactions on Graphics by Siggraph Asia 2014"], ["v2", "Sat, 16 May 2015 03:49:35 GMT  (7612kb,D)", "http://arxiv.org/abs/1412.7725v2", "TOG minor revision"]], "COMMENTS": "referred to ACM Transactions on Graphics by Siggraph Asia 2014", "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG", "authors": ["zhicheng yan", "hao zhang", "baoyuan wang", "sylvain paris", "yizhou yu"], "accepted": false, "id": "1412.7725"}, "pdf": {"name": "1412.7725.pdf", "metadata": {"source": "CRF", "title": "Automatic Photo Adjustment Using Deep Learning", "authors": ["Zhicheng Yan", "Hao Zhang", "Baoyuan Wang"], "emails": ["zyan3@illinois.edu,", "hao@cs.cmu.edu,", "baoyuanw@microsoft.com,", "sparis@adobe.com,", "yizhouy@acm.org.", "permissions@acm.org."], "sections": [{"heading": null, "text": "Automatic Photo Adjustment Using Deep Learning Zhicheng Yan University of Illinois at Urbana Champaign Hao Zhang\u2020 Carnegie Mellon University Baoyuan Wang Microsoft Research Sylvain Paris Adobe Research Yizhou Yu The University of Hong Kong and University of Illinois at Urbana Champaign\nPhoto retouching enables photographers to invoke dramatic visual impressions by artistically enhancing their photos through stylistic color and tone adjustments. However, it is also a time-consuming and challenging task that requires advanced skills beyond the abilities of casual photographers. Using an automated algorithm is an appealing alternative to manual work but such an algorithm faces many hurdles. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Further, these adjustments are often spatially varying. Because of these characteristics, existing automatic algorithms are still limited and cover only a subset of these challenges. Recently, deep machine learning has shown unique abilities to address hard problems that resisted machine algorithms for long. This motivated us to explore the use of deep learning in the context of photo editing. In this paper, we explain how to formulate the automatic photo adjustment problem in a way suitable for this approach. We also introduce an image descriptor that accounts for the local semantics of an image. Our experiments demonstrate that our deep learning formulation applied using these descriptors successfully capture sophisticated photographic styles. In particular and unlike previous techniques, it can model local adjustments that depend on the image semantics. We show on several examples that this yields results that are qualitatively and quantitatively better than previous work.\nCategories and Subject Descriptors: I.4.3 [Image Processing and Computer Vision]: Enhancement; I.4.10 [Image Processing and Computer Vision]: Representation\u2014Statistical\nAuthors\u2019 email addresses: zyan3@illinois.edu, hao@cs.cmu.edu, baoyuanw@microsoft.com, sparis@adobe.com, yizhouy@acm.org. \u2020This work was conducted when Hao Zhang was an intern at Microsoft Research. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c\u00a9 YYYY ACM 0730-0301/YYYY/14-ARTXXX $10.00\nDOI 10.1145/XXXXXXX.YYYYYYY http://doi.acm.org/10.1145/XXXXXXX.YYYYYYY\nAdditional Key Words and Phrases: Color Transforms, Feature Descriptors, Neural Networks, Photo Enhancement"}, {"heading": "1. INTRODUCTION", "text": "With the prevalence of digital imaging devices and social networking, sharing photos through social media has become quite popular. A common practice in this type of photo sharing is artistic enhancement of photos by various Apps such as Instagram. In general, such photo enhancement is artistic because it not only tries to correct photographic defects (under/over exposure, poor contrast, etc.) but also aims to invoke dramatic visual impressions by stylistic or even exaggerated color and tone adjustments. Traditionally, high-quality enhancement is usually hand-crafted by a well-trained artist through extensive labor.\nIn this work, we study the problem of learning artistic photo enhancement styles from image exemplars. Specifically, given a set of image pairs, each representing a photo before and after pixel-level tone and color enhancement following a particular style, we wish to learn a computational model so that for a novel input photo we can apply the learned model to automatically enhance the photo following the same style.\nLearning a high-quality artistic photo enhancement style is challenging for several reasons. First, photo adjustment is often a highly empirical and perceptual process that relates the pixel colors in an enhanced image to the information embedded in the original image in a complicated manner. Learning an enhancement style needs to extract an accurate quantitative relationship underlying this process. This quantitative relationship is likely to be complex and highly nonlinear especially when the enhancement style requires spatially varying local adjustments. It is nontrivial to learn a computational model capable of representing such a complicated relationship accurately, and large-scale training data is likely to be necessary. Therefore, we seek a learning model scalable with respect to both the feature dimension and data size and efficiently computable with high-dimensional, large-scale data.\nSecond, an artistic enhancement is typically semantics-aware. An artist does not see individual pixels; instead he/she sees semantically meaningful objects (humans, cars, animals, etc.) and determines the type of adjustments to improve the appearance of the objects. For example, it is likely that an artist pays more attention to improve the appearance of a human figure than a region of sky in the same\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nar X\niv :1\n41 2.\n77 25\nv1 [\ncs .C\nV ]\n2 4\nD ec\n2 01\n4\nphoto. We would like to incorporate this semantics-awareness in our learning problem. One challenge is the representation of semantic information in learning so that the learned model can perform image adjustments according to the specific content as human artists do.\nWe present an automatic photo enhancement method based on deep machine learning. This approach has recently accumulated impressive successes in domains such as computer vision and speech analysis for which the semantics of the data plays a major role, e.g., [Vincent et al. 2008; Krizhevsky et al. 2012]. This motivated us to explore the use of this class of techniques in our context. To address the challenges mentioned above, we cast exemplar-based photo adjustment as a regression problem, and use a Deep Neural Network (DNN) with multiple hidden layers to represent the highly nonlinear and spatially varying color mapping between input and enhanced images. A deep neural network (DNN) is a universal approximator that can represent arbitrarily complex continuous functions [Hornik et al. 1989]. It is also a compact model which is readily scalable with respect to high-dimensional, large-scale data.\nFeature design is a key issue that can significantly affect the effectiveness of DNN. To make sure the learned color mapping responds to complex color and semantic information, we design informative yet discriminative feature descriptors that serve as the input to the DNN. For each input image pixel, its feature descriptor consists of three components, which reflect respectively the statistical or semantic information at the pixel, contextual, and global levels. The global feature descriptor is based on global image statistics, whereas the context feature descriptor is based on semantic information extracted from a large neighborhood around the pixel. Understanding image semantics has been made possible with recent advances in scene understanding and object detection. We use existing algorithms to annotate all input image pixels and the semantics information from the annotated images are incorporated into a novel context feature descriptor.\nContributions. In summary, our proposed photo enhancement technique has the following contributions.\n\u2014It introduces the first automatic photo adjustment framework based on deep neural networks. A variety of normal and artistic photo enhancement styles can be achieved by training a distinct model for each enhancement style. The quality of our results is superior to that of existing methods.\n\u2014Our framework adopts informative yet discriminative image feature descriptors at the pixel, contextual and global levels. Our context descriptor exploits semantic analysis over multiscale spa-\ntial pooling regions. It has achieved improved performance over a single pooling region.\n\u2014Our method also includes an effective algorithm for choosing a representative subset of photos from a large collection so that a photo enhancement model trained over the chosen subset can still produce high-quality results on novel testing images.\nWhile a contribution of our work is the application of deep machine learning in a new context, we use a standard learning procedure and do not claim any contribution in the design of the learning algorithm itself. Similarly, while we propose a possible design for semantic context descriptor, and demonstrate its effectiveness, a comprehensive exploration of the design space for such descriptors is beyond the scope of this paper.\nComplete source codes and datasets used by our system are publicly available on Github 1 ."}, {"heading": "2. RELATED WORK", "text": "Traditional image enhancement rules are primarily determined empirically. There are many software tools to perform fully automatic color correction and tone adjustment, such as Adobe Photoshop, Google Auto Awesome, and Microsoft Office Picture Manager. In addition to these tools, there exists much research on either interactive [Lischinski et al. 2006; An and Pellacini 2008] or automatic [Bae et al. 2006; Cohen-Or et al. 2006] color and tone adjustment. Automatic methods typically operate on the entire image in a global manner without taking image content into consideration. To address this issue, Kaufman et al. [2012] introduces an automatic method that first detects semantic content, including faces, sky as well as shadowed salient regions, and then applies a sequence of empirically determined steps for saturation, contrast as well as exposure adjustment. However, the limit of this approach is that output style is hard-coded in the algorithm and cannot be easily tuned to achieve a desired style. In comparison and as we shall see, our data-driven approach can easily be trained to produce a variety of styles. Further, these techniques rely on a fixed pipeline that is inherently limited in its ability to achieve user-preferred artistic enhancement effects, especially the exaggerated and dramatic ones. In practice, a fixedpipeline technique works well for a certain class of adjustments and only produces approximate results for effects outside this class. For instance, Bae et al. [2006] do well with tonal global transforms but\n1https://github.com/stephenyan1984/dl-image-enhance https://github.com/stephenyan1984/cuda convnet plus\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\ndo not model local edits, and Kaufman et al. [2012] perform well on a predetermined set of semantic categories but does not handle elements outside this set. In comparison, deep learning provides a universal approximator that is trained on a per-style basis, which is key to the success of our approach.\nAnother line of research for photo adjustment is primarily datadriven. Learning based image enhancement [Kang et al. 2010; Joshi et al. 2010; Caicedo et al. 2011; Bychkovsky et al. 2011] and image restoration [Dale et al. 2009] have shown promising results and therefore received much attention. Kang et al. [2010] found that image quality assessment is actually very much personalized, which results in an automatic method for learning individual preferences in global photo adjustment. Bychkovsky et al. [2011] introduces a method based on Gaussian processes for learning tone mappings according to global image statistics. Since these methods were designed for global image adjustment, they do not consider local image contexts and cannot produce spatially varying local enhancements. Wang et al. [2011] proposes a method based on piecewise approximation for learning color mapping functions from exemplars. It does not consider semantic or contextual information either. In addition, it is not fully automatic, and relies on interactive soft segmentation. It is infeasible for this technique to automatically enhance a collection of images. In comparison, this paper proposes a scalable framework for learning user-defined complex enhancement effects from exemplars. It explicitly performs generic image semantic analysis, and its image enhancement models are trained using feature descriptors constructed from semantic analysis results.\nHwang et al. [2012] proposes a context-aware local image enhancement technique. This technique first searches for the most similar images and then the most similar pixels within them, and finally apply a combination of the enhancement parameters at the most similar pixels to the considered pixel in the new test image. With a sufficiently large image database, this method works well. But in practice, nearest-neighbor search requires a fairly large training set that is challenging to create and slow to search, thereby limiting the scalability of this approach. Another difference with our approach is that, to locate the most similar pixels, this method uses low- and mid-level features (i.e., color and SIFT) whereas we also consider high-level semantics. We shall see in the result section that these differences have a significant impact on the adjustment quality in several cases."}, {"heading": "3. A DEEP LEARNING MODEL", "text": "Let us now discuss how we cast exemplar-based photo adjustment as a regression problem, and how we set up a DNN to solve this regression problem. A photo enhancement style is represented by a set of exemplar image pairs \u039b = {Ik, Jk}mk=1, where Ik and Jk are respectively the images before and after enhancement. Our premise is that there exists an intrinsic color mapping function F that maps each pixel\u2019s color in Ik to its corresponding pixel\u2019s color in Jk for every k. Our goal is to train an approximate function F\u0303 using \u039b so that F\u0303 may be applied to new images to enhance the same style there. For a pixel pi in image Ik, the value of F\u0303 is simply the color of image Jk at pixel pi, whereas the input of F\u0303 is more complex because F\u0303 depends on not only the color of pi in Ik but also additional local and global information extracted from Ik, thus we formulate F\u0303 as a parametric function F\u0303(\u0398, xi), where \u0398 represents the parameters and xi represents the feature vector at pi that encompasses the color of pi in Ik as well as additional local and global information. With this formulation, training the function\nF\u0303 using \u039b becomes computing the parameters \u0398 from training data \u039b through nonlinear regression.\nHigh-frequency pixelwise color variations are difficult to model because they force us to choose a mapping function which is sensitive to high-frequency details. Such a mapping function often leads to noisy results in relatively smooth regions. To tackle this problem we use a color basis vector V (ci) at pixel pi to rewrite F\u0303 as F\u0303 = \u03a6(\u0398, xi)V (ci), which expresses the mapped color, F\u0303 , as the result of applying the color transform matrix \u03a6(\u0398, xi) to the color basis vector V (ci). V (ci) is a vector function taking different forms when it works with different types of color transforms. In this paper we work in the CIE Lab color space, and the color at pi is ci = [Liaibi]T and V (ci) = [Li ai bi 1]T if we use 3x4 affine color transforms. If we use 3x10 quadratic color transforms, then V (ci) = [L2i a 2 i b 2 i Liai Libi aibi Li ai bi 1]. Since the per-pixel color basis vector V (ci) varies at similar frequencies as pixel colors, it can absorb much high-frequency color variation. By factorizing out the color variation associated with V (ci), we can let \u03a6(\u0398, xi) focus on modeling the spatially smooth but otherwise highly nonlinear part of F\u0303 .\nWe learn \u03a6(\u0398, xi) by solving the following least squares minimization problem defined over all training pixels sampled from \u039b:\narg min \u03a6\u2208H n\u2211 i \u2016 \u03a6(\u0398, xi)V (ci)\u2212 yi \u20162, (1)\nwhereH represents the function space of \u03a6(\u0398, xi) and n is the total number of training pixels. In this paper, we represent \u03a6(\u0398, xi) as a DNN with multiple hidden layers.\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nL\na\nb\nInput image\nVisualization of 3 x 10 coefficients of the quadratic color transform\nFig. 3. (Left) Input image, and (Right) visualization of its per-pixel quadratic color transforms, \u03a6(\u0398, xi), each of which is a 3\u00d7 10 matrix. Each image on the right visualizes one coefficient in this matrix at all pixel locations. Coefficients are linearly mapped to [0,1] in each visualization image for better contrast. This visualization illustrates two properties of the quadratic color transforms: 1) they are spatially varying and 2) they are smooth with much high-frequency content suppressed."}, {"heading": "3.1 Neural Network Architecture and Training", "text": "Our neural network follows a standard architecture that we describe below for the sake of completeness.\nMulti-layer deep neural networks have proven to be able to represent arbitrarily complex continuous functions [Hornik et al. 1989]. Each network is an acyclic graph, each node of which is a neuron. Neurons are organized in a number of layers, including an input layer, one or more hidden layers, and an output layer. The input layer directly maps to the input feature vector, i.e. xi in our problem. The output layer maps to the elements of the color transform, \u03a6(\u0398, x\u03bd). Each neuron within a hidden layer or the output layer takes as input the responses from all the neurons in the preceding layer. Each connection between a pair of neurons is associated with a weight. Let us denote vlj as the output of the j-th neuron in the l-th layer. Then vlj is expressed as follows:\nvlj = g ( wlj0 +\n\u2211 k>0 wljkv l\u22121 k\n) (2)\nwhere wljk is the weight associated with the connection between the j-th neuron in the l-layer and the k-th neuron in the (l \u2212 1)-th layer, and g(z) is an activation function which is typically nonlinear. We choose the rectified linear unit (ReLU) [Krizhevsky et al. 2012], g(z) = max(0, z), as the activation function in our networks. Compared with other widely used activation functions, such as the hyperbolic tangent, g(z) = tanh(z) = 2/(1 + e\u22122z) \u2212 1, or the sigmoid, h(x) = (1 + e\u2212x)\u22121, ReLU has a few advantages, including inducing sparsity in the hidden units and accelerating the convergence of the training process. Note that there is no nonlinear activation function for neurons in the output layer. The output of a neuron in the output layer is only a linear combination of its inputs from the preceding layer. Figure 2 shows the overall architecture, which has two extra layers (yellow and purple neurons) above the output layer for computing the product between the color transform and the color basis vector. Given a neural network architecture for color mapping, H in (1) should be the function space spanned by all neural networks with the same architecture but different weight parameters \u0398.\nOnce the network architecture has been fixed, given a training dataset, we use the classic error backpropagation algorithm to train the weights. In addition, we apply the Dropout training strategy [Krizhevsky et al. 2012; Hinton et al. 2012], which has been shown very useful for improving the generalization capability.\nWe set the output of each neuron in the hidden layers to zero with probability 0.5. Those neurons that have been \u201cdropped out\u201d in this way do not contribute to the forward pass and do not participate in error backpropagation. Our experiments show that adding Dropout during training typically reduces the relative prediction error on testing data by 2.1%, which actually makes a significant difference in the visual quality of the enhanced results.\nFigure 3 visualizes the per-pixel quadratic color transforms, \u03a6(\u0398, xi), generated by a trained DNN for one example image. We can see that the learned color mappings are smooth in most of the local regions."}, {"heading": "4. FEATURE DESCRIPTORS", "text": "Our feature descriptor (xi) at a sample pixel pi serves as the input layer in the neural network. It has three components, xi = (xpi , x c i , x g i ), where x p i represents pixelwise features, x c i represents contextual features computed for a local region surrounding pi, and xgi represents global features computed for the entire image where pi belongs. The details about these three components follow."}, {"heading": "4.1 Pixelwise Features", "text": "Pixelwise features reflect high-resolution pixel-level image variations, and are indispensable for learning spatially varying photo\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nenhancement models. They are defined as xpi = (ci, pi), where ci represents the average color in the CIELab color space within the 3x3 neighborhood, and pi = (xi, yi) denotes the normalized sample position within the image."}, {"heading": "4.2 Global Features", "text": "In photographic practice, global attributes and overall impressions, such as the average intensity of an image, at least have partial influence on artists when they decide how to enhance an image. We therefore incorporate global image features in our feature representation. Specifically, we adopt six types of global features proposed in [Bychkovsky et al. 2011], including intensity distribution, scene brightness, equalization curves, detail-weighted equalization curves, highlight clipping, and spatial distribution, which altogether give rise to a 207-dimensional vector."}, {"heading": "4.3 Contextual Features", "text": "Our contextual features try to characterize the distribution of semantic categories, such as sky, building, car, person, and tree, in an image. Such features are extracted from semantic analysis results within a local region surrounding the sample pixel. Typical image semantic analysis algorithms include scene parsing [Tighe and Lazebnik 2010; Liu et al. 2011] and object detection [Viola and Jones 2001; Felzenszwalb et al. 2008; Wang et al. 2013]. Scene parsing tries to label every pixel in an image with its semantic category. Object detection on the other hand trains one highly specialized detector for every category of objects (such as dogs). Scene parsing is good at labeling categories (such as grass, roads, and sky) that have no characteristic shape but relatively consistent texture. These categories have a large scale, and typically form the background of an image. Object detectors are better at locating categories (such as persons and cars), which are better characterized by their overall shape than local appearance. These categories have a smaller scale, and typically occupy the foreground of an image. Because these two types of techniques are complementary to each other, we perform semantic analysis using a combination of scene parsing and object detection algorithms. Figure 5 illustrates one fusion example of the scene parsing and detection results.\nWe use existing algorithms to automatically annotate all input image pixels and the semantics information from the annotated images are gathered into a novel context feature descriptor. During pixel annotation, we perform scene parsing using the state-of-theart algorithm in [Tighe and Lazebnik 2010]. The set of semantic categories, Sp, during scene parsing include such object types as sky, road, river, field and grass. . After the scene parsing step, we obtain a parsing map, denoted as Ip, each pixel of which receives one category label from Sp, indicating that with a high probability, the corresponding pixel in the input image is covered by a semantic instance in that category. We further apply the state-of-the-art object\ndetector in [Wang et al. 2013] to detect the pixels covered by a predefined set of foreground object types,Od, which include person, train, bus and building. After the detection step, we obtain one confidence map for each predefined type. We fuse all confidence maps into one by choosing, at every pixel, the object label that has the highest confidence value. This fused detection map is denoted as Id. We further merge Id with Ip so that those pixel labels from Id with confidence larger than a predefined threshold are used to overwrite the corresponding labels from Ip. Since scene parsing and object detection results tend to be noisy, we rely on voting and automatic image segmentation to perform label cleanup in the merged label map. Within each image segment, we reset the label at every pixel to the one that appears most frequently in the segment. In our experiments, we adopt the image segmentation algorithm in [Arbelaez et al. 2011]. This cleaned map becomes our final semantic label map, Ilabel.\nGiven the final semantic label map for the entire input image, we construct a contextual feature descriptor for each sample pixel to represent multiscale object distributions in its surroundings. For a sample point pi, we first define a series of nested square regions, {R0, R1, . . . , R\u03c4}, all centered at pi. The edge length of these regions follows a geometric series, i.e. \u03bbk = 3\u03bbk\u22121(k = 1, . . . , \u03c4), making our feature representation more sensitive to the semantic contents at nearby locations than those farther away. We further subdivide the ring between every two consecutive squares, Rk+1 \u2212Rk, into eight rectangles, as shown in Figure 4. Thus, we end up with a total of 9\u03c4 + 1 regions, including both the original regions in the series as well as regions generated by subdivision. For each of these regions, we compute a semantic label histogram, where the number of bins is equal to the total number of semantic categories, N = |Sp \u22c3 Od|. Note that the histogram for Rk is the sum of the histograms for the nine smaller regions within Rk. Such spatial pooling can make our feature representation more robust and better tolerate local geometric deformations. The final contextual feature descriptor at pi is defined to be the concatenation of all these semantic label histograms. Our multiscale context descriptor is partially inspired by shape contexts [Belongie et al. 2002]. However, unlike the shape context descriptor, our regions and subregions are either rectangles or squares, which facilitate fast histogram computation based on integral images (originally called summed area tables) [Viola and Jones 2001]. In practice, we pre-computeN integral images, one for each semantic category. Then the value of each histogram bin can be calculated within constant time, which is extremely fast compared with the computation of shape contexts. To the best of our knowledge, our method is the first one that explicitly constructs semantically meaningful contextual descriptors for learning complex image enhancement models.\nIt is important to verify whether the complexity of our contextual features is necessary in learning complex spatially varying local adjustment effects. We have compared our results against those\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nobtained without contextual features as well as those obtained from simpler contextual features based on just one pooling region (vs. our 28 multiscale regions) at the same size as our largest region. From Figure 11, we can see that our contextual features are able to produce local adjustment results closest to the ground truth.\nDiscussion. The addition of this semantic component into our feature vectors is a major difference with previous work. As shown in Figure 11 and in the result section, the design that we propose for this component is effective and produces a significant improvement in practice. That said, we acknowledge that other options may be possible and we believe that exploring the design space of semantic descriptors is an exciting avenue for future work."}, {"heading": "5. TRAINING DATA SAMPLING AND SELECTION", "text": ""}, {"heading": "5.1 Superpixel Based Sampling", "text": "When training a mapping function using a set of images, we prefer not to make use of all the pixels as such a dense sampling would result in unbalanced training data. For example, we could have too many pixels from large \u201csky\u201d regions while relatively few from smaller \u201cperson\u201d regions, which could eventually result in a serious bias in the trained mapping function. In addition, an overly dense sampling unnecessarily increases the training cost, as we need to handle millions of pixel samples. Therefore, we apply a superpixel based method to collect training samples. For each training image I , we first apply the graph-based segmentation [Felzenszwalb and Huttenlocher 2004] to divide the image into small homogeneous yet irregularly shaped patches, each of which is called a superpixel. Note that a superpixel in a smooth region may be larger than one in a region with more high-frequency details. We require that the color transform returned by our mapping function at the centroid of a superpixel be used for predicting with sufficient accuracy the adjusted color of all pixels within the same superpixel. To avoid bias, we randomly sample a fixed number of pixels from every superpixel. Let \u03bd be any superpixel from the original images (before adjustment) in \u039b, and S\u03bd be the set of pixels sampled from \u03bd. We revise the cost function in (1) as follows to reflect our superpixel-based sampling and local smoothness requirement.\u2211\n\u03bd \u2211 j\u2208S\u03bd \u2016 \u03a6(\u0398, x\u03bd)V (cj)\u2212 yj \u20162, (3)\nwhere \u0398 represents the set of trained weights in the neural network, x\u03bd is the feature vector constructed at the pixel closest to the centroid of \u03bd, V (cj) denotes the color basis vector of a sample pixel within \u03bd, and yj denotes the adjusted color of the same sample within \u03bd."}, {"heading": "5.2 Cross-Entropy Based Image Selection", "text": "In example-based photo enhancement, example images that demonstrate a certain enhancement style often need to be manually prepared by human artists. It is a labor intensive task to adjust many images as each image has multiple attributes and regions that can be adjusted. Therefore, it is much desired to pre-select a small number of representative training images to reduce the amount of human work required. On the other hand, to make a learned model achieve a strong prediction capability, it is necessary for the selected training images to have a reasonable coverage of the feature space.\nIn this section, we introduce a cross-entropy based scheme for selecting a subset of representative training images from a large collection. We first learn a codebook of feature descriptors with K = 400 codewords by running K-means clustering on feature descriptors collected from all training images. Then every original\nAlgorithm 1: Small Training Set Selection Input: A large image collection, \u2126I ; The desired number of\nrepresentative images, md Output: A subset \u2126 with md images selected from \u2126I\n1 Initialize \u2126\u2190 \u2205 2 for i = 1 to md do 3 I\u2217 = arg maxI\u2208\u2126I\u2212\u2126\u2212 \u2211 j H\u2126 \u2032 (j) logH\u2126 \u2032 (j), 4 where \u2126\u2032 = \u2126 \u222a {I}; 5 \u2126 = \u2126 \u222a {I\u2217}"}, {"heading": "6 end", "text": "feature descriptor can find its closest codeword in the codebook via vector quantization, and each image can be viewed as \u201ca bag of\u201d codewords by quantizing all the feature descriptors in the image. We further build a histogram for every image using the codewords in the codebook as histogram bins. The value in a histogram bin is equal to the number of times the corresponding codeword appears in the image. Let Hk be the histogram for image Ik. For any subset of images \u2126 from an initial image collection \u2126I , we compute the accumulated histogram H\u2126 by simply performing elementwise summation over the individual histograms of the images in \u2126. We further evaluate the representative power of \u2126 using the cross entropy of H\u2126. That is, Entropy(H\u2126) = \u2212 \u2211 j H\u2126(j) logH\u2126(j), where H\u2126(j) denotes the j-th element of H\u2126. A large cross entropy implies that the codewords corresponding to the histogram bins are evenly distributed in the images in \u2126 and vice versa. Thus, to encourage an even coverage of the feature space, the set of selected images essentially need to be the solution of the following expensive combinatorial optimization,\n\u2126 = arg max \u2126\u2208\u2126I \u2212 \u2211 j H\u2126(j) logH\u2126(j). (4)\nIn practice, we seek an approximate solution by progressively adding one image to \u2126 every time until we have a desired number of images in the subset. Every time the added image maximizes the cross entropy of the expanded subset. This process is illustrated in Algorithm 1."}, {"heading": "6. OVERVIEW OF EXPERIMENTS", "text": "Our proposed method is well suited for learning complex and highly nonlinear photo enhancement styles, especially when the style requires challenging spatially varying local enhancements. Successful local enhancement may not only rely on the content in a specific local region, but also contents in its surrounding areas. In that sense, such operations could easily result in complex effects that require stylistic or even exaggerated color transforms, making previous global methods (e.g., [Bychkovsky et al. 2011]) and local empirical methods (e.g., [Kaufman et al. 2012]) inapplicable. In contrast, our method was designed to address such challenges with the help of powerful contextual features and the strong regression capability of deep neural networks.\nTo fully evaluate our method, we hired one professional photographer who carefully retouched three different stylistic local effects using hundreds of photos. Section 7 reports experiments we have conducted to evaluate the performance of our method. Although our technique was designed to learn complex local effects, it can be readily applied to global image adjustments without any difficulty. Experiments in Section 8 and the supplemental materials show that our technique achieves superior performance both visually and nu-\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nmerically when compared with other state-of-the-art methods on the MIT-Adobe Fivek dataset. To objectively evaluate the effectiveness of our method, we have further conducted two user studies (Section 8.3) and obtained very positive results."}, {"heading": "6.1 Experimental Setup", "text": "Neural Network Setup. Throughout all the experiments in this paper, we use a fixed DNN with one input layer, two hidden layers, and one output layer (Figure 2). The number of neurons in the hidden layers were set empirically to 192, and the number of neurons in the output layer were set equal to the number of coefficients in the predicted color transform. Our experiments have confirmed that quadratic color transforms can more faithfully reproduce the colors in adjusted images than affine color transforms. Therefore, there are 30 neurons in the output layer, 10 for each of the three color channels.\nData Sampling. Since we learn pixel-level color mappings, every pixel within the image is a potential training sample. In practice, we segment each image into around 7,000 superpixels, from each of which we randomly select 10 pixels. Therefore, for example, even if we only have 70 example image pairs for learning one specific local effect, the number of training samples can be as large as 4.9 million. Such a large-scale training set can largely eliminate the risk of overfitting. It typically takes a few hours to finish training the neural network on a medium size training dataset with hundreds of images. Nevertheless, a trained neural network only needs 0.4 second to enhance a 512-pixel wide test image.\nImage Enhancement with Learned Color Mappings. Once we have learned the parameters (weights) of the neural network, during the image enhancement stage, we apply the same feature extraction pipeline to an input image as in the training stage. That is, we first perform scene parsing and object detection, and then apply graph-based segmentation to obtain superpixels. Likewise, we also extract a feature vector at the centroid of every superpixel, and apply the color transform returned by the neural network to every pixel within the superpixel. Specifically, the adjusted color at pixel pi is computed as yi = \u03a6(\u0398, x\u03bdi)V (ci), where \u03bdi is the superpixel that covers pi."}, {"heading": "7. LEARNING LOCAL ADJUSTMENTS", "text": ""}, {"heading": "7.1 Three Stylistic Local Effects", "text": "We manually downloaded 115 images from Flickr and resized them such that their larger dimension has 512 pixels. 70 images were chosen for training and the remaining 45 images for testing. A professional photographer used Photoshop to retouch these 115 images and produce the datasets for three different stylistic local effects. She could perform a wide range of operations to adjust the images, including selecting local objects/areas with the region selection tool, creating layers with layer masks, blending different layers using various modes, just to name a few. To reduce subjective variation during retouching, she used the \u201cactions\u201d tool, which records a sequence of operations, which can be repeatedly applied to selected image regions.\nThe first local effect \u201dForeground Pop-Out\u201d was created by increasing both the contrast and color saturation of foreground salient objects/regions, while decreasing the color saturation of the background. Before performing these operations, foreground salient regions need to be interactively segmented out using region selection tools in Photoshop. Such segmented regions were only used for dataset production, and they are not used in our enhancement\npipeline. This local effect makes foreground objects more visually vivid while making the background less distractive. Figure 6 (b) and (c) show three examples of our automatically enhanced results and groundtruth results from the photographer. Refer to the supplemental materials for the training data as well as our enhanced testing photos.\nOur second effect \u201dLocal Xpro\u201d was created by generalizing the popular \u201dcross processing\u201d effect in a local manner. Within Photoshop, the photographer first predefined multiple \u201dProfiles\u201d, each of which is specifically tailored for one of the semantic categories used in scene parsing and object detection in section 4.3. All the profiles share a common series of operations, such as the adjustment of individual color channels, color blending across color channels, hue/saturation adjustment as well as brightness/contrast manipulation, just to name a few. Nonetheless, each profile defines a distinct set of adjustment parameters tailored for its corresponding category. When retouching a photo, the photographer used region selection tools to isolate image regions and then applied one suitable profile to each image region according to the specific semantic content within that region. To avoid artifacts along region boundaries, she could also slightly adjust the color/tone of local regions after the application of profiles. Although the profiles roughly follow the \u201dcross processing\u201d style, the choice of local profiles and additional minor image editing were heavily influenced by the photographer\u2019s personal taste which can be naturally learned through exemplars. Figure 6 (d)&(e) show three examples in this effect, and compare our enhanced results against groundtruth results. Figure 1 shows another example of this effect.\nTo further increase diversity and complexity, we asked the photographer to create a third local effect \u201dWatercolor\u201d, which tries to mimic certain aspects of the \u201dwatercolor\u201d painting style. For example, watercolors tend to be brighter with lower saturation. Within a single brush region, the color variation also tends to be limited. The photographer first applied similar operations as in the Foreground Pop-Out effect to the input images, including increasing both contrast and saturation of foreground regions as well as decreasing those of background regions. In addition, the brightness of both foreground and background regions are increased by different amounts. She further created two layers of brush effects from the same brightened image, using larger \u201cbrushes\u201d on one layer and a smaller one on the other. On the first layer, the brush size for the foreground and the background are also different. Finally, these two layers are composited together using the \u2019Lighten\u2019 mode in Photoshop. Overall, this effect results in highly complex and spatially varying color transforms, which force the neural network to heavily rely on local contextual features during regression.\nFigure 6 (f)&(g) show the enhanced results of three testing examples and their corresponding groundtruth results. To simulate brush strokes, after applying the same color transform to all pixels in a superpixel, we calculate the average color within the superpixel and fill the superpixel with it. See another example of Watercolor effect as well as visualized superpixels in Fig 7. Our automatic results look visually similar to the ones produced by the photographer. Refer to the supplemental materials for more examples enhanced with this effect. Note that our intention here is not rigorously simulating watercolors, but experimentally validating that our technique is able to accurately learn such complex local adjustments.\nTo successfully learn an enhancement effect, it is important to make the adjustments on individual training images consistent. In practice, we have found the following strategies are helpful in increasing such consistency across an image set. First, as artistic adjustment of an image involves the personal taste of the photographer, the result could be quite different from different photographers.\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nTherefore, we always define a retouching style using photos adjusted by the same photographer. That means, even for the same input content, retouched results by different photographers are always defined as different styles. Second, we inform the photographer the semantic object categories that our scene parsing and object detection algorithms are aware of. Consequently, she can apply similar adjustments to visual objects in the same semantic category. Third, we use the \u201dactions\u201d tool in Photoshop to faithfully record the \u201dProfiles\u201d that should be applied to different semantic categories. This improves the consistency of color transforms applied to image regions with similar content and context."}, {"heading": "7.2 Spatially Varying Color Mappings", "text": "It is important to point out that the underlying color mappings in the local effect datasets are truly not global. They spatially vary within the image domain. To verify this, we collect pixels from each semantic region of an image. By drawing scatter plots for different semantic regions using pixel color pairs from the input and retouched images, we are able to visualize the spatially varying color transforms. See such an example in Figure 9, which clearly shows that the color transforms differ in the sky, building, grass and road regions. Also, we can see that our method can successfully learn such spatially varying complex color transforms. We further conducted a comparison against [Wang et al. 2011], which adopts a local piecewise approximation approach. However, due to the lack of discriminative contextual features, their learned adjustment parameters tend to be similar across different regions (Figure 8)."}, {"heading": "7.3 Generalization Capability", "text": "Here we verify the generalization capability of the DNN based photo adjustment models we trained using 70 image pairs. As mentioned earlier, the actual number of training samples far exceeds the number of training image pairs because we use thousands of superpixels within each training image pair. As shown in Fig. 10, we apply our trained models to novel testing images with significant visual differences from any images in the training set. The visual objects in these images have either unique appearances or unique spatial configurations. In Fig 10top, the mountain in the input image has\nan appearance and spatial layout that are different from the training images. In Fig 10bottom, the appearances and spatial configuration of the car and people are also quite different from those of the training images. In despite of these differences, our trained DNN models are still able to adjust the input images in a plausible way."}, {"heading": "7.4 Effectiveness of Contextual Features", "text": "We demonstrate the importance of contextual features in learning local adjustments in this subsection. First, we calculate theL2 distance in the 3D CIELab color space between input images and ground truth produced by the photographer for all local effect datasets as shown in the second column of Table I. They numerically reflect the magnitude of adjustments the photographer made to the input images. Second, we numerically compare the testing errors of our enhanced results with and without the contextual feature in the third and fourth columns of Table I. Our experiments show that without contextual features, testing errors of our enhanced results tend to be relatively high. The mean L2 error in the 3D CIELab color space reaches 9.27, 9.51 and 9.61 respectively for the Foreground Pop-Out, Local Xpro and Watercolor effects. On the other hand, by including our proposed contextual feature, all errors drop significantly to 7.08, 7.71 and 7.20, indicating the necessity of such features.\nTo validate the effectiveness of our multiscale spatial pooling schema in our contextual feature design, we have experimented with a simpler yet more intuitive contextual feature descriptor with just one pooling region (vs. our 28 multiscale regions) at the same size as our largest region, and found that such simple contextual features are helpful in reducing the errors but not as effective as ours. Taking\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nthe local Watercolor painting effect as an example, we observed the corresponding mean L2 error is 8.28, which drops from 9.61, but still obviously higher than our multiscale features 7.20. This is because, with multiscale pooling regions, our features can achieve a certain degree of translation and rotation invariance, which is crucial for the histogram based representation. We have also performed visual comparisons. Fig. 11 shows one such example. We can see that without our contextual feature, local regions in the enhanced photo might exhibit severe color deviation from the ground truth."}, {"heading": "7.5 Effectiveness of Learning Color Transforms", "text": "As shown in Figure 3, the use of color transforms helps absorb highfrequency color variations and enables DNN to regress the spatially smooth but otherwise highly nonlinear part of the color mapping. To highlight the benefits of using color transforms, we train a different DNN to regress the retouched colors directly. The DNN architecture is similar to the one described in section 6.1 except that there are only 3 neurons in the output layer, which represent the enhanced CIELab color. We compare the testing L2 errors on the Foregronud Pop-Out and Local Xpro datasets in Table II. On both datasets, the testing error increases by more than 55% which indicates the use of color transforms is beneficial in our task."}, {"heading": "7.6 DNN Architecture", "text": "The complexity of our DNN based model is primarily determined by the number of hidden layers and the number of neurons in each layer. Note that the complexity of the DNN architecture should meet the inherent complexity of the learning task. If the DNN did not have the sufficient complexity to handle the given task, the trained model would not even be able to accurately learn all the samples in the training set. On the other hand, if the complexity of the DNN exceeds the inherent complexity of the given task, there exists the risk of overfitting and the trained model would not be able to generalize well on novel testing data even though it could make the training error very small.\nThe nature of the learning task in this paper is a regression problem. It has been shown that a feedforward neural network with a single hidden layer [Hornik et al. 1989] can be used as a universal regressor and the necessary number of neurons in the hidden layer varies with the inherent complexity of the given regression problem. In practice, however, it is easier to achieve a small training error with a deeper network that has a relatively small number of neurons in the hidden layers. To assess the impact of the design choices of the DNN architecture, we evaluate DNNs with a varying number of hidden layers and neurons. We keep a held-out set of 30 images for validation and vary the number of training images from 40 to 85 at a step size of 15 to evaluate the impact of the size of the training set. We repeat the experiments for five times with random training and testing partitions and report the averaged results. The Foreground Pop-Out dataset is used in this study. Fig 12 summarizes our experimental results. Overall, neural networks with a single hidden layer deliver inferior performance than deeper networks. DNNs with 3 hidden layers do not perform as well as those with 2 hidden layers. For a DNN with 2 hidden layers, when the number of training im-\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nages exceeds 70, the testing error does not significantly improve any more. In summary, DNNs with 2 hidden layers achieve low testing errors and execute faster than those with 3 hidden layers in both training and testing stages. Therefore, we finally use a DNN with 2 hidden layers and 192 neurons each throughout this paper."}, {"heading": "7.7 Comparison with Other Regression Methods", "text": "Our DNN proves to be effective for regressing spatially varying complex color transforms on the three local effect datasets. It is also of great interest to evaluate the performance of other regressors on our datasets. Specifically, we chose to compare DNN against two popular regression methods, Lasso [Tibshirani 1996] and random forest [Breiman 2001]. Both Lasso and random forest are scalable to the large number of training samples used in DNN training. We\nuse Lasso and a random forest to directly regress target CIELab colors using the same feature set as in DNN training, including pixelwise features, global features and contextual features. The hyperparameters of both Lasso and the random forest are tuned using cross validation. A comparison of L2 errors is summarized in Table III. DNN significantly outperforms Lasso in all three local effect datasets, and obtains lower testing errors than the random forest on both Foreground Pop-Out and Watercolor datasets. On the Local Xpro dataset, the random forest slightly outperforms DNN. However, after visual inspection, we found that the color mappings generated by the random forest are not spatially smooth and blocky artifacts are prevalent in the enhanced images, as shown in Figure 13."}, {"heading": "8. LEARNING GLOBAL ADJUSTMENTS", "text": ""}, {"heading": "8.1 MIT-Adobe FiveK Dataset", "text": "The MIT-Adobe FiveK dataset [Bychkovsky et al. 2011] contains 5000 raw images, each of which was retouched by five well trained photographers, which results in five groups of global adjustment styles. As we learn pixel-level color mappings, there would be 175 million of training samples in total if half of the images are used for training.\nWe have compared our method with [Hwang et al. 2012] using the same experimental settings and testing datasets in that work. Two testing datasets were used in [Hwang et al. 2012]. (1)\u201cRandom 250\u201d: 250 randomly selected testing images from group C of the MIT-Adobe FiveK dataset (hence 4750 training images) and (2) \u201cHigh Variance 50\u201d: 50 images selected for testing from group C\nTable IV. Comparison of mean L2 errors obtained with our method and previous methods on the MIT-Adobe FiveK dataset. The target style is Expert C.\nMethod 2500(L) Ran. 250(L,a,b) H.50(L,a,b)\n[Bychkovsky et al. 2011]\n5.82 N/A N/A\n[Hwang et al. 2012]\nN/A 15.01 12.03\nOur method 5.68 9.85 8.36\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nof the MIT-Adobe FiveK dataset (hence 4950 images for training). Comparison results on numerical errors are shown in the second and third columns of Table IV, from which we can see our method is capable of achieving much better prediction performance in terms of mean L2 errors on both predefined datasets. Figure 14 further shows the error histograms of our method and [Hwang et al. 2012] on these two testing datasets. The errors produced by our method are mostly concentrated at the lower end of the histograms. Figure 15 shows a visual comparison, from which we can see our enhanced result is closer to the ground truth produced by the photographer. Such performance differences could be explained as follows. The technique in [Hwang et al. 2012] is based on nearest-neighbor search, which requires a fairly large training set that is slow to search. As a result, this technique divides similarity based search into two levels. It first searches for the most similar images and then the most similar pixels within them. While this two-level strategy accelerates the search, a large percentage of similar pixels does not even have the chance to be utilized because the search at the image level leaves out dissimilar images that may still contain many similar pixels. On the other hand, our deep neural network based method is a powerful nonlinear regression technique that considers all the training data simultaneously. Thus our method has a stronger extrapolation capability than the nearest-neighbor based approach in [Hwang et al. 2012], which only exploits a limited number of nearest neighbors. For the same reason, the nearest-neighbor based approach in [Hwang et al. 2012] is also more sensitive to noisy and inconsistent adjustments in the training data. In another comparison with [Bychkovsky et al. 2011], we follow the same setting used in that work, which experimented on 2500 training images from group C and reported the mean error on the L channel (CIELAB color space) only. As shown in the first column of Table IV, we obtained a slightly smaller mean error on the L channel on the remaining 2500 testing images.\nTo validate the effectiveness of our cross-entropy based training set selection method (Algorithm 1), we have monitored the testing errors by varying the number of training images selected by our method, and compared them with both naive random selection and the sensor placement method used in [Bychkovsky et al. 2011] (Figure 16). Interestingly, when the random selection scheme is used, our neural network based solution achieves significantly better accuracy than the Gaussian Process based method. This is primarily due to the strong nonlinear regression power exhibited by deep neural networks and the rich contextual feature representation built from semantic analysis. When compared with sensor placement, our cross-entropy based method also achieves better performance especially when the number of selected images is small, which further indicates our method is superior for learning enhancement styles from a small number of training images."}, {"heading": "8.2 Instagram Dataset", "text": "Instagram has become one of the most popular Apps on mobile phones. In Instagram, hundreds of filters can be applied to achieve\ndifferent artistic color and tone effects. For example, the frequently used \u201cLo-Fi\u201d filter boosts contrast and brings out warm tones; the \u201cRise\u201d filter adds a golden glow while \u201cHudson\u201d casts a cool light.\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nFor each specific effect, we randomly chose 50 images from MITAdobe FiveK, and let Instagram enhance each of them. Among the resulting 50 pairs of images, half of them were used for training, and the other half were for testing. We have verified whether images adjusted by the trained color mapping functions are similar to the ground truth produced by Instagram, which has the flavor of a reverse engineering task. Our experiments indicate that Instagram effects are relatively easy to learn using our method. Figure 17 shows the learning results for two popular effects."}, {"heading": "8.3 User Studies", "text": "To perform a visual comparison between our results and those produced by [Hwang et al. 2012] in an objective way, we collected all the images from the two datasets, \u201cRandom 250\u201d and \u201cHigh variance 50\u201d, and randomly chose 50, including 10 indoor images and 40 outdoor images, to be used in our user study. For each of these 50 testing images, we also collected the groundtruth images and the enhanced images produced with our method and [Hwang et al. 2012]. Then we invited 33 participants, including 12 females and 21 males, with ages ranging from 21 to 28. These participants had little experience of using any professional photo adjustment tools but did have experience with photo enhancement Apps such as \u201cInstagram\u201d. The experiment was carried out by asking each participant to open a static website using a prepared computer and a 24-inch monitor with a 1920x1080 resolution. For each test image, we first show the input and the groundtruth image pair to let the participants know how the input image was enhanced by the photographer (retoucher C). Then we show two enhanced images automatically generated with our method and Hwang et al. in a random left/right layout\nwithout disclosing which one was enhanced by our method. The participant was asked to compare them with the ground truth and vote on one of the following three choices: (a) \u201cThe left image was enhanced better\u201d, (b) \u201cThe right image was enhanced better\u201d, and (c) \u201cHard to choose\u201d. In this way, we collected 33x50=1650 votes distributed among the three choices. Figure 18 shows a comparison of the voting results, from which we can see that enhanced images produced by our method received most of the votes in both indoor and outdoor categories. This comparison indicates that, from a visual perspective, our method can produce much better enhanced images than [Hwang et al. 2012].\nOur second user study tries to verify whether our method has the capability to enhance a target effect in a statistically significant manner. To conduct this study, we chose 30 test images from one of the local effect datasets described in Section 7.1 as our test data. We asked 20 participants from the first study to join our second study. The interface was designed as follows. On top of the screen, we show as the ground truth the enhanced image produced by the photographer we hired, below which we show a pair of images with the left being the original image and the right being the enhanced image produced by our method. Then we asked the participant to assign a score to both the input and enhanced images by considering two criteria at the same time: (1) how closely this image conforms to the impression given by the ground truth, (2) the visual quality of the image. In other words, if the enhanced image looks visually pleasing and closer to the ground truth, it should receive a higher score. For the convenience of the participants, we simply discretized the range of scores into 10 levels. If an image looks extremely close to the ground truth, it should be scored 10. At the end, we collected two sets of scores for the original and enhanced images, respectively. We then conducted the paired T-test on the two sets of scores and found that the two-tail p-value is p \u2248 10\u221210, and t = 1.96, indicating that our approach has significantly enhanced the desired effect from a statistical point of view."}, {"heading": "9. CONCLUSIONS AND DISCUSSIONS", "text": "In this paper, we have demonstrated the effectiveness of deep learning in automatic photo adjustment. We cast this problem as learning a highly nonlinear mapping function by taking the bundled features as the input layer of a deep neural network. The bundled features include a pixelwise descriptor, a global descriptor, as well as a novel contextual descriptor which is built on top of scene parsing and object detection. We have conducted extensive experiments on a number of effects including both conventional and artistic ones. Our experiments show that the proposed approach is able to effectively learn computational models for automatic spatially-varying photo adjustment.\nACM Transactions on Graphics, Vol. VV, No. N, Article XXX, Publication date: Month YYYY.\nLimitations. Our approach relies on both scene parsing and object detection to build contextual features. However, in general, these are still challenging problems in computer vision and pattern recognition. Mislabeling in the semantic map can propagate into contextual features and adversely affect photo adjustment. Fig 19(a) shows one such example for the Foreground Pop-Out effect. The \u2018sea\u2019 on the right side is mistakenly labeled as \u2018mountain\u2019 and its saturation and contrast are incorrectly increased. As both scene parsing and object detection are rapidly developing areas, more accurate techniques are emerging and could be adopted by our system to produce more reliable semantic label maps.\nAnother failure case is shown in Fig 19(b), where the adjustments in group C of the MIT-Adobe FiveK dataset are learnt. Our method produces insufficient brightness adjustment, which leads to dimmer result than the ground truth. In fact, the L2 distance between the input image and the ground truth is 38.63, which is significantly higher than the mean distance 17.40 of the dataset. As our DNN is\ntrained using all available training samples, individual adjustments significantly deviating from the average adjustment for a semantic object type are likely to be treated as outliers and cannot be correctly learnt.\nOur system employs a deep fully connected neural network to regress spatially varying color transforms. There exist many design choices in the DNN architecture, including the number of hidden layers, the number of neurons in each layer, and the type of neural activation functions. They together give rise to a time-consuming trial-and-error process in search of a suitable DNN architecture for the given task. In addition, DNN behaves as a black box and it is not completely clear how the network combines features at different scales and predicts the final color transforms. In fact, interpreting the internal representations of deep neural networks is still an ongoing research topic [Zeiler and Fergus 2013; Szegedy et al. 2013]."}, {"heading": "ACKNOWLEDGMENTS", "text": "We are grateful to Vladimir Bychkovsky and Sung Ju Hwang for fruitful discussions and suggestions. This work was partially supported by Hong Kong Research Grants Council under General Research Funds (HKU17209714)."}], "references": [{"title": "Appprop: all-pairs appearance-space edit propagation", "author": ["X. AN", "F. PELLACINI"], "venue": "ACM Trans. Graph. 27, 3.", "citeRegEx": "AN and PELLACINI,? 2008", "shortCiteRegEx": "AN and PELLACINI", "year": 2008}, {"title": "Contour detection and hierarchical image segmentation", "author": ["P. ARBELAEZ", "M. MAIRE", "C. FOWLKES", "J. MALIK"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 33, 5, 898\u2013916.", "citeRegEx": "ARBELAEZ et al\\.,? 2011", "shortCiteRegEx": "ARBELAEZ et al\\.", "year": 2011}, {"title": "Two-scale tone management for photographic look", "author": ["S. BAE", "S. PARIS", "F. DURAND"], "venue": "ACM Trans. Graph. 25, 3, 637\u2013645.", "citeRegEx": "BAE et al\\.,? 2006", "shortCiteRegEx": "BAE et al\\.", "year": 2006}, {"title": "Shape matching and object recognition using shape contexts", "author": ["S. BELONGIE", "J. MALIK", "J. PUZICHA"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 24, 4 (Apr.), 509\u2013522.", "citeRegEx": "BELONGIE et al\\.,? 2002", "shortCiteRegEx": "BELONGIE et al\\.", "year": 2002}, {"title": "Random forests", "author": ["L. BREIMAN"], "venue": "Machine learning 45, 1, 5\u201332.", "citeRegEx": "BREIMAN,? 2001", "shortCiteRegEx": "BREIMAN", "year": 2001}, {"title": "Learning photographic global tonal adjustment with a database of input/output image pairs", "author": ["V. BYCHKOVSKY", "S. PARIS", "E. CHAN", "F. DURAND"], "venue": "Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition. CVPR \u201911. 97\u2013104.", "citeRegEx": "BYCHKOVSKY et al\\.,? 2011", "shortCiteRegEx": "BYCHKOVSKY et al\\.", "year": 2011}, {"title": "Collaborative personalization of image enhancement", "author": ["J. CAICEDO", "A. KAPOOR", "S.B. KANG"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. 249\u2013256.", "citeRegEx": "CAICEDO et al\\.,? 2011", "shortCiteRegEx": "CAICEDO et al\\.", "year": 2011}, {"title": "Color harmonization", "author": ["D. COHEN-OR", "O. SORKINE", "R. GAL", "T. LEYVAND", "XU", "Y.-Q."], "venue": "ACM Trans. Graph. 25, 3 (jul), 624\u2013630.", "citeRegEx": "COHEN.OR et al\\.,? 2006", "shortCiteRegEx": "COHEN.OR et al\\.", "year": 2006}, {"title": "Image restoration using online photo collections", "author": ["K. DALE", "M. JOHNSON", "K. SUNKAVALLI", "W. MATUSIK", "H. PFISTER"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. 2217\u20132224.", "citeRegEx": "DALE et al\\.,? 2009", "shortCiteRegEx": "DALE et al\\.", "year": 2009}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["P. FELZENSZWALB", "D. MCALLESTER", "D. RAMANAN"], "venue": "IEEE Conf. on Computer Vision and Pattern Recognition.", "citeRegEx": "FELZENSZWALB et al\\.,? 2008", "shortCiteRegEx": "FELZENSZWALB et al\\.", "year": 2008}, {"title": "Efficient graphbased image segmentation", "author": ["P.F. FELZENSZWALB", "D.P. HUTTENLOCHER"], "venue": "Int. J. Comput. Vision 59, 2 (Sept.), 167\u2013181.", "citeRegEx": "FELZENSZWALB and HUTTENLOCHER,? 2004", "shortCiteRegEx": "FELZENSZWALB and HUTTENLOCHER", "year": 2004}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. HINTON", "N. SRIVASTAVA", "A. KRIZHEVSKY", "I. SUTSKEVER", "R. SALAKHUTDINOV"], "venue": "CoRR abs/1207.0580.", "citeRegEx": "HINTON et al\\.,? 2012", "shortCiteRegEx": "HINTON et al\\.", "year": 2012}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. HORNIK", "M. STINCHCOMBE", "H. WHITE"], "venue": "Neural Netw. 2, 5 (July), 359\u2013366.", "citeRegEx": "HORNIK et al\\.,? 1989", "shortCiteRegEx": "HORNIK et al\\.", "year": 1989}, {"title": "Context-based automatic local image enhancement", "author": ["S.J. HWANG", "A. KAPOOR", "S.B. KANG"], "venue": "Proceedings of the 12th European Conference on Computer Vision - Volume Part I. ECCV\u201912. 569\u2013582.", "citeRegEx": "HWANG et al\\.,? 2012", "shortCiteRegEx": "HWANG et al\\.", "year": 2012}, {"title": "Personal photo enhancement using example images", "author": ["N. JOSHI", "W. MATUSIK", "E.H. ADELSON", "D.J. KRIEGMAN"], "venue": "ACM Trans. Graph. 29, 2 (Apr.), 12:1\u201312:15.", "citeRegEx": "JOSHI et al\\.,? 2010", "shortCiteRegEx": "JOSHI et al\\.", "year": 2010}, {"title": "Personalization of image enhancement", "author": ["S.B. KANG", "A. KAPOOR", "D. LISCHINSKI"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. 1799\u20131806.", "citeRegEx": "KANG et al\\.,? 2010", "shortCiteRegEx": "KANG et al\\.", "year": 2010}, {"title": "Content-aware automatic photo enhancement", "author": ["L. KAUFMAN", "D. LISCHINSKI", "M. WERMAN"], "venue": "Comp. Graph. Forum 31, 8 (Dec.), 2528\u2013 2540.", "citeRegEx": "KAUFMAN et al\\.,? 2012", "shortCiteRegEx": "KAUFMAN et al\\.", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. KRIZHEVSKY", "I. SUTSKEVER", "G.E. HINTON"], "venue": "Advances in Neural Information Processing Systems 25, P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds. 1106\u20131114.", "citeRegEx": "KRIZHEVSKY et al\\.,? 2012", "shortCiteRegEx": "KRIZHEVSKY et al\\.", "year": 2012}, {"title": "Interactive local adjustment of tonal values", "author": ["D. LISCHINSKI", "Z. FARBMAN", "M. UYTTENDAELE", "R. SZELISKI"], "venue": "ACM Trans. Graph. 25, 3, 646\u2013653.", "citeRegEx": "LISCHINSKI et al\\.,? 2006", "shortCiteRegEx": "LISCHINSKI et al\\.", "year": 2006}, {"title": "Nonparametric scene parsing via label transfer", "author": ["C. LIU", "J. YUEN", "A. TORRALBA"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 33, 12, 2368?382.", "citeRegEx": "LIU et al\\.,? 2011", "shortCiteRegEx": "LIU et al\\.", "year": 2011}, {"title": "Intriguing properties of neural networks", "author": ["C. SZEGEDY", "W. ZAREMBA", "I. SUTSKEVER", "J. BRUNA", "D. ERHAN", "I. GOODFELLOW", "R. FERGUS"], "venue": "arXiv preprint arXiv:1312.6199.", "citeRegEx": "SZEGEDY et al\\.,? 2013", "shortCiteRegEx": "SZEGEDY et al\\.", "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. TIBSHIRANI"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), 267\u2013 288.", "citeRegEx": "TIBSHIRANI,? 1996", "shortCiteRegEx": "TIBSHIRANI", "year": 1996}, {"title": "Superparsing: Scalable nonparametric image parsing with superpixels", "author": ["J. TIGHE", "S. LAZEBNIK"], "venue": "Proceedings of the 11th European Conference on Computer Vision: Part V. ECCV\u201910. 352\u2013365.", "citeRegEx": "TIGHE and LAZEBNIK,? 2010", "shortCiteRegEx": "TIGHE and LAZEBNIK", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. VINCENT", "H. LAROCHELLE", "Y. BENGIO", "MANZAGOL", "P.-A."], "venue": "International Conference on Machine Learning. 1096\u20131103.", "citeRegEx": "VINCENT et al\\.,? 2008", "shortCiteRegEx": "VINCENT et al\\.", "year": 2008}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. VIOLA", "M. JONES"], "venue": "IEEE Conf. on Computer Vision and Pattern Recognition.", "citeRegEx": "VIOLA and JONES,? 2001", "shortCiteRegEx": "VIOLA and JONES", "year": 2001}, {"title": "Example-based image color and tone style enhancement", "author": ["B. WANG", "Y. YU", "XU", "Y.-Q."], "venue": "ACM SIGGRAPH 2011 Papers. SIGGRAPH \u201911. 64:1\u201364:12.", "citeRegEx": "WANG et al\\.,? 2011", "shortCiteRegEx": "WANG et al\\.", "year": 2011}, {"title": "Regionlets for generic object detection", "author": ["WANG X.", "YANG M.", "ZHU S.", "LIN", "Y."], "venue": "ICCV\u201913: Proc. IEEE 14th International Conf. on Computer Vision.", "citeRegEx": "X. et al\\.,? 2013", "shortCiteRegEx": "X. et al\\.", "year": 2013}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M.D. ZEILER", "R. FERGUS"], "venue": "arXiv preprint arXiv:1311.2901.", "citeRegEx": "ZEILER and FERGUS,? 2013", "shortCiteRegEx": "ZEILER and FERGUS", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": ", [Vincent et al. 2008; Krizhevsky et al. 2012].", "startOffset": 2, "endOffset": 47}, {"referenceID": 17, "context": ", [Vincent et al. 2008; Krizhevsky et al. 2012].", "startOffset": 2, "endOffset": 47}, {"referenceID": 12, "context": "A deep neural network (DNN) is a universal approximator that can represent arbitrarily complex continuous functions [Hornik et al. 1989].", "startOffset": 116, "endOffset": 136}, {"referenceID": 18, "context": "In addition to these tools, there exists much research on either interactive [Lischinski et al. 2006; An and Pellacini 2008] or automatic [Bae et al.", "startOffset": 77, "endOffset": 124}, {"referenceID": 0, "context": "In addition to these tools, there exists much research on either interactive [Lischinski et al. 2006; An and Pellacini 2008] or automatic [Bae et al.", "startOffset": 77, "endOffset": 124}, {"referenceID": 2, "context": "2006; An and Pellacini 2008] or automatic [Bae et al. 2006; Cohen-Or et al. 2006] color and tone adjustment.", "startOffset": 42, "endOffset": 81}, {"referenceID": 7, "context": "2006; An and Pellacini 2008] or automatic [Bae et al. 2006; Cohen-Or et al. 2006] color and tone adjustment.", "startOffset": 42, "endOffset": 81}, {"referenceID": 0, "context": "2006; An and Pellacini 2008] or automatic [Bae et al. 2006; Cohen-Or et al. 2006] color and tone adjustment. Automatic methods typically operate on the entire image in a global manner without taking image content into consideration. To address this issue, Kaufman et al. [2012] introduces an automatic method that first detects semantic content, including faces, sky as well as shadowed salient regions, and then applies a sequence of empirically determined steps for saturation, contrast as well as exposure adjustment.", "startOffset": 6, "endOffset": 278}, {"referenceID": 0, "context": "2006; An and Pellacini 2008] or automatic [Bae et al. 2006; Cohen-Or et al. 2006] color and tone adjustment. Automatic methods typically operate on the entire image in a global manner without taking image content into consideration. To address this issue, Kaufman et al. [2012] introduces an automatic method that first detects semantic content, including faces, sky as well as shadowed salient regions, and then applies a sequence of empirically determined steps for saturation, contrast as well as exposure adjustment. However, the limit of this approach is that output style is hard-coded in the algorithm and cannot be easily tuned to achieve a desired style. In comparison and as we shall see, our data-driven approach can easily be trained to produce a variety of styles. Further, these techniques rely on a fixed pipeline that is inherently limited in its ability to achieve user-preferred artistic enhancement effects, especially the exaggerated and dramatic ones. In practice, a fixedpipeline technique works well for a certain class of adjustments and only produces approximate results for effects outside this class. For instance, Bae et al. [2006] do well with tonal global transforms but", "startOffset": 6, "endOffset": 1160}, {"referenceID": 15, "context": "Learning based image enhancement [Kang et al. 2010; Joshi et al. 2010; Caicedo et al. 2011; Bychkovsky et al. 2011] and image restoration [Dale et al.", "startOffset": 33, "endOffset": 115}, {"referenceID": 14, "context": "Learning based image enhancement [Kang et al. 2010; Joshi et al. 2010; Caicedo et al. 2011; Bychkovsky et al. 2011] and image restoration [Dale et al.", "startOffset": 33, "endOffset": 115}, {"referenceID": 6, "context": "Learning based image enhancement [Kang et al. 2010; Joshi et al. 2010; Caicedo et al. 2011; Bychkovsky et al. 2011] and image restoration [Dale et al.", "startOffset": 33, "endOffset": 115}, {"referenceID": 5, "context": "Learning based image enhancement [Kang et al. 2010; Joshi et al. 2010; Caicedo et al. 2011; Bychkovsky et al. 2011] and image restoration [Dale et al.", "startOffset": 33, "endOffset": 115}, {"referenceID": 8, "context": "2011] and image restoration [Dale et al. 2009] have shown promising results and therefore received much attention.", "startOffset": 28, "endOffset": 46}, {"referenceID": 10, "context": "do not model local edits, and Kaufman et al. [2012] perform well on a predetermined set of semantic categories but does not handle elements outside this set.", "startOffset": 30, "endOffset": 52}, {"referenceID": 5, "context": "2011; Bychkovsky et al. 2011] and image restoration [Dale et al. 2009] have shown promising results and therefore received much attention. Kang et al. [2010] found that image quality assessment is actually very much personalized, which results in an automatic method for learning individual preferences in global photo adjustment.", "startOffset": 6, "endOffset": 158}, {"referenceID": 5, "context": "2011; Bychkovsky et al. 2011] and image restoration [Dale et al. 2009] have shown promising results and therefore received much attention. Kang et al. [2010] found that image quality assessment is actually very much personalized, which results in an automatic method for learning individual preferences in global photo adjustment. Bychkovsky et al. [2011] introduces a method based on Gaussian processes for learning tone mappings according to global image statistics.", "startOffset": 6, "endOffset": 356}, {"referenceID": 5, "context": "2011; Bychkovsky et al. 2011] and image restoration [Dale et al. 2009] have shown promising results and therefore received much attention. Kang et al. [2010] found that image quality assessment is actually very much personalized, which results in an automatic method for learning individual preferences in global photo adjustment. Bychkovsky et al. [2011] introduces a method based on Gaussian processes for learning tone mappings according to global image statistics. Since these methods were designed for global image adjustment, they do not consider local image contexts and cannot produce spatially varying local enhancements. Wang et al. [2011] proposes a method based on piecewise approximation for learning color mapping functions from exemplars.", "startOffset": 6, "endOffset": 650}, {"referenceID": 5, "context": "2011; Bychkovsky et al. 2011] and image restoration [Dale et al. 2009] have shown promising results and therefore received much attention. Kang et al. [2010] found that image quality assessment is actually very much personalized, which results in an automatic method for learning individual preferences in global photo adjustment. Bychkovsky et al. [2011] introduces a method based on Gaussian processes for learning tone mappings according to global image statistics. Since these methods were designed for global image adjustment, they do not consider local image contexts and cannot produce spatially varying local enhancements. Wang et al. [2011] proposes a method based on piecewise approximation for learning color mapping functions from exemplars. It does not consider semantic or contextual information either. In addition, it is not fully automatic, and relies on interactive soft segmentation. It is infeasible for this technique to automatically enhance a collection of images. In comparison, this paper proposes a scalable framework for learning user-defined complex enhancement effects from exemplars. It explicitly performs generic image semantic analysis, and its image enhancement models are trained using feature descriptors constructed from semantic analysis results. Hwang et al. [2012] proposes a context-aware local image enhancement technique.", "startOffset": 6, "endOffset": 1305}, {"referenceID": 12, "context": "Multi-layer deep neural networks have proven to be able to represent arbitrarily complex continuous functions [Hornik et al. 1989].", "startOffset": 110, "endOffset": 130}, {"referenceID": 17, "context": "We choose the rectified linear unit (ReLU) [Krizhevsky et al. 2012], g(z) = max(0, z), as the activation function in our networks.", "startOffset": 43, "endOffset": 67}, {"referenceID": 17, "context": "In addition, we apply the Dropout training strategy [Krizhevsky et al. 2012; Hinton et al. 2012], which has been shown very useful for improving the generalization capability.", "startOffset": 52, "endOffset": 96}, {"referenceID": 11, "context": "In addition, we apply the Dropout training strategy [Krizhevsky et al. 2012; Hinton et al. 2012], which has been shown very useful for improving the generalization capability.", "startOffset": 52, "endOffset": 96}, {"referenceID": 5, "context": "Specifically, we adopt six types of global features proposed in [Bychkovsky et al. 2011], including intensity distribution, scene brightness, equalization curves, detail-weighted equalization curves, highlight clipping, and spatial distribution, which altogether give rise to a 207-dimensional vector.", "startOffset": 64, "endOffset": 88}, {"referenceID": 22, "context": "Typical image semantic analysis algorithms include scene parsing [Tighe and Lazebnik 2010; Liu et al. 2011] and object detection [Viola and Jones 2001; Felzenszwalb et al.", "startOffset": 65, "endOffset": 107}, {"referenceID": 19, "context": "Typical image semantic analysis algorithms include scene parsing [Tighe and Lazebnik 2010; Liu et al. 2011] and object detection [Viola and Jones 2001; Felzenszwalb et al.", "startOffset": 65, "endOffset": 107}, {"referenceID": 24, "context": "2011] and object detection [Viola and Jones 2001; Felzenszwalb et al. 2008; Wang et al. 2013].", "startOffset": 27, "endOffset": 93}, {"referenceID": 9, "context": "2011] and object detection [Viola and Jones 2001; Felzenszwalb et al. 2008; Wang et al. 2013].", "startOffset": 27, "endOffset": 93}, {"referenceID": 22, "context": "During pixel annotation, we perform scene parsing using the state-of-theart algorithm in [Tighe and Lazebnik 2010].", "startOffset": 89, "endOffset": 114}, {"referenceID": 1, "context": "In our experiments, we adopt the image segmentation algorithm in [Arbelaez et al. 2011].", "startOffset": 65, "endOffset": 87}, {"referenceID": 3, "context": "Our multiscale context descriptor is partially inspired by shape contexts [Belongie et al. 2002].", "startOffset": 74, "endOffset": 96}, {"referenceID": 24, "context": "However, unlike the shape context descriptor, our regions and subregions are either rectangles or squares, which facilitate fast histogram computation based on integral images (originally called summed area tables) [Viola and Jones 2001].", "startOffset": 215, "endOffset": 237}, {"referenceID": 10, "context": "For each training image I , we first apply the graph-based segmentation [Felzenszwalb and Huttenlocher 2004] to divide the image into small homogeneous yet irregularly shaped patches, each of which is called a superpixel.", "startOffset": 72, "endOffset": 108}, {"referenceID": 5, "context": ", [Bychkovsky et al. 2011]) and local empirical methods (e.", "startOffset": 2, "endOffset": 26}, {"referenceID": 16, "context": ", [Kaufman et al. 2012]) inapplicable.", "startOffset": 2, "endOffset": 23}, {"referenceID": 25, "context": "We further conducted a comparison against [Wang et al. 2011], which adopts a local piecewise approximation approach.", "startOffset": 42, "endOffset": 60}, {"referenceID": 25, "context": "Comparison with [Wang et al. 2011] on the Local Xpro effect.", "startOffset": 16, "endOffset": 34}, {"referenceID": 25, "context": "Top Left: Input image; Top Right: enhanced image by [Wang et al. 2011]; Bottom Left: enhanced image by our approach ;Bottom Right:enhanced image by photographer.", "startOffset": 52, "endOffset": 70}, {"referenceID": 12, "context": "It has been shown that a feedforward neural network with a single hidden layer [Hornik et al. 1989] can be used as a universal regressor and the necessary number of neurons in the hidden layer varies with the inherent complexity of the given regression problem.", "startOffset": 79, "endOffset": 99}, {"referenceID": 21, "context": "Specifically, we chose to compare DNN against two popular regression methods, Lasso [Tibshirani 1996] and random forest [Breiman 2001].", "startOffset": 84, "endOffset": 101}, {"referenceID": 4, "context": "Specifically, we chose to compare DNN against two popular regression methods, Lasso [Tibshirani 1996] and random forest [Breiman 2001].", "startOffset": 120, "endOffset": 134}, {"referenceID": 5, "context": "The MIT-Adobe FiveK dataset [Bychkovsky et al. 2011] contains 5000 raw images, each of which was retouched by five well trained photographers, which results in five groups of global adjustment styles.", "startOffset": 28, "endOffset": 52}, {"referenceID": 13, "context": "We have compared our method with [Hwang et al. 2012] using the same experimental settings and testing datasets in that work.", "startOffset": 33, "endOffset": 52}, {"referenceID": 13, "context": "Two testing datasets were used in [Hwang et al. 2012].", "startOffset": 34, "endOffset": 53}, {"referenceID": 5, "context": "[Bychkovsky et al. 2011] 5.", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "[Hwang et al. 2012] N/A 15.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "Figure 14 further shows the error histograms of our method and [Hwang et al. 2012] on these two testing datasets.", "startOffset": 63, "endOffset": 82}, {"referenceID": 13, "context": "The technique in [Hwang et al. 2012] is based on nearest-neighbor search, which requires a fairly large training set that is slow to search.", "startOffset": 17, "endOffset": 36}, {"referenceID": 13, "context": "Thus our method has a stronger extrapolation capability than the nearest-neighbor based approach in [Hwang et al. 2012], which only exploits a limited number of nearest neighbors.", "startOffset": 100, "endOffset": 119}, {"referenceID": 13, "context": "For the same reason, the nearest-neighbor based approach in [Hwang et al. 2012] is also more sensitive to noisy and inconsistent adjustments in the training data.", "startOffset": 60, "endOffset": 79}, {"referenceID": 5, "context": "In another comparison with [Bychkovsky et al. 2011], we follow the same setting used in that work, which experimented on 2500 training images from group C and reported the mean error on the L channel (CIELAB color space) only.", "startOffset": 27, "endOffset": 51}, {"referenceID": 5, "context": "To validate the effectiveness of our cross-entropy based training set selection method (Algorithm 1), we have monitored the testing errors by varying the number of training images selected by our method, and compared them with both naive random selection and the sensor placement method used in [Bychkovsky et al. 2011] (Figure 16).", "startOffset": 295, "endOffset": 319}, {"referenceID": 13, "context": "Input Image Ground Truth Our Result [Hwang et al. 2012]", "startOffset": 36, "endOffset": 55}, {"referenceID": 13, "context": "Visual comparison with [Hwang et al. 2012].", "startOffset": 23, "endOffset": 42}, {"referenceID": 13, "context": "Left: Input image; Middle Left: groundtruth enhanced image by expert C; Middle Right: enhanced image by our approach; Right: enhanced image by [Hwang et al. 2012].", "startOffset": 143, "endOffset": 162}, {"referenceID": 13, "context": "To perform a visual comparison between our results and those produced by [Hwang et al. 2012] in an objective way, we collected all the images from the two datasets, \u201cRandom 250\u201d and \u201cHigh variance 50\u201d, and randomly chose 50, including 10 indoor images and 40 outdoor images, to be used in our user study.", "startOffset": 73, "endOffset": 92}, {"referenceID": 13, "context": "For each of these 50 testing images, we also collected the groundtruth images and the enhanced images produced with our method and [Hwang et al. 2012].", "startOffset": 131, "endOffset": 150}, {"referenceID": 13, "context": "This comparison indicates that, from a visual perspective, our method can produce much better enhanced images than [Hwang et al. 2012].", "startOffset": 115, "endOffset": 134}, {"referenceID": 13, "context": "A comparison of user voting results between our approach and [Hwang et al. 2012]", "startOffset": 61, "endOffset": 80}, {"referenceID": 27, "context": "In fact, interpreting the internal representations of deep neural networks is still an ongoing research topic [Zeiler and Fergus 2013; Szegedy et al. 2013].", "startOffset": 110, "endOffset": 155}, {"referenceID": 20, "context": "In fact, interpreting the internal representations of deep neural networks is still an ongoing research topic [Zeiler and Fergus 2013; Szegedy et al. 2013].", "startOffset": 110, "endOffset": 155}], "year": 2014, "abstractText": "Photo retouching enables photographers to invoke dramatic visual impressions by artistically enhancing their photos through stylistic color and tone adjustments. However, it is also a time-consuming and challenging task that requires advanced skills beyond the abilities of casual photographers. Using an automated algorithm is an appealing alternative to manual work but such an algorithm faces many hurdles. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Further, these adjustments are often spatially varying. Because of these characteristics, existing automatic algorithms are still limited and cover only a subset of these challenges. Recently, deep machine learning has shown unique abilities to address hard problems that resisted machine algorithms for long. This motivated us to explore the use of deep learning in the context of photo editing. In this paper, we explain how to formulate the automatic photo adjustment problem in a way suitable for this approach. We also introduce an image descriptor that accounts for the local semantics of an image. Our experiments demonstrate that our deep learning formulation applied using these descriptors successfully capture sophisticated photographic styles. In particular and unlike previous techniques, it can model local adjustments that depend on the image semantics. We show on several examples that this yields results that are qualitatively and quantitatively better than previous work.", "creator": "LaTeX with hyperref package"}}}