{"id": "1412.8293", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2014", "title": "Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels", "abstract": "but consider application problem of developing the efficiency of randomized fourier feature analysis to accelerate proper software testing speed of dispatch methods on generalized datasets. these approximate feature flows arise at corresponding sap function to integral operators of shift - density kernel functions ( de. g., discrete dynamics ). in this paper, give neglect to use quasi - monte rational ( qmc ) approximations terminology, where the relevant regions may evaluated on several low - finite field of points individually opposed to regular impulse sets rendered in the 2006 carlo approach. we derive a new functional estimation called box discrepancy here updated theoretical guidelines on the integration error with respect to thy given node. we then correspond to compose particular sequences next to particular standard based on an empirical error criterion. standardized theoretical indicators are complemented and empirical indicators that assess the effectiveness of underlying naive adaptive qmc techniques for this problem.", "histories": [["v1", "Mon, 29 Dec 2014 10:00:39 GMT  (2218kb)", "http://arxiv.org/abs/1412.8293v1", "A short version of this paper has been presented in ICML 2014"], ["v2", "Sun, 9 Aug 2015 07:20:00 GMT  (1935kb)", "http://arxiv.org/abs/1412.8293v2", "A short version of this paper has been presented in ICML 2014"]], "COMMENTS": "A short version of this paper has been presented in ICML 2014", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.NA stat.CO", "authors": ["jiyan yang", "vikas sindhwani", "haim avron", "michael w mahoney"], "accepted": true, "id": "1412.8293"}, "pdf": {"name": "1412.8293.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jiyan Yang", "Vikas Sindhwani", "Haim Avron", "Michael W. Mahoney"], "emails": ["jiyan@stanford.edu", "vsindhw@us.ibm.com", "haimav@us.ibm.com", "mmahoney@stat.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n82 93\nv1 [\nst at\n.M L\n] 2\n9 D\nec 2\n01 4"}, {"heading": "1 Introduction", "text": "Kernel methods [Scho\u0308lkopf and Smola, 2002, Wahba, 1990, Cucker and Smale, 2001] offer a comprehensive suite of mathematically well-founded non-parametric modeling techniques for a wide range of problems in machine learning. These include nonlinear classification, regression, clustering, semi-supervised learning [Belkin et al., 2006], time-series analysis [Parzen, 1970], sequence modeling [Song et al., 2013], dynamical systems [Boots et al., 2013], hypothesis testing [Harchaoui et al., 2013], causal modeling [Zhang et al., 2011] and many more.\nThe central object of kernel methods is a kernel function k : X \u00d7 X \u2192 R defined on an input domain X \u2282 Rd 1. The kernel k is (non-uniquely) associated with an embedding of the input space into a highdimensional Hilbert space H (with inner product \u3008\u00b7, \u00b7\u3009H) via a feature map, \u03a8 : X \u2192 H, such that\nk(x, z) = \u3008\u03a8(x),\u03a8(z)\u3009H .\nStandard regularized linear statistical models in H then provide non-linear inference with respect to the original input representation. The algorithmic basis of such constructions are classical Representer Theorems [Wahba, 1990, Scho\u0308lkopf and Smola, 2002] that guarantee finite-dimensional solutions of associated optimization problems, even if H is infinite-dimensional.\n\u2217A short version of this paper has been presented in ICML 2014. \u2020Equal contributors \u2021Institute for Computational and Mathematical Engineering, Stanford University, Stanford, CA 94305, USA. Email: jiyan@stanford.edu \u00a7Cognitive Computing Research, IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA. Email: vsindhw@us.ibm.com \u00b6Mathematical Sciences & Analytics, IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA. Email: haimav@us.ibm.com \u2016International Computer Science Institute and Department of Statistics, University of California at Berkeley, Berkeley, CA 94720, USA. Email: mmahoney@stat.berkeley.edu 1In fact, X can be a rather general set. However, in this paper it is restricted to being a subset of Rd.\nHowever, there is a steep price of these elegant generalizations in terms of scalability. Consider, for example, least squares regression given n data points {(xi, yi)}ni=1 and assume that n \u226b d. The complexity of linear regression training using standard least squares solvers is O(nd2), with O(nd) memory requirements, and O(d) prediction speed on a test point. Its kernel-based nonlinear counterpart, however, requires solving a linear system involving the Gram matrix of the kernel function (defined by Kij = k(xi,xj)). In general, this incurs O(n3 + n2d) complexity for training, O(n2) memory requirements, and O(nd) prediction time for a single test point \u2013 none of which are particularly appealing in \u201cbig data\u201d settings. Similar conclusions apply to other algorithms such as Kernel PCA.\nThis is rather unfortunate, since non-parametric models, such as the ones produced by kernel methods, are particularly appealing in a big data settings as they can adapt to the full complexity of the underlying domain, as uncovered by increasing dataset sizes. It is well-known that imposing strong structural constraints upfront for the purpose of allowing an efficient solution (in the above example: a linear hypothesis space) often limits, both theoretically and empirically, the potential to deliver value on large amounts of data. Thus, as big data becomes pervasive across a number of application domains, it has become necessary to be able to develop highly scalable algorithms for kernel methods.\nRecent years have seen intensive research on improving the scalability of kernel methods; we review some recent progress in the next section. In this paper, we revisit one of the most successful techniques, namely the randomized construction of a family of low-dimensional approximate feature maps proposed by Rahimi and Recht [2007]. These randomized feature maps, \u03a8\u0302 : X \u2192 Cs, provide low-distortion approximations for (complex-valued) kernel functions k : X \u00d7 X \u2192 C:\nk(x, z) \u2248 \u3008\u03a8\u0302(x), \u03a8\u0302(z)\u3009Cs (1)\nwhere Cs denotes the space of s-dimensional complex numbers with the inner product, \u3008\u03b1, \u03b2\u3009Cs = \u2211s i=1 \u03b1i\u03b2 \u2217 i , with z\u2217 denoting the conjugate of the complex number z (though Rahimi and Recht [2007] also define realvalued feature maps for real-valued kernels, our technical exposition is simplified by adopting the generality of complex-valued features). The mapping \u03a8\u0302(\u00b7) is now applied to each of the data points, to obtain a randomized feature representation of the data. We then apply a simple linear method to these random features. That is, if our data is {(xi, yi)}ni=1 we learn on {(zi, yi)}ni=1 where zi = \u03a8\u0302(xi). As long as s is sufficiently smaller than d, this leads to more scalable solutions, e.g., for regression we get back to O(ns2) training and O(sd) prediction time, with O(ns) memory requirements. This technique is immensely successful, and has been used in recent years to obtain state-of-the-art accuracies for some classical datasets [Huang et al., 2014, Sindhwani and Avron, 2014].\nThe starting point of Rahimi and Recht [2007] is a celebrated result that characterizes the class of positive definite functions:\nDefinition 1. A function g : Rd 7\u2192 C is a positive definite function if for any set of m points, x1 . . .xm \u2208 Rd, the m\u00d7m matrix A defined by Aij = g(xi \u2212 xj) is positive semi-definite.\nTheorem 2 (Bochner [1933]). A complex-valued function g : Rd 7\u2192 C is positive definite if and only if it is the Fourier Transform of a finite non-negative Borel measure \u00b5 on Rd, i.e.,\ng(x) = \u00b5\u0302(x) =\n\u222b\nRd\ne\u2212ix Twd\u00b5(w), \u2200x \u2208 Rd .\nWithout loss of generality, we assume henceforth that \u00b5(\u00b7) is a probability measure with associated probability density function p(\u00b7).\nA kernel function k : Rd \u00d7 Rd 7\u2192 C on Rd is called shift-invariant if k(x, z) = g(x \u2212 z), for some positive definite function g : Rd 7\u2192 C. Bochner\u2019s theorem implies that a scaled shift-invariant kernel can\ntherefore be put into one-to-one correspondence with a density p(\u00b7) such that,\nk(x, z) = g(x \u2212 z) = \u222b\nRd\ne\u2212i(x\u2212z) Twp(w)dw . (2)\nFor the most notable member of the shift-invariant family of kernels \u2013 the Gaussian kernel:\nk(x, z) = e\u2212 \u2016x\u2212z\u20162 2 2\u03c32 ,\nthe associated density is again Gaussian N (0, \u03c3\u22122Id). The integral representation of the kernel (2) may be approximated as follows:\nk(x, z) =\n\u222b\nRd\ne\u2212i(x\u2212z) T wp(w)dw\n\u2248 1 s\ns \u2211\nj=1\ne\u2212i(x\u2212z) T ws\n= \u3008\u03a8\u0302S(x), \u03a8\u0302S(z)\u3009Cs ,\nthrough the feature map,\n\u03a8\u0302S(x) = 1\u221a s [ e\u2212ix Tw1 . . . e\u2212ix Tws ] \u2208 Cs . (3)\nThe subscript S denotes dependence of the feature map on the sequence S = {w1, . . . ,ws}. The goal of this work is to improve the convergence behavior of this approximation, so that a smaller s can be used to get the same quality of approximation to the kernel function. This is motivated by recent work that showed that in order to obtain state-of-the-art accuracy on some important datasets, a very large number of random features is needed [Huang et al., 2014, Sindhwani and Avron, 2014].\nOur point of departure from the work of Rahimi and Recht [2007] is the simple observation that when w1, . . . ,ws are are drawn from the distribution defined by the density function p(\u00b7), the approximation in (3) may be viewed as a standard Monte Carlo (MC) approximation to the integral representation of the kernel. Instead of using plain MC approximation, we propose to use the low-discrepancy properties of Quasi-Monte Carlo (QMC) sequences to reduce the integration error in approximations of the form (3). A self-contained overview of Quasi-Monte Carlo techniques for high-dimensional integration problems is provided in Section 2.3. In Section 3, we describe how QMC techniques apply to our setting.\nWe then proceed to providing an average case theoretical analysis of the integration error for any given sequence S (Section 4). This bound motivates an optimization problem over the sequence S whose minimizer provides adaptive QMC sequences fine tuned to our kernels (Section 5).\nFinally, empirical results (Section 6) clearly demonstrate the superiority of QMC techniques over the MC feature maps [Rahimi and Recht, 2007], the correctness of our theoretical analysis and the potential value of adaptive QMC techniques for large-scale kernel methods."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Notation", "text": "We use i both for subscript and for denoting \u221a \u22121, relying on the context to distinguish between the two. We use y, z, . . . to denote scalars. We use w, t,x . . . to denote vectors, and use wi to denote the i-th coordinate of vectors w. Furthermore, in a sequence of vectors, we use wi to denote the i-th element of the sequence and use wij to denote the j-th coordinate of vector wi. Given x1, . . . ,xn, the Gram matrix is defined\nas K \u2208 Rn\u00d7n where Kij = k(xi,xj) for i, j = 1, . . . , n. We denote the error function by erf(\u00b7), i.e., erf(z) =\n\u222b z 0 e \u2212z2dz for z \u2208 C; see Weideman [1994] and Mori [1983] for more details. In \u201cMC sequence\u201d we mean points drawn randomly either from the unit cube or certain distribution that will be clear from the text. For \u201cQMC sequence\u201d we mean a deterministic sequence designed to reduce the integration error. Typically, it will be a low-discrepancy sequence on the unit cube.\nIt is also useful to recall the definition of Reproducing Kernel Hilbert Space (RKHS).\nDefinition 3 (Reproducing Kernel Hilbert Space). A reproducing kernel Hilbert space (RKHS) is a Hilbert Space H : X \u2192 C that possesses a reproducing kernel, i.e., a function h : X \u00d7 X \u2192 C for which the following hold for all x \u2208 X and f \u2208 H:\n\u2022 h(x, \u00b7) \u2208 H\n\u2022 \u3008f, h(x, \u00b7)\u3009H = f(x) (Reproducing Property)\nEquivalently, RKHSs are Hilbert spaces with bounded, continuous evaluation functionals. Informally, they are Hilbert spaces with the nice property that if two functions f, g \u2208 H are close in the sense of the distance derived from the norm in H (i.e., \u2016f \u2212 g\u2016H is small), then their values f(x), g(x) are also close for all x \u2208 X ; in other words, the norm controls the pointwise behavior of functions in H."}, {"heading": "2.2 Related Work", "text": "In this section we discuss related work on scalable kernel methods. Relevant work on QMC methods is discussed in the next subsection.\nScalability has long been identified as a key challenge associated with deploying kernel methods in practice. One dominant line of work constructs low-rank approximations of the Gram matrix, either using data-oblivious randomized feature maps to approximate the kernel function, or using sampling techniques such as the classical Nystro\u0308m method [Williams and Seeger, 2001]. In its vanilla version, the latter approach - Nystro\u0308m method - samples points from the dataset, computes the columns of the Gram matrix that corresponds to the sampled data points, and uses this partial computation of the Gram matrix to construct an approximation to the entire Gram matrix. More elaborate techniques exist, both randomized and deterministic; see Gittens and Mahoney [2013] for a thorough treatment.\nMore relevant to our work is the randomized feature mapping approach. Pioneered by the seminal paper of Rahimi and Recht [2007], the core idea is to construct, for a given kernel on a data domain X , a transformation \u03a8\u0302 : X \u2192 Cs such that k(x, z) \u2248 \u3008\u03a8\u0302(x), \u03a8\u0302(z)\u3009Cs . Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7).\nSubsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) =\n\u2211d i=1min(xi, zi). Vedaldi and Zisserman [2011] developed\nfeature maps for \u03b3-homogeneous kernels. Sreekanth et al. [2010] developed feature maps for generalized RBF kernels k(x, z) = g(D(x, z)2) where g(\u00b7) is a positive definite function, and D(\u00b7, \u00b7) is a distance metric. Kar and Karnick [2012] suggested feature maps for dot-product kernels. The feature maps is based on the Maclaurin expansion, which is guaranteed to be non-negative due to a classical result of Schoenberg [1942]. Pham and Pagh [2013] suggested feature maps for the polynomial kernels. Their construction leverages known techniques from sketching theory. It can also be shown that their feature map is an oblivious\nsubspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms.\nOur work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small. In contrast, our work tries to design w1, . . . ,ws so that less features will be necessary to get the same quality of kernel approximation.\nSeveral other scalable approaches for large-scale kernel methods have been suggested over the years, starting from approaches such as chunking and decomposition methods proposed in the early days of SVM optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale Gauss process regression. There are also approaches that are more specific to the objective function at hand, e.g., Keerthi et al. [2006] build a kernel expansion greedily to optimize the SVM objective function. Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated in terms of the minimum enclosing ball problem.\nFor a broader discussion of these methods, and others, see Bottou et al. [2007]."}, {"heading": "2.3 Quasi-Monte Carlo Techniques: an Overview", "text": "In this section we provide a self-contained overview of Quasi-Monte Carlo (QMC) techniques. For brevity, we restrict our discussion to background that is necessary for understanding subsequent sections. We refer the interested reader to the excellent reviews by Caflisch [1998] and Dick et al. [2013], and the recent book Leobacher and Pillichschammer [2014] for a much more detailed exposition.\nConsider the task of computing an approximation of the following integral\nId[f ] =\n\u222b\n[0,1]d f(x)dx . (4)\nOne can observe that if x is a random vector uniformly distributed over [0, 1]d then Id[f ] = E [f(x)]. An empirical approximation to the expected value can be computed by drawing a random point set S = {w1, . . . ,ws} independently from [0, 1]d, and computing:\nIS [f ] = 1\ns\n\u2211 w\u2208S f(w) .\nThis is the Monte Carlo (MC) method. Define the integration error with respect to the point set S as\n\u01ebS[f ] = |Id(f)\u2212 IS(f)| .\nWhen S is drawn randomly, the Central Limit Theorem asserts that if s = |S| is large enough then \u01ebS[f ] \u2248 \u03c3[f ]s\u22121/2\u03bd where \u03bd is a standard normal random variable, and \u03c3[f ] is the square-root of the variance of f ; that is,\n\u03c32[f ] =\n\u222b\n[0,1]d (f(x)\u2212 Id(f))2 dx .\nIn other words, the root mean square error of the Monte Carlo method is,\n(\nES\n[ \u01ebS [f ] 2 ])1/2 \u2248 \u03c3[f ]s\u22121/2. (5)\nTherefore, the Monte Carlo method converges at a rate of O(s\u22121/2). The aim of QMC methods is to improve the convergence rate by using a deterministic low-discrepancy sequence to construct S, instead of randomly sampling points. The underlying intuition is illustrated in Figure 1, where we plot a set of 1000 two-dimensional random points (left graph), and a set of 1000 twodimensional points from a quasi-random sequence (Halton sequence; right graph). In the random sequence we see that there is an undesired clustering of points, and as a consequence empty spaces. Clusters add little to the approximation of the integral in those regions, while in the empty spaces the integrand is not sampled. This lack of uniformity is due to the fact that Monte Carlo samples are independent of each other. By carefully designing a sequence of correlated points to avoid such clustering effects, QMC attempts to avoid this phenomena, and thus provide faster convergence to the integral.\nThe theoretical apparatus for designing such sequences are inequalities of the form\n\u01ebS(f) \u2264 D(S)V (f) ,\nin which V (f) is a measure of the variation or difficulty of integrating f(\u00b7) and D(S) is a sequencedependent term that typically measures the discrepancy, or degree of deviation from uniformity, of the sequence S. For example, the expected Monte Carlo integration error decouples into a variance term, and s\u22121/2 as in (5).\nA prototypical inequality of this sort is the following remarkable and classical result:\nTheorem 4 (Koksma-Hlawka inequality). For any function f with bounded variation, and sequence S = {w1, . . . ,ws}, the integration error is bounded above as follows,\n\u01ebS [f ] \u2264 D\u22c6(S)VHK [f ] ,\nwhere VHK is the Hardy-Krause variation of f (see Niederreiter [1992]), which is defined in terms of the following partial derivatives,\nVHK [f ] = \u2211\nI\u2282[d],I 6=\u2205\n\u222b\n[0,1]|I|\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2202f\n\u2202uI\n\u2223 \u2223 \u2223 \u2223\nuj=1,j /\u2208I\n\u2223 \u2223 \u2223 \u2223 \u2223 duI , (6)\nand D\u22c6 is the star discrepancy defined by\nD\u22c6(S) = sup x\u2208[0,1]d |disrS(x)| ,\nwhere disrS is the local discrepancy function\ndisrS(x) = Vol(Jx)\u2212 |{i : wi \u2208 Jx}|\ns\nwith Jx = [0, x1)\u00d7 [0, x2)\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [0, xd) with Vol(Jx) = \u220fd j=1 xj .\nGiven x, the second term in disrS(x) is an estimate of the volume of Jx, which will be accurate if the points in S are uniform enough. D\u22c6(S) measures the maximum difference between the actual volume of the subregion Jx and its estimate for all x in [0, 1]d.\nAn infinite sequence w1,w2, . . . is defined to be a low-discrepancy sequence if, as a function of s, D\u22c6({w1, . . . ,ws}) = O((log s)d/s). Several constructions are know to be low-discrepancy sequences. One notable example is the Halton sequences, which are defined as follows. Let p1, . . . , pd be the first d prime numbers. The Halton sequence w1,w2, . . . of dimension d is defined by\nwi = (\u03c6p1(i), . . . , \u03c6pd(i))\nwhere for integers i \u2265 0 and b \u2265 2 we have\n\u03c6b(i) = \u221e \u2211\na=1\niab \u2212a\nin which i0, i1, \u00b7 \u00b7 \u00b7 \u2208 {0, 1, . . . , b\u2212 1} is given by the unique decomposition\ni =\n\u221e \u2211\na=1\niab a\u22121 .\nIt is outside the scope of this paper to describe all these constructions in detail. However we mention that in addition to the Halton sequences, other notable members are Sobol\u2019 sequences, Faure sequences, Niederreiter sequences, and more (see Dick et al. [2013], Section 2). We also mention that it is conjectured that the O((log s)d/s) rate for star discrepancy decay is optimal.\nThe classical QMC theory, which is based on the Koksma-Hlawka inequality and low discrepancy sequences, thus achieves a convergence rate of O((log s)d/s). While this is asymptotically superior to O(s\u22121/2) for a fixed d, it requires s to be exponential in d for the improvement to manifest. As such, in the past QMC methods were dismissed as unsuitable for very high-dimensional integration.\nHowever, several authors noticed that QMC methods perform better than MC even for very highdimensional integration [Sloan and Wozniakowski, 1998, Dick et al., 2013]2. Contemporary QMC literature explains and expands on these empirical observations, by leveraging the structure of the space in which the integrand function lives, to derive more refined bounds and discrepancy measures, even when classical measures of variation such as (6) are unbounded. This literature has evolved along at least two directions: one, where worst-case analysis is provided under the assumption that the integrands live in a Reproducing Kernel Hilbert Space (RKHS) of sufficiently smooth and well-behaved functions (see Dick et al. [2013], Section 3) and second, where the analysis is done in terms of average-case error, under an assumed probability distribution over the integrands, instead of worst-case error [Wozniakowski, 1991, Traub and Wozniakowski, 1994]. We refrain from more details, as these are essentially the paths that the analysis in Section 4 follows for our specific setting.\n2Also see: \u201cOn the unreasonable effectiveness of QMC\u201d, I.H. Sloan https://mcqmc.mimuw.edu.pl/Presentations/sloan.pdf\nAlgorithm 1 Quasi-Random Fourier Features Input: Shift-invariant kernel k, size s. Output: Feature map \u03a8\u0302(x) : Rd 7\u2192 Cs.\n1: Find p, the inverse Fourier transform of k. 2: Generate a low discrepancy sequence t1, . . . , ts. 3: Transform the sequence: wi = \u03a6\u22121(ti) by (7). 4: Set \u03a8\u0302(x) = \u221a\n1 s\n[\ne\u2212ix Tw1 , . . . , e\u2212ix Tws\n]\n."}, {"heading": "3 QMC Feature Maps: Our Algorithm", "text": "We assume that the density function in (2) can be written as p(x) = \u220fd j=1 pj(xj), where pj(\u00b7) is a univariate density function. The density functions associated to many shift-invariant kernels, e.g., Gaussian, Laplacian and Cauchy, admits such a form.\nThe QMC method is generally applicable to integrals over a unit cube. So typically integrals of the form (2) are handled by first generating a low discrepancy sequence t1, . . . , ts \u2208 [0, 1]d, and transforming it into a sequence w1, . . . ,ws in Rd, instead of drawing the elements of the sequence from p(\u00b7) as in the MC method.\nTo convert (2) to an integral over the unit cube, a simple change of variables suffices. For t \u2208 Rd, define\n\u03a6\u22121(t) = ( \u03a6\u221211 (t1), . . . ,\u03a6 \u22121 d (td) ) \u2208 Rd , (7)\nwhere \u03a6j(\u00b7) is the cumulative distribution function (CDF) of pj(\u00b7), for j = 1, . . . , d. By setting w = \u03a6\u22121(t), then (2) can be equivalently written as\n\u222b\nRd\ne\u2212i(x\u2212z) Twp(w)dw =\n\u222b\n[0,1]d e\u2212i(x\u2212z) T\u03a6\u22121(t)dt .\nThus, a low discrepancy sequence t1, . . . , ts \u2208 [0, 1]d can be transformed using wi = \u03a6\u22121(ti), which is then plugged into (3) to yield the QMC feature map. This simple procedure is summarized in Algorithm 1. QMC feature maps are analyzed in the next section."}, {"heading": "4 Theoretical Analysis and Average Case Error Bounds", "text": "The proofs for assertions made in this section and the next can be found in the Appendix. The goal of this section is to develop a framework for analyzing the approximation quality of the QMC feature maps described in the previous section (Algorithm 1). We need to develop such a framework since the classical Koksma-Hlawka inequality cannot be applied to our setting, as the following proposition shows:\nProposition 5. For any p(x) = \u220fd j=1 pj(xj), where pj(\u00b7) is a univariate density function, let\n\u03a6\u22121(t) = ( \u03a6\u221211 (t1), . . . ,\u03a6 \u22121 d (td) ) .\nFor a fixed u \u2208 Rd, consider fu(t) = e\u2212iu T\u03a6\u22121(t), t \u2208 [0, 1]d. The Hardy-Krause variation of fu(\u00b7) is unbounded. That is, one of the integrals in the sum (6) is unbounded.\nOur framework is based on a new discrepancy measure, box discrepancy, that characterizes integration error for the set of integrals defined with respect to the underlying data domain. Throughout this section we use the convention that S = {w1, . . . ,ws}, and the notation X\u0304 = {x\u2212 z | x, z \u2208 X}.\nGiven a probability density function p(\u00b7) and S, we define the integration error \u01ebS,p[f ] of a function f(\u00b7) with respect to p(\u00b7) and the s samples as,\n\u01ebS,p[f ] =\n\u2223 \u2223 \u2223 \u2223 \u2223 \u222b\nRd\nf(x)p(x)dx \u2212 1 s\ns \u2211\ni=1\nf(wi)\n\u2223 \u2223 \u2223 \u2223 \u2223 .\nWe are interested in characterizing the behavior of \u01ebS,p[f ] on f \u2208 FX where\nFX\u0304 = { fu(x) = e \u2212iuTx, u \u2208 X\u0304 } .\nAs is common in modern QMC analysis [Leobacher and Pillichschammer, 2014, Dick et al., 2013], our analysis is based on setting up a Reproducing Kernel Hilbert Space of \u201cnice\u201d functions that is related to integrands that we are interested in, and using properties of the RKHS to derive bounds on the integration error. In particular, the integration error of integrands in a RKHS can be bounded using the following proposition.\nProposition 6 (Integration Error in a RKHS). Let H be a RKHS with kernel h(\u00b7, \u00b7). Assume that \u03ba = supx\u2208Rd h(x,x) < \u221e. Then, for all f \u2208 H we have,\n\u01ebS,p[f ] \u2264 \u2016f\u2016HDh,p(S) , (8)\nwhere\nDh,p(S) 2 =\n\u2225 \u2225 \u2225 \u2225 \u2225 \u222b\nRd\nh(\u03c9, \u00b7)p(\u03c9)d\u03c9 \u2212 1 s\ns \u2211\nl=1\nh(wl, \u00b7) \u2225 \u2225 \u2225 \u2225\n\u2225\n2\nH (9)\n=\n\u222b\nRd\n\u222b\nRd\nh(\u03c9, \u03c6)p(\u03c9)p(\u03c6)d\u03c9d\u03c6 \u2212 2 s\ns \u2211\nl=1\n\u222b\nRd\nh(wl, \u03c9)p(\u03c9)d\u03c9 + 1\ns2\ns \u2211\nl=1\ns \u2211\nj=1\nh(wl,wj) .\nRemark 7. In the theory of RKHS embeddings of probability distributions [Smola et al., 2007, Sriperumbudur et al., 2010], the function\n\u00b5h,p(x) =\n\u222b\nRd\nh(\u03c9,x)p(\u03c9)d\u03c9\nis known as the kernel mean embedding of p(\u00b7). The function\n\u00b5\u0302h,p,S(x) = 1\ns\ns \u2211\nl=1\nh(wl,x)\nis then the empirical mean map.\nThe RKHS we use is as follows. For a vector b \u2208 Rd, let us define b = {u \u2208 Rd | |uj | \u2264 bj}. Let\nF b = { fu(x) = e \u2212iuTx, u \u2208 b } ,\nand consider the space of functions that admit an integral representation over F b of the form\nf(x) =\n\u222b\nu\u2208 b f\u0302(u)e\u2212iu T xdu where f\u0302(u) \u2208 L2( b) . (10)\nThis space is associated with bandlimited functions, i.e., functions with compactly-supported inverse Fourier transforms, which are of fundamental importance in the Shannon-Nyquist sampling theory. Under a natural choice of inner product, these spaces are called Paley-Wiener spaces and they constitute a RKHS.\nProposition 8 (Kernel of Paley-Wiener RKHS [Berlinet and Thomas-Agnan, 2004, Yao, 1967, Peloso, 2011]). By PWb, denote the space of functions which admit the representation in (10), with the inner product \u3008f, g\u3009PWb = (2\u03c0)2d\u3008f\u0302 , g\u0302\u3009L2( b). PWb is a RKHS with kernel function,\nsincb(u,v) = \u03c0 \u2212d\nd \u220f\nj=1\nsin (bj(uj \u2212 vj)) uj \u2212 vj .\nFor notational convenience, in the above we define sin(b \u00b7 0)/0 to be b. Furthermore, \u3008f, g\u3009PWb = \u3008f, g\u3009L2( b).\nFor the kernel function described above, the discrepancy measure Dh,S in Proposition 6 can be written explicitly.\nTheorem 9 (Discrepancy in PWb). Suppose that p(\u00b7) is a probability density function, and that we can write p(x) =\n\u220fd j=1 pj(xj) where each pj(\u00b7) is a univariate probability density function as well. Let \u03d5j(\u00b7)\nbe the characteristic function associated with pj(\u00b7). Then,\nDsincb,p(S) 2 = \u03c0\u2212d\nd \u220f\nj=1\n\u222b bj\n\u2212bj |\u03d5j(\u03b2)|2d\u03b2 \u2212\n2(2\u03c0)\u2212d\ns\ns \u2211\nl=1\nd \u220f\nj=1\n\u222b bj\n\u2212bj \u03d5j(\u03b2)e\niwlj\u03b2d\u03b2 +\n1\ns2\ns \u2211\nl=1\ns \u2211\nj=1\nsincb(wl,wj) . (11)\nThis naturally leads to the definition of the box discrepancy, analogous to the star discrepancy described in Theorem 4.\nDefinition 10 (Box Discrepancy). The box discrepancy of a sequence S with respect to p(\u00b7) is defined as,\nD bp (S) = Dsincb,p(S) .\nFor notational convenience, we generally omit the b from D bp (S) as long as it is clear from the context. The worse-case integration error bound for Paley-Wiener spaces is stated in the following as a corollary of Proposition 6.\nCorollary 11 (Integration Error in PWb). For f \u2208 PWb we have\n\u01ebS,p[f ] \u2264 \u2016f\u2016PWbD p (S) .\nNow, if bj = supu\u2208X\u0304 |uj | then X\u0304 \u2282 b, so FX\u0304 \u2282 F b. Since we wish to bound the integration error on functions in FX\u0304 , it suffices to bound the integration error on F b. Unfortunately, while F b defines PWb, the functions in it, being not square integrable, are not members of PWb, so the above results do not directly apply to them. However, their damped approximations of the form f\u0303u(x) = e\u2212iu\nTx sinc(Tx) are members of PWb with \u2016f\u0303\u2016PWb = 1\u221aT . Hence, we expect D p to provide a discrepancy measure for integrating functions in F b. More formally, the expected square error of an integrand drawn from a uniform distribution over F b is proportional to the square discrepancy measure D p (S). This result is in the spirit of similar average case analysis in the QMC literature [Wozniakowski, 1991, Traub and Wozniakowski, 1994].\nTheorem 12 (Average Case Error). Let U(F b) denote the uniform distribution on F b. That is, f \u223c U(F b) denotes f = fu where fu(x) = e\u2212iu\nTx and u is randomly drawn from a uniform distribution on b. We have,\nEf\u223cU(F b) [ \u01ebS,p[f ] 2 ]\n= \u03c0d\n\u220fd j=1 bj\nD p (S) 2 .\nWe now give an explicit formula for D p (S) for the case that p(\u00b7) is the density function of the multivariate Gaussian distribution with zero mean and independent components. This is an important special case since this is the density function that is relevant for the Gaussian kernel.\nCorollary 13 (Discrepancy for Gaussian Distribution). Let p(\u00b7) be the d-dimensional multivariate Gaussian density function with zero mean and covariance matrix equal to diag(\u03c3\u221221 , . . . , \u03c3 \u22122 d ). We have,\nD p (S) 2 =\n1\ns2\ns \u2211\nl=1\ns \u2211\nj=1\nsincb(wl,wj)\u2212\n2\ns\ns \u2211\nl=1\nd \u220f\nj=1\nclj Re\n(\nerf\n(\nbj \u03c3j \u221a 2 \u2212 i\u03c3jwlj\u221a 2\n))\n+\nd \u220f\nj=1\n\u03c3j 2 \u221a \u03c0 erf\n(\nbj \u03c3j\n)\n, (12)\nwhere\nclj =\n(\n\u03c3j\u221a 2\u03c0\n)\ne\u2212 \u03c32j w 2 lj 2 .\nIntuitively, the box discrepancy of the Gaussian kernel can be interpreted as follows. The function sinc(x) = sin(x)/x achieves its maximum at x = 0 and minimizes at discrete values of x decaying to 0 as |x| goes to \u221e. Hence the first term in (12) tends to be minimized when the pairwise distance between wj are sufficiently separated. Also, the shape of the cumulative distribution function of Gaussian distribution requires tj to be close to the boundary of the unit cube. As for second term, the original expression is -2s \u2211s l=1 \u222b Rd h(wl, \u03c9)p(\u03c9)d\u03c9. This term encourages the sequence wl to mimic samples from p(\u03c9). Since p(\u03c9) concentrates its mass around \u03c9 = 0, the wj also concentrates around 0 to maximize the integral and therefore tj is driven closer to the center of the unit cube. Sequences with low box discrepancy therefore optimize a tradeoff between these competing terms."}, {"heading": "Discrepancy of Monte-Carlo Sequences.", "text": "We now derive an expression for the expected discrepancy of Monte-Carlo sequences, and show that it decays as O(s\u22121/2). This is useful since via an averaging argument we are guaranteed that there exists sets for which the discrepancy behaves O(s\u22121/2).\nCorollary 14. Suppose t1, . . . , ts are chosen uniformly from [0, 1]d. Let wi = \u03a6\u22121(ti), for i = 1, . . . , s. Assume that \u03ba = supx\u2208Rd h(x,x) < \u221e. Then\nE [ Dh,p(S) 2 ] = 1\ns\n\u222b\nRd\nh(\u03c9, \u03c9)p(\u03c9)d\u03c9 \u2212 1 s\n\u222b\nRd\n\u222b\nRd\nh(\u03c9, \u03c6)p(\u03c9)p(\u03c6)d\u03c9d\u03c6 .\nAgain, we can derive specific formulas for the Gaussian density. The following is straightforward from Corollary 14. We omit the proof.\nCorollary 15. Let p(\u00b7) be the d-dimensional multivariate Gaussian density function with zero mean and covariance matrix equal to diag(\u03c3\u221221 , . . . , \u03c3 \u22122 d ). Suppose t1, . . . , ts are chosen uniformly from [0, 1]\nd. Let wi = \u03a6 \u22121(ti), for i = 1, . . . , s. Then,\nE [ D p (S) 2 ]\n= 1\ns\n \u03c0\u2212d d \u220f\nj=1\nbj \u2212 d \u220f\nj=1\n\u03c3j 2 \u221a \u03c0 erf\n(\nbj \u03c3j\n)\n\n . (13)"}, {"heading": "5 Learning Adaptive QMC Sequences", "text": "For simplicity, in this section we assume that p(\u00b7) is the density function of Gaussian distribution with zero mean. We also omit the subscript p from D p . Similar analysis and equations can be derived for other density functions.\nError characterization via discrepancy measures like (12) is typically used in the QMC literature to prescribe sequences whose discrepancy behaves favorably. It is clear that for the box discrepancy, a meticulous design is needed for a high quality sequence and we leave this to future work. Instead, in this work, we use the fact that unlike the star discrepancy (4), the box discrepancy is a smooth function of the sequence with a closed-form formula. This allows us to both evaluate various candidate sequences, and select the one with the lowest discrepancy, as well as to adaptively learn a QMC sequence that is specialized for our problem setting via numerical optimization. The basis is the following proposition, which gives an expression for the gradient of D (S).\nProposition 16 (Gradient of Box Discrepancy). Define the following scalar functions and variables,\nsinc\u2032(z) = cos(z) z \u2212 sin(z) z2 , sinc\u2032b(z) = b \u03c0 sinc\u2032(bz) ;\ncj =\n(\n\u03c3j\u221a 2\u03c0\n)\n, j = 1, . . . , d ;\ngj(x) = cje \u2212\n\u03c32j\n2 x2 Re\n(\nerf\n[\nbj \u03c3j \u221a 2 \u2212 i\u03c3jx\u221a 2\n])\n;\ng\u2032j(x) = \u2212\u03c32jxgj(x) + \u221a 2\n\u03c0 cj\u03c3je\n\u2212 b2j\n2\u03c32 j sin(bjx) .\nIn the above ,we define sinc\u2032(0) to be 0. Then, the elements of the gradient vector of D are given by,\n\u2202D \u2202wlj = 2 s2\ns \u2211\nm=1 m6=l\n\nbj sinc \u2032 bj (wlj , wmj)\n\u220f q 6=j sincbq (wlq, wmq)\n\n\u2212\n2 s g\u2032j(wlj)\n\n\n\u220f q 6=j gq(wlq)\n\n . (14)\nWe explore two possible approaches for finding sequences based on optimizing the box discrepancy, namely global optimization and greedy optimization. The latter is closely connected to \u201cherding\u201d algorithms [Welling, 2009]."}, {"heading": "Global Adaptive Sequences.", "text": "The task is posed in terms minimization of the box discrepancy function (12) over the space of sequences of s vectors in Rd: S\u2217 = argminS=(w1...ws)\u2208Rds D (S) .\nThe gradient can be plugged into any first order numerical solver for non-convex optimization. We use non-linear conjugate gradient in our experiments (Section 6.2).\nThe above learning mechanism can be extended in various directions. For example, QMC sequences for n-point rank-one Lattice Rules [Dick et al., 2013] are integral fractions of a lattice defined by a single generating vector v. This generating vector may be learnt via local minimization of the box discrepancy."}, {"heading": "Greedy Adaptive Sequences.", "text": "Starting with S0 = \u2205, for t \u2265 1, let St = {w1, . . . ,wt}. At step t+1, we solve the following optimization problem, wt+1 = argminw\u2208Rd D\n(St \u222a {w}) . (15) Set St+1 = St \u222a {wt+1} and repeat the above procedure. The gradient of the above objective is also given in (14). Again, we use non-linear conjugate gradient in our experiments (Section 6.2).\nThe greedy adaptive procedure is closely related to the herding algorithm, recently presented by Welling [2009]. Applying the herding algorithm to PWb and p(\u00b7), and using our notation, the points w1,w2, . . . are generated using the following iteration\nwt+1 \u2208 arg max w\u2208Rd \u3008zt(\u00b7), h(w, \u00b7)\u3009PWb zt+1(x) \u2261 zt(x) + \u00b5h,p(x)\u2212 h(w,x) .\nIn the above, z0, z1, . . . is a series of functions in PWb. The literature is not specific on the initial value of z0, with both z0 = 0 and z0 = \u00b5h,p suggested. Either way, it is always the case that zt = z0 + t(\u00b5h,p \u2212 \u00b5\u0302h,p,St) where St = w1, . . . ,wt.\nChen et al. [2010] showed that under some additional assumptions, the herding algorithm, when applied to a RKHS H, greedily minimizes \u2016\u00b5h,p \u2212 \u00b5\u0302h,p,St\u20162H, which, recall, is equal to Dh,p(St). Thus, under certain assumptions, herding and (15) are equivalent. Chen et al. [2010] also showed that under certain restrictions on the RKHS, herding will reduce the discrepancy in a ratio of O(1/t). However, it is unclear whether those restrictions hold for PWb and p(\u00b7). Indeed, Bach et al. [2012] recently shown that these restrictions never hold for infinite-dimensional RKHS, as long as the domain is compact. This result does not immediately apply to our case since Rd is not compact."}, {"heading": "6 Experiments", "text": "In this section we report experiments with both classical QMC sequences and adaptive sequences learnt from box discrepancy minimization."}, {"heading": "6.1 Experiments With Classical QMC Sequences", "text": "We examine the behavior of classical low-discrepancy sequences when compared to random Fourier features (i.e., MC). We consider four sequences: Halton, Sobol\u2019, Lattice Rules, and Digital Nets. For Halton and Sobol\u2019, we use the implementation available in MATLAB3. For Lattice Rules and Digital Nets, we use\n3http://www.mathworks.com/help/stats/quasi-random-numbers.html\npublicly available implementations4 . For the low-discrepancy sequence, we use scrambling and shifting techniques recommended in the QMC literature (see Dick et al. [2013] for details). For Sobol\u2019, Lattice Rules and Digital Nets, scrambling introduces randomization and hence variance. For Halton sequence, scrambling is deterministic, and there is no variance. The generation of these sequences is extremely fast, and quite negligible when compared to the time for any reasonable downstream use. For example, for census dataset with size 18,000 by 119, if we choose the number of random features s = 2000, the running time for performing kernel ridge regression model is more than 2 minutes, while the time of generating the QMC sequences is only around 0.2 seconds (Digital Nets sequence takes longer, but not much longer) and that of MC sequence is around 0.01 seconds. Therefore, we do not report running times as these are essentially the same across methods.\nIn all experiments, we work with a Gaussian kernel. For learning, we use regularized least square classification on the feature mapped dataset, which can be thought of as a form of approximate kernel ridge regression. For each dataset, the bandwidth \u03c3 is set by using 5-fold cross-validation in favor of the Monte Carlo approach."}, {"heading": "Quality of Kernel Approximation", "text": "In our setting, the most natural and fundamental metric for comparison is the quality of approximation of the Gram matrix. We examine how close K\u0303 (defined by K\u0303ij = k\u0303(xi,xj) where k\u0303(\u00b7, \u00b7) = \u3008\u03a8\u0302S(\u00b7), \u03a8\u0302S(\u00b7)\u3009 is the kernel approximation) is to the Gram matrix K of the exact kernel.\nWe examine four datasets: cpu (6554 examples, 21 dimensions), census (a subset chosen randomly with 5,000 examples, 119 dimensions), USPST (1,506 examples, 250 dimensions after PCA) and mnist (a subset chosen randomly with 5,000 examples, 250 dimensions after PCA). The reason we do subsampling on large datasets is to be able to compute the full exact Gram matrix for comparison purposes. The reason we use dimensionality reduction on mnist is that the maximum dimension supported by the Lattice Rules implementation we use is 250. To measure the quality of approximation we use both \u2016K\u2212 K\u0303\u20162/\u2016K\u20162 and \u2016K\u2212 K\u0303\u2016F /\u2016K\u2016F . The plots are shown in Figure 2.\nWe can clearly see that classical low-discrepancy sequences consistently produce better approximations to the Gram matrix. Among the four classical QMC sequences, the Digital Nets, Lattice Rules and Halton sequences yield much lower error. Similar results were observed for other datasets (not reported here). Although using randomized variants of QMC sequences may incur some variance, the variance is quite small compared to that of the MC random features.\nScrambled (whether deterministic or randomized) QMC sequences almost uniformly yield higher accuracies than non-randomized QMC sequences. As an example, we show the quality of Gram matrix approximation by using randomized and non-randomized QMC sequences on USPST and cpu (a subset of size 2000) in Figure 3.\nDoes better Gram matrix approximation translate to lower generalization errors? We consider two regression datasets, cpu and census, and use (approximate) kernel ridge regression to build a regression model. The ridge parameter is set by the optimal value we obtain via 5-fold crossvalidation on the training set by using the MC sequence. Table 1 summarizes the results.\nAs we see, for cpu, all the sequences behave similarly, with the Halton sequence yielding the lowest test error. For census, the advantage of using Halton sequence is significant (almost 20% reduction in generalization error) followed by Digital Nets and Sobol\u2019. In addition, MC sequence tends to generate higher variance across all the sampling size. Overall, QMC sequences, especially Halton, outperform MC sequences on these datasets.\n4http://people.cs.kuleuven.be/ dirk.nuyens/qmc-generators/\nWhen performed on classification datasets by using the same learning model, with a moderate range of s, e.g., less than 2000, the QMC sequences do not yield accuracy improvements over the MC sequence with the same consistency as in the regression case. The connection between kernel approximation and the performance in downstream applications is outside the scope of the current paper. Worth mentioning in this regard, is the recent work by Bach [2013], which analyses the connection between Nystro\u0308m approximations of the Gram matrix, and the regression error, and the work of El Alaoui and Mahoney [2014] on kernel methods with statistical guarantees."}, {"heading": "Behavior of Box Discrepancy", "text": "Next, we examine if D is predictive of the quality of approximation. We compute the normalized square box discrepancy values (i.e., \u03c0d( \u220fd\nj=1 bj) \u22121D (S)2) for the different sequences with different sample sizes\ns. Note that while the bounding box b is set based on observed ranges of feature values in the dataset, the actual distribution of points X\u0304 encountered inside that box might be far from uniform. In the graphs, the expected normalized square box discrepancy values for MC are computed using (13).\nIn Figure 4, as we can see, for cpu if we limit the scope of s to [0, 800], which is also the range of s for\ncpu in Figure 2, we can see a strong correlation between the quality of approximation and the discrepancy values. Interestingly, Lattice Rules sequences start with low discrepancy, but it does not decrease with increasing s. For census, using the original bounding box yielded very little difference between sequences (graph not shown). Instead, we plot the discrepancy when measured on the central part of the bounding box (i.e., b/2 instead of b), which is equal to the integration error averaged over that part of the bounding box. Presumably, points from X\u0304 concentrate in that region, and they may be more relevant for downstream predictive task. Again, we see strong correlation between the quality of approximation and the discrepancy values."}, {"heading": "6.2 Experiments With Adaptive QMC", "text": "The goal of this subsection is to provide a proof-of-concept for learning adaptive QMC sequences, using the two adaptive QMC schemes described in Section 5. We demonstrate that QMC sequences can be improved to produce better approximation to the Gram matrix, and that can sometimes lead to improved generalization error.\nNote that the running time of learning the adaptive sequences is less relevant in our experimental setting for the following reasons. Given the values of s, d, b and \u03c3 the optimization of a sequence needs only to be done once. There is some flexibility in these parameters: d can be adjusted by adding zero features or by doing PCA on the input; one can use longer or shorter sequences; and the data can be forced to a fit a particular bounding box using (possibly non-equal) scaling of the features (this, in turn, affects the choice of the \u03c3) . Since designing adaptive QMC sequences is data-independent with applicability to a variety of downstream applications of kernel methods, it is quite conceivable to generate many point sets in advance and to use them for many learning tasks. Furtheremore, the total size of the sequences (s\u00d7d) is independent of the number of examples n, which is the dominant term in large scale learning settings.\nWe name the two sequences as Global Adaptive and Greedy Adaptive, respectively. For Global Adaptive, the Halton sequence is used as the initial setting of the optimization variables S. For Greedy Adaptive, when optimizing for wt, the t-th point in the Halton sequence is used as the initial point. In both cases, we use non-linear conjugate gradient to perform numerical optimization."}, {"heading": "Integral Approximation", "text": "We begin by examining the integration error over the unit square by using three different sequences, namely, MC, Halton and adaptive QMC sequences. The integral is of the form \u222b\n[0,1]2 e \u2212iuT tdt where u\nspans the unit square. The error is plotted in Figure 5. We see that MC sequences concentrate most of the error reduction near the origin. The Halton sequence gives significant improvement expanding the region of low integration error. Adaptive QMC sequences give another order of magnitude improvement in integration error which is now diffused over the entire unit square; the estimation of such sequences is \u201caware\u201d of the full integration region. In fact, by controlling the box size (see plot labeled b/2), adaptive sequences can be made to focus in a specified sub-box which can help with generalization if the actual data distribution is better represented by this sub-box."}, {"heading": "Quality of Kernel Approximation", "text": "In Figure 6 and Figure 7 we examine how various metrics (discrepancy, maximum squared error, mean squared error, norm of the error) on the Gram matrix approximation evolve during the optimization process for both adaptive sequences. Since learning the adaptive sequences on dataset with low dimensional features is more affordable, the experiment is performed on two such datasets, namely, cpu and housing.\nFor Global Adaptive, we fixed s = 100 and examine how the performance evolves as the number of iterations grows. In Figure 6 (a) we examine the behavior on cpu. We see that all metrics go down as the iteration progresses. This supports our hypothesis that by optimizing the box discrepancy we can improve the approximation of the Gram matrix. Figure 6 (b), which examines the same metrics on the scaled version of the housing dataset, has some interesting behaviors. Initially all metrics go down, but eventually all the metrics except the box-discrepancy start to go up; the box-discrepancy continues to go down. One plausible explanation is that the integrands are not uniformly distributed in the bounding box, and that by optimizing the expectation over the entire box we start to overfit it, thereby increasing the error in those regions of the\nbox where integrands actually concentrate. One possible way to handle this is to optimize closer to the center of the box (e.g., on b/2), under the assumption that integrands concentrate there. In Figure 6 (c) we try this on the housing dataset. We see that now the mean error and the norm error are much improved, which supports the interpretation above. But the maximum error eventually goes up. This is quite reasonable as the outer parts of the bounding box are harder to approximate, so the maximum error is likely to originate from there. Subsequently, we stop the adaptive learning of the QMC sequences early, to avoid the actual error from going up due to averaging.\nFor Greedy Adaptive, we examine its behavior as the number of points increases. In Figure 7 (a) and (b), as expected, as the number of points in the sequence increases, the box discrepancy goes down. This is also translated to non-monotonic decrease in the other metrics of Gram matrix approximation. However, unlike the global case, we see in Figure 7 (c), when the points are generated by optimizing on a smaller box b/2, the resulting metrics become higher for a fixed number of points. One plausible explanation can be the following. Although the Greedy Adaptive sequence inherits a fast convergence rate, potentially it might need a large number of points to achieve certain low magnitude of discrepancy. Hence, as shown in the plots, when the number of points is below 500, the quality of the optimization is not good enough to provide a good approximation the Gram matrix. For example, one can check when the number of points is 100, the discrepancy value of the Greedy Adaptive sequence is higher than that of the Global Adaptive sequence with more than 10 iterations."}, {"heading": "Generalization Error", "text": "We use the same two learning algorithms for learning adaptive sequences as the previous subsection, and use them for doing approximate kernel ridge regression. The ridge parameter is set by the value which is near-optimal for both sequences in 5-fold cross-validation on the training set. Table 2 summarizes the results.\nFor cpu, adaptive sequences sequences can yield lower test error when the sampling size is small (since the test error is already low, around 3%, such improvement in accuracy is not trivial). Greedy approach seems to give slightly better results. When s = 500 or even larger (not reported here), the performance of the sequences are very close. For census, the adaptive sequences do not show any benefit until s is 1200. Afterwards we can see at least one of the two adaptive sequences can yield much lower error than Halton sequence for each sampling size. However, in some cases, adaptive sequences sometimes produce errors that are bigger than the unoptimized sequences.\nIn most cases, the adaptive sequence on the central part of the bounding box outperforms the adaptive sequence on the entire box. This is likely due to the non-uniformity phenomena discussed before."}, {"heading": "7 Conclusion and Future Work", "text": "Recent work on applying kernel methods to very large datasets, has shown their ability to achieve stateof-the-art accuracies that sometimes match those attained by Deep Neural Networks (DNN) [Huang et al., 2014]. Key to these results is the ability to apply kernel method to such datasets. The random features approach, originally due to Rahimi and Recht [2007], as emerged as a key technology for scaling up kernel methods [Sindhwani and Avron, 2014].\nClose examination of those empirical results reveals that to achieve state-of-the-art accuracies, a very large number of random features was needed. For example, on TIMIT, a classical speech recognition dataset, over 200,000 random features were used in order to match DNN performance [Huang et al., 2014]. It is clear that improving the efficiency of random features can have a significant impact on our ability to scale up kernel methods, and potentially get even higher accuracies.\nThis paper is the first to exploit high-dimensional approximate integration techniques from the QMC literature in this context, with promising empirical results backed by rigorous theoretical analyses. Avenues for future work include incorporating stronger data-dependence in the estimation of adaptive sequences and analyzing how resulting Gram matrix approximations translate into downstream performance improvements for a variety of large-scale learning tasks."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Josef Dick for useful pointers to literature about improvement of the QMC sequences; Ha Quang Minh for several discussions on Paley-Wiener spaces and RKHS theory; the anonymous ICML reviewers for pointing out the connection to herding and other helpful comments. This research was supported by the XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA8750-12-C-0323. This work was done while J. Yang was a summer intern at IBM Research."}, {"heading": "A Technical Details", "text": "In this section we give detailed proofs of the assertions made in Section 4 and 5."}, {"heading": "A.1 Proof of Proposition 5", "text": "Recall, for any t \u2208 Rd, for \u03a6\u22121(t), we mean ( \u03a6\u221211 (t1), . . . ,\u03a6 \u22121 d (td) ) \u2208 Rd, where \u03a6j(\u00b7) is the CDF of pj(\u00b7).\nFrom fu(t) = e\u2212iu T\u03a6\u22121(t), for any j = 1, . . . , d, we have\n\u2202f(t) \u2202tj = (\u2212i) uj pj(\u03a6 \u22121 j (tj)) e\u2212iu T\u03a6\u22121j (t) .\nThus,\n\u2202df(t)\n\u2202t1 \u00b7 \u00b7 \u00b7 \u2202td =\nd \u220f\nj=1\n(\n(\u2212i) uj pj(\u03a6 \u22121 j (tj))\n)\ne\u2212iu T\u03a6\u22121j (t) .\nIn (6), when I = [d],\n\u222b\n[0,1]|I|\n\u2223 \u2223 \u2223 \u2223 \u2202f\n\u2202uI\n\u2223 \u2223 \u2223 \u2223 dtI = \u222b\n[0,1]d\n\u2223 \u2223 \u2223 \u2223 \u2202df(t)\n\u2202t1 \u00b7 \u00b7 \u00b7 \u2202td\n\u2223 \u2223 \u2223 \u2223 dt1 \u00b7 \u00b7 \u00b7 dtd\n=\n\u222b\n[0,1]d\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 d \u220f\nj=1\n(\n(\u2212i) uj pj(\u03a6 \u22121 j (tj))\n)\ne\u2212iu T\u03a6\u22121j (t)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 dt1 \u00b7 \u00b7 \u00b7 dtd\n=\n\u222b\n[0,1]d\nd \u220f\nj=1\n\u2223 \u2223 \u2223 \u2223 \u2223\nuj\npj(\u03a6 \u22121 j (tj))\n\u2223 \u2223 \u2223 \u2223 \u2223 dt1 \u00b7 \u00b7 \u00b7 dtd\n= d \u220f\nj=1\n(\n\u222b\n[0,1]\n\u2223 \u2223 \u2223 \u2223 \u2223\nuj\npj(\u03a6 \u22121 j (tj))\n\u2223 \u2223 \u2223 \u2223 \u2223 dtj ) . (16)\nWith a change of variable, \u03a6j(tj) = vj , for j = 1, . . . , d, (16) becomes\nd \u220f\nj=1\n(\n\u222b\n[0,1]\n\u2223 \u2223 \u2223 \u2223 \u2223 (\nuj\npj(\u03a6 \u22121 j (tj))\n)\u2223\n\u2223 \u2223 \u2223 \u2223 dtj\n)\n= d \u220f\nj=1\n(\u222b\nR\n|uj |dvj ) = \u221e .\nAs this is a term in (6), we know that VHK [fu(t)] is unbounded."}, {"heading": "A.2 Proof of Proposition 6", "text": "We need the following lemmas, across which we share some notation.\nLemma 17. Assuming that \u03ba = supx\u2208Rd h(x,x) < \u221e, if f \u2208 H, where H is an RKHS with kernel h(\u00b7, \u00b7), the integral \u222b\nRd f(x)p(x)dx is finite.\nProof. For notational convenience, we note that \u222b\nRd\nf(x)p(x)dx = E [f(X)] ,\nwhere E [\u00b7] denotes expectation and X is a random variable distributed according to the probability density p(\u00b7) on Rd.\nNow consider a linear functional T that maps f to E [f(X)], i.e.,\nT [f ] = E [f(X)] . (17)\nThe linear functional T is a bounded linear functional on the RKHS H. To see this: |E [f(X)] | \u2264 E [|f(X)|] (Jensen\u2019s Inequality)\n\u2264 E [|\u3008f, h(X, \u00b7)\u3009H|] (Reproducing Property) \u2264 \u2016f\u2016HE [\u2016h(X, \u00b7)\u2016H] (Cauchy-Schwartz) \u2264 \u2016f\u2016HE [ \u221a h(X,X) ] = \u221a \u03ba < \u221e .\nThis shows that the integral \u222b\nRd f(x)p(x)dx exists.\nLemma 18. The mean \u00b5h,p(u) = \u222b Rd h(u,x)p(x)dx is in H. In addition, for any f \u2208 H,\nE [f(X)] =\n\u222b\nRd\nf(x)p(x)dx = \u3008f, \u00b5h,p\u3009H . (18)\nProof. From the Riesz Representation Theorem, every bounded linear functional on H admits an inner product representation. Therefore, for T defined in (17), there exists \u00b5h,p \u2208 H such that,\nT [f ] = E [f(X)] = \u3008f, \u00b5h,p\u3009H . Therefore we have, \u3008f, \u00b5h,p\u3009H = \u222b\nf(x)p(x)dx for all f \u2208 H. For any z, choosing f(\u00b7) = h(z, \u00b7), where h(\u00b7, \u00b7) is the kernel associated with H, and invoking the reproducing property we see that,\n\u00b5h,p(z) = \u3008h(z, \u00b7), \u00b5h,p\u3009H = \u222b\nRd\nh(z,x)p(x)dx .\nThe proof of Proposition 6 follows from the existence Lemmas above, and the following steps.\n\u01ebS,p[f ] =\n\u2223 \u2223 \u2223 \u2223 \u2223 \u222b\nRd\nf(x)p(x)dx\u2212 1 s\ns \u2211\nl=1\nf(wl)\n\u2223 \u2223 \u2223 \u2223 \u2223\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 \u3008f, \u00b5h,p\u3009H \u2212 1 s s \u2211\nl=1\n\u3008f, h(wl, \u00b7)\u3009H \u2223 \u2223 \u2223 \u2223\n\u2223\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 \u3008f, \u00b5h,p \u2212 1 s s \u2211\nl=1\n(wl, \u00b7)\u3009H \u2223 \u2223 \u2223 \u2223\n\u2223\n\u2264 \u2016f\u2016H\n\u2225 \u2225 \u2225 \u2225 \u2225 \u00b5h,p \u2212 1 s s \u2211\nl=1\nh(wl, \u00b7) \u2225 \u2225 \u2225 \u2225\n\u2225 H = \u2016f\u2016HDh,p(S) ,\nwhere Dh,p(S) is given as follows,\nDh,p(S) 2 =\n\u2225 \u2225 \u2225 \u2225 \u2225 \u00b5h,p \u2212 1 s s \u2211\nl=1\nh(wl, \u00b7) \u2225 \u2225 \u2225 \u2225\n\u2225\n2\nH\n= \u3008\u00b5h,p, \u00b5h,p\u3009H \u2212 2\ns\ns \u2211\nl=1\n\u3008\u00b5h,p, h(wl, \u00b7)\u3009H + 1\ns2\ns \u2211\nl=1\ns \u2211\nj=1\n\u3008h(wl, \u00b7), h(wj , \u00b7)\u3009H\n= E [\u00b5h,p(X)]\u2212 2\ns\ns \u2211\nl=1\nE [h(wl, \u00b7)] + 1\ns2\ns \u2211\nl=1\ns \u2211\nj=1\nh(wl,wj)\n=\n\u222b\nRd\n\u222b\nRd\nh(\u03c9, \u03c6)p(\u03c9)p(\u03c6)d\u03c9d\u03c6 \u2212 2 s\ns \u2211\nl=1\n\u222b\nRd\nh(wl, \u03c9)p(\u03c9)d\u03c9 + 1\ns2\ns \u2211\nl=1\ns \u2211\nj=1\nh(wl,wj) ."}, {"heading": "A.3 Proof of Theorem 9", "text": "We apply (9) to the particular case of h = sincb. We have\n\u222b\nRd\n\u222b\nRd\nh(\u03c9, \u03c6)p(\u03c9)p(\u03c6)d\u03c9d\u03c6 = \u03c0\u2212d \u222b\nRd\n\u222b\nRd\nd \u220f\nj=1\nsin(bj(\u03c9j \u2212 \u03c6j)) \u03c9j \u2212 \u03c6j pj(\u03c9j)pj(\u03c6j)d\u03c9d\u03c6\n= \u03c0\u2212d d \u220f\nj=1\n\u222b\nR\n\u222b\nR\nsin(bj(\u03c9j \u2212 \u03c6j)) \u03c9j \u2212 \u03c6j pj(\u03c9j)pj(\u03c6j)d\u03c9jd\u03c6j ,\nand\ns \u2211\nl=1\n\u222b\nRd\nh(wl, \u03c9)p(\u03c9)d\u03c9 = \u03c0 \u2212d\ns \u2211\nl=1\n\u222b\nRd\nd \u220f\nj=1\nsin(bj(wlj \u2212 \u03c9j)) wlj \u2212 \u03c9j pj(\u03c9j)d\u03c9\n= \u03c0\u2212d s \u2211\nl=1\nd \u220f\nj=1\n\u222b\nRd sin(bj(wlj \u2212 \u03c9j)) wlj \u2212 \u03c9j pj(\u03c9j)d\u03c9j .\nSo we can consider each coordinate on its own. Fix j. We have\n\u222b\nR\nsin(bjx)\nx pj(x)d =\n\u222b\nR\n\u222b bj\n0 cos(\u03b2x)pj(x)d\u03b2dx\n= 1\n2\n\u222b bj\n\u2212bj\n\u222b\nR\nei\u03b2xp(x)dxd\u03b2\n= 1\n2\n\u222b bj\n\u2212bj \u03d5j(\u03b2)d\u03b2 .\nThe interchange in the second line is allowed since the pj(x) makes the function integrable (with respect to x).\nNow fix w \u2208 R as well. Let hj(x, y) = sin(bj(x\u2212 y))/\u03c0(x\u2212 y). We have \u222b\nR\nhj(\u03c9,w)pj(\u03c9)d\u03c9 = \u03c0 \u22121\n\u222b\nR\nsin(bj(\u03c9 \u2212w)) \u03c9 \u2212 w pj(\u03c9)d\u03c9\n= \u03c0\u22121 \u222b\nR\nsin(bjx)\nx pj(x+ w)dx\n= (2\u03c0)\u22121 \u222b bj\n\u2212bj \u03d5j(\u03b2)e\niw\u03b2d\u03b2 ,\nwhere the last equality follows from first noticing that the characteristic function associated with the density function x 7\u2192 pj(x+ w) is \u03b2 7\u2192 \u03d5(\u03b2)eiw\u03b2 , and then applying the previous inequality.\nWe also have,\n\u222b\nR\n\u222b\nR\nsin(bj(x\u2212 y)) x\u2212 y pj(x)pj(y)dxdy = \u222b\nR\n\u222b\nR\n\u222b bj\n0 cos(\u03b2(x\u2212 y))pj(x)pj(y)d\u03b2dxdy\n= 1\n2\n\u222b\nR\n\u222b\nR\n\u222b bj\n\u2212bj ei\u03b2(x\u2212y)pj(x)pj(y)d\u03b2dxdy\n= 1\n2\n\u222b bj\n\u2212bj\n\u222b\nR\n\u222b\nR\nei\u03b2(x\u2212y)pj(x)pj(y)dxdyd\u03b2\n= 1\n2\n\u222b bj\n\u2212bj\n(\u222b\nR\nei\u03b2xpj(x)dx\n)(\u222b\nR\ne\u2212i\u03b2ypj(y)dy ) d\u03b2\n= 1\n2\n\u222b bj\n\u2212bj \u03d5j(\u03b2)\u03d5j(\u03b2)\n\u2217d\u03b2\n= 1\n2\n\u222b bj\n\u2212bj |\u03d5j(\u03b2)|2d\u03b2 .\nThe interchange at the third line is allowed because of pj(x)pj(y). In the last line we use the fact that the \u03d5j(\u00b7) is Hermitian."}, {"heading": "A.4 Proof of Theorem 12", "text": "Let b > 0 be a scalar, and let u \u2208 [\u2212b, b] and z \u2208 R. We have, \u222b \u221e\n\u2212\u221e e\u2212iux sin(b(x\u2212 z)) \u03c0(x\u2212 z) dx = e \u2212iuz \u222b \u221e \u2212\u221e e\u2212i2\u03c0 u 2b y sin(\u03c0y) \u03c0y dy\n= e\u2212iuz rect(u/2b) = e\u2212iuz .\nIn the above, rect is the function that is 1 on [\u22121/2, 1/2] and zero elsewhere. The last equality implies that for every u \u2208 b and every x \u2208 Rd we have\nfu(x) =\n\u222b\nRd\nfu(y) sincb(y,x)dy .\nThat is, the reproducing property holds on fu(\u00b7), even though fu /\u2208 PWb.\nWe now have for every u \u2208 b,\n\u01ebS,p[fu] =\n\u2223 \u2223 \u2223 \u2223 \u2223 \u222b\nRd\nfu(x)p(x)dx \u2212 1\ns\ns \u2211\ni=1\nf(wi)\n\u2223 \u2223 \u2223 \u2223 \u2223\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 \u222b\nRd\n\u222b\nRd\nfu(y) sincb(y,x)dyp(x)dx \u2212 1\ns\ns \u2211\ni=1\n\u222b\nRd\nfu(y) sincb(y,wi)dy\n\u2223 \u2223 \u2223 \u2223 \u2223\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 \u222b\nRd\nfu(y)\n[\n\u222b\nRd\nsincb(y,x)p(x)dx \u2212 1\ns\ns \u2211\ni=1\nsincb(y,wi)\n]\ndy\n\u2223 \u2223 \u2223 \u2223 \u2223 .\nLet us denote\nrS(y) =\n\u222b\nRd\nsincb(y,x)p(x)dx \u2212 1\ns\ns \u2211\ni=1\nsincb(y,wi) .\nSo,\n\u01ebS,p[fu] =\n\u2223 \u2223 \u2223 \u2223 \u222b\nRd\nfu(y)rS(y)dy\n\u2223 \u2223 \u2223 \u2223 .\nThe function rS(\u00b7) is square-integrable, so it has a Fourier transform r\u0302S(\u00b7). The above formula is exactly the value of r\u0302S(u). That is, \u01ebS,p[fu] = |r\u0302S(u)| . Now,\nEf\u223cU(F b) [ \u01ebS,p[f ] 2 ] = Eu\u223cU( b) [ \u01ebS,p[fu] 2 ]\n=\n\u222b\nu\u2208 b |r\u0302S(u)|2\n\n\nd \u220f\nj=1\n2bj\n\n\n\u22121\ndu\n=\n\n\nd \u220f\nj=1\n2bj\n\n\n\u22121\n\u2016r\u0302S\u20162L2\n= (2\u03c0)d\n\u220fd j=1 2bj\n\u2016rS\u20162PWb\n= \u03c0d\n\u220fd j=1 bj\nD p (S) 2 .\nThe equality before the last follows from Plancherel formula and the equality of the norm in PWb to the L2-norm. The last equality follows from the fact that rS is exactly the expression used in the proof of Proposition 6 to derive D p ."}, {"heading": "A.5 Proof of Corollary 13", "text": "In this case, p(x) = \u220fd j=1 pj(xj) where pj(\u00b7) is the density function of N (0, 1/\u03c3j). The characteristic function associated with pj(\u00b7) is \u03d5j(\u03b2) = e \u2212 \u03b2 2 2\u03c32 j . We apply (11) directly.\nFor the first term, since\n\u222b bj\n0 |\u03d5j(\u03b2)|2d\u03b2 =\n\u222b bj 0 e \u2212\u03b2\n2\n\u03c32 j d\u03b2\n= \u03c3j\n\u222b bj/\u03c3j\n0 e\u2212y 2 dy\n= \u03c3j \u221a \u03c0\n2 erf\n(\nbj \u03c3j\n)\n,\nwe have\n\u03c0\u2212d d \u220f\nj=1\n\u222b bj\n0 |\u03d5j(\u03b2)|2d\u03b2 =\nd \u220f\nj=1\n\u03c3j 2 \u221a \u03c0 erf\n(\nbj \u03c3j\n)\n. (19)\nFor the second term, since\n\u222b bj\n\u2212bj \u03d5j(\u03b2)e\niwlj\u03b2d\u03b2 =\n\u222b bj\n\u2212bj e\u2212\n\u03b22j 2\u03c32 +iwlj\u03b2d\u03b2\n= e\u2212 \u03c3jwlj 2\n\u222b bj \u2212bj e \u2212 ( \u03b2\u221a 2\u03c3j \u2212i\u03c3jwlj\u221a 2 ) 2 d\u03b2\n= \u221a 2\u03c3je \u2212 \u03c32j w 2 lj 2 \u222b bj\u221a 2\u03c3j\n\u2212 bj\u221a 2\u03c3j\ne \u2212 ( y\u2212i\u03c3jwlj\u221a 2 ) 2 dy\n= \u221a 2\u03c3je \u2212 \u03c32j w 2 lj 2 \u222b bj\u221a 2\u03c3j \u2212i\u03c3jwlj\u221a 2\n\u2212 bj\u221a 2\u03c3j \u2212i\u03c3jwlj\u221a 2\ne\u2212z 2 dz\n= \u221a \u03c0\u03c3j\u221a 2 e\u2212 \u03c32j w 2 lj 2 ( erf ( \u2212 bj\u221a 2\u03c3j \u2212 i\u03c3jwlj\u221a 2 ) \u2212 erf ( bj\u221a 2\u03c3j \u2212 i\u03c3jwlj\u221a 2 ))\n= \u221a 2\u03c0\u03c3je \u2212 \u03c32j w 2 lj 2 Re\n(\nerf\n(\n\u2212 bj\u221a 2\u03c3j \u2212 i\u03c3jwlj\u221a 2\n))\n,\nwe have\n2 s (2\u03c0)\u2212d\ns \u2211\nl=1\nd \u220f\nj=1\n\u222b bj\n\u2212bj \u03d5j(\u03b2)e\niwlj\u03b2d\u03b2 = 2\ns\ns \u2211\nl=1\nd \u220f\nj=1\n\u03c3j\u221a 2\u03c0 e\u2212 \u03c32j w 2 lj 2 Re\n(\nerf\n(\n\u2212 bj\u221a 2\u03c3j \u2212 i\u03c3jwlj\u221a 2\n))\n. (20)\nCombining (19), (20) and (11), (12) follows."}, {"heading": "A.6 Proof of Proposition 16", "text": "Before we compute the derivative, we prove two auxiliary lemmas.\nLemma 19. Let x \u2208 Rd be a variable and z \u2208 Rd be fixed vector. Then,\n\u2202 sincb(x, z)\n\u2202xj = bj sinc\n\u2032 bj (xj, zj)\n\u220f q 6=j sincbq(xq, zq) . (21)\nWe omit the proof as it is a simple computation that follows from the definition of sincb.\nLemma 20. The derivative of the scalar function f(x) = Re [ e\u2212ax 2 erf (c+ idx) ] , for real scalars a, c, d\nis given by, \u2202f\n\u2202x = \u22122axe\u2212ax2 Re [erf (c+ idx)] + 2d\u221a \u03c0 e\u2212ax 2 ed 2x2\u2212c2 sin(2cdx) .\nProof. Since\nf(x) = 1\n2\n(\ne\u2212ax 2 erf(c+ idx) + ( e\u2212ax 2 erf(c+ idx) )\u2217)\n= 1\n2\n(\ne\u2212ax 2 erf(c+ idx) + e\u2212ax 2 erf(c\u2212 idx) ) , (22)\nit suffices to compute the the derivative g(x) = e\u2212ax 2\nerf(c+ idx). Let k(x) = erf(c+ idx). We have\ng\u2032(x) = \u22122axe\u2212ax2k(x) + e\u2212ax2k\u2032(x) . (23)\nSince\nk(x) = erf(c+ idx)\n= 2\u221a \u03c0\n\u222b c+idx\n0 e\u2212z 2 dz\n= 2\u221a \u03c0\n(\u222b c\n0 e\u2212z 2 dz +\n\u222b c+idx\nc e\u2212z 2 dz\n)\n= 2\u221a \u03c0\n( \u222b c\n0 e\u2212y 2 dy + (id)\n\u222b x\n0 e\u2212(c+idt) 2 dt\n)\n, (24)\nwe have\nk\u2032(x) = 2\u221a \u03c0 e\u2212(c+idx) 2 = 2d\u221a \u03c0 ed 2x2\u2212c2(sin(2cdx) + i cos(2cdx)) . (25)\nWe now have\nf \u2032(x) = 1\n2\n( g\u2032(x) + (g\u2217(x))\u2032 )\n= 1\n2\n( g\u2032(x) + (g\u2032(x))\u2217 )\n= 1\n2\n( \u22122axe\u2212ax2(k(x) + k\u2217(x)) + e\u2212ax2(k\u2032(x) + (k\u2032(x))\u2217) )\n= 1\n2\n(\n\u22124axe\u2212ax2 Re [erf (c+ idx)] + e\u2212ax2 4d\u221a \u03c0 ed 2x2\u2212c2 sin(2cdx) )\n= \u22122axe\u2212ax2 Re [erf (c+ idx)] + 2d\u221a \u03c0 e\u2212ax 2 ed 2x2\u2212c2 sin(2cdx) . (26)"}, {"heading": "A.7 Proof of Corollary 14", "text": "The proof is similar to the proof of Theorem 3.6 of Dick et al. [2013]. Notice that since supx\u2208Rd h(x,x) < \u221e, we have \u222b\nRd h(x,x)p(x)dx < \u221e. From Lemma 18 we know that\n\u222b\nRd h(\u00b7,y)p(y)dy \u2208 H, hence from\nLemma 17, we have \u222b\nRd\n\u222b\nRd h(x,y)p(x)p(y)dxdy < \u221e.\nBy (9), we have\nDh,p(S) 2 =\n\u222b\nRd\n\u222b\nRd\nh(\u03c9, \u03c6)p(\u03c9)p(\u03c6)d\u03c9d\u03c6\n\u22122 s\ns \u2211\nl=1\n\u222b\nRd\nh(wl, \u03c9)p(\u03c9)d\u03c9\n+ 1\ns2\ns \u2211\nl=1\nh(wl,wl) + 1\ns2\ns \u2211\nl,j=1,l 6=j h(wl,wj) .\nThen,\nE [ Dh,p(S) 2 ] =\n\u222b\n[0,1]d \u00b7 \u00b7 \u00b7\n\u222b\n[0,1]d\n(\n\u222b\nRd\n\u222b\nRd\nh(\u03c9, \u03c6)p(\u03c9)p(\u03c6)d\u03c9d\u03c6 \u2212 2 s\ns \u2211\nl=1\n\u222b\nRd\nh(\u03a6\u22121(tl), \u03c9)p(\u03c9)d\u03c9\n+ 1\ns2\ns \u2211\nl=1\nh(\u03a6\u22121(tl),\u03a6 \u22121(tl)) +\n1\ns2\ns \u2211\nl,j=1,l 6=j h(\u03a6\u22121(tl),\u03a6 \u22121(tj))\n\n dt1 \u00b7 \u00b7 \u00b7 dts .\nObviously, the first is a constant which is independent to t1, . . . , ts. Since all the terms are finite, we can interchange the integral and the sum among rest terms. In the second term, for each l, the only dependence on t1, . . . , ts is tl, hence all the other tj can be integrated out. That is,\n\u222b\n[0,1]d \u00b7 \u00b7 \u00b7\n\u222b\n[0,1]d\n\u222b\nRd\nh(\u03a6\u22121(tl), \u03c9)p(\u03c9)d\u03c9dt1 \u00b7 \u00b7 \u00b7 dts = \u222b\n[0,1]d\n\u222b\nRd\nh(\u03a6\u22121(tl), \u03c9)p(\u03c9)d\u03c9dtl\n=\n\u222b\nRd\n\u222b\nRd\nh(\u03c6, \u03c9)p(\u03c6)p(\u03c9)d\u03c6d\u03c9.\nAbove, the last equality comes from a change of variable, i.e., tl = (\u03a61(\u03c61), . . . ,\u03a6d(\u03c6d)). Similar operations can be done for the third and fourth term. Combining all of these, we have the following,\nE [ Dh,p(S) 2 ] =\n\u222b\nRd\n\u222b\nRd\nh(\u03c9, \u03c6)p(\u03c9)p(\u03c6)d\u03c9d\u03c6 \u2212 2 \u222b\nRd\n\u222b\nRd\nh(\u03c9, \u03c6)p(\u03c9)p(\u03c6)d\u03c9d\u03c6\n+ 1\ns\n\u222b\nRd\nh(\u03c9, \u03c9)p(\u03c9)d\u03c9 + s\u2212 1 s \u222b\nRd\n\u222b\nRd\nh(\u03c9, \u03c6)p(\u03c9)p(\u03c6)d\u03c9d\u03c6\n= 1\ns\n\u222b\nRd\nh(\u03c9, \u03c9)p(\u03c9)d\u03c9 \u2212 1 s\n\u222b\nRd\n\u222b\nRd\nh(\u03c9, \u03c6)p(\u03c9)p(\u03c6)d\u03c9d\u03c6 .\nProof of Proposition 16. For the first term in (12), that is 1s2 \u2211s m=1 \u2211s r=1 sincb(wm,wr), to compute the partial derivative of wlj , we only have to consider when at least m or r is equal to l. If m = j = l, by definition, the corresponding term in the summation is one. Hence, we only have to consider the case when m 6= r. By symmetry, it is equivalent to compute the partial derivative of the following function 2 s2 \u2211s m=1,m6=l sincb(wl,wm). Applying Lemma 19, we get the first term in (14).\nNext, for the last term in (12), we only have consider the term associated with one in the summation\nand the term associated with j in the product. Since (\n\u03c3j\u221a 2\u03c0\n) e\u2212 \u03c32j w 2 lj 2 Re ( erf (\nbj \u03c3j \u221a 2 \u2212 i\u03c3jwlj\u221a 2\n))\nsatisfies the\nformulation in Lemma 20, we can simply apply Lemma 20 and get its derivative with respect to wlj . Equation (14) follows by combining these terms."}], "references": [{"title": "Subspace embeddings for the polynomial kernel", "author": ["H. Avron", "H.L. Nguy \u0303\u0302en", "D.P. Woodruff"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Avron et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2014}, {"title": "Sharp analysis of low-rank kernel matrix approximations", "author": ["F. Bach"], "venue": "In Proceedings of the 26th Conference on Learning Theory (COLT),", "citeRegEx": "Bach.,? \\Q2013\\E", "shortCiteRegEx": "Bach.", "year": 2013}, {"title": "On the equivalence between herding and conditional gradient algorithms", "author": ["F. Bach", "S. Lacoste-Julien", "G. Obozinski"], "venue": "In Proceeding of the 29th International Conference in Machine Learning (ICML),", "citeRegEx": "Bach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Belkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2006}, {"title": "Reproducing Kernel Hilbert Spaces in Probability and Statistics", "author": ["A. Berlinet", "C. Thomas-Agnan"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Berlinet and Thomas.Agnan.,? \\Q2004\\E", "shortCiteRegEx": "Berlinet and Thomas.Agnan.", "year": 2004}, {"title": "Monotone funktionen, Stieltjes integrale und harmonische analyse", "author": ["S. Bochner"], "venue": "Math. Ann.,", "citeRegEx": "Bochner.,? \\Q1933\\E", "shortCiteRegEx": "Bochner.", "year": 1933}, {"title": "Hilbert space embeddings of predictive state representations", "author": ["B. Boots", "A. Gretton", "G.J. Gordon"], "venue": "In Conference Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Boots et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2013}, {"title": "Large-scale Kernel Machines", "author": ["L. Bottou", "O. Chapelle", "D. DeCoste", "J. Weston (Editors"], "venue": null, "citeRegEx": "Bottou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2007}, {"title": "Monte Carlo and Quasi-Monte Carlo methods", "author": ["R.E. Caflisch"], "venue": "Acta Numerica, 7:1\u201349,", "citeRegEx": "Caflisch.,? \\Q1998\\E", "shortCiteRegEx": "Caflisch.", "year": 1998}, {"title": "Super-samples from kernel herding", "author": ["Y. Chen", "M. Welling", "A. Smola"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "On the mathematical foundations of learning", "author": ["F. Cucker", "S. Smale"], "venue": "Bull. Amer. Math. Soc.,", "citeRegEx": "Cucker and Smale.,? \\Q2001\\E", "shortCiteRegEx": "Cucker and Smale.", "year": 2001}, {"title": "High-dimensional integration: The Quasi-Monte Carlo way", "author": ["J. Dick", "F.Y. Kuo", "I.H. Sloan"], "venue": "Acta Numerica, 22:133\u2013288,", "citeRegEx": "Dick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dick et al\\.", "year": 2013}, {"title": "Fast Randomized Kernel Methods With Statistical Guarantees", "author": ["A. El Alaoui", "M.W. Mahoney"], "venue": "ArXiv e-prints,", "citeRegEx": "Alaoui and Mahoney.,? \\Q2014\\E", "shortCiteRegEx": "Alaoui and Mahoney.", "year": 2014}, {"title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "author": ["A. Gittens", "M.W. Mahoney"], "venue": "In Proc. of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Gittens and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Gittens and Mahoney.", "year": 2013}, {"title": "Compact random feature maps", "author": ["R. Hamid", "A. Gittens", "Y. Xiao", "D. DeCoste"], "venue": "In Proc. of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Hamid et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hamid et al\\.", "year": 2014}, {"title": "Kernel-based methods for hypothesis testing: A unified view", "author": ["Z. Harchaoui", "F. Bach", "O. Cappe", "E. Moulines"], "venue": "Signal Processing Magazine,", "citeRegEx": "Harchaoui et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Harchaoui et al\\.", "year": 2013}, {"title": "Kernel methods match Deep Neural Networks on TIMIT", "author": ["P. Huang", "H. Avron", "T. Sainath", "V. Sindhwani", "B. Ramabhadran"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Random feature maps for dot product kernels", "author": ["P. Kar", "H. Karnick"], "venue": "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS-12),", "citeRegEx": "Kar and Karnick.,? \\Q2012\\E", "shortCiteRegEx": "Kar and Karnick.", "year": 2012}, {"title": "Building support vector machines with reduced classifier complexity", "author": ["S.S. Keerthi", "O. Chapelle", "D. DeCoste"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Keerthi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Keerthi et al\\.", "year": 2006}, {"title": "Fastfood \u2013 Approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "In Proc. of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Introduction to Quasi-Monte Carlo Integration and Applications", "author": ["G. Leobacher", "F. Pillichschammer"], "venue": null, "citeRegEx": "Leobacher and Pillichschammer.,? \\Q2014\\E", "shortCiteRegEx": "Leobacher and Pillichschammer.", "year": 2014}, {"title": "Random Fourier approximations for skewed multiplicative histogram kernels", "author": ["F. Li", "C. Ionescu", "C. Sminchisescu"], "venue": "Pattern Recognition,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Max-margin additive classifiers for detection", "author": ["S. Maji", "A.C. Berg"], "venue": "In IEEE 12th International Conference on Computer Vision (ICCV),", "citeRegEx": "Maji and Berg.,? \\Q2009\\E", "shortCiteRegEx": "Maji and Berg.", "year": 2009}, {"title": "A method for evaluation of the error function of real and complex variable with high relative accuracy", "author": ["M. Mori"], "venue": "Publ. RIMS, Kyoto Univ.,", "citeRegEx": "Mori.,? \\Q1983\\E", "shortCiteRegEx": "Mori.", "year": 1983}, {"title": "Random number generation and Quasi-Monte Carlo methods", "author": ["H. Niederreiter"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Niederreiter.,? \\Q1992\\E", "shortCiteRegEx": "Niederreiter.", "year": 1992}, {"title": "Statistical inference on time series by RKHS methods", "author": ["E. Parzen"], "venue": "In Proceedings of 12th Biennial Seminar Canadian Mathematical Congress on Time Series and Stochastic Processes: convexity and combinatorics,", "citeRegEx": "Parzen.,? \\Q1970\\E", "shortCiteRegEx": "Parzen.", "year": 1970}, {"title": "Classical spaces of holomorphic functions", "author": ["M.M. Peloso"], "venue": "Technical report, Universit\u2018 di Milano,", "citeRegEx": "Peloso.,? \\Q2011\\E", "shortCiteRegEx": "Peloso.", "year": 2011}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["N. Pham", "R. Pagh"], "venue": "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Pham and Pagh.,? \\Q2013\\E", "shortCiteRegEx": "Pham and Pagh.", "year": 2013}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "Positive definite functions on spheres", "author": ["I.J. Schoenberg"], "venue": "Duke Mathematical Journal, 9(1):96\u2013108,", "citeRegEx": "Schoenberg.,? \\Q1942\\E", "shortCiteRegEx": "Schoenberg.", "year": 1942}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization and Beyond", "author": ["B. Sch\u00f6lkopf", "A. Smola", "editors"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2002}, {"title": "High-performance kernel machines with implicit distributed optimization and randomization", "author": ["V. Sindhwani", "H. Avron"], "venue": "In JSM Proceedings, Tradeoffs in Big Data Modeling - Section on Statistical Computing,", "citeRegEx": "Sindhwani and Avron.,? \\Q2014\\E", "shortCiteRegEx": "Sindhwani and Avron.", "year": 2014}, {"title": "When are Quasi-Monte Carlo algorithms efficient for high dimensional integrals", "author": ["I.H. Sloan", "H. Wozniakowski"], "venue": "Journal of Complexity,", "citeRegEx": "Sloan and Wozniakowski.,? \\Q1998\\E", "shortCiteRegEx": "Sloan and Wozniakowski.", "year": 1998}, {"title": "Scholkopf. A hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song"], "venue": "In Algorithmic Learning Theory (ALT),", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Hilbert space embeddings of Hidden Markov Models", "author": ["L. Song", "B. Boots", "S. Siddiqi", "G. Gordon", "A. Smola"], "venue": "In Proceedings of the 30th International Conference in Machine Learning (ICML),", "citeRegEx": "Song et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Song et al\\.", "year": 2013}, {"title": "Generalized RBF feature maps for efficient detection", "author": ["V. Sreekanth", "A. Vedaldi", "C.V. Jawahar", "A. Zisserman"], "venue": "In Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Sreekanth et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sreekanth et al\\.", "year": 2010}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Scholkopf", "G. Lanckriet"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "Breaking intractability", "author": ["J.F. Traub", "H. Wozniakowski"], "venue": "Scientific American,", "citeRegEx": "Traub and Wozniakowski.,? \\Q1994\\E", "shortCiteRegEx": "Traub and Wozniakowski.", "year": 1994}, {"title": "Core vector machines: Fast svm training on very large data sets", "author": ["I.W. Tsang", "J.T. Kwok", "P. Cheung"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Tsang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsang et al\\.", "year": 2005}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "Pattern Analysis and Machine Intellingence,", "citeRegEx": "Vedaldi and Zisserman.,? \\Q2011\\E", "shortCiteRegEx": "Vedaldi and Zisserman.", "year": 2011}, {"title": "Spline Models for Observational Data. Society for Industrial and Applied Mathematics, Philadelphia", "author": ["G. Wahba", "editor"], "venue": "PA, USA,", "citeRegEx": "Wahba and editor.,? \\Q1990\\E", "shortCiteRegEx": "Wahba and editor.", "year": 1990}, {"title": "Computation of the complex error function", "author": ["J.A.C. Weideman"], "venue": "SIAM Journal of Numerical Analysis,", "citeRegEx": "Weideman.,? \\Q1994\\E", "shortCiteRegEx": "Weideman.", "year": 1994}, {"title": "Herding dynamical weights to learn", "author": ["M. Welling"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Welling.,? \\Q2009\\E", "shortCiteRegEx": "Welling.", "year": 2009}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C.K.I. Williams", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Williams and Seeger.,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2001}, {"title": "Average case complexity of multivariate integration", "author": ["H. Wozniakowski"], "venue": "Bull. Amer. Math. Soc.,", "citeRegEx": "Wozniakowski.,? \\Q1991\\E", "shortCiteRegEx": "Wozniakowski.", "year": 1991}, {"title": "Random Laplace feature maps for semigroup kernels on histograms", "author": ["J. Yang", "V. Sindhwani", "Q. Fan", "H. Avron", "M. Mahoney"], "venue": "In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Applications of Reproducing Kernel Hilbert Spaces - bandlimited signal models", "author": ["K. Yao"], "venue": "Inform. Control,", "citeRegEx": "Yao.,? \\Q1967\\E", "shortCiteRegEx": "Yao.", "year": 1967}, {"title": "Scholkopf. Kernel based conditional independence test and application in causal discovery", "author": ["K. Zhang", "J. Peters", "D. Janzing"], "venue": "In Confernece on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "These include nonlinear classification, regression, clustering, semi-supervised learning [Belkin et al., 2006], time-series analysis [Parzen, 1970], sequence modeling [Song et al.", "startOffset": 89, "endOffset": 110}, {"referenceID": 25, "context": ", 2006], time-series analysis [Parzen, 1970], sequence modeling [Song et al.", "startOffset": 30, "endOffset": 44}, {"referenceID": 34, "context": ", 2006], time-series analysis [Parzen, 1970], sequence modeling [Song et al., 2013], dynamical systems [Boots et al.", "startOffset": 64, "endOffset": 83}, {"referenceID": 6, "context": ", 2013], dynamical systems [Boots et al., 2013], hypothesis testing [Harchaoui et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 15, "context": ", 2013], hypothesis testing [Harchaoui et al., 2013], causal modeling [Zhang et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 47, "context": ", 2013], causal modeling [Zhang et al., 2011] and many more.", "startOffset": 25, "endOffset": 45}, {"referenceID": 26, "context": "In this paper, we revisit one of the most successful techniques, namely the randomized construction of a family of low-dimensional approximate feature maps proposed by Rahimi and Recht [2007]. These randomized feature maps, \u03a8\u0302 : X \u2192 Cs, provide low-distortion approximations for (complex-valued) kernel functions k : X \u00d7 X \u2192 C: k(x, z) \u2248 \u3008\u03a8\u0302(x), \u03a8\u0302(z)\u3009Cs (1) where Cs denotes the space of s-dimensional complex numbers with the inner product, \u3008\u03b1, \u03b2\u3009Cs = \u2211s i=1 \u03b1i\u03b2 \u2217 i , with z\u2217 denoting the conjugate of the complex number z (though Rahimi and Recht [2007] also define realvalued feature maps for real-valued kernels, our technical exposition is simplified by adopting the generality of complex-valued features).", "startOffset": 168, "endOffset": 192}, {"referenceID": 26, "context": "In this paper, we revisit one of the most successful techniques, namely the randomized construction of a family of low-dimensional approximate feature maps proposed by Rahimi and Recht [2007]. These randomized feature maps, \u03a8\u0302 : X \u2192 Cs, provide low-distortion approximations for (complex-valued) kernel functions k : X \u00d7 X \u2192 C: k(x, z) \u2248 \u3008\u03a8\u0302(x), \u03a8\u0302(z)\u3009Cs (1) where Cs denotes the space of s-dimensional complex numbers with the inner product, \u3008\u03b1, \u03b2\u3009Cs = \u2211s i=1 \u03b1i\u03b2 \u2217 i , with z\u2217 denoting the conjugate of the complex number z (though Rahimi and Recht [2007] also define realvalued feature maps for real-valued kernels, our technical exposition is simplified by adopting the generality of complex-valued features).", "startOffset": 168, "endOffset": 558}, {"referenceID": 15, "context": "This technique is immensely successful, and has been used in recent years to obtain state-of-the-art accuracies for some classical datasets [Huang et al., 2014, Sindhwani and Avron, 2014]. The starting point of Rahimi and Recht [2007] is a celebrated result that characterizes the class of positive definite functions: Definition 1.", "startOffset": 141, "endOffset": 235}, {"referenceID": 5, "context": "Theorem 2 (Bochner [1933]).", "startOffset": 11, "endOffset": 26}, {"referenceID": 28, "context": "Finally, empirical results (Section 6) clearly demonstrate the superiority of QMC techniques over the MC feature maps [Rahimi and Recht, 2007], the correctness of our theoretical analysis and the potential value of adaptive QMC techniques for large-scale kernel methods.", "startOffset": 118, "endOffset": 142}, {"referenceID": 16, "context": "This is motivated by recent work that showed that in order to obtain state-of-the-art accuracy on some important datasets, a very large number of random features is needed [Huang et al., 2014, Sindhwani and Avron, 2014]. Our point of departure from the work of Rahimi and Recht [2007] is the simple observation that when w1, .", "startOffset": 173, "endOffset": 285}, {"referenceID": 43, "context": "One dominant line of work constructs low-rank approximations of the Gram matrix, either using data-oblivious randomized feature maps to approximate the kernel function, or using sampling techniques such as the classical Nystr\u00f6m method [Williams and Seeger, 2001].", "startOffset": 235, "endOffset": 262}, {"referenceID": 30, "context": ", erf(z) = \u222b z 0 e \u2212z2dz for z \u2208 C; see Weideman [1994] and Mori [1983] for more details.", "startOffset": 40, "endOffset": 56}, {"referenceID": 18, "context": ", erf(z) = \u222b z 0 e \u2212z2dz for z \u2208 C; see Weideman [1994] and Mori [1983] for more details.", "startOffset": 60, "endOffset": 72}, {"referenceID": 12, "context": "More elaborate techniques exist, both randomized and deterministic; see Gittens and Mahoney [2013] for a thorough treatment.", "startOffset": 72, "endOffset": 99}, {"referenceID": 12, "context": "More elaborate techniques exist, both randomized and deterministic; see Gittens and Mahoney [2013] for a thorough treatment. More relevant to our work is the randomized feature mapping approach. Pioneered by the seminal paper of Rahimi and Recht [2007], the core idea is to construct, for a given kernel on a data domain X , a transformation \u03a8\u0302 : X \u2192 Cs such that k(x, z) \u2248 \u3008\u03a8\u0302(x), \u03a8\u0302(z)\u3009Cs .", "startOffset": 72, "endOffset": 253}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels.", "startOffset": 9, "endOffset": 537}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi).", "startOffset": 9, "endOffset": 654}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi). Vedaldi and Zisserman [2011] developed feature maps for \u03b3-homogeneous kernels.", "startOffset": 9, "endOffset": 766}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi). Vedaldi and Zisserman [2011] developed feature maps for \u03b3-homogeneous kernels. Sreekanth et al. [2010] developed feature maps for generalized RBF kernels k(x, z) = g(D(x, z)2) where g(\u00b7) is a positive definite function, and D(\u00b7, \u00b7) is a distance metric.", "startOffset": 9, "endOffset": 840}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi). Vedaldi and Zisserman [2011] developed feature maps for \u03b3-homogeneous kernels. Sreekanth et al. [2010] developed feature maps for generalized RBF kernels k(x, z) = g(D(x, z)2) where g(\u00b7) is a positive definite function, and D(\u00b7, \u00b7) is a distance metric. Kar and Karnick [2012] suggested feature maps for dot-product kernels.", "startOffset": 9, "endOffset": 1014}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi). Vedaldi and Zisserman [2011] developed feature maps for \u03b3-homogeneous kernels. Sreekanth et al. [2010] developed feature maps for generalized RBF kernels k(x, z) = g(D(x, z)2) where g(\u00b7) is a positive definite function, and D(\u00b7, \u00b7) is a distance metric. Kar and Karnick [2012] suggested feature maps for dot-product kernels. The feature maps is based on the Maclaurin expansion, which is guaranteed to be non-negative due to a classical result of Schoenberg [1942]. Pham and Pagh [2013] suggested feature maps for the polynomial kernels.", "startOffset": 9, "endOffset": 1202}, {"referenceID": 5, "context": "Invoking Bochner\u2019s theorem, a classical result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x \u2212 y) for some positive definite function g(\u00b7). Subsequently, there has been considerable effort given to extending this technique to other classes of kernels. While the original feature maps suggested by Rahimi and Recht were randomized, some of the maps proposed in the literature are, in fact, deterministic. Li et al. [2010] use Bochner\u2019s theorem to provide random features to the wider class of group-invariant kernels. Maji and Berg [2009] suggested random features for the intersection kernel k(x, z) = \u2211d i=1min(xi, zi). Vedaldi and Zisserman [2011] developed feature maps for \u03b3-homogeneous kernels. Sreekanth et al. [2010] developed feature maps for generalized RBF kernels k(x, z) = g(D(x, z)2) where g(\u00b7) is a positive definite function, and D(\u00b7, \u00b7) is a distance metric. Kar and Karnick [2012] suggested feature maps for dot-product kernels. The feature maps is based on the Maclaurin expansion, which is guaranteed to be non-negative due to a classical result of Schoenberg [1942]. Pham and Pagh [2013] suggested feature maps for the polynomial kernels.", "startOffset": 9, "endOffset": 1224}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014].", "startOffset": 151, "endOffset": 171}, {"referenceID": 38, "context": "Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated in terms of the minimum enclosing ball problem.", "startOffset": 56, "endOffset": 76}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms.", "startOffset": 152, "endOffset": 299}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, .", "startOffset": 152, "endOffset": 518}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, .", "startOffset": 152, "endOffset": 581}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small.", "startOffset": 152, "endOffset": 841}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small. In contrast, our work tries to design w1, . . . ,ws so that less features will be necessary to get the same quality of kernel approximation. Several other scalable approaches for large-scale kernel methods have been suggested over the years, starting from approaches such as chunking and decomposition methods proposed in the early days of SVM optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale Gauss process regression. There are also approaches that are more specific to the objective function at hand, e.g., Keerthi et al. [2006] build a kernel expansion greedily to optimize the SVM objective function.", "startOffset": 152, "endOffset": 1683}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small. In contrast, our work tries to design w1, . . . ,ws so that less features will be necessary to get the same quality of kernel approximation. Several other scalable approaches for large-scale kernel methods have been suggested over the years, starting from approaches such as chunking and decomposition methods proposed in the early days of SVM optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale Gauss process regression. There are also approaches that are more specific to the objective function at hand, e.g., Keerthi et al. [2006] build a kernel expansion greedily to optimize the SVM objective function. Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated in terms of the minimum enclosing ball problem. For a broader discussion of these methods, and others, see Bottou et al. [2007]. 2.", "startOffset": 152, "endOffset": 2093}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small. In contrast, our work tries to design w1, . . . ,ws so that less features will be necessary to get the same quality of kernel approximation. Several other scalable approaches for large-scale kernel methods have been suggested over the years, starting from approaches such as chunking and decomposition methods proposed in the early days of SVM optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale Gauss process regression. There are also approaches that are more specific to the objective function at hand, e.g., Keerthi et al. [2006] build a kernel expansion greedily to optimize the SVM objective function. Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated in terms of the minimum enclosing ball problem. For a broader discussion of these methods, and others, see Bottou et al. [2007]. 2.3 Quasi-Monte Carlo Techniques: an Overview In this section we provide a self-contained overview of Quasi-Monte Carlo (QMC) techniques. For brevity, we restrict our discussion to background that is necessary for understanding subsequent sections. We refer the interested reader to the excellent reviews by Caflisch [1998] and Dick et al.", "startOffset": 152, "endOffset": 2418}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small. In contrast, our work tries to design w1, . . . ,ws so that less features will be necessary to get the same quality of kernel approximation. Several other scalable approaches for large-scale kernel methods have been suggested over the years, starting from approaches such as chunking and decomposition methods proposed in the early days of SVM optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale Gauss process regression. There are also approaches that are more specific to the objective function at hand, e.g., Keerthi et al. [2006] build a kernel expansion greedily to optimize the SVM objective function. Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated in terms of the minimum enclosing ball problem. For a broader discussion of these methods, and others, see Bottou et al. [2007]. 2.3 Quasi-Monte Carlo Techniques: an Overview In this section we provide a self-contained overview of Quasi-Monte Carlo (QMC) techniques. For brevity, we restrict our discussion to background that is necessary for understanding subsequent sections. We refer the interested reader to the excellent reviews by Caflisch [1998] and Dick et al. [2013], and the recent book Leobacher and Pillichschammer [2014] for a much more detailed exposition.", "startOffset": 152, "endOffset": 2441}, {"referenceID": 0, "context": "subspace embedding, and this observation provides stronger theoretical guarantees then point-wise error bounds prevalent in the feature map literature [Avron et al., 2014]. By invoking a variant of Bochner\u2019s theorem that replaces the Fourier transform with the Laplace transform, Yang et al. [2014] obtained randomized feature maps for semigroup kernels on histograms. Our work is more in-line with recent efforts on scaling up the random features, so that learning and prediction can be done faster. Le et al. [2013] return to the original construction of Rahimi and Recht [2007], and devise a clever distribution of random samples w1,w2, . . . ,ws that is structured so that the generation of random features can be done much faster. They showed that only a very limited concession in term of convergence rate is made. Hamid et al. [2014], working on the polynomial kernel, suggest first generating a very large amount of random features, and then applying them a low-distortion embedding based the Fast Johnson-Lindenstruass Transform, so the make the final size of the mapped vector rather small. In contrast, our work tries to design w1, . . . ,ws so that less features will be necessary to get the same quality of kernel approximation. Several other scalable approaches for large-scale kernel methods have been suggested over the years, starting from approaches such as chunking and decomposition methods proposed in the early days of SVM optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale Gauss process regression. There are also approaches that are more specific to the objective function at hand, e.g., Keerthi et al. [2006] build a kernel expansion greedily to optimize the SVM objective function. Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated in terms of the minimum enclosing ball problem. For a broader discussion of these methods, and others, see Bottou et al. [2007]. 2.3 Quasi-Monte Carlo Techniques: an Overview In this section we provide a self-contained overview of Quasi-Monte Carlo (QMC) techniques. For brevity, we restrict our discussion to background that is necessary for understanding subsequent sections. We refer the interested reader to the excellent reviews by Caflisch [1998] and Dick et al. [2013], and the recent book Leobacher and Pillichschammer [2014] for a much more detailed exposition.", "startOffset": 152, "endOffset": 2499}, {"referenceID": 24, "context": ",ws}, the integration error is bounded above as follows, \u01ebS [f ] \u2264 D(S)VHK [f ] , where VHK is the Hardy-Krause variation of f (see Niederreiter [1992]), which is defined in terms of the following partial derivatives, VHK [f ] = \u2211", "startOffset": 132, "endOffset": 152}, {"referenceID": 11, "context": "However we mention that in addition to the Halton sequences, other notable members are Sobol\u2019 sequences, Faure sequences, Niederreiter sequences, and more (see Dick et al. [2013], Section 2).", "startOffset": 160, "endOffset": 179}, {"referenceID": 11, "context": "However we mention that in addition to the Halton sequences, other notable members are Sobol\u2019 sequences, Faure sequences, Niederreiter sequences, and more (see Dick et al. [2013], Section 2). We also mention that it is conjectured that the O((log s)d/s) rate for star discrepancy decay is optimal. The classical QMC theory, which is based on the Koksma-Hlawka inequality and low discrepancy sequences, thus achieves a convergence rate of O((log s)d/s). While this is asymptotically superior to O(s\u22121/2) for a fixed d, it requires s to be exponential in d for the improvement to manifest. As such, in the past QMC methods were dismissed as unsuitable for very high-dimensional integration. However, several authors noticed that QMC methods perform better than MC even for very highdimensional integration [Sloan and Wozniakowski, 1998, Dick et al., 2013]2. Contemporary QMC literature explains and expands on these empirical observations, by leveraging the structure of the space in which the integrand function lives, to derive more refined bounds and discrepancy measures, even when classical measures of variation such as (6) are unbounded. This literature has evolved along at least two directions: one, where worst-case analysis is provided under the assumption that the integrands live in a Reproducing Kernel Hilbert Space (RKHS) of sufficiently smooth and well-behaved functions (see Dick et al. [2013], Section 3) and second, where the analysis is done in terms of average-case error, under an assumed probability distribution over the integrands, instead of worst-case error [Wozniakowski, 1991, Traub and Wozniakowski, 1994].", "startOffset": 160, "endOffset": 1410}, {"referenceID": 42, "context": "The latter is closely connected to \u201cherding\u201d algorithms [Welling, 2009].", "startOffset": 56, "endOffset": 71}, {"referenceID": 11, "context": "For example, QMC sequences for n-point rank-one Lattice Rules [Dick et al., 2013] are integral fractions of a lattice defined by a single generating vector v.", "startOffset": 62, "endOffset": 81}, {"referenceID": 8, "context": "For example, QMC sequences for n-point rank-one Lattice Rules [Dick et al., 2013] are integral fractions of a lattice defined by a single generating vector v. This generating vector may be learnt via local minimization of the box discrepancy. Greedy Adaptive Sequences. Starting with S0 = \u2205, for t \u2265 1, let St = {w1, . . . ,wt}. At step t+1, we solve the following optimization problem, wt+1 = argminw\u2208Rd D (St \u222a {w}) . (15) Set St+1 = St \u222a {wt+1} and repeat the above procedure. The gradient of the above objective is also given in (14). Again, we use non-linear conjugate gradient in our experiments (Section 6.2). The greedy adaptive procedure is closely related to the herding algorithm, recently presented by Welling [2009]. Applying the herding algorithm to PWb and p(\u00b7), and using our notation, the points w1,w2, .", "startOffset": 63, "endOffset": 729}, {"referenceID": 7, "context": "Chen et al. [2010] showed that under some additional assumptions, the herding algorithm, when applied to a RKHS H, greedily minimizes \u2016\u03bch,p \u2212 \u03bc\u0302h,p,St\u20162H, which, recall, is equal to Dh,p(St).", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "Chen et al. [2010] showed that under some additional assumptions, the herding algorithm, when applied to a RKHS H, greedily minimizes \u2016\u03bch,p \u2212 \u03bc\u0302h,p,St\u20162H, which, recall, is equal to Dh,p(St). Thus, under certain assumptions, herding and (15) are equivalent. Chen et al. [2010] also showed that under certain restrictions on the RKHS, herding will reduce the discrepancy in a ratio of O(1/t).", "startOffset": 0, "endOffset": 277}, {"referenceID": 1, "context": "Indeed, Bach et al. [2012] recently shown that these restrictions never hold for infinite-dimensional RKHS, as long as the domain is compact.", "startOffset": 8, "endOffset": 27}, {"referenceID": 11, "context": "For the low-discrepancy sequence, we use scrambling and shifting techniques recommended in the QMC literature (see Dick et al. [2013] for details).", "startOffset": 115, "endOffset": 134}, {"referenceID": 1, "context": "Worth mentioning in this regard, is the recent work by Bach [2013], which analyses the connection between Nystr\u00f6m approximations of the Gram matrix, and the regression error, and the work of El Alaoui and Mahoney [2014] on kernel methods with statistical guarantees.", "startOffset": 55, "endOffset": 67}, {"referenceID": 1, "context": "Worth mentioning in this regard, is the recent work by Bach [2013], which analyses the connection between Nystr\u00f6m approximations of the Gram matrix, and the regression error, and the work of El Alaoui and Mahoney [2014] on kernel methods with statistical guarantees.", "startOffset": 55, "endOffset": 220}, {"referenceID": 16, "context": "7 Conclusion and Future Work Recent work on applying kernel methods to very large datasets, has shown their ability to achieve stateof-the-art accuracies that sometimes match those attained by Deep Neural Networks (DNN) [Huang et al., 2014].", "startOffset": 220, "endOffset": 240}, {"referenceID": 31, "context": "The random features approach, originally due to Rahimi and Recht [2007], as emerged as a key technology for scaling up kernel methods [Sindhwani and Avron, 2014].", "startOffset": 134, "endOffset": 161}, {"referenceID": 16, "context": "For example, on TIMIT, a classical speech recognition dataset, over 200,000 random features were used in order to match DNN performance [Huang et al., 2014].", "startOffset": 136, "endOffset": 156}, {"referenceID": 16, "context": "7 Conclusion and Future Work Recent work on applying kernel methods to very large datasets, has shown their ability to achieve stateof-the-art accuracies that sometimes match those attained by Deep Neural Networks (DNN) [Huang et al., 2014]. Key to these results is the ability to apply kernel method to such datasets. The random features approach, originally due to Rahimi and Recht [2007], as emerged as a key technology for scaling up kernel methods [Sindhwani and Avron, 2014].", "startOffset": 221, "endOffset": 391}, {"referenceID": 11, "context": "6 of Dick et al. [2013]. Notice that since supx\u2208Rd h(x,x) < \u221e, we have \u222b Rd h(x,x)p(x)dx < \u221e.", "startOffset": 5, "endOffset": 24}], "year": 2017, "abstractText": "We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large datasets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo (QMC) approximations instead, where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem.", "creator": "LaTeX with hyperref package"}}}