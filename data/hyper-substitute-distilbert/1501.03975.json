{"id": "1501.03975", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2015", "title": "Stochastic Gradient Based Extreme Learning Machines For Online Learning of Advanced Combustion Engines", "abstract": "in this article, a time gradient based online learning interface named extreme learning machines ( sara ) is called ( sg - if ). a diffusion criterion based aka ram approach is used helped understand both asymptotic stability of estimation operators and stability in the estimated parameters suitable for reconstruction of nonlinear analytic systems. the jena algorithm not totally guarantees visibility, but heavily reduces the entropy demand compared to the os - elm approach based applied complex least squares. simulations demonstrating this demonstrate the sensitivity of the algorithm on overall real - world scenario, numerical advanced internal configuration identification problem must considered. proton algorithm first applied a two case studies : an online graph computed for system identification of a homogeneous dynamic compression simulation ( ems ) engine by experimental online classification problem ( ms class factor ) simulation identifying the preferred operating envelope of the engine engine. the conclusions indicate their measurement accuracy of previously proposed sg - ic is comparable to this of the vision - upon - the - horizon wireless costly knowledge facilitating a reduction given computational independence.", "histories": [["v1", "Fri, 16 Jan 2015 13:18:34 GMT  (5914kb,D)", "http://arxiv.org/abs/1501.03975v1", "This paper was written as an extract from my PhD thesis (July 2013) and so references may not be to date as of this submission (Jan 2015). The article is in review and contains 10 figures, 35 references"]], "COMMENTS": "This paper was written as an extract from my PhD thesis (July 2013) and so references may not be to date as of this submission (Jan 2015). The article is in review and contains 10 figures, 35 references", "reviews": [], "SUBJECTS": "cs.NE cs.LG cs.SY", "authors": ["vijay manikandan janakiraman", "xuanlong nguyen", "dennis assanis"], "accepted": false, "id": "1501.03975"}, "pdf": {"name": "1501.03975.pdf", "metadata": {"source": "CRF", "title": "Stochastic Gradient Based Extreme Learning Machines For Online Learning of Advanced Combustion Engines", "authors": ["Vijay Manikandan Janakiraman", "XuanLong Nguyen", "Dennis Assanis"], "emails": ["vijai@umich.edu."], "sections": [{"heading": null, "text": "Index Terms\u2014Stochastic Gradient, Extreme Learning Machines, Online Learning, Online Classification, System Identification, Class Imbalance Learning, Lyapunov Stability, Homogeneous Charge Compression Ignition, Operating Envelope Model, Misfire Prediction, Engine Diagnostics, Engine Control.\nI. INTRODUCTION\nHomogeneous Charge Compression Ignition (HCCI) Engines are of significant interest to the automotive industry owing to their ability to reduce emissions and fuel consumption significantly compared to traditional spark ignition and compression ignition engines [2], [3], [4]. The highly efficient operation of HCCI is achieved using advanced control strategies such as exhaust gas recirculation (EGR) [5], variable valve timings (VVT) [6], intake charge heating [7] among others. Such complex manipulations of the system results in a highly nonlinear behavior [8] with a narrow region of stable operation [9], [10].\nControl of HCCI combustion is a major challenge for automotive application. Several factors contribute to the challenge including the absence of a direct trigger for combustion, narrow operating range and high sensitivity to disturbances. To address the issue, advanced model based control methods\nVijay Manikandan Janakiraman is currently with UARC @ NASA Ames Research Center, Moffett Field, CA, USA. He was previously with the Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA. E-mail: vijai@umich.edu.\nXuanLong Nguyen is with the Department of Statistics, University of Michigan, Ann Arbor, MI, USA.\nDennis Assanis is with the Stony Brook University, NY, USA.\nare common where the control actions are often made using a predictive model of the engine [11], [6], [12]. As alternatives to physics based modeling that might involve significant development time and associated costs, data based approaches were introduced [13], [14], [15] that takes advantage of the extensive experimentation that is performed during the engine calibration process.\nThe key requirement for a model based control of an HCCI engine is the ability to accurately predict the engine state variables for several operating cycles ahead of time, so that a control action with a known effect can be applied to the engine. Further, in order to be vigilant against the engine drifting towards instabilities such as misfire, ringing, knock, etc [16], [17], the operating limits of the engine particularly in transients, is required. In order to develop controllers and operate the engine in a stable manner, both models of the engine operating envelope as well as models of engine state variables are necessary.\nThe state variables of an engine can be defined as the fundamental quantities that represent the state of operation of the engine. As a consequence, these variables also influence the performance of the engine such as fuel efficiency, emissions and stability, and are required to be monitored/regulated. For this work, the net mean effective pressure (NMEP) and the phasing of combustion event (CA50) with respect to the engine\u2019s top dead center [13] are considered representative states that represents the quality of engine operation. More fundamental state variables such as in-cylinder temperature, pressure, chemical composition of combustion mixtures can be considered but these variables cannot be measured feasibly on a production engine.\nThe HCCI engine has a narrow region of stable operation defined by an operating envelope. The dynamic operating envelope of an engine can be defined as a stable region in the operating space of the engine. The significance of the operating envelope and data based modeling approaches are recently introduced by the authors [15]. Knowledge of the operating envelope is crucial for designing efficient controllers for the following reasons. The developer can get insights on the actuator extremes [18], such as the minimum and maximum quantity of fuel to be injected into the engine at a given speed and load conditions. The actuator extremes can then be used to enforce constraints on the control variables for desired engine operation. Furthermore, an operating envelope model could enable designing efficient engine diagnostic systems based on predictive analytics. For instance, a misfire event is\nar X\niv :1\n50 1.\n03 97\n5v 1\n[ cs\n.N E\n] 1\n6 Ja\nn 20\n15\na lack of combustion which produces no work output from the engine. The misfired fuel enters the exhaust system increasing emissions of hydrocarbon and carbon monoxide [19], [20]. When the engine misfires, pollutant levels may be higher than normal. Real time monitoring of the exhaust emission control system and engine misfire detection are essential to meet requirements on On-Board Diagnostic (OBD) regulations. The envelope model can be used to alarm the onboard diagnostics if the engine is about to misfire owing to changes in system or operating conditions.\nData based modeling approaches for the HCCI engine state variables and dynamic operating envelope were demonstrated using neural networks [13], support vector machines [14], extreme learning machines [21] by the authors. However, the previous research considered an offline approach where the data collected from engine experiments were taken offline and models were developed using computer workstations that had high processing and memory. However, a key requirement in advancing the capabilities of data based HCCI modeling task is to perform online learning for the following reasons. The models developed offline are valid only in the controlled experimental conditions. For instance, the experiments are performed at a controlled ambient temperature, pressure and humidity conditions. As a result, the models developed are valid for the specified conditions and a when the models are implemented, for instance, on a vehicle, the expectation is that the model works on a wide range of climatic conditions that the vehicle is exposed to, possibly conditions that were not experimented. Hence, an online adaptation to learn the behavior of the system at new/unfamiliar situations is required. Also, since the offline models are developed directly from experimental data, they may perform poorly in certain operating regions where the density of experimental data is low. As more data becomes available in such regions, an online mechanism can be used to adapt to such data. In addition, the engine produces high velocity streaming data; operating at about 2500 revolutions per minute, an in-cylinder pressure sensor can produce about 1.8 million data observations per day. It becomes infeasible to store this data for offline model development. Thus, an online learning framework that processes every data observation, updates the model and throws away the data is required for advanced engines like HCCI.\nOnline learning algorithms exist for linear and nonlinear models. For combustion engine applications, algorithms involving linear models are common in adaptive control. However, for a system like the HCCI engine, linear models may be insufficient to capture the complex dynamics and the authors showed that nonlinear identification models outperformed linear models, particularly for predicting several steps ahead in time [13]. While numerous techniques for online learning do exist in machine learning literature, a complete survey is beyond the scope of this article. The recent paper on online sequential extreme learning machines (OS-ELM) [1] surveys popular online learning algorithms in the context of classification and regression and develops an efficient algorithm based on recursive least squares. The OSELM algorithm seems to be the present state of the art for classification/regression problems achieving high generaliza-\ntion accuracies, global optimal solution and in quick time. In spite of its known advantages, an over-parameterized ELM suffers from ill-conditioning problem when a recursive least squares type update is performed (as in OS-ELM). This sometimes results in poor regularization behavior [22], [23], [24], [25], which leads to an unbounded growth of the model parameters and unbounded model predictions. If decisions are made as the model is updated (as in case of adaptive control for instance [26]), it is vital for the parameter estimation to be stable so that model based decisions are valid. Hence a guarantee of stability and boundedness is of extreme importance. To address this issue, a stable online learning algorithm based on stochastic gradient descent is developed and stability is proved using Lyapunov stability theory. Although Lyapunov based approaches are popular in control theory, notable prior work for online learning include a Lyapunov approach applied for identification using radial basis function neural networks [27] and GLO-MAP models [28]. The parameter update in such methods involves complex gradient calculation in real time or first estimating a linear model and then estimating a nonlinear difference using orthonormal polynomial basis functions. The approach proposed in this paper aims to retain the simplicity and generalization power of ELM and OS-ELM algorithms, and introduce stability in parameter estimation so that such online models could be used for real-time control purposes.\nThe objective of this article is to develop a stable online learning algorithm for ELM models using stochastic gradients and apply to the HCCI engine modeling problem. The contributions of the paper are as follows. A novel online learning algorithm based on stochastic gradient descent for extreme learning machines is developed. The stability of parameter estimation for dynamic systems is proved using a Lyapunov stbility approach. The application of the stochastic gradient ELM algorithm to the complex HCCI engine identification is the first application (to our best knowledge) of online learning schemes to HCCI engines. This includes both the online state estimation problem as well as the online operating boundary estimation problem.\nThe remainder of the article is organized as follows. The ELM modeling approach is described in Section II along with algorithm details on batch (offline) learning as well as the present state of the art - the OS-ELM. In Section III, the stochastic gradient based ELM algorithm is derived along with stability proof. In Section IV, the background on HCCI engine and experimentation are discussed. Sections V and VI cover the discussions on the application of the SG-ELM algorithm on the two applications, followed by conclusions in Section VII."}, {"heading": "II. EXTREME LEARNING MACHINES", "text": "Extreme Learning Machine (ELM) is an emerging learning paradigm for multi-class classification and regression problems [29], [30]. An advantage of the ELM method is that the training speed is extremely fast, thanks to the random assignment of input layer parameters which do not require adaptation to the data. In such a setup, the output layer parameters can be analytically determined using a least squares approach. Some\nof the attractive features of ELM [29] include the universal approximation capability of ELM, the convex optimization problem of ELM resulting in the smallest training error without getting trapped in local minima, closed form solution of ELM eliminating iterative training and better generalization capability of ELM [30].\nConsider the following data set {(x1, y1), ..., (xN , yN )} \u2208 ( X ,Y ) , (1)\nwhere N denotes the number of training samples, X denotes the space of the input features and Y denotes labels whose nature differentiate the learning problem in hand. For instance, if Y takes integer values {1,2,3,..} then the problem is referred to as classification and if Y takes real values, it becomes a regression problem. ELMs are well suited for solving both regression and classification problems faster than state of the art algorithms [30]. A further distinction could be made depending on the availability of training data during the learning process, as offline learning (or batch learning) and online learning (or sequential learning). Offline learning could make use of all training data simultaneously as all data is available to the algorithm. In addition, as the models are developed offline, efficient use of available computational resources could be made enabling offline algorithms to solve complex optimization problems. Typically, the accuracy of the modeling task takes priority over both computational demand and training time. On the other hand, situations where data is available as high velocity steams where it not feasible to store all data and make inference in quick time, or in situations where the inference is simultaneously made along with adaptation of model to incoming data, online learning is preferred. In an online learning setting, data is available oneby-one and needs to be processed with limited computational effort and storage. Further, inference is required to be made with each new available data along with the ones recorded in the past. In this work, the online setting is considered where a stable online learning algorithm is proposed that is compared with the offline approach and existing online learning method."}, {"heading": "A. Batch (Offline) ELM", "text": "When the entire training data is available and a model is required to be learned using all the training data, batch learning is adopted. In this case, the ELM algorithm involves solving the following optimization problem\nmin W\n{ \u2016HW \u2212 Y \u20162 + \u03bb\u2016W\u20162 } (2)\nHT = \u03c8(WTr x(k) + br) \u2208 Rnh\u00d71, (3)\nwhere \u03bb represents the regularization coefficient, Y represents the vector of outputs or targets, \u03c8 represents the hidden layer activation function (sigmoidal, sinusoidal, radial basis etc [30]) and Wr \u2208 Rn\u00d7nh ,W \u2208 Rnh\u00d7yd represents the input and output layer parameters respectively. Here, n represents the dimension of inputs x(k), nh represents the number of hidden neurons of the ELM model, H represents the hidden layer output matrix and yd represents the dimension of outputs Y . The matrix Wr consists of randomly assigned elements\nthat maps the input vector to a high dimensional feature space while br \u2208 Rnh is a bias component assigned in a random manner similar to Wr. The number of hidden neurons determines the expressive power of the transformed feature space. The elements can be assigned based on any continuous random distribution [30] and remains fixed during the learning process. Hence the training reduces to a single step calculation given by equation (4). The ELM decision hypothesis can be expressed as in equation (5) for classification and equation (6) for regression. It should be noted that the hidden layer and the corresponding activation functions give a nonlinear mapping of the data, which if eliminated, becomes a linear least squares (Linear LS) model and is considered as one of the baseline models in this study.\nW \u2217 = ( HTH + \u03bbI )\u22121 HTY (4)\nf(x) = sgn ( WT [\u03c8(WTr x+ br)] ) . (5)\nf(x) = WT [\u03c8(WTr x+ br)] (6)\nSince training involves a linear least squares solution with a convex objective function, the solution obtained by ELM is extremely fast and is a global optimum for the chosen nh, Wr and br. The above formulation for classification (5), is not designed to handle imbalanced or skewed data sets. As a modification to weigh the minority class data more, a simple weighting method can be incorporated in the ELM objective function (2) as\nmin W\n{ (HW \u2212 Y )T \u0393(HW \u2212 Y ) + \u03bbWTW } (7)\n\u0393 =  \u03b31 0 . . 0 0 \u03b32 . . 0 . . . . 0 0 0 . . \u03b3N  \u03b3i = { 1 majority class data r \u00d7 fs minority class data\n(8)\nwhere \u0393 represents the weight matrix, r represents the ratio of number of majority class data to number minority class data and fs represents a scaling factor to be tuned for a given data set [15]. This results in the training step given by equation (9) and the decision hypothesis takes the same form as in equation (5):\nW \u2217 = ( HT \u0393H + \u03bbI )\u22121 HT \u0393Y. (9)"}, {"heading": "B. Online Sequential ELM (OS-ELM)", "text": "The OS-ELM [1] is a recursive version of the batch ELM algorithm. This version of the algorithm is used for online learning purposes where data is processed one-by-one or chunk-by-chunk and the model parameters are updated after which the used data is not required to be stored. In this process, training involves two steps - initialization step and sequential learning step. During the initialization step, a set of data observations (N0) are required to initialize the H0 and W0 by solving the following optimization problem\nmin W0\n{ \u2016H0W0 \u2212 Y0\u20162 + \u03bb\u2016W0\u20162 } (10)\nH0 = [g(W T r x0 + br)] T \u2208 RN0\u00d7nh . (11)\nThe solution W0 is given by\nW0 = K \u22121 0 H T 0 Y0 (12)\nwhere K0 = HT0 H0 + \u03bbI . Suppose given another new data x1, the problem becomes\nmin W1 \u2225\u2225\u2225\u2225[ H0H1 ] W1 \u2212 [ Y0 Y1 ]\u2225\u2225\u2225\u22252 . (13) The solution can be derived as\nW1 = W0 +K \u22121 1 H T 1 (Y1 \u2212H1W0) K1 = K0 +H T 1 H1.\nBased on the above, a generalized recursive algorithm for updating the least-squares solution can be computed as follows\nMk+1 = Mk \u2212MkHTk+1(I +Hk+1MkHTK+1)\u22121Hk+1Mk (14)\nWk+1 = Wk +Mk+1H T k+1(Yk+1 \u2212Hk+1Wk) (15)\nwhere M represents the covariance of the parameter estimate."}, {"heading": "III. STOCHASTIC GRADIENT BASED ELM ALGORITHM", "text": "In this section, the proposed online learning algorithm using stochastic gradient descent (SGD) is developed for the extreme learning machine models for both classification and regression problems. SGD methods have been popular for several decades for performing online learning but with severe limitations on poor optimization and slow convergence rates. However, only recently, the asymptotic behavior of SGD methods has been analyzed indicating that SGD methods can be very powerful for learning large data sets [31], [32]. SGD based algorithms have been developed for Adaline networks, perceptron models, K-means, SVM and Lasso [31]. In this work, the SGD algorithm is developed for extreme learning machines showing good potential for online learning of high velocity (streaming) data.\nThe justification of SGD based algorithms in machine learning can be briefly discussed as follows. In any learning problem, three types of errors are encountered, namely the approximation error, the estimation error and the optimization error [31], and the expected risk Eexp(f) and the empirical risk Eemp for a supervised learning problemd can be given by\nEexp(f) =\n\u222b l(f(x), y)dP (x, y)\nEemp(f) = 1\nN N\u2211 i=1 l(f(xi), yi)\nLet f\u2217 = argminfEexp(f) be the best possible prediction function. In practice, the prediction function is chosen from a family of parametric functions denoted by F . Let f\u2217F = argminf\u2208FEexp(f) be the best prediction function chosen from a parameterized family of functions F . When a training data set becomes available, the empirical risk becomes a proxy for the expected risk for the learning problem [33]. Let f\u0304\u2217F = argminf\u2208FEemp(f) be the solution that minimizes\nthe empirical risk. However, the global solution is not typically obtained because of computational limitations and hence the solution of the learning problem is reduced to finding f\u0304F = argminf\u2208FEemp(f).\nUsing the above setup, the approximation error (Eapp) is the error introduced in approximating the true function space with a family of functions F , the estimation error (Eest) is the error introduced in optimizing over Eemp(f) instead of Eexp(f), the optimization error (Eopt) is the error induced as a result of stopping the optimization to f\u0304F . The total error Etot can be expressed as\nEapp = Eexp(f \u2217)\u2212 Eexp(f\u2217F ) Eest = Eexp(f \u2217 F )\u2212 Eemp(f\u0304\u2217F ) Eopt = Eemp(f\u0304 \u2217 F )\u2212 Eemp(f\u0304F )\nEtot = Eapp + Eest + Eopt\nThe following observations are taken from the asymptotic analysis of SGD algorithms [31], [34].\n1) The empirical risk Eemp(f) is only a surrogate for the expected risk Eexp(f) and hence an increased effort to minimize Eopt may not translate to better learning. In fact, if Eopt is very low, there is a good chance that the prediction function will over-fit the training data. 2) SGD are worst optimization algorithms (in terms of reducing Eopt) but they minimize the expected risk relatively quickly. Therefore, in the large scale setup, when the limiting factor is computational time rather than the number of examples, SGD algorithms perform asymptotically better. 3) SGD results in a faster convergence when the loss function has strong convexity properties.\nThe last observation is key in developing the algorithm based on ELM models. The ELM models have a squared loss function and when the hidden neurons are randomly assigned and fixed, the training translates to solving a convex optimization problem. Hence the ELM model can be a good candidate to perform SGD type learning and hence the motivation for this study. The SGD based algorithm can be derived for the ELM models as follows."}, {"heading": "A. Algorithm Formulation", "text": "Let (xi, yi) where i = 1, 2, ..N be the streaming data in consideration. The data can be considered to be available to the algorithm from a one-by-one continuous stream or artificially sampled one-by-one from a very large data set. Let the ELM empirical risk be defined as follows\nJ(W ) = min W\n1\n2 N\u2211 i=1 \u2016yi \u2212 \u03c6Ti W\u20162\n= min W\n{ 1\n2 \u2016y1 \u2212 \u03c6T1W\u20162 + ..+\n1 2 \u2016yN \u2212 \u03c6TNW\u20162 } = min\nW {J1(W ) + J2(W ) + ..+ JN (W )} . (16)\nwhere W \u2208 Rnh\u00d7yd , yi \u2208 R1\u00d7yd \u03c6 \u2208 Rnh\u00d7yd is the hidden layer output (see HT in equation (3)). If an error ei \u2208 R1\u00d7yd\ncan be defined as (yi \u2212 \u03c6Ti W ), the learning objective for a data observation i can be given by\nJi(W ) = 1\n2 eTi ei\n= 1\n2 (yi \u2212 \u03c6Ti W )T (yi \u2212 \u03c6Ti W )\n= 1\n2 yTi yi +\n1 2 WT\u03c6i\u03c6 T i W \u2212 yTi \u03c6Ti W\n\u2202Ji \u2202W = \u03c6i\u03c6 T i W \u2212 \u03c6iyi = \u03c6i(\u03c6Ti W \u2212 yi)\n= \u2212\u03c6iei. (17)\nIn a regular gradient descent (GD) algorithm, the gradient of J(W ) is used to update the model parameters as follows.\n\u2202J\n\u2202W = \u2202J1 \u2202W + \u2202J2 \u2202W + ..+ \u2202JN \u2202W\n\u21d2 \u2202J \u2202W = \u2212\u03c61e1 \u2212 \u03c62e2 \u2212 ..\u2212 \u03c6NeN\nWk+1 = Wk \u2212 \u0393SG \u2202J\n\u2202W = Wk + \u0393SG(\u03c61e1) + ..+ \u0393SG(\u03c6NeN ) (18)\nwhere k is the iteration count, \u0393SG \u2208 mathbbRnh\u00d7nh represents the step size or update gain matrix for the GD algorithm.\nIt can be seen from equation (18) that the parameter matrix W is updated based on gradients calculated from all the available examples. If the number of data observations is large, the gradient calculation can take enormous computational effort. The stochastic gradient descent algorithm considers one example at a time and updates W based on gradients calculated from (xi, yi) as shown in\nWi+1 = Wi + \u0393SG(\u03c6iei). (19)\nFrom equation (18), it is clear that the optimal W is a function of gradients calculated from all the examples. As a result, as more data becomes available, W converges close to its optimal value in SGD algorithm. Processing data one-by-one significantly reduces the computational requirement and the algorithm is scalable to large data sets. More importantly, for the online learning of HCCI engine dynamic considered in this work, the SGD algorithm becomes a strong candidate.\nIn order to handle class imbalance learning, the algorithm in (19) can be modified by weighting the minority class data more. The modified algorithm can be expressed as\nWi+1 = Wi + \u0393imb\u0393SG(\u03c6iei) (20)\nwhere \u0393imb = r \u00d7 fs, r and fs represent the imbalance ratio (a running count of majority class data to minority class data until that instant) and the scaling factor that needs to be tuned to obtain tradeoffs between high false positives and missed detections for a given application."}, {"heading": "B. Stability Analysis", "text": "The stability analysis of the SGD based ELM algorithm can be derived as follows. The ELM structure makes the analysis simple and similar to that of a linear gradient based algorithm [35].\nThe instantaneous prediction error ei (Here the error e and output y are transposed as opposed to their previous definition in Section III-A for ease of derivations) can be expressed in terms of parametric error (W\u0303 = W\u2217 \u2212W ) as\nei = yi \u2212WT\u03c6i = WT\u2217 \u03c6i \u2212WT\u03c6i = W\u0303T\u03c6i (21)\nwhere W\u2217 represents true model parameters. Further, the parametric error dynamics can be obtained as follows.\nW\u0303i+1 = W\u2217 \u2212Wi+1 = W\u2217 \u2212Wi \u2212 \u0393SG\u03c6ieTi = W\u0303i \u2212 \u0393SG\u03c6ieTi (22)\nConsider the following positive definite, decrescent and radially unbounded [35] Lyapunov function V\nV (W\u0303 ) = tr(W\u0303T \u0393\u22121SGW\u0303 ) (23)\nwhere tr represents the trace of a matrix.\n\u2206V (W\u0303i) = V (W\u0303i+1)\u2212 V (W\u0303i) = tr(W\u0303Ti+1\u0393 \u22121 SGW\u0303i+1)\u2212 tr(W\u0303 T i \u0393 \u22121 SGW\u0303i)\n= tr((W\u0303i \u2212 \u0393SG\u03c6ieTi )T \u0393\u22121SG(W\u0303i \u2212 \u0393SG\u03c6ie T i ))\n\u2212tr(W\u0303Ti \u0393\u22121SGW\u0303i) = tr(\u22122W\u0303Ti \u03c6ieTi + ei\u03c6Ti \u0393SG\u03c6ieTi ) = tr(\u22122eieTi + ei\u03c6Ti \u0393SG\u03c6ieTi ) = \u22122eTi ei + eTi ei\u03c6Ti \u0393SG\u03c6i = \u22122eTi ei + eTi \u03c6Ti \u0393SG\u03c6iei = \u2212eTi MSGei (24)\nwhere MSG = 2\u2212\u03c6Ti \u0393SG\u03c6i. It can be seen that Vi+1\u2212Vi \u2264 0 if MSG > 0 or 2\u2212 \u03c6Ti \u0393SG\u03c6i > 0 or\n0 < \u03bbmax(\u0393SG) < 2 (25)\nWhen (25) is satisfied, V (W\u0303 ) \u2265 0 is non-increasing in i and the limit\nlim k\u2192\u221e V (W\u0303 ) = V\u221e (26)\nexists. From (24),\nVi+1 \u2212 Vi = \u2212eTi MSGei \u221e\u2211 i=0 (Vi+1 \u2212 Vi) = \u2212 \u221e\u2211 i=0 eTi MSGei\n\u21d2 \u221e\u2211 i=0 eTi MSGei = V (0)\u2212 V\u221e <\u221e (27)\n(28)\nAlso, \u221e\u2211 i=0 eTi Iei \u2264 \u221e\u2211 i=0 eTi MSGei <\u221e (29)\nwhen MSG > I or when\n\u03bbmax(\u0393SG) < 1. (30)\nHence, when (30) is satisfied, ei \u2208 L2. From (19), (Wi+1 \u2212 Wi) \u2208 L2 \u2229 L\u221e. Using discrete time Barbalat\u2019s lemma [36],\nlim i\u2192\u221e ei = 0 (31)\nlim i\u2192\u221e Wi+1 = Wi (32)\nHence, the SGD learning law in (19) guarantees that the estimated output y\u0302i converges to the actual output yi and the model parameters W converge to some constant values. The parameters converge to the true parameters W\u2217 only under conditions of persistence of excitation [35] in input signals of the system (amplitude and frequency richness of x). Further, using boundedness of Vi, ei \u2208 L\u221e which guarantees that the online model predictions are bounded as long as the system output is bounded. As the error between the true model and the estimation model converges to zero, the estimation model becomes a one-step ahead predictive model of the nonlinear system. The evaluation of the SG-ELM algorithm is performed using application to a complex HCCI engine identification problem."}, {"heading": "IV. HOMOGENEOUS CHARGE COMPRESSION IGNITION ENGINE", "text": "The algorithms discussed in Section II are applied to streaming sensory data from a gasoline HCCI engine for demonstrating an online learning framework for HCCI engine modeling. The engine specifications are listed in Table I [13]. A schematic of the experimental setup and instrumentation is shown in Fig. 1. HCCI is achieved by auto-ignition of the gas mixture in the cylinder. The fuel is injected early in the intake stroke and given sufficient time to mix with air forming a homogeneous mixture. A large fraction of exhaust gas from the previous cycle is retained to elevate the temperature and hence the reaction rates of the fuel and air mixture. The variable valve timing capability of the engine enables trapping suitable quantities of exhaust gas in the cylinder.\nThe engine can be controlled using precalculated inputs such as injected fuel mass (FM in mg/cyc), crank angle at intake valve opening (IVO), crank angle at exhaust valve closing (EVC), crank angle at start of fuel injection (SOI). The valve events are measured in degrees after exhaust top dead center (deg eTDC) while SOI is measured in degrees after combustion top dead center (deg cTDC). Other important\nphysical variables that influence the performance of HCCI combustion include intake manifold temperature Tin, intake manifold pressure Pin, mass flow rate of air at intake m\u0307in, exhaust gas temperature Tex, exhaust manifold pressure Pex, coolant temperature Tc, fuel to air ratio (FA) etc. The engine performance metrics are given by combustion phasing indicated by the crank angle at 50% mass fraction burned (CA50), combustion work output given by net indicated mean effective pressure (NMEP, sometimes abbreviated as IMEP). The combustion features calculated using in-cylinder pressure such as CA50, NMEP are determined from the high speed incylinder pressure measurements. For further reading on HCCI combustion and related variables, please refer [37]."}, {"heading": "A. Experiment Design", "text": "In order to identify both models for HCCI state variables as well as models for dynamic operating boundary in transient operation, appropriate experiment design to obtain transient data from the engine is required. The modeled variables such as engine states and operating envelope are dynamic variables and in order to capture both transient and steady state behavior, a set of dynamic experiments is conducted at constant rotational speeds and naturally aspirated conditions (no supercharging/turbocharging) by varying FM, IVO, EVC and SOI in a uniformly random manner. Every input step involves the engine making a transition between two set conditions and the transition (transients or dynamics) is recorded as temporal data. In order to capture several such transients, an amplitude modulated pseudo-random binary sequence (APRBS) has been used to design the excitation signals. APRBS enables exciting the engine at different amplitudes and frequencies suitable for the identification problem considered in this work. The data is sampled using the AVL Indiset acquisition system where in-cylinder pressure is sensed every crank angle using which the combustion features NMEP, CA50 are determined on a per-combustion cycle basis. More details on HCCI combustion and experiments can be found in [13], [14], [15]"}, {"heading": "B. HCCI Instabilities", "text": "A subset of the data collected from the engine is shown in Fig. 2 where it can be observed that for some combinations of the inputs (left figures), the HCCI engine misfires (seen in the right figures where NMEP drops below 0 bar). HCCI operation is limited by several phenomena that lead to undesirable engine behavior. As described in [38], the HCCI operating range is conceptually constrained to a small region of permissible unburned (pre-combustion) and burned (postcombustion) charge temperature states. As previously noted, sufficiently high unburned gas temperatures are required to achieve ignition in the HCCI operating range without which complete misfire will occur. If the resulting combustion cannot achieve sufficiently high burned gas temperatures, commonly occurring in conditions with low fuel to diluent ratios or late combustion phasing, various degrees of quenching can occur resulting in reduced work output and increased hydrocarbon and carbon monoxide emissions. Under some conditions, this\nmay lead to high cyclic variation due to the positive feedback loop existing through the trapped residual gas [16], [17]. Operation with high burned gas temperature, although stable and commonly reached at higher fueling rates where the fuel to diluent ratio is also high, yields high heat release and thus pressure rise rates that may pose challenges for engine noise and durability constraints. A discussion of the temperatures at which these phenomena occur may be found in [38]."}, {"heading": "C. Learning The HCCI Engine Data", "text": "In the HCCI modeling problem, both the inputs and the outputs of the engine are available as sensor measurements and hence supervised learning can be employed. The HCCI engine is a nonlinear dynamic system and sensor measurements represent discrete time sequences. The input-output behavior can be modeled using a nonlinear auto regressive model with exogenous input (NARX) [39] as follows\ny(k) = fNARX [u(k \u2212 1), .., u(k \u2212 nu), y(k \u2212 1), .., y(k \u2212 ny)] (33)\nwhere u(k) \u2208 Rud and y(k) \u2208 Ryd represent the inputs and outputs of the system respectively, k represents the discrete\ntime index, fNARX(.) represents the nonlinear function mapping specified by the model, nu, ny represent the number of past input and output samples required (order of the system) while ud and yd represent the dimension of inputs and outputs respectively. Let x represent the augmented input vector obtained by appending the input and output measurements from the system.\nx = [u(k \u2212 1), .., u(k \u2212 nu), y(k \u2212 1), .., y(k \u2212 ny)]T (34)\nThe input measurement sequence can be converted to the form of training data\n{(x1, y1), ..., (xN , yN )} \u2208 ( X ,Y ) (35)\nwhere N denotes the number of training samples, X denotes the space of the input features (Here X = Rudnu+ydny and Y = R for regression and Y = {+1,\u22121} for a binary classification). The above conversion of system measurements to training data is a natural definition for a series-parallel model architecture and the models can be used for a one-step ahead prediction (OSAP) i.e., given a set of measurements until time index k, the model predicts the output at time k+ 1 (see equation (36)). A parallel architecture on the other hand can be used to perform multiple step ahead predictions\n(MSAP) by feeding back the predictions of the OSAP model in a recurrent manner (see equation (37)). The series-parallel and parallel architectures are well explained in [40].\ny\u0302(k + 1) = f\u0302NARX [u(k), .., u(k \u2212 nu + 1), y(k), .., y(k \u2212 ny + 1)] (36)\ny\u0302(k+npred) = f\u0302NARX [u(k+npred\u22121), .., u(k\u2212nu+npred), y\u0302(k + npred \u2212 1), .., y\u0302(k \u2212 ny + npred)] (37)\nThe OSAP model is used for training as existing simple training algorithms can be used and once the model becomes accurate for OSAP, it can be converted to a MSAP model in a straightforward manner. The MSAP model can be used for making long term predictions useful for predictive control [6], [41], [21]."}, {"heading": "V. APPLICATION CASE STUDY 1: ONLINE REGRESSION LEARNING FOR SYSTEM IDENTIFICATION OF AN HCCI ENGINE.", "text": "As mentioned earlier, a key requirement for model based control of the HCCI engine is the ability to accurately predict the engine state variables for several operating cycles ahead of time, so that a control action with a known impact can be applied to the engine. The state variables of an engine are the fundamental quantities that represent the engine\u2019s state of operation. As a consequence, these variables also influence the performance of the engine such as fuel efficiency, emissions and stability, and are required to be monitored/regulated. In this section, the NMEP and CA50 are considered indicative of engine state variables and are estimated based on control inputs alone, so that the resulting models can be used for\npredictive control. This section details the experiments, model training and validation of the identified models.\nFor the HCCI control oriented modeling, an online regression learning framework is developed. In contrast to the existing linear system identification [6], a nonlinear identification is employed. Typical features of nonlinear identification such as slow convergence and complex parameter update make existing methods practically unsuitable for complex systems. In this work, these shortcomings are eliminated making the approach suitable for the complex HCCI engine problem in hand."}, {"heading": "A. Model Structure and Evaluation Metric", "text": "For the purpose of demonstration, the variables NMEP and CA50 are considered as outputs whereas the control variables such as fueling (FM), exhaust valve closing (EVC) and fuel injection timing (SOI) are considered inputs. Transient data from the HCCI engine at a constant speed of 1800 RPM and naturally aspirated conditions is used. A NARX model as shown in section IV-C is considered where u = [FM EV C SOI]T and y = [NMEP CA50]T , nu and ny chosen as 1 (tuned by trial and error). The nonlinear model approximating fNARX is initialized to an extreme learning machine model with random input layer weights and random values for the covariance matrices and output layer weights. Four different models are considered including the state of the art OS-ELM algorithm, the proposed SG-ELM algorithm, a baseline offline (batch) ELM (O-ELM) and a baseline linear system identification model. The purpose of the baseline offline ELM algorithm is to evaluate the efficiency of the online learning models in learning the HCCI behavior completely as an offline ELM model would do. The offline ELM model is expected to produce an accurate model as it has sufficient time, computation and utilization of all training data simultaneously to learn the HCCI behavior sufficiently well. The purpose of the linear baseline model is to justify the use of a nonlinear model for HCCI dynamics.\nAll the nonlinear models consist of 100 hidden units with fixed randomized input layer parameters. About 11000 cycles of data is considered one-by-one as it is sampled by the engine ECU and model parameters updated in a sequential manner. After the training phase, the parameter update is switched off and the models are evaluated for the next 5100 cycles of data for one step ahead predictions. Further, to evaluate if the learned models represent the actual HCCI dynamics, the multi-step ahead prediction of the models are compared using about 600 cycles of data. It should be noted that both the onestep ahead and multi-step ahead evaluations were done using data unseen during the training phase.\nThe parameters of each of the models are tuned to accurately represent the given dataset. As recommended by OS-ELM [1], about 800 cycles of data was used for initializing the output layer parameters W0 and covariance matrix M0 (see equations (14) and (15)). The initialization was performed using the batch ELM algorithm [30]. In order to have a fair comparison, the W0 is used as an initial condition for both OSELM and SG-ELM. The only parameter of SG-ELM, namely\nthe gradient step size was tuned to be \u0393SG = 0.0008 I100 for best accuracy. This was determined using trial and error and the value of \u0393SG had a significant impact on the prediction accuracy. A detailed analysis on the robustness of \u0393SG is outside the scope of this paper and will be considered for future.\nThe performance of the models are measured using normalized root mean squared error (RMSE) given by\nRMSE = \u221a\u221a\u221a\u221a 1 n n\u2211 i=1 yd\u2211 j=1 (yij \u2212 y\u0302ij)2 (38)\nwhere both yij and y\u0302 i j are normalized to lie between -1 and +1."}, {"heading": "B. Results and Discussion", "text": "On performing online learning, it can be observed from Fig. 3 that the parameters of OS-ELM grow more aggressively as compared to the SG-ELM. In spite of both models having the same initial conditions, the step size parameter \u0393SG for SGELM gives additional control over the parameter growth and keep them bounded as proved in section III-B. On the other hand, OS-ELM doesn\u2019t have any control over the parameter evolution. It is governed by the evolution of the co-variance matrix M (14). It is expected that the co-variance matrix M would add stability to the parameter evolution but in practice, it tends to be more aggressive leading to potential instabilities as reported by [22], [23], [24], [25]. As a consequence, the parameter values for SG-ELM remain small compared to the OS-ELM (the norm of estimated parameters for OS-ELM is 16.64 and SG-ELM is 3.71). This has a significant implication in the statistical learning theory [42]. A small norm of model parameters implies a simpler model which results in good generalization. Although this effect is slightly reflected in the results summarized in prediction results summarized in Table II (see MSAP RMSE for SG-ELM being the lowest), it is not significantly better for this problem possibly because of incomplete convergence. The value of \u0393SG has to be tuned correctly along with sufficient training data in order to ensure parameter convergence. Ultimately, the online learning mechanism is aimed to run along with the engine and hence the slow convergence may not be an issue in a vehicle application.\nThe prediction results as well as training time for the online models are compared in Table II. It can be observed that the computational time for SG-ELM is significantly less (about 4.6 times) compared to OS-ELM indicating the SG-ELM features a faster learning. The reduction in computation is expected to\nbe more pronounced as the dimension and complexity of the data increase. It could be seen from Table II that the one-step ahead prediction accuracies (OSAP RMSE) of the nonlinear models are similar with OS-ELM winning marginally. On the other hand, the multi-step prediction accuracies (MSAP RMSE) are similar for the nonlinear models with SG-ELM performing marginally better. The MSAP accuracy reflect the generalization performance of the model and is more crucial for the modeling problem as the models ultimately feed its prediction to a predictive control framework that requires accurate and robust predictions of the engine several steps ahead of time. From our understanding on model complexity and generalization error, a model that is less complex (indicated by minimum norm of parameters [30], [33]) tend to generalize better, which is again demonstrated by SG-ELM. The performance of the linear baseline model is significantly low compared to the nonlinear models justifying adopting a nonlinear identification for the HCCI engine problem.\nThe MSAP predictions of the models are summarized in Figures 4a-4d where model predictions for NMEP and CA50 are compared against real experimental data. Here the model is initialized using the experimental data at the first instant and allowed to make predictions recursively for several steps ahead. It can be seen that the nonlinear models outperform the linear model and at the same time the online learning models perform similar to the offline trained models indicating that online learning can fully identify the engine behavior at the operating condition where the data is collected. It should be noted that this task is a case of multi-input multioutput modeling which adds some limitations to the SG-ELM methods. When the model complexity increases, the SG-ELM require more excitations for convergence, as opposed to OSELM which converges more aggressively (although at the loss of stability). Further, the tuning of gradient step size \u0393SG may be time-consuming for systems predicting multiple outputs with different noise characteristics. The OS-ELM on the other hand is much more elegant as there are no parameters to be tuned once properly initialized."}, {"heading": "VI. APPLICATION CASE STUDY 2: ONLINE CLASSIFICATION LEARNING (WITH CLASS IMBALANCE)", "text": "FOR IDENTIFYING THE DYNAMIC OPERATING ENVELOPE OF AN HCCI ENGINE\nThe problem considered in this case study is to develop a predictive model of the dynamic operating envelope of the HCCI engine. For developing stable model based controller for HCCI engines, it is necessary to prevent the engine drifting towards instabilities such as misfire, ringing, knock, etc [16], [17]. To this end, a dynamic operating envelope of the HCCI engine was developed using machine learning models [15]. However, the modeling was performed offline. In this paper, an online learning framework for modeling the operating envelope of HCCI engine is developed using both OS-ELM and SG-ELM algorithms.\nIn this paper, the operating envelope defined by two common HCCI unstable modes - a complete misfire and a high variability combustion (a more detailed description is given\nin section IV-B) is studied. The problem of identifying the HCCI operating envelope using experimental data can be posed as a classification problem. The engine sensor data can be manually labeled as stable or unstable depending on engine based heuristics. Further, the engine dynamic data consists of a large number of stable class data compared to unstable class data, which introduces an imbalance in class proportions. As a result, the problem can be posed as a class imbalance learning of a binary classification decision boundary. For class imbalance learning, a cost-sensitive approach that modifies the objective function of the learning system to weigh the minority class data more heavily, is preferred over under-sampling and over-sampling approaches [15].\nOnline learning algorithms using OS-ELM, SG-ELM are compared for classification performance. The above nonlinear models are compared against a baseline linear classification model and an offline trained nonlinear ELM model to make similar justifications as in the previous case study. The linear baseline model is included to justify the benefits of adopting a nonlinear model while the offline trained model is included to show the effectiveness of online algorithms in capturing the underlying behavior."}, {"heading": "A. Model Structure and Evaluation Metric", "text": "The HCCI operating envelope is a function of the engine control inputs and engine physical variables such as temperature, pressure, flow rate etc. Also, the envelope is a dynamic system and so a predictive model requires the measurement history up to an order of Nh. The dynamic classifier model can be given by\ny\u0302k+1 = sgn(f(xk)) (39)\nwhere sign(.) represents the sign function, y\u0302k+1 indicates model prediction for the future cycle k + 1, f(.) can take any structure depending on the learning algorithm and xk is given by\nxk = [IV O,EV C, FM,SOI, Tin, Pin, m\u0307in,\nTex, Pex, Tc, FA,NMEP,CA50] T (40)\nat cycle k upto cycle k \u2212 Nh + 1. In the following sections, the function f(.) is learned using the available engine experimental data using the two online ELM algorithms. The engine measurements and their time histories (defined by xk) are considered inputs to the model while the stability labels are considered outputs. The feature vector is of dimension n=39 includes sensor measurements such as FM, IVO, EVC, SOI, Tc, Tin, Pin, m\u0307in, Tex, Pex, NMEP, CA50 and FA along with Nh = 1 cycles of history (see (40)). The engine\nexperimental data is split into training and testing sets. The training set consists of about 14300 cycles of data processed one-by-one as sampled by the engine ECU. After the training phase, the parameter update is switched off and the models are evaluated for the next 6200 cycles of data for one step ahead classification. The ratio of number of majority class data to number minority class data (r) for the training set is about 4.5:1 and for the testing set is 9:1. The nonlinear model approximating f(.) is initialized to an extreme learning machine model with random input layer weights and random values for the covariance matrices and output layer weights. All the nonlinear models consist of 10 hidden units with fixed randomized input layer parameters. Similar to the previous case study, a small portion of the training data is used to initialize the ELM model parameters as well as the covariance matrix. The SG-ELM parameter \u0393SG is tuned to be 0.001 I10 using trial and error. A weighted classification version of the algorithms is developed to handle the class imbalance problem. The minority class data is weighted higher by r times fs where r is the imbalance ratio of the training data and is computed online as the ratio of the number of majority class to number of minority class data until that instant. For the class imbalance problem considered here, a conventional classifier metric like the overall misclassification rate cannot be used as it would find a biased classifier, i.e., it would find a classifier that ignores the minority class data. For instance, a data set that has 95% of majority class data (with label +1) would achieve 95% classification accuracy by predicting all the labels to be +1 which is obviously undesirable. Hence the following evaluation metric used for skewed data sets is considered. Let TP and TN represent the total number of positive and negative class data classified correctly by the classifier. If N+ and N\u2212 represent the total number of positive and negative class data respectively, the true positive rate (TPR) and true negative rate (TNR), geometric mean (GM) of TPR and TNR, and the total accuracy\n(TA) of the classifier can be defined as follows [43]. It should be noted that the total accuracy and geometric mean weights the accuracy of majority and minority classes equally, i.e., they have high values only when both classes of data are classified correctly.\nTPR = TP\nN+\nTNR = TN\nN\u2212\nGM = \u221a TPR\u00d7 TNR\nTA = 0.5(TPR+ TNR). (41)"}, {"heading": "B. Results and Discussion", "text": "The results of online imbalance classification can be summarized in Table III where computational time as well as classification performance can be compared. It can be observed that the developed classification models perform well for the HCCI boundary identification problem (see average accuracies of all models are above 80%). The problem is mildly nonlinear as linear models achieve similar accuracies as that of their nonlinear counterparts. Both OS-ELM and SG-ELM perform well and achieve results similar to an offline model indicating completeness of learning. The SG-ELM has a slight advantage in terms of computational efficiency. The algorithm is simple and requires about half of the time required to train an OSELM model. Further, for the considered classification problem, the prediction accuracy of SG-ELM is slightly better than OSELM indicating the suitability of SGD based online learning for the HCCI problem. A subtle advantage observed for the OS-ELM is that, although the combined accuracy is slightly inferior to that of the SG-ELM, the accuracies of the positive examples and negative examples are very close to each other indicating that the model is well balanced to predict both majority class as well as minority class data well. The SGELM on the other hand, in spite of fine-tuning the parameters, fails to achieve this. A further tuning can be done to improve the accuracy of a particular class of data, typically sacrificing some accuracy predicting the other. The predictions of the online SG-ELM model is shown in Fig. 5.\nThe models developed using OS-ELM and SG-ELM algorithms are used to make predictions on unseen engine inputs and class predictions are summarized in Fig. 5, while quantitative results are included in Table III. As mentioned earlier, the operating envelope is a decision boundary in the input space within which any input operates the HCCI in\na stable manner and any input outside the envelope might operate the engine in an unstable manner. The HCCI state variables such as NMEP, CA50 and engine sensor observations such as Tin, Pin, m\u0307in, Tex, Pex, Tc at time instant k, along with engine control inputs such as FM, EVC, SOI at time instant k+ 1, are given as input to the models (see (40)). The model predictions at time k + 1 are obtained. The engine\u2019s actual response at time k+ 1 is also recorded. A data point is marked in red if the model predicts the engine operation to be unstable (-1) while it is marked in green if the model predicts the data point to be stable (+1). In the figures, a dotted line in the NMEP plot indicates the misfire limit, a dotted ellipse in CA50 plot indicates high variability instability mode while a dotted rectangle indicates misclassified predictions by model. To understand the variation of NMEP and CA50 with changes in control inputs, the fueling input (abbreviated as FM) is also included in the plots. It should be understood that FM is not the only input for prediction and the signals are defined as in equation (40) but only the fueling input is shown in the plots owing to space constraints.\nIt can be seen from the above plots that as a whole, both OS-ELM and SG-ELM models classify the HCCI engine data fairly well in spite of the high amplitude noise inherent in the HCCI experimental data. The data consists of step changes in FM, EVC and SOI and whenever a \u2018bad\u2019 combination of inputs is chosen, the engine either misfires completely (see NMEP fall below misfire limit) or exhibits high variability combustion (see dotted ellipses). The goal of this work as stated previously, is to predict if a future HCCI combustion event is stable or unstable based on available measurements. The results summarized in Table III indicates that the developed models indeed accomplished the goal with a reasonable accuracy. From Fig. 5, it is observed that the OS-ELM has some clear misclassifications in predicting stable class data (see dotted rectangles in the plots) while this is not observed for SGELM. This is not surprising as the true positive rate of OSELM model is much lesser compared to that of SG-ELM (see Table III). On the other hand, the SG-ELM has an inferior accuracy in predicting the unstable modes but is not clearly evident in the data sets used in Fig. 5 ."}, {"heading": "VII. CONCLUSION", "text": "A stochastic gradient descent based online learning algorithm for ELM has been developed, that guarantees stability in parameter estimation suitable for control purposes. Further, the SG-ELM demands less computation compared to the OS-ELM algorithm, as the covariance estimation step is eliminated. A stability proof is developed based on Lyapunov approach. However, the SG-ELM algorithm might involve tedious tuning of step-size parameter as well as suffer from slow convergence.\nThe SG-ELM and OS-ELM algorithms are applied to develop models for state variables and dynamic operating envelope of a HCCI engine to assist in model based control. The results from this article suggest that good generalization performance can be achieved using both OS-ELM and SGELM methods but the SG-ELM might have an advantage in terms of stability, crucial for designing robust control systems.\nAlthough the SG-ELM appears to perform well in the HCCI identification problem, a comprehensive analysis and evaluation on several benchmark data sets is required and will be considered for future. From an application perspective, interesting areas for exploration include implementing the algorithm in real-time hardware, exploring a wide operating range of HCCI operation and development of controllers."}, {"heading": "ACKNOWLEDGMENT", "text": "This material is based upon work supported by the Department of Energy and performed as a part of the ACCESS project consortium (Robert Bosch LLC, AVL Inc., Emitec Inc.) under the direction of PI Hakan Yilmaz, Robert Bosch, LLC. X. Nguyen is supported in part by NSF Grants CCF1115769 and ACI-1047871.\nDISCLAIMER\nThis report was prepared as an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "In this article, a stochastic gradient based online<lb>learning algorithm for Extreme Learning Machines (ELM) is<lb>developed (SG-ELM). A stability criterion based on Lyapunov<lb>approach is used to prove both asymptotic stability of estima-<lb>tion error and stability in the estimated parameters suitable<lb>for identification of nonlinear dynamic systems. The developed<lb>algorithm not only guarantees stability, but also reduces the<lb>computational demand compared to the OS-ELM approach<lb>[1] based on recursive least squares. In order to demonstrate<lb>the effectiveness of the algorithm on a real-world scenario, an<lb>advanced combustion engine identification problem is considered.<lb>The algorithm is applied to two case studies: An online regression<lb>learning for system identification of a Homogeneous Charge<lb>Compression Ignition (HCCI) Engine and an online classification<lb>learning (with class imbalance) for identifying the dynamic<lb>operating envelope of the HCCI Engine. The results indicate<lb>that the accuracy of the proposed SG-ELM is comparable to<lb>that of the state-of-the-art but adds stability and a reduction in<lb>computational effort.", "creator": "LaTeX with hyperref package"}}}