{"id": "1501.05882", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2015", "title": "Efficient local search limitation strategy for single machine total weighted tardiness scheduling with sequence-dependent setup times", "abstract": "this case demonstrated the single linear resource weighted tardiness cases with sequence - timing setup times, usually calculated ~ $ 1 | s _ { ij } | \\ sum \\ _ jt _ j $. [ parallel $ \\ mathcal { np } $ - averaging problem, each job has considerable expected processing time, one date beyond a weight. for each pair of orders $ 1a $ and $ j $, there may also possible random time delayed starting selection process $ j $ negative case priority probability is obtained not after $ i $. the director is manually determine a schedule that minimizes corresponding total weighted value, where arrival date of it job comes identical to its reporting time minus its due date, better case the allocation being completely placed only below its due moment, and eventually equal to zero otherwise. exhibiting largely its complexity, this problem is little adequately researched using heuristics. her aim of constructing one is to develop a strict yet effective limitation aiming that filling up the fixed search strategy saves a significant loss in computing optimal quality. such strategy detection of predictable filtering mechanism normally prevents unpromising procedure to render correct. the selective strategy has insights embedded in a model population based framework from the competition and tested comparing classical statistical instances. computational experiments revealed while each limitation had provided the metaheuristic procedures be extremely competitive when compared to other algorithms since the literature, which it allowed accurate placement of a large number of regulatory structures guaranteed a significant increase in the target process and, subsequently, high quality performances could improve achieved in extra matter of awhile.", "histories": [["v1", "Fri, 23 Jan 2015 17:20:50 GMT  (85kb)", "https://arxiv.org/abs/1501.05882v1", "26 pages, 4 figures"], ["v2", "Mon, 26 Jan 2015 02:41:26 GMT  (85kb)", "http://arxiv.org/abs/1501.05882v2", "26 pages, 4 figures"], ["v3", "Mon, 30 Nov 2015 21:13:24 GMT  (93kb)", "http://arxiv.org/abs/1501.05882v3", "32 pages, 4 figures"]], "COMMENTS": "26 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["anand subramanian", "katyanne farias"], "accepted": false, "id": "1501.05882"}, "pdf": {"name": "1501.05882.pdf", "metadata": {"source": "CRF", "title": "Efficient local search limitation strategy for single machine total weighted tardiness scheduling with sequence-dependent setup times", "authors": ["Anand Subramanian", "Katyanne Farias"], "emails": ["anand@ct.ufpb.br,", "katyannefaraujo@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n05 88\n2v 3\n[ cs\n.A I]\n3 0\nThis paper concerns the single machine total weighted tardiness scheduling with sequence-dependent\nsetup times, usually referred as 1|sij | \u2211\nwjTj . In this NP-hard problem, each job has an associated processing time, due date and a weight. For each pair of jobs i and j, there may be a setup time before starting to process j in case this job is scheduled immediately after i. The objective is to determine a schedule that minimizes the total weighted tardiness, where the tardiness of a job is equal to its completion time minus its due date, in case the job is completely processed only after its due date, and is equal to zero otherwise. Due to its complexity, this problem is most commonly solved by heuristics. The aim of this work is to develop a simple yet effective limitation strategy that speeds up the local search procedure without a significant loss in the solution quality. Such strategy consists of a filtering mechanism that prevents unpromising moves to be evaluated. The proposed strategy has been embedded in a local search based metaheuristic from the literature and tested in classical benchmark instances. Computational experiments revealed that the limitation strategy enabled the metaheuristic to be extremely competitive when compared to other algorithms from the literature, since it allowed the use of a large number of neighborhood structures without a significant increase in the CPU time and, consequently, high quality solutions could be achieved in a matter of seconds. In addition, we analyzed the effectiveness of the proposed strategy in two other well-known metaheuristics. Further experiments were also carried out on benchmark instances of problem 1|sij | \u2211 Tj ."}, {"heading": "1 Introduction", "text": "This paper deals with the single machine total weighted tardiness scheduling with sequence-\ndependent setup times, a well-known problem in the scheduling literature, which can be\ndefined as follows. Given a set of jobs J = {1, . . . , n} to be scheduled on a single ma-\nchine, for each job j \u2208 J , let pj be the processing time and dj be the due date with a non-negative weight wj. Also, consider a setup time sij that is required before starting to process job j \u2208 J in case this job is scheduled immediately after job i \u2208 J . The objective is to determine a schedule that minimizes the total weighted tardiness \u2211\nwjTj , where the\ntardiness Tj of a job j \u2208 J depends on its associated completion time Cj and is given by max{Cj \u2212 dj , 0}. Based on the notation proposed in Graham et al. (1979) we will hereafter denote this problem as 1|sij| \u2211 wjTj.\nA special version of problem 1|sij| \u2211 wjTj arises when setup times are not considered.\nSuch version is usually referred to as 1|| \u2211\nwjTj , which is known to be NP-hard in a\nstrong sense Lawler (1977); Lenstra et al. (1977). Therefore, problem 1|sij| \u2211 wjTj is also NP-hard since it includes problem 1|| \u2211\nwjTj as a particular case.\nGiven the complexity of problem 1|sij| \u2211 wjTj , most methods proposed in the literature are based on (meta)heuristics. In particular, local search based metaheuristics had been\nquite effective in generating high quality solutions for this problem (Kirlik and Og\u0306uz, 2012;\nXu et al., 2013; Subramanian et al., 2014). Nevertheless, one of the main limitations of\nsuch methods is the computational cost of evaluating a move during the local search.\nTypically, the number of possible moves in classical neighborhoods such as insertion and swap is O(n2), whereas the complexity of evaluating each move from these neighborhoods\nis O(n), when performed in a straightforward fashion. Hence, the overall complexity of examining these neighborhoods is O(n3).\nRecently, Liao et al. (2012) presented a sophisticated method that reduces the com-\nplexity of enumerating and evaluating all moves from the aforementioned neighborhoods from O(n3) to O(n2logn). However, in practice, this procedure is only useful for very\nlarge instances. For example, they showed empirically that the advantage of using their\napproach for the neighborhood swap starts to be visibly significant only for instances with\nmore than around 1000 jobs.\nThe aim of this work is to develop a simple yet effective strategy that speeds up the local\nsearch procedure without a significant loss in the solution quality. Such strategy consists of\na filtering mechanism that prevents unpromising moves to be evaluated. The idea behind\nthis approach relies on a measurement that must be computed at runtime. In our case,\nwe use the setup variation, which can be computed in O(1) time to estimate if a move is\npromising or not to be evaluated. Moreover, the bottleneck of traditional implementations\nlimits the number of neighborhoods to be explored in the local search. Our proposed\nstrategy enables the use of a large number of neighborhoods, such as moving blocks of\nconsecutive jobs, which is likely to lead to high quality solutions. In fact, Xu et al. (2013)\nshowed that this type of neighborhood may indeed lead to better solutions for problem 1|sij| \u2211 wjTj .\nThe proposed local search strategy has been tested in classical benchmark instances\nusing the ILS-RVND metaheuristic (Subramanian, 2012). Computational experiments\nrevealed that the limitation strategy enabled the metaheuristic to be extremely competi-\ntive when compared to other algorithms from the literature, since it allowed the use of a\nlarge number of neighborhood structures without a significant increase in the CPU time\nand, consequently, high quality solutions could be achieved in a matter of seconds. In\naddition, we analyzed the effectiveness of the proposed strategy in two other well-known\nmetaheuristics. Further experiments were also carried out on benchmark instances of problem 1|sij| \u2211 Tj .\nThe remainder of the paper is organized as follows. Section 2 presents a review of the\nalgorithms proposed in the literature for solving the problem 1|sij| \u2211 wjTj . Section 3 describes the proposed local search methodology, including the neighborhood structures and\nthe new limitation strategy. Section 4 presents the computational experiments. Section\n5 contains the concluding remarks of this work."}, {"heading": "2 Literature Review", "text": "One of the first methods proposed for problem 1|sij| \u2211 wjTj was that of Raman et al. (1989). It consists of a constructive heuristic based on a static dispatching rule. Lee et al.\n(1997) later developed a three-phase method, where the first performs a statistical analysis\non the instance to define the parameters to be used, the second is a constructive heuristic\nthat is based on dynamic dispatching rule called Apparent Tardiness Cost with Setups\n(ATCS), while the third performs a local search by means of insertion and swap moves.\nThe 1|sij| \u2211 wjTj literature remained practically unchanged for nearly two decades until Cicirello and Smith (2005) developed five randomized based (meta)heuristic ap-\nproaches, more precisely, Limited Discrepancy Search (LDS); Heuristic-Biased Stochastic\nSampling (HBSS); Value-Biased Stochastic Sampling (VBSS); hill-climbing incorporated\nto VBSS (VBSS-HC) and finally a Simulated Annealing (SA) algorithm. The authors\nalso proposed a set of 120 instances that has become the most used benchmark dataset\nfor the problem. Cicirello (2006) later implemented a Genetic Algorithm (GA) with a\nnew operator called Non-Wrapping Order Crossover (NWOX) that maintains not only\nthe relative order of the jobs in a sequence, but also the absolute one.\nThree metaheuristics were implemented by Lin and Ying (2007), more specifically,\nSA, GA and Tabu Search (TS), whereas Ant Colony Optimization (ACO) based algo-\nrithms were put forward by Liao and Cheng (2007), Anghinolfi and Paolucci (2008) and\nMandahawi et al. (2011). The latter authors actually developed a variant of ACO, called\nMax-Min Ant System (MMAS), that was capable of generating better solutions when\ncompared to the other two.\nValente and Alves (2008) developed a Beam Search (BS) algorithm, while a Discrete\nParticle Swarm Optimization (DSPO) heuristic was proposed by Anghinolfi and Paolucci\n(2009). Tasgetiren et al. (2009) put forward a Discrete Differential Evolution (DDE) al-\ngorithm that was enhanced by the NEH constructive heuristic of Nawaz et al. (1983)\ncombined with concepts of the metaheuristic Greedy Randomized Adaptive Search Pro-\ncedure (GRASP) (Feo and Resende, 1995) and some priority rules such as the Earliest\nWeighted Due Date (EWDD) and ATCS. The authors also implemented destruction and\nconstruction procedures to determine a mutant population. Boz\u0307ejko (2010) proposed\na parallel Scatter Search (ParSS) algorithm combined with a Path-Relinking scheme.\nChao and Liao (2012) implemented a so-called Discrete Electromagnetism-like Mecha-\nnism (DEM), which is a metaheuristic based on the electromagnetic theory of attraction\nand repulsion.\nKirlik and Og\u0306uz (2012) developed a Generalized Variable Neighborhood Search (GVNS)\nalgorithm combined with a Variable Neighborhood Descent (VND) procedure for the local\nsearch composed of the following neighborhoods: swap, 2-block insertion and job (1-block)\ninsertion. Xu et al. (2013) suggested an Iterated Local Search (ILS) approach that uses a\nl-block insertion neighborhood structure with l \u2208 {1, 2, . . . , 18}. The authors empirically\nshowed that this neighborhood was capable of finding better solutions when compared to\njob insertion and 2-block insertion. The perturbation mechanism also applies the same\ntype of move, but with l \u2208 {18, 19, . . . , 30}.\nSubramanian et al. (2014) also implemented an ILS algorithm, but combined with a\nRandomized VND procedure (RVND) that uses the neighborhoods insertion, 2-block in-\nsertion, 3-block insertion, swap and block reverse (a.k.a. twist). Diversification moves\nare performed using the double-bridge perturbation (Martin et al., 1991), which was orig-\ninally developed for the Traveling Salesman Problem (TSP). An alternative version of\nthe proposed ILS-RVND algorithm that accepts solutions with same cost during the local\nsearch was suggested. In this latter approach, a tabu list is used to avoid cycling.\nDeng and Gu (2014) put forward an Enhanced Iterated Greedy (EIG) algorithm that\ngenerates an initial solution using the ATCS heuristic, performs local search with swap\nand insertion moves and perturbs a solution by applying successive insertion moves. Some\nelimination rules were also suggested for the swap neighborhood.\nXu et al. (2014) proposed different versions of a Hybrid Evolutionary Algorithm (HEA)\nby combining two population updating strategies with three crossover operators, including\nthe linear order crossover operator (LOX). The initial population is generated at random\nand a local search is applied using the neighborhood l-block as in Xu et al. (2013). The\nversion that yielded the best results was denoted LOX\u2295B. Guo and Tang (2015) suggested\na Scatter Search (SS) based algorithm that combines many ideas from different methods\nsuch as ATCS, VNS and DDE with a new adaptive strategy for updating the reference\nset.\nTo the best of our knowledge, the ILS-RVND heuristic of Subramanian et al. (2014),\nthe ILS of Xu et al. (2013) and the HEA of Xu et al. (2014) are the best algorithms, at least in terms of solution quality, proposed for problem 1|sij| \u2211 wjTj .\nTanaka and Araki (2013) proposed an exact algorithm, called Successive Sublimation\nDynamic Programming (SSDP), that was capable of solving instances of the benchmark\ndataset of Cicirello and Smith (2005). At first, a column generation or a conjugate subgra-\ndient procedure is applied over the lagrangean relaxation of the original problem, solved\nby dynamic programming. Next, some constraints are added to the relaxation until there\nis no difference between the lower and upper bounds, which increases the number of\nstates in the dynamic programming. Unnecessary states are removed in order to decrease\nthe computational time and the memory used. A branching scheme is integrated to the\nmethod to solve the harder instances. Despite capable of solving all instances to opti-\nmality, the computational time was, on average, rather large, more specifically 32424.55\nseconds, varying from 0.54 seconds to 30 days.\nTable 1 shows a summary of the methods proposed for problem 1|sij| \u2211 wjTj. For the sake of comparison we also included, when applicable, the neighborhoods used in each\nmethod. It can be observed that most algorithms used the neighborhoods insertion and\nswap. Moreover, it is interesting to notice that almost all population based algorithms\nrely on local search at a given step of the method."}, {"heading": "3 Local Search Methodology", "text": "In this section we describe in detail the local search methodology adopted in this work.\nAt first, we introduce the proposed local search limitation strategy used to speed up the\nlocal search process. Next, we present the neighborhood structures considered in our\nimplementation and how we compute the move evaluation using a block representation.\nFinally, we show how we have embedded the developed approach into the ILS-RVND\nmetaheuristic (Subramanian, 2012)."}, {"heading": "3.1 Limitation Strategy", "text": "Enumerating and evaluating all moves of a given neighborhood of a 1|sij| \u2211 wjTj solution are usually very time consuming, often being the bottleneck of local search based\nalgorithms proposed for this problem. Liao et al. (2012) developed a rather complicated procedure that runs in O(n2logn) time for preprocessing the auxiliary data stuctures nec-\nessary for performing the move evaluation of the neighborhoods swap, insertion and block\nreverse in constant time. Nevertheless, they themselves showed, for example, that for\nthe neighborhood swap, their approach only start to be notably superior for instances\ncontaining more than \u2248 1000 jobs. Therefore, it was thought advisable to use the simple\nand straightforward move evaluation procedures in Section 3.2, even though their resulting overall complexity is O(n3). However, instead of enumerating all possible moves from\neach neighborhood, we decided to evaluate only a subset of them, selected by means of a\nnovel local search limitation strategy.\nOur proposed local search limitation strategy is based on a very simple filtering mech-\nanism that efficiently chooses the neighboring solutions to be evaluated during the search.\nThe criterion used to decide weather a move should be evaluated or not is based on the\nsetup variation of that move, which can be computed in O(1) time. In other words, for\nevery potential move, one computes the setup variation and the move is only considered\nfor evaluation if the variation does not exceed a given threshold value. The motivation\nfor adopting such criterion was based on the empirical observation that the larger the\nincrease on the total setup of a sequence due to a move, the smaller the probability of\nimprovement on the total weighted tardiness.\nIt is worthy of note that, very recently, Guo and Tang (2015) had independently de-\nveloped a similar but not identical approach to disregard unpromising moves based on\nthe total setup time instead of the setup variation. More specifically, moves are not con-\nsidered for evaluation if the total setup time of a solution to be evaluated is greater than\na threshold value that is computed by multiplying the average setup of the instance by\na random number selected from the interval between 0.2 and 0.3. However, they did not\nreport any result of the impact of this strategy on the performance of the local search.\nLet N be the set of neighborhoods used in a local search algorithm. A threshold value\nmax\u2206sv for the setup variation is assigned for each neighborhood v \u2208 N , meaning that a move from a neighborhood v \u2208 N will only be evaluated if its associated setup variation\nis smaller than or equal to max\u2206sv . Since an adequate value for each max\u2206sv may be highly sensitive to the instance data as well as the neighborhood, we did not impose any\npredetermined value for them. Instead, they are estimated during a learning phase, when\na preliminary local search is performed without any filter.\nThe learning phase occurs during the first iterations of the local search. Initially,\nmax\u2206sv, \u2200v \u2208 N , is set to a sufficiently large number M so as to allow the evaluation of any move, i.e., no filter is applied at this stage. Let \u2206sv, \u2200v \u2208 N , be a list composed of the values of the setup variations associated to each improving move of a particular\nneighborhood. This list is updated with the addition of a new value every time an\nimproving move occurs.\nAt the end of the learning phase, the list \u2206sv, \u2200v \u2208 N , is sorted in ascending order. Define \u03b8 as an input parameter, where 0 \u2264 \u03b8 \u2264 1. The element from this ordered list\nassociated to the position \u230a\u03b8 \u00b7 |\u2206sv|\u230b is then chosen as threshold value for the parameter max\u2206sv, \u2200v \u2208 N . Note that the larger the \u03b8, the larger the max\u2206sv and thus more moves are likely to be evaluated, leading to a more conservative limitation policy.\nConsider an example where \u2206sswap = [\u22126,\u22124,\u22124,\u22122, 0, 1, 4, 7, 12, 20]. If \u03b8 = 0.95, then \u230a\u03b8 \u00b7 |\u2206sv|\u230b = \u230a0.95 \u00b7 10\u230b = 9. The value of the parameter max\u2206sswap will thus be the one associated with the position 9 of \u2206sswap, that is, max\u2206sswap = 12.\nNote that one need not necessarily perform an exhaustive local search, i.e. enumerate\nall possibles moves from a neighborhood during the learning phase. What is really relevant\nis the size of the list \u2206sv. On one hand, if the size of the sample (|\u2206sv|) is too small and not really representative, then an inaccurate value for max\u2206sv will be estimated. On the other hand, storing a large sample may imply in spending a considerable amount\nof iterations in the learning phase, which can dramatically affect the performance of the\nalgorithm in terms of CPU time.\nThe presented limitation strategy can be easily embedded into any local search based\nmetaheuristic. For example, in case of multi-start metaheuristics such as GRASP, one can\nconsider the learning phase (i.e., perform the search without any filters) only for a given\nnumber of preliminary iterations before triggering the limitation scheme. In case of other\nmetaheuristics that systematically alternate between intensification and diversification\nsuch as TS, VNS and ILS, one can choose different stopping criteria for the learning\nphase such as the number of calls to the local search procedure or even a time limit.\nPopulation based metaheuristics that rely on local search for obtaining good solutions\ncan also employ a similar scheme."}, {"heading": "3.2 Neighborhood structures", "text": "In order to better describe the neighborhood structures employed during the local search\nwe use the following block representation. Let \u03c0 = {\u03c00, \u03c01, \u03c02, . . . , \u03c0n} be an arbitrary sequence composed of n jobs, where \u03c00 = 0 is a dummy job that is used for considering the setup s0\u03c01 required for processing the first job of the sequence. A given block B can be defined as a subsequence of consecutive jobs. Figure 1 shows an example of a sequence of\n10 jobs (plus the dummy job) divided into 4 blocks, namely B0 = \u03c00, B1 = {\u03c01, \u03c02, \u03c03, \u03c04}, B2 = {\u03c05, \u03c06, \u03c07} and B3 = {\u03c08, \u03c09, \u03c010}.\nLet a and b be the position of the first and last jobs of a block Bt in the sequence, respectively, and let b\u2032 be the position of the last job of the predecessor block Bt\u22121 in the sequence with associated completion time C b\u2032. The cost of the block Bt, i.e., its total weighted tardiness, can be computed in O(|Bt|) steps as shown in Alg. 1. Note that if |Bt| = 1, the procedure will not execute the loop from a + 1 to b described in lines 7-12\nbecause in this case a = b which implies in b < a + 1.\nAlgorithm 1 CompCostBlock\n1: Procedure CompCostBlock(b\u2032, a, b, \u03c0, C b\u2032) 2: cost \u2190 0 3: Ctemp \u2190 C b\u2032 + s\u03c0\nb\u2032 \u03c0a + p\u03c0a \u22b2 Global variable that stores a temporary completion time\n4: if Ctemp > d\u03c0a then \u22b2 d\u03c0a is the due date of job \u03c0a 5: cost\u2190 w\u03c0a \u00d7 (Ctemp\u2212 d\u03c0a ) \u22b2 w\u03c0a is the weight of job \u03c0a 6: end if 7: for a\u2032 = a + 1 . . . b do 8: Ctemp\u2190 Ctemp + s\u03c0\na\u2032\u22121 \u03c0 a\u2032 + p\u03c0 a\u2032\n9: if Ctemp > d\u03c0 a\u2032 then\n10: cost\u2190 cost+ w\u03c0 a\u2032 \u00d7 (Ctemp \u2212 d\u03c0 a\u2032 ) 11: end if 12: end for 13: return cost 14: end CompCostBlock.\nIt is easy to verify that the total cost of a sequence can be obtained by the sum of the\ncosts of the blocks defined for that sequence. In the example given in Fig. 1 the total\ncost of the sequence is equal to the sum of the cost of blocks B0, B1, B2 and B3.\nWhen performing move evaluations it is quite useful to define an auxiliary data struc-\nture that stores the cumulated weighted tardiness up to a certain position of the sequence.\nTherefore, we have decided to define an array g for that purpose. For example, the el-\nement g5 of the array stores the cumulated weighted tardiness up to the 5th position of the sequence.\nWe now proceed to a detailed description of the neighborhood structures used in our\napproach.\n3.2.1 Swap\nThe swap neighborhood structure simply consists of exchanging the position of two jobs in\nthe sequence, as depicted in Figure 2. It is possible to observe that the modified solution\ncan be divided into 6 blocks. Alg. 2 shows how to compute the cost of a solution in\nO(n\u2212 i) steps. Note that one can always assume that i < j for every swap move.\nAlgorithm 2 CompCostSwap\n1: Procedure CompCostSwap(\u03c0, i, j, g, C) 2: f \u2190 gi\u22121 \u22b2 Variable that stores the cost to be evaluated 3: Ctemp \u2190 C\u03c0i\u22121 4: f \u2190 f + CompCostBlock(i\u2212 1, j, j, \u03c0, Ctemp) \u22b2 Cost of block 4 5: f \u2190 f + CompCostBlock(j, i+ 1, j \u2212 1, \u03c0, Ctemp) \u22b2 Cost of block 3 6: f \u2190 f + CompCostBlock(j \u2212 1, i, i, \u03c0, Ctemp) \u22b2 Cost of block 2 7: f \u2190 f + CompCostBlock(i, j + 1, n, \u03c0,Ctemp) \u22b2 Cost of block 5 8: return f 9: end CompCostSwap.\nAlg. 3 presents the pseudocode of the neighborhood swap considering the limitation strategy described in Section 3.1. At first, the best sequence \u03c0\u2217 is considered to be the\nsame as the original one, i.e., \u03c0 (line 2). Next, for every pair of positions i and j (with\nj > i) of \u03c0 (lines 3-16), the setup variation is computed in constant time (line 5). If this\nvariation is smaller than or equal to max\u2206sswap (lines 6-14), then the cost of sequence \u03c0\u2032, which is a neighbor of \u03c0 generated by exchanging the position of customers \u03c0i and \u03c0j , is computed using Alg. 2 (line 7). In case of improvement (lines 6-13), the best current\nsolution is updated (line 9) and if the learning phase is activated, that is, if max\u2206sswap = M , list \u2206sswap is updated by adding the value associated to the setup variation computed in line 5 (lines 10-12). Finally, the procedure returns the best sequence \u03c0\u2217 found during\nthe search (line 17).\nAlgorithm 3 Swap\n1: Procedure Swap(\u03c0, g, C,\u2206sswap, max\u2206sswap) 2: \u03c0\u2217 \u2190 \u03c0; f\u2217 \u2190 f(\u03c0); 3: for i = 1 . . . n\u2212 1 do 4: for j = i+ 1 . . . n do 5: setupV ariation = \u2212s\u03c0i\u22121\u03c0i \u2212 s\u03c0i\u03c0i+1 \u2212 s\u03c0j\u22121\u03c0j \u2212 s\u03c0j\u03c0j+1\n+s\u03c0i\u22121\u03c0j + s\u03c0j\u03c0i+1 + s\u03c0j\u22121\u03c0i + s\u03c0i\u03c0j+1 6: if setupV ariation \u2264 max\u2206sswap then 7: f(\u03c0\u2032) = CompCostSwap(\u03c0, i, j, l, g, C) \u22b2 \u03c0\u2032 is a neighbor of \u03c0 8: if f(\u03c0\u2032) < f\u2217 then 9: \u03c0\u2217 \u2190 \u03c0\u2032; f\u2217 \u2190 f(\u03c0\u2032) 10: if max\u2206sswap = M then \u22b2 Learning phase activated 11: \u2206sswap \u2190 \u2206sswap \u222a setupV ariation 12: end if 13: end if 14: end if 15: end for 16: end for 17: return \u03c0\u2217; 18: end Swap.\n3.2.2 l-block insertion\nThe l-block insertion neighborhood consists of moving a block of length l forward (i < j)\nor backward (i > j), as shown in Figs. 3 and 4, respectively. In this case the resulting\nsolution in both cases can be represented by 5 blocks. Algs. 4 and 5 describe how to\nevaluate a cost of moving a block forward and backward in O(n\u2212 i) and O(n\u2212 j) steps,\nrespectively.\nunder evaluation is already worse than the best current cost. If so, the evaluation can\nbe interrupted because it is already known that the solution under evaluation cannot be\nbetter than the current one. In some cases this may help speeding up the move evaluation\nprocess.\nLet L be a set composed of different values for the parameter l. In practice one can\ndefine a single neighborhood considering all values of l \u2208 L at once (see Xu et al. (2013)),\nor define multiple neighborhoods where each of them is associated with a given value of\nl \u2208 L. In our implementation we decided for the latter option.\nAlg. 6 shows the pseudocode of the neighborhood l-block taking into account the\nlimitation strategy described earlier. This algorithm, which is divided into two parts,\nfollows the same rationale of Alg. 3 in both of them. The main difference is related to\nthe range of the positions i and j, which depends on the value of l. In part one (lines\n3-16) the search is performed forward, whereas in part 2 it is performed backward (lines\n17-30).\nAlgorithm 6 l-blockInsertion\n1: Procedure l-blockInsertion(\u03c0, l, g, C,\u2206sl-block,max\u2206sl-block) 2: \u03c0\u2217 \u2190 \u03c0; f\u2217 \u2190 f(\u03c0); \u22b2 Moving the l-block forward 3: for i = 1 . . . n\u2212 l do 4: for j = i+ 1 . . . n do 5: setupV ariation = \u2212s\u03c0i\u22121\u03c0i \u2212 s\u03c0i\u22121+l\u03c0i+l \u2212 s\u03c0j\u03c0j+1\n+s\u03c0i\u22121\u03c0i+l + s\u03c0j\u03c0i + s\u03c0i\u22121+l\u03c0j+1 6: if setupV ariation \u2264 max\u2206sl-block then 7: f(\u03c0\u2032) = CompCostl-blockF(\u03c0, i, j, l, g, C) \u22b2 \u03c0\u2032 is a neighbor of \u03c0 8: if f(\u03c0\u2032) < f\u2217 then 9: \u03c0\u2217 \u2190 \u03c0\u2032; f\u2217 \u2190 f(\u03c0\u2032) 10: if max\u2206sl-block = M then \u22b2 Learning phase activated 11: \u2206sl-block \u2190 \u2206sl-block \u222a setupV ariation; 12: end if 13: end if 14: end if 15: end for 16: end for \u22b2 Moving the l-block backward 17: for i = 2 . . . n\u2212 l + 1 do 18: for j = 1 . . . i\u2212 1 do 19: setupV ariation = \u2212s\u03c0j\u22121\u03c0j \u2212 s\u03c0i\u22121\u03c0i \u2212 s\u03c0i\u22121+l\u03c0i+1 +s\u03c0j\u22121\u03c0i + s\u03c0j\u22121+l\u03c0j + s\u03c0i\u22121\u03c0i+1 20: if setupV ariation \u2264 max\u2206sl-block then 21: f(\u03c0\u2032) = CompCostl-blockB(\u03c0, i, j, l, g, C) \u22b2 \u03c0\u2032 is a neighbor of \u03c0 22: if f(\u03c0\u2032) < f\u2217 then 23: \u03c0\u2217 \u2190 \u03c0\u2032; f\u2217 \u2190 f(\u03c0\u2032) 24: if max\u2206sl-block = M then \u22b2 Learning phase activated 25: \u2206sl-block \u2190 \u2206sl-block \u222a setupV ariation 26: end if 27: end if 28: end if 29: end for 30: end for 31: return \u03c0\u2217; 32: end l-blockInsertion."}, {"heading": "3.3 Embedding the proposed approach in the ILS-RVND metaheuristic", "text": "In this section we explain how we embedded the proposed approach in the ILS-RVND\nmetaheuristic (Subramanian, 2012). The reason for choosing this metaheuristic to test our\nlocal search limitation strategy is that it was found capable of generating highly competitive results not only for problem 1|sij| \u2211 wjTj (Subramanian et al., 2014), as mentioned in Section 2, but also for other combinatorial optimization problems (Subramanian et al.,\n2010; Subramanian, 2012; Silva et al., 2012; Penna et al., 2013; Subramanian and Battarra,\n2013; Martinelli et al., 2013; Vidal et al., 2015). Moreover, the referred method relies on\nvery few parameters and it can be considered relatively simple, which makes it quite\npractical and also easy to implement.\nAlg. 7 highlights the differences between the original version of ILS-RVND and a\nnew one, called ILS-RVNDFast, that includes the additional steps (line 3 and lines 17- 22) described in Section 3.1 for speeding up the the local search. It can be observed\nthat the learning phase is limited to the first iteration of the algorithm, where the list\n\u2206sv, \u2200v \u2208 N , is populated during the local search (line 9). The value of max\u2206sv, \u2200v \u2208 N is then estimated after the end of the first iteration. Parameters IR and IILS correspond to the number of restarts of the metaheuristic and the number of consecutive ILS itera-\ntions without improvements, respectively. An exception occurs during the learning phase,\nwhich is more costly, where the number of ILS iterations is IILS/2. In both algorithms the initial solution (line 5) is generated using a simple randomized insertion heuristic,\nwhereas the perturbation (line 14) is performed by a mechanism called double-bridge,\nwhich consists of exchanging two blocks of a sequence at random. The reader is referred\nto Subramanian et al. (2014) for implementation details about these procedures.\nThe local search method of both algorithms is performed by a RVND procedure, which\nconsists of selecting an unexplored neighborhood at random whenever another one fails to\nfind an improved solution. In case of improvement, all neighborhoods are reconsidered to\nbe explored. Note that in both algorithms the set L associated to the l-block neighborhood\nis provided as an input parameter to be tuned (see Section 4.1), as opposed to the ILS-\nRVND presented in Subramanian et al. (2014) where this set was predefined as L =\n{1, 2, 3}, that is, the l-block neighborhoods were limited to 1-, 2- and 3-block insertion.\nIn addition, the neighborhood block reverse was also used in a restricted fashion by the\nauthors, but it was not considered here because it did not seem to be crucial for obtaining\nhigh quality solutions.\nOne last remark is that the algorithm stops when a sequence \u03c0 with cost f(\u03c0) = 0 is\nfound. When this happens it is clear that an optimal solution was obtained and thus there\nis no point in continuing the search. This was not originally considered in the ILS-RVND\npresented in Subramanian et al. (2014), but it has been considered here in both versions.\nD ep a rta m en to d e E n g en h a ria d e P ro d u c\u0327a\u0303 o \u2013 U F P B\nW o rk in g P a p er\nAlgorithm 7 ILS-RVND vs ILS-RVNDFast 1: Procedure ILS-RVND(IR, IILS , L) 2: f\u2217 \u2190 \u221e 3: 4: for iter = 1 . . . IR do 5: \u03c0 \u2190 GenerateInitialSolution() 6: \u03c0\u0302 \u2190 \u03c0 7: iterILS \u2190 0 8: while iterILS \u2264 IILS do 9: \u03c0 \u2190 RVND(\u03c0,L) 10: if f(\u03c0) < f(\u03c0\u0302) then 11: \u03c0\u0302 \u2190 \u03c0 12: iterILS \u2190 0 13: end if 14: \u03c0 \u2190 Perturb(\u03c0\u0302) 15: iterILS \u2190 iterILS + 1 16: end while 17: 18: 19: 20: 21: 22: 23: if f(\u03c0\u0302) < f\u2217 then 24: \u03c0\u2217 \u2190 \u03c0\u0302; f\u2217 \u2190 f(\u03c0\u0302) 25: end if 26: end for 27: return \u03c0\u2217 28: end ILS-RVND.\nProcedure ILS-RVNDFast(IR, IILS , \u03b8, L) f\u2217 \u2190 \u221e max\u2206sv \u2190 M ;\u2206sv \u2190 NULL,\u2200v \u2208 N for iter = 1 . . . IR do\n\u03c0 \u2190 GenerateInitialSolution() \u03c0\u0302 \u2190 \u03c0 iterILS \u2190 0 while iterILS \u2264 IILS do \u22b2 or iterILS \u2264 IILS/2 when iter = 0\n\u03c0 \u2190 RVND(\u03c0,L,\u2206s,max\u2206s) if f(\u03c0) < f(\u03c0\u0302) then\n\u03c0\u0302 \u2190 \u03c0 iterILS \u2190 0\nend if \u03c0 \u2190 Perturb(\u03c0\u0302) iterILS \u2190 iterILS + 1\nend while if iter = 1 then for v = 1 . . . |N | do\n\u2206sv \u2190 sort(\u2206sv) max\u2206sv \u2190 value associated to the position \u230a\u03b8 \u00b7 |\u2206sv|\u230b of list \u2206sv\nend for\nend if if f(\u03c0\u0302) < f\u2217 then \u03c0\u2217 \u2190 \u03c0\u0302; f\u2217 \u2190 f(\u03c0\u0302) end if\nend for return \u03c0\u2217 end ILS-RVNDFast.\n1 3"}, {"heading": "4 Computational Experiments", "text": "The ILS-RVND and ILS-RVNDFast algorithms were coded in C++ and the experiments were performed in an Intel Core i7 with 3.40 GHz and 16 GB of RAM running under\nLinux Mint 13. Only a single thread was used in our testing.\nThe 120 instances of Cicirello and Smith (2005) were used to evaluate the performance\nof the proposed algorithms. Each of them has 60 jobs and is characterized by three\nparameters: \u03c4 , which is related to the tightness of the due date; R, which specifies\nthe range of the due dates; \u03b7, which refers to the size of the average setup time with\nrespect to the size of the average processing time. The authors created 12 groups of 10\ninstances by combining the following parameters: \u03c4 = {0.3, 0.6, 0.9}, R = {0.25, 0.75}\nand \u03b7 = {0.25, 0.75}, as shown in Table 2."}, {"heading": "4.1 Parameter Tuning", "text": "To have a better idea of the impact of the modifications introduced in the original ILS-\nRVND, we decided to use the same configuration for the number of ILS iterations as in\nSubramanian et al. (2014), that is, IILS = 4 \u00d7 n. However, since each iteration of the algorithm became much faster after implementing the limitation strategy (see Section 4.3)\nwe decided to set IR = 20, instead of IR = 10, as in Subramanian et al. (2014), because it seemed to provide a better compromise between solution quality and computational time.\nA set of 17 challenging instances was selected for tuning the parameters L and \u03b8, namely\ninstances 1, 2, 3, 4, 5, 7, 8. 9, 10, 11, 13, 14, 15, 16, 18, 20 and 24. The criterion used for\nchoosing these instances was based on the difficulty faced by the ILS-RVND implemented\nin Subramanian et al. (2014) in finding their corresponding optimal solutions.\nLet Best Gap be the gap between the best solution found in 10 runs and the optimal\nsolution; Worst Gap be the gap between the worst solution found in 10 runs and the\noptimal solution; and Avg. Gap be the average gap between the average solution of 10\nruns and the optimal solution. In the tables presented hereafter, Arithm. Mean of Best\nGaps (%) corresponds to the arithmetic mean of the Best Gaps, Geom. Mean of Avg.\nGaps (%) denotes the geometric mean of the Avg. Gaps, Geom. Mean of Worst Gaps (%)\nindicates the geometric mean of the Worst Gaps and Arithm. Mean of Avg. Time (s) is\nthe arithmetic mean of the average times in seconds. Given a set of q positive numbers,\nthe geometric mean can be defined as the qth root of the product of all numbers of the\nset. The reason for using the geometric mean is because it normalizes the different ranges,\nthat is, the possible significant differences among the Gaps. Hence, the geometric mean\nwas adopted in some of the cases as an attempt to perform a fair comparison between the\nsolutions found for each configuration tested. However, we used the arithmetic mean for\nthe Best Gaps because in many cases the best solution found coincided with the optimal\nsolution. In this case, the gap is equal to zero and thus the value is disregarded from the\ncomputation of the geometric mean, which may result in a misleading result when many\nvalues are not considered.\nWe have tested 34 different combination of neighborhoods, more precisely, we consid-\nered L = {1, . . . , 4} to L = {1, . . . , 20} incrementing one l-block neighborhood at a time\nand we tried each possibility with and without swap. Therefore, the goal of this exper-\niment was not only to calibrate the parameter L, but also to investigate the benefits of\nincluding the neighborhood swap. For this testing, we have arbitrarily set \u03b8 = 0.90 and we\nran the algorithm 10 times for each instance. Table 3 shows the average results obtained\nfor the 17 instances mentioned above with the different combinations of neighborhoods.\nWe decided to adopt the configuration L = {1, . . . , 13} + swap because it seemed to offer\na good balance between solution quality and computational time.\nFive different values for the parameter \u03b8 was tested, in particular, 0,80, 0.85, 0.90, 0.95\nand 1.00, and the results can be found in Table 4. As expected, it can be observed that the\naverage computational time is directly proportional to \u03b8 since more moves are considered\nfor evaluation as the value of \u03b8 increases. Note that \u03b8 = 1.00 implies that max\u2206sv is equal to the largest setup variation associated to an improving move of neighborhood\nv \u2208 N that occurred during the learning phase. We decided to adopt \u03b8 = 0.90 because\nsolutions of similar quality were obtained when compared to \u03b8 = 1.00, but approximately\nfour times faster."}, {"heading": "4.2 Comparison with the literature", "text": "In this section we compare the best, average and worst results found by ILS-RVNDFast over 10 runs with the best methods available in the literature. We specify below the algorithms\nconsidered for comparison as well as the type of result reported by the associated work.\nACOAP: Ant Colony Optimization of Anghinolfi and Paolucci (2008). Best of 10\nruns.\nDPSO: Discrete Particle Swarm Optimization of Anghinolfi and Paolucci (2009). Best\nof 10 runs.\nDDE: Discrete Differential Evolutionary heuristic of Tasgetiren et al. (2009). Best of\n10 runs.\nGVNS: General Variable Neighborhood Search of Kirlik and Og\u0306uz (2012). Best of 20\nruns.\nILSXLC: Iterated Local Search of Xu et al. (2013). Best and average of 100 runs. ILS-RVNDSBP: Iterated Local Search + Randomized Variable Neighborhood Descent\nof Subramanian et al. (2014). Best and average of 10 runs.\nLOX\u2295B: Hybrid Evolutionary Algorithm of Xu et al. (2014). Best and average of 20\nruns.\nOpt: Optimal solution found by the exact algorithm of Tanaka and Araki (2013).\nTable 5 presents the results found by ILS-RVNDFast and also by the algorithms mentioned above, except ACOAP for space restriction reasons. The average times reported in this table are in seconds. It can be observed that the proposed algorithm was capable\nof finding the optimal solution for all instances, except for instance 24. Moreover, it is\npossible to verify that the mean of the average times of ILS-RVNDFast was approximately 13 seconds.\nTable 6 presents a summary of the results found by ILS-RVNDFast compared with those achieved by several heuristics from the literature. A direct comparison with some\nof the algorithms such as ACOAP, DPSO, DDE and GVNS becomes quite hard because the average results were not reported by the authors. With respect to the best solutions,\nour algorithm clearly outperforms these four by a good margin. Even our average and\nworst solutions are, in most cases, better or equal than the best ones of such methods.\nThe average and worst solutions of ILS-RVNDFast are always equal or better than the average solutions of ILSXLC. The same happened to ILS-RVNDSBP and LOX\u2295B, except for few cases where the worst solutions of ILS-RVNDFast were not better than the average solution of these two methods. Worst solutions were only reported for ILS-\nRVNDSBP, where all them are either equal or worse than those of ILS-RVNDFast. The results suggest that our algorithm is very competitive in terms of solution quality.\nsolutions found by the algorithms that were chosen for comparison. In this case the\nstopping criterion no longer depends on the number of restarts, but on a target value\nfrom the literature. The algorithm was executed 10 times for each instance and we report\nthe mean (arithmetic), median, minimum and maximum of the average times by group\nrequired to find or improve the target value. We also report the average values considering\nall instances at once (last column of the table).\nOn one hand, it can be observed from Table 7 that ILS-RVNDFast was capable of finding or improving the best solutions of ACOAP, DPSO, DDE, GVNS and ILS-RVNDSBP in\nonly 2.0, 2.8, 4.6, 6.2 and 7.9 seconds, on average, respectively. On the other hand,\nILS-RVNDFast spent, on average, 32.6, 158.3 and 161.2 seconds to find or improve the best solutions of ILSXLC, LOX\u2295B and the optimal solutions, respectively. However, if we disregard instance 24, these values significantly decrease to 9.2, 18.0 and 20.9 seconds,\nrespectively.\nMoreover, ILS-RVNDFast found or improved the average solutions of ILSXLC, ILSRVNDSBP and LOX\u2295B in only 1.9, 4.1 and 3.0 seconds, respectively, on average. It is worth mentioning that the average solutions of ILSXLC in Xu et al. (2013) and LOX\u2295B in Xu et al. (2014) were obtained in 100 seconds on an Intel Core i3 3.10 GHz with 2.0 GB\nof RAM, whereas the average solutions of ILS-RVNDSBP were obtained in 23.4 seconds on an Intel Core i5 3.20 GHz with 4.0 GB of RAM. The hardware configuration of these two\nmachines is slightly inferior than the one used in our experiments (Intel Core i7 with 3.40\nGHz and 16 GB of RAM), and thus does not justify the considerable difference in terms of\nCPU time between ILS-RVNDFast and the three other methods. Therefore, from Tables 5-7, we can conclude that ILS-RVNDFast is, on average, remarkably faster and clearly more efficient than those three heuristic algorithms, which are the best ones available in the literature for problem 1|sij| \u2211 wjTj ."}, {"heading": "4.3 Impact of the proposed local search limitation strategy", "text": "In this section we investigate the effect of the proposed local search limitation strategy\non the performance of the algorithm. We start by analyzing the average percentage of\nmoves that were not evaluated per neighborhood in all groups of instances, as shown in\nTable 8. It can be verified that the proportion of moves that were not considered for\nevaluation varied according to the characteristics of the group, ranging, on average, from\n69.6% (Group 7) to 98.8% (Group 2).\nconduct an experiment to verify the level of accuracy of the limitation strategy. Table 9\nshows, for each neighborhood and for each group, the average percentage of improving\nmoves that were not evaluated, here denoted as lost improving moves. We can observe\nthat the average percentage of lost improving moves was relatively small in all cases (never\nmore than 11%), thus ratifying the effectiveness of the proposed limitation strategy.\nspent by each neighborhood in a complete execution of ILS-RVND and ILS-RVNDFast, respectively. The impact of the limitation strategy on the average time spent during the\nlocal search is clearly visible. It is possible to verify that a neighborhood search in ILS-\nRVNDFast is, on average, about 6 times faster than in ILS-RVND. Also, we can see that the level of CPU time reduction for each group is proportional to the number of moves\nthat were not evaluated, as shown in Table 8.\nILS-RVNDFast. We did not report the gap values for Groups 3 and 4 because the optimal solution for all instances, except for instance 24, is zero. The geometric means of Group\n7 were also not reported in the table because the average gap of almost all instances were\nzero. In this last analysis we can observe that the speedup achieved by ILS-RVNDFast does not come at the expense of solution quality. The results demonstrate that there is\nno clear difference between both algorithms when it comes to the average gap between\nthe average/best solutions and the optimal solution."}, {"heading": "4.4 Impact of the proposed limitation strategy on the performance of other", "text": "In order to validate the generality of the proposed limitation strategy, we performed\nexperiments with other local search based metaheuristics, namely GRASP and VNS. In\nthe section we present the results obtained by such metaheuristics with and without the\naddition of such strategy."}, {"heading": "4.4.1 GRASP", "text": "GRASP (Feo and Resende, 1995) is a multi-start metaheuristic where at each restart a\nsolution is generated using a greedy randomized approach and then possibly improved by\na local search procedure. The level of greediness/randomness of the constructive phase is\ncontrolled by a parameter 0 \u2264 \u03b1 \u2264 1. In our implementation the value of \u03b1 was chosen\nat random from the set {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}. The local search is performed by the\nRVND procedure with the same neighborhoods used in the ILS algorithm. The number\nof restarts was set to 5000, where the first 250 restarts are associated with the learning\nphase, which is equivalent to 5% of the number of restarts, as in ILS, while the value of\n\u03b8 was set to the same used in ILS, i.e., 0.90.\nTable 13 shows the average results of 10 runs found by GRASP without and with\nthe limitation strategy (GRASPFast). We can observe that GRASPFast was, on average, roughly 2 times faster than GRASP without significantly affecting the quality of\nthe solutions obtained, thus illustrating the effectiveness of the limitation strategy when\nconsidering a standard and non-tailored implementation of the metaheuristic."}, {"heading": "4.4.2 VNS", "text": "VNS (Mladenovic\u0301 and Hansen, 1997) is a local search based metaheuristic that alternates\nbetween local search (intensification) and perturbation (diversification) procedures. The\nlocal optimal solutions are perturbed by means of one of the existing neighborhood op-\nerators. A local search is then applied over this perturbed solution. If the local optimal\nsolution is not improved, a different neighborhood is used to perturb such incumbent so-\nlution. RVND was used as the local search procedure and the total number of iterations\nwas set to 1000, where the first 5% (50 iterations) are related to the learning phase. The\nvalue of \u03b8 and the neighborhoods were the same adopted in ILS and GRASP.\nThe average results of 10 runs obtained by VNS without and with the limitation\nstrategy (VNSFast) can be found in Table 14. There were practically no difference in terms of solution quality between VNS and VNSFast, but the latter was approximately 2 times faster than the first. The results suggest that the proposed limitation strategy was\nalso effective for a straightforward implementation of this metaheuristic.\n4.5 Results for problem 1|sij| \u2211 Tj\nWith a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to\n45 jobs, and those suggested by Gagne\u0301 et al. (2002), also containing 32 instances but\nranging from 55 to 85 jobs. In this case the configuration adopted after tuning the\nparameters by using the same rationale presented in Section 4.1 was L = {1, . . . , 13} +\nswap and \u03b8 = 0.75.\nWe compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and Og\u0306uz,\n2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algo-\nrithms listed below along with the type of result presented.\nACOGPG: Ant Colony of Gagne\u0301 et al. (2002). Best, worse, average and median of 20 runs for the instances proposed by the authors. Furthermore, the best solutions found by\nACOGPG for the instances of Rubin and Ragatz (1995) were reported by Liao and Cheng (2007).\nTabuGGP: TS and VNS of Gagne\u0301 et al. (2005). Best of 10 runs for the instances of Gagne\u0301 et al. (2002), while the results found for the instances of Rubin and Ragatz (1995)\nwere reported by Ying et al. (2009).\nGRASPGS: GRASP of Gupta and Smith (2006). Best, worse, average and median\nof 20 runs for the instances of Gagne\u0301 et al. (2002).\nACOLJ: Ant Colony of Liao and Cheng (2007). Best of 10 runs for all instances. ILSANK: Iterated Local Search of Arroyo et al. (2009). Best, worse and average of 20\nruns for the instances of Gagne\u0301 et al. (2002).\nIG: Iterated Greedy of Ying et al. (2009). Best of 10 runs for all instances.\nOpt: Optimal solution found by the exact algorithm of Tanaka and Araki (2013), ex-\ncept for instances 851 and 855. For these instances, the authors provide a lower bound of\n357 and 254, respectively.\nTables 15 and 16 compare the results found by ILS-RVNDFast with the best known methods found in the literature. The proposed algorithm was capable of finding the\noptimal solutions of all cases, except for instances 751, 851 and 855. The average com-\nputational time was 0.7 seconds for the instances of Rubin and Ragatz (1995) and 12.5\nseconds for the instances of Gagne\u0301 et al. (2002).\nA summary of the comparison between ILS-RVNDFast and the other methods from the literature is presented in Table 17. From this table, it can be observed that ILS-RVNDFast visibly outperforms the existing heuristics in terms of solution quality. Moreover, our best\nand average solutions were never worse than those found by other heuristics.\nTable 18 summarizes the results found by ILS-RVND and ILS-RVNDFast. The geometric mean of the instances involving 15 and 25 jobs were not reported because the gaps\nof both groups were zero. Regarding the quality of the solution obtained, both methods\nproduced similar results, but ILS-RVNDFast was, on average, approximately 7 times faster\nthan ILS-RVND."}, {"heading": "5 Concluding Remarks", "text": "In this paper we proposed a simple but very efficient local search limitation strategy for problem 1|sij| \u2211 wjTj. This strategy aims at speeding up the local search process by avoiding evaluations of unpromising moves. The setup variation is used to estimate\nwhether or not a move should be evaluated. In particular, a move is only evaluated if\nthe setup variation is smaller than a given threshold value, which in turn depends on the\ncharacteristics of the instance and on the neighborhood structure. Therefore, instead of\ntuning a value for this threshold a priori, we developed a procedure that automatically\nestimates a value for this parameter. Furthermore, we presented a detailed description of\nhow the limitation strategy can be incorporated into the neighborhoods swap and l-block\ninsertion.\nThe proposed approach was embedded in the ILS-RVND algorithm Subramanian\n(2012), which is a simple local search based metaheuristic that was successfully applied to many combinatorial optimization problems, including the 1|sij| \u2211 wjTj Subramanian et al. (2014). This enhanced version of the algorithm was denoted as ILS-RVNDFast. Extensive computational experiments were carried out on well-known benchmark instances and the\nresults obtained suggest that ILS-RVNDFast is capable of producing extremely competitive results both in terms of average solutions and CPU time. When analyzing the impact\nof the limitation strategy, it was possible to confirm that the high speedups achieved did\nnot come at the expense of solution quality. As a result, a considerable number of neigh-\nborhoods could be used without significantly increasing the CPU time, which was crucial\nfor the high performance of ILS-RVNDFast.\nWe performed similar experiments with standard and non-tailored implementations of\nother well-known local search based metaheuristics, namely GRASP and VNS, and the\nversions with the limitation strategy were approximately 2 times faster, without significant\nloss in solution quality, than those without the inclusion of such strategy. Finally, we also performed experiments with ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj and the results obtained ratified the effectiveness of the limitation strategy in a related\nproblem."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Arthur Kramer for valuable discussions concerning the\nmove evaluation schemes. This research was partially supported by the Conselho Nacional\nde Desenvolvimento Cient\u0301\u0131fico e Tecnolo\u0301gico (CNPq), grant 471158/2012-7."}], "references": [{"title": "A new ant colony optimization approach for the single machine total weighted tardiness scheduling problem", "author": ["D. Anghinolfi", "M. Paolucci"], "venue": "International Journal of Operations Research,", "citeRegEx": "Anghinolfi and Paolucci,? \\Q2008\\E", "shortCiteRegEx": "Anghinolfi and Paolucci", "year": 2008}, {"title": "A new discrete particle swarm optimization approach for the single-machine total weighted tardiness scheduling problem with sequence-dependent setup times", "author": ["D. Anghinolfi", "M. Paolucci"], "venue": "European Journal of Operational Research, v. 193,", "citeRegEx": "Anghinolfi and Paolucci,? \\Q2009\\E", "shortCiteRegEx": "Anghinolfi and Paolucci", "year": 2009}, {"title": "Iterative local search heuristic for the single machine scheduling problem with sequence dependent setup times and due dates", "author": ["J.E.C. Arroyo", "G.V.P. Nunes", "E.H. Kamke"], "venue": "HIS (1),", "citeRegEx": "Arroyo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Arroyo et al\\.", "year": 2009}, {"title": "Parallel path relinking method for the single machine total weighted tardiness problem with sequence-dependent setups", "author": ["W. Bo\u017cejko"], "venue": "Journal of Intelligent Manufacturing, v. 21,", "citeRegEx": "Bo\u017cejko,? \\Q2010\\E", "shortCiteRegEx": "Bo\u017cejko", "year": 2010}, {"title": "A discrete electromagnetism-like mechanism for single machine total weighted tardiness problem with sequence-dependent setup times", "author": ["Chao", "C.-W", "Liao", "C.-J"], "venue": "Applied Soft Computing, v. 12,", "citeRegEx": "Chao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chao et al\\.", "year": 2012}, {"title": "Non-wrapping order crossover: An order preserving crossover operator that respects absolute position", "author": ["V.A. Cicirello"], "venue": "Proceedings of the Genetic and Evolutionary Computation Conference,", "citeRegEx": "Cicirello,? \\Q2006\\E", "shortCiteRegEx": "Cicirello", "year": 2006}, {"title": "Enhancing stochastic search performance by value-biased randomization of heuristics", "author": ["V.A. Cicirello", "S.F. Smith"], "venue": "Journal of Heuristics,", "citeRegEx": "Cicirello and Smith,? \\Q2005\\E", "shortCiteRegEx": "Cicirello and Smith", "year": 2005}, {"title": "An iterated greedy algorithm for the single-machine total weighted tardiness problem with sequence-dependent setup times", "author": ["G. Deng", "X. Gu"], "venue": "International Journal of Systems Science, v. 45,", "citeRegEx": "Deng and Gu,? \\Q2014\\E", "shortCiteRegEx": "Deng and Gu", "year": 2014}, {"title": "Greedy randomized adaptive search procedures", "author": ["T.A. Feo", "M.G.C. Resende"], "venue": "Journal of Global Optimization, v. 6,", "citeRegEx": "Feo and Resende,? \\Q1995\\E", "shortCiteRegEx": "Feo and Resende", "year": 1995}, {"title": "Using metaheuristic compromise programming for the solution of multiple-objective scheduling problems", "author": ["C. Gagn\u00e9", "M. Gravel", "W.L. Price"], "venue": "Journal of the Operational Research Society, v. 56,", "citeRegEx": "Gagn\u00e9 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gagn\u00e9 et al\\.", "year": 2005}, {"title": "Comparing an aco algorithm with other heuristics for the single machine scheduling problem with sequence-dependent setup times", "author": ["C. Gagn\u00e9", "W.L. Price", "M. Gravel"], "venue": "The Journal of the Operational Research Society, v. 53,", "citeRegEx": "Gagn\u00e9 et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gagn\u00e9 et al\\.", "year": 2002}, {"title": "An improved scatter search algorithm for the single machine total weighted tardiness scheduling problem with sequence-dependent setup times", "author": ["Q. Guo", "L. Tang"], "venue": "Applied Soft Computing,", "citeRegEx": "Guo and Tang,? \\Q2015\\E", "shortCiteRegEx": "Guo and Tang", "year": 2015}, {"title": "Algorithms for single machine total tardiness scheduling with sequence dependent setups", "author": ["S.R. Gupta", "J.S. Smith"], "venue": "European Journal of Operational Research, v. 175,", "citeRegEx": "Gupta and Smith,? \\Q2006\\E", "shortCiteRegEx": "Gupta and Smith", "year": 2006}, {"title": "A variable neighborhood search for minimizing total weighted tardiness with sequence dependent setup times on a single machine", "author": ["G. Kirlik", "C. O\u011fuz"], "venue": "Computers & Operations Research, v. 39,", "citeRegEx": "Kirlik and O\u011fuz,? \\Q2012\\E", "shortCiteRegEx": "Kirlik and O\u011fuz", "year": 2012}, {"title": "A \u201dpseudopolynomial\u201d algorithm for sequencing jobs to minimize total tardiness", "author": ["E.L. Lawler"], "venue": "Studies in Integer Programming,", "citeRegEx": "Lawler,? \\Q1977\\E", "shortCiteRegEx": "Lawler", "year": 1977}, {"title": "A heuristic to minimize the total weighted tardiness with sequence-dependent setups", "author": ["Y.H. Lee", "K. Bhaskaran", "M. Pinedo"], "venue": "IIE Transactions, v. 29,", "citeRegEx": "Lee et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1997}, {"title": "Complexity of machine scheduling problems", "author": ["J. Lenstra", "A.R. Kan", "P. Brucker"], "venue": "Studies in Integer Programming,", "citeRegEx": "Lenstra et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Lenstra et al\\.", "year": 1977}, {"title": "A variable neighborhood search for minimizing single machine weighted earliness and tardiness with common due date", "author": ["Liao", "C.-J", "Cheng", "C.-C"], "venue": "Computers & Industrial Engineering, v. 52,", "citeRegEx": "Liao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2007}, {"title": "An ant colony optimization for single-machine tardiness scheduling with sequence-dependent setups", "author": ["Liao", "C.-J", "Juan", "H.-C"], "venue": "Computers & Operations Research, v. 34,", "citeRegEx": "Liao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2007}, {"title": "Neighborhood search procedures for single machine tardiness scheduling with sequence-dependent setups", "author": ["Liao", "C.-J", "Tsou", "H.-H", "Huang", "K.-L"], "venue": "Theoretical Computer Science,", "citeRegEx": "Liao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2012}, {"title": "Solving single-machine total weighted tardiness problems with sequence-dependent setup times by meta-heuristics", "author": ["Lin", "S.-W", "Ying", "K.-C"], "venue": "The International Journal of Advanced Manufacturing Technology, v", "citeRegEx": "Lin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2007}, {"title": "A max-min ant system to minimize total tardiness on a single machine with sequence dependet setup times implementing a limited budget local search", "author": ["N. Mandahawi", "S. Al-Shihabi", "S. Altarazi"], "venue": "International Journal of Research & Reviews in Applied Sciences,", "citeRegEx": "Mandahawi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mandahawi et al\\.", "year": 2011}, {"title": "Large-step markov chains for the traveling salesman problem", "author": ["O. Martin", "S.W. Otto", "E.W. Felten"], "venue": "Complex Systems,", "citeRegEx": "Martin et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Martin et al\\.", "year": 1991}, {"title": "Improved bounds for large scale capacitated arc routing problem", "author": ["R. Martinelli", "M. Poggi", "A. Subramanian"], "venue": "Computers & Operations Research, v. 40,", "citeRegEx": "Martinelli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martinelli et al\\.", "year": 2013}, {"title": "Variable neighborhood search", "author": ["N. Mladenovi\u0107", "P. Hansen"], "venue": "Computers & Operations Research, v. 24,", "citeRegEx": "Mladenovi\u0107 and Hansen,? \\Q1997\\E", "shortCiteRegEx": "Mladenovi\u0107 and Hansen", "year": 1997}, {"title": "A heuristic algorithm for the m-machine, n-job flow-shop sequencing problem", "author": ["M. Nawaz", "E.E.E. Jr.", "I. Ham"], "venue": "Omega, v. 11,", "citeRegEx": "Nawaz et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Nawaz et al\\.", "year": 1983}, {"title": "An iterated local search heuristic for the heterogeneous fleet vehicle routing problem", "author": ["P. Penna", "A. Subramanian", "L. Ochi"], "venue": "Journal of Heuristics, v. 19,", "citeRegEx": "Penna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Penna et al\\.", "year": 2013}, {"title": "Real-time scheduling of an automated manufacturing center", "author": ["N. Raman", "R.V. Rachamadugu", "F. Talbot"], "venue": "European Journal of Operational Research, v. 40,", "citeRegEx": "Raman et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Raman et al\\.", "year": 1989}, {"title": "Scheduling in a sequence dependent setup environment with genetic search", "author": ["P.A. Rubin", "G.L. Ragatz"], "venue": "Computers & Operations Research, v. 22,", "citeRegEx": "Rubin and Ragatz,? \\Q1995\\E", "shortCiteRegEx": "Rubin and Ragatz", "year": 1995}, {"title": "A simple and effective metaheuristic for the minimum latency problem", "author": ["M.M. Silva", "A. Subramanian", "T. Vidal", "L.S. Ochi"], "venue": "European Journal of Operational Research, v. 221,", "citeRegEx": "Silva et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2012}, {"title": "Heuristic, Exact and Hybrid Approaches for Vehicle Routing Problems", "author": ["A. Subramanian"], "venue": "PhD thesis,", "citeRegEx": "Subramanian,? \\Q2012\\E", "shortCiteRegEx": "Subramanian", "year": 2012}, {"title": "An iterated local search algorithm for the travelling salesman problem with pickups and deliveries", "author": ["A. Subramanian", "M. Battarra"], "venue": "Journal of the Operational Research Society, v. 64,", "citeRegEx": "Subramanian and Battarra,? \\Q2013\\E", "shortCiteRegEx": "Subramanian and Battarra", "year": 2013}, {"title": "A parallel heuristic for the vehicle routing problem with simultaneous pickup and delivery", "author": ["A. Subramanian", "L. Drummond", "C. Bentes", "L. Ochi", "R. Farias"], "venue": "Computers & Operations Research,", "citeRegEx": "Subramanian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Subramanian et al\\.", "year": 2010}, {"title": "An iterated local search heuristic for the single machine total weighted tardiness scheduling problem with sequencedependent setup times", "author": ["A. Subramanian", "M. Battarra", "C.N. Potts"], "venue": "International Journal of Production Research,", "citeRegEx": "Subramanian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Subramanian et al\\.", "year": 2014}, {"title": "An exact algorithm for the single-machine total weighted tardiness problem with sequence-dependent setup times", "author": ["S. Tanaka", "M. Araki"], "venue": "Computers & Operations Research, v. 40,", "citeRegEx": "Tanaka and Araki,? \\Q2013\\E", "shortCiteRegEx": "Tanaka and Araki", "year": 2013}, {"title": "A discrete differential evolution algorithm for the single machine total weighted tardiness problem with sequence dependent setup times", "author": ["M.F. Tasgetiren", "Pan", "Q.-K", "Liang", "Y.-C"], "venue": "Computers & Operations Research, v. 36,", "citeRegEx": "Tasgetiren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tasgetiren et al\\.", "year": 2009}, {"title": "Beam search algorithms for the single machine total weighted tardiness scheduling problem with sequence-dependent setups", "author": ["J.M. Valente", "R.A. Alves"], "venue": "Computers & Operations Research, v. 35,", "citeRegEx": "Valente and Alves,? \\Q2008\\E", "shortCiteRegEx": "Valente and Alves", "year": 2008}, {"title": "Hybrid metaheuristics for the clustered vehicle routing problem", "author": ["T. Vidal", "M. Battarra", "A. Subramanian", "G. Erdo\u01e7an"], "venue": "Computers & Operations Research.,", "citeRegEx": "Vidal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vidal et al\\.", "year": 2015}, {"title": "Iterated local search for single-machine scheduling with sequence-dependent setup times to minimize total weighted tardiness", "author": ["H. Xu", "Z. L\u00fc", "T. Cheng"], "venue": "Journal of Scheduling,", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "A study of hybrid evolutionary algorithms for single machine scheduling problem with sequence-dependent setup times", "author": ["H. Xu", "Z. L\u00fc", "A. Yin", "L. Shen", "U. Buscher"], "venue": "Computers & Operations Research,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Sequencing single-machine tardiness problems with sequence dependent setup times using an iterated greedy heuristic", "author": ["Ying", "K.-C", "Lin", "S.-W", "Huang", "C.-Y"], "venue": "Expert Systems with Applications, v. 36,", "citeRegEx": "Ying et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ying et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 13, "context": "In particular, local search based metaheuristics had been quite effective in generating high quality solutions for this problem (Kirlik and O\u011fuz, 2012; Xu et al., 2013; Subramanian et al., 2014).", "startOffset": 128, "endOffset": 194}, {"referenceID": 38, "context": "In particular, local search based metaheuristics had been quite effective in generating high quality solutions for this problem (Kirlik and O\u011fuz, 2012; Xu et al., 2013; Subramanian et al., 2014).", "startOffset": 128, "endOffset": 194}, {"referenceID": 33, "context": "In particular, local search based metaheuristics had been quite effective in generating high quality solutions for this problem (Kirlik and O\u011fuz, 2012; Xu et al., 2013; Subramanian et al., 2014).", "startOffset": 128, "endOffset": 194}, {"referenceID": 30, "context": "The proposed local search strategy has been tested in classical benchmark instances using the ILS-RVND metaheuristic (Subramanian, 2012).", "startOffset": 117, "endOffset": 136}, {"referenceID": 13, "context": "Such version is usually referred to as 1|| \u2211 wjTj , which is known to be NP-hard in a strong sense Lawler (1977); Lenstra et al.", "startOffset": 99, "endOffset": 113}, {"referenceID": 13, "context": "Such version is usually referred to as 1|| \u2211 wjTj , which is known to be NP-hard in a strong sense Lawler (1977); Lenstra et al. (1977). Therefore, problem 1|sij| \u2211 wjTj is also NP-hard since it includes problem 1|| \u2211 wjTj as a particular case.", "startOffset": 99, "endOffset": 136}, {"referenceID": 13, "context": "In particular, local search based metaheuristics had been quite effective in generating high quality solutions for this problem (Kirlik and O\u011fuz, 2012; Xu et al., 2013; Subramanian et al., 2014). Nevertheless, one of the main limitations of such methods is the computational cost of evaluating a move during the local search. Typically, the number of possible moves in classical neighborhoods such as insertion and swap is O(n), whereas the complexity of evaluating each move from these neighborhoods is O(n), when performed in a straightforward fashion. Hence, the overall complexity of examining these neighborhoods is O(n). Recently, Liao et al. (2012) presented a sophisticated method that reduces the complexity of enumerating and evaluating all moves from the aforementioned neighborhoods from O(n) to O(nlogn).", "startOffset": 129, "endOffset": 656}, {"referenceID": 13, "context": "In particular, local search based metaheuristics had been quite effective in generating high quality solutions for this problem (Kirlik and O\u011fuz, 2012; Xu et al., 2013; Subramanian et al., 2014). Nevertheless, one of the main limitations of such methods is the computational cost of evaluating a move during the local search. Typically, the number of possible moves in classical neighborhoods such as insertion and swap is O(n), whereas the complexity of evaluating each move from these neighborhoods is O(n), when performed in a straightforward fashion. Hence, the overall complexity of examining these neighborhoods is O(n). Recently, Liao et al. (2012) presented a sophisticated method that reduces the complexity of enumerating and evaluating all moves from the aforementioned neighborhoods from O(n) to O(nlogn). However, in practice, this procedure is only useful for very large instances. For example, they showed empirically that the advantage of using their approach for the neighborhood swap starts to be visibly significant only for instances with more than around 1000 jobs. The aim of this work is to develop a simple yet effective strategy that speeds up the local search procedure without a significant loss in the solution quality. Such strategy consists of a filtering mechanism that prevents unpromising moves to be evaluated. The idea behind this approach relies on a measurement that must be computed at runtime. In our case, we use the setup variation, which can be computed in O(1) time to estimate if a move is promising or not to be evaluated. Moreover, the bottleneck of traditional implementations limits the number of neighborhoods to be explored in the local search. Our proposed strategy enables the use of a large number of neighborhoods, such as moving blocks of consecutive jobs, which is likely to lead to high quality solutions. In fact, Xu et al. (2013) showed that this type of neighborhood may indeed lead to better solutions for problem 1|sij| \u2211 wjTj .", "startOffset": 129, "endOffset": 1889}, {"referenceID": 8, "context": "(1983) combined with concepts of the metaheuristic Greedy Randomized Adaptive Search Procedure (GRASP) (Feo and Resende, 1995) and some priority rules such as the Earliest Weighted Due Date (EWDD) and ATCS.", "startOffset": 103, "endOffset": 126}, {"referenceID": 18, "context": "One of the first methods proposed for problem 1|sij| \u2211 wjTj was that of Raman et al. (1989). It consists of a constructive heuristic based on a static dispatching rule.", "startOffset": 72, "endOffset": 92}, {"referenceID": 9, "context": "Lee et al. (1997) later developed a three-phase method, where the first performs a statistical analysis on the instance to define the parameters to be used, the second is a constructive heuristic that is based on dynamic dispatching rule called Apparent Tardiness Cost with Setups (ATCS), while the third performs a local search by means of insertion and swap moves.", "startOffset": 0, "endOffset": 18}, {"referenceID": 2, "context": "The 1|sij| \u2211 wjTj literature remained practically unchanged for nearly two decades until Cicirello and Smith (2005) developed five randomized based (meta)heuristic approaches, more precisely, Limited Discrepancy Search (LDS); Heuristic-Biased Stochastic Sampling (HBSS); Value-Biased Stochastic Sampling (VBSS); hill-climbing incorporated to VBSS (VBSS-HC) and finally a Simulated Annealing (SA) algorithm.", "startOffset": 89, "endOffset": 116}, {"referenceID": 2, "context": "The 1|sij| \u2211 wjTj literature remained practically unchanged for nearly two decades until Cicirello and Smith (2005) developed five randomized based (meta)heuristic approaches, more precisely, Limited Discrepancy Search (LDS); Heuristic-Biased Stochastic Sampling (HBSS); Value-Biased Stochastic Sampling (VBSS); hill-climbing incorporated to VBSS (VBSS-HC) and finally a Simulated Annealing (SA) algorithm. The authors also proposed a set of 120 instances that has become the most used benchmark dataset for the problem. Cicirello (2006) later implemented a Genetic Algorithm (GA) with a new operator called Non-Wrapping Order Crossover (NWOX) that maintains not only the relative order of the jobs in a sequence, but also the absolute one.", "startOffset": 89, "endOffset": 538}, {"referenceID": 2, "context": "The 1|sij| \u2211 wjTj literature remained practically unchanged for nearly two decades until Cicirello and Smith (2005) developed five randomized based (meta)heuristic approaches, more precisely, Limited Discrepancy Search (LDS); Heuristic-Biased Stochastic Sampling (HBSS); Value-Biased Stochastic Sampling (VBSS); hill-climbing incorporated to VBSS (VBSS-HC) and finally a Simulated Annealing (SA) algorithm. The authors also proposed a set of 120 instances that has become the most used benchmark dataset for the problem. Cicirello (2006) later implemented a Genetic Algorithm (GA) with a new operator called Non-Wrapping Order Crossover (NWOX) that maintains not only the relative order of the jobs in a sequence, but also the absolute one. Three metaheuristics were implemented by Lin and Ying (2007), more specifically, SA, GA and Tabu Search (TS), whereas Ant Colony Optimization (ACO) based algorithms were put forward by Liao and Cheng (2007), Anghinolfi and Paolucci (2008) and Mandahawi et al.", "startOffset": 89, "endOffset": 802}, {"referenceID": 2, "context": "The 1|sij| \u2211 wjTj literature remained practically unchanged for nearly two decades until Cicirello and Smith (2005) developed five randomized based (meta)heuristic approaches, more precisely, Limited Discrepancy Search (LDS); Heuristic-Biased Stochastic Sampling (HBSS); Value-Biased Stochastic Sampling (VBSS); hill-climbing incorporated to VBSS (VBSS-HC) and finally a Simulated Annealing (SA) algorithm. The authors also proposed a set of 120 instances that has become the most used benchmark dataset for the problem. Cicirello (2006) later implemented a Genetic Algorithm (GA) with a new operator called Non-Wrapping Order Crossover (NWOX) that maintains not only the relative order of the jobs in a sequence, but also the absolute one. Three metaheuristics were implemented by Lin and Ying (2007), more specifically, SA, GA and Tabu Search (TS), whereas Ant Colony Optimization (ACO) based algorithms were put forward by Liao and Cheng (2007), Anghinolfi and Paolucci (2008) and Mandahawi et al.", "startOffset": 89, "endOffset": 948}, {"referenceID": 0, "context": "Three metaheuristics were implemented by Lin and Ying (2007), more specifically, SA, GA and Tabu Search (TS), whereas Ant Colony Optimization (ACO) based algorithms were put forward by Liao and Cheng (2007), Anghinolfi and Paolucci (2008) and Mandahawi et al.", "startOffset": 208, "endOffset": 239}, {"referenceID": 0, "context": "Three metaheuristics were implemented by Lin and Ying (2007), more specifically, SA, GA and Tabu Search (TS), whereas Ant Colony Optimization (ACO) based algorithms were put forward by Liao and Cheng (2007), Anghinolfi and Paolucci (2008) and Mandahawi et al. (2011). The latter authors actually developed a variant of ACO, called Max-Min Ant System (MMAS), that was capable of generating better solutions when compared to the other two.", "startOffset": 208, "endOffset": 267}, {"referenceID": 0, "context": "Three metaheuristics were implemented by Lin and Ying (2007), more specifically, SA, GA and Tabu Search (TS), whereas Ant Colony Optimization (ACO) based algorithms were put forward by Liao and Cheng (2007), Anghinolfi and Paolucci (2008) and Mandahawi et al. (2011). The latter authors actually developed a variant of ACO, called Max-Min Ant System (MMAS), that was capable of generating better solutions when compared to the other two. Valente and Alves (2008) developed a Beam Search (BS) algorithm, while a Discrete Particle Swarm Optimization (DSPO) heuristic was proposed by Anghinolfi and Paolucci (2009).", "startOffset": 208, "endOffset": 463}, {"referenceID": 0, "context": "Three metaheuristics were implemented by Lin and Ying (2007), more specifically, SA, GA and Tabu Search (TS), whereas Ant Colony Optimization (ACO) based algorithms were put forward by Liao and Cheng (2007), Anghinolfi and Paolucci (2008) and Mandahawi et al. (2011). The latter authors actually developed a variant of ACO, called Max-Min Ant System (MMAS), that was capable of generating better solutions when compared to the other two. Valente and Alves (2008) developed a Beam Search (BS) algorithm, while a Discrete Particle Swarm Optimization (DSPO) heuristic was proposed by Anghinolfi and Paolucci (2009). Tasgetiren et al.", "startOffset": 208, "endOffset": 612}, {"referenceID": 0, "context": "Three metaheuristics were implemented by Lin and Ying (2007), more specifically, SA, GA and Tabu Search (TS), whereas Ant Colony Optimization (ACO) based algorithms were put forward by Liao and Cheng (2007), Anghinolfi and Paolucci (2008) and Mandahawi et al. (2011). The latter authors actually developed a variant of ACO, called Max-Min Ant System (MMAS), that was capable of generating better solutions when compared to the other two. Valente and Alves (2008) developed a Beam Search (BS) algorithm, while a Discrete Particle Swarm Optimization (DSPO) heuristic was proposed by Anghinolfi and Paolucci (2009). Tasgetiren et al. (2009) put forward a Discrete Differential Evolution (DDE) algorithm that was enhanced by the NEH constructive heuristic of Nawaz et al.", "startOffset": 208, "endOffset": 638}, {"referenceID": 0, "context": "Three metaheuristics were implemented by Lin and Ying (2007), more specifically, SA, GA and Tabu Search (TS), whereas Ant Colony Optimization (ACO) based algorithms were put forward by Liao and Cheng (2007), Anghinolfi and Paolucci (2008) and Mandahawi et al. (2011). The latter authors actually developed a variant of ACO, called Max-Min Ant System (MMAS), that was capable of generating better solutions when compared to the other two. Valente and Alves (2008) developed a Beam Search (BS) algorithm, while a Discrete Particle Swarm Optimization (DSPO) heuristic was proposed by Anghinolfi and Paolucci (2009). Tasgetiren et al. (2009) put forward a Discrete Differential Evolution (DDE) algorithm that was enhanced by the NEH constructive heuristic of Nawaz et al. (1983) combined with concepts of the metaheuristic Greedy Randomized Adaptive Search Procedure (GRASP) (Feo and Resende, 1995) and some priority rules such as the Earliest Weighted Due Date (EWDD) and ATCS.", "startOffset": 208, "endOffset": 775}, {"referenceID": 0, "context": "Three metaheuristics were implemented by Lin and Ying (2007), more specifically, SA, GA and Tabu Search (TS), whereas Ant Colony Optimization (ACO) based algorithms were put forward by Liao and Cheng (2007), Anghinolfi and Paolucci (2008) and Mandahawi et al. (2011). The latter authors actually developed a variant of ACO, called Max-Min Ant System (MMAS), that was capable of generating better solutions when compared to the other two. Valente and Alves (2008) developed a Beam Search (BS) algorithm, while a Discrete Particle Swarm Optimization (DSPO) heuristic was proposed by Anghinolfi and Paolucci (2009). Tasgetiren et al. (2009) put forward a Discrete Differential Evolution (DDE) algorithm that was enhanced by the NEH constructive heuristic of Nawaz et al. (1983) combined with concepts of the metaheuristic Greedy Randomized Adaptive Search Procedure (GRASP) (Feo and Resende, 1995) and some priority rules such as the Earliest Weighted Due Date (EWDD) and ATCS. The authors also implemented destruction and construction procedures to determine a mutant population. Bo\u017cejko (2010) proposed a parallel Scatter Search (ParSS) algorithm combined with a Path-Relinking scheme.", "startOffset": 208, "endOffset": 1093}, {"referenceID": 0, "context": "Three metaheuristics were implemented by Lin and Ying (2007), more specifically, SA, GA and Tabu Search (TS), whereas Ant Colony Optimization (ACO) based algorithms were put forward by Liao and Cheng (2007), Anghinolfi and Paolucci (2008) and Mandahawi et al. (2011). The latter authors actually developed a variant of ACO, called Max-Min Ant System (MMAS), that was capable of generating better solutions when compared to the other two. Valente and Alves (2008) developed a Beam Search (BS) algorithm, while a Discrete Particle Swarm Optimization (DSPO) heuristic was proposed by Anghinolfi and Paolucci (2009). Tasgetiren et al. (2009) put forward a Discrete Differential Evolution (DDE) algorithm that was enhanced by the NEH constructive heuristic of Nawaz et al. (1983) combined with concepts of the metaheuristic Greedy Randomized Adaptive Search Procedure (GRASP) (Feo and Resende, 1995) and some priority rules such as the Earliest Weighted Due Date (EWDD) and ATCS. The authors also implemented destruction and construction procedures to determine a mutant population. Bo\u017cejko (2010) proposed a parallel Scatter Search (ParSS) algorithm combined with a Path-Relinking scheme. Chao and Liao (2012) implemented a so-called Discrete Electromagnetism-like Mechanism (DEM), which is a metaheuristic based on the electromagnetic theory of attraction and repulsion.", "startOffset": 208, "endOffset": 1206}, {"referenceID": 22, "context": "Diversification moves are performed using the double-bridge perturbation (Martin et al., 1991), which was originally developed for the Traveling Salesman Problem (TSP).", "startOffset": 73, "endOffset": 94}, {"referenceID": 5, "context": "Deng and Gu (2014) put forward an Enhanced Iterated Greedy (EIG) algorithm that generates an initial solution using the ATCS heuristic, performs local search with swap and insertion moves and perturbs a solution by applying successive insertion moves.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Deng and Gu (2014) put forward an Enhanced Iterated Greedy (EIG) algorithm that generates an initial solution using the ATCS heuristic, performs local search with swap and insertion moves and perturbs a solution by applying successive insertion moves. Some elimination rules were also suggested for the swap neighborhood. Xu et al. (2014) proposed different versions of a Hybrid Evolutionary Algorithm (HEA) by combining two population updating strategies with three crossover operators, including the linear order crossover operator (LOX).", "startOffset": 0, "endOffset": 339}, {"referenceID": 5, "context": "Deng and Gu (2014) put forward an Enhanced Iterated Greedy (EIG) algorithm that generates an initial solution using the ATCS heuristic, performs local search with swap and insertion moves and perturbs a solution by applying successive insertion moves. Some elimination rules were also suggested for the swap neighborhood. Xu et al. (2014) proposed different versions of a Hybrid Evolutionary Algorithm (HEA) by combining two population updating strategies with three crossover operators, including the linear order crossover operator (LOX). The initial population is generated at random and a local search is applied using the neighborhood l-block as in Xu et al. (2013). The version that yielded the best results was denoted LOX\u2295B.", "startOffset": 0, "endOffset": 671}, {"referenceID": 5, "context": "Deng and Gu (2014) put forward an Enhanced Iterated Greedy (EIG) algorithm that generates an initial solution using the ATCS heuristic, performs local search with swap and insertion moves and perturbs a solution by applying successive insertion moves. Some elimination rules were also suggested for the swap neighborhood. Xu et al. (2014) proposed different versions of a Hybrid Evolutionary Algorithm (HEA) by combining two population updating strategies with three crossover operators, including the linear order crossover operator (LOX). The initial population is generated at random and a local search is applied using the neighborhood l-block as in Xu et al. (2013). The version that yielded the best results was denoted LOX\u2295B. Guo and Tang (2015) suggested a Scatter Search (SS) based algorithm that combines many ideas from different methods such as ATCS, VNS and DDE with a new adaptive strategy for updating the reference set.", "startOffset": 0, "endOffset": 753}, {"referenceID": 5, "context": "Deng and Gu (2014) put forward an Enhanced Iterated Greedy (EIG) algorithm that generates an initial solution using the ATCS heuristic, performs local search with swap and insertion moves and perturbs a solution by applying successive insertion moves. Some elimination rules were also suggested for the swap neighborhood. Xu et al. (2014) proposed different versions of a Hybrid Evolutionary Algorithm (HEA) by combining two population updating strategies with three crossover operators, including the linear order crossover operator (LOX). The initial population is generated at random and a local search is applied using the neighborhood l-block as in Xu et al. (2013). The version that yielded the best results was denoted LOX\u2295B. Guo and Tang (2015) suggested a Scatter Search (SS) based algorithm that combines many ideas from different methods such as ATCS, VNS and DDE with a new adaptive strategy for updating the reference set. To the best of our knowledge, the ILS-RVND heuristic of Subramanian et al. (2014), the ILS of Xu et al.", "startOffset": 0, "endOffset": 1018}, {"referenceID": 5, "context": "Deng and Gu (2014) put forward an Enhanced Iterated Greedy (EIG) algorithm that generates an initial solution using the ATCS heuristic, performs local search with swap and insertion moves and perturbs a solution by applying successive insertion moves. Some elimination rules were also suggested for the swap neighborhood. Xu et al. (2014) proposed different versions of a Hybrid Evolutionary Algorithm (HEA) by combining two population updating strategies with three crossover operators, including the linear order crossover operator (LOX). The initial population is generated at random and a local search is applied using the neighborhood l-block as in Xu et al. (2013). The version that yielded the best results was denoted LOX\u2295B. Guo and Tang (2015) suggested a Scatter Search (SS) based algorithm that combines many ideas from different methods such as ATCS, VNS and DDE with a new adaptive strategy for updating the reference set. To the best of our knowledge, the ILS-RVND heuristic of Subramanian et al. (2014), the ILS of Xu et al. (2013) and the HEA of Xu et al.", "startOffset": 0, "endOffset": 1047}, {"referenceID": 5, "context": "Deng and Gu (2014) put forward an Enhanced Iterated Greedy (EIG) algorithm that generates an initial solution using the ATCS heuristic, performs local search with swap and insertion moves and perturbs a solution by applying successive insertion moves. Some elimination rules were also suggested for the swap neighborhood. Xu et al. (2014) proposed different versions of a Hybrid Evolutionary Algorithm (HEA) by combining two population updating strategies with three crossover operators, including the linear order crossover operator (LOX). The initial population is generated at random and a local search is applied using the neighborhood l-block as in Xu et al. (2013). The version that yielded the best results was denoted LOX\u2295B. Guo and Tang (2015) suggested a Scatter Search (SS) based algorithm that combines many ideas from different methods such as ATCS, VNS and DDE with a new adaptive strategy for updating the reference set. To the best of our knowledge, the ILS-RVND heuristic of Subramanian et al. (2014), the ILS of Xu et al. (2013) and the HEA of Xu et al. (2014) are the best algorithms, at least in terms of solution quality, proposed for problem 1|sij| \u2211 wjTj .", "startOffset": 0, "endOffset": 1079}, {"referenceID": 5, "context": "Deng and Gu (2014) put forward an Enhanced Iterated Greedy (EIG) algorithm that generates an initial solution using the ATCS heuristic, performs local search with swap and insertion moves and perturbs a solution by applying successive insertion moves. Some elimination rules were also suggested for the swap neighborhood. Xu et al. (2014) proposed different versions of a Hybrid Evolutionary Algorithm (HEA) by combining two population updating strategies with three crossover operators, including the linear order crossover operator (LOX). The initial population is generated at random and a local search is applied using the neighborhood l-block as in Xu et al. (2013). The version that yielded the best results was denoted LOX\u2295B. Guo and Tang (2015) suggested a Scatter Search (SS) based algorithm that combines many ideas from different methods such as ATCS, VNS and DDE with a new adaptive strategy for updating the reference set. To the best of our knowledge, the ILS-RVND heuristic of Subramanian et al. (2014), the ILS of Xu et al. (2013) and the HEA of Xu et al. (2014) are the best algorithms, at least in terms of solution quality, proposed for problem 1|sij| \u2211 wjTj . Tanaka and Araki (2013) proposed an exact algorithm, called Successive Sublimation Dynamic Programming (SSDP), that was capable of solving instances of the benchmark dataset of Cicirello and Smith (2005).", "startOffset": 0, "endOffset": 1204}, {"referenceID": 5, "context": "Tanaka and Araki (2013) proposed an exact algorithm, called Successive Sublimation Dynamic Programming (SSDP), that was capable of solving instances of the benchmark dataset of Cicirello and Smith (2005). At first, a column generation or a conjugate subgradient procedure is applied over the lagrangean relaxation of the original problem, solved by dynamic programming.", "startOffset": 177, "endOffset": 204}, {"referenceID": 5, "context": "LDS, HBSS, VBSS Cicirello and Smith (2005) 2005 VBSS-HC, SA Not reported", "startOffset": 16, "endOffset": 43}, {"referenceID": 36, "context": "Insertion, 2-block swap, Valente and Alves (2008) 2008 BS 3-block swap", "startOffset": 25, "endOffset": 50}, {"referenceID": 13, "context": "Insertion, Kirlik and O\u011fuz (2012) 2012 GVNS 2-block insertion, swap", "startOffset": 11, "endOffset": 34}, {"referenceID": 30, "context": "Insertion, 2-block insertion, Subramanian et al. (2014) 2014 ILS-RVND 3-Insertion, swap, block reverse", "startOffset": 30, "endOffset": 56}, {"referenceID": 30, "context": "Finally, we show how we have embedded the developed approach into the ILS-RVND metaheuristic (Subramanian, 2012).", "startOffset": 93, "endOffset": 112}, {"referenceID": 16, "context": "Liao et al. (2012) developed a rather complicated procedure that runs in O(nlogn) time for preprocessing the auxiliary data stuctures necessary for performing the move evaluation of the neighborhoods swap, insertion and block reverse in constant time.", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "It is worthy of note that, very recently, Guo and Tang (2015) had independently developed a similar but not identical approach to disregard unpromising moves based on the total setup time instead of the setup variation.", "startOffset": 42, "endOffset": 62}, {"referenceID": 38, "context": "In practice one can define a single neighborhood considering all values of l \u2208 L at once (see Xu et al. (2013)), or define multiple neighborhoods where each of them is associated with a given value of l \u2208 L.", "startOffset": 94, "endOffset": 111}, {"referenceID": 30, "context": "In this section we explain how we embedded the proposed approach in the ILS-RVND metaheuristic (Subramanian, 2012).", "startOffset": 95, "endOffset": 114}, {"referenceID": 33, "context": "The reason for choosing this metaheuristic to test our local search limitation strategy is that it was found capable of generating highly competitive results not only for problem 1|sij| \u2211 wjTj (Subramanian et al., 2014), as mentioned in Section 2, but also for other combinatorial optimization problems (Subramanian et al.", "startOffset": 193, "endOffset": 219}, {"referenceID": 32, "context": ", 2014), as mentioned in Section 2, but also for other combinatorial optimization problems (Subramanian et al., 2010; Subramanian, 2012; Silva et al., 2012; Penna et al., 2013; Subramanian and Battarra, 2013; Martinelli et al., 2013; Vidal et al., 2015).", "startOffset": 91, "endOffset": 253}, {"referenceID": 30, "context": ", 2014), as mentioned in Section 2, but also for other combinatorial optimization problems (Subramanian et al., 2010; Subramanian, 2012; Silva et al., 2012; Penna et al., 2013; Subramanian and Battarra, 2013; Martinelli et al., 2013; Vidal et al., 2015).", "startOffset": 91, "endOffset": 253}, {"referenceID": 29, "context": ", 2014), as mentioned in Section 2, but also for other combinatorial optimization problems (Subramanian et al., 2010; Subramanian, 2012; Silva et al., 2012; Penna et al., 2013; Subramanian and Battarra, 2013; Martinelli et al., 2013; Vidal et al., 2015).", "startOffset": 91, "endOffset": 253}, {"referenceID": 26, "context": ", 2014), as mentioned in Section 2, but also for other combinatorial optimization problems (Subramanian et al., 2010; Subramanian, 2012; Silva et al., 2012; Penna et al., 2013; Subramanian and Battarra, 2013; Martinelli et al., 2013; Vidal et al., 2015).", "startOffset": 91, "endOffset": 253}, {"referenceID": 31, "context": ", 2014), as mentioned in Section 2, but also for other combinatorial optimization problems (Subramanian et al., 2010; Subramanian, 2012; Silva et al., 2012; Penna et al., 2013; Subramanian and Battarra, 2013; Martinelli et al., 2013; Vidal et al., 2015).", "startOffset": 91, "endOffset": 253}, {"referenceID": 23, "context": ", 2014), as mentioned in Section 2, but also for other combinatorial optimization problems (Subramanian et al., 2010; Subramanian, 2012; Silva et al., 2012; Penna et al., 2013; Subramanian and Battarra, 2013; Martinelli et al., 2013; Vidal et al., 2015).", "startOffset": 91, "endOffset": 253}, {"referenceID": 37, "context": ", 2014), as mentioned in Section 2, but also for other combinatorial optimization problems (Subramanian et al., 2010; Subramanian, 2012; Silva et al., 2012; Penna et al., 2013; Subramanian and Battarra, 2013; Martinelli et al., 2013; Vidal et al., 2015).", "startOffset": 91, "endOffset": 253}, {"referenceID": 30, "context": "The reader is referred to Subramanian et al. (2014) for implementation details about these procedures.", "startOffset": 26, "endOffset": 52}, {"referenceID": 30, "context": "The reader is referred to Subramanian et al. (2014) for implementation details about these procedures. The local search method of both algorithms is performed by a RVND procedure, which consists of selecting an unexplored neighborhood at random whenever another one fails to find an improved solution. In case of improvement, all neighborhoods are reconsidered to be explored. Note that in both algorithms the set L associated to the l-block neighborhood is provided as an input parameter to be tuned (see Section 4.1), as opposed to the ILSRVND presented in Subramanian et al. (2014) where this set was predefined as L = {1, 2, 3}, that is, the l-block neighborhoods were limited to 1-, 2- and 3-block insertion.", "startOffset": 26, "endOffset": 585}, {"referenceID": 30, "context": "The reader is referred to Subramanian et al. (2014) for implementation details about these procedures. The local search method of both algorithms is performed by a RVND procedure, which consists of selecting an unexplored neighborhood at random whenever another one fails to find an improved solution. In case of improvement, all neighborhoods are reconsidered to be explored. Note that in both algorithms the set L associated to the l-block neighborhood is provided as an input parameter to be tuned (see Section 4.1), as opposed to the ILSRVND presented in Subramanian et al. (2014) where this set was predefined as L = {1, 2, 3}, that is, the l-block neighborhoods were limited to 1-, 2- and 3-block insertion. In addition, the neighborhood block reverse was also used in a restricted fashion by the authors, but it was not considered here because it did not seem to be crucial for obtaining high quality solutions. One last remark is that the algorithm stops when a sequence \u03c0 with cost f(\u03c0) = 0 is found. When this happens it is clear that an optimal solution was obtained and thus there is no point in continuing the search. This was not originally considered in the ILS-RVND presented in Subramanian et al. (2014), but it has been considered here in both versions.", "startOffset": 26, "endOffset": 1221}, {"referenceID": 5, "context": "The 120 instances of Cicirello and Smith (2005) were used to evaluate the performance of the proposed algorithms.", "startOffset": 21, "endOffset": 48}, {"referenceID": 5, "context": "Table 2: Group of instances generated by Cicirello and Smith (2005) for problem 1|sij | \u2211 wjTj", "startOffset": 41, "endOffset": 68}, {"referenceID": 30, "context": "To have a better idea of the impact of the modifications introduced in the original ILSRVND, we decided to use the same configuration for the number of ILS iterations as in Subramanian et al. (2014), that is, IILS = 4 \u00d7 n.", "startOffset": 173, "endOffset": 199}, {"referenceID": 30, "context": "To have a better idea of the impact of the modifications introduced in the original ILSRVND, we decided to use the same configuration for the number of ILS iterations as in Subramanian et al. (2014), that is, IILS = 4 \u00d7 n. However, since each iteration of the algorithm became much faster after implementing the limitation strategy (see Section 4.3) we decided to set IR = 20, instead of IR = 10, as in Subramanian et al. (2014), because it seemed to provide a better compromise between solution quality and computational time.", "startOffset": 173, "endOffset": 429}, {"referenceID": 30, "context": "To have a better idea of the impact of the modifications introduced in the original ILSRVND, we decided to use the same configuration for the number of ILS iterations as in Subramanian et al. (2014), that is, IILS = 4 \u00d7 n. However, since each iteration of the algorithm became much faster after implementing the limitation strategy (see Section 4.3) we decided to set IR = 20, instead of IR = 10, as in Subramanian et al. (2014), because it seemed to provide a better compromise between solution quality and computational time. A set of 17 challenging instances was selected for tuning the parameters L and \u03b8, namely instances 1, 2, 3, 4, 5, 7, 8. 9, 10, 11, 13, 14, 15, 16, 18, 20 and 24. The criterion used for choosing these instances was based on the difficulty faced by the ILS-RVND implemented in Subramanian et al. (2014) in finding their corresponding optimal solutions.", "startOffset": 173, "endOffset": 829}, {"referenceID": 0, "context": "ACOAP: Ant Colony Optimization of Anghinolfi and Paolucci (2008). Best of 10 runs.", "startOffset": 34, "endOffset": 65}, {"referenceID": 0, "context": "ACOAP: Ant Colony Optimization of Anghinolfi and Paolucci (2008). Best of 10 runs. DPSO: Discrete Particle Swarm Optimization of Anghinolfi and Paolucci (2009). Best of 10 runs.", "startOffset": 34, "endOffset": 160}, {"referenceID": 0, "context": "ACOAP: Ant Colony Optimization of Anghinolfi and Paolucci (2008). Best of 10 runs. DPSO: Discrete Particle Swarm Optimization of Anghinolfi and Paolucci (2009). Best of 10 runs. DDE: Discrete Differential Evolutionary heuristic of Tasgetiren et al. (2009). Best of 10 runs.", "startOffset": 34, "endOffset": 256}, {"referenceID": 13, "context": "GVNS: General Variable Neighborhood Search of Kirlik and O\u011fuz (2012). Best of 20 runs.", "startOffset": 46, "endOffset": 69}, {"referenceID": 13, "context": "GVNS: General Variable Neighborhood Search of Kirlik and O\u011fuz (2012). Best of 20 runs. ILSXLC: Iterated Local Search of Xu et al. (2013). Best and average of 100 runs.", "startOffset": 46, "endOffset": 137}, {"referenceID": 13, "context": "GVNS: General Variable Neighborhood Search of Kirlik and O\u011fuz (2012). Best of 20 runs. ILSXLC: Iterated Local Search of Xu et al. (2013). Best and average of 100 runs. ILS-RVNDSBP: Iterated Local Search + Randomized Variable Neighborhood Descent of Subramanian et al. (2014). Best and average of 10 runs.", "startOffset": 46, "endOffset": 275}, {"referenceID": 13, "context": "GVNS: General Variable Neighborhood Search of Kirlik and O\u011fuz (2012). Best of 20 runs. ILSXLC: Iterated Local Search of Xu et al. (2013). Best and average of 100 runs. ILS-RVNDSBP: Iterated Local Search + Randomized Variable Neighborhood Descent of Subramanian et al. (2014). Best and average of 10 runs. LOX\u2295B: Hybrid Evolutionary Algorithm of Xu et al. (2014). Best and average of 20 runs.", "startOffset": 46, "endOffset": 362}, {"referenceID": 13, "context": "GVNS: General Variable Neighborhood Search of Kirlik and O\u011fuz (2012). Best of 20 runs. ILSXLC: Iterated Local Search of Xu et al. (2013). Best and average of 100 runs. ILS-RVNDSBP: Iterated Local Search + Randomized Variable Neighborhood Descent of Subramanian et al. (2014). Best and average of 10 runs. LOX\u2295B: Hybrid Evolutionary Algorithm of Xu et al. (2014). Best and average of 20 runs. Opt: Optimal solution found by the exact algorithm of Tanaka and Araki (2013).", "startOffset": 46, "endOffset": 470}, {"referenceID": 5, "context": "Table 5: Results for the instances of Cicirello and Smith (2005)", "startOffset": 38, "endOffset": 65}, {"referenceID": 38, "context": "It is worth mentioning that the average solutions of ILSXLC in Xu et al. (2013) and LOX\u2295B in Xu et al.", "startOffset": 63, "endOffset": 80}, {"referenceID": 38, "context": "It is worth mentioning that the average solutions of ILSXLC in Xu et al. (2013) and LOX\u2295B in Xu et al. (2014) were obtained in 100 seconds on an Intel Core i3 3.", "startOffset": 63, "endOffset": 110}, {"referenceID": 8, "context": "GRASP (Feo and Resende, 1995) is a multi-start metaheuristic where at each restart a solution is generated using a greedy randomized approach and then possibly improved by a local search procedure.", "startOffset": 6, "endOffset": 29}, {"referenceID": 24, "context": "VNS (Mladenovi\u0107 and Hansen, 1997) is a local search based metaheuristic that alternates between local search (intensification) and perturbation (diversification) procedures.", "startOffset": 4, "endOffset": 33}, {"referenceID": 13, "context": "We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al.", "startOffset": 63, "endOffset": 86}, {"referenceID": 33, "context": "We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al.", "startOffset": 100, "endOffset": 126}, {"referenceID": 39, "context": ", 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algorithms listed below along with the type of result presented.", "startOffset": 18, "endOffset": 35}, {"referenceID": 23, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al.", "startOffset": 194, "endOffset": 218}, {"referenceID": 8, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al. (2002), also containing 32 instances but ranging from 55 to 85 jobs.", "startOffset": 294, "endOffset": 314}, {"referenceID": 8, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al. (2002), also containing 32 instances but ranging from 55 to 85 jobs. In this case the configuration adopted after tuning the parameters by using the same rationale presented in Section 4.1 was L = {1, . . . , 13} + swap and \u03b8 = 0.75. We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algorithms listed below along with the type of result presented. ACOGPG: Ant Colony of Gagn\u00e9 et al. (2002). Best, worse, average and median of 20 runs for the instances proposed by the authors.", "startOffset": 294, "endOffset": 896}, {"referenceID": 8, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al. (2002), also containing 32 instances but ranging from 55 to 85 jobs. In this case the configuration adopted after tuning the parameters by using the same rationale presented in Section 4.1 was L = {1, . . . , 13} + swap and \u03b8 = 0.75. We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algorithms listed below along with the type of result presented. ACOGPG: Ant Colony of Gagn\u00e9 et al. (2002). Best, worse, average and median of 20 runs for the instances proposed by the authors. Furthermore, the best solutions found by ACOGPG for the instances of Rubin and Ragatz (1995) were reported by Liao and Cheng (2007).", "startOffset": 294, "endOffset": 1076}, {"referenceID": 8, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al. (2002), also containing 32 instances but ranging from 55 to 85 jobs. In this case the configuration adopted after tuning the parameters by using the same rationale presented in Section 4.1 was L = {1, . . . , 13} + swap and \u03b8 = 0.75. We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algorithms listed below along with the type of result presented. ACOGPG: Ant Colony of Gagn\u00e9 et al. (2002). Best, worse, average and median of 20 runs for the instances proposed by the authors. Furthermore, the best solutions found by ACOGPG for the instances of Rubin and Ragatz (1995) were reported by Liao and Cheng (2007). TabuGGP: TS and VNS of Gagn\u00e9 et al.", "startOffset": 294, "endOffset": 1115}, {"referenceID": 8, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al. (2002), also containing 32 instances but ranging from 55 to 85 jobs. In this case the configuration adopted after tuning the parameters by using the same rationale presented in Section 4.1 was L = {1, . . . , 13} + swap and \u03b8 = 0.75. We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algorithms listed below along with the type of result presented. ACOGPG: Ant Colony of Gagn\u00e9 et al. (2002). Best, worse, average and median of 20 runs for the instances proposed by the authors. Furthermore, the best solutions found by ACOGPG for the instances of Rubin and Ragatz (1995) were reported by Liao and Cheng (2007). TabuGGP: TS and VNS of Gagn\u00e9 et al. (2005). Best of 10 runs for the instances of Gagn\u00e9 et al.", "startOffset": 294, "endOffset": 1159}, {"referenceID": 8, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al. (2002), also containing 32 instances but ranging from 55 to 85 jobs. In this case the configuration adopted after tuning the parameters by using the same rationale presented in Section 4.1 was L = {1, . . . , 13} + swap and \u03b8 = 0.75. We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algorithms listed below along with the type of result presented. ACOGPG: Ant Colony of Gagn\u00e9 et al. (2002). Best, worse, average and median of 20 runs for the instances proposed by the authors. Furthermore, the best solutions found by ACOGPG for the instances of Rubin and Ragatz (1995) were reported by Liao and Cheng (2007). TabuGGP: TS and VNS of Gagn\u00e9 et al. (2005). Best of 10 runs for the instances of Gagn\u00e9 et al. (2002), while the results found for the instances of Rubin and Ragatz (1995) were reported by Ying et al.", "startOffset": 294, "endOffset": 1217}, {"referenceID": 8, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al. (2002), also containing 32 instances but ranging from 55 to 85 jobs. In this case the configuration adopted after tuning the parameters by using the same rationale presented in Section 4.1 was L = {1, . . . , 13} + swap and \u03b8 = 0.75. We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algorithms listed below along with the type of result presented. ACOGPG: Ant Colony of Gagn\u00e9 et al. (2002). Best, worse, average and median of 20 runs for the instances proposed by the authors. Furthermore, the best solutions found by ACOGPG for the instances of Rubin and Ragatz (1995) were reported by Liao and Cheng (2007). TabuGGP: TS and VNS of Gagn\u00e9 et al. (2005). Best of 10 runs for the instances of Gagn\u00e9 et al. (2002), while the results found for the instances of Rubin and Ragatz (1995) were reported by Ying et al.", "startOffset": 294, "endOffset": 1287}, {"referenceID": 8, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al. (2002), also containing 32 instances but ranging from 55 to 85 jobs. In this case the configuration adopted after tuning the parameters by using the same rationale presented in Section 4.1 was L = {1, . . . , 13} + swap and \u03b8 = 0.75. We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algorithms listed below along with the type of result presented. ACOGPG: Ant Colony of Gagn\u00e9 et al. (2002). Best, worse, average and median of 20 runs for the instances proposed by the authors. Furthermore, the best solutions found by ACOGPG for the instances of Rubin and Ragatz (1995) were reported by Liao and Cheng (2007). TabuGGP: TS and VNS of Gagn\u00e9 et al. (2005). Best of 10 runs for the instances of Gagn\u00e9 et al. (2002), while the results found for the instances of Rubin and Ragatz (1995) were reported by Ying et al. (2009). GRASPGS: GRASP of Gupta and Smith (2006).", "startOffset": 294, "endOffset": 1323}, {"referenceID": 8, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al. (2002), also containing 32 instances but ranging from 55 to 85 jobs. In this case the configuration adopted after tuning the parameters by using the same rationale presented in Section 4.1 was L = {1, . . . , 13} + swap and \u03b8 = 0.75. We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algorithms listed below along with the type of result presented. ACOGPG: Ant Colony of Gagn\u00e9 et al. (2002). Best, worse, average and median of 20 runs for the instances proposed by the authors. Furthermore, the best solutions found by ACOGPG for the instances of Rubin and Ragatz (1995) were reported by Liao and Cheng (2007). TabuGGP: TS and VNS of Gagn\u00e9 et al. (2005). Best of 10 runs for the instances of Gagn\u00e9 et al. (2002), while the results found for the instances of Rubin and Ragatz (1995) were reported by Ying et al. (2009). GRASPGS: GRASP of Gupta and Smith (2006). Best, worse, average and median of 20 runs for the instances of Gagn\u00e9 et al.", "startOffset": 294, "endOffset": 1365}, {"referenceID": 8, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al. (2002), also containing 32 instances but ranging from 55 to 85 jobs. In this case the configuration adopted after tuning the parameters by using the same rationale presented in Section 4.1 was L = {1, . . . , 13} + swap and \u03b8 = 0.75. We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algorithms listed below along with the type of result presented. ACOGPG: Ant Colony of Gagn\u00e9 et al. (2002). Best, worse, average and median of 20 runs for the instances proposed by the authors. Furthermore, the best solutions found by ACOGPG for the instances of Rubin and Ragatz (1995) were reported by Liao and Cheng (2007). TabuGGP: TS and VNS of Gagn\u00e9 et al. (2005). Best of 10 runs for the instances of Gagn\u00e9 et al. (2002), while the results found for the instances of Rubin and Ragatz (1995) were reported by Ying et al. (2009). GRASPGS: GRASP of Gupta and Smith (2006). Best, worse, average and median of 20 runs for the instances of Gagn\u00e9 et al. (2002). ACOLJ: Ant Colony of Liao and Cheng (2007).", "startOffset": 294, "endOffset": 1450}, {"referenceID": 8, "context": "With a view of better assessing the robustness of the limitation strategy, we decided to test ILS-RVND and ILS-RVNDFast on benchmark instances of problem 1|sij| \u2211 Tj , namely those generated by Rubin and Ragatz (1995), containing 32 instances ranging from 15 to 45 jobs, and those suggested by Gagn\u00e9 et al. (2002), also containing 32 instances but ranging from 55 to 85 jobs. In this case the configuration adopted after tuning the parameters by using the same rationale presented in Section 4.1 was L = {1, . . . , 13} + swap and \u03b8 = 0.75. We compare the results found by ILS-RVNDFast not only withGVNS (Kirlik and O\u011fuz, 2012), ILS-RVNDSBP (Subramanian et al., 2014) and LOX\u2295B (Xu et al., 2014), where in the latter the authors only reported the best of 100 executions, but also with the algorithms listed below along with the type of result presented. ACOGPG: Ant Colony of Gagn\u00e9 et al. (2002). Best, worse, average and median of 20 runs for the instances proposed by the authors. Furthermore, the best solutions found by ACOGPG for the instances of Rubin and Ragatz (1995) were reported by Liao and Cheng (2007). TabuGGP: TS and VNS of Gagn\u00e9 et al. (2005). Best of 10 runs for the instances of Gagn\u00e9 et al. (2002), while the results found for the instances of Rubin and Ragatz (1995) were reported by Ying et al. (2009). GRASPGS: GRASP of Gupta and Smith (2006). Best, worse, average and median of 20 runs for the instances of Gagn\u00e9 et al. (2002). ACOLJ: Ant Colony of Liao and Cheng (2007). Best of 10 runs for all instances.", "startOffset": 294, "endOffset": 1494}, {"referenceID": 2, "context": "ILSANK: Iterated Local Search of Arroyo et al. (2009). Best, worse and average of 20 runs for the instances of Gagn\u00e9 et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 2, "context": "ILSANK: Iterated Local Search of Arroyo et al. (2009). Best, worse and average of 20 runs for the instances of Gagn\u00e9 et al. (2002). IG: Iterated Greedy of Ying et al.", "startOffset": 33, "endOffset": 131}, {"referenceID": 2, "context": "ILSANK: Iterated Local Search of Arroyo et al. (2009). Best, worse and average of 20 runs for the instances of Gagn\u00e9 et al. (2002). IG: Iterated Greedy of Ying et al. (2009). Best of 10 runs for all instances.", "startOffset": 33, "endOffset": 174}, {"referenceID": 2, "context": "ILSANK: Iterated Local Search of Arroyo et al. (2009). Best, worse and average of 20 runs for the instances of Gagn\u00e9 et al. (2002). IG: Iterated Greedy of Ying et al. (2009). Best of 10 runs for all instances. Opt: Optimal solution found by the exact algorithm of Tanaka and Araki (2013), ex-", "startOffset": 33, "endOffset": 288}, {"referenceID": 26, "context": "7 seconds for the instances of Rubin and Ragatz (1995) and 12.", "startOffset": 31, "endOffset": 55}, {"referenceID": 9, "context": "5 seconds for the instances of Gagn\u00e9 et al. (2002).", "startOffset": 31, "endOffset": 51}, {"referenceID": 28, "context": "Table 15: Results for the instances of Rubin and Ragatz (1995)", "startOffset": 39, "endOffset": 63}, {"referenceID": 34, "context": "7 1: Value smaller than the optimum reported by Tanaka and Araki (2013).", "startOffset": 48, "endOffset": 72}, {"referenceID": 9, "context": "Table 16: Results for the instances of Gagn\u00e9 et al. (2002)", "startOffset": 39, "endOffset": 59}, {"referenceID": 34, "context": "5 1: Best upper bound found by Tanaka and Araki (2013); optimality not proven.", "startOffset": 31, "endOffset": 55}, {"referenceID": 30, "context": "The proposed approach was embedded in the ILS-RVND algorithm Subramanian (2012), which is a simple local search based metaheuristic that was successfully applied to many combinatorial optimization problems, including the 1|sij| \u2211 wjTj Subramanian et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 30, "context": "The proposed approach was embedded in the ILS-RVND algorithm Subramanian (2012), which is a simple local search based metaheuristic that was successfully applied to many combinatorial optimization problems, including the 1|sij| \u2211 wjTj Subramanian et al. (2014). This enhanced version of the algorithm was denoted as ILS-RVNDFast.", "startOffset": 61, "endOffset": 261}], "year": 2015, "abstractText": "This paper concerns the single machine total weighted tardiness scheduling with sequence-dependent setup times, usually referred as 1|sij | \u2211 wjTj . In this NP-hard problem, each job has an associated processing time, due date and a weight. For each pair of jobs i and j, there may be a setup time before starting to process j in case this job is scheduled immediately after i. The objective is to determine a schedule that minimizes the total weighted tardiness, where the tardiness of a job is equal to its completion time minus its due date, in case the job is completely processed only after its due date, and is equal to zero otherwise. Due to its complexity, this problem is most commonly solved by heuristics. The aim of this work is to develop a simple yet effective limitation strategy that speeds up the local search procedure without a significant loss in the solution quality. Such strategy consists of a filtering mechanism that prevents unpromising moves to be evaluated. The proposed strategy has been embedded in a local search based metaheuristic from the literature and tested in classical benchmark instances. Computational experiments revealed that the limitation strategy enabled the metaheuristic to be extremely competitive when compared to other algorithms from the literature, since it allowed the use of a large number of neighborhood structures without a significant increase in the CPU time and, consequently, high quality solutions could be achieved in a matter of seconds. In addition, we analyzed the effectiveness of the proposed strategy in two other well-known metaheuristics. Further experiments were also carried out on benchmark instances of problem 1|sij | \u2211 Tj .", "creator": "LaTeX with hyperref package"}}}