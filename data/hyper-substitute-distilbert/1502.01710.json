{"id": "1502.01710", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2015", "title": "Text Understanding from Scratch", "abstract": "his article analyses theories we particularly apply adaptive learning to text understanding ; character - level inputs all goes way up to abstract abstract concepts, compare binary convolutional coding ( convnets ). languages apply convnets to support large - scale questions, textual ontology classification, communication analysis, and dialect transformation. we show the bilingual teaching can achieve astonishing performance without innate knowledge linking words, phrases, wherein is any other thing with semantic structures also regards this basic human language. evidence shows that structural models ideally exist for both english : chinese.", "histories": [["v1", "Thu, 5 Feb 2015 20:45:19 GMT  (85kb,D)", "http://arxiv.org/abs/1502.01710v1", null], ["v2", "Tue, 7 Apr 2015 21:32:01 GMT  (85kb,D)", "http://arxiv.org/abs/1502.01710v2", "This article contains a notification regarding dataset issues in the first page"], ["v3", "Sun, 7 Jun 2015 03:45:02 GMT  (85kb,D)", "http://arxiv.org/abs/1502.01710v3", "Dataset duplicate issues fixed"], ["v4", "Tue, 8 Sep 2015 04:42:29 GMT  (86kb,D)", "http://arxiv.org/abs/1502.01710v4", "This technical report is superseded by a paper entitled \"Character-level Convolutional Networks for Text Classification\",arXiv:1509.01626. It has considerably more experimental results and a rewritten introduction"], ["v5", "Mon, 4 Apr 2016 02:40:48 GMT  (86kb,D)", "http://arxiv.org/abs/1502.01710v5", "This technical report is superseded by a paper entitled \"Character-level Convolutional Networks for Text Classification\",arXiv:1509.01626. It has considerably more experimental results and a rewritten introduction"]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["xiang zhang", "yann lecun"], "accepted": false, "id": "1502.01710"}, "pdf": {"name": "1502.01710.pdf", "metadata": {"source": "META", "title": "Text Understanding from Scratch", "authors": ["Xiang Zhang", "Yann LeCun"], "emails": ["XIANG@CS.NYU.EDU", "YANN@CS.NYU.EDU"], "sections": [{"heading": "1. Introduction", "text": "Text understanding consists in reading texts formed in natural languages, determining the explicit or implicit meaning of each elements such as words, phrases, sentences and paragraphs, and making inferences about the implicit or explicit properties of these texts(Norvig, 1987). This problem has been traditionally difficult because of the extreme variability in language formation(Linell, 1982). To date, most ways to handle text understanding, be it a handcrafted parsing program or a statistically learnt model, have been resorted to the means of matching words statistics.\nSo far, most machine learning approaches to text understanding consist in tokenizing a string of characters into structures such as words, phrases, sentences and paragraphs, and then apply some statistical classification algorithm onto the statistics of such structures(Soderland, 2001). These techniques work well enough when applied to a narrowly defined domain, but the prior knowledge required is not cheap \u2013 they need to pre-define a dictionary of interested words, and the structural parser needs to handle many special variations such as word morphological changes and ambiguous chunking. These requirements make text understanding more or less specialized to a particular language \u2013 if the language is changed, many things\nmust be engineered from scratch.\nWith the advancement of deep learning and availability of large datasets, methods of handling text understanding using deep learning techniques have gradually become available. One technique which draws great interests is word2vec(Mikolov et al., 2013b). Inspired by traditional language models, this technique constructs representation of words into a vector of fixed length trained under a large corpus. Based on the hope that machines may make sense of languages in a formal fashion, many researchers have tried to train a neural network for understanding texts based the features extracted from it or similar techniques, to name a few, (Frome et al., 2013)(Gao et al., 2013)(Le & Mikolov, 2014)(Mikolov et al., 2013a)(Pennington et al., 2014). Most of these techniques try to apply word2vec or similar techniques with an engineered language model.\nOn the other hand, some researchers have also tried to train a neural network from word level with little structural engineering(Collobert et al., 2011b)(Kim, 2014)(Johnson & Zhang, 2014)(dos Santos & Gatti, 2014). In these works, a word level feature extractor such as lookup table(Collobert et al., 2011b) or word2vec(Mikolov et al., 2013b) is used to feed a temporal ConvNet(LeCun et al., 1998). After training, ConvNets worked for both structured prediction tasks such as part-of-speech tagging and named entity recognition, and text understanding tasks such as sentiment analysis and sentence classification. They claim good results for various tasks, but the datasets and models are relatively small and there are still some engineered layers to represent structures such as words, phrases and sentences.\nIn this article we show that text understanding can be handled by a deep learning system without artificially imbedding knowledge about words, phrases, sentences or any other syntactic or semantic structures associated with a language. We apply temporal ConvNets(LeCun et al., 1998) to various large-scale text understanding tasks, in which the inputs are quantized characters and the outputs are abstract properties of the text. Our approach is one that \u2018learns from scratch\u2019, in the following 2 senses\n1. ConvNets do not require knowledge of words \u2013 work-\nar X\niv :1\n50 2.\n01 71\n0v 1\n[ cs\n.L G\n] 5\nF eb\n2 01\n5\ning with characters is fine. This renders a word-based feature extractor (such as LookupTable(Collobert et al., 2011b) or word2vec(Mikolov et al., 2013b)) unnecessary. All previous works start with words instead of characters, which is difficult to apply a convolutional layer directly due to its high dimensionality.\n2. ConvNets do not require knowledge of syntax or semantic structures \u2013 inference directly to high-level targets is fine. This also invalidates the assumption that structured predictions and language models are necessary for high-level text understanding.\nOur apporach is partly inspired by ConvNet\u2019s success in computer vision. It has outstanding performance in various image recognition tasks(Girshick et al., 2013)(Krizhevsky et al., 2012)(Sermanet et al., 2013). These successful results usually involve some end-to-end ConvNet model that learns hierarchical representation from raw pixels(Girshick et al., 2013)(Zeiler & Fergus, 2014). Similarly, we hypothesize that when trained from raw characters, temporal ConvNet is able to learn the hierarchical representations of words, phrases and sentences in order to understand text."}, {"heading": "2. ConvNet Model Design", "text": "In this section, we introduce the design of ConvNets for text understanding. The design is modular, where the gradients are obtained by back-propagation(Rumelhart et al., 1986) to perform optimization."}, {"heading": "2.1. Key Modules", "text": "The main component in our model is the temporal convolutional module, which simply computes a 1-D convolution between input and output. Suppose we have a discrete input function g(x) \u2208 [1, l] \u2192 R and a discrete kernel function f(x) \u2208 [1, k] \u2192 R. The convolution h(y) \u2208 [1, b(l \u2212 k + 1)/dc] \u2192 R between f(x) and g(x) with stride d is defined as\nh(y) = k\u2211 x=1 f(x) \u00b7 g(y \u00b7 d\u2212 x+ c),\nwhere c = k \u2212 d+ 1 is an offset constant. Just as in traditional convolutional networks in vision, the module is parameterized by a set of such kernel functions fij(x) (i = 1, 2, . . . ,m and j = 1, 2, . . . , n) which we call weights, on a set of inputs gi(x) and outputs hj(y). We call each gi (or hj) an input (or output) frame, and m (or n) input (or output) frame size. The outputs hj(y) is obtained by a sum over i of the convolutions between gi(x) and fij(x).\nOne key module that helped us to train deeper models is temporal max-pooling. It is the same as spatial maxpooling module used in computer vision(Boureau et al.,\n2010a), except that it is in 1-D. Given a discrete input function g(x) \u2208 [1, l] \u2192 R, the max-pooling function h(y) \u2208 [1, b(l \u2212 k + 1)/dc]\u2192 R of g(x) is defined as\nh(y) = k\nmax x=1\ng(y \u00b7 d\u2212 x+ c),\nwhere c = k \u2212 d+ 1 is an offset constant. This very pooling module enabled us to train ConvNets deeper than 6 layers, where all others fail. The analysis by (Boureau et al., 2010b) might shed some light on this.\nThe non-linearity used in our model is the rectifier or thresholding function h(x) = max{0, x}, which makes our convolutional layers similar to rectified linear units (ReLUs)(Nair & Hinton, 2010). We always apply this function after a convolutional or linear module, therefore we ommit its appearance in the following. The algorithm used in training our model is stochastic gradient descent (SGD) with a minibatch of size 128, using momentum(Polyak, 1964)(Sutskever et al., 2013) 0.9 and initial step size 0.01 which is halved every 3 epoches for 10 times. The training method and parameters apply to all of our models. Our implementation is done using Torch 7(Collobert et al., 2011a)."}, {"heading": "2.2. Character quantization", "text": "Our model accepts a sequence of encoded characters as input. The encoding is done by prescribing an alphabet of size m for the input language, and then quantize each character using 1-of-m encoding. Then, the sequence of characters is transformed to a sequence of such m sized vectors with fixed length l. Any character exceeding length l is ignored, and any characters that are not in the alphabet including blank characters are quantized as all-zero vectors. Inspired by how long-short term memory (RSTM)(Hochreiter & Schmidhuber, 1997) work, we quantize characters in backward order. This way, the latest reading on characters is always placed near the beginning of the output, making it easy for fully connected layers to associate correlations with the latest memory. The input to our model is then just a set of frames of length l, and the frame size is the alphabet size m.\nOne interesting thing about this quantization is that visually it is quite similar to Braille(Braille, 1829) used for assisting blind reading, except that our encoding is more compact. Figure 1 depicts this fact. It seems that when trained properly, humans can learn to read binary encoding of languages. This offers interesting insights and inspiration to why our approach could work.\nThe alphabet used in all of our models consists of 69 characters, including 26 english letters, 10 digits and 33 other characters. They include:\nabcdefghijklmnopqrstuvwxyz0123456789\n-,;.!?:\u2019\u2019\u2019/\\|_@#$%\u02c6&*\u02dc\u2018+-=<>()[]{}\nBefore feeding the input to our model, no normalization is done. This is because the input is already quite sparse by itself, with many zeros scattered around. Our models can learn from this simple quantization without problems."}, {"heading": "2.3. Model Design", "text": "We designed 2 ConvNets \u2013 one large and one small. They are both 9 layers deep with 6 convolutional layers and 3 fully-connected layers, with different number of hidden units and frame sizes. Figure 2 gives an illustration.\nThe input have number of frames equal to 69 due to our character quantization method, and the length of each frame is dependent on the problem. We also insert 2 dropout(Hinton et al., 2012) modules in between the 3 fully-connected layers to regularize. They have dropout probability of 0.5. Table 1 lists the configurations for convolutional layers, and table 2 lists the configurations for fully-connected (linear) layers.\nBefore starting training the models, we randomize the weights using Gaussian distributions. The mean and stan-\ndard deviation used for initializing the large model is (0, 0.02), and small model (0, 0.05).\nFor different problems the input lengths are different, and so are the frame lengths. From our model design, it is easy to know that given input length l0, the output frame length after the last convolutional layer (but before any of the fully-connected layers) is l6 = (l0\u221296)/27. This number multiplied with the frame size at layer 6 will give the input dimension the first fully-connected layer accepts."}, {"heading": "2.4. Data Augmentation using Thesaurus", "text": "Many researchers have found that appropriate data augmentation techniques are useful for controlling generalization error for deep learning models. These techniques usually work well when we could find appropriate invariance properies that the model should possess. For example, in image recognition a model should have some controlled invariance towards changes in translating, scaling, rotating and flipping of the input image. Similarly, in speech recognition we usually augment data by adding artificial noise background and changing the tone or speed of speech signal(Hannun et al., 2014).\nIn terms of texts, it is not reasonable to augment the data using signal transformations as done in image or speech recognition, because the exact order of characters may form rigorous syntactic and semantic meaning. Therefore, the best way to do data augmentation would have been using human rephrases of sentences, but this is unrealistic and expensive due the large volume of samples in our datasets. As a result, the most natural choice in data augmentation for us is to replace words or phrases with their synonyms.\nWe experimented data augmentation by using an English thesaurus, which is obtained from the mytheas component used in LibreOffice1 project. That thesaurus in turn was obtained from WordNet(Fellbaum, 2005), where every synonym to a word or phrase is ranked by the semantic closeness to the most frequently seen meaning.\nTo do synonym replacement for a given text, we need to\n1http://www.libreoffice.org/\nanswer 2 questions: which words in the text should be replaced, and which synonym from the thesaurus should be used for the replacement. To decide on the first question, we extract all replaceable words from the given text and randomly choose r of them to be replaced. The probability of number r is determined by a geometric distribution with parameter p in which P [r] \u223c pr. The index s of the synonym chosen given a word is also determined by a another geometric distribution in which P [s] \u223c qs. This way, the probability of a synonym chosen becomes smaller when it moves distant from the most frequently seen meaning.\nIt is worth noting that models trained using our large-scale datasets hardly require data augmentation, since their generalization errors are already pretty good. We will still report the results using this new data augmentation technique with p = 0.5 and q = 0.5."}, {"heading": "2.5. Comparison Models", "text": "Since we have constructed several large-scale datasets from scratch, there is no previous publication for us to obtain a comparison with other methods. Therefore, we also implemented two fairly standard models using previous methods: the bag-of-words model, and a bag-of-centroids model via word2vec(Mikolov et al., 2013b).\nThe bag-of-words model is pretty straightforward. For each dataset, we count how many times each word appears in the training dataset, and choose 5000 most frequent ones as the bag. Then, we use multinomial logistic regression as the classifier for this bag of features.\nAs for the word2vec model, we first ran k-means on the word vectors learnt from Google News corpus with k = 5000, and then use a bag of these centroids for multinomial logistic regression. This model is quite similar to the bagof-words model in that the number of features is also 5000.\nOne difference between these two models is that the features for bag-of-words model are different for different datasets, whereas for word2vec they are the same. This could be one reason behind the phenomenon that bag-ofwords consistently out-performs word2vec in our experiments. It might also be the case that the hope for linear separability of word2vec is not valid at all. That being said, our own ConvNet models consistently out-performs both."}, {"heading": "3. Datasets and Results", "text": "In this part we show the results obtained from various datasets. The unfortunate fact in literature is that there is no openly accessible dataset that is large enough or with labels of sufficient quality for us, although the research on text understanding has been conducted for tens of years. Therefore, we propose several large-scale datasets, in hopes that\ntext understanding can rival the success of image recognition when large-scale datasets such as ImageNet(Deng et al., 2009) became available."}, {"heading": "3.1. DBpedia Ontology Classification", "text": "DBpedia is a crowd-sourced comminity effort to extract structured information from Wikipedia(Lehmann et al., 2014). The English version of the DBpedia knowledge base provides a consistent ontology, which is shallow and cross-domain. It has been manually created based on the most commonly used infoboxes within Wikipedia. Some ontology classes in DBpedia contain hudrends of thousands of samples, which are ideal candidates to construct an ontology classification dataset.\nThe DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes from DBpedia 2014. They are listed in table 3. From each of thse 14 ontology classes, we randomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size of the training dataset is 560,000 and testing dataset 70,000.\nBefore feeding the data to the models, we concatenate the title and short abstract together to form a single input for each sample. The length of input used was l0 = 1014, therefore the frame length after last convolutional layer is l6 = 34. Using an NVIDIA Tesla K40, Training takes about 5 hours per epoch for the large model, and 2 hours for the small model. Table 4 shows the classification results.\nThe results from table 4 indicate both good training and testing errors from our models, with some improvement from thesaurus augmentation. We believe this is a first ev-\nidence that a learning machine does not require knowledge about words, phrases, sentences, paragraphs or any other syntactical or semantic structures to understand text. That being said, we want to point out that ConvNets by their design have the capacity to learn such structured knowledge.\nFigure 3 is a visualization of some kernel weights in the first layer of the large model trained without thesaurus augmentation. Each block represents a randomly chosen kernel, with its horizontal direction iterates over input frames and verticle direction over kernel size. In the visualization, black (or white) indicates large negative (or positive) values, and gray indicates values near zero. It seems very interesting that the network has learnt to care more about the variations in letters than other characters. This phenomenon is observed in models for all of the datasets."}, {"heading": "3.2. Amazon Review Sentiment Analysis", "text": "The purpose of sentiment analysis is to identify and extract subjective information in different kinds of source materials. This task, when presented with the text written by some user, could be formulated as a normal classification problem in which each class represents a degree indicator for user\u2019s subjective view. One example is the score system used from Amazon, which is a discrete score from 1 to 5 indicating user\u2019s subjective rating of a product. The rating usually comes with a review text, which is a valuable source for us to construct a sentiment analysis dataset.\nWe obtained an Amazon review dataset from the Stanford Network Analysis Project (SNAP), which spans 18 years with 34,686,770 reviews from 6,643,669 users on 2,441,053 products(McAuley & Leskovec, 2013). This dataset contains review texts of extremely variate character lengths from 3 to 32,788, in which the mean is around\n764. To construct a sentiment analysis dataset, we chose review texts with character lengths between 100 and 1000. Apart from constructing from the original 5 score labels, we also construct a sentiment polarity dataset in which labels 1 and 2 are converted to negative and 4 and 5 positive. Table 5 lists the number of samples for each score and the number sampled for the 2 dataset."}, {"heading": "1 2,746,559 2,206,886 1,250,000 2,200,000", "text": ""}, {"heading": "2 1,791,219 1,290,278 1,250,000 1,250,000", "text": "We ignored score 3 for polarity dataset because some texts in that score are not obviously negative or positive. Many researchers have shown that with some random text, the inter-rater consensus on polarity is only about 60% - 80%(Gamon & Aue, 2005)(Kim & Hovy, 2004)(Strapparava & Mihalcea, 2008)(Viera et al., 2005)(Wiebe et al., 2001)(Wilson et al., 2005). We believe that by picking out score 3, the labels would have higher quality with a clearer indication of positivity or negativity. We could have included a third \u201cneutral\u201d class, but that would significantly reduce the number of samples for each class since sample imbalance is not desirable.\nFor the full score dataset, we randomly selected 1,000,000 samples for each score for training and 250,000 samples for testing. The size of training set is then 5,000,000 and testing 1,250,000. For the polarity dataset, we randomly selected 3,000,000 samples for each positive or negative label as training set and 450,000 samples for testing. In total, the polarity dataset has 6,000,000 training samples and 900,000 testing samples.\nBecause we limit the maximum length of the text to be 1000, we can safely set the input length to be 1014 and use the same configuration as the DBpedia model. Models for Amazon review datasets took significantly more time to go over each epoch. The time taken for the large model per epoch is about a 5 days, and small model 2 days, with the polarity training taking a little bit longer. Table 6 and table 7 list the results on full score dataset and polarity dataset, respectively.\nIt seems that our models work much better on the polarity dataset than the full score dataset. This is to be expected, since full score prediction means more confusion between nearby score labels. To demonstrate this, figure 4 shows the training and testing confusion matrices."}, {"heading": "3.3. Yahoo! Answers Topic Classification", "text": "Yahoo! Answers is a web site where people post questions and answers, all of which are public to any web user willing to browse or download them. We obtained Yahoo! Answers Comprehensive Questions and Answers version 1.0 dataset through the Yahoo! Webscope program. The data they have collected is the Yahoo! Answers corpus as of October 25th, 2007. It includes all the questions and their corresponding answers. The corpus contains 4,483,032 questions and their answers. In addition to question and answer text, the corpus contains a small amount of metadata, i.e., which answer was selected as the best answer, and the category and sub-category that was assigned to each question.\nWe constructed a topic classification dataset from this corpus using 10 largest main categories. They are listed in table 8. Each class contains 140,000 training samples and 5,000 testing samples. Therefore, the total number of training samples is 1,400,000 and testing samples 50,000 in this dataset. From all the answers and other meta-information, we only used the best answer content and the main category\ninformation.\nThe Yahoo! Answers dataset also contains questions and answers of various lengths, up to 4000 characters. During training we still set the input length to be 1014 and truncate the rest if necessary. But before truncation, we concatenated the question title, question content and best answer content in reverse order so that the question title and content are less likely to be truncated. It takes about 1 day for one epoch on the large model, and about 8 hours for the small model. Table 9 details the results on this dataset.\nOne interesting thing from the results on Yahoo! Answers dataset is that both training and testing accuracy values are quite small compared to the results we obtained from other datasets, whereas the generalization error is pretty good. One hypothesis for this is that there are some intrinsic confusions in determining between some classes given a pair of question and answer.\nFigure 5 shows the confusion matrix for the large model without thesaurus augmentation. It indicates relatively large confusion for classes \u201cSocienty & Culter\u201d, \u201cEducation & Reference\u201d, and \u201cBusiness & Finance\u201d."}, {"heading": "3.4. News Categorization in English", "text": "News is one of the largest parts of the entire web today, which makes it a good candidate to build text understanding models. We obtained the AG\u2019s corpus of news article on the web2. It contains 496,835 categorized news articles from more than 2000 news sources. We choose 4 largest categories from this corpus to construct our dataset, using only the title and description fields.\nTable 10 is a summary of the dataset. From each category, we randomly chose 40,000 samples as training and 1,100 as testing. The total number of training samples is then\n2http://www.di.unipi.it/\u02dcgulli/AG_corpus_ of_news_articles.html\n160,000 and testing 4,400. Compared to other datasets we have constructed, this dataset is relatively small. Therefore the time taken for one epoch using the large model is only 3 hours, and about 1 hour for the small model.\nSimilarly as our previous experiments, we also use an input length of 1014 for this dataset after title and description are concatenated. The actual resulting maximum length of all the inputs is 9843, but the mean is only around 232.\nTable 11 lists the results. It shows a sign of overfitting from our models, which suggests that to achieve good text understanding results ConvNets require a large corpus in order to learn from scratch."}, {"heading": "3.5. News Categorization in Chinese", "text": "One immediate advantage from our dictionary-free design is its applicability to other kinds of human languages. Our simple approach only needs an alphabet of the target language using one-of-n encoding. For languages such as Chinese, Japanese and Korean where there are too many characters, one can simply use its romanized (or latinized) transcription and quantize them just like in English. Better yet, the romanization or latinization is usually phonemic or phonetic, which rivals the success of deep learning in speech recognition(Hannun et al., 2014). Here we investigate one example: news categorization in Chinese.\nThe dataset we obtained consists of the SogouCA and SogouCS news corpra(Wang et al., 2008), containing in total 2,909,551 news articles in various topic channels. Among them, about 2,644,110 contain both a title and some content. We then labeled the each piece of news using its URL, by mannually classify the their domain names. This gives us a large corpus of news articles labeled with their categories. There are a large number categories but most of them contain only few articles. We choose 5 categories \u2013 \u201csports\u201d, \u201cfinance\u201d, \u201centertainment\u201d, \u201cautomobile\u201d and \u201ctechnology\u201d. The number of training samples selected for each class is 150,000 and testing 10,000, as table 12 shows.\nThe romanization or latinization form we have used is Pinyin, which is a phonetic system for transcribing the\nMandarin pronunciations. During this procedure, we used the pypinyin package combined with jieba Chinese segmentation system. The resulting Pinyin text had each tone appended their finals as numbers between 1 and 4.\nSimilar as before, we concatenate title and content to form an input sample. The texts has a wide range of lengths from 14 to 810959. Therefore, during data aqcuisition procedure we constrain the length to stay between 100 and 1014 whenever possible. In the end, we also apply same models as before to this dataset, for which the input length is 1014. We ignored thesaurus augmentation for this dataset. Table 13 lists the results.\nThe input for a bag-of-words model is obtained by considering each Pinyin at Chinese character level as a word. These results indicate consistently good performance from our ConvNet models, even though it is completely a different kind of human language. This is one evidence to our belief that ConvNets can be applied to any human language in similar ways for text understanding tasks."}, {"heading": "4. Outlook and Conclusion", "text": "In this article we provide a first evidence on ConvNets\u2019 applicability to text understanding tasks from scratch, that is, ConvNets do not need any knowledge on the syntactic or semantic structure of a language to give good benchmarks text understanding. This evidence is in contrast with various previous approaches where a dictionary of words is a necessary starting point, and usually structured parsing is hard-wired into the model(Collobert et al., 2011b)(Kim, 2014)(Johnson & Zhang, 2014)(dos Santos & Gatti, 2014).\nDeep learning models have been known to have good representations across domains or problems, in particular for image recognition(Razavian et al., 2014). How good the learnt representations are for language modeling is also one interesting question to ask in the future. Beyond that, we can also consider how to apply unsupervised learning to language models learnt from scratch. Previous embedding methods(Collobert et al., 2011b)(Mikolov et al., 2013b)(Le & Mikolov, 2014) have shown that predicting words or other patterns missing from the input could be useful. We are eager to see how to apply these transfer learning and unsupervised learning techniques with our models.\nRecent research shows that it is possible to generate text description of images from the features learnt in a deep image recognition model, using either fragment embeddings(Karpathy et al., 2014) or recurrent neural networks such as long-short term memory (LSTM)(Vinyals et al., 2014). The models in this article show very good ability for understanding natural languages, and we are interested in using the features from our model to generate a response sentence in similar ways. If this could be successful, conversational systems could have a big advancement.\nIt is also worth noting that natural language in its essence is time-series in disguise. Therefore, one natural extended application for our approach is towards time-series data, in which a hierarchical feature extraction mechanism could bring some improvements over the recurrent and regression models used widely today.\nIn this article we only apply ConvNets to text understanding for its semantic or sentiment meaning. One other apparrent extension is towards traditional NLP tasks such as chunking, named entity recognition (NER) and part-ofspeech (POS) tagging. To do them, one would need to adapt our models to structured outputs. This is very similar to the seminal work by Collobert and Weston(Collobert et al., 2011b), except that we probably no longer need to construct a dictionary and start from words. Our work also makes it easy to extend these models to other human languages.\nOne final possibility from our model is learning from symbolic systems such as mathematical equations, logic expressions or programming languages. Zaremba and Sutskever(Zaremba & Sutskever, 2014) have shown that it is possible to approximate program executing using a recurrent neural network. We are also eager to see how similar projects could work out using our ConvNet models.\nWith so many possibilities, we believe that ConvNet models for text understanding could go beyond from what this article shows and bring important insights towards artificial intelligence in the future."}, {"heading": "Acknowledgement", "text": "We gratefully acknowledge the support of NVIDIA Corporation with the donation of 2 Tesla K40 GPUs used for this research."}], "references": [{"title": "Learning mid-level features for recognition", "author": ["Boureau", "Y-L", "Bach", "Francis", "LeCun", "Yann", "Ponce", "Jean"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Boureau", "Y-Lan", "Ponce", "Jean", "LeCun", "Yann"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"], "venue": "In CVPR09,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["dos Santos", "Cicero", "Gatti", "Maira"], "venue": "In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Wordnet and wordnets", "author": ["Fellbaum", "Christiane"], "venue": "Encyclopedia of Language and Linguistics,", "citeRegEx": "Fellbaum and Christiane.,? \\Q2005\\E", "shortCiteRegEx": "Fellbaum and Christiane.", "year": 2005}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome", "Andrea", "Corrado", "Greg S", "Shlens", "Jon", "Bengio", "Samy", "Dean", "Jeff", "Mikolov", "Tomas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Learning semantic representations for the phrase translation model", "author": ["Gao", "Jianfeng", "He", "Xiaodong", "Yih", "Wen-tau", "Deng", "Li"], "venue": "arXiv preprint arXiv:1312.0482,", "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Girshick", "Ross B", "Donahue", "Jeff", "Darrell", "Trevor", "Malik", "Jitendra"], "venue": "CoRR, abs/1311.2524,", "citeRegEx": "Girshick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2013}, {"title": "DeepSpeech: Scaling up endto-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": null, "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "CoRR, abs/1412.1058,", "citeRegEx": "Johnson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Karpathy", "Andrej", "Joulin", "Armand", "Fei-Fei", "Li"], "venue": "CoRR, abs/1406.5679,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Determining the sentiment of opinions", "author": ["Kim", "Soo-Min", "Hovy", "Eduard"], "venue": "In Proceedings of the 20th International Conference on Computational Linguistics,", "citeRegEx": "Kim et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2004}, {"title": "Convolutional neural networks for sentence classification", "author": ["Kim", "Yoon"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim and Yoon.,? \\Q2014\\E", "shortCiteRegEx": "Kim and Yoon.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Hidden factors and hidden topics: Understanding rating dimensions with review text", "author": ["McAuley", "Julian", "Leskovec", "Jure"], "venue": "In Proceedings of the 7th ACM Conference on Recommender Systems,", "citeRegEx": "McAuley et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Mikolov", "Tomas", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1309.4168,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Inference in text understanding", "author": ["Norvig", "Peter"], "venue": "In AAAI, pp", "citeRegEx": "Norvig and Peter.,? \\Q1987\\E", "shortCiteRegEx": "Norvig and Peter.", "year": 1987}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T. Polyak"], "venue": "{USSR} Computational Mathematics and Mathematical Physics,", "citeRegEx": "Polyak,? \\Q1964\\E", "shortCiteRegEx": "Polyak", "year": 1964}, {"title": "CNN features off-theshelf: an astounding baseline for recognition", "author": ["Razavian", "Ali Sharif", "Azizpour", "Hossein", "Sullivan", "Josephine", "Carlsson", "Stefan"], "venue": "CoRR, abs/1403.6382,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hintont", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "CoRR, abs/1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Building a machine learning based text understanding system", "author": ["Soderland", "Stephen"], "venue": "Proc. IJCAI-2001 Workshop on Adaptive Text Extraction and Mining,", "citeRegEx": "Soderland and Stephen.,? \\Q2001\\E", "shortCiteRegEx": "Soderland and Stephen.", "year": 2001}, {"title": "Learning to identify emotions in text", "author": ["Strapparava", "Carlo", "Mihalcea", "Rada"], "venue": "In Proceedings of the 2008 ACM Symposium on Applied Computing,", "citeRegEx": "Strapparava et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Strapparava et al\\.", "year": 2008}, {"title": "Understanding interobserver agreement: the kappa statistic", "author": ["Viera", "Anthony J", "Garrett", "Joanne M"], "venue": "Fam Med,", "citeRegEx": "Viera et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Viera et al\\.", "year": 2005}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "CoRR, abs/1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Automatic online news issue construction in web environment", "author": ["Wang", "Canhui", "Zhang", "Min", "Ma", "Shaoping", "Ru", "Liyun"], "venue": "In Proceedings of the 17th International Conference on World Wide Web,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Identifying Collocations for Recognizing Opinions", "author": ["Wiebe", "Janyce M", "Wilson", "Theresa", "Bell", "Matthew"], "venue": "In Proceedings of the ACL/EACL Workshop on Collocation,", "citeRegEx": "Wiebe et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2001}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "CoRR, abs/1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In Computer Vision\u2013 ECCV", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "This article demontrates that we can apply deep learning to text understanding from characterlevel inputs all the way up to abstract text concepts, using temporal convolutional networks(LeCun et al., 1998) (ConvNets).", "startOffset": 185, "endOffset": 205}, {"referenceID": 7, "context": "Based on the hope that machines may make sense of languages in a formal fashion, many researchers have tried to train a neural network for understanding texts based the features extracted from it or similar techniques, to name a few, (Frome et al., 2013)(Gao et al.", "startOffset": 234, "endOffset": 254}, {"referenceID": 8, "context": ", 2013)(Gao et al., 2013)(Le & Mikolov, 2014)(Mikolov et al.", "startOffset": 7, "endOffset": 25}, {"referenceID": 24, "context": ", 2013a)(Pennington et al., 2014).", "startOffset": 8, "endOffset": 33}, {"referenceID": 19, "context": ", 2013b) is used to feed a temporal ConvNet(LeCun et al., 1998).", "startOffset": 43, "endOffset": 63}, {"referenceID": 19, "context": "We apply temporal ConvNets(LeCun et al., 1998) to various large-scale text understanding tasks, in which the inputs are quantized characters and the outputs are abstract properties of the text.", "startOffset": 26, "endOffset": 46}, {"referenceID": 9, "context": "It has outstanding performance in various image recognition tasks(Girshick et al., 2013)(Krizhevsky et al.", "startOffset": 65, "endOffset": 88}, {"referenceID": 17, "context": ", 2013)(Krizhevsky et al., 2012)(Sermanet et al.", "startOffset": 7, "endOffset": 32}, {"referenceID": 28, "context": ", 2012)(Sermanet et al., 2013).", "startOffset": 7, "endOffset": 30}, {"referenceID": 9, "context": "These successful results usually involve some end-to-end ConvNet model that learns hierarchical representation from raw pixels(Girshick et al., 2013)(Zeiler & Fergus, 2014).", "startOffset": 126, "endOffset": 149}, {"referenceID": 27, "context": "The design is modular, where the gradients are obtained by back-propagation(Rumelhart et al., 1986) to perform optimization.", "startOffset": 75, "endOffset": 99}, {"referenceID": 25, "context": "The algorithm used in training our model is stochastic gradient descent (SGD) with a minibatch of size 128, using momentum(Polyak, 1964)(Sutskever et al.", "startOffset": 122, "endOffset": 136}, {"referenceID": 11, "context": "We also insert 2 dropout(Hinton et al., 2012) modules in between the 3 fully-connected layers to regularize.", "startOffset": 24, "endOffset": 45}, {"referenceID": 10, "context": "Similarly, in speech recognition we usually augment data by adding artificial noise background and changing the tone or speed of speech signal(Hannun et al., 2014).", "startOffset": 142, "endOffset": 163}, {"referenceID": 4, "context": "Therefore, we propose several large-scale datasets, in hopes that text understanding can rival the success of image recognition when large-scale datasets such as ImageNet(Deng et al., 2009) became available.", "startOffset": 170, "endOffset": 189}, {"referenceID": 31, "context": "Many researchers have shown that with some random text, the inter-rater consensus on polarity is only about 60% 80%(Gamon & Aue, 2005)(Kim & Hovy, 2004)(Strapparava & Mihalcea, 2008)(Viera et al., 2005)(Wiebe et al.", "startOffset": 182, "endOffset": 202}, {"referenceID": 34, "context": ", 2005)(Wiebe et al., 2001)(Wilson et al.", "startOffset": 7, "endOffset": 27}, {"referenceID": 10, "context": "Better yet, the romanization or latinization is usually phonemic or phonetic, which rivals the success of deep learning in speech recognition(Hannun et al., 2014).", "startOffset": 141, "endOffset": 162}, {"referenceID": 33, "context": "The dataset we obtained consists of the SogouCA and SogouCS news corpra(Wang et al., 2008), containing in total 2,909,551 news articles in various topic channels.", "startOffset": 71, "endOffset": 90}, {"referenceID": 26, "context": "Deep learning models have been known to have good representations across domains or problems, in particular for image recognition(Razavian et al., 2014).", "startOffset": 129, "endOffset": 152}, {"referenceID": 14, "context": "Recent research shows that it is possible to generate text description of images from the features learnt in a deep image recognition model, using either fragment embeddings(Karpathy et al., 2014) or recurrent neural networks such as long-short term memory (LSTM)(Vinyals et al.", "startOffset": 173, "endOffset": 196}, {"referenceID": 32, "context": ", 2014) or recurrent neural networks such as long-short term memory (LSTM)(Vinyals et al., 2014).", "startOffset": 74, "endOffset": 96}], "year": 2015, "abstractText": "This article demontrates that we can apply deep learning to text understanding from characterlevel inputs all the way up to abstract text concepts, using temporal convolutional networks(LeCun et al., 1998) (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.", "creator": "LaTeX with hyperref package"}}}