{"id": "1502.03044", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2015", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "abstract": "demonstrated by recent work in machine translation and object detection, we evolve an attention based model ai automatically learns adequately probe the content obtained knowledge. parameters describe tasks we can train accuracy evaluation of the deterministic manner at direct backpropagation simulation and stochastically control adopting variance variational lower bound. researcher also expect enhanced monitoring after the infant utilizes enhanced to automatically learn to fix its gaze on salient animals while generating the corresponding quantities in the output sequence. participants validate dynamic framework using sensors with % - of - public - art performance per other benchmark datasets : mri, flickr30k and ms coco.", "histories": [["v1", "Tue, 10 Feb 2015 19:18:29 GMT  (8243kb,D)", "http://arxiv.org/abs/1502.03044v1", null], ["v2", "Wed, 11 Feb 2015 02:58:54 GMT  (8243kb,D)", "http://arxiv.org/abs/1502.03044v2", null], ["v3", "Tue, 19 Apr 2016 16:43:09 GMT  (8243kb,D)", "http://arxiv.org/abs/1502.03044v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["kelvin xu", "jimmy ba", "ryan kiros", "kyunghyun cho", "aaron c courville", "ruslan salakhutdinov", "richard s zemel", "yoshua bengio"], "accepted": true, "id": "1502.03044"}, "pdf": {"name": "1502.03044.pdf", "metadata": {"source": "META", "title": "Show, Attend and Tell: Neural Image CaptionGeneration with Visual Attention", "authors": ["Kelvin Xu", "Jimmy Lei Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "emails": ["KELVIN.XU@UMONTREAL.CA", "JIMMY@PSI.UTORONTO.CA", "RKIROS@CS.TORONTO.EDU", "KYUNGHYUN.CHO@UMONTREAL.CA", "AARON.COURVILLE@UMONTREAL.CA", "RSALAKHU@CS.TORONTO.EDU", "ZEMEL@CS.TORONTO.EDU", "FIND-ME@THE.WEB"], "sections": [{"heading": "1. Introduction", "text": "Automatically generating captions of an image is a task very close to the heart of scene understanding \u2014 one of the primary goals of computer vision. Not only must caption generation models be powerful enough to solve the computer vision challenges of determining which objects are in an image, but they must also be capable of capturing and expressing their relationships in a natural language. For this reason, caption generation has long been viewed as a difficult problem. It is a very important challenge for machine learning algorithms, as it amounts to mimicking the remarkable human ability to compress huge amounts of salient visual infomation into descriptive language.\nDespite the challenging nature of this task, there has been a recent surge of research interest in attacking the image caption generation problem. Aided by advances in training neural networks (Krizhevsky et al., 2012) and large classification datasets (Russakovsky et al., 2014), recent work\nhas significantly improved the quality of caption generation using a combination of convolutional neural networks (convnets) to obtain vectorial representation of images and recurrent neural networks to decode those representations into natural language sentences (see Sec. 2).\nOne of the most curious facets of the human visual system is the presence of attention (Rensink, 2000; Corbetta & Shulman, 2002). Rather than compress an entire image into a static representation, attention allows for salient features to dynamically come to the forefront as needed. This is especially important when there is a lot of clutter in an image. Using representations (such as those from the top layer of a convnet) that distill information in image down to the most salient objects is one effective solution that has been widely adopted in previous work. Unfortunately, this has one potential drawback of losing information which could be useful for richer, more descriptive captions. Using more low-level representation can help preserve this information. However working with these features necessitates a powerful mechanism to steer the model to information important to the task at hand.\nIn this paper, we describe approaches to caption generation that attempt to incorporate a form of attention with\nar X\niv :1\n50 2.\n03 04\n4v 1\n[ cs\n.L G\n] 1\n0 Fe\ntwo variants: a \u201chard\u201d attention mechanism and a \u201csoft\u201d attention mechanism. We also show how one advantage of including attention is the ability to visualize what the model \u201csees\u201d. Encouraged by recent advances in caption generation and inspired by recent success in employing attention in machine translation (Bahdanau et al., 2014) and object recognition (Ba et al., 2014; Mnih et al., 2014), we investigate models that can attend to salient part of an image while generating its caption.\nThe contributions of this paper are the following: \u2022 We introduce two attention-based image caption gen-\nerators under a common framework (Sec. 3.1): 1) a \u201csoft\u201d deterministic attention mechanism trainable by standard back-propagation methods and 2) a \u201chard\u201d stochastic attention mechanism trainable by maximizing an approximate variational lower bound or equivalently by REINFORCE (Williams, 1992). \u2022 We show how we can gain insight and interpret the results of this framework by visualizing \u201cwhere\u201d and \u201cwhat\u201d the attention focused on. (see Sec. 5.4) \u2022 Finally, we quantitatively validate the usefulness of attention in caption generation with state of the art performance (Sec. 5.3) on three benchmark datasets: Flickr8k (Hodosh et al., 2013) , Flickr30k (Young et al., 2014) and the MS COCO dataset (Lin et al., 2014)."}, {"heading": "2. Related Work", "text": "In this section we provide relevant background on previous work on image caption generation and attention. Recently, several methods have been proposed for generating image descriptions. Many of these methods are based on recurrent neural networks and inspired by the successful use of sequence to sequence training with neural networks for machine translation (Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014). One major reason image caption generation is well suited to the encoder-decoder framework (Cho et al., 2014) of machine translation is because it is analogous to \u201ctranslating\u201d an image to a sentence.\nThe first approach to use neural networks for caption generation was Kiros et al. (2014a), who proposed a multimodal log-bilinear model that was biased by features from the image. This work was later followed by Kiros et al. (2014b) whose method was designed to explicitly allow a natural way of doing both ranking and generation. Mao et al. (2014) took a similar approach to generation but replaced a feed-forward neural language model with a recurrent one. Both Vinyals et al. (2014) and Donahue et al. (2014) use LSTM RNNs for their models. Unlike Kiros et al. (2014a) and Mao et al. (2014) whose models see the image at each time step of the output word sequence, Vinyals et al. (2014) only show the image to the RNN at the beginning. Along\nwith images, Donahue et al. (2014) also apply LSTMs to videos, allowing their model to generate video descriptions.\nAll of these works represent images as a single feature vector from the top layer of a pre-trained convolutional network. Karpathy & Li (2014) instead proposed to learn a joint embedding space for ranking and generation whose model learns to score sentence and image similarity as a function of R-CNN object detections with outputs of a bidirectional RNN. Fang et al. (2014) proposed a three-step pipeline for generation by incorporating object detections. Their model first learn detectors for several visual concepts based on a multi-instance learning framework. A language model trained on captions was then applied to the detector outputs, followed by rescoring from a joint image-text embedding space. Unlike these models, our proposed attention framework does not explicitly use object detectors but instead learns latent alignments from scratch. This allows our model to go beyond \u201cobjectness\u201d and learn to attend to abstract concepts.\nPrior to the use of neural networks for generating captions, two main approaches were dominant. The first involved generating caption templates which were filled in based on the results of object detections and attribute discovery (Kulkarni et al. (2013), Li et al. (2011), Yang et al. (2011), Mitchell et al. (2012), Elliott & Keller (2013)). The second approach was based on first retrieving similar captioned images from a large database then modifying these retrieved captions to fit the query (Kuznetsova et al., 2012; 2014). These approaches typically involved an intermediate \u201cgeneralization\u201d step to remove the specifics of a caption that are only relevant to the retrieved image, such as the name of a city. Both of these approaches have since fallen out of favour to the now dominant neural network methods.\nThere has been a long line of previous work incorporating attention into neural networks for vision related tasks. Some that share the same spirit as our work include Larochelle & Hinton (2010); Denil et al. (2012); Tang et al. (2014). In particular however, our work directly extends the work of Bahdanau et al. (2014); Mnih et al. (2014); Ba et al. (2014)."}, {"heading": "3. Image Caption Generation with Attention Mechanism", "text": ""}, {"heading": "3.1. Model Details", "text": "In this section, we describe the two variants of our attention-based model by first describing their common framework. The main difference is the definition of the \u03c6 function which we describe in detail in Section 4. We denote vectors with bolded font and matrices with capital letters. In our description below, we suppress bias terms for readability."}, {"heading": "3.1.1. ENCODER: CONVOLUTIONAL FEATURES", "text": "Our model takes a single raw image and generates a caption y encoded as a sequence of 1-of-K encoded words.\ny = {y1, . . . ,yC} , yi \u2208 RK\nwhere K is the size of the vocabulary and C is the length of the caption.\nWe use a convolutional neural network in order to extract a set of feature vectors which we refer to as annotation vectors. The extractor produces L vectors, each of which is a D-dimensional representation corresponding to a part of the image.\na = {a1, . . . ,aL} , ai \u2208 RD\nIn order to obtain a correspondence between the feature vectors and portions of the 2-D image, we extract features from a lower convolutional layer unlike previous work which instead used a fully connected layer. This allows the decoder to selectively focus on certain parts of an image by selecting a subset of all the feature vectors."}, {"heading": "3.1.2. DECODER: LONG SHORT-TERM MEMORY NETWORK", "text": "We use a long short-term memory (LSTM) network (Hochreiter & Schmidhuber, 1997) that produces a caption by generating one word at every time step conditioned on a context vector, the previous hidden state and the previously generated words. Our implementation of LSTM\nclosely follows the one used in Zaremba et al. (2014) (see Fig. 4). Using Ts,t : Rs \u2192 Rt to denote a simple affine transformation with parameters that are learned,\n it ft ot gt  =  \u03c3 \u03c3 \u03c3 tanh TD+m+n,n Eyt\u22121ht\u22121 z\u0302t  (1) ct = ft ct\u22121 + it gt (2) ht = ot tanh(ct). (3)\nHere, it, ft, ct, ot, ht are the input, forget, memory, output and hidden state of the LSTM, respectively. The vector z\u0302 \u2208 RD is the context vector, capturing the visual information associated with a particular input location, as explained below. E \u2208 Rm\u00d7K is an embedding matrix. Let m and n denote the embedding and LSTM dimensionality respectively and \u03c3 and be the logistic sigmoid activation and element-wise multiplication respectively.\nIn simple terms, the context vector z\u0302t (equations (1)\u2013(3)) is a dynamic representation of the relevant part of the image input at time t. We define a mechanism \u03c6 that computes z\u0302t from the annotation vectors ai, i = 1, . . . , L corresponding to the features extracted at different image locations. For each location i, the mechanism generates a positive weight \u03b1i which can be interpreted either as the probability that location i is the right place to focus for producing the next word (the \u201chard\u201d but stochastic attention mechanism), or as the relative importance to give to location i in blending the ai\u2019s together. The weight \u03b1i of each annotation vector ai is computed by an attention model fatt for which we use a multilayer perceptron conditioned on the previous hidden state ht\u22121. The soft version of this attention mechanism was introduced by Bahdanau et al. (2014). For emphasis, we note that the hidden state varies as the output RNN advances in its output sequence: \u201cwhere\u201d the network looks next depends on the sequence of words that has already been generated.\neti =fatt(ai,ht\u22121) (4) \u03b1ti = exp(eti)\u2211L k=1 exp(etk) . (5)\nOnce the weights (which sum to one) are computed, the context vector z\u0302t is computed by\nz\u0302t = \u03c6 ({ai} , {\u03b1i}) , (6)\nwhere \u03c6 is a function that returns a single vector given the set of annotation vectors and their corresponding weights. The details of \u03c6 function are discussed in Sec. 4.\nThe initial memory state and hidden state of the LSTM are predicted by an average of the annotation vectors fed\nthrough two separate MLPs (init,c and init,h):\nc0 = finit,c( 1\nL L\u2211 i ai)\nh0 = finit,h( 1\nL L\u2211 i ai)\nIn this work, we use a deep output layer (Pascanu et al., 2014) to compute the output word probability given the LSTM state, the context vector and the previous word:\np(yt|a,yt\u221211 ) \u221d exp(Lo(Eyt\u22121 + Lhht + Lz z\u0302t)) (7)\nWhere Lo \u2208 RK\u00d7m, Lh \u2208 Rm\u00d7n, Lz \u2208 Rm\u00d7D, and E are learned parameters initialized randomly."}, {"heading": "4. Learning Stochastic \u201cHard\u201d vs Deterministic \u201cSoft\u201d Attention", "text": "In this section we discuss two alternative mechanisms for the attention model fatt: stochastic attention and deterministic attention."}, {"heading": "4.1. Stochastic \u201cHard\u201d Attention", "text": "We represent the location variable st as where the model decides to focus attention when generating the tth word. st,i is an indicator one-hot variable which is set to 1 if the i-th location (out of L) is the one used to extract visual features. By treating the attention locations as intermediate latent variables, we can assign a multinoulli distribution parametrized by {\u03b1i}, and view z\u0302t as a random variable:\np(st,i = 1 | sj<t,a) = \u03b1t,i (8) z\u0302t = \u2211 i st,iai. (9)\nWe define a new objective function Ls that is a variational lower bound on the marginal log-likelihood log p(y | a) of observing the sequence of words y given image features a. The learning algorithm for the parametersW of the models can be derived by directly optimizing Ls:\nLs = \u2211 s p(s | a) log p(y | s,a)\n\u2264 log \u2211 s p(s | a)p(y | s,a)\n= log p(y | a) (10)\n\u2202Ls \u2202W = \u2211 s p(s | a) [ \u2202 log p(y | s,a) \u2202W +\nlog p(y | s,a)\u2202 log p(s | a) \u2202W\n] . (11)\nEquation 11 suggests a Monte Carlo based sampling approximation of the gradient with respect to the model parameters. This can be done by sampling the location st from a multinouilli distribution defined by Equation 8.\ns\u0303t \u223c MultinoulliL({\u03b1i})\n\u2202Ls \u2202W \u2248 1 N N\u2211 n=1 [ \u2202 log p(y | s\u0303n,a) \u2202W +\nlog p(y | s\u0303n,a)\u2202 log p(s\u0303 n | a)\n\u2202W\n] (12)\nA moving average baseline is used to reduce the variance in the Monte Carlo estimator of the gradient, following Weaver & Tao (2001). Similar, but more complicated variance reduction techniques have previously been used by Mnih et al. (2014) and Ba et al. (2014). Upon seeing the kth mini-batch, the moving average baseline is estimated as an accumulated sum of the previous log likelihoods with exponential decay:\nbk = 0.9\u00d7 bk\u22121 + 0.1\u00d7 log p(y | s\u0303k,a)\nTo further reduce the estimator variance, an entropy term on the multinouilli distribution H[s] is added. Also, with probability 0.5 for a given image, we set the sampled attention location s\u0303 to its expected value \u03b1. Both techniques improve the robustness of the stochastic attention learning algorithm. The final learning rule for the model is then the\nfollowing:\n\u2202Ls \u2202W \u2248 1 N N\u2211 n=1 [ \u2202 log p(y | s\u0303n,a) \u2202W +\n\u03bbr(log p(y | s\u0303n,a)\u2212 b) \u2202 log p(s\u0303n | a)\n\u2202W + \u03bbe\n\u2202H[s\u0303n]\n\u2202W ] where, \u03bbr and \u03bbe are two hyper-parameters set by crossvalidation. As pointed out and used in Ba et al. (2014) and Mnih et al. (2014), this is formulation is equivalent to the REINFORCE learning rule (Williams, 1992), where the reward for the attention choosing a sequence of actions is a real value proportional to the log likelihood of the target sentence under the sampled attention trajectory.\nIn making a hard choice at every point, \u03c6 ({ai} , {\u03b1i}) from Equation 6 is a function that returns a sampled ai at every point in time based upon a multinouilli distribution parameterized by \u03b1."}, {"heading": "4.2. Deterministic \u201cSoft\u201d Attention", "text": "Learning stochastic attention requires sampling the attention location st each time, instead we can take the expectation of the context vector z\u0302t directly,\nEp(st|a)[z\u0302t] = L\u2211 i=1 \u03b1t,iai (13)\nand formulate a deterministic attention model by computing a soft attention weighted annotation vector \u03c6 ({ai} , {\u03b1i}) = \u2211L i \u03b1iai as introduced by Bahdanau et al. (2014). This corresponds to feeding in a soft \u03b1\nweighted context into the system. The whole model is smooth and differentiable under the deterministic attention, so learning end-to-end is trivial by using standard backpropagation.\nLearning the deterministic attention can also be understood as approximately optimizing the marginal likelihood in Equation 10 under the attention location random variable st from Sec. 4.1. The hidden activation of LSTM ht is a linear projection of the stochastic context vector z\u0302t followed by tanh non-linearity. To the first order Taylor approximation, the expected value Ep(st|a)[ht] is equal to computing ht using a single forward prop with the expected context vector Ep(st|a)[z\u0302t]. Considering Eq. 7, let nt = Lo(Eyt\u22121+Lhht+Lz z\u0302t), nt,i denotes nt computed by setting the random variable z\u0302 value to ai. We define the normalized weighted geometric mean for the softmax kth word prediction:\nNWGM [p(yt = k | a)] = \u220f i exp(nt,k,i)\np(st,i=1|a)\u2211 j \u220f i exp(nt,j,i) p(st,i=1|a)\n= exp(Ep(st|a)[nt,k])\u2211 j exp(Ep(st|a)[nt,j ])\nThe equation above shows the normalized weighted geometric mean of the caption prediction can be approximated well by using the expected context vector, where E[nt] = Lo(Eyt\u22121 + LhE[ht] + LzE[z\u0302t]). It shows that the NWGM of a softmax unit is obtained by applying softmax to the expectations of the underlying linear projections. Also, from the results in (Baldi & Sadowski, 2014), NWGM [p(yt = k | a)] \u2248 E[p(yt = k | a)] under softmax activation. That means the expectation of the outputs over all possible attention locations induced by random variable st is computed by simple feedforward propagation with expected context vector E[z\u0302t]. In other words, the deterministic attention model is an approximation to the marginal likelihood over the attention locations."}, {"heading": "4.2.1. DOUBLY STOCHASTIC ATTENTION", "text": "By construction, \u2211 i \u03b1ti = 1 as they are the output of a softmax. In training the deterministic version of our model we introduce a form of doubly stochastic regularization, where we also encourage \u2211 t \u03b1ti \u2248 1. This can be interpreted as encouraging the model to pay equal attention to every part of the image over the course of generation. In our experiments, we observed that this penalty was important quantitatively to improving overall BLEU score and that qualitatively this leads to more rich and descriptive captions. In addition, the soft attention model predicts a gating scalar \u03b2 from previous hidden state ht\u22121 at each time step t, such that, \u03c6 ({ai} , {\u03b1i}) = \u03b2 \u2211L i \u03b1iai, where \u03b2t = \u03c3(f\u03b2(ht\u22121)). We notice our attention weights put more emphasis on the objects in the images by including\nthe scalar \u03b2.\nConcretely, the model is trained end-to-end by minimizing the following penalized negative log-likelihood:\nLd = \u2212 log(P (y|x)) + \u03bb L\u2211 i (1\u2212 C\u2211 t \u03b1ti) 2 (14)"}, {"heading": "4.3. Training Procedure", "text": "Both variants of our attention model were trained with stochastic gradient descent using adaptive learning rate algorithms. For the Flickr8k dataset, we found that RMSProp (Tieleman & Hinton, 2012) worked best, while for Flickr30k/MS COCO dataset we used the recently proposed Adam algorithm (Kingma & Ba, 2014) .\nTo create the annotations ai used by our decoder, we used the Oxford VGGnet (Simonyan & Zisserman, 2014) pretrained on ImageNet without finetuning. In principle however, any encoding function could be used. In addition, with enough data, we could also train the encoder from scratch (or fine-tune) with the rest of the model. In our experiments we use the 14\u00d714\u00d7512 feature map of the fourth convolutional layer before max pooling. This means our decoder operates on the flattened 196 \u00d7 512 (i.e L \u00d7 D) encoding.\nAs our implementation requires time proportional to the length of the longest sentence per update, we found training on a random group of captions to be computationally wasteful. To mitigate this problem, in preprocessing we build a dictionary mapping the length of a sentence to the corresponding subset of captions. Then, during training we randomly sample a length and retrieve a mini-batch of size 64 of that length. We found that this greatly improved convergence speed with no noticeable diminishment in performance. On our largest dataset (MS COCO), our soft attention model took less than 3 days to train on an NVIDIA Titan Black GPU.\nIn addition to dropout (Srivastava et al., 2014), the only other regularization strategy we used was early stopping on BLEU score. We observed a breakdown in correlation between the validation set log-likelihood and BLEU in the later stages of training during our experiments. Since BLEU is the most commonly reported metric, we used BLEU on our validation set for model selection.\nIn our experiments with soft attention, we also used Whetlab1 (Snoek et al., 2012; 2014) in our Flickr8k experiments. Some of the intuitions we gained from hyperparameter regions it explored were especially important in our Flickr30k and COCO experiments.\nWe make our code for these models based in Theano 1https://www.whetlab.com/\n(Bergstra et al., 2010) publicly available upon publication to encourage future research in this area."}, {"heading": "5. Experiments", "text": "We describe our experimental methodology and quantitative results which validate the effectiveness of our model for caption generation."}, {"heading": "5.1. Data", "text": "We report results on the popular Flickr8k and Flickr30k dataset which has 8,000 and 30,000 images respectively as well as the more challenging Microsoft COCO dataset which has 82,783 images. The Flickr8k/Flickr30k dataset both come with 5 reference sentences per image, but for the MS COCO dataset, some of the images have references in excess of 5 which for consistency across our datasets we discard. We applied only basic tokenization to MS COCO so that it is consistent with the tokenization present in Flickr8k and Flickr30k.\nResults for our attention-based architecture are reported in\nTable 4.2.1. We report results with the frequently used BLEU metric2 which is the standard in the caption generation literature. We report BLEU from 1 to 4 without a brevity penalty. There has been, however, criticism\n2We verified that our BLEU evaluation code matches the authors of Vinyals et al. (2014), Karpathy & Li (2014) and Kiros et al. (2014b). For fairness, we only compare against results for which we have verified that our BLEU evaluation code is the same. With the upcoming release of the COCO evaluation server, we will include comparison results with all other recent image captioning models.\nof BLEU, so in addition we report another common metric METEOR (Denkowski & Lavie, 2014), and compare whenever possible."}, {"heading": "5.2. Evaluation Procedures", "text": "A few challenges exist for comparison, which we explain here. The first is a difference in choice of convolutional feature extractor. For identical decoder architectures, using more recent architectures such as GoogLeNet or Oxford VGG Szegedy et al. (2014), Simonyan & Zisserman (2014) can give a boost in performance over using the AlexNet (Krizhevsky et al., 2012). In our evaluation, we compare directly only with results which use the comparable GoogLeNet/Oxford VGG features, but for METEOR comparison we note some results that use AlexNet.\nThe second challenge is a single model versus ensemble comparison. While other methods have reported performance boosts by using ensembling, in our results we report a single model performance.\nFinally, there is challenge due to differences between dataset splits. In our reported results, we use the predefined splits of Flickr8k. However, one challenge for the Flickr30k and COCO datasets is the lack of standardized splits. As a result, we report with the publicly available splits3 used in previous work (Karpathy & Li, 2014). In our experience, differences in splits do not make a substantial difference in overall performance, but we note the differences where they exist.\n3http://cs.stanford.edu/people/karpathy/ deepimagesent/"}, {"heading": "5.3. Quantitative Analysis", "text": "In Table 4.2.1, we provide a summary of the experiment validating the quantitative effectiveness of attention. We obtain state of the art performance on the Flickr8k, Flickr30k and MS COCO. In addition, we note that in our experiments we are able to significantly improve the state of the art performance METEOR on MS COCO that we speculate is connected to some of the regularization techniques we used 4.2.1 and our lower level representation. Finally, we also note that we are able to obtain this performance using a single model without an ensemble."}, {"heading": "5.4. Qualitative Analysis: Learning to attend", "text": "By visualizing the attention component learned by the model, we are able to add an extra layer of interpretability to the output of the model (see Fig. 1). Other systems that have done this rely on object detection systems to produce candidate alignment targets (Karpathy & Li, 2014). Our approach is much more flexible, since the model can attend to \u201cnon object\u201d salient regions.\nThe 19-layer OxfordNet uses stacks of 3x3 filters meaning the only time the feature maps decrease in size are due to the max pooling layers. The input image is resized so that the shortest side is 256 dimensional with preserved aspect ratio. The input to the convolutional network is the center cropped 224x224 image. Consequently, with 4 max pooling layers we get an output dimension of the top convolutional layer of 14x14. Thus in order to visualize the attention weights for the soft model, we simply upsample the weights by a factor of 24 = 16 and apply a Gaussian filter. We note that the receptive fields of each of the 14x14 units are highly overlapping.\nAs we can see in Figure 2 and 3, the model learns alignments that correspond very strongly with human intuition. Especially in the examples of mistakes, we see that it is possible to exploit such visualizations to get an intuition as to why those mistakes were made. We provide a more extensive list of visualizations in Appendix A for the reader."}, {"heading": "6. Conclusion", "text": "We propose an attention based approach that gives state of the art performance on three benchmark datasets using the BLEU and METEOR metric. We also show how the learned attention can be exploited to give more interpretability into the models generation process, and demonstrate that the learned alignments correspond very well to human intuition. We hope that the results of this paper will encourage future work in using visual attention. We also expect that the modularity of the encoder-decoder approach combined with attention to have useful applications in other domains."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We acknowledge the support of the following organizations for research funding and computing support: NSERC, Samsung, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs and CIFAR. The authors would also like to thank Nitish Srivastava for assistance with his ConvNet package as well as preparing the Oxford convolutional network and Relu Patrascu for helping with numerous infrastructure related problems."}, {"heading": "A. Appendix", "text": "Visualizations from our \u201chard\u201d (a) and \u201csoft\u201d (b) attention model. White indicates the regions where the model roughly attends to (see section 5.4).\nA stop sign\nwith a stop sign\non it .\n(a) A stop sign with a stop sign on it.\n(b) A stop sign is on a road with a mountain in the background.\nFigure 10.\nA man in\na suit and a\nhat holding a remote\ncontrol .\n(a) A man in a suit and a hat holding a remote control.\n(b) A man wearing a hat and a hat on a skateboard.\nFigure 11.\nA little girl\nsitting on a couch\nwith a teddy bear\n.\n(a) A little girl sitting on a couch with a teddy bear.\n(b) A little girl sitting on a bed with a teddy bear.\nA man is\nstanding on a beach\nwith a surfboard .\n(a) A man is standing on a beach with a surfboard.\n(b) A person is standing on a beach with a surfboard.\nA man and\na woman riding a\nboat in the water\n.\n(a) A man and a woman riding a boat in the water.\n(b) A group of people sitting on a boat in the water.\nFigure 12.\nA man is\nstanding in a market\nwith a large amount\nof food .\n(a) A man is standing in a market with a large amount of food.\n(b) A woman is sitting at a table with a large pizza.\nFigure 13.\nA group of\npeople standing next to\neach other .\n(a) A group of people standing next to each other.\n(b) A man is talking on his cell phone while another man watches.\nFigure 15."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Inspired by recent work in machine translation<lb>and object detection, we introduce an attention<lb>based model that automatically learns to describe<lb>the content of images. We describe how we<lb>can train this model in a deterministic manner<lb>using standard backpropagation techniques and<lb>stochastically by maximizing a variational lower<lb>bound. We also show through visualization how<lb>the model is able to automatically learn to fix its<lb>gaze on salient objects while generating the cor-<lb>responding words in the output sequence. We<lb>validate the use of attention with state-of-the-<lb>art performance on three benchmark datasets:<lb>Flickr8k, Flickr30k and MS COCO.", "creator": "LaTeX with hyperref package"}}}