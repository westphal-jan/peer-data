{"id": "1502.03248", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2015", "title": "Off-Policy Reward Shaping with Ensembles", "abstract": "functional - advantage design shaping ( pbrs ) advocates an effective but convenient metric or speed consumer reinforcement learning by leveraging domain knowledge. while that is proven to usually preserve economic rewards, product effect on achieving speed is determined by differing size likely its potential competitors, who, in turn, consists on both the primary heuristic & simulation scale. knowing which heuristic will prove effective if testing their options thereof, currently determining the resulting scale requires analytics, simulations of which introduce fundamental sample complexity. we specify our pbrs framework that checks their speed, only does not incur any sample conflicts. for this, companies propose professionals simultaneously learn an ensemble of inputs, shaped w. r. e. many heuristics and maintains a reservation through considerations. the group policy is then obtained statistical prediction. the task needs significantly be accomplished by efficiently independently reliably learn off - policy : requirements fulfilled by the recent concept architecture, which professionals take as organizational core. we demonstrate empirically that ( 1 ) robust ensemble policy constitutes roughly the base policy, through its single - tailed components, whereupon ( 2 ) stable integration requires a general circle of scales doing equally least physically well as one with externally tuned variables.", "histories": [["v1", "Wed, 11 Feb 2015 10:27:15 GMT  (510kb,D)", "http://arxiv.org/abs/1502.03248v1", "Short version at AAMAS-15, in submission to ALA-15"], ["v2", "Mon, 23 Mar 2015 13:35:59 GMT  (510kb,D)", "http://arxiv.org/abs/1502.03248v2", "To be presented at ALA-15. Short version to appear at AAMAS-15"]], "COMMENTS": "Short version at AAMAS-15, in submission to ALA-15", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["anna harutyunyan", "tim brys", "peter vrancx", "ann nowe"], "accepted": false, "id": "1502.03248"}, "pdf": {"name": "1502.03248.pdf", "metadata": {"source": "CRF", "title": "Off-Policy Reward Shaping with Ensembles", "authors": ["Anna Harutyunyan", "Tim Brys", "Peter Vrancx"], "emails": ["aharutyu@vub.ac.be", "tbrys@vub.ac.be", "pvrancx@vub.ac.be", "anowe@vub.ac.be"], "sections": [{"heading": null, "text": "We formulate a PBRS framework that reduces learning speed, but does not incur extra sample complexity. For this, we propose to simultaneously learn an ensemble of policies, shaped w.r.t. many heuristics and on a range of scales. The target policy is then obtained by voting. The ensemble needs to be able to efficiently and reliably learn off-policy: requirements fulfilled by the recent Horde architecture, which we take as our basis. We demonstrate empirically that (1) our ensemble policy outperforms both the base policy, and its single-heuristic components, and (2) an ensemble over a general range of scales performs at least as well as one with optimally tuned components.\nCategories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning\nGeneral Terms Algorithms, Experimentation\nKeywords reinforcement learning, potential-based reward shaping, offpolicy learning, horde"}, {"heading": "1. INTRODUCTION", "text": "The powerful ability of reinforcement learning (RL) [25] to find optimal policies tabula rasa, is also the source of its main weakness: infeasibly long running times. As the problems RL tackles get larger, it becomes increasingly important to leverage all possible knowledge about the domain\nAppears in: Proceedings of the 14th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2015), Bordini, Elkind, Weiss, Yolum (eds.), May, 4\u20138, 2015, Istanbul, Turkey. Copyright c\u00a9 2015, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.\nat hand. One paradigm to inject such knowledge into the reinforcement learning problem is potential-based reward shaping (PBRS) [20]. Aside from repeatedly demonstrated efficacy in reducing learning speed [1, 5, 4, 24], the principal strength of PBRS lies in its ability to preserve optimal policies. Moreover, it is the only1 reward shaping scheme that is guaranteed to do so [20]. At the heart of PBRS methods lies the potential function. Intuitively, it expresses the \u201cdesirability\u201d of a state, defining the shaping reward on a transition to be the difference in potentials of the transitioning states. States may be desirable by many criteria. The pursuit of designing a potential function that accurately encapsulates the \u201ctrue\u201d desirability is meaningless, as it would solve the task at hand [20], and remove the need for learning altogether. However, one can usually suggest many simple heuristic criteria that improve performance in different situations. Choosing the most effective heuristic amongst them without a test comparison, is typically infeasible, and carrying out such a comparison implies added sample complexity, that may be unaffordable. Moreover, heuristics may contribute complementary knowledge that cannot be leveraged in isolation [4].\nThe choice of a heuristic is merely one of the two deciding factors for the performance of a potential function. The other (and one that is even less intuitive) is scaling. An effective heuristic with a sub-optimal scaling factor may make no difference at all, if the factor is too small, or dominate the base reward and distract the learner,2 if the factor is too large. Typically, one is required to tune the scaling factor beforehand, which requires extra environment samples, and is infeasible in realistic problems.\nWe wish to devise a PBRS framework that is capable of reducing learning speed, without introducing extra sample complexity. To this end, rather than learn a single policy shaped with the most effective heuristic on its optimal scale, we propose to maintain an ensemble of policies that all learn from the same experience, but are shaped w.r.t. different heuristics and different scaling factors. The deployment of our ensemble thus does not require any additional environment samples, and frees the designer up to benefit from PBRS, equipped only with a set of intuitive heuristic rules, with no necessary knowledge of their performance and value magnitudes.\nBecause (for the purpose of not requiring extra environ-\n1Given no knowledge of the environment dynamics. 2The agent will eventually still uncover the optimal policy, but instead of helping him get there faster, reward shaping would slow the learning down.\nar X\niv :1\n50 2.\n03 24\n8v 1\n[ cs\n.A I]\n1 1\nFe b\n20 15\nment samples), all member-policies learn to maximize different reward functions from the same experience, the learning needs to be reliable off-policy. Because the introduced computational complexity (for each of the additional memberpolicies) amounts to that of the off-policy learner, we wish for the learning to be as efficient as possible. The recently introduced Horde architecture [26] is well-suited to be the basis of our ensemble, due to its general off-policy convergence guarantees and computational efficiency. In contrast to the previous uses of Horde [21], we exploit its power to learn a single task, but from multiple viewpoints.\nThe convergence guarantees of Horde require a latent learning scenario [15], i.e. one of (off-policy) learning under a fixed (or slowly changing) behavior policy. This scenario is particularly relevant to real-world applications, where failure is highly penalized and the usual trial-and-error tactic is implausible, e.g. robotic setups. One could imagine the agent following a safe exploratory policy, while learning the target control policy, and only executing the target policy after it is learnt. That is the scenario we focus on in this paper. Note that the conventional interpretation of PBRS to steer exploration [6], does not apply here, as the behavior is unaffected by the target policy, and is kept fixed. This work (and its precursor [8]) provides, to our knowledge, the first validation of PBRS effective in such a latent setting.\nOur contribution is two-fold: (1) we formulate and validate a PBRS framework as a policy ensemble, that is capable of reducing learning speed without adding extra sample complexity, and that does so with general convergence guarantees. Specifically, we demonstrate how such an ensemble can be used to lift the problems of both the choice of the potential function and its scaling, thus removing the need of behind-the-scenes tuning necessary before deployment; and (2) we validate PBRS to be effective in a latent off-policy setting, in which it cannot steer the exploration strategy.\nIn the following section we give an overview of the preliminaries. Section 3 motivates our approach further, while Section 4 describes the proposed architecture and the voting techniques used to obtain the target ensemble policy. Section 5 presents empirical results in two classical benchmarks, and Section 6 concludes."}, {"heading": "2. BACKGROUND", "text": "We assume the usual RL framework [25], in which the agent interacts with its (typically) Markovian environment at discrete time steps t = 1, 2, . . .. Formally, a Markov Decision Process (MDP) [22] is a tuple M = \u3008S,A, \u03b3, T , R\u3009, where: S is a set of states, A is a set of actions, \u03b3 \u2208 [0, 1] is the discounting factor, T = {Psa(\u00b7)|s \u2208 S, a \u2208 A} are the next state transition probabilities with Psa(s\n\u2032) specifying the probability of state s\u2032 occuring upon taking action a from state s, R : S \u00d7 A \u00d7 S \u2192 R is the reward function with R(s, a, s\u2032) giving the expected value of the reward that will be received when a is taken in state s, and rt+1 denoting the component of R at time t.\nA (stochastic) Markovian policy \u03c0 : S \u00d7 A \u2192 [0, 1] is a probability distribution over actions at each state, s.t. \u03c0(s, a) gives the probability of action a being taken from state s under policy \u03c0. In the deterministic case, we will take \u03c0(s) = a to mean \u03c0(s, a) = 1.\nValue-based methods encode policies through value functions, which denote expected cumulative reward obtained while following the policy. We focus on state-action value\nfunctions. In a discounted setting:\nQ\u03c0(s, a) = ET ,\u03c0 [ \u221e\u2211 t=0 \u03b3trt+1|s0 = s, a0 = a ]\n(1)\nAn action a\u2217 is greedy in a state s, if it is the action of maximum value in s. A (deterministic) policy is greedy, if it picks the greedy action in each state:\n\u03c0(s) = arg max a\nQ\u03c0(s, a),\u2200s \u2208 S (2)\nA policy \u03c0\u2217 is optimal if its value is largest:\nQ\u2217(s, a) = sup \u03c0 Q\u03c0(s, a), \u2200s \u2208 S, \u2200a \u2208 A\nThe learning is on-policy if the behavior policy \u03c0b that the agent is following is the same as the target policy \u03c0 that the agent is evaluating. Otherwise, it is off-policy. Given \u03c0b, the values of the optimal greedy policy can be learned incrementally through the following Q-learning [30] update:\nQt+1(st, at) = Qt(st, at) + \u03b1t\u03b4t (3)\n\u03b4t = rt + \u03b3 max a\u2217\u2208A\nQt(st+1, a \u2217)\u2212Qt(st, at) (4)\nwhere Qt is an estimate of Q \u03c0 at time t, \u03b1t \u2208 (0, 1) is the learning rate at time t, at is chosen according to \u03c0b, \u03b4t is the temporal-difference (TD) error of the transition. st+1 is drawn according to T , given st and at, and a\u2217 is the greedy action w.r.t. Qt in st+1 Given tabular representation, this process is shown to converge to the correct value estimates (the TD-fixpoint) in the limit under standard approximation conditions [9].\nWhen the state or action spaces are too large, or continuous, tabular representations do not suffice and one needs to use function approximation (FA). The state (or state-action) space is then represented through a set of features \u03c6, and the algorithms learn the value of a parameter vector \u03b8. In the (common) linear case:\nQt(s, a) = \u03b8 T t \u03c6s,a, \u2200s \u2208 S,\u2200a \u2208 A (5)\nand Eq. (3) becomes:\n\u03b8t+1 = \u03b8t + \u03b1t\u03b4t\u03c6t, (6)\nwhere we slightly abuse notation by letting \u03c6t denote the state-action features \u03c6st,at , and \u03b4t is still computed according to Eq. (4).\nIn the next two subsections we present the core ingredients to our approach."}, {"heading": "2.1 Horde", "text": "FA is known to cause off-policy bootstrapping methods (such as Q-learning) to diverge even on simple problems [2, 28]. The family of gradient temporal difference (GTD) methods provides a solution for this issue, and guarantees offpolicy convergence under FA, given a fixed (or slowly changing behavior) [26]. Previously, similar guarantees were provided only by second-order batch methods (e.g. LSTD [3]),\nunsuitable for online learning. GTD methods are the first to maintain these guarantees, while maintaining the (time and space) complexity linear in the size of the state space. Note that linearity is a lower bound on what is achievable, because it is required to simply store and access the learning vectors. As a consequence, GTD methods scale well to the number of value functions (policies) learnt [19], and due to the inherent off-policy setting, can do so from a single stream of environment interactions (or experience). Sutton et al. [27] formalize this idea in a framework of parallel offpolicy learners, called Horde. They demonstrate Horde to be able to learn thousands of predictive and goal-oriented value functions in real-time from a single unsupervised stream of sensorimotor experience. There have been further successful applications of Horde in realistic robotic setups [21].\nOn the technical level,3 GTD methods are based on the idea of performing gradient descent on a reformulated objective function, which ensures convergence to the projected TD-fixpoint, by introducing a gradient bias into the TDupdate [26]. Mechanistically, it requires maintaining and learning a second set of weights w, along with \u03b8, and performing the following updates:\n\u03b8t+1 = \u03b8t + \u03b1t\u03b4t\u03c6t \u2212 \u03b1\u03b3\u03c6\u2032t(\u03c6Tt wt) (7) wt+1 = wt + \u03b2t(\u03b4t \u2212 \u03c6Tt wt)\u03c6t (8)\nwhere \u03b4t is computed still computed with Eq. (4), and \u03c6 \u2032 t is the feature vector of the next state and action. This is a simpler form of the GTD-update, namely that of TDC [26]. GQ(\u03bb) [14] augments this update with eligibility traces.\nConvergence is one of the two theoretical hurdles with offpolicy learning under FA. The other has to do with the quality of solutions under off-policy sampling, which may, in general, fall far from optimum, even when the approximator can represent the true value function well. In, to our knowledge, the only work that addresses this issue, Kolter [10] gives a way of constraining the solution space to achieve stronger qualitative guarantees, but his algorithm has quadratic complexity and thus is not scalable. Since scalability is crucial in our framework, Horde remains the only plausible convergent architecture available."}, {"heading": "2.2 Reward Shaping", "text": "Reward shaping augments the true reward signal R with an additional shaping reward F , provided by the designer. The shaping reward is intended to guide the agent, when the environmental rewards are sparse or uninformative, in order to speed up learning. In its most general form:\nR\u2032 = R+ F (9)\nBecause tasks are identified by their reward function, modifying the reward function needs to be done with care, in order to not alter the task, or else reward shaping can slow down or even prevent finding the optimal policy [23]. Ng et al. [20] show that grounding the shaping rewards in state potentials is both necessary and sufficient for ensuring preservation of the (optimal) policies of the original MDP. Potentialbased reward shaping (PBRS) maintains a potential function \u03a6 : S \u2192 R, and defines the auxiliary reward function F as:\nF (s, a, s\u2032) = \u03b3\u03a6(s\u2032)\u2212 \u03a6(s) (10) 3Please refer to Maei\u2019s dissertation for the full details [13].\nwhere \u03b3 is the discounting factor of the MDP. We refer to the rewards, value functions and policies, augmented with shaping rewards as shaped. Shaped policies converge to the same (optimal) policies as the base learner, but differ during the learning process."}, {"heading": "3. A HORDE OF SHAPINGS", "text": "The key insight in ensemble learning is that the strength of an ensemble lies in the diversity its components contribute [11]. In the RL context, this diversity can be expressed through several aspects, related to dimensions of the learning process: (1) diversity of experience, (2) diversity of algorithms and (3) diversity of reward signals. Diversity of experience naturally implies high sample complexity, and assumes either a multi-agent setup, or learning in stages. Diversity of algorithms (given the same experience) is computationally costly, as it requires separate representations, and one needs to be particular about the choice of algorithms due to convergence considerations.4 In the context of our aim of improving learning speed, without introducing complexity elsewhere, we focus on the latter aspect of diversity: diversity of reward signals.\nPBRS is an elegant and theoretically attractive approach to introducing diversity into the reward function, by drawing from the available domain knowledge. Such knowledge can often be described as a set of simple heuristics. Combining the corresponding potentials beforehand na\u0308\u0131vely (e.g. with linear scalarization) may result in information loss, when the heuristics counterweigh each other, and introduce further scaling issues, since the relative magnitudes of the potential functions may differ. Maintaining the shapings separately has recently been shown to be a more robust and effective approach [4]. Under the requirements of convergence and efficiency, maintaining such an ensemble of policies learning in parallel and shaped with different potentials, is only possible via the Horde architecture, which is the approach we take in this paper. Thus, the proposed ensemble is the first of its kind to possess general convergence guarantees.\nHorde\u2019s demonstrated ability to learn thousands of policies in parallel in real time [27, 19] allows to consider large ensembles, at little computational cost. While defining thousands of distinct heuristics is rarely sensible, each heuristic may be learnt on many different scaling factors. This not only frees one from having to tune the scaling factor a priori (one of the issues we focus on in this paper), but potentially allows for automatically dynamic scaling, corresponding to state-dependent shaping magnitudes.\nShaping Off-Policy The effects of PBRS on the learning process are usually considered to lie in the guidance of exploration during learning [6, 17, 20]. Laud and DeJong [12] formalize this by showing that the difficulty of learning is most dependent on the reward horizon, a measure of the number of decisions a learning agent must make before experiencing accurate feedback, and that reward shaping artificially reduces this horizon. In our latent setting we assume no control over the agent\u2019s behavior. The performance benefits then can be explained by faster knowledge propagation through the TD updates, which we now observe decoupled from guidance of\n4See the discussion on convergence in Section 6.1.2 of van Hasselt\u2019s dissertation [29].\nexploration. Reward shaping in such off-policy settings is not well studied or understood, and these effects are of independent interest."}, {"heading": "4. ARCHITECTURE", "text": "We are now ready to describe the architecture of our ensemble (Fig. 1).We maintain our Horde of shapings as a set D of Greedy-GQ(\u03bb)-learners [14]. Given a set of potential functions \u03a6 = {\u03a61, . . .\u03a6`} a range of scaling factors ci = \u3008ci1, . . . ciki\u3009 for each \u03a6i, and the base reward function R, the ensemble reward function is a vector:\nR = R+ \u3008F\u03a61 c11 , F\u03a61 c12 , . . . , F \u03a6` c` k` \u3009 (11)\nwhere F\u03a6i cij is the potential-based shaping reward given by Eq. (10) w.r.t. the potential function \u03a6i and scaled with the factor cij . For notational clarity, we will take F i j to mean F\u03a6i cij (i.e. the shaping w.r.t. to the i-th potential function on the j-th scaling factor), and Rij = R+ F i j . We allow the ensemble the option to include the base learner. We adopt the terminology of Sutton et al. [27], and refer to individual agents within Horde as demons. Each demon dij learns a greedy policy \u03c0 i j w.r.t. its reward R i j . Recall that our latent setting implies that the learning is guided by a fixed behavior policy \u03c0b, with \u03c0 i j all learning in parallel from the experience generated by \u03c0b. Because each policy \u03c0ij is available separately at each step, an ensemble policy can be devised by collecting votes on action preferences from all demons dij . The ensemble is also latent, and not executed until the learning has ended. Note that because PBRS preserves all of the optimal policies from the original problem [20], the ensemble policy does too.\nIn this paper we have considered two voting schemes: majority voting and rank voting, which are elaborated below. The architecture is certainly not limited to these choices."}, {"heading": "4.1 Ensemble Policy", "text": "To the best of our knowledge, both voting methods were first used in the context of RL agents by Wiering and Van Hasselt [31]. In both methods, each demon d casts a vote vd : S\u00d7A\u2192 N0, s.t. vd(s, a) is the preference value of action a in state s. The voting scheme then is defined for policies, rather than value functions, which mitigates the magnitude bias.5 The ensemble policy acts greedily (with ties broken randomly) w.r.t. the cumulative preference values P :\nP (st, a) = \u2211 d\u2208D vd(st, a), \u2200a \u2208 A (12)\nThe voting scheme determines the manner in which vd are assigned.\nMajority voting Each demon d casts a vote of 1 for its most preferred action, and a vote of 0 for the others. I.e.:\nvd(s, a) =\n{ 1 if Q(s, a) = max\na\u2217 Q(s, a\u2217)\n0 otherwise. (13)\n5Note that even though the shaped policies are the same upon convergence \u2013 the value functions are not.\nRank voting Each demon greedily ranks its n actions, from n \u2212 1 for its most, to 0 for its least preferred actions. We slightly modify the formulation from [31], by ranking Q-values, instead of policy probabilities. I.e. vd(s, a) > vd(s, a \u2032), if and only if Qd(s, a) > Qd(s, a \u2032).\nlinear function approximation block are the features of the transition (two state-action pairs), with their intersections with \u03b8ij representing weights. a \u2032 is a vector of greedy actions at s\u2032\nw.r.t. to each policy \u03c0ij . Note that in this latent settings, all\ninteractions with the environment happen only in the upper left corner."}, {"heading": "5. EXPERIMENTS", "text": "We now present the empirical studies that validate the efficacy of our ensemble architecture w.r.t. both the choice of heuristic and the choice of scale. We first consider the scenario of choosing between heuristics, and evaluate an ensemble consisting of shapings with appropriate scaling factors. The experiments show that the ensemble policy performs at least as well as the best heuristic. We then turn to the problem of scaling, and demonstrate that ensembles on both narrow and broad ranges of scales perform at least as well as the one w.r.t. the optimal scaling factors.\nWe carry out our experiments on two common benchmark problems. In both problems, the behavior policy is a uniform distribution over all actions at each time step. The evaluation is done by interrupting the base learner every z episodes and executing the queried greedy policy once. No learning is allowed during evaluation.\nWe evaluated the ensembles w.r.t. both voting schemes from Sec. 4.1, and found the (sum) performance to be not significantly different (p > 0.05), with rank voting performing slightly better. To keep the clarity of focus, below we\nonly present the results for the rank voting scheme, but emphasize that the performance is not conditional on this choice."}, {"heading": "5.1 Mountain Car", "text": "We begin with the classical benchmark domain of mountain car [25]. The task is to drive an underpowered car up a hill. The (continuous) state of the system is composed of the current position (in [\u22121.2, 0.6]) and the current velocity (in [\u22120.07, 0.07]) of the car. Actions are discrete, a throttle of {\u22121, 0, 1}. The agent starts at the position \u22120.5 and a velocity of 0, and the goal is at the position 0.6. The rewards are \u22121 for every time step. An episode ends when the goal is reached, or when 2000 steps have elapsed. The state space is approximated with the standard tile-coding technique [25], using ten tilings of 10\u00d710, with a parameter vector learnt for each action.\nIn this domain we define three intuitive shaping potentials:\nPosition Encourage progress to the right (in the direction of the goal). This potential is flawed by design, since in order to get to the goal, one needs to first move away from it.\n\u03a61(x) = x\u0304 (14)\nHeight Encourage higher positions (potential energy):\n\u03a62(x) = h\u0304 (15)\nSpeed Encourage higher speeds (kinetic energy):\n\u03a63(x) = |\u00af\u0307x|2 (16)\nHere x = \u3008x, x\u0307\u3009 is the state (position and velocity), and a\u0304 denotes the normalization of a onto [0, 1].\nWe used \u03b3 = 0.99. The learning parameters were tuned w.r.t. the base learner and shared among all demons: \u03bb = 0.4, \u03b2 = 0.0001, \u03b1 = 0.1, where \u03bb is the trace decay parameter, \u03b2 the step size for the second set of weights w in Greedy-GQ, and \u03b1 the step size for the main parameter vector \u03b8. We ran 1000 independent runs of 100 episodes each, with evaluation occuring every 5 episodes (z = 5).\n5.1.1 Choice of Heuristic In this experiment6 we address the question of the choice\nbetween heuristics. We thus consider ensembles composed of the demons shaped with the three shaping potential functions \u03a61,\u03a62 and \u03a63, and scaled with factors c1, c2, c3 that have been tuned beforehand. We associate the learner di with d\u03a6ici .\nWhen evaluating the shapings individually, we witness d3 to perform best amongst the three. To examine the quality of our ensembles w.r.t. the quality of its components, we consider two scenarios: E1 = \u3008d1, d2\u3009 of two demons and E2 = \u3008d1, d2, d3\u3009 of three demons. This corresponds to having ensemble consisting of two comparable shapings, and an ensemble with one clearly most efficient shaping. Thus, ideally, we would like E1 to outperform both d1 and d2 and E2 to at least match the performance of d3.\nFig. 2 presents the learning performance of the base agent, the demons d1, d2, d3 shaped with single potentials, and the two ensembles E1 and E2, mentioned above. We witness the\n6This experiment first appeared in the early version of this work [8].\nindividual shapings alone to aid the learning significantly. E1 follows d1 at first, when its performance is better, but switches to d2, when the performance of d1 levels out. This is because d1 (as is appropriate with its position shaping) persists on going right in the beginning of an episode, and this strategy, while effective at first, results in a plateau of a higher number of steps. The ensemble policy is able to avoid this by incorporating information from d2. E2, the ensemble of all three shapings, begins better than both d1 and d2, but slightly worse than d3, the most effective shaping. It, however, quickly catches up to d3, with the overall performance of E2 and d3 being statistically indistinguishable.\nThus, the performance of the ensembles meets our desiderata: when there is clearly a best component, an ensemble statistically matches it, otherwise it outperforms all of its components.\n5.1.2 Choice of Scale The previous set of experiments assumed access to the\nbest scaling factors c1, c2, c3. In practice obtaining these requires tuning each shaping prior to the use of the ensemble, a scenario we aim to avoid. In this section we demonstrate that ensembles on a range of scales perform at least as well, as those with cherry-picked components.\nNamely, we consider two scaling ranges C1 = \u300820, 40, 60, 80, 100\u3009 and C2 = \u30081, 10, 102, 103, 104\u3009, with the first being a reasonably close range to the optimal scales from the previous section, and the second being a general sweep, with no intuition or knowledge of the optimal scale. Before we proceed further, we illustrate the effect a scaling factor can have on the performance of a single shaping. Fig. 3 gives a comparison of the performance of the shaping potential \u03a62 over the (reasonable) scaling range C1. Even small differences in scale have dramatic effect on the shaping\u2019s performance.\nNow let EC1 and EC2 be the ensembles w.r.t. all three shapings on C1 and C2, resp., each totaling in 16 demons (including the base learner). We compare EC1 and EC2 with E2 (the ensemble w.r.t. the three shapings with tuned scaling factors, from the first experiment). We illustrate the\nrange of performances of shapings for each scale range, by additionally plotting the average of the runs of each shaping across each scale. I.e. for the range Cj , and shaping \u03a6i, at each episode, this is the average of the rewards obtained by the demons di1, d i 2,. . .,d i |Cj | in that episode.\nFig. 4 presents the results. EC1 and EC2 are both statistically the same (p > 0.05) as the tuned ensemble E2, despite their components having a much wider range of performance."}, {"heading": "5.2 Cart-Pole", "text": "We now validate our framework on the problem of cartpole [18]. The task is to balance a pole on top of a moving cart for as long as possible. The (continuous) state s con-\ntains the angle \u03be and angular velocity \u03be\u0307 of the pole, and the position x and velocity x\u0307 of the cart. There are two\nactions: a small positive and a small negative force applied to the cart. A pole falls if |\u03be| > \u03c0\n4 , which terminates the\nepisode. The track is bounded within [\u22124, 4], but the sides are \u201csoft\u201d; the cart does not crash upon hitting them. The reward function penalizes a pole drop, and is 0 elsewhere. An episode terminates successfully, if the pole was balanced for 1000 steps. The state space is approximated with tile coding, using ten tilings of 10 \u00d7 10 over all 4 dimensions, with a parameter vector learnt for each action.\nWe define two potential functions, corresponding to the angle and angular speed of the pole.\nAngle Discourage angles far from the equilibrium:\n\u03a61(s) = \u2212 \u00af|\u03be| 2\n(17)\nAngular speed Discourage high speeds (which are likelier to result in dropping the pole):\n\u03a62(s) = \u2212| \u00af\u0307\u03be|2 (18)\nWe used \u03b3 = 0.99. The learning parameters were tuned w.r.t. the base learner and set to \u03bb = 0.7, \u03b1 = 0.1 and \u03b2 = 0.001. These settings were shared among all demons. We ran 100 independent runs of a 1000 episode each, with evaluation occuring every 50 episodes (z = 50).\n5.2.1 Choice of Heuristic and Scale In this experiment we evaluate the problems of the choice\nof the heuristic and its scale jointly. We consider a general scaling range C = \u30081, 10, 102, 103, 104\u3009, and three ensembles: E1C resp. E 2 C only comprised of the demons shaped w.r.t. \u03a61 resp. \u03a62 across C (5 demons each), and EC containing all 11 demons (including the base learner). As before, we illustrate the range of performances of shapings across the range of scales by, for each shaping, plotting the average performance of the demons w.r.t. that shaping across the entire scale range. I.e. for the shaping \u03a6i, at each episode, this is the average of the rewards obtained by the demons di1, d i 2,. . .,d i |C| in that episode.\ncart-pole. The dashed lines (for each of the two shapings) denote the mean performance of the demons w.r.t. C, and plotted as a reference for the performance of the ensemble components. Note that there is no single demon with this performance. The performances of the global ensemble EC\nfollows the (more effective) first shaping, in the end matching the performance of the corresponding ensemble E1C .\nFig. 5 shows the results. All ensembles (and ensemble averages) improve over the base learner. The performance of E2C , the ensemble over the second shaping, matches that of the average from that ensemble, since all of its components perform similarly. On the other hand, E1C , the ensemble over the first shaping, does much better than the corresponding average. The global ensemble EC over all of the demons starts out better than both E1C and E 2 C , then levels at the average performance of the (better) first shaping, and finally matches the performance of E1C . The global ensemble EC thus correctly identifies both which shaping to follow: its performance always follows (or is better than) that of the more efficient first shaping (either on average, or the ensemble E1C), and on what scales: the final performance of EC matches that of E1C , significantly improving over the average across the scale range."}, {"heading": "6. CONCLUSIONS", "text": "In this work we described a novel off-policy PBRS ensemble architecture that is able to reduce learning speed in a latent setting, without requiring the extra sample complexity introduced by the steps of tuning the heuristic and its scale, typical to PBRS. We avoid these steps by learning an ensemble of policies w.r.t. many heuristics and scaling factors simultaneously. Our ensemble is the first of its kind to possess general convergence guarantees, while staying efficient, as it leverages the recent Horde architecture to learn a single task well. Our experiments validate the use of PBRS in the latent setting, and demonstrate the efficacy of the proposed ensemble. Namely, we show that the ensemble policy over both broad and narrow ranges of scales performs at least as well as the one over a set of optimally pre-tuned components, which in turn performs at least as well as its best component-heuristic.\nFuture Directions In this work we have assumed a shared set of parameters between the demons, an immediate extension would be to maintain demons that learn w.r.t. different parameters. This is similar to the approach of Marivate and Littman [16], who learn to solve many variants of a problem for the best parameter settings in a generalized MDP. In our case the MDP (dynamics) will remain shared, but the individual parameters of the demons will vary.\nIt would be worthwhile to evaluate the framework w.r.t. different ensemble techniques that induce the target ensemble policy. This would be especially useful in domains where only select scaling factors of select heuristics offer improvement: taking a global majority vote over such an ensemble will likely not be as effective, as trying to determine which subset of demons to consider. One could, e.g., use confidence measures [4] to identify these demons.\nInstead of shaping demons with static potential functions, one could consider maintaining a layer of demons that each learn some potential function [17, 7], which are, in turn, fed into the layer of shaped demons who contribute to the ensemble policy. One needs to be realistic about attainability of learning this in time, since as argued by Ng et al. [20], the best potential function correlates with the optimal value function V \u2217, learning which would solve the base problem itself and render the potentials pointless."}, {"heading": "7. REFERENCES", "text": "[1] J. Asmuth, M. L. Littman, and R. Zinkov.\nPotential-based shaping in model-based reinforcement learning. In Proceedings of AAAI, pages 604\u2013609, 2008.\n[2] L. Baird. Residual algorithms: Reinforcement learning with function approximation. In In Proceedings of ICML, pages 30\u201337, 1995.\n[3] S. J. Bradtke, A. G. Barto, and P. Kaelbling. Linear least-squares algorithms for temporal difference learning. In Machine Learning, pages 22\u201333, 1996.\n[4] T. Brys, A. Nowe\u0301, D. Kudenko, and M. E. Taylor. Combining multiple correlated reward shaping signals by measuring confidence. In Proceedings of AAAI, 2014.\n[5] S. Devlin, D. Kudenko, and M. Grzes. An empirical study of potential-based reward shaping and advice in complex, multi-agent systems. Advances in Complex Systems (ACS), 14(02):251\u2013278, 2011.\n[6] M. Grzes. Improving Exploration in Reinforcement Learning through Domain Knowledge and Parameter Analysis. PhD thesis, University of York, 2010.\n[7] M. Grzes and D. Kudenko. Online learning of shaping rewards in reinforcement learning. Neural Networks, 23(4):541 \u2013 550, 2010. Proceedings of ICANN.\n[8] A. Harutyunyan, T. Brys, P. Vrancx, and A. Nowe\u0301. Off-policy shaping ensembles in reinforcement learning. In Proceedings of ECAI, pages 1021\u20131022, 2014.\n[9] T. Jaakkola, M. I. Jordan, and S. P. Singh. On the convergence of stochastic iterative dynamic programming algorithms. Neural computation, 6(6):1185\u20131201, 1994.\n[10] J. Z. Kolter. The fixed points of off-policy td. In Advances in Neural Information Processing Systems, pages 2169\u20132177, 2011.\n[11] A. Krogh and J. Vedelsby. Neural network ensembles, cross validation, and active learning. In Advances in Neural Information Processing Systems, pages 231\u2013238. MIT Press, 1995.\n[12] A. Laud and G. DeJong. The influence of reward on the speed of reinforcement learning: An analysis of shaping. In Proceedings of ICML. AAAI Press, 2003.\n[13] H. Maei. Gradient Temporal-Difference Learning Algorithms. PhD thesis, University of Alberta, 2011.\n[14] H. Maei and R. Sutton. GQ(\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. In Proceedings of the Third Conf. on Artificial General Intelligence., 2010.\n[15] H. Maei, C. Szepesva\u0301ri, S. Bhatnagar, and R. Sutton. Toward off-policy learning control with function approximation. In J. Fu\u0308rnkranz and T. Joachims, editors, Proceedings of ICML 2010, pages 719\u2013726, 2010.\n[16] V. Marivate and M. Littman. An ensemble of linearly combined reinforcement-learning agents. AAAI Workshops, 2013.\n[17] B. Marthi. Automatic shaping and decomposition of reward functions. In Proceedings of ICML, ICML \u201907, pages 601\u2013608, New York, NY, USA, 2007. ACM.\n[18] D. Michie and R. A. Chambers. Boxes: An experiment in adaptive control. In E. Dale and D. Michie, editors,\nMachine Intelligence. Oliver and Boyd, Edinburgh, UK, 1968.\n[19] J. Modayil, A. White, P. M. Pilarski, and R. S. Sutton. Acquiring a broad range of empirical knowledge in real time by temporal-difference learning. In Systems, Man, and Cybernetics (SMC), 2012 IEEE International Conference on, pages 1903\u20131910. IEEE, 2012.\n[20] A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In In Proceedings of ICML, pages 278\u2013287. Morgan Kaufmann, 1999.\n[21] P. Pilarski, M. Dawson, T. Degris, J. Carey, K. Chan, J. Hebert, and R. Sutton. Adaptive artificial limbs: a real-time approach to prediction and anticipation. Robotics Automation Magazine, IEEE, 20(1):53\u201364, March 2013.\n[22] M. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY, USA, 1st edition, 1994.\n[23] J. Randl\u00f8v and P. Alstr\u00f8m. Learning to drive a bicycle using reinforcement learning and shaping. In Proceedings of ICML, 1998.\n[24] M. Snel and S. Whiteson. Learning potential functions and their representations for multi-task reinforcement learning. Autonomous Agents and Multi-Agent Systems, 28(4):637\u2013681, 2014.\n[25] R. Sutton and A. Barto. Reinforcement learning: An introduction, volume 116. Cambridge Univ Press, 1998.\n[26] R. Sutton, H. Maei, D. Precup, S. Bhatnagar, D. Silver, C. Szepesva\u0301ri, and E. Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In In Proceedings of ICML, 2009.\n[27] R. Sutton, J. Modayil, M. Delp, T. Degris, P. Pilarski, A. White, and D. Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In Proceedings of AAMAS, pages 761\u2013768, Richland, SC, 2011. International Foundation for Autonomous Agents and Multiagent Systems.\n[28] J. N. Tsitsiklis and B. V. Roy. An analysis of temporal-difference learning with function approximation. Technical report, IEEE Transactions on Automatic Control, 1997.\n[29] H. van Hasselt. Insights in reinforcement learning : formal analysis and empirical evaluation of temporal-difference learning algorithms. PhD thesis, Utrecht University, 2011.\n[30] C. J. C. H. Watkins and P. Dayan. Q-learning. Machine Learning, 8(3):272\u2013292, 1992.\n[31] M. Wiering and H. van Hasselt. Ensemble algorithms in reinforcement learning. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 38(4):930\u2013936, Aug 2008."}], "references": [{"title": "Potential-based shaping in model-based reinforcement learning", "author": ["J. Asmuth", "M.L. Littman", "R. Zinkov"], "venue": "Proceedings of AAAI, pages 604\u2013609", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "In Proceedings of ICML, pages 30\u201337", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto", "P. Kaelbling"], "venue": "Machine Learning, pages 22\u201333", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Combining multiple correlated reward shaping signals by measuring confidence", "author": ["T. Brys", "A. Now\u00e9", "D. Kudenko", "M.E. Taylor"], "venue": "Proceedings of AAAI", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical study of potential-based reward shaping and advice in complex", "author": ["S. Devlin", "D. Kudenko", "M. Grzes"], "venue": "multi-agent systems. Advances in Complex Systems (ACS), 14(02):251\u2013278", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving Exploration in Reinforcement Learning through Domain Knowledge and Parameter Analysis", "author": ["M. Grzes"], "venue": "PhD thesis, University of York", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning of shaping rewards in reinforcement learning", "author": ["M. Grzes", "D. Kudenko"], "venue": "Neural Networks, 23(4):541 \u2013 550", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Off-policy shaping ensembles in reinforcement learning", "author": ["A. Harutyunyan", "T. Brys", "P. Vrancx", "A. Now\u00e9"], "venue": "Proceedings of ECAI, pages 1021\u20131022", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "On the convergence of stochastic iterative dynamic programming algorithms", "author": ["T. Jaakkola", "M.I. Jordan", "S.P. Singh"], "venue": "Neural computation, 6(6):1185\u20131201", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "The fixed points of off-policy td", "author": ["J.Z. Kolter"], "venue": "Advances in Neural Information Processing Systems, pages 2169\u20132177", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural network ensembles", "author": ["A. Krogh", "J. Vedelsby"], "venue": "cross validation, and active learning. In Advances in Neural Information Processing Systems, pages 231\u2013238. MIT Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "The influence of reward on the speed of reinforcement learning: An analysis of shaping", "author": ["A. Laud", "G. DeJong"], "venue": "Proceedings of ICML. AAAI Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Gradient Temporal-Difference Learning Algorithms", "author": ["H. Maei"], "venue": "PhD thesis, University of Alberta", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "GQ(\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H. Maei", "R. Sutton"], "venue": "Proceedings of the Third Conf. on Artificial General Intelligence.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Toward off-policy learning control with function approximation", "author": ["H. Maei", "C. Szepesv\u00e1ri", "S. Bhatnagar", "R. Sutton"], "venue": "J. F\u00fcrnkranz and T. Joachims, editors, Proceedings of ICML 2010, pages 719\u2013726", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "An ensemble of linearly combined reinforcement-learning agents", "author": ["V. Marivate", "M. Littman"], "venue": "AAAI Workshops", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic shaping and decomposition of reward functions", "author": ["B. Marthi"], "venue": "Proceedings of ICML, ICML \u201907, pages 601\u2013608, New York, NY, USA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Boxes: An experiment in adaptive control", "author": ["D. Michie", "R.A. Chambers"], "venue": "E. Dale and D. Michie, editors,  Machine Intelligence. Oliver and Boyd, Edinburgh, UK", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1968}, {"title": "Acquiring a broad range of empirical knowledge in real time by temporal-difference learning", "author": ["J. Modayil", "A. White", "P.M. Pilarski", "R.S. Sutton"], "venue": "Systems, Man, and Cybernetics (SMC), 2012 IEEE International Conference on, pages 1903\u20131910. IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "In Proceedings of ICML, pages 278\u2013287. Morgan Kaufmann", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Adaptive artificial limbs: a real-time approach to prediction and anticipation", "author": ["P. Pilarski", "M. Dawson", "T. Degris", "J. Carey", "K. Chan", "J. Hebert", "R. Sutton"], "venue": "Robotics Automation Magazine,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M. Puterman"], "venue": "John Wiley & Sons, Inc., New York, NY, USA, 1st edition", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning to drive a bicycle using reinforcement learning and shaping", "author": ["J. Randl\u00f8v", "P. Alstr\u00f8m"], "venue": "Proceedings of ICML", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning potential functions and their representations for multi-task reinforcement learning", "author": ["M. Snel", "S. Whiteson"], "venue": "Autonomous Agents and Multi-Agent Systems, 28(4):637\u2013681", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Reinforcement learning: An introduction", "author": ["R. Sutton", "A. Barto"], "venue": "volume 116. Cambridge Univ Press", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R. Sutton", "H. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "In Proceedings of ICML", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P. Pilarski", "A. White", "D. Precup"], "venue": "Proceedings of AAMAS, pages 761\u2013768, Richland, SC", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B.V. Roy"], "venue": "Technical report, IEEE Transactions on Automatic Control", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "Insights in reinforcement learning : formal analysis and empirical evaluation of temporal-difference learning algorithms", "author": ["H. van Hasselt"], "venue": "PhD thesis, Utrecht University,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Q-learning", "author": ["C.J.C.H. Watkins", "P. Dayan"], "venue": "Machine Learning, 8(3):272\u2013292", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1992}, {"title": "Ensemble algorithms in reinforcement learning. Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["M. Wiering", "H. van Hasselt"], "venue": "IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}], "referenceMentions": [{"referenceID": 24, "context": "The powerful ability of reinforcement learning (RL) [25] to find optimal policies tabula rasa, is also the source of its main weakness: infeasibly long running times.", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "One paradigm to inject such knowledge into the reinforcement learning problem is potential-based reward shaping (PBRS) [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "Aside from repeatedly demonstrated efficacy in reducing learning speed [1, 5, 4, 24], the principal strength of PBRS lies in its ability to preserve optimal policies.", "startOffset": 71, "endOffset": 84}, {"referenceID": 4, "context": "Aside from repeatedly demonstrated efficacy in reducing learning speed [1, 5, 4, 24], the principal strength of PBRS lies in its ability to preserve optimal policies.", "startOffset": 71, "endOffset": 84}, {"referenceID": 3, "context": "Aside from repeatedly demonstrated efficacy in reducing learning speed [1, 5, 4, 24], the principal strength of PBRS lies in its ability to preserve optimal policies.", "startOffset": 71, "endOffset": 84}, {"referenceID": 23, "context": "Aside from repeatedly demonstrated efficacy in reducing learning speed [1, 5, 4, 24], the principal strength of PBRS lies in its ability to preserve optimal policies.", "startOffset": 71, "endOffset": 84}, {"referenceID": 19, "context": "Moreover, it is the only reward shaping scheme that is guaranteed to do so [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "The pursuit of designing a potential function that accurately encapsulates the \u201ctrue\u201d desirability is meaningless, as it would solve the task at hand [20], and remove the need for learning altogether.", "startOffset": 150, "endOffset": 154}, {"referenceID": 3, "context": "Moreover, heuristics may contribute complementary knowledge that cannot be leveraged in isolation [4].", "startOffset": 98, "endOffset": 101}, {"referenceID": 25, "context": "The recently introduced Horde architecture [26] is well-suited to be the basis of our ensemble, due to its general off-policy convergence guarantees and computational efficiency.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "In contrast to the previous uses of Horde [21], we exploit its power to learn a single task, but from multiple viewpoints.", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "The convergence guarantees of Horde require a latent learning scenario [15], i.", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": "Note that the conventional interpretation of PBRS to steer exploration [6], does not apply here, as the behavior is unaffected by the target policy, and is kept fixed.", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "This work (and its precursor [8]) provides, to our knowledge, the first validation of PBRS effective in such a latent setting.", "startOffset": 29, "endOffset": 32}, {"referenceID": 24, "context": "We assume the usual RL framework [25], in which the agent interacts with its (typically) Markovian environment at discrete time steps t = 1, 2, .", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "Formally, a Markov Decision Process (MDP) [22] is a tuple M = \u3008S,A, \u03b3, T , R\u3009, where: S is a set of states, A is a set of actions, \u03b3 \u2208 [0, 1] is the discounting factor, T = {Psa(\u00b7)|s \u2208 S, a \u2208 A} are the next state transition probabilities with Psa(s \u2032) specifying the probability of state s\u2032 occuring upon taking action a from state s, R : S \u00d7 A \u00d7 S \u2192 R is the reward function with R(s, a, s\u2032) giving the expected value of the reward that will be received when a is taken in state s, and rt+1 denoting the component of R at time t.", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "Formally, a Markov Decision Process (MDP) [22] is a tuple M = \u3008S,A, \u03b3, T , R\u3009, where: S is a set of states, A is a set of actions, \u03b3 \u2208 [0, 1] is the discounting factor, T = {Psa(\u00b7)|s \u2208 S, a \u2208 A} are the next state transition probabilities with Psa(s \u2032) specifying the probability of state s\u2032 occuring upon taking action a from state s, R : S \u00d7 A \u00d7 S \u2192 R is the reward function with R(s, a, s\u2032) giving the expected value of the reward that will be received when a is taken in state s, and rt+1 denoting the component of R at time t.", "startOffset": 135, "endOffset": 141}, {"referenceID": 0, "context": "A (stochastic) Markovian policy \u03c0 : S \u00d7 A \u2192 [0, 1] is a probability distribution over actions at each state, s.", "startOffset": 44, "endOffset": 50}, {"referenceID": 29, "context": "Given \u03c0b, the values of the optimal greedy policy can be learned incrementally through the following Q-learning [30] update:", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "Qt in st+1 Given tabular representation, this process is shown to converge to the correct value estimates (the TD-fixpoint) in the limit under standard approximation conditions [9].", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "FA is known to cause off-policy bootstrapping methods (such as Q-learning) to diverge even on simple problems [2, 28].", "startOffset": 110, "endOffset": 117}, {"referenceID": 27, "context": "FA is known to cause off-policy bootstrapping methods (such as Q-learning) to diverge even on simple problems [2, 28].", "startOffset": 110, "endOffset": 117}, {"referenceID": 25, "context": "The family of gradient temporal difference (GTD) methods provides a solution for this issue, and guarantees offpolicy convergence under FA, given a fixed (or slowly changing behavior) [26].", "startOffset": 184, "endOffset": 188}, {"referenceID": 2, "context": "LSTD [3]),", "startOffset": 5, "endOffset": 8}, {"referenceID": 18, "context": "As a consequence, GTD methods scale well to the number of value functions (policies) learnt [19], and due to the inherent off-policy setting, can do so from a single stream of environment interactions (or experience).", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "[27] formalize this idea in a framework of parallel offpolicy learners, called Horde.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "There have been further successful applications of Horde in realistic robotic setups [21].", "startOffset": 85, "endOffset": 89}, {"referenceID": 25, "context": "On the technical level, GTD methods are based on the idea of performing gradient descent on a reformulated objective function, which ensures convergence to the projected TD-fixpoint, by introducing a gradient bias into the TDupdate [26].", "startOffset": 232, "endOffset": 236}, {"referenceID": 25, "context": "This is a simpler form of the GTD-update, namely that of TDC [26].", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "GQ(\u03bb) [14] augments this update with eligibility traces.", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "In, to our knowledge, the only work that addresses this issue, Kolter [10] gives a way of constraining the solution space to achieve stronger qualitative guarantees, but his algorithm has quadratic complexity and thus is not scalable.", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "Because tasks are identified by their reward function, modifying the reward function needs to be done with care, in order to not alter the task, or else reward shaping can slow down or even prevent finding the optimal policy [23].", "startOffset": 225, "endOffset": 229}, {"referenceID": 19, "context": "[20] show that grounding the shaping rewards in state potentials is both necessary and sufficient for ensuring preservation of the (optimal) policies of the original MDP.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Please refer to Maei\u2019s dissertation for the full details [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "The key insight in ensemble learning is that the strength of an ensemble lies in the diversity its components contribute [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 3, "context": "Maintaining the shapings separately has recently been shown to be a more robust and effective approach [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 26, "context": "Horde\u2019s demonstrated ability to learn thousands of policies in parallel in real time [27, 19] allows to consider large ensembles, at little computational cost.", "startOffset": 85, "endOffset": 93}, {"referenceID": 18, "context": "Horde\u2019s demonstrated ability to learn thousands of policies in parallel in real time [27, 19] allows to consider large ensembles, at little computational cost.", "startOffset": 85, "endOffset": 93}, {"referenceID": 5, "context": "The effects of PBRS on the learning process are usually considered to lie in the guidance of exploration during learning [6, 17, 20].", "startOffset": 121, "endOffset": 132}, {"referenceID": 16, "context": "The effects of PBRS on the learning process are usually considered to lie in the guidance of exploration during learning [6, 17, 20].", "startOffset": 121, "endOffset": 132}, {"referenceID": 19, "context": "The effects of PBRS on the learning process are usually considered to lie in the guidance of exploration during learning [6, 17, 20].", "startOffset": 121, "endOffset": 132}, {"referenceID": 11, "context": "Laud and DeJong [12] formalize this by showing that the difficulty of learning is most dependent on the reward horizon, a measure of the number of decisions a learning agent must make before experiencing accurate feedback, and that reward shaping artificially reduces this horizon.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "2 of van Hasselt\u2019s dissertation [29].", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "We maintain our Horde of shapings as a set D of Greedy-GQ(\u03bb)-learners [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "[27], and refer to individual agents within Horde as demons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Note that because PBRS preserves all of the optimal policies from the original problem [20], the ensemble policy does too.", "startOffset": 87, "endOffset": 91}, {"referenceID": 30, "context": "To the best of our knowledge, both voting methods were first used in the context of RL agents by Wiering and Van Hasselt [31].", "startOffset": 121, "endOffset": 125}, {"referenceID": 30, "context": "We slightly modify the formulation from [31], by ranking Q-values, instead of policy probabilities.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "We begin with the classical benchmark domain of mountain car [25].", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "The state space is approximated with the standard tile-coding technique [25], using ten tilings of 10\u00d710, with a parameter vector learnt for each action.", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "Here x = \u3008x, \u1e8b\u3009 is the state (position and velocity), and \u0101 denotes the normalization of a onto [0, 1].", "startOffset": 96, "endOffset": 102}, {"referenceID": 7, "context": "This experiment first appeared in the early version of this work [8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 17, "context": "We now validate our framework on the problem of cartpole [18].", "startOffset": 57, "endOffset": 61}], "year": 2017, "abstractText": "Potential-based reward shaping (PBRS) is an effective and popular technique to speed up reinforcement learning by leveraging domain knowledge. While PBRS is proven to always preserve optimal policies, its effect on learning speed is determined by the quality of its potential function, which, in turn, depends on both the underlying heuristic and the scale. Knowing which heuristic will prove effective requires testing the options beforehand, and determining the appropriate scale requires tuning, both of which introduce additional sample complexity. We formulate a PBRS framework that reduces learning speed, but does not incur extra sample complexity. For this, we propose to simultaneously learn an ensemble of policies, shaped w.r.t. many heuristics and on a range of scales. The target policy is then obtained by voting. The ensemble needs to be able to efficiently and reliably learn off-policy: requirements fulfilled by the recent Horde architecture, which we take as our basis. We demonstrate empirically that (1) our ensemble policy outperforms both the base policy, and its single-heuristic components, and (2) an ensemble over a general range of scales performs at least as well as one with optimally tuned components.", "creator": "LaTeX with hyperref package"}}}