{"id": "1502.04156", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2015", "title": "Towards Biologically Plausible Deep Learning", "abstract": "differences in long criticised deep learning algorithms as little present current knowledge of neurobiology. we explore more biologically integrated versions of statistical representation learning, focusing principally from abstract posterior learning but developing coherent generalized architecture that properly account for supervised, unsupervised intrinsic predictable learning. the starting point remarks that every matrix learning methodology believed to control objective weight levels ( interval - timing - dependent plasticity ) requires be interpreted within binary descent on zero objective function so far lest the response sensors push forward rates towards approximation values of underlying standard procedure ( be it supervised, biased, dependent reward - bound ). such specific main contention is that this speaks to a form of the variational binary representation, i. e., with better assumptions not exact posteriors, implemented by fuzzy dynamics. principal contribution of this reason is in the procedure cannot remain optimal assuming exact states in these above linear interpretation generally be estimated using an approximation thus only requires propagating variables forward and backward, with pairs of vector learning to form a denoising \u03b4 - trace. finally, to extend the theory about the marginal sequences finding pseudo - encoders to find dense sampling schemes based on the generative interpretation concerning sparse auto - blocks, and similarly set all sufficient bounds over explicit learning tasks.", "histories": [["v1", "Sat, 14 Feb 2015 01:11:25 GMT  (264kb,D)", "http://arxiv.org/abs/1502.04156v1", null], ["v2", "Wed, 25 Nov 2015 04:13:44 GMT  (264kb,D)", "http://arxiv.org/abs/1502.04156v2", null], ["v3", "Tue, 9 Aug 2016 01:57:09 GMT  (293kb,D)", "http://arxiv.org/abs/1502.04156v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "dong-hyun lee", "jorg bornschein", "thomas mesnard", "zhouhan lin"], "accepted": false, "id": "1502.04156"}, "pdf": {"name": "1502.04156.pdf", "metadata": {"source": "META", "title": "Towards Biologically Plausible Deep Learning", "authors": ["Yoshua Bengio", "Dong-Hyun Lee", "Jorg Bornschein"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Deep learning and artificial neural networks have taken their inspiration from brains, but mostly for the form of the computation performed (with much of the biology, such as the presence of spikes remaining to be accounted for). However, what is lacking currently is a credible machine learning interpretation of the learning rules that seem to exist in biological neurons that would explain joint training of a deep neural network, i.e., accounting for credit assign-\nment through a long chain of neural connections. Solving the credit assignment problem therefore means identifying neurons and weights that are responsible for a desired outcome and changing parameters accordingly. Whereas back-propagation offers a machine learning answer, it is not biologically plausible, as discussed in the next paragraph. Finding a biologically plausible machine learning approach for credit assignment in deep networks is the main longterm question to which this paper contributes.\nLet us first consider the claim that state-of-the-art deep learning algorithms rely on mechanisms that seem biologically implausible, such as gradient back-propagation, i.e., the mechanism for computing the gradient of an objective function with respect to neural activations and parameters. The following difficulties can be raised regarding the biological plausibility of back-propagation: (1) the back-propagation computation (coming down from the output layer to lower hidden layers) is purely linear, whereas biological neurons interleave linear and non-linear operations, (2) if the feedback paths known to exist in the brain (with their own synapses and maybe their own neurons) were used to propagate credit assignment by backprop, they would need precise knowledge of the derivatives of the non-linearities at the operating point used in the corresponding feedforward computation on the feedforward path1, (3) similarly, these feedback paths would have to use exact symmetric weights (with the same connectivity, transposed) of the feedforward connections,2 (4) real neurons communicate by (possibly stochastic) binary values (spikes), not by clean continuous values, (5) the computation would have to be precisely clocked to alternate between feedforward and back-propagation phases (since the latter needs the former\u2019s results), and (6) it is not clear where the output targets would come from. The approach proposed in this paper has the ambition to address all these issues, although some question marks as to a possible biological implementations remain, and of course many details of the biology that need to be accounted for are not covered\n1and with neurons not all being exactly the same, it could be difficult to match the right estimated derivatives\n2this is known as the weight transport problem (Lillicrap et al., 2014)\nar X\niv :1\n50 2.\n04 15\n6v 1\n[ cs\n.L G\n] 1\n4 Fe\nb 20\n15\nhere.\nNote that back-propagation is used not just for classical supervised learning but also for many unsupervised learning algorithms, including all kinds of auto-encoders: sparse auto-encoders (Ranzato et al., 2007; Goodfellow et al., 2009), denoising auto-encoders (Vincent et al., 2008), contractive auto-encoders (Rifai et al., 2011), and more recently, variational auto-encoders (Kingma & Welling, 2014). Other unsupervised learning algorithms exist which do not rely on back-propagation, such as the various Boltzmann machine learning algorithms (Hinton & Sejnowski, 1986; Smolensky, 1986; Hinton et al., 2006; Salakhutdinov & Hinton, 2009). Boltzmann machines are probably the most biologically plausible learning algorithms for deep architectures that we currently know, but they also face several question marks in this regard, such as the weight transport problem ((3) above) to achieve symmetric weights, and the positive-phase vs negative-phase synchronization question (similar to (5) above).\nOur starting point (Sec. 2) proposes an interpretation of the main learning rule observed in biological synapses: SpikeTiming-Dependent Plasticity (STDP). Following up on the ideas presented in Hinton\u2019s 2007 talk (Hinton, 2007), we first argue that STDP could be seen as stochastic gradient descent if only the neuron was driven by a feedback signal that either increases or decreases the neuron\u2019s firing rate in proportion to the gradient of an objective function with respect to the neuron\u2019s voltage potential.\nIn Sec. 3 we then argue that the above interpretation suggests that neural dynamics (which creates the above changes in neuronal activations thanks to feedback and lateral connections) correspond to inference towards neural configurations that are more consistent with each other and with the observations (inputs, targets, or rewards). This view extends Hinton\u2019s supervised learning proposal to the unsupervised generative setting. It naturally suggests that the training procedure corresponds to a form of variational EM (Neal & Hinton, 1999) (see Sec.3), possibly based on MAP (maximum a posteriori) or MCMC (Markov Chain Monte-Carlo) approximations. In Sec. 4 we show how this mathematical framework suggests a training procedure for a deep generative network with many layers of latent variables. However, the above interpretation would still require to compute some gradients. Another contribution (Sec. 6) is to show that one can estimate these gradients via an approximation that only involves ordinary neural computation and no explicit derivatives, following previous work on target propagation (Bengio, 2014; Lee et al., 2014).\nAlthough our primary justification for the proposed learning algorithm corresponds to a deep directed graphical model, it turns out that the proposed learning mechanism can be interpreted as training a denoising auto-encoder. As\ndiscussed in Sec. 5 these alternative interpretations of the model provide different ways to sample from it, and we found that better samples could be obtained."}, {"heading": "2. STDP as Stochastic Gradient Descent", "text": "Spike-Timing-Dependent Plasticity or STDP is believed to be the main form of synaptic change in neurons (Markram & Sakmann, 1995; Gerstner et al., 1996) and it relates the expected change in synaptic weights to the timing difference between post-synaptic spikes and pre-synaptic spikes. Although it is the result of experimental observations in biological neurons, its interpretation as part of a learning procedure that could explain learning in deep networks remains unclear. This paper aims at proposing such an interpretation, starting from the proposal made by Hinton (2007), but extending these ideas towards unsupervised generative modeling of the data.\nWhat has been observed in STDP is that the weights change if there is a pre-synaptic spike in the temporal vicinity of a post-synaptic spike: that change is positive if the postsynaptic spike happens just after the pre-synaptic spike, negative if it happens just before. As suggested in Hinton\u2019s talk, this also corresponds to a temporal derivative filter applied to the post-synaptic firing rate, at the time of the pre-synaptic spike. To illustrate this, consider the situation in which two neurons Ni and Nk impinge on neuron Nj , and each neuron, say Ni, has a voltage potential Vi which, when above threshold, makes the neuron send out a spike Si with probability (called rate) Ri. If Rk increases after a spike Si, in average (over Sk), that will increase Vj and Rj and thus the probability of Nj\u2019s post-synaptic spike. That will come right after the Ni spike, yielding an increase in the synaptic weight Wij as per STDP. However, if Rk decreases after a spike Si, this decreases the probability ofNj spiking after Ni\u2019s spike, or equivalently, making the probability of Nj\u2019s spike occuring before Ni\u2019s spike larger than the probability of Nj\u2019s spike occuring after Ni\u2019s spike, i.e., making Vj and Rj smaller. According to STDP, this situation would then correspond to a decrease in the synaptic weight. In conclusion, these arguments suggest that STDP can be interpreted as follows:\n\u2206Wij \u221d Si\u2206Vj , (1)\nwhere \u2206 indicates the temporal change, Si indicates the pre-synaptic spike (from neuron i), and Vj indicates the post-synaptic voltage potential (of neuron j).\nClearly, the consequence is that if the change \u2206Vj corresponds to improving some objective function J , then STDP corresponds to approximate stochastic gradient descent in that objective function. With this view, STDP would implement the delta rule (gradient descent on a onelayer network) if the post-synaptic activation changes in the direction of the gradient."}, {"heading": "3. Variational EM with Learned Approximate Inference", "text": "To take advantage of the above statement, the dynamics of the neural network must be such that neural activities move towards better values of some objective function J . Hence we would like to define such an objective function in a way that is consistent with the actual neural computation being performed (for fixed weights W ), in the sense that the expected temporal change of the voltage potentials approximately corresponds to increases in J . In this paper, we are going to consider the voltage potentials as the central variables of interest which influence J and consider them as latent variables V (denoted h below to keep machine learning interpretation general), while we will consider the actual spike trains S as non-linear noisy corruptions of V , a form of quantization (with the \u201cnoise level\u201d controlled either by the integration time or the number of redundant neurons in an ensemble (Legenstein & Maass, 2014). This view makes the application of the denoising auto-encoder theorems discussed in Sec. 5 more straightforward.\nThe main contribution of this paper is to propose and give support to the hypothesis that J comes out of a variational bound on the likelihood of the data. Variational bounds have been proposed to justify various learning algorithms for generative models (Hinton et al., 1995) (Sec. 7). To keep the mapping to biology open, consider such bounds and the associated criteria that may be derived from them, using an abstract notation with observed variable x and latent variable h. If we have a model p(x, h) of their joint distribution, as well as some approximate inference mechanism defining a conditional distribution q\u2217(H|x), the observed data log-likelihood log p(x) can be decomposed as\nlog p(x) = log p(x) \u2211 h q\u2217(h|x)\n= \u2211 h q\u2217(h|x) log p(x, h)q \u2217(h|x) p(h|x)q\u2217(h|x) =Eq\u2217(H|x)[log p(x,H)] + H[q \u2217(H|x)]\n+ KL(q\u2217(H|x)||p(H|x)), (2)\nwhere H[] denotes entropy and KL(||) the KullbackLeibler (KL) divergence, and where we have used sums but integrals should be considered when the variables are continuous. Since both the entropy and the KL-divergence are non-negative, we can either bound the log-likelihood via\nlog p(x) \u2265 Eq\u2217(H|x)[log p(x,H)] + H[q\u2217(H|x)], (3)\nor if we care only about optimizing p, log p(x) \u2265 Eq\u2217(H|x)[log p(x,H)]. (4)\nThe idea of variational bounds as proxies for the loglikelihood is that as far as optimizing p is concerned, i.e., dropping the entropy term which does not depend on p,\nthe bound becomes tight when q\u2217(H|x) = p(H|x). This suggests that q\u2217(H|x) should approximate p(H|x). Fixing q\u2217(H|x) = p(H|x) and optimizing p with q fixed is the EM algorithm. Here (and in general) this is not possible so we consider variational methods in which q\u2217(H|x) approximates but does not reach p(H|x).\nWe propose to decompose q\u2217(H|x) in two components: parametric initialization q0(H|x) = q(H|x) and iterative inference, implicitly defining q\u2217(H|x) = qT (H|x) via a deterministic or stochastic update, or transition operator\nqt(H|x) = A(x) qt\u22121(H|x). (5)\nThe variational bound suggests that A(x) should gradually bring qt(H|x) closer to p(H|x). At the same time, to make sure that a few steps will be sufficient to approach p(H|x), one may add a term in the objective function to make q0(H|x) closer to p(H|x), as well as to encourage p(x, h) to favor solutions p(H|x) that can be easily approximated by qt(H|x) even for small t.\nFor this purpose, consider as training objective a regularized variational MAP-EM criterion (for a given x):\nJ = log p(x, h) + \u03b1 log q(h|x), (6)\nwhere h is a free variable (for each x) initialized from q(H|x) and then iteratively updated to approximately maximize J . The total objective function is just the average of J over all examples after having performed inference (the approximate maximization over h for each x). A reasonable variant would not just encourage q = q0 to generate h (given x), but all the qt\u2019s for t > 0 as well. Alternatively, the iterative inference could be performed by stochastically increasing J , i.e., via a Markov chain which may correspond to probabilistic inference with spiking neurons (Pecevski et al., 2011). The corresponding variational MAP or variational MCMC algorithm would be as in Algorithm 1. For the stochastic version one would inject noise when updating h. Variational MCMC (de Freitas et al., 2001) can be used to approximate the posterior, e.g., as in the model from Salimans et al. (2014). However, a rejection step does not look very biologically plausible (both for the need of returning to a previous state and for the need to evaluate the joint likelihood, a global quantity). On the other hand, a biased MCMC with no rejection step, such as the stochastic gradient Langevin MCMC of Welling & Teh (2011) can work very well in practice."}, {"heading": "4. Training a Deep Generative Model", "text": "There is strong biological evidence of a distinct pattern of connectivity between cortical areas that distinguishes between \u201cfeedforward\u201d and \u201cfeedback\u201d connections (Douglas et al., 1989) at the level of the microcircuit of cortex (i.e., feedforward and feedback connections do not land in the same type of cells). Furthermore, the feedforward connec-\nAlgorithm 1 Variational MAP (or MCMC) SGD algorithm for gradually improving the agreement between the values of the latent variables h and the observed data x. q(h|x) is a learned parametric initialization for h, p(h) is a parametric prior on the latent variables, and p(x|h) specifies how to generate x given h. Objective function J is defined in Eq. 6 Learning rates \u03b4 and respectively control the optimization of h and of parameters \u03b8 (of both q and p).\nInitialize h \u223c q(h|x) for t = 1 to T do h\u2190 h+ \u03b4 \u2202J\u2202h (optional: add noise for MCMC) end for \u03b8 \u2190 \u03b8 + \u2202J\u2202\u03b8\ntions form a directed acyclic graph with nodes (areas) updated in a particular order, e.g., in the visual cortex (Felleman & Essen, 1991). So consider Algorithm 1 with h decomposed into multiple layers, with the conditional independence structure of a directed graphical model structured as a chain, both for p (going down) and for q (going up):\np(x, h) = p(x|h(1)) ( M\u22121\u220f k=1 p(h(k)|h(k+1)) ) p(h(M))\nq(h|x) = q(h(1)|x) M\u22121\u220f k=1 q(h(k+1)|h(k)). (7)\nThis clearly decouples the updates associated with each layer, for both h and \u03b8, making these updates \u201clocal\u201d to the layer k, based on \u201cfeedback\u201d from layer k \u2212 1 and k + 1. Nonetheless, thanks to the iterative nature of the updates of h, all the layers are interacting via both feedforward (q(h(k)|h(k\u22121))) and feedback (p(h(k)|h(k\u22121)) paths. Denoting x = h(0) to simplify notation, the h update would thus consist in moves of the form h(k) \u2190h(k) + \u03b4 \u2202 \u2202h(k) ( log(p(h(k\u22121)|h(k))p(h(k)|h(k+1)))\n+ \u03b1 log(q(h(k)|h(k\u22121))q(h(k+1)|h(k))) ) ,\n(8)\nwhere \u03b1 is as in Eq. 6. No back-propagation is needed for the above derivatives when h(k) is on the left hand side of the conditional probability bar. Sec. 6 deals with the right hand side case. For the left hand side case, e.g., p(h(k)|h(k+1)) a conditional Gaussian with mean \u00b5 and variance \u03c32, the gradient with respect to h(k) is simply \u00b5\u2212h(k) \u03c32 . Note that there is an interesting interpretation of such a deep model: the layers above h(k) provide a complex implicitly defined prior for p(h(k))."}, {"heading": "5. Alternative Interpretations as Denoising Auto-Encoder", "text": "By inspection of Algorithm 1, one can observe that this algorithm trains p(x|h) and q(h|x) to form complementary\npairs of an auto-encoder (since the input of one is the target of the other and vice-versa). Note that from that point of view any of the two can act as encoder and the other as decoder for it, depending on whether we start from h or from x. In the case of multiple latent layers, each pair of conditionals q(h(k+1)|h(k)) and p(h(k)|h(k+1)) forms a symmetric auto-encoder, i.e., either one can act as the encoder and the other as the corresponding decoder, since they are trained with the same (h(k), h(k+1)) pairs (but with reversed roles of input and target).\nIn addition, if noise is injected, e.g., in the form of the quantization induced by a spike train, then the trained auto-encoders are actually denoising auto-encoders, which means that both the encoders and decoders are contractive: in the neighborhood of the observed (x, h) pairs, they map neighboring \u201ccorrupted\u201d values to the \u201cclean\u201d (x, h) values."}, {"heading": "5.1. Joint Denoising Auto-Encoder with Latent Variables", "text": "This suggests considering a special kind of \u201cjoint\u201d denoising auto-encoder which has the pair (x, h) as \u201cvisible\u201d variable, an auto-encoder that implicitly estimates an underlying p(x, h). The transition operator3 for that joint visiblelatent denoising auto-encoder is the following in the case of a single hidden layer:\n(x\u0303, h\u0303)\u2190 corrupt(x, h) h \u223c q(h|x\u0303) x \u223c p(x|h\u0303), (9)\nwhere the corruption may correspond to the stochastic quantization induced by the neuron non-linearity and spiking process. In the case of a middle layer h(k) in a deeper model, the transition operator must account for the fact that h(k) can either be reconstructed from above or from below, yielding, with probability say 12 ,\nh(k) \u223c p(h(k)|h\u0303(k+1)), (10)\nand with one minus that probability,\nh(k) \u223c q(h(k)|h\u0303(k\u22121)). (11)\nSince this interpretation provides a different model, it also provides a different way of generating samples. Especially for shallow, we have found that better samples could be obtained in this way, i.e., running the Markov chain with the above transition operator for a few steps.\nThere might be a geometric interpretation for the improved quality of the samples when they are obtained in this way,\n3See Theorem 1 from Bengio et al. (2013) for the generative interpretation of denoising auto-encoders: it basically states that one can sample from the model implicitly estimated by a denoising auto-encoder by simply alternating noise injection (corruption), encoding and decoding, these forming each step of a generative Markov chain.\ncompared to the directed generative model that was defined earlier. Denote q\u2217(x) the empirical distribution of the data, which defines a joint q\u2217(h, x) = q\u2217(x)q\u2217(h|x). Consider the likely situation where p(x, h) is not well matched to q\u2217(h, x) because for example the parametrization of p(h) is not powerful enough to capture the complex structure in the empirical distribution q\u2217(h) obtained by mapping the training data through the encoder and inference q\u2217(h|x). Typically, q\u2217(x) would concentrate on a manifold and the encoder would not be able to completely unfold it, so that q\u2217(h) would contain complicated structure with pockets or manifolds of high probability. If p(h) is a simple factorized model, then it will generate values of h that do not correspond well to those seen by the decoder p(x|h) when it was trained, and these out-of-manifold samples in h-space are likely to be mapped to out-of-manifold samples in xspace. One solution to this problem is to increase the capacity of p(h) (e.g., by adding more layers on top of h). Another is to make q(h|x) more powerful (which again can be achieved by increasing the depth of the model, but this time by inserting additional layers below h). Now, there is a cheap way of obtaining a very deep directed graphical model, by unfolding the Markov chain of an MCMC-based generative model for a fixed number of steps, i.e., considering each step of the Markov chain as an extra \u201clayer\u201d in a deep directed generative model, with shared parameters across these layers. As we have seen that there is such an interpretation via the joint denoising auto-encoder over both latent and visible, this idea can be immediately applied. We know that each step of the Markov chain operator moves its input distribution closer to the stationary distribution of the chain. So if we start from samples from a very broad (say factorized) prior p(h) and we iteratively encode/decode them (injecting noise appropriately as during training) by successively sampling from p(x|h) and then from q(h|x), the resulting h samples should end up looking more like those seen during training (i.e., from q\u2217(h))."}, {"heading": "5.2. Latent Variables as Corruption", "text": "There is another interpretation of the training procedure, also as a denoising auto-encoder, which has the advantage of producing a generative procedure that is the same as the inference procedure except for x being unclamped.\nWe return again to the generative interpretation of the denoising criterion for auto-encoders, but this time we consider the non-parametric process q\u2217(h|x) as a kind of corruption of x that yields the h used as input for reconstructing the observed x via p(x|h). Under that interpretation, a valid generative procedure consists at each step in first performing inference, i.e., sampling h from q\u2217(h|x), and second sampling from p(x|h). Iterating these steps generates x\u2019s according to the Markov chain whose stationary distribution is an estimator of the data generating distribution that produced the training x\u2019s (Bengio et al., 2013).\nThis view does not care about how q\u2217(h|x) is constructed, but it tells us that if p(x|h) is trained to maximize reconstruction probability, then we can sample in this way from the implicitly estimated model.\nWe have also found good results using this procedure (Algorithm 2 below), and from the point of view of biological plausibility, it would make more sense that \u201cgenerating\u201d should involve the same operations as \u201cinference\u201d, except for the input being observed or not."}, {"heading": "6. Targetprop instead of Backprop", "text": "In Algorithm 1 and the related stochastic variants Eq. 8 suggests that back-propagation (through one layer) is still needed when h(k) is on the right hand side of the conditional probability bar, e.g., to compute \u2202p(h\n(k\u22121)|h(k)) \u2202h(k)\n. Such a gradient is also the basic building block in backpropagation for supervised learning: we need to back-prop through one layer, e.g. to make h(k) more \u201ccompatible\u201d with h(k\u22121). This provides a kind error signal, which in the case of unsupervised learning comes from the sensors, and in the case of supervised learning, comes from the layer holding the observed \u201ctarget\u201d.\nBased on recent theoretical results on denoising autoencoders, we propose the following estimator (up to a scaling constant) of the required gradient, which is related to previous work on \u201ctarget propagation\u201d (Bengio, 2014; Lee et al., 2014) or targetprop for short. To make notation simpler, we focus below on the case of two layers h and x with \u201cencoder\u201d q(h|x) and \u201cdecoder\u201d p(x|h), and we want to estimate \u2202 log p(x|h)\u2202h . We start with the special case where p(x|h) is a Gaussian with mean g(h) and q(h|x) is Gaussian with mean f(x), i.e., f and g are the deterministic components of the encoder and decoder respectively. The proposed estimator is then\n\u2206\u0302h = f(x)\u2212 f(g(h))\n\u03c32h , (12)\nwhere \u03c32h is the variance of the noise injected in q(h|x).\nLet us now justify this estimator. Theorem 2 by Alain & Bengio (2013) states that in a denoising auto-encoder with reconstruction function r(x) = decode(encode(x)), a well-trained auto-encoder estimates the log-score via the difference between its reconstruction and its input:\nr(x)\u2212 x \u03c32 \u2192 \u2202 log p(x) \u2202x ,\nwhere \u03c32 is the variance of injected noise, and p(x) is the implicitly estimated density. We are now going to consider two denoising auto-encoders and apply this theorem to them. First, we note that the gradient \u2202 log p(x|h)\u2202h that we wish to estimate can be decomposed as follows:\n\u2202 log p(x|h) \u2202h = \u2202 log p(x, h) \u2202h \u2212 \u2202 log p(h) \u2202h .\nHence it is enough to estimate \u2202 log p(x,h)\u2202h as well as \u2202 log p(h)\n\u2202h . The second one can be estimated by considering the auto-encoder which estimates p(h) implicitly and for which g is the encoder (with g(h) the \u201ccode\u201d for h) and f is the decoder (with f(g(h)) the \u201creconstruction\u201d of h). Hence we have that f(g(h))\u2212h\n\u03c32h is an estimator of \u2202 log p(h)\u2202h .\nThe other gradient can be estimated by considering the joint denoising auto-encoder over (x, h) introduced in the previous section. The (noise-free) reconstruction function for that auto-encoder is\nr(x, h) = (g(h), f(x)).\nHence f(x)\u2212h \u03c32h is an estimator of \u2202 log p(x,h)\u2202h . Combining the two estimators, we get\n(f(x)\u2212 h) \u03c32h \u2212 (f(g(h))\u2212 h) \u03c32h = f(x)\u2212 f(g(h)) \u03c32h ,\nwhich corresponds to Eq. 12.\nAnother way to obtain the same formula from a geometric perspective is illustrated in Figure 1. It was introduced in Lee & Bengio (2014) in the context of a backprop-free algorithm for training a denoising auto-encoder."}, {"heading": "7. Related Work", "text": "The main inspiration for the proposed framework is the biological implementation of back-propagation proposed by Hinton (2007). In that talk, Hinton suggests that STDP corresponds to a gradient update step with the gradient on the voltage potential corresponding to its temporal derivative. To obtain the supervised back-propagation update in the proposed scenario would require symmetric weights and synchronization of the computations in terms of feedforward and feedback phases.\nOur proposal extends these ideas to include unsupervised learning, avoids the need for symmetric weights, and exploits inference to obtain targets and a probabilistic interpretation as the optimization of a variational bound on the\nAlgorithm 2 Inference, training and generative procedures used in Experiment 1, for a model with three layers x, h1, h2. fi() is the feedforward map from layer i \u2212 1 to layer i and gi() is the feedback map from layer i to layer i \u2212 1, with x = h0 being layer 0.\nDefine INFERENCE(x, N=15, \u03b4=0.1, \u03b1=0.001): Feedforward pass: h1 \u2190 f1(x), h2 \u2190 f2(h1) for t = 1 to N do h2 \u2190 h2 + \u03b4(f2(h1)\u2212 f2(g2(h2))) h1 \u2190 h1 + \u03b4(f1(x)\u2212 f1(g1(h1))) + \u03b1(g2(h2)\u2212 h1) end for Return h1, h2\nDefine TRAIN() for x in training set do\ndo INFERENCE(x) train each layer (both fl and gl) by taking Gaussiancorrupted value of other layer as input and the clean inferred value as target (i.e. applying the delta rule). For the top sigmoid layer, we sample 3 binary values and average them as a spike-like corruption. end for Compute the mean and variance of the h2 values inferred in the training set. Multiply the variances by 4. Define p(h2) as sampling from this Gaussian.\nDefine GENERATE(): Sample h2 from p(H2) for t = 1 to 3 do h1, h2 \u2190 INFERENCE(x,\u03b1 = 0.3) x\u2190 g1(h1) end for Return x\nlikelihood. There is also an interesting connection with an earlier proposal for a more biologically plausible implementation of supervised back-propagation (Xie & Seung, 2003) which also relies on iterative inference (a deterministic relaxation in that case), but needs symmetric weights.\nAnother important inspiration is Predictive Sparse Decomposition (PSD) (Kavukcuoglu et al., 2008). PSD is a special case of Algorithm 1 when there is only one layer and the encoder q(h|x), decoder p(x|h), and prior p(h) have a specific form which makes p(x, h) a sparse coding model and q(h|x) a fast parametric approximation of the correct posterior. Our proposal extends PSD by providing a justification for the training criterion as a variational bound, by generalizing to multiple layers of latent variables, and by providing associated generative procedures.\nThe combination of a parametric approximate inference machine (the encoder) and a generative decoder (each with possibly several layers of latent variables) is an old theme\nthat was started with the Wake-Sleep algorithm (Hinton et al., 1995) and which finds very interesting instantiations in the variational auto-encoder (Kingma & Welling, 2014; Kingma et al., 2014) and the reweighted wake-sleep algorithm (Bornschein & Bengio, 2014). Two important differences with the approach proposed here is that here we avoid back-propagation thanks to an inference step that approximates the posterior. In this spirit, see the recent work introducing MCMC inference for the variational auto-encoder Salimans et al. (2014).\nThe proposal made here also owes a lot to the idea of target propagation introduced in Bengio (2014); Lee et al. (2014), to which it adds the idea that in order to find a target that is consistent with both the input and the final output target, it makes sense to perform iterative inference, reconciling the bottom-up and top-down pressures. Addressing the weight transport problem (the weight symmetry constraint) was also done for the supervised case using feedback alignment (Lillicrap et al., 2014): even if the feedback weights do not exactly match the feedforward weights, the latter learn to align to the former and \u201cback-propagation\u201d (with the wrong feedback weights) still works.\nThe targetprop formula avoiding back-propagation through one layer is actually the same as proposed by Lee & Bengio (2014) for backprop-free auto-encoders. What has been added here is a justification of this specific formula based on the denoising auto-encoder theorem from Alain & Bengio (2013), and the empirical validation of its ability to climb the joint likelihood for variational inference."}, {"heading": "8. Experimental Validation", "text": "Figure 2 shows generated samples obtained after training on MNIST with Algorithm 2 (derived from the considerations of Sec.s 4, 5 and 6). The network has two hid-\nden layers, h1 with 1000 softplus units and h2 with 100 sigmoid units (which can be considered biologically plausible (Glorot et al., 2011)). We trained for 20 epochs, with minibatches of size 100 to speed-up computation using GPUs. Results can be reproduced from code at Using the Parzen density estimator previously used for that data, we obtain an estimated log-likelihood LL=236 (using a standard deviation of 0.2 for the Parzen density estimator, chosen with the validation set), which is about the same or better as was obtained for contractive autoencoders (Rifai et al., 2011) (LL=121), deeper generative stochastic networks (Bengio et al., 2014) (LL=214) and generative adversarial networks (Goodfellow et al., 2014) (LL=225). In accordance with Algorithm 2, the variances of the conditional densities are 1, and the top-level prior is ignored during most of training (as if it was a very broad, uniform prior) and only set to the Gaussian by the end of training, before generation (by setting the parameters of p(h2) to the empirical mean and variance of the projected training examples at the top level). Figure 3 shows that the targetprop updates (instead of the gradient updates) allow the inference process to indeed smoothly increase the joint likelihood. Note that if we sample using the directed graphical model p(x|h)p(h), the samples are not as good and LL=126, suggesting as discussed in Sec. 5 that additional inference and encode/decode iterations move h towards values that are closer to q\u2217(h) (the empirical distribution of inferred states from training examples). The experiment illustrated in Figure 4 shows that the proposed inference mechanism can be used to fill-in missing values with a trained model. The model is the same that was trained using Algorithm 2 (with samples shown in Figure 2). 20 iterations steps of encode/decode as described below were performed, with a call to INFERENCE (to maximize p(x, h) over h) for each step, with a slight modification. Instead of using f1(x)\u2212 f1(g1(h)) to account for\nthe pressure of x upon h (towards maximizing p(x|h)), we used f1(xv, gm(h)) \u2212 f(g(h)), where xv is the part of x that is visible (clamped) while gm(h) is the part of the output of g(h) that concerns the missing (corrupted) inputs. This formula was derived from the same consideration as for Eq. 12, but where the quantity of interest is \u2202 log p(x\nv|h) \u2202h\nrather than \u2202 log p(x|h)\u2202h , and we consider that the reconstruction of h, given xv , fills-in the missing inputs (xm) from gm(h)."}, {"heading": "9. Future Work and Conclusion", "text": "We consider this paper as an exploratory step towards explaining a central aspect of the brain\u2019s learning algorithm: credit assignment through many layers. As argued by Bengio (2014); Lee et al. (2014), departing from backpropagation could be useful not just for biological plausibility but from a machine learning point of view as well: by working on the \u201ctargets\u201d for the intermediate layers, we may avoid the kind of reliance on smoothness and derivatives that characterizes back-propagation, as these techniques can in principle work even with highly non-linear transformations for which gradients are often near 0, e.g., with stochastic binary units (Lee et al., 2014). Besides the connection between STDP and variational EM, an important contribution of this paper is to show that the \u201ctargetprop\u201d update which estimates the gradient through one layer can be used for inference, yielding systematic improvements in the joint likelihood and allowing to learn a good generative model. Another interesting contribution is that the variational EM updates, with noise added, can also be interpreted as training a denoising auto-encoder over both visible and latent variables, and that iterating from the associated Markov chain yields better samples than those obtained from the directed graphical model estimated by variational EM.\nMany directions need to be investigated to follow-up on the work reported here. An important element of neural\ncircuitry is the strong presence of lateral connections between nearby neurons in the same area. In the proposed framework, an obvious place for such lateral connections is to implement the prior on the joint distribution between nearby neurons, something we have not explored in our experiments. For example, Garrigues & Olshausen (2008) have discussed neural implementations of the inference involved in sparse coding based on the lateral connections.\nAlthough we have found that \u201cinjecting noise\u201d helped training a better model, more theoretical work needs to be done to explore this replacement of a MAP-based inference by an MCMC-like inference, which should help determine how and how much of this noise should be injected.\nWhereas this paper focused on unsupervised learning, these ideas could be applied to supervised learning and reinforcement learning as well. For reinforcement learning, an important role of the proposed algorithms is to learn to predict rewards, although a more challenging question is how the MCMC part could be used to simulate future events. For both supervised learning and reinforcement learning, we would probably want to add a mechanism that would give more weight to minimizing prediction (or reconstruction) error for some of the observed signals (e.g. y is more important to predict than x).\nFinally, a lot needs to be done to connect in more detail the proposals made here with biology, including neural implementation using spikes with Poisson rates as the source of signal quantization and randomness, taking into account the constraints on the sign of the weights depending on whether the pre-synaptic neuron is inhibitory or excitatory, etc. In addition, although the operations proposed here are backprop-free, they may still require some kinds of synchronizations (or control mechanism) and specific connectivity to be implemented in brains."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Jyri Kivinen, Tim Lillicrap and Saizheng Zhang for feedback and discussions, as well as NSERC, CIFAR, Samsung and Canada Research Chairs for funding, and Compute Canada for computing resources."}], "references": [{"title": "What regularized auto-encoders learn from the data generating distribution", "author": ["Alain", "Guillaume", "Bengio", "Yoshua"], "venue": "In ICLR\u20192013. also arXiv report 1211.4246,", "citeRegEx": "Alain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Alain et al\\.", "year": 2013}, {"title": "How auto-encoders could provide credit assignment in deep networks via target propagation", "author": ["Bengio", "Yoshua"], "venue": "Technical report,", "citeRegEx": "Bengio and Yoshua.,? \\Q2014\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2014}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In NIPS\u20192013,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "Eric", "Alain", "Guillaume", "Yosinski", "Jason"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Reweighted wakesleep", "author": ["Bornschein", "J\u00f6rg", "Bengio", "Yoshua"], "venue": "Technical report,", "citeRegEx": "Bornschein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bornschein et al\\.", "year": 2014}, {"title": "Variational MCMC", "author": ["de Freitas", "Nando", "H\u00f8jen-S\u00f8rensen", "Pedro", "Jordan", "Michael I", "Russell", "Stuart"], "venue": null, "citeRegEx": "Freitas et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Freitas et al\\.", "year": 2001}, {"title": "A canonical microcircuit for neocortex", "author": ["R.J. Douglas", "K.A. Martin", "D. Whitteridge"], "venue": "Neural Computation,", "citeRegEx": "Douglas et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Douglas et al\\.", "year": 1989}, {"title": "Distributed hierarchical processing in the primate cerebral cortex", "author": ["Felleman", "Daniel J", "Essen", "David C. Van"], "venue": "Cerebral Cortex,", "citeRegEx": "Felleman et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Felleman et al\\.", "year": 1991}, {"title": "A neuronal learning rule for sub-millisecond temporal coding", "author": ["W. Gerstner", "R. Kempter", "J.L. van Hemmen", "H. Wagner"], "venue": "Nature, 386:76\u201378,", "citeRegEx": "Gerstner et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Gerstner et al\\.", "year": 1996}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In AISTATS\u20192011,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Measuring invariances in deep networks", "author": ["Goodfellow", "Ian", "Le", "Quoc", "Saxe", "Andrew", "Ng"], "venue": "In NIPS\u20192009,", "citeRegEx": "Goodfellow et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "Generative adversarial networks", "author": ["Goodfellow", "Ian J", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In NIPS\u20192014,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "How to do backpropagation in a brain", "author": ["Hinton", "Geoffrey E"], "venue": "Invited talk at the NIPS\u20192007 Deep Learning Workshop,", "citeRegEx": "Hinton and E.,? \\Q2007\\E", "shortCiteRegEx": "Hinton and E.", "year": 2007}, {"title": "The wake-sleep algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M"], "venue": "Science, 268:1558\u20131161,", "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "Technical report, Computational and Biological Learning Lab, Courant Institute,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2008}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "In NIPS\u20192014,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Durk P", "Welling", "Max"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Backprop-free autoencoders. NIPS\u20192014", "author": ["Lee", "Dong-Hyun", "Bengio", "Yoshua"], "venue": "Deep Learning workshop,", "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Ensembles of spiking neurons with noise support optimal probabilistic inference in a dynamically changing environment", "author": ["Legenstein", "Robert", "Maass", "Wolfgang"], "venue": "PLOS Computational Biology,", "citeRegEx": "Legenstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Legenstein et al\\.", "year": 2014}, {"title": "Random feedback weights support learning in deep neural networks", "author": ["Lillicrap", "Timothy P", "Cownden", "Daniel", "Tweed", "Douglas B", "Akerman", "Colin J"], "venue": null, "citeRegEx": "Lillicrap et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2014}, {"title": "Action potentials propagating back into dendrites triggers changes in efficacy", "author": ["H. Markram", "B. Sakmann"], "venue": "Soc. Neurosci. Abs,", "citeRegEx": "Markram and Sakmann,? \\Q1995\\E", "shortCiteRegEx": "Markram and Sakmann", "year": 1995}, {"title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "author": ["R.M. Neal", "G.E. Hinton"], "venue": null, "citeRegEx": "Neal and Hinton,? \\Q1999\\E", "shortCiteRegEx": "Neal and Hinton", "year": 1999}, {"title": "Probabilistic inference in general graphical models through sampling in stochastic networks of spiking neurons", "author": ["Pecevski", "Dejan", "Buesing", "Lars", "Maass", "Wolfgang"], "venue": "PLOS Computational Biology,", "citeRegEx": "Pecevski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pecevski et al\\.", "year": 2011}, {"title": "Efficient learning of sparse representations with an energybased model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "In NIPS\u20192006,", "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "Contractive autoencoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah", "Vincent", "Pascal", "Muller", "Xavier", "Glorot", "Bengio", "Yoshua"], "venue": "In ICML\u20192011,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In AISTATS\u20192009,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Markov Chain Monte Carlo and Variational Inference: Bridging the Gap", "author": ["T. Salimans", "D.P. Kingma", "M. Welling"], "venue": null, "citeRegEx": "Salimans et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2014}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["Smolensky", "Paul"], "venue": "Parallel Distributed Processing,", "citeRegEx": "Smolensky and Paul.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky and Paul.", "year": 1986}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "ICML", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["Welling", "Max", "Teh", "Yee-Whye"], "venue": "In ICML\u20192011,", "citeRegEx": "Welling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2011}, {"title": "Equivalence of backpropagation and contrastive Hebbian learning in a layered network", "author": ["Xie", "Xiaohui", "Seung", "H. Sebastian"], "venue": "Neural Computation,", "citeRegEx": "Xie et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 20, "context": "and with neurons not all being exactly the same, it could be difficult to match the right estimated derivatives this is known as the weight transport problem (Lillicrap et al., 2014) ar X iv :1 50 2.", "startOffset": 158, "endOffset": 182}, {"referenceID": 24, "context": "Note that back-propagation is used not just for classical supervised learning but also for many unsupervised learning algorithms, including all kinds of auto-encoders: sparse auto-encoders (Ranzato et al., 2007; Goodfellow et al., 2009), denoising auto-encoders (Vincent et al.", "startOffset": 189, "endOffset": 236}, {"referenceID": 10, "context": "Note that back-propagation is used not just for classical supervised learning but also for many unsupervised learning algorithms, including all kinds of auto-encoders: sparse auto-encoders (Ranzato et al., 2007; Goodfellow et al., 2009), denoising auto-encoders (Vincent et al.", "startOffset": 189, "endOffset": 236}, {"referenceID": 29, "context": ", 2009), denoising auto-encoders (Vincent et al., 2008), contractive auto-encoders (Rifai et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 25, "context": ", 2008), contractive auto-encoders (Rifai et al., 2011), and more recently, variational auto-encoders (Kingma & Welling, 2014).", "startOffset": 35, "endOffset": 55}, {"referenceID": 14, "context": "Other unsupervised learning algorithms exist which do not rely on back-propagation, such as the various Boltzmann machine learning algorithms (Hinton & Sejnowski, 1986; Smolensky, 1986; Hinton et al., 2006; Salakhutdinov & Hinton, 2009).", "startOffset": 142, "endOffset": 236}, {"referenceID": 18, "context": "6) is to show that one can estimate these gradients via an approximation that only involves ordinary neural computation and no explicit derivatives, following previous work on target propagation (Bengio, 2014; Lee et al., 2014).", "startOffset": 195, "endOffset": 227}, {"referenceID": 8, "context": "Spike-Timing-Dependent Plasticity or STDP is believed to be the main form of synaptic change in neurons (Markram & Sakmann, 1995; Gerstner et al., 1996) and it relates the expected change in synaptic weights to the timing difference between post-synaptic spikes and pre-synaptic spikes.", "startOffset": 104, "endOffset": 152}, {"referenceID": 8, "context": "Spike-Timing-Dependent Plasticity or STDP is believed to be the main form of synaptic change in neurons (Markram & Sakmann, 1995; Gerstner et al., 1996) and it relates the expected change in synaptic weights to the timing difference between post-synaptic spikes and pre-synaptic spikes. Although it is the result of experimental observations in biological neurons, its interpretation as part of a learning procedure that could explain learning in deep networks remains unclear. This paper aims at proposing such an interpretation, starting from the proposal made by Hinton (2007), but extending these ideas towards unsupervised generative modeling of the data.", "startOffset": 130, "endOffset": 580}, {"referenceID": 13, "context": "Variational bounds have been proposed to justify various learning algorithms for generative models (Hinton et al., 1995) (Sec.", "startOffset": 99, "endOffset": 120}, {"referenceID": 23, "context": ", via a Markov chain which may correspond to probabilistic inference with spiking neurons (Pecevski et al., 2011).", "startOffset": 90, "endOffset": 113}, {"referenceID": 5, "context": "Variational MCMC (de Freitas et al., 2001) can be used to approximate the posterior, e.g., as in the model from Salimans et al. (2014). However, a rejection step does not look very biologically plausible (both for the need of returning to a previous state and for the need to evaluate the joint likelihood, a global quantity).", "startOffset": 21, "endOffset": 135}, {"referenceID": 5, "context": "Variational MCMC (de Freitas et al., 2001) can be used to approximate the posterior, e.g., as in the model from Salimans et al. (2014). However, a rejection step does not look very biologically plausible (both for the need of returning to a previous state and for the need to evaluate the joint likelihood, a global quantity). On the other hand, a biased MCMC with no rejection step, such as the stochastic gradient Langevin MCMC of Welling & Teh (2011) can work very well in practice.", "startOffset": 21, "endOffset": 454}, {"referenceID": 6, "context": "There is strong biological evidence of a distinct pattern of connectivity between cortical areas that distinguishes between \u201cfeedforward\u201d and \u201cfeedback\u201d connections (Douglas et al., 1989) at the level of the microcircuit of cortex (i.", "startOffset": 165, "endOffset": 187}, {"referenceID": 2, "context": "See Theorem 1 from Bengio et al. (2013) for the generative interpretation of denoising auto-encoders: it basically states that one can sample from the model implicitly estimated by a denoising auto-encoder by simply alternating noise injection (corruption), encoding and decoding, these forming each step of a generative Markov chain.", "startOffset": 19, "endOffset": 40}, {"referenceID": 2, "context": "Iterating these steps generates x\u2019s according to the Markov chain whose stationary distribution is an estimator of the data generating distribution that produced the training x\u2019s (Bengio et al., 2013).", "startOffset": 179, "endOffset": 200}, {"referenceID": 18, "context": "Based on recent theoretical results on denoising autoencoders, we propose the following estimator (up to a scaling constant) of the required gradient, which is related to previous work on \u201ctarget propagation\u201d (Bengio, 2014; Lee et al., 2014) or targetprop for short.", "startOffset": 209, "endOffset": 241}, {"referenceID": 15, "context": "Another important inspiration is Predictive Sparse Decomposition (PSD) (Kavukcuoglu et al., 2008).", "startOffset": 71, "endOffset": 97}, {"referenceID": 13, "context": "that was started with the Wake-Sleep algorithm (Hinton et al., 1995) and which finds very interesting instantiations in the variational auto-encoder (Kingma & Welling, 2014; Kingma et al.", "startOffset": 47, "endOffset": 68}, {"referenceID": 16, "context": ", 1995) and which finds very interesting instantiations in the variational auto-encoder (Kingma & Welling, 2014; Kingma et al., 2014) and the reweighted wake-sleep algorithm (Bornschein & Bengio, 2014).", "startOffset": 88, "endOffset": 133}, {"referenceID": 13, "context": "that was started with the Wake-Sleep algorithm (Hinton et al., 1995) and which finds very interesting instantiations in the variational auto-encoder (Kingma & Welling, 2014; Kingma et al., 2014) and the reweighted wake-sleep algorithm (Bornschein & Bengio, 2014). Two important differences with the approach proposed here is that here we avoid back-propagation thanks to an inference step that approximates the posterior. In this spirit, see the recent work introducing MCMC inference for the variational auto-encoder Salimans et al. (2014).", "startOffset": 48, "endOffset": 541}, {"referenceID": 20, "context": "Addressing the weight transport problem (the weight symmetry constraint) was also done for the supervised case using feedback alignment (Lillicrap et al., 2014): even if the feedback weights do not exactly match the feedforward weights, the latter learn to align to the former and \u201cback-propagation\u201d (with the wrong feedback weights) still works.", "startOffset": 136, "endOffset": 160}, {"referenceID": 18, "context": "The proposal made here also owes a lot to the idea of target propagation introduced in Bengio (2014); Lee et al. (2014), to which it adds the idea that in order to find a target that is consistent with both the input and the final output target, it makes sense to perform iterative inference, reconciling the bottom-up and top-down pressures.", "startOffset": 102, "endOffset": 120}, {"referenceID": 9, "context": "den layers, h1 with 1000 softplus units and h2 with 100 sigmoid units (which can be considered biologically plausible (Glorot et al., 2011)).", "startOffset": 118, "endOffset": 139}, {"referenceID": 25, "context": "2 for the Parzen density estimator, chosen with the validation set), which is about the same or better as was obtained for contractive autoencoders (Rifai et al., 2011) (LL=121), deeper generative stochastic networks (Bengio et al.", "startOffset": 148, "endOffset": 168}, {"referenceID": 3, "context": ", 2011) (LL=121), deeper generative stochastic networks (Bengio et al., 2014) (LL=214) and generative adversarial networks (Goodfellow et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 11, "context": ", 2014) (LL=214) and generative adversarial networks (Goodfellow et al., 2014) (LL=225).", "startOffset": 53, "endOffset": 78}, {"referenceID": 18, "context": ", with stochastic binary units (Lee et al., 2014).", "startOffset": 31, "endOffset": 49}, {"referenceID": 18, "context": "As argued by Bengio (2014); Lee et al. (2014), departing from backpropagation could be useful not just for biological plausibility but from a machine learning point of view as well: by working on the \u201ctargets\u201d for the intermediate layers, we may avoid the kind of reliance on smoothness and derivatives that characterizes back-propagation, as these techniques can in principle work even with highly non-linear transformations for which gradients are often near 0, e.", "startOffset": 28, "endOffset": 46}], "year": 2015, "abstractText": "Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-TimingDependent Plasticity) can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.", "creator": "LaTeX with hyperref package"}}}