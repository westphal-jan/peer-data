{"id": "1502.05113", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2015", "title": "Temporal Embedding in Convolutional Neural Networks for Robust Learning of Abstract Snippets", "abstract": "the prediction assumes temporal error - series currently challenging due to various types of data bugs and weaknesses. originally, planners propose a novel algorithm exploring directional drift - enhanced convolutional delay propagation ( arc ) where learn repeatedly - matched - yet - hidden structural elements in periodical time - graphs, nicknamed radial snippets, for predicting future consequences. our model comprises convolutional spatial networks and such a block - path with its potential neighbors in the desired domain potentially aligning it they support dominant patterns governing independent dataset. the framework is robust using distortions such turbulence in the specified variable and demonstrates potential correlation power for circular process - series.", "histories": [["v1", "Wed, 18 Feb 2015 04:25:23 GMT  (395kb,D)", "http://arxiv.org/abs/1502.05113v1", "a submission to kdd 15'"]], "COMMENTS": "a submission to kdd 15'", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["jiajun liu", "kun zhao", "brano kusy", "ji-rong wen", "raja jurdak"], "accepted": false, "id": "1502.05113"}, "pdf": {"name": "1502.05113.pdf", "metadata": {"source": "CRF", "title": "Temporal Embedding in Convolutional Neural Networks for Robust Learning of Abstract Snippets", "authors": ["\u2020Jiajun Liu", "\u2020Kun Zhao", "\u2020Brano Kusy", "\u2217Ji-rong Wen"], "emails": ["raja.jurdak}@csiro.au", "jrwen@ruc.edu.cn"], "sections": [{"heading": null, "text": "We conduct extensive experiments and discover that the proposed model shows significant and consistent advantages over existing methods on a variety of data modalities ranging from human mobility to household power consumption records. Empirical results indicate that the model is robust to various factors such as number of samples, variance of data, numerical ranges of data etc. The experiments also verify that the intuition behind the model can be generalized to multiple data types and applications and promises significant improvement in prediction performances across the datasets studied."}, {"heading": "1. INTRODUCTION", "text": "The behaviors of many of the world\u2019s inhabitants are fundamentally bound by the cycle of the sun and the moon which creates day and night. It is the reason why across the days of an average person, there often exist periodical patterns for their mobility or more generally, their behavior [26, 27]. Utilizing such re-occurring patterns could drastically benefit various modern ubiquitous applications. For example, the ability to predict a day\u2019s power consumption of many individual houses at midday will be profoundly beneficial for the smart grid to manage dynamically its power\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nsupply resources. While in the scenario of smart location tracking [14, 34], with a replenish-able energy budget the system either aims to minimize the energy efficiency of location tracking, or attempts to maximize the tracking accuracy given a fixed energy budget. A crucial challenge involved in such a smart tracking system is to estimate at any time of day how much further the moving entities will move for the remainder of the day. Ideally, with a greater estimated value of the total travel distance, the system will employ a more conservative sampling strategy (lower sampling frequencies) to cover as much as possible of the whole trip using the restricted energy budget, whereas a more aggressive strategy (higher sampling frequencies) will be favored on the presence of a smaller estimated total travel distance, so that better tracking precision will be achieved. Clearly the estimation of the entity\u2019s daily travel distance using partial information is a challenging yet crucial ingredient for the system\u2019s success.\nApproaches have been proposed to predict generic timeseries and many of them have capitalized on the phenomenon that for each individual there often exist re-occurring small fragments of time (which we call \u201csnippets\u201d) in their histories. By detecting and reusing such snippets, we are able to reconstruct a day with the elements from previous relevant days. We show an example of snippet learning for daily traveling time prediction and the difficulties it faces by using a commuter\u2019s daily routines. It is worth noting that throughout the entire paper, we assume that besides the time-series itself, no other support information such as locations are available to the prediction algorithm. For example, to predict a day\u2019s travel distance, the algorithm\u2019s only input is a partial time-series of the distances traveled in each interval. With a 30-minutes interval, the whole day will have 48 time-series entries, and we aim to use the first half of them to predict the accumulated travel distance for the whole day.\nImagine that a person in our example has two usual routines: 1) on workdays the person goes to work by a particular bus line that stops outside the apartment every 8 a.m., and arrives at the workplace around 9 a.m. The person gets lunch around 12 p.m. at someplace near the workplace everyday, and finishes work around 5 p.m., 2) on weekends the person prefers going to the beach in the morning and coming home in the evening. In the ideal case, the person begins and finishes the same activity at the exact same time on every workday, and the resulting time-series for travel distances would be identical across days. With snippets, a time-series for a workday would then be transformed into a series of snippets like <\u2206dwalking to bus stop A,\nar X\niv :1\n50 2.\n05 11\n3v 1\n[ cs\n.L G\n] 1\n8 Fe\nb 20\n\u2206d10km ride on bus line 1, \u2206dwalking into office, \u2206dworking, ...>. Now to predict how much further the moving object will move for the remainder of the day at a certain time on the day (e.g. midday), we are left with a simple task. For every interval of the snippet sequence in the example, if the current day shows an identical partial time-series for that interval, the person is likely to be working that day and is likely to yield the same total travel distance as any other workday. The same method works for the weekends too.\nIn reality, such patterns do repeat themselves, only not in such a perfectly aligned way but instead often on a shifted timeline and at a differing pace. Instead of having high coherences at all times between two working days of a person, in reality a day\u2019s time-series may often be partially similar to and partially divergent from another day\u2019s, posing a serious challenge for the aforementioned prediction method. There are many possible causes which prevent a perfect resembler for a snippet sequence from happening. For example, the bus in the morning may be 20 minutes late, or the person may wait for a coffee to miss the bus he/she is supposed to take. Then, the person may have a later than usual lunch at work. Finally, the person on one day decides to do usual item A/B in the order of B/A. Coupled with the huge number of non-work-related locations a person could go to and the numerous possible sequences of visiting them, the resulting time-series could have a huge variety of distortions to the regular time-series. In such cases, how to effectively learn representative snippets and how to use them effectively remains a major challenge.\nTo solve this complex problem, we adopt the concept of snippets but take a step forward and propose a robust learning and time-series prediction model to systematically reduce the effect of such distortions. Specifically, we make the following contributions in this paper:\n\u2022 We propose a novel regression model, which is based on convolutional neural networks, to solve the robust snippets learning and periodical time-series prediction problem.\n\u2022 We propose a novel technique called temporal embedding to improve the classical convolutional neural networks\u2019 capability for learning robust snippets and for predicting accurately. We design a network layer based on this concept, devise a complete four layer network (TeNet) for regression, and solve the corresponding backpropagation problem. We also offer a detailed case study to illustrate the effect of temporal embedding.\n\u2022 We conduct extensive experiments on 15 individual datasets representing three data modalities and one synthetic dataset to evaluate the advantages and characteristics of the proposed model.\nThe rest of the paper is organized as follows. Next in Section 2 we present the background and relevant literature of the problem studied. In Section 3 we give the intuition behind TeNet, describe in detail the technique of temporal embedding and other layers of TeNet, and offer solutions to the backpropagation of TeNet. We then enter Section 4 and evaluate the proposed model. Finally we conclude our work in Section 5.\n2. BACKGROUND AND RELATED WORK\nLearning abstract features (with neural networks in many cases) has been extensively studied in recent years and has proved effective in many applications. For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22]. These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].\nThe main advantage of such methods is that they have a strong capability of unravelling the hidden hierarchical structure of data to derive representative features. Moving from a shallower architecture to a deeper architecture, these models progressively detect essential components of the data from local parts like strokes in human handwriting, to global compositions such as digits or objects. Among the variations of neural networks, inspired by biological processes [20], convolutional networks in particular excel in finding such abstract features that are robust to geometric variations in images [18]. Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition [1, 8, 12] and natural language processing [6, 7].\nNow we consider the periodical time-series prediction problem for data such as daily traveling distances or daily household power consumptions. To tackle this problem, conventionally statistical models such as autoregression and its variants are strongly favored. While in the past decade, realizing there is abstract and structural information beneath the raw numeric values in the time-series, researchers have experimented to discover such patterns by clustering or \u201cmotif\u201d discovery [23, 26, 27]. Though conceptually similar, these \u201cmotifs\u201d usually are concrete subsequences that are restricted by specific mathematical definitions, which differentiate themselves from the concept of abstract, representative snippets in our paper. However, how to design a method that can find abstract patterns as well as predict future values, that meanwhile is robust to various temporal distortions and misalignment, is yet to be answered. Inspired by the success of convolutional neural networks, we investigate using convolution-based neural networks to address this problem."}, {"heading": "3. THE MODEL", "text": ""}, {"heading": "3.1 Intuition", "text": "The two main challenges for the periodical time-series prediction are: 1) how to find representative snippets for the prediction of future changes; and 2) how to minimize the effect of distortions in the temporal domain and get accurate regression results. Here we examine the two challenges separately and propose solutions to them from a neural networks perspective.\nThe first challenge, i.e. snippet learning, involves finding abstract sequences in the training time-series. Naturally there is an assumption that the snippets should only be of moderate length. For example, if we were to predict daily\noutput size 1x6 2x1x2 1x3 1x1\noutputt x t\ninput convolution\nmax-pooling tanh\nsigmoid\nl1-regularized least-squares\ntemporal embedding\nW(1) (3x6), b(1)(6) W (2) (2x3)\nb(2)(2)\nW(3) (4x3), b(3)(3)\ninput size 1x6 1x6 2x1x2 1x3\nW(4) (3x1), b(4)(1)\nFigure 1: An instance of TeNet for 6-d input. It is composed of a sparsely connected temporal embedding layer, a convolution/pooling layer with two filters of size 1\u00d7 3 and pool size 1\u00d7 2 (following the conventions in constructing convolutional neural networks, the convolution layer and max-pooling layer are illustrated as a single layer), a fully-connected sigmoid layer that transforms the feature map from size 2\u00d7 1\u00d7 2 into 1\u00d7 3, and finally an l1-regularized least-squares regression layer that yields the predicted value. W (l), b(l) are the weights and bias of the connections between layers l and l+1. Connections with the same colors in the convolution layer indicate that those connections share the same weights, and the two shaded areas represent the two feature maps from the filters. The dimensionalities of the weights, the input and the output for each layer are provided at the bottom. Biases are not illustrated in this figure.\nhuman mobility, a time window of from one half-hour to a few hours would be a reasonable setting, as intuitively such a period of time should be enough to cover most of the common trips in daily life. Hence in the prediction model, we examine such periods of time using a convolutional approach. We create randomly initialized filters that have a given, moderate length as the length of the target snippets. In 2D image classification tasks, filters in convolutional neural networks are often used as edge detectors, while in ours, the filters will act as \u201csnippet detectors\u201d. In the training phase, the weights for the filters will be adjusted during the backpropagation so that they respond maximally to the reoccurring and significant components in the training data.\nWe then solve the second challenge by adding a \u201ctemporal embedding\u201d operation in the neural network. The temporal embedding process provides a supervised way of denoising subspace learning. When dealing with time-series, a na\u0308\u0131ve technique is to \u201cshift\u201d the training data forward and backward along the timeline. For example, a shifting routine with windows size 1 would transform a training sample x =< x1, x2, ..., xd >\u2192 y into three training samples x =< 0, x1, x2, ..., xd\u22121 >\u2192 y, x =< x2, x3, ...xd, 0 >\u2192 y, x =< x1, x2, ..., xd >\u2192 y. Though useful sometimes, this na\u0308\u0131ve approach introduces heavy noise by including artificial training samples that may never actually happen in the real world. Also it is unable to benefit case where the order of the subsequence is changed. We argue that the na\u0308\u0131ve technique can evolve to a much more effective approach called temporal embedding that integrates into the learning process mechanisms for removing distortions. With temporal embedding, two temporally-shifted copies are created for each sample during the learning process, and then the original sample and the two shifted copies are encoded into a single sample so that the processed sample will not only carry its own information, but also bear a piece of information\nfor each of its shifted neighbors. Again, the weights for the encoding are learned in a supervised way during backpropagation.\nNext we present an overview of the TeNet model."}, {"heading": "3.2 Model Overview", "text": "We propose a convolutional neural network to learn the snippets from the periodic time-series as illustrated in Figure 1. The model has three invisible layers, namely the temporal embedding layer, the convolution/max-pooling layer, and the sigmoid layer. The output layer is an l1-regularized least squares regression layer. The illustrated model is an example instantiation of the proposed model, with the input size, embedding window size, number of snippets, snippet size, max-pooling and sigmoid layer sizes to be 6, 1, 2, (1,3) and (1,2) and 3 respectively. The model implements the following work flow:\n1. It takes an input sample, and applies the temporal embedding. This layer transforms the sample into a denser representation with not only the sample itself but also information of its potential temporal neighbors. The weights of the transformation are iteratively updated during the training process.\n2. The embedded input is sent into a convolution layer where a set of filters, or snippet detectors, scan through the sample using the convolution operator. Each snippet will be convolved against the sample, resulting in a feature map considered as the snippet\u2019s response to that sample.\n3. The snippets\u2019 responses to the sample, being supposedly sparse and representative, are input into a sigmoid layer to combine some of the responses into higher-level and more abstract representations in lower dimensions.\nThis transformation also involves a set of weights that is learned over the training process.\n4. Finally the abstract representation of the sample is used to perform an l1-regularized least-squares regression to obtain the predicted value. The intuition behind the l1 regularization is that if we consider the previous layer\u2019s output, ie. the high-level neuron\u2019s responses to the sample, as high-level pattern recognizers responses to the signal, a sparse solution will utilize the most significant responses and hence will be less sensitive to noise [21, 25].\nIn the following subsections we discuss the layers separately in detail. In the rest of the paper, the technical details of the neural network will be described mostly in vector forms, and we will use the assumptions and notations listed in Table 1."}, {"heading": "3.3 Temporal Embedding", "text": "The temporal embedding layer aims to align less dominant samples to the dominant patterns by reducing the temporal distortions and misalignment (e.g. shifting or skewed sequence of events), corresponding to two cases in our previous example: 1) the commuter starts the day 30 minutes earlier than usual, so every event in the morning rush hour is shifted ahead equally by 30 minutes , 2) for some reason the commuter does not take the usual bus line which directly stops at his workplace, instead he/she takes a train and walks 1km to work from the station. In the resulting time-series we will see two distinct effects as a result of 1) and 2). For example, assume that on normal day the travel distance time-series segment in the morning will be v =< 0, 1, 2, 4, 1, 0 >, then for case 1 we will have u =< 1, 2, 4, 1, 0, 0 >, and in case 2 it will be u =< 0, 1, 4, 2, 1, 0 >. Now we assume both cases happen on the same day, giving us u =< 1, 4, 2, 1, 0 >, which is heavily distorted from v. It is a significant challenge for a prediction algorithm to realize that for the two days the travel distances should be very similar despite the sequences and the values of their time-series are so different.\nTemporal embedding addresses this issue, by optimally embedding a value\u2019s temporal neighbors into itself, so that for the whole dataset the dominant pattern remains unchanged but the distorted patterns are realigned. The layer is configured by one hyperparameter dte that controls how many neighbors of an element in each direction should be\nembedded to the element itself (the embedding size). This layer has 2\u00d7dte+1 sets of parameters, represented by matrices W\n(1) lj ,W (1) m and W (1) rj \u2208 Rd (1)\u00d7d(1) , and the same number\nof constant sparse matrices W\u0303lj , W\u0303m and W\u0303rj \u2208 R d(1)\u00d7d(1) . The subscriptions l and r represent the direction of the neighbors on the timeline, and j here means the weights for the jth neighbor in the final embedding. In the case of dte = 1, there are three W matrices and three W\u0303 matrices in this layer. The six matrices together implement the embedding operators. Here we use the input dimensions in Figure 1 (where d(1) = 6) as an example for how this layer works.\nThe constant matrices, are defined as:\nW\u0303 (1) l1 =  0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0  (1)\nW\u0303 (1)r1 = W\u0303 (1)T l1 , W\u0303 (1)m = I = eye(d (1)) (2)\nWeights in W (1) l1 ,W (1) m W (1) r1 that correspond to the 1s in W\u0303l1 , W\u0303m and W\u0303r1 represent the weights for the embedding of the sample\u2019s left neighbor (forward), the sample itself and its right neighbor (backward) respectively, and they are initialized with corresponding constant matrices respectively. The layer\u2019s output is subsequently defined as follows:\nz(1) = a(1) \u00b7 (W (1)l1 W\u0303l1 +W (1) r1 W\u0303r1 +W (1) m W\u0303m) + b(l)\n(3)\ng(1)(z(1)) = z(1) (4)\nW\u0303 (1) enforces a constraint that the connections between this layer and its input are restricted, and only the weights at the desired neighboring positions for each element are used in the final embedding for that element. The layer yields the temporal embedded output g(1)(z(1)) \u2208 Rd (1), or 6 in this example, as the output of the layer. One can also use the sigmoid function as the activation function in the temporal embedding layer, though our experiments show that the difference it makes on the prediction accuracy is insignificant (most of the times adding the sigmoid activation will slightly decrease the prediction accuracy).\nThe layer\u2019s output is a vector of the same size as the input, however the embedded sample is now significantly more robust to temporal distortions. With temporal embedding, the model detects dominant patterns in the training time-series, and tries to correct the systematical distortions within the specified time window. Using the commuter example, the model will find that the person\u2019s regular time for the bus to work, and will try to realign the systematical misalignment on those unusual days. Some readers may argue that a simple moving average algorithm might be able to solve the distortion problem; however temporal embedding is far more effective, as the concrete example below shows.\nDiscussion and Case Study. Recall our example with v and u, where v represents the dominant pattern in the dataset, while u represents a day that in fact will yield a similar end-of-day result but shows very distorted patterns in its time-series. Now given the parameter matrices Wl,Wm,Wr and the constant matrices\nW\u0303l, W\u0303m, W\u0303r initialized as in Equation 2, our objective is to realign u with v by eliminating the distortion, and meanwhile keeping v as unchanged as possible, which is effectively equivalent to solving the following minimization problem in Equation 7:\nvt = v \u00b7 (Wl W\u0303l +Wm W\u0303m +Wr W\u0303r) + b (5) ut = u \u00b7 (Wl W\u0303l +Wm W\u0303m +Wr W\u0303r) + b (6) arg min\nWl,Wr ||vt \u2212 v||2 + ||ut \u2212 v||2 (7)\nwhere vt and ut are the embedded new time-series. By\nsolving the optimization, the non-zero weights in Wl W\u0303l, Wm W\u0303m and Wr W\u0303r are determined as < 0, 0.61, 0.24, 0.44, 1 >, < 0,\u22120.22, 0.4,\u22120.15 > and< 0.66, 0.24, 2.1, 1 > respectively. Now vt and ut can be calculated according to Equations 5 and 6, and we subsequently investigate how temporal embedding performs in terms of preserving v and realigning u to v, compared with the moving average approach, with vs and us being the output of v and u of a moving average of window size 3 (vi = < vi\u22121, vi, vi+1 >).\nTable 3 measures the relations between the vectors before and after the transformations with three metrics, namely squared error, intersection and Pearson\u2019s correlation. First we note that u is so distorted that the correlation between v and u is merely 0.11, which can be considered \u201cuncorrelated\u201d. Now we examine the differences between the effects of temporal embedding and moving average.\nIdeally, the transformation should show the following properties: 1) since v represents the reoccurring pattern in the training set, we want vt to be as unchanged as possible after the transformation 2) after the transformation, ut should be as similar to vt as possible, indicating that the misalignments in u has been minimized and u is realigned to the representative sample v. We verify the two aspects by examining the relations between v and vt, and that between ut and vt, and observe that temporal embedding has achieved both goals.\nFirst we observe that vt is identical to v (with 0 squared error), while ut has been transformed to a form that is perfectly identical to v and vt now, with the dominant values at the second and third positions swapped and realigned to the third and forth position to be more inline with v. However, we can see moving average resulted in a squared error of 2 between v and vs, showing that v has not been preserved successfully in the transformation. Second, though moving average does strengthen the relation between v and u by reducing the squared error (4.5 \u2192 3.1) and by increasing the similarity by intersection, it has even resulted in a drop in the correlation (0.11 \u2192 0.02 compared with the original v\nand u). We conclude its result is clearly less successful compared to temporal embedding (4.5 \u2192 0 in squared error, 4\u2192 8 in intersection, and 0.11\u2192 1 ).\nIt is worth noting that although the temporal embedding layer in the proposed neural network is not exactly the same as in Equation 7 as it does not have knowledge initially about which samples hold the representative patterns, as the training proceeds, the weights will progressively favor the reoccurring patterns, and eventually approach the solution of Equation 7. Next we describe the convolution, the max-pooling and the sigmoid layers."}, {"heading": "3.4 Convolution, Max-pooling and Sigmoid", "text": "The convolution/pooling layer performs a series of discrete 1-d convolutions W (2) i \u2217 a\n(2) with a specified number of filters nf of a specified length df . Each of the filters \u201csweeps\u201d through the entire input signal and takes the input signal segment at the corresponding position as input. With a filter kernel W (2) i =< W (2) idf ,W (2) idf\u22121, ...,W (2) i1 > (taking the convention of reversely-ordered weights for convolution kernels and outputs), the ith filter\u2019s output has the kth element:\n(W (2) i \u2217 a (2))[k] = df\u2211 p=1 W (2) ip a (2) k+p\u22121 (8)\nIn the example in Figure 1 we have set two filters with size 1x3, hence in the convolution layer, each neuron will only be connected to three neurons from the temporal embedding layer. Such sparse connectivity between the filters to their inputs enforces that the convolution layer will be focusing on finding the local snippets with moderate lengths.\nThough the convolution traverses the entire time-series in a sliding-window style and seemingly has a positive effect in reducing the temporal distortions, it is very different from temporal embedding. The main factor differentiating them is in the weight-sharing scheme (see Figure 1). A filter in the convolution layer has its weights shared among all its output neurons (meaning a filter is sliding through the data, trying to match the same particular pattern), while in temporal embedding each neuron has individualized weights to enable optimal local embedding for each position. Such flexibility enables it to identify and realign much more complex distortions and misalignments. For example, given v =< 0, 1, 2, 4, 1, 0 >, convolution will not be able to recognize the close relation between u =< 1, 4, 2, 1, 0, 0 > and v because of the heavy distortions in both the positions and the sequences. In the experiments we will also show that without the temporal embedding layer, convolutional neural network does not work well on such time-series.\nThe output of the convolution will be of the size nf \u00d7 d(\u2212d\nf+1). In Figure 1\u2019s example where nf = 1, d = 6, df = 3, we have the 8 neurons in the convolution layer. The output is then received by the max-pooling layer, where only the maximal value is kept from any pool of 1 \u00d7 2. The filter\u2019s output will hence be down sampled and transformed by an element-wise hyperbolic tangent function, reducing the output to 4-dimensional. Then as the last hidden layer, the sigmoid layer will perform a projection from the convolution/pooling\u2019s output to a further reduced dimension as a means of both learning non-linear features and dimension reduction. Finally, the input is transformed into a dense, robust and representative feature representation of 1\u00d73. Intuitively we can consider the sigmoid layer as a higher-level\nfeature learner, after the convolution layer has discovered those relatively more \u201clocal\u201d snippets.\n3.5 l1-regularized Least-squares The output layer of the proposed model is a l1-regularized least-squares regression layer, defined as:\ng(4)(a(4)) = a(4) \u00b7W (4)T + b(4) (9)\nwith the cost function in the from of:\nJ(W, b;x, y) = 1\n2 ||a(4) \u00b7W (4)T + b(4) \u2212 y||2 + \u03bb||W (4)||1 (10)\nwhere \u03bb is a hyperparameter for the weight of the regularization term.\nThe advantage of using the l1 regularizer over l2 is that the l1 regularizer forces the optimization to find a sparse solution that only uses the most distinctive high-level features to conjure the final prediction [21, 25]. With the l2 regularizer the weights tend to have smaller variance, often making the model spread the energy thinly across all features, hence making the model less distinctive and less accurate."}, {"heading": "3.6 Backpropagation", "text": "The parameters in the network are updated by stochastic gradient descent. In particular, W (4) can be learned by:\n\u2202J(W, b;x, y)\n\u2202W (4) = a(4)T (a(4) \u00b7W (4)T + b(4) \u2212 y) + \u03bb sign(W (4)T )\nWhere sign() is the sign of a vector. One can speed up this optimization process using the methods proposed in [28].\nTo update the parameters in the temporal embedding layer, taking W (1) l as an example, we apply the chain rule and arrive at:\n\u2202J(W, b;x, y)\n\u2202W (1) l1\n= \u2202J(W, b;x, y) \u2202g(1) \u2202g\n(1)\n\u2202z(1) \u2202z\n(1)\n\u2202W (1) l1\n= \u2202J(W, b;x, y) \u2202z(1) \u2202z\n(1)\n\u2202W (1) l\n= \u03b4 (1) l\n\u2202z(1)\n\u2202W (1) l\n(11)\nSince the element-wise product has the property:\na(1) \u00b7 (W (1)l W\u0303 (1) l ) = (a (1) W\u0303 (1)l ) \u00b7W (1) l (12)\nwe have the partial derivative of z(l) w.r.t. W (1) l as:\n\u2202z(1)\n\u2202W (1) l\n= a(1) W\u0303 (1)l (13)\nWe calculate the error propagates from layer 2 to layer 1 as:\n\u03b4(1) = \u2202J(W, b;x, y) \u2202z(2) \u2202z\n(2)\n\u2202g(1) \u2202g\n(1)\n\u2202z(1)\n= df\u2211 i=1 \u03b4 (2) i \u2202W (2) i \u2217 a (2) \u2202a(2) = df\u2211 i=1 \u03b4 (2) i \u2217 flip(W (2) i ) (14)\nwhere flip() returns the input vector in reversed order. With the convolution layer\u2019s back propagated error being \u03b4(2) (which can be calculated by the method described in [16]), \u2202J(W,b;x,y) \u2202W (1) can therefore be updated with the gradient:\n\u2202J(W, b;x, y)\n\u2202W (1) l\n= df\u2211 i=1 [ \u03b4 (2) i \u2217 flip(W (2) i ) ] (a(1) W\u0303 (1)l ) (15)\nW (1) l and W (1) m can be updated using similar procedures. Meanwhile, b(1) is updated with the gradient:\n\u2202J(W, b;x, y)\n\u2202b(1) = df\u2211 i=1 \u03b4 (2) i (16)\nNext we present the experimental results and offer indepth analysis and discussion."}, {"heading": "4. EXPERIMENTS", "text": "In the experiments, we conduct extensive tests on the proposed model, with 15 individual datasets and 4 competitive methods. The goals of the experimental studies are fourfold: 1) to evaluate the prediction performance of the proposed model, in terms of prediction accuracy, and compare it with the competitive models; 2) to evaluate the model\u2019s behavior and sensitivity to features of diverse datasets; 3) to investigate the isolated effects of temporal embedding; and 4) to visualize the snippets and show how they work with intermediate values from the learning process."}, {"heading": "4.1 Datasets", "text": "To support the comprehensive evaluation, we use a variety of univariate, periodical time-series datasets that represent three modalities, ranging from human mobility patterns to household power consumption. The reason we choose these modalities is that the behaviors they represent are expected to exhibit complex periodical patterns in daily cycles, which is an ideal testbed for the proposed model to demonstrate its capability of discovering and capturing such abstract features and to test its robustness to various factors.\nThe first modality is Human Mobility - daily traveling Distance (HMD) in kilometers, and the second is Human Mobility - daily traveling Time (HMT) in minutes. Both modalities are extracted from the LifeMap [4]) that contains human mobility traces collected from eight individuals, spanning from a few months to around two years. In total there are 52,819 position fixations, most of which are from regular sampling every two to five minutes. HMD is the total displacement for an individual in a day, and HMT is accumulated from short-term movements calculated as follows: for each five minute interval, if the individual\u2019s displacement is higher than 500 meters 1, then the five-minute period is counted as \u201ctraveling\u201d and is accumulated to the daily total traveling time.\nThe third modality is daily Household Power Consumption (HPC). Two datasets are used for this modality, i.e. household power consumption datasets from France2 and Australia3 (HPC-FR, HPC-AU). HPC-FR consists of 2,075,259 active power consumption in watt sampled every minute for 48 months from a single household. HPC-AU consists of 618, 189 household power meter readings in kwatt hour sampled every 30 minutes from 31 households for up to 29 months.\nTo prepare the data, we developed a program to extract only the samples that have complete (or nearly complete) day cycles, meaning that every data sample used must have regular readings in each period of time in a complete day. To obtain meaningful results, only individuals with more than 150 days of records are used in the experiments.\n1median errors of localization with assisted GPS, WiFi positioning and cellular network positioning are reported to be 8, 74 and 600 m [33] 2https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption 3http://data.gov.au/dataset/sample-household-electricity-time-of-use-data\nFor the human mobility datasets, we use the two individuals\u2019 datasets with the highest quality of data in terms of timespan (>150 days) and sampling frequency. We extract the traveling distances and traveling times for each interval (e.g. a 30 minutes interval creates 48-d time-series for a day), and use the resulting time-series for the experiments. Similar preprocessing is applied on the power consumption datasets. After preprocessing, each time-series sample has d elements as x =< x1, ..., xd >, each xi is the occurred value in the corresponding time interval (non-cumulative).\nFor each individual dataset, we randomly divide the samples equally into three folds: the training set, the validation set and the test set. The model is trained using the training set, and is then tested on the validation set. Such crossvalidation is performed on the same individual dataset for five times with random splits, and the reported performance is the averaged value cross the five iterations. The settings of hyperparameters with the best validation performance are kept as the hyperparamters of the model. Finally we test the model on the test set and report the performance."}, {"heading": "4.2 Evaluation Settings", "text": "For evaluation we consider the periodical accumulation prediction problem, where each input x\u2032 \u2208 Rd \u2032 (d\u2032 < d) is a head segment of a complete x and corresponds to a target value y = \u2211d i=0 xi representing the periodical accumulation. Clearly the model can be used to perform other types of prediction such as time-series forecast or k-ahead prediction. Due to space limit here we use periodical accumulation prediction as a showcase for TeNet\u2019s performance advantages.\nTeNet is implemented using Python with the Theano framework4. For comparison, we consider four competitive methods, namely Support Vector regression with Linear kernel (SVLN), Support Vector regression with Radial Basis kernel (SVSIG), Support Vector regression with Polynomial kernel (SVPOLY), and Multiple Kernel Regression (MKR) [24].\nThe parameter selection criterion for the SV-family is that we carefully tune the parameters (error margin), d (degree of kernel function), and \u03b3 (kernel coefficient) for kernels. Each parameter\u2019s value is selected from the sets \u2208 {10\u22125, 10\u22124, ...1, ..., 104, 105}, d \u2208 {1, 2, 3}, \u03b3 \u2208 {10\u22125, 10\u22124,\n4http://deeplearning.net/software/theano/\n...1, ..., 104, 105} respectively, so in total there are 363 combinations for each model. For each test run, during training we iterate through every combination of ,d and \u03b3\u2019s candidate values, and keep the values that generate the highest accuracy on the validation set, then use these parameters on the test set and report the results. For comparable evaluation against MKR, we use an offline implementation where test samples are not used to update the parameters, and the number of support vectors is set to 120 for matching the parameter size of TeNet. The hyperparameter selection of TeNet follows the same procedure. We provide more details in Section 4.6.2.\nFor most of the experiments d is set to 28, meaning for each day, the time-series up to 2pm is known to the model. Selecting this particular number is because considering humans rarely remain active from 12am to 4am and the values in that period are almost all zeros, the first 28d represent information from exactly half of the active period from 4am to 12am of the next day. Such setting is challenging in the sense that the gap between 2pm to 12am next day is substantial and it leaves numerous possible outcomes for the daily accumulation. The complexity involved hence provides insight about how well the proposed and the competitive models can capture an individual\u2019s daily patterns and make prediction from limited information.\nNext we present the experimental results for the proposed method and the competitive methods, and also offer indepth discussion about hyperparameter tuning and about the effect of temporal embedding."}, {"heading": "4.3 Prediction for Periodical Accumulation", "text": "Table 3 studies the prediction performances of the proposed method and four competitive methods on 15 individual datasets of three different modalities, evaluated by average HitRate(HR)@20% and 30%, Mean Squared Error (MSE) and Mean absolute Relative Error (MRE). Using four metrics is due to that for datasets with long-tailed values (which human behaviors can often be characterized to be [11]), as an absolute measurement, MSE alone is not an ideal metric to evaluate a regression method\u2019s performance because it is heavily biased by samples in the long tail [31, 32]. Therefore we mainly use relative measures for the evaluation while keeping MSE as a reference.\nThe highlighted numbers in red, black, magenta and blue indicate the winning performance on that dataset under the corresponding metric ( magenta\u2192HR@20%, blue\u2192HR@30%, red\u2192RE, black\u2192MSE). Multiple highlighted numbers with the same color in a row indicate multiple winners under that metric on that dataset. We also report some of the properties, i.e. the total number of samples n, the numeric range [~], and standard deviation \u03c3, for each individual dataset. A closer look at these dataset statistics suggests large varieties in terms of number of samples (from 156 to 874), numerical ranges (0.2 to 345) and variances ( \u03c3 from 2.8 to 47). To present the reader with more intuitive and meaningful results, the numbers shown are unnormalized.\nGenerally, the distribution of the highlighted and winning performances shows that TeNet achieved best results in most of the cases, with a few but non-systematical exceptions spread across the competitive methods. Out of the 15 individual datasets, TeNet has won 14 entries in HR@20%, 15 entries in HR@30%, 13 entries in MRE, and 7 entries in MSE, showing a superior performance among the evaluated models. SVLN and SVSIG show least competitive results by having 1, 0, 2, 1 and 1, 0, 1, 0 winning performances respectively. SVPOLY obtains slightly better results with 3, 2, 3, 0 wins. MKR on the other hand, has shown comparable results in MSE but far less competitive results in other metrics, by having 0, 0, 1, 8 wins. In addition, we find that MKR is less robust to larger numerical ranges such as in HMD-8, HMD-12, HMT-8, and HMT-12, while TeNet demonstrates consistent performances cross all datasets.\nTo compare the methods quantatitively, we plot Figure 2 and show each method\u2019s mean average scores cross all individual datasets (MSE is normalized with the maximum MSE among the methods in each entry). On the 15 individual dataset, TeNet achieved best average performance under all four metrics. Taking a TeNet vs. all approach, we find TeNet\u2019s performance and the average of other methods\u2019\nperformance under HR@20%, HR@30%, MRE and MSE are 69 vs. 60, 84 vs. 78, 0.22 vs. 0.27 and 34 vs. 51 respectively, showing that TeNet makes a relative improvement of 15%, 8%, 19% and 33% respectively under the corresponding metric. Then if we investigate TeNet vs. the best among the rest, with HR@20% 69 and HR@30% 84, TeNet beats the second best HR@20% 61 (SVLN, SVPOLY) by 8, the second best HR@30% 78 (SVLN, SVPOLY) by 6; on MRE and MSE, TeNet\u2019s average errors are 0.22 and 34, while the second bests are 0.24 and 40 (MKR). Hence for all 15 individual dataset, in average TeNet marks an 13% increase in HR@20%, an 8% increase in HR@30%, a 9.1% decrease in MRE and a 15% decrease in MRE to the second best method under each corresponding metric. We also observe that though in all 15 individual datasets TeNet obtained the best performance under HR@30%, the average winning margin is the smallest than those under other metrics. This is because HR@30% is a relative looser measurement than other metrics, which leads to the result that less accurate prediction tends to have similar performances. However, the consistent advantage of TeNet in not only HR@30% but all four metrics still suggests that it has the best prediction accuracy. We hence conclude that TeNet has shown consistent advantages which are robust to variations in the data modality as well as the statistics characteristics of the data.\nWe further examine TeNet\u2019s ability to scale up its learning effectiveness with a growing sample size or an increasing complexity of the data. Taking MRE for example, we measure two correlations using Pearson\u2019s correlation coefficient: 1) the correlation between the averaged performance advantage ( 1 n \u2211n i min({re SV \u2217,MKR i }) \u2212 re TeNet i ) and the sample size, 2) the correlation between the averaged performance advantage and the entropy, for each individual dataset. The measurements yield correlation coefficients 0.7 and 0.79 respectively, suggesting a strong correlation between each set of the variables. Such patterns mean that as the sample size or the complexity of the data grows, TeNet is able to learn more effective than other methods to achieve better performance. The correlations are also visually identifiable as we plot the the performance advantage ratios in Figure 3."}, {"heading": "4.4 The Effect of d", "text": "Figure 4 illustrates the effect of the feature dimensionality d on the prediction accuracy. Here we use HPC-AU-8 as a case study. Figure 4(a) shows the changes of MRE and normalized MSE to a growing d. Unsurprisingly, both errors decrease monotonically as d increases, from 1, 0.35 at d = 8 to 0.08, 0.07 at d = 44. Figure 4(b) depicts how the HR responds to a growing d. Again, we see monotonic growths (almost, except for d = 16) in HR@20% and HR@30%. These results confirm that TeNet can effectively use the additional\ninformation and in the mean time has received little impact from the noise in the additional dimensions."}, {"heading": "4.5 The Effect of Temporal Embedding", "text": "In Section 3.3 we discussed how hypothetically temporal embedding would boost the performance of the model by automatically realigning the distorted time-series to the dominating patterns in a dataset, and verified it with a case study on a synthetic example. To further validate this hypothesis on real data, we create a designated dataset from HPC-AU-8 by performing the following procedure:\n1. We run a clustering with the affinity propagation method in [10], and find the top 10 exemplars.\n2. We take the exemplars and generate 300 synthetic samples (30 for each exemplar) by distorting the exemplars with randomly selected operations such as swapping two neighboring segments or shifting the data forward and backward. They are equally split into training, validation and test set.\n3. We train a model with a modified classical convolutional neural network fore regression (CNN, input \u2192 convolution/pooling\u2192 sigmoid\u2192 l1-linear regression) without temporal embedding, and a model with TeNet, and examine the performance differences.\nThe results are reported in Table 4. We observe that with the temporal embedding layer, the prediction accuracy has been improved by more than a half (15.5 to 6.4, 0.34 to 0.12) for MSE and MRE, and for about 100%/40% in HitRate@20% and 30%. This shows that temporal embedding is able to learn the weights which are conceptually equivalent to a reverse operation for the distortions and misalignments."}, {"heading": "4.6 Discussion", "text": "We present a visualization of the random snippets and learned snippets for the first cross-validation iteration on HPC-AU-8 in Figure 5. Each cell is a snippet, a segment of time-series the model deems representative. The figures show some noteworthy properties. Firstly the random snippets are fairly dense, while the learned ones are much more sparse, meaning that in most of cases there are only a smaller number of spikes and valleys in each learned snippets. Secondly, the sparsity of the learned snippets is also accompanied by a visually identifiable high distinctiveness across the\nlearned snippets, which means snippets learned tend to be different from one another because they effectively capture different patterns in the training data. Both properties suggest that the snippets are truly learning from the patterns in the dataset and both properties have a positive effect on the model\u2019s prediction accuracy.\n4.6.2 Selection of Hyperparameters As an issue often posed to complex learning models in-\ncluding neural networks, how to select the hyperparameters is an open question studied by many [15]. There are six hyperparameters in the proposed model:\nIn this paper, since the sizes of the datasets are moderate, we use an intuitive approach to find the hyperparameters for the testing. The selection and testing processes follows that described in the third paragraph of Section 4.2. One can also use the greedy hyperparameter selection processed described in [15]. We also used two optional data preprocessing, i.e. high pass filtering to denoise, and data shifting to synthesize more training data. The activation of each technique is subject to a control parameter which is tuned using the same process.\nNote that since all the hidden nodes in layers 2, 3 output small values only, with the settings we used for experiments, the regression layer\u2019s ability to predict larger numbers (e.g. >1000) is limited. To predict larger numbers, one can consider either rescaling the data or setting smaller \u03bb to adjust to the numerical range of the specific dataset.\n4.6.3 Network Depth and Number of Parameters The proposed model has a moderate number of layers\n(four if we count the convolution/pooling as one), and hence a moderate number of parameters to estimate. For example, with d = 28, dte = 1 (one Wl and one Wr), n f and df set to 20 and 5, n(3) = 12, we have:\u2211 {|W \u2217|, |b\u2217|} = (3\u00d7 28 + 28) + (20\u00d7 5 + 20)\n+(240\u00d7 12 + 12) + (12 + 1) = 3197 (17)\nIt is possible to add more layers to construct a deeper architecture based on temporal embedding and convolution. However, the data itself must be complex enough to provide more potential for the model to exploit. Given the granularity of daily human behaviors, for the task of predicting modalities such as traveling distance/time and power consumption, a deeper architecture has only limited effect."}, {"heading": "5. CONCLUSION", "text": "Motivated by the observation that regularities in periodical time-series sometimes manifest at different moments and at varied paces, in this paper we propose a technique\ncalled temporal embedding and devise a convolutional neural network-based learning model called TeNet, which is robust to temporal distortions and misalignments, to learn abstract features. First we present TeNet and discuss the intuition behind it using a case study, and then describe the technical details for the whole network architecture, and solve the backpropagation problem for the proposed model. In the experiments we use an extensive range of real-life periodical data that covers three modalities to compare the performances of the proposed model against competitive methods. We find that in average TeNet achieves 8% to 33% advantage against other methods in difference metrics and the advantage scales up with a growing sample size used in training. We also find that the accuracy of TeNet increases almost monotonically with a growing d, indicating the model is effective in utilizing more information and while remaining robust to noise. We also create a set of synthetic data from the real-life data to demonstrate the effect of temporal embedding and successfully show its capability of realigning distorted and misaligned data. At the end of the experiment we also offer an in-depth discussion about hyperparameter selection, data preprocessing, network depth and number of parameters, and present a visualization of the learned snippets. Beyond the periodical accumulation prediction problem, we expect Tenet to be useful for general time-series predictions ranging from forecasts to k-ahead prediction."}, {"heading": "6. REFERENCES", "text": "[1] O. Abdel-Hamid, A. Mohamed, H. Jiang, L. Deng,\nG. Penn, and D. Yu. Convolutional neural networks for speech recognition. IEEE/ACM Transactions on Audio, Speech & Language Processing, 22(10):1533\u20131545, 2014.\n[2] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1\u2013127, 2009. [3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. NIPS, 19:153, 2007. [4] Y. Chon, E. Talipov, H. Shin, and H. Cha. CRAWDAD data set yonsei/lifemap (v. 2012-01-03). Downloaded from http://crawdad.org/yonsei/lifemap/, Jan. 2012. [5] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. Flexible, high performance convolutional neural networks for image classification. In IJCAI, pages 1237\u20131242, 2011. [6] R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML, pages 160\u2013167. ACM, 2008. [7] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493\u20132537, 2011. [8] L. Deng, J. Li, J. Huang, K. Yao, D. Yu, F. Seide, M. L. Seltzer, G. Zweig, X. He, J. Williams, Y. Gong, and A. Acero. Recent advances in deep learning for speech research at microsoft. In ICASSP, pages 8604\u20138608, 2013. [9] A. Fischer and C. Igel. Training restricted boltzmann machines: An introduction. Pattern Recognition, 47(1):25\u201339, 2014. [10] B. J. Frey and D. Dueck. Clustering by passing messages between data points. Science, 315(5814):972\u2013976, February 2007. [11] M. C. Gonzalez, C. A. Hidalgo, and A.-L. Barabasi. Understanding individual human mobility patterns. Nature, 453(7196):779\u2013782, 2008. [12] A. Y. Hannun, C. Case, J. Casper, B. C. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng. Deep speech: Scaling up end-to-end speech recognition. CoRR, abs/1412.5567, 2014.\n[13] G. E. Hinton. A practical guide to training restricted boltzmann machines. In G. Montavon, G. B. Orr, and K. Mu\u0308ller, editors, Neural Networks: Tricks of the Trade - Second Edition, volume 7700 of Lecture Notes in Computer Science, pages 599\u2013619. Springer, 2012. [14] R. Jurdak, P. Sommer, B. Kusy, N. Kottege, C. Crossman, A. Mckeown, and D. Westcott. Camazotz: Multimodal activity-based gps sampling. In IPSN, 2013. [15] H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin. Exploring strategies for training deep neural networks. Journal of Machine Learning Research, 10:1\u201340, 2009. [16] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, Nov 1998. [17] H. Lee, C. Ekanadham, and A. Y. Ng. Sparse deep belief net model for visual area V2. In NPIS, pages 873\u2013880, 2007. [18] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, pages 609\u2013616, 2009. [19] H. Lee, P. Pham, Y. Largman, and A. Y. Ng. Unsupervised feature learning for audio classification using convolutional deep belief networks. In NIPS, pages 1096\u20131104, 2009. [20] M. Matsugu, K. Mori, Y. Mitari, and Y. Kaneda. Subject independent facial expression recognition with robust face detection using a convolutional neural network. Neural Networks, 16(5-6):555\u2013559, 2003. [21] A. Y. Ng. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In ICML, page 78, 2004. [22] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In ICML, pages 689\u2013696, 2011. [23] T. Rakthanmanon, B. J. L. Campana, A. Mueen, G. E. A. P. A. Batista, M. B. Westover, Q. Zhu, J. Zakaria, and E. J. Keogh. Searching and mining trillions of time series subsequences under dynamic time warping. In SIGKDD, pages 262\u2013270, 2012. [24] D. Sahoo, S. C. H. Hoi, and B. Li. Online multiple kernel regression. In SIGKDD, pages 293\u2013302, 2014. [25] M. Schmidt. Least squares optimization with l1-norm regularization. CS542B Project Report, 2005. [26] C. M. Schneider, V. Belik, T. Couronne\u0301, Z. Smoreda, and M. C. Gonza\u0301lez. Unravelling daily human mobility motifs. Journal of The Royal Society Interface, 10(84):20130246, 2013. [27] C. M. Schneider, C. Rudloff, D. Bauer, and M. C. Gonza\u0301lez. Daily travel behavior: Lessons from a week-long survey for the extraction of human mobility motifs related information. In ACM SIGKDD International Workshop on Urban Computing, page 3, 2013. [28] Y. Tsuruoka, J. Tsujii, and S. Ananiadou. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty. In ACL 2009, pages 477\u2013485, 2009. [29] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, pages 1096\u20131103, 2008. [30] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11:3371\u20133408, 2010. [31] D. Wang, C. Song, and A.-L. Barabasi. Quantifying long-term scientific impact. Science, 342(6154):127\u2013132, October 2013. [32] D. Wang, C. Song, H.-W. Shen, and A.-L. Barabasi. Response to comment on \u201dquantifying long-term scientific impact\u201d. Science, 345(6193), July 2014. [33] P. A. Zandbergen. Accuracy of iphone locations: A comparison of assisted gps, wifi and cellular positioning. Transactions in GIS, 13(s1):5\u201325, 2009.\n[34] K. Zhao, R. Jurdak, J. Liu, D. Westcott, B. Kusy, H. Parry, P. Sommer, and A. McKeown. Optimal le\u0301vy-flight foraging in a finite landscape. Journal of The Royal Society Interface, 12(104), 2015."}], "references": [{"title": "Convolutional neural networks for speech recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "L. Deng", "G. Penn", "D. Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech & Language Processing, 22(10):1533\u20131545,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, 2(1):1\u2013127,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "NIPS, 19:153,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "CRAWDAD data set yonsei/lifemap (v", "author": ["Y. Chon", "E. Talipov", "H. Shin", "H. Cha"], "venue": "2012-01-03). Downloaded from http://crawdad.org/yonsei/lifemap/, Jan.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "IJCAI, pages 1237\u20131242,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML, pages 160\u2013167. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P.P. Kuksa"], "venue": "Journal of Machine Learning Research, 12:2493\u20132537,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Recent advances in deep learning for speech research at microsoft", "author": ["L. Deng", "J. Li", "J. Huang", "K. Yao", "D. Yu", "F. Seide", "M.L. Seltzer", "G. Zweig", "X. He", "J. Williams", "Y. Gong", "A. Acero"], "venue": "ICASSP, pages 8604\u20138608,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Training restricted boltzmann machines: An introduction", "author": ["A. Fischer", "C. Igel"], "venue": "Pattern Recognition, 47(1):25\u201339,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustering by passing messages between data points", "author": ["B.J. Frey", "D. Dueck"], "venue": "Science, 315(5814):972\u2013976, February", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Understanding individual human mobility patterns", "author": ["M.C. Gonzalez", "C.A. Hidalgo", "A.-L. Barabasi"], "venue": "Nature, 453(7196):779\u2013782,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A.Y. Hannun", "C. Case", "J. Casper", "B.C. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G.E. Hinton"], "venue": "G. Montavon, G. B. Orr, and K. M\u00fcller, editors, Neural Networks: Tricks of the Trade - Second Edition, volume 7700 of Lecture Notes in Computer Science, pages 599\u2013619. Springer,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Camazotz: Multimodal activity-based gps sampling", "author": ["R. Jurdak", "P. Sommer", "B. Kusy", "N. Kottege", "C. Crossman", "A. Mckeown", "D. Westcott"], "venue": "IPSN,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring strategies for training deep neural networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "Journal of Machine Learning Research, 10:1\u201340,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324, Nov", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "NPIS, pages 873\u2013880,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "ICML, pages 609\u2013616,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "P. Pham", "Y. Largman", "A.Y. Ng"], "venue": "NIPS, pages 1096\u20131104,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Subject independent facial expression recognition with robust face detection using a convolutional neural network", "author": ["M. Matsugu", "K. Mori", "Y. Mitari", "Y. Kaneda"], "venue": "Neural Networks, 16(5-6):555\u2013559,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Feature selection, l 1 vs", "author": ["A.Y. Ng"], "venue": "l 2 regularization, and rotational invariance. In ICML, page 78,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "ICML, pages 689\u2013696,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Searching and mining trillions of time series subsequences under dynamic time warping", "author": ["T. Rakthanmanon", "B.J.L. Campana", "A. Mueen", "G.E.A.P.A. Batista", "M.B. Westover", "Q. Zhu", "J. Zakaria", "E.J. Keogh"], "venue": "SIGKDD, pages 262\u2013270,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Online multiple kernel regression", "author": ["D. Sahoo", "S.C.H. Hoi", "B. Li"], "venue": "SIGKDD, pages 293\u2013302,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Least squares optimization with l1-norm regularization", "author": ["M. Schmidt"], "venue": "CS542B Project Report,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Unravelling daily human mobility motifs", "author": ["C.M. Schneider", "V. Belik", "T. Couronn\u00e9", "Z. Smoreda", "M.C. Gonz\u00e1lez"], "venue": "Journal of The Royal Society Interface, 10(84):20130246,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Daily travel behavior: Lessons from a week-long survey for the extraction of human mobility motifs related information", "author": ["C.M. Schneider", "C. Rudloff", "D. Bauer", "M.C. Gonz\u00e1lez"], "venue": "ACM SIGKDD International Workshop on Urban Computing, page 3,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty", "author": ["Y. Tsuruoka", "J. Tsujii", "S. Ananiadou"], "venue": "ACL 2009, pages 477\u2013485,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P. Manzagol"], "venue": "ICML, pages 1096\u20131103,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol"], "venue": "Journal of Machine Learning Research, 11:3371\u20133408,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Quantifying long-term scientific impact", "author": ["D. Wang", "C. Song", "A.-L. Barabasi"], "venue": "Science, 342(6154):127\u2013132, October", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Response to comment on \u201dquantifying long-term scientific impact", "author": ["D. Wang", "C. Song", "H.-W. Shen", "A.-L. Barabasi"], "venue": "Science, 345(6193), July", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Accuracy of iphone locations: A comparison of assisted gps, wifi and cellular positioning", "author": ["P.A. Zandbergen"], "venue": "Transactions in GIS, 13(s1):5\u201325,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimal l\u00e9vy-flight foraging in a finite landscape", "author": ["K. Zhao", "R. Jurdak", "J. Liu", "D. Westcott", "B. Kusy", "H. Parry", "P. Sommer", "A. McKeown"], "venue": "Journal of The Royal Society Interface, 12(104),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "It is the reason why across the days of an average person, there often exist periodical patterns for their mobility or more generally, their behavior [26, 27].", "startOffset": 150, "endOffset": 158}, {"referenceID": 26, "context": "It is the reason why across the days of an average person, there often exist periodical patterns for their mobility or more generally, their behavior [26, 27].", "startOffset": 150, "endOffset": 158}, {"referenceID": 13, "context": "While in the scenario of smart location tracking [14, 34], with a replenish-able energy budget the system either aims to minimize the energy efficiency of location tracking, or attempts to maximize the tracking accuracy given a fixed energy budget.", "startOffset": 49, "endOffset": 57}, {"referenceID": 33, "context": "While in the scenario of smart location tracking [14, 34], with a replenish-able energy budget the system either aims to minimize the energy efficiency of location tracking, or attempts to maximize the tracking accuracy given a fixed energy budget.", "startOffset": 49, "endOffset": 57}, {"referenceID": 2, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 31, "endOffset": 48}, {"referenceID": 1, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 31, "endOffset": 48}, {"referenceID": 14, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 31, "endOffset": 48}, {"referenceID": 17, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 31, "endOffset": 48}, {"referenceID": 8, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 31, "endOffset": 48}, {"referenceID": 18, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 272, "endOffset": 280}, {"referenceID": 21, "context": "For instance, numerous studies [3, 2, 15, 18, 9] have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well[19, 22].", "startOffset": 272, "endOffset": 280}, {"referenceID": 12, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 16, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 12, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 145, "endOffset": 157}, {"referenceID": 28, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 284, "endOffset": 295}, {"referenceID": 2, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 284, "endOffset": 295}, {"referenceID": 29, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 284, "endOffset": 295}, {"referenceID": 17, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 411, "endOffset": 418}, {"referenceID": 4, "context": "These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model [13, 17, 13], to sparse autoencoders that introduce an unsupervised \u201cdenoising\u201d mechanism to remove insignificant, noisy signals from data [29, 3, 30], to using convolution as an effective way to learn representative features robust to geometric locations of images [18, 5].", "startOffset": 411, "endOffset": 418}, {"referenceID": 19, "context": "Among the variations of neural networks, inspired by biological processes [20], convolutional networks in particular excel in finding such abstract features that are robust to geometric variations in images [18].", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "Among the variations of neural networks, inspired by biological processes [20], convolutional networks in particular excel in finding such abstract features that are robust to geometric variations in images [18].", "startOffset": 207, "endOffset": 211}, {"referenceID": 0, "context": "Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition [1, 8, 12] and natural language processing [6, 7].", "startOffset": 133, "endOffset": 143}, {"referenceID": 7, "context": "Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition [1, 8, 12] and natural language processing [6, 7].", "startOffset": 133, "endOffset": 143}, {"referenceID": 11, "context": "Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition [1, 8, 12] and natural language processing [6, 7].", "startOffset": 133, "endOffset": 143}, {"referenceID": 5, "context": "Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition [1, 8, 12] and natural language processing [6, 7].", "startOffset": 176, "endOffset": 182}, {"referenceID": 6, "context": "Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition [1, 8, 12] and natural language processing [6, 7].", "startOffset": 176, "endOffset": 182}, {"referenceID": 22, "context": "While in the past decade, realizing there is abstract and structural information beneath the raw numeric values in the time-series, researchers have experimented to discover such patterns by clustering or \u201cmotif\u201d discovery [23, 26, 27].", "startOffset": 223, "endOffset": 235}, {"referenceID": 25, "context": "While in the past decade, realizing there is abstract and structural information beneath the raw numeric values in the time-series, researchers have experimented to discover such patterns by clustering or \u201cmotif\u201d discovery [23, 26, 27].", "startOffset": 223, "endOffset": 235}, {"referenceID": 26, "context": "While in the past decade, realizing there is abstract and structural information beneath the raw numeric values in the time-series, researchers have experimented to discover such patterns by clustering or \u201cmotif\u201d discovery [23, 26, 27].", "startOffset": 223, "endOffset": 235}, {"referenceID": 20, "context": "the high-level neuron\u2019s responses to the sample, as high-level pattern recognizers responses to the signal, a sparse solution will utilize the most significant responses and hence will be less sensitive to noise [21, 25].", "startOffset": 212, "endOffset": 220}, {"referenceID": 24, "context": "the high-level neuron\u2019s responses to the sample, as high-level pattern recognizers responses to the signal, a sparse solution will utilize the most significant responses and hence will be less sensitive to noise [21, 25].", "startOffset": 212, "endOffset": 220}, {"referenceID": 20, "context": "The advantage of using the l1 regularizer over l2 is that the l1 regularizer forces the optimization to find a sparse solution that only uses the most distinctive high-level features to conjure the final prediction [21, 25].", "startOffset": 215, "endOffset": 223}, {"referenceID": 24, "context": "The advantage of using the l1 regularizer over l2 is that the l1 regularizer forces the optimization to find a sparse solution that only uses the most distinctive high-level features to conjure the final prediction [21, 25].", "startOffset": 215, "endOffset": 223}, {"referenceID": 27, "context": "One can speed up this optimization process using the methods proposed in [28].", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "With the convolution layer\u2019s back propagated error being \u03b4 (which can be calculated by the method described in [16]), \u2202J(W,b;x,y) \u2202W (1) can therefore be updated with the gradient:", "startOffset": 111, "endOffset": 115}, {"referenceID": 3, "context": "Both modalities are extracted from the LifeMap [4]) that contains human mobility traces collected from eight individuals, spanning from a few months to around two years.", "startOffset": 47, "endOffset": 50}, {"referenceID": 32, "context": "network positioning are reported to be 8, 74 and 600 m [33] 2https://archive.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "For comparison, we consider four competitive methods, namely Support Vector regression with Linear kernel (SVLN), Support Vector regression with Radial Basis kernel (SVSIG), Support Vector regression with Polynomial kernel (SVPOLY), and Multiple Kernel Regression (MKR) [24].", "startOffset": 270, "endOffset": 274}, {"referenceID": 10, "context": "Using four metrics is due to that for datasets with long-tailed values (which human behaviors can often be characterized to be [11]), as an absolute measurement, MSE alone is not an ideal metric to evaluate a regression method\u2019s performance because it is heavily biased by samples in the long tail [31, 32].", "startOffset": 127, "endOffset": 131}, {"referenceID": 30, "context": "Using four metrics is due to that for datasets with long-tailed values (which human behaviors can often be characterized to be [11]), as an absolute measurement, MSE alone is not an ideal metric to evaluate a regression method\u2019s performance because it is heavily biased by samples in the long tail [31, 32].", "startOffset": 298, "endOffset": 306}, {"referenceID": 31, "context": "Using four metrics is due to that for datasets with long-tailed values (which human behaviors can often be characterized to be [11]), as an absolute measurement, MSE alone is not an ideal metric to evaluate a regression method\u2019s performance because it is heavily biased by samples in the long tail [31, 32].", "startOffset": 298, "endOffset": 306}, {"referenceID": 9, "context": "We run a clustering with the affinity propagation method in [10], and find the top 10 exemplars.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "As an issue often posed to complex learning models including neural networks, how to select the hyperparameters is an open question studied by many [15].", "startOffset": 148, "endOffset": 152}, {"referenceID": 14, "context": "One can also use the greedy hyperparameter selection processed described in [15].", "startOffset": 76, "endOffset": 80}], "year": 2015, "abstractText": "The prediction of periodical time-series remains challenging due to various types of data distortions and misalignments. Here, we propose a novel model called Temporal embeddingenhanced convolutional neural Network (TeNet) to learn repeatedly-occurring-yet-hidden structural elements in periodical time-series, called abstract snippets, for predicting future changes. Our model uses convolutional neural networks and embeds a time-series with its potential neighbors in the temporal domain for aligning it to the dominant patterns in the dataset. The model is robust to distortions and misalignments in the temporal domain and demonstrates strong prediction power for periodical time-series. We conduct extensive experiments and discover that the proposed model shows significant and consistent advantages over existing methods on a variety of data modalities ranging from human mobility to household power consumption records. Empirical results indicate that the model is robust to various factors such as number of samples, variance of data, numerical ranges of data etc. The experiments also verify that the intuition behind the model can be generalized to multiple data types and applications and promises significant improvement in prediction performances across the datasets studied.", "creator": "LaTeX with hyperref package"}}}