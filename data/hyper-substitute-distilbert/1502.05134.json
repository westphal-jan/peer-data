{"id": "1502.05134", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2015", "title": "Supervised cross-modal factor analysis for multiple modal data classification", "abstract": "in this algorithm seo discovered the problem of learning from certain dynamic data forming prevention of document classification. investigating this problem, each document is presented two different modals totaling graphics, i. st., an image and a tensor. cross - modal factor analysis ( cfa ) researches further proposed to project the mentally different shapes of data to a multiple message space, so that the search the correlated image among relevant text can be performed directly in this space. a report after cfa is historically it has ignored the general information. in this paper, students improve cfa by incorporating the supervision measures to represent and classify both image and text arrangements of graphics. participants project both logo and feature designs to exhibit matching data space by factor analysis, and then train a different label predictor in presenting likely scenario to use its class diagram information. the factor specific parameter and document structure itself are learned jointly by solving existing single objective function. with this objective function, we minimize the distance to distinct projections of image and images of about same location, and the classification error of the projection measured by hinge loss regression. the similar result is tested by an alternate research methodology in the optimal implementation. experiments across two possible quantitative spatial document data rooms show valuable advantage of providing similarity layout under certain cfa tests.", "histories": [["v1", "Wed, 18 Feb 2015 06:55:07 GMT  (91kb)", "http://arxiv.org/abs/1502.05134v1", null], ["v2", "Tue, 18 Aug 2015 05:20:59 GMT  (80kb)", "http://arxiv.org/abs/1502.05134v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jingbin wang", "yihua zhou", "kanghong duan", "jim jing-yan wang", "halima bensmail"], "accepted": false, "id": "1502.05134"}, "pdf": {"name": "1502.05134.pdf", "metadata": {"source": "CRF", "title": "Supervised cross-modal factor analysis", "authors": ["Jingbin Wang", "Haoxiang Wang", "Yujin Tu", "Kanghong Duan", "Zhenxin Zhan", "Srikanth Chekuri"], "emails": ["jingbinwang1@outlook.com", "wanghaoxiang1102@hotmail.com", "yujintu1@yahoo.com", "kanghongduan@outlook.com", "zhenxin.zhan.dr@gmail.com", "schekuria9@hotmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n05 13\n4v 1\n[ cs\n.L G\n] 1\n8 Fe\nIndex Terms\u2014Multiple modal learning, Cross-modal factor analysis, Supervised learning\nI. INTRODUCTION\nIn this paper, we deal with the problem of learning from multiple modal data [1], [2]. Traditional data representation, classification and retrieval problems usually focus on single modal data [3], [4]. For example, for the problem of text classification, we usually only consider using a data set of text to train a classifier [5], [6]. While for the problem of image representation, only the images are considered to learn the representation parameters [7], [8]. However, in modern information landscape, the data is usually composed of different modals. For example, in video clips, there are two modals of data, e.g. image sequence data and audio data. Moreover, in a research article, there are not only the text data, but also the image data. Learning from these multiple modal data has attracted much attention from both machine learning and multimedia information processing communities\n[9], [10]. Recently, cross-modal factor analysis (CFA) has been proposed to project different modal of data to a shared feature space so that classification or retrieval can be performed cross data modal [11]. It assumes that each document is composed of two modals of data, e.g., image and text, and try to learn a projection matrix for each modal, so that the projections of the two modals of a document can be as close to each other as possible. With these projections, we can project any data of one of the two modals to the shared data space, and then perform retrieval cross these two modals. Moreover, since the data of different modals can be projected to a shared data space, we can also train a classifier from both the modals for the classification of data of any modal. CFA is further extended to its kernel version in [12].\nIn this paper, we consider the problem of learning a classifier from multiple modal data to classify a single modal data. In a training set of documents, each document is composed of data of two modals, e.g., an image and a text. We can first apply CFA to project two modals to a shared space and then learn a classifier in this space by using the class label information. However, in the phase of projection, the class label information has been ignored by CFA. Actually, without using supervision information provided by class label information, it cannot be guaranteed that the projections are discriminative enough. Although we can apply powerful classification methods after the projection of multiple modal data, the discriminative information which is necessary for classification may has been lost during the projection procedure. Thus it is very necessary to include the available class label information in the CFA projection. Surprisingly, no work has been done to incorporate the supervision information contained by the class labels to improve the discriminative ability of CAF. To fill this gap, in this paper, we proposed the first supervised CFA method by regularizing the projections of different modals by the class label information.\nThe contributions of this paper are of two folds:\n1) For the first time, we propose the formulation of super-\nvised CFA. We propose to project the data of two modals to a shared data space by orthonormal transformations, and use a linear class label function to predict the class labels of the training multiple modal documents. To formulate the problem, we propose to minimize the difference between the projections of two modals of the same document, and simultaneously to minimize the classification error of both the two modal measured by hinge loss. In this way, the learning of orthonormal transformation matrices of multiple modal projections and the class label predictor parameter are unified, and the predictor learning can regularize the learning of orthonormal transformation matrices to improve the discriminative ability of the multiple modal projections. 2) We also develop an iterative algorithm to optimize the constrained minimization problem of this formulation. The projection parameter and the predictor parameter are optimized alternately in an iterative algorithm. The orthonormal transformation matrices are optimized by fixing class label predictor parameter matrix and solving a singular value decomposition (SVD) problem [13], [14]. The class label predictor parameter matrix is solved by fixing orthonormal transformation matrices and solving a quadratic programming (QP) problem. [15], [16]\nThe rest parts of this paper are organized as follows: in section II we introduce the proposed supervised CFA (SupCFA), in section III the proposed method is evaluated experimentally, and in section IV, the paper is concluded."}, {"heading": "II. PROPOSED METHOD", "text": ""}, {"heading": "A. Problem formulation", "text": "We assume that we have a training set of n documents D = {D1, \u00b7 \u00b7 \u00b7 , Dn}, where Di is the i-th document. Each documents is comprised of a image component and a accompanying text, i.e., Di = (Ii, Ti), where Ii \u2208 RdI is a dI - dimensional feature space of the image, Ti \u2208 RdI is a a dT - dimensional feature of the text. These documents are assumed to belongs to m classes, and for the i-th document, we define a class label vector yi = [yi1, \u00b7 \u00b7 \u00b7 , yim] \u2208 {+1,\u22121}\nm to indicate which class it belongs to. The j-th dimension of yi, yij = +1 if Di belongs to the j-th class, and yij = \u22121 otherwise. The goal of cross-modal classification is to learn a predictor from the training set, and use the predictor to predict the class label vector of given a test text (image) query T (I).\nTo this end, we first project the images and texts of documents to a shared d-dimensional feature space by orthonormal transformations, and then learn a linear prediction function in the share space to predict the class label vector. The projection in the image and text space are given as follows,\nIi \u2192 Ii\u2126I \u2208 R d, Ti \u2192 Ti\u2126T \u2208 R d (1)\nwhere \u2126I \u2208 RdI\u00d7d is the orthonormal transformation matrix of the image data, and \u2126T \u2208 RdT\u00d7d is the orthonormal transformation matrix of the text data. CFA assumes that the projections of the image and text of a single document should\nbe as close to each other as possible, and the squared \u21132 norm distance between Ii\u2126I and Ti\u2126T is minimized over all the training documents,\nmin \u2126I ,\u2126T\nn \u2211\ni=1\n\u2016\u2126IIi \u2212 \u2126TTi\u2016 2\n2\ns.t. \u2126\u22a4I \u2126I = Id\u00d7d,\u2126 \u22a4 T\u2126T = Id\u00d7d,\n(2)\nwhere Id\u00d7d is a dI \u00d7 dI identity matrix. To predict the class label vector yi from the projections of image and text of the i-th document, we try to learn a linear function as follows,\nyi \u2190 fW (Ii\u2126I) = (Ii\u2126I)W, yi \u2190 fW (Ti\u2126T ) = (\u2126TTi)W, (3)\nwhere W \u2208 Rd\u00d7m is the predictor parameter matrix. To learn W , we minimize its squared \u21132 norm and the hinge loss of the predictor over both the images and texts of all the training documents,\nmin W \u2016W\u201622 + C1\nn \u2211\ni=1\n(\u03bei + \u03b5i)\ns.t. h\u2212 (Ii\u2126I)Wy\u22a4i \u2264 \u03bei, \u03bei \u2265 0,\nh\u2212 (\u2126TTi)Wy\u22a4i \u2264 \u03b5i, \u03b5i \u2265 0, i = 1, \u00b7 \u00b7 \u00b7 , n.\n(4)\nwhere \u03bei is the slack variable of the hinge loss over the image of the i-th document, \u03b5i is that of the text of the i-th document, h is a parameter of the hinge loss function, and C1 is a tradeoff parameter. The minimization of \u2016W\u201622 is to reduce the complexity of the predictor and also to seek a large margin. The hinge loss is applied to reduce the prediction error.\nThe formulation of the proposed method is the combination of (2) and (4), which is as follows,\nmin W,\u2126I ,\u2126T\n1 2 \u2016W\u201622 + C1\nn \u2211\ni=1\n(\u03bei + \u03b5i) + C2\nn \u2211\ni=1\n\u2016Ii\u2126I \u2212 Ti\u2126T \u2016 2\n2\ns.t. h\u2212 (Ii\u2126I)Wy\u22a4i \u2264 \u03bei, \u03bei \u2265 0,\nh\u2212 (Ti\u2126T )Wy\u22a4i \u2264 \u03b5i, \u03b5i \u2265 0, i = 1, \u00b7 \u00b7 \u00b7 , n, \u2126\u22a4I \u2126I = Id\u00d7d,\u2126 \u22a4 T\u2126T = Id\u00d7d,\n(5) where C2 is another tradeoff parameter. Please note that in this formulation, not only the orthonormal transformation matrices are variables, but also the predictor parameter. In this way, the representation and the classification of image and text modals are unified. Both the images and texts are mapped to a shared space and then a shared predictor are applied to classy them."}, {"heading": "B. Optimization", "text": "To solve the problem in (5), we write the Lagrange function as follows,\nL = 1\n2 \u2016W\u201622 + C1\nn \u2211\ni=1\n(\u03bei + \u03b5i) + C2\nn \u2211\ni=1\n\u2016Ii\u2126I \u2212 Ti\u2126T \u2016 2\n2\n+\nn \u2211\ni=1\n\u03b1i ( h\u2212 (Ii\u2126I)Wy\u22a4i \u2212 \u03bei ) \u2212\nn \u2211\ni=1\n\u03b2i\u03bei\n+\nn \u2211\ni=1\n\u03b3i ( h\u2212 (Ti\u2126T )Wy\u22a4i \u2212 \u03b5i ) \u2212\nn \u2211\ni=1\n\u03b4i\u03b5i\n\u2212 Tr ( \u0393\u22a4 ( \u2126\u22a4I \u2126I \u2212 Id\u00d7d )) \u2212 Tr ( \u2206\u22a4 ( \u2126\u22a4T\u2126T \u2212 Id\u00d7d ))\n, (6)\nwhere \u03b1i, \u03b2i, \u03b3i, \u03b4i, i = 1, \u00b7 \u00b7 \u00b7 , n, \u0393 and \u2206 are Lagrange multiplier variables. To seek the minimization of the objective, we set the partial derivative of L with regard to W , \u03bei and \u03b5i to zero respectively,\n\u2202L\n\u2202W = W \u2212\nn \u2211\ni=1\n\u03b1i(Ii\u2126I) \u22a4yi \u2212\nn \u2211\ni=1\n\u03b3i(Ti\u2126T ) \u22a4yi = 0,\n\u21d2 W =\nn \u2211\ni=1\n\u03b1i(Ii\u2126I) \u22a4yi +\nn \u2211\ni=1\n\u03b3i(Ti\u2126T ) \u22a4yi,\n\u2202L \u2202\u03bei = C1 \u2212 \u03b1i \u2212 \u03b2i = 0 \u21d2 C1 \u2212 \u03b1i = \u03b2i \u2265 0 \u21d2 \u03b1i \u2264 C1 \u2202L \u2202\u03b5i = C1 \u2212 \u03b3i \u2212 \u03b4i = 0 \u21d2 C1 \u2212 \u03b3i = \u03b4i \u2265 0 \u21d2 \u03b3i \u2264 C1.\n(7) By substituting (7) to (6), we have\nL =\u2212 1\n2\nn \u2211\ni,j=1\n\u03b1i\u03b1jTr ( \u2126\u22a4I I \u22a4 i yiy \u22a4 j Ij\u2126I )\n+\nn \u2211\ni,j=1\n\u03b1i\u03b3jTr ( \u2126IIiy\u22a4i yjTj\u2126T )\n\u2212 1\n2\nn \u2211\ni,j=1\n\u03b3i\u03b3jTr ( \u2126\u22a4T T \u22a4 i yiy \u22a4 j Tj\u2126T )\n+ C2\nn \u2211\ni=1\nTr ( \u2126\u22a4I I \u22a4 i Ii\u2126I ) \u2212 2C2\nn \u2211\ni=1\nTr ( \u2126\u22a4I I \u22a4 i Ti\u2126T )\n+\nn \u2211\ni=1\nTr ( \u2126\u22a4T T \u22a4 i Ti\u2126T )\n+ h\nn \u2211\ni=1\n(\u03b1i + \u03b3i)\n\u2212 Tr ( \u0393\u22a4 ( \u2126\u22a4I \u2126I \u2212 Id\u00d7d )) \u2212 Tr ( \u2206\u22a4 ( \u2126\u22a4T\u2126T \u2212 Id\u00d7d ))\n. (8)\nThe optimization problem is transferred to the following coupled problem,\nmax \u03b1i|ni=1,\u03b3i| n i=1 ,\u0393,\u2206 min \u2126I ,\u2126T L,\ns.t. 0 \u2264 \u03b1i \u2264 C1, 0 \u2264 \u03b3i \u2264 C1, i = 1, \u00b7 \u00b7 \u00b7 , n,\n\u0393 \u2265 0,\u2206 \u2265 0. (9)\nTo solve this problem, we employ the alternate optimization strategy. We optimize \u03b1i|ni=1, \u03b3i| n i=1 and \u0393,\u2206,\u2126I ,\u2126T alternately in an iterative algorithm. In each iteration, we first fix \u0393,\u2206,\u2126I ,\u2126T and optimize \u03b1i|ni=1, \u03b3i| n i=1, and then we fix \u03b1i| n i=1, \u03b3i| n i=1 and optimize \u0393,\u2206,\u2126I ,\u2126T .\n1) Optimizing \u03b1i|ni=1 and \u03b3i| n i=1: Fixing \u0393,\u2206,\u2126I and \u2126T , and removing the terms irrelevant to \u03b1i|ni=1 and \u03b3i| n i=1 from (8), we rewrite (9) as\nmax \u03b1i|ni=1,\u03b3i| n i=1\n\u2212 1\n2\nn \u2211\ni,j=1\n\u03b1i\u03b1jTr ( \u2126\u22a4I I \u22a4 i yiy \u22a4 j Ij\u2126I )\n+\nn \u2211\ni,j=1\n\u03b1i\u03b3jTr ( \u2126IIiy\u22a4i yjTj\u2126T )\n\u2212 1\n2\nn \u2211\ni,j=1\n\u03b3i\u03b3jTr ( \u2126\u22a4T T \u22a4 i yiy \u22a4 j Tj\u2126T )\n+ h\nn \u2211\ni=1\n(\u03b1i + \u03b3i)\ns.t. 0 \u2264 \u03b1i \u2264 C1, 0 \u2264 \u03b3i \u2264 C1, i = 1, \u00b7 \u00b7 \u00b7 , n.\n(10)\nThis problem can be solved as a QP problem.\n2) Optimizing \u2126I and \u2126T : By fixing \u03b1i|ni=1 and \u03b3i| n i=1, and removing terms irrelevant to \u0393,\u2206,\u2126I and \u2126T from (8), we rewrite (9) as\nmax \u0393,\u2206 min \u2126I ,\u2126T\n\u2212 1\n2\nn \u2211\ni,j=1\n\u03b1i\u03b1jTr ( \u2126\u22a4I I \u22a4 i yiy \u22a4 j Ij\u2126I )\n+\nn \u2211\ni,j=1\n\u03b1i\u03b3jTr ( \u2126\u22a4I I \u22a4 i yiy \u22a4 j Tj\u2126T )\n\u2212 1\n2\nn \u2211\ni,j=1\n\u03b3i\u03b3jTr ( \u2126\u22a4T T \u22a4 i yiy \u22a4 j Tj\u2126T )\n+ C2\nn \u2211\ni=1\nTr ( \u2126\u22a4I I \u22a4 i Ii\u2126I )\n\u2212 2C2\nn \u2211\ni=1\nTr ( \u2126\u22a4I I \u22a4 i Ti\u2126T )\n+\nn \u2211\ni=1\nTr ( \u2126\u22a4T T \u22a4 i Ti\u2126T )\n\u2212 Tr ( \u0393\u22a4 ( \u2126\u22a4I \u2126I \u2212 Id\u00d7d )) \u2212 Tr ( \u2206\u22a4 (\n\u2126\u22a4T\u2126T \u2212 Id\u00d7d )) ,\ns.t. \u0393 \u2265 0,\u2206 \u2265 0.\n(11)\nThe primal problem of this dual problem is\nmin \u2126I ,\u2126T\n\u2212 1\n2\nn \u2211\ni,j=1\n\u03b1i\u03b1jTr ( \u2126\u22a4I I \u22a4 i yiy \u22a4 j Ij\u2126I )\n+\nn \u2211\ni,j=1\n\u03b1i\u03b3jTr ( \u2126\u22a4I I \u22a4 i yiy \u22a4 j Tj\u2126T )\n\u2212 1\n2\nn \u2211\ni,j=1\n\u03b3i\u03b3jTr ( \u2126\u22a4T T \u22a4 i yiy \u22a4 j Tj\u2126T )\n+ C2\nn \u2211\ni=1\nTr ( \u2126\u22a4I I \u22a4 i Ii\u2126I ) \u2212 2C2\nn \u2211\ni=1\nTr ( \u2126\u22a4I I \u22a4 i Ti\u2126T )\n+\nn \u2211\ni=1\nTr ( \u2126\u22a4T T \u22a4 i Ti\u2126T )\ns.t. \u2126\u22a4I \u2126I = Id\u00d7d,\u2126 \u22a4 T\u2126T = Id\u00d7d.\n(12) Using the constrains \u2126\u22a4I \u2126I = Id\u00d7d and \u2126 \u22a4 T\u2126T \u2212 Id\u00d7d, we can remove some constant terms from (12) and rewrite it as\nmin \u2126I ,\u2126T\nn \u2211\ni,j=1\n\u03b1i\u03b3jTr ( \u2126\u22a4I I \u22a4 i yiy \u22a4 j Tj\u2126T )\n\u2212 2C2\nn \u2211\ni=1\nTr ( \u2126\u22a4I I \u22a4 i Ti\u2126T )\n= Tr\n\n\u2126\u22a4I\n\n\nn \u2211\ni,j=1\n\u03b1i\u03b3jI \u22a4 i yiy \u22a4 j Tj \u2212 2C2\nn \u2211\ni=1\nI\u22a4i Ti\n\n\u2126T\n\n\ns.t. \u2126\u22a4I \u2126I = Id\u00d7d,\u2126 \u22a4 T\u2126T = Id\u00d7d.\n(13) The equivalent problem is\nmax \u2126I ,\u2126T\nTr ( \u2126\u22a4I Z\u2126T )\ns.t. \u2126\u22a4I \u2126I = Id\u00d7d,\u2126 \u22a4 T\u2126T = Id\u00d7d.\n(14)\nwhere\nZ =\nn \u2211\ni,j=1\n\u03b1i\u03b3jI \u22a4 i yiy \u22a4 j Tj \u2212 2C2\nn \u2211\ni=1\nI\u22a4i Ti \u2208 R dI\u00d7dT . (15)\nThe optimal matrices \u2126I and \u2126T can be obtained by a SVD of the matrix Z , i.e.,\nZ = \u2126I\u03a3\u2126 \u22a4 T (16)\nwhere \u03a3 is the matrix of singular values of Z ."}, {"heading": "C. Algorithm", "text": "Based on the optimization results, we can design an iterative algorithm as Algorithm 1. The iterations are repeated T times, and the t-th each iteration, the variables \u2126tI , \u2126 t T , \u03b1 t i| n i=1 and \u03b3ti | n i=1 are updated alternately. Finally the predictor parameter W is calculated from the updated variables."}, {"heading": "III. EXPERIMENTS", "text": "In this section, we will investigate the proposed algorithm experimentally.\nAlgorithm 1 Iterative learning algorithm of supervised crossmodal factor analysis.\nInput: A training set of n documents {(I1, Ti), \u00b7 \u00b7 \u00b7 , (In, Tn)}, and corresponding class label vector set {y1, \u00b7 \u00b7 \u00b7 , yn}; Input: Tradeoff parameters C1, C2, and maximum iteration number T ; Initialize orthonormal transformation matrices \u21260I and \u2126 0 T ; for t = 1, \u00b7 \u00b7 \u00b7 , T do Fix \u2126t\u22121I and \u2126 t\u22121 T , and update \u03b1 t i| n i=1 and \u03b3 t i | n i=1 by\nsolving the problem in (10); Fix \u03b1ti| n i=1 and \u03b3 t i | n i=1 , and update Z\nt as in (15); Update \u2126tI and \u2126 t T by applying SVD to Z\nt as in (16); end for Calculate predictor function parameter matrix W from \u2126TI , \u2126TT , \u03b1 T i | n i=1 and \u03b3 T i | n i=1 (7). Output: The orthonormal transformation matrices \u2126TI and \u2126TT and predictor function parameter matrix W ."}, {"heading": "A. Experiment setup", "text": "1) Data sets: In this experiment, we used two different document data sets composed of images and texts. The first data set is the TVGraz database [17], which is a multimodal database of object categories composed of textual and visual features. The documents belongs to 10 of 256 classes of Caltech-256. 1,000 webpages are retrieved for each of the 10 classes, and 2,058 image-text pairs are collected. Each imagetext pair is composed to a document, thus we have 2,058 documents of 10 classes in this data set.\nThe seconde data set is the Wikipedia database, which is selected from Wikipedia featured article database, and Wikipedia featured article database contains documents of 30 classes. Because most of the classes of Wikipedia featured articles database contains very few documents, we only choose the 10 classes with the most documents. Moreover, each featured article usually have more than one images and sections, so we split each featured article to several documents. Each documents contains a section of a featured article, and the images placed to this section. In this way, we have in total 7,114 documents. Moreover, we remove the documents which have more than one image, and the documents which have a text with less than 70 words. Finally, we have 2,866 documents in total in our experiment.\nThe images in the documents are represented as feature vectors using the bag-of-features method, while the texts in the documents are represented as feature vectors using the bag-ofwords method.\n2) Experiment protocol: To conduct the experiment, we used the 10-fold cross validation strategy. The entire data set is split to 10 folds randomly, and each fold was used as a test set in turn, while the remaining 9 sets were combined and used as a training set. The proposed learning algorithm was performed to the training set to learn the orthonormal transformation matrices and the predictor parameter matrix,\nand then they are used to represent and classify the individual images and texts in the test set. The classification accuracy is measured by the classification rate as follows,\nclassification rate\n= number of correctly classified images and texts\ntotal number of test images and texts .\n(17)"}, {"heading": "B. Results", "text": "1) Comparison to unsupervised CFA: We first compare the proposed supervised version of CFA against unsupervised CFA methods on the problem of image/text classification problem. We considered the original CFA method [11] and its kernel version (CFAker) [12] as data representation methods, and used a SVM as a classifier. The boxplots of classification rates of the 10-fold cross validations of the compared methods over two data sets are given in Fig. 1. It is clear that the proposed SupCFA outperforms the two unsupervised CFA methods completely. The low quartile of the SupCFA classification rates is even higher than the upper quartiles of the tow compared methods. This is not surprising at all because SupCFA is the only method which can explore the supervision information to improve the discriminative ability of crossmodal factor analysis, while CFA and CFAker ignore the class label information at all. Moreover, it seems that CFAker outperforms CFA due to its usage of kernel tricks.\n2) Sensitivity to parameters: There are two tradeoff parameters C1 and C2 in our algorithm, and we are also interested in the sensitivity of the proposed algorithm to these parameters. The plot of classification rates against different C1 and C2 values are given in Fig. 2. From Fig. 2(a), we can see that when C1 is increased from 1 to 100, the classification rates keep increasing. This indicates that a higher weights of hinges loss function can boost the classification performance within some range of C1 values. However, when C1 is increased\nfrom 100 to 1000, no significant performance improvement is observed, meaning that in this range, the algorithm is stable to the parameter C1. From Fig. 2(b), we can see that the classification rate also tends to increase with larger C2 value. However, the overall performance is stable to C2 compared to C1.\n3) Algorithm convergency: Since the proposed algorithm ia an iterative algorithm, it is important to study its convergency over iterations. We plot the curve of objective function over different iterations in Fig. 3. It could be seen that the objective function is reduced significantly in the first 30 iterations, and it tends to converge after the 30-th iteration."}, {"heading": "IV. CONCLUSIONS AND FUTURE WORKS", "text": "In this paper, we proposed the first supervised CFA method for the presentation and classification of multiple modal data. The proposed method not only projects data of different modals to a shared data space like CFA, but also tries to learn a predictor to predict the class labels from this data space. Moreover, the learning of projection and class label prediction parameters are learned within a single objective function. By optimizing this objective function with regard to\nboth projection and class label prediction parameters jointly, the class label information is used to guild the learning of CFA parameters. The experiment results show that the supervised CFA outperforms both linear and kernel versions of CFA without considering class label information. Our method can also be used in other applications such as image representation [18], malware detection [19], [20], [21], and bioinformatics [22], [23], [24]."}], "references": [{"title": "A generic neural network for multi-modal sensorimotor learning", "author": ["F. Carenzi", "P. Bendahan", "V. Roschin", "A. Frolov", "P. Gorce", "M. Maier"], "venue": "Neurocomputing, vol. 58-60, pp. 525\u2013533, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "An advanced multi-modal method for human authentication featuring biometrics data and tokenised random numbers", "author": ["A. Lumini", "L. Nanni"], "venue": "Neurocomputing, vol. 69, no. 13-15, pp. 1706\u20131710, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamic sampling-based interpolation algorithm for representation of clickable moving object in collaborative video annotation", "author": ["K.-S. Lee", "A. Nurzid Rosli", "I. Ariesthea Supandi", "G.-S. Jo"], "venue": "Neurocomputing, vol. 146, pp. 291\u2013300, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Classification of geological structure using ground penetrating radar and laplace transform artificial neural networks", "author": ["P. Szymczyk", "M. Szymczyk"], "venue": "Neurocomputing, vol. 148, pp. 354\u2013362, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Text classification with self-organizing maps: Some lessons learned", "author": ["D. Merkl"], "venue": "Neurocomputing, vol. 21, no. 1-3, pp. 61\u201377, 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Boosting na\u0131\u0308ve bayes text classification using uncertainty-based selective sampling", "author": ["H.-J. Kim", "J.-U. Kim", "Y.-G. Ra"], "venue": "Neurocomputing, vol. 67, no. 1-4 SUPPL., pp. 403\u2013410, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Multimodal representation, indexing, automated annotation and retrieval of image collections via non-negative matrix factorization", "author": ["J. Caicedo", "J. BenAbdallah", "F. Gonz\u00e1lez", "O. Nasraoui"], "venue": "Neurocomputing, vol. 76, no. 1, pp. 50\u201360, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Hypergraph-based multi-example ranking with sparse representation for transductive learning image retrieval", "author": ["C. Hong", "J. Zhu"], "venue": "Neurocomputing, vol. 101, pp. 94\u2013103, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "On the role of correlation and abstraction in cross-modal multimedia retrieval", "author": ["J. Costa Pereira", "E. Coviello", "G. Doyle", "N. Rasiwasia", "G. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 36, no. 3, pp. 521\u2013535, March 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Continuum regression for cross-modal multimedia retrieval", "author": ["Y. Chen", "L. Wang", "W. Wang", "Z. Zhang"], "venue": "2012 19th IEEE International Conference on Image Processing (ICIP 2012), 2012, pp. 1949 \u2013 52.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimedia content processing through cross-modal association", "author": ["D. Li", "N. Dimitrova", "M. Li", "I.K. Sethi"], "venue": "Proceedings of the eleventh ACM international conference on Multimedia. ACM, 2003, pp. 604\u2013611.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Kernel cross-modal factor analysis for multimodal information fusion", "author": ["Y. Wang", "L. Guan", "A.N. Venetsanopoulos"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 2384\u20132387.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Singular value decomposition based minutiae matching method for finger vein recognition", "author": ["F. Liu", "G. Yang", "Y. Yin", "S. Wang"], "venue": "Neurocomputing, vol. 145, pp. 75\u201389, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Denoising of 3d magnetic resonance images by using higher-order singular value decomposition", "author": ["X. Zhang", "Z. Xu", "N. Jia", "W. Yang", "Q. Feng", "W. Chen", "Y. Feng"], "venue": "Medical Image Analysis, vol. 19, no. 1, pp. 75\u201386, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Finite time dual neural networks with a tunable activation function for solving quadratic programming problems and its application", "author": ["P. Miao", "Y. Shen", "X. Xia"], "venue": "Neurocomputing, vol. 143, pp. 80\u201389, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A dynamic programming heuristic for the quadratic knapsack problem", "author": ["F. Fomeni", "A. Letchford"], "venue": "INFORMS Journal on Computing, vol. 26, no. 1, pp. 173\u2013182, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Tvgraz: Multi-modal learning of object categories by combining textual and visual features", "author": ["I. Khan", "A. Saffari", "H. Bischof"], "venue": "AAPR Workshop, 2009, pp. 213\u2013224.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "An effective image representation method using kernel classification", "author": ["H. Wang", "J. Wang"], "venue": "2014 IEEE 26th International Conference on Tools with Artificial Intelligence (ICTAI 2014), 2014, pp. 853\u2013858.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "An evasion and counter-evasion study in malicious websites detection", "author": ["L. Xu", "Z. Zhan", "S. Xu", "K. Ye"], "venue": "2014 IEEE Conference on Communications and Network Security (CNS), 2014, pp. 265\u2013273.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Characterizing honeypot-captured cyber attacks: Statistical framework and case study", "author": ["Z. Zhan", "M. Xu", "S. Xu"], "venue": "IEEE Transactions on Information Forensics and Security, vol. 8, no. 11, pp. 1775\u20131789, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-layer detection of malicious websites", "author": ["L. Xu", "Z. Zhan", "S. Xu", "K. Ye"], "venue": "Proceedings of the third ACM conference on Data and application security and privacy, 2013, pp. 141\u2013152.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Computational modeling of magnetic nanoparticle targeting to stent surface under high gradient field", "author": ["S. Wang", "Y. Zhou", "J. Tan", "J. Xu", "J. Yang", "Y. Liu"], "venue": "Computational mechanics, vol. 53, no. 3, pp. 403\u2013412, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Structure design of vascular stents", "author": ["Y. Liu", "J. Yang", "Y. Zhou", "J. Hu"], "venue": "Multiscale Simulations and Mechanics of Biological Materials, pp. 301\u2013 317, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Biomarker binding on an antibody-functionalized biosensor surface: The influence of surface properties, electric field, and coating density", "author": ["Y. Zhou", "W. Hu", "B. Peng", "Y. Liu"], "venue": "The Journal of Physical Chemistry C, vol. 118, no. 26, pp. 14 586\u201314 594, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION In this paper, we deal with the problem of learning from multiple modal data [1], [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "INTRODUCTION In this paper, we deal with the problem of learning from multiple modal data [1], [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "Traditional data representation, classification and retrieval problems usually focus on single modal data [3], [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 3, "context": "Traditional data representation, classification and retrieval problems usually focus on single modal data [3], [4].", "startOffset": 111, "endOffset": 114}, {"referenceID": 4, "context": "For example, for the problem of text classification, we usually only consider using a data set of text to train a classifier [5], [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "For example, for the problem of text classification, we usually only consider using a data set of text to train a classifier [5], [6].", "startOffset": 130, "endOffset": 133}, {"referenceID": 6, "context": "While for the problem of image representation, only the images are considered to learn the representation parameters [7], [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "While for the problem of image representation, only the images are considered to learn the representation parameters [7], [8].", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": "Learning from these multiple modal data has attracted much attention from both machine learning and multimedia information processing communities [9], [10].", "startOffset": 146, "endOffset": 149}, {"referenceID": 9, "context": "Learning from these multiple modal data has attracted much attention from both machine learning and multimedia information processing communities [9], [10].", "startOffset": 151, "endOffset": 155}, {"referenceID": 10, "context": "Recently, cross-modal factor analysis (CFA) has been proposed to project different modal of data to a shared feature space so that classification or retrieval can be performed cross data modal [11].", "startOffset": 193, "endOffset": 197}, {"referenceID": 11, "context": "CFA is further extended to its kernel version in [12].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "The orthonormal transformation matrices are optimized by fixing class label predictor parameter matrix and solving a singular value decomposition (SVD) problem [13], [14].", "startOffset": 160, "endOffset": 164}, {"referenceID": 13, "context": "The orthonormal transformation matrices are optimized by fixing class label predictor parameter matrix and solving a singular value decomposition (SVD) problem [13], [14].", "startOffset": 166, "endOffset": 170}, {"referenceID": 14, "context": "[15], [16] The rest parts of this paper are organized as follows: in section II we introduce the proposed supervised CFA (SupCFA), in section III the proposed method is evaluated experimentally, and in section IV, the paper is concluded.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[15], [16] The rest parts of this paper are organized as follows: in section II we introduce the proposed supervised CFA (SupCFA), in section III the proposed method is evaluated experimentally, and in section IV, the paper is concluded.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "The first data set is the TVGraz database [17], which is a multimodal database of object categories composed of textual and visual features.", "startOffset": 42, "endOffset": 46}, {"referenceID": 10, "context": "We considered the original CFA method [11] and its kernel version (CFAker) [12] as data representation methods, and used a SVM as a classifier.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "We considered the original CFA method [11] and its kernel version (CFAker) [12] as data representation methods, and used a SVM as a classifier.", "startOffset": 75, "endOffset": 79}], "year": 2017, "abstractText": "In this paper we study the problem of learning from multiple modal data for purpose of document classification. In this problem, each document is composed two different modals of data, i.e., an image and a text. Cross-modal factor analysis (CFA) has been proposed to project the two different modals of data to a shared data space, so that the classification of a image or a text can be performed directly in this space. A disadvantage of CFA is that it has ignored the supervision information. In this paper, we improve CFA by incorporating the supervision information to represent and classify both image and text modals of documents. We project both image and text data to a shared data space by factor analysis, and then train a class label predictor in the shared space to use the class label information. The factor analysis parameter and the predictor parameter are learned jointly by solving one single objective function. With this objective function, we minimize the distance between the projections of image and text of the same document, and the classification error of the projection measured by hinge loss function. The objective function is optimized by an alternate optimization strategy in an iterative algorithm. Experiments in two different multiple modal document data sets show the advantage of the proposed algorithm over other CFA methods.", "creator": "LaTeX with hyperref package"}}}