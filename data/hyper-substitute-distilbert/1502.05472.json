{"id": "1502.05472", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2015", "title": "On the Effects of Low-Quality Training Data on Information Extraction from Clinical Reports", "abstract": "since past last five reports there he been a amount of work on input extraction, clinical surveys, i. de., basic algorithms capable of extracting, from other databases and descriptive texts that regularly generated during everyday medicinal practice, assessment of concepts relevant between such practice. most of anything way suggests about methods based nonlinear functional learning, i. e., methods for training an algorithm extraction system besides consistently annotated examples. actually a lot and work has been devoted to devising actual methods they simulate more consistently more accurate information extractors, no work involved getting spent to investigating the performance of the quality of training data on the practice stream. higher variance in training data often derives from mere fact that the person who has conceived the system heavily derived from the one against whose consistency underlying automatically applied components must remain evaluated. continuing her paper we test the impact of patient data quality issues supporting the achievement of information extraction systems as testified to the clinical analyst. we often proceed by tracking the accuracy deriving from training components annotated by the specific coder ( yahoo. com., the one you usually also annotated supervised testing data, and method setting judgment exercises consistently produce ), with sufficient accuracy deriving from training data annotated by each numerical model. the weights indicate that, thus the disagreement whether the two coders ( as reflected on global training profile ) stands substantial, zero difference is ( legally enough ) already solely statistically significant.", "histories": [["v1", "Thu, 19 Feb 2015 06:04:40 GMT  (911kb)", "https://arxiv.org/abs/1502.05472v1", "12 figures"], ["v2", "Wed, 4 Mar 2015 08:08:49 GMT  (933kb)", "http://arxiv.org/abs/1502.05472v2", "Submitted for publication"]], "COMMENTS": "12 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["diego marcheggiani", "fabrizio sebastiani"], "accepted": false, "id": "1502.05472"}, "pdf": {"name": "1502.05472.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["FABRIZIO SEBASTIANI"], "emails": ["permissions@acm.org."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n05 47\n2v 2\n[ cs\n.L G\n] 4\nM ar\n2 01\n5\nAA:1\nAuthor\u2019s addresses: Diego Marcheggiani, Istituto di Scienza e Tecnologie dell\u2019Informazione, Consiglio Nazionale delle Ricerche, 56124 Pisa, Italy; Fabrizio Sebastiani, Qatar Computing Research Institute, Qatar Foundation, PO Box 5825, Doha, Qatar. Fabrizio Sebastiani is currently on leave from Consiglio Nazionale delle Ricerche, Italy. The order in which the authors are listed is purely alphabetical; each author has given an equally important contribution to this work. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c\u00a9 YYYY ACM 1936-1955/YYYY/01-ARTA $15.00 DOI:http://dx.doi.org/10.1145/0000000.0000000\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY."}, {"heading": "On the Effects of Low-Quality Training Data on Information Extraction from Clinical Reports", "text": "DIEGO MARCHEGGIANI, Consiglio Nazionale delle Ricerche FABRIZIO SEBASTIANI, Qatar Computing Research Institute\nIn the last five years there has been a flurry of work on information extraction from clinical documents, i.e.,\non algorithms capable of extracting, from the informal and unstructured texts that are generated during\neveryday clinical practice, mentions of concepts relevant to such practice. Most of this literature is about\nmethods based on supervised learning, i.e., methods for training an information extraction system from\nmanually annotated examples. While a lot of work has been devoted to devising learning methods that\ngenerate more and more accurate information extractors, no work has been devoted to investigating the\neffect of the quality of training data on the learning process. Low quality in training data often derives\nfrom the fact that the person who has annotated the data is different from the one against whose judgment\nthe automatically annotated data must be evaluated. In this paper we test the impact of such data quality\nissues on the accuracy of information extraction systems as applied to the clinical domain. We do this by\ncomparing the accuracy deriving from training data annotated by the authoritative coder (i.e., the one who\nhas also annotated the test data, and by whose judgment we must abide), with the accuracy deriving from\ntraining data annotated by a different coder. The results indicate that, although the disagreement between\nthe two coders (as measured on the training set) is substantial, the difference is (surprisingly enough) not\nalways statistically significant.\nCategories and Subject Descriptors: Information systems [Information retrieval]: Retrieval tasks and goals\u2014Clustering and Classification; Computing methodologies [Machine learning]: Learning paradigms\u2014Supervised learning\nGeneral Terms: Algorithm, Design, Experimentation, Measurements\nAdditional Key Words and Phrases: Information Extraction, Annotation quality, Radiology Reports, Medical Reports, Clinical Narratives, Machine Learning\nACM Reference Format: Diego Marcheggiani and Fabrizio Sebastiani, 2015. On the Effects of Low-Quality Training Data on Information Extraction from Clinical Reports. ACM J. Data Inform. Quality V, N, Article A (January YYYY), 17 pages. DOI:http://dx.doi.org/10.1145/0000000.0000000"}, {"heading": "1. INTRODUCTION", "text": "In the last five years there has been a flurry of work (see e.g., [Kelly et al. 2014; Pradhan et al. 2014; Sun et al. 2013; Suominen et al. 2013; Uzuner et al. 2012; Uzuner et al. 2011]) on information extraction from clinical documents, i.e., on algorithms capable of extracting, from the informal and unstructured texts that are generated during everyday clinical practice (e.g., admission reports, radiological reports, discharge summaries, clinical notes), mentions of concepts relevant to such practice. Most of this literature is about methods based on supervised learning, i.e., methods for training an information extraction system from manually annotated examples. While a lot of work has been devoted to devising text representation methods and variants of the aforementioned supervised learning methods that generate more and more accurate information extractors, no work has been devoted to investigating the effects of the quality of training data on the learning process. Issues of quality in the training data may arise for different reasons:\n(1) In several organizations it is often the case that the original annotation is performed by coders (a.k.a. \u201cannotators\u201d, or \u201cassessors\u201d) as a part of a daily routine in which fast turnaround, rather than annotation quality, is the main goal of the\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\ncoders and/or of the organization. An example is the (increasingly frequent) case in which annotation is performed via crowdsourcing using instruments such as, e.g., Mechanical Turk, CrowdFlower, etc.1 [Grady and Lease 2010; Snow et al. 2008]. (2) In many organizations it is also the case that annotation work is usually carried out by junior staff (e.g., interns), since having it accomplished by senior employees would make costs soar. (3) It is often the case that the coders entrusted with the annotation work were not originally involved in designing the tagset (i.e., the set of concepts whose mentions are sought in the documents). As a result, the coders may have a suboptimal understanding of the true meaning of these concepts, or of how their mentions are meant to look like, which may negatively affect the quality of their annotation. (4) The data used for training the system may sometimes be old or outdated, with the annotations not reflecting the current meaning of the concepts anymore. This is an example of a phenomenon, called concept drift [Quin\u0303onero-Candela et al. 2009; Sammut and Harries 2011], which is well known in machine learning.\nWe may summarize all the cases mentioned above by saying that, should the training data be independently re-annotated by an authoritative coder (hereafter indicated as C\u03b1), the resulting annotations would be, to a certain extent, more reliable. We would also be able to precisely measure this difference in reliability, by measuring the intercoder agreement (via measures such as Cohen\u2019s kappa \u2013 see e.g., [Artstein and Poesio 2008; Di Eugenio and Glass 2004]) between the training data Tr as coded by C\u03b1 and the training data as coded by whoever else originally annotated them (whom we will call, for simplicity, the non-authoritative coder \u2013 hereafter indicated as C\u03b2). In the rest of this paper we will take the authoritative coder C\u03b1 to be the coder whose annotations are to be taken as correct, i.e., considered as the \u201cgold standard\u201d. As a consequence we may assume that C\u03b1 is the coder who, once the system is trained and deployed, has also the authority to evaluate the accuracy of the automatic annotation (i.e., decide which annotations are correct and which are not)2. In this case, intercoder (dis)agreement measures the amount of noise that is introduced in the training data by having them annotated by a coder C\u03b2 different from the authoritative coder C\u03b1. It is natural to expect the accuracy of an information extraction system to be lower if the training data have been annotated by a non-authoritative coder C\u03b2 , and higher if they have been annotated by C\u03b1 herself. However, note that this is not a consequence of the fact that C\u03b1 is more experienced, or senior, or reliable, than C\u03b2 . Rather, it is a consequence of the fact that standard supervised learning algorithms are based on the assumption that the training set and the test set are identically and independently distributed (the so-called i.i.d. assumption), i.e., that both sets are randomly drawn from the same distribution. As a result, these algorithms learn to replicate the subjective annotation style of their supervisors, i.e., of those who have annotated the training data. This means that we may expect accuracy to be higher simply when the coder of the training set and the coder of the test set are the same person, and to be lower when the two coders are different, irrespective of how experienced, or senior, or reliable, they are. In other words, the very fact that a coder is entrusted with the task of evaluating the automatic annotations (i.e., of annotating the test set) makes this coder authoritative by definition. For this reason, the authoritative coder C\u03b1 may equivalently be defined as \u201cthe coder who has annotated the test set\u201d (or: \u201cthe coder\n1https://www.mturk.com/, http://crowdflower.com/ 2In some organizations this authoritative coder may well be a fictional entity, e.g., several coders may be equally experienced and thus equally authoritative. However, without loss of generality we will hereafter assume that C\u03b1 exists and is unique.\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\nwhose judgments we adhere to when evaluating the accuracy of the system\u201d), and the non-authoritative coder C\u03b2 may equivalently be defined as \u201ca coder different from the authoritative coder\u201d. The above arguments point to the fact that the impact of training data quality \u2013 under its many facets discussed in items (1)-(4) above \u2013 on the accuracy of information extraction systems may be measured by\n(1) evaluating the accuracy of the system in an authoritative setting (i.e., both training and test sets annotated by the authoritative coder C\u03b1), and then (2) evaluating the loss in accuracy, with respect to the authoritative setting, that derives from working instead in a non-authoritative setting (i.e., test set annotated by C\u03b1 and training set annotated by a non-authoritative coder C\u03b2) 3."}, {"heading": "1.1. Our contribution", "text": "In this paper we test the impact of training data quality on the accuracy of information extraction systems as applied to the clinical domain. We do this by testing the accuracy of two state-of the-art systems on a dataset of radiology reports (originally discussed in [Esuli et al. 2013]) in which a portion of the data has independently been annotated by two different experts. In other words, we try to answer the question: \u201cWhat is the consequence of the fact that my training data are not sterling quality? that the coders who produced them are not entirely dependable? Howmuch am I going to lose in terms of accuracy of the trained system?\u201d In these experiments we not only test the \u201cpure\u201d authoritative and non-authoritative settings described above, but we also test partially authoritative settings, in which increasingly large portions of the training data as annotated by C\u03b1 are replaced with the corresponding portions as annotated by C\u03b2 , thus simulating the presence of incrementally higher amounts of noise. For each setting we compute the intercoder agreement between the two training sets; this allows us to study the relative loss in extraction accuracy as a function of the agreement between authoritative and non-authoritative assessor as measured on the training set. Since in many practical situations it is easy to compute (or estimate) the intercoder disagreement between (a) the coder to whom we would ideally entrust the annotation task (e.g., a senior expert in the organization), and (b) the coder to whomwe can indeed entrust it given time and cost constraints (e.g., a junior member of staff), this will give the reader a sense of how much intercoder disagreement generates how much loss in extraction accuracy. The rest of the paper is organized as follows. Section 2 reviews related work on information extraction from clinical documents, and on establishing the relations between training data quality and extraction accuracy. In Sections 3 and 4 we describe experiments that attempt to quantify the degradation in extraction accuracy that derives from low-quality training data, with Section 3 devoted to spelling out the experimental setting and Section 4 devoted instead to presenting and discussing the results. Section 5 concludes, discussing avenues for further research.\n3In the domain of classification the authoritative and non-authoritative settings have also been called selfclassification and cross-classification, respectively [Webber and Pickens 2013]. We depart from this terminology in order to avoid any confusion with self-learning (which refers to retraining a classifier by using, as additional training examples, examples the classifier itself has classified) and cross-lingual classification (which denotes a variant of text classification which exploits synergies between training data expressed in different languages).\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY."}, {"heading": "2. RELATED WORK", "text": ""}, {"heading": "2.1. Information extraction from clinical documents", "text": "Most works on information extraction from clinical documents rely on methods based on supervised learning, i.e., methods for training an information extraction system from manually annotated examples. Support vector machines (SVMs \u2013 [Jiang et al. 2011; Li et al. 2008; Sibanda et al. 2006]), hidden Markov models (HMMs \u2013 [Li et al. 2010]), and (especially) conditional random fields (CRFs \u2013 [Esuli et al. 2013; Gupta et al. 2014; Jiang et al. 2011; Jonnalagadda et al. 2012; Li et al. 2008; Patrick and Li 2010; Torii et al. 2011; Wang and Patrick 2009]) have been the learners of choice in this field, due to their good performance and to the existence of publicly available implementations. In recent years, research on the analysis of clinical texts has been further boosted by the existence of \u201cshared tasks\u201d on this topic, such as the seminal i2b2 series (\u201cInformatics for Integrating Biology and the Bedside\u201d \u2013 [Sun et al. 2013; Uzuner et al. 2012; Uzuner et al. 2011]), the 2013 [Suominen et al. 2013] and 2014 [Kelly et al. 2014] editions of ShARe/CLEF eHealth, and the Semeval-2014 Task 7 \u201cAnalysis of Clinical Text\u201d [Pradhan et al. 2014]. In these shared tasks the goal is to competitively evaluate information extraction tools that recognise mentions of various concepts of interest (e.g., mentions of diseases and disorders) as appearing in discharge summaries, and in electrocardiogram reports, echocardiograph reports, and radiology reports."}, {"heading": "2.2. Low-quality training data and prediction accuracy", "text": "The literature on the effects of suboptimal training data quality on prediction accuracy is extremely scarce, even within the machine learning literature at large. An early such study is [Rossin and Klein 1999], which looks at these issues in the context of learning to predict prices of mutual funds from economic indicators. Differently from us, the authors work with noise artificially inserted in the training set, and not with naturally occurring noise. From experiments run with a linear regression model they reach the bizarre conclusion that \u201cthe predictive accuracy (...) is better when errors exist in training data than when training data are free of errors\u201d, while the opposite conclusion is (somehow more expectedly) reached from experiments run with a neural networks model. A similar study, in which the context is predicting the average air temperature in distributed heating systems, was carried out in [Jassar et al. 2009]; yet another study, in which the goal was predicting the production levels of palm oil via a neural network, is [Khamis et al. 2005]. In the context of a biomedical information extraction task4 Haddow and Alex [2008] examined the situation in which training data annotated by two different coders are available, and they found that higher accuracy is obtained by using both versions at the same time than by attempting to reconcile them or using just one of them. Their use case is different from ours, since in the case we discuss we assume that only one set of annotations, those of the non-authoritative coder, are available as training data. Note also that training data independently annotated by more than one coder are rarely available in practice. Closer to our application context, Esuli and Sebastiani [2013] have thoroughly studied the effect of suboptimal training data quality in text classification. However, in their case the degradation in the quality of the training data is obtained, for mere experimental purposes, via the insertion of artificial noise, due to the fact that their datasets did not contain data annotated by more than one coder. As a result, it is\n4Biomedical IE is different from clinical IE, in that the latter (unlike the former) is usually characterized by idiosyncratic abbreviations, ungrammatical sentences, and sloppy language in general. See [Meystre et al. 2008, p. 129] for a discussion of this point.\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\nnot clear how well the type of noise they introduce models naturally occurring noise. Webber and Pickens [2013] also address the text classification task (in the context of ediscovery from legal texts), but differently from [Esuli and Sebastiani 2013] they work with naturally occurring noise; differently from the present work, the multiply-coded training data they use were coded by one coder known to be an expert coder and another coder known to be a junior coder. Our work instead (a) focuses on information extraction, and (2) does not make any assumption on the relative level of expertise of the two coders."}, {"heading": "3. EXPERIMENTAL SETTING", "text": ""}, {"heading": "3.1. Basic notation and terminology", "text": "Let us fix some basic notation and terminology. Let X be a set of texts, where we view each text x \u2208 X as a sequence x = \u3008x1, . . . , x|x|\u3009 of textual units (or simply t-units), such that odd-numbered t-units are tokens (i.e., word occurrences) and even-numbered tunits are separators (i.e., sequences of blanks and punctuation symbols), and such that xt1 occurs before xt2 in the text (noted xt1 xt2 ) if and only if t1 \u2264 t2. We dub |x| the length of the text. Let C = {c1, . . . , cm} be a predefined set of concepts (a.k.a. tags, or markables), or tagset. We take information extraction (IE) to be the task of determining, for each x \u2208 X and for each cr \u2208 C, a sequence yr = \u3008yr1, . . . , yr|x|\u3009 of labels yrt \u2208 {cr, cr}, which indicates which t-units in the text are labelled with tag cr and which are not. Since each cr \u2208 C is dealt with independently of the other concepts in C, we hereafter drop the r subscript and, without loss of generality, treat IE as the binary task of determining, given text x and concept c, a sequence y = \u3008y1, . . . , y|x|\u3009 of labels yt \u2208 {c, c}. T-units labelled with a concept c usually come in coherent sequences, or \u201cmentions\u201d. Hereafter, amention \u03c3 of text x for concept cwill be a pair (xt1 , xt2) consisting of a start token xt1 and an end token xt2 such that (i) xt1 xt2 , (ii) all t-units xt1 xt xt2 are labelled with concept c, and (iii) the token that immediately precedes xt1 and the one that immediately follows xt2 are not labelled with concept c. In general, a text x may contain zero, one, or several mentions for concept c. In the above definitions we consider separators to be also the object of tagging in order for the IE system to correctly identify consecutive mentions. For instance, given the expression \u201cBarack Obama, Hillary Clinton\u201d the perfect IE system will attribute the PersonName tag to the tokens \u201cBarack\u201d, \u201cObama\u201d, \u201cHillary\u201d, \u201cClinton\u201d, and to the separators (in this case: blank spaces) between \u201cBarack\u201d and \u201cObama\u201d and between \u201cHillary\u201d and \u201cClinton\u201d, but not to the separator \u201c, \u201d between \u201cObama\u201d and \u201cHillary\u201d. If the IE system does so, this means that it has correctly identified the boundaries of the two mentions \u201cBarack Obama\u201d and \u201cHillary Clinton\u201d5."}, {"heading": "3.2. Dataset", "text": "The dataset we have used to test the ideas discussed in the previous sections is the UmbertoI(RadRep) dataset first presented in [Esuli et al. 2013], consisting of a set of 500 free-text mammography reports written (in Italian) by medical personnel of the\n5Note that the above notation is not able to represent \u201cdiscontiguous mentions\u201d, i.e., mentions containing gaps, and \u201coverlapping mentions\u201d, i.e., multiple mentions sharing one or more tokens. This is not a serious limitation for our research, since the above notation can be easily extended to deal with both phenomena (e.g., by introducing unique mention identifiers and having each t-unit be associated with zero, one, or several such identifiers), and since the dataset we use for our experimentation contains neither discontinuous nor overlapping mentions. We prefer to keep the notation simple, since the issue we focus on in this paper (the consequences on extraction accuracy of suboptimal training data quality) can be considered largely independent of the expressive power of the markup language.\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\nIstituto di Radiologia of Policlinico Umberto I, Roma, IT. The dataset is annotated according to 9 concepts relevant to the field of radiology and mammography: BIR (\u201cOutcome of the BIRADS test\u201d), ITE (\u201cTechnical Info\u201d), IES (\u201cIndications obtained from the Exam\u201d), TFU (\u201cFollowup Therapies\u201d), DEE (\u201cDescription of Enhancement\u201d), PAE (\u201cPresence/Absence of Enhancements\u201d), ECH (\u201cOutcomes of Surgery\u201d), DEP (\u201cProsthesis Description\u201d), and LLO (\u201cLocoregional Lymph Nodes\u201d). Note that we had no control on the design of the concept set, on its range, and on its granularity, since the choice of the concepts was entirely under the responsibility of Policlinico Umberto I. We thus take both the concept set and the dataset as given. Mentions of these concepts are present in the reports according to fairly irregular patterns. In particular, a given concept (a) need not be instantiated in all reports, and (b) may be instantiated more than once (i.e., by more than one mention) in the same report. Mentions instantiating different concepts may overlap, and the order of presentation of the different concepts varies across the reports. On average, there are 0.87 mentions for each concept in a given report, and the average mention length is 17.33 words. The reports were annotated by two equally expert radiologists, Coder1 and Coder2; 191 reports were annotated by Coder1 only, 190 reports were annotated by Coder2 only, and 119 reports were annotated independently by Coder1 and Coder2. From now on we will call these sets 1-only, 2-only and Both, respectively; Both(1) will identify the Both set as annotated by Coder1, and Both(2)will identify the Both set as annotated by Coder2. The annotation activity was preceded by an alignment phase, in which Coder1 and Coder2 jointly annotated 20 reports (not included in this dataset) in order to align their understanding of the meaning of the concepts. Table I reports the distribution of annotations across concepts, at token and mention level, for the two coders; see [Esuli et al. 2013, Section 4.2] for a more detailed description of the UmbertoI(RadRep) dataset that includes additional stats6."}, {"heading": "3.3. Learning algorithms", "text": "As the learning algorithms we have tested both linear-chain conditional random fields (LC-CRFs - [Lafferty et al. 2001; Sutton and McCallum 2007; Sutton and McCallum 2012]), in Charles Sutton\u2019s GRMM implementation7, and hidden Markov support vector machines (HM-SVMs - [Altun et al. 2003]), in Thorsten Joachims\u2019s SVMhmm implementation8. Both are supervised learning algorithms explicitly devised for sequence labelling, i.e., for learning to label (i.e., to annotate) items that naturally occur in sequences and such that the label of an item may depend on the features and/or on the labels of other items that precede or follow it in the sequence (which is indeed the case for the tokens in a text)9. LC-CRFs are members of the class\n6No other dataset is used in this paper since we were not able to locate another dataset of annotated clinical texts that contains reports annotated by more than one coder and is at the same time publicly available. 7http://mallet.cs.umass.edu/grmm/ 8http://www.cs.cornell.edu/people/tj/svm light/svm hmm.html 9Note that only tokens, and not separators, are explicitly labelled. The reason is that both LC-CRFs and HM-SVMs actually use the so-called IOB labelling scheme, according to which, for each concept cr \u2208 C, a\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\nof graphical models, a family of probability distributions that factorize according to an underlying graph [Wainwright and Jordan 2008]; see [Sutton and McCallum 2012] for a full mathematical explanation of LC-CRFs. HM-SVMs are an instantiation of \u201cSVMs for structured output prediction\u201d (SVM struct) [Tsochantaridis et al. 2005] for the sequence labelling task, and have already been used in clinical information extraction (see e.g., [Tang et al. 2012; Zhang et al. 2014]). In HM-SVMs the learning procedure is based on a large-margin approach typical of SVMs, which, differently from LC-CRFs, can learn non-linear discriminant functions via kernel functions. Both learners need each token xt to be represented by a vector xt of features\n10. In this work we have used a set of features which includes one feature representing the word of which the token is an instance, one feature representing its stem, one feature representing its part of speech, eight features representing its prefixes and suffixes (the first and the last n characters of the token, with n = 1, 2, 3, 4), one feature representing information on token capitalization (i.e., whether the token is all uppercase, all lowercase, first letter uppercase, or mixed case), and 4 \u201cpositional\u201d features [Esuli et al. 2013, Section 3.3] that indicate in which half, 3rd, 4th, or 5th, respectively, of the text the token occurs in."}, {"heading": "3.4. Evaluation measures", "text": "3.4.1. Classification accuracy. As a measure of classification accuracy we use, similarly to [Esuli et al. 2013], the token-and-separator variant (proposed in [Esuli and Sebastiani 2010]) of the well-known F1 measure, according to which an information extraction system is evaluated on an event space consisting of all the t-units in the text. In other words, each t-unit xt (rather than each mention, as in the traditional \u201csegmentation F-score\u201d model [Suzuki et al. 2006]) counts as a true positive, true negative, false positive, or false negative for a given concept cr, depending on whether xt belongs to cr or not in the predicted annotation and in the true annotation. This model has the advantage that it credits a system for partial success (i.e., degree of overlap between a predicted mention and a true mention for the same concept), and that it penalizes both overannotation and underannotation. As is well-known, F1 is the harmonic mean of precision (\u03c0 = TP TP+FN ) and recall (\u03c1 = TP TP+FP ), and is defined as\nF1 = 2\u03c0\u03c1\n\u03c0 + \u03c1 =\n2 \u00b7 TP\nTP + FN \u00b7\nTP\nTP + FP TP\nTP + FN +\nTP\nTP + FP\n= 2TP\n2TP + FP + FN (1)\nwhere TP , FP , and FN stand for the numbers of true positives, false positives, and false negatives, respectively. It is easy to observe that F1 is equivalent to TP divided by the arithmetic mean of the actual positives and the predicted positives (or, alternatively, the product of \u03c0 and \u03c1 divided by their arithmetic mean). Note that F1 is undefined when TP = FP = FN = 0; in this case we take F1 to equal 1, since the system has correctly annotated all t-units as negative.\ntoken can be labelled as Br (the beginning token of a mention of cr), Ir (a token which is inside a mention of cr but is not its beginning token), and Or (a token that is outside any mention of cr). As a result, a separator is (implicitly) labelled with concept cr if and only if it precedes a token labelled with Ir. We may think of the notation of Section 3.1 as an abstract markup language, and of the IOB notation as a concrete markup language, in the sense that the notation of Section 3.1 is easier to understand (and will also make the evaluation measure discussed in Section 3.4.1 easier to understand) while IOB is actually used by the learning algorithms. The two notations are equivalent in expressive power. 10Note that only tokens, and not separators, are explicitly represented in vectorial form, the reasons being the same already discussed in Footnote 9.\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\nWe compute F1 across the entire test set, i.e., we generate a single contingency table by putting together all t-units in the test set, irrespectively of the document they belong to. We then compute both microaveraged F1 (denoted by F \u00b5 1 ) and macroaveraged F1 (F M 1 ). F \u00b5 1 is obtained by (i) computing the concept-specific values TPr, FPr and FNr, (ii) obtaining TP as the sum of the TPr \u2019s (same for FP and FN ), and then (iii) applying Equation 1. FM1 is obtained by first computing the concept-specific F1 values and then averaging them across the cr \u2019s.\n3.4.2. Intercoder agreement. Intercoder agreement (ICA), or the lack thereof (intercoder disagreement), has been widely studied for over a century (see e.g., [Krippendorff 2004] for an introduction). As a phenomenon, disagreement among coders naturally occurs when units of content need to be annotated by humans according to their semantics (i.e., when the occurrences of specific concepts need to be recognized within these units of content). Such disagreement derives from the fact that semantic content is a highly subjective notion: different coders might disagree with each other as to what the semantics of, say, a given piece of text is, and it is even the case that the same coder might at times disagree with herself (i.e., return different codes when coding the same unit of content at different times). ICA may be measured by the relative frequency of the units of content on which coders agree, usually normalized by the probability of chance agreement.Manymetrics for ICA have been proposed over the years, \u201cCohen\u2019s kappa\u201d probably being the most famous and widely used (\u201cScott\u2019s pi\u201d and \u201cKrippendorff \u2019s alpha\u201d are others); sometimes (see e.g., [Chapman and Dowling 2006; Esuli et al. 2013]) functions that were not explicitly developed for measuring ICA (such as F1, that was developed for measuring binary classification accuracy) are used. The levels of ICA that are recorded in actual experiments vary a lot across experiments, types of content, and types of concepts that are to be recognized in the units of content under investigation. This extreme variance depends on factors such as \u201cannotation domain, number of categories in a coding scheme, number of annotators in a project, whether annotators received training, the intensity of annotator training, the annotation purpose, and the method used for the calculation of percentage agreements\u201d [Bayerl and Paul 2011]. The actual meaning of the concepts the coders are asked to recognize is a factor of special importance, to the extent that a concept on which very low levels of ICA are reached may be deemed, because of this very fact, ill-defined. For measuring intercoder agreement we use Cohen\u2019s kappa (noted \u03ba), defined as\n\u03ba = P (A)\u2212 P (E)\n1\u2212 P (E) (2)\n= (P (p = t = c) + P (p = t = c))\u2212 (P (p = c)P (t = c) + P (p = c)P (t = c))\n1\u2212 (P (p = c)P (t = c) + P (p = c)P (t = c))\n=\nTP + TN\nn \u2212 ((\nTP + FP n )( TP + FN n ) + ( FN + TN n )( FP + TN n ))\n1\u2212 (( TP + FP n )( TP + FN n ) + ( FN + TN n )( FP + TN n ))\nwhere P (A) denotes the probability (i.e., relative frequency) of agreement, P (E) denotes the probability of chance agreement, and n is the total number of examples (see [Artstein and Poesio 2008; Di Eugenio and Glass 2004] for details); here, we use the shorthand p = c (resp., t = c) to mean that the predicted label (resp., true label) is c (analogously for c). We opt for kappa since it is the most widely known, and best understood, measure of ICA. For Cohen\u2019s kappa too we work at the t-unit level, i.e., for\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\neach t-unit xt we record whether the two coders agree on whether xt is labelled or not with the concept c of interest. Incidentally, note that (as observed in [Esuli and Sebastiani 2010]) we can compute Cohen\u2019s kappa only thanks to the fact that (as discussed in Section 3.4.1) we conduct our evaluation at the t-unit level (rather at the mention level). Those who conduct their evaluation at the mention level (e.g., [Chapman and Dowling 2006]) find that they are unable to do so, since in order to be defined kappa needs the notion of a true negative to be also defined, and this is undefined at the mention level. Evaluation at the mention level thus prevents the use of kappa and leaves F1 as the only choice."}, {"heading": "4. RESULTS", "text": ""}, {"heading": "4.1. Experimental protocol", "text": "In [Esuli et al. 2013], experiments on the UmbertoI(RadRep) dataset were run using either 1-only and/or 2-only (i.e., the portions of the data that only one coder had annotated) as training data and Both(1) and/or Both(2) (i.e., the portion of the data that both coders had annotated, in both versions) as test data. In this paper we switch the roles of training set and test set, i.e., use Both(1) or Both(2) as training set (since for the purpose of this paper we need training data with multiple, alternative annotations) and 1-only or 2-only as test set. Specifically, we run two batches of experiments, Batch1 and Batch2. In Batch1 Coder1 plays the role of the authoritative coder (C\u03b1) and Coder2 plays the role of the non-authoritative coder (C\u03b2), while in Batch2 Coder2 plays the role of C\u03b1 and Coder1 plays the role of C\u03b2 . Each of the two batches of experiments is composed of:\n(1) An experiment using the authoritative setting, i.e., both training and test data are annotated by C\u03b1. This means training on Both(1) and testing on 1-only (Batch1) and training on Both(2) and testing on 2-only (Batch2). (2) An experiment using the non-authoritative setting, i.e., training data annotated by C\u03b2 and test data annotated by C\u03b1. This means training on Both(2) and testing on 1-only (Batch1) and training on Both(1) and testing on 2-only (Batch2). (3) Experiments using the partially authoritative setting, i.e., test data annotated by C\u03b1, and training data annotated in part by C\u03b2 (\u03bb% of the training documents, chosen at random) and in part by C\u03b1 (the remaining (100 \u2212 \u03bb)% of the training documents). We call \u03bb the corruption ratio of the training set; \u03bb = 0 obviously corresponds to the fully authoritative setting while \u03bb = 100 corresponds to the non-authoritative setting. We run experiments for each \u03bb \u2208 {10, 20, ..., 80, 90} by monotonically adding, for increasing values of \u03bb, new randomly chosen elements (10% at a time) to the set of training documents annotated by C\u03b2 . Since the choice of training data annotated by C\u03b2 is random, we repeat the experiment 10 times for each value of \u03bb \u2208 {10, 20, ..., 80, 90}, each time with a different random such choice.\nFor each of the above train-and test experiment we compute the intercoder agreement \u03ba(Tr, corr\u03bb(Tr)) between the non-corrupted version of the training set Tr and the (partially or fully) corrupted version corr\u03bb(Tr) for a given value of \u03bb. We then take the average among the 10 values of \u03ba(Tr, corr\u03bb(Tr)) deriving from the 10 different experiments run for a given value of \u03bb and denote it as \u03ba(\u03bb); this value indicates the average intercoder agreement that derives by \u201ccorrupting\u201d \u03bb% of the documents in the training set, i.e., by using for them the annotations performed by the non-authoritative coder. For each of the above train-and test experiment we also compute the extraction accuracy (via both F\u00b51 and F M 1 ) and the relative loss in extraction accuracy that results from the given corruption ratio.\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY."}, {"heading": "4.2. Results and discussion", "text": "Table II reports extraction accuracy figures for the authoritative and non-authoritative settings, for both learners, both batches of experiments, and along with the resulting intercoder agreement values. Figure 1 illustrates the results of our experiments by plotting F1 as a function of the corruption ratio \u03bb, using LC-CRFs and HM-SVMs as the learning algorithm, respectively; for each value of \u03bb, the corresponding level of interannotator agreement \u03ba(\u03bb) (as averaged across the two batches) is also indicated. Figure 2 plots instead precision and recall as a function of \u03bb for the LC-CRFs experiments, while Figure 3 does the same for the HM-SVMs experiments.\n4.2.1. Macroaveraged values are lower than microaveraged ones. A first fact to be observed is that macroaveraged (FM1 ) results are always lower than the corresponding microaveraged (F\u00b51 ) results. This is unsurprising, and conforms to a well-known pattern. In fact, microaveraged effectiveness scores are heavily influenced by the accuracy obtained on the concepts most frequent in the test set (i.e., on the ones that label many test tunits); for these concepts accuracy tends to be higher, since these concepts also tend to be more frequent in the training set. Conversely, in macroaveraged effectiveness measures, each concept counts the same, which means that the low-frequency concepts (which tend to be the low-performing ones too) have as much of an impact as the high-frequency ones. See [Debole and Sebastiani 2005, pp. 591\u2013593] for a thorough discussion of this point in a text classification context.\n4.2.2. HM-SVMs outperform LC-CRFs. A second fact that emerges is that HM-SVMs outperform LC-CRFs, on both batches, both settings (authoritative and nonauthoritative), and both evaluation measures (F\u00b5\n1 and FM1 ); e.g., on the authoritative\nsetting, and as an average across the two batches, HM-SVMs obtain F\u00b51 = 0.819 (while LC-CRFs obtain 0.795) and FM1 = 0.724 (while LC-CRFs obtain 0.713). Aside from their different levels of effectiveness, the two learners behave in a qualitatively similar way as a function of \u03bb, as evident from a comparison of Figures 2 and 3. However, we will not dwell on this fact any further since the relative performance of the learning algorithms is not the main focus of the present study; as will be evident in the discussion that follows, most insights obtained from the LC-CRFs experiments are qualitatively confirmed by the HM-SVMs experiments, and vice versa.\n4.2.3. Coder1 generates less accuracy than Coder2. A third fact that may be noted (from Table II) is that, for \u03bb = 0, there is a substantive difference in accuracy values between the two coders, with Coder2 usually generating higher accuracy than Coder1. This fact can be especially appreciated at the macroaveraged level (where for LC-CRFs we have FM1 = 0.674 for Coder1 and F M 1 = 0.752 for Coder2, and for HM-SVMs we\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\nhave FM1 = 0.693 for Coder1 and F M 1 = 0.754 for Coder2), while the difference is less clearcut at the microaveraged level (where for LC-CRFs we have FM1 = .0.783 for Coder1 and FM1 = 0.808 for Coder2, and for HM-SVMs we have F M 1 = 0.820 for Coder1 and FM1 = 0.817 for Coder2); this indicates that the codes where Coder2 especially shines are the low-frequency ones. In principle, there might be several reasons for this difference in accuracy values between the two coders. The documents in 2-only might be \u201ceasier\u201d to code automatically than those in 1-only; or the distributions of Both(1) and 1-only might be less similar to each other than the distributions of Both(2) and 2-only, thus verifying the i.i.d. assumption to a higher degree; or Coder2 might simply be more self-consistent in her annotation style than Coder1. In order to check whether the last of these three hypotheses is true we have performed four k-fold cross-validation (k-FCV) experiments (for Both(1) and Both(2), and for LC-CRFs and HM-SVMs, in all combinations), using k = 20. Intuitively, a higher accuracy value resulting from a k-FCV test means a higher level of self-consistency, since if the same coding style is consistently used to label a dataset, a system tends to encounter in the testing phase the same labelling patterns it has encountered in the training phase, which is conducive to higher accuracy. Of course, the results of such a\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\ntest are difficult to interpret if the goal is to assess the self-consistency of a coder in absolute terms (since we do not know what values of F1 correspond to what levels of self-consistency), but they are not if the goal is simply to establish which of the two is the more self-consistent, since the two experiments are run on the same documents. The results of our two k-FCV experiments are reported in Table III. From this table we can see that the accuracy on Both(2) is substantially higher than the one obtained on Both(1), thus indicating that Coder2 is indeed more self-consistent than Coder1. This is thus the likely explanation of the higher levels of accuracy obtained on the dataset annotated, for both training and test, by Coder2.\n4.2.4. Overannotation and underannotation. A fourth, even more interesting fact we may observe is that accuracy as a function of the corruption ratio varies much less for Batch1 than for Batch2, since for this latter we witness a much more substantial drop in going\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\nfrom \u03bb = 0 to \u03bb = 100. We conjecture that this may be due to the different annotation style of the two coders; the rest of this subsection will be devoted to explaining the rationale of this conjecture. As evident from Table I, Coder2 annotates, as instances of the concepts of interest, more mentions (+15.7%) and also more tokens per mention (+15.6%) than Coder1; relatively to each other, Coder1 is thus an underannotator while Coder2 is an overannotator. Since, as noted in Section 1, learning algorithms learn to replicate the subjective annotation style of their supervisors, a system trained on data annotated by an overannotator will itself tend to overannotate; conversely, a system trained by an underannotator will itself tend to underannotate. Overannotation results in more true positives and more false positives. The plots in Figures 2 and 3 show that when, as a consequence of increased values of \u03bb, the number of training documents annotated by an overannotator increases (as is the case of Batch1), precision suffers somehow (due to the fact that, along with more true positives, there are also more false positives), but this is compensated by an increase in recall (due to an increased number of true positives); as a result, as shown in Figure 1 (and in Table II too), the drop in F1 resulting from moving to \u03bb = 0 to \u03bb = 100 is very limited. Figures 2 and 3 instead show that when, as a consequence of increased values of \u03bb, the number of training documents annotated by an underannotator increases (as is the case for Batch2), recall drops substantially (due to the decreased number of true positives), and this drop is not compensated by the stability of precision (which is due\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\nto the combined effect of a decrease in true positives and a decrease in false positives); as a result, as shown in Figure 1 (see also Table II), the drop in F1 resulting from moving to \u03bb = 0 to \u03bb = 100 is much more substantial than for Batch1. In order to check whether the decreases in accuracy between the \u03bb = 0 and the \u03bb = 100 settings is statistically significant we have performed an approximate randomization test (ART) [Chinchor et al. 1993]. In this test the difference is considered statistically significant if the resulting p value is < 0.05. Two advantages of the ART are that\n(1) unlike the t-test, the ART does not require the data to be normally distributed; (2) unlike the Wilcoxon signed-rank test, the ART can be applied to multivariate non-\nlinear evaluation measures such as F1 [Yeh 2000].\nThe results of our statistical significance tests are reported in Table IV. These results essentially confirm the observations above, i.e., that in Batch2 the drop in performance resulting from having the training set annotated by the non-authoritative coder (instead of the authoritative one) is not statistically significant, while (with the exception of the F\n\u00b5 1 results for HM-SVMs) it is statistically significant for Batch1.\n4.2.5. Caveats. The experiments discussed in this paper do not allow us to reach hard conclusions about the robustness of information extraction systems to imperfect training data quality, for several reasons:\n(1) The results obtained should be confirmed by additional experiments carried out on other datasets; unfortunately, as noted in Footnote 6, we were not able to locate any other publicly available dataset with the required characteristics (that is, containing at least some doubly annotated documents). (2) The dataset used here is representative of only a specific type of imperfect training data quality, i.e., the one deriving from the fact that the training data were annotated by a coder different (albeit equally expert) from the one who annotated the test set. Other types do exist, however, as noted in the introduction. (3) Even the results reported here are somehow contradictory, since a statistically significant drop in performance was observed in Batch1 while no such statistically significant drop was observed in Batch2.\nHowever, one interesting fact that has emerged from the present study (and that will need to be confirmed by additional experiments, should other datasets become available) is that, as argued in detail in Section 4.2.4, the lack of a statistically significant drop in performance observed in Batch2 seems to be due to the fact that the nonauthoritative coder who annotated the training set had an overannotating behaviour. Thismight suggest (emphasis meaning that prudence should be exercised) that, should there be a need for having a training set annotated by someone different from the au-\nACM Journal of Data and Information quality, Vol. V, No. N, Article A, Publication date: January YYYY.\nthoritative coder, underannotation should be discouraged much more than overannotation."}, {"heading": "5. CONCLUSIONS", "text": "Few researchers have investigated the loss in accuracy that occurs when a supervised learning algorithm is fed with training data of suboptimal quality. We have done this for the first time in the case of information extraction systems (trained via supervised learning) as applied to the detection of mentions of concepts of interest in medical notes. Specifically, we have tested to what extent extraction accuracy suffers when the person who has annotated the test data (the \u201cauthoritative coder\u201d), whom we must assume to be the person to whose judgment we conform irrespectively of her level of expertise, is different from the person who has labelled the training data (the \u201cnonauthoritative coder\u201d). Our experimental results, that we have obtained on a dataset of 500 mammography reports annotated according to 9 concepts of interest, are somehow surprising, since they indicate that the resulting drop in accuracy is not always statistically significant. In our experiments, no statistically significant drop was observed when the non-authoritative coder had a tendency to overannotate, while a substantial, statistically significant drop was observed when the non-authoritative coder was an underannotator; however, experiments on more doubly (or evenmultiply) annotated datasets will be needed to confirm or disconfirm these initial findings. Since labelling cost is an important issue in the generation of training data (with senior coders costing much more than junior ones, and with internal coders costing much more than \u201cmechanical turkers\u201d), results of this kind may give important indications as to the cost-effectiveness of low-cost annotation work. This paper is a first attempt to investigate the impact of less-than-sterling training data quality on the accuracy of medical concept extraction systems, and more work is needed to validate the conjectures that we have made based on our experimental results. As repeatedly mentioned in this paper, one limit of the present work is the fact that only one dataset was used for the experiments. This was due to the unfortunate lack of other publicly available medical datasets that contain (at least a subset of) textual records independently labelled by two different coders; datasets with these characteristics have been used in the past in published research but are not made available to the rest of the scientific community. We hope that the increasing importance of text mining applications in clinical practice, and the importance of shared datasets for fostering advances in this field, will generate a new kind of awareness on the need to make more datasets available to the scientific community."}], "references": [{"title": "Hidden Markov Support Vector Machines", "author": ["Yasemin Altun", "Ioannis Tsochantaridis", "Thomas Hofmann."], "venue": "Proceedings of the 20th International Conference on Machine Learning (ICML 2003). Washington DC, USA, 3\u201310.", "citeRegEx": "Altun et al\\.,? 2003", "shortCiteRegEx": "Altun et al\\.", "year": 2003}, {"title": "Inter-Coder Agreement for Computational Linguistics", "author": ["Ron Artstein", "Massimo Poesio."], "venue": "Computational Linguistics 34, 4 (2008), 555\u2013596.", "citeRegEx": "Artstein and Poesio.,? 2008", "shortCiteRegEx": "Artstein and Poesio.", "year": 2008}, {"title": "What Determines Inter-Coder Agreement in Manual Annotations? A Meta-Analytic Investigation", "author": ["Petra S. Bayerl", "Karsten I. Paul."], "venue": "Computational Linguistics 37, 4 (2011), 699\u2013725.", "citeRegEx": "Bayerl and Paul.,? 2011", "shortCiteRegEx": "Bayerl and Paul.", "year": 2011}, {"title": "Inductive creation of an annotation schema for manually indexing clinical conditions from emergency department reports", "author": ["Wendy W. Chapman", "John N. Dowling."], "venue": "Journal of Biomedical Informatics 39, 2 (2006), 196\u2013208.", "citeRegEx": "Chapman and Dowling.,? 2006", "shortCiteRegEx": "Chapman and Dowling.", "year": 2006}, {"title": "Evaluating message understanding systems: An analysis of the third message understanding conference (MUC-3)", "author": ["Nancy Chinchor", "David D. Lewis", "Lynette Hirschman."], "venue": "Computational Linguistics 19, 3 (1993), 409\u2013449.", "citeRegEx": "Chinchor et al\\.,? 1993", "shortCiteRegEx": "Chinchor et al\\.", "year": 1993}, {"title": "An Analysis of the Relative Hardness of Reuters-21578 Subsets", "author": ["Franca Debole", "Fabrizio Sebastiani."], "venue": "Journal of the American Society for Information Science and Technology 56, 6 (2005), 584\u2013596.", "citeRegEx": "Debole and Sebastiani.,? 2005", "shortCiteRegEx": "Debole and Sebastiani.", "year": 2005}, {"title": "The kappa statistic: A second look", "author": ["Barbara Di Eugenio", "Michael Glass."], "venue": "Computational Linguistics 30, 1 (2004), 95\u2013101.", "citeRegEx": "Eugenio and Glass.,? 2004", "shortCiteRegEx": "Eugenio and Glass.", "year": 2004}, {"title": "An Enhanced CRFs-based System for Information Extraction from Radiology Reports", "author": ["Andrea Esuli", "Diego Marcheggiani", "Fabrizio Sebastiani."], "venue": "Journal of Biomedical Informatics 46, 3 (2013), 425\u2013 435.", "citeRegEx": "Esuli et al\\.,? 2013", "shortCiteRegEx": "Esuli et al\\.", "year": 2013}, {"title": "Evaluating Information Extraction", "author": ["Andrea Esuli", "Fabrizio Sebastiani."], "venue": "Proceedings of the Conference on Multilingual and Multimodal Information Access Evaluation (CLEF 2010). Padova, IT, 100\u2013 111.", "citeRegEx": "Esuli and Sebastiani.,? 2010", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2010}, {"title": "Training Data Cleaning for Text Classification", "author": ["Andrea Esuli", "Fabrizio Sebastiani."], "venue": "ACM Transactions on Information Systems 31, 4 (2013).", "citeRegEx": "Esuli and Sebastiani.,? 2013", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2013}, {"title": "Crowdsourcing document relevance assessment with Mechanical Turk", "author": ["Catherine Grady", "Matthew Lease."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. Los Angeles, US, 172\u2013179.", "citeRegEx": "Grady and Lease.,? 2010", "shortCiteRegEx": "Grady and Lease.", "year": 2010}, {"title": "Induced lexico-syntactic patterns improve information extraction from online medical forums", "author": ["Sonal Gupta", "Diana L. MacLean", "Jeffrey Heer", "Christopher D Manning."], "venue": "Journal of the American Medical Informatics Association 21, 5 (2014), 902\u2013909.", "citeRegEx": "Gupta et al\\.,? 2014", "shortCiteRegEx": "Gupta et al\\.", "year": 2014}, {"title": "Exploiting Multiply Annotated Corpora in Biomedical Information Extraction Tasks", "author": ["Barry Haddow", "Beatrice Alex."], "venue": "Proceedings of the 6th Conference on Language Resources and Evaluation (LREC 2008). Marrakech, MA.", "citeRegEx": "Haddow and Alex.,? 2008", "shortCiteRegEx": "Haddow and Alex.", "year": 2008}, {"title": "Impact of Data Quality on Predictive Accuracy of ANFISbased Soft Sensor Models", "author": ["Surinder Jassar", "Zaiyi Liao", "Lian Zhao."], "venue": "Proceedings of the 2009 IEEE World Congress on Engineering and Computer Science (WCECS 2009), Vol. II. San Francisco, US.", "citeRegEx": "Jassar et al\\.,? 2009", "shortCiteRegEx": "Jassar et al\\.", "year": 2009}, {"title": "A study of machine-learning-based approaches to extract clinical entities and their assertions from discharge summaries", "author": ["Min Jiang", "Yukun Chen", "Mei Liu", "S. Trent Rosenbloom", "Subramani Mani", "Joshua C. Denny", "Hua Xu."], "venue": "Journal of the American Medical Informatics Association 18, 5 (2011), 601\u2013 606.", "citeRegEx": "Jiang et al\\.,? 2011", "shortCiteRegEx": "Jiang et al\\.", "year": 2011}, {"title": "Enhancing clinical concept extraction with distributional semantics", "author": ["Siddhartha Jonnalagadda", "Trevor Cohen", "Stephen Wu", "Graciela Gonzalez."], "venue": "Journal of Biomedical Informatics 45, 1 (2012), 129\u2013 140.", "citeRegEx": "Jonnalagadda et al\\.,? 2012", "shortCiteRegEx": "Jonnalagadda et al\\.", "year": 2012}, {"title": "Overview of the ShARe/CLEF eHealth Evaluation Lab 2014", "author": ["Liadh Kelly", "Lorraine Goeuriot", "Hanna Suominen", "Tobias Schreck", "Gondy Leroy", "Danielle L. Mowery", "Sumithra Velupillai", "Wendy W. Chapman", "David Mart\u0131\u0301nez", "Guido Zuccon", "Jo\u00e3o R.M. Palotti"], "venue": "In Proceedings of the 5th International Conference of the CLEF Initiative (CLEF", "citeRegEx": "Kelly et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kelly et al\\.", "year": 2014}, {"title": "The effects of outliers data on neural network performance", "author": ["Azme Khamis", "Zuhaimy Ismail", "Khalid Haron", "Ahmad T. Mohammed."], "venue": "Journal of Applied Sciences 5, 8 (2005), 1394\u20131398.", "citeRegEx": "Khamis et al\\.,? 2005", "shortCiteRegEx": "Khamis et al\\.", "year": 2005}, {"title": "Content analysis: An introduction to its methodology", "author": ["Klaus Krippendorff."], "venue": "Sage, Thousand Oaks, US.", "citeRegEx": "Krippendorff.,? 2004", "shortCiteRegEx": "Krippendorff.", "year": 2004}, {"title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."], "venue": "Proceedings of the 18th International Conference on Machine Learning (ICML 2001). Williamstown, US, 282\u2013289.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Conditional Random Fields and Support Vector Machines for Disorder Named Entity Recognition in Clinical Texts", "author": ["Dingcheng Li", "Karin Kipper-Schuler", "Guergana Savova."], "venue": "Proceedings of the ACL Workshop on Current Trends in Biomedical Natural Language Processing (BioNLP 2008). Columbus, US, 94\u201395.", "citeRegEx": "Li et al\\.,? 2008", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Section classification in clinical notes using supervised hidden Markov model", "author": ["Ying Li", "Sharon Lipsky Gorman", "No\u00e9mie Elhadad."], "venue": "Proceedings of the 2nd ACM International Health Informatics Symposium. Arlington, US, 744\u2013750.", "citeRegEx": "Li et al\\.,? 2010", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Extracting Information from Textual Documents in the Electronic Health Record: A Review of Recent Research", "author": ["StephaneM. Meystre", "Guerguana K. Savova", "Karin C. Kipper-Schuler", "John F. Hurdle."], "venue": "IMIA Yearbook of Medical Informatics, A. Geissbuhler and C. Kulikowski (Eds.). Schattauer Publishers, Stuttgart, DE, 128\u2013144.", "citeRegEx": "Meystre et al\\.,? 2008", "shortCiteRegEx": "Meystre et al\\.", "year": 2008}, {"title": "High accuracy information extraction of medication information from clinical notes: 2009 i2b2 medication extraction challenge", "author": ["Jon Patrick andMing Li."], "venue": "Journal of the American Medical Informatics Association 17 (2010), 524\u2013527.", "citeRegEx": "Li.,? 2010", "shortCiteRegEx": "Li.", "year": 2010}, {"title": "SemEval-2014 Task 7: Analysis of Clinical Text", "author": ["Sameer Pradhan", "No\u00e9mie Elhadad", "Wendy Chapman", "Suresh Manandhar", "Guergana Savova."], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014). Dublin, IE, 54\u201362.", "citeRegEx": "Pradhan et al\\.,? 2014", "shortCiteRegEx": "Pradhan et al\\.", "year": 2014}, {"title": "Data Errors in Neural Network and Linear RegressionModels: An Experimental Comparison", "author": ["Donald F. Rossin", "Barbara D. Klein."], "venue": "Data Quality Journal 5, 1 (1999).", "citeRegEx": "Rossin and Klein.,? 1999", "shortCiteRegEx": "Rossin and Klein.", "year": 1999}, {"title": "Concept drift", "author": ["Claude Sammut", "Michael Harries."], "venue": "Encyclopedia of Machine Learning, Claude Sammut and Geoffrey I. Webb (Eds.). Springer, Heidelberg, DE, 202\u2013205.", "citeRegEx": "Sammut and Harries.,? 2011", "shortCiteRegEx": "Sammut and Harries.", "year": 2011}, {"title": "Syntactically-informed semantic category recognition in discharge summaries", "author": ["Tawanda Sibanda", "Tian He", "Peter Szolovits", "\u00d6zlem Uzuner."], "venue": "Proceedings of the Annual Symposium of the American Medical Informatics Association (AMIA 2006). Washington, US, 714\u2013718.", "citeRegEx": "Sibanda et al\\.,? 2006", "shortCiteRegEx": "Sibanda et al\\.", "year": 2006}, {"title": "Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks", "author": ["Rion Snow", "Brendan O\u2019Connor", "Daniel Jurafsky", "Andrew Y. Ng"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008). Honolulu,", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Evaluating temporal relations in clinical text: 2012 i2b2 Challenge", "author": ["Weiyi Sun", "Anna Rumshisky", "\u00d6zlem Uzuner."], "venue": "Journal of the American Medical Informatics Association 20, 5 (2013), 806\u2013813.", "citeRegEx": "Sun et al\\.,? 2013", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "Overview of the ShARe/CLEF eHealth Evaluation Lab 2013", "author": ["Hanna Suominen", "Sanna Salanter\u00e4", "Sumithra Velupillai", "Wendy W. Chapman", "Guergana Savova", "Noemie Elhadad", "Sameer Pradhan", "Brett R. South", "Danielle L. Mowery", "Gareth J. Jones", "Johannes Leveling", "Liadh Kelly", "Lorraine Goeuriot", "David Martinez", "Guido Zuccon."], "venue": "Proceedings of the 4th International Conference of the CLEF Initiative (CLEF 2013). Valencia, ES, 212\u2013231.", "citeRegEx": "Suominen et al\\.,? 2013", "shortCiteRegEx": "Suominen et al\\.", "year": 2013}, {"title": "An Introduction to Conditional Random Fields for Relational Learning", "author": ["Charles Sutton", "Andrew McCallum."], "venue": "Introduction to Statistical Relational Learning, Lise Getoor and Ben Taskar (Eds.). The MIT Press, Cambridge, US, 93\u2013127.", "citeRegEx": "Sutton and McCallum.,? 2007", "shortCiteRegEx": "Sutton and McCallum.", "year": 2007}, {"title": "An Introduction to Conditional Random Fields", "author": ["Charles Sutton", "Andrew McCallum."], "venue": "Foundations and Trends in Machine Learning 4, 4 (2012), 267\u2013373.", "citeRegEx": "Sutton and McCallum.,? 2012", "shortCiteRegEx": "Sutton and McCallum.", "year": 2012}, {"title": "Training Conditional Random Fields with Multivariate Evaluation Measures", "author": ["Jun Suzuki", "Erik McDermott", "Hideki Isozaki."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL (ACL/COLING 2006). Sydney, AU, 217\u2013224.", "citeRegEx": "Suzuki et al\\.,? 2006", "shortCiteRegEx": "Suzuki et al\\.", "year": 2006}, {"title": "Clinical entity recognition using structural support vector machines with rich features", "author": ["Buzhou Tang", "Hongxin Cao", "Yonghui Wu", "Min Jiang", "Hua Xu."], "venue": "Proceedings of the 6th ACM International Workshop on Data and Text Mining in Biomedical Informatics (DTMBIO 2012). Maui, US, 13\u201320.", "citeRegEx": "Tang et al\\.,? 2012", "shortCiteRegEx": "Tang et al\\.", "year": 2012}, {"title": "Using machine learning for concept extraction on clinical documents from multiple data sources", "author": ["Manabu Torii", "Kavishwar Wagholikar", "Hongfang Liu."], "venue": "Journal of the American Medical Informatics Association 18, 5 (2011), 580\u2013587.", "citeRegEx": "Torii et al\\.,? 2011", "shortCiteRegEx": "Torii et al\\.", "year": 2011}, {"title": "Large Margin Methods for Structured and Interdependent Output Variables", "author": ["Ioannis Tsochantaridis", "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun."], "venue": "Journal of Machine Learning Research 6 (2005), 1453\u20131484.", "citeRegEx": "Tsochantaridis et al\\.,? 2005", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Evaluating the state of the art in coreference resolution for electronic medical records", "author": ["\u00d6zlem Uzuner", "Andreea Bodnari", "Shuying Shen", "Tyler Forbush", "John Pestian", "Brett R. South."], "venue": "Journal of the American Medical Informatics Association 19, 5 (2012), 786\u2013791.", "citeRegEx": "Uzuner et al\\.,? 2012", "shortCiteRegEx": "Uzuner et al\\.", "year": 2012}, {"title": "2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text", "author": ["\u00d6zlem Uzuner", "Brett R. South", "Shuying Shen", "Scott L. DuVall."], "venue": "Journal of the American Medical Informatics Association 18, 5 (2011), 552\u2013556.", "citeRegEx": "Uzuner et al\\.,? 2011", "shortCiteRegEx": "Uzuner et al\\.", "year": 2011}, {"title": "Graphical Models, Exponential Families, and Variational Inference", "author": ["Martin J. Wainwright andMichael I. Jordan."], "venue": "Foundations and Trends in Machine Learning 1, 1/2 (2008), 1\u2013305.", "citeRegEx": "Jordan.,? 2008", "shortCiteRegEx": "Jordan.", "year": 2008}, {"title": "Cascading classifiers for named entity recognition in clinical notes", "author": ["Yefeng Wang", "Jon Patrick."], "venue": "Proceedings of the RANLP 2009 Workshop on Biomedical Information Extraction. Borovets, BG, 42\u201349.", "citeRegEx": "Wang and Patrick.,? 2009", "shortCiteRegEx": "Wang and Patrick.", "year": 2009}, {"title": "Assessor disagreement and text classifier accuracy", "author": ["William Webber", "Jeremy Pickens."], "venue": "Proceedings of the 36th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2013). Dublin, IE, 929\u2013932.", "citeRegEx": "Webber and Pickens.,? 2013", "shortCiteRegEx": "Webber and Pickens.", "year": 2013}, {"title": "More accurate tests for the statistical significance of result differences", "author": ["Alexander S. Yeh."], "venue": "Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000). Saarbr\u00fccken, DE, 947\u2013953.", "citeRegEx": "Yeh.,? 2000", "shortCiteRegEx": "Yeh.", "year": 2000}, {"title": "UTH CCB: A report for SemEval 2014 \u2013 Task 7 Analysis of Clinical Text", "author": ["Yaoyun Zhang", "Jingqi Wang", "Buzhou Tang", "Yonghui Wu", "Min Jiang", "Yukun Chen", "Hua Xu."], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014). Dublin, IE, 802\u2013806.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": ", [Kelly et al. 2014; Pradhan et al. 2014; Sun et al. 2013; Suominen et al. 2013; Uzuner et al. 2012; Uzuner et al. 2011]) on information extraction from clinical documents, i.", "startOffset": 2, "endOffset": 121}, {"referenceID": 24, "context": ", [Kelly et al. 2014; Pradhan et al. 2014; Sun et al. 2013; Suominen et al. 2013; Uzuner et al. 2012; Uzuner et al. 2011]) on information extraction from clinical documents, i.", "startOffset": 2, "endOffset": 121}, {"referenceID": 29, "context": ", [Kelly et al. 2014; Pradhan et al. 2014; Sun et al. 2013; Suominen et al. 2013; Uzuner et al. 2012; Uzuner et al. 2011]) on information extraction from clinical documents, i.", "startOffset": 2, "endOffset": 121}, {"referenceID": 30, "context": ", [Kelly et al. 2014; Pradhan et al. 2014; Sun et al. 2013; Suominen et al. 2013; Uzuner et al. 2012; Uzuner et al. 2011]) on information extraction from clinical documents, i.", "startOffset": 2, "endOffset": 121}, {"referenceID": 37, "context": ", [Kelly et al. 2014; Pradhan et al. 2014; Sun et al. 2013; Suominen et al. 2013; Uzuner et al. 2012; Uzuner et al. 2011]) on information extraction from clinical documents, i.", "startOffset": 2, "endOffset": 121}, {"referenceID": 38, "context": ", [Kelly et al. 2014; Pradhan et al. 2014; Sun et al. 2013; Suominen et al. 2013; Uzuner et al. 2012; Uzuner et al. 2011]) on information extraction from clinical documents, i.", "startOffset": 2, "endOffset": 121}, {"referenceID": 28, "context": "[Grady and Lease 2010; Snow et al. 2008].", "startOffset": 0, "endOffset": 40}, {"referenceID": 7, "context": "We do this by testing the accuracy of two state-of the-art systems on a dataset of radiology reports (originally discussed in [Esuli et al. 2013]) in which a portion of the data has independently been annotated by two different experts.", "startOffset": 126, "endOffset": 145}, {"referenceID": 14, "context": "Support vector machines (SVMs \u2013 [Jiang et al. 2011; Li et al. 2008; Sibanda et al. 2006]), hidden Markov models (HMMs \u2013 [Li et al.", "startOffset": 32, "endOffset": 88}, {"referenceID": 20, "context": "Support vector machines (SVMs \u2013 [Jiang et al. 2011; Li et al. 2008; Sibanda et al. 2006]), hidden Markov models (HMMs \u2013 [Li et al.", "startOffset": 32, "endOffset": 88}, {"referenceID": 27, "context": "Support vector machines (SVMs \u2013 [Jiang et al. 2011; Li et al. 2008; Sibanda et al. 2006]), hidden Markov models (HMMs \u2013 [Li et al.", "startOffset": 32, "endOffset": 88}, {"referenceID": 21, "context": "2006]), hidden Markov models (HMMs \u2013 [Li et al. 2010]), and (especially) conditional random fields (CRFs \u2013 [Esuli et al.", "startOffset": 37, "endOffset": 53}, {"referenceID": 7, "context": "2010]), and (especially) conditional random fields (CRFs \u2013 [Esuli et al. 2013; Gupta et al. 2014; Jiang et al. 2011; Jonnalagadda et al. 2012; Li et al. 2008; Patrick and Li 2010; Torii et al. 2011; Wang and Patrick 2009]) have been the learners of choice in this field, due to their good performance and to the existence of publicly available implementations.", "startOffset": 59, "endOffset": 221}, {"referenceID": 11, "context": "2010]), and (especially) conditional random fields (CRFs \u2013 [Esuli et al. 2013; Gupta et al. 2014; Jiang et al. 2011; Jonnalagadda et al. 2012; Li et al. 2008; Patrick and Li 2010; Torii et al. 2011; Wang and Patrick 2009]) have been the learners of choice in this field, due to their good performance and to the existence of publicly available implementations.", "startOffset": 59, "endOffset": 221}, {"referenceID": 14, "context": "2010]), and (especially) conditional random fields (CRFs \u2013 [Esuli et al. 2013; Gupta et al. 2014; Jiang et al. 2011; Jonnalagadda et al. 2012; Li et al. 2008; Patrick and Li 2010; Torii et al. 2011; Wang and Patrick 2009]) have been the learners of choice in this field, due to their good performance and to the existence of publicly available implementations.", "startOffset": 59, "endOffset": 221}, {"referenceID": 15, "context": "2010]), and (especially) conditional random fields (CRFs \u2013 [Esuli et al. 2013; Gupta et al. 2014; Jiang et al. 2011; Jonnalagadda et al. 2012; Li et al. 2008; Patrick and Li 2010; Torii et al. 2011; Wang and Patrick 2009]) have been the learners of choice in this field, due to their good performance and to the existence of publicly available implementations.", "startOffset": 59, "endOffset": 221}, {"referenceID": 20, "context": "2010]), and (especially) conditional random fields (CRFs \u2013 [Esuli et al. 2013; Gupta et al. 2014; Jiang et al. 2011; Jonnalagadda et al. 2012; Li et al. 2008; Patrick and Li 2010; Torii et al. 2011; Wang and Patrick 2009]) have been the learners of choice in this field, due to their good performance and to the existence of publicly available implementations.", "startOffset": 59, "endOffset": 221}, {"referenceID": 35, "context": "2010]), and (especially) conditional random fields (CRFs \u2013 [Esuli et al. 2013; Gupta et al. 2014; Jiang et al. 2011; Jonnalagadda et al. 2012; Li et al. 2008; Patrick and Li 2010; Torii et al. 2011; Wang and Patrick 2009]) have been the learners of choice in this field, due to their good performance and to the existence of publicly available implementations.", "startOffset": 59, "endOffset": 221}, {"referenceID": 29, "context": "In recent years, research on the analysis of clinical texts has been further boosted by the existence of \u201cshared tasks\u201d on this topic, such as the seminal i2b2 series (\u201cInformatics for Integrating Biology and the Bedside\u201d \u2013 [Sun et al. 2013; Uzuner et al. 2012; Uzuner et al. 2011]), the 2013 [Suominen et al.", "startOffset": 224, "endOffset": 281}, {"referenceID": 37, "context": "In recent years, research on the analysis of clinical texts has been further boosted by the existence of \u201cshared tasks\u201d on this topic, such as the seminal i2b2 series (\u201cInformatics for Integrating Biology and the Bedside\u201d \u2013 [Sun et al. 2013; Uzuner et al. 2012; Uzuner et al. 2011]), the 2013 [Suominen et al.", "startOffset": 224, "endOffset": 281}, {"referenceID": 38, "context": "In recent years, research on the analysis of clinical texts has been further boosted by the existence of \u201cshared tasks\u201d on this topic, such as the seminal i2b2 series (\u201cInformatics for Integrating Biology and the Bedside\u201d \u2013 [Sun et al. 2013; Uzuner et al. 2012; Uzuner et al. 2011]), the 2013 [Suominen et al.", "startOffset": 224, "endOffset": 281}, {"referenceID": 30, "context": "2011]), the 2013 [Suominen et al. 2013] and 2014 [Kelly et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 16, "context": "2013] and 2014 [Kelly et al. 2014] editions of ShARe/CLEF eHealth, and the Semeval-2014 Task 7 \u201cAnalysis of Clinical Text\u201d [Pradhan et al.", "startOffset": 15, "endOffset": 34}, {"referenceID": 24, "context": "2014] editions of ShARe/CLEF eHealth, and the Semeval-2014 Task 7 \u201cAnalysis of Clinical Text\u201d [Pradhan et al. 2014].", "startOffset": 94, "endOffset": 115}, {"referenceID": 13, "context": "A similar study, in which the context is predicting the average air temperature in distributed heating systems, was carried out in [Jassar et al. 2009]; yet another study, in which the goal was predicting the production levels of palm oil via a neural network, is [Khamis et al.", "startOffset": 131, "endOffset": 151}, {"referenceID": 17, "context": "2009]; yet another study, in which the goal was predicting the production levels of palm oil via a neural network, is [Khamis et al. 2005].", "startOffset": 118, "endOffset": 138}, {"referenceID": 10, "context": "In the context of a biomedical information extraction task Haddow and Alex [2008] examined the situation in which training data annotated by two different coders are available, and they found that higher accuracy is obtained by using both versions at the same time than by attempting to reconcile them or using just one of them.", "startOffset": 59, "endOffset": 82}, {"referenceID": 8, "context": "Closer to our application context, Esuli and Sebastiani [2013] have thoroughly studied the effect of suboptimal training data quality in text classification.", "startOffset": 35, "endOffset": 63}, {"referenceID": 38, "context": "Webber and Pickens [2013] also address the text classification task (in the context of ediscovery from legal texts), but differently from [Esuli and Sebastiani 2013] they work with naturally occurring noise; differently from the present work, the multiply-coded training data they use were coded by one coder known to be an expert coder and another coder known to be a junior coder.", "startOffset": 0, "endOffset": 26}, {"referenceID": 7, "context": "The dataset we have used to test the ideas discussed in the previous sections is the UmbertoI(RadRep) dataset first presented in [Esuli et al. 2013], consisting of a set of 500 free-text mammography reports written (in Italian) by medical personnel of the", "startOffset": 129, "endOffset": 148}, {"referenceID": 19, "context": "As the learning algorithms we have tested both linear-chain conditional random fields (LC-CRFs - [Lafferty et al. 2001; Sutton and McCallum 2007; Sutton and McCallum 2012]), in Charles Sutton\u2019s GRMM implementation, and hidden Markov support vector machines (HM-SVMs - [Altun et al.", "startOffset": 97, "endOffset": 171}, {"referenceID": 0, "context": "2001; Sutton and McCallum 2007; Sutton and McCallum 2012]), in Charles Sutton\u2019s GRMM implementation, and hidden Markov support vector machines (HM-SVMs - [Altun et al. 2003]), in Thorsten Joachims\u2019s SVM implementation.", "startOffset": 154, "endOffset": 173}, {"referenceID": 36, "context": "HM-SVMs are an instantiation of \u201cSVMs for structured output prediction\u201d (SVM ) [Tsochantaridis et al. 2005] for the sequence labelling task, and have already been used in clinical information extraction (see e.", "startOffset": 79, "endOffset": 107}, {"referenceID": 34, "context": ", [Tang et al. 2012; Zhang et al. 2014]).", "startOffset": 2, "endOffset": 39}, {"referenceID": 43, "context": ", [Tang et al. 2012; Zhang et al. 2014]).", "startOffset": 2, "endOffset": 39}, {"referenceID": 7, "context": "As a measure of classification accuracy we use, similarly to [Esuli et al. 2013], the token-and-separator variant (proposed in [Esuli and Sebastiani 2010]) of the well-known F1 measure, according to which an information extraction system is evaluated on an event space consisting of all the t-units in the text.", "startOffset": 61, "endOffset": 80}, {"referenceID": 33, "context": "In other words, each t-unit xt (rather than each mention, as in the traditional \u201csegmentation F-score\u201d model [Suzuki et al. 2006]) counts as a true positive, true negative, false positive, or false negative for a given concept cr, depending on whether xt belongs to cr or not in the predicted annotation and in the true annotation.", "startOffset": 109, "endOffset": 129}, {"referenceID": 7, "context": ", [Chapman and Dowling 2006; Esuli et al. 2013]) functions that were not explicitly developed for measuring ICA (such as F1, that was developed for measuring binary classification accuracy) are used.", "startOffset": 2, "endOffset": 47}, {"referenceID": 7, "context": "In [Esuli et al. 2013], experiments on the UmbertoI(RadRep) dataset were run using either 1-only and/or 2-only (i.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "In order to check whether the decreases in accuracy between the \u03bb = 0 and the \u03bb = 100 settings is statistically significant we have performed an approximate randomization test (ART) [Chinchor et al. 1993].", "startOffset": 182, "endOffset": 204}], "year": 2015, "abstractText": "Author\u2019s addresses: Diego Marcheggiani, Istituto di Scienza e Tecnologie dell\u2019Informazione, Consiglio Nazionale delle Ricerche, 56124 Pisa, Italy; Fabrizio Sebastiani, Qatar Computing Research Institute, Qatar Foundation, PO Box 5825, Doha, Qatar. Fabrizio Sebastiani is currently on leave from Consiglio Nazionale delle Ricerche, Italy. The order in which the authors are listed is purely alphabetical; each author has given an equally important contribution to this work. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c \u00a9 YYYY ACM 1936-1955/YYYY/01-ARTA $15.00 DOI:http://dx.doi.org/10.1145/0000000.0000000", "creator": "LaTeX with hyperref package"}}}