{"id": "1502.05925", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Feature-Budgeted Random Forest", "abstract": "we take unique explanations for cost - time cost reduction, where complete optimization always available like testing, alternatively during prediction - theory, valuable feature can only be acquired earning an additional cost. we take another novel random equation letting here handle experimental error for a weakly - specified { \\ petit average } conditional acquisition strategy. while random companies yield great item performance, prototypes don't explicitly excel in feature costs and only represent low correlation among structures, which raises expectations. our random forest grows trees with low production cost and high strength responses on purely computed cost - weighted - impurity scales. theoretically, we establish near - optimal acquisition yield guarantees allowing our algorithm. empirically, choosing item number some benchmark datasets we demonstrate superior accuracy - cost properties against big - above - the - art prediction - time increases.", "histories": [["v1", "Fri, 20 Feb 2015 16:42:40 GMT  (530kb,D)", "http://arxiv.org/abs/1502.05925v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["feng nan", "joseph wang", "venkatesh saligrama"], "accepted": true, "id": "1502.05925"}, "pdf": {"name": "1502.05925.pdf", "metadata": {"source": "META", "title": "Feature-Budgeted Random Forest", "authors": ["Feng Nan", "Joseph Wang", "Venkatesh Saligrama"], "emails": ["FNAN@BU.EDU", "JOEWANG@BU.EDU", "SRV@BU.EDU"], "sections": [{"heading": "1. Introduction", "text": "In many applications such as surveillance and retrieval, we acquire measurements for an entity, and features for a query in order to make a prediction. Features can be expensive and complementary, namely, knowledge of previously acquired feature values often renders acquisition of another feature redundant. In these cases, the goal is to maximize prediction performance given a constraint on the average feature acquisition cost. Our proposed approach is to learn decision rules for prediction-time cost reduction (Kanani & Melville, 2008) from training data in which the full set of features and ground truth labels are available for training.\nWe propose a novel random forest learning algorithm to\nminimize prediction error for a user-specified average feature acquisition budget. Random forests (Breiman, 2001) construct a collection of trees, wherein each tree is grown by random independent data sampling & feature splitting, producing a collection of independent identically distributed trees. The resulting classifiers are robust, are easy to train, and yield strong generalization performance.\nAlthough well suited to unconstrained supervised learning problems, applying random forests in the case of prediction-time budget constraints presents a major challenge. First, random forests do not account for feature acquisition costs. If two features have similar utility in terms of power to classify examples but have vastly different costs, random forest is just as likely to select the high cost feature as the low cost alternative. This is obviously undesirable. Second, a key element of random forest performance is the diversity amongst trees (Breiman, 2001). Empirical evidence suggest a strong connection between diversity and performance, and generalization error is bounded not only with respect to the strength of individual trees but also the correlation between trees (Breiman, 2001). High diversity amongst trees constructed without regard for acquisition cost results in trees using a wide range of features, and therefore a high acquisition cost (See Section 3).\nThus, ensuring a low acquisition cost on the forest hinges on growing each tree with high discriminative power and low acquisition cost. To this end, we propose to learn decision trees that incorporates feature acquisition cost. Our random forest grows trees based on greedy minimax costweighted-impurity splits. Although the problem of learning decision trees with optimally low-cost is computationally intractable, we show that our greedy approach outputs trees whose cost is closely bounded with respect to the optimal cost. Using these low cost trees, we construct random forests with high classification performance and low prediction-time feature acquisition cost. ar X\niv :1\n50 2.\n05 92\n5v 1\n[ st\nat .M\nL ]\n2 0\nFe b\n20 15\nAbstractly, our algorithm attempts to solve an empirical risk minimization problem subject to a budget constraint. At each step in the algorithm, we add low-cost trees to the random forest to reduce the empirical risk until the budget constraint is met. The resulting random forest adaptively acquires features during prediction time, with features only acquired when used by a split in the tree. In summary, our algorithm is greedy and easy to train. It can not only be parallelized, but also lends itself to distributed databases. Empirically, it does not overfit and has low generalization error. Theoretically, we can characterize the feature acquisition cost for each tree and for the random forest. Empirically, on a number of benchmark datasets we demonstrate superior accuracy-cost curves against state-of-the-art prediction-time algorithms.\nRelated Work: The problem of learning from full training data for prediction-time cost reduction (MacKay, 1992; Kanani & Melville, 2008) has been extensively studied. One simple structure for incorporating costs into learning is through detection cascades (Viola & Jones, 2001; Zhang & Zhang, 2010; Chen et al., 2012), where cheap features are used to discard examples belonging to the negative class. Different from our apporach these approaches require a fixed order of features to be acquired and do not generalize well to multi-class. Bayesian approaches have been proposed which model the system as a POMDP (Ji & Carin, 2007; Kapoor & Horvitz, 2009; Gao & Koller, 2011), however they require estimation of the underlying probability distributions. To overcome the need to estimate distributions, reinforcement learning (Karayev et al., 2013; BusaFekete et al., 2012; Dulac-Arnold et al., 2011) and imitation learning (He et al., 2012) approaches have also been studied, where the reward or oracle action is predicted, however these generally require classifiers capable of operating on a wide range of missing feature patterns.\nSupervised learning approaches with prediction-time budgets have previously been studied under an empirical risk minimization framework to learn budgeted decision trees (Xu et al., 2013; Kusner et al., 2014; Trapeznikov & Saligrama, 2013; Wang et al., 2014b;a). In this setting, construction of budgeted decision cascades or trees has been proposed by learning complex decision functions at each node and leaf, outputting a tree of classifiers which adaptively select sensors/features to be acquired for each new example. Common to these systems is a decision structure, which is a priori fixed. The entire structure is parameterized by complex decision functions for each node, which are then optimized using various objective functions. In contrast we build a random forest of trees where each tree is grown greedily so that global collection of random trees meets the budget constraint.\nConstruction of simple decision trees with low costs has\nalso been studied for discrete function evaluation problems (Cicalese et al., 2014; Moshkov, 2010; Bellala et al., 2012). Different from our work these trees operate on discrete data to minimize function evaluations, with no notion of test time prediction or cost.\nAs for Random forests despite their widespread use in supervised learning, to our knowledge they have not been applied to prediction-time cost reduction."}, {"heading": "2. Feature-Budgeted Random Forest", "text": "We first present the general problem of learning under prediction-time budgets similar to the formulation in (Trapeznikov & Saligrama, 2013; Wang et al., 2014b). Suppose example/label pairs (x, y) are distributed as (x, y)\nd\u223c H . The goal is to learn a classifier f from a family of functions F that minimizes expected loss subject to a budget constraint:\nmin f\u2208F\nExy [L(y, f(x))] , s.t. Ex [C (f, x)] \u2264 B, (1)\nwhere L(y, y\u0302) is a loss function, C(f, x) is the cost of evaluating the function of f on example x andB is a user specified budget constraint. In this paper, we assume that the feature acquisition cost C(f, x) is a modular function of the support of the features used by function f on example x, that is acquiring each feature has a fixed constant cost. Without the cost constraint, the problem is equivalent to a supervised learning problem, however, adding the cost constraint makes this a combinatorial problem (Xu et al., 2013). In practice, we are not given the distribution but instead are given a set of training data (x1, y1), . . . , (xn, yn) drawn IID with (xi, yi)\nd\u223c H . We can then minimize the empirical loss subject to a budget constraint:\nmin f\u2208F\n1\nn n\u2211 i=1 L(yi, f(xi)), s.t. 1 n n\u2211 i=1 C (f, xi) \u2264 B. (2)\nIn our context the classifier f is a random forest, T , consisting of K random trees, D1, D2, . . . , DK , that are learnt on training data. Consequently, the expected cost for an instance x during prediction-time can be written as follows:\nEf [Ex [C (f, x)]] \u2264 K\u2211 j=1 EDj [Ex [C (Dj , x)]] (3)\nwhere, in the RHS we are averaging with respect to the random trees. As the trees in a random forest are identically distributed the RHS scales with the number of trees. This upper-bound captures the typical behavior of a random forest due to the low feature correlation among trees.\nAs a result of this observation, the problem of learning a budgeted random forest can be viewed as equivalent to the\nproblem of finding decision trees with low expected evaluation cost and error. This motivates our algorithm BUDGETRF, where greedily constructed decision trees with provably low feature acquisition cost are added until the budget constraint is met according to validation data. The returned random forest is a feasible solution to (1) with strong empirical performance."}, {"heading": "2.1. Our Algorithm", "text": "During Training: As shown in Algorithm 1, there are seven inputs to BUDGETRF: impurity function F , prediction-time feature acquisition budget B, a cost vector C \u2208 <m that contains the acquisition cost of each feature, training class labels ytr and data matrixXtr \u2208 <n\u00d7m, where n is the number of samples and m is the number of features, validation class labels ytv and data matrix Xtv . Note that the impurity function F needs to be admissible, which essentially means monotone and supermodular. We defer the formal definition and theoretical results to Section 2.2. For now it is helpful to think of an impurity function F as measuring the heterogeneity of a set of examples. Intuitively, F is large for a set of examples with mostly different labels and small for a set with mostly the same label.\nAlgorithm 1 BUDGETRF 1: procedure BUDGETRF(F,B,C, ytr,Xtr, ytv,Xtv) 2: T \u2190 \u2205. 3: while Average cost using validation set on T \u2264 B\ndo 4: Randomly sample n training data with replace-\nment to form X(i) and y(i). 5: Train T \u2190 GREEDYTREE(F,C, y(i), X(i)). 6: T \u2190 T \u222a T . 7: return T \\T .\nSubroutine - GREEDYTREE 8: procedure GREEDYTREE(F,C, y,X ) 9: S \u2190 (y,X) . the current set of examples 10: if F (S) = 0 then return 11: for each feature t = 1 to m do 12: Compute R(t) := min\ngt\u2208Gt max i\u2208outcomes c(t) F (S)\u2212F (Sigt ) ,\n. risk for feature t 13: where Sigt is the set of examples in S that has\noutcome i using classifier gt with feature t. 14: t\u0302\u2190 argmintR(t) 15: g\u0302 \u2190 argmin\ngt\u0302\u2208Gt\u0302 max\ni\u2208outcomes c(t\u0302) F (S)\u2212F (Sig t\u0302 )\n16: Make a node using feature t\u0302 and classifier g\u0302. 17: for each outcome i of g\u0302 do 18: GREEDYTREE(F,C, yig\u0302, X i g\u0302) to append as\nchild nodes.\nBUDGETRF iteratively builds decision trees by calling\nGREEDYTREE as a subroutine on a sampled subset of examples from the training data until the budget B is exceeded as evaluated using the validation data. The ensemble of trees are then returned as output. As shown in subroutine GREEDYTREE, the tree building process is greedy and recursive. If the given set of examples have zero impurity as measured by F , they are returned as a leaf node. Otherwise, compute the risk R(t) for each feature t, which involves searching for a classifier gt among the family of classifiers Gt that minimizes the maximum impurity among its outcomes. Intuitively, a feature with the least R(t) can uniformly reduce the impurity among all its child nodes the most with the least cost. Therefore such a feature t\u0302 is chosen along with the corresponding classifier g\u0302. The set of examples are then partitioned using g\u0302 to different child nodes at which GREEDYTREE is recursively applied. Note that we allow the algorithm to reuse the same feature for the same example in GREEDYTREE.\nDuring Prediction: Given a test example and a decision forest T returned by BUDGETRF, we run the example through each tree in T and obtained a predicted label from each tree. The final predicted label is simply the majority vote among all the trees.\nDifferent from random forest, we incorporate feature acquisition costs in the tree building subroutine GREEDYTREE with the hope of reducing costs while maintaining low classification error. Our main theoretical contribution is to propose a broad class of admissible impurity functions such that on any given set of n\u2032 examples the tree constructed by GREEDYTREE will have max-cost bounded by O(log n\u2032) times the optimal max-cost tree."}, {"heading": "2.2. Bounding the Cost of Each Tree", "text": "Given a set of examples S with features and corresponding labels, a classification tree D has a feature-classifier pair associated with each internal node. A test example is routed from the root of D to a leaf node directed by the outcomes of the classifiers along the path; the test example is then labeled to be the majority class among training examples in the leaf node it reaches. The feature acquisition cost of an example s \u2208 S on D, denoted as cost(D, s), is the sum of all feature costs incurred along the root-to-leaf path in D traced by s. Note that if s encounters a feature multiple times in the path, the feature cost contributes to cost(D, s) only once because subsequent use of a feature already acquired for the test example incurs no additional cost. We define the total max-cost as\nCost(D) = max s\u2208S cost(D, s).\nWe aim to build a decision tree for any given set of examples such that the max-cost is minimized. Note that the max-cost criterion bounds the expected cost criterion of\nEq. 3. While this bound could be loose we show later (see Sec. 2.4) that by parameterizing a suitable class of impurity functions, the max-costs of our GREEDYTREE solution can be \u201csmoothened\u201d so that it approaches the expected-cost.\nFirst define the following terms: n\u2032 is the number of examples input to GREEDYTREE and m is the number of features, each of which has (a vector of) real values; F is the given impurity function; F (S) is the impurity on the set of examples S; DF is the family of decision trees with F (L) = 0 for any of its leaf L; each feature has a cost c(t); a family of classifiers Gt is associated with feature t; CostF (S) is the max-cost of the tree constructed by GREEDYTREE using impurity function F on S; and assume no feature is used more than once on the same example in the optimal decision tree among DF that achieves the minimum max-cost, which we denote as OPT (S) for the given input set of examples S. Note the assumption here is a natural one if the complexity of Gt is high enough. We show the O(log n\u2032) approximation holds for the maxcost of the optimal testing strategy using the GREEDYTREE subroutine if the impurity function F is admissible.\nDefinition A function F of a set of examples is admissible if it satisfies the following five properties: (1) Nonnegativity: F (G) \u2265 0 for any set of examplesG; (2) Purity: F (G) = 0 if G consists of examples of the same class; (3) Monotonicity: F (G) \u2265 F (R),\u2200R \u2286 G; (4) Supermodularty: F (G \u222a j) \u2212 F (G) \u2265 F (R \u222a j) \u2212 F (R) for any R \u2286 G and example j /\u2208 R; (5) log(F (S)) = O(log n\u2032).\nSince the set S is always finite, by scaling F we can assume the smallest non-zero impurity of F is 1. Let \u03c4 and g\u0302\u03c4 be the first feature and classifier selected by GREEDYTREE at the root and let Sig\u0302\u03c4 be the set of examples in S that has outcome i using classifier g\u0302\u03c4 . Note the optimization of classifier in Line (12) of Algorithm 1 needs not to be exact. We say GREEDYTREE is \u03bb-greedy if g\u0302\u03c4 is chosen such that\nmax i\u2208outcomes\nc(\u03b3)\nF (S)\u2212 F (Sig\u0302\u03c4 ) \u2264 min gt\u2208Gt max i\u2208outcomes\n\u03bbc(t)\nF (S)\u2212 F (Sigt) ,\nfor some constant \u03bb \u2265 1. By definition of max-cost,\nCostF (S) OPT (S) \u2264 c(\u03c4) + max i CostF (S i g\u0302\u03c4 ) OPT (S) ,\nbecause feature \u03c4 could be selected multiple times by GREEDYTREE along a path and the feature cost c(\u03c4) contributes only once to the cost of the path.\nLet q be such that CostF (S q g\u0302\u03c4 ) = max\ni CostF (S i g\u0302\u03c4 ). We\nfirst provide a lemma to lower bound the optimal cost, which will later be used to prove a bound on the cost of the tree.\nLemma 2.1 Let F be monotone and supermodular; let \u03c4 and g\u0302\u03c4 be the first feature and classifier chosen by GREEDYTREE \u03bb-greedily on the set of examples S, then\nc(\u03c4)F (S)/(F (S)\u2212 F (Sqg\u0302\u03c4 )) \u2264 \u03bbOPT (S).\nProof Let D\u2217 \u2208 DF be a tree with optimal max-cost. Let v be an arbitrarily chosen internal node in D\u2217, let \u03b3 be the feature associated with v and g\u2217\u03b3 the corresponding classifier. Let R \u2286 S be the set of examples associated with the leaves of the subtree rooted at v. Let i be such that c(\u03c4)/(F (S) \u2212 F (Sig\u0302\u03c4 )) is maximized. Let gmin\u03b3 = argmin\ng\u03b3\u2208G\u03b3 max\ni\u2208outcomes c(\u03b3) F (S)\u2212F (Sig\u03b3 ) . Let w be such that\nc(\u03b3)/(F (S)\u2212 F (Swgmin\u03b3 )) is maximized; similarly let j be such that c(\u03b3)/(F (S) \u2212 F (Sjg\u2217\u03b3 )) is maximized. We then have:\nc(\u03c4) F (S)\u2212 F (Sqg\u0302\u03c4 ) \u2264 c(\u03c4) F (S)\u2212 F (Sig\u0302\u03c4 ) \u2264 \u03bbc(\u03b3) F (S)\u2212 F (Swgmin\u03b3 )\n\u2264 \u03bbc(\u03b3) F (S)\u2212 F (Sjg\u2217\u03b3 ) \u2264 \u03bbc(\u03b3) F (R)\u2212 F (Rjg\u2217\u03b3 ) . (4)\nThe first inequality follows from the definition of i. The second inequality follows from the \u03bb-greedy choice at the root. The third inequality follows from the minimization over classifiers given feature \u03b3. To show the last inequality, we have to show F (S)\u2212F (Sjg\u2217\u03b3 ) \u2265 F (R)\u2212F (R j g\u2217\u03b3 ). This follows from the fact that Sjg\u2217\u03b3 \u222aR \u2286 S andR j g\u2217\u03b3\n= Sjg\u2217\u03b3 \u2229R and therefore F (S) \u2265 F (Sjg\u2217\u03b3 \u222a R) \u2265 F (S j g\u2217\u03b3 ) + F (R) \u2212 F (Rjg\u2217\u03b3 ), where the first inequality follows from monotonicity and the second follows from the definition of supermodularity.\nFor a node v, let S(v) be the set of examples associated with the leaves of the subtree rooted at v. Let v1, v2, . . . , vp be a root-to-leaf path on D\u2217 as follows: v1 is the root of the tree, and for each i = 1, . . . , p \u2212 1 the node vi+1 is a child of vi associated with the branch of j that maximizes c(ti)/(F (S)\u2212F (Sjg\u2217ti )), where ti is the test associated with vi. It follows from (4) that\n[F (S(vi))\u2212 F (S(vi+1))]c(\u03c4) \u03bb(F (S)\u2212 F (Sqg\u0302\u03c4 )) \u2264 cti . (5)\nSince the cost of the path from v1 to vp is no larger than the max cost of the D\u2217, we have that\nOPT (S) \u2265 p\u22121\u2211 i=1 cti\n\u2265 c(\u03c4) \u03bb(F (S)\u2212 F (Sqg\u0302\u03c4 )) p\u22121\u2211 i=1 (F (S(vi))\u2212 F (S(vi+1)) = c(\u03c4)(F (S)\u2212 F (S(vp)) \u03bb(F (S)\u2212 F (Sqg\u0302\u03c4 )) = c(\u03c4)F (S) \u03bb(F (S)\u2212 F (Sqg\u0302\u03c4 )) .\nThe main theorem of this section is the following.\nTheorem 2.2 GREEDYTREE constructs a decision tree achieving O(log n\u2032)-factor approximation of the optimal max-cost in DF on the set S of n\u2032 examples if F is admissible and no feature is used more than once on any path of the optimal tree.\nProof This is an inductive proof:\nCostF (S) OPT (S) \u2264 c(\u03c4) + CostF (S\nq g\u0302\u03c4 )\nOPT (S) (6)\n\u2264 c(\u03c4) OPT (S) + CostF (S\nq g\u0302\u03c4 )\nOPT (Sqg\u0302\u03c4 ) (7)\n\u2264 \u03bb F (S)\u2212 F (Sqg\u0302\u03c4 ) F (S) + CostF (S q g\u0302\u03c4 )\nOPT (Sqg\u0302\u03c4 ) (8)\n\u2264 \u03bb log( F (S) F (Sqg\u0302\u03c4 ) ) + \u03bb log(F (Sqg\u0302\u03c4 )) + 1 (9) = \u03bb log(F (S)) + 1 = O(log(n\u2032)). (10)\nThe inequality in (7) follows from the fact that OPT (S) \u2265 OPT (Sqg\u0302\u03c4 ). (8) follows from Lemma 2.1. The first term in (9) follows from the inequality xx+1 \u2264 log(1 + x) for x > \u22121 and the second term follows from the induction hypothesis that for each G \u2282 S, CostF (G)/OPT (G) \u2264 \u03bb log(F (G)) + 1. If F (G) = 0 for some set of examples G, we define CostF (G)/OPT (G) = 1.\nWe can verify the base case of the induction as follows. if F (G) = 1, which is the smallest non-zero impurity of F on subsets of examples S, we claim that the optimal decision tree chooses the feature with the smallest cost among those that can reduce the impurity function F :\nOPT (G) = min t|\u2203gt,s.t. F (Gigt )=0,\u2200i\u2208outcomes c(t).\nSuppose otherwise, the optimal tree chooses first a feature t with a child nodeG\u2032 such that F (G\u2032) = 1 and later chooses another feature t\u2032 such that all the child nodes of G\u2032 by gt\u2032 has zero impurity, then t\u2032 could have been chosen in the first place to reduce all child nodes of G to zero impurity by supermodularity of F . On the other hand, R(t) =\u221e in GREEDYTREE for the features that cannot reduce impurity and R(t) = c(t) for those features that can. So the algorithm would pick the feature among those that can reduce impurity and have the smallest cost. Thus, we have shown that CostF (G)/OPT (G) = 1 \u2264 \u03bb log(F (G)) + 1 for the base case."}, {"heading": "2.3. Admissible Impurity Functions", "text": "A wide range of functions falls into the class of admissible impurity functions. We employ a particular function called\nthreshold-Pairs in our paper defined as\nF\u03b1(G) = \u2211 i6=j [[niG \u2212 \u03b1]+[n j G \u2212 \u03b1]+ \u2212 \u03b1 2]+, (11)\nwhere niG denotes the number of objects in G that belong to class i, [x]+ = max(x, 0) and \u03b1 is a threshold parameter. We include the proof of the following lemma in the Appendix.\nLemma 2.3 F\u03b1(G) is admissible.\nNeither entropy nor Gini index satisfies the notion of admissibility because they are not monotonic set functions, that is a subset of examples does not necessarily have a smaller entropy or Gini index compared to the entire set. Therefore traditional decision tree learning algorithms do not incorporate feature costs and have no guarantee on the max-cost as stated in our paper. We have studied more impurity functions that are admissible such as the polynomials and Powers family of functions. After conducting experiments on smaller datasets we noted that they do not offer significant advantage over the threshold-Pairs used in this paper. Please see Appendix for more details."}, {"heading": "2.4. Discussions of the Algorithm", "text": "Before concluding the BUDGETRF algorithm and its analysis, we discuss further various design issues as well as their implications.\nChoice of threshold \u03b1. In subroutine GREEDYTREE, each tree is greedily built until a minimum leaf impurity is met, then added to the random forest. The threshold \u03b1 can be used to trade-off between average tree depth and number of trees. A lower \u03b1 results in deeper trees with higher classification power and acquisition cost. As a result, fewer trees are added to the random forest before the budget constraint is met. Conversely, a higher \u03b1 yields shallower trees with poorer classification performance, however due to the low cost of each tree, many are added to the random forest before the budget constraint is met. As such, \u03b1 can be viewed as a bias-variance trade-off. In practice, it is selected using validation dataset.\nAnother observation we make is that the choice of \u03b1 can potentially lead to different feature choice when used in GREEDYTREE. To illustrate this point, consider the toy example in Figure 1. A set G has 30 examples in class 1 (circles) and 30 examples in Class 2 (triangles). Two features t1 and t2 are available to the algorithm at equal cost. Feature t1 has only one classifier in Gt1 as drawn on the upper left of the figure, which can separate 20 examples of Class 2 from the rest of the examples while t2 has only one classifier in Gt2 as drawn on the lower left of the figure, which evenly divides the examples into halves with equal\nnumber of examples from Class 1 and Class 2 in either half. Intuitively, t2 is not a useful feature from a classification point of view because it cannot separate examples based on class at all. This is reflected in the right plot of Figure 1: choosing t2 increases cost but does not reduce classification error while choosing t1 reduces the error to 16 . If \u03b1 is set to 0 in the threshold-Pairs, feature t2 will be chosen due to the fact that Pairs biases towards feature-classifiers with balanced outcomes. In contrast, setting \u03b1 = 8 leads to feature t1, and therefore may be preferable (see Appendix).\nMinimax-splits. The splitting criterion in the subroutine GREEDYTREE is based on the worst case impurity among child nodes, we call such splits minimax-splits as opposed to expected-splits, which is based on the expected impurity among child nodes. Using minimax-splits, our theoretical guarantee is a bound on the max-cost of individual trees. Note such minimax-splits have been shown to lead to expected-cost bound as well in the setting of GBS (Nowak, 2008); an interesting future research direction is to show whether minimax-splits can lead to a bound on the expected-cost of individual trees in our setting.\nSmoothened Max-Costs. We emphasize that by adjusting \u03b1 in threshold-Pairs function - essentially allowing some error, the max-costs of the GREEDYTREE solution can be \u201csmoothened\u201d so that it approaches the expectedcost. Consider the synthetic example as shown in Figure 2. Here we consider a multi-class classification example to demonstrate the effect of \u201csmoothened\u201d max-cost of the tree approaching the expected-cost. Consider a data set composed of 1024 examples belonging to 4 classes with 10 binary features available. Assume that is no two examples that have the same set of feature values. Note that by\nfixing the acquisition order of the features, the set of feature values maps each example to an integer in the range [0, 1023]. From this mapping, we give the examples in the ranges [1, 255] , [257, 511] , [513, 767], and [769, 1023] the labels 1, 2, 3, and 4, respectively, and the examples 0, 256, 512, and 768 the labels 2, 3, 4, and 1, respectively (Figure 2 shows the data projected to the first two features). Suppose each feature carries a unit cost. By Kraft\u2019s Inequality (Cover & Thomas, 1991), the optimal max-cost in order to correctly classify every object is 10, however, using only t1 and t2 as selected by the greedy algorithm, leads to a correct classification of all but 4 objects, as shown in Figure 3. Thus, the max-cost of the early stopped tree is only 2 - much closer to the expected-cost."}, {"heading": "3. Experiments", "text": "For establishing baseline comparisons we apply BUDGETRF on 4 real world benchmarked datasets. The first one has varying feature acquisition costs in terms of computation time and the purpose is to show our algorithm can achieve high accuracy during prediction while saving massive amount of feature acquisition time. The other 3 datasets do not have explicit feature costs; instead, we assign a unit cost to each feature uniformly. The purpose is to demonstrate our algorithm can achieve low test error using only a small fraction of features. Note our algorithm is adaptive, meaning it acquires different features for different examples during testing. So the feature costs in the plots\nshould be understood as an average of costs for all test examples. We use CSTC (Xu et al., 2013) and ASTC (Kusner et al., 2014) for comparison because they have been shown to have state-of-the-art cost-error performance. For comparison purposes we use the same configuration of training/validation/test splits as in ASTC/CSTC. The algorithm parameters for ASTC are set using the same configuration as in (Kusner et al., 2014). We report values for CSTC from (Kusner et al., 2014). In all our experiments we use the threshold-Pairs (11) as impurity function. We use stumps as the family of classifiers Gt for all features t. The optimization of classifiers in line 12 of Algorithm 1 is approximated by randomly generating 80, 40 and 20 stumps if the number of examples exceeds 2000, 500 and less than 500, respectively and select the best among them. All results from our algorithm were obtained by taking an average of 10 runs and standard deviations are reported using error bars.\nYahoo! Learning to Rank: (Chapelle et al.) We evaluate BUDGETRF on a real world budgeted learning problem: Yahoo! Learning to Rank Challenge 1. The dataset consists of 473, 134 web documents and 19, 944 queries.\n1http://webscope.sandbox.yahoo.com/catalog.php?datatype=c\nGiven a set of training query-document pairs together with relevance ranks of documents for each query, the Challenge is to learn an algorithm which takes a new query and its set of associated documents and outputs the rank of these documents with respect to the new query. Each example xi contains 519 features of a query-document pair. Each of these features is associated with an acquisition cost in the set {1, 5, 20, 50, 100, 150, 200}, which represents the units of time required for extraction and is provided by a Yahoo! employee. The labels are binarized so that yi = 0 means the document is unrelated to the query in xi whereas yi = 1 means the document is relevant to the query. There are 141, 397/146, 769/184, 968 examples in training/validation/test sets. We use the Average Precision@5 as performance metric, same as that used in (Kusner et al., 2014). To evaluate a predicted ranking for a test query, first sort the documents in decreasing order of the predicted ranks - that is, the more relevant documents predicted by the algorithm come before those that are deemed irrelevant. Take the top 5 documents in this order and reveal their true labels. If all of the documents are indeed relevant (y = 1), then the precision score is increased by 1; otherwise, if the first unrelated document appears in position 1 \u2264 j \u2264 5, increase the precision score by j\u221215 .\nFinally, the precision score is averaged over the set of test queries. We run BUDGETRF using the threshold \u03b1 = 0 for the threshold-Pairs impurity function. To incorporate prediction confidence we simply run a given test example through the forest of trees to leaf nodes and aggregate the number of training examples at these leaf nodes for class 0 and 1 seperately. The ratio of class 1 examples over the sum of class 1 and 0 examples gives the confidence of relevance. The comparison is shown in plot (a) of Figure 4. The precision for BUDGETRF rises much faster than ASTC and CSTC. At an average feature cost of 70, BUDGETRF already exceeds the precision that ASTC/CSTC can achieve using feature cost of 450 and more. In this experiment the maximum number of trees we build is 140; the precision is set to rise even higher if we were to use more trees. BUDGETRF thus represents a better ranking algorithm requiring much less wait time for users of the search engine.\nMiniBooNE Particle Identification Data Set: (Frank & Asuncion) The MiniBooNE data set is a binary classification task, with the goal of distinguishing electron neutrinos (signal) from muon neutrinos (background). Each data point consists of 50 experimental particle identification variables (features). There are 45, 523/19, 510/65, 031 examples in training/validation/test sets. We apply BUDGETRF with a set of 10 values of \u03b1 = [0, 2, 4, 6, 8, 10, 15, 25, 35, 45]. For each \u03b1 we build a forest of maximum 40 trees using BUDGETRF. Each point on the BUDGETRF curve in (b) of Figure 4 corresponds to a \u03b1 setting and the number of trees that meet the budget level. The final \u03b1 is chosen using validation set. Our algorithm clearly achieves lower test error than both ASTC and CSTC on every point of the budget level. Indeed, using just about 6 features on average out of 50 , BUDGETRF achieves lower test error than what can be achieved by ASTC or CSTC using any number of features.\nForest Covertype Data Set: (Frank & Asuncion) The Forest data set contains cartographic variables to predict 7 forest cover types. Each example contains 54 (10 continuous and 44 binary) features. There are 36, 603/15, 688/58, 101 examples in training/validation/test sets. We use the same \u03b1 values as in MiniBooNE. The final \u03b1 is chosen using validation set. In (c) of Figure 4, ASTC and CSTC struggles to decrease test error even at high feature budget whereas the test error of BUDGETRF decreases rapidly as more features are acquired. We believe this dramatic performance difference is partly due to the distinct advantage of BUDGETRF in handling mixed continuous and discrete (categorical) data where the optimal decision function is highly non-linear.\nCIFAR-10: (Krizhevsky, 2009) CIFAR-10 data set consists of 32x32 colour images in 10 classes. 400 features for each image are extracted using technique described in\n(Coates & Ng, 2011). The data are binarized by combining the first 5 classes into one class and the others into the second class. There are 19, 761/8, 468/10, 000 examples in training/validation/test sets. As shown in (d) of Figure 4 BUDGETRF initially has higher test error than ASTC when the budget is low; from a budget about 90 onward BUDGETRF outperforms ASTC while it outperforms CSTC on the entire curve. An important trend we see is that the errors for both ASTC and CSTC start to increase after some budget level. This indicates an issue of overfitting with these methods. We do not see such an issue with BUDGETRF.\nAs a general comment, we observe that in low-cost regions using higher \u03b1 achieves lower test error whereas setting \u03b1 = 0 leads to low test error at a higher cost. This is consistent with our intuition that setting a high value for \u03b1 terminates the tree building process early and thus saves on cost, as a consequence more trees can be built within the budget. But as budget increases, more and more trees are added to the forest, the prediction power does not grow as fast as setting \u03b1 to low values because the individual trees are not as powerful.\nComments on standard Random Forest Cost is not incorporated in the standard random forest (RF) algorithm. One issue that arises is how to incorporate budget constraint. Our strategy was to limit the number of trees in the RF to control the cost. But this does not work well even if the acquisition costs are uniform for all features. We implemented Matlab version of RF with the default settings on the Forest, MiniBooNE and CIFAR datasets: fraction of input data to sample with replacement from the input data for growing each new tree is 1; number of variables to select at random for each decision split is set to 8; minimum number of observations per tree leaf is 1. Compared to our BUDGETRF algorithm using threshold-Pairs impurity with \u03b1 = 0, the feature cost for RF is much higher as shown in Table 1. For example in the Forest experiment, after building 10 trees, RF uses 63.04% of total number of features for an average test example whereas BUDGETRF uses only 23.21%. In terms of test error BUDGETRF achieves 0.1364, 0.0786 and 0.3600 for Forest, MiniBooNE and CIFAR respectively using 10 trees, quite competitive to 0.1318, 0.0803 and 0.3594 obtained by RF.2 For Yahoo! Rank dataset, RF does even worse because some features have very high cost and yet RF still uses them just like the less expensive features, resulting in high cost.\n2average over 10 repeated runs of RF and BUDGETRF."}, {"heading": "4. Conclusion and Future Work", "text": "We propose a novel algorithm to solve the budgeted learning problem. Our approach is to build a random forest of low cost trees with theoretical guarantees. We demonstrate that our algorithm performance far exceeds the state-ofthe-art algorithms on 4 real world benchmarked datasets. While we have explored the greedy algorithm based on minimax-splits, similar algorithm can be proposed based on expected-splits. An interesting future work is to examine the theoretical and empirical properties of such algorithms."}], "references": [{"title": "Group-based active query selection for rapid diagnosis in time-critical situations", "author": ["G Bellala", "S.K. Bhavnani", "C. Scott"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Bellala et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellala et al\\.", "year": 2012}, {"title": "Fast classification using sparse decision dags", "author": ["R. Busa-Fekete", "D. Benbouzid", "B. K\u00e9gl"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2012}, {"title": "Classifier cascade: Tradeoff between accuracy and feature evaluation cost", "author": ["M. Chen", "Z. Xu", "K.Q. Weinberger", "O. Chapelle", "D. Kedem"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Diagnosis determination: decision trees optimizing simultaneously worst and expected testing cost", "author": ["F Cicalese", "Laber", "E. S", "A.M. Saettler"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Cicalese et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cicalese et al\\.", "year": 2014}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.G. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Coates and Ng,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng", "year": 2011}, {"title": "Datum-wise classification: a sequential approach to sparsity", "author": ["G. Dulac-Arnold", "L. Denoyer", "P. Preux", "P. Gallinari"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Dulac.Arnold et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2011}, {"title": "Active classification based on value of classifier", "author": ["T. Gao", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gao and Koller,? \\Q2011\\E", "shortCiteRegEx": "Gao and Koller", "year": 2011}, {"title": "Imitation learning by coaching", "author": ["H He", "H Daume III", "J. Eisner"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Cost-sensitive feature acquisition and classification", "author": ["S Ji", "L. Carin"], "venue": "Pattern Recognition,", "citeRegEx": "Ji and Carin,? \\Q2007\\E", "shortCiteRegEx": "Ji and Carin", "year": 2007}, {"title": "Prediction-time Active FeatureValue Acquisition for Cost-Effective Customer Targeting", "author": ["P. Kanani", "P. Melville"], "venue": "In Advances In Neural Information Processing Systems (NIPS),", "citeRegEx": "Kanani and Melville,? \\Q2008\\E", "shortCiteRegEx": "Kanani and Melville", "year": 2008}, {"title": "Breaking boundaries: Active information acquisition across learning and diagnosis", "author": ["A Kapoor", "E. Horvitz"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Kapoor and Horvitz,? \\Q2009\\E", "shortCiteRegEx": "Kapoor and Horvitz", "year": 2009}, {"title": "Dynamic feature selection for classification on a budget", "author": ["S Karayev", "M Fritz", "T. Darrell"], "venue": "In International Conference on Machine Learning (ICML): Workshop on Prediction with Sequential Models,", "citeRegEx": "Karayev et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karayev et al\\.", "year": 2013}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Krizhevsky", "Alex"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Feature-cost sensitive learning with submodular trees of classifiers", "author": ["M Kusner", "W Chen", "Q Zhou", "E Zhixiang", "K Weinberger", "Y. Chen"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Kusner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kusner et al\\.", "year": 2014}, {"title": "Information-based objective functions for active data selection", "author": ["D.J.C. MacKay"], "venue": "Neural computation,", "citeRegEx": "MacKay,? \\Q1992\\E", "shortCiteRegEx": "MacKay", "year": 1992}, {"title": "Greedy algorithm with weights for decision tree construction", "author": ["M.J. Moshkov"], "venue": "Fundam. Inf.,", "citeRegEx": "Moshkov,? \\Q2010\\E", "shortCiteRegEx": "Moshkov", "year": 2010}, {"title": "Generalized binary search", "author": ["Nowak", "Robert"], "venue": "Proceedings of the 46th Allerton Conference on Communications, Control, and Computing,", "citeRegEx": "Nowak and Robert.,? \\Q2008\\E", "shortCiteRegEx": "Nowak and Robert.", "year": 2008}, {"title": "Supervised sequential classification under budget constraints", "author": ["K Trapeznikov", "V. Saligrama"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Trapeznikov and Saligrama,? \\Q2013\\E", "shortCiteRegEx": "Trapeznikov and Saligrama", "year": 2013}, {"title": "Robust Real-time Object Detection", "author": ["P Viola", "M. Jones"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Viola and Jones,? \\Q2001\\E", "shortCiteRegEx": "Viola and Jones", "year": 2001}, {"title": "Model selection by linear programming", "author": ["J. Wang", "T. Bolukbasi", "K Trapeznikov", "V. Saligrama"], "venue": "In European Conference on Computer Vision, pp", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "An lp for sequential learning under budgets", "author": ["J Wang", "K Trapeznikov", "V. Saligrama"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Costsensitive tree of classifiers", "author": ["Z Xu", "M Kusner", "M Chen", "K.Q. Weinberger"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "A Survey of Recent Advances in Face Detection", "author": ["C. Zhang", "Zhang"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 14, "context": "Related Work: The problem of learning from full training data for prediction-time cost reduction (MacKay, 1992; Kanani & Melville, 2008) has been extensively studied.", "startOffset": 97, "endOffset": 136}, {"referenceID": 2, "context": "One simple structure for incorporating costs into learning is through detection cascades (Viola & Jones, 2001; Zhang & Zhang, 2010; Chen et al., 2012), where cheap features are used to discard examples belonging to the negative class.", "startOffset": 89, "endOffset": 150}, {"referenceID": 11, "context": "To overcome the need to estimate distributions, reinforcement learning (Karayev et al., 2013; BusaFekete et al., 2012; Dulac-Arnold et al., 2011) and imitation learning (He et al.", "startOffset": 71, "endOffset": 145}, {"referenceID": 5, "context": "To overcome the need to estimate distributions, reinforcement learning (Karayev et al., 2013; BusaFekete et al., 2012; Dulac-Arnold et al., 2011) and imitation learning (He et al.", "startOffset": 71, "endOffset": 145}, {"referenceID": 7, "context": ", 2011) and imitation learning (He et al., 2012) approaches have also been studied, where the reward or oracle action is predicted, however these generally require classifiers capable of operating on a wide range of missing feature patterns.", "startOffset": 31, "endOffset": 48}, {"referenceID": 3, "context": "Construction of simple decision trees with low costs has also been studied for discrete function evaluation problems (Cicalese et al., 2014; Moshkov, 2010; Bellala et al., 2012).", "startOffset": 117, "endOffset": 177}, {"referenceID": 15, "context": "Construction of simple decision trees with low costs has also been studied for discrete function evaluation problems (Cicalese et al., 2014; Moshkov, 2010; Bellala et al., 2012).", "startOffset": 117, "endOffset": 177}, {"referenceID": 0, "context": "Construction of simple decision trees with low costs has also been studied for discrete function evaluation problems (Cicalese et al., 2014; Moshkov, 2010; Bellala et al., 2012).", "startOffset": 117, "endOffset": 177}, {"referenceID": 21, "context": "Without the cost constraint, the problem is equivalent to a supervised learning problem, however, adding the cost constraint makes this a combinatorial problem (Xu et al., 2013).", "startOffset": 160, "endOffset": 177}, {"referenceID": 13, "context": "Comparison of BUDGETRF against ASTC (Kusner et al., 2014) and CSTC (Xu et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 21, "context": ", 2014) and CSTC (Xu et al., 2013) on 4 real world datasets.", "startOffset": 17, "endOffset": 34}, {"referenceID": 21, "context": "We use CSTC (Xu et al., 2013) and ASTC (Kusner et al.", "startOffset": 12, "endOffset": 29}, {"referenceID": 13, "context": ", 2013) and ASTC (Kusner et al., 2014) for comparison because they have been shown to have state-of-the-art cost-error performance.", "startOffset": 17, "endOffset": 38}, {"referenceID": 13, "context": "The algorithm parameters for ASTC are set using the same configuration as in (Kusner et al., 2014).", "startOffset": 77, "endOffset": 98}, {"referenceID": 13, "context": "We report values for CSTC from (Kusner et al., 2014).", "startOffset": 31, "endOffset": 52}, {"referenceID": 13, "context": "We use the Average Precision@5 as performance metric, same as that used in (Kusner et al., 2014).", "startOffset": 75, "endOffset": 96}], "year": 2015, "abstractText": "We seek decision rules for prediction-time cost reduction, where complete data is available for training, but during prediction-time, each feature can only be acquired for an additional cost. We propose a novel random forest algorithm to minimize prediction error for a user-specified average feature acquisition budget. While random forests yield strong generalization performance, they do not explicitly account for feature costs and furthermore require low correlation among trees, which amplifies costs. Our random forest grows trees with low acquisition cost and high strength based on greedy minimax cost-weighted-impurity splits. Theoretically, we establish near-optimal acquisition cost guarantees for our algorithm. Empirically, on a number of benchmark datasets we demonstrate superior accuracy-cost curves against state-of-the-art prediction-time algorithms.", "creator": "LaTeX with hyperref package"}}}