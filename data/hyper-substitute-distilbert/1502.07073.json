{"id": "1502.07073", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2015", "title": "Strongly Adaptive Online Learning", "abstract": "\\ emph { robust adaptive algorithms } is computers whose objective unit { \\ detection / error level } is close always optimal. we developed a reduction that can transform locally low - regret algorithms to strongly adaptive. as a consequence, we transform simple, easily efficient, strongly adaptive algorithms without a handful of functions.", "histories": [["v1", "Wed, 25 Feb 2015 07:24:40 GMT  (18kb)", "http://arxiv.org/abs/1502.07073v1", null], ["v2", "Mon, 8 Jun 2015 15:10:11 GMT  (18kb)", "http://arxiv.org/abs/1502.07073v2", null], ["v3", "Fri, 19 Jun 2015 07:31:45 GMT  (19kb)", "http://arxiv.org/abs/1502.07073v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit daniely", "alon gonen", "shai shalev-shwartz"], "accepted": true, "id": "1502.07073"}, "pdf": {"name": "1502.07073.pdf", "metadata": {"source": "CRF", "title": "Strongly Adaptive Online Learning", "authors": ["Amit Daniely", "Alon Gonen", "Shai Shalev-Shwartz"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n07 07\n3v 1\n[ cs\n.L G\n] 2\n5 Fe\nStrongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems."}, {"heading": "1 Introduction", "text": "Coping with changing environments and rapidly adapting to changes is a key component in so many undertakings. A broker is highly rewarded from rapidly adjusting to new trends. A reliable routing algorithm must respond quickly to congestion. A web advertiser should adjust itself to new ads and to changes in the taste of its users. A politician can also benefit from quickly adjusting to changes in the public opinion. And the list goes on.\nMost current algorithms and theoretical analysis focus on relatively stationary environments. In statistical learning, an algorithm should perform well on the training distribution. Even in online learning, an algorithm should usually compete with the best strategy (from a pool), that is fixed and does not change over time.\nOur main focus is to investigate to which extent such algorithms can be modified to cope with changing environments.\nWe consider a general online learning framework that encompasses various online learning problems including prediction with expert advice, online classification, online convex optimization and more. In this framework, a learning scenario is defined by a decision set D, a context space C and a set L of realvalued loss functions defined overD. The learner sequentially observes a context ct P C and then picks an action xt P D. Next, a loss function \u2113t P L is revealed and the learner suffers a loss \u2113tpxtq.\nOften, algorithms in such scenarios are evaluated by comparing their performance to the performance of the best strategy from a pool of strategies (usually,\n\u02daDept. of Mathematics, The Hebrew University, Jerusalem, Israel :School of Computer Science, The Hebrew University, Jerusalem, Isreal ;School of Computer Science, The Hebrew University, Jerusalem, Isreal\nthis pool is simply all strategies that play the same action all the time). Concretely, the regret, RApT q, of an algorithm A is defined as its cumulative loss minus the cumulative loss of the best strategy in the pool. The rational behind this evaluation metric is that one of the strategies in the pool is reasonably good during the entire course of the game. However, when the environment is changing, different strategies will be good in different periods. As we do not want to make any assumption on the duration of each of these periods, we would like to guarantee that our algorithm performs well on every interval I \u201c rq, ss \u0102 rT s. Clearly, we cannot hope to have a regret bound which is better than what we have for algorithms that are tested only on I. If this barrier is met, we say that the corresponding algorithm is strongly adaptive1 .\nSurprisingly maybe, our main result shows that for many learning problems strongly adaptive algorithms exist. Concretely, we show a simple \u201cmetaalgorithm\u201d that can use any online algorithm (that was possibly designed to have just small standard regret) as a black box, and produces a new algorithm that is designed to have a small regret on every interval. We show that if the original algorithm have a regret bound of RpT q, then the produced algorithm has, on every interval rq, ss of size \u03c4 :\u201c |I|, regret that is very close to Rp\u03c4q (see a precise statement in Section 1.2). Moreover, the running time of the new algorithm at round t is just O plogptqq times larger than that of the original algorithm. As an immediate corollary we obtain strongly adaptive algorithms for a handful of online problems including prediction with expert advice, online convex optimization, and more.\nFurthermore, we show that strong adaptivity is stronger than previously suggested adaptivity properties including the adaptivity notion of [HS07] and the tracking notion of [HW98]. Namely, strongly adaptive algorithms are also adaptive (in the sense of [HS07]), and have a near optimal tracking regret (in the sense of [HW98]). We conclude our discussion by showing that strong adaptivity can not be achieved with bandit feedback."}, {"heading": "1.1 Problem setting", "text": ""}, {"heading": "A Framework for Online Learning", "text": "Many learning problems can be described as a repeated game between the learner and the environment, which we describe below.\nA learning scenario is determined by a triplet pD,C,Lq, whereD is a decision space, C is a set of contexts, and L is a set of loss functions from D to r0, 1s. Extending the results to general bounded losses is straightforward. The number of rounds, denoted T , might be 8 and is unknown to the learner. At each time t P rT s, the learner sees a context ct P C, and then chooses an action xt P D. Simultaneously, the environment chooses a loss function \u2113t P L. Then, the action xt is revealed to the environment, and the loss function \u2113t is revealed\n1See a precise definition in Section 1.1. Also, see Section 1.3 for a weaker notion of adaptive algorithms that was studied in [HS07].\nto the learner which suffers the loss \u2113tpxtq. We list below some examples of families of learning scenarios.\n\u2022 Learning with expert advice [CBFH`97]. Here, there is no context (formally, C consists of a single element), D is a finite set of size N (each element in this set corresponds to an expert), and L consists of all functions from D to r0, 1s.\n\u2022 Online convex optimization [Zin03]. Here, there is no context as well, D is a convex set, and L is a collection of convex functions from D to r0, 1s.\n\u2022 Classification. Here, C is some set, D is a finite set, and L consists of all functions from D to t0, 1u that are indicators of a single element.\n\u2022 Regression. Here, C is a subset of an Euclidean space, D \u201c r0, 1s, and L consists of all functions of the form \u2113py\u0302q \u201c py \u00b4 y\u0302q2 for y P r0, 1s.\nA learning problem is a quadruple P \u201c pD,C,L,Wq, where W is a benchmark of strategies that is used to evaluate the performance of algorithms. Here, each strategy w P W makes a prediction xtpwq P D based on some rule. We make the assumption that the prediction xt of each strategy is fully determined by the game\u2019s history at the time of the prediction. I.e., by pc1, \u21131q, . . . , pct\u00b41, \u2113t\u00b41q, ct. Usually, W consists of very simple strategies. For example, in context-less scenarios (like learning with expert advice and online convex optimization), W is often identified with D, and the strategy corresponding to x P D simply predicts x at each step. In contextual problems (such as classification and regression), W is often a collection of functions from C to D (a hypothesis class), and the prediction of the strategy corresponding to h : C \u00d1 D at time t is simply hpctq.\nThe cumulative loss of w P W at time T is LwpT q \u201c \u0159T t\u201c1 \u2113tpxtpwqq and the cumulative loss of an algorithm A is LApT q \u201c \u0159T\nt\u201c1 \u2113tpxtq. The cumulative regret of A is RApT q \u201c LApT q \u00b4 infwPW LwpT q. We define the regret, RPpT q, of the learning problem P as the best possible regret bound. Namely, RPpT q is the minimal number for which there exists an algorithm A such that for every environment RApT q \u010f RPpT q. We say that an algorithm A has low regret if RApT q \u201c O ppoly plogT qRPpT qq for every environment.\nWe note that both the learner and the environment can make random decisions. In that case, the quantities defined above refer to the expected value of the corresponding terms.\nStrongly Adaptive Regret\nLet I \u201c rq, ss :\u201c tq, q ` 1, . . . , su \u010e rT s. The loss of w P W during the interval I is LwpIq \u201c \u0159s\nt\u201cq \u2113tpxtpwqq and the loss of an algorithm A during the interval I is LApIq \u201c \u0159r\nt\u201cq \u2113tpxtq. The regret of A during the interval I is RApIq \u201c LApIq \u00b4 infwPW LwpIq. The strongly adaptive regret of A at time T is the function\nSA-RegretTAp\u03c4q \u201c max I\u201crq,q`\u03c4\u00b41s\u0102rT s RApIq\nWe say that A is strongly adaptive if for every environment, SA-RegretTAp\u03c4q \u201c O ppoly plogT q \u00a8 RPp\u03c4qq."}, {"heading": "1.2 Our Results", "text": ""}, {"heading": "A strongly adaptive meta-algorithm", "text": "Achieving strongly adaptive regret seems more challenging than ensuring low regret. Nevertheless, we show that often, low-regret algorithms can be transformed into a strongly adaptive algorithms with a little extra computational cost.\nConcretely, fix a learning scenario pD,C,Lq. We derive a strongly adaptive meta-algorithm, that can use any algorithm B (that presumably have low regret w.r.t. some learning problem) as a black-box. We call our meta-algorithm Strongly Adaptive Online Learner (SAOL). The specific instantiation of SAOL that uses B as the black box is denoted SAOLB.\nFix a set W of strategies and an algorithm B whose regret w.r.t. W satisfies\nRBpT q \u010f C \u00a8 T\u03b1, (1)\nwhere \u03b1 P p0, 1q, and C \u0105 0 is some scalar. The properties of SAOLB are summarized in the theorem below. The description of the algorithm and the proof of Theorem 1 are given in Section 2.\nTheorem 1\n1. For every interval I \u201c rq, ss \u010e N,\nRSAOLBpIq \u010f 4\n2\u03b1 \u00b4 1C|I| \u03b1 ` 40 logps` 1q|I| 12 .\n2. In particular, if \u03b1 \u011b 1 2 and B has low regret, then SAOLB is strongly\nadaptive.\n3. The runtime of SAOL at time t is at most logpt ` 1q times the runtime per-iteration of B.\nFrom part 2, we can derive strongly adaptive algorithms for many online problems. Two examples are outlined below.\n\u2022 Prediction with N experts advice. The Multiplicative Weights (MW) algorithm has regret \u010f 2 a lnpNqT . Hence, for every I \u201c rq, ss \u010e rT s,\nRSAOLMWpIq \u201c O \u00b4\u00b4 a logpNq ` logps` 1q \u00af a |I| \u00af .\n\u2022 Online convex optimization with G-Lipschitz loss functions over a convex set D \u010e Rd of diameter B. Online Gradient Descent (OGD) has regret \u010f 3BG ? T . Hence, for every I \u201c rq, ss \u010e rT s,\nRSAOLOGDpIq \u201c O \u00b4 pBG` logps` 1qq a |I| \u00af .\nComparison to (weak) adaptivity and tracking\nSeveral alternative measures for coping with changing environment were proposed in the literature. The two that are most related to our work are tracking regret [HW98] and adaptive regret [HS07] (other notions are briefly discussed in Section 1.3).\nAdaptivity, as defined in [HS07], is a weaker requirement than strong adaptivity. The adaptive regret of a learner A at time T is maxI\u010erT s RApIq. An algorithm is called adaptive if its adaptive regret is O ppoly plogT qRPpT qq. For online convex optimization problems for which there exists an algorithm with regret bound RpT q, [HS07] derived an efficient algorithm whose adaptive regret is at most RpT q logpT q `O \u02c6 b T log3pT q \u02d9 , thus establishing adaptive algorithms\nfor many online convex optimization problems. For the case where the loss functions are \u03b1-exp concave, they showed an algorithm with adaptive regret Op 1\n\u03b1 log2pT qq (we note that according to our definition this algorithm is in fact strongly adaptive). A main difference between adaptivity and strong adaptivity, is that in many problems, adaptive algorithms are not guaranteed to perform well on small intervals. For example, for many problems including online convex optimization and learning with expert advice, the best possible adaptive regret is \u2126p ? T q. Such a bound is meaningless for intervals of size Op ? T q. We note that in many scenarios (e.g. routing, paging, news headlines promotion) it is highly desired to perform well even on very small intervals.\nThe problem of \u201ctracking the best expert\u201d was studied in [HW98] (see also, [BW03]). In that problem, originally formulated for the learning with expert advice problem, learning algorithms are compared to all strategies that shift from one expert to another a bounded number of times. They derived an efficient algorithm, named Fixed-Share, which attains near-optimal regret bound of a\nTmplogpT q ` logpNqq versus the best strategy that shifts between \u010f m experts. (Interestingly, a recent work [CBGLS12] showed that the Fixed-Share algorithm is in fact (weakly) adaptive). As we show in Section 3, strongly adaptive algorithms enjoys near-optimal tracking regret in the experts problem, and in fact, in many other problems (e.g., online convex optimization). We note that as with (weakly) adaptive algorithms, algorithms with optimal tracking regret are not guaranteed to perform well on small intervals.\nStrong adaptivity with bandit feedback\nIn the so-called bandit setting, the loss functions \u2113t is not exposed to the learner. Rather, the learner just gets to see the loss, \u2113tpxtq, that he has suffered. In Section 4 we prove that there are no strongly adaptive algorithms that can cope with bandit feedback. Even in the simple experts problem we show that for every \u01eb \u0105 0, there is no algorithm whose strongly adaptive regret is O ` \u03c41\u00b4\u01eb \u00a8 poly plogT q \u02d8\n. Investigating possible alternative notions and/or weaker guarantees in the bandit setting is mostly left for future work."}, {"heading": "1.3 Related Work", "text": "Maybe the most relevant previous work, from which we borrowmany of our techniques is [BM07]. They focused on the expert setting and proposed a strengthened notion of regret using time selection functions, which are functions from the time interval rT s to r0, 1s. The regret of a learner A with respect to a time selection function I is defined byRI\nA pT q \u201c maxiPrNs\n\u00b4\n\u0159T t\u201c1 Iptq\u2113tpxtq \u00b4 \u0159T t\u201c1 Iptq\u2113tpiq \u00af ,\nwhere \u2113tpiq is the loss of expert i at time t. This setting can be viewed as a generalization of the sleeping expert setting [FSSW97]. For a fixed set I consisting of M time selection functions, they proved a regret bound of Op a\n|I| logpNMqq`logpNMqq with respect to each time selection function I P I. We observe that if we let I be the set of all indicator functions of intervals (note that |I| \u201c `\nT 2\n\u02d8\n\u201c \u0398pT 2q), we obtain a strongly adaptive algorithm for learning with expert advice. However, the (multiplicative) computational overhead of our algorithm (w.r.t. the standard MW algorithm) at time t is \u0398plogptqq, whereas the computational overhead of their algorithm is \u0398pT 2q. Furthermore, our setting is much more general than the expert setting.\nAnother related, but somewhat orthogonal line of work [Zin03, HW13, RS13, JRSS15] studies drifting environments. The focus of those papers is on scenarios where the environment is changing slowly over time."}, {"heading": "2 Reducing Adaptive Regret to Standard Re-", "text": "gret\nIn this section we present our strongly adaptive meta-algorithm, named Strongly Adaptive Online Learner (SAOL). For the rest of this section we fix a learning scenario pD,C,Lq and an algorithm B that operates in this scenario (think of B as a low regret algorithm).\nWe first give a high level description of SAOL. The basic idea is to run an instance of B on each interval I from an appropriately chosen set of intervals, denoted I. The instance corresponding to I is denoted BI , and can be thought as an expert that gives his advice for the best action at each time slot in I. The algorithm weights the various BI \u2019s according to their performance in the past, in a way that instances with better performance get more weight. The exact weighting is a variant of the multiplicative weights rule. At each step, SAOL chooses at random one of the BI \u2019s and follows his advice. The probability of choosing each BI is proportional to its weight. Next, we give more details.\nThe choice of I. The weighting procedure will ensure that SAOL will perform optimally of every I P I. Therefore, the choice of I exhibits the following tradeoff. On one hand, I should be large, since we want that optimal performance on intervals in I will result in an optimal performance on every interval. On the other hand, we would like to keep I small, since running many instances of B in parallel will result with a large computational cost. To balance\nthese desires, we let I \u201c \u010f\nkPNYt0u\nIk ,\nwhere for all k P NY t0u,\nIk \u201c \u010f\niPN\nri \u00a8 2k, pi` 1q \u00a8 2k \u00b4 1s\nWe denote by ACTIVEptq :\u201c tI : t P Iu ,\nthe set of active intervals at time t. We note that2 |ACTIVEptq| \u201c tlogptqu ` 1. Hence, the running time of SAOL is at time t is at most plogptq ` 1q times larger than the running time of B. On the other hand, as we show in the proof, we can cover every interval by intervals from I, in a way that will guarantee small regret on the covered interval, provided that we have small regret on the covering intervals.\nThe weighting method. Let xtpIq be the action taken by BI at time t. The instantaneous regret of SAOL w.r.t. BI at time t is rtpIq \u201c \u2113tpxtq\u00b4\u2113tpxtpIqq (maybe it is more natural to look on \u2113tpxtpIqq\u00b4\u2113tpxtq, yet it would be technically easier to work with \u2113tpxtq \u00b4 \u2113tpxtpIqq). As explained above, SAOL maintains weights over the BI \u2019s. For I \u201c rq, ss, the weight of BI at time t is denoted wtpIq. For t \u0103 q, BI is not active yet, so we let wtpIq \u201c 0. At time t \u201c q, we set wtpIq \u201c \u03b7I :\u201c min \"\n1{2, 1? |I|\n*\n. The update rule for any t P rq, sq, is\nwt`1pIq \u201c wtpIqp1 ` \u03b7I \u00a8 rtpIqq . (2)\nOverall, we have\nwtpIq \u201c\n$\n\u2019 &\n\u2019 % 0 t R I \u03b7I t \u201c q wt\u00b41pIqp1` \u03b7I \u00a8 rt\u00b41pIqq t P pq, ss\n(3)\nwhere \u03b7I :\u201c min \"\n1{2, 1? |I|\n*\n. Note that the regret is always between r\u00b41, 1s, and \u03b7I P p0, 1q, therefore weights are always positive during the lifetime of the corresponding expert. Also, the weight of BI decreases (increases) if its loss is higher (lower) than the predicted loss.\nThe overall weight at time t is defined byWt \u201c \u0159 IPI wtpIq \u201c \u0159 IPACTIVEptq wtpIq. Finally, a probability distribution over the experts at time t is defined by\nptpIq \u201c wtpIq Wt .\n2Indeed, t is contained in at most one interval from each Ik and in zero intervals from Ik for k \u0105 logptq.\nNote that the probability mass assigned to any inactive instance is zero. The probability distribution pt determines the action of SAOL at time t. Namely, we have xt \u201c xtpIq with probability ptpIq. A pseudo-code of SAOL is detailed in Algorithm 1.\nAlgorithm 1 Strongly Adaptive Online Learner (with blackbox algorithm B)\nInitialize: w1pIq \u201c # 1{2 I \u201c r1, 1s 0 o.w. for t \u201c 1 to T do Let Wt \u201c \u0159\nIPACTIVEptq wtpIq Choose I P ACTIVEptq w.p. ptpIq \u201c wtpIqWt Predict xtpIq. Update weights according to Equation (3)\nend for"}, {"heading": "2.1 Proof Sketch of Theorem 1", "text": "In this section we sketch the proof of Theorem 1. A full proof is detailed in Appendix A. The analysis of SAOL is divided into two parts. The first challenge is to prove the theorem for the intervals in I (see Lemma 2). Then, the theorem should be extended to any interval (end of Appendix A).\nLet us start with the first task. Our first observation is that for every interval I, the regret of SAOL during the interval I is equal to\n(SAOL\u2019s regret relatively to Bi ` the regret of Bi) (4)\n(during the interval I). Since the regret of BI during the interval I is already guaranteed to be small (Equation (1)), the problem of ensuring low regret during each of the intervals in I is reduced to the problem of ensuring low regret with respect to each of the BI \u2019s (during I).\nWe next prove that the regret of SAOL with respect to the B1is is small. Our analysis is similar to the proof of [BM07][Theorem 16]. Both of these proofs are similar to the analysis of the Multiplicative Weights Update (MW) method. The main idea is to define a potential function and relate it both to the loss of the learner and the loss of the best expert.\nTo this end, we start by defining pseudo-weights over the experts (the BI \u2019s). With a slight abuse of notation, we define Iptq \u201c 1rtPIs. For any I \u201c rq, ss P I, the pseudo-weight of BI is defined by:\nw\u0303tpIq \u201c\n$\n\u2019 \u2019 \u2019 &\n\u2019 \u2019 \u2019 % 0 t \u0103 q 1 t \u201c q w\u0303t\u00b41pIq \u00a8 p1` \u03b7I \u00a8 rt\u00b41pIqq s` 1 \u011b t \u0105 q w\u0303spIq t \u0105 s` 1\nNote that wtpIq \u201c \u03b7I \u00a8 Iptq \u00a8 w\u0303tpIq . The potential function we consider is the overall pseudo-weight at time t, W\u0303t \u201c \u0159\nIPI w\u0303tpIq. The following lemma is a useful consequence of our definitions. Lemma 1 For every t \u011b 1,\nW\u0303t \u010f tplogptq ` 1q . Through straightforward calculations, we conclude the proof of Theorem 1 for any interval in I.\nLemma 2 For every I \u201c rq, ss P I, s \u00ff\nt\u201cq\nrtpIq \u010f 5 logps` 1q a |I| .\nHence, according to Equation (4),\nRSAOLB pIq \u010f C \u00a8 |I|\u03b1 ` 5 logps` 1q a |I|\nThe extension of the theorem to any interval relies on some useful properties of the set I (see Lemma 4). Roughly speaking, any interval I \u010e rT s can be partitioned into two sequences of intervals from I, such that the lengths of the intervals in each sequence decay at an exponential rate (Lemma 5). The theorem now follows by bounding the regret during the interval I by the sum of the regrets during the intervals in the above two sequences, and by using the fact that the lengths decay exponentially."}, {"heading": "3 Strongly Adaptive Regret Is Stronger Than", "text": "Tracking Regret\nIn this section we relate the notion of strong adaptivity to that of tracking regret, and show that algorithms with small strongly adaptive regret also have small tracking regret. Let us briefly review the problem of tracking. For simplicity, we concentrate on context-less learning problems, and on the case where the set of strategies coincides with the decision space (though the result can be straightforwardly generalized). Fix a decision space D and a family L of loss functions. A compound action is a sequence \u03c3 \u201c p\u03c31, . . . , \u03c3T q P DT . Since there is no hope in competing w.r.t. all sequences3, a typical restriction of the problem is to bound the number of switches in each sequence. For a positive integer m, the class of compound actions with at most m switches is defined by\nBm \u201c # \u03c3 P DT : sp\u03c3q :\u201c T\u00b41 \u00ff\nt\u201c1\n1r\u03c3t`1\u2030\u03c3ts \u010f m + . (5)\n3It is easy to prove a lower bound of order T for this problem\nThe notions of loss and regret naturally extend to this setting. For example, the cumulative loss of a compound action \u03c3 P Bm is defined by L\u03c3pT q \u201c \u0159T\nt\u201c1 \u2113tp\u03c3tq. The tracking regret of an algorithm A w.r.t. the class Bm is defined by\nTracking-RegretmA pT q \u201c LApT q \u00b4 inf \u03c3PBm L\u03c3pT q .\nThe following theorem bounds the tracking regret of algorithms with bounds on the strongly adaptive regret. In particular, of SAOL.\nTheorem 2 Let A be a learning algorithm with SA-RegretAp\u03c4q \u010f C\u03c4\u03b1. Then,\nTracking-RegretmA pT q \u010f CT\u03b1m1\u00b4\u03b1\nProof Let \u03c3 P Bm. Let I1, . . . , Im be the intervals that correspond to \u03c3. Clearly, the tracking regret w.r.t. \u03c3 is bounded by the sum of the regrets of during the intervals I1, . . . , Im. Hence, and using Ho\u0308lder\u2019s inequality, we have\nLApT q \u00b4 L\u03c3pT q \u010f m \u00ff\ni\u201c1\nRApIiq\n\u010f C m \u00ff\ni\u201c1\n|Ii|\u03b1\n\u010f C \u02dc m \u00ff\ni\u201c1\n1 1 1\u00b4\u03b1\n\u00b81\u00b4\u03b1 \u02dc m \u00ff\ni\u201c1\n|Ii| \u00b8\u03b1\n\u010f Cm1\u00b4\u03b1T\u03b1\nRecall that for the problem of prediction with expert advice, the strongly adaptive regret of SAOL (with, say, Multiplicative Weights as a black box) is O \u00b4 p a lnpNq ` logpT qq?\u03c4 \u00af . Hence, we obtain a tracking bound ofO \u00b4 p a lnpNq ` logpT qq ? mT \u00af .\nUp to a a logpT q factor, this bound is asymptotically equivalent to the bound of the Fixed-Share Algorithm of [HW98]4. Also, up to logpT q factor, the bound is optimal. One advantage of SAOL over Fixed-Share is that SAOL is parameterfree. In particular, SAOL does not need to know5 m."}, {"heading": "4 Strongly Adaptive Regret in The Bandit Set-", "text": "ting\nIn this section we consider the challenge of achieving adaptivity in the bandit setting. Following our notation, in the bandit setting, only the loss incured by\n4For the comparison, we rely on a simplified form of the bound of the Fixed-Share algorithm. This simplified form can be found, for example, in http://web.eecs.umich.edu/~jabernet/eecs598course/web/notes/lec5_091813.pdf\n5The parameters the Fixed-Share do depend on m\nthe learner, \u2113tpxtq, is revealed at the end of each round (rather then the loss function, \u2113t). For many online learning problems for which there exists an efficient low-regret algorithm in the full information model, a simple reduction from the bandit setting to the full information setting (for example, see [SS11][Theorem 4.1]) yields an efficient low-regret bandit algorithm. Furthermore, it is often the case that the dependence of the regret on T is not affected by the lack of information. For example, for the Multi-armed bandit (MAB) problem [ACBFS02] (which is the bandit version of the the problem of prediction with expert advice), the above reduction yields an algorithm with near optimal regret bound of 2 ? TN logN .\nA natural question is whether adaptivity can be achieved with bandit feedback. Few positive results are known. For example, applying the aforementioned reduction on the Fixed-Share algorithm results with an efficient bandit learner whose tracking regret is O \u00b4 a TmplnpNq ` lnpT qqN \u00af .\nThe next theorem shows that with bandit feedback there are no algorithms with non-trivial bounds on the strongly adaptive regret. We focus on the MAB problem with two arms (experts) but it is easy to generalize the result to any nondegenerate online problem. Recall that in this problem is context-less, that W \u201c D \u201c te1, e2u and that L \u201c r0, 1sD. Theorem 3 For all \u01eb \u0105 0, there is no algorithm for MAB with strongly adaptive regret of O ` \u03c41\u00b4\u01ebpoly plogT q \u02d8 .\nThe idea of the proof is simple. Suppose toward a contradiction that A is an algorithm with strongly adaptive regret of O ` \u03c41\u00b4\u01ebpoly plogT q \u02d8\n. This means that the regret of A on every interval I of length T \u01eb 2 is non trivial (i.e. op|I|q). Intuitively, this means that both arms must be inspected at least once during I. Suppose now that one of the arms is always superior to the second (say, has loss zero while the other has loss one). By the above argument, the algorithm will still inspect the bad arm at least once in every T \u01eb 2 time slots. Those inspections will result in a regret of T T \u01eb 2\n\u201c T 1\u00b4 \u01eb2 . This, however, is a contradiction, since the strongly adaptive regret bound implies that the standard regret of A is o ` T 1\u00b4 \u01eb 2 \u02d8\n. This idea is formalized in the following lemma. It implies Theorem 3 as for A\nwith strongly adaptive regret of O ` \u03c41\u00b4\u01ebpoly plogT q \u02d8 we can take k \u201c O ` T 1\u00b4 \u01eb 2 \u02d8 and reach a contradiction as the lemma implies that on some segment I of size T k \u201c \u2126 ` T \u01eb 2 \u02d8 , the regret of A is \u2126 ` T \u01eb 2 \u02d8 which grows faster than |I|1\u00b4\u01ebpolyplog T q\nLemma 3 Let A be an algorithm with regret bounded\nRApT q \u010f k \u201c kpT q ,\nThen, there exists an interval I \u010e rT s of size \u2126pT {kq with\nRApIq \u201c \u2126p|I|q .\nProof Assume for simplicity that 4k divides T . Consider the environment E0 , in which @t, \u2113tpe1q \u201c 0.5, \u2113tpe2q \u201c 1. Let U \u0102 rT s be the (possibly random)\nset of time slots in which the algorithm chooses e2 when the environment is E0. Since the regret is at most k, we have ErU s \u010f 2k. It follows that for some segment I \u0102 rT s of size \u011b T\n4k we have Er|U X I|s \u010f 1 2 . Indeed, otherwise, if\nrT s \u201c I1 Y\u0308 . . . Y\u0308 I4k is the partition of the interval rT s into 4k disjoint and consecutive intervals of size T\n4k we will have Er|U |s \u201c \u01594kj\u201c1 Er|U X Ij |s \u0105 2k.\nNow, since |U X I| is a non-negative integer, w.p. \u011b 1 2 we have |U X I| \u201c 0. Namely, w.p. \u011b 1 2 A does not inspect e2 during the interval I when it runs against E0. Consider now the environment E that is identical to E0, besides that @t P I, ltpe2q \u201c 0. By the argument above, w.p. \u011b 12 , the operation of A on E is identical to its operation on E0. In particular, the regret on I when A plays against E is, w.p. \u011b 1 2 , |I| 2 , and in total, \u011b 1 2 \u00a8 1 2 \u00a8 |I|."}, {"heading": "Acknowledgments", "text": "We thank Yishay Mansour and Sergiu Hart for valuable discussions."}, {"heading": "A Proof of Theorem 1", "text": "A.1 Proving Theorem 1 to Any Interval in I\nProof (of Lemma 1) The proof is by induction on t. For t \u201c 1, we have W\u03031 \u201c w\u03031pr1, 1sq \u201c 1 .\nNext, we assume that the claim holds for any t1 \u010f t and prove it for t`1. Since |trq, ss P I : q \u201c tu| \u010f tlogptqu ` 1 for all t \u011b 1, we have\nW\u0303t`1 \u201c \u00ff\nI\u201crq,ssPI\nw\u0303t`1pIq\n\u201c \u00ff\nI\u201crt`1,ssPI\nw\u0303t`1pIq ` \u00ff\nI\u201crq,ssPI: q\u010ft\nw\u0303t`1pIq\n\u010f logpt` 1q ` 1` \u00ff\nI\u201crq,ssPI: q\u010ft\nw\u0303t`1pIq .\nNext, according to the induction hypothesis, we have \u00ff\nI\u201crq,ssPI: q\u010ft\nw\u0303t`1pIq \u201c \u00ff\nI\u201crq,ssPI: q\u010ft\nw\u0303tpIqp1 ` \u03b7I \u00a8 Iptq \u00a8 rtpIqq\n\u201c W\u0303t ` \u00ff\nIPI\n\u03b7I \u00a8 Iptq \u00a8 rtpIq \u00a8 w\u0303tpIq\n\u010f tplogptq ` 1q ` \u00ff\nIPI\nwtpIq \u00a8 rtpIq .\nHence,\nW\u0303t`1 \u010f tplogptq ` 1q ` logpt` 1q ` 1` \u00ff\nIPI\nwtpIq \u00a8 rtpIq\n\u010f pt` 1qplogpt` 1q ` 1q ` \u00ff\nIPI\nwtpIq \u00a8 rtpIq .\nWe complete the proof by showing that \u0159 IPI wtpIq \u00a8 rtpIq \u201c 0. Since xt \u201c xI,t with probability ptpIq for every I P I, we obtain\n\u00ff\nIPI\nwtpIq \u00a8 rIptq \u201c Wt \u00ff\nIPI\nptpIqp\u2113tpxtq \u00b4 \u2113tpxtpIqqq\n\u201c Wtp\u2113tpxtq \u00b4 \u2113tpxtqq \u201c 0 .\nCombining the above inequalities, we conclude the lemma.\nProof (of Lemma 2) Fix some I \u201c rq, ss P I. We need to show that s \u00ff\nt\u201cq\nrtpIq \u010f 5 logps` 1q a |I| .\nSince weights are non-negative, we clearly have\nw\u0303s`1pIq \u010f W\u0303s`1 \u010f ps` 1qplogps` 1q ` 1q ,\nHence, lnpw\u0303s`1pIqq \u010f lnps` 1q ` lnplogps` 1q ` 1q . (6)\nwhere the last inequality follows from Lemma 1. Next, we note that\nw\u0303s`1pIq \u201c s \u017a\nt\u201cq\np1` \u03b7I \u00a8 Iptq \u00a8 rtpIqq \u201c s \u017a\nt\u201cq\np1` \u03b7I \u00a8 rtpIqq .\nNoting that \u03b7I P p0, 1{2q and using the inequality lnp1`xq \u011b x\u00b4x2 which holds for every x P p0, 1{2q, we obtain\nlnpw\u0303s`1pIqq \u201c s \u00ff\nt\u201cq\nlnp1 ` \u03b7I \u00a8 rtpIqq\n\u011b s \u00ff\nt\u201cq\n\u03b7I \u00a8 rtpIq \u00b4 s \u00ff\nt\u201cq\np\u03b7I \u00a8 rtpIqq2\n\u011b \u03b7Ip s \u00ff\nt\u201cq\nrtpIq \u00b4 \u03b7I |I|q . (7)\nCombining Equation (7) and Equation (6) and dividing by \u03b7I , we obtain\ns \u00ff\nt\u201cq\nrtpIq \u010f \u03b7I |I| ` \u03b7\u00b41plnps` 1q ` lnplogps` 1q ` 1qq\n\u010f \u03b7I |I| ` \u03b7\u00b41plogps` 1q ` logps` 1qq \u010f \u03b7I |I| ` 2\u03b7\u00b41 logps` 1q ,\nwhere the second inequality follows from the inequality x \u011b lnp1 ` xq. Substituting \u03b7I :\u201c min \"\n1{2, 1? |I|\n*\n, we conclude the lemma.\nA.2 Extending The Theorem to Any Interval\nIn the next part we complete the proof of Theorem 1 by extending Lemma 2 to every interval.\nBefore proceeding, we set up an additional notation and also make some simple but useful observations regarding the properties of the set I (defined in Section 2).\nFor an interval J \u010e N, we define the restriction of I to J by I|J . That is, I|J \u201c tI P I : I \u010e Ju. We next list some useful properties of the set I that follow immediately from its definition (thus, we do not prove these claims).\nLemma 4\n1. The size of every interval I P I is 2j for some j P NY t0u.\n2. For every j P NYt0u, the left endpoint of the the leftmost interval I whose size is 2j is 2j. Thus, the size of every interval which is located to the left of I is smaller than |I| \u201c 2j.\n3. Let I \u201c rq, ss P I be an interval and let I 1 \u201c rq1, q\u00b4 1s be another interval of size 2j |I| for some j \u010f 0. Then, I 1 P I.\n4. Let I P I be an interval and let I 1 \u201c rs` 1, s1s be a consecutive interval of size 2j|I| for some j \u010f 0. Then, I 1 P I.\n5. Let I \u201c rq, ss P I be an interval of size 2j for some j P N Y t0u. Then, (exactly) one of the intervals rq, q` 2j`1\u00b4 1s, rs` 1, s` 2j`1s (whose size is 2j`1) belongs to I.\nThe following lemma is a key tool for extending Lemma 2 to any interval.\nLemma 5 Let I \u201c rq, ss \u010e N be an arbitrary interval. Then, the interval I can be paritioned into two finite sequences of disjoint and consecutive intervals, denoted pI\u00b4k, . . . , I0q \u010e I|I and pI1, I2, . . . , Ipq \u010e I|I , such that\np@i \u011b 1q |I\u00b4i|{|I\u00b4i`1| \u010f 1{2 .\np@i \u011b 2q |Ii|{|Ii\u00b41| \u010f 1{2 .\nWe next prove the lemma. Whenever we mention Property 1, . . . , 5, we refer to Property 1, . . . , 5 of Lemma 4. Proof Let b0 \u201c maxt|I 1| : I 1 P I|Iu be the maximal size of any interval I 1 P I that is contained in I. Among all of these intervals, let I0 be the leftmost interval, i.e., we define\nq0 :\u201c argmintq1 : rq1, q1 ` b0 \u00b4 1s P I|Iu s0 \u201c q0 ` b0 \u00b4 1 Io \u201c rq0, s0s .\nStarting from q0 \u00b4 1, we define a sequence of disjoint and consecutive intervals (in a reversed order), denoted pI\u00b41, . . . , I\u00b4kq, as follows:\nrq\u00b41, s\u00b41s :\u201c I\u00b41 :\u201c argmax\nI1\u201crq1,s1sPI|rq,q0\u00b41s:\ns1\u201cq0\u00b41\n|I 1|\n...\nrq\u00b4i, s\u00b4is :\u201c I\u00b4i :\u201c argmax\nI1\u201crq1,s1sPI|rq,q\u00b4i`1\u00b41s:\ns1\u201cq\u00b4i`1\u00b41\n|I 1|\n...\nClearly, this sequence is finite and the left endpoint of the leftmost interval, I\u00b4k, is q. Denote the size of I\u00b4i by b\u00b4i. We next prove that for every i \u011b 1, b\u00b4i{b\u00b4i`1 \u201c 2j for some j \u010f \u00b41. We note that according to Property 1, it suffices to show that b\u00b4i \u0103 b\u00b4i`1 for every i \u011b 1. We use induction. The base case follows from the minimality of I0. We next assume that the claim holds for every i P t1, . . . , k\u00b4 1u and prove for k. Assume by contradiction that b\u00b4k \u011b b\u00b4k`1. Consider the interval I\u0302\u00b4k`1 which is obtained by concatenating a copy of I\u00b4k`1 to its left\n6. It follows that I\u0302\u00b4k`1 is an interval of size 2b\u00b4k`1 which is contained in rq, q\u00b4k`2 \u00b4 1s and its right endpoint is q\u00b4k`2 \u00b4 1. According to the induction hypothesis, |I\u0302\u00b4k`1| \u201c 2b\u00b4k`1 \u201c 2j \u00a8 b\u00b4k`2 for some j \u010f 0. It follows from Property 3 that I\u0302\u00b4k`1 P I|I , contradicting the maximality of I\u00b4k`1.\nSimilarly, starting from s0 ` 1, we define a sequence of disjoint and consec6Formally, I\u0302\u00b4k`1 :\u201c rq\u00b4k`1 \u00b4 b\u00b4k`1, q\u00b4k`1 \u00b4 1s Y I\u00b4k`1.\nutive intervals, denoted pI1, . . . , Ipq:\nrq1, s1s :\u201c I1 :\u201c argmax\nI1\u201crq1,s1sPI|rs0`1,ss:\nq1\u201cs0`1\n|I 1|\n...\nrqi, sis :\u201c Ii :\u201c argmax\nI1\u201crq1,s1sPI|rsi\u00b41`1,ss:\nq1\u201csi\u00b41`1\n|I 1|\n...\nClearly, this sequence is finite and the right endpoint of the rightmost interval, Ip, is s. Denote the size of Ii by bi. We next prove that for every i \u011b 2, bi{bi\u00b41 \u201c 2j for some j \u010f \u00b41. According to Property 1, it suffices to prove that bi \u0103 bi\u00b41 for every i \u011b 2. For this purpose, we first note that b1 \u010f b0; this follows immediately from the definition of b0. Hence, we may assume that bi{bi\u00b41 P t2j : j \u010f 0u for every i P t1, . . . , p \u00b4 1u and prove that bp \u0103 bp\u00b41. Assume by contradiction that bp \u011b bp\u00b41. Consider the interval I\u0302p\u00b41 which is obtained by concatenating a copy of Ip\u00b41 to its right. It follows that I\u0302p\u00b41 is an interval of size 2bp\u00b41 which is contained in rsp\u00b42 ` 1, ss and its left endpoint is sp\u00b42 ` 1. According to the induction hypothesis, |I\u0302p\u00b41| \u201c 2bp\u00b41 \u201c 2j \u00a8 bp\u00b42 for some j \u010f 1. We need to consider the following two cases:\n\u2022 Assume first that j \u010f 0 (thus, bp\u00b41{bp\u00b42 \u010f 1{2). Then, it follows from Property 4 that I\u0302p\u00b41 P I|I , contradicting the maximality of Ip\u00b41.\n\u2022 Assume that j \u201c 1 (i.e., bp\u00b41 \u201c bp\u00b42). Then, using Property 5, we obtain a contradiction to the maximality of Ik\u00b42.\nWe are now ready to complete the proof of Theorem 1. Proof (of Theorem 1) Consider an arbitrary interval I \u201c rq, ss \u010e rT s, and let I \u201c \u0164\u00a8 pi\u201c\u00b4k Ii be the partition described in Lemma 5. Then,\nRSAOLBpIq \u010f \u00ff\ni\u010f0\nRSAOLBpIiq\n` \u00ff\ni\u011b1\nRSAOLBpIiq . (8)\nWe next bound the first term in the the right-hand side of Equation (8). Ac-\ncording to Lemma 2, we obtain that\n\u00ff\ni\u010f0\nRSAOLB pIiq \u010f C \u00ff\ni\u010f0\n|Ii|\u03b1\n` 5 \u00ff\ni\u010f0\nlogpsi ` 1q|Ii|1{2\n\u010f C \u00ff\ni\u010f0\n|Ii|\u03b1\n` 5 logps` 1q \u00ff\ni\u010f0\n|Ii|1{2 .\nAccording to Lemma 5,\n\u00ff\ni\u010f0\n|Ii|\u03b1 \u010f 8 \u00ff\ni\u201c0\np2\u00b4i|I|q\u03b1\n\u201c 2 \u03b1\n2\u03b1 \u00b4 1 |I| \u03b1\n\u010f 2 2\u03b1 \u00b4 1 |I| \u03b1 .\nSimilarly, we have\n\u00ff\ni\u010f0\n|Ii|1{2 \u010f ? 2?\n2\u00b4 1 |I|1{2 \u010f 4|I| 12 .\nCombining the three last inequalities, we obtain that\n\u00ff\ni\u010f0\nRSAOLBpIiq \u010f 2\n2\u03b1 \u00b4 1C|I| \u03b1 ` 20 logps` 1q|I| 12 .\nThe second term of the right-hand side of Equation (8) is bounded identically. Hence,\nRSAOLBpIq \u010f 4\n2\u03b1 \u00b4 1C|I| \u03b1 ` 40 logps` 1q|I| 12 ."}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "From external to internal regret", "author": ["Avrim Blum", "Yishay Monsour"], "venue": "Journal of Machine Learning,", "citeRegEx": "Blum and Monsour.,? \\Q2007\\E", "shortCiteRegEx": "Blum and Monsour.", "year": 2007}, {"title": "Tracking a small set of experts by mixing past posteriors", "author": ["Olivier Bousquet", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Warmuth.,? \\Q2003\\E", "shortCiteRegEx": "Bousquet and Warmuth.", "year": 2003}, {"title": "How to use expert advice", "author": ["Nicolo Cesa-Bianchi", "Yoav Freund", "David Haussler", "David P Helmbold", "Robert E Schapire", "Manfred K Warmuth"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "A new look at shifting", "author": ["Nicolo Cesa-Bianchi", "Pierre Gaillard", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "regret. CoRR,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "Using and combining predictors that specialize", "author": ["Yoav Freund", "Robert E Schapire", "Yoram Singer", "Manfred K Warmuth"], "venue": "In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Adaptive algorithms for online decision problems", "author": ["Elad Hazan", "C Seshadhri"], "venue": "In Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "Hazan and Seshadhri.,? \\Q2007\\E", "shortCiteRegEx": "Hazan and Seshadhri.", "year": 2007}, {"title": "Tracking the best expert", "author": ["Mark Herbster", "Manfred K Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Herbster and Warmuth.,? \\Q1998\\E", "shortCiteRegEx": "Herbster and Warmuth.", "year": 1998}, {"title": "Online optimization in dynamic environments", "author": ["Eric C Hall", "Rebecca M Willett"], "venue": "arXiv preprint arXiv:1307.5944,", "citeRegEx": "Hall and Willett.,? \\Q2013\\E", "shortCiteRegEx": "Hall and Willett.", "year": 2013}, {"title": "Online optimization: Competing with dynamic comparators", "author": ["Ali Jadbabaie", "Alexander Rakhlin", "Shahin Shahrampour", "Karthik Sridharan"], "venue": "arXiv preprint arXiv:1501.06225,", "citeRegEx": "Jadbabaie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jadbabaie et al\\.", "year": 2015}, {"title": "Optimization, learning, and games with predictable sequences", "author": ["Sasha Rakhlin", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2013}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": null, "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [], "year": 2017, "abstractText": "Strongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems.", "creator": "LaTeX with hyperref package"}}}