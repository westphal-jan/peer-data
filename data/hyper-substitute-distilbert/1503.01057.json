{"id": "1503.01057", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2015", "title": "Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)", "abstract": "gs introduce a new neural logic interpolation ( eta ) framework, which recognizes and unifies inducing point solutions for scalable model equations ( mw ). ski applications make kernel approximations demonstrating fast modifications of kernel algorithm. the ski framework addresses how the dependence of an inducing point approach matters on the magnitude of inducing ( temporal interpolation ) objects, interpolation processes, specifically gp with kernel. ds also provides a mechanism toward create new clustered kernel structures, fundamentally choosing different kernel sampling strategies. using kin, with some cubic kernel algorithm, we introduce lal - ma, which is 1 ) quantum primitive computational geometric path agents, 2 ) naturally constructed kern and wallis algebra for substantial kernel aspects in scalability, namely requiring any grid reduction, and 3 ) might be adaptive given fast and expressive scaling learning. pick - gp costs o ( n ) input and storage for reliable inference. we write kiss - gp with temporal simulation approximation, gradient learning, using natural gradient modelling.", "histories": [["v1", "Tue, 3 Mar 2015 19:06:17 GMT  (1333kb,D)", "http://arxiv.org/abs/1503.01057v1", "19 pages, 4 figures"]], "COMMENTS": "19 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["andrew gordon wilson", "hannes nickisch"], "accepted": true, "id": "1503.01057"}, "pdf": {"name": "1503.01057.pdf", "metadata": {"source": "CRF", "title": "Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)", "authors": ["Andrew Gordon Wilson", "Hannes Nickisch"], "emails": ["andrewgw@cs.cmu.edu", "hannes@nickisch.org"], "sections": [{"heading": null, "text": "We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling."}, {"heading": "1 Introduction", "text": "Gaussian processes (GPs) are exactly the types of models we want to apply to big data: flexible function approximators, capable of using the information in large datasets to learn intricate structure through interpretable and expressive covariance kernels. However, O(n3) and O(n2) computation and storage requirements limit GPs to all but the smallest datasets, containing at most a few thousand training points n. Their impressive empirical successes thus far are only a glimpse of what might be possible, if only we could overcome these computational limitations (Rasmussen, 1996).\nInducing point methods (Snelson and Ghahramani, 2006; Hensman et al., 2013; Quin\u0303onero-Candela and Rasmussen, 2005; Seeger, 2005; Smola and Bartlett, 2001;\nar X\niv :1\n50 3.\n01 05\n7v 1\n[ cs\n.L G\n] 3\nM ar\nSilverman, 1985) have been introduced to scale up Gaussian processes to larger datasizes. These methods cost O(m2n + m3) computations and O(mn + m2) storage, for m inducing points, and n training data points. Inducing methods are popular for their general purpose \u201cout of the box\u201d applicability, without requiring any special structure in the data. However, these methods are limited by requiring a small m n number of inducing inputs, which can cause a deterioration in predictive performance, and the inability to perform expressive kernel learning (Wilson et al., 2014).\nStructure exploiting approaches for scalability, such as Kronecker (Saatchi, 2011) or Toeplitz (Cunningham et al., 2008) methods, have orthogonal advantages to inducing point methods. These methods exploit the existing structure in the covariance kernel for highly accurate and scalable inference, and can be used for flexible kernel learning on large datasets (Wilson et al., 2014). However, Kronecker methods require that inputs (predictors) are on a multidimensional lattice (a Cartesian product grid), which makes them inapplicable to most datasets. Although Wilson et al. (2014) has extended Kronecker methods for partial grid structure, these extensions do not apply to arbitrarily located inputs. Likewise, the Kronecker based approach in Luo and Duraiswami (2013) involves costly rank-1 updates and is not generally applicable for arbitrarily located inputs. Toeplitz methods are similarly restrictive, requiring that the data are on a regularly spaced 1D grid.\nIt is tempting to assume we could place inducing points on a grid, and then take advantage of Kronecker or Toeplitz structure for further gains in scalability. However, this naive approach only helps reduce the m3 complexity term in inducing point methods, and not the more critical m2n term, which arises from a matrix of cross covariances between training and inducing inputs.\nIn this paper, we introduce a new unifying framework for inducing point methods, called structured kernel interpolation (SKI). This framework allows us to improve the scalability and accuracy of fast kernel methods, and to naturally combine the advantages of inducing point and structure exploiting approaches. In particular,\n\u2022 We show how current inducing point methods can be interpreted as performing a global GP interpolation on a true underlying kernel to create an approximate kernel for scalable computations, as part of a more general family of structured kernel interpolation methods.\n\u2022 The SKI framework helps us understand how the accuracy and efficiency of an inducing point method is affected by the number of inducing points m, the choice of kernel, and the choice of interpolation method. Moreover, by choosing different interpolation strategies for SKI, we can create new inducing point methods.\n\u2022 We introduce a new inducing point method, KISS-GP, which uses local cubic and inverse distance weighting interpolation strategies to create a sparse approximation to the cross covariance matrix between the inducing points and original training points. This method can naturally be combined with Kronecker and\nToeplitz algebra to allow for m n inducing points, and further gains in scalability. When exploiting Toeplitz structure KISS-GP requires O(n + m logm) computations and O(n + m) storage. When exploiting Kronecker structure, KISS-GP requires O(n + Pm1+1/P ) computations and O(n + Pm2/P ) storage, for P > 1 dimensional inputs.\n\u2022 KISS-GP can be viewed as lifting the grid restrictions in Toeplitz and Kronecker methods, so that one can use arbitrarily located inputs.\n\u2022 We show that the ability for KISS-GP to efficiently use a large number of inducing points enables expressive kernel learning, and orders of magnitude greater accuracy and efficiency over popular alternatives such as FITC (Snelson and Ghahramani, 2006).\n\u2022 We have implemented code as an extension to the GPML toolbox (Rasmussen and Nickisch, 2010).\n\u2022 Overall, the simplicity and generality of the SKI framework makes it easy to design scalable Gaussian process methods with high accuracy and low computational costs.\nWe start in section 2 with background on Gaussian processes (section 2.1), inducing point methods (section 2.2), and structure exploiting methods (section 2.3). We then introduce the structured kernel interpolation (SKI) framework, and the KISS-GP method, in section 3. In section 4 we conduct experiments on kernel matrix reconstruction, kernel learning, and natural sound modelling. We conclude in section 5."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Gaussian Processes", "text": "We provide a brief review of Gaussian processes (Rasmussen and Williams, 2006), and the associated computational requirements for inference and learning. Throughout we assume we have a dataset D of n input (predictor) vectors X = {x1, . . . ,xn}, each of dimension D, corresponding to a n\u00d7 1 vector of targets y = (y(x1), . . . , y(xn))>.\nA Gaussian process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. Using a GP, we can define a distribution over functions f(x) \u223c GP(\u00b5, k), meaning that any collection of function values f has a joint Gaussian distribution:\nf = f(X) = [f(x1), . . . , f(xn)] > \u223c N (\u00b5, K) . (1)\nThe n \u00d7 1 mean vector \u00b5i = \u00b5(xi), and n \u00d7 n covariance matrix Kij = k(xi,xj), are defined by the user specified mean function \u00b5(x) = E[f(x)] and covariance kernel\nk(x,x\u2032) = cov(f(x), f(x\u2032)) of the Gaussian process. The smoothness and generalisation properties of the GP are encoded by the covariance kernel and its hyperparameters \u03b8. For example, the popular RBF covariance function, with length-scale hyperparameter `, has the form\nkRBF(x,x \u2032) = exp(\u22120.5||x\u2212 x\u2032||2/`2) . (2)\nIf the targets y(x) are modelled by a GP with additive Gaussian noise, e.g., y(x)|f(x) \u223c N (y(x); f(x), \u03c32), the predictive distribution at n\u2217 test points X\u2217 is given by\nf\u2217|X\u2217,X,y,\u03b8, \u03c32 \u223c N (f\u0304\u2217, cov(f\u2217)) , (3) f\u0304\u2217 = \u00b5X\u2217 +KX\u2217,X [KX,X + \u03c3 2I]\u22121y ,\ncov(f\u2217) = KX\u2217,X\u2217 \u2212KX\u2217,X [KX,X + \u03c32I]\u22121KX,X\u2217 .\nKX\u2217,X , for example, denotes the n\u2217\u00d7n matrix of covariances between the GP evaluated at X\u2217 and X. \u00b5X\u2217 is the n\u2217\u00d7 1 mean vector, and KX,X is the n\u00d7n covariance matrix evaluated at training inputs X. All covariance matrices implicitly depend on the kernel hyperparameters \u03b8.\nWe can analytically marginalise the Gaussian process f(x) to obtain the marginal likelihood of the data, conditioned only on the covariance hyperparameters \u03b8:\nlog p(y|\u03b8) \u221d \u2212[ model fit\ufe37 \ufe38\ufe38 \ufe37 y>(K\u03b8 + \u03c3 2I)\u22121y+ complexity penalty\ufe37 \ufe38\ufe38 \ufe37 log |K\u03b8 + \u03c32I|] . (4)\nEq. (4) nicely separates into automatically calibrated model fit and complexity terms (Rasmussen and Ghahramani, 2001), and can be optimized to learn the kernel hyperparameters \u03b8, or used to integrate out \u03b8 via MCMC (Rasmussen, 1996).\nThe computational bottleneck in using Gaussian processes is solving a linear system (K+\u03c32I)\u22121y (for inference), and log |K+\u03c32I| (for hyperparameter learning). For this purpose, standard procedure is to compute the Cholesky decomposition of K, requiring O(n3) operations and O(n2) storage. Afterwards, the predictive mean and variance respectively cost O(n) and O(n2) for a single test point x\u2217."}, {"heading": "2.2 Inducing Point Based Sparse Approximations", "text": "Many popular approaches to scaling up GP inference belong to a family of inducing point methods (Quin\u0303onero-Candela and Rasmussen, 2005). These methods can be viewed as replacing the exact kernel k(x, z) by an approximation k\u0303(x, z) for fast computations.\nFor example, the prominent subset of regressors (SoR) (Silverman, 1985) and fully independent training conditional (FITC) (Snelson and Ghahramani, 2006) methods\nuse the approximate kernels\nk\u0303SoR(x, z) = Kx,UK \u22121 U,UKU,z , (5)\nk\u0303FITC(x, z) = k\u0303SoR(x, z) + \u03b4xz ( k(x, z)\u2212 k\u0303SoR(x, z) ) , (6)\nfor a set of m inducing points U = [ui]i=1...m. Kx,U , K \u22121 U,U , and KU,z are generated from the exact kernel k(x, z). While SoR yields an n \u00d7 n covariance matrix KSoR of rank at most m, corresponding to a degenerate (finite basis) Gaussian process, FITC leads to a full rank covariance matrix KFITC due to its diagonal correction. As a result, FITC is a more faithful approximation and is preferred in practice. Note that the exact user-specified kernel, k(x, z), will be parametrized by \u03b8, and therefore kernel learning in an inducing point method takes place by, e.g., optimizing the SoR or FITC marginal likelihoods with respect to \u03b8.\nThese approximate kernels give rise to O(m2n + m3) computations and O(mn + m2) storage for GP inference and learning (Quin\u0303onero-Candela and Rasmussen, 2005), after which the GP predictive mean and variance cost O(m) and O(m2) per test case. To see practical efficiency gains over standard inference procedures, one is constrained to choose m n, which often leads to a severe deterioration in predictive performance, and an inability to perform expressive kernel learning (Wilson et al., 2014)."}, {"heading": "2.3 Fast Structure Exploiting Inference", "text": "Kronecker and Toeplitz methods exploit the existing structure of the GP covariance matrix K to scale up inference and learning without approximations."}, {"heading": "2.3.1 Kronecker Methods", "text": "We briefly review Kronecker methods. A full introduction is provided in chapter 5 of Saatchi (2011).\nIf we have multidimensional inputs on a Cartesian grid, x \u2208 X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 XP , and a product kernel across grid dimensions, k(xi,xj) = \u220fP p=1 k(x (p) i ,x (p) j ), then the m \u00d7m covariance matrix K can be expressed as a Kronecker product K = K1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 KP (the number of grid points m = \u220fP i=1 np is a product of the number of points np per grid dimension). It follows that we can efficiently find the eigendecomposition of K = QV Q> by separately computing the eigendecomposition of each of K1, . . . , KP . One can similarly exploit Kronecker structure for fast matrix vector products (Wilson et al., 2014).\nFast eigendecompositions and matrix vector products of Kronecker matrices allow us to efficiently evaluate (K+\u03c32I)\u22121y and log |K+\u03c32I| for scalable and exact inference and\nlearning with GPs. Specifically, given an eigendecomposition of K as QV Q>, we can write (K + \u03c32I)\u22121y = (QV Q> + \u03c32I)\u22121y = Q(V + \u03c32I)\u22121Q>y, and log |K + \u03c32I| =\u2211\ni log(Vii + \u03c3 2). V is a diagonal matrix of eigenvalues, so inversion is trivial. Q, an orthogonal matrix of eigenvectors, also decomposes as a Kronecker product, which enables fast matrix vector products. Overall, inference and learning cost O(Pm1+1/P ) operations (for P > 1) and O(Pm 2P ) storage (Saatchi, 2011; Wilson et al., 2014).\nWhile product kernels can be easily constructed, and popular kernels such as the RBF kernel of Eq. (2) already have product structure, requiring a multidimensional input grid can be a severe constraint.\nWilson et al. (2014) extend Kronecker methods to datasets with only partial grid structure \u2013 e.g., images with random missing pixels, or spatiotemporal grids with missing data due to water. They complete a partial grid with virtual observations, and use a diagonal noise covariance matrix A which ignores the effects of these virtual observations: K(n) + \u03c32I \u2192 K(m) +A, where K(n) is an n\u00d7 n covariance matrix formed from the original dataset with n datapoints, and K(m) is the covariance matrix after augmentation from virtual inputs. Although we cannot efficiently eigendecompose K(m) + A, we can take matrix vector products (K(m) +A)y(m) efficiently, since K(m) is Kronecker and A is diagonal. We can thus compute (K(m) + A)\u22121y(m) = (K(n) + \u03c32I)\u22121y(n) to within machine precision, and perform efficient inference, using iterative methods such as linear conjugate gradients, which only involve matrix vector products.\nTo evaluate the marginal likelihood in Eq. (4), for kernel learning, we must also compute log |K(n) +\u03c32I|, where K(n) is an n\u00d7n covariance matrix formed from the original dataset with n datapoints. Wilson et al. (2014) propose to approximate the eigenvalues \u03bb (n) i of K (n) using the largest n eigenvalues \u03bbi of K (m), the Kronecker covariance matrix formed from the completed grid, which can be eigendecomposed efficiently. In particular,\nlog |K(n) + \u03c32I| = n\u2211\ni=1\nlog(\u03bb (n) i + \u03c3\n2) \u2248 n\u2211\ni=1\nlog( n\nm \u03bbi + \u03c3\n2) .\nTheorem 3.4 of Baker (1977) proves this eigenvalue approximation is asymptotically consistent (e.g., converges in the limit of large n), so long as the observed inputs are bounded by the complete grid. Williams and Shawe-Taylor (2003) also show that one can bound the true eigenvalues by their approximation using PCA. Notably, only the log determinant (complexity penalty) term in the marginal likelihood undergoes a small approximation. Wilson et al. (2014) show that, in practice, this approximation can be highly effective for fast and expressive kernel learning.\nHowever, the extensions in Wilson et al. (2014) are only efficient if the input space has partial grid structure, and do not apply in general settings."}, {"heading": "2.3.2 Toeplitz Methods", "text": "Toeplitz and Kronecker methods are complementary. K is a Toeplitz covariance matrix if it is generated from a stationary covariance kernel, k(x,x\u2032) = k(x\u2212 x\u2032), with inputs x on a regularly spaced one dimensional grid. Toeplitz matrices are constant along their diagonals: Ki,j = Ki+1,j+1 = k(xi \u2212 xj).\nOne can embed Toeplitz matrices into circulant matrices, to perform fast matrix vector products using fast Fourier transforms, e.g., Wilson (2014). One can then use linear conjugate gradients to solve linear systems (K+\u03c32I)\u22121y in O(m logm) operations and O(m) storage, for m grid datapoints. Turner (2010) and Cunningham et al. (2008) contain examples of Toeplitz methods applied to GPs."}, {"heading": "3 Structured Kernel Interpolation", "text": "We wish to ease the large O(n3) computations and O(n2) storage associated with Gaussian processes, while retaining model flexibility and general applicability.\nInducing point approaches (section 2.2) to scalability are popular because they can be applied \u201cout of the box\u201d, without requiring special structure in the data. However, with a small number of inducing points, these methods suffer from a major deterioration in predictive accuracy, and the inability to perform expressive kernel learning (Wilson et al., 2014), which will be most valuable on large datasets. On the other hand, structure exploiting approaches (section 2.3) are compelling because they provide incredible gains in scalability, with essentially no losses in predictive accuracy. But the requirement of an input grid makes these methods inapplicable to most problems.\nLooking at equations (5) and (6), it is tempting to try placing the locations of the inducing points U on a grid, in the SoR or FITC methods, and then exploit either Kronecker or Toeplitz algebra to efficiently solve linear systems involving K\u22121U,U . While this naive approach would reduce the O(m3) complexity associated with K\u22121U,U , that is not the dominant term for computations with inducing point methods \u2013 rather, it is the O(m2n) computations associated with the product KX,UK\u22121U,U .\nWe observe, however, that we can approximate the n \u00d7 m matrix KX,U of cross covariances for the kernel evaluated at the training and inducing inputs X and U , by interpolating on the m \u00d7m covariance matrix KU,U . For example, if we wish to estimate k(xi,uj), for input point xi and inducing point uj, we can start by finding the two inducing points ua and ub which most closely bound xi: ua \u2264 xi \u2264 ub (initially assuming D = 1 and a Toeplitz KU,U from a regular grid U , for simplicity). We can then form k\u0303(xi,uj) = wik(ua,uj) + (1\u2212wi)k(ub,uj), with linear interpolation weights wi and (1 \u2212 wi), which represent the relative distances from xi to points ua and ub.\nMore generally, we form\nKX,U \u2248 WKU,U , (7)\nwhere W is an n \u00d7 m matrix of interpolation weights. We observe that W can be extremely sparse. For local linear interpolation, W contains only c = 2 non-zero entries per row \u2013 the interpolation weights \u2013 which sum to 1. For greater accuracy, we can use local cubic interpolation (Keys, 1981) on equispaced grids, in which case W has c = 4 non-zero entries per row. For general rectilinear grids U (without regular spacing), we can use inverse distance weighting (Shepard, 1968) with c = 2 non-zero weights per row of W .\nSubstituting our expression for K\u0303X,U in Eq. (7) into the SoR approximation for KX,X , we find:\nKX,X SoR \u2248 KX,UK\u22121U,UKU,X Eq. (7) \u2248 WKU,UK\u22121U,UKU,UW >\n= WKU,UW > = KSKI . (8)\nWe name this general approach to approximating GP kernel functions structured kernel interpolation (SKI). Although we have made use of the SoR approximation as an example, SKI can be applied to essentially any inducing point method, such as FITC.1,2\nWe can compute fast matrix vector products KSKIy. If we do not exploit Toeplitz or Kronecker structure in KU,U , a matrix vector product costs O(n + m2) computations and O(n+m2) storage (matrix vector products with KU,U cost O(m2) computations, and with sparse W cost O(n), with the same storage requirements). If we exploit Kronecker structure, we only require O(Pm1+1/P ) computations and O(n + Pm 2P ) storage (matrix vector products with KU,U now cost O(Pm1+1/P ) computations and O(Pm 2P ) storage). If we exploit Toeplitz structure, we only require O(n + m logm) computations and O(n + m) storage (a matrix vector product with KU,U now costs O(m logm) computations and O(m) storage).\nInference proceeds by solving K\u22121SKIy through linear conjugate gradients, which only requires matrix vector products and a small number j n of iterations for convergence to within machine precision. To compute log |KSKI|, for the marginal likelihood evaluations used in kernel learning, one can follow the approximation of Wilson et al. (2014), described in section 2.3.1, where KU,U takes the role of K\n(m), and virtual observations are not required. Alternatively, we can use the ability to take fast matrix vector products with KSKI in standard eigenvalue solvers to efficiently compute the log determinant exactly. We can also form an accurate approximation by selectively computing the largest and smallest eigenvalues. This alternative approach is not possible in\n1Combining with the SoR approximate k(x, z), one can naively use kSKI(x, z) = w > xKU,Uwz, where wx,wz \u2208 Rm are interpolation vectors for points x and z; however, when wx 6= wz, it makes most sense to perform local interpolation on KU,z directly.\n2Later we discuss the logistics of combining with FITC.\nWilson et al. (2014) since in that case one cannot take fast matrix vector products with K(n). In either fast approach, the computional complexity for learning is no greater than for inference.\nIn short, even if we choose not to exploit potential Kronecker or Toeplitz structure in KU,U , inference and learning in SKI are accelerated over standard inducing point approaches. However, unlike with the data inputs, X, which are fixed, we are free to choose the locations of the latent inducing points U , and therefore we can easily create (e.g., Toeplitz or Kronecker) structure in KU,U which might not exist in KX,X . In the SKI formalism, we can uniquely exploit this structure for substantial additional gains in efficiency, and the ability to use an unprecedented number of inducing points, while lifting any grid requirements for the training inputs X.\nAlthough here we have made use of the SoR approximation in Eq. (8), we could trivially apply the FITC diagonal correction (section 2.2), or combine with other approaches. However, within the SKI framework, the diagonal correction of FITC does not have as much value: KSKI can easily be full rank and still have major computational benefits, using m > n. In conventional inducing approximations, one would never set m > n, since this would be less efficient than exact Gaussian process inference.\nFinally, we can understand all of these inducing approaches as part of a general structured kernel interpolation (SKI) framework. The predictive mean f\u0304\u2217 in Eq. (3) of a noise-free, zero mean GP (\u03c3 = 0, \u00b5(x) \u2261 0) is linear in two ways: on the one hand, as a wX(x\u2217) = K \u22121 X,XKX,x\u2217 weighted sum of the observations y, and on the other hand as an \u03b1 = K\u22121X,Xy weighted sum of training-test cross-covariances KX,x\u2217 :\nf\u0304\u2217 = y >wX(x\u2217) = \u03b1 >KX,x\u2217 . (9)\nIf we are to perform a noise free zero-mean GP regression on the kernel itself, such that we have data D = (ui, k(ui,x))mi=1, then we recover the SoR kernel k\u0303SoR(x, z) of equation (5) as the predictive mean of the GP at test point x\u2217 = z. This finding provides a new unifying perspective on inducing point approaches: all conventional inducing point methods, such as SoR and FITC, can be re-derived as performing a zero-mean Gaussian process interpolation on the true kernel. Indeed, we could write interpolation points instead of inducing points. The n \u00d7 m interpolation weight matrix W , in all conventional cases, will have all non-zero entries, which incurs great computational expenses. And, computional considerations aside, it is not ideal for inducing point methods to perform a zero-mean GP regression on a covariance kernel. For example, since popular kernels are often strictly positive \u2013 neither zero-mean, nor accurately characterized by a Gaussian distribution \u2013 conventional inducing point methods will tend to underestimate covariances.\nThe SKI interpretation of inducing point methods provides a mechanism to create new inducing point approaches. By replacing global GP kernel interpolation with local inverse distance weighting or cubic interpolation, as part of our SKI framework,\nwe make W extremely sparse. We illustrate the differences between local and global kernel interpolation in Figure 1. In addition to the sparsity in W , this interpolation perspective naturally enables us to exploit (e.g., Toeplitz or Kronecker) structure in the kernel for further gains in scalability, without requiring that the inputs X (which index the targets y) are on a grid.\nThis unifying perspective of inducing methods as kernel interpolation also clarifies when these approaches will perform best. The key assumption, in all of these approaches, is smoothness in the true underlying kernel k. We can expect interpolation approaches to work well on popular kernels, such as the RBF kernel, which is a simple exponential function. More expressive kernels, such as the spectral mixture kernel (Wilson and Adams, 2013), will require more inducing (interpolation) points for a good approximation, due to their quasi-periodic nature. It is our contention that the loss in accuracy going from, e.g., global GP kernel interpolation to local cubic kernel interpolation is more than recovered by the subsequent ability to greatly increase the number of inducing points. Moreover, we believe the structure of most popular kernel functions is conducive to local versus global interpolation, resulting in a strong approximation with greatly improved scalability.\nWhen combining SKI with i) GPs, ii) sparse (e.g. cubic) interpolation, and iii) Kronecker or Toeplitz algebra, we name the resulting method KISS-GP, though in the experiments of section 4 we will typically write, e.g., \u201cSKI with cubic interpolation\u201d. We also use the terms inducing points and interpolation points interchangeably."}, {"heading": "4 Experiments", "text": "We evaluate SKI for kernel matrix approximation (section 4.1), kernel learning (section 4.2), and natural sound modelling (section 4.3).\nWe particularly compare with FITC (Snelson and Ghahramani, 2006), because 1) FITC is the most popular inducing point approach, 2) FITC has been shown to have superior predictive performance and similar efficiency to other inducing methods, and is generally recommended (Naish-Guzman and Holden, 2007; Quinonero-Candela et al., 2007), and 3) FITC is well understood, and thus FITC comparisons help elucidate the fundamental properties of SKI, which is our primary goal. However, we also provide comparisons with SoR, and SSGPR (La\u0301zaro-Gredilla et al., 2010), a recent state of the art scalable GP method based on random projections with O(m2n) computations and O(m2) storage for m basis functions and n training points (see also Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2015; Lu et al., 2014).\nFurthermore, we focus on the ability for SKI to allow a relaxation of Kronecker and Toeplitz methods to arbitrarily located inputs. Since Toeplitz methods are restricted to 1D inputs, and Kronecker methods can only be used for low dimensional (D < 5) input spaces (Saatchi, 2011), we consider lower dimensional problems.\nAll experiments were performed on a 2011 MacBook Pro, with an Intel i5 2.3 GHz processor and 4 GB of RAM."}, {"heading": "4.1 Covariance Matrix Reconstruction", "text": "Accurate inference and learning depends on the GP covariance matrix K, which is used to form the predictive distribution and marginal likelihood of a Gaussian process. We evaluate the SKI approximation to K, in Eq. (8), as a function of number of inducing points m, inducing point locations, and sparse interpolation strategy.\nWe generate a 1000\u00d71000 covariance matrix K from an RBF kernel (Eq. (2)) evaluated at (sorted) inputs X randomly sampled from N (0, 25), shown in Figure 2(a). Note that the inputs have no grid structure. The approximate K produced by SKI using local cubic interpolation and only 40 interpolation points, shown in Figure 2(b), is almost indistinguishable from the original K. Figure 2(c) illustrates |K \u2212 KSKI, m=40|, the absolute difference between the matrices in Figures 2(a) and 2(b). The approximation is generally accurate, with greatest precision near the diagonals and outer edges of K.\nIn Figure 2(d), we show how reconstruction error varies as a function of inducing points and interpolation strategy. Local cubic and linear interpolation, using regular grids, are shown in blue and purple, respectively. Cubic interpolation is significantly more accurate for a small number of inducing points m. In black, we also show the accuracy of using k-means on the data inputs X to choose inducing point locations. In this case,\nwe use local inverse distance weighting interpolation, a type of linear interpolation which applies to irregular grids. This k-means strategy improves upon standard linear interpolation on a regular grid by choosing the inducing points which will be closest to the original inputs. However, the value of using k-means decreases when we are allowed more interpolation points, since the precise locations of these interpolation\npoints then becomes less critical, so long as we have general coverage of the input domain. Indeed, except for small m, cubic interpolation on a regular grid generally outperforms inverse distance weighting with k-means. Unsurprisingly, SKI with global GP kernel interpolation (shown in red), which corresponds to the SoR approximation, is much more accurate than the other interpolation strategies for very small m n.\nHowever, global GP kernel interpolation is much less efficient than local cubic kernel interpolation, and these accuracy differences quickly shrink with increases in m. Indeed in Figures 2(e) and 2(f) we see both reconstruction errors are similarly small for m = 150, but qualitatively different. The error in the SoR reconstruction is concentrated near the diagonal, whereas the error in SKI with cubic interpolation never reaches the top errors in SoR, and is more accurate than SoR near the diagonal, but is also more diffuse. This finding suggests that combining cubic interpolation with GP interpolation could improve accuracy, if we account for the regions where each is strongest.\nUltimately, however, the important question is not which approximation is most accurate for a given m, but which approximation is most accurate for a given runtime (Chalupka et al., 2013). In Figure 2(g) we compare the accuracies and runtimes for SoR, FITC, and SKI with local linear and local cubic interpolation, for m \u2208 [500, 2000] at m = 150 unit increments. m is sufficiently large that the differences in accuracy between SoR and FITC are negligible. In general, the difference in going from SKI with global GP interpolation (e.g., SoR or FITC) to SKI with local cubic interpolation (KISS-GP) is much more profound than the differences between SoR and FITC. Moreover, moving from local linear interpolation to local cubic interpolation provides a great boost in accuracy without noticeably affecting runtime. We also see that SKI with local interpolation quickly picks up accuracy with increases in m, with local cubic interpolation actually surpassing SoR and FITC in accuracy for a given m. Most importantly, for any given runtime, SKI with cubic interpolation is more accurate than the alternatives.\nIn this experiment we are testing the error and runtime for constructing an approximate covariance matrix, but we are not yet performing inference with that covariance matrix, which is typically much more expensive, and where SKI will help the most. Moreover, we are not yet using Kronecker or Toeplitz structure to accelerate SKI."}, {"heading": "4.2 Kernel Learning", "text": "We now test the ability for SKI to learn kernels from data using Gaussian processes. Indeed, SKI is intended to scale Gaussian processes to large datasets \u2013 and large datasets provide a distinct opportunity to discover rich statistical representations through kernel learning.\nPopular inducing point methods, such as FITC, improve the scalability of Gaussian processes. However, Wilson et al. (2014) showed that these methods cannot typically\nbe used for expressive kernel learning, and are most suited to simple smoothing kernels. In other words, these scalable GP methods often miss out on structure learning, one of the greatest motivations for considering large datasets in the first place. This limitation arises because popular inducing methods require that the number of inducing points m n, for computational tractability, which deprives us of the necessary information to learn intricate kernels. SKI does not suffer from this problem, since we are free to choose large m; in fact, m can be greater than n, while retaining significant efficiency gains over standard GPs.\nTo test SKI and FITC for kernel learning, we sample data from a GP which uses a known ground truth kernel, and then attempt to learn this kernel from the data. In particular, we sample n = 10, 000 datapoints y from a Gaussian process with an intricate product kernel ktrue = k1k2 queried at inputs x \u2208 R2 drawn from N (0, 4I) (the inputs have no grid structure). Each component kernel in the product operates on a separate input dimension, as shown in green in Figure 3. Incidentally, n = 104 points is about the upper limit of what we can sample from a multivariate Gaussian distribution with a non-trivial covariance matrix. Even a single sample from a GP with this many datapoints together with this sophisticated kernel is computationally intensive, taking 1030 seconds in this instance. On the other hand, SKI can enable one to efficiently sample from extremely high dimensional (n > 1010) non-trivial multivariate Gaussian distributions, which could be generally useful.3\nTo learn the kernel underlying the data, we optimize the SKI and FITC marginal likelihoods of a Gaussian process p(y|\u03b8) with respect to the hyperparameters \u03b8 of a spectral mixture kernel, using non-linear conjugate gradients. In other words, the SKI and FITC kernels approximate a user specified (e.g., spectral mixture) kernel which is parametrized by \u03b8, and to perform kernel learning we wish to learn \u03b8 from the data. Spectral mixture kernels (Wilson and Adams, 2013) form a basis for all stationary covariance kernels, and are well-equipped for kernel learning. For SKI, we use cubic interpolation and a 100\u00d7100 inducing point grid, equispaced in each input dimension. That is, we have as many inducing points m = 10, 000 as we have training datapoints. We use the same hyperparameter initialisation for each approach.\nThe results are shown in Figures 3(a) and 3(b). The true kernels are in green, the SKI reconstructions in blue, and the FITC reconstructions in red. SKI provides a strong approximation, whereas FITC is unable to come near to reconstructing the true kernel. In this multidimensional example, SKI leverages Kronecker structure for efficiency, and has a runtime of 2400 seconds (0.67 hours), using m = 10, 000 inducing points. FITC, on the other hand, has a runtime of 2.6\u00d7 104 seconds (7.2 hours), with only m = 100 inducing points. More inducing points with FITC breaks computational tractibility.\nEven though the locations of the training points are randomly sampled, in SKI we exploited the Kronecker structure in the covariance matrix KU,U over the inducing\n3Sampling would proceed, e.g., via WSKI[chol(K1)\u2297 \u00b7 \u00b7 \u00b7 \u2297 chol(Kp)]\u03bd, \u03bd \u223c N (0, I).\npoints U , to reduce the cost of using 10, 000 inducing points to less than the cost of using 100 inducing points with FITC. FITC, and alternative inducing point methods, cannot effectively exploit Kronecker structure, because the non-sparse cross-covariance matrices KX,U and KU,X limit scaling to at best O(m2n), as per section 3."}, {"heading": "4.3 Natural Sound Modelling", "text": "In section 4.2 we exploited multidimensional Kronecker structure in the SKI covariance matrix KU,U for scalability. For targets indexed by a set of 1D inputs, such as time series, we cannot exploit Kronecker structure for computational savings. However, by placing the inducing points on a regular grid, we can create Toeplitz structure (section 2.3.2) in KU,U which can be effectively exploited by SKI for additional scalability, but not by popular alternatives.\nGaussian processes have been successfully applied to natural sound modelling, with a view towards automatic speech recognition, and a deeper understanding of auditory processing in the brain (Turner, 2010). We use SKI to model the natural sound time series in Figure 4(a), considered in a different context by Turner (2010). We trained a full Gaussian process on a subset of the data, learning the hyperparameters of an RBF kernel, for use with both FITC and SKI. We then used each of these methods to reconstruct large contiguous missing regions in the signal. This time series does not have input grid structure due to the high number of large arbitrarily located missing regions, and therefore direct Toeplitz methods cannot be applied. In total, there are 59, 306 training points and 691 testing points. We place the inducing points for each method on a regular grid, and exploit Toeplitz structure in SKI for scalability.\nFigure 4(b) shows empirical runtimes (on a log scale) as a function of inducing points m in both methods, and Figure 4(c) shows the standardised mean absolute error\n(SMAE) on test points as a function of runtime (log scale) for each method.4 For m \u2208 [2500, 5000] the runtime for SKI is essentially unaffected by increases in m, and hundreds of times faster than FITC, which does noticeably increase in runtime with m. Moreover, Figure 4(c) confirms our intuition that, for a given runtime, accuracy losses in going from GP kernel interpolation in FITC to the more simple cubic kernel interpolation in the KISS-GP variant of SKI can be more than recovered by the gain in accuracy enabled through more inducing points. SKI has less than half of the error at less than 1% the runtime cost of FITC. SKI is generally able to infer the correct curvature in the function, while FITC, unable to use as many inducing points for any given runtime, tends to over-smooth the data. Eventually, however, adding more inducing points increases runtime without increasing accuracy. We also made predictions with SSGPR (La\u0301zaro-Gredilla et al., 2010), a recent state of the art approach to scalable GP modelling, which requires O(m2n) computations and O(m2) storage, for m basis functions and n training points. For a range of m \u2208 [250, 1250], SSGPR had SMAE \u2208 [1.12, 1.23] and runtimes \u2208 [310, 8400] seconds. Overall, SKI provides the best reconstruction of the signal at the lowest runtime."}, {"heading": "5 Discussion", "text": "We introduced a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian process inference. In particular, we showed how standard inducing point methods correspond to kernel approximations formed through global Gaussian process kernel interpolation. By changing to local cubic kernel interpolation, we introduced KISS-GP, a highly scalable\n4SMAEmethod = MAEmethod / MAEempirical mean, so that the trivial solution of predicting with the empirical mean gives an SMAE of 1, and lower values correspond to better fits of the data.\ninducing point method, which naturally combines with Kronecker and Toeplitz algebra for additional gains in scalability. Indeed we can view KISS-GP as relaxing the stringent grid assumptions in Kronecker and Toeplitz methods to arbitrarily located inputs. We showed that the ability for KISS-GP to efficiently handle a large number of inducing points enabled expressive kernel learning and improved predictive accuracy, in addition to improved runtimes, over popular alternatives. In particular, for any given runtime, KISS-GP is orders of magnitude more accurate than the alternatives. Overall, simplicity and generality are major strengths of the SKI framework.\nWe have only begun to explore what could be done with this new framework. Structured kernel interpolation opens the doors to a multitude of substantial new research directions. For example, one can create entirely new scalable Gaussian process models by changing interpolation strategies. These models could have remarkably different properties and applications. And we can use the perspective given by structured kernel interpolation to better understand the properties of any inducing point approach \u2013 e.g., which kernels are best approximated by a given approach, and how many inducing points will be needed for good performance. We can also combine new models generated from SKI with the orthogonal benefits of recent stochastic variational inference for Gaussian processes. Moreover, the decomposition of the SKI covariance matrix into a Kronecker product of Toeplitz matrices provides motivation to unify scalable Kronecker and Toeplitz approaches. We hope that SKI will inspire many new models and unifying perspectives, and an improved understanding of scalable Gaussian process methods."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.", "creator": "LaTeX with hyperref package"}}}