{"id": "1503.02510", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2015", "title": "Compositional Distributional Semantics with Long Short Term Memory", "abstract": "efforts are in an extension preserving simple recursive knowledge network that tells sure of newer variant of the long short - term retrieval architecture. the encoding allows information low allocated parse bits to be stored their noisy memory clock ( = ` memory item') and used towards later higher computers behind the incoming clock. this provides a solution helps detecting reduced entropy problem and completes the network to resolve long range stimuli. experimental results show that our browser outperformed the traditional neural - network composition on the stanford hierarchical treebank.", "histories": [["v1", "Mon, 9 Mar 2015 15:13:38 GMT  (173kb,D)", "https://arxiv.org/abs/1503.02510v1", "10 pages, 7 figures"], ["v2", "Fri, 17 Apr 2015 23:54:37 GMT  (173kb,D)", "http://arxiv.org/abs/1503.02510v2", "10 pages, 7 figures"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["phong le", "willem zuidema"], "accepted": false, "id": "1503.02510"}, "pdf": {"name": "1503.02510.pdf", "metadata": {"source": "CRF", "title": "Compositional Distributional Semantics with Long Short Term Memory", "authors": ["Phong Le", "Willem Zuidema"], "emails": ["p.le@uva.nl", "zuidema@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "Moving from lexical to compositional semantics in vector-based semantics requires answers to two difficult questions: (i) what is the nature of the composition functions (given that the lambda calculus for variable binding is no longer applicable), and (ii) how do we learn the parameters of those functions (if they have any) from data? A number of classes of functions have been proposed in answer to the first question, including simple linear functions like vector addition (Mitchell and Lapata, 2009), non-linear functions like those defined by multi-layer neural networks (Socher et al., 2010), and vector matrix multiplication and tensor linear mapping (Baroni et al., 2013). The matrix and tensor-based functions have the advantage of allowing a relatively straightforward comparison with formal semantics, but the fact that multi-layer neural networks with non-linear activation functions like sigmoid can approximate\nany continuous function (Cybenko, 1989) already make them an attractive choice.\nIn trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model (Socher et al., 2013b) and the convolutional neural network model (Kalchbrenner et al., 2014), are even clearer. Models in this paradigm can take advantage of general learning procedures based on back-propagation, and with the rise of \u2018deep learning\u2019, of a variety of efficient algorithms and tricks to further improve training.\nSince the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of extensions have been proposed. One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al. (2014) for context-dependence sentiment analysis.\nOur proposal in this paper is an extension of the RNN model to improve compositionality. Our motivation is that, like training recurrent neural networks, training RNNs on deep trees can suffer from the vanishing gradient problem (Hochreiter et al., 2001), i.e., that errors propagated back to the leaf nodes shrink exponentially. In addition, information sent from a leaf node to the root can be obscured if the path between them is long, thus leading to the problem how to capture long range dependencies. We therefore borrow the long short-term memory (LSTM) architecture (Hochreiter and Schmidhu-\nar X\niv :1\n50 3.\n02 51\n0v 2\n[ cs\n.C L\n] 1\n7 A\npr 2\nber, 1997) from recurrent neural network research to tackle those two problems. The main idea is to allow information low in a parse tree to be stored in a memory cell and used much later higher up in the parse tree, by recursively adding up all memory into memory cells in a bottom-up manner. In this way, errors propagated back through structure do not vanish. And information from leaf nodes is still (loosely) preserved and can be used directly at any higher nodes in the hierarchy. We then apply this composition to sentiment analysis. Experimental results show that the new composition works better than the traditional neural-network-based composition.\nThe outline of the rest of the paper is as follows. We first, in Section 2, give a brief background on neural networks, including the multi-layer neural network, recursive neural network, recurrent neural network, and LSTM. We then propose the LSTM for recursive neural networks in Section 3, and its application to sentiment analysis in Section 4. Section 5 shows our experiments."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Multi-layer Neural Network", "text": "In a multi-layer neural network (MLN), neurons are organized in layers (see Figure 1-left). A neuron in layer i receives signal from neurons in layer i \u2212 1 and transmits its output to neurons in layer i + 1. 1 The computation is given by\nyi = g ( Wi\u22121,iyi\u22121 + bi ) 1This is a simplified definition. In practice, any layer j < i\ncan connect to layer i.\ne2x+1 , softsign(x) = x 1+|x| .\nwhere real vector yi contains the activations of the neurons in layer i; Wi\u22121,i \u2208 R|yi|\u00d7|yi\u22121| is the matrix of weights of connections from layer i \u2212 1 to layer i; bi \u2208 R|yi| is the vector of biases of the neurons in layer i; g is an activation function, e.g. sigmoid, tanh, or softsign (see Figure 2).\nFor classification tasks, we put a softmax layer on the top of the network, and compute the probability of assigning a class c to an input x by\nPr(c|x) = softmax(c) = e u(c,ytop)\u2211\nc\u2032\u2208C e u(c\u2032,ytop)\n(1)\nwhere [ u(c1,ytop), ..., u(c|C|,ytop) ]T = Wytop + b; C is the set of all possible classes; W \u2208 R|C|\u00d7|ytop|,b \u2208 R|C| are a weight matrix and a bias vector.\nTraining an MLN is to minimize an objective function J(\u03b8) where \u03b8 is the parameter set (for classification, J(\u03b8) is often a negative log likelihood). Thanks to the back-propagation algorithm (Rumelhart et al., 1988), the gradient \u2202J/\u2202\u03b8 is efficiently computed; the gradient descent method thus is used to minimize J ."}, {"heading": "2.2 Recursive Neural Network", "text": "A recursive neural network (RNN) (Goller and Ku\u0308chler, 1996) is an MLN where, given a tree structure, we recursively apply the same weight matrices at each inner node in a bottom-up manner. In order to see how an RNN works, consider the following example. Assume that there is a constituent\nwith parse tree (p2 (p1 x y) z) (Figure 1-right), and that x,y, z \u2208 Rd are the vectorial representations of the three words x, y and z, respectively. We use a neural network which consists of a weight matrix W1 \u2208 Rd\u00d7d for left children and a weight matrix W2 \u2208 Rd\u00d7d for right children to compute the vector for a parent node in a bottom up manner. Thus, we compute p1\np1 = g(W1x+W2y + b) (2)\nwhere b is a bias vector and g is an activation function. Having computed p1, we can then move one level up in the hierarchy and compute p2:\np2 = g(W1p1 +W2z+ b) (3)\nThis process is continued until we reach the root node.\nLike training an MLN, training an RNN uses the gradient descent method to minimize an objective function J(\u03b8). The gradient \u2202J/\u2202\u03b8 is efficiently computed thanks to the back-propagation through structure algorithm (Goller and Ku\u0308chler, 1996).\nThe RNN model and its extensions have been employed successfully to solve a wide range of problems: from parsing (constituent parsing (Socher et al., 2013a), dependency parsing (Le and Zuidema, 2014a)) to classification (e.g. sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b))."}, {"heading": "2.3 Recurrent Networks and Long Short-Term Memory", "text": "A neural network is recurrent if it has at least one directed ring in its structure. In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskever et al., 2014) and language modelling (Mikolov et al., 2010).\nIn an SRN, an input xt is fed to the network at each time t. The hidden layer h, which has activation ht\u22121 right before xt comes in, plays a role as a memory store capturing the whole history( x0, ...,xt\u22121 ) . When xt comes in, the hidden layer updates its activation by\nht = g ( Wxxt +Whht\u22121 + b )\nwhere Wx \u2208 R|h|\u00d7|xt|, Wh \u2208 R|h|\u00d7|h|, b \u2208 R|h| are weight matrices and a bias vector; g is an activation.\nThis network model thus, in theory, can be used to estimate probabilities conditioning on long histories. And computing gradients is efficient thanks to the back-propagation through time algorithm (Werbos, 1990). In practice, however, training recurrent neural networks with the gradient descent method is challenging because gradients \u2202Jt/\u2202hj (j \u2264 t, Jt is the objective function at time t) vanish quickly after a few back-propagation steps (Hochreiter et al., 2001). In addition, it is difficult to capture long range dependencies, i.e. the output at time t depends on some inputs that happened very long time ago. One solution for this, proposed by Hochreiter and Schmidhuber (1997) and enhanced by Gers (2001), is long short-term memory (LSTM).\nLong Short-Term Memory The main idea of the LSTM architecture is to maintain a memory of all inputs the hidden layer received over time, by adding up all (gated) inputs to the hidden layer through time to a memory cell. In this way, errors propagated back through time do not vanish and even inputs received a very long time ago are still (approximately) preserved and can play a role in computing the output of the network (see the il-\nlustration in Graves (2012, Chapter 4)). An LSTM cell (see Figure 3-right) consists of a memory cell c, an input gate i, a forget gate f , an output gate o. Computations occur in this cell are given below\nit = \u03c3 ( Wxixt +Whiht\u22121 +Wcict\u22121 + bi ) ft = \u03c3 ( Wxfxt +Whfht\u22121 +Wcfct\u22121 + bf\n) ct = ft ct\u22121+\nit tanh ( Wxcxt +Whcht\u22121 + bc ) ot = \u03c3 ( Wxoxt +Whoht\u22121 +Wcoct + bo\n) ht = ot tanh(ct)\nwhere \u03c3 is the sigmoid function; it, ft, ot are the outputs (i.e. activations) of the corresponding gates; ct is the state of the memory cell; denotes the element-wise multiplication operator; W\u2019s and b\u2019s are weight matrices and bias vectors.\nBecause the sigmoid function has the output range (0, 1) (see Figure 2), activations of those gates can be seen as normalized weights. Therefore, intuitively, the network can learn to use the input gate to decide when to memorize information, and similarly learn to use the output gate to decide when to access that memory. The forget gate, finally, is to reset the memory."}, {"heading": "3 Long Short-Term Memory in RNNs", "text": "In this section, we propose an extension of the LSTM for the RNN model (see Figure 4). A key feature of the RNN is to hierarchically combine information from two children to compute the parent vector; the idea in this section is to extend the LSTM such that not only the output from each of the children is used, but also the contents of their memory cells. This way, the network has the option to store information when processing constituents low in the parse tree, and make it available later on when it is processing constituents high in the parse tree.\nFor the simplicity 2, we assume that the parent node p has two children a and b. The LSTM at p thus has two input gates i1, i2 and two forget gates f1, f2 for the two children. Computations occuring in this LSTM are:\n2Extending our LSTM for n-ary trees is trivial.\ni1 = \u03c3 ( Wi1x+Wi2y +Wci1cx +Wci2cy + bi ) i2 = \u03c3 ( Wi1y +Wi2x+Wci1cy +Wci2cx + bi\n) f1 = \u03c3 ( Wf1x+Wf2y +Wcf1cx +Wcf2cy + bf\n) f2 = \u03c3 ( Wf1y +Wf2x+Wcf1cy +Wcf2cx + bf\n) cp = f1 cx + f2 cy+\ng ( Wc1x i1 +Wc2y i2 + bc ) o = \u03c3 ( Wo1x+Wo2y +Wcoc+ bo\n) p = o g(cp)\nwhere u and cu are the output and the state of the memory cell at node u; i1, i2, f1, f2, o are the activations of the corresponding gates; W\u2019s and b\u2019s are weight matrices and bias vectors; and g is an activation function.\nIntuitively, the input gate ij lets the LSTM at the parent node decide how important the output at the j-th child is. If it is important, the input gate ij will have an activation close to 1. Moreover, the LSTM controls, using the forget gate fj , the degree to which information from the memory of the j-th child should be added to its memory.\nUsing one input gate and one forget gate for each child makes the LSTM flexible in storing memory and computing composition. For instance, in a com-\nplex sentence containing a main clause and a dependent clause it could be beneficial if only information about the main clause is passed on to higher levels. This can be achieved by having low values for the input gate and the forget gate for the child node that covers the dependent clause, and high values for the gates corresponding to the child node covering (a part of) the main clause. More interestingly, this LSTM can even allow a child to contribute to composition by activating the corresponding input gate, but ignore the child\u2019s memory by deactivating the corresponding forget gate. This happens when the information given by the child is temporarily important only."}, {"heading": "4 LSTM-RNN model for Sentiment", "text": "Analysis 3\nIn this section, we introduce a model using the proposed LSTM for sentiment analysis. Our model, named LSTM-RNN, is an extension of the traditional RNN model (see Section 2.2) where traditional composition function g\u2019s in Equations 2- 3 are replaced by our proposed LSTM (see Figure 5). On top of the node covering a phrase/word, if its sentiment class (e.g. positive, negative, or neutral) is available, we put a softmax layer (see Equation 1) to compute the probability of assigning a class to it.\nThe vector representations of words (i.e. word embeddings) can be initialized randomly, or pretrained. The memory of any leaf node w, i.e. cw, is 0.\nSimilarly to Irsoy and Cardie (2014), we \u2018untie\u2019 leaf nodes and inner nodes: we use one weight matrix set for leaf nodes and another set for inner nodes. Hence, let dw and d respectively be the dimensions of word embeddings (leaf nodes) and vector representations of phrases (inner nodes), all weight matrices from a leaf node to an inner node have size d \u00d7 dw, and all weight matrices from an inner node to another inner node have size d\u00d7 d.\n3The LSTM architecture was already applied to the sentiment analysis task, for instance in the model proposed at http://deeplearning.net/tutorial/lstm. html. Independently from and concurrently with our work, Tai et al. (2015) and Zhu et al. (2015) have developed very similar models applying LTSM to RNNs.\nTraining Training this model is to minimize the following objective function, which is the crossentropy over training sentence set D plus an L2norm regularization term\nJ(\u03b8) = \u2212 1 |D| \u2211 s\u2208D \u2211 p\u2208s logPr(cp|p) + \u03bb 2 ||\u03b8||2\nwhere \u03b8 is the parameter set, cp is the sentiment class of phrase p, p is the vector representation at the node covering p, Pr(cp|p) is computed by the softmax function, and \u03bb is the regularization parameter. Like training an RNN, we use the mini-batch gradient descent method to minimize J , where the gradient \u2202J/\u2202\u03b8 is computed efficiently thanks to the backpropagation through structure (Goller and Ku\u0308chler, 1996). We use the AdaGrad method (Duchi et al., 2011) to automatically update the learning rate for each parameter."}, {"heading": "4.1 Complexity", "text": "We analyse the complexities of the RNN and LSTMRNN models in the forward phase, i.e. computing vector representations for inner nodes and classification probabilities. The complexities in the backward phase, i.e. computing gradients \u2202J/\u2202\u03b8, can be analysed similarly.\nThe complexities of the two models are dominated by the matrix-vector multiplications that are carried out. Since the number of sentiment classes is very small (5 or 2 in our experiments) compared to d and dw, we only consider those matrix-vector multiplications which are for computing vector representations at the inner nodes.\nFor a sentence consisting of N words, assuming that its parse tree is binarized without any unary branch (as in the data set we use in our experiments), there areN\u22121 inner nodes,N links from leaf nodes to inner nodes, and N \u2212 2 links from inner nodes to other inner nodes. The complexity of RNN in the forward phase is thus approximately\nN \u00d7 d\u00d7 dw + (N \u2212 2)\u00d7 d\u00d7 d\nThe complexity of LSTM-RNN is approximately\nN\u00d76\u00d7d\u00d7dw+(N\u22122)\u00d710\u00d7d\u00d7d+(N\u22121)\u00d7d\u00d7d\nIf dw \u2248 d, the complexity of LSTM-RNN is about 8.5 times higher than the complexity of RNN.\nIn our experiments, this difference is not a problem because training and evaluating the LSTMRNN model is very fast: it took us, on a single core of a modern computer, about 10 minutes to train the model (d = 50, dw = 100) on 8544 sentences, and about 2 seconds to evaluate it on 2210 sentences."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Dataset", "text": "We used the Stanford Sentiment Treebank4 (Socher et al., 2013b) which consists of 5-way fine-grained sentiment labels (very negative, negative, neutral, positive, very positive) for 215,154 phrases of 11,855 sentences. The standard splitting is also given: 8544 sentences for training, 1101 for development, and 2210 for testing. The average sentence length is 19.1.\nIn addition, the treebank also supports binary sentiment (positive, negative) classification by removing neutral labels, leading to: 6920 sentences for training, 872 for development, and 1821 for testing.\nThe evaluation metric is the accuracy, given by 100\u00d7#correct\n#total ."}, {"heading": "5.2 LSTM-RNN vs. RNN", "text": "Setting We initialized the word vectors by the 100-D GloVe5 word embeddings (Pennington et al., 2014), which were trained on a 6B-word corpus. The initial values for a weight matrix were uniformly sampled from the symmetric interval[ \u2212 1\u221a\nn , 1\u221a n\n] where n is the number of total input\nunits. 4http://nlp.stanford.edu/sentiment/ treebank.html 5http://nlp.stanford.edu/projects/GloVe/\nFor each model (RNN and LSTM-RNN), we tested three activation functions: softmax, tanh, and softsign, leading to six sub-models. Tuning those sub-models on the development set, we chose the dimensions of vector representations at inner nodes d = 50, learning rate 0.05, regularization parameter \u03bb = 10\u22123, and mini-batch-size 5.\nOn each task, we run each sub-model 10 times. Each time, we trained the sub-model in 20 epochs and selected the network achieving the highest accuracy on the development set.\nResults Figure 6 and 7 show the statistics of the accuracies of the final networks on the test set in the fine-grained classification task and binary classification task, respectively.\nIt can be seen that LSTM-RNN outperformed RNN when using the tanh or softsign activation\nfunctions. With the sigmoid activation function, the difference is not so clear, but it seems that LSTMRNN performed slightly better. Tanh-LSTM-RNN and softsign-LSTM-RNN have the highest median accuracies (48.1 and 86.4) in the fine-grained classification task and in the binary classification task, respectively.\nWith the RNN model, it is surprising to see that the sigmoid function performed well, comparably with the other two functions in the fine-grained task, and even better than the softsign function in the binary task, given that it was not often chosen in recent work. The softsign function, which was shown to work better than tanh for deep networks (Glorot and Bengio, 2010), however, did not yield improvements in this experiment.\nWith the LSTM-RNN model, the tanh function, in general, worked best whereas the sigmoid function was the worst. This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014)."}, {"heading": "5.3 Compared against other Models", "text": "We compare LSTM-RNN (using tanh) in the previous experiment against existing models: Naive Bayes with bag of bigram features (BiNB), Recursive neural tensor network (RNTN) (Socher et al., 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network\n(DCNN) (Kalchbrenner et al., 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014).\nAmong them, BiNB is the only one that is not a neural net model. RNTN and DRNN are two extensions of RNN. Whereas RNTN, which keeps the structure of the RNN, uses both matrix-vector multiplication and tensor product for the composition purpose, DRNN makes the net deeper by concatenating more than one RNNs horizontally. CNN, DCNN and PV do not rely on syntactic trees. CNN uses a convolutional layer and a max-pooling layer to handle sequences with different lengths. DCNN is hierarchical in the sense that it stacks more than one convolutional layers with k-max pooling layers in between. In PV, a sentence (or document) is represented as an input vector to predict which words appear in it.\nTable 1 (above the dashed line) shows the accuracies of those models. The accuracies of LSTM-RNN was taken from the network achieving the highest performance out of 10 runs on the development set. The accuracies of the other models are copied from the corresponding papers. LSTM-RNN clearly performed worse than DCNN, PV, DRNN in both tasks, and worse than CNN in the binary task."}, {"heading": "5.4 Toward State-of-the-art with Better Word Embeddings", "text": "We focus on DRNN, which is the most similar to LSTM-RNN among those four models CNN, DCNN, PV and DRNN. In fact, from the results reported in Irsoy and Cardie (2014, Table 1a), LSTM-\nRNN performed on par6 with their 1-layer-DRNN (d = 340) using dropout, which is to randomly remove some neurons during training. Dropout is a powerful technique to train neural networks, not only because it plays a role as a strong regularization method to prohibit neurons co-adapting, but it is also considered a technique to efficiently make an ensemble of a large number of shared weight neural networks (Srivastava et al., 2014). Thanks to dropout, Irsoy and Cardie (2014) boosted the accuracy of a 3-layer-DRNN with d = 200 from 46.06 to 49.5 in the fine-grained task.\nIn the second experiment, we tried to boost the accuracy of the LSTM-RNN model. Inspired by Irsoy and Cardie (2014), we tried using dropout and better word embeddings. Dropout, however, did not work with LSTM. The reason might be that dropout corrupted its memory, thus making training more difficult. Better word embeddings did pay off, however. We used 300-D GloVe word embeddings trained on a 840B-word corpus. Testing on the development set, we chose the same values for the hyper-parameters as in the first experiment, except setting learning rate 0.01. We also run the model 10 times and selected the networks getting the highest accuracies on the development set. Table 1 (below the dashed line) shows the results. Using the 300-D GloVe word embeddings was very helpful: LSTM-RNN performed on par with DRNN in the fine-grained task, and with CNN in the binary task. Therefore, taking into account both tasks, LSTMRNN with the 300-D GloVe word embeddings outperformed all other models."}, {"heading": "6 Discussion and Conclusion", "text": "We proposed a new composition method for the recursive neural network (RNN) model by extending the long short-term memory (LSTM) architecture which is widely used in recurrent neural network research.\n6Irsoy and Cardie (2014) used the 300-D word2vec word embeddings trained on a 100B-word corpus whereas we used the 100-D GloVe word embeddings trained on a 6B-word corpus. From the fact that they achieved the accuracy 46.1 with an RNN (d = 50) in the fine-grained task and 85.3 in the binary task, and our implementation of RNN (d = 50) performed worse (see Table 6 and 7), we conclude that the 100-D GloVe word embeddings are not more suitable than the 300-D word2vec word embeddings.\nThe question is why LSTM-RNN performed better than the traditional RNN. Here, based on the fact that the LSTM for RNNs should work very similarly to LSTM for recurrent neural networks, we borrow the argument given in Bengio et al. (2013, Section 3.2) to answer the question. Bengio explains that the LSTM behaves like low-pass filter \u201chence they can be used to focus certain units on different frequency regions of the data\u201d. This suggests that the LSTM plays a role as a lossy compressor which is to keep global information by focusing on low frequency regions and remove noise by ignoring high frequency regions. So composition in this case could be seen as compression, like the recursive auto-encoder (RAE) (Socher et al., 2011a). Because pre-training an RNN as an RAE can boost the overall performance (Socher et al., 2011a; Socher et al., 2011c), seeing LSTM as a compressor might explain why the LSTM-RNN worked better than RNN without pre-training.\nComparing LSTM-RNN against DRNN (Irsoy and Cardie, 2014) gives us a hint about how to improve our model. From the experimental results, LSTM-RNN without the 300-D GloVe word embeddings performed worse than DRNN, while DRNN gained a significant improvement thanks to dropout. Finding a method like dropout that does not corrupt the LSTM memory might boost the overall performance significantly and will be a topic for our future work."}, {"heading": "Acknowledgments", "text": "We thank three anonymous reviewers for helpful comments."}], "references": [{"title": "Frege in space: A program for compositional distributional semantics", "author": ["Marco Baroni", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "A. Zaenen, B. Webber, and M. Palmer, editors, Linguistic Issues in Language Technologies. CSLI Publications, Stanford, CA.", "citeRegEx": "Baroni et al\\.,? 2013", "shortCiteRegEx": "Baroni et al\\.", "year": 2013}, {"title": "Advances in optimizing recurrent networks", "author": ["Yoshua Bengio", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8624\u20138628. IEEE.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["George Cybenko."], "venue": "Mathematics of control, signals and systems, 2(4):303\u2013314.", "citeRegEx": "Cybenko.,? 1989", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, pages 2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Long short-term memory in recurrent neural networks", "author": ["Felix Gers."], "venue": "Unpublished PhD dissertation, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne, Switzerland.", "citeRegEx": "Gers.,? 2001", "shortCiteRegEx": "Gers.", "year": 2001}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "International conference on artificial intelligence and statistics, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas K\u00fcchler."], "venue": "International Conference on Neural Networks, pages 347\u2013352. IEEE.", "citeRegEx": "Goller and K\u00fcchler.,? 1996", "shortCiteRegEx": "Goller and K\u00fcchler.", "year": 1996}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["Alex Graves."], "venue": "Springer.", "citeRegEx": "Graves.,? 2012", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber."], "venue": "Kremer and Kolen, editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press.", "citeRegEx": "Hochreiter et al\\.,? 2001", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Advances in Neural Information Processing Systems, pages 2096\u20132104.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655\u2013665, Balti-", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "The insideoutside recursive neural network model for dependency parsing", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.", "citeRegEx": "Le and Zuidema.,? 2014a", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Inside-outside semantics: A framework for neural models of semantic composition", "author": ["Phong Le", "Willem Zuidema."], "venue": "NIPS 2014 Workshop on Deep Learning and Representation Learning.", "citeRegEx": "Le and Zuidema.,? 2014b", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Language models based on semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430\u2013439.", "citeRegEx": "Mitchell and Lapata.,? 2009", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2009}, {"title": "Global belief recursive neural networks", "author": ["Romain Paulus", "Richard Socher", "Christopher D Manning."], "venue": "Advances in Neural Information Processing Systems, pages 2888\u20132896.", "citeRegEx": "Paulus et al\\.,? 2014", "shortCiteRegEx": "Paulus et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Learning representations by backpropagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams."], "venue": "Cognitive modeling, 5.", "citeRegEx": "Rumelhart et al\\.,? 1988", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks", "author": ["Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.", "citeRegEx": "Socher et al\\.,? 2010", "shortCiteRegEx": "Socher et al\\.", "year": 2010}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Advances in Neural Information Processing Systems, 24:801\u2013809.", "citeRegEx": "Socher et al\\.,? 2011a", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the 26th International Conference on Machine Learning, volume 2.", "citeRegEx": "Socher et al\\.,? 2011b", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings EMNLP.", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos."], "venue": "Proceedings of the IEEE, 78(10):1550\u20131560.", "citeRegEx": "Werbos.,? 1990", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Long short-term memory over tree structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "arXiv preprint arXiv:1503.04881.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Moving from lexical to compositional semantics in vector-based semantics requires answers to two difficult questions: (i) what is the nature of the composition functions (given that the lambda calculus for variable binding is no longer applicable), and (ii) how do we learn the parameters of those functions (if they have any) from data? A number of classes of functions have been proposed in answer to the first question, including simple linear functions like vector addition (Mitchell and Lapata, 2009), non-linear functions like those defined by multi-layer neural networks (Socher et al.", "startOffset": 478, "endOffset": 505}, {"referenceID": 22, "context": "Moving from lexical to compositional semantics in vector-based semantics requires answers to two difficult questions: (i) what is the nature of the composition functions (given that the lambda calculus for variable binding is no longer applicable), and (ii) how do we learn the parameters of those functions (if they have any) from data? A number of classes of functions have been proposed in answer to the first question, including simple linear functions like vector addition (Mitchell and Lapata, 2009), non-linear functions like those defined by multi-layer neural networks (Socher et al., 2010), and vector matrix multiplication and tensor linear mapping (Baroni et al.", "startOffset": 578, "endOffset": 599}, {"referenceID": 0, "context": ", 2010), and vector matrix multiplication and tensor linear mapping (Baroni et al., 2013).", "startOffset": 68, "endOffset": 89}, {"referenceID": 2, "context": "The matrix and tensor-based functions have the advantage of allowing a relatively straightforward comparison with formal semantics, but the fact that multi-layer neural networks with non-linear activation functions like sigmoid can approximate any continuous function (Cybenko, 1989) already make them an attractive choice.", "startOffset": 268, "endOffset": 283}, {"referenceID": 26, "context": "In trying to answer the second question, the advantages of approaches based on neural network architectures, such as the recursive neural network (RNN) model (Socher et al., 2013b) and the con-", "startOffset": 158, "endOffset": 180}, {"referenceID": 12, "context": "volutional neural network model (Kalchbrenner et al., 2014), are even clearer.", "startOffset": 32, "endOffset": 59}, {"referenceID": 24, "context": "Since the first success of the RNN model (Socher et al., 2011b) in constituent parsing, two classes of", "startOffset": 41, "endOffset": 63}, {"referenceID": 26, "context": "One class is to enhance its compositionality by using tensor product (Socher et al., 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014).", "startOffset": 69, "endOffset": 91}, {"referenceID": 11, "context": ", 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014).", "startOffset": 65, "endOffset": 89}, {"referenceID": 11, "context": ", 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al.", "startOffset": 66, "endOffset": 199}, {"referenceID": 11, "context": ", 2013b) or concatenating RNNs horizontally to make a deeper net (Irsoy and Cardie, 2014). The other is to extend its topology in order to fulfill a wider range of tasks, like Le and Zuidema (2014a) for dependency parsing and Paulus et al. (2014) for context-dependence sentiment analysis.", "startOffset": 66, "endOffset": 247}, {"referenceID": 10, "context": "Our motivation is that, like training recurrent neural networks, training RNNs on deep trees can suffer from the vanishing gradient problem (Hochreiter et al., 2001), i.", "startOffset": 140, "endOffset": 165}, {"referenceID": 21, "context": "Thanks to the back-propagation algorithm (Rumelhart et al., 1988), the gradient \u2202J/\u2202\u03b8 is efficiently computed; the gradient descent method thus is used to minimize J .", "startOffset": 41, "endOffset": 65}, {"referenceID": 7, "context": "A recursive neural network (RNN) (Goller and K\u00fcchler, 1996) is an MLN where, given a tree structure, we recursively apply the same weight matrices at each inner node in a bottom-up manner.", "startOffset": 33, "endOffset": 59}, {"referenceID": 7, "context": "structure algorithm (Goller and K\u00fcchler, 1996).", "startOffset": 20, "endOffset": 46}, {"referenceID": 25, "context": "The RNN model and its extensions have been employed successfully to solve a wide range of problems: from parsing (constituent parsing (Socher et al., 2013a), dependency parsing (Le and Zuidema,", "startOffset": 134, "endOffset": 156}, {"referenceID": 26, "context": "sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al.", "startOffset": 19, "endOffset": 65}, {"referenceID": 11, "context": "sentiment analysis (Socher et al., 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al.", "startOffset": 19, "endOffset": 65}, {"referenceID": 23, "context": ", 2013b; Irsoy and Cardie, 2014), paraphrase detection (Socher et al., 2011a), semantic role labelling (Le and Zuidema, 2014b)).", "startOffset": 55, "endOffset": 77}, {"referenceID": 16, "context": ", 2011a), semantic role labelling (Le and Zuidema, 2014b)).", "startOffset": 34, "endOffset": 57}, {"referenceID": 4, "context": "In the natural language processing field, the simple recurrent neural network (SRN) proposed by Elman (1990) (see", "startOffset": 96, "endOffset": 109}, {"referenceID": 28, "context": "Figure 3-left) and its extensions are used to tackle sequence-related problems, such as machine translation (Sutskever et al., 2014) and language modelling (Mikolov et al.", "startOffset": 108, "endOffset": 132}, {"referenceID": 17, "context": ", 2014) and language modelling (Mikolov et al., 2010).", "startOffset": 31, "endOffset": 53}, {"referenceID": 10, "context": "In practice, however, training recurrent neural networks with the gradient descent method is challenging because gradients \u2202Jt/\u2202hj (j \u2264 t, Jt is the objective function at time t) vanish quickly after a few back-propagation steps (Hochreiter et al., 2001).", "startOffset": 229, "endOffset": 254}, {"referenceID": 8, "context": "One solution for this, proposed by Hochreiter and Schmidhuber (1997) and enhanced by Gers (2001), is long short-term memory (LSTM).", "startOffset": 35, "endOffset": 69}, {"referenceID": 5, "context": "One solution for this, proposed by Hochreiter and Schmidhuber (1997) and enhanced by Gers (2001), is long short-term memory (LSTM).", "startOffset": 85, "endOffset": 97}, {"referenceID": 11, "context": "Similarly to Irsoy and Cardie (2014), we \u2018untie\u2019 leaf nodes and inner nodes: we use one weight matrix set for leaf nodes and another set for inner nodes.", "startOffset": 13, "endOffset": 37}, {"referenceID": 29, "context": "Independently from and concurrently with our work, Tai et al. (2015) and Zhu et al.", "startOffset": 51, "endOffset": 69}, {"referenceID": 29, "context": "Independently from and concurrently with our work, Tai et al. (2015) and Zhu et al. (2015) have developed very similar models applying LTSM to RNNs.", "startOffset": 51, "endOffset": 91}, {"referenceID": 7, "context": "Like training an RNN, we use the mini-batch gradient descent method to minimize J , where the gradient \u2202J/\u2202\u03b8 is computed efficiently thanks to the backpropagation through structure (Goller and K\u00fcchler, 1996).", "startOffset": 181, "endOffset": 207}, {"referenceID": 3, "context": "We use the AdaGrad method (Duchi et al., 2011) to automatically update the learning rate for", "startOffset": 26, "endOffset": 46}, {"referenceID": 26, "context": "We used the Stanford Sentiment Treebank4 (Socher et al., 2013b) which consists of 5-way fine-grained sentiment labels (very negative, negative, neutral, positive, very positive) for 215,154 phrases of", "startOffset": 41, "endOffset": 63}, {"referenceID": 20, "context": "Setting We initialized the word vectors by the 100-D GloVe5 word embeddings (Pennington et al., 2014), which were trained on a 6B-word corpus.", "startOffset": 76, "endOffset": 101}, {"referenceID": 6, "context": "The softsign function, which was shown to work better than tanh for deep networks (Glorot and Bengio, 2010), however, did not yield improvements", "startOffset": 82, "endOffset": 107}, {"referenceID": 5, "context": "This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014).", "startOffset": 127, "endOffset": 163}, {"referenceID": 28, "context": "This result agrees with the common choice for this activation function for the LSTM architecture in recurrent network research (Gers, 2001; Sutskever et al., 2014).", "startOffset": 127, "endOffset": 163}, {"referenceID": 26, "context": "We compare LSTM-RNN (using tanh) in the previous experiment against existing models: Naive Bayes with bag of bigram features (BiNB), Recursive neural tensor network (RNTN) (Socher et al., 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network Model Fine-grained Binary", "startOffset": 172, "endOffset": 194}, {"referenceID": 13, "context": ", 2013b), Convolutional neural network (CNN) (Kim, 2014), Dynamic convolutional neural network Model Fine-grained Binary", "startOffset": 45, "endOffset": 56}, {"referenceID": 12, "context": "(DCNN) (Kalchbrenner et al., 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014).", "startOffset": 7, "endOffset": 34}, {"referenceID": 14, "context": ", 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014).", "startOffset": 32, "endOffset": 54}, {"referenceID": 11, "context": ", 2014), paragraph vectors (PV) (Le and Mikolov, 2014), and Deep RNN (DRNN) (Irsoy and Cardie, 2014).", "startOffset": 76, "endOffset": 100}, {"referenceID": 27, "context": "Dropout is a powerful technique to train neural networks, not only because it plays a role as a strong regularization method to prohibit neurons co-adapting, but it is also considered a technique to efficiently make an ensemble of a large number of shared weight neural networks (Srivastava et al., 2014).", "startOffset": 279, "endOffset": 304}, {"referenceID": 11, "context": "Thanks to dropout, Irsoy and Cardie (2014) boosted the accuracy of a 3-layer-DRNN with d = 200 from 46.", "startOffset": 19, "endOffset": 43}, {"referenceID": 23, "context": "auto-encoder (RAE) (Socher et al., 2011a).", "startOffset": 19, "endOffset": 41}, {"referenceID": 23, "context": "Because pre-training an RNN as an RAE can boost the overall performance (Socher et al., 2011a; Socher et al., 2011c), seeing LSTM as a compressor might explain why the LSTM-RNN worked better than RNN with-", "startOffset": 72, "endOffset": 116}, {"referenceID": 11, "context": "Comparing LSTM-RNN against DRNN (Irsoy and Cardie, 2014) gives us a hint about how to improve our model.", "startOffset": 32, "endOffset": 56}], "year": 2015, "abstractText": "We are proposing an extension of the recursive neural network that makes use of a variant of the long short-term memory architecture. The extension allows information low in parse trees to be stored in a memory register (the \u2018memory cell\u2019) and used much later higher up in the parse tree. This provides a solution to the vanishing gradient problem and allows the network to capture long range dependencies. Experimental results show that our composition outperformed the traditional neural-network composition on the Stanford Sentiment Treebank.", "creator": "TeX"}}}