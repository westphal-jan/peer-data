{"id": "1503.03594", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2015", "title": "Efficient Learning of Linear Separators under Bounded Noise", "abstract": "codes study calculating cost of suitable separators in $ \\ au ^ d $ in the presence of normal ( a. sin. a massart ) noise. \u03b1 is a conceptual generalization of the sequential classification noise model, the efficient adversary can isolate each example $ x $ with estimated $ \\ eta ( 0 ) \\ leq \\ eta $. we provide the robust empirical information algebra that can learn uniformly resistance to arbitrarily small excess error in this noise expression under the uniform distribution over mixed game ball in $ \\ re ^ d $, for maximum probability outcome x $ \\ g $. previously widely studied use highly informal learning theory specifically in various context whereby reliable objective convergence achieved, computationally efficient integration in this model has remained unreliable. computational work provides the hard evidence that codes desire indeed switch efficiently under arbitrarily small total variation in polynomial problems under the realistic noise model and its opens'promising new and exciting line of research.", "histories": [["v1", "Thu, 12 Mar 2015 05:38:19 GMT  (1039kb,D)", "http://arxiv.org/abs/1503.03594v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CC", "authors": ["pranjal awasthi", "maria-florina balcan", "nika haghtalab", "ruth urner"], "accepted": false, "id": "1503.03594"}, "pdf": {"name": "1503.03594.pdf", "metadata": {"source": "CRF", "title": "Efficient Learning of Linear Separators under Bounded Noise", "authors": ["Pranjal Awasthi", "Maria-Florina Balcan", "Nika Haghtalab", "Ruth Urner"], "emails": ["pawashti@cs.princeton.edu", "ninamf@cs.cmu.edu", "nhaghtal@cs.cmu.edu", "rurner@tuebingen.mpg.de"], "sections": [{"heading": null, "text": "We additionally provide lower bounds showing that popular algorithms such as hinge loss minimization and averaging cannot lead to arbitrarily small excess error under Massart noise, even under the uniform distribution. Our work instead, makes use of a margin based technique developed in the context of active learning. As a result, our algorithm is also an active learning algorithm with label complexity that is only a logarithmic the desired excess error ."}, {"heading": "1 Introduction", "text": "Overview Linear separators are the most popular classifiers studied in both the theory and practice of machine learning. Designing noise tolerant, polynomial time learning algorithms that achieve arbitrarily small excess error rates for linear separators is a long-standing question in learning theory. In the absence of noise (when the data is realizable) such algorithms exist via linear programming [11]. However, the problem becomes significantly harder in the presence of label noise. In particular, in this work we are concerned with designing algorithms that can achieve error OPT + which is arbitrarily close to OPT, the error of the best linear separator, and run in time polynomial in 1 and d (as usual, we call the excess error). Such strong guarantees are only known for the well studied random classification noise model [7]. In this work, we provide the first algorithm that can achieve arbitrarily small excess error, in truly polynomial time, for bounded noise, also called Massart noise [28], a much more realistic and widely studied noise model in statistical learning theory [9]. We additionally show strong lower bounds under the same noise model for two other computationally efficient learning algorithms (hinge loss minimization and the averaging algorithm), which could be of independent interest. Motivation The work on computationally efficient algorithms for learning halfspaces has focused on two different extremes. On one hand, for the very stylized random classification noise model (RCN), where each\nar X\niv :1\n50 3.\n03 59\n4v 1\n[ cs\n.L G\n] 1\n2 M\nar 2\nexample x is flipped independently with equal probability \u03b7, several works have provided computationally efficient algorithms that can achieve arbitrarily small excess error in polynomial time [7, 30, 5] \u2014 note that all these results crucially exploit the high amount of symmetry present in the RCN noise. At the other extreme, there has been significant work on much more difficult and adversarial noise models, including the agnostic model [25] and malicious noise models [24]. The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd\u22121 achieves excess error cOPT [2], for some large constant c. While interesting from a technical point of view, guarantees of this form are somewhat troubling from a statistical point of view, as they are inconsistent, in the sense there is a barrier O(OPT), after which we cannot prove that the excess error further decreases as we get more and more samples. In fact, recent evidence shows that this is unavoidable for polynomial time algorithms for such adversarial noise models [12].\nOur Results In this work we identify a realistic and widely studied noise model in the statistical learning theory, the so called Massart noise [9], for which we can prove much stronger guarantees. Massart noise can be thought of as a generalization of the random classification noise model where the label of each example x is flipped independently with probability \u03b7(x) < 1/2. The adversary has control over choosing a different noise rate \u03b7(x) \u2264 \u03b7 for every example x with the only constraint that \u03b7(x) \u2264 \u03b7. From a statistical point of view, it is well known that under this model, we can get faster rates compared to worst case joint distributions [9]. In computational learning theory, this noise model was also studied, but under the name of malicious misclassification noise [29, 31]. However due to its highly unsymmetric nature, til date, computationally efficient learning algorithms in this model have remained elusive. In this work, we provide the first computationally efficient algorithm achieving arbitrarily small excess error for learning linear separators.\nFormally, we show that there exists a polynomial time algorithm that can learn linear separators to error OPT+ and run in poly(d, 1 ) when the underlying distribution is the uniform distribution over the unit ball in <d and the noise of each example is upper bounded by a constant \u03b7 (independent of the dimension).\nAs mentioned earlier, a result of this form was only known for random classification noise. From a technical point of view, as opposed to random classification noise, where the error of each classifier scales uniformly under the observed labels, the observed error of classifiers under Masasart noise could change drastically in a non-monotonic fashion. This is due to the fact that the adversary has control over choosing a different noise rate \u03b7(x) \u2264 \u03b7 for every example x. As a result, as we show in our work (see Section 4), standard algorithms such as the averaging algorithm [30] which work for random noise can only achieve a much poorer excess error (as a function of \u03b7) under Massart noise. Technically speaking, this is due to the fact that Massart noise can introduce high correlations between the observed labels and the component orthogonal to the direction of the best classifier.\nIn face of these challenges, we take an entirely different approach than previously considered for random classification noise. Specifically, we analyze a recent margin based algorithm of [2]. This algorithm was designed for learning linear separators under agnostic and malicious noise models, and it was shown to achieve an excess error of cOPT for a constant c. By using new structural insights, we show that there exists a constant \u03b7 (independent of the dimension), so that if we use Massart noise where the flipping probability is upper bounded by \u03b7, we can use a modification of the algorithm in [2] and achieve arbitrarily small excess error. One way to think about this result is that we define an adaptively chosen sequence of hinge loss minimization problems around smaller and smaller bands around the current guess for the target. We show by relating the hinge loss and 0/1-loss together with a careful localization analysis that these will\ndirect us closer and closer to the optimal classifier, allowing us to achieve arbitrarily small excess error rates in polynomial time.\nGiven that our algorithm is an adaptively chosen sequence of hinge loss minimization problems, one might wonder what guarantee one-shot hinge loss minimization could provide. In Section 5, we show a strong negative result: for every \u03c4 , and \u03b7 \u2264 1/2, there is a noisy distribution D\u0303 over <d \u00d7 {0, 1} satisfying Massart noise with parameter \u03b7 and an > 0, such that \u03c4 -hinge loss minimization returns a classifier with excess error \u2126( ). This result could be of independent interest. While there exists earlier work showing that hinge loss minimization can lead to classifiers of large 0/1-loss [6], the lower bounds in that paper employ distributions with significant mass on discrete points with flipped label (which is not possible under Massart noise) at a very large distance from the optimal classifier. Thus, that result makes strong use of the hinge loss\u2019s sensitivity to errors at large distance. Here, we show that hinge loss minimization is bound to fail under much more benign conditions.\nOne appealing feature of our result is the algorithm we analyze is in fact naturally adaptable to the active learning or selective sampling scenario (intensively studied in recent years [19, 13, 20], where the learning algorithms only receive the classifications of examples when they ask for them. We show that, in this model, our algorithms achieve a label complexity whose dependence on the error parameter is polylogarithmic (and thus exponentially better than that of any passive algorithm). This provides the first polynomial-time active learning algorithm for learning linear separators under Massart noise. We note that prior to our work only inefficient algorithms could achieve the desired label complexity under Massart noise [4, 20].\nRelated Work The agnostic noise model is notoriously hard to deal with computationally and there is significant evidence that achieving arbitrarily small excess error in polynomial time is hard in this model [1, 18, 12]. For this model, under our distributional assumptions, [23] provides an algorithm that learns linear separators in <d to excess error at most , but whose running time poly(dexp(1/ )). Recent work show evidence that the exponential dependence on 1/ is unavoidable in this case [26] for the agnostic case. We side-step this by considering a more structured, yet realistic noise model.\nMotivated by the fact that many modern machine learning applications have massive amounts of unannotated or unlabeled data, there has been significant interest in designing active learning algorithms that most efficiently utilize the available data, while minimizing the need for human intervention. Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20]. However, despite many efforts, except for very simple noise models (random classification noise [5] and linear noise [16]), to date there are no known computationally efficient algorithms with provable guarantees in the presence of Massart noise that can achieve arbitrarily small excess error.\nWe note that work of [21] provides computationally efficient algorithms for both passive and active learning under the assumption that the hinge loss (or other surrogate loss) minimizer aligns with the minimizer of the 0/1-loss. In our work (Section 5), we show that this is not the case under Massart noise even when the marginal over the instance space is uniform, but still provide a computationally efficient algorithm for this much more challenging setting."}, {"heading": "2 Preliminaries", "text": "We consider the binary classification problem; that is, we work on the problem of predicting a binary label y for a given instance x. We assume that the data points (x, y) are drawn from an unknown underlying\ndistribution D\u0303 over X \u00d7 Y , where X = <d is the instance space and Y = {\u22121, 1} is the label space. For the purpose of this work, we consider distributions where the marginal of D\u0303 over X is a uniform distribution on a d-dimensional unit ball. We work with the class of all homogeneous halfspaces, denoted byH = {sign(w \u00b7 x) : w \u2208 <d}. For a given halfspace w \u2208 H, we define the error of w with respect to D\u0303, by errD\u0303(w) = Pr(x,y)\u223cD\u0303[sign(w \u00b7 x) 6= y].\nWe examine learning halfspaces in the presence of Massart noise. In this setting, we assume that the Bayes optimal classifier is a linear separator w\u2217. Note that w\u2217 can have a non-zero error. Then Massart noise with parameter \u03b2 > 0 is a condition such that for all x, the conditional label probability is such that\n|Pr(y = 1|x)\u2212 Pr(y = \u22121|x)| \u2265 \u03b2. (1)\nEquivalently, we say that D\u0303 satisfies Massart noise with parameter \u03b2, if an adversary construct D\u0303 by first taking the distribution D over instances (x, sign(w\u2217 \u00b7 x)) and then flipping the label of an instance x with probability at most 1\u2212\u03b22 .\n1 Also note that under distribution D\u0303, w\u2217 remains the Bayes optimal classier. In the remainder of this work, we refer to D\u0303 as the \u201cnoisy\u201d distribution and to distribution D over instances (x, sign(w\u2217 \u00b7 x)) as the \u201cclean\u201d distribution.\nOur goal is then to find a halfspace w that has small excess error, as compared to the Bayes optimal classifier w\u2217. That is, for any > 0, find a halfspace w, such that errD\u0303(w) \u2212 errD\u0303(w\n\u2217) \u2264 . Note that the excess error of any classifier w only depends on the points in the region where w and w\u2217 disagree. So, errD\u0303(w)\u2212 errD\u0303(w \u2217) \u2264 \u03b8(w,w \u2217)\n\u03c0 . Additionally, under Massart noise the amount of noise in the disagreement region is also bounded by 1\u2212\u03b22 . It is not difficult to see that under Massart noise,\n\u03b2 \u03b8(w,w\u2217)\n\u03c0 \u2264 errD\u0303(w)\u2212 errD\u0303(w\n\u2217). (2)\nIn our analysis, we frequently examine the region within a certain margin of a halfspace. For a halfspace w and margin b, let Sw,b be the set of all points that fall within a margin b from w, i.e., Sw,b = {x : |w \u00b7 x| \u2264 b}. For distributions D\u0303 and D, we indicate the distribution conditioned on Sw,b by D\u0303w,b and Dw,b, respectively. In the remainder of this work, we refer to the region Sw,b as \u201cthe band\u201d.\nIn our analysis, we use hinge loss, as a convex surrogate function for the 0/1-loss. For a halfspace w, we use \u03c4 -normalized hinge loss that is defined as `(w, x, y) = max{0, 1 \u2212 (w\u00b7x)y\u03c4 }. For a labeled sample set W , let `(w,W ) = 1|W | \u2211 (x,y)\u2208W `(w, x, y) be the empirical hinge loss of a vector w with respect to W ."}, {"heading": "3 Computationally Efficient Algorithm for Massart Noise", "text": "In this section, prove our main result for learning half-spaces in presence of Massart noise. We focus on the case where D is the uniform distribution on the d-dimensional unit ball. Our main Theorem is as follows.\nTheorem 1. Let the optimal bayes classifier be a half-space denoted by w\u2217. Assume that the massart noise condition holds for some \u03b2 > 1 \u2212 3.6 \u00d7 10\u22126. Then for any , \u03b4 > 0, Algorithm 1 with \u03bb = 10\u22128, \u03b1k = 0.038709\u03c0(1\u2212\u03bb)k\u22121, bk\u22121 = 2.3463\u03b1k\u221ad , and \u03c4k = \u221a 2.50306 (3.6\u00d710\u22126)1/4bk\u22121, runs in polynomial time, proceeds in s = O(log 1 ) rounds, where in round k it takes nk = poly(d, exp(k), log( 1 \u03b4 )) unlabeled samples and mk = O(d(d+ log(k/\u03b4))) labels and with probability (1\u2212 \u03b4) returns a linear separator that has excess error (compared to w\u2217) of at most .\n1Note that the relationship between Massart noise parameter \u03b2, and the maximum flipping probability discussed in the introduction \u03b7, is \u03b7 = 1\u2212\u03b2\n2 .\nNote that in the above theorem and Algorithm 1, the value of \u03b2 is unknown to the algorithm, and therefore, our results are adaptive to values of \u03b2 within the acceptable range defined by the theorem.\nThe algorithm described above is similar to that of [2] and uses an iterative margin-based approach. The algorithm runs for s = log 1\n1\u2212\u03bb (1 ) rounds for a constant \u03bb \u2208 (0, 1]. By induction assume that our algorithm\nproduces a hypothesis wk\u22121 at round k \u2212 1 such that \u03b8(wk\u22121, w\u2217) \u2264 \u03b1k. We satisfy the base case by using an algorithm of [27]. At round k, we sample mk labeled examples from the conditional distribution D\u0303wk\u22121,bk\u22121 which is the uniform distribution over {x : |wk\u22121 \u00b7 x| \u2264 bk\u22121}. We then choose wk from the set of all hypothesis B(wk\u22121, \u03b1k) = {w : \u03b8(w,wk\u22121) \u2264 \u03b1k} such that wk minimizes the empirical hinge loss over these examples. Subsequently, as we prove in detail later, \u03b8(wk, w\u2217) \u2264 \u03b1k+1. Note that for any w, the excess error of w is at most the error of w on D\u0303 when the labels are corrected according to w\u2217, i.e., errD\u0303(w) \u2212 errD\u0303(w \u2217) \u2264 errD(w). Moreover, when D is uniform, errD(w) = \u03b8(w \u2217,w) \u03c0 . Hence, \u03b8(ws, w \u2217) \u2264 \u03c0 implies that ws has excess error of at most .\nThe algorithm described below was originally introduced to achieve an error of c \u00b7err(w\u2217) for some constant c in presence of adversarial noise. Achieving a small excess error err(w\u2217)+ is a much more ambitious goal \u2013 one that requires new technical insights. Our two crucial technical innovations are as follow: We first make a key observation that under Massart noise, the noise rate over any conditional distribution D\u0303 is still at most 1\u2212\u03b22 . Therefore, as we focus on the distribution within the band, our noise rate does not increase. Our second technical contribution is a careful choice of parameters. Indeed the choice of parameters, upto a constant, plays an important role in tolerating a constant amount of Massart noise. Using these insights, we show that the algorithm by [2] can indeed achieve a much stronger guarantee, namely arbitrarily small excess error in presence of Massart noise. That is, for any , this algorithm can achieve error of err(w\u2217) + in the presence of Massart noise.\nAlgorithm 1 EFFICIENT ALGORITHM FOR ARBITRARILY SMALL EXCESS ERROR FOR MASSART NOISE Input: A distribution D\u0303. An oracle that returns x and an oracle that returns y for a (x, y) sampled from D\u0303. Permitted excess error and probability of failure \u03b4. Parameters: A learning rate \u03bb; a sequence of sample sizes mk; a sequence of angles of the hypothesis space \u03b1k; a sequence of widths of the labeled space bk; a sequence of thresholds of hinge-loss \u03c4k. Algorithm:\n1. Take poly(d, 1\u03b4 ) samples and run poly(d, 1 \u03b4 )-time algorithm by [27] to find a half-spacew0 with excess\nerror 0.0387089 such that \u03b8(w\u2217, w0) \u2264 0.038709\u03c0 (Refer to Appendix C)\n2. Draw m1 examples (x, y) from D\u0303 and put them into a working set W .\n3. For k = 1, . . . , log( 1 1\u2212\u03bb )\n(1 ) = s.\n(a) Find vk such that \u2016vk \u2212wk\u22121\u2016 < \u03b1k (as a result vk \u2208 B(wk\u22121, \u03b1k)), that minimizes the empirical hinge loss over W using threshold \u03c4k. That is `\u03c4k(vk,W ) \u2264 minw\u2208B(wk\u22121,\u03b1k) `\u03c4k(w,W ) + 10\u22128.\n(b) Clear the working set W .\n(c) Normalize vk to wk = vk\u2016vk\u20162 . Until mk+1 additional examples are put in W , draw an example x\nfrom D\u0303. If |wk \u00b7 x| \u2265 bk, then reject x, else put (x, y) into W .\nOutput: Return ws, which has excess error with probability 1\u2212 \u03b4.\nOverview of our analysis: Similar to [2], we divide errD(wk) to two categories; error in the band, i.e., on x \u2208 Swk\u22121,bk\u22121 , and error outside the band, on x 6\u2208 Swk\u22121,bk\u22121 . We choose bk\u22121 and \u03b1k such that, for every hypothesis w \u2208 B(wk\u22121, \u03b1k) that is considered at step k, the probability mass outside the band such that w and w\u2217 also disagree is very small (Lemma 5). Therefore, the error associated with the region outside the band is also very small. This motivates the design of the algorithm to only minimize the error in the band. Furthermore, the probability mass of the band is also small enough such that for errD(wk) \u2264 \u03b1k+1 to hold, it suffices for wk to have a small constant error over the clean distribution restricted to the band, namely Dwk\u22121,bk\u22121 .\nThis is where minimizing hinge loss in the band comes in. As minimizing the 0/1-loss is NP-hard, an alternative method for finding wk with small error in the band is needed. Hinge loss that is a convex loss function can be efficiently minimized. So, we can efficiently find wk that minimizes the empirical hinge loss of the sample drawn from D\u0303wk\u22121,bk\u22121 . To allow the hinge loss to remain a faithful proxy of 0/1-loss as we focus on bands with smaller widths, we use a normalized hinge loss function defined by `\u03c4 (w, x, y) = max{0, 1\u2212 w\u00b7xy\u03c4 }.\nA crucial part of our analysis involves showing that if wk minimizes the empirical hinge loss of the sample set drawn from D\u0303wk\u22121,bk\u22121 , it indeed has a small 0/1-error on Dwk\u22121,bk\u22121 . To this end, we first show that when \u03c4k is proportional to bk, the hinge loss of w\u2217 on Dwk\u22121,bk\u22121 , which is an upper bound on the 0/1-error of wk in the band, is itself small (Lemma 1). Next, we notice that under Massart noise, the noise rate in any marginal of the distribution is still at most 1\u2212\u03b22 . Therefore, focusing the distribution in the band does not increase the probability of noise in the band. Moreover, the noise points in the band are close to the decision boundary so intuitively speaking, they can not increase the hinge loss too much. Using these insights we can show that the hinge loss of wk on D\u0303wk\u22121,bk\u22121 is close to its hinge loss on Dwk\u22121,bk\u22121 (Lemma 2).\nProof of Theorem 1 and related lemmas\nTo prove Theorem 1, we first introduce a series of lemmas concerning the behavior of hinge loss in the band. These lemmas build up towards showing that wk has error of at most a fixed small constant in the band.\nFor ease of exposition, for any k, let Dk and D\u0303k represent Dwk\u22121,bk\u22121 and D\u0303wk\u22121,bk\u22121 , respectively, and `(\u00b7) represent `\u03c4k(\u00b7). Furthermore, let c = 2.3463, such that bk\u22121 = c\u03b1k\u221a d\n. Our first lemma, whose proof appears in Appendix B, provides an upper bound on the true hinge error\nof w\u2217 on the clean distribution in the band.\nLemma 1. E(x,y)\u223cDk`(w \u2217, x, y) \u2264 0.665769 \u03c4b .\nThe next Lemma compares the true hinge loss of any w \u2208 B(wk\u22121, \u03b1k) on two distributions, D\u0303k and Dk. It is clear that the difference between the hinge loss on these two distributions is entirely attributed to the noise points and their margin from w. A key insight in the proof of this lemma is that as we concentrate in the band, the probability of seeing a noise point remains under 1\u2212\u03b22 . This is due to the fact that under Massart noise, each label can be changed with probability at most 1\u2212\u03b22 . Furthermore, by concentrating in the band all points are close to the decision boundary of wk\u22121. Since w is also close in angle to wk\u22121, then points in the band are also close to the decision boundary of w. Therefore the hinge loss of noise points in the band can not increase the total hinge loss of w by too much.\nLemma 2. For any w such that w \u2208 B(wk\u22121, \u03b1k), we have\n|E(x,y)\u223cDk`(w, x, y)\u2212 E(x,y)\u223cD\u0303k`(w, x, y)| \u2264 1.092 \u221a\n2 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k .\nProof. Let N be the set of noise points. We have,\n|E(x,y)\u223cD\u0303k`(w, x, y)\u2212 E(x,y)\u223cDk`(w, x, y)| = |E(x,y)\u2208D\u0303k (`(w, x, y)\u2212 `(w, x, sign(w \u2217 \u00b7 x)) |\n\u2264 E(x,y)\u223cD\u0303k (1x\u2208N (`(w, x, y)\u2212 `(w, x,\u2212y)))\n\u2264 2E(x,y)\u223cD\u0303k\n( 1x\u2208N\n|w \u00b7 x| \u03c4k ) \u2264 2 \u03c4k \u221a Pr (x,y)\u223cD\u0303k (x \u2208 N)\u00d7 \u221a E(x,y)\u223cD\u0303k(w \u00b7 x) 2 (By Cauchy Shwarz)\n\u2264 2 \u03c4k\n\u221a 1\u2212 \u03b2\n2 \u221a \u03b12k d\u2212 1 + b2k\u22121 (By Definition 4.1 of [2] for uniform)\n\u2264 \u221a 2 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k\n\u221a d\n(d\u2212 1)c2 + 1\n\u2264 1.092 \u221a 2 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k\n(for d > 20, c > 1)\nFor a labeled sample set W drawn at random from D\u0303k, let cleaned(W ) be the set of samples with the labels corrected by w\u2217, i.e., cleaned(W ) = {(x, sign(w\u2217 \u00b7 x)) : for all (x, y) \u2208 W}. Then by standard VC-dimension bounds (Proof included in Appendix B) there is mk \u2208 O(d(d + log(k/d))) such that for any randomly drawn set W of mk labeled samples from D\u0303k, with probability 1 \u2212 \u03b42(k+k2) , for any w \u2208 B(wk\u22121, \u03b1k),\n|E(x,y)\u223cD\u0303k`(w, x, y)\u2212 `(w,W )| \u2264 10 \u22128, (3)\n|E(x,y)\u223cDk`(w, x, y)\u2212 `(w, cleaned(W ))| \u2264 10 \u22128. (4)\nOur next lemma is a crucial step in our analysis of Algorithm 1. This lemma proves that ifwk \u2208 B(wk\u22121, \u03b1k) minimizes the empirical hinge loss on the sample drawn from the noisy distribution in the band, namely D\u0303wk\u22121,bk\u22121 , then with high probability wk also has a small 0/1-error with respect to the clean distribution in the band, i.e., Dwk\u22121,bk\u22121 .\nLemma 3. There exists mk \u2208 O(d(d+ log(k/d))), such that for a randomly drawn labeled sampled set W of size mk from D\u0303k, and for wk such that wk has the minimum empirical hinge loss on W between the set of all hypothesis in B(wk\u22121, \u03b1k), with probability 1\u2212 \u03b42(k+k2) ,\nerrDk(wk) \u2264 0.757941 \u03c4k bk\u22121\n+ 3.303 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k + 3.28\u00d7 10\u22128.\nProof Sketch First, we note that the true 0/1-error of wk on any distribution is at most its true hinge loss on that distribution. Lemma 1 provides an upper bound on the true hinge loss on distribution Dk. Therefore, it remains to create a connection between the empirical hinge loss of wk on the sample drawn from D\u0303k to its true hinge loss on distribution Dk. This, we achieve by using the generalization bounds of Equations 3 and 4 to connect the empirical and true hinge loss of wk and w\u2217, and using Lemma 2 to connect the hinge of wk and w\u2217 in the clean and noisy distributions.\nProof of Theorem 1 For ease of exposition, let c = 2.3463. Recall that \u03bb = 10\u22128, \u03b1k = 0.038709\u03c0(1 \u2212 \u03bb)k\u22121, bk\u22121 = c\u03b1k\u221ad , \u03c4k = \u221a 2.50306 (3.6\u00d7 10\u22126)1/4bk\u22121, and \u03b2 > 1\u2212 3.6\u00d7 10\u22126.\nNote that for any w, the excess error of w is at most the error of w on the clean distribution D, i.e., errD\u0303(w) \u2212 errD\u0303(w \u2217) \u2264 errD(w). Moreover, for uniform distribution D, errD(w) = \u03b8(w \u2217,w) \u03c0 . Hence, to show that w has excess error, it suffices to show that errD(w) \u2264 . Our goal is to achieve excess error of 0.038709(1\u2212 \u03bb)k at round k. This we do indirectly by bounding errD(wk) at every step. We use induction. For k = 0, we use the algorithm for adversarial noise model by [27], which can achieve excess error of if errD\u0303(w \u2217) < 2 256 log(1/ ) (Refer to Appendix C for more details). For Massart noise, errD\u0303(w \u2217) \u2264 1\u2212\u03b22 . So, for our choice of \u03b2, this algorithm can achieve excess error of 0.0387089 in poly(d, 1\u03b4 ) samples and run-time. Furthermore, using Equation 2, \u03b8(w0, w \u2217) < 0.038709\u03c0.\nAssume that at round k\u22121, errD(wk\u22121) \u2264 0.038709(1\u2212\u03bb)k\u22121. We will show that wk, which is chosen by the algorithm at round k, also has errD(wk) \u2264 0.038709(1\u2212 \u03bb)k.\nFirst note that errD(wk\u22121) \u2264 0.038709(1 \u2212 \u03bb)k\u22121 implies \u03b8(wk\u22121, w\u2217) \u2264 \u03b1k. Let S = Swk\u22121,bk\u22121 indicate the band at round k. We divide the error of wk to two parts, error outside the band and error inside of the band. That is\nerrD(wk) = Pr x\u223cD [x /\u2208 S and (wk \u00b7 x)(w\u2217 \u00b7 x) < 0] + Pr x\u223cD [x \u2208 S and (wk \u00b7 x)(w\u2217 \u00b7 x) < 0].\nFor the first part, i.e., error outside of the band, Prx\u223cD[x /\u2208 S and (wk \u00b7 x)(w\u2217 \u00b7 x) < 0] is at most\nPr x\u223cD [x /\u2208 S and (wk \u00b7 x)(wk\u22121 \u00b7 x) < 0] + Pr x\u223cD [x /\u2208 S and (wk\u22121 \u00b7 x)(w\u2217 \u00b7 x) < 0] \u2264 2\u03b1k \u03c0 e\u2212 c2(d\u22122) 2d ,\nwhere this inequality holds by the application of Lemma 5 and the fact that \u03b8(wk\u22121, wk) \u2264 \u03b1k and \u03b8(wk\u22121, w\n\u2217) \u2264 \u03b1k. For the second part, i.e., error inside the band\nPr x\u223cD [x \u2208 S and (wk \u00b7 x)(w\u2217 \u00b7 x) < 0] = errDk(wk) Pr x\u223cD [x \u2208 S]\n\u2264 errDk(wk) Vd\u22121 Vd 2 bk\u22121 (By Lemma 4)\n\u2264 errDk(wk) c \u03b1k\n\u221a 2(d+ 1)\n\u03c0d , where the last transition holds by the fact that Vd\u22121Vd \u2264 \u221a d+1 2\u03c0 [8]. Replacing an upper bound on errDk(wk) from Lemma 3, to show that errD(wk) \u2264 \u03b1k+1\u03c0 , it suffices to show that the following inequality holds.( 0.757941\n\u03c4k bk\u22121\n+ 3.303 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k\n+ 3.28\u00d7 10\u22128 ) c \u03b1k \u221a 2(d+ 1)\n\u03c0d + 2\u03b1k \u03c0 e\u2212 c2(d\u22122) 2d \u2264 \u03b1k+1 \u03c0 .\nWe simplify this inequality as follows.( 0.757941\n\u03c4k bk\u22121\n+ 3.303 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k\n+ 3.28\u00d7 10\u22128 ) c \u221a 2\u03c0(d+ 1)\nd + 2e\u2212\nc2(d\u22122) 2d \u2264 1\u2212 \u03bb.\nReplacing in the r.h.s., the values of c = 2.3463, and \u03c4k = \u221a\n2.50306(3.6\u00d7 10\u22126)1/4bk\u22121, we have(\u221a 2.50306(3.6\u00d7 10\u22126)1/4 + \u221a 2.50306 \u221a 1\u2212 \u03b2\n(3.6\u00d7 10\u22126)1/4 + 3.28\u00d7 10\u22128\n) c \u221a 2\u03c0(d+ 1)\nd + 2e\u2212\nc2(d\u22122) 2d\n\u2264 5.88133 ( 2 \u221a 2.50306(3.6\u00d7 10\u22126)1/4 + 3.28\u00d7 10\u22128 ) \u221a21\n20 + 0.167935 (For d > 20)\n\u2264 0.998573 < 1\u2212 \u03bb\nTherefore, errD(wk) \u2264 0.038709(1\u2212 \u03bb)k. Sample complexity analysis: We require mk labeled samples in the band Swk\u22121,bk\u22121 at round k. By Lemma 4, the probability that a randomly drawn sample from D\u0303 falls in Swk\u22121,bk\u22121 is at leastO(bk\u22121 \u221a d) = O((1 \u2212 \u03bb)k\u22121). Therefore, we need O((1 \u2212 \u03bb)k\u22121mk) unlabeled samples to get mk examples in the band with probability 1\u2212 \u03b4\n8(k+k2) . So, the total unlabeled sample complexity is at most\ns\u2211 k=1 O ( (1\u2212 \u03bb)k\u22121mk ) \u2264 s s\u2211 k=1 mk \u2208 O ( 1 log ( d )( d+ log log(1/ ) \u03b4 )) .\n4 Average Does Not Work\nOur algorithm described in the previous section uses convex loss minimization (in our case, hinge loss) in the band as an efficient proxy for minimizing the 0/1 loss. The Average algorithm introduced by [30] is another computationally efficient algorithm that has provable noise tolerance guarantees under certain noise models and distributions. For example, it achieves arbitrarily small excess error in the presence of random classification noise and monotonic noise when the distribution is uniform over the unit sphere. Furthermore, even in the presence of a small amount of malicious noise and less symmetric distributions, Average has been used to obtain a weak learner, which can then be boosted to achieve a non-trivial noise tolerance [27]. Therefore it is natural to ask, whether the noise tolerance that Average exhibits could be extended to the case of Massart noise under the uniform distribution? We answer this question in the negative. We show that the lack of symmetry in Massart noise presents a significant barrier for the one-shot application of Average, even when the marginal distribution is completely symmetric. Additionally, we also discuss obstacles in incorporating Average as a weak learner with the margin-based technique.\nIn a nutshell, Average takesm sample points and their respective labels,W = {(x1, y1), . . . , (xm, ym)}, and returns 1m \u2211m i=1 x\niyi. Our main result in this section shows that for a wide range of distributions that are very symmetric in nature, including the Gaussian and the uniform distribution, there is an instance of Massart noise under which Average can not achieve an arbitrarily small excess error.\nTheorem 2. For any continuous distributionD with a p.d.f. that is a function of the distance from the origin only, there is a noisy distribution D\u0303 over X \u00d7{0, 1} that satisfies Massart noise condition in Equation 1 for some parameter \u03b2 > 0 and Average returns a classifier with excess error \u2126(\u03b2(1\u2212\u03b2)1+\u03b2 ).\nProof. Let w\u2217 = (1, 0, . . . , 0) be the target halfspace. Let the noise distribution be such that for all x, if x1x2 < 0 then we flip the label of x with probability 1\u2212\u03b22 , otherwise we keep the label. Clearly, this satisfies Massart noise with parameter \u03b2. Let w be expected vector returned by Average. We first show that w is far from w\u2217 in angle. Then, using Equation 2 we show that w has large excess error.\nFirst we examine the expected component of w that is parallel to w\u2217, i.e., w \u00b7 w\u2217 = w1. For ease of exposition, we divide our analysis to two cases, one for regions with no noise (first and third quadrants)\nand second for regions with noise (second and fourth quadrants). Let E be the event that x1x2 > 0. By symmetry, it is easy to see that Pr[E] = 1/2. Then\nE[w \u00b7 w\u2217] = Pr(E) E[w \u00b7 w\u2217|E] + Pr(E\u0304) E[w \u00b7 w\u2217|E\u0304] For the first term, for x \u2208 E the label has not changed. So, E[w \u00b7 w\u2217|E] = E[|x1| |E] = \u222b 1 0 zf(z). For the second term, the label of each point stays the same with probability 1+\u03b22 and is flipped with probability 1\u2212\u03b2 2 . Hence, E[w \u00b7 w \u2217|E] = \u03b2 E[|x1| |E] = \u03b2 \u222b 1 0 zf(z). Therefore, the expected parallel component of w\nis E[w \u00b7 w\u2217] = 1+\u03b22 \u222b 1 0 zf(z)\nNext, we examine w2, the orthogonal component of w on the second coordinate. Similar to the previous case for the clean regions E[w2|E] = E[|x2| |E] = \u222b 1 0 zf(z). Next, for the second and forth quadrants, which are noisy, we have\nE(x,y)\u223cD\u0303[x2y|x1x2 < 0] = ( 1 + \u03b2\n2 ) \u222b 0 \u22121 z f(z) 2 + ( 1\u2212 \u03b2 2 ) \u222b 0 \u22121 (\u2212z)f(z) 2\n(Fourth quadrant)\n+ ( 1 + \u03b2\n2 ) \u222b 1 0 (\u2212z)f(z) 2 + ( 1\u2212 \u03b2 2 ) \u222b 1 0 z f(z) 2 (Second quadrant)\n= \u2212(1 + \u03b2 2 ) \u222b 1 0 z f(z) 2 + ( 1\u2212 \u03b2 2 ) \u222b 1 0 z f(z) 2\n\u2212 (1 + \u03b2 2 ) \u222b 1 0 z f(z) 2 + ( 1\u2212 \u03b2 2 ) \u222b 1 0 z f(z) 2 (By symmetry)\n= \u2212\u03b2 \u222b 1 0 zf(z).\nSo, w2 = ( 1\u2212\u03b2 2 ) \u222b 1 0 zf(z). Therefore \u03b8(w,w \u2217) = arctan(1\u2212\u03b21+\u03b2 ) \u2265 1\u2212\u03b2 (1+\u03b2) . By Equation 2, we have errD\u0303(w)\u2212 errD\u0303(w \u2217) \u2265 \u03b2 \u03b8(w,w \u2217) \u03c0 \u2265 \u03b2 1\u2212\u03b2 \u03c0(1+\u03b2) .\nOur margin-based analysis from Section 3 relies on using hinge-loss minimization in the band at every round to efficiently find a halfspace wk that is a weak learner for Dk, i.e., errDk(wk) is at most a small constant, as demonstrated in Lemma 3. Motivated by this more lenient goal of finding a weak learner, one might ask whether Average, as an efficient algorithm for finding low error halfspaces, can be incorporated with the margin-based technique in the same way as hinge loss minimization? We argue that the marginbased technique is inherently incompatible with Average.\nThe Margin-based technique maintains two key properties at every step: First, the angle between wk and wk\u22121 and the angle between wk\u22121and w\u2217 are small, and as a result \u03b8(w\u2217, wk) is small. Second, wk is a weak learner with errDk\u22121(wk) at most a small constant. In our work, hinge loss minimization in the band guarantees both of these properties simultaneously by limiting its search to the halfspaces that are close in angle to wk\u22121 and limiting its distribution to Dwk\u22121,bk\u22121 . However, in the case of Average as we concentrate in the band Dwk\u22121,bk\u22121 we bias the distributions towards its orthogonal component with respect to wk\u22121. Hence, an upper bound on \u03b8(w\u2217, wk\u22121) only serves to assure that most of the data is orthogonal to w\u2217 as well. Therefore, informally speaking, we lose the signal that otherwise could direct us in the direction of w\u2217. More formally, consider the construction from Theorem 2 such that wk\u22121 = w\u2217 = (1, 0, . . . , 0). In distribution Dwk\u22121,bk\u22121 , the component of wk that is parallel to wk\u22121 scales down by the width of the band, bk\u22121. However, as most of the probability stays in a band passing through the origin in any log-concave (including Gaussian and uniform) distribution, the orthogonal component of wk remains almost unchanged. Therefore, \u03b8(wk, w\u2217) = \u03b8(wk, wk\u22121) \u2208 \u2126( 1\u2212\u03b2bk\u22121(1+\u03b2)) \u2265 ( (1\u2212\u03b2) \u221a d (1+\u03b2)\u03b1k\u22121 ) ."}, {"heading": "5 Hinge Loss Minimization Does Not Work", "text": "Hinge loss minimization is a widely used technique in Machine Learning. In this section, we show that, perhaps surprisingly, hinge loss minimization does not lead to arbitrarily small excess error even under very small noise condition, that is it is not consistent. (Note that in our setting of Massart noise, consistency is the same as achieving arbitrarily small excess error, since the Bayes optimal classifier is a member of the class of halfspaces).\nIt has been shown earlier that hinge loss minimization can lead to classifiers of large 0/1-loss [6]. However, the lower bounds in that paper employ distributions with significant mass on discrete points with flipped label (which is not possible under Massart noise) at a very large distance from the optimal classifier. Thus, that result makes strong use of the hinge loss\u2019s sensitivity to errors at large distance. Here, we show that hinge loss minimization is bound to fail under much more benign conditions. More concretely, we show that for every parameter \u03c4 , and arbitrarily small bound on the probability of flipping a label, \u03b7 = 1\u2212\u03b22 , hinge loss minimization is not consistent even on distributions with a uniform marginal over the unit ball in <2, with the Bayes optimal classifier being a halfspace and the noise satisfying the Massart noise condition with bound \u03b7. That is, there exists a constant \u2265 0 and a sample size m( ) such that hinge loss minimization returns a classifier of excess error at least with high probability over sample size of at least m( ).\nHinge loss minimization does approximate the optimal hinge loss. We show that this does not translate into an agnostic learning guarantee for halfspaces with respect to the 0/1-loss even under very small noise conditions. Let P\u03b2 be the class of distributions D\u0303 with uniform marginal over the unit ball B1 \u2286 <2, the Bayes classifier being a halfspace w, and satisfying the Massart noise condition with parameter \u03b2. Our lower bound for hinge loss minimization is stated as follows.\nTheorem 3. For every hinge-loss parameter \u03c4 \u2265 0 and every Massart noise parameter 0 \u2264 \u03b2 < 1, there exists a distribution D\u0303\u03c4,\u03b2 \u2208 P\u03b2 (that is, a distribution over B1 \u00d7 {\u22121, 1} with uniform marginal over B1 \u2286 <2 satisfying the \u03b2-Massart condition) such that \u03c4 -hinge loss minimization is not consistent on D\u0303\u03c4,\u03b2 with respect to the class of halfspaces. That is, there exists an \u2265 0 and a sample size m( ) such that hinge loss minimization will output a classifier of excess error larger (with high probability over samples of size at least m( )).\nProof idea To prove the above result, we define a subclass of P\u03b1,\u03b7 \u2286 P\u03b2 consisting of well structured distributions. We then show that for every hinge parameter \u03c4 and every bound on the noise \u03b7, there is a distribution D\u0303 \u2208 P\u03b1,\u03b7 on which \u03c4 -hinge loss minimization is not consistent.\nw*\nw\n\u237a\n\u237a\nA\nA\n\u237a/2\nhw hw*\nB\nBD\nD\nFigure 1: P\u03b1,\u03b7\nIn the remainder of this section, we use the notation hw for the classifier associated with a vector w \u2208 B1, that is hw(x) = sign(w \u00b7 x), since for our geometric construction it is convenient to differentiate between the two. We define a family P\u03b1,\u03b7 \u2286 P\u03b2 of distributions D\u0303\u03b1,\u03b7, indexed by an angle \u03b1 and a noise parameter \u03b7 as follows. Let the Bayes optimal classifier be linear h\u2217 = hw\u2217 for a unit vector w\u2217. Let hw be the classifier that is defined by the unit vector w at angle \u03b1 from w\u2217. We partition the unit ball into areas A, B and D as in the Figure 5. That is A consists of the two wedges of disagreement between hw and hw\u2217 and the wedge where the two classifiers agree is divided into B (points that are closer to hw than to hw\u2217) and D\n(points that are closer to hw\u2217 than to hw). We now flip the labels of all points in A and B with probability \u03b7 = 1\u2212\u03b22 and leave the labels deterministic according to hw\u2217 in the area D.\nMore formally, points at angle between \u03b1/2 and \u03c0/2 and points at angle between \u03c0 + \u03b1/2 and \u2212\u03c0/2 from w\u2217 are labeled per hw\u2217(x) with conditional label probability 1. All other points are labeled \u2212hw\u2217(x)\nwith probability \u03b7 and hw\u2217(x) with probability (1 \u2212 \u03b7). Clearly, this distribution satisfies Massart noise conditions in Equation 1 with parameter \u03b2.\nThe goal of the above construction is to design distributions where vectors along the direction of w have smaller hinge loss of those along the direction of w\u2217. Observe that the noise in the are A will tend to \u201ceven out\u201d the difference in hinge loss between w and w\u2217 (since are A is symmetric with respect to these two directions). The noise in area B however will \u201chelp w\u201d: Since all points in area B are closer to the hyperplane defined by w than to the one defined by w\u2217, vector w\u2217 will pay more in hinge loss for the noise in this area. In the corresponding area D of points that are closer to the hyperplane defined by w\u2217 than to the one defined by w we do not add noise, so the cost for both w and w\u2217 in this area is small.\nWe show that for every \u03b1, from a certain noise level \u03b7 on, w\u2217(or any other vector in its direction) is not the expected hinge minimizer on D\u0303\u03b1,\u03b7. We then argue that thereby hinge loss minimization will not approximate w\u2217 arbitrarily close in angle and can therefore not achieve arbitrarily small excess 0/1-error. Overall, we show that for every (arbitrarily small) bound on the noise \u03b70 and hinge parameter \u03c40, we can choose an angle \u03b1 such that \u03c40-hinge loss minimization is not consistent for distribution D\u0303\u03b1,\u03b70 . The details of the proof can be found in the Appendix, Section D."}, {"heading": "6 Conclusions", "text": "Our work is the first to provide a computationally efficient algorithm under the Massart noise model, a distributional assumption that has been identified in statistical learning to yield fast (statistical) rates of convergence. While both computational and statistical efficiency is crucial in machine learning applications, computational and statistical complexity have been studied under disparate sets of assumptions and models. We view our results on the computational complexity of learning under Massart noise also as a step towards bringing these two lines of research closer together. We hope that this will spur more work identifying situations that lead to both computational and statistical efficiency to ultimately shed light on the underlying connections and dependencies of these two important aspects of automated learning.\nAcknowledgments This work was supported in part by NSF grants CCF-0953192, CCF-1451177, CCF1422910, a Sloan Research Fellowshp, a Microsoft Research Faculty Fellowship, and a Google Research Award."}, {"heading": "A Probability Lemmas For The Uniform Distribution", "text": "The following probability lemmas are used throughout this work. Variation of these lemmas are presented in previous work in terms of their asymptotic behavior [2, 4, 22]. Here, we focus on finding bounds that are tight even when the constants are concerned. Indeed, the improved constants in these bounds are essential to tolerating Massart noise with \u03b2 > 1\u2212 3.6\u00d7 10\u22126.\nThroughout this section, let D be the uniform distribution over a d-dimensional ball. Let f(\u00b7) indicate the p.d.f. of D. For any d, let Vd be the volume of a d-dimensional unit ball. Ratios between volumes of the unit ball in different dimensions are commonly used to find the probability mass of different regions under the uniform distribution. Note that for any d\nVd\u22122 Vd = d 2\u03c0 .\nThe following bound due to [8] proves useful in our analysis.\u221a d\n2\u03c0 \u2264 Vd\u22121 Vd \u2264 \u221a d+ 1 2\u03c0\nThe next lemma provides an upper and lower bound for the probability mass of a band in uniform distribution.\nLemma 4. Let u be any unit vector in <d. For all a, b \u2208 [\u2212 C\u221a d , C\u221a d ], such that C < d/2, we have\n|b\u2212 a|2\u2212C Vd\u22121 Vd \u2264 Pr x\u223cD [u \u00b7 x \u2208 [a, b]] \u2264 |b\u2212 a|Vd\u22121 Vd .\nProof. We have\nPr x\u223cD [u \u00b7 x \u2208 [a, b]] = Vd\u22121 Vd \u222b b a (1\u2212 z2)(d\u22121)/2 dz.\nFor the upper bound, we note that the integrant is at most 1, so Prx\u223cD[u \u00b7 x \u2208 [a, b]] \u2264 Vd\u22121Vd |b \u2212 a| . For the lower bound, note that since a, b \u2208 [\u2212 C\u221a\nd , C\u221a d ], the integrant is at least (1 \u2212 Cd ) (d\u22121)/2. We know that\nfor any x \u2208 [0, 0.5], 1 \u2212 x > 4\u2212x. So, assuming that d > 2C, (1 \u2212 Cd ) (d\u22121)/2 \u2265 4\u2212 C d (d\u22121)/2 \u2265 2\u2212C Prx\u223cD[u \u00b7 x \u2208 [a, b]] \u2265 |b\u2212 a|2\u2212C Vd\u22121Vd .\nLemma 5. Let u and v be two unit vectors in <d and let \u03b1 = \u03b8(u, v). Then,\nPr x\u223cD [sign(u \u00b7 x) 6= sign(w \u00b7 x) and |u \u00b7 x| > c \u03b1\u221a d ] \u2264 \u03b1 \u03c0 e\u2212 c2(d\u22122) 2d\nProof. Without the loss of generality, we can assume u = (1, 0, . . . , 0) and w = (cos(\u03b1), sin(\u03b1), 0, . . . , 0). Consider the projection of D on the first 2 coordinates. Let E be the event we are interested in. We first show that for any x = (x1, x2) \u2208 E, \u2016x\u20162 > c/ \u221a d. Consider x1 \u2265 0 (the other case is symmetric). If x \u2208 E, it must be that \u2016x\u20162 sin(\u03b1) \u2265 c\u03b1\u221ad . So, \u2016x\u20162 = c \u03b1 sin(\u03b1) \u221a d \u2265 c\u221a d .\nNext, we consider a circle of radius c\u221a d < r < 1 around the center, indicated by S(r). Let A(r) = S(r) \u2229 E be the arc of such circle that is in E. Then the length of such arc is the arc-length that falls in the disagreement region, i.e., r\u03b1, minus the arc-length that falls in the band of width c\u03b1\u221a\nd . Note, that for every\nx \u2208 A(r), \u2016x\u20162 = r, so f(x) = Vd\u22122Vd (1\u2212 \u2016x\u2016 2)(d\u22122)/2 = Vd\u22122 Vd (1\u2212 r2)(d\u22122)/2.\nPr x\u223cD [sign(u \u00b7 x) 6=sign(w \u00b7 x) and |u \u00b7 x| > \u03b1\u221a d ] = 2 \u222b 1 c\u221a d (r\u03b1\u2212 c\u03b1\u221a d )f(r) dr\n= 2 \u222b \u221ad/c 1 ( rc\u221a d \u03b1\u2212 c\u03b1\u221a d )f( cr\u221a d ) c\u221a d dr (change of variable z = r \u221a d/c )\n= 2 Vd\u22122 Vd\nc2\u03b1\nd \u222b \u221ad/c 1 (r \u2212 1)(1\u2212 c 2r2 d )(d\u22122)/2 dr\n= c2\u03b1\n\u03c0 \u222b \u221ad/c 1 (r \u2212 1)e\u2212 r2(d\u22122) 2d dr\n\u2264 c 2\u03b1\n\u03c0 \u222b \u221ad 1 (r \u2212 1) (d\u22122)c2r\nd\n(\u22121)(\u2212(d\u2212 2)c 2r\nd )e\u2212\n(d\u22122)c2r2 2d dr\n\u2264 \u03b1 \u03c0 \u222b \u221ad/c 1 (\u22121)(\u2212(d\u2212 2)c 2r d )e\u2212 (d\u22122)c2r2 2d dr\n\u2264 \u03b1 \u03c0\n[ \u2212 e\u2212 (d\u22122)r2 2d ]r=\u221ad/c r=1\n\u2264 \u03b1 \u03c0 (e\u2212 c2(d\u22122) 2d \u2212 e\u2212(d\u22122)/2) \u2264 \u03b1 \u03c0 e\u2212 c2(d\u22122) 2d"}, {"heading": "B Proofs of Margin-based Lemmas", "text": "Proof of Lemma 1 Let L(w\u2217) = E(x,y)\u223cDk`(w \u2217, x, y), \u03c4 = \u03c4k, and b = bk\u22121. First note that for our choice of b \u2264 2.3463\u00d7 0.0121608 1\u221a d , using Lemma 4 we have that\nPr x\u223cD\n[|wk\u22121 \u00b7 x| < b] \u2265 2 b\u00d7 2\u22120.285329.\nNote that L(w\u2217) is maximized when w\u2217 = wk\u22121. Then\nL(w\u2217) \u2264 2 \u222b \u03c4 0 (1\u2212 a \u03c4 )f(a) da Prx\u223cD[|wk\u22121 \u00b7 x| < b] \u2264 \u222b \u03c4 0 (1\u2212 a \u03c4 )(1\u2212 a 2)\u2212(d\u22121)/2 da b 2\u22120.285329 .\nFor the numerator:\u222b \u03c4 0 (1\u2212 a \u03c4 )(1\u2212 a2)\u2212(d\u22121)/2 da \u2264 \u222b \u03c4 0 (1\u2212 a \u03c4 )e\u2212a 2(d\u22121)/2 da\n\u2264 1 2 \u222b \u03c4 \u2212\u03c4 e\u2212a 2(d\u22121)/2 da\u2212 1 \u03c4 \u222b \u03c4 0 ae\u2212a 2(d\u22121)/2 da\n\u2264 \u221a\n\u03c0\n2(d\u2212 1) erf\n( \u03c4 \u221a d\u2212 1\n2\n) \u2212 1\n(d\u2212 1)\u03c4 (1\u2212 e\u2212(d\u22121)\u03c42/2)\n\u2264 \u221a\n\u03c0\n2(d\u2212 1)\n\u221a 1\u2212 e\u2212\u03c42(d\u22121) \u2212 1\n(d\u2212 1)\u03c4\n( (d\u2212 1)\u03c42\n2 \u2212 1 2 ( (d\u2212 1)\u03c42 2 )2 )\n(By Taylor expansion)\n\u2264 \u03c4 \u221a \u03c0\n2 \u2212 \u03c4 2 + 1 8 (d\u2212 1)\u03c43\n\u2264 \u03c4(0.5462 + 1 8 (d\u2212 1)\u03c42) \u2264 0.5463\u03c4 (By 1 8 (d\u2212 1)\u03c42 < 2\u00d7 10\u22124)\nWhere the last inequality follows from the fact that for our choice of parameters \u03c4 \u2264 \u221a 2.50306(3.6\u00d710\u22126)1/4b\u221a\nd <\n0.003\u221a d , so 18(d\u2212 1)\u03c4 2 < 10\u22125. Therefore,\nL(w\u2217) \u2264 0.5463\u00d7 20.285329 \u03c4 b \u2264 0.665769\u03c4 b .\nProof of Lemma 3 Note that the convex loss minimization procedure returns a vector vk that is not necessarily normalized. To consider all vectors in B(wk\u22121, \u03b1k), at step k, the optimization is done over all\nvectors v (of any length) such that \u2016wk\u22121 \u2212 v\u2016 < \u03b1k. For all k, \u03b1k < 0.038709\u03c0 (or 0.0121608), so \u2016vk\u20162 \u2265 1\u2212 0.0121608, and as a result `(wk,W ) \u2264 1.13844 `(vk,W ). We have,\nerrDk(wk) \u2264 E(x,y)\u223cDk`(wk, x, y) \u2264 E(x,y)\u223cD\u0303k`(wk, x, y) + ( 1.092 \u221a 2 \u221a 1\u2212 \u03b2 bk\u22121 \u03c4k ) (By Lemma 2)\n\u2264 `(wk,W ) + 1.092 \u221a 2 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k + 10\u22128 (By Equation 3)\n\u2264 1.13844 `(vk,W ) + 1.092 \u221a 2 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k + 10\u22128 (By \u2016vk\u20162 \u2265 1\u2212 0.0121608)\n\u2264 1.13844 `(w\u2217,W ) + 1.092 \u221a 2 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k + 2.14\u00d7 10\u22128 (By vk minimizing the hinge-loss)\n\u2264 1.13844 E(x,y)\u223cD\u0303k`(w \u2217, x, y) + 1.092\n\u221a 2 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k + 3.28\u00d7 10\u22128 (By Equation 3)\n\u2264 1.13844 E(x,y)\u223cDk`(w \u2217, x, y) + 2.13844\n( 1.092 \u221a 2 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k\n) + 3.28\u00d7 10\u22126 (By Lemma 2)\n\u2264 0.757941 \u03c4k bk\u22121\n+ 3.303 \u221a\n1\u2212 \u03b2 bk\u22121 \u03c4k + 3.28\u00d7 10\u22128 (By Lemma 1)\nLemma 6. For any constant c\u2032, there is mk \u2208 O(d(d + log(k/d))) such that for a randomly drawn set W of mk labeled samples from D\u0303k, with probability 1\u2212 \u03b4k+k2 , for any w \u2208 B(wk\u22121, \u03b1k),\n|E(x,y)\u223cD\u0303k (`(w, x, y)\u2212 `(w,W )) | \u2264 c \u2032,\n|E(x,y)\u223cDk (`(w, x, y)\u2212 `(w, cleaned(W ))) | \u2264 c \u2032.\nProof. By Lemma H.3 of [2], `(w, x, y) = O( \u221a d) for all (x, y) \u2208 Swk\u22121,bk\u22121 and \u03b8(w,wk\u22121) \u2264 rk. We get the result by applying Lemma H.2 of [2].\nC Initialization\nWe initialize our margin based procedure with the algorithm from [27]. The guarantees mentioned in [27] hold as long as the noise rate is \u03b7 \u2264 c 2log 1/ . [27] do not explicitly compute the constant but it is easy to check that c \u2264 1256 . This can be computed from inequality 17 in the proof of Lemma 16 in [27]. We need the l.h.s. to be at least 2/2. On the r.h.s., the first term is lower bounded by 2/512. Hence, we need the second term to be at most 255512 2. The second term is upper bounded by 4c2 2. This implies that c \u2264 1/256."}, {"heading": "D Hinge Loss Minimization", "text": "In this section, we show that hinge loss minimization is not consistent in our setup, that is, that it does not lead to arbitrarily small excess error. We let Bd1 denote the unit ball in R\nd. In this section, we will only work with d = 2, thus we set B1 = B21 .\nRecall that the \u03c4 -hinge loss of a vector w \u2208 <d on an example (x, y) \u2208 <d \u00d7 {\u22121, 1} is defined as follows:\n`\u03c4 (w, x, y) = max\n{ 0, 1\u2212 y(w \u00b7 x)\n\u03c4 } For a distribution D\u0303 over <d \u00d7 {\u22121, 1}, we let LD\u0303\u03c4 denote the expected hinge loss over D, that is\nLD\u0303\u03c4 (w) = E(x,y)\u223cD\u0303`\u03c4 (w, x, y).\nIf clear from context, we omit the superscript and write L\u03c4 (w) for LD\u0303\u03c4 (w). Let A\u03c4 be the algorithm that minimizes the empirical \u03c4 -hinge loss over a sample. That is, for W = {(x1, y1), . . . , (xm, ym)}, we have\nA\u03c4 (W ) \u2208 argminw\u2208B1 1 |W | \u2211\n(x,y)\u2208W\n`\u03c4 (w, x, y).\nHinge loss minimization over halfspaces converges to the optimal hinge loss over all halfspace (it is \u201chinge loss consistent\u201d). That is, for all > 0 there is a sample size m( ) such that for all distributions D\u0303, we have\nEW\u223cD\u0303m [L D\u0303 \u03c4 (A\u03c4 (W ))] \u2264 min w\u2208B1 LD\u0303\u03c4 (w) + .\nIn this section, we show that this does not translate into an agnostic learning guarantee for halfspaces with respect to the 0/1-loss. Moreover, hinge loss minimization is not even consistent with respect to the 0/1-loss even when restricted to a rather benign classes of distributions P . Let P\u03b2 be the class of distributions D\u0303 with uniform marginal over the unit ball in <2, the Bayes classifier being a halfspace w, and satisfying the Massart noise condition with parameter \u03b2. We show that there is a distribution D\u0303 \u2208 P\u03b2 and an \u2265 0 and a sample size m0 such that hinge loss minimization will output a classifier of excess error larger than on expectation over samples of size larger than m0. More precisely, for all m \u2265 m0:\nEW\u223cD\u0303m [L D\u0303 \u03c4 (A\u03c4 (W ))] > min\nw\u2208B1 errD\u0303(w) + .\nFormally, our lower bound for hinge loss minimization is stated as follows.\nTheorem 3 (Restated). For every hinge-loss parameter \u03c4 \u2265 0 and every Massart noise parameter 0 \u2264 \u03b2 < 1, there exists a distribution D\u0303\u03c4,\u03b2 \u2208 P\u03b2 (that is, a distribution overB1\u00d7{\u22121, 1} with uniform marginal over B1 \u2286 <2 satisfying the \u03b2-Massart condition) such that \u03c4 -hinge loss minimization is not consistent on P\u03c4,\u03b2 with respect to the class of halfspaces. That is, there exists an \u2265 0 and a sample size m( ) such that hinge loss minimization will output a classifier of excess error larger than (with high probability over samples of size at least m( )).\nIn the section, we use the notation hw for the classifier associated with a vector w \u2208 B1, that is hw(x) = sign(w \u00b7 x), since for our geometric construction it is convenient to differentiate between the two. The rest of this section is devoted to proving the above theorem.\nA class of distributions\nLet \u03b7 = 1\u2212\u03b22 . We define a family P\u03b1,\u03b7 \u2286 P\u03b2 of distributions D\u0303\u03b1,\u03b7, indexed by an angle \u03b1 and a noise parameter \u03b7 as follows. We let the marginal be uniform over the unit ball B1 \u2286 <2 and let the Bayes optimal classifier be linear h\u2217 = hw\u2217 for a unit vector w\u2217. Let hw be the classifier that is defined by the unit vector w at angle \u03b1 from w\u2217. We partition the unit ball into areas A, B and D as in the Figure 2. That is A consists of the two wedges of disagreement between hw and hw\u2217 and the wedge where the two classifiers agree is divided in B (points that are closer to hw than to hw\u2217) and D (points that are closer to hw\u2217 than to hw). We now \u201cadd noise \u03b7\u201d at all points in areas A and B and leave the labels deterministic according to hw\u2217 in the area D.\nMore formally, points at angle between \u03b1/2 and \u03c0/2 and points at angle between \u03c0 + \u03b1/2 and \u2212\u03c0/2 from w\u2217 are labeled with hw\u2217(x) with (conditional) probability 1. All other points are labeled \u2212hw\u2217(x) with probability \u03b7 and hw\u2217(x) with probability (1\u2212 \u03b7).\nUseful lemmas\nThe following lemma relates the \u03c4 -hinge loss of unit length vectors to the hinge loss of arbitrary vectors in the unit ball. It will allow us to focus our attention to comparing the \u03c4 -hinge loss of unit vectors for \u03c4 > \u03c40, instead of having to argue about the \u03c40 hinge loss of vectors of arbitrary norms in B1.\nLemma 7. Let \u03c4 > 0 and 0 < \u03bb \u2264 1. Letw andw\u2217 be two vectors of unit length. ThenL\u03c4 (\u03bbw) < L\u03c4 (\u03bbw\u2217) if and only if L\u03c4/\u03bb(w) < L\u03c4/\u03bb(w\u2217).\nProof. By the definition of the hinge loss, we have\n`\u03c4 (\u03bbw, x, y) = max\n( 0, 1\u2212 y(\u03bbw \u00b7 x)\n\u03c4\n) = max ( 0, 1\u2212 y(w \u00b7 x)\n\u03c4/\u03bb\n) = `\u03c4/\u03bb(w, x, y).\nLemma 8. Let \u03c4 > 0, for any D\u0303 \u2208 P\u03b1,\u03b7 let w\u03c4 denote the halfspace that minimizes the \u03c4 -hinge loss with respect to D\u0303. If \u03b8(w\u2217, w\u03c4 ) > 0, then hinge loss minimization is not consistent for the 0/1-loss.\nProof. First we show that the hinge loss minimizer is never the vector 0. Note that LD\u0303\u03c4 (0) = 1 (for all \u03c4 > 0). Consider the case \u03c4 \u2265 1, we show that w\u2217 has \u03c4 -hinge loss strictly smaller than 1. Integrating the hinge loss over the unit ball using polar coordinates, we get\nLD\u0303\u03c4 (w\u2217) < 2\n\u03c0\n( (1\u2212 \u03b7) \u222b 1 0 \u222b \u03c0 0 (1\u2212 z \u03c4 sin(\u03d5)) z d\u03d5 dz + \u03b7 \u222b 1 0 \u222b \u03c0 0 (1 + z \u03c4 sin(\u03d5)) z d\u03d5 dz ) = 2\n\u03c0\n( (1\u2212 \u03b7) \u222b 1 0 \u222b \u03c0 0 z \u2212 z 2 \u03c4 sin(\u03d5) d\u03d5 dz + \u03b7 \u222b 1 0 \u222b \u03c0 0 z + z2 \u03c4 sin(\u03d5) d\u03d5 dz ) = 1 + 2\n\u03c0\n( (1\u2212 2\u03b7) \u222b 1 0 \u222b \u03c0 0 \u2212z 2 \u03c4 sin(\u03d5) d\u03d5 dz ) = 1\u2212 2\n\u03c0\n( (1\u2212 2\u03b7) \u222b 1 0 \u222b \u03c0 0 z2 \u03c4 sin(\u03d5) d\u03d5 dz ) < 1.\nFor the case of \u03c4 < 1, we have L\u03c4 (\u03c4w\u2217) = L1(w\u2217) < 1.\nThus, (0, 0) is not the hinge-minimizer. Then, by the assumption of the lemma w\u03c4 has some positive angle \u03b3 to the w\u2217. Furthermore, for all 0 \u2264 \u03bb \u2264 1, LD\u0303\u03c4 (w\u03c4 ) < LD\u0303\u03c4 (\u03bbw\u2217). Since w 7\u2192 LD\u0303\u03c4 (w) is a continuous function we can choose an > 0 such that\nLD\u0303\u03c4 (w\u03c4 ) + /2 < LD\u0303\u03c4 (\u03bbw\u2217)\u2212 /2.\nfor all 0 \u2264 \u03bb \u2264 1 (note that the set {\u03bbw\u2217 | 0 \u2264 \u03bb \u2264 1} is compact). Now, we can choose an angle \u00b5 < \u03b3 such that for all vectors v at angle at most \u00b5 from w\u2217, we have\nLD\u0303\u03c4 (v) \u2265 min 0\u2264\u03bb\u22641 LD\u0303\u03c4 (\u03bbw\u2217)\u2212 /2\nSince hinge loss minimization will eventually (in expectation over large enough samples) output classifiers of hinge loss strictly smaller than LD\u0303\u03c4 (w\u03c4 ) + /2, it will then not output classifiers of angle smaller than \u00b5 to w\u2217. By Equation 2, for all w, errD\u0303(w)\u2212 errD\u0303(w \u2217) > \u03b2 \u03b8(w,w \u2217)\n\u03c0 , therefore, the excess error of a the classfier returned by hinge loss minimization is lower bounded by a constant \u03b2 \u00b5\u03c0 . Thus, hinge loss minimization is not consistent with respect to the 0/1-loss.\nProof of Theorem 3\nWe will show that, for every bound on the noise \u03b70 and for every every \u03c40 \u2265 0 there is an \u03b10 > 0, such that the unit length vector w has strictly lower \u03c4 -hinge loss than the unit length vector w\u2217 for all \u03c4 \u2265 \u03c40. By Lemma 7, this implies that for every bound on the noise \u03b70 and for every \u03c40 there is an \u03b10 > 0 such that for all 0 < \u03bb \u2264 1 we have L\u03c40(\u03bbw) < L\u03c40(\u03bbw\u2217). This implies that the hinge minimizer is not a multiple of w\u2217 and so is at a positive angle to w\u2217. Now Lemma 8 tells us that hinge loss minimization is not consistent for the 0/1-loss.\nw*\nw\n\u237a\n\u237a\nA\nA\n\u237a/2\nhw hw*\nB\nBD\nD\nIn the sequel, we will now focus on the unit length vectors w and w\u2217 and show how to choose \u03b10 as a function of \u03c40 and \u03b70. We let cA denote the hinge loss of hw\u2217 on one wedge (one half of) area A when the labels are correct and dA that hinge loss on that same area when the labels are not correct. Analogously, we define cB,dB, cD and dD. For example, for \u03c4 \u2265 1, we have (integrating the hinge loss over the unit ball using polar coordinates)\nNow we can express the hinge loss of both hw\u2217 and hw in terms of these quantities. For hw\u2217 we have"}, {"heading": "L\u03c4 (hw\u2217) = 2 \u00b7 (\u03b7(dA + dB) + (1\u2212 \u03b7)(cA + cB) + cD) .", "text": "For hw, note that area B relates to hw as area D relates to hw\u2217 (and vice versa). Thus, the roles of B and D are exchanged for hw. That is, for example, for the noisy version of area B the classifier hw pays dD. We have\nL\u03c4 (hw) = 2 \u00b7 (\u03b7(cA + dD) + (1\u2212 \u03b7)(dA + cD) + cB) .\nThis yields\nL\u03c4 (hw)\u2212 L\u03c4 (hw\u2217) = 2 \u00b7 ((1\u2212 2\u03b7)(dA\u2212 cA)\u2212 \u03b7((dB\u2212 cB)\u2212 (dD\u2212 cD))) .\nWe now define area C as the points at angle between \u03c0\u2212\u03b1/2 and \u03c0+\u03b1/2 from w\u2217 (See Figure 3). We let cC and dC be defined analogously to the above.\nNote that dA + dB\u2212 dD = dC and cA + cB\u2212 cD = cC. Thus we get\nL\u03c4 (hw)\u2212 L\u03c4 (hw\u2217) =2 \u00b7 ((1\u2212 2\u03b7)(dA\u2212 cA)\u2212 \u03b7((dB\u2212 cB)\u2212 (dD\u2212 cD))) =2 \u00b7 ((1\u2212 \u03b7)(dA\u2212 cA)\u2212 \u03b7((dB\u2212 cB) + (dA\u2212 cA)\u2212 (dD\u2212 cD))) =2 \u00b7 ((1\u2212 \u03b7)(dA\u2212 cA)\u2212 \u03b7((dC\u2212 cC))) .\nIf \u03b7 > \u03b7(\u03b1, \u03c4) := (dA\u2212cA)(dA\u2212cA)+(dC\u2212cC) , then we get L\u03c4 (hw) \u2212 L\u03c4 (hw\u2217) < 0 and thus hw having smaller hinge loss than hw\u2217 . Thus, \u03b7(\u03b1, \u03c4) signifies the amount of noise from which onward, w will have smaller hinge loss than w\u2217\nGiven \u03c40 \u2265 0, choose \u03b1 small enough (we can always choose the angle \u03b1 sufficiently small for this) so that the area A is included in the \u03c40-band around w\u2217. We have for all \u03c4 \u2265 \u03c40:\n(dA\u2212 cA) = 2 \u03c0 \u222b 1 0 \u222b \u03b1 0 z2 \u03c4 sin(\u03d5) d\u03d5 dz\n= 2\n3\u03c0 \u222b \u03b1 0 1 \u03c4 sin(\u03d5) d\u03d5\n= 2\n3\u03c0\u03c4 [\u2212 cos(\u03d5)]\u03b10\n= 2\n3\u03c0\u03c4 (1\u2212 cos(\u03b1)).\nFor the area C we now consider the case of \u03c4 \u2265 1 and \u03c4 < 1 separately. For \u03c4 \u2265 1 we get\n(dC\u2212 cC) = 4 \u03c0 \u222b 1 0 \u222b \u03c0 2\n\u03c0\u2212\u03b1 2\nz2\n\u03c4 sin(\u03d5) d\u03d5 dz\n= 4\n3\u03c0\n\u222b \u03c0 2\n\u03c0\u2212\u03b1 2\n1 \u03c4 sin(\u03d5) d\u03d5\n= 4\n3\u03c0\u03c4 cos\n( \u03c0 \u2212 \u03b1\n2 ) = 4 3\u03c0\u03c4 sin (\u03b1 2 ) .\nThus, for \u03c4 \u2265 1 we get\n\u03b7(\u03b1, \u03c4) = (dA\u2212 cA)\n(dA\u2212 cA) + (dC + cC) = 1\u2212 cos(\u03b1) 1\u2212 cos(\u03b1) + 2 sin(\u03b1/2) .\nWe call this quantity \u03b71(\u03b1) since, given that \u03c4 \u2265 1, it does not depend on \u03c4 :\n\u03b71(\u03b1) = (dA\u2212 cA)\n(dA\u2212 cA) + (dC + cC) = 1\u2212 cos(\u03b1) 1\u2212 cos(\u03b1) + 2 sin(\u03b1/2) .\nObserve that lim\u03b1\u21920 \u03b71(\u03b1) = 0. This will yield the first condition on the angle \u03b1: Given some bound on the allowed noise \u03b70, we can choose an \u03b1 small enough so that \u03b71(\u03b1) \u2264 \u03b70/2. Then, for the distribution D\u0303\u03b1,\u03b70 we have L\u03c4 (w) < L\u03c4 (w\u2217) for all \u03c4 \u2265 1.\nWe now consider the case \u03c4 < 1. For this case we lower bound (dC\u2212 cC) as follows. We have\ndC = 2\n\u03c0 \u222b 1 0 \u222b \u03c0 2\n\u03c0\u2212\u03b1 2\nz + z2\n\u03c4 sin(\u03d5) d\u03d5 dz\n= \u03b1\n2\u03c0 +\n2\n\u03c0 \u222b 1 0 \u222b \u03c0 2\n\u03c0\u2212\u03b1 2\nz2\n\u03c4 sin(\u03d5) d\u03d5 dz\n= \u03b1\n2\u03c0 +\n2 3\u03c4\u03c0 sin (\u03b1 2 ) .\nhw*\n\u237a/2C\n\ud835\udf0f T\nFigure 5: Area T\nWe now provide an upper bound on cC by integrating over a the triangular shape T (see Figure 4). Note that this bound on cC is actually exact if \u03c4 \u2264 cos(\u03b1/2) and only a strict upper bound for cos(\u03b1/2) < \u03c4 < 1. We have\ncC \u2264 (cT ) = 2 \u03c0 \u00b7 \u222b \u03c4 0 (1\u2212 z \u03c4 )(z tan(\u03b1/2)) dz\n= 2 \u03c0 \u00b7 \u222b \u03c4 0 z tan(\u03b1/2)\u2212 z 2 \u03c4 tan(\u03b1/2) dz = \u03c42\n3\u03c0 tan (\u03b1 2 ) .\nThus we get\n(dC\u2212 cC) \u2265 (dC\u2212 (cT )) = 1 \u03c0\n( \u03b1\n2 +\n2 3\u03c4 sin (\u03b1 2 ) \u2212 \u03c4 2 3 tan (\u03b1 2 )) .\nThis yields, for the case \u03c4 \u2264 1\n\u03b7(\u03b1, \u03c4) = 2 3(1\u2212 cos(\u03b1))\n2 3(1\u2212 cos(\u03b1)) + 2 3 sin(\u03b1) + \u03b1\u03c4 2 \u2212\n\u03c43\n3 tan( \u03b1 2 )\nWe call this quantity \u03b72(\u03b1, \u03c4) to differentiate it from \u03b71(\u03b1). Again, it is easy to show that we have lim\u03b1\u21920 \u03b72(\u03b1, \u03c4) = 0 for every \u03c4 . Thus, for a fixed \u03c40, we can choose an angle \u03b1 small enough so that L\u03c40(w) \u2264 L\u03c40(w\u2217).\nTo argue that we will then also have L\u03c4 (w) \u2264 L\u03c4 (w\u2217) for all \u03c4 \u2265 \u03c40, we show that, for a fixed angle \u03b1, the function \u03b7(\u03b1, \u03c4) gets smaller as \u03c4 grows. For this, it suffices to show that g(\u03c4) = \u03c4 \u03b12 \u2212 \u03c43 3 tan( \u03b1 2 ) is monotonically increasing with \u03c4 for \u03c4 \u2264 1. We have\ng\u2032(\u03c4) = \u03b1 2 \u2212 \u03c4\n2\n2 tan (\u03b1 2 ) .\nSince we have \u03c42 \u2264 1 and 2\u03b1 tan(\u03b12 ) \u2265 1 for 0 \u2264 \u03b1 \u2264 \u03c0/3, we get that (for sufficiently small \u03b1) g\u2032(\u03c4) \u2265 0 and thus g(\u03c4) is monotonically increasing for 0 \u2264 \u03c4 \u2264 1 as desired.\nSummarizing, for a given \u03c40 and \u03b70, we can always choose \u03b10 sufficiently small so that both \u03b71(\u03b10) < \u03b702 and \u03b72(\u03b10, \u03c4) < \u03b702 for all \u03c4 \u2265 \u03c40 and thus L D\u0303\u03b10,\u03b70 \u03c4 (w) < L D\u0303\u03b10,\u03b70 \u03c4 (w\u2217) for all \u03c4 \u2265 \u03c40. This completes the proof."}], "references": [{"title": "The hardness of approximate optima in lattices, codes, and systems of linear equations", "author": ["Sanjeev Arora", "L\u00e1szl\u00f3 Babai", "Jacques Stern", "Z. Sweedyk"], "venue": "In Proceedings of the 34th IEEE Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "The power of localization for efficiently learning linear separators with noise", "author": ["Pranjal Awasthi", "Maria Florina Balcan", "Philip M. Long"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Agnostic active learning", "author": ["Maria-Florina Balcan", "Alina Beygelzimer", "John Langford"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Margin based active learning", "author": ["Maria-Florina Balcan", "Andrei Z. Broder", "Tong Zhang"], "venue": "In Proceedings of the 20th Annual Conference on Learning Theory (COLT),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Statistical active learning algorithms", "author": ["Maria-Florina Balcan", "Vitaly Feldman"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Minimizing the misclassification error rate using a surrogate convex loss", "author": ["Shai Ben-David", "David Loker", "Nathan Srebro", "Karthik Sridharan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "A polynomial-time algorithm for learning noisy linear threshold functions", "author": ["Avrim Blum", "Alan Frieze", "Ravi Kannan", "Santosh Vempala"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "The simplex method, volume 1 of Algorithms and Combinatorics: Study and Research Texts", "author": ["Karl-Heinz Borgwardt"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "Theory of classification: a survey of recent advances", "author": ["Olivier Bousquet", "St\u00e9phane Boucheron", "Gabor Lugosi"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Minimax bounds for active learning", "author": ["Rui M. Castro", "Robert D. Nowak"], "venue": "In Proceedings of the 20th Annual Conference on Learning Theory, (COLT),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "author": ["Nello Cristianini", "John Shawe-Taylor"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "From average case complexity to improper learning complexity", "author": ["Amit Daniely", "Nati Linial", "Shai Shalev-Shwartz"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Coarse sample complexity bounds for active learning", "author": ["Sanjoy Dasgupta"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Active learning", "author": ["Sanjoy Dasgupta"], "venue": "Encyclopedia of Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A general agnostic active learning algorithm", "author": ["Sanjoy Dasgupta", "Daniel Hsu", "Claire Monteleoni"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Selective sampling and active learning from single and multiple teachers", "author": ["Ofer Dekel", "Claudio Gentile", "Karthik Sridharan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Yoav Freund", "H. Sebastian Seung", "Eli Shamir", "Naftali Tishby"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Hardness of learning halfspaces with noise", "author": ["Venkatesan Guruswami", "Prasad Raghavendra"], "venue": "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["Steve Hanneke"], "venue": "In Proceedings of the 24rd International Conference on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Theory of disagreement-based active learning", "author": ["Steve Hanneke"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Surrogate losses in passive and active learning", "author": ["Steve Hanneke", "Liu Yang"], "venue": "CoRR, abs/1207.3772,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Agnostically learning halfspaces", "author": ["Adam Tauman Kalai", "Adam R. Klivans", "Yishay Mansour", "Rocco A. Servedio"], "venue": "SIAM Journal on Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "On agnostic boosting and parity learning", "author": ["Adam Tauman Kalai", "Yishay Mansour", "Elad Verbin"], "venue": "In Proceedings of the 40th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Learning in the presence of malicious errors (extended abstract)", "author": ["Michael J. Kearns", "Ming Li"], "venue": "In Proceedings of the 20th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1988}, {"title": "Toward efficient agnostic learning", "author": ["Michael J. Kearns", "Robert E. Schapire", "Linda Sellie"], "venue": "In Proceedings of the 5th Annual Conference on Computational Learning Theory (COLT),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1992}, {"title": "Embedding hard learning problems into gaussian space. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques", "author": ["Adam R. Klivans", "Pravesh Kothari"], "venue": "(AP- PROX/RANDOM),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Learning halfspaces with malicious noise", "author": ["Adam R. Klivans", "Philip M. Long", "Rocco A. Servedio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Risk bounds for statistical learning", "author": ["Pascal Massart", "lodie Ndlec"], "venue": "The Annals of Statistics, 34(5):2326\u20132366,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "A formal model of hierarchical concept learning", "author": ["Ronald L. Rivest", "Robert H. Sloan"], "venue": "Information and Computation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1994}, {"title": "Efficient algorithms in computational learning theory", "author": ["Rocco A. Servedio"], "venue": "Harvard University,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}], "referenceMentions": [{"referenceID": 10, "context": "In the absence of noise (when the data is realizable) such algorithms exist via linear programming [11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 6, "context": "Such strong guarantees are only known for the well studied random classification noise model [7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 27, "context": "In this work, we provide the first algorithm that can achieve arbitrarily small excess error, in truly polynomial time, for bounded noise, also called Massart noise [28], a much more realistic and widely studied noise model in statistical learning theory [9].", "startOffset": 165, "endOffset": 169}, {"referenceID": 8, "context": "In this work, we provide the first algorithm that can achieve arbitrarily small excess error, in truly polynomial time, for bounded noise, also called Massart noise [28], a much more realistic and widely studied noise model in statistical learning theory [9].", "startOffset": 255, "endOffset": 258}, {"referenceID": 6, "context": "example x is flipped independently with equal probability \u03b7, several works have provided computationally efficient algorithms that can achieve arbitrarily small excess error in polynomial time [7, 30, 5] \u2014 note that all these results crucially exploit the high amount of symmetry present in the RCN noise.", "startOffset": 193, "endOffset": 203}, {"referenceID": 29, "context": "example x is flipped independently with equal probability \u03b7, several works have provided computationally efficient algorithms that can achieve arbitrarily small excess error in polynomial time [7, 30, 5] \u2014 note that all these results crucially exploit the high amount of symmetry present in the RCN noise.", "startOffset": 193, "endOffset": 203}, {"referenceID": 4, "context": "example x is flipped independently with equal probability \u03b7, several works have provided computationally efficient algorithms that can achieve arbitrarily small excess error in polynomial time [7, 30, 5] \u2014 note that all these results crucially exploit the high amount of symmetry present in the RCN noise.", "startOffset": 193, "endOffset": 203}, {"referenceID": 24, "context": "At the other extreme, there has been significant work on much more difficult and adversarial noise models, including the agnostic model [25] and malicious noise models [24].", "startOffset": 136, "endOffset": 140}, {"referenceID": 23, "context": "At the other extreme, there has been significant work on much more difficult and adversarial noise models, including the agnostic model [25] and malicious noise models [24].", "startOffset": 168, "endOffset": 172}, {"referenceID": 22, "context": "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd\u22121 achieves excess error cOPT [2], for some large constant c.", "startOffset": 204, "endOffset": 215}, {"referenceID": 26, "context": "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd\u22121 achieves excess error cOPT [2], for some large constant c.", "startOffset": 204, "endOffset": 215}, {"referenceID": 1, "context": "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd\u22121 achieves excess error cOPT [2], for some large constant c.", "startOffset": 204, "endOffset": 215}, {"referenceID": 1, "context": "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd\u22121 achieves excess error cOPT [2], for some large constant c.", "startOffset": 349, "endOffset": 352}, {"referenceID": 11, "context": "In fact, recent evidence shows that this is unavoidable for polynomial time algorithms for such adversarial noise models [12].", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "Our Results In this work we identify a realistic and widely studied noise model in the statistical learning theory, the so called Massart noise [9], for which we can prove much stronger guarantees.", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "From a statistical point of view, it is well known that under this model, we can get faster rates compared to worst case joint distributions [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 28, "context": "In computational learning theory, this noise model was also studied, but under the name of malicious misclassification noise [29, 31].", "startOffset": 125, "endOffset": 133}, {"referenceID": 29, "context": "As a result, as we show in our work (see Section 4), standard algorithms such as the averaging algorithm [30] which work for random noise can only achieve a much poorer excess error (as a function of \u03b7) under Massart noise.", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "Specifically, we analyze a recent margin based algorithm of [2].", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "By using new structural insights, we show that there exists a constant \u03b7 (independent of the dimension), so that if we use Massart noise where the flipping probability is upper bounded by \u03b7, we can use a modification of the algorithm in [2] and achieve arbitrarily small excess error.", "startOffset": 237, "endOffset": 240}, {"referenceID": 5, "context": "While there exists earlier work showing that hinge loss minimization can lead to classifiers of large 0/1-loss [6], the lower bounds in that paper employ distributions with significant mass on discrete points with flipped label (which is not possible under Massart noise) at a very large distance from the optimal classifier.", "startOffset": 111, "endOffset": 114}, {"referenceID": 18, "context": "One appealing feature of our result is the algorithm we analyze is in fact naturally adaptable to the active learning or selective sampling scenario (intensively studied in recent years [19, 13, 20], where the learning algorithms only receive the classifications of examples when they ask for them.", "startOffset": 186, "endOffset": 198}, {"referenceID": 12, "context": "One appealing feature of our result is the algorithm we analyze is in fact naturally adaptable to the active learning or selective sampling scenario (intensively studied in recent years [19, 13, 20], where the learning algorithms only receive the classifications of examples when they ask for them.", "startOffset": 186, "endOffset": 198}, {"referenceID": 19, "context": "One appealing feature of our result is the algorithm we analyze is in fact naturally adaptable to the active learning or selective sampling scenario (intensively studied in recent years [19, 13, 20], where the learning algorithms only receive the classifications of examples when they ask for them.", "startOffset": 186, "endOffset": 198}, {"referenceID": 3, "context": "We note that prior to our work only inefficient algorithms could achieve the desired label complexity under Massart noise [4, 20].", "startOffset": 122, "endOffset": 129}, {"referenceID": 19, "context": "We note that prior to our work only inefficient algorithms could achieve the desired label complexity under Massart noise [4, 20].", "startOffset": 122, "endOffset": 129}, {"referenceID": 0, "context": "Related Work The agnostic noise model is notoriously hard to deal with computationally and there is significant evidence that achieving arbitrarily small excess error in polynomial time is hard in this model [1, 18, 12].", "startOffset": 208, "endOffset": 219}, {"referenceID": 17, "context": "Related Work The agnostic noise model is notoriously hard to deal with computationally and there is significant evidence that achieving arbitrarily small excess error in polynomial time is hard in this model [1, 18, 12].", "startOffset": 208, "endOffset": 219}, {"referenceID": 11, "context": "Related Work The agnostic noise model is notoriously hard to deal with computationally and there is significant evidence that achieving arbitrarily small excess error in polynomial time is hard in this model [1, 18, 12].", "startOffset": 208, "endOffset": 219}, {"referenceID": 22, "context": "For this model, under our distributional assumptions, [23] provides an algorithm that learns linear separators in <d to excess error at most , but whose running time poly(dexp(1/ )).", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "Recent work show evidence that the exponential dependence on 1/ is unavoidable in this case [26] for the agnostic case.", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 12, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 2, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 3, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 18, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 14, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 9, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 13, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 19, "context": "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].", "startOffset": 363, "endOffset": 397}, {"referenceID": 4, "context": "However, despite many efforts, except for very simple noise models (random classification noise [5] and linear noise [16]), to date there are no known computationally efficient algorithms with provable guarantees in the presence of Massart noise that can achieve arbitrarily small excess error.", "startOffset": 96, "endOffset": 99}, {"referenceID": 15, "context": "However, despite many efforts, except for very simple noise models (random classification noise [5] and linear noise [16]), to date there are no known computationally efficient algorithms with provable guarantees in the presence of Massart noise that can achieve arbitrarily small excess error.", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "We note that work of [21] provides computationally efficient algorithms for both passive and active learning under the assumption that the hinge loss (or other surrogate loss) minimizer aligns with the minimizer of the 0/1-loss.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": "The algorithm described above is similar to that of [2] and uses an iterative margin-based approach.", "startOffset": 52, "endOffset": 55}, {"referenceID": 26, "context": "We satisfy the base case by using an algorithm of [27].", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "Using these insights, we show that the algorithm by [2] can indeed achieve a much stronger guarantee, namely arbitrarily small excess error in presence of Massart noise.", "startOffset": 52, "endOffset": 55}, {"referenceID": 26, "context": "Take poly(d, 1\u03b4 ) samples and run poly(d, 1 \u03b4 )-time algorithm by [27] to find a half-spacew0 with excess error 0.", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "Overview of our analysis: Similar to [2], we divide errD(wk) to two categories; error in the band, i.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "1 of [2] for uniform)", "startOffset": 5, "endOffset": 8}, {"referenceID": 26, "context": "For k = 0, we use the algorithm for adversarial noise model by [27], which can achieve excess error of if errD\u0303(w \u2217) < 2 256 log(1/ ) (Refer to Appendix C for more details).", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "where the last transition holds by the fact that Vd\u22121 Vd \u2264 \u221a d+1 2\u03c0 [8].", "startOffset": 68, "endOffset": 71}, {"referenceID": 29, "context": "The Average algorithm introduced by [30] is another computationally efficient algorithm that has provable noise tolerance guarantees under certain noise models and distributions.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "Furthermore, even in the presence of a small amount of malicious noise and less symmetric distributions, Average has been used to obtain a weak learner, which can then be boosted to achieve a non-trivial noise tolerance [27].", "startOffset": 220, "endOffset": 224}, {"referenceID": 5, "context": "It has been shown earlier that hinge loss minimization can lead to classifiers of large 0/1-loss [6].", "startOffset": 97, "endOffset": 100}, {"referenceID": 0, "context": "[1] Sanjeev Arora, L\u00e1szl\u00f3 Babai, Jacques Stern, and Z.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Pranjal Awasthi, Maria Florina Balcan, and Philip M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Maria-Florina Balcan, Alina Beygelzimer, and John Langford.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Maria-Florina Balcan, Andrei Z.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Maria-Florina Balcan and Vitaly Feldman.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Shai Ben-David, David Loker, Nathan Srebro, and Karthik Sridharan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Avrim Blum, Alan Frieze, Ravi Kannan, and Santosh Vempala.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Karl-Heinz Borgwardt.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Olivier Bousquet, St\u00e9phane Boucheron, and Gabor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Rui M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Nello Cristianini and John Shawe-Taylor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Amit Daniely, Nati Linial, and Shai Shalev-Shwartz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Sanjoy Dasgupta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Sanjoy Dasgupta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Sanjoy Dasgupta, Daniel Hsu, and Claire Monteleoni.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Ofer Dekel, Claudio Gentile, and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Yoav Freund, H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Venkatesan Guruswami and Prasad Raghavendra.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Steve Hanneke.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Steve Hanneke.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Steve Hanneke and Liu Yang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Adam Tauman Kalai, Adam R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Adam Tauman Kalai, Yishay Mansour, and Elad Verbin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Michael J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Michael J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Adam R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Adam R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] Pascal Massart and lodie Ndlec.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] Ronald L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] Rocco A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Variation of these lemmas are presented in previous work in terms of their asymptotic behavior [2, 4, 22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 3, "context": "Variation of these lemmas are presented in previous work in terms of their asymptotic behavior [2, 4, 22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 21, "context": "Variation of these lemmas are presented in previous work in terms of their asymptotic behavior [2, 4, 22].", "startOffset": 95, "endOffset": 105}, {"referenceID": 7, "context": "The following bound due to [8] proves useful in our analysis.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "3 of [2], `(w, x, y) = O( \u221a d) for all (x, y) \u2208 Swk\u22121,bk\u22121 and \u03b8(w,wk\u22121) \u2264 rk.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "2 of [2].", "startOffset": 5, "endOffset": 8}, {"referenceID": 26, "context": "We initialize our margin based procedure with the algorithm from [27].", "startOffset": 65, "endOffset": 69}, {"referenceID": 26, "context": "The guarantees mentioned in [27] hold as long as the noise rate is \u03b7 \u2264 c 2 log 1/ .", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "[27] do not explicitly compute the constant but it is easy to check that c \u2264 1 256 .", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "This can be computed from inequality 17 in the proof of Lemma 16 in [27].", "startOffset": 68, "endOffset": 72}], "year": 2015, "abstractText": "We study the learnability of linear separators in < in the presence of bounded (a.k.a Massart) noise. This is a realistic generalization of the random classification noise model, where the adversary can flip each example xwith probability \u03b7(x) \u2264 \u03b7. We provide the first polynomial time algorithm that can learn linear separators to arbitrarily small excess error in this noise model under the uniform distribution over the unit ball in <, for some constant value of \u03b7. While widely studied in the statistical learning theory community in the context of getting faster convergence rates, computationally efficient algorithms in this model had remained elusive. Our work provides the first evidence that one can indeed design algorithms achieving arbitrarily small excess error in polynomial time under this realistic noise model and thus opens up a new and exciting line of research. We additionally provide lower bounds showing that popular algorithms such as hinge loss minimization and averaging cannot lead to arbitrarily small excess error under Massart noise, even under the uniform distribution. Our work instead, makes use of a margin based technique developed in the context of active learning. As a result, our algorithm is also an active learning algorithm with label complexity that is only a logarithmic the desired excess error .", "creator": "LaTeX with hyperref package"}}}