{"id": "1503.03903", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2015", "title": "Approximating Sparse PCA from Incomplete Data", "abstract": "intuitive study algorithms well one enables implement composite principal components of a data previously performed a sketch formed - error calculation of its bits. properties show that for a wide class of optimization optimization, if projective surveyor is close ( in the spectral context ) to his only kernel matrix, then one can achieve a near absolute solution to the optimization computation just using the sketch. in particular, we use finer knowledge to obtain sparse principal components and smoothing rules for \\ map { quad } data representations in \\ stat { z } dimensions, \\ precision { o ( \\ epsilon ^ { - 3 } \\ tilde log \\ max \\ { m, n \\ } ) } elements gives an \\ math { \\ partial } - additive query to the sparse pca matrix ( \\ math { \\ tilde 2 } is the default rank encoding the data matrix ). we seek integer formulation only on computation, text, biological perspective 3d concepts. elementary results show usually not mandatory are we calculate a recover completely sparse texture through the weighted data, but by using robust sparse sketches, the running current decrease through fixed factor of 21 or more.", "histories": [["v1", "Thu, 12 Mar 2015 22:16:55 GMT  (1000kb)", "http://arxiv.org/abs/1503.03903v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IT cs.NA math.IT stat.ML", "authors": ["abhisek kundu", "petros drineas", "malik magdon-ismail"], "accepted": true, "id": "1503.03903"}, "pdf": {"name": "1503.03903.pdf", "metadata": {"source": "CRF", "title": "Approximating Sparse PCA from Incomplete Data", "authors": ["Abhisek Kundu", "Petros Drineas", "Malik Magdon-Ismail"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n03 90\n3v 1\n[ cs\n.L G\n] 1\n2 M\nar 2"}, {"heading": "1 Introduction", "text": "Principal components analysis constructs a low dimensional subspace of the data such that projection of the data onto this subspace preserves as much information as possible (or equivalently maximizes the variance of the projected data). The earliest reference to principal components analysis (PCA) is in Pearson [1901]. Since then, PCA has evolved into a classic tool for data analysis. A challenge for the interpretation of the principal components (or factors) is that they can be linear combinations of all the original variables. When the original variables have direct physical significance (e.g. genes in biological applications or assets in financial applications) it is desirable to have factors which have loadings on only a small number of the original variables. These interpretable factors are sparse principal components (SPCA).\nThe question we address is not how to better perform sparse PCA; rather, it is whether one can perform sparse PCA on incomplete data and be assured some degree of success. (Read: can one do sparse PCA when you have a small sample of data points and those data points have missing features?). Incomplete data is a situation that one is confronted with all too often in machine learning. For example, with userrecommendation data, one does not have all the ratings of any given user. Or in a privacy preserving setting, a client may not want to give you all entries in the data matrix. In such a setting, our goal is to show that if the samples that you do get are chosen carefully, the sparse PCA features of the data can be recovered within some provable error bounds. A significant part of this work is to demonstrate our algorithms on a variety of data sets.\nMore formally, The data matrix is A \u2208 Rm\u00d7n (m data points in n dimensions). Data matrices often have low effective rank. Let Ak be the best rank-k approximation to A; in practice, it is often possible to choose a small value of k for which \u2016A\u2212Ak\u20162 is small. The best rank-k approximation Ak is obtained by projecting A onto the subspace spanned by its top-k principal components Vk, which is the n\u00d7 k matrix\n\u2217Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY, kundua2@rpi.edu. \u2020Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY, drinep@cs.rpi.edu. \u2021Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY, magdon@cs.rpi.edu.\ncontaining the top-k right singular vectors of A. These top-k principal components are the solution to the variance maximization problem:\nVk = argmax V\u2208Rn\u00d7k,VTV=I\ntrace(VTATAV).\nWe denote the maximum variance attainable by OPTk, which is the sum of squares of the top-k singular values of A. To get sparse principal components, you add a sparsity constraint to the optimization problem: every column of V should have at most r non-zero entries (the sparsity parameter r is an input),\nSk = argmax V\u2208Rn\u00d7k ,VTV=I,\u2016V(i)\u20160\u2264r\ntrace(VTATAV). (1)\nThe sparse PCA problem is itself a very hard problem that is not only NP-hard, but also inapproximable [Magdon-Ismail, 2015] There are many heuristics for obtaining sparse factors [Cadima and Jolliffe, 1995, Trendafilov et al., 2003, Zou et al., 2006, d\u2019Aspremont et al., 2007, 2008, Moghaddam et al., 2006, Shen and Huang, 2008] including some approximation algorithms with provable guarantees Asteris et al. [2014]. The existing research typically addresses the task of getting just the top principal component (k = 1). While the sparse PCA problem is hard and interesting, it is not the focus of this work.\nWe address the question: What if you do not know A, but only have a sparse sampling of some of the entries in A (incomplete data)? The sparse sampling is used to construct a sketch of A, denoted A\u0303. There is not much else to do but solve the sparse PCA problem with the sketch A\u0303 instead of the full data A to get S\u0303k,\nS\u0303k = argmax V\u2208Rn\u00d7k ,VTV=I,\u2016V(i)\u20160\u2264r\ntrace(VT A\u0303 T A\u0303V). (2)\nWe study how S\u0303k performs as an approximation to Sk with respective to the objective that we are trying to optimize, namely trace(STATAS) \u2014 the quality of approximation is measured with respect to the true A. We show that the quality of approximation is controlled by how well A\u0303 T A\u0303 approximates ATA as measured by the spectral norm of the deviation ATA \u2212 A\u0303T A\u0303. This is a general result that does not rely on how one constructs the sketch A\u0303.\nTheorem 1 (Sparse PCA from a Sketch) Let Sk be a solution to the sparse PCA problem that solves (1), and S\u0303k a solution to the sparse PCA problem for the sketch A\u0303 which solves (2). Then,\ntrace(S\u0303 T\nkA TAS\u0303k) \u2265 trace(STkATASk)\u2212 2k\u2016ATA\u2212 A\u0303 T A\u0303\u20162.\nTheorem 1 says that if we can closely approximate A with A\u0303, then we can compute, from A\u0303, sparse components which capture almost as much variance as the optimal sparse components computed from the full data A.\nIn our setting, the sketch A\u0303 is computed from a sparse sampling of the data elements in A (incomplete data). To determine which elements to sample, and how to form the sketch, we leverage some recent results in elementwise matrix completion (Kundu et al. [2015]). In a nutshell, if one samples larger data elements with higher probability than smaller data elements, then, for the resulting sketch A\u0303, the error \u2016ATA \u2212 A\u0303T A\u0303\u20162 will be small. The details of the sampling scheme and how the error depends on the number of samples is given in Section 2.1. Combining the bound on \u2016A \u2212 A\u0303\u20162 from Theorem 4 in Section 2.1 with Theorem 1, we get our main result:\nTheorem 2 (Sampling Complexity for Sparse PCA) Sample s data-elements from A \u2208 Rm\u00d7n to form the sparse sketch A\u0303 using Algorithm 1. Let Sk be a solution to the sparse PCA problem that solves (1),\nand let S\u0303k, which solves (2), be a solution to the sparse PCA problem for the sketch A\u0303 formed from the s sampled data elements. Suppose the number of samples s satisfies\ns \u2265 2k 2\n\u01eb2\n( \u03c12 + \u01eb\u03b3\n3k\n)\nlog\n(\nm+ n\n\u03b4\n)\n(\u03c12 and \u03b3 are dimensionless quantities that depend only on A). Then, with probability at least 1\u2212 \u03b4\ntrace(S\u0303 T kA TAS\u0303k) \u2265 trace(STkATASk)\u2212 \u01eb(2 + \u01eb/k)\u2016A\u201622.\nThe dependence of \u03c12 and \u03b3 on A are given in Section 2.1. Roughly speaking, we can ignore the term with \u03b3 since it is multiplied by \u01eb/k, and \u03c12 = O(k\u0303max{m,n}), where k\u0303 is the stable (numerical) rank of A. To paraphrase Theorem 2, when the stable rank is a small constant, with O(k2 max{m,n}) samples, one can recover almost as good sparse principal components as with all data (the price being a small fraction of the optimal variance, since OPTk \u2265 \u2016A\u201622). As far as we know, this is the first result to show that it is possible to provably recover sparse PCA from incomplete data. We also give an application of Theorem 1 to running sparse PCA after \u201cdenoising\u201d the data using a greedy thresholding algorithm that sets the small elements to zero (see Theorem 3). Such denoising is appropriate when the observed matrix has been element-wise perturbed by small noise, and the uncontaminated data matrix is sparse and contains large elements. We show that if an appropriate fraction of the (noisy) data is set to zero, one can still recover sparse principal components. This gives a principled approach to regularizing sparse PCA in the presence of small noise when the data is sparse.\nNot only do our algorithms preserve the quality of the sparse principal components, but iterative algorithms for sparse PCA, whose running time is proportional to the number of non-zero entries in the input matrix, benefit from the sparsity of A\u0303. Our experiments show about five-fold speed gains while producing near-comparable sparse components using less than 10% of the data.\nDiscussion. In summary, we show that one can recover sparse PCA from incomplete data while gaining computationally at the same time. Our result holds for the optimal sparse components from A versus from A\u0303. One cannot efficiently find these optimal components (since the problem is NP-hard to even approximate), so one runs a heuristic, in which case the approximation error of the heuristic would have to be taken into account. Our experiments show that using the incomplete data with the heuristics is just as good as those same heuristics with the complete data.\nIn practice, one may not be able to sample the data, but rather the samples are given to you. Our result establishes that if the samples are chosen with larger values being more likely, then one can recover sparse PCA. In practice one has no choice but to run the sparse PCA on these sampled elements and hope. Our theoretical results suggest that the outcome will be reasonable. This is because, while we do not have specific control over what samples we get, the samples are likely to represent the larger elements. For example, with user-product recommendation data, users are more likely to rate items they either really like (large positive value) or really dislike (large negative value).\nNotation. We use bold uppercase (e.g., X) for matrices and bold lowercase (e.g., x) for column vectors. The i-th row of X is X(i), and the i-th column of X is X\n(i). Let [n] denote the set {1, 2, ..., n}. E(X) is the expectation of a random variable X; for a matrix, E(X) denotes the element-wise expectation. For a matrix X \u2208 Rm\u00d7n, the Frobenius norm \u2016X\u2016F is \u2016X\u2016 2 F = \u2211m,n i,j=1X 2 ij , and the spectral (operator) norm \u2016X\u20162 is \u2016X\u20162 = max\u2016y\u20162=1 \u2016Xy\u20162. We also have the \u21131 and \u21130 norms: \u2016X\u2016\u21131 = \u2211m,n\ni,j=1 |Xij| and \u2016X\u20160 (the number of non-zero entries in X). The k-th largest singular value of X is \u03c3k(X). and log x is the natural logarithm of x."}, {"heading": "2 Sparse PCA from a Sketch", "text": "In this section, we will prove Theorem 1 and give a simple application to zeroing small fluctuations as a way to regularize to noise. In the next section we will use a more sophisticated way to select the elements of the matrix allowing us to tolerate a sparser matrix (more incomplete data) but still recovering sparse PCA to reasonable accuracy.\nTheorem 1 will be a corollary of a more general result, for a class of optimization problems involving a Lipschitz objective function over an arbitrary (not necessarily convex) domain. Let f(V,X) be a function that is defined for a matrix variable V and a matrix parameter X. The optimization variable V is in some feasible set S which is arbitrary. The parameter X is also arbitrary. We assume that f is Lipschitz in X with Lipschitz constant \u03b3. So,\n|f(V,X)\u2212 f(V, X\u0303)| \u2264 \u03b3(X)\u2016X\u2212 X\u0303\u20162 \u2200V \u2208 S.\n(Note we allow the Lipschitz constant to depend on X but not V.) The next lemma is the key tool we need to prove Theorem 1 and it may be on independent interest in other optimization settings. We are interested in maximizing f(V,X) w.r.t. V to obtain V\u2217. But, we only have an approximation X\u0303 for X, and so we maximize f(V, X\u0303) to obtain V\u0303 \u2217 , which will be a suboptimal solution with respect to X. We wish to bound f(V\u2217,X)\u2212 f(V\u0303\u2217,X) which quantifies how suboptimal V\u0303\u2217 is w.r.t. X.\nLemma 1 (Surrogate optimization bound) Let f(V,X) be \u03b3-Lipschitz w.r.t. X over the domain V \u2208 S . Define\nV\u2217 = argmax V\u2208S\nf(V,X); V\u0303 \u2217 = argmax\nV\u2208S f(V, X\u0303).\nThen, f(V\u2217,X)\u2212 f(V\u0303\u2217,X) \u2264 2\u03b3(X)\u2016X\u2212 X\u0303\u20162.\nIn the lemma, the function f and the domain S are arbitrary. In our setting, X \u2208 Rn\u00d7n, the domain S = {V \u2208 Rn\u00d7k;VTV = Ik; \u2016V(j)\u20160 \u2264 r}, and f(V,X) = trace(VTXV). We first show that f is Lipschitz w.r.t. X with \u03b3 = k (a constant independent of X). Let the representation of V by its columns be V = [v1, . . . ,vk]. Then,\n|trace(VTXV)\u2212 trace(VT X\u0303V)| = |trace((X\u2212 X\u0303)VVT )| \u2264 k \u2211\ni=1\n\u03c3i(X\u2212 X\u0303) \u2264 k\u2016X\u2212 X\u0303\u20162\nwhere, \u03c3i(A) is the i-th largest singular value of A (we used Von-neumann\u2019s trace inequality and the fact that VVT is a k-dimensional projection). Now, by Lemma 1,\ntrace(V\u2217TXV\u2217)\u2212 trace(V\u0303\u2217TXV\u0303\u2217) \u2264 2k\u2016X\u2212 X\u0303\u20162.\nTheorem 1 follows by setting X = ATA and X\u0303 = A\u0303 T A\u0303.\nGreedy thresholding. We give the simplest scenario of incomplete data where Theorem 1 gives some reassurance that one can compute good sparse principal components. Suppose the smallest data elements have been set to zero. This can happen, for example, if only the largest elements are measured, or in a noisy setting if the small elements are treated as noise and set to zero. So\nA\u0303ij =\n{\nAij |Aij | \u2265 \u03b4; 0 |Aij | < \u03b4.\nRecall k\u0303 = \u2016A\u20162F /\u2016A\u201622 (stable rank of A), and define \u2016A\u03b4\u20162F = \u2211 |Aij |<\u03b4 A2ij . Let A = A\u0303 + \u2206. By construction, \u2016\u2206\u20162F = \u2016A\u03b4\u20162F . Then,\n\u2016ATA\u2212 A\u0303T A\u0303\u20162 = \u2016AT\u2206+\u2206TA\u2212\u2206T\u2206\u20162 \u2264 2\u2016A\u20162\u2016\u2206\u20162 + \u2016\u2206\u201622. (3)\nSuppose the zeroing of elements only loses a fraction of the energy in A, i.e. \u03b4 is selected so that \u2016A\u03b4\u20162F \u2264 \u01eb2\u2016A\u20162F /k\u0303; that is an \u01eb/k\u0303 fraction of the total variance in A has been lost in the unmeasured (or zero) data. Then\n\u2016\u2206\u20162 \u2264 \u2016\u2206\u2016F \u2264 \u01eb \u221a\nk\u0303 \u2016A\u2016F = \u01eb\u2016A\u20162.\nTheorem 3 Suppose that A\u0303 is created from A by zeroing all elements that are less than \u03b4, and \u03b4 is such that the truncated norm satisfies \u2016A\u03b4\u201622 \u2264 \u01eb2\u2016A\u20162F /k\u0303. Then the sparse PCA solution V\u0303 \u2217 satisfies\ntrace(V\u0303 \u2217T AAV\u0303 \u2217 ) \u2265 trace(V\u2217TAATV\u2217)\u2212 2k\u01eb\u2016A\u201622(2 + \u01eb).\nTheorem 3 shows that it is possible to recover sparse PCA after setting small elements to zero. This is appropriate when most of the elements in A are small noise and a few of the elements in A contain large data elements. For example if your data consists of sparse O( \u221a nm) large elements (of magnitude, say, 1) and many nm\u2212O(\u221anm) small elements whose magnitude is o(1/\u221anm) (high signal-to-noise setting), then \u2016A\u03b4\u201622/\u2016A\u201622 \u2192 0 and with just a sparse sampling of the O( \u221a nm) large elements (very incomplete data), one recovers near optimal sparse PCA. Greedily keeping only the large elements of the matrix requires a particular structure in A to work, and it is based on a crude Frobenius-norm bound for the spectral error. In Section 2.1, we use recent results in element-wise matrix sparsification to choose the elements in a randomized way, with a bias toward large elements. With high probability, one can directly bound the spectral error and hence get better performance. But first, let us prove Lemma 1\nA Proof of Lemma 1. We need the following lemma.\nLemma 2 Let f and g be functions on a domain S . Then,\nsup x\u2208S f(x)\u2212 sup y\u2208S g(y) \u2264 sup x\u2208S (f(x)\u2212 g(x)).\nProof: sup x\u2208S (f(x)\u2212 g(x)) \u2265 f(x)\u2212 g(x) \u2265 f(x)\u2212 sup y\u2208S g(y), \u2200x \u2208 S.\nSince the RHS holds for all x, it follows that supx\u2208S(f(x) \u2212 g(x)) is an upper bound for f(x) \u2212 supy\u2208S g(U), and hence\nsup x\u2208S (f(x)\u2212 g(x)) \u2265 sup x\u2208S\n(\nf(x)\u2212 sup y\u2208S g(y)\n)\n= sup x\u2208S f(x)\u2212 sup y\u2208S g(y).\n\u22c4\nAlgorithm 1 Hybrid (\u21131, \u21132)-Element Sampling Input: A \u2208 Rm\u00d7n; # samples s; probabilities {pij}. 1: Set A\u0303 = 0m\u00d7n. 2: for t = 1 . . . s (i.i.d. trials with replacement) do 3: Randomly sample indices (it, jt) \u2208 [m]\u00d7 [n] with P [(it, jt) = (i, j)] = pij. 4: Update A\u0303: A\u0303ij \u2190 A\u0303ij + Aij\ns \u00b7 pij .\n5: return A\u0303 (with at most s non-zero entries).\nProof:(Lemma 1) Suppose that maxV\u2208S f(V,X) is attained at V\u2217 and maxV\u2208S f(V, X\u0303) is attained at V\u0303 \u2217 , and define \u01eb = f(V\u2217,X)\u2212 f(V\u0303\u2217,X). We have that\n\u01eb = f(V\u2217,X)\u2212 f(V\u0303\u2217, X\u0303) + f(V\u0303\u2217, X\u0303)\u2212 f(V\u0303\u2217,X) = max\nV f(V,X)\u2212max U f(U, X\u0303) + f(V\u0303\n\u2217 , X\u0303)\u2212 f(V\u0303\u2217,X)\n\u2264 max V\n( f(V,X)\u2212 f(V, X\u0303) ) + f(V\u0303 \u2217 , X\u0303)\u2212 f(V\u0303\u2217,X),\nwhere the last step follows from Lemma 2. Therefore,\n|\u01eb| \u2264 max V\n\u2223 \u2223 \u2223 f(V,X)\u2212 f(V, X\u0303) \u2223 \u2223 \u2223 + |f(V\u0303\u2217, X\u0303)\u2212 f(V\u0303\u2217,X)|\n\u2264 max V \u03b3(X)\u2016X\u2212 X\u0303\u20162 + \u03b3(X)\u2016X \u2212 X\u0303\u20162 = 2\u03b3(X)\u2016X\u2212 X\u0303\u20162.\n(We used the Lipschitz condition in the second step.) \u22c4"}, {"heading": "2.1 An (\u21131, \u21132)-Sampling Based Sketch", "text": "In the previous section, we created the sketch by deterministically setting the small data elements to zero. Instead, we could randomly select the data elements to keep. It is natural to bias this random sampling toward the larger elements. Therefore, we define sampling probabilities for each data element Aij which are proportional to a mixture of the absolute value and square of the data element:\npij = \u03b1 |Aij | \u2016A\u2016\u21131 + (1\u2212 \u03b1) A2ij \u2016A\u20162F , (4)\nwhere \u03b1 \u2208 (0, 1] is a mixing parameter. Such a sampling probability was used in Kundu et al. [2015] to sample data elements in independent trials to get a sketch A\u0303. We repeat the prototypical algorithm for element-wise matrix sampling in Algorithm 1. Note that unlike with the deterministic zeroing of small elements, in this sampling scheme, one samples the element Aij with probability pij and then rescales it by 1/pij . To see the intuition for this rescaling, consider the expected outcome for a single sample:\nE[A\u0303ij] = pij \u00b7 (Aij/pij) + (1\u2212 pij) \u00b7 0 = Aij;\nthat is, A\u0303 is a sparse but unbiased estimate for A. This unbiasedness holds for any choice of the sampling probabilities pij defined over the elements of A in Algorithm 1. However, for an appropriate choice of the sampling probabilities, we get much more than unbiasedness; we can control the spectral norm of the\ndeviation, \u2016A \u2212 A\u0303\u20162. In particular, the hybrid-(\u21131, \u21132) distribution in (4) was analyzed in Kundu et al. [2015], where they suggest an optimal choice for the mixing parameter \u03b1\u2217 which minimizes the theoretical bound on \u2016A\u2212 A\u0303\u20162. This algorithm to choose \u03b1\u2217 is summarized in Algorithm 2.\nUsing the probabilities in (4) to create the sketch A\u0303 using Algorithm 1, with \u03b1\u2217 selected using Algorithm 2, one can prove a bound for \u2016A\u2212A\u0303\u20162. We state a simplified version of the bound from Kundu et al. [2015] in Theorem 4.\nTheorem 4 (Kundu et al. [2015]) Let A \u2208 Rm\u00d7n and let \u01eb > 0 be an accuracy parameter. Define probabilities pij as in (4) with \u03b1\u2217 chosen using Algorithm 2. Let A\u0303 be the sparse sketch produced using Algorithm 1 with a number of samples\ns \u2265 2 \u01eb2 ( \u03c12 + \u03b3\u01eb/3 ) log\n(\nm+ n\n\u03b4\n)\n,\nwhere\n\u03c12 = k\u0303 \u00b7max{m,n}\n\u03b1 \u00b7 k\u0303 \u00b7 \u2016A\u20162\u2016A\u2016\u21131 + (1\u2212 \u03b1)\n, and \u03b3 \u2264 1 + \u221a mnk\u0303\n\u03b1 .\nThen, with probability at least 1\u2212 \u03b4, \u2016A\u2212 A\u0303\u20162 \u2264 \u01eb \u2016A\u20162 .\nProof: Follows from the bound in Kundu et al. [2015]. \u22c4 Recall that k\u0303 is the stable rank of A. In practice, \u03b1\u2217 is bounded away from 0 and 1, and so s = O(\u01eb\u22122k\u0303max{m,n}) samples suffices to get a sketch A\u0303 for which \u2016A \u2212 A\u0303\u20162 \u2264 \u01eb\u2016A\u2016. This is exactly what we need to prove Theorem 2.\nProof of Theorem 2. The number of samples s in Theorem 2 corresponds to the number of samples needed in Theorem 4 with the error tolerance \u01eb/k. Using (3) (where \u2206 = A \u2212 A\u0303) and Theorem 4, we have that\n\u2016ATA\u2212 A\u0303T A\u0303\u20162 \u2264 2\u01eb\nk \u2016A\u201622 +\n\u01eb2 k2 \u2016A\u201622. (5)\nUsing (5) in Theorem 1 gives Theorem 2."}, {"heading": "3 Experiments", "text": "We show the experimental performance of sparse PCA from a sketch using several real data matrices. As we mentioned, sparse PCA is NP-Hard, and so we must use heuristics. These heuristics are discussed next, followed by the data, the experimental design and finaly the results."}, {"heading": "3.1 Algorithms for Sparse PCA", "text": "Let G (ground truth) denote the algorithm which computes the principal components (which may not be sparse) of the full data matrix A; the optimal variance is OPTk. We consider six heuristics for getting sparce principal components.\nGmax,r The r largest-magnitude entries in each principal component generated by G. Gsp,r r-sparse components using the Spasm toolbox of Sjstrand et al. [2012] with A. Hmax,r The r largest entries of the principal components for the (\u21131, \u21132)-sampled sketch A\u0303. Hsp,r r-sparse components using Spasm with the (\u21131, \u21132)-sampled sketch A\u0303. Umax,r The r largest entries of the principal components for the uniformly sampled sketch A\u0303. Usp,r r-sparse components using Spasm with the uniformly sampled sketch A\u0303.\nAlgorithm 2 Optimal Mixing Parameter \u03b1\u2217 Input: A \u2208 Rm\u00d7n. 1: Define two functions of \u03b1 that depend on A:\n\u03c12(\u03b1) = max\n\n\n max i\nn \u2211\nj=1\n\u03beij,max j\nm \u2211\ni=1\n\u03beij\n\n\n\n\u2212 \u03c32min(A);\n\u03b3(\u03b1) = max i,j:\nAij 6=0\n\n \n  \u2016A\u2016\u21131 \u03b1+ (1\u2212 \u03b1)\u2016A\u2016\u21131 \u00b7|Aij |\n\u2016A\u20162F\n\n \n \n+ \u2016A\u20162 ;\nwhere,\n\u03beij = \u2016A\u20162F/ ( \u03b1 \u00b7 \u2016A\u20162F |Aij | \u00b7 \u2016A\u2016\u21131 + (1 \u2212 \u03b1) ) , for Aij 6= 0.\n2: Find \u03b1\u2217 \u2208 (0, 1] to minimize \u03c12(\u03b1) + \u03b3(\u03b1)\u01eb \u2016A\u20162 /3. 3: return \u03b1\u2217\nThe outputs of an algorithm Z are sparse principal components V, and the metric we are interested in is the variance, f(Z) = trace(VTATAV), where A is the original centered data. We consider the following statistics.\nf(Gmax,r) f(Gsp,r)\nRelative loss of greedy thresholding versus Spasm, illustrating the value of a good sparse PCA algorithm. Our sketch based algorithms do not address this loss.\nf(Hmax/sp,r) f(Gmax/sp,r)\nRelative loss of using the (\u21131, \u21132)-sketch A\u0303 instead of complete data A. A ratio close to 1 is desired.\nf(Umax/sp,r) f(Gmax/sp,r)\nRelative loss of using the uniform sketch A\u0303 instead of complete data A. A benchmark to highlight the value of a good sketch.\nWe also report on the computation time for the algorithms. We show results to confirm that sparse PCA algorithms using the (\u21131, \u21132)-sketch are nearly comparable to those same algorithms on the complete data; and, computing from a sparse sketch has a running time that is reduced proportionately to the sparsity."}, {"heading": "3.2 Data Sets", "text": "We show results on image, text, stock, and gene expression data. We briefly describe the datasets below.\nDigit Data (m = 2313, n = 256): We use the Hull [1994] handwritten zip-code digit images (300 pixels/inch in 8-bit gray scale). Each pixel is a feature (normalized to be in [\u22121, 1]). Each 16 \u00d7 16 digit image forms a row of the data matrix A. We focus on three digits: \u201c6\u201d (664 samples), \u201c9\u201d (644 samples), and \u201c1\u201d (1005 samples).\nTechTC Data (m = 139, n = 15170): We use the Technion Repository of Text Categorization Dataset (TechTC, see Gabrilovich and Markovitch [2004]) from the Open Directory Project (ODP). Each documents is represented as a probability distribution over a bag-of-words, with words being the features\n\u2013 we removed words with fewer than 5 letters. Each of the 139 documents forms a row in the data.\nStock Data (m = 7056, n = 1218): We use S&P100 stock market data of prices for 1218 stocks collected between 1983 and 2011. This temporal dataset has 7056 snapshots of stock prices. The prices of each day form a row of the data matrix and a principal component represents an \u201cindex\u201d of sorts \u2013 each stock is a feature.\nGene Expression Data (m = 107, n = 22215): We use GSE10072 gene expression data for lung cancer from the NCBI Gene Expression Omnibus database. There are 107 samples (58 lung tumor cases and 49 normal lung controls) forming the rows of the data matrix, with 22,215 probes (features) from the GPL96 platform annotation table."}, {"heading": "3.3 Results", "text": "We report results for primarily the top principal component (k = 1) which is the case most considered in the literature. When k > 1, our results do not qualitatively change.\nHandwritten Digits. Using Algorithm 2, the optimal mixing parameter is \u03b1\u2217 = 0.42. We sample approximately 7% of the elements from the centered data using (\u21131, \u21132)-sampling, as well as uniform sampling. The performance for small of r is shown in Table 1, including the running time \u03c4 .\nFor this data, f(Gmax,r)/f(Gsp,r) \u2248 0.23 (r = 10), so it is important to use a good sparse PCA algorithm. We see from Table 1 that the (\u21131, \u21132)-sketch significantly outperforms the uniform sketch. A more extensive comparison of recovered variance is given in Figure 2(a). We also observe a speed-up of a factor of about 6 for the (\u21131, \u21132)-sketch. We point out that the uniform sketch is reasonable for the digits data because most data elements are close to either +1 or \u22121, since the pixels are either black or white.\nWe show a visualization of the principal components in Figure 1. We observe that the sparse components from the (\u21131, \u21132)-sketch are almost identical to the sparse components from the complete data.\nTechTC Data. Algorithm 2 gives optimal mixing parameter \u03b1\u2217 = 1. We sample approximately 5% of the elements from the centered data using our (\u21131, \u21132)-sampling, as well as uniform sampling. The performance for small r is shown in Table 2, including the running time \u03c4 .\nFor this data, f(Gmax,r)/f(Gsp,r) \u2248 0.84 (r = 10). We observe a very significant performance difference between the (\u21131, \u21132)-sketch and uniform sketch. A more extensive comparison of recovered variance is given in Figure 2(b). We also observe a speed-up of a factor of about 6 for the (\u21131, \u21132)-sketch. Unlike the digits data which is uniformly near \u00b11, the text data is \u201cspikey\u201d and now it is important to sample with a bias toward larger elements, which is why the uniform-sketch performs very poorly.\nAs a final comparison, we look at the actual sparse top component with sparsity parameter r = 10. The topic IDs in the TechTC data are 10567=\u201dUS: Indiana: Evansville\u201d and 11346=\u201dUS: Florida\u201d. The top-10 features (words) in the full PCA on the complete data are shown in Table 3.\nIn Table 4 we show which words appear in the top sparse principal component with sparsity r = 10 using various sparse PCA algorithms. We observe that the sparse PCA from the (\u21131, \u21132)-sketch with only 5% of the data sampled matches quite closely with the same sparse PCA algorithm using the complete data (Gmax/sp,r matches Hmax/sp,r).\nStock Data. Algorithm 2 gives optimal mixing parameter \u03b1\u2217 = 0.11. We sample about 2% of the non-zero elements from the centered data using our (\u21131, \u21132)-sampling, as well as uniform sampling. The performance for small r is shown in Table 5, including the running time \u03c4 .\nFor this data, f(Gmax,r)/f(Gsp,r) \u2248 0.96 (r = 10). We observe a very significant performance difference between the (\u21131, \u21132)-sketch and uniform sketch. A more extensive comparison of recovered variance is given in Figure 2(c). We also observe a speed-up of a factor of about 4 for the (\u21131, \u21132)-sketch. Similar to TechTC data this dataset is also \u201cspikey\u201d, and consequently biased sampling toward larger elements significantly outperforms the uniform-sketch.\nWe now look at the actual sparse top component with sparsity parameter r = 10. The top-10 features (stocks) in the full PCA on the complete data are shown in Table 6. In Table 7 we show which stocks appear in the top sparse principal component using various sparse PCA algorithms. We observe that the sparse PCA from the (\u21131, \u21132)-sketch with only 2% of the non-zero elements sampled matches quite closely with the same sparse PCA algorithm using the complete data (Gmax/sp,r matches Hmax/sp,r).\n1we computed \u03b1\u2217 numerically in the range [0.1, 1].\nGene Expression Data. Algorithm 2 gives optimal mixing parameter \u03b1\u2217 = 0.92. We sample about 9% of the elements from the centered data using our (\u21131, \u21132)-sampling, as well as uniform sampling. The performance for small r is shown in Table 8, including the running time \u03c4 .\nFor this data, f(Gmax,r)/f(Gsp,r) \u2248 0.05 (r = 10) which means a good sparse PCA algorithm is\nimperative. We observe a very significant performance difference between the (\u21131, \u21132)-sketch and uniform sketch. A more extensive comparison of recovered variance is given in Figure 2(d). We also observe a speed-up of a factor of about 4 for the (\u21131, \u21132)-sketch. Similar to TechTC data this dataset is also \u201cspikey\u201d, and consequently biased sampling toward larger elements significantly outperforms the uniform-sketch.\nAlso, we look at the actual sparse top component with sparsity parameter r = 10. The top-10 features (probes) in the full PCA on the complete data are shown in Table 9.\nIn Table 10 we show which probes appear in the top sparse principal component with sparsity r = 10 using various sparse PCA algorithms. We observe that the sparse PCA from the (\u21131, \u21132)-sketch with only 9% of the elements sampled matches reasonably with the same sparse PCA algorithm using the complete data (Gmax/sp,r matches Hmax/sp,r).\nFinally, we validate the genes corresponding to the top probes in the context of lung cancer. Table 11 lists the top twelve gene symbols in Table 9. Note that a gene can occure multiple times in principal component since genes can be associated with different probes.\nGenes like SFTPC, AGER, WIF1, and FABP4 are down-regulated in lung cancer, while SPP1 is upregulated (see the functional gene grouping at: www.sabiosciences.com/rt_pcr_product/HTML/PAHS-13\nCo-expression analysis on the set of eight genes for Hmax,r and Hsp,r using the tool ToppFun (toppgene.cchmc.org/ shows that all eight genes appear in a list of selected probes characterizing non-small-cell lung carcinoma (NSCLC) in [Hou et al., 2010, Table S1]. Further, AGER and FAM107A appear in the top five highly discriminative genes in [Hou et al., 2010, Table S3]. Additionally, AGER, FCN3, SPP1, and ADH1B appear among the 162 most differentiating genes across two subtypes of NSCLC and normal lung cancer in [Dracheva et al., 2007, Supplemental Table 1]. Such findings show that our method can identify, from incomplete data, important genes for complex diseases like cancer. Also, notice that our sampling-based method is able to identify additional important genes, such as, FCN3 and FAM107A in top ten genes."}, {"heading": "3.4 Performance of Other Sketches", "text": "We briefly report on other options for sketching A. First, we consider suboptimal \u03b1 (not \u03b1\u2217 from Algorithm 2) in (4) to construct a suboptimal hybrid distribution. We use this distribution in proto-Algorithm 1 to construct a sparse sketch. Figure 3 reveals that a good sketch using the optimal \u03b1\u2217 is important.\nSecond, another popular sketching method using element wise sparsification is to sample elements not biasing toward larger elements but rather toward elements whose leverage scores are high. See Chen et al. [2014] for the detailed form of the leverage score sampling probabilities (which are known to work well\nin other settings can be plugged into our proto-Algorithm 1). Let A be a m\u00d7 n matrix of rank \u03c1, and its SVD is given by A = U\u03a3VT . Then, we define \u00b5i (row leverage scores), \u03bdj (column leverage scores), and element-wise leverage scores plev as follows:\n\u00b5i = \u2016U(i)\u201622, \u03bdj = \u2016V(j)\u201622, plev = 1\n2 \u00b7 \u00b5i + \u03bdj (m+ n)\u03c1 + 1 2mn , i \u2208 [m], j \u2208 [n]\nAt a high level, the leverage score of element (i, j) is proportional to the squared norms of the ith row of the left singular matrix and the jth row of the right singular matrix. Such leverage score sampling is different from uniform sampling only for low rank matrices or low rank approximations to matrices, so we used a low rank approximation to the data matrix. We construct such low-rank approximation by projecting a dataset onto a low dimensional subspace. We notice that the datasets projected onto the space spanned by top few principal components preserve the linear structure of the data. For example, Digit data show good separation of digits when projected onto the top three PCA\u2019s. For TechTC and Gene data the top two respective PCA\u2019s are good enough to form a low-dimensional subspace where the datasets show reasonable separation of two classes of samples. For the stock data we use top three PCA\u2019s because the stable rank is close to 2.\nLet Lsp,r be the r-sparse components using Spasm for the leverage score sampled sketch A\u0303. Figure 4 shows that leverage score sampling is not as effective as the optimal hybrid (\u21131, \u21132)-sampling for sparse PCA of low-rank data.\nConclusion. It is possible to use a sparse sketch (incomplete data) to recover nearly as good sparse principal components as you would have gotten with the complete data. We mention that, while Gmax which uses the largest weights in the unconstrained PCA does not perform well with respect to the variance, it does identify good features. A simple enhancement to Gmax is to recalibrate the sparse component after identifying the features - this is an unconstrained PCA problem on just the columns of the data matrix corresponding to the features. This method of recalibrating can be used to improve any sparse PCA algorithm.\nOur algorithms are simple and efficient, and many interesting avenues for further research remain. Can the sampling complexity for the top-k sparse PCA be reduced from O(k2) to O(k). We suspect that this should be possible by getting a better bound on \u2211k\ni=1 \u03c3i(A TA \u2212 A\u0303T A\u0303); we used the crude\nbound k\u2016ATA \u2212 A\u0303T A\u0303\u20162. We also presented a general surrogate optimization bound which may be of interest in other applications. In particular, it is pointed out in Magdon-Ismail and Boutsidis [2015] that though PCA optimizes variance, a more natural way to look at PCA is as the linear projection of the data that minimizes the information loss. Magdon-Ismail and Boutsidis [2015] gives efficient algorithms to find sparse linear dimension reduction that minimizes information loss \u2013 the information loss of sparse PCA can be considerably higher than optimal. To minimize information loss, the objective to maximize is f(V) = trace(ATAV(AV)\u2020A). It would be interesting to see whether one can recover sparse lowinformation-loss linear projectors from incomplete data."}], "references": [{"title": "Non-negative sparse PCA with provable guarantees", "author": ["M. Asteris", "D. Papailiopoulos", "A. Dimakis"], "venue": "In Proc. ICML,", "citeRegEx": "Asteris et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Asteris et al\\.", "year": 2014}, {"title": "Loadings and correlations in the interpretation of principal components", "author": ["J. Cadima", "I. Jolliffe"], "venue": "Applied Statistics,", "citeRegEx": "Cadima and Jolliffe.,? \\Q1995\\E", "shortCiteRegEx": "Cadima and Jolliffe.", "year": 1995}, {"title": "Coherent Matrix Completion", "author": ["Y Chen", "S Bhojanapalli", "S Sanghavi", "R Ward"], "venue": "Proceedings of International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["Alexandre d\u2019Aspremont", "Laurent El Ghaoui", "Michael I. Jordan", "Gert R.G. Lanckriet"], "venue": "SIAM Review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["Alexandre d\u2019Aspremont", "Francis Bach", "Laurent El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2008}, {"title": "Distinguishing Lung Tumours From Normal Lung Based on a Small Set of Genes", "author": ["T. Dracheva", "R. Philip", "W. Xiao", "AG Gee"], "venue": "In Lung Cancer,", "citeRegEx": "Dracheva et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dracheva et al\\.", "year": 2007}, {"title": "Text categorization with many redundant features: using aggressive feature selection to make SVMs competitive with C4.5", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "In Proceedings of International Conference on Machine Learning,", "citeRegEx": "Gabrilovich and Markovitch.,? \\Q2004\\E", "shortCiteRegEx": "Gabrilovich and Markovitch.", "year": 2004}, {"title": "Gene Expression-Based Classification of Non-Small Cell Lung Carcinomas and Survival Prediction", "author": ["J. Hou", "J. Aerts", "B. den Hamer"], "venue": "In PLoS One, page 5(4):e10312,", "citeRegEx": "Hou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2010}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "In IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Hull.,? \\Q1994\\E", "shortCiteRegEx": "Hull.", "year": 1994}, {"title": "Recovering PCA from Hybrid-(l1, l2) Sparse Sampling of Data Elements", "author": ["A. Kundu", "P. Drineas", "M. Magdon-Ismail"], "venue": "In http://arxiv.org/pdf/1503.00547v1.pdf,", "citeRegEx": "Kundu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kundu et al\\.", "year": 2015}, {"title": "NP-hardness and inapproximability of sparse pca", "author": ["M. Magdon-Ismail"], "venue": "arxiv report: http://arxiv.org/abs/1502.05675,", "citeRegEx": "Magdon.Ismail.,? \\Q2015\\E", "shortCiteRegEx": "Magdon.Ismail.", "year": 2015}, {"title": "Generalized spectral bounds for sparse LDA", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "In Proc. ICML,", "citeRegEx": "Moghaddam et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2006}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "Philosophical Magazine,", "citeRegEx": "Pearson.,? \\Q1901\\E", "shortCiteRegEx": "Pearson.", "year": 1901}, {"title": "Sparse principal component analysis via regularized low rank matrix approximation", "author": ["Haipeng Shen", "Jianhua Z. Huang"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Shen and Huang.,? \\Q2008\\E", "shortCiteRegEx": "Shen and Huang.", "year": 2008}, {"title": "Spasm: A matlab toolbox for sparse statistical modeling", "author": ["K. Sjstrand", "L.H. Clemmensen", "R. Larsen", "B. Ersbll"], "venue": "In Journal of Statistical Software (Accepted for publication),", "citeRegEx": "Sjstrand et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sjstrand et al\\.", "year": 2012}, {"title": "A modified principal component technique based on the lasso", "author": ["N. Trendafilov", "I.T. Jolliffe", "M. Uddin"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Trendafilov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Trendafilov et al\\.", "year": 2003}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Computational & Graphical Statistics,", "citeRegEx": "Zou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 12, "context": "The earliest reference to principal components analysis (PCA) is in Pearson [1901]. Since then, PCA has evolved into a classic tool for data analysis.", "startOffset": 68, "endOffset": 83}, {"referenceID": 10, "context": "The sparse PCA problem is itself a very hard problem that is not only NP-hard, but also inapproximable [Magdon-Ismail, 2015] There are many heuristics for obtaining sparse factors [Cadima and Jolliffe, 1995, Trendafilov et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 0, "context": ", 2006, Shen and Huang, 2008] including some approximation algorithms with provable guarantees Asteris et al. [2014]. The existing research typically addresses the task of getting just the top principal component (k = 1).", "startOffset": 95, "endOffset": 117}, {"referenceID": 9, "context": "To determine which elements to sample, and how to form the sketch, we leverage some recent results in elementwise matrix completion (Kundu et al. [2015]).", "startOffset": 133, "endOffset": 153}, {"referenceID": 9, "context": "Such a sampling probability was used in Kundu et al. [2015] to sample data elements in independent trials to get a sketch \u00c3.", "startOffset": 40, "endOffset": 60}, {"referenceID": 9, "context": "In particular, the hybrid-(l1, l2) distribution in (4) was analyzed in Kundu et al. [2015], where they suggest an optimal choice for the mixing parameter \u03b1\u2217 which minimizes the theoretical bound on \u2016A\u2212 \u00c3\u20162.", "startOffset": 71, "endOffset": 91}, {"referenceID": 9, "context": "In particular, the hybrid-(l1, l2) distribution in (4) was analyzed in Kundu et al. [2015], where they suggest an optimal choice for the mixing parameter \u03b1\u2217 which minimizes the theoretical bound on \u2016A\u2212 \u00c3\u20162. This algorithm to choose \u03b1\u2217 is summarized in Algorithm 2. Using the probabilities in (4) to create the sketch \u00c3 using Algorithm 1, with \u03b1\u2217 selected using Algorithm 2, one can prove a bound for \u2016A\u2212\u00c3\u20162. We state a simplified version of the bound from Kundu et al. [2015] in Theorem 4.", "startOffset": 71, "endOffset": 476}, {"referenceID": 9, "context": "In particular, the hybrid-(l1, l2) distribution in (4) was analyzed in Kundu et al. [2015], where they suggest an optimal choice for the mixing parameter \u03b1\u2217 which minimizes the theoretical bound on \u2016A\u2212 \u00c3\u20162. This algorithm to choose \u03b1\u2217 is summarized in Algorithm 2. Using the probabilities in (4) to create the sketch \u00c3 using Algorithm 1, with \u03b1\u2217 selected using Algorithm 2, one can prove a bound for \u2016A\u2212\u00c3\u20162. We state a simplified version of the bound from Kundu et al. [2015] in Theorem 4. Theorem 4 (Kundu et al. [2015]) Let A \u2208 Rm\u00d7n and let \u01eb > 0 be an accuracy parameter.", "startOffset": 71, "endOffset": 521}, {"referenceID": 9, "context": "Proof: Follows from the bound in Kundu et al. [2015]. \u22c4 Recall that k\u0303 is the stable rank of A.", "startOffset": 33, "endOffset": 53}, {"referenceID": 14, "context": "Gsp,r r-sparse components using the Spasm toolbox of Sjstrand et al. [2012] with A.", "startOffset": 53, "endOffset": 76}, {"referenceID": 8, "context": "Digit Data (m = 2313, n = 256): We use the Hull [1994] handwritten zip-code digit images (300 pixels/inch in 8-bit gray scale).", "startOffset": 43, "endOffset": 55}, {"referenceID": 6, "context": "TechTC Data (m = 139, n = 15170): We use the Technion Repository of Text Categorization Dataset (TechTC, see Gabrilovich and Markovitch [2004]) from the Open Directory Project (ODP).", "startOffset": 109, "endOffset": 143}, {"referenceID": 2, "context": "See Chen et al. [2014] for the detailed form of the leverage score sampling probabilities (which are known to work well", "startOffset": 4, "endOffset": 23}, {"referenceID": 10, "context": "In particular, it is pointed out in Magdon-Ismail and Boutsidis [2015] that though PCA optimizes variance, a more natural way to look at PCA is as the linear projection of the data that minimizes the information loss.", "startOffset": 36, "endOffset": 71}, {"referenceID": 10, "context": "In particular, it is pointed out in Magdon-Ismail and Boutsidis [2015] that though PCA optimizes variance, a more natural way to look at PCA is as the linear projection of the data that minimizes the information loss. Magdon-Ismail and Boutsidis [2015] gives efficient algorithms to find sparse linear dimension reduction that minimizes information loss \u2013 the information loss of sparse PCA can be considerably higher than optimal.", "startOffset": 36, "endOffset": 253}], "year": 2015, "abstractText": "We study how well one can recover sparse principal components of a data matrix using a sketch formed from a few of its elements. We show that for a wide class of optimization problems, if the sketch is close (in the spectral norm) to the original data matrix, then one can recover a near optimal solution to the optimization problem by using the sketch. In particular, we use this approach to obtain sparse principal components and show that for m data points in n dimensions, O(\u01eb\u22122k\u0303max{m,n}) elements gives an \u01eb-additive approximation to the sparse PCA problem (k\u0303 is the stable rank of the data matrix). We demonstrate our algorithms extensively on image, text, biological and financial data. The results show that not only are we able to recover the sparse PCAs from the incomplete data, but by using our sparse sketch, the running time drops by a factor of five or more.", "creator": "LaTeX with hyperref package"}}}