{"id": "1503.05055", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2015", "title": "Combining partially independent belief functions", "abstract": "the modeling of belief functions manages uncertainty and also proposes a family of combination guidelines detailing aggregate opinions of several goods. similar composite solutions encourage evidential actions where sources are aligned ; other whether are suited may combine evidential parameters held adjoining selected sources. in extended paper they yield two suggested contributions : first we suggest integrated method to exclude vendors'excess of independence that may ease the choice and the uniformly appropriate set of combination rules. second, colleagues propose a new combination rule that combines care of sources'magnitude of independence. the proposed framework is set on generated mass functions.", "histories": [["v1", "Tue, 17 Mar 2015 14:04:38 GMT  (105kb)", "http://arxiv.org/abs/1503.05055v1", "Decision Support Systems, Elsevier, 2015"]], "COMMENTS": "Decision Support Systems, Elsevier, 2015", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mouna chebbah", "arnaud martin", "boutheina ben yaghlane"], "accepted": false, "id": "1503.05055"}, "pdf": {"name": "1503.05055.pdf", "metadata": {"source": "CRF", "title": "Combining partially independent belief functions", "authors": ["Mouna Chebbah", "Arnaud Martinb", "Boutheina Ben Yaghlanec"], "emails": ["Mouna.Chebbah@univ-rennes1.fr", "Arnaud.Martin@univ-rennes1.fr", "boutheina.yaghlane@ihec.rnu.tn"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n05 05\n5v 1\n[ cs\n.A I]\n1 7\nM ar\nThe theory of belief functions manages uncertainty and also proposes a set of combination rules to aggregate opinions of several sources. Some combination rules mix evidential information where sources are independent; other rules are suited to combine evidential information held by dependent sources. In this paper we have two main contributions: First we suggest a method to quantify sources\u2019 degree of independence that may guide the choice of the more appropriate set of combination rules. Second, we propose a new combination rule that takes consideration of sources\u2019 degree of independence. The proposed method is illustrated on generated mass functions. Keywords: Theory of belief functions, Combination rules, Clustering, Independence, Sources independence, Combination rule choice"}, {"heading": "1. Introduction", "text": "Uncertainty theories like the theory of probabilities, the theory of fuzzy sets [1], the theory of possibilities [2] and the theory of belief functions [3, 4] model and manage uncertain data. The theory of belief functions can deal with imprecise\nEmail addresses: Mouna.Chebbah@univ-rennes1.fr (Mouna Chebbah), Arnaud.Martin@univ-rennes1.fr (Arnaud Martin), boutheina.yaghlane@ihec.rnu.tn (Boutheina Ben Yaghlane)\nPreprint submitted to Elsevier March 18, 2015\nand/or uncertain data provided by several belief holders and also combine them.\nCombining several evidential information held by distinct belief holders aggregates their points of view by stressing common points. In the theory of belief functions, many combination rules are proposed, some of them like [2, 5, 6, 7, 8, 9] are fitted to the aggregation of evidential information provided by cognitively independent sources whereas the cautious, bold [10] and mean combination rules can be applied when sources are cognitively dependent. The choice of combination rules depends on sources independence.\nSome researches are focused on doxastic independence of variables such as [11, 12]; others [4, 13] tackled cognitive and evidential independence of variables. This paper is focused on measuring the independence of sources and not that of variables. We suggest a statistical approach to estimate the independence of sources on the bases of all evidential information that they provide. The aim of estimating the independence of sources is to guide the choice of the combination rule to be used when combining their evidential information.\nWe propose also a new combination rule to aggregate evidential information and take into account the independence degree of their sources. The proposed combination rule is weighted with that degree of independence leading to the conjunctive rule [14] when sources are fully independent and to the cautious rule [10] when they are fully dependent.\nIn the sequel, we introduce in Section 2 preliminaries of the theory of belief functions. In the Section 3, an evidential clustering algorithm is detailed. This clustering algorithm will be used in the first step of the independence measure process. Independence measure is then detailed in Section 4. It is estimated in four steps: In the first step the clustering algorithm is applied. Second a mapping between clusters is performed; then independence of clusters and sources are deduced in the last two steps. Independence is learned for only two sources and then generalized for\na greater number of sources. A new combination rule is proposed in the Section 5 taking into account the independence degree of sources. The proposed method is tested on random mass functions in Section 6. Finally, conclusions are drawn."}, {"heading": "2. Theory of belief functions", "text": "The theory of belief functions was introduced by Dempster [3] and formalized by Shafer [4] to model imperfect data. The frame of discernment also called universe of discourse, \u2126 = {\u03c91,\u03c92, . . . ,\u03c9N}, is an exhaustive set of N mutually exclusive hypotheses \u03c9i. The power set 2\u2126 is a set of all subsets of \u2126; it is made of hypotheses and unions of hypotheses from \u2126. The basic belief assignment (BBA) commonly called mass function is a function defined on the power set 2\u2126 and spans the interval [0,1] such that:\n\u2211 A\u2286\u2126 m(A) = 1 (1)\nA basic belief mass (BBM) also called mass, m(A), is a degree of faith on the truth of A. The BBM, m(A), is a degree of belief on A which can be committed to its subsets if further information justifies it [7].\nSubsets A having a strictly positive mass are called focal elements. Union of all focal elements is called core. Shafer [4] assumed a normality condition such that m( /0) = 0, thereafter Smets [14] relaxed this condition in order to tolerate m( /0)> 0.\nThe frame of discernment can also be a focal element; its BBM, m(\u2126), is inter-\npreted as a degree of ignorance. In the case of total ignorance, m(\u2126) = 1.\nA simple support function is a mass function with two focal elements including\nthe frame of discernment. A simple support function m is defined as follows:\nm(A) =\n    \n   \n1\u2212w i f A = B for some B \u2282 \u2126 w if A = \u2126 0 otherwise\n(2)\nWhere A is a focus of that simple support function and w \u2208 [0,1] is its weight. A simple support function is simply noted Aw. A nondogmatic mass function can be obtained by the combination of several simple support functions. Therefore, any nondogmatic mass function can be decomposed into several support functions using the canonical decomposition proposed by Smets [15].\nThe belief function (bel) is computed from a BBA m. The amount bel(A) is the\nminimal belief on A justified by available information on B (B \u2286 A):\nbel(A) = \u2211 B\u2286A,B 6= /0 m(B) (3)\nThe plausibility function (pl) is also derived from a BBA m. The amount pl(A) is the maximal belief on A justified by information on B which are not contradictory with A (A\u2229B 6= /0):\npl(A) = \u2211 A\u2229B 6= /0 m(B) (4)\nPignistic transformation computes pignistic probabilities from mass functions in the purpose of making a decision. The pignistic probability of a single hypothesis A is given by:\nBetP(A) = \u2211 B\u2286\u2126,B 6= /0 |B\u2229A| |B| m(B) 1\u2212m( /0) . (5)\nDecision is made according to the maximum pignistic probability. The single point having the greatest BetP is the most likely hypothesis."}, {"heading": "2.1. Discounting", "text": "Sources of information are not always reliable, they can be unreliable or even a little bit reliable. Taking into account reliability of sources, we adjust their beliefs proportionally to degrees of reliability. Discounting mass functions is a way of taking consideration of sources\u2019 reliabilities into their mass functions. If reliability\nrate \u03b1 of a source is known or can be quantified; discounting its mass function m is defined as follows: \n\n\nm\u03b1(A) = \u03b1 \u00d7m(A) ,\u2200A \u2282 \u2126 m\u03b1(\u2126) = 1\u2212\u03b1 \u00d7 (1\u2212m(\u2126)) (6)\nThis discounting operator can be used not only to take consideration of source\u2019s reliability, but also to consider any information which can be integrated into the mass function, (1\u2212\u03b1) is called discounting rate."}, {"heading": "2.2. Combination rules", "text": "In the theory of belief functions, a great number of combination rules are used to summarize a set of mass functions into only one. Let s1 and s2 be two distinct and cognitively independent sources providing two different mass functions m1 and m2 defined on the same frame of discernment \u2126. Combining these mass functions induces a third one m12 defined on the same frame of discernment \u2126.\nThere is a great number of combination rules [2, 5, 6, 7, 8, 9], but we enumerate in this section only Dempster, conjunctive, disjunctive, Yager, Dubois and Prade, mean, cautious and bold combination rules. The first combination rule was proposed by Dempster in [3] to combine two distinct mass functions m1 and m2 as follows:\nm1\u22952(A) = (m1 \u2295m2)(A) =\n     \n    \n\u2211 B\u2229C=A m1(B)\u00d7m2(C)\n1\u2212 \u2211 B\u2229C= /0\nm1(B)\u00d7m2(C) \u2200A \u2286 \u2126, A 6= /0\n0 i f A = /0\n(7)\nThe BBM of the empty set is null (m( /0) = 0). This rule verifies the normality condition and works under a closed world where \u2126 is exhaustive.\nIn order to solve the problem highlighted by Zadeh\u2019s counter example [16]\nwhere Dempster\u2019s rule of combination produced unsatisfactory results, many combination rules appeared. Smets [14] proposed an open world where a positive mass can be allocated to the empty set. Hence the conjunctive rule of combination for two mass functions m1 and m2 is defined as follows:\nm1 \u2229\u00a92(A) = (m1 \u2229\u00a9m2)(A) = \u2211 B\u2229C=A m1(B)\u00d7m2(C) (8)\nEven if Smets [17] interpreted the BBM, m1 \u2229\u00a92( /0), as an amount of conflict between evidences that induced m1 and m2; that amount is not really a conflict because it includes a certain degree of auto-conflict due to the non-idempotence of the conjunctive combination [18].\nThe conjunctive rule is used only when both sources are reliable. Smets [14] proposed also to use a disjunctive combination when an unknown source is unreliable. The disjunctive rule of combination is defined for two BBAs m1 and m2 as follows:\nm1 \u222a\u00a92(A) = (m1 \u222a\u00a9m2)(A) = \u2211 B\u222aC=A m1(B)\u00d7m2(C) (9)\nYager in [8] interpreted m( /0) as an amount of ignorance; consequently it is allocated to \u2126. Yager\u2019s rule of combination is also defined to combine two mass functions m1 and m2 as follows: \n   \n   \nmY (X) = m1 \u2229\u00a92(X) \u2200X \u2282 \u2126, X 6= /0 mY (\u2126) = m1 \u2229\u00a92(\u2126)+m1 \u2229\u00a92( /0) mY ( /0) = 0\n(10)\nDubois and Prade\u2019s solution [2] was to affect the mass resulting from the combination of conflicting focal elements to the union of these subsets:\n  \n \nmDP(B) = m1 \u2229\u00a92(B)+ \u2211 A\u2229X= /0, A\u222aX=B m1(X)m2(A) \u2200A \u2286 \u2126, A 6= /0 mDP( /0) = 0\n(11)\nConjunctive, disjunctive and Dempster\u2019s rules are associative and commutative, but Yager and Dubois and Prade\u2019s rules are not associative, even if they are commutative. Unfortunately, all combination rules described above are not idempotent because m \u2229\u00a9m 6= m and m \u222a\u00a9m 6= m.\nMean combination rule detailed in [6], mMean, of two mass functions m1 and m2 is the average of these ones. Therefore, for each focal element A of M mass functions, the combined one is defined as follows:\nmMean(A) = 1 M\nM\n\u2211 i=1 mi(A) (12)\nBesides idempotence, this combination rule verifies normality condition (m( /0) = 0) if combined mass functions are normalized (\u2200i \u2208 M, mi( /0) = 0). We note also that this combination rule is commutative but not associative.\nAll combination rules described above work under a strong assumption of cognitive independence since they are used to combine mass functions induced by two distinct sources. This strong assumption is always assumed but never verified. Denoeux [10], proposed a family of conjunctive and disjunctive rules based on triangular norms and conorms. Cautious and bold rules are members of that family and combine mass functions for which independence assumption is not verified. Cautious combination of two mass functions m1 and m2 issued from probably dependent sources is defined as follows:\nm1 \u2227\u00a9m2 = \u2229\u00a9A\u2282\u2126 A w1(A)\u2227w2(A) (13)\nWhere Aw1(A) and Aw2(A) are simple support functions focused on A with weights\nw1 and w2 issued from the canonical decomposition [15] of m1 and m2 respectively, note also that \u2227 is a min operator of simple support functions weights. The bold and cautious combination rules are commutative, associative and idempotent.\nTo summarize, the choice of the combination rule is based on the dependence\nof sources. Combination rules like [2, 5, 6, 7, 8] combine mass functions which sources are independent, whereas cautious, bold and mean rules are the most fitted to combine mass functions issued from dependent sources.\nIn this paper, we propose a method to quantify sources\u2019 degrees of independence that may be used in a new mixed combination rule. In fact, we propose a statistical approach to learn sources\u2019 degrees of independence from all provided evidential information. Indeed, two sets of evidential information assessed by two different sources are classified into two sets of clusters. Clusters of both sources are matched and the independence of each couple of matched clusters is quantified in order to estimate sources\u2019 degrees of independence. Therefore, a clustering technique is used to gather similar objects into the same cluster in order to study the source\u2019s overall behavior. Before introducing our learning method, we detail in the next section the evidential clustering algorithm that will be used in the learning of sources\u2019 degrees of independence."}, {"heading": "3. Evidential clustering", "text": "In this paper, we propose a new clustering technique to classify objects; their attributes values are evidential and classes are unknown. Proposed clustering algorithm uses a distance on belief functions given by Jousselme et al. [19] such as proposed by Ben Hariz et al. [20].\nBen Hariz et al. [20] detailed a belief K-modes classifier in which Jousselme distance [19] is adapted to quantify distances between objects and clusters modes. These are sets of mass functions; each one is the combination of an attribute\u2019s values of all objects classified into that cluster. An object is attributed to the cluster having the minimum distance to its mode.\nTemporal complexity of clustering algorithm proposed by Ben Hariz et al. [20]\nis quite high as clusters modes and distances are computed in each iteration. The combination by the mean rule to compute modes values leads to mass functions with a high number of focal elements. Hence, the bigger the cluster is, the least significant is the distance.\nWe propose a clustering technique to classify objects that attributes values are uncertain. However uncertainty is modeled with the theory of belief functions detailed in Section 2. In the proposed algorithm, we do not use any cluster mode to avoid the growth of focal elements number in clusters modes. Temporal complexity is also significantly reduced because all distances are computed only once.\nIn this section, K is the number of clusters Clk (1 \u2264 k \u2264 K); n is the number of objects to be classified; nk is the number of objects classified into cluster Clk; oi are objects to classify oi : 1 \u2264 i \u2264 n; c is the number of evidential attributes a j : 1 \u2264 j \u2264 c which domains are \u2126a j and finally mi j is a mass function value of attribute \u201c j\u201d for object \u201ci\u201d. Mass functions mi j can be certain, probabilistic, possibilistic, evidential and even missing.\nTo classify objects oi into K clusters, we use a clustering algorithm with a distance on belief functions given by [19]. The number of clusters K is assumed to be known. Proposed clustering technique is based on a distance which quantifies how much is far an object oi from a cluster Clk. This distance is the mean of distances between oi, and all objects oq that are classified into cluster Clk as follows:\nD(oi,Clk) = 1 nk\nnk\n\u2211 q=1 dist(oi,oq) (14)\nand\ndist(oi,oq) = 1 c\nc\n\u2211 j=1 d(mi j,mq j) (15)\nwith :\nd(mi j,mq j) =\n\u221a\n1 2 (mi j \u2212mq j)tD(mi j \u2212mq j) (16)\nsuch that :\nD(A,B) =\n \n 1 if A = B = /0 |A\u2229B| |A\u222aB| \u2200A,B \u2208 2 \u2126a j (17)\nEach object is affected to the most similar cluster in an iterative way till reaching an unchanged cluster partition. It is obvious that clusters number K must be known. Temporal complexity of the proposed algorithm is significantly optimized as pairwise distances are computed once a time from the beginning. We do not use any cluster mode. Consequently, there will be no problem of increasing number of focal elements because attributes values are not combined. Indeed, the evidential clustering algorithm provides a cluster partition that minimizes distances between objects into the same cluster and maximizes the distance between objects classified into different clusters. The main asset of the evidential clustering algorithm according to the belief K-modes proposed by Ben Hariz et al. [20] is the optimization of the temporal complexity. In fact, run-time of the evidential clustering algorithm is improved. The optimization of run-time depends on the size of the frame of discernment |\u2126a j |, the number of clusters K and number of objects n. For example, figure 1 shows a big gain in the run-time of evidential clustering according to the belief K-modes when the number of mass functions varies, n \u2208 [10,1000]. Temporal complexity of the evidential clustering algorithm is optimized and that optimization is especially noticed when the number of mass functions to classify is high and also when the frame of discernment contains many hypotheses. Thanks to the improve of the temporal complexity, this clustering algorithm is used in the following sections."}, {"heading": "4. Learning sources independence degree", "text": "In this section we extend paper [21] for many sources, and propose a combination rule emphasizing sources independence degree. In the theory of prob-\nabilities, two hypotheses X and Y are assumed to be statistically independent if P(X \u2229Y ) = P(X)\u00d7P(Y) or P(X |Y ) = P(X). In the context of the theory of belief functions, Shafer [4] defined cognitive and evidential independence.\nDefinition 1. \u201cTwo frames of discernment may be called cognitively independent with respect to the evidence if new evidence that bears on only one of them will not change the degree of support for propositions discerned by the other\u201d 1.\nThe cognitive independence is a weak independence; two variables are independent with respect to a mass function if new evidence that bears on only one of the two variables does not change propositions discerned by the other one. For two variables X and Y such that \u2126X and \u2126Y their domains (frames of discernment) and \u2126X \u00d7\u2126Y the product space of domains \u2126X and \u2126Y . Variables X and Y are cognitively independent with respect to m\u2126X\u00d7\u2126Y if:\npl\u2126X\u00d7\u2126Y (x,y) = pl\u2126X\u00d7\u2126Y\u2193\u2126X (x)\u00d7 pl\u2126X\u00d7\u2126Y \u2193\u2126Y (y) (18)\n1[4], page 149\nNote that \u2126X \u00d7\u2126Y \u2193 \u2126X is the marginalization of \u2126X \u00d7\u2126Y in \u2126X [7, 22]. Shafer [4] defined also a strong independence called evidential independence as follows:\nDefinition 2. \u201cTwo frames of discernment are evidentially independent with respect to a support function if that support function could be obtained by combining evidence that bears on only one of them with evidence that bears on only the other\u201d.\nTwo variables are evidentially independent if their joint mass function can be obtained by combining marginal mass functions that bears on each one of them. Variables X and Y are evidentially independent with respect to m\u2126X\u00d7\u2126Y if: \n\n\npl\u2126X\u00d7\u2126Y (x,y) = pl\u2126X\u00d7\u2126Y\u2193\u2126X (x)\u00d7 pl\u2126X\u00d7\u2126Y\u2193\u2126Y (y) bel\u2126X\u00d7\u2126Y (x,y) = bel\u2126X\u00d7\u2126Y\u2193\u2126X (x)\u00d7bel\u2126X\u00d7\u2126Y\u2193\u2126Y (y) (19)\nIndependence can also be defined in terms of irrelevance. The knowledge of the value of one variable does not change the belief on the other one. In the theory of belief functions, irrelevance is based on the conditioning. Variables X and Y are irrelevant with respect to m, IRm(X ,Y ) if the marginal mass function on X is obtained by conditioning the joint mass function on values y of Y and marginalizing this conditioned joint mass function on X :\nm\u2126X\u00d7\u2126Y\u2193\u2126X[y] (x) \u221d m \u2126X\u00d7\u2126Y \u2193\u2126X (x) (20)\nNote that proportionality \u221d is replaced by equality when m\u2126X\u00d7\u2126Y \u2193\u2126X[y] and m \u2126X\u00d7\u2126Y\u2193\u2126X are normalized.\nDoxastic independence is especially proposed in the theory of belief functions\nby [11, 12] and it is defined as follows: Definition 3. \u201cTwo variables are considered as doxastically independent only when they are irrelevant and this irrelevance is preserved under Dempster\u2019s rules of combination\u201d.\nIn other words, two variables X and Y are doxastically independent if they are irrelevant with respect to m\u2295m0 when they are irrelevant with respect to m and m0. Indeed, if X and Y are irrelevant according to any mass function m and if they are also irrelevant with respect to another mass function m0; they are assumed to be doxastically independent if they are irrelevant with respect to the orthogonal sum of m and m0 . Thus, if IRm(X ,Y ), IRm0(X ,Y ) and IRm\u2295m0(X ,Y ) is verified then X and Y are doxastically independent.\nThis paper is not focused on variables independence [11, 12, 4] but on sources independence. Sources independence is computed according to a set of different belief functions provided by each source separately. Sources are dependent when all their beliefs are correlated, there is a link between all mass functions they provide. This problem is not tackled till now, we noticed a lack of references treating this problem. To study sources independence, a great number of mass functions provided by both sources is needed. This set of mass functions must be defined on the same frame of discernment according to the same problems. For example, two distinct doctors provide n diagnoses in the examination of the same n patients. In that case, the frame of discernment contains all diseases and is already the same for both doctors. We define sources independence as follows:\nDefinition 4. Two sources are cognitively independent if they do not communicate and if their evidential corpora are different.\nDefinition 5. Evidential corpus is the set of all pieces of evidence held by a source.\nNot only communicating sources are considered dependent but also sources having the same background of knowledge since their beliefs are correlated. The aim of estimating sources independence is either to guide the choice of combination rules when aggregating their beliefs, or to integrate this degree of independence in a new combination rule.\nIn this paper, mass functions provided by two sources are studied in order to reveal any dependence between them. In the following, we define an independence measure Id, (Id(s1,s2)), as the independence of s1 on s2 verifying the following axioms:\n1. Non-negativity: The independence of a source s1 on another source s2, Id(s1,s2)\ncannot be negative, it is either positive or null.\n2. Normalization: The degree of independence Id is a degree over [0,1], it is\nnull when the first source is dependent on the second one, equal to 1 when it is completely independent and a degree from [0,1] otherwise.\n3. Non-symmetry: In the case where s1 is independent on s2, s2 is not necessar-\nily independent on s1. Even if s1 and s2 are mutually independent, degrees of independence are not necessarily equal.\n4. Identity: Any source is completely dependent on itself and Id(s1,s1) = 0.\nIf s1 and s2 are independent, there will be no correlation between their mass functions. The main idea of this paper is: First, classify mass functions provided by each source separately. Then, study similarities between cluster partitions to reveal any dependence between sources. By using clustering algorithm, sources overall behavior is studied. The proposed method is in three steps: First, mass functions of each source are classified. Then, similar clusters are matched. Finally, weights of linked clusters and sources independence are quantified."}, {"heading": "4.1. Clustering", "text": "Clustering algorithm detailed in Section 3 is used to classify two sets of n mass functions respectively provided by sources s1 and s2. Clustering algorithm is performed on all mass functions of s1 independently of the clustering performed on those of s2. We remind that all mass functions of both sources are defined on the\nsame frame of discernment and so considered as values of only one attribute when classifying their corresponding objects. For the same example of doctors, patients are objects to classify according to an attribute disease. Values of this attribute are mass functions defined on the frame of discernment enumerating all possible diseases. Distance (14) can be simplified as follows because we have only one attribute:\nD(oi,Clk) = 1 nk\nnk\n\u2211 q=1 d(mi,mq) (21)\nIn this paper, we fix the number of clusters to the number of hypotheses in the frame of discernment. In a classification point of view, number of hypotheses is the number of possible classes. For example, the frame of discernment of the attribute disease enumerates all possible diseases. Hence, when a doctor examines a patient, he gives a mass function as a classification of the patient in some possible diseases."}, {"heading": "4.2. Cluster matching", "text": "After clustering technique, both mass functions provided by s1 and s2 are distributed separately on K clusters. In this section, we try to find a mapping between clusters in order to link those containing the same objects. If clusters are perfectly linked, meaning all objects are classified similarly for both sources, we can conclude that sources are dependent as they are choosing similar focal elements (not contradictory at least) when providing mass functions for same objects. If clusters are weakly linked, sources choose similar focal elements for different objects and so they are independent. Clusters independence degree is proportional to the number of objects similarly classified. More clusters contain the same objects, more they are dependent as they are correlated.\nWe note Cl1k1 where 1 \u2264 k1 \u2264 K for clusters of s1 and Cl 2 k2 where 1 \u2264 k2 \u2264 K\nfor those of s2. The similarity between two clusters Cl1k1 and Cl 2 k2 is the proportion\nof objects simultaneously classified into Cl1k1 and Cl 2 k2 :\n\u03b2 ikik j = \u03b2 i(Cliki ,Cl j k j ) =\n|Cliki \u2229Cl j k j |\n|Cliki | (22)\nwith i, j \u2208 {1,2} and i 6= j. \u03b2 1k1k2 quantifies a proportion of objects classified\nsimultaneously in clusters Cl1k1 and Cl 2 k2 with regard to objects in Cl1k1 , analogically \u03b2 2k2k1 is a proportion of objects simultaneously in Cl 1 k1 and Cl2k2 with regard to those in Cl2k2 . Note that \u03b2 1 k1k2 6= \u03b2 2k2k1 since the number of objects classified into Cl 1 k1 and Cl2k2 are different (|Cl 1 k1 |6=|Cl2k2 |).\nWe remind that \u03b2 1 are similarities towards s1 and \u03b2 2 are those towards s2. It\nis obvious that \u03b2 i(Cliki ,Cl j k j ) = 0 when Cliki and Cl j k j do not contain any common object; however they are completely different. \u03b2 i(Cliki ,Cl j k j ) = 1 when these clusters are strongly similar so they contain the same objects. A similarity matrix M1 containing similarities of clusters of s1 according to those of s2 (\u03b2 1), and M2 the similarity matrix between clusters of s2 and those of s1 (\u03b2 2) are defined as follows:\nM1 =\n\n         \n\u03b2 111 \u03b2 112 . . . \u03b2 11K . . . . . . . . . . . . \u03b2 1k1 \u03b2 1k2 . . . \u03b2 1kK . . . . . . . . . . . . \u03b2 1K1 \u03b2 1K2 . . . \u03b2 1KK\n\n         \nand M2 =\n\n         \n\u03b2 211 \u03b2 212 . . . \u03b2 21K . . . . . . . . . . . . \u03b2 2k1 \u03b2 2k2 . . . \u03b2 2kK . . . . . . . . . . . . \u03b2 2K1 \u03b2 2K2 . . . \u03b2 2KK\n\n          (23)\nWe note that M1 and M2 are different since \u03b2 1k1k2 6= \u03b2 2 k2k1 . Clusters of s1 are matched to those of s2 according to maximum of \u03b2 1 such that each cluster Cl1k1 is linked to only one cluster Cl2k2 and each cluster Cl 2 k2 has only one cluster Cl1k1 linked to it. The idea is to link iteratively clusters having the maximal \u03b2 1 in M1 then eliminate these clusters and the corresponding line and column from the matrix until having a bijective cluster matching. Algorithm 1 details cluster matching process. We note that different matchings are obtained for s1 and s2 because M1 and M2\nare different. This algorithm is iterative and the number of iteration is equal to the\nAlgorithm 1 Cluster matching Require: Similarity matrix M .\n1: while M is not empty do 2: Find max(M) and indexes c and l of clusters having this maximal similarity.\n3: Map clusters l and c. 4: Delete line l and column c from M. 5: end while 6: return Cluster matching.\nnumber of clusters K. Even if this algorithm is quite simple, it provides a matching of clusters in order to compare evidential information provided by both sources. The assignment algorithm proposed in [23] for square matrices and that for rectangular matrices [24] can also be used to minimize the dissimilarity between matched clusters. Other methods for cluster matching [25] and [26] can also be used."}, {"heading": "4.3. Cluster independence", "text": "Once cluster matching is obtained, a degree of independence/dependence of matched clusters is quantified in this step. A set of matched clusters is obtained for both sources and a mass function can be used to quantify each couple of clusters independence. Assume that cluster Cl1k1 is matched to Cl 2 k2 , a mass function m\u2126I 2 defined on the frame of discernment \u2126I = {Dependent Dep, Independent Ind}\n2We note the frame of discernment in the mass functions to avoid confusion.\ndescribes how much this couple of clusters is independent or dependent as follows:\n    \n   \nm\u2126I ,ikik j (Dep) = \u03b1 i ki \u03b2 1kik j m\u2126I ,ikik j (Ind) = \u03b1 i ki (1\u2212\u03b2 1kik j ) m\u2126I ,ikik j (Dep\u222a Ind) = 1\u2212\u03b1 i ki\n(24)\nA mass function quantifies the degree of independence of each couple of clusters according to each source; m\u2126I ,ikik j is a mass function for the independence of each linked clusters Cliki and Cl j k j according to si with i, j \u2208 {1,2} and i 6= j. Coefficient \u03b1 iki is used to take into account of number of mass functions in each cluster Clki of the source i. Reliability factor \u03b1 iki is not the reliability of any source but it can be seen as the reliability of the clusters independence estimation. Consequently, independence estimation is more reliable when clusters contain enough mass functions. For example, assume two clusters; one containing only one mass function and the second one containing 100 mass functions. It is obvious that the independence estimation of the second cluster is more precise and significant than the independence estimation of the first one. Reliability factors \u03b1 iki are proportional to the number of hypotheses in the frame of discernment | \u2126 |, and the number of objects classified in Cliki as follows:\n\u03b1 iki = f (| \u2126 |, |Cl i ki |) (25)\nThe bigger | \u2126 | is, the more mass functions are needed to have a reliable cluster independence estimation. For example, if | \u2126 |= 5 then there are 25 possible focal elements, also independence estimation of a cluster containing 20 objects cannot be precise. No existing method to define such function f . Hence, we use simple heuristics as follows:\n\u03b1 iki = 1\u2212 1\n|Cliki | 1 |\u2126|\n(26)\nAs shown in figure 2, if | \u2126 | and number of mass functions in a cluster are big enough, cluster independence mass function is almost not discounted. Reliability factor is an increasing function of | \u2126 | and |Cliki | which favors big clusters 3."}, {"heading": "4.4. Sources independence", "text": "Obtained mass functions quantify each matched clusters independence according to each source. Therefore, K mass functions are obtained for each source such that each mass function quantifies the independence of each couple of matched clusters. The combination of K mass functions for each source using the mean, defined by equation (12), is a mass function m\u2126I defining the whole independence of one source on another one:\nm\u2126I ,si(A) = 1K\nK\n\u2211 ki=1 m\u2126I ,ikik j (A) \u2200A \u2286 2 \u2126 (27)\nWith k j is the cluster matched to ki according to si. Two different mass functions m\u2126I ,s1 and m\u2126I ,s2 are obtained for s1 and s2 respectively. We note that m\u2126I ,s1\n3Big clusters are those containing enough mass functions according to | \u2126 |.\nis the combination of K mass functions representing the independence of matched clusters according to s1 defined using equation (24). Mass functions m\u2126I ,s1 and m\u2126I ,s2 are different since cluster matchings are different which verifies the axiom of non-symmetry. \u03b2 1k1k2 ,\u03b2 2 k2k1 \u2208 [0,1] verify the non-negativity and the normalization axioms. Finally, pignistic probabilities are computed from these mass functions in order to decide about sources independence Id such that: \n\n\nId(s1,s2) = BetP(Ind) Id(s1,s2) = BetP(Dep) (28)\nIf Id(s1,s2) > Id(s1,s2) we claim that sources s1 and s2 are independent otherwise they are dependent."}, {"heading": "4.5. General case", "text": "The method detailed above estimates the independence of one source on another one. Independence measure is non-symmetric because if a source s1 is independent on a source s2 then s2 is not necessarily independent on s1 and even if it is the case, degrees of independence are not necessarily the same.\nIt is wise to choose the minimum independence from Id(s1,s2) and Id(s2,s1) as the overall independence. Consequently, if at least one of two sources is dependent on the other, then sources are considered dependent. In other words, two sources are independent only if they are mutually independent. Hence, overall independence that is denoted I(s1,s2) is given by:\nI(s1,s2) = min(Id(s1,s2), Id(s2,s1)) (29)\nWe note that I(s1,s2) is non-negative, normalized, symmetric and identical. We define an independence measure, noted I, generalizing the independence for more than two sources verifying the following axioms:\n1. Non-negativity: Many sources independence {s1,s2,s3, . . . ,sns}, noted\nId(s1,s2, . . . ,sns) cannot be negative, it is either positive or null.\n2. Normalization: Sources independence I is a degree in [0,1]. The minimum\n0 is reached when sources are completely dependent and the maximum 1 is reached when they are completely independent.\n3. Symmetry: I(s1,s2,s3, . . . ,sns) is the sources\u2019 overall independence and\nI(s1,s2,s3, . . . ,sns) = I(s2,s1,s3, . . . ,sns) = I(s3,s1,s2, . . . ,sns).\n4. Identity: I(s1,s1,s1) = 0. It is obvious that any source is completely depen-\ndent on itself.\n5. Increasing with inclusion: I(s1,s2) \u2264 I(s1,s2,s3), more there are sources,\nmore they are likely to be independent.\nTo compute the overall independence of ns sources {s1,s2, . . . ,sns}, independencies of pairs of sources are computed and the maximum4 independence is the sources overall independence:\nI(s1,s2, . . . ,sns) = max(I(si,s j)), \u2200i \u2208 [1,ns] , j \u2208]i,ns] (30)\nor equivalently:\nI(s1,s2, . . . ,sns) = max(min(Id(si,s j), Id(s j,si))), \u2200i, j \u2208 [1,ns] i 6= j (31)\nIndependence degree of sources is then integrated in the combination step using the following mixed combination rule."}, {"heading": "5. Combination rule", "text": "Combination rules using conjunctive and/or disjunctive rules such as [2, 5, 6, 7, 8] are used when sources are completely independent but cautious and bold\n4The maximum is used to insure the property of increasing with inclusion.\nrules [10] tolerate redundant information and consequently can be used to combine mass functions which sources are dependent. In the combination step, sources dependence or independence hypothesis is intuitively made without any possibility of check. Sources independence degree is neither 0 nor 1 but a level over [0,1]. The main question is \u201cwhich combination rule to use when combining partially independent\\dependent mass functions?\u201d\nIn this paper, we propose a new mixed combination rule using conjunctive and cautious rules detailed in equations (8) and (13). In the case of totally dependent sources (where independence is 0), the cautious and proposed mixed combination rules are similar; whereas in the case of totally independent sources (independence is 1), the conjunctive and proposed combination rules are similar. In the case of an independence degree in ]0,1[, combined mass function is the average of conjunctive and cautious combinations weighted by sources\u2019 independence degree.\nAssume that two sources s1 and s2 are independent with a degree \u03b3 such that \u03b3 = I(s1,s2); m1 and m2 are mass functions provided by s1 and s2. The proposed mixed combination rule is defined as follows:\nmMixed(A) = \u03b3 \u2217m \u2229\u00a9(A)+ (1\u2212 \u03b3)\u2217m \u2227\u00a9(A), \u2200A \u2286 \u2126 (32)\nThe degree of independence of a set of sources is given by equation (30), and the mixed combination of a set of mass functions {m1,m2, . . . ,mns} provided by sources {s1,s2, . . . ,sns} is also a weighted average such that:\n\u03b3 = I(s1,s2, . . . ,sns) (33)\nProperties of the proposed mixed combination rule:\n\u2022 Commutativity: Conjunctive and cautious rules are commutative. Indepen-\ndence measure is symmetric because sources\u2019 degree of independence is the same for a set of sources. Then the proposed rule is commutative.\n\u2022 Associativity: Conjunctive and cautious rule are associative but the proposed\nrule is not because independence degree of n sources and n+ 1 ones is not necessarily the same.\n\u2022 Idempotent: Degree of independence of one source to itself is 0, in that case\nthe proposed rule is equivalent to the cautious rule. As the cautious rule is idempotent, it is the case of the proposed mixed rule.\n\u2022 Neutral element: Mixed combination rule does not have any neutral element.\n\u2022 Absorbing element: No absorbing element also.\nExample. Assume a frame of discernment \u2126 = {a,b,c} and two sources s1 and s2 providing two mass functions m1 and m2. Table 1 illustrates conjunctive and cautious combinations as well as mixed combination in the cases where \u03b3 = 0, \u03b3 = 0.3, \u03b3 = 0.6 and \u03b3 = 1. When \u03b3 = 0, mixed and cautious combinations are equivalent; when \u03b3 = 1, mixed and conjunctive combinations are equivalent, otherwise it is a weighted average by \u03b3 \u2208]0,1[.\nFinally, to illustrate the proposed mixed combination rule and compare it to other combination rules, three mass functions are generated randomly using algorithm 2. These mass functions are combined with conjunctive, Dempster, Yager, disjunctive, cautious and mean combination rules. They are also combined with the mixed combination rule with different independence levels. Figure 3 illustrates distances5 between the mixed combination with several degrees of independence and combined mass functions using conjunctive, Dempster, Yager, disjunctive, cautious and mean combination rules. Distances between mixed combination with several independence degrees; and Yager, disjunctive, mean and\n5Jousselme distance detailed in equation (16).\nDempster\u2019s rules are linear and decreasing proportionally to \u03b3 ."}, {"heading": "6. Experiments", "text": "Because of the lack of real evidential data, we use generated mass functions to test the method detailed above. Moreover, it is difficult to simulate all situations with all possible combinations of focal elements for several degrees of independence between sources. First, we generate two sets of mass functions for two sources s1 and s2; then we illustrate for three sources."}, {"heading": "6.1. Generated data depiction", "text": "Generating sets of n mass functions for several sources depends on sources\nindependence. We discern cases of independent and dependent sources."}, {"heading": "6.1.1. Independent sources", "text": "In general, to generate mass functions some information are needed: the number of hypotheses in the frame of discernment, | \u2126 | and the number of mass functions. We note that number of focal elements, and masses are chosen randomly.\nIn the case of independent sources, masses can be anywhere and focal elements of both sources are chosen independently. Mass functions of s1 and s2 are generated following algorithm 2. We note that focal elements, their number and BBMs are chosen randomly according to the universal low.\nAlgorithm 2 Independent mass functions generating Require: |\u2126|, n : number of mass functions\n1: for i = 1 to n do 2: Choose randomly | F |, the number of focal elements on [1, |2\u2126|]. 3: Choose randomly | F | focal elements noted F . 4: Divvy the interval [0,1] into |F| continuous sub-intervals. 5: Focal elements BBMs are intervals sizes. 6: end for 7: return n mass functions"}, {"heading": "6.1.2. Dependent sources", "text": "The case of dependent sources is a bit difficult to simulate as several scenarios\ncan occur. In this section, we will try to illustrate the most common situations. Generated mass functions for dependent sources are supposed to be consistent and do not enclose any internal conflict [27]. Consistent mass functions contain at least one focal element common to all focal sets. Figure 4 illustrates a consistent mass function where all focal elements {A, B, C, D} intersect.\nAlgorithm 3 generates a set of n consistent mass functions6 defined on a frame of discernment of size | \u2126 |. In the case of dependent sources, they are almost consistent and at least one of them is dependent on the other. To simulate the case where one source is dependent on another one, consistent mass functions of the first one are generated following algorithm 3, then those of the second source are generated knowing decisions of the first one. Algorithm 4 generates a set of mass functions that are dependent on another set of mass functions. Dependence is due to the knowledge of other source\u2019s decisions.\n6Conflict within such mass functions is null.\nAlgorithm 3 Consistent mass functions generating Require: |\u2126|, n : number of mass functions\n1: for i = 1 to n do 2: Choose randomly a focal set \u03c9i (it can be a single point) from \u2126. 3: Find the set S of all focal sets including \u03c9i. 4: Choose randomly | F |, the number of focal elements on [1, |S|]. 5: Choose randomly | F | focal elements from S noted F . 6: Divvy the interval [0,1] into |F| continuous sub-intervals. 7: BBMs of focal elements are intervals sizes. 8: end for 9: return n consistent mass functions"}, {"heading": "6.2. Results of tests", "text": "Algorithms detailed in the previous section are used to test some cases of sources\u2019 dependence and independence. We note that in extreme cases where mass functions are certain or even when focal elements do not intersect; maximal values of independence are obtained. In the case of perfect dependence; mass functions have the same focal elements; however, clusters contain mass functions with consistent focal elements. Clustering is performed according to focal elements and clusters are perfectly linked."}, {"heading": "6.2.1. Independent sources", "text": "In this paragraph, mass functions are independent. Focal elements and BBMs are randomly chosen ensuing algorithm 2. For tests, we choose | \u2126 |= 5 which is considered as medium-sized frame of discernment and n = 100. Table 2 illustrates the mean of 100 tests in the case of independent sources. The mean of 100 tests for two dependent sources yields to a degree of independence \u03b3 = 0.68, thus sources are independent. Assume that m1 and m2, given in table 1, are provided by two\nAlgorithm 4 Dependent mass functions generating Require: |\u2126|, n : number of mass functions, d decision of another source\n1: for i = 1 to n do 2: Find the set S of all focal sets including d. 3: Choose randomly | F |, the number of focal elements on [1, |S|]. 4: Choose randomly | F | focal elements from S noted F . 5: Divvy the interval [0,1] into |F| continuous sub-intervals. 6: Focal elements BBMs are intervals sizes. 7: end for 8: return n consistent mass functions\nsources s1 and s2 which independence degree is given in table 2. Combination of m1 and m2 is given in table 3.\nTo illustrate the case of three independent sources, three sets of 100 independent mass functions are generated following algorithm 2 with | \u2126 |= 5. The mean of 100 tests are illustrated in table 4."}, {"heading": "6.2.2. Dependent sources", "text": "In the case of dependent sources, mass functions are generated ensuing algorithms 3 and 4. For tests, we choose | \u2126 |= 5 and n = 100. We generate 100 mass\nfunctions of both s1 and s2 for 100 times and then compute the average of Id(s1,s2), Id(s2,s1) and I(s1,s2). Table 2 illustrates the mean of 100 independence degrees of two dependent sources providing each one 100 randomly generated mass functions. These sources are dependent with a degree 1\u2212 \u03b3 = 0.66. In table 3, m1 and m2 are combined using the mixed rule when \u03b3 = 0.34. To illustrate the case of three dependent sources, three sets of 100 dependent mass functions are generated following algorithms 3 and 4 when | \u2126 |= 5. The mean of 100 degrees of independence are illustrated in table 5.\nFinally, assume that m1, m2 and m3 of table 6 are three mass functions defined on a frame of discernment \u2126 = {a,b,c} and provided by three dependent sources. The mixed combined mass function when their degree of independence is \u03b3 = 0.35 is also given in table 6."}, {"heading": "7. Conclusion", "text": "In this paper, we proposed a method to learn sources cognitive independence in order to use the appropriate combination rule either when sources are cognitively dependent or independent. Sources are cognitively independent if they are different; not communicating and they have distinct evidential corpora. The proposed statistical approach is based on a clustering algorithm applied to mass functions provided by several sources. A pair of sources independence is deduced from weights of linked clusters after a matching of their clusters. Independence degree of sources can either guide the choice of the combination rule if it is either 1 or 0; when it is a degree over ]0,1[, we propose a new combination rule that weights the conjunctive and cautious combinations with sources\u2019 independence degree."}], "references": [{"title": "Fuzzy sets", "author": ["L.A. Zadeh"], "venue": "Information and Control 8 (3) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1965}, {"title": "Representation and combination of uncertainty with belief functions and possibility measures", "author": ["D. Dubois", "H. Prade"], "venue": "Computational Intelligence 4 (3) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1988}, {"title": "Upper and lower probabilities induced by a multivalued mapping", "author": ["A.P. Dempster"], "venue": "The Annals of Mathematical Statistics 38 (2) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1967}, {"title": "A mathematical theory of evidence", "author": ["G. Shafer"], "venue": "Princeton University Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1976}, {"title": "Toward a combination rule to deal with partial conflict and specificity in belief functions theory", "author": ["A. Martin", "C. Osswald"], "venue": "in: International Conference on Information Fusion, Qu\u00e9bec, Canada", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Combining belief functions when evidence conflicts", "author": ["C.K. Murphy"], "venue": "Decision Support Systems 29 (1) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "The transferable belief model", "author": ["P. Smets", "R. Kennes"], "venue": "Artificial Intelligence 66 (2) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1994}, {"title": "On the Dempster-Shafer framework and new combination rules", "author": ["R.R. Yager"], "venue": "Information Sciences 41 (2) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1987}, {"title": "How to preserve the conflict as an alarm in the combination of belief functions", "author": ["E. Lef\u00e8vre", "Z. Elouedi"], "venue": "Decision Support Systems 56 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Conjunctive and disjunctive combination of belief functions induced by nondistinct bodies of evidence", "author": ["T. Den\u0153ux"], "venue": "Artificial Intelligence 172 (2-3) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Belief function independence: I", "author": ["B. Ben Yaghlane", "P. Smets", "K. Mellouli"], "venue": "The marginal case, International Journal of Approximate Reasoning 29 (1) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Belief function independence: II", "author": ["B. Ben Yaghlane", "P. Smets", "K. Mellouli"], "venue": "The conditional case, International Journal of Approximate Reasoning 31 (1- 2) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Belief functions: The disjunctive rule of combination and the generalized Bayesian theorem", "author": ["P. Smets"], "venue": "International Journal of Approximate Reasoning 9 (1) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "The combination of evidence in the transferable belief model", "author": ["P. Smets"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 12 (5) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "The canonical decomposition of a weighted belief", "author": ["P. Smets"], "venue": "in: International Joint Conference on Artificial Intelligence, Vol. 2, Morgan Kaufman, Montr\u00e9al, Qu\u00e9bec, Canada", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "A mathematical theory of evidence (book review)", "author": ["L.A. Zadeh"], "venue": "AI magazine 5 (3) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1984}, {"title": "The nature of the unnormalized beliefs encountered in the transferable belief model", "author": ["P. Smets"], "venue": "in: D. Dubois, M. P. Wellman (Eds.), International conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann, Stanford, California, USA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "Conflict measure for the discounting operation on belief functions", "author": ["A. Martin", "A.-L. Jousselme", "C. Osswald"], "venue": "in: International Conference on Information Fusion, Cologne, Germany", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "A new distance between two bodies of evidence", "author": ["A.-L. Jousselme", "D. Grenier", "E. Boss\u00e9"], "venue": "Information Fusion 2 (2) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Clustering approach using belief function theory", "author": ["S. Ben Hariz", "Z. Elouedi", "K. Mellouli"], "venue": "in: J. Euzenat, J. Domingue (Eds.), 7th Conference of the European Society for Fuzzy Logic and Technology, Vol. 4183 of Lecture Notes in Computer Science, Atlantis Press, Varna, Bulgaria", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "About sources dependence in the theory of belief functions", "author": ["M. Chebbah", "A. Martin", "B. Ben Yaghlane"], "venue": "in: T. Den\u0153ux, M.-H. Masson (Eds.), International Conference on Belief Functions, Vol. 164 of Advances in Intelligent and Soft Computing, Springer Berlin Heidelberg, Compi\u00e8gne, France", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Uncertainty Management in Information Systems: From Needs to Solutions", "author": ["P. Smets", "R. Kruse"], "venue": "Springer US, Boston", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Algorithms for the Assignment and Transportation Problems", "author": ["J. Munkres"], "venue": "Journal of the Society for Industrial and Applied Mathematics 5 (1) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1957}, {"title": "An Extension of the Munkers Algorithm for the Assignement Problem to Rectangular Matrices", "author": ["F. Bourgeois", "J.-C. Lassalle"], "venue": "Communication of the ACM 12 (14) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1971}, {"title": "A multi-view voting method to combine unsupervised classifications", "author": ["C. Wemmert", "P. Gan\u00e7arski"], "venue": "in: IASTED International Conference on Artificial Intelligence and Applications, M\u00e1laga, Spain", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "Collaborative multi-strategy classification: Application to per-pixel analysis of images", "author": ["P. Gan\u00e7arski", "C. Wemmert"], "venue": "in: International Workshop on Mul- 34  timedia Data Mining: Mining Integrated Media and Complex Data, Chicago, Illinois, USA", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Conflicts within and between belief functions", "author": ["M. Daniel"], "venue": "in: IPMU", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Introduction Uncertainty theories like the theory of probabilities, the theory of fuzzy sets [1], the theory of possibilities [2] and the theory of belief functions [3, 4] model and manage uncertain data.", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "Introduction Uncertainty theories like the theory of probabilities, the theory of fuzzy sets [1], the theory of possibilities [2] and the theory of belief functions [3, 4] model and manage uncertain data.", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "Introduction Uncertainty theories like the theory of probabilities, the theory of fuzzy sets [1], the theory of possibilities [2] and the theory of belief functions [3, 4] model and manage uncertain data.", "startOffset": 165, "endOffset": 171}, {"referenceID": 3, "context": "Introduction Uncertainty theories like the theory of probabilities, the theory of fuzzy sets [1], the theory of possibilities [2] and the theory of belief functions [3, 4] model and manage uncertain data.", "startOffset": 165, "endOffset": 171}, {"referenceID": 1, "context": "In the theory of belief functions, many combination rules are proposed, some of them like [2, 5, 6, 7, 8, 9] are fitted to the aggregation of evidential information provided by cognitively independent sources whereas the cautious, bold [10] and mean combination rules can be applied when sources are cognitively dependent.", "startOffset": 90, "endOffset": 108}, {"referenceID": 4, "context": "In the theory of belief functions, many combination rules are proposed, some of them like [2, 5, 6, 7, 8, 9] are fitted to the aggregation of evidential information provided by cognitively independent sources whereas the cautious, bold [10] and mean combination rules can be applied when sources are cognitively dependent.", "startOffset": 90, "endOffset": 108}, {"referenceID": 5, "context": "In the theory of belief functions, many combination rules are proposed, some of them like [2, 5, 6, 7, 8, 9] are fitted to the aggregation of evidential information provided by cognitively independent sources whereas the cautious, bold [10] and mean combination rules can be applied when sources are cognitively dependent.", "startOffset": 90, "endOffset": 108}, {"referenceID": 6, "context": "In the theory of belief functions, many combination rules are proposed, some of them like [2, 5, 6, 7, 8, 9] are fitted to the aggregation of evidential information provided by cognitively independent sources whereas the cautious, bold [10] and mean combination rules can be applied when sources are cognitively dependent.", "startOffset": 90, "endOffset": 108}, {"referenceID": 7, "context": "In the theory of belief functions, many combination rules are proposed, some of them like [2, 5, 6, 7, 8, 9] are fitted to the aggregation of evidential information provided by cognitively independent sources whereas the cautious, bold [10] and mean combination rules can be applied when sources are cognitively dependent.", "startOffset": 90, "endOffset": 108}, {"referenceID": 8, "context": "In the theory of belief functions, many combination rules are proposed, some of them like [2, 5, 6, 7, 8, 9] are fitted to the aggregation of evidential information provided by cognitively independent sources whereas the cautious, bold [10] and mean combination rules can be applied when sources are cognitively dependent.", "startOffset": 90, "endOffset": 108}, {"referenceID": 9, "context": "In the theory of belief functions, many combination rules are proposed, some of them like [2, 5, 6, 7, 8, 9] are fitted to the aggregation of evidential information provided by cognitively independent sources whereas the cautious, bold [10] and mean combination rules can be applied when sources are cognitively dependent.", "startOffset": 236, "endOffset": 240}, {"referenceID": 10, "context": "Some researches are focused on doxastic independence of variables such as [11, 12]; others [4, 13] tackled cognitive and evidential independence of variables.", "startOffset": 74, "endOffset": 82}, {"referenceID": 11, "context": "Some researches are focused on doxastic independence of variables such as [11, 12]; others [4, 13] tackled cognitive and evidential independence of variables.", "startOffset": 74, "endOffset": 82}, {"referenceID": 3, "context": "Some researches are focused on doxastic independence of variables such as [11, 12]; others [4, 13] tackled cognitive and evidential independence of variables.", "startOffset": 91, "endOffset": 98}, {"referenceID": 12, "context": "Some researches are focused on doxastic independence of variables such as [11, 12]; others [4, 13] tackled cognitive and evidential independence of variables.", "startOffset": 91, "endOffset": 98}, {"referenceID": 13, "context": "The proposed combination rule is weighted with that degree of independence leading to the conjunctive rule [14] when sources are fully independent and to the cautious rule [10] when they are fully dependent.", "startOffset": 107, "endOffset": 111}, {"referenceID": 9, "context": "The proposed combination rule is weighted with that degree of independence leading to the conjunctive rule [14] when sources are fully independent and to the cautious rule [10] when they are fully dependent.", "startOffset": 172, "endOffset": 176}, {"referenceID": 2, "context": "Theory of belief functions The theory of belief functions was introduced by Dempster [3] and formalized by Shafer [4] to model imperfect data.", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "Theory of belief functions The theory of belief functions was introduced by Dempster [3] and formalized by Shafer [4] to model imperfect data.", "startOffset": 114, "endOffset": 117}, {"referenceID": 0, "context": "The basic belief assignment (BBA) commonly called mass function is a function defined on the power set 2\u03a9 and spans the interval [0,1] such that: \u2211 A\u2286\u03a9 m(A) = 1 (1) A basic belief mass (BBM) also called mass, m(A), is a degree of faith on the truth of A.", "startOffset": 129, "endOffset": 134}, {"referenceID": 6, "context": "The BBM, m(A), is a degree of belief on A which can be committed to its subsets if further information justifies it [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 3, "context": "Shafer [4] assumed a normality condition such that m( / 0) = 0, thereafter Smets [14] relaxed this condition in order to tolerate m( / 0)> 0.", "startOffset": 7, "endOffset": 10}, {"referenceID": 13, "context": "Shafer [4] assumed a normality condition such that m( / 0) = 0, thereafter Smets [14] relaxed this condition in order to tolerate m( / 0)> 0.", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "Where A is a focus of that simple support function and w \u2208 [0,1] is its weight.", "startOffset": 59, "endOffset": 64}, {"referenceID": 14, "context": "Therefore, any nondogmatic mass function can be decomposed into several support functions using the canonical decomposition proposed by Smets [15].", "startOffset": 142, "endOffset": 146}, {"referenceID": 1, "context": "There is a great number of combination rules [2, 5, 6, 7, 8, 9], but we enumerate in this section only Dempster, conjunctive, disjunctive, Yager, Dubois and Prade, mean, cautious and bold combination rules.", "startOffset": 45, "endOffset": 63}, {"referenceID": 4, "context": "There is a great number of combination rules [2, 5, 6, 7, 8, 9], but we enumerate in this section only Dempster, conjunctive, disjunctive, Yager, Dubois and Prade, mean, cautious and bold combination rules.", "startOffset": 45, "endOffset": 63}, {"referenceID": 5, "context": "There is a great number of combination rules [2, 5, 6, 7, 8, 9], but we enumerate in this section only Dempster, conjunctive, disjunctive, Yager, Dubois and Prade, mean, cautious and bold combination rules.", "startOffset": 45, "endOffset": 63}, {"referenceID": 6, "context": "There is a great number of combination rules [2, 5, 6, 7, 8, 9], but we enumerate in this section only Dempster, conjunctive, disjunctive, Yager, Dubois and Prade, mean, cautious and bold combination rules.", "startOffset": 45, "endOffset": 63}, {"referenceID": 7, "context": "There is a great number of combination rules [2, 5, 6, 7, 8, 9], but we enumerate in this section only Dempster, conjunctive, disjunctive, Yager, Dubois and Prade, mean, cautious and bold combination rules.", "startOffset": 45, "endOffset": 63}, {"referenceID": 8, "context": "There is a great number of combination rules [2, 5, 6, 7, 8, 9], but we enumerate in this section only Dempster, conjunctive, disjunctive, Yager, Dubois and Prade, mean, cautious and bold combination rules.", "startOffset": 45, "endOffset": 63}, {"referenceID": 2, "context": "The first combination rule was proposed by Dempster in [3] to combine two distinct mass functions m1 and m2 as follows:", "startOffset": 55, "endOffset": 58}, {"referenceID": 15, "context": "In order to solve the problem highlighted by Zadeh\u2019s counter example [16]", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "Smets [14] proposed an open world where a positive mass can be allocated to the empty set.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "Even if Smets [17] interpreted the BBM, m1 \u2229 \u00a92( / 0), as an amount of conflict between evidences that induced m1 and m2; that amount is not really a conflict because it includes a certain degree of auto-conflict due to the non-idempotence of the conjunctive combination [18].", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "Even if Smets [17] interpreted the BBM, m1 \u2229 \u00a92( / 0), as an amount of conflict between evidences that induced m1 and m2; that amount is not really a conflict because it includes a certain degree of auto-conflict due to the non-idempotence of the conjunctive combination [18].", "startOffset": 271, "endOffset": 275}, {"referenceID": 13, "context": "Smets [14] proposed also to use a disjunctive combination when an unknown source is unreliable.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "The disjunctive rule of combination is defined for two BBAs m1 and m2 as follows: m1 \u222a \u00a92(A) = (m1 \u222a \u00a9m2)(A) = \u2211 B\u222aC=A m1(B)\u00d7m2(C) (9) Yager in [8] interpreted m( / 0) as an amount of ignorance; consequently it is allocated to \u03a9.", "startOffset": 144, "endOffset": 147}, {"referenceID": 1, "context": "Dubois and Prade\u2019s solution [2] was to affect the mass resulting from the combination of conflicting focal elements to the union of these subsets:", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Mean combination rule detailed in [6], mMean, of two mass functions m1 and m2 is the average of these ones.", "startOffset": 34, "endOffset": 37}, {"referenceID": 9, "context": "Denoeux [10], proposed a family of conjunctive and disjunctive rules based on triangular norms and conorms.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "m1 \u2227 \u00a9m2 = \u2229 \u00a9A\u2282\u03a9 A w1(A)\u2227w2(A) (13) Where Aw1(A) and Aw2(A) are simple support functions focused on A with weights w1 and w2 issued from the canonical decomposition [15] of m1 and m2 respectively, note also that \u2227 is a min operator of simple support functions weights.", "startOffset": 166, "endOffset": 170}, {"referenceID": 1, "context": "Combination rules like [2, 5, 6, 7, 8] combine mass functions which sources are independent, whereas cautious, bold and mean rules are the most fitted to combine mass functions issued from dependent sources.", "startOffset": 23, "endOffset": 38}, {"referenceID": 4, "context": "Combination rules like [2, 5, 6, 7, 8] combine mass functions which sources are independent, whereas cautious, bold and mean rules are the most fitted to combine mass functions issued from dependent sources.", "startOffset": 23, "endOffset": 38}, {"referenceID": 5, "context": "Combination rules like [2, 5, 6, 7, 8] combine mass functions which sources are independent, whereas cautious, bold and mean rules are the most fitted to combine mass functions issued from dependent sources.", "startOffset": 23, "endOffset": 38}, {"referenceID": 6, "context": "Combination rules like [2, 5, 6, 7, 8] combine mass functions which sources are independent, whereas cautious, bold and mean rules are the most fitted to combine mass functions issued from dependent sources.", "startOffset": 23, "endOffset": 38}, {"referenceID": 7, "context": "Combination rules like [2, 5, 6, 7, 8] combine mass functions which sources are independent, whereas cautious, bold and mean rules are the most fitted to combine mass functions issued from dependent sources.", "startOffset": 23, "endOffset": 38}, {"referenceID": 18, "context": "[19] such as proposed by Ben Hariz et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] detailed a belief K-modes classifier in which Jousselme distance [19] is adapted to quantify distances between objects and clusters modes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] detailed a belief K-modes classifier in which Jousselme distance [19] is adapted to quantify distances between objects and clusters modes.", "startOffset": 70, "endOffset": 74}, {"referenceID": 19, "context": "[20]", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "To classify objects oi into K clusters, we use a clustering algorithm with a distance on belief functions given by [19].", "startOffset": 115, "endOffset": 119}, {"referenceID": 19, "context": "[20] is the optimization of the temporal complexity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "For example, figure 1 shows a big gain in the run-time of evidential clustering according to the belief K-modes when the number of mass functions varies, n \u2208 [10,1000].", "startOffset": 158, "endOffset": 167}, {"referenceID": 20, "context": "Learning sources independence degree In this section we extend paper [21] for many sources, and propose a combination rule emphasizing sources independence degree.", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "Figure 1: Run-time optimization of the evidential clustering and the belief K-modes [20] according to n \u2208 [10,1000], | \u03a9a j |= 5 and K = 5", "startOffset": 84, "endOffset": 88}, {"referenceID": 9, "context": "Figure 1: Run-time optimization of the evidential clustering and the belief K-modes [20] according to n \u2208 [10,1000], | \u03a9a j |= 5 and K = 5", "startOffset": 106, "endOffset": 115}, {"referenceID": 3, "context": "In the context of the theory of belief functions, Shafer [4] defined cognitive and evidential independence.", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "1[4], page 149", "startOffset": 1, "endOffset": 4}, {"referenceID": 6, "context": "Note that \u03a9X \u00d7\u03a9Y \u2193 \u03a9X is the marginalization of \u03a9X \u00d7\u03a9Y in \u03a9X [7, 22].", "startOffset": 61, "endOffset": 68}, {"referenceID": 21, "context": "Note that \u03a9X \u00d7\u03a9Y \u2193 \u03a9X is the marginalization of \u03a9X \u00d7\u03a9Y in \u03a9X [7, 22].", "startOffset": 61, "endOffset": 68}, {"referenceID": 3, "context": "Shafer [4] defined also a strong independence called evidential independence as follows: Definition 2.", "startOffset": 7, "endOffset": 10}, {"referenceID": 10, "context": "Doxastic independence is especially proposed in the theory of belief functions by [11, 12] and it is defined as follows: Definition 3.", "startOffset": 82, "endOffset": 90}, {"referenceID": 11, "context": "Doxastic independence is especially proposed in the theory of belief functions by [11, 12] and it is defined as follows: Definition 3.", "startOffset": 82, "endOffset": 90}, {"referenceID": 10, "context": "This paper is not focused on variables independence [11, 12, 4] but on sources independence.", "startOffset": 52, "endOffset": 63}, {"referenceID": 11, "context": "This paper is not focused on variables independence [11, 12, 4] but on sources independence.", "startOffset": 52, "endOffset": 63}, {"referenceID": 3, "context": "This paper is not focused on variables independence [11, 12, 4] but on sources independence.", "startOffset": 52, "endOffset": 63}, {"referenceID": 0, "context": "Normalization: The degree of independence Id is a degree over [0,1], it is null when the first source is dependent on the second one, equal to 1 when it is completely independent and a degree from [0,1] otherwise.", "startOffset": 62, "endOffset": 67}, {"referenceID": 0, "context": "Normalization: The degree of independence Id is a degree over [0,1], it is null when the first source is dependent on the second one, equal to 1 when it is completely independent and a degree from [0,1] otherwise.", "startOffset": 197, "endOffset": 202}, {"referenceID": 22, "context": "The assignment algorithm proposed in [23] for square matrices and that for rectangular matrices [24] can also be used to minimize the dissimilarity between matched clusters.", "startOffset": 37, "endOffset": 41}, {"referenceID": 23, "context": "The assignment algorithm proposed in [23] for square matrices and that for rectangular matrices [24] can also be used to minimize the dissimilarity between matched clusters.", "startOffset": 96, "endOffset": 100}, {"referenceID": 24, "context": "Other methods for cluster matching [25] and [26] can also be used.", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "Other methods for cluster matching [25] and [26] can also be used.", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "\u03b2 1 k1k2 ,\u03b2 2 k2k1 \u2208 [0,1] verify the non-negativity and the normalization axioms.", "startOffset": 21, "endOffset": 26}, {"referenceID": 0, "context": "Normalization: Sources independence I is a degree in [0,1].", "startOffset": 53, "endOffset": 58}, {"referenceID": 1, "context": "Combination rule Combination rules using conjunctive and/or disjunctive rules such as [2, 5, 6, 7, 8] are used when sources are completely independent but cautious and bold", "startOffset": 86, "endOffset": 101}, {"referenceID": 4, "context": "Combination rule Combination rules using conjunctive and/or disjunctive rules such as [2, 5, 6, 7, 8] are used when sources are completely independent but cautious and bold", "startOffset": 86, "endOffset": 101}, {"referenceID": 5, "context": "Combination rule Combination rules using conjunctive and/or disjunctive rules such as [2, 5, 6, 7, 8] are used when sources are completely independent but cautious and bold", "startOffset": 86, "endOffset": 101}, {"referenceID": 6, "context": "Combination rule Combination rules using conjunctive and/or disjunctive rules such as [2, 5, 6, 7, 8] are used when sources are completely independent but cautious and bold", "startOffset": 86, "endOffset": 101}, {"referenceID": 7, "context": "Combination rule Combination rules using conjunctive and/or disjunctive rules such as [2, 5, 6, 7, 8] are used when sources are completely independent but cautious and bold", "startOffset": 86, "endOffset": 101}, {"referenceID": 9, "context": "rules [10] tolerate redundant information and consequently can be used to combine mass functions which sources are dependent.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "Sources independence degree is neither 0 nor 1 but a level over [0,1].", "startOffset": 64, "endOffset": 69}, {"referenceID": 0, "context": "4: Divvy the interval [0,1] into |F| continuous sub-intervals.", "startOffset": 22, "endOffset": 27}, {"referenceID": 26, "context": "Generated mass functions for dependent sources are supposed to be consistent and do not enclose any internal conflict [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "6: Divvy the interval [0,1] into |F| continuous sub-intervals.", "startOffset": 22, "endOffset": 27}, {"referenceID": 0, "context": "5: Divvy the interval [0,1] into |F| continuous sub-intervals.", "startOffset": 22, "endOffset": 27}], "year": 2015, "abstractText": "The theory of belief functions manages uncertainty and also proposes a set of combination rules to aggregate opinions of several sources. Some combination rules mix evidential information where sources are independent; other rules are suited to combine evidential information held by dependent sources. In this paper we have two main contributions: First we suggest a method to quantify sources\u2019 degree of independence that may guide the choice of the more appropriate set of combination rules. Second, we propose a new combination rule that takes consideration of sources\u2019 degree of independence. The proposed method is illustrated on generated mass functions.", "creator": "LaTeX with hyperref package"}}}