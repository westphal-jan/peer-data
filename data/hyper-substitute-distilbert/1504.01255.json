{"id": "1504.01255", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2015", "title": "Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding", "abstract": "coding literature presents a theoretical analysis termed multi - view embedding - - where embedding material can differ characterized from unlabeled material or the task simulator projecting first view from another. we prove analytical limitation involves teaching learning under certain conditions. numerical result explains the effectiveness the assessing existing programs such as word embedding. improved on this theory, teams propose a formal semi - dependent developmental facility that learns a multi - view embedding including small incomplete object - convolutional neural networks. the principles derived after this perspective outperforms state - thru - the - computer modeling on sentiment interpretation and topic discovery.", "histories": [["v1", "Mon, 6 Apr 2015 10:42:07 GMT  (251kb,D)", "http://arxiv.org/abs/1504.01255v1", null], ["v2", "Thu, 24 Sep 2015 11:32:44 GMT  (184kb,D)", "http://arxiv.org/abs/1504.01255v2", "The older version has a different title, and the results there are obsolete. The current version is to appear in NIPS 2015"], ["v3", "Sun, 1 Nov 2015 15:26:16 GMT  (184kb,D)", "http://arxiv.org/abs/1504.01255v3", "v1 has a different title, and the results there are obsolete. The current version is to appear in NIPS 2015"]], "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG", "authors": ["rie johnson", "tong zhang 0001"], "accepted": true, "id": "1504.01255"}, "pdf": {"name": "1504.01255.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Learning with Multi-View Embedding: Theory and Application with Convolutional Neural Networks", "authors": ["Rie Johnson", "Tong Zhang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "While machine learning methods have been successfully applied to NLP tasks, supervised learning methods require labeled training data, which is costly to obtain. On the other hand, unlabeled text data is often available in large amounts with little cost. Therefore, semi-supervised methods, which can make use of unlabeled data as well as labeled data, have an advantage. Among a number of semi-supervised methods that have been proposed [5, 23], there are two notable types of method. One involves guessing labels of unlabeled data, either explicitly or implicitly, e.g., EM, co-training [4], and transductive SVM [9]. Although this type of method can produce impressive performance, there is a risk that unlabeled data hurts performance via label contamination if the data violates the assumptions underlying the methods.\nAnother type, which we focus on in this work, learns a feature embedding from unlabeled data for use in supervised learning. Here we use the term embedding loosely to mean a structure-preserving function applied to features. When the features generated by the embedding are used as additional features, this approach has a low risk of performance degradation since if the generated features are useless, they would be mostly ignored by the supervised learner. In NLP, an empirically successful example is word embedding learned from unlabeled data through the tasks that essentially predict the neighboring words from words [3, 6, 20, 19]. When used as additional features, the obtained word vectors often improve the performance of supervised NLP systems [21]. Theoretically, this approach is related to two-view feature learning by [2], which originated from alternating structure optimization [1]. It learns a linear embedding from unlabeled data through auxiliary tasks such as predicting a word from the features associated with its surrounding words. The learned embedding improved the performance of NLP tasks such as named entity chunking.\nWord vector learning is often motivated intuitively; e.g., the expectation is that the word prediction training would place the vectors for similar words (appearing in similar contexts) close to each other. Though it is convincing, some more theoretical insight may be helpful for further development. [14] has shown that\nar X\niv :1\n50 4.\n01 25\n5v 1\n[ st\nat .M\nL ]\n6 A\npr 2\n01 5\none instance of word2vec [18] is implicit factorization of a word-context matrix of shifted PMI; however, [14] did not show why factorization of this matrix would be useful. [2] has given theoretical justification of two-view feature learning, but this analysis is limited to the linear case where SVD is used.\nGiven the above context, one of the purposes of this paper is to present a theoretical analysis of multiview embedding (defined later), which is obtained through unsupervised training on the task of predicting one view from another. We prove that under certain conditions multi-view embeddings are useful in supervised tasks. This analysis can be regarded as a generalization of [2] to non-linear cases, allowing it to handle neural network-based word embeddings and its extensions to text region embedding investigated in this work.\nOur theoretical result naturally leads to a semi-supervised learning framework, which requires multiple views of data. While the availability of such views are problem specific, there are classes of models that come with natural definitions of views built into the model structure. Convolutional neural networks (CNN) are such models. CNN is a type of neural network that can make use of the internal structure of data such as word order. In essence, CNN internally learns a non-linear embedding of small regions of data (e.g., \u201creally love it\u201d), which makes it particularly suitable for learning multi-view embeddings. Furthermore, a recent study [10] has shown that CNN exceeds state-of-the-art supervised performances on text categorization due to its ability to directly embed small text regions into vectors in the supervised setting. Motivated by this finding, we propose a new semi-supervised learning framework for CNN, which learns multi-view embeddings of small text regions from unlabeled data. The effectiveness of this approach is demonstrated on text categorization."}, {"heading": "2 Multi-View Embedding", "text": "In this section we first present a theoretical analysis of multi-view embedding and then apply it to some existing methods."}, {"heading": "2.1 Theory of multi-view embedding", "text": "Suppose that we observe two views (X1, X2) \u2208 X1 \u00d7 X2 of the input, and a target label Y \u2208 Y of interest, where X1 and X2 are finite discrete sets. Like [2], we assume conditional independence of two views, but we relax1 it as follows.\nAssumption 1. Assume that there exists a set of hidden statesH such that X1, X2, and Y are conditionally independent given h inH, and that the rank of matrix [P (X1, X2)] is |H|.\nFor example, on sentiment classification,Hmight consist of relevant concepts such as \u201chandy\u201d, \u201cpricy\u201d, and so on. Essentially, Assumption 1 states that two views and labels are related to each other only through the concepts inH, and that relations between the two views collectively capture all the concepts inH; thus, relations between the two views can reveal useful information for the task. We show that one can exploit such informative two-view relations through what we call multi-view embeddings2.\nDefinition 1 (multi-view embedding). A function f1 is a multi-view embedding of X1 w.r.t. X2 if there exists a function g1 such that P (X2|X1) = g1(f1(X1), X2) for any (X1, X2) \u2208 X1 \u00d7X2.\n1 [2] assumed conditional independence of X1 and X2 given label Y . 2 We focus on the two-view scenario for notational simplicity, but extension to more views is straightforward. Therefore, we\nsay \u201cmulti-view\u201d rather than \u201ctwo-view\u201d.\nThat is, f1(X1) is as good as X1 itself for the purpose of predicting X2. In other words, a multi-view embedding of a view (X1) preserves everything required to predict another view (X2).\nTheorem 1. Consider a multi-view embedding f1 of X1 w.r.t. X2. Under Assumption 1, there exists a function q1 such that P (Y |X1) = q1(f1(X1), Y ). Further consider a multi-view embedding f2 of X2 w.r.t. X1. Then, under Assumption 1, there exists a function q such that P (Y |X1, X2) = q(f1(X1), f2(X2), Y ). The proof can be found in the Appendix; it uses matrix decomposition derived from P (X2|X1) =\u2211 h\u2208H P (X2|h)P (h|X1). The theorem says that, when the assumption holds, the features produced by a multi-view embedding are as good as the original views for the purpose of predicting the target classes. One may wonder why such embedded features are useful if they are only as good as the original views. An advantage arises when they are of much lower dimensionality than the original views. A model with lower dimensional representation requires fewer model parameters to train and makes the corresponding learning problem simpler. Note that in reality, one can only approximate the function q in Theorem 1 through training. With a fixed amount of training data, \u2018goodness\u2019 of this approximation is inversely related to the complexity of the problem. Therefore, if the multi-view embedding lowers the complexity of the problem (by being low-dimensional) without losing the predictive power of the original view, then it helps to improve the approximation of q, i.e., we obtain a more accurate predictor.\nMoreover, the vector proximity motivation of word embedding (i.e., similar words being mapped to similar vectors), mentioned above, can also be derived from our definition of multi-view embedding. If X \u20321 and X \u2032\u2032 1 in X1 are similar to each other in terms of their relations to X2 (i.e., P (X2|X \u20321) \u2248 P (X2|X \u2032\u20321 ) for all X2 \u2208 X2), then their low-dimensional multi-view embedded vectors f1(X \u20321) and f1(X \u2032\u20321 ) tend to become close to each other since that is how information required to predict X2 can be packed into lowdimensional vectors, assuming smooth g1 (predictor). In that sense, a multi-view embedding exposes the proximity structure of X1 with respect to the relations to X2. While usefulness of such vector representation has been intuitively noted, our theorem shows that it is useful for supervised learning if the relations between two views well reflect the concepts relevant to the supervised task.\nThus the theorem suggests the following general semi-supervised learning framework:\n1. Unsupervised learning: Learn a multi-view embedding from unlabeled data through tasks that predict one view from another.\n2. Supervised learning: Use the learned embedding to provide input to the supervised task."}, {"heading": "2.2 Case study of multi-view embedding", "text": "Let us examine some existing methods from the view point of multi-view embedding learning.\nWord vectors As mentioned above, there have been studies to represent words by low-dimensional dense vectors through unsupervised learning. We focus on word2vec [18, 19] due to its popularity.\nword2vec skip-gram The word2vec skip-gram task essentially predicts words within some distance from word w using a word vector representing w as features. To see the correspondence to multi-view embedding learning, consider a token tagging task such as POS tagging. The feature mappings useful for this task would be the current word (view-1: X1) and its context (view-2: X2). The skip-gram task is to predict the context (X2) from the current word (X1) using the word vector (f1(X1)) as features, i.e., approximating P (X2|X1)\nby g1(f1(X1), X2) (as in Definition 1) where g1 is the classifier that uses the word vectors as features. Therefore, the obtained mapping from words to word vectors is a multi-view embedding of words w.r.t. the context. Whether the word2vec vectors are effective for a certain task depends on how well Assumption 1 holds, i.e., how well the relations between a word and its context correspond to the factors relevant to the task.\nword2vec cbow On the other hand, the word2vec continuous bow (cbow) task does not produce a multiview embedding. It predicts a word (X2) from the context (X1) (note that the roles of the word and its context are reversed), and the context is represented by the sum of word vectors of the constituent words. Through this task, one can learn a multi-view embedding of context (i.e., word sequences with a hole in the center), but the word vectors (additive decomposition of the context vector) do not fit our definition. In other words, these word vectors are trained so that their sum is predictive, but not necessarily individually. This might be the reason that cbow generally underperformed skip-gram in [18].\nParagraph vectors [13] proposed paragraph vectors (p-vec), which represent variable-sized text by lowdimensional vectors. Though we omit details, their unsupervised task predicts a word in paragraph p using p-vec for p (optionally also using word vectors in the context); thus, views overlap and strongly violate the conditional independence assumption. Our theorem does not apply to p-vec."}, {"heading": "3 Multi-View Learning for CNN for Text", "text": "Based on our theoretical analysis, we propose a semi-supervised learning framework for CNN. As we will see below, CNN internally learns to embed small text regions into low-dimensional continuous vectors. This, as well as the fact that it comes with natural definitions of views built into the model structure, makes CNN particularly suitable for multi-view embedding learning.\nCNN, originally developed for image, is a type of neural network that can make use of the internal structure of data such as word order. One approach to adapting CNN from image (low-dimensional dense data) to high-dimensional text data is to first convert words into word vectors before feeding them to CNN [7, 12, 11]. When word vectors are learned from large unlabeled data through multi-view learning, this could be a very effective semi-supervised method, though untested on text categorization3. A more recent approach by [10] is to apply CNN directly to high-dimensional one-hot vectors, resulting in performances superior to other supervised methods on text categorization.\nIn this section, we first focus on one-hot CNN of [10] to explore application of multi-view embedding learning and then later show that the proposed framework subsumes the CNN with pre-trained word vectors as well. We start with a brief review of one-hot CNN."}, {"heading": "3.1 One-hot CNN for text [10]", "text": "CNN is a feed-forward network equipped with convolution layers interleaved with pooling layers. As illustrated in Figure 1, a convolution layer consists of computation units, each of which responds to a small region of input (e.g., tokens in window of size 2), and the small regions collectively cover the entire data\n3 This type of CNN has been used for word sequence tagging tasks and classification of short sentences, but not for categorization of full-length documents in general.\n(e.g., a document). It is these small regions that we consider as views later. A computation unit associated with the `-th region of input x computes:\n\u03c3(W \u00b7 r`(x) + b) , (1)\nwhere r`(x) \u2208 Rq is the region vector that represents the `-th region. Weight matrix W \u2208 Rm\u00d7q and bias vector b \u2208 Rm are shared by all the units in the same layer, and they are learned through training. The region vector r`(x) can be either a concatenation of one-hot vectors or bow: e.g., for \u201clove it\u201d\nI it love I it love r`(x) =[ 0 0 1 | 0 1 0 ]> (2)\nI it love r`(x) =[ 0 1 1 ] > (3)\nThe bow representation (3) loses word order within the region but is more robust to data sparsity, enables a large region size such as 20, and speeds up training by having fewer parameters. CNN with (2) is called seq-CNN and CNN with (3) bow-CNN. The region size and stride (distance between the region centers) are meta-parameters. Note that we used a tiny three-word vocabulary for the vector examples above to save space, and a vocabulary of typical applications could be much larger such as 100K words.\n\u03c3 in (1) is a component-wise non-linear function (e.g., applying \u03c3(x) = max(x, 0) to each vector component). Thus, each computation unit generates an m-dimensional vector where m is the number of weight vectors (W\u2019s rows, or neurons). In other words, a convolution layer is the embedding of text regions, which produces an m-dim vector for each text region. The m-dim vectors from all the text regions of each document are aggregated by the pooling layer, by either component-wise maximum (max-pooling) or average (average-pooling), and used by the top layer (a linear classifier) as features for classification.\nHere we focused on the convolution layer due to its relevance to our work; for other details, [10] should be consulted."}, {"heading": "3.2 Multi-view semi-supervised CNN for text", "text": "Let B be the base model CNN for the task of interest. The semi-supervised framework we propose takes the following two steps.\n1. Unsupervised learning: Regard small regions of B\u2019s convolution layer as views. Define a task to predict adjacent regions from each region. Train CNN U for this task on unlabeled data.\n2. Supervised learning: Integrate U\u2019s convolution layer, which serves as the multi-view embedding, into B. Train this final model with labeled data of the intended task.\nThese two steps are described in the next two sections, respectively."}, {"heading": "3.2.1 Unsupervised learning of multi-view embedding", "text": "We regard the small regions of B\u2019s convolution layer as views and generate a task to predict adjacent regions from each region. As illustrated in Figure 2, we learn this task by CNN U with one convolution layer. Given a document x, for each text region indexed by `, the convolution layer of U computes:\nu`(x) = \u03c3 (U) ( W(U) \u00b7 r(U)` (x) + b (U) ) , (4)\nwhich is the same as (1) except for the superscript \u201c(U)\u201d to indicate that these entities belong to U . The top layer uses u`(x) as features to predict the adjacent regions. W(U) and b(U) (and the top-layer parameters) are learned through training. It is this convolution layer that we will transfer to supervised learning in the next step.\nRelation to Theorem 1 To see the correspondence to Theorem 1, it helps to consider a sub-task that assigns a label (e.g., positive/negative) to each text region (e.g., \u201clove it\u201d) instead of the ultimate task of categorizing the entire document. This is sensible because CNN makes prediction by building up from these small regions. In a document \u201cI really love it !\u201d as in Figure 2, the clues for predicting a label of \u201clove it\u201d are \u201clove it\u201d itself (view-1: X1) and its context \u201cI really\u201d and \u201c!\u201d (view-2: X2). U is trained to predict X2 from X1, i.e., to approximate P (X2|X1) by g1(f1(X1), X2)) as in Definition 1, and functions f1 and g1 are embodied by the convolution layer and the top layer, respectively. Theorem 1 says that if Assumption 1 holds, then the multi-view embedding f1, which is U\u2019s convolution layer, is useful for the target task (e.g., determining positive/negative).\nNote that the goal here is to learn embedding f1 effective for the target task as in the first half of the theorem P (Y |X1) = q1(f1(X1), Y ), and we do not learn embedding of X2 (context). However, as shown in Figure 2, all the text regions participate in training not only as part of X2 but also as X1 as the window slides; therefore, nothing is wasted."}, {"heading": "3.2.2 Final supervised learning", "text": "To integrate U\u2019s convolution layer (the multi-view embedding) trained above into the base model B, there are two options. The first option is replacement, which replaces B\u2019s convolution layer with that of U , similar to pre-training in the neural network literature. The second option is add-on, which uses the output of U\u2019s convolution layer as additional features fed to B\u2019s convolution layer. We do this by replacing (1) in B with:\n\u03c3 ( W \u00b7 r`(x) +W\u2032 \u00b7 u`(x) + b ) , (5)\nwhere u`(x) is defined by (4), i.e., u`(x) is the result of the multi-view embedding applied to the `-th region. We train this model with the labeled data of the task; that is, we update the weights W, W\u2032, bias b, and the weights/bias in the top layer so that the designated loss function is minimized on the labeled training data.\nWith both options, W(U) and b(U) can be either fixed or updated for fine-tuning, and in this work we fix them for simplicity.\nRelation to Theorem 1 The theorem implies that if Assumption 1 holds, the features produced by the multi-view embedding are as good as the original view, which might make one wonder if inclusion of the original view (as in the add-on option) is redundant. Empirically, however, there are cases where better results are obtained by add-on.\nOne plausible reason for this is that in real applications, the assumption holds only approximately. For example, two adjacent text regions are assumed to be related only through relevant concepts (e.g., \u201cpricy\u201d), but in reality, e.g., adjacent regions may have syntactic relations such as \u201cthe\u201d being often followed by a noun, which may violate the assumption on semantic tasks. Another consideration is that even though unlabeled data may be inexpensive, the amount (and training time one can spend) is finite, and the obtained embedding may only be approximately multi-view embedding.\nFor these reasons, it is practical to have the add-on option. Essentially, the add-on option uses the embedding learned from unlabeled data to reduce the model complexity, while using the original features to compensate for the possible loss of information. Note that this model still can reduce the overall complexity of supervised learning in spite of the seeming increase of the feature dimensionality. This is because the predictor using the original features becomes simpler (e.g. the model parameters have a smaller 2-norm) as it only needs to complement the predictor using the new features (which is simple due to low dimensionality), as analyzed in [1]."}, {"heading": "3.3 Pre-trained word vector-based CNN", "text": "As mentioned above, one approach to adapting CNN to text is to first convert words to word vectors. However, note that the word vector conversion layer can also be regarded as a special convolution layer with region size 1 and \u03c3(x) = x that takes one-hot vectors as input. With this interpretation, the wordvector CNN can be regarded as a one-hot CNN with two convolution layers, as illustrated in Figure 3. Then application of multi-view embedding learning to the first convolution layer (i.e., the word vector conversion layer) would lead to the unsupervised task of predicting surrounding words4 from each word. That is what,\n4 Note that the example in Figure 2 only uses one adjacent region on each side as the target, but more than one adjacent region (several words in this case) can also be used.\ne.g., the word2vec skip-gram task does. Therefore, CNN with pre-trained word vectors in previous studies is subsumed by the semi-supervised CNN framework (with the replacement option) described in this paper, if the word vectors are trained appropriately."}, {"heading": "4 Experiments", "text": "We report experiments on text categorization. Our code and settings for reproducing the experiments will be made available publicly on the internet."}, {"heading": "4.1 Tasks and data", "text": "We used three datasets used in [10]: IMDB, Elec, and RCV1. IMDB5 is a dataset of movie reviews, and Elec6 consists of Amazon electronics product reviews [16]. The task associated with IMDB and Elec is sentiment classification to assign positive/negative to each review. The task we tested on RCV17 was singlelabel categorization with the 55 second-level topics, using the same training and test sets8 as in Table 2 of [10].\nUnlabeled data IMDB comes with an unlabeled set of 50K reviews. To facilitate comparison with [13], we used this set and the training set as unlabeled data. For Elec, we chose 200K reviews from the same data source, so that they are disjoint from the training and test sets, and that their reviewed products are disjoint from the test set9. The unlabeled set we used will be made available publicly on the internet. On RCV1, the articles in a 10-month period were used as unlabeled data, which are disjoint from the training and the test sets. Table 1 summarizes the data."}, {"heading": "4.2 CNN with multi-view embedding", "text": "We experimented with semi-supervised CNN with multi-view embedding (mvCNN in short) of the following two types.\n\u2022 mvCNNo: it takes one-hot CNN (Fig.1) with region size > 1 as the base model and learns a region embedding from unlabeled data.\n\u2022 mvCNNw: it takes word-vector CNN (Fig.3) as the base model and learns word embedding from unlabeled data.\n5 http://ai.stanford.edu/\u02dcamaas/data/sentiment/ 6 http://riejohnson.com/cnn_data.html 7 http://trec.nist.gov/data/reuters/reuters.html 8 Defined at http://riejohnson.com/cnn_data.html. 9 This makes the task harder and more realistic, considering the situation that new products come out after training."}, {"heading": "4.2.1 Implementation of mvCNN", "text": "Unsupervised training minimized weighted square loss \u2211\ni,j \u03b1i,j(zi[j] \u2212 pi[j])2 where i goes through the data points, z represents the adjacent regions (prediction target), and p is the model output. (Recall that the objective of unsupervised training is to predict adjacent regions from each region.) Though there are several ways to encode the target regions, in our experiments we simply set z to be the concatenation of two bow vectors of adjacent regions on the left and right, while only retaining the 30K most frequent words of the unlabeled data with vocabulary control (described below). For this purpose, we used one region on each side for region embedding, and 5 words on each side for word embedding learning. The weights \u03b1i,j were set to balance the loss originating from the presence and absence of words and to speed up training, similar to word2vec negative sampling.\nThe theory assumes that views are related to each other only through the concepts relevant to the target classes; however, adjacent regions often have syntactic relations, which are undesirable on sentiment or topic classification. To meet the assumption as much as possible, a simple heuristic we found effective is to remove function words (articles, pronouns, and propositions) from (and only from) the target regions. This vocabulary control often led to small accuracy improvements while speeding up unsupervised training. All the reported results were obtained with vocabulary control. On RCV1, instead of a small list of function words, we used the stop-word list provided by [15].\nThe rest basically follows [10]. Recall that one-hot CNN can be either seq-CNN or bow-CNN. For supervised learning, we used seq-CNN on IMDB/Elec and bow-CNN on RCV1. For unsupervised learning, we used bow-CNN as it appeared to perform no worse with faster training. Activation was fixed to rectifier (\u03c3(x) = max(x, 0)); optimization was done by SGD. Response normalization, a technique commonly used on image, was applied to scale the output of the convolution/pooling layer v at each location by multiplying (1 + |v|2)\u22121/2. Supervised training was done with square loss and L2 regularization, and dropout [8] was optionally applied to the input of the top layer. All the characters were converted to lower case (which was done for all the tested methods)."}, {"heading": "4.3 Model selection", "text": "Importantly, on all the tested methods, tuning of meta-parameters (e.g., the regularization parameter) was done by testing the models on the held-out portion of the training data, and then the models were re-trained with the chosen meta-parameters using the entire training data."}, {"heading": "4.4 Results", "text": "The error rate results are shown in Table 2. In these experiements, for meaningful comparison, all the CNN models were constrained to have exactly one convolution layer (excluding the word vector layer if any) with 1000 neurons. The supervised baselines are the best-performing CNN within these constraints, which are one-hot seq-CNN (region size 3; one max-pooling unit) on IMDB and Elec and one-hot bow-CNN (region size 20; 10 average-pooling units) on RCV1, as in [10]10; we will review the performance of more complex CNN from [10] later in Tables 3 and 4. With mvCNN, the dimensionality of multi-view embedding was fixed to 100. Performances with embeddings of various dimensionalities are shown in Figure 4.\nThe first thing to note from Table 2 is that on all three datasets, mvCNN models (except for mvCNNw on RCV1) clearly outperform the best-performing supervised CNN. This confirms the effectiveness of the\n10 [10] reports 9.33 with bow-CNN on RCV1 while Table 2 reports 9.07. The difference is due to the use ([10]) and unuse (this work) of the stopword list.\nframework we propose. On sentiment classification (IMDB and Elec), mvCNNo and mvCNNw performed similarly well, except when the embedding dimensionality was relatively low (Figure 4) where mvCNNo outperformed mvCNNw. For both types, the region size chosen by model selection was 5, larger than 3 for supervised CNN. This indicates that unlabeled data enabled effective use of larger regions which are more predictive but might suffer from data sparsity in supervised settings.\nOn topic classification (RCV1), mvCNNw performed poorly, not only underperforming mvCNNo but also the best-performing supervised bow-CNN. Larger contexts (up to 20 words) for word embedding learning were also tested but did not help. This is apparently due to the overwhelming superiority of bow-CNN to the word-vector CNN (recall that mvCNNw is word-vector CNN with word embedding, and mvCNNo on RCV1 is bow-CNN with region embedding).\nGiven that, we explored integration of word embedding into bow-CNN and found it effective to use the average of the word vectors over each text region as the additional features (in place of u`(x) in (5)). The performance of this model on RCV1 is shown in Figure 4 (right). It outperforms the supervised baseline but visibly underperforms mvCNNo. This results from the difference between the embedded text regions and the average of embedded words; the former is a direct result of multi-view embedding trained to represent text regions (of size 20), and the latter is not. This result supports our approach to learn embedding of text regions of arbitrary sizes.\nReplacement vs. add-on The replacement/add-on option was determined by model selection, and it turned out that for mvCNNo, add-on was always chosen, and that for mvCNNw replacement was chosen except when the embedding dimensionality was as low as 10 or 25. This illustrates interesting differences between these two models. Apparently, mvCNNo\u2019s original view (e.g., \u201creally love it\u201d) is much more predictive than mvCNNw\u2019s (e.g., \u201creally\u201d), and mvCNNo\u2019s multi-view embedding learning is more complex than mvCNNw\u2019s due to the multi-word nature. This makes it harder for mvCNNo\u2019s multi-view embedding to reach the degree of predictiveness so that the original view is no longer useful in the supervised training; in this situation, the add-on option has an advantage.\nCombining mvCNNo and mvCNNw Indeed, what is learned from unlabeled data with these two approaches appears to be different enough so that combining mvCNNo and mvCNNw leads to further performance improvement (mvCNNo&w; Table 2\u2019s last row). This was done by using the region vector of mvCNNw\u2019s second layer (rounded rectangles surrounding triangles in Figure 3; concatenation of word vectors) as additional features to each computation unit of mvCNNo\u2019s convolution layer. That is, each computation unit receives two types of additional features, those resulting from region embedding and word embedding.\nIMDB and Elec: previous results In Table 3, we compare the error rate of our best model mvCNNo&w 6.66 on IMDB with the previous best results. To our knowledge, the previous best non-ensemble result on IMDB is 7.46 by paragraph vectors [13], which used the same unlabeled data we used. [17] produced 7.43 by combining three independently-trained models including a semi-supervised model. Even though our model is non-ensemble, our error rate is better by over 0.7%. The results demonstrate the effectiveness\nof our approach. As shown in Tables 3 and 4, the previous best supervised performance on IMDB and Elec was achieved by seq2-bown-CNN, equipped with three convolution layers in parallel. Note that for simplicity, we limited the mvCNN experiments to the networks with only one convolution layer with 1000 neurons. One may be able to obtain higher accuracy by increasing the number of neurons and/or applying multi-view learning to each layer of multi-layer CNN such as seq2-CNN or seq2-bown-CNN but we did not pursue it in this work.\nRCV1: previous results To compare with the benchmark results in [15, 10], we tested mvCNNo on the multi-label task with the LYRL04 split [15] on RCV1, in which more than one out of 103 categories can be assigned to each document. For this experiment, we used the stopword list as in [15, 10] so that the results are directly comparable. Since the LYRL04 split divides the entire corpus into the training set and the test set, there is no room for making a disjoint unlabeled set. We used the entire test set as unlabeled data \u2013 the transductive learning setting. As shown in Table 5, mvCNNo outperforms the best SVM of [15] and the best CNN of [10].\nStandard semi-supervised methods Many of the standard semi-supervised methods are not applicable to CNN. We tested transductive SVM (TSVM) with bow vectors of 1, 2, and 3-grams using SVMlight11 to compare with the supervised SVM results12. TSVM underperformed the corresponding SVM (30K) on two of the three datasets (Table 2). Since co-training is a meta-learner, it can be used with CNN. However, many repetitions of CNN training (as the labeled set expands) is extremely time-consuming, and we could not afford to follow our protocol of parameter tuning and re-training. We instead report the best (and unrealistic) co-training performances obtained by optimizing the meta-parameters13 including when to stop on the test data. Still, co-training clearly underperformed mvCNN. The results demonstrate the difficulty of benefiting from unlabeled data on these tasks, given that the size of the labeled data is relatively large. The poor results of these standard methods might be due to the difference in focus: smaller labeled data vs. higher performance (our focus)."}, {"heading": "4.5 Examples of predictive text regions", "text": "To obtain some insight into what was learned from unlabeled data, we show the text regions (from the IMDB training set) that contribute the most to the activation of the influential neurons (i.e., the neurons\n11 http://svmlight.joachims.org/ 12 Note that for feasibility, we only used the 30K most frequent n-grams in the TSVM experiments, thus, showing the SVM results also with 30K vocabulary for comparison, though on some datasets SVM performance can be improved by use of all the n-grams (e.g., 5 million n-grams on IMDB). This is because the computational cost of TSVM (single-core) turned out to be high, taking several days even with 30K vocabulary.\n13 Two types of view split were tested: random split of vocabulary and split into the first and last half of each document.\nhighly weighted in the top layer). The model is mvCNNo with the add-on option, and therefore, activation of a neuron is computed as in (5): \u03c3 (W \u00b7 r`(x) +W\u2032 \u00b7 u`(x) + b). For each text region, we measured the contributions of the one-hot vectors r`(x) and the multi-view embedding feature u`(x) by zeroing out one of them in (5) in turn. We show the text regions that activated the top 3 negative-sentiment neurons the most:\n\u2022 one-hot: \u201cwas very poor and as\u201d, \u201cwhat a disappointment. as\u201d, \u201ca 4 out of 10\u201d\n\u2022 multi-view: \u201clacklustre, unimaginative, implausible\u201d, \u201chorrid acting and lazy scriptwriting\u201d, \u201cof unwatchable drivel that wastes\u201d\nThough all convey strong negativity, there is a prominent difference: common words (one-hot) vs. less common words (multi-view). It shows that non-embedding and embedding features complement each other to achieve high accuracy."}, {"heading": "5 Conclusion", "text": "This paper presented a theoretical analysis of multi-view learning. The result explained the effectiveness of some existing word embedding methods and led to a new and more general semi-supervised learning framework that learns a multi-view embedding of small text regions (not limited to a single word) with CNN for use in supervised CNN. The experimental results demonstrated the effectiveness of this approach."}, {"heading": "Appendix A Proof of Theorem 1", "text": "Proof. First, assume that X1 contains d1 elements, and X2 contains d2 elements, and |H| = k. The independence and rank condition in Assumption 1 implies the decomposition\nP (X2|X1) = \u2211 h\u2208H P (X2|h)P (h|X1)\nis of rank k if we consider P (X2|X1) as a d2 \u00d7 d1 matrix (which we denote by A). Now we may also regard P (X2|h) as a d2 \u00d7 k matrix (which we denote by B), and P (h|X1) as a k \u00d7 d1 matrix (which we denote by C). From the matrix equation A = BC, we obtain C = (B>B)\u22121B>A. Consider the k \u00d7 d2 matrix U = (B>B)\u22121B>. Then we know that its elements correspond to a function of (h,X2) \u2208 H \u00d7 X2. Therefore the relationship C = UA implies that there exists a function u(h,X2) such that\n\u2200h \u2208 H : P (h|X1) = \u2211\nX2\u2208X2\nP (X2|X1)u(h,X2).\nUsing the definition of embedding in Definition 1, we obtain P (h|X1) = \u2211\nX2\u2208X2\ng1(f1(X1), X2)u(h,X2).\nDefine t1(a1, h) = \u2211\nX2 g1(a1, X2)u(h,X2), then for any h \u2208 H we have\nP (h|X1) = t1(f1(X1), h). (6)\nSimilarly, there exists a function t2(a2, h) such that for any h \u2208 H\nP (h|X2) = t2(f2(X2), h). (7)\nObserve that\nP (Y |X1) = \u2211 h\u2208H P (Y, h|X1) = \u2211 h\u2208H P (h|X1)P (Y |h,X1)\n= \u2211 h\u2208H P (h|X1)P (Y |h) = \u2211 h\u2208H t1(f1(X1), h)P (Y |h)\nwhere the third equation has used the assumption that Y is indpendent of X1 given h and the last equation has used (6). By defining q1(a1, Y ) = \u2211 h\u2208H t1(a1, h)P (Y |h), we obtain P (Y |X1) = q1(f1(X1), Y ), as desired. Further observe that\nP (Y |X1, X2) = \u2211 h\u2208H P (Y, h|X1, X2)\n= \u2211 h\u2208H P (h|X1, X2)P (Y |h,X1, X2)\n= \u2211 h\u2208H P (h|X1, X2)P (Y |h), (8)\nwhere the last equation has used the assumption that Y is independent of X1 and X2 given h. Note that\nP (h|X1, X2) = P (h,X1, X2)\nP (X1, X2) = P (h,X1, X2)\u2211 h\u2032\u2208H P (h \u2032, X1, X2)\n= P (h)P (X1|h)P (X2|h)\u2211\nh\u2032\u2208H P (h \u2032)P (X1|h\u2032)P (X2|h\u2032)\n= P (h,X1)P (h,X2)/P (h)\u2211\nh\u2032\u2208H P (h \u2032, X1)P (h\u2032, X2)/P (h\u2032)\n= P (h|X1)P (h|X2)/P (h)\u2211\nh\u2032\u2208H P (h \u2032|X1)P (h\u2032|X2)/P (h\u2032)\n= t1(f1(X1), h)t2(f2(X2), h)/P (h)\u2211\nh\u2032\u2208H t1(f1(X1), h \u2032)t2(f2(X2), h\u2032)/P (h\u2032)\n,\nwhere the third equation has used the assumption thatX1 is independent ofX2 given h, and the last equation has used (6) and (7). The last equation means that P (h|X1, X2) is a function of (f1(X1), f2(X2), h). That is, there exists a function t\u0303 such that P (h|X1, X2) = t\u0303(f1(X1), f2(X2), h). From (8), this implies that\nP (Y |X1, X2) = \u2211 h\u2208H t\u0303(f1(X1), f2(X2), h)P (Y |h).\nNow the theorem follows by defining q(a1, a2, Y ) = \u2211 h\u2208H t\u0303(a1, a2, h)P (Y |h)."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Rie K. Ando", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Two-view feature generation model for semi-supervised learning", "author": ["Rie K. Ando", "Tong Zhang"], "venue": "In Proceedings of ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Marchine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["Avrim Blum", "Tom Mitchell"], "venue": "In Proceedings of COLT,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "editors"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Transductive inference for text classification using support vector machines", "author": ["Thorsten Joachims"], "venue": "In Proceedings of ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Proceedings of NAACL HLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A convolutional neural network for modeling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of EMNLP, pages 1746\u20131751,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "RCV1: A new benchmark collection for text categorization research", "author": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": "Journal of Marchine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Hidden factors and hidden topics: Understanding rating dimensions with review text", "author": ["Julian McAuley", "Jure Leskovec"], "venue": "In RecSys,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "author": ["Gr\u00e9goire Mesnil", "Tomas Mikolov", "Marc\u2019Aurelio Ranzato", "Yoshua Bengio"], "venue": "Feb 2015 version),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of Wordshop at ICLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Rainov", "Yoshua Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D. Manning"], "venue": "In Proceedings of ACL (short paper),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Semi-supervised learning literature survey", "author": ["Xiaojin Zhu"], "venue": "Technical Report 1530,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "Among a number of semi-supervised methods that have been proposed [5, 23], there are two notable types of method.", "startOffset": 66, "endOffset": 73}, {"referenceID": 22, "context": "Among a number of semi-supervised methods that have been proposed [5, 23], there are two notable types of method.", "startOffset": 66, "endOffset": 73}, {"referenceID": 3, "context": ", EM, co-training [4], and transductive SVM [9].", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": ", EM, co-training [4], and transductive SVM [9].", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "In NLP, an empirically successful example is word embedding learned from unlabeled data through the tasks that essentially predict the neighboring words from words [3, 6, 20, 19].", "startOffset": 164, "endOffset": 178}, {"referenceID": 5, "context": "In NLP, an empirically successful example is word embedding learned from unlabeled data through the tasks that essentially predict the neighboring words from words [3, 6, 20, 19].", "startOffset": 164, "endOffset": 178}, {"referenceID": 19, "context": "In NLP, an empirically successful example is word embedding learned from unlabeled data through the tasks that essentially predict the neighboring words from words [3, 6, 20, 19].", "startOffset": 164, "endOffset": 178}, {"referenceID": 18, "context": "In NLP, an empirically successful example is word embedding learned from unlabeled data through the tasks that essentially predict the neighboring words from words [3, 6, 20, 19].", "startOffset": 164, "endOffset": 178}, {"referenceID": 20, "context": "When used as additional features, the obtained word vectors often improve the performance of supervised NLP systems [21].", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "Theoretically, this approach is related to two-view feature learning by [2], which originated from alternating structure optimization [1].", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "Theoretically, this approach is related to two-view feature learning by [2], which originated from alternating structure optimization [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 13, "context": "[14] has shown that", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "one instance of word2vec [18] is implicit factorization of a word-context matrix of shifted PMI; however, [14] did not show why factorization of this matrix would be useful.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "one instance of word2vec [18] is implicit factorization of a word-context matrix of shifted PMI; however, [14] did not show why factorization of this matrix would be useful.", "startOffset": 106, "endOffset": 110}, {"referenceID": 1, "context": "[2] has given theoretical justification of two-view feature learning, but this analysis is limited to the linear case where SVD is used.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "This analysis can be regarded as a generalization of [2] to non-linear cases, allowing it to handle neural network-based word embeddings and its extensions to text region embedding investigated in this work.", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "Furthermore, a recent study [10] has shown that CNN exceeds state-of-the-art supervised performances on text categorization due to its ability to directly embed small text regions into vectors in the supervised setting.", "startOffset": 28, "endOffset": 32}, {"referenceID": 1, "context": "Like [2], we assume conditional independence of two views, but we relax1 it as follows.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "1 [2] assumed conditional independence of X1 and X2 given label Y .", "startOffset": 2, "endOffset": 5}, {"referenceID": 17, "context": "We focus on word2vec [18, 19] due to its popularity.", "startOffset": 21, "endOffset": 29}, {"referenceID": 18, "context": "We focus on word2vec [18, 19] due to its popularity.", "startOffset": 21, "endOffset": 29}, {"referenceID": 17, "context": "This might be the reason that cbow generally underperformed skip-gram in [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Paragraph vectors [13] proposed paragraph vectors (p-vec), which represent variable-sized text by lowdimensional vectors.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "One approach to adapting CNN from image (low-dimensional dense data) to high-dimensional text data is to first convert words into word vectors before feeding them to CNN [7, 12, 11].", "startOffset": 170, "endOffset": 181}, {"referenceID": 11, "context": "One approach to adapting CNN from image (low-dimensional dense data) to high-dimensional text data is to first convert words into word vectors before feeding them to CNN [7, 12, 11].", "startOffset": 170, "endOffset": 181}, {"referenceID": 10, "context": "One approach to adapting CNN from image (low-dimensional dense data) to high-dimensional text data is to first convert words into word vectors before feeding them to CNN [7, 12, 11].", "startOffset": 170, "endOffset": 181}, {"referenceID": 9, "context": "A more recent approach by [10] is to apply CNN directly to high-dimensional one-hot vectors, resulting in performances superior to other supervised methods on text categorization.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "In this section, we first focus on one-hot CNN of [10] to explore application of multi-view embedding learning and then later show that the proposed framework subsumes the CNN with pre-trained word vectors as well.", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "1 One-hot CNN for text [10] CNN is a feed-forward network equipped with convolution layers interleaved with pooling layers.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "I it love r`(x) =[ 0 1 1 ] > (3)", "startOffset": 17, "endOffset": 26}, {"referenceID": 0, "context": "I it love r`(x) =[ 0 1 1 ] > (3)", "startOffset": 17, "endOffset": 26}, {"referenceID": 9, "context": "Here we focused on the convolution layer due to its relevance to our work; for other details, [10] should be consulted.", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "the model parameters have a smaller 2-norm) as it only needs to complement the predictor using the new features (which is simple due to low dimensionality), as analyzed in [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 9, "context": "On RCV1, we used the same training and test sets as in Table 2 of [10].", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "1 Tasks and data We used three datasets used in [10]: IMDB, Elec, and RCV1.", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "IMDB5 is a dataset of movie reviews, and Elec6 consists of Amazon electronics product reviews [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "The task we tested on RCV17 was singlelabel categorization with the 55 second-level topics, using the same training and test sets8 as in Table 2 of [10].", "startOffset": 148, "endOffset": 152}, {"referenceID": 12, "context": "To facilitate comparison with [13], we used this set and the training set as unlabeled data.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "On RCV1, instead of a small list of function words, we used the stop-word list provided by [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "The rest basically follows [10].", "startOffset": 27, "endOffset": 31}, {"referenceID": 7, "context": "Supervised training was done with square loss and L2 regularization, and dropout [8] was optionally applied to the input of the top layer.", "startOffset": 81, "endOffset": 84}, {"referenceID": 9, "context": "The supervised baselines are the best-performing CNN within these constraints, which are one-hot seq-CNN (region size 3; one max-pooling unit) on IMDB and Elec and one-hot bow-CNN (region size 20; 10 average-pooling units) on RCV1, as in [10]10; we will review the performance of more complex CNN from [10] later in Tables 3 and 4.", "startOffset": 238, "endOffset": 242}, {"referenceID": 9, "context": "The supervised baselines are the best-performing CNN within these constraints, which are one-hot seq-CNN (region size 3; one max-pooling unit) on IMDB and Elec and one-hot bow-CNN (region size 20; 10 average-pooling units) on RCV1, as in [10]10; we will review the performance of more complex CNN from [10] later in Tables 3 and 4.", "startOffset": 302, "endOffset": 306}, {"referenceID": 9, "context": "This confirms the effectiveness of the 10 [10] reports 9.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "The difference is due to the use ([10]) and unuse (this work) of the stopword list.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "IMDB Elec RCV1 SVM 1-3grams (all) [10] 9.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "69 SVM 1-3grams (30K) [10] 10.", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": "NB+SVM 1-2grams [22] 8.", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "78 Ensemble NB-LM 1-3grams [17] 8.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "13 \u2013 seq2-CNN (1K\u00d72) [10] 8.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "04 \u2013 seq2-CNN (3K\u00d72) [10] 7.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "94 \u2013 seq2-bown-CNN [10] 7.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "67 \u2013 Paragraph vectors [13] 7.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "43 Ensemble+ + NB-LM [17] Unlabeled data mvCNNo&w [Ours] 6.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "SVM 1-3grams [10] 8.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "71 \u2013 NB-LM 1-3grams [10] 8.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "11 \u2013 seq2-CNN [10] 7.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "48 \u2013 seq2-bown-CNN [10] 7.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "46 by paragraph vectors [13], which used the same unlabeled data we used.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "[17] produced 7.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "models micro-F macro-F extra resource SVM [15] 81.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "7 \u2013 CNN [10] 84.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "RCV1: previous results To compare with the benchmark results in [15, 10], we tested mvCNNo on the multi-label task with the LYRL04 split [15] on RCV1, in which more than one out of 103 categories can be assigned to each document.", "startOffset": 64, "endOffset": 72}, {"referenceID": 9, "context": "RCV1: previous results To compare with the benchmark results in [15, 10], we tested mvCNNo on the multi-label task with the LYRL04 split [15] on RCV1, in which more than one out of 103 categories can be assigned to each document.", "startOffset": 64, "endOffset": 72}, {"referenceID": 14, "context": "RCV1: previous results To compare with the benchmark results in [15, 10], we tested mvCNNo on the multi-label task with the LYRL04 split [15] on RCV1, in which more than one out of 103 categories can be assigned to each document.", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "For this experiment, we used the stopword list as in [15, 10] so that the results are directly comparable.", "startOffset": 53, "endOffset": 61}, {"referenceID": 9, "context": "For this experiment, we used the stopword list as in [15, 10] so that the results are directly comparable.", "startOffset": 53, "endOffset": 61}, {"referenceID": 14, "context": "As shown in Table 5, mvCNNo outperforms the best SVM of [15] and the best CNN of [10].", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "As shown in Table 5, mvCNNo outperforms the best SVM of [15] and the best CNN of [10].", "startOffset": 81, "endOffset": 85}], "year": 2016, "abstractText": "This paper presents a theoretical analysis of multi-view embedding \u2013 feature embedding that can be learned from unlabeled data through the task of predicting one view from another. We prove its usefulness in supervised learning under certain conditions. The result explains the effectiveness of some existing methods such as word embedding. Based on this theory, we propose a new semi-supervised learning framework that learns a multi-view embedding of small text regions with convolutional neural networks. The method derived from this framework outperforms state-of-the-art methods on sentiment classification and topic categorization.", "creator": "LaTeX with hyperref package"}}}