{"id": "1505.02729", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2015", "title": "Sample Complexity of Learning Mahalanobis Distance Metrics", "abstract": "metric learning sets design strategy above limited computational space that puts prediction quality for information model concept at hand. in real work we provide coarse - style sample complexity algorithms for supervised metric learning. we enable basically lower - standard upper - bounds constraints that the sample complexity resides with the baseline dimension when no decision become made about the underlying sampled distribution. however, by leveraging the structure of the data distribution, we show that some can achieve approaches that specify likelihood - tuned always indicate specific assumption of intrinsic complexity for a given dataset. conditional analysis _ codes augmenting the best global optimization criterion with a minimum norm - linked stimulus can help adapt to a concept's global significance, yielding objective conclusions. constraints on benchmark datasets validate our analysis helps understand that regularizing the metric can help identifying why signal even inside the sensor does exponential amounts computational noise.", "histories": [["v1", "Mon, 11 May 2015 18:55:42 GMT  (51kb,D)", "http://arxiv.org/abs/1505.02729v1", "26 pages, 1 figure"]], "COMMENTS": "26 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["nakul verma", "kristin branson"], "accepted": true, "id": "1505.02729"}, "pdf": {"name": "1505.02729.pdf", "metadata": {"source": "CRF", "title": "Sample Complexity of Learning Mahalanobis Distance Metrics", "authors": ["Nakul Verma", "Kristin Branson"], "emails": ["verman@janelia.hhmi.org;", "bransonk@janelia.hhmi.org"], "sections": [{"heading": "1 Introduction", "text": "In many machine learning tasks, data is represented in a high-dimensional Euclidean space where each dimension corresponds to some interesting measurement of the observation. Often, practitioners include a variety of measurements in hopes that some combination of these features will capture the relevant information. While it is natural to represent such data in a Real space of measurements, there is no reason to expect that using Euclidean (L2) distances to compare the observations will be necessarily useful for the task at hand. Indeed, the presence of uninformative or mutually correlated measurements simply inflates the L2-distances between pairs of observations, rendering distance-based comparisons ineffective.\nMetric learning has emerged as a powerful technique to learn a good notion of distance or a metric in the representation space that can emphasize the feature combinations that help in the predication task while suppressing the contribution of spurious measurements. The past decade has seen a variety of successful metric learning algorithms that leverage various attributes of the problem domain. A few notable examples include exploiting class labels to find a Mahalanobis distance metric that maximizes the distance between dissimilar observations while minimizing distances between similar ones to improve classification quality (Weinberger & Saul, 2009; Davis et al., 2007), \u2217email: verman@janelia.hhmi.org; corresponding author. \u2020email: bransonk@janelia.hhmi.org\nar X\niv :1\n50 5.\n02 72\n9v 1\n[ cs\n.L G\n] 1\n1 M\nand explicitly optimizing for a downstream prediction task such as information retrieval (McFee & Lanckriet, 2010).\nDespite the popularity of metric learning methods, few studies have focused on studying how the problem complexity scales with key attributes of a given dataset. For instance, how do we expect the generalization error to scale\u2014both theoretically and practically\u2014as one varies the number of informative and uninformative measurements, or changes the noise levels?\nHere we study supervised metric learning more formally and gain a better understanding of how different modalities in data affect the metric learning problem. We develop two general frameworks for PAC-style analysis of supervised metric learning. We can categorize the popular metric learning algorithms into an empirical error minimization problem in one of the two frameworks. The first generic framework, the distance-based metric learning framework, uses class label information to derive distance constraints. The key objective is to learn a metric that on average yields smaller distances between examples from the same class than those from different classes. Some popular algorithms that optimize for such distance-based objectives include Mahalanobis Metric for Clustering (MMC) by Xing et al. (2002) and Information Theoretic Metric Learning (ITML) by Davis et al. (2007). Instead of using distance comparisons as a proxy, however, one can also optimize for a specific prediction task directly. The second generic framework, the classifier-based metric learning framework, explicitly incorporates the hypothesis associated with the prediction task of interest to learn effective distance metrics. A few interesting examples in this regime include the work by McFee & Lanckriet (2010) that finds metrics that improve ranking quality in information retrieval tasks, and the work by Shaw et al. (2011) that learns metrics that help predict connectivity structure in networked data.\nOur analysis shows that in both frameworks, the sample complexity scales with the representation dimension for a given dataset (Lemmas 1 and 3), and this dependence is necessary in the absence of any specific assumptions on the underlying data distribution (Lemmas 2 and 4). By considering any Lipschitz loss, our results generalize previous sample complexity results (see our discussion in Section 6) and, for the first time in the literature, provide matching lower bounds.\nIn light of the observation made earlier that data measurements often include uninformative or weakly informative features, we expect a metric that yields good generalization performance to deemphasize such features and accentuate the relevant ones. We can thus formalize the metric learning complexity of a given dataset in terms of the intrinsic complexity d of the metric that reweights the features in a way that yields the best generalization performance. (For Mahalanobis distance metrics, we can characterize the intrinsic complexity by the norm of the matrix representation of the metric.) We refine our sample complexity result and show a dataset-dependent bound for both frameworks that scales with dataset\u2019s intrinsic metric learning complexity d (Corollary 7).\nTaking guidance from our dataset-dependent result, we propose a simple variation on the empirical risk minimizing (ERM) algorithm that, when given an i.i.d. sample, returns a metric (of complexity d\u0302) that jointly minimizes the observed sample bias and the expected intra-class variance for metrics of fixed complexity d\u0302. This bias-variance balancing algorithm can be viewed as a structural risk minimizing algorithm that provides better generalization performance than an ERM algorithm and justifies norm-regularization of weighting metrics in the optimization criteria for metric learning.\nFinally, we evaluate the practical efficacy of our proposed norm-regularization criteria with some popular metric learning algorithms on benchmark datasets (Section 5). Our experiments highlight that the norm-regularization indeed helps in learning weighting metrics that better adapt to the signal in data in high-noise regimes."}, {"heading": "2 Preliminaries", "text": "Given a representation space X = RD of D real-valued measurements of observations of interest, the goal of metric learning is to learn a metric M (that is, a D \u00d7 D real-valued weighting matrix on X; to remove arbitrary scaling we shall assume that the maximum singular value of M , that is, \u03c3max(M) = 1)1 that minimizes some notion of error on data drawn from an unknown underlying distribution D on X \u00d7 {0, 1}. Specifically, we want to find the metric\nM\u2217 := argminM\u2208M err(M,D),\nfrom the class of metricsM under consideration, that is,M := {M |M \u2208 RD\u00d7D, \u03c3max(M) = 1}. For supervised metric learning, this error is typically label-based and can be defined in multiple reasonable ways. As discussed earlier, we explore two intuitive regimes for defining error.\nDistance-based error. A popular criterion for quantifying error in metric learning is by comparing distances amongst points drawn from the underlying data distribution. Ideally, we want a weighting metric M that brings data from the same class closer together than those from opposite classes. In a distance-based framework, a natural way to accomplish this is to find a weighting M that yields shorter distances between pairs of observations from the same class than those from different classes. By penalizing how often and by how much the distances violate these constraints gives rise to the particular form of the error.\nLet the variable z = (x, y) denote a random draw from D with x \u2208 X as the observation and y \u2208 {0, 1} its associated label, and let \u03bb denote how severely one wants to penalize the distance violations, then a natural definition of distance-based error becomes:\nerr\u03bbdist(M,D) := Ez1,z2\u223cD [ \u03c6\u03bb ( \u03c1 M (x1, x2), Y )] ,\nfor a generic distance-based loss function \u03c6\u03bb(\u03c1 M , Y ), that computes the degree of violation between weighted distance \u03c1 M\n(x1, x2) := \u2016M(x1 \u2212 x2)\u20162 and the label agreement Y := 1[y1 = y2] among a pair z1 = (x1, y1) and z2 = (x2, y2) drawn from D.\nAn example instantiation of \u03c6 popular in literature encourages metrics that yield distances that are no more than some upper limit U between observations from the same class, and distances that are no less than some lower limit L between those from different classes (for some U < L). Thus\n\u03c6\u03bbL,U (\u03c1M , Y ) :=\n{ min{1, \u03bb[\u03c1\nM \u2212U ] + } if Y = 1\nmin{1, \u03bb[L\u2212 \u03c1M ]+} otherwise , (1)\nwhere [A] + := max{0, A}. Xing et al. (2002) optimize an efficiently computable variant of this criterion, in which they look for a metric that keeps the total pairwise distance amongst the observations from the same class less than a constant while maximizing the total pairwise distance amongst the observations from opposite classes. The variant proposed by Davis et al. (2007) explicitly includes the upper and lower limits with an added regularization on the learned M to be close to a pre-specified metric of interest M0.\nWhile we discuss loss-functions \u03c6 that handle distances between a pair of observations, it is easy to extend to distances among triplets. Rather than having hard upper and lower limits which every\n1Note that we are looking at the linear form of the metric M ; usually the corresponding quadratic form MTM is discussed in the literature, which is necessarily positive semi-definite.\npair of the same and the opposite classes must obey, a triplet-based comparison typically focuses on relative distances between three observations at a time. A natural instantiation in this case becomes:\n\u03c6\u03bbtriple(\u03c1M(x1, x2), \u03c1M(x1, x3), (y1, y2, y3)) := { min{1, \u03bb[\u03c1M(x1, x2)\u2212 \u03c1M(x1, x3)]+} if y1 = y2 6= y3 0 otherwise ,\nfor a triplet (x1, y1), (x2, y2), (x3, y3) drawn from D. Weinberger & Saul (2009) discuss an interesting variant of this, in which instead of looking at all triplets in a given training sample, they focus on triplets of observations in local neighborhoods and learn a metric that maintains a gap or a margin among distances between observations from the same class and those from the opposite class. Improving the quality of distance comparisons in local neighborhoods directly affects the nearest neighbor performance, making this a popular technique.\nClassifier-based Error. Distance comparisons typically act as a surrogate for a specific downstream prediction task. If we want a metric that directly optimizes for a task, we need to explicitly incorporate the hypothesis class being used for that task while finding a good weighting metric.\nThis simple but effective insight has been used recently by McFee & Lanckriet (2010) for improving ranking results in information retrieval problems by explicitly incorporating ranking losses while learning an effective weighting metric. Shaw et al. (2011) also follow this principle and explicitly include network topology constraints to learn a weighting metric that can better predict the connectivity structure in social and web networks.\nWe can formalize the classifier-based metric learning framework by considering a fixed hypothesis classH of interest on the measurement domain. To keep the discussion general, we shall assume that the hypotheses are real-valued and can be regarded as a measure of confidence in classification, that is, each h \u2208 H is of the form h : X \u2192 [0, 1]. (One can obtain the binary predictions from h by a simple thesholding at 1/2.) Then, the error induced by a particular weighting metric M on the measurement space X can be defined as the best possible error that can be obtained by hypotheses inH, that is\nerrhypoth(M,D) := inf h\u2208H\nE(x,y)\u223cD [ 1 [ |h(Mx)\u2212 y| \u2265 1/2 ]] .\nWe shall study how this error scales with various key parameters of the metric learning problem."}, {"heading": "3 Learning a Metric from Samples", "text": "In any practical setting, we estimate the ideal weighting metric M\u2217 by minimizing the empirical version of the error criterion from a finite size sample from D.\nLet Sm denote a sample of size m, and err(M,Sm) denote the empirical error on the sample Sm (the exact definitions of Sm and the form of err(M,Sm) are discussed later). We can then define the empirical risk minimizing metric based on m samples as M\u2217m := argminM err(M,Sm). Most practical algorithms, of course, return some approximation of M\u2217m, and thus it is important to compare the generalization ability of M\u2217m to that of theoretically optimal M \u2217. That is, how\nerr(M\u2217m,D)\u2212 err(M\u2217,D) (2)\nconverges as the sample size m grows."}, {"heading": "3.1 Distance-Based Error Analysis", "text": "Given an i.i.d. sequence of observations z1, z2, . . . from D, we can pair the observations together to form a paired sample Sm = {(z1, z2), (z3, z4), . . . , (z2m\u22121, z2m)} = {(z1,i, z2,i)}mi=1 of size m, and define the sample based distance error err\u03bbdist(M,Sm) induced by a metric M as\nerr\u03bbdist(M,Sm) := 1\nm m\u2211 i=1 \u03c6\u03bb ( \u03c1M(x1,i, x2,i),1[y1,i = y2,i] ) .\nThen for any bounded support distribution D (that is, each (x, y) \u223c D, \u2016x\u2016 \u2264 B < \u221e), we have the following convergence result.2\nLemma 1 Fix any sample size m, and let Sm be an i.i.d. paired sample of size m from an unknown bounded distribution D (with bound B). For any distance-based loss function \u03c6\u03bb that is \u03bb-Lipschitz in the first argument, with probability at least 1\u2212 \u03b4 over the draw of Sm,\nsup M\u2208M\n[ err\u03bbdist(M,D)\u2212 err\u03bbdist(M,Sm) ] \u2264 O ( \u03bbB2 \u221a D ln(1/\u03b4)\nm\n) .\nUsing this lemma we can get the desired convergence rate (Eq. 2). Fix M\u2217 \u2208 M, then for any 0 < \u03b4 < 1 and m \u2265 1, with probability at least 1\u2212 \u03b4, we have\nerr\u03bbdist(M \u2217 m,D)\u2212 err\u03bbdist(M\u2217,D)\n= err\u03bbdist(M \u2217 m,D)\u2212 err\u03bbdist(M\u2217m, Sm) + err\u03bbdist(M\u2217m, Sm)\u2212 err\u03bbdist(M\u2217, Sm)\n+ err\u03bbdist(M \u2217, Sm)\u2212 err\u03bbdist(M\u2217,D)\n\u2264 O ( \u03bbB2 \u221a D ln(1/\u03b4)\nm\n) + \u221a ln(2/\u03b4)\n2m\n= O ( \u03bbB2 \u221a D ln(1/\u03b4)\nm\n) ,\nby noting (i) err\u03bbdist(M \u2217 m, Sm) \u2264 err\u03bbdist(M\u2217, Sm), since M\u2217m is empirical error minimizing on Sm, and (ii) by using Hoeffding\u2019s inequality on the fixed M\u2217 to conclude that with probability at least\n1\u2212 \u03b4/2, err\u03bbdist(M\u2217, Sm)\u2212 err\u03bbdist(M\u2217,D) \u2264 \u221a ln(2/\u03b4) 2m .\nThus to achieve a specific estimation error rate , the number of samplesm = \u2126 (( \u03bbB2 )2 D ln( 1\u03b4 ) ) are sufficient to conclude, with confidence at least 1\u2212 \u03b4, the empirical risk minimizing metric M\u2217m will have estimation error of at most . This shows that one never needs more than a number proportional to the representation dimension D examples to achieve the desired level of accuracy.\nSince typical applications have a large representation dimension, it is instructive to study if such a strong dependency on D necessary. It turns out that even for simple distance-based loss functions like \u03c6\u03bbL,U (cf. Eq. 1), there are data distributions for which one cannot get away with fewer than linear in D samples and ensure good estimation errors. In particular we have the following.\nLemma 2 Let A be any algorithm that, given an i.i.d. sample Sm (of size m) from a fixed unknown bounded support distribution D, returns a weighting metric fromM that minimizes the empirical\n2We only present the results for paired distance comparisons; the results are easily extended to triplet-based comparisons.\nerror with respect to distance-based loss function \u03c6\u03bbL,U . There exist \u03bb \u2265 0, 0 \u2264 U < L, such that for all 0 < , \u03b4 < 1/64, there exists a bounded support distribution D, such that if m \u2264 D+1512 2 then\nPSm [ err\u03bbdist(A(Sm),D)\u2212 err\u03bbdist(M\u2217,D) > ] > \u03b4.\nWhile this may seem discouraging for large-scale applications of metric learning, note that here we made no assumptions about the underlying structure of the data distribution D, making this a worst-case analysis. As the individual features in real-world datasets contain varying amounts of information for good classification performance, one hopes for a more relaxed dependence on D for metric learning in these settings. This is explored in Section 4."}, {"heading": "3.2 Classifier-Based Error Analysis", "text": "In this setting, we can use an i.i.d. sequence of observations z1, z2, . . . from D to obtain the sample Sm = {zi}mi=1 of size m directly. To analyze the generalization ability of the weighting metrics optimized with respect to an underlying hypothesis class H, we need to effectively analyze the classification complexity of H. The scale sensitive version of VC-dimension, also known as the \u201cfat-shattering dimension\u201d, of a real-valued hypothesis class (denoted by Fat\u03b3(H)) encodes the right notion of classification complexity and provides an intuitive way to relate the generalization error to the empirical error at a margin \u03b3 (see for instance the work of Anthony & Bartlett (1999) for an excellent discussion).\nIn the context of metric learning with respect to a fixed hypothesis class, define the empirical error at a margin \u03b3 as\nerr\u03b3hypoth(M,Sm) := inf h\u2208H\n1\nm \u2211 (xi,yi)\u2208Sm 1[Margin(h(Mxi), yi) < \u03b3],\nwhere Margin(y\u0302, y) := { y\u0302 \u2212 1/2 if y = 1\n1/2\u2212 y\u0302 otherwise .\nThen for any bounded support distribution D (that is, each (x, y) \u223c D, \u2016x\u2016 \u2264 B < \u221e), we have the following convergence result that relates the estimation error rate of the weighting metrics with that of the fat-shattering dimension of the underlying base hypothesis class.\nLemma 3 Let H be a \u03bb-Lipschitz base hypothesis class. Pick any 0 < \u03b3 < 1/2, and let m \u2265 Fat\u03b3/16(H) \u2265 1. Then with probability at least 1 \u2212 \u03b4 over an i.i.d. draw of sample Sm (of size m) from a bounded unknown distribution D (with bound B) on X \u00d7 {0, 1},\nsup M\u2208M\n[ errhypoth(M,D)\u2212 err\u03b3hypoth(M,Sm) ] \u2264 O\n(\u221a 1\nm ln\n1 \u03b4 + D2 m ln D 0 + Fat\u03b3/16(H) m ln (m \u03b3 )) .\nwhere 0 := min{\u03b32 , 1 2\u03bbB }, and Fat\u03b3/16(H) is the fat-shattering dimension of the base hypothesis classH at margin \u03b3/16.\nUsing a similar line of argument as before, we can bound the key quantity of interest (Eq. 2) and conclude for any 0 < \u03b3 < 1/2 and any m \u2265 1, with probability \u2265 1\u2212 \u03b4\nerrhypoth(M\u2217m,D)\u2212 err \u03b3 hypoth(M \u2217,D) = O\n(\u221a D2 ln(D/ 0)\nm + Fat\u03b3/16(H) ln(m/\u03b4\u03b3) m\n) .\nHere 0 = min{\u03b32 , 1 2\u03bbB } for a \u03bb-Lipschitz hypothesis classH. Thus to achieve a specific estimation error rate , the number of samples m = \u2126 ( D2 ln(\u03bbDB/\u03b3)+Fat\u03b3/16(H) ln(1/\u03b4\u03b3)\n2\n) suffices to say, with\nconfidence at least 1 \u2212 \u03b4, the empirical risk minimizing metric M\u2217m will have estimation error at most .\nIt is interesting to note that the task of finding an optimal metric only additively increases the sample complexity over the complexity of finding the optimal hypothesis from the underlying hypothesis class.\nIn contrast to the sample complexity of distance-based framework (c.f. Lemma 1), here we get a quadratic dependence on the representation dimension. The following lemma shows that a strong dependence on the representation dimension is necessary in absence of any specific assumptions on the underlying data distribution and the base hypothesis class.\nLemma 4 Pick any 0 < \u03b3 < 1/8. Let H be a base hypothesis class of \u03bb-Lipschitz functions mapping from X = RD into the interval [1/2 \u2212 4\u03b3, 1/2 + 4\u03b3] that is closed under addition of constants. That is\nh \u2208 H =\u21d2 h\u2032 \u2208 H, where h\u2032 : x\u2192 h(x) + c for all c.\nThen for any classification algorithm A, and for any B \u2265 1, there exists \u03bb \u2265 0, for all 0 < , \u03b4 < 1/64, there exists a bounded support distribution D (with bound B) such that if m ln2m < O (\nD2+d 2 ln(1/\u03b32) ) PSm\u223cD[errhypoth(h \u2217,D) > err\u03b3hypoth(A(Sm),D) + ] > \u03b4,\nwhere d := Fat768\u03b3(H) is the fat-shattering dimension ofH at margin 768\u03b3."}, {"heading": "4 Data with Uninformative and Weakly Informative Features", "text": "Different measurements have varying degrees of \u201cinformation content\u201d for the particular supervised classification task of interest. Any algorithm or analysis that studies the design of effective comparisons between observations must account for this variability.\nTo get a solid footing for our study, we introduce the concept of metric learning complexity of a given dataset. Our key observation is that a metric that yields good generalization performance should emphasize relevant features while suppressing the contribution of spurious features. Thus, a good metric reflects the quality of individual feature measurements of data and their relative value for the learning task. We can leverage this and define the metric learning complexity of a given dataset as the intrinsic complexity d of the weighting metric that yields the best generalization performance for that dataset (if multiple metrics yield best performance, we select the one with minimum d). A natural way to characterize the intrinsic complexity of a weighting metric M is via the norm of the matrix representation of M . Using metric learning complexity as our gauge for the richness of the feature set in a given dataset, we can refine our analysis in both our canonical metric learning frameworks."}, {"heading": "4.1 Distance-Based Refinement", "text": "We start with the following refinement of the distance-based metric learning sample complexity for a class of Frobenius norm-bounded weighting metrics.\nLemma 5 LetM be any class of weighting metrics on the feature space X = RD. Fix any sample size m, and let Sm be an i.i.d. paired sample of size m from an unknown bounded distribution D on X \u00d7 {0, 1} (with bound B). For any distance-based loss function \u03c6\u03bb that is \u03bb-Lipschitz in the first argument, with probability at least 1\u2212 \u03b4 over the draw of Sm,\nsup M\u2208M\n[ err\u03bbdist(M,D)\u2212 err\u03bbdist(M,Sm) ] \u2264 O ( \u03bbB2 \u221a d ln(1/\u03b4)\nm\n) ,\nwhere d is a uniform upperbound on the Frobenius norm of the quadratic form of weighting metrics inM, that is, supM\u2208M \u2016MTM\u20162F \u2264 d.\nObserve that if our dataset has a low metric learning complexity (say, d D), then considering an appropriate class of norm-bounded weighting metrics can help sharpen the sample complexity result, yielding a dataset-dependent bound. We discuss how to automatically adapt to the right complexity class in Section 4.3 below."}, {"heading": "4.2 Classifier-Based Refinement", "text": "Effective data-dependent analysis of classifier-based metric learning requires accounting for potentially complex interactions between an arbitrary base hypothesis class and the distortion induced by a weighting metric to the unknown underlying data distribution. To make the analysis tractable while still keeping our base hypothesis class H general, we shall assume that H is a class of two layer feed-forward neural networks. Recall that for any smooth target function f\u2217, a two layer feed-forward neural network (with appropriate number of hidden units and connection weights) can approximate f\u2217 arbitrarily well (Hornik et al., 1989), so this class is flexible enough to incorporate most reasonable target hypotheses.\nMore formally, define the base hypothesis class of two layer feed-forward neural network with K hidden units as\nH2-net\u03c3\u03b3 := { x 7\u2192 K\u2211 i=1 wi \u03c3 \u03b3(vi \u00b7 x) \u2223\u2223\u2223 \u2016w\u20161 \u2264 1, \u2016vi\u20161 \u2264 1}, where \u03c3\u03b3 : R \u2192 [\u22121, 1] is a smooth, strictly monotonic, \u03b3-Lipschitz activation function with \u03c3\u03b3(0) = 0. Then for the generalization error of a weighting metric M defined with respect to any classifier-based \u03bb-Lipschitz loss function \u03c6\u03bb\nerr\u03bbhypoth(M,D) := inf h\u2208H2-net\n\u03c3\u03b3\nE(x,y)\u223cD [ \u03c6\u03bb ( h(Mx), y )] ,\nwe have the following.3\nLemma 6 LetM be any class of weighting metrics on the feature space X = RD. For any \u03b3 > 0, let H2-net\u03c3\u03b3 be a two layer feed-forward neural network base hypothesis class (as defined above) and \u03c6\u03bb be a classifier-based loss function that \u03bb-Lipschitz in its first argument. Fix any sample size m, and let Sm be an i.i.d. sample of size m from an unknown bounded distribution D on X \u00d7 {0, 1} (with bound B). Then with probability at least 1\u2212 \u03b4,\nsup M\u2208M\n[ err\u03bbhypoth(M,D)\u2212 err\u03bbhypoth(M,Sm) ] \u2264 O ( B\u03bb\u03b3 \u221a d ln(D/\u03b4)\nm\n) ,\n3Since we know the functional form of the base hypothesis class H (i.e., a two layer feed-forward neural net), we can provide a more precise bound than leaving it as Fat(H).\nwhere d is a uniform upperbound on the Frobenius norm of the quadratic form of weighting metrics inM, that is, supM\u2208M \u2016MTM\u20162F \u2264 d."}, {"heading": "4.3 Automatically Adapting to Intrinsic Complexity", "text": "Note that while Lemmas 5 and 6 provide a sample complexity bound that is tuned to the metric learning complexity of a given dataset, these results are not useful directly since one cannot select the correct norm bounded classM a priori (as the underlying distribution D is unknown).\nFortunately, by considering an appropriate sequence of norm-bounded classes of weighting metrics, we can provide a uniform bound that automatically adapts to the intrinsic complexity of the unknown underlying data distribution D. In particular, we have the following.\nCorollary 7 Fix anym, and let Sm be an i.i.d. sample of sizem from an unknown bounded distribution D (with bound B). DefineMd := {M | \u2016MTM\u20162\nF \u2264 d}, and consider the nested sequence of\nweighting metric classM1 \u2282M2 \u2282 \u00b7 \u00b7 \u00b7 . Let \u00b5d be any non-negative measure across the sequence Md such that \u2211 d \u00b5d = 1 (for d = 1, 2, \u00b7 \u00b7 \u00b7 ). Then for any \u03bb \u2265 0, with probability at least 1 \u2212 \u03b4, for all d = 1, 2, \u00b7 \u00b7 \u00b7 , and all Md \u2208Md,\n[ err\u03bb(Md,D)\u2212 err\u03bb(Md, Sm) ] \u2264 O ( C \u00b7B\u03bb \u221a d ln(1/\u03b4\u00b5d)\nm\n) , (3)\nwhere C := B for distance-based error, or C := \u03b3 \u221a\nlnD for classifier-based error (with base hypothesis classH2-net\u03c3\u03b3 ).\nIn particular, for a data distribution D that has metric learning complexity at most d \u2208 N, if there are m \u2265 \u2126 ( d(CB\u03bb)2 ln(1/\u03b4\u00b5d)\n2 ) samples, then with probability at least 1\u2212 \u03b4[\nerr\u03bb(M regm ,D)\u2212 err\u03bb(M\u2217,D) ] \u2264 O( ),\nforM regm :=argminM\u2208M [ err\u03bb(M,Sm) + \u039bMdM ] , where \u039b M := CB\u03bb \u221a ln (\n1 \u03b4\u00b5 dM\n) /m and d\nM :=\u2308\n\u2016MTM\u20162 F\n\u2309 .\nObserve that the measure (\u00b5d) above encodes our prior belief on the complexity classMd from which a target metric is selected by a metric learning algorithm given the training sample Sm. In absence of any prior beliefs, \u00b5d can be simply set to 1/D (for d = 1, . . . , D) for unit spectral-norm weighting metrics.\nThus, for an unknown underlying data distribution D with metric learning complexity d, with number of samples just proportional to d, we can find a good weighting metric.\nThis result also highlights that the generalization error of any weighting metric returned by an algorithm is proportional to the (smallest) norm-bounded class to which it belongs (cf. Eq. 3). If two metrics M1 and M2 have similar empirical errors on a given sample, but have different intrinsic complexities, then the expected risk of the two metrics can be considerably different. We expect the metric with lower intrinsic complexity to yield better generalization error. This partly explains the observed empirical success of various types of norm-regularized optimization criteria for finding the optimal weighting metric (Lim et al., 2013; Law et al., 2014).\nUsing this as a guiding principle, we can design an improved optimization criteria for metric learning problems that jointly minimizes the sample error and a Frobenius norm regularization penalty. In particular,\nmin M\u2208M\nerr(M,Sm) + \u039b \u2016MTM\u20162F (4)\nfor any error criteria \u2018err\u2019 used in a downstream prediction task of interest and a regularization hyper-parameter \u039b proportional to m\u22121/2. We explore the practical efficacy of this augmented optimization on some representative applications below."}, {"heading": "5 Empirical Evaluation", "text": "Our analysis shows that the generalization error of metric learning can scale with the representation dimension, and regularization can help mitigate this by adapting to the intrinsic metric learning complexity of the given dataset. We want to explore to what degree these effects manifest in practice.\nWe select two popular metric learning algorithms, LMNN by Weinberger & Saul (2009) and ITML by Davis et al. (2007), that are designed to find metrics that improve nearest-neighbor classification quality. These algorithms have varying degrees of regularization built into their optimization criteria: LMNN implicitly regularizes the metric via its \u201clarge margin\u201d criterion, while ITML allows for explicit regularization by letting the practitioners specify a \u201cprior\u201d weighting metric. We modified the LMNN optimization criteria as per Eq. (4) to also allow for an explicit norm-regularization controlled by the trade-off parameter \u039b.\nWe can evaluate how the unregularized criteria (i.e., unmodified LMNN, or ITML with the prior set to the identity matrix) compares to the regularized criteria (i.e., modified LMNN with best \u039b, or ITML with the prior set to a low-rank matrix).\nDatasets. We use the UCI benchmark datasets for our experiments: IRIS (4 dim., 150 samples), WINE (13 dim., 178 samples) and IONOSPHERE (34 dim., 351 samples) datasets (Bache & Lichman, 2013). Each dataset has a fixed (unknown) intrinsic dimension; we can vary the representation dimension by augmenting each dataset with synthetic correlated noise of varying dimensions, simulating regimes where datasets contain large numbers of uninformative features.\nEach UCI dataset is augmented with synthetic D-dimensional correlated noise as follows. We first sample a covariance matrix \u03a3D from unit-scale Wishart distribution (that is, let A be a D \u00d7D Gaussian random matrix with entry Aij \u223c N(0, 1) drawn i.i.d., and set \u03a3D := ATA). Then each sample xi from the dataset is appended independently by drawing noise vector x\u03c3 \u223c N(0,\u03a3D).\nExperimental setup. We varied the ambient noise dimension D between 0 and 500 dimensions and\nadded it to the UCI datasets, creating the noise-augmented datasets. Each noise-augmented dataset was randomly split between 70% training, 10% validation, and 20% test samples.\nWe used the default settings for each algorithm. For regularized LMNN, we picked the best performing trade-off parameter \u039b from {0, 0.1, 0.2, ..., 1} on the validation set. For regularized ITML, we seeded with the rank-one discriminating metric, i.e., we set the prior as the matrix with all zeros, except the diagonal entry corresponding to the most discriminating coordinate set to one.\nAll the reported results were averaged over 20 runs.\nResults. Figure 1 shows the nearest-neighbor performance (with k = 3) of LMNN and ITML on noise-augmented UCI datasets. Notice that the unregularized versions of both algorithms (dashed red lines) scale poorly when noisy features are introduced. As the number of uninformative features grows, the performance of both algorithms quickly degrades to that of classification performance in the original unweighted space with no metric learning (solid gray line), showing poor adaptability to the signal in the data.\nInterestingly, neither of the unregularized algorithms performs consistently better than the other on datasets with high noise: ITML yields better results on WINE, whereas LMNN seems better for IONOSPHERE, and both algorithms yield similar performance on IRIS.\nThe regularized versions of both algorithms (solid blue lines) significantly improve the classification performance. Remarkably, regularized ITML shows almost no degradation in classification performance, even in very high noise regimes, demonstrating a strong robustness to noise.\nThese results underscore the value of regularization in metric learning, showing that regularization encourages adaptability to the intrinsic complexity and improved robustness to noise."}, {"heading": "6 Discussion and Conclusion", "text": "Previous theoretical work on metric learning has focused almost exclusively on analyzing the generalization error of variants of the optimization criteria for the distance-based metric learning framework.\nJin et al. (2009), for instance, analyzed the generalization ability of regularized, convex-loss optimization criteria for pairwise distances via an algorithmic stability analysis. They derive an interesting sample complexity result that is sublinear in \u221a D for datasets of representation dimension D. They discuss that the sample complexity can potentially be independent of D, but do not characterize specific instances or classes of problems where this may be possible.\nLikewise, recent work by Bellet & Habrard (2012) uses algorithmic robustness to analyze the generalization ability for pairwise- and triplet-based distance metric learning. Their analysis relies on the existence of a partition of the input space, such that in each cell of the partition, the training loss and test loss does not deviate much (robustness criteria). Note that their sample complexity bound scales with the partition size, which in general can be exponential in the representation dimension.\nPerhaps the works most similar to our approach are the sample complexity analyses by Bian & Tao (2011) and Cao et al. (2013). Bian & Tao (2011) analyze the consistency of the ERM criterion for metric learning. They show a O(m\u22121/2) rate of convergence for the ERM with m samples to the expected risk for thresholds on bounded convex losses for distance-based metric learning. Our upper-bound in Lemma 1 generalizes this result by considering arbitrary (possibly non-convex) distance-based Lipschitz losses and explicitly shows the dependence on the representation dimensionD. Cao et al. (2013) provide an alternate analysis based on norm regularization of the weighting metric for distance-based metric learning. Their result parallels our norm-regularized criterion in\nLemma 5. While they focus on analyzing a specific optimization criterion \u2013 thresholds on the hinge loss with norm-regularization, our result holds for general Lipschitz losses.\nIt is worth emphasizing that none of these related works discuss the importance of or leverage the intrinsic structure in data for the metric learning problem. Our results in Section 4 formalize an intuitive notion of dataset\u2019s intrinsic complexity for metric learning and show sample complexity rates that are finely tuned to this metric learning complexity.\nThe classifier-based framework we discuss has parallels with the kernel learning literature. The typical focus in kernel learning is to analyze the generalization ability of the hypothesis class of linear separators in general Hilbert spaces (Ying & Campbell, 2009; Cortes et al., 2010). Our work provides a complementary analysis for learning explicit linear transformations of the given representation space for arbitrary hypotheses classes.\nOur theoretical analysis partly justifies the empirical success of norm-based regularization as well. Our empirical results show that such regularization not only helps in designing new metric learning algorithms (Lim et al., 2013; Law et al., 2014), but can even benefit existing metric learning algorithms in high-noise regimes."}, {"heading": "A Appendix: Various Proofs", "text": ""}, {"heading": "A.1 Proof of Lemma 1", "text": "Let P be the probability measure induced by the random variable (X, Y ), where X := (x, x\u2032), Y := 1[y = y\u2032], st. ((x, y), (x\u2032, y\u2032)) \u223c (D\u00d7D).\nDefine function class\nF := { fM : X 7\u2192 \u2016M(x\u2212 x\u2032)\u20162 \u2223\u2223\u2223\u2223\u2223 M \u2208MX = (x, x\u2032) \u2208 (X \u00d7X) } ,\nand consider any loss function \u03c6\u03bb(\u03c1, Y ) that is \u03bb-Lipschitz in the first argument. Then, we are interested in bounding the quantity\nsup fM\u2208F\nE(X,Y )\u223cP [\u03c6\u03bb(fM (X), Y )]\u2212 1\nm m\u2211 i=1 \u03c6\u03bb(fM (Xi), Yi),\nwhere Xi := (x1,i, x2,i), Yi := 1[y1,i = y2,i] from the paired sample Sm = {((x1,i, y1,i), (x2,i, y2,i))}mi=1. Define x\u0304i := x1,i \u2212 x2,i for each Xi = (x1,i, x2,i). Then, the Rademacher complexity4 of our function class F (with respect to the distribution P) is bounded, since (let \u03c31, . . . , \u03c3m denote independent uniform {\u00b11}-valued random variables)\nRm(F ,P) := EXi,\u03c3i i\u2208[m] [ sup fM\u2208F 1 m m\u2211 i=1 \u03c3ifM (Xi) ]\n= 1\nm EXi,\u03c3i i\u2208[m] sup M\u2208M [ m\u2211 i=1 \u03c3ix\u0304 T iM TMx\u0304i ] = 1\nm EXi,\u03c3i i\u2208[m] sup M\u2208M, s.t.\n[ajk]jk:=M TM\n[\u2211 j,k ajk m\u2211 i=1 \u03c3ix\u0304 j i x\u0304 k i ]\n\u2264 1 m EXi,\u03c3i i\u2208[m] sup M\u2208M\n[ \u2016MTM\u2016F (\u2211 j,k ( m\u2211 i=1 \u03c3ix\u0304 j i x\u0304 k i )2)1/2]\n\u2264 \u221a D\nm EXi,i\u2208[m]\n( E\u03c3i,i\u2208[m] \u2211 j,k ( m\u2211 i=1 \u03c3ix\u0304 j i x\u0304 k i )2)1/2\n=\n\u221a D\nm EXi,i\u2208[m] (\u2211 j,k m\u2211 i=1 ( x\u0304ji )2( x\u0304ki )2)1/2\n=\n\u221a D\nm EXi,i\u2208[m] ( m\u2211 i=1 \u2016x\u0304i\u20164 )1/2\n=\n\u221a D\nm E(xi,x\u2032i)\u223c(D |X\u00d7D |X),\ni\u2208[m]\n( m\u2211 i=1 \u2016xi \u2212 x\u2032i\u20164 )1/2\n4See the definition of Rademacher complexity in the statement of Lemma 8.\n\u2264 \u221a D\nm\n( E(x,x\u2032)\u223c(D |X\u00d7D |X)\u2016x\u2212 x \u2032\u20164 )1/2\n\u2264 4B2 \u221a D\nm ,\nwhere the second inequality is by noting that supM\u2208M \u2016MTM\u2016F \u2264 \u221a D for the class of weighting metricsM := { M |M \u2208 RD\u00d7D, \u03c3max(M) = 1 } .\nRecall that D has bounded support (with bound B). Thus, by noting that \u03c6\u03bb is 8B2 bounded function that is \u03bb-Lipschitz in the first argument, we can apply Lemma 8 and get the desired uniform deviation bound.\nLemma 8 [Rademacher complexity of bounded Lipschitz loss functions Bartlett & Mendelson (2002)] Let D be a fixed unknown distribution over X \u00d7{\u22121, 1}, and let Sm be an i.i.d. sample of size m from D. Given a hypothesis class H \u2282 RX and a loss function ` : R\u00d7{\u22121, 1} \u2192 R, such that ` is c-bounded, and is \u03bb-Lipschitz in the first argument, that is, sup(y\u2032,y)\u2208R\u00d7{\u22121,1} |`(y\u2032, y)| \u2264 c, and |`(y\u2032, y)\u2212 `(y\u2032\u2032, y)| \u2264 \u03bb|y\u2032 \u2212 y\u2032\u2032|, we have the following:\nfor any 0 < \u03b4 < 1, with probability at least 1\u2212 \u03b4, every h \u2208 H satisfies\nerr(` \u25e6 h,D) \u2264 err(` \u25e6 h, Sm) + 2\u03bbRm(H,D) + c \u221a 2 ln(1/\u03b4)\nm ,\nwhere\n\u2022 err(` \u25e6 h,D) := Ex,y\u223cD[`(h(x), y)], \u2022 err(h, Sm) := 1m \u2211 (xi,yi)\u2208Sm `(h(xi), yi),\n\u2022 Rm(H,D) is the Rademacher complexity of the function classH with respect to the distribution D given m i.i.d. samples, and is defined as:\nRm(H,D) := E xi\u223cD |X , \u03c3i\u223cunif{\u00b11},\ni\u2208[m]\n[ sup h\u2208H 1 m m\u2211 i=1 \u03c3ih(xi) ] ,\nwhere \u03c3i are independent uniform {\u00b11}-valued random variables."}, {"heading": "A.2 Proof of Lemma 2", "text": "We shall exhibit a finite class of bounded support distributions D, such that ifD is chosen uniformly at random from D, the expectation (over the random choice ofD) of the probability of failure (that is, generalization error of the metric returned by A compared to that of the optimal metric exceeds the specified tolerance level ) is at least \u03b4. This implies that for some distribution in D the probability of failure is at least \u03b4 as well.\nLet \u2206D := {x0, . . . , xD} be a set of D + 1 points that from the vertices of a regular unitsimplex from the underlying space X = RD as per Definition 1 (see below). For a fixed parameter 0 < \u03b1 < 1 (exact value determined later), define D as the class of all distributions D on X \u00d7{0, 1} such that:\n\u2022 D assigns zero probability to all sets not intersecting \u2206D \u00d7 {0, 1}.\n\u2022 for each i = 0, . . . , D, either\n\u2013 P[(xi, 1)] = (1 + \u221a \u03b1)/2 and P[(xi, 0)] = (1\u2212 \u221a \u03b1)/2, or \u2013 P[(xi, 1)] = (1\u2212 \u221a \u03b1)/2 and P[(xi, 0)] = (1 + \u221a \u03b1)/2.\nFor concreteness, we shall use a specific instantiation of \u03c6\u03bbL,U in err \u03bb dist with U = 0, L = 4/D\nand \u03bb = D/4.\nProof overview. We first show, by the construction of the distributions under consideration in D, the sample error and the generalization error minimizing metrics over any D \u2208 D belong to a restricted class of weighting matrices (Eq. 5). We then make a second simplification by noting that finding these (sample- and generalization-) error minimizing metrics (in the restricted class) is equivalent to solving a binary classification problem (Eq. 6). This reduction to binary classification enables us to use VC-style lower bounding techniques to give a lower bound on the sample complexity. We now fill in the details.\nConsider a subset of weighting metrics M0-1 that map points in \u2206D to exactly one of two possible points that are (squared) distance at least 4/D apart, that is,\nM0-1 := {M |M \u2208M,\u2203z0, z1 \u2208 RD,\u2200x \u2208 \u2206D, Mx \u2208 {z0, z1} and \u2016z0 \u2212 z1\u20162 \u2265 4/D}.\nNow pick any D \u2208 D, let Sm be an i.i.d. paired sample from D. Observe that both the samplebased and the distribution-based error minimizing weighting metric fromM on D also belongs to M0-1. That is, (c.f. Lemma 10)\nargminM\u2208M errdist(M,D) = argminM\u2208M0-1 errdist(M,D) argminM\u2208M errdist(M,Sm) = argminM\u2208M0-1 errdist(M,Sm). (5)\nA reduction to binary classification on product space. For each M \u2208 M0-1, we associate a classifier fM : (\u2206D \u00d7 \u2206D) \u2192 {0, 1} defined as (xi, xj) 7\u2192 1[Mxi = Mxj ]. Now, consider the probability measureP induced by the random variable (X, Y ), where X := (x, x\u2032), Y := 1[y = y\u2032], s.t. ((x, y), (x\u2032, y\u2032)) \u223c ( D |(\u2206D\u00d7{0,1}) \u00d7D |(\u2206D\u00d7{0,1}) ) . It is easy to check that for all M \u2208M0-1\nerr\u03bbdist(M,D) = E(X,Y )\u223cP [ 1[fM (X) 6= Y ] ] err\u03bbdist(M,Sm) = 1\nm \u2211 ((x,y),(x\u2032,y\u2032))\u2208Sm 1 [ fM ((x, x \u2032)) 6= 1[y = y\u2032] ] . (6)\nDefine\n\u03b7(X) := PY\u223cP|Y |X [Y = 1|X] = P(y,y\u2032)\u223c(D\u00d7D)|(y,y\u2032)|(x,x\u2032) [y = y \u2032|x, x\u2032]\n=\n{ 1 2 + \u03b1 2 if P(y|x) = P(y\n\u2032|x\u2032) 1 2 \u2212 \u03b1 2 if P(y|x) 6= P(y \u2032|x\u2032) . (7)\nObserve that \u03b7(X) is the Bayes error rate at X for distribution P . Since, by construction ofM0-1, the class {fM}M\u2208M0-1 contains a classifier that achieves the Bayes error rate, the optimal classifier\nf\u2217 := argminfM E(X,Y )\u223cP 1[fM (X) 6= Y ] necessarily has f \u2217(X) = 1[\u03b7(X) > 12 ] (for all X). Then, for any fM ,\nE(X,Y )\u223cP [ 1[fM (X) 6= Y ] ] \u2212 E(X,Y )\u223cP [ 1[f\u2217(X) 6= Y ] ] = EX\u223cP|X [ \u03b7(X) ( 1[f\u2217(X) = 1]\u2212 1[fM (X) = 1]\n) + (1\u2212 \u03b7(X)) ( 1[f\u2217(X) = 0]\u2212 1[fM (X) = 0]\n)] = EX\u223cP|X [ (2\u03b7(X)\u2212 1) ( 1[f\u2217(X) = 1]\u2212 1[fM (X) = 1]\n)] = EX\u223cP|X [ 2|\u03b7(X)\u2212 1/2| \u00b7 1[fM (X) 6= f\u2217(X)]\n] = 2\u03b1\n(D + 1)2 \u2211 i>j [ 1[fM ((xi, xj)) 6= f\u2217((xi, xj))] ] , (8)\nwhere (i) the second to last equality is by noting that f\u2217(X) 6= 1 \u21d0\u21d2 \u03b7(X) \u2264 1/2, and (ii) the last equality is by noting Eq. (7), fM ((xi, xi)) = f\u2217((xi, xi)) = 1 for all i and f((xi, xj)) = f((xj , xi)) for all f . For notational simplicity, we shall define Xi,j := (xi, xj).\nNow, for a given paired sample Sm, let N(Sm) := (Ni)i (for all 0 \u2264 i \u2264 D), where Ni is the number of occurrences of the point xi in Sm. Then for any fM ,\nESm\n[ 1\n(D + 1)2 \u2211 i>j 1[fM (Xi,j) 6= f\u2217(Xi,j)]\n]\n= 1\n(D + 1)2 \u2211 i>j PSm [fM (Xi,j) 6= f\u2217(Xi,j)]\n= 1\n(D + 1)2 \u2211 i>j \u2211 N\u2208ND+1 PSm [fM (Xi,j) 6= f\u2217(Xi,j)|N(Sm) = N ] \u00b7P[N(Sm) = N ]\n= 1\n(D + 1)2 \u2211 N\u2208ND+1 P[N(Sm) = N ] \u00b7 \u2211 i>j PSm [fM (Xi,j) 6= f\u2217(Xi,j)|Ni, Nj ]\n\u2265 1 (D + 1)2 \u2211 N\u2208ND+1 P[N(Sm) = N ] \u00b7 \u2211 i>j 1 4\n( 1\u2212 \u221a\u221a\u221a\u221a1\u2212 exp(\u2212(max{Ni, Nj}+ 1)\u03b12 1\u2212 \u03b12 ))\n\u2265 1 4 D D + 1\n( 1\u2212 \u221a\u221a\u221a\u221a1\u2212 exp(\u2212((2m/(D + 1)) + 1)\u03b12 1\u2212 \u03b12 ))\n\u2265 1 8\n( 1\u2212 \u221a\u221a\u221a\u221a1\u2212 exp(\u2212((2m/(D + 1)) + 1)\u03b12 1\u2212 \u03b12 )) ,\nwhere (i) the first inequality is by applying Lemma 11, (ii) the second inequality is by assuming WLOG Ni \u2265 Nj , and noting that the expression above is convex in Ni so one can apply Jensen\u2019s inequality and by observing that E[Ni] = 2m/(D+ 1) and that there are total D(D+ 1) summands for i > j, and (iii) the last inequality is by noting that D \u2265 1. Now, let B denote the r.h.s. quantity above. Then by recalling that for any [0, 1]-valued random variable Z, P(Z > \u03b3) > EZ\u2212 \u03b3 (for all 0 < \u03b3 < 1), we have\nPSm [ 1 (D + 1)2 \u2211 i>j 1[fM ((xi, xj)) 6= f\u2217((xi, xj))] > \u03b3B ] > (1\u2212 \u03b3)B.\nOr equivalently, by combining Eqs. (5), (6) and (8), we have ED\u223cunif(D)PSm\u223cD [ errdist(A(Sm),D)\u2212 errdist(M\u2217D,D) > 2\u03b1\u03b3B ] > (1\u2212 \u03b3)B,\nwhere M\u2217D := argminM\u2208M errdist(M,D) and A(Sm) is any metric returned by empirical error minimizing algorithm. Now, if (cond. 1) B \u2265 \u03b4/1 \u2212 \u03b3 and (cond. 2) \u2264 2\u03b3\u03b1B, it follows that for some D \u2208 D\nPSm\u223cD [ errdist(A(Sm),D)\u2212 errdist(M\u2217D,D) > ] > \u03b4. (9)\nNow, to satisfy cond. 1 & 2, we shall select \u03b3 = 1\u2212 16\u03b4. Then cond. 1 follows if\nm \u2264 (D + 1) 2\n( 1\u2212 \u03b12\n\u03b12 ln(4/3)\u2212 1\n) .\nChoosing parameter \u03b1 = 8 /\u03b3 (and by noting B \u2265 1/16 by cond. 1 for choice of \u03b3 and m), cond. 2 is satisfied as well. Hence,\nm \u2264 (D + 1) 2\n( (1\u2212 16\u03b4)2 \u2212 (8 )2\n64 2 ln(4/3)\u2212 1\n)\nimplies Eq. (9). Moreover, if 0 < , \u03b4 < 1/64 then m \u2264 (D+1)512 2 would suffice.\nDefinition 1 Define n+ 1 vectors \u2206n = {v0, . . . , vn}, with each vi \u2208 Rn as\nv0,j = \u22121\u221a n\nfor 1 \u2264 j \u2264 n\nvi,j =\n{ (n\u22121) \u221a n+1+1\nn \u221a n if i = j \u2212( \u221a n+1\u22121) n \u221a n otherwise for 1 \u2264 i, j \u2264 n\nFact 9 [properties of vertices of a regular n-simplex] Let \u2206n = {v0, . . . , vn} be a set of n + 1 vectors in Rn as per Definition 1. Then, \u2206n defines vertices of a regular n-simplex circumscribed in a unit (n\u2212 1)-sphere, with\n(i) \u2016vi\u20162 = 1 (for all i), and\n(ii) \u2016vi \u2212 vj\u20162 = 2(n+ 1)/n (for i 6= j).\nMoreover, for any non-empty bi-partition of \u2206n into \u2206 (1) n and \u2206 (2) n with |\u2206(1)n | = k and |\u2206(2)n | = n + 1 \u2212 k, define a(1) and a(2) the means (centroids) of the points in \u2206(1)n and \u2206(2)n respectively. Then, we also have\n(i) (a(1) \u2212 a(2)) \u00b7 (a(i) \u2212 vj) = 0 (for i \u2208 {1, 2}, and vj \u2208 \u2206(i)n ).\n(ii) \u2016a(1) \u2212 a(2)\u20162 = (n+1) 2\nkn(n+1\u2212k) \u2265 4 n , for 1 \u2264 k \u2264 n.\nLemma 10 Let \u2206D be a set of D + 1 points {X0, . . . , XD} in RD as per Definition 1, and let D be an arbitrary distribution over \u2206D \u00d7 {0, 1}. Define Pi := 1[PD[(Xi, 1)] > 1/2]. Define \u03a0 := {\u03c0 : \u2206D \u2192 RD} be the collection of all functions that maps points in \u2206D to arbitrary points in RD. Define\nf((x, y),(x\u2032, y\u2032);\u03c0) :=\n{ min{1, D4 \u2016\u03c0(x)\u2212 \u03c0(x\n\u2032)\u20162} if y = y\u2032 min{1, [1\u2212 D4 \u2016\u03c0(x)\u2212 \u03c0(x \u2032)\u20162]+} if y 6= y\u2032 .\nLet E(\u03c0) := E(x,y),(x\u2032,y\u2032)\u223cD\u00d7D[f((x, y), (x\u2032, y\u2032);\u03c0)] and E\u2217 := inf\u03c0 E(\u03c0). Then, for any \u03c0\u0304 \u2208 \u03a0 such that\n(i) \u03c0\u0304(Xi) = \u03c0\u0304(Xj), if Pi = Pj\n(ii) \u2016\u03c0\u0304(Xi)\u2212 \u03c0\u0304(Xj)\u20162 \u2265 4D , if Pi 6= Pj , we have that E(\u03c0\u0304) = E\u2217. Moreover, define A\u0304 as \u2022 A\u0304 := A1\u2212A0\u2016A1\u2212A0\u2016 , where A0 := mean(Xi) such that Pi = 0, and A1 := mean(Xi) such that Pi = 1 (if exists at least one Pi = 0 and at least one Pi = 1).\n\u2022 A\u0304 := 0, i.e. the zero vector in RD (otherwise). And let M be a D \u00d7D matrix (with \u03c3max(M) = 1) defined as\nM := A\u0304A\u0304 T .\nThen the map \u03c0M : x 7\u2192 Mx constitutes a map that satisfies conditions (i) and (ii) and thus E(\u03c0M ) = E\u2217.\nProof. The proof follows from the geometric properties of \u2206D and Fact 9.\nLemma 11 Given two random variables \u03b11 and \u03b12, each uniformly distributed on {\u03b1\u2212, \u03b1+} independently, where \u03b1\u2212 = 1/2 \u2212 /2 and \u03b1+ = 1/2 + /2 with 0 < < 1. Suppose that \u03be11 , . . . , \u03be1m and \u03be21 , . . . , \u03be 2 m are two i.i.d. sequences of {0, 1}-valued random variables with P(\u03be1i = 1) = \u03b11 and P(\u03be2i = 1) = \u03b12 for all i. Then, for any likelihood maximizing function f from {0, 1}m to {\u03b1\u2212, \u03b1+} that estimates the bias \u03b11 and \u03b12 from the samples,\nP [( f(\u03be11 , . . . , \u03be 1 m) 6= \u03b11 and f(\u03be21 , . . . , \u03be2m) = \u03b12 ) ,\nor ( f(\u03be11 , . . . , \u03be 1 m) = \u03b11 and f(\u03be 2 1 , . . . , \u03be 2 m) 6= \u03b12 )] > 1\n4\n( 1\u2212 \u221a 1\u2212 exp (\u22122dm/2e 2 1\u2212 2 )) .\nProof. Note that P [( f(\u03be11 , . . . , \u03be 1 m) 6= \u03b11 and f(\u03be21 , . . . , \u03be2m) = \u03b12 ) , or ( f(\u03be11 , . . . , \u03be 1 m) = \u03b11 and f(\u03be 2 1 , . . . , \u03be 2 m) 6= \u03b12 )] = P[f(\u03be11 , . . . , \u03be 1 m) 6= \u03b11] \u00b7P[f(\u03be21 , . . . , \u03be2m) = \u03b12] + P[f(\u03be11 , . . . , \u03be1m) = \u03b11] \u00b7P[f(\u03be21 , . . . , \u03be2m) 6= \u03b12]\n\u2265 1 2 P [ f(\u03be11 , . . . , \u03be 1 m) 6= \u03b11 ] + 1 2 P [ f(\u03be21 , . . . , \u03be 2 m) 6= \u03b12 ] > 1\n4\n( 1\u2212 \u221a 1\u2212 exp (\u22122dm/2e 2 1\u2212 2 )) ,\nwhere the first inequality is by noting that a likelihood maximizing f will select the correct bias better than random (which has probability 1/2), and the second inequality is by applying Lemma 12.\nLemma 12 [Lemma 5.1 of Anthony & Bartlett (1999)] Suppose that \u03b1 is a random variable uniformly distributed on {\u03b1\u2212, \u03b1+}, where \u03b1\u2212 = 1/2\u2212 /2 and \u03b1+ = 1/2 + /2, with 0 < < 1. Suppose that \u03be1, . . . , \u03bem are i.i.d. {0, 1}-valued random variables with P(\u03bei = 1) = \u03b1 for all i. Let f be a function from {0, 1}m to {\u03b1\u2212, \u03b1+}. Then\nP [ f(\u03be1, . . . , \u03bem) 6= \u03b1 ] > 1\n4\n( 1\u2212 \u221a 1\u2212 exp (\u22122dm/2e 2 1\u2212 2 )) ."}, {"heading": "A.3 Proof of Lemma 3", "text": "For any M \u2208 M define real-valued hypothesis class on domain X as HM := {x 7\u2192 h(Mx) : h \u2208 H} and define\nF := {x 7\u2192 h(Mx) : M \u2208M, h \u2208 H} = \u22c3 M HM .\nObserve that a uniform convergence of errors induced by the functions inF implies convergence of the class of weighted matrices as well.\nNow for any domain X , real-valued hypothesis class G \u2282 [0, 1]X , margin \u03b3 > 0, and a sample S \u2282 X , define\ncov\u03b3(G, S) := { C \u2282 G \u2223\u2223\u2223 \u2200g \u2208 G,\u2203g\u2032 \u2208 C, maxs\u2208S |g(s)\u2212 g\u2032(s)| \u2264 \u03b3 } as the set of \u03b3-covers of S by G. Let \u03b3-covering number of G for any integer m > 0 be defined as\nN\u221e(\u03b3,G,m) := max S\u2282X:|S|=m min C\u2208cov\u03b3(G,S) |C|,\nwith the minimizing cover C called as the minimizing (\u03b3,m)-cover of G\nNow, for the given \u03b3, we will first estimate the \u03b3-covering number of F , that is, N\u221e(\u03b3,F ,m). For any M \u2208 M, let HM be the minimizing (\u03b3/2,m)-cover of HM . Note that |HM | = N\u221e(\u03b3/2,HM ,m) \u2264 N\u221e(\u03b3/2,H,m) (because MX \u2282 X). Now letM be an -spectral cover ofM (that is, for every M \u2208M, exists M \u2032 \u2208M such that \u03c3max(M \u2212M \u2032) \u2264 ), and define\nF\u0304 := {x 7\u2192 h(Mx) : M \u2208M , h \u2208 HM}.\nNote that |F\u0304 | \u2264 |M ||HI | \u2264 N\u221e(\u03b3/2,H,m)(1 + 2D/ )D 2\n(c.f. Lemma 13). Observe that F\u0304 is a (\u03b3/2 + B\u03bb )-cover of F , since (i) for any f \u2208 F (formed by combining, say, M0 \u2208 M and h0 \u2208 H), exists f\u0304 \u2208 F\u0304 , namely the f\u0304 formed by M\u03040 such that \u03c3max(M0 \u2212 M\u03040) \u2264 , and (ii) h\u03040 \u2208 HM\u03040 such that |h0(M\u03040x)\u2212 h\u03040(M\u03040x)| \u2264 \u03b3/2 (for all x \u2208 X). So, (for any x \u2208 X)\n|f(x)\u2212 f\u0304(x)| = |h0(M0x)\u2212 h\u03040(M\u03040x)| \u2264 |h0(M0x)\u2212 h0(M\u03040x)|\n+|h0(M\u03040x)\u2212 h\u03040(M\u03040x)| \u2264 \u03bb\u2016M0x\u2212 M\u03040x\u2016+ \u03b3/2 \u2264 \u03bb\u03c3max(M0 \u2212 M\u03040)\u2016x\u2016+ \u03b3/2 \u2264 \u03bb B + \u03b3/2.\nSo, if we pick = min{ 12\u03bbB , \u03b3 2 }, it follows that\nN\u221e(\u03b3,F ,m) \u2264 |F\u0304 | \u2264 N\u221e(\u03b3/2,H,m)(1 + 2D/ )D 2 .\nBy noting Lemmas 14 and 15, it follows that\nPSm\u223cD [ \u2203f \u2208 F : err(f) \u2265 err\u03b3(f, Sm) + \u03b1 ] \u2264 4 ( 1 + 2D )D2(128m \u03b32\n)Fat\u03b3/16(H) ln( 32emFat\u03b3/16(H)\u03b3 )e\u2212\u03b12m/8. The lemma follows by bounding this failure probability with at most \u03b4.\nLemma 13 [ -spectral coverings of D \u00d7D matrices] LetM := {M |M \u2208 RD\u00d7D, \u03c3max(M) = 1} be the set of matrices with unit spectral norm. DefineM as the -cover ofM, that is, for every M \u2208M, there exists M \u2032 \u2208M such that \u03c3max(M \u2212M \u2032) \u2264 . Then for all > 0, there existsM such that |M | \u2264 ( 1 + 2D )D2 .\nProof. Fix any > 0 and letN /D be a minimal size ( /D)-cover of Euclidean unit ball BD in RD. That is, for any v \u2208 BD, there exists v\u2032 \u2208 N /D such that \u2016v \u2212 v\u2032\u2016 \u2264 /D. Using standard volume arguments (see e.g. proof of Lemma 5.2 of Vershynin (2010)), we know that |N /D| \u2264 ( 1 + 2D )D . Define\nM := { M \u2032 \u2223\u2223M \u2032 = [v\u20321 \u00b7 \u00b7 \u00b7 v\u2032D] \u2208 RD\u00d7D, v\u2032i \u2208 N /D}. ThenM constitutes as an -cover ofM, since for any M = [v1 \u00b7 \u00b7 \u00b7 vD] \u2208 M there exists M \u2032 = [v\u20321 \u00b7 \u00b7 \u00b7 v\u2032D] \u2208M , in particular M \u2032 such that \u2016vi \u2212 v\u2032i\u2016 \u2264 /D (for all i). Then\n\u03c3max(M \u2212M \u2032) \u2264 \u2016M \u2212M \u2032\u2016F = \u2211 i \u2016vi \u2212 v\u2032i\u2016 \u2264 .\nWithout loss of generality we can assume that each M \u2032 \u2208 M , \u03c3max(M \u2032) = 1. Moreover, by construction, |M | \u2264 ( 1 + 2D )D2 .\nLemma 14 [extension of Theorem 12.8 of Anthony & Bartlett (1999)] Let H be a set of real functions from a domain X to the interval [0, 1]. Let \u03b3 > 0. Then for all m \u2265 1,\nN\u221e(\u03b3,H,m) < c0(4m/\u03b32) Fat\u03b3/4(H) ln 4emFat\u03b3/4(H)\u03b3 .\nfor some universal constant c0.\nProof. Theorem 12.8 of Anthony & Bartlett (1999) asserts this for m \u2265 Fat\u03b3/4(H) \u2265 1 with c0 = 2. Now, if 1 \u2264 m < Fat\u03b3/4(H), for some universal constant c\u2032, we have N\u221e(\u03b3,H,m) \u2264 (c\u2032/\u03b3)m \u2264 (c\u2032/\u03b3)Fat\u03b3/4(H).\nLemma 15 [Theorem 10.1 of Anthony & Bartlett (1999)] Suppose thatH is a set of real-valued functions defined on domainX . LetD be any probability distribution onZ = X\u00d7{0, 1}, 0 \u2264 \u2264 1, real \u03b3 > 0 and integer m \u2265 1. Then,\nPSm\u223cD [ \u2203h \u2208 H : err(h) \u2265 err\u03b3(h, Sm) + ] \u2264 2N\u221e (\u03b3 2 ,H, 2m ) e\u2212 2m/8,\nwhere Sm is an i.i.d. sample of size m from D."}, {"heading": "A.4 Proof of Lemma 4", "text": "For any fixed 0 < \u03b3 < 1/8 and the given bounded class of distributions with bound B \u2265 1, consider a (1/B)-bi-Lipschitz base hypothesis class H that maps hypothesis from the domain X to [1/2\u2212 4\u03b3, 1/2 + 4\u03b3], and define\nF := {x 7\u2192 h(Mx) : M \u2208M, h \u2208 H}.\nNote that finding M that minimizes errhypoth is equivalent to finding f that minimizes error on F . Using Lemma 19, we have for any 0 < \u03b3 < 1/2, the sample complexity of F is (for all 0 < , \u03b4 < 1/64)\nm \u2265 Fat2\u03b3(\u03c04\u03b3(F)) 320 2 , (10)\nwhere \u03c04\u03b3(F) is the (4\u03b3)-squashed function class of F (see Definition 2 below). We lower bound Fat2\u03b3(\u03c04\u03b3(F)) in terms of fat-shattering dimension ofH to yield the lemma.\nTo this end we shall first define the (\u03b3,m)-covering and packing number of a generic real-valued hypothesis class G. For any domain X , real-valued hypothesis class G \u2282 [0, 1]X , margin \u03b3 > 0, and a sample S \u2282 X , define\ncov\u03b3(G, S) := { C \u2282 G \u2223\u2223\u2223 \u2200g \u2208 G,\u2203g\u2032 \u2208 C, maxs\u2208S |g(s)\u2212 g\u2032(s)| \u2264 \u03b3 } ,\npak\u03b3(G, S) :=\n{ P \u2282 G \u2223\u2223\u2223 \u2200g 6= g\u2032 \u2208 P, maxs\u2208S |g(s)\u2212 g\u2032(s)| \u2265 \u03b3 }\nas the set of \u03b3-covers (resp. \u03b3-packings) of S by G. Let \u03b3-covering number (resp. \u03b3-packing number) of G for any integer m > 0 be defined as\nN\u221e(\u03b3,G,m) := max S\u2282X:|S|=m min C\u2208cov\u03b3(G,S) |C|,\nP\u221e(\u03b3,G,m) := max S\u2282X:|S|=m max P\u2208pak\u03b3(G,S) |P |\nwith the minimizing cover C (resp. maximizing packing P ) called as the minimizing (\u03b3,m)-cover (resp. maximizing (\u03b3,m)-packing) of G.\nWith these definitions, we have the following (for some universal constant c0).\nc0 ( m 16\u03b32 )Fat2\u03b3(\u03c04\u03b3(F)) ln(em/2\u03b3) \u2265 N\u221e(8\u03b3, \u03c04\u03b3(F),m) [Lemma 14]\n\u2265 P\u221e(16\u03b3, \u03c04\u03b3(F),m) [Lemma 17] \u2265 ( 1\n32\u03b3\n)D2 P\u221e(48\u03b3, \u03c04\u03b3(H),m) [see (*) below]\n= ( 1\n32\u03b3\n)D2 P\u221e(48\u03b3,H,m) [by the choice ofH]\n\u2265 ( 1\n32\u03b3\n)D2 N\u221e(48\u03b3,H,m) [Lemma 17]\n\u2265 ( 1\n32\u03b3\n)D2 eFat768\u03b3(H)/8. [Lemma 18] (11)\n(*) We show that P\u221e(16\u03b3, \u03c04\u03b3(F),m) \u2265 (1/32\u03b3)D 2P\u221e(48\u03b3, \u03c04\u03b3(H),m), by exhibiting a set"}, {"heading": "S \u2282 \u03c04\u03b3(F) of size (1/32\u03b3)D", "text": "2P\u221e(48\u03b3, \u03c04\u03b3(H),m) that is a (16\u03b3)-packing of \u03c04\u03b3(F).\nLet \u03c04\u03b3(H48\u03b3) \u2282 \u03c04\u03b3(H) be a maximal (32\u03b3)-packing of \u03c04\u03b3(H) (that is, a maximal set such that for all distinct (\u03c04\u03b3 \u25e6 h), (\u03c04\u03b3 \u25e6 h\u2032) \u2208 \u03c04\u03b3(H48\u03b3), exists x \u2208 X such that |\u03c04\u03b3(h(x)) \u2212 \u03c04\u03b3(h \u2032(x))| \u2265 48\u03b3). Fix (exact value determined later), and define\nS := { x 7\u2192 (\u03c04\u03b3 \u25e6 h)(Mx) \u2223\u2223\u2223 (\u03c04\u03b3 \u25e6 h) \u2208 \u03c04\u03b3(H48\u03b3), M \u2208M } ,\nwhereM is a -spectral net ofM, that is, for all M \u2208 M, exists M \u2032 \u2208 M such that \u03c3max(M \u2212 M \u2032) \u2264 , and for all distinct M \u2032,M \u2032\u2032 \u2208M , \u03c3max(M \u2032 \u2212M \u2032\u2032) \u2265 /2.\nThen for any two distinct f, f \u2032 \u2208 S , such that f(x) = (\u03c04\u03b3 \u25e6 h)(Mx) and f \u2032(x) = (\u03c04\u03b3 \u25e6 h\u2032)(M \u2032x), we have\n\u2022 (case 1) h and h\u2032 are distinct. In this case, there exists x \u2208 X , s.t.\n|f(x)\u2212 f \u2032(x)| =|\u03c04\u03b3(h(Mx))\u2212 \u03c04\u03b3(h\u2032(M \u2032x))| \u2265 |\u03c04\u03b3(h(Mx))\u2212 \u03c04\u03b3(h\u2032(Mx))| \u2212 |\u03c04\u03b3(h\u2032(Mx))\u2212 \u03c04\u03b3(h\u2032(M \u2032x))| \u2265 48\u03b3 \u2212 (1/B)\u03c3max(M \u2212M \u2032)\u2016x\u2016 \u2265 48\u03b3 \u2212 (1/B) B = 48\u03b3 \u2212 .\n\u2022 (case 2) h, h\u2032 same but M and M \u2032 distinct. In this case, there exists x (with \u2016x\u2016 = 1) s.t.\n|f(x)\u2212 f \u2032(x)| = |\u03c04\u03b3(h(Mx))\u2212 \u03c04\u03b3(h(M \u2032x))| = |h(Mx)\u2212 h(M \u2032x)| \u2265 B\u2016(M \u2212M \u2032)x\u2016 \u2265 B \u00b7 min\nM 6=M \u2032\u2208M \u03c3max(M \u2212M \u2032)\n\u2265 B( /2).\nThus, by setting = 32\u03b3, distinct classifiers f, f \u2032 \u2208 S32\u03b3 are at least 16\u03b3 apart (since B \u2265 1). Hence S32\u03b3 forms a (16\u03b3)-packing of \u03c04\u03b3(F). Therefore, the packing number\nP\u221e(16\u03b3, \u03c04\u03b3(F),m) \u2265 |S32\u03b3 | = |M32\u03b3 ||H48\u03b3 | \u2265 (1/32\u03b3)D 2 P\u221e(48\u03b3, \u03c04\u03b3(H),m).\nThus, from Eq. (11), it follows that\nFat2\u03b3(\u03c04\u03b3(F)) \u2265 \u2126 (D2 ln(1/\u03b3) + Fat768\u03b3(H)\nln(m/\u03b32) ln(m/\u03b3)\n) .\nCombining this with Eq. (10), the lemma follows.\nLemma 16 [ -spectral packings of D \u00d7D matrices] LetM := {M | M \u2208 RD\u00d7D, \u03c3max(M) = 1} be the set of matrices with unit spectral norm. DefineM \u2282 M as the -packing ofM, that is, for every distinct M,M \u2032 \u2208M , \u03c3max(M \u2212M \u2032) \u2265 . Then for all > 0, there existsM such that |M | \u2265 ( 1 2 )D2 .\nProof. Fix any > 0 and let P be a maximal size -packing of Euclidean unit ball BD in RD. That is, for all distinct v, v\u2032 \u2208 BD, \u2016v \u2212 v\u2032\u2016 \u2265 . Using standard volume arguments (see e.g. proof of Lemma 5.2 of Vershynin (2010)), we know that |P | \u2265 ( 1 2 )D . Define\nM := { M \u2032 \u2223\u2223M \u2032 = [v\u20321 \u00b7 \u00b7 \u00b7 v\u2032D] \u2208 RD\u00d7D, v\u2032i \u2208 P }. Then M constitutes as an -packing of M, since for any distinct M,M \u2032 \u2208 M such that M = [v1 \u00b7 \u00b7 \u00b7 vD] and M \u2032 = [v\u20321 \u00b7 \u00b7 \u00b7 v\u2032D], we have\n\u03c3max(M \u2212M \u2032) \u2265 max i \u2016vi \u2212 v\u2032i\u2016 \u2265 .\nWithout loss of generality we can assume that each M \u2208 M , \u03c3max(M) = 1. Moreover, by construction, |M | \u2265 ( 1 2 )D2 .\nLemma 17 [follows from Theorem 12.1 of Anthony & Bartlett (1999)] For any real valued hypothesis classH into [0, 1], all m \u2265 1, and 0 < \u03b3 < 1/2,\nP\u221e(2\u03b3,H,m) \u2264 N\u221e(\u03b3,H,m) \u2264 P\u221e(\u03b3,H,m).\nLemma 18 [Theorem 12.10 of Anthony & Bartlett (1999)] Let H be a set of real functions from a domain X to the interval [0, 1]. Let \u03b3 > 0. Then for m \u2265 Fat16\u03b3(H),\nN\u221e(\u03b3,H,m) \u2265 eFat16\u03b3(H)/8.\nLemma 19 [Theorem 13.5 of Anthony & Bartlett (1999)] Suppose that H is a set of real-valued functions mapping into the interval [0, 1] that is closed under addition of constants, that is,\nh \u2208 H =\u21d2 h\u2032 \u2208 H, where h\u2032 : x\u2192 h(x) + c for all c.\nPick any 0 < \u03b3 < 1/2. Then for any metric learning algorithm A for all 0 < , \u03b4 < 1/64, there exists a distribution D such that if m \u2264 d320 2 , then\nPSm\u223cD[err(h \u2217,D) > err\u03b3(A(Sm),D) + ] > \u03b4\nwhere d := Fat2\u03b3(\u03c04\u03b3(H)) \u2265 1 is the fat-shattering dimension of \u03c04\u03b3(H)\u2014the (4\u03b3)-squashed function class ofH, see Definition 2 below\u2014at margin 2\u03b3.\nDefinition 2 [squashing function] For any 0 < \u03b3 < 1/2, define the squashing function \u03c0\u03b3 : R \u2192 [1/2\u2212 \u03b3, 1/2 + \u03b3] as\n\u03c0\u03b3(\u03b1) = { 1/2 + \u03b3 if \u03b1 \u2265 1/2 + \u03b3 1/2\u2212 \u03b3 if \u03b1 \u2264 1/2\u2212 \u03b3 \u03b1 otherwise .\nMoreover, for a collection F of functions into R, define \u03c0\u03b3(F ) := {\u03c0\u03b3 \u25e6 f | f \u2208 F}."}, {"heading": "A.5 Proof of Lemma 5", "text": "Let P be the probability measure induced by the random variable (X, Y ), where X := (x, x\u2032), Y := 1[y = y\u2032], st. ((x, y), (x\u2032, y\u2032)) \u223c (D\u00d7D).\nDefine function class\nF := { fM : X 7\u2192 \u2016M(x\u2212 x\u2032)\u20162 \u2223\u2223\u2223\u2223\u2223 M \u2208MX = (x, x\u2032) \u2208 (X \u00d7X) } ,\nFollowing the steps of proof of Lemma 1, we can conclude that the Rademacher complexity of F is bounded. In particular,\nRm(F) \u2264 4B2 \u221a\nsupM\u2208M \u2016MTM\u20162F m .\nThe result follows by noting that \u03c6 is \u03bb-Lipschitz in the first argument and by applying Lemma 8."}, {"heading": "A.6 Proof of Lemma 6", "text": "Consider the function class\nF := { fv,M : x 7\u2192 v \u00b7Mx \u2223\u2223 \u2016v\u20161 \u2264 1,M \u2208M}, and define the composition class\nF\u03c3 := { x 7\u2192\nK\u2211 i=1 wi\u03c3 \u03b3(fi(x)) \u2223\u2223\u2223 \u2016wi\u20161 \u2264 1, f1, . . . , fK \u2208 F } .\nThen, first note that the Gaussian complexity ofF (with respect to the distributionD) is bounded, since (let g1, . . . , gm denote independent standard Gaussian random variables)\nGm(F ,D) := Exi\u223cD |X gi,i\u2208[m]\n[ sup\nfv,M\u2208F\n1\nm m\u2211 i=1 gifv,M (xi)\n]\n= 1\nm Exi\u223cD |X gi,i\u2208[m] [ sup M\u2208M \u2016v\u20161\u22641 v \u00b7 m\u2211 i=1 gi(Mxi) ]\n= 1\nm Exi\u223cD |X gi,i\u2208[m]\n[ max j sup M\u2208M m\u2211 i=1 gi(Mxi)j ]\n\u2264 1 m Exi\u223cD |X gi,i\u2208[m] max j\u2208[D] [ m\u2211 i=1 gi sup M\u2208M \u2223\u2223(Mxi)j\u2223\u2223]\n\u2264 c ln 1 2 (D)\nm Exi\u223cDX max j,j\u2032\u2208[D]\n( Egi [ m\u2211 i=1 gi ( sup M\u2208M \u2223\u2223(Mxi)j\u2223\u2223\u2212 sup M \u2032\u2208M \u2223\u2223(M \u2032xi)j\u2032 \u2223\u2223)]2) 1 2\n= c ln\n1 2 (D) m Exi\u223cDX max j,j\u2032\u2208[D] ( m\u2211 i=1 [ sup M\u2208M \u2223\u2223(Mxi)j\u2223\u2223\u2212 sup M \u2032\u2208M \u2223\u2223(M \u2032xi)j\u2032\u2223\u2223]2) 1 2\n\u2264 c\u2032B \u221a d lnD\nm ,\nwhere (i) second to last inequality is by applying Lemma 20, (ii) c, c\u2032 are absolute constants, (iii) d := supM\u2208M \u2016MTM\u20162F . Note that bounding the Gaussian complexity also bounds the Rademacher complexity by Lemma 21.\nFinally by noting thatF\u03c3 is a \u03b3-Lipschitz composition class ofF and \u03c6\u03bb is a classification based loss function that is \u03bb-Lipschitz in the first argument, we can apply Lemma 8 yielding the desired result.\nLemma 20 [Lemma 20 of Bartlett & Mendelson (2002)] Let Z1, . . . , ZD be random variables such that each Zj = \u2211m i=1 aijgi, where each gi is independent N(0, 1) random variables. Then there is an absolute constant c such that\nEgi max j Zj \u2264 c ln\n1 2 (D) max\nj,j\u2032\n\u221a Egi(Zj \u2212 Zj\u2032)2.\nLemma 21 [Lemma 4 of Bartlett & Mendelson (2002)] There are absolute constants c and C such that for every class F and every integer m\ncRm(F ,D) \u2264 Gm(F ,D) \u2264 C ln(m)Rm(F ,D),\nwhereR and G are Rademacher and Gaussian complexities of a function class F with respect to the distribution D respectively."}, {"heading": "A.7 Proof of Corollary 7", "text": "The conclusion of Eq. (3) is immediate by dividing the given failure probability \u03b4 across the sequence M1,M2, \u00b7 \u00b7 \u00b7 such that \u03b4\u00b5d failure probability is associated with class Md, then apply Lemma 5 (for distance based metric learning) or Lemma 6 (for classifier based metric learning) to each class Md individually, and finally combining the individual deviations together with a union bound.\nFor the second part, for any M \u2208M define dM and \u039bM as per the lemma statement. Then with probability at least 1\u2212 \u03b4\nerr\u03bb(M regm ,D)\u2212 err\u03bb(M\u2217,D) \u2264 err\u03bb(M regm , Sm) + dM regm \u039bM regm \u2212 err \u03bb(M\u2217,D)\n\u2264 err\u03bb(M\u2217, Sm) + dM\u2217\u039bM\u2217 \u2212 err \u03bb(M\u2217,D) \u2264 O(d M\u2217\u039bM\u2217 ) = O( ),\nwhere (i) the first inequality is by applying Eq. (3) on weighting metric M regm (with failure probability set to \u03b4/2), (ii) the second inequality is by noting that M regm is the (regularized) sample error minimizer as per the lemma statement, (iii) the third inequality is by applying Eq. (3) on weighting metric M\u2217 (with failure probability set to \u03b4/2), and (iv) the last equality by noting the definitions of \u039bM\u2217 and our choice of m."}], "references": [{"title": "Neural network learning: Theoretical foundations", "author": ["M. Anthony", "P. Bartlett"], "venue": null, "citeRegEx": "Anthony and Bartlett,? \\Q1999\\E", "shortCiteRegEx": "Anthony and Bartlett", "year": 1999}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Bartlett and Mendelson,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson", "year": 2002}, {"title": "Robustness and generalization for metric learning", "author": ["A. Bellet", "A. Habrard"], "venue": "CoRR, abs/1209.1086,", "citeRegEx": "Bellet and Habrard,? \\Q2012\\E", "shortCiteRegEx": "Bellet and Habrard", "year": 2012}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "CoRR, abs/1306.6709,", "citeRegEx": "Bellet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bellet et al\\.", "year": 2014}, {"title": "Learning a distance metric by empirical loss minimization", "author": ["W. Bian", "D. Tao"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Bian and Tao,? \\Q2011\\E", "shortCiteRegEx": "Bian and Tao", "year": 2011}, {"title": "Generalization bounds for metric and similarity learning", "author": ["Q. Cao", "Z. Guo", "Y. Ying"], "venue": "CoRR, abs/1207.5437,", "citeRegEx": "Cao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "New generalization bounds for learning kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Generalization classification via regularized similarity learning", "author": ["Z. Guo", "Y. Ying"], "venue": "Neural Computation,", "citeRegEx": "Guo and Ying,? \\Q2014\\E", "shortCiteRegEx": "Guo and Ying", "year": 2014}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural Networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Regularized distance metric learning: Theory and algorithm", "author": ["R. Jin", "S. Wang", "Y. Zhou"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Jin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2009}, {"title": "Fantope regularization in metric learning", "author": ["M.T. Law", "N. Thome", "M. Cord"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Law et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Law et al\\.", "year": 2014}, {"title": "Robust structural metric learning", "author": ["D.K.H. Lim", "B. McFee", "G.R.G. Lanckriet"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Lim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2013}, {"title": "Metric learning to rank", "author": ["B. McFee", "G.R.G. Lanckriet"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "McFee and Lanckriet,? \\Q2010\\E", "shortCiteRegEx": "McFee and Lanckriet", "year": 2010}, {"title": "Learning a distance metric from a network", "author": ["B. Shaw", "B. Huang", "T. Jebara"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Shaw et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shaw et al\\.", "year": 2011}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "In Compressed Sensing, Theory and Applications", "citeRegEx": "Vershynin,? \\Q2010\\E", "shortCiteRegEx": "Vershynin", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Generalization bounds for learning the kernel", "author": ["Y. Ying", "C. Campbell"], "venue": "Conference on Computational Learning Theory (COLT),", "citeRegEx": "Ying and Campbell,? \\Q2009\\E", "shortCiteRegEx": "Ying and Campbell", "year": 2009}], "referenceMentions": [{"referenceID": 7, "context": "A few notable examples include exploiting class labels to find a Mahalanobis distance metric that maximizes the distance between dissimilar observations while minimizing distances between similar ones to improve classification quality (Weinberger & Saul, 2009; Davis et al., 2007), \u2217email: verman@janelia.", "startOffset": 235, "endOffset": 280}, {"referenceID": 15, "context": "Some popular algorithms that optimize for such distance-based objectives include Mahalanobis Metric for Clustering (MMC) by Xing et al. (2002) and Information Theoretic Metric Learning (ITML) by Davis et al.", "startOffset": 124, "endOffset": 143}, {"referenceID": 7, "context": "(2002) and Information Theoretic Metric Learning (ITML) by Davis et al. (2007). Instead of using distance comparisons as a proxy, however, one can also optimize for a specific prediction task directly.", "startOffset": 59, "endOffset": 79}, {"referenceID": 7, "context": "(2002) and Information Theoretic Metric Learning (ITML) by Davis et al. (2007). Instead of using distance comparisons as a proxy, however, one can also optimize for a specific prediction task directly. The second generic framework, the classifier-based metric learning framework, explicitly incorporates the hypothesis associated with the prediction task of interest to learn effective distance metrics. A few interesting examples in this regime include the work by McFee & Lanckriet (2010) that finds metrics that improve ranking quality in information retrieval tasks, and the work by Shaw et al.", "startOffset": 59, "endOffset": 491}, {"referenceID": 7, "context": "(2002) and Information Theoretic Metric Learning (ITML) by Davis et al. (2007). Instead of using distance comparisons as a proxy, however, one can also optimize for a specific prediction task directly. The second generic framework, the classifier-based metric learning framework, explicitly incorporates the hypothesis associated with the prediction task of interest to learn effective distance metrics. A few interesting examples in this regime include the work by McFee & Lanckriet (2010) that finds metrics that improve ranking quality in information retrieval tasks, and the work by Shaw et al. (2011) that learns metrics that help predict connectivity structure in networked data.", "startOffset": 59, "endOffset": 606}, {"referenceID": 16, "context": "Xing et al. (2002) optimize an efficiently computable variant of this criterion, in which they look for a metric that keeps the total pairwise distance amongst the observations from the same class less than a constant while maximizing the total pairwise distance amongst the observations from opposite classes.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "The variant proposed by Davis et al. (2007) explicitly includes the upper and lower limits with an added regularization on the learned M to be close to a pre-specified metric of interest M0.", "startOffset": 24, "endOffset": 44}, {"referenceID": 14, "context": "Shaw et al. (2011) also follow this principle and explicitly include network topology constraints to learn a weighting metric that can better predict the connectivity structure in social and web networks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "Recall that for any smooth target function f\u2217, a two layer feed-forward neural network (with appropriate number of hidden units and connection weights) can approximate f\u2217 arbitrarily well (Hornik et al., 1989), so this class is flexible enough to incorporate most reasonable target hypotheses.", "startOffset": 188, "endOffset": 209}, {"referenceID": 12, "context": "This partly explains the observed empirical success of various types of norm-regularized optimization criteria for finding the optimal weighting metric (Lim et al., 2013; Law et al., 2014).", "startOffset": 152, "endOffset": 188}, {"referenceID": 11, "context": "This partly explains the observed empirical success of various types of norm-regularized optimization criteria for finding the optimal weighting metric (Lim et al., 2013; Law et al., 2014).", "startOffset": 152, "endOffset": 188}, {"referenceID": 7, "context": "We select two popular metric learning algorithms, LMNN by Weinberger & Saul (2009) and ITML by Davis et al. (2007), that are designed to find metrics that improve nearest-neighbor classification quality.", "startOffset": 95, "endOffset": 115}, {"referenceID": 9, "context": "Jin et al. (2009), for instance, analyzed the generalization ability of regularized, convex-loss optimization criteria for pairwise distances via an algorithmic stability analysis.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Jin et al. (2009), for instance, analyzed the generalization ability of regularized, convex-loss optimization criteria for pairwise distances via an algorithmic stability analysis. They derive an interesting sample complexity result that is sublinear in \u221a D for datasets of representation dimension D. They discuss that the sample complexity can potentially be independent of D, but do not characterize specific instances or classes of problems where this may be possible. Likewise, recent work by Bellet & Habrard (2012) uses algorithmic robustness to analyze the generalization ability for pairwise- and triplet-based distance metric learning.", "startOffset": 0, "endOffset": 522}, {"referenceID": 9, "context": "Jin et al. (2009), for instance, analyzed the generalization ability of regularized, convex-loss optimization criteria for pairwise distances via an algorithmic stability analysis. They derive an interesting sample complexity result that is sublinear in \u221a D for datasets of representation dimension D. They discuss that the sample complexity can potentially be independent of D, but do not characterize specific instances or classes of problems where this may be possible. Likewise, recent work by Bellet & Habrard (2012) uses algorithmic robustness to analyze the generalization ability for pairwise- and triplet-based distance metric learning. Their analysis relies on the existence of a partition of the input space, such that in each cell of the partition, the training loss and test loss does not deviate much (robustness criteria). Note that their sample complexity bound scales with the partition size, which in general can be exponential in the representation dimension. Perhaps the works most similar to our approach are the sample complexity analyses by Bian & Tao (2011) and Cao et al.", "startOffset": 0, "endOffset": 1082}, {"referenceID": 5, "context": "Perhaps the works most similar to our approach are the sample complexity analyses by Bian & Tao (2011) and Cao et al. (2013). Bian & Tao (2011) analyze the consistency of the ERM criterion for metric learning.", "startOffset": 107, "endOffset": 125}, {"referenceID": 5, "context": "Perhaps the works most similar to our approach are the sample complexity analyses by Bian & Tao (2011) and Cao et al. (2013). Bian & Tao (2011) analyze the consistency of the ERM criterion for metric learning.", "startOffset": 107, "endOffset": 144}, {"referenceID": 5, "context": "Perhaps the works most similar to our approach are the sample complexity analyses by Bian & Tao (2011) and Cao et al. (2013). Bian & Tao (2011) analyze the consistency of the ERM criterion for metric learning. They show a O(m\u22121/2) rate of convergence for the ERM with m samples to the expected risk for thresholds on bounded convex losses for distance-based metric learning. Our upper-bound in Lemma 1 generalizes this result by considering arbitrary (possibly non-convex) distance-based Lipschitz losses and explicitly shows the dependence on the representation dimensionD. Cao et al. (2013) provide an alternate analysis based on norm regularization of the weighting metric for distance-based metric learning.", "startOffset": 107, "endOffset": 593}, {"referenceID": 6, "context": "The typical focus in kernel learning is to analyze the generalization ability of the hypothesis class of linear separators in general Hilbert spaces (Ying & Campbell, 2009; Cortes et al., 2010).", "startOffset": 149, "endOffset": 193}, {"referenceID": 12, "context": "Our empirical results show that such regularization not only helps in designing new metric learning algorithms (Lim et al., 2013; Law et al., 2014), but can even benefit existing metric learning algorithms in high-noise regimes.", "startOffset": 111, "endOffset": 147}, {"referenceID": 11, "context": "Our empirical results show that such regularization not only helps in designing new metric learning algorithms (Lim et al., 2013; Law et al., 2014), but can even benefit existing metric learning algorithms in high-noise regimes.", "startOffset": 111, "endOffset": 147}, {"referenceID": 15, "context": "2 of Vershynin (2010)), we know that |N /D| \u2264 ( 1 + 2D )D .", "startOffset": 5, "endOffset": 22}, {"referenceID": 15, "context": "2 of Vershynin (2010)), we know that |P | \u2265 ( 1 2 )D .", "startOffset": 5, "endOffset": 22}], "year": 2015, "abstractText": "Metric learning seeks a transformation of the feature space that enhances prediction quality for the given task at hand. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lowerand upper-bounds showing that the sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. However, by leveraging the structure of the data distribution, we show that one can achieve rates that are fine-tuned to a specific notion of intrinsic complexity for a given dataset. Our analysis reveals that augmenting the metric learning optimization criterion with a simple norm-based regularization can help adapt to a dataset\u2019s intrinsic complexity, yielding better generalization. Experiments on benchmark datasets validate our analysis and show that regularizing the metric can help discern the signal even when the data contains high amounts of noise.", "creator": "LaTeX with hyperref package"}}}