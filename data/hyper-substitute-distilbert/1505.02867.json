{"id": "1505.02867", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2015", "title": "The Boundary Forest Algorithm for Online Supervised and Unsupervised Learning", "abstract": "first studied further corresponding instance - like learning algorithm calling persistent zombie forest ( bf ) algorithm, that can look effective for adaptive and unsupervised learning. thus algorithm monitors buffer forest carrying keys whose nodes store previously seen examples. messages can be shown data updates one at a resolution and updates itself online, hence connectivity maintains naturally online. few instance - based protocols have inherent property towards being simultaneously fast, depending the solution does. this is clear hence programs / neurons needs to respond to input data in waiting time. small number of children attending each node requires not controlled strictly versus moves underneath the executing processes, or maintains the strategy very flexible with regards to if data manifolds us can learn. our test its generalization performance and speed between a range of benchmark standards and thanks to real program analysis engages the state of the art. empirically we find that training time specified as o ( dnlog ( n ) ) bounded thus as o ( dlog ( n ) ), where d indicate local dimensionality and n the amount processing data,", "histories": [["v1", "Tue, 12 May 2015 03:45:11 GMT  (6519kb,D)", "http://arxiv.org/abs/1505.02867v1", "7 pages, 4 figs, 1 page supp. info"]], "COMMENTS": "7 pages, 4 figs, 1 page supp. info", "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.IR stat.ML", "authors": ["charles mathy", "nate derbinsky", "jos\u00e9 bento", "jonathan rosenthal", "jonathan s yedidia"], "accepted": true, "id": "1505.02867"}, "pdf": {"name": "1505.02867.pdf", "metadata": {"source": "META", "title": "The Boundary Forest Algorithm for Online Supervised and Unsupervised Learning", "authors": ["Charles Mathy", "Nate Derbinsky", "Jos\u00e9 Bento", "Jonathan Rosenthal", "Jonathan Yedidia"], "emails": ["@disneyresearch.com", "@wit.edu", "@bc.edu", "@disneyresearch.com", "@disneyresearch.com"], "sections": [{"heading": "Introduction", "text": "The ability to learn from large numbers of examples, where the examples themselves are often high-dimensional, is vital in many areas of machine learning. Clearly, the ability to generalize from training examples to test queries is a key feature that any learning algorithm must have, but there are several other features that are also crucial in many practical situations. In particular, we seek a learning algorithm that is: (i) fast to train, (ii) fast to query, (iii) able to deal with arbitrary data distributions, and (iv) able to learn incrementally in an online setting. Algorithms that satisfy all these properties, particularly (iv), are hard to come by, however they are of immediate importance in problems such as real time computer vision, robotic control, and more generally, problems which involve learning from and responding quickly to streaming data.\nWe present here the Boundary Forest (BF) algorithm that satisfies all these properties, and as a bonus, is transparent and easy to implement. The data structure underlying the BF algorithm is a collection of boundary trees (BTs). The nodes in a BT each store a training example. The BT structure can\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nbe efficiently queried at query time and quickly modified to incorporate new data points during training. The word \u201cboundary\u201d in the name relates to its use in classification, where most of the nodes in a BT will be near the boundary between different classes. The method is nonparametric and can learn arbitrarily shaped boundaries, as the tree structure is determined from the data and not fixed a priori. The BF algorithm is very flexible; in essentially the same form, it can be used for classification, regression and nearest neighbor retrieval problems."}, {"heading": "Related work", "text": "There are several existing methods, including KD-trees (Friedman, Bentley, and Finkel 1977), Geometric Nearneighbor Access Trees (Brin 1995), and Nearest Vector trees (Lejsek, Jo\u0301nsson, and Amsaleg 2011) that build tree search structures on large datasets (see (Samet 2006) for an extensive bibliography). These algorithms typically need batch access to the entire dataset before constructing their trees, in which case they may outperform the BF, however we are interested in an online setting. Two well known treebased algorithms that allow online insertion are cover trees (Beygelzimer, Kakade, and Langford 2006) and ball trees. The ball tree online insertion algorithm (Omohundro 1989) is rather costly, requiring a volume minimizing step at each addition. The cover tree, on the other, has a cheap online insertion algorithm, and it comes with guarantees of query time scaling as O(c6logN) where N is the amount of data and c the so-called expansion constant, which is related to the intrinsic dimensionality of the data. We will compare to cover trees below. Note that c in fact depends on N as it is defined as a worse case computation over the data set. It can also diverge from adding a single point.\nTree-based methods can be divided into those that rely on calculating metric distances between points to move down the tree, and those that perform cheaper computations. Examples of the former include cover trees , ball trees and the BF algorithm we present here. Examples of the latter include random decision forests (RFs) and kd trees (Friedman, Bentley, and Finkel 1977) . In cases where it is hard to find a useful subset of informative features, metric-based methods may give better results, otherwise it is of course preferable to make decisions with fewer features as this makes traversing the trees cheaper. Like other metric-based methods, the BF\nar X\niv :1\n50 5.\n02 86\n7v 1\n[ cs\n.L G\n] 1\n2 M\nay 2\n01 5\ncan immediately be combined with random projections to obtain speedup, as it is known by the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss 1984) that the number of projections needed to maintain metric distances only grows as log(D) as the data dimensionality D grows. There has been work on creating online versions of RFs (Kalal, Matas, and Mikolajczyk 2009) and kd trees. In fact, kd trees typically scale no better than brute force in higher than 20 dimensions (Muja and Lowe 2009) , but multiple random kd trees have been shown to overcome this difficulty. We will compare to offline RFs and online random kd trees (the latter implemented in the highly optimized library FLANN (SilpaAnan and Hartley 2008)) below.\nThe naive nearest neighbor algorithm is online, and there is extensive work on trying to reduce the number of stored nearest neighbors to reduce space and time requirements (Aha, Kibler, and Albert 1991). As we will show later, we can use some of these methods in our approach. In particular, for classification the Condensed Nearest Neighbor algorithm (Wilson and Martinez 2000) only adds a point if the previously seen points misclassify it. This allows for a compression of the data and significantly accelerates learning, and we use the same idea in our method. Previous algorithms that generate a tree search structure would have a hard time doing this, as they need enough data from the outset to build the tree."}, {"heading": "The Boundary Forest algorithm", "text": "A boundary forest is a collection of nT rooted trees. Each tree consists of nodes representing training examples, with edges between nodes created during training as described below. The root node of each tree is the starting point for all queries using that tree. Each tree is shown a training example or queried at test time independently of the other trees; thus one can trivially parallelize training and querying.\nEach example has a D-dimensional real position x and a \u201clabel\u201d vector c(x) associated with it (for retrieval problems, one can think of c(x) as being equal to x, as we will explain below). For example, if one is dealing with a 10-class classification problem, we could associate a 10-dimensional indicator vector c(x) with each point x.\nOne must specify a metric associated with the positions x, which takes two data points x and y and outputs a real number d(x, y). Note that in fact this \u201cmetric\u201d can be any real function, as we do not use any metric properties, but for the purpose of this paper we always use a proper metric function. Another parameter that one needs to specify is an integer k which represents the maximum number of child nodes connected to any node in the tree.\nGiven a query point y, and a boundary tree T , the algorithm moves through the tree starting from the root node, and recursively compares the distance to the query point from the current node and from its children, moving to and recursing at the child node that is closest to the query, unless the current node is closest and has fewer children than k, in which case it returns the current node. This greedy procedure finds a \u201clocally closest\u201d example to the query, in the sense that none of the children of the locally closest node are closer. Note that the algorithm is not allowed to stop at\na point that already has k children, because it could potentially get a new child if the current training point is added. As we will show, having finite k can significantly improve speed at low or negligible cost in performance.\nAlgorithm 1 The Boundary Tree (BT) algorithm associated data rooted tree - the ith node has position xi and label vector c(xi) the real threshold for comparison of label vectors d the metric for comparing positions dc the metric for comparing label vectors k the maximum number of children per node (k > 1)\n1: procedure BTQUERY(y) 2: input y the query position 3: output node v in Boundary Tree. 4: start initialize v at the root node v0 5: do 6: defineAv to be the set of nodes consisting of the chil-\ndren of v in the Boundary Tree 7: if the number of children of v is smaller than k then\nadd v to Av end if 8: let vmin = argminw\u2208Avd(w, x), i.e. the node in Av\nthat is closest to y (choose randomly from any ties) 9: if vmin = v then break end if\n10: v \u2190 vmin 11: end do 12: return v 13: end procedure\n14: procedure BTTRAIN(y,c(y)) 15: input 16: position y 17: label vector c(y) 18: start vmin = BOUNDARYTREEQUERY(y) 19: if dc(c(y), c(vmin)) > then 20: create node vnew in Boundary Tree with position y\nand label c(y) 21: add edge from vmin to vnew 22: end if 23: end procedure\nOnce each tree has processed a query, one is left with a set of nT locally closest nodes. What happens next depends on the task at hand: if we are interested in retrieval, then we take the closest of those nT locally closest nodes to y as the approximate nearest neighbor. For regression, given the positions xi of the locally closest nodes and their associated vector c(xi) one must combine them to form an estimate for c(y). Many options exist, but in this paper we use a Shepard weighted average (Shepard 1968), where the weights are the inverse distances, so that the estimate is\nc(y) = \u2211 i c(xi)/d(xi, y)\u2211 i 1/d(xi, y) . (1)\nFor classification, as described above we use an indicator function c(x) for the training points. Our answer is the class\nAlgorithm 2 The Boundary Forest (BF) algorithm (see algorithm 1 for definition of subroutines called here) associated data Forest of nT BTs: BF = {BT1, . . . , BTnT } all BTs have same associated d, dc, , k E the estimator function that takes in a position x, a set of nT nodes vi consisting of positions and label vectors, and outputs a label vector E(x, v1, . . . , vnT ) initialization start with nT training points, at positions y1, . . . , ynT and with respective labels c(y1), . . . , c(ynT ). Call BFINITIALIZATION(y1, c(y1), . . . , ynT , cnT )\n1: procedure BFQUERY(y) 2: input position y 3: start 4: for i from 1 to nT 5: vi = BTi.BTQUERY(y) 6: end for 7: return E(y, v1, . . . , vnT ) 8: end procedure\n9: procedure BFTRAIN(y,c(y)) 10: input 11: position y 12: label vector c(y) 13: start 14: for i from 1 to nT 15: call BTi.BTTRAIN(y, c(y)) 16: end for 17: end procedure\n18: procedure BFINITIALIZATION(y1,c(y1),. . .,ynT ,cnT ) 19: input 20: positions y1, . . . , ynT 21: label vectors c(y1), . . . , c(ynT ) 22: start 23: for i from 1 to nT 24: set position of root node of BTi to be yi, and its label\nvector c(yi) 25: for j from 1 to nT 26: if i 6= j then call BTi.BTTRAIN(yj , c(yj)) 27: end for 28: end for 29: end procedure\ncorresponding to the coordinate with the largest value from c(y); we again use Shepard\u2019s method to determine c(y). Thus, for a three-class problem where the computed c(y) was [0.5, 0.3, 0.2], we would return the first class as the estimate. For regression, the output of the BF is simply the the Shepard weighted average of the locally closest nodes xi output by each tree. Finally, for retrieval we take the node x\u2217 of the locally closest nodes xi that is closest to the query y.\nGiven a training example with position z and \u201clabel\u201d vector c(z), we first query each tree in the forest using z as we just described. Each tree independently outputs a locally closest node xi and decides whether a new node should be created with associated position z and label c(z) and connected by an edge to xi. The decision depends once again on the task: for classification, the node is created if c(xi) 6= c(z). For regression, one has to define a threshold and create the node if |(c(xi) \u2212 c(z)| > . Intuitively, the example is added to a tree if and only if the current tree\u2019s prediction of the label was wrong and needs to be corrected. For retrieval, we add all examples to the trees.\nIf all BTs in a BF were to follow the exact same procedure, they would all be the same. To decorrelate the trees, the simplest procedure is to give them each a different root. ConsiderBF = {BT1, . . . , BTnT }, a BF of nT trees. What we do in practice is take the first nT points, make point i the root node of BTi, and use as the other nT \u2212 1 initial training points for each BTi a random shuffling of the remaining nT \u2212 1 nodes. We emphasize that after the first nT training nodes (which are a very small fraction of the examples) the algorithm is strictly online.\nFor the algorithm just described, we find empirically that query time scales as a power law in the amount of data N with a power \u03b1 smaller than 2, which implies that training time scales as N1+\u03b1 (since training time is the integral of query time over N ). We can get much better scaling by adding a simple change: we set a maximum k of the number of children a node in the BF can have. The algorithm cannot stop at a node with k children. With this change, query time scales as log(N) and training time as Nlog(N), and if k is large enough performance is not negatively impacted. The memory scales linearly with the number of nodes added, which is linear in the amount of dataN for retrieval, and typically sublinear for classification or regression as points are only added if misclassified. We only store pointers to data in each tree, thus the main space requirement comes from a single copy of each stored data point and does not grow with nT .\nThe BF algorithm has a very appealing property which we call immediate one-shot learning: if it is shown a training example and immediately afterwards queried at that point, it will get the answer right. In practice, we find that the algorithm gets zero or a very small error on the training set after one pass through it (less than 1% for all data sets below).\nA pseudo-code summary of the algorithm for building Boundary Trees and Boundary Forests is given in Algorithms 1 and 2 respectively."}, {"heading": "Scaling properties", "text": "To study the scaling properties of the BF algorithm, we now focus on its use for retrieval. Consider examples drawn uniformly from within a hypercube in D dimensions. The qualitative results we will discuss are general: we tested a mixture of Gaussians of arbitrary size and orientation, and real datasets such as MNIST treated as a retrieval problem, and obtained the same picture. We will show results for a uniformly sampled hypercube and unlabeled MNIST. Note that we interpret raw pixel intensity values as vectors for MNIST without any preprocessing, and throughout this paper the Euclidean metric is used for all data sets. We will be presenting scaling fits to the different lines, ruling out one scaling law over another. In those cases, our procedure was to take the first half of the data points, fit them separately to one scaling law and to the one we are trying to rule out, and look at the rms error over the whole line for both fits. The fits we present have rms error at least 5 times smaller than the ruled out fits.\nDenote by N the number of training examples shown to a BF algorithm using nT trees and a maximum number of children per node k. Recall that for retrieval on a query point y, the BF algorithm returns a training example x\u2217 which is the closest of the locally closest nodes from each tree to the query. To assess the performance, we take all training examples, order them according to their distance from y, and ask where x\u2217 falls in this ordering. We say x\u2217 is in the f best fraction if it is among the fN closest points to y. In Fig. 1 we show the fraction f obtained if we require 99% probability that x\u2217 is within the fN closest examples to y. We see that the fraction f approaches zero as a power law as the number of training examples N increases.\nNext we consider the query time of the BF as a function of the number of examples N it has seen so far. Note that training and query time are not independent: since training involves a query of each BT in the BF, followed by adding the node to the BT which takes negligible time, training time is the integral of query time over N . In Fig. 2 we\nplot the query time (measured in numbers of metric comparisons per tree, as this is the computational bottleneck for the BF algorithm) as a function of N , for examples drawn randomly from the 100-dimensional hypercube and for unlabeled MNIST. What we observe is that if k = \u221e, query time scales sublinearly in N , with a power that depends on the dataset, but smaller than 0.5. However, for finite k, the scaling is initially sublinear but then it switches to logarithmic. This switch happens around the time when nodes in the BT start appearing with the number of children equal to k.\nTo understand what is going on, we consider an artificial situation where all points in the space are equidistant, which removes any from the problem. In this case, we once again have a root node where we start, and we will go from a node to one of its children recursively until we stop, at which point we connect a new node to the node we stopped at. The rule for traversal is as follows: if a node has q children, then with probability 1/(q+1) we stop at this node and connect a new node to it, while with probability q/(q+1) we go down one of its children, all with equal probability 1/(q + 1).\nThe query time for this artificial tree is (2N)0.5 for large N (plus subleading corrections), as shown in Fig. 3 (a). To understand why, consider the root node. If it has q \u2212 1 children, the expected time to add a new child is q. Therefore the expected number of steps for the root node to have q children scales as q2/2. Thus the number of children of the root node, and the number of metric comparisons made at the root grows as \u221a 2N (set q = \u221a 2N ). We find that numerically the number of metric comparisons scales around 1.02 \u221a 2N , which indicates that the metric comparisons to the root\u2019s children is the main computational cost. The reason is that the root\u2019s children have \u223c N1/4 children, as can be seen by repeating the previous scaling argument. If, on the other hand, we set k to be finite, initially the tree will\nbehave as though k was infinite, until the root node has k children, at which point it builds a tree where the query time grows logarithmically, as one would expect of an approximately balanced tree with k children or less per node.\nIn data sets with a metric, we find the power law when k = \u221e to be smaller than 0.5. Intuitively, this occurs because new children of the root must be closer to the root than any of its other children, therefore they reduce the probability that further query points are closer to the root than its children. As the dimensionality D increases, this effect diminishes, as new points have increasingly small inner products with each other, and if all points were orthogonal you do not see this bias. In Fig. 3(b) we plot the power \u03b1 in the scaling O(N\u03b1) of the query time of a BT trained on data drawn uniformly from the D-dimensional hypercube, and find that as D increases, \u03b1 approaches 0.5 from below, which is consistent with the phenomenon just described.\nWe now compare to the cover tree (CT) algorithm 1. For fair comparison, we train the CT adding the points using online insertion, and when querying we use the CT as an approximate nearest neighbor (ANN) algorithm (to our knowledge, this version of the CT which is defined in (Beygelzimer, Kakade, and Langford 2006) has not been studied previously). In the ANN incarnation of CT, one has to define a parameter , such that when queried with a point p the CT outputs a point q it was trained on such that d(p, q) \u2264 (1 + )dmin(p) where dmin(p) is the distance to the closest point to p that the CT was trained on. We set\n1We adapted the implementation of (Crane, D.N. 2011) - which was the only implementation we could find of the online version of CT - to handle approximate nearest neighbor search.\n= 10 ( has little effect on performance or speed: see Appendix for results with = 0.1).\nAnother important parameter is the base parameter b. It is set to 2 in the original proofs, however the original creators suggest that a value smaller than 2 empirically leads to better results, and their publicly available code has as a default the value 1.3, which is the value we use. Changing this value can decrease training time at the cost of increasing testing time, however the scaling with amount of data remains qualitatively the same (see Appendix for results with b = 1.1). Note that the cover tree algorithm is not scale invariant, while the BF is: if all the features in the data set are rescaled by the same parameter, the BF will do the exact same thing, which is not true of the CT. Also, for the CT the metric must satisfy the triangle inequality, and it is not obvious if it is parallelizable.\nIn Fig. 4 we train a BF with nT = 50 and k = 50, and a CT with = 10 and b = 1.3 on uniform random data drawn from the 100-dimensional hypercube. We find for this example that training scales quadratically, and querying linearly with the number of points N for the CT , while they scale as Nlog(N) and log(N) for the BF as seen before. While for the BF training time is the integral over query time, for CT insertion and querying are different. We find that the CT scaling is worse than the BF scaling."}, {"heading": "Numerical results", "text": "The main claim we substantiate in this section is that the BF as a classification or regression algorithm has accuracy comparable to the K-nearest neighbors (K-NN) algorithm on real datasets, with a fraction of the computational time, while maintaining the desirable property of learning incrementally. Since the traversal of the BF is dictated by the metric, the algorithm relies on metric comparisons being informative. Thus, if certain features are much more important than others, BF, like other metric-based methods will perform poorly, unless one can identify or learn a good metric.\nWe compare to the highly optimized FLANN (Muja and Lowe 2009) implementation of multiple random kd trees (Rkd). This algorithm gave the best performance of the ones\navailable in FLANN. We found 4 kd trees gave the best results. One has to set an upper limit to the number of points the kd trees are allowed to visit, which we set to 10% of the training points, a number which led the R-kd to give good performance compared to 1 \u2212 NN . Note that R-kd is not parallelizable: the results from each tree at each step inform how to move in the others.\nThe datasets we discuss in this section are available at the LIBSVM(Chang and Lin 2008) repository. Note that for MNIST, we use a permutation-invariant metric based on raw pixel intensities (for easy comparison with other algorithms) even though other metrics could be devised which give better generalization performance.2 For the BF we set nT = 50, k = 50 for all experiments, and for the RF we use 50 trees and 50 features (see Appendix for other choices of parameters). We use a laptop with a 2.3 GHz Intel I7 CPU with 16GB RAM running Mac OS 10.8.5.\nWe find that the BF has similar accuracy to k\u2212NN with a computational cost that scales better, and also the BF is faster than the cover tree, and faster to query than randomized kd trees in most cases (for a CT with = 0.1 shown in appendix, CT becomes faster to train but query time becomes even slower). The results for classification are shown in tables A-4 and A-5.\nWe have also studied the regret, i.e. how much accuracy one loses being online over being offline. In the offline BF each tree gets an independently reshuffled version of the data. Regret is small for all data sets tested, less than 10% of the error rate.\nThe training and testing times for the classification benchmarks are shown in Table A-4, and the error rates in Table A-5. For more results, We find that indeed BF has similar error rates to k-NN, and the sum of training and testing time is a fraction of that for naive k-NN. We emphasize that the main advantage of BFs is the ability to quickly train on and respond to arbitrarily large numbers of examples (because of logarithmic scaling) as would be obtained in an online streaming scenario. To our knowledge, these properties are unique to BFs as compared with other approximate nearest neighbor schemes.\nWe also find that for some datasets the offline Random Forest classifier has a higher error rate, and the total training and testing time is higher. Note also that the offline Random Forest needs to be retrained fully if we change the amount of data. On the other hand, there are several data sets for which RFs out-perform BFs, namely those for which it is possible to identify informative sub-sets of features. Furthermore, we generally find that training is faster for BFs than RFs because BFs do not have to solve a complicated optimization problem, but at test time RFs are faster than BFs because computing answers to a small number of decision tree questions is faster than computing distances. On the other hand, online R-kd is faster to train since it only does single feature comparisons at each node in the trees, however since it uses less informative decisions than metric comparisons it ends up searching a large portion of the previously seen data\n2A simple HOG metric gives the BF a 1.1% error rate on MNIST."}, {"heading": "35.47 13.8 380 404 11.5 51.4 625", "text": ""}, {"heading": "16.20 5.2 433 485 65.7 172.5 1.32", "text": "points, which makes it slower to test. Note that our main point was to compare to algorithms that do metric comparisons, but these comparisons are informative as well."}, {"heading": "Conclusion and future work", "text": "We have described and studied a novel online learning algorithm with empirical Nlog(N) training and log(N) querying scaling with the amount of data N , and similar performance to k \u2212NN .\nThe speed of this algorithm makes it appropriate for applications such as real-time machine learning, and metric learning(Weinberger, Blitzer, and Saul 2006). Interesting future avenues would include: combining the BF with random projections, analyzing speedup and impact on performance; testing a real time tracking scenario, possibly first passing the raw pixels through a feature extractor."}, {"heading": "Additional results for CT", "text": ""}, {"heading": "Additional results for k \u2212NN and RF", "text": ""}], "references": [{"title": "Instancebased learning algorithms", "author": ["D.W. Aha", "D. Kibler", "M.K. Albert"], "venue": "Machine Learning 6:37\u201366.", "citeRegEx": "Aha et al\\.,? 1991", "shortCiteRegEx": "Aha et al\\.", "year": 1991}, {"title": "Cover tree for nearest neighbor", "author": ["A. Beygelzimer", "S. Kakade", "J. Langford"], "venue": "Proceedings of the 2006 23rd International Conference on Machine Learning.", "citeRegEx": "Beygelzimer et al\\.,? 2006", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2006}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning 45:5\u201332.", "citeRegEx": "Breiman,? 2001", "shortCiteRegEx": "Breiman", "year": 2001}, {"title": "Near neighbor search in large metric spaces", "author": ["S. Brin"], "venue": "Proceedings of the 21st International Conference on Very Large Data Bases, 574\u2013584.", "citeRegEx": "Brin,? 1995", "shortCiteRegEx": "Brin", "year": 1995}, {"title": "LIBSVM Data: Classification, Regression, and Multi-label", "author": ["Chang", "C.-C.", "Lin", "C.-J."], "venue": "http://www.csie.ntu.edu.tw/ \u0303cjlin/ libsvmtools/datasets/. [Online; accessed 4June-2014].", "citeRegEx": "Chang et al\\.,? 2008", "shortCiteRegEx": "Chang et al\\.", "year": 2008}, {"title": "Cover-Tree", "author": ["D.N. Crane"], "venue": "http://www.github. com/DNCrane/Cover-Tree. [Online; accessed 1-July2014].", "citeRegEx": "Crane,? 2011", "shortCiteRegEx": "Crane", "year": 2011}, {"title": "An algorithm for finding best matches in logarithmic expected time", "author": ["J. Friedman", "J. Bentley", "R. Finkel"], "venue": "ACM Trans. on Mathematical Software 3:209\u2013226.", "citeRegEx": "Friedman et al\\.,? 1977", "shortCiteRegEx": "Friedman et al\\.", "year": 1977}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newsletter 11(1):10\u2013", "citeRegEx": "Hall et al\\.,? 2009", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Extensions of lipschitz maps into a hilbert space", "author": ["W. Johnson", "J. Lindenstrauss"], "venue": "Contemp. Math. 26:189\u2013 206.", "citeRegEx": "Johnson and Lindenstrauss,? 1984", "shortCiteRegEx": "Johnson and Lindenstrauss", "year": 1984}, {"title": "Online learning of robust object detectors during unstable tracking", "author": ["Z. Kalal", "J. Matas", "K. Mikolajczyk"], "venue": "IEEE Transactions on Online Learning for Computer Vision 1417 \u2013 1424.", "citeRegEx": "Kalal et al\\.,? 2009", "shortCiteRegEx": "Kalal et al\\.", "year": 2009}, {"title": "NV-tree: nearest neighbors at the billion scale", "author": ["H. Lejsek", "B.P. J\u00f3nsson", "L. Amsaleg"], "venue": "Proceedings of the 1st ACM International Conference on Multimedia.", "citeRegEx": "Lejsek et al\\.,? 2011", "shortCiteRegEx": "Lejsek et al\\.", "year": 2011}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D.G. Lowe"], "venue": "VISAPP (1), 331\u2013340.", "citeRegEx": "Muja and Lowe,? 2009", "shortCiteRegEx": "Muja and Lowe", "year": 2009}, {"title": "Five balltree construction algorithms", "author": ["S.M. Omohundro"], "venue": "International Computer Science Institute Berkeley.", "citeRegEx": "Omohundro,? 1989", "shortCiteRegEx": "Omohundro", "year": 1989}, {"title": "Foundations of Multidimensional and Metric Data Structures", "author": ["H. Samet"], "venue": "Morgan Kaufman.", "citeRegEx": "Samet,? 2006", "shortCiteRegEx": "Samet", "year": 2006}, {"title": "A two-dimensional interpolation function for irregularly-spaced data", "author": ["D. Shepard"], "venue": "Proceedings of the 1968 23rd ACM national conference, 517\u2013524. ACM.", "citeRegEx": "Shepard,? 1968", "shortCiteRegEx": "Shepard", "year": 1968}, {"title": "Optimised kd-trees for fast image descriptor matching", "author": ["C. Silpa-Anan", "R. Hartley"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, 1\u20138. IEEE.", "citeRegEx": "Silpa.Anan and Hartley,? 2008", "shortCiteRegEx": "Silpa.Anan and Hartley", "year": 2008}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "J. Blitzer", "L. Saul"], "venue": "Advances in neural information processing systems 18:1473.", "citeRegEx": "Weinberger et al\\.,? 2006", "shortCiteRegEx": "Weinberger et al\\.", "year": 2006}, {"title": "Reduction techniques for instance-based learning algorithms", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Machine Learning 38:257\u2013286.", "citeRegEx": "Wilson and Martinez,? 2000", "shortCiteRegEx": "Wilson and Martinez", "year": 2000}, {"title": "i.e. the number of features of the dataset rounded to the closest integer (the value recommended by (Breiman", "author": [], "venue": null, "citeRegEx": "\u221a,? \\Q2001\\E", "shortCiteRegEx": "\u221a", "year": 2001}], "referenceMentions": [{"referenceID": 3, "context": "There are several existing methods, including KD-trees (Friedman, Bentley, and Finkel 1977), Geometric Nearneighbor Access Trees (Brin 1995), and Nearest Vector trees (Lejsek, J\u00f3nsson, and Amsaleg 2011) that build tree search structures on large datasets (see (Samet 2006) for an extensive bibliography).", "startOffset": 129, "endOffset": 140}, {"referenceID": 13, "context": "There are several existing methods, including KD-trees (Friedman, Bentley, and Finkel 1977), Geometric Nearneighbor Access Trees (Brin 1995), and Nearest Vector trees (Lejsek, J\u00f3nsson, and Amsaleg 2011) that build tree search structures on large datasets (see (Samet 2006) for an extensive bibliography).", "startOffset": 260, "endOffset": 272}, {"referenceID": 12, "context": "The ball tree online insertion algorithm (Omohundro 1989) is rather costly, requiring a volume minimizing step at each addition.", "startOffset": 41, "endOffset": 57}, {"referenceID": 8, "context": "can immediately be combined with random projections to obtain speedup, as it is known by the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss 1984) that the number of projections needed to maintain metric distances only grows as log(D) as the data dimensionality D grows.", "startOffset": 121, "endOffset": 153}, {"referenceID": 11, "context": "In fact, kd trees typically scale no better than brute force in higher than 20 dimensions (Muja and Lowe 2009) , but multiple random kd trees have been shown to overcome this difficulty.", "startOffset": 90, "endOffset": 110}, {"referenceID": 17, "context": "In particular, for classification the Condensed Nearest Neighbor algorithm (Wilson and Martinez 2000) only adds a point if the previously seen points misclassify it.", "startOffset": 75, "endOffset": 101}, {"referenceID": 14, "context": "Many options exist, but in this paper we use a Shepard weighted average (Shepard 1968), where the weights are the inverse distances, so that the estimate is", "startOffset": 72, "endOffset": 86}, {"referenceID": 11, "context": "We compare to the highly optimized FLANN (Muja and Lowe 2009) implementation of multiple random kd trees (Rkd).", "startOffset": 41, "endOffset": 61}, {"referenceID": 7, "context": "For 1\u2212NN , 3\u2212NN andRF we use the Weka(Hall et al. 2009) implementation.", "startOffset": 37, "endOffset": 55}, {"referenceID": 2, "context": "trees and \u221a D features per node (recommended in (Breiman 2001)), andCT with b = 1.", "startOffset": 48, "endOffset": 62}], "year": 2015, "abstractText": "We describe a new instance-based learning algorithm called the Boundary Forest (BF) algorithm, that can be used for supervised and unsupervised learning. The algorithm builds a forest of trees whose nodes store previously seen examples. It can be shown data points one at a time and updates itself incrementally, hence it is naturally online. Few instance-based algorithms have this property while being simultaneously fast, which the BF is. This is crucial for applications where one needs to respond to input data in real time. The number of children of each node is not set beforehand but obtained from the training procedure, which makes the algorithm very flexible with regards to what data manifolds it can learn. We test its generalization performance and speed on a range of benchmark datasets and detail in which settings it outperforms the state of the art. Empirically we find that training time scales as O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and N the amount of data.", "creator": "TeX"}}}