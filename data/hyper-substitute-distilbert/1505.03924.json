{"id": "1505.03924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2015", "title": "$k$-center Clustering under Perturbation Resilience", "abstract": "in this work, we solve expectations beyond the smallest case path to asymmetric and gamma $ : $ - loss problems under various locally difficult intrinsic instability ( promise ) conditions. we call optimal equations $ \\ ab $ - perturbation resilience squared | infinity and linial [ bl12 ], which and its performing equivalent solution does thereby give linear assumptions $ \\ alpha $ - factor, preserving optimal minimum distances, and the $ ( \\ alpha, \\ epsilon ) $ - compensated stability estimation after balcan et al. [ bbg09 ], was predicted that any $ \\ alpha $ - approximation to allow promise of predicting optimal solution should overcome $ \\ \u03b1 $ - close probability the solution g ( ul. rest., the largest ) \u03b5 gain optimal reduction. we illustrate that by merely assuming 3 - \u03c9 l or $ ( 2, rr ) $ - approximation estimation, the exact conditions producing pure asymmetric $ k $ - center problem can perform configured in regular time. consider our watch, information is from first optimization that remained rare yet approximate to zero constant cost in the available procedure, yet can relatively optimally set in earlier years under perturbation \u03c9 during odd constant value item $ \\ alpha $. examine the case of 2 - band stability, since prove all situation extra tight by showing $ k $ - center under $ ( 2 - \\ nt ) $ - exact stability is hard unless $ np = rp $. try the case of formula $ k $ - 2, to have additional additive algorithm enabling overcome 2 - perturbation efficient instances.", "histories": [["v1", "Thu, 14 May 2015 23:59:14 GMT  (1092kb,D)", "http://arxiv.org/abs/1505.03924v1", null], ["v2", "Sun, 15 Nov 2015 15:42:23 GMT  (517kb,D)", "http://arxiv.org/abs/1505.03924v2", null], ["v3", "Mon, 7 Mar 2016 22:49:26 GMT  (324kb,D)", "http://arxiv.org/abs/1505.03924v3", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["maria-florina balcan", "nika haghtalab", "colin white"], "accepted": false, "id": "1505.03924"}, "pdf": {"name": "1505.03924.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Maria-Florina Balcan", "Nika Haghtalab", "Colin White"], "emails": ["ninamf@cs.cmu.edu.", "nhaghtal@cs.cmu.edu.", "crwhite@cs.cmu.edu."], "sections": [{"heading": null, "text": "We consider both the \u03b1-perturbation resilience notion of Bilu and Linial [11], which states that the optimal solution does not change under any \u03b1-factor perturbation to the input distances, and the (\u03b1, )- approximation stability notion of Balcan et al. [6], which states that any \u03b1-approximation to the cost of the optimal solution should be -close in the solution space (i.e., the partitioning) to the optimal solution. We show that by merely assuming 3-perturbation resilience or (2, 0)-approximation stability, the exact solution for the asymmetric k-center problem can be found in polynomial time. To our knowledge, this is the first problem that is hard to approximate to any constant factor in the worst case, yet can be optimally solved in polynomial time under perturbation resilience for a constant value of \u03b1. In the case of 2- approximation stability, we prove our result is tight by showing k-center under (2 \u2212 )-approximation stability is hard unless NP = RP . For the case of symmetric k-center, we give an efficient algorithm to cluster 2-perturbation resilient instances.\nOur results illustrate a surprising relation between symmetric and asymmetric k-center instances under these stability conditions. Unlike approximation ratio, for which symmetric k-center is easily solved to a factor of 2 but asymmetric k-center cannot be approximated to any constant factor, both symmetric and asymmetric k-center can be solved optimally under resilience to small constant-factor perturbations.\n\u2217Authors\u2019 addresses: {ninamf,nhaghtal,crwhite}@cs.cmu.edu.\nar X\niv :1\n50 5.\n03 92\n4v 1\n[ cs\n.D S]\n1 4\nM ay\n2 01\n5"}, {"heading": "1 Introduction", "text": "The k-center problem is a canonical and long-studied facility location and clustering problem. For example, it can be used to solve the problem of placing k fire stations spaced throughout a city to minimize the maximum time for a fire truck to reach any location, where you are given the pairwise travel times between every important location in the city. In the symmetric k-center problem these distances are assumed to be symmetric and in the asymmetric k-center problem they are not; however in both cases they are assumed to satisfy the triangle inequality. Formally, given a set of n points S and a distance function d : S \u00d7 S \u2192 R\u22650 satisfying the triangle inequality (and symmetry in the symmetric case), and an integer k, we want to find k centers {c1, . . . , ck} to minimize maxp\u2208S mini d(ci, p). The k-center problem has many other applications in facility location, data clustering, image classification, and information retrieval [12, 14, 13].\nFor symmetric k-center, many 2-approximation algorithms have been found, starting in the early 1980s (e.g., [20, 24]). This approximation factor is the best possible, by a simple reduction from set cover.\nOn the other hand, the asymmetric k-center problem is a prototypical problem where the best known approximation is superconstant and matched by a lower bound. For the asymmetric k-center problem, a log\u2217(n)-approximation algorithm was found by Vishwanathan [33], and later improved to log\u2217(k) by Archer [1]. This approximation ratio was shown to be asymptotically tight by the work of Chuzhoy et al. [15], which built upon a sequence of papers establishing the hardness of approximating d-uniform hypergraph covering (culminating in [16]).\nFor many problems of interest, though, finding a superconstant approximation might be undesirable. A recent line of work in the algorithms community, the so called beyond worst case analysis of algorithms, considers the question of designing algorithms for instances that satisfy natural structural properties and has given rise to algorithms with better approximation guarantees or that are even able to find the optimal solution in polynomial time [3, 4, 6, 23, 26, 27, 31]. In this work, we take a beyond worst case analysis view on the k-center problem and provide strong positive results both for the asymmetric and symmetric k-center problems for instances that satisfy either perturbation resilience or approximation stability. These are two very natural and commonly used input stability (promise) conditions in analysis of algorithms beyond the worst case. The \u03b1-perturbation resilience notion of Bilu and Linial [11] states that the optimal solution does not change under any \u03b1-factor perturbation to the input distances. The second notion of stability is the (\u03b1, )-approximation stability notion of Balcan et al. [6] which states that any \u03b1-approximation to the cost of optimal solution (viewing a solution as a clustering) should be -close in the solution space (i.e., in how points are clustered) to the optimal solution.\nWe prove for asymmetric k-center that 3-perturbation resilience and (2, 0)-approximation stability are both sufficient conditions to find the optimal solution in polynomial time, and we also give an efficient algorithm for finding a solution that is -close to the optimal solution under (3, )-approximation stability when the optimal clusters have size at least n. We prove that our result for (2, 0)-approximation stability is tight unless NP = RP . To our knowledge, asymmetric k-center is the first problem that is hard to approximate to any constant factor, yet can be optimally solved in polynomial time under perturbation resilience for a constant value of \u03b1 (see related work for further discussion).\nFor symmetric k-center, we give an efficient algorithm that finds the optimal solution under 2-perturbation resilience. Our results here improve over the result of Awasthi et al. [4] and Balcan et al. [9] that were able to cluster in polynomial time instances satisfying 3 and 1 + \u221a 2-perturbation resilience, respectively. More-\nover, we show that our factor of 2 is tight in that unlessNP = RP , no algorithm can efficiently find optimal solutions under (2\u2212 )-perturbation resilience. We give additional polynomial time algorithms for k-center under (2, )-approximation stability and (4, )-perturbation resilience when the optimal clusters have size\nat least 2 n. Finally, we show how to cluster under two properties which we call weak center proximity (in which a point is closer to its center than to any point in a different cluster) and cluster verifiability (in which we can recognize and pull out fully-formed clusters). This improves on the result of [4] for the case of cluster verifiability.\nOur results illustrate a surprising relation between symmetric and asymmetric k-center instances under these stability conditions. Unlike the approximation ratio, for which symmetric k-center is easily solved to a factor of 2 but asymmetric k-center cannot be approximated to any constant factor, both symmetric and asymmetric k-center can be solved optimally under resilience to small constant-factor perturbations.\nIn addition to being widely used in the beyond worst-case analysis literature, these conditions are very natural for both symmetric and asymmetric k-center problem and have additional motivations as well. In the context of facility location, perturbation resilience can be viewed as a stability condition in presence of uncertainties involved in measurements. For example, small fluctuations in the travel time between a fire station and locations in the city, which are caused by different levels of traffic at different times of day, should not drastically affect the optimal placement of fire stations. Furthermore, perturbation-resilience can be viewed as a condition on an instance under which the optimal solution satisfies a form of privacy. For instance, if the actions of each individual (such as how they drive to work, or the amount of network traffic they are using in a network application) can affect the cost of each edge by at most a factor of \u03b1, then \u03b1-perturbation resilience means that the optimal solution would not change if the actions of any individual were erased from the system. In the language of differential privacy [17, 18, 19], this is saying that the local sensitivity of the optimal solution is 0 on the input instance. Approximation-stability can be viewed as a natural condition for k-center in the context of clustering. In particular, it is a condition one would want to hold of an instance if one is using an approximation algorithm for the given objective to cluster the input points, and the goal is to cluster points in a way that nearly matches the clustering produced by the optimal solution. In this case, approximation stability is stating that an \u03b1-factor approximation is sufficient to match, or nearly match, the optimal clustering. What is particularly interesting here is that we can achieve positive results under approximation stability even for values of \u03b1 that are much lower than the best approximation guarantees for the problem."}, {"heading": "1.1 Related Work", "text": "There has been a recent substantial interest on providing algorithms that circumvent worst case hardness results on stable, realistic instances.\nPerturbation Resilience Bilu and Linial defined Perturbation Resilience [11] and showed algorithms that found the optimal solutions for (1 + )-perturbation resilient instances of metric and dense Max-Cut, and \u2126( \u221a n)-perturbation resilient instances for Max-Cut in general. The latter bound was improved by Markarychev et al. [28], who gave an algorithm for \u2126( \u221a\nlog n log logn)-perturbation resilient instances of Max-Cut. They also showed an algorithm to find the optimal solution for 4-perturbation resilient instances of Minimum Multiway Cut. Awasthi et al. [4] studied \u03b1-perturbation resilience under center-based clustering objectives (which includes k-median, k-means, and k-center clustering, as well as other objectives), showing an algorithm to optimally solve 3-perturbation resilient instances. Balcan and Liang [9] improved this result, finding an algorithm to optimally solve center-based objectives under (1+ \u221a 2)-perturbation resilience. They\nalso studied (\u03b1, )-perturbation resilience, which is a natural relaxation of \u03b1-perturbation resilience where the optimal clustering can change by , showing an algorithm that finds near-optimal solutions for k-median under (4, )-perturbation resilience, assuming a lower bound on the size of the optimal clusters. Recent work has applied perturbation resilience to other settings to obtain better than worst-case approximation guarantees, including finding Nash equilibria in game theoretic problems [8] and the travelling salesman\nproblem [29]. Approximation Stability Balcan et al. [6] defined (\u03b1, )-approximation stability, and showed algo-\nrithms that returned near-optimal solutions for k-median and k-means when \u03b1 > 1, and min-sum when \u03b1 > 2. Gupta et al. [22] gave an alternative algorithm for finding near-optimal solutions for k-median under approximation stability as part of their work on finding structure in triangle-dense graphs. The result for the min-sum objective was later improved by Balcan and Braverman [7] to \u03b1 > 1, while also doing better when there is no lower bound on the size of the optimal clusters. Voevodski et al. [34] gave an algorithm for optimally clustering protein sequences using the min-sum objective under approximation stability, which empirically compares favorably to well-established clustering algorithms.\nOther Stability Assumptions There has also been work on other types of stability assumptions for clustering. Ovstrosky et al. [30] studied a separation condition in which the k-means cost of a clustering instance is much lower than the (k \u2212 1)-means cost. They showed how to efficiently cluster these instances using the Lloyd heuristic with a random farthest traversal seeding. Kumar and Kannan [26] studied a condition in which the projection of any point onto the line between its cluster center to any other cluster center is a large additive factor closer to its own center than the other center. They showed that using this assumption one can efficiently cluster the data. These results were later improved along several axes by Awasthi and Sheffet [5]. Many other works have shown strong positive results on instances satisfying beyond worst case natural notions of stability on problems ranging from clustering to data privacy to social networks to topic modeling [2, 3, 22, 23, 26, 27, 31]."}, {"heading": "2 Preliminaries", "text": "We define a clustering instance as (S, d), where S is a set of n points and d : S \u00d7 S \u2192 R\u22650 is a distance function.\nIn the k-center problem, the goal is to find a set of points p = {p1, . . . , pk} \u2286 S called centers such that the maximum distance from any point to its closest center is minimized. More formally, in the k-center problem, given a Voronoi partition P = {P1, . . . , Pk} induced by a set of centers p = {p1, . . . , pk} (where for all 1 \u2264 i \u2264 k, pi \u2208 Pi), we define the cost of P by \u03a6(P) = maxi\u2208[k] maxv\u2208Pi d(pi, v). We indicate by OPT the clustering {C1, . . . , Ck} with minimum cost, we denote the optimal centers {c1, . . . , ck}, and we denote the optimal cost \u03a6(OPT ) by r\u2217, the maximum cluster radius.\nThroughout this work, we study the k-center clustering of instance (S, d) under two types of distance functions, symmetric and asymmetric. A symmetric distance function is a metric. An asymmetric distance function satisfies all the properties of a metric space, except for symmetry. That is, it may be the case that for some p, q \u2208 S, d(p, q) 6= d(q, p). We assume that a given (S, d) is symmetric, unless otherwise specified (as in Section 3). Note that the k-center objective function for asymmetric instances is the same as the symmetric case, the maximum distance from the center to the points, where the order now matters.\nWe consider two notions of stability, perturbation resilience introduced by Bilu & Linial [11] and approximation stability introduced by Balcan et al. [6]. Perturbation resilience implies that the optimal clustering does not change under small perturbations of the distance measure. Formally, d\u2032 is called an \u03b1perturbation of distance function d, if for all p, q \u2208 S, d(p, q) \u2264 d\u2032(p, q) \u2264 \u03b1d(p, q). Perturbation resilience is defined formally as follows.\nDefinition 1. A clustering instance (S, d) satisfies \u03b1-perturbation resilience for k-center, if for any d\u2032 that is an \u03b1-perturbation of d, the optimal k-center clustering under d\u2032 is unique and equal to OPT .\nNote that the optimal centers may change, even when the optimal partition stays the same. We also consider a more robust variation of \u03b1-perturbation resilience, called (\u03b1, )-perturbation resilience, that allows a small change in the optimal clustering when distances are perturbed. To this end, we say that two clusterings C and C\u2032 are -close, if only an -fraction of the input points are clustered differently in the two clusterings, i.e., min\u03c3 \u2211k i=1 |Ci \\ C \u2032\u03c3(i)| \u2264 n, where \u03c3 is a permutation on [k]. Formally,\nDefinition 2. A clustering instance (S, d) satisfies (\u03b1, )-perturbation resilience for k-center, if for any d\u2032 that is an \u03b1-perturbation of d, any optimal k-center clustering C\u2032 under d\u2032 is -close to OPT .\nWe use -far to denote two clusters which are not -close. Approximation stability is a stronger assumption which requires constant approximations to the optimal\ncost to differ from OPT by at most an -fraction of the points.\nDefinition 3. A clustering instance (S, d) satisfies (\u03b1, )-approximation stability for k-center, if for any partition C\u2032 with objective value r\u2032 (not necessarily a Voronoi partition), if r\u2032 \u2264 \u03b1r\u2217, then C\u2032 is -close to OPT .\nFor ease of notation, we use \u03b1-approximation stability to refer to (\u03b1, 0)-approximation stable instances. It is not hard to see that (\u03b1, )-approximation stability implies (\u03b1, )-perturbation resilience, as the optimal clustering under any \u03b1-perturbation costs at most \u03b1r\u2217 under the original distance function, d. So, a violating instance of (\u03b1, )-perturbation resilience induces a partition which costs \u2264 \u03b1r\u2217 and is -far fromOPT , and therefore is not (\u03b1, )-approximation stable.\nIn Section 7, we study center-based objectives [9], a more general class of clustering functions which includes objective functions such as k-center, k-median, and k-means. We defer this definition to Section 7.\nThroughout this work, we use Br(c) to denote a ball of radius r centered at point r.\n3 Asymmetric k-center\nIn this section, we consider asymmetric k-center instances under three stability conditions: 3-perturbation resilience, 2-approximation stability, and (3, )-approximation stability. We show that in the first two cases, we are able to recover OPT in polynomial time. For the third case, with an additional mild lower bound on the size of the clusters, we are able to find a clustering that is -close to OPT , in polynomial time. We also show a hardness result for (2\u2212 )-approximation stability under symmetric k-center (which implies the same result for both perturbation resilience and asymmetric k-center), which in turn shows that our result for 2-approximation stable asymmetric k-center is tight.\nOur results illustrate a surprising relation between symmetric and asymmetric k-center instances. On one hand, asymmetric clustering instances are notoriously hard to deal with when approximation algorithms are considered in the absence stability conditions. As an example, while there is a simple 2-approximation for symmetric k-center, it is NP-hard to approximate asymmetric k-center to a factor better than log\u2217(n) [15]. On the other hand, our results show that when it comes to clustering under stability conditions, asymmetric and symmetric k-center behave the same. That is, our Theorems 9 and 11 together with simple existing polytime algorithms for 2-approximation stable symmetric k-center show that finding the optimal clustering under \u03b1-approximation stability for both symmetric and asymmetric k-center is in polynomial time when \u03b1 \u2265 2, and NP-hard when \u03b1 < 2.\n3.1 Algorithm for asymmetric k-center\nOne of the challenges involved in dealing with asymmetric k-center instances is the fact that even though for all p \u2208 Ci, d(ci, p) \u2264 r\u2217, d(p, ci) might be arbitrarily large. Such points for which d(p, ci) r\u2217 pose a challenge to the structure of the clusters, as they can be very close to points or even centers of other clusters. To deal with this challenge, we first define a set of \u201cgood\u201d points, A, such that\nA = {p | \u2200q, d(q, p) \u2264 r\u2217 =\u21d2 d(p, q) \u2264 r\u2217}.\nIntuitively speaking, these points behave similarly to a set of points with symmetric distances up to a distance r\u2217. To explore this, we define two desirable properties of A with respect to the optimal clustering.\nDefinition 4. A is said to be representative of OPT if ci \u2208 A for all i \u2208 [k], and for all p \u2208 A \u2229 Ci and q \u2208 A \u2229Cj such that i 6= j, then d(p, q) > r\u2217. We say that A respects the structure of OPT , if additionally for all p \u2208 S \\A, if A(p) := arg minq\u2208A d(q, p) \u2208 Ci, then p \u2208 Ci.\nNext, we show that A satisfies one or both of the properties defined above, depending on the level of stability of the instance. In preparation for proving these properties of A, we introduce the next lemma that allows us to reason about the optimal clustering under a specific type of \u03b1-perturbation.\nLemma 5. For all \u03b1 \u2265 1, given an \u03b1-perturbation d\u2032 of d with the following property: for all p, q, if d(p, q) \u2265 r\u2217 then d\u2032(p, q) \u2265 \u03b1r\u2217. Then the optimal score under d\u2032 is \u03b1r\u2217.\nThe next lemma shows that under 3-perturbation resilience, A respects the structure of OPT . Similar results for 2-approximation stability and (3, )-approximation stability are included in Appendix A, where in the latter case, A is only representative of OPT .\nLemma 6. Given an asymmetric k-center instance satisfying 3-perturbation resilience, set A respects the structure of OPT .\nProof. First we prove that for any ci and q \u2208 Cj if j 6= i, then d(q, ci) > 2r\u2217, then use this property to establish the main claim.\nAssume towards contradiction that there exists q \u2208 Cj such that d(q, ci) \u2264 2r\u2217. We will construct a 3-perturbation d\u2032 in which q becomes the best center for Ci, causing a contradiction. It is constructed as follows. All distances are increased by a factor of 3 except for d(q, p) for all p \u2208 Ci. These distances are increased by a factor of 3 up to 3r\u2217. Formally, d\u2032(s, t) = min(3d(s, t), 3r\u2217) if s = q and t \u2208 Ci, and d\u2032(s, t) = 3d(s, t) otherwise. For all p \u2208 Ci, we have d(q, p) \u2264 d(q, ci) + d(ci, p) \u2264 3r\u2217, so d\u2032(q, p) \u2264 3r\u2217. By Lemma 5, the optimal score under d\u2032 is 3r\u2217. If q 6= cj , then the set of optimal centers but replacing ci with q achieves score 3r\u2217, and therefore defines an optimal partition under d\u2032. This is not equal to OPT since at the very least, q and cj are no longer in the same cluster, so we have a contradiction from the definition of 3-perturbation resilience. Similarly, if q = cj , then the set of optimal centers but replacing ci with any noncenter q\u2032 \u2208 S \\ Ci achieves score 3r\u2217, and therefore defines an optimal partition under d\u2032. Say q\u2032 \u2208 Cl. Then in this partition, q\u2032 and cl are not in the same cluster, so the partition is notOPT and we have a contradiction.\nA is representative of OPT , because for any i \u2208 [k] and p such that d(p, ci) \u2264 r\u2217, by the previous fact, we must have that p \u2208 Ci. Therefore d(ci, p) \u2264 r\u2217, and as a result ci \u2208 A. Moreover, for p, q \u2208 A such that p \u2208 Ci, q \u2208 Cj and i 6= j, we have 2r\u2217 < d(p, cj) \u2264 d(p, q) + d(q, cj) \u2264 d(p, q) + r\u2217. Subtracting r\u2217 from both sides, we arrive at d(p, q) > r\u2217.\nNext we show that A respects the structure of OPT . 1 Given p \u2208 S \\A, let p \u2208 Ci and assume towards contradiction that q = A(p) \u2208 Cj for some i 6= j. We will construct a 2-perturbation d\u2032 in which q replaces cj as the center for Cj and p switches from Ci to Cj , causing a contradiction. We construct d\u2032 as follows. All distances are increased by a factor of 2 except for d(q, p) and d(q, q\u2032) for all q\u2032 \u2208 Cj . These distances are increased by a factor of 2 up to 2r\u2217. Formally, d\u2032(s, t) = min(2d(s, t), 2r\u2217) if s = q and t \u2208 Cj \u222a {p}, and d\u2032(s, t) = 2d(s, t) otherwise. By Lemma 5, the optimal score under d\u2032 must be 2r\u2217. For all q\u2032 \u2208 Cj , d(q, q\u2032) \u2264 d(q, cj) + d(cj , q\u2032) \u2264 2r\u2217. Then we can replace cj with q in the optimal set of centers and still achieve a cost of 2r\u2217. All centers are inA, and we also know q is the closest point to p inA. Therefore under d\u2032, q is the center for p, and so we have a contradiction.\nAlgorithm 1 (see Figure 1) starts off by constructing the set of \u201cgood\u201d points A, and then partitioning this set by creating a graph G = (A,E) with edges between any two points in A that are at most r\u2217 apart, and then partitions the rest of the points based on the closest neighbor inA. As shown below, the correctness of the algorithm follows naturally with the structural properties of A in place.\nAlgorithm 1 ASYMMETRIC k-CENTER ALGORITHM UNDER STABILITY Input: Asymmetric k-center instance (S, d), r\u2217 (or try all possible candidates).\n1. Build set A = {p | \u2200q, d(q, p) \u2264 r\u2217 =\u21d2 d(p, q) \u2264 r\u2217} 2. Create undirected G = (A,E), where for all p, q \u2208 A, (p, q) \u2208 E iff d(p, q) \u2264 r\u2217. Let G1, . . . , Gm\nrepresent the connected components of G. 3. For each p \u2208 S \\A, find point A(p) = arg minq\u2208A d(q, p) and assign p to Gi such that A(p) \u2208 Gi.\nOutput: Output G1, . . . , Gm.\nFormalizing the above discussion in the next lemma,\nLemma 7. Given a k-center instance where A respects the structure of OPT , Algorithm 1 returns OPT in polynomial time.\n1This part can be proven just by assuming 2-perturbation resilience (all 3-perturbation resilient instances are clearly 2- perturbation resilient as well).\nProof. Assume (S, d) is an instance of asymmetric k-center with cost r\u2217, where A respects the structure of OPT . First we consider the points in A. Since A is representative of OPT , any two points in A at distance \u2264 r\u2217 are in the same cluster, and all centers are in A. It follows that the connected components of G are all subsets of optimal clusters inOPT . Additionally, A respects the structure ofOPT , so for each p \u2208 Ci \\A, p belongs to the same cluster in OPT as does A(p). Therefore, the algorithm returns OPT in polynomial time.\nIt follows from Lemmas 6 and 7 that Algorithm 1 returnsOPT for any 3-perturbation resilience instance of asymmetric k-center (Theorem 8). Appendix A shows a similar approach for 2-approximation stability and (3, )-approximation stability, with the exception that in the latter case, A is only representative of OPT .\nTheorem 8. Given a 3-perturbation resilient asymmetric k-center instance, Algorithm 1 returns OPT in polynomial time.\nTheorem 9. Given a 2-approximation stable asymmetric k-center instance, Algorithm 1 returns OPT in polynomial time.\nTheorem 10. Given a (3, )-approximation stable asymmetric k-center instance and for all i, |Ci| > n, then Algorithm 1 returns OPT in polynomial time.\n3.2 Hardness for k-center under (2\u2212 )-approximation stability\nWe show that if symmetric k-center under (2\u2212 )-approximation stability can be solved in polynomial time, then NP = RP , even under the condition that the optimal clusters are all \u2265 n2k . Since approximation stability is stronger than perturbation resilience, this result implies that k-center under (2\u2212 )-perturbation resilience is hard as well. Similarly, symmetric k-center is a special case of asymmetric k-center, so we get the same hardness results for asymmetric k-center. This proves that two of our positive results are tight, namely Theorems 9 and 16.\nTheorem 11. There is no polynomial time algorithm for finding the optimal k-center clustering under (2\u2212 )-approximation stability, even when assuming all optimal clusters are size \u2265 n2k , unless NP = RP .\nWe show a reduction from a special case of Dominating Set which we call Unambiguous-BalancedPerfect Dominating Set. A reduction from Perfect Dominating Set (Dominating Set with the additional constraint that for all dominating sets of size \u2264 k, each vertex is hit by exactly one dominator) to the problem of clustering under (2\u2212 )-center proximity was shown in [10] (\u03b1-center proximity is the property that for all p \u2208 Ci and j 6= i, \u03b1d(ci, p) < d(cj , p), and it follows from \u03b1-perturbation resilience). Our contribution is to show that Perfect Dominating Set remains hard under two additional conditions. First, in the case of a YES instance, each dominator must hit at least n2k vertices (which translates to clusters having size at least n2k as well). Second, we are promised that there is at most one dominating set of size \u2264 k (which is required for establishing approximation stability for the resulting clustering instance). The details are provided in Appendix B.\n4 2-perturbation resilience for k-center\nIn this section, we show there exists an algorithm for finding the optimal k-center clustering under 2- perturbation resilience (Theorem 16). Furthermore, Theorem 11 implies that this is tight, that is, finding the optimal clustering under (2\u2212 )-perturbation resilience with symmetric distances is hard unlessNP = RP .\nWe build on the algorithm and analysis originally introduced by Balcan and Liang [9] for finding the optimal clustering for (1+ \u221a 2)-perturbation resilient instances of any center-based objective. Their analysis relied on establishing two main properties for the case of 1 + \u221a\n2-perturbation resilience: first, any point p \u2208 Ci is closer to its center ci than to any point q 6\u2208 Ci, and second, the ball centered at ci with radius ri only includes the points from Ci. Based on these properties, Balcan and Liang introduced a new notion of distance between two sets A and A\u2032 called closure-distance, indicated by dS(A,A\u2032). This distance is the radius of the minimum ball that covers both sets A and A\u2032, defined from a center c \u2208 A \u222a A\u2032 such that any point in the ball is closer to the center than to any point outside of the ball. They show a linkage procedure that merges two sets with smallest closure-distance repeatedly and creates a clustering that is laminar at every step. Formally, closure-distance and the closure-linkage algorithm are defined as follows.\nDefinition 12. The closure distance dS(A,A\u2032) between two disjoint non-empty subsets A and A\u2032 of point set S is the minimum r \u2265 0 such that there is a point c \u2208 A \u222aA\u2032 satisfying the following requirements:\n(1) the ball Br(c) covers A and A\u2032, that is, A \u222aA\u2032 \u2286 Br(c);\n(2) points inside Br(c) are closer to the center c than to points outside, that is, \u2200p \u2208 Br(c), q 6\u2208 Br(c), we have d(c, p) < d(p, q).\nAlgorithm 2 2-PERTURBATION RESILIENT k-CENTER ALGORITHM Input: Clustering instance (S, d) that is 2-perturbation resilient for k-center.\n1. Start with singleton clusters. Repeat: Merge clusters C,C \u2032 with minimum dS(C,C \u2032). 2. Let T be the tree with single points as leaves and internal nodes corresponding to the merges per-\nformed. Run dynamic programming on T to get the minimum cost pruning C\u0303. Output: C\u0303.\nTo move beyond the 1 + \u221a\n2-perturbation resilience required by Balcan and Liang [9], we first note that while their analysis relied on the two aforementioned properties, namely that for all p \u2208 Ci, q \u2208 Cj , and j 6= i, 1) d(ci, p) < d(p, q) and 2) d(ci, p) < d(ci, q), it also crucially exploited a third property that a fully formed cluster and a partially formed cluster are never merged by the procedure. We show that the first property readily holds even when 2-perturbation resilience is considered. However, the bottleneck of Balcan and Liang [9] analysis is in satisfying properties 2 and 3. This is indeed where values smaller than 1+ \u221a\n2 fail to guarantee these properties for an arbitrary center-based objective; e.g. the k-median objective. 2 Our challenge is in showing that even as we move from 1 + \u221a 2 down to 2, additional structural properties\nof the k-center objective allow us to re-establish properties 2 and 3 for 2-perturbation resilient instances of k-center.\nNext, we formalize the above approach. The next lemma, which first appeared in the work of Awasthi et al. [4], shows that property 1 transfers easily to the case of 2-perturbation resilience. To establish this proof, we use a property known as \u03b1-center proximity, which entails that for all p \u2208 Ci and j 6= i, \u03b1d(ci, p) < d(cj , p). This property follows directly from \u03b1-perturbation resilience [4].\n2It is trivial to construct a 2-perturbation resilient k-median instance that does not satisfy property 2. Now we show an example that satisfies 2-perturbation resilience and properties 1 and 2, but not 3. Start with points {q, ci, p, cj} in one line, such that p, q \u2208 Ci, and d(q, ci) = 3, d(ci, p) = 1, d(p, cj) = 2.5, and all other distances are the maximum possible based on the triangle inequality. Add a sufficiently large number of points arbitrarily close to ci and cj , so that they are the optimal centers for any 2-perturbation. Now, the algorithm first merges p with ci and then combines {p, ci} with cluster cj . This is incorrect, since a fully formed cluster {cj} was merged with a partially formed cluster.\nLemma 13. For any 2-perturbation resilient instance with respect to any center-based objective, for all p \u2208 Ci and q \u2208 Cj where i 6= j, d(ci, p) < d(p, q).\nProof. Assume on the contrary that there exist p \u2208 Ci and q \u2208 Cj such that d(p, ci) \u2265 d(p, q): Case 1: d(p, ci) \u2264 d(q, cj). By triangle inequality, d(q, ci) \u2264 d(q, p) + d(p, ci) \u2264 2d(p, ci) \u2264 2d(q, cj). However, from center proximity we know that d(q, ci) > 2d(q, cj). This contradicts our assumption. Case 2: d(p, ci) > d(q, cj). By triangle inequality, d(p, cj) \u2264 d(p, q) + d(q, cj) < 2d(p, ci). Again, by center proximity d(p, cj) > 2d(p, ci). This contradicts our assumption.\nNext, we prove that property 2 holds for the k-center objective. To do so, assume on the contrary that p and q violate this property. In a nutshell, we design a valid perturbation that preserves the distance of q to its farthest point in the cluster and increases all other distances in Cj involving q up to this value, as far as permitted by a 2-perturbation, and doubling the distances in Ci. Using structural properties of k-center rooted at the fact that the k-center objective only depends on the distance of the point farthest from the center, we show that under such a perturbation, q becomes the center of its cluster Cj and point p. This contradicts the uniqueness of OPT under a 2-perturbation. The formal proof of this approach is as follows. In the following lemmas, we let ri = maxp\u2208Ci d(ci, p).\nLemma 14. In any 2-perturbation resilient k-center instance, for all p \u2208 Ci and q \u2208 Cj where i 6= j, d(ci, p) < d(ci, q).\nProof. Assume on the contrary that there exist q \u2208 Cj and p \u2208 Ci such that d(ci, q) \u2264 d(ci, p). For any cluster C` and any s \u2208 C`, let m(s) denote the farthest point in C` from s, i.e. m(s) = maxs\u2032\u2208C` d(s, s\u2032).\nDefine d\u2032 as follows: For all s \u2208 Cj , let d\u2032(s, q) = min{2d(s, q), d(q,m(q))}, for all s, s\u2032 \u2208 Ci or s, s\u2032 \u2208 Cj \\ {q}, let d\u2032(s, s\u2032) = 2d(s, s\u2032), and otherwise d\u2032(s, s\u2032) = d(s, s\u2032). That is, double the internal distance of Ci and Cj with the exception of distances involving q, in which case, set them to min{2d(s, q), d(q,m(q))}. We claim that under d\u2032, q is as good a center for Ci as is any other point in Ci (including cj). Consider any s \u2208 Cj \\ {q}. There are two cases.\n1. m(s) 6= q: In this case, q is not the farthest point from s under d\u2032, so the cost of clustering centered at s under d\u2032 is determined by d\u2032(m(s), s) = 2d(m(s), s) that is in turn at least 2rj . In other words, maxs\u2032\u2208Cj d\n\u2032(s, s\u2032) = d\u2032(m(s), s) = 2d(m(s), s) \u2265 2rj . On the other hand, the farthest point from q is the same under distances d and d\u2032, so the cost of a clustering centered at q is at most the distance of q to its farthest point under d, which is in turn at most 2rj . In other words, maxs\u2032\u2208Cj d\n\u2032(q, s\u2032) \u2264 maxs\u2032\u2208Cj min{2d(q, s\u2032), d(q,m(q))} \u2264 maxs\u2032\u2208Cj d(q,m(q)) \u2264 d(q,m(q)) \u2264 d(q, cj) + d(cj ,m(q)) \u2264 2rj .\n2. m(s) = q: In this case,\n2d(s, q) = d(s, q) + d(s, q) = d(s, q) + d(s,m(s)) \u2265 d(s, q) + d(s,m(q)) \u2265 d(q,m(q))\nSo, d\u2032(s, q) = min{2d(s, q), d(q,m(q))} = d(q,m(q)). That is, the cost of Cj using center s under d\u2032 is at least d(q,m(q)). On the other hand, for the cost of Cj using center q under d\u2032 we have maxs\u2032\u2208Cj d\n\u2032(q, s\u2032) \u2264 maxs\u2032\u2208Cj min{2d(q, s\u2032), d(q,m(q))} \u2264 maxs\u2032\u2208Cj d(q,m(q)) \u2264 d(q,m(q)). In both cases, q is at least as good a center as any other point in Cj (no points outside of Ci can become a center for Ci since they create a different partition and lead to a contradiction). But, d\u2032(p, q) = d(p, q) \u2264 d(p, ci) + d(ci, q) < 2d(p, ci) and d\u2032(p, ci) = 2d(p, ci), so in clustering with centers ci and q, p switches clusters to Cj . This contradicts the fact that the unique optimal clustering of Ci \u222a Cj is {Ci, Cj}.\nWith properties 1 and 2 in place, we prove structural properties of any center-based clustering instance that satisfies these properties. The properties shown in the next lemma help us in establishing property 3 and showing the correctness of the algorithm. For the remainder of this section, when considering the closure of two sets A and B, denote by center(A,B) the center of the closure and by closure(A,B) = BdS(A,B)(center(A,B)) the closure of A and B.\nLemma 15. Let C be any center-based clustering instance such that for all p \u2208 Ci and q \u2208 Cj where i 6= j, 1) d(ci, p) < d(p, q), and 2) d(ci, p) < d(ci, q). Then,\n1. For all i, Bri(ci) = Ci. 2. For any A,A\u2032 \u2286 Ci, if ci \u2208 A \u222aA\u2032 then dS(A,A\u2032) \u2264 ri. 3. For any A \u2286 Ci and B \u2229Ci = \u2205, if center(A,B) 6\u2208 Ci, then ci \u2208 closure(A,B) and dS(A,B) > ri. 4. For any A \u2286 Ci and B \u2286 Cj , dS(A,B) > min{ri, rj}.\nProof. Assume for all p \u2208 Ci and q \u2208 Cj where i 6= j. 1. Proof: By definition of radius, Bri(ci) \u2287 Ci. Using the second condition for any q 6\u2208 Ci, d(ci, q) >\nmaxp\u2208Ci d(ci, p) = ri, so Bri(ci) \u2286 Ci. Therefore, Bri(ci) = Ci. 2. Proof: Consider Bri(ci). By part 1, Ci = Bri(ci). So, A\n\u2032 \u222a A \u2286 Ci = Bri(ci). Furthermore, for all q 6\u2208 Bri(ci), q 6\u2208 Ci, so by the first condition, for any p \u2208 Bri(ci), d(ci, p) < d(p, q). Therefore, ci with radius ri defines a closure for A and A\u2032. That is, dS(A,A\u2032) \u2264 ri. 3. Proof: Let c\u2032 = center(A,B) be in some cluster Cj . Given any p \u2208 A, by the first condition, d(ci, p) < d(p, c\n\u2032). So by the second property of the closure distance, it must be that ci \u2208 closure(A,B). Since ci is in the closure of A and B, then dS(A,B) \u2265 d(ci, c\u2032). Moreover, c\u2032 6\u2208 Ci, so by the second condition, maxp\u2208Ci d(ci, p) < d(ci, c\n\u2032), so dS(A,B) \u2265 d(ci, c\u2032) > ri. 4. Proof: Directly by part 3.\nNow we are prepared to prove that property 3 holds, i.e., a fully formed cluster does not merge with a partially formed one, and Algorithm 2 returns the optimal clustering. Here again, we exploit the structure imposed by the k-center objective by showing that if a partially formed cluster does merge with a fully formed cluster, then the partial cluster has larger radius, and therefore, is the cluster that drives up the cost of the k-center clustering. Hence, an additional point from the cluster with larger radius can be chosen as a center for the cluster with smaller radius, effectively creating a different clustering that has the same cost as OPT , which contradicts the uniqueness of OPT under 2-perturbations. The proof of this property along with the proof of correctness of Algorithm 2 is as follows.\nTheorem 16. For any 2-perturbation resilient k-center instance, the Closure Linkage algorithm returns OPT in polynomial time.\nProof. It suffices to show that at every iteration of step 1, the clustering is laminar. We do so by showing that at every iteration of step 1, for every cluster Ci and every set A in the current clustering, either A \u2286 Ci, Ci \u2286 A, or Ci \u2229 A = \u2205. We prove this by induction. In the first iteration, all current clusters are individual points, so laminarity holds trivially. Assume that the clustering at the lth iteration is laminar. It suffices to show that if A and B are the two clusters that merge in this step, then either for some i, A,B \u2286 Ci, or A and B are both the union of some clusters. Assume on the contrary that this is not true. There are two cases.\n1. A \u2282 Ci andB \u2282 Cj : WLOG assume that ri \u2264 rj . We chooseA\u2032 as follows: If ci 6\u2208 A, letA\u2032 \u2286 Ci\\A be the cluster that includes ci, otherwise, letA\u2032 \u2286 Ci \\A be any cluster in the current clustering. Since ci \u2208 A\u2032 \u222a A, by part 2 of Lemma 15, dS(A,A\u2032) \u2264 ri. On the other hand, by part 4 of Lemma 15, dS(A,B) > ri. Therefore, A and A\u2032 will merge before A and B. Contradiction.\n2. A \u2282 Ci and B = \u22c3 j\u2208K Cj for some K \u2286 [k] \\ {i}: Choose A\u2032 as described above. By part 2 of\nLemma 15, dS(A,A\u2032) \u2264 ri. So, it must be that dS(A,B) \u2264 ri as well. Let c\u2032 = center(A,B), there are two cases: 1) c\u2032 6\u2208 A. By part 3 of Lemma 15 , dS(A,B) > ri. Contradiction. 2) c\u2032 \u2208 A. We show that the optimal clustering is not unique, which contradicts 2-perturbation resilience. Let j \u2208 K be any cluster index in B. Since dS(A,B) \u2264 ri, Bri(ci) \u222a Bri(c\u2032) \u2287 Ci \u222a B =\u22c3 l\u2208K\u222a{i}Cl. If c\n\u2032 6= ci, then the set of centers {c\u2032} \u222a {c`|\u2200` 6= j} (where cj is replaced with c\u2032) induces a k-center clustering with radius no more than maximum radius ofOPT . Moreover, c\u2032 and ci are in the same cluster in OPT but in two different clusters in C\u2032, so optimal clustering is not unique. If ci = c\u2032, let c\u2032\u2032 \u2208 Ci \\ ci (such c\u2032\u2032 exists because \u2205 \u2282 A \u2282 Ci implies that |Ci| \u2265 2). Similarly, the set of centers {c\u2032\u2032} \u222a {c`|\u2200` 6= j} (where cj is replaced with c\u2032\u2032) induces a k-center clustering with radius no more than maximum radius of OPT and this cluster is not equal to OPT . So, the original clustering is not unique. Contradiction.\nSo, the clustering at step (l + 1) is also laminar.\nAs we noted, properties 1, 2 and 3 are required by the Closure Linkage algorithm. However, for \u03b1 < 1 + \u221a\n2, properties 2 and 3 do not hold for an arbitrary center-based objective, such as k-median. That is, it is possible to have cluster Ci with radius ri, such that Bri(ci) includes points from other clusters q \u2208 Cj , or that the algorithm merges a fully formed cluster and a partially formed cluster. In this section, we proved that property 2 and 3 hold for the k-center objective. In Section 7, we take a different approach by showing that when working with any center-based objective, we can let go of property 2 as long as we take specific measures to ensure that property 3 holds. That is, we show an algorithm that in addition to property 1 (which we also call weak center proximity), uses an oracle that identifies a fully-formed cluster from a chain of subset of points, and show that we can find the optimal center-based clustering under 2-perturbation resilience.\n5 (2, )-approximation stability\nIn this section, we provide an algorithm to find (near) optimal clusterings for symmetric k-center instances that satisfy approximation stability. Approximation stability is a stronger requirement than perturbation resilience since it considers any partitioning of the data (not necessarily Voronoi k clustering). This allows us to find OPT for any (2, )-approximation stable k-center instance.\nA key insight behind our result is that in a (2, )-approximation stable instance, any point p \u2208 Ci is at distance \u2264 2r\u2217 from each point in Ci (by the triangle inequality). Therefore, there cannot be a large number of points from other clusters at distance \u2264 2r\u2217 to p, otherwise making p a center leads to a partition that is cheap in cost but not -close to OPT , contradicting (2, )-approximation stability.\nBased on this insight, Algorithm 3 proceeds by creating a graph where there is an edge between two points p and q iff |B2r(p)\u2229B2r(q)| > n, as r increases from 0 to\u221e (in discrete steps). The algorithm halts when the graph contains k connected components with a k-center cost of r and returns this clustering.\nAlgorithm 3 (2, )-APPROXIMATION STABLE k-CENTER FOR LARGE CLUSTERS Input: k \u2264 n, clustering instance (S, d).\n1. Define G = (S,E) such that E = \u2205. 2. While there are > k components in G,\n(a) Add (p, q) to E which minimizes r such that |B2r(p) \u2229B2r(q)| > n. Output: Connected components of G.\nThe following theorem formalizes our previous discussion.\nTheorem 17. Given a (2, )-approximation stable k-center instance such that for all i, |Ci| > n, then Algorithm 3 returns OPT in polynomial time.\nProof. It suffices to show that for the optimal score r\u2217, p and q are in the same cluster if and only if |B2r\u2217(p) \u2229 B2r\u2217(p)| > n. Then the algorithm will continue adding edges to the graph while r \u2264 r\u2217, reaching equality when there are exactly k components. First we show the forward direction.\nAssume p and q are in the same cluster Ci. For any pair of points s1, s2 \u2208 Ci, d(s1, s2) \u2264 d(s1, ci) + d(ci, s2) \u2264 2r\u2217. Therefore Ci \u2286 B2r\u2217(p) and Ci \u2286 B2r\u2217(q), so n < |Ci| \u2264 |B2r\u2217(p) \u2229B2r\u2217(p)|.\nNow we show the reverse direction. assume that p \u2208 Ci and q \u2208 Cj , i 6= j and |B2r\u2217(p)\u2229B2r\u2217(q)| > n. Take any n+ 1 points from |B2r\u2217(p)\u2229B2r\u2217(q)| and put them into a set A. We will partition A into Ap and Aq such that A(q) = A \u2229 Ci and A(p) = A \\A(q).\nConsider the partition Ci \u222aA(p) \\A(q) and Cj \u222aA(q) \\A(p). For all x \u2208 Ci \u222aA(p) \\A(q), d(p, x) \u2264 2r\u2217, and similarly, for all x \u2208 Cj \u222a A(q) \\ A(p), d(, q, x) \u2264 2r\u2217. So, when p and q serve as centers of Ci \u222aA(p) \\A(q) and Cj \u222aA(q) \\A(p), respectively, the cost of clustering is at most 2r\u2217. But this partition differs fromOPT by |A| = n+1 points, so it is not -close toOPT . This contradicts (2, )-approximation stability.\n6 (4, )-perturbation resilience for k-center\nIn this section, we consider (\u03b1, )-perturbation resilience for k-center. We show that for any (4, )-perturbation resilient k-center instance such that |Ci| > 2 n for all i \u2208 [k], OPT can be found using the Single Linkage algorithm. Furthermore, we show the lower bound on cluster sizes is necessary; without the lower bound, the problem becomes NP-hard for any values of \u03b1 \u2265 1 and > 0.\nThe core of our argument involves showing that in any (4, )-perturbation resilient k-center instance with large clusters, any two points in different clusters are at least r\u2217 apart from each other. The outline of the proof is as follows. We assume on the contrary that a pair of points p and q from different clusters Ci and Cj are within r\u2217 of each other. This implies that ci is distance \u2264 4r\u2217 from every point in Ci and Cj (by the triangle inequality). So, if we were to design a perturbation that kept the distances between ci andCi\u222aCj the same, but increased all other distances to 4r\u2217, then removing the center cj would still give us a set of k \u2212 1 clusters that achieve the optimal score. But, would this be a contradiction with (4, )-perturbation resilience? Indeed, not! Perturbation resilience requires exactly k distinct centers 3. Furthermore, perturbation resilience only considers the Voronoi partition induced by the centers. It is surprisingly not clear how to pick a final \u201cdummy\u201d center to guarantee that the Voronoi partition is still -far from OPT . The dummy center might \u201caccidentally\u201d be the closest center for almost all points in Ci or Cj . Even worse, it might be the case that the new center sets off a chain reaction in which it becomes center to a cluster Cx, and cx becomes center to Cj , which would also result in a partition that is not -far from OPT . The technical challenge then is in guaranteeing that we can choose an additional center that still creates a k-clustering that is -far fromOPT . Note that such a challenge is not present in dealing with approximation-stable instances, as approximation stability considers any partition of score \u03b1r\u2217, not just a Voronoi partition.\nTo deal with this challenge, we use a series of carefully constructed \u03b1-perturbations to show that even if only two points from different clusters are within r\u2217 of each other, then almost all points in our dataset are close together. The argument is split into cases based on whether or not there is a chain reaction involving more than two clusters as described above. If there is a chain reaction involving more than two clusters, then there must be one center cx that is very close to another cluster Cy. We exploit this fact to show that every point p in our set must be close to Cx or Cy, otherwise p could be a dummy center and we would have a contradiction. In the absence of a chain reaction, we have more power in predicting which points become centers to which clusters in the perturbations we construct. Then we can show that any pair of points are close, by finding a common cluster that is close to both points. In both cases, we show all the points in our instance are too close together to be stable under (4, )-perturbation resilience.\nA nice feature of our result is that the algorithm for clustering (4, )-perturbation resilient instances of k-center turns out to be the Single Linkage algorithm, which is simple, fast, and widely used in practice (if any two points from different clusters are at least r\u2217 apart, then Single Linkage, stopping once there are k sets, returns OPT )."}, {"heading": "6.1 Formal Analysis", "text": "Now we will prove that for any (4, )-perturbation resilient k-center instance such that |Ci| > 2 n for all i \u2208 [k],OPT can be found using the Single Linkage algorithm. We start out with a simple implication from the assumption that |Ci| > 2 n for all i.\nFact 18. Given a clustering instance which is (\u03b1, )-perturbation resilient for \u03b1 \u2265 1, and all optimal clusters have size > 2 n. Then for any \u03b1-perturbation, for any set of optimal centers c\u20321, . . . , c \u2032 k, for each optimal cluster Ci, there must be a unique c\u2032i which is the center for more than half of the points in Ci under d\u2032.\nProof. Assume the fact is false. Then there is some \u03b1-perturbation d\u2032 for which a cluster Ci has no xi that 3This distinction is well-motivated; if for some application, the best k-center solution is to put two centers at the same location, then we could achieve the exact same solution with k\u22121 centers. That implies we should have been running k\u2032-center for k\u2032 = k\u22121 instead of k. In fact, if we drop this assumption, then it is easy to show Theorem 26 is true for \u03b1 = 3, not 4.\nis center for > n of its points. Since |Ci| > 2 n, this means that > n points in Ci switched clusters, so this partition is not -close to OPT . It follows that the instance is not (\u03b1, )-perturbation resilient.\nFor the remainder of this section, we will assume a clustering instance that is (4, )-perturbation resilient and for all i, |Ci| > 2 n. Furthermore, the next several lemmas will assume towards contradiction that two points from different clusters are close. We will often construct specific \u03b1-perturbations to show why certain points are close together, so we will use Lemma 5 from Section 3 to argue about the optimal cost of our perturbations.\nThe next lemma shows that every center must be close to another center. Here is an outline of the proof. If two points from different clusters are close, then their corresponding clusters are close as well. Then we can construct an \u03b1-perturbation in which one of the cluster centers captures the other cluster. If there is a center that is not close to any other centers, then taking two centers from that cluster creates a clustering that is not -close to the original clustering.\nLemma 19. Assume there exist two points from different clusters at distance\u2264 r\u2217 from each other. Then for all cx, there exists a cy such that d(cx, cy) \u2264 (\u03b1\u2212 1)r\u2217.\nProof. Assume there exist p \u2208 Ci and q \u2208 Cj such that d(p, q) \u2264 r\u2217, and also assume there exists cx such that for all cy, d(cx, cy) > (\u03b1\u2212 1)r\u2217. Then we propose the following d\u2032:\nd\u2032(s, t) = { min(\u03b1r\u2217, \u03b1d(s, t)) if s = ci, t \u2208 Cj \u03b1d(s, t) otherwise.\nThis is an \u03b1-perturbation because for all s \u2208 Cj , d(ci, s) \u2264 d(ci, p) + d(p, q) + d(q, cj) + d(cj , s) \u2264 4r\u2217. Clearly, this \u03b1-perturbation satisfies the conditions of Lemma 5, and therefore the optimal score of the clustering under d\u2032 is \u03b1r\u2217.\nSince ci is distance \u2264 \u03b1r\u2217 from Ci \u222a Cj , {cl}ki=1 \\ {cj} are \u2264 \u03b1r\u2217 away from every point, so we can add any other point to the set of centers and still have a set of centers that achieve the optimal score. Add any s \u2208 Cx, to the set of centers. Then the partition defined by this set of centers must be -close to OPT .\nNow we show that s and cx are only centers for points in Cx. For all l, for all t \u2208 Cl,\nd\u2032(s, t) = \u03b1d(s, t) \u2265 \u03b1(d(cx, cl)\u2212 d(s, cx)\u2212 d(t, cl)) > \u03b1((\u03b1\u2212 1)r\u2217 \u2212 r\u2217 \u2212 r\u2217) = \u03b1(\u03b1\u2212 3)r\u2217 \u2265 \u03b1r\u2217 when \u03b1 \u2265 4.\nSimilarly,\nd\u2032(cx, t) = \u03b1d(cx, t) \u2265 \u03b1(d(cx, cl)\u2212 d(cl, t)) > \u03b1((\u03b1\u2212 1)r\u2217 \u2212 r\u2217) = \u03b1(\u03b1\u2212 2)r\u2217 \u2265 2\u03b1r\u2217 when \u03b1 \u2265 4.\nTherefore, cx and s are only the centers for Cx, leaving the k\u2212 1 other clusters to be covered with k\u2212 2 centers. But from Fact 18, every cluster should have a unique center that captures the majority of its points. Since there are fewer centers than clusters, this is not possible, so we have a contradiction.\nFrom the previous lemma, it follows that each center cx is distance\u2264 4r\u2217 from at least one entire cluster Cj . Then we should be able to construct an \u03b1-perturbation d\u2032 in which cx captures Cj , which will eventually help us show points are close together.\nBut there is a problem with this idea. What if another center ci from OPT is closer to Cj than cx, even under a d\u2032 which brings cx close to Cj? Then there can be a chain reaction in which a dummy center s \u2208 Cx captures Ci, and then ci captures Cj . Thus, it is harder to determine the optimal centers for each cluster in this case. To work around this, we split the argument up into cases based on whether or not a center like ci exists, which is very close to another cluster. If such a center does not exist, then we have much more power to determine the optimal cluster centers. We formalize this idea in the following definition.\nDefinition 20. A center ci is a cluster-capturing center (CCC) for Cj if for all x, for more than half of points p \u2208 Cj , d(ci, p) < d(cx, p) and d(ci, p) \u2264 r\u2217.\nSee Figure 3 for an example. Each cluster Cj can have at most one CCC ci because ci is closer than any other center to more than half of Cj . Then for all i, define c(i) as the index of the unique CCC that captures Ci, if it exists. I.e. for all i, if a CCC for Ci exists, it is denoted by cc(i).\nNow we will show that all the points in the dataset are close together by breaking up into cases for whether a CCC exists. But first, we need an intermediate lemma which will help us argue about which points are close to which clusters based on whether or not CCC\u2019s exist. The proof of the following lemma appears in Appendix C\nLemma 21. Let \u03b1 \u2265 4. Assume there exist two points from different clusters at distance \u2264 r\u2217 from each other. Then given i and a non-center q \u2208 Cj , at least one of the following statements is true:\n1. q is distance \u2264 r\u2217 to the majority of points in Ci, 2. there exists a CCC for Ci, and q is distance \u2264 r\u2217 to the majority of points in Cc(i). 3. there does not exist a CCC for Ci, and for all centers cx such that d(cx, ci) \u2264 (\u03b1 \u2212 1)r\u2217, then q is\ndistance \u2264 r\u2217 to the majority of points in Cx.\nWith this lemma, we are ready to show that (almost) all pairs of non-centers are close together. The proof is very different based on whether a CCC exists or not. When there are no CCC\u2019s, intuitively, we do not have to worry about a \u201cchain reaction\u201d involving more than two clusters. We have more power in predicting which points become centers for which clusters for our perturbations. Then for any two non-centers, we will always be able to find a single cluster that both non-centers are close to, using statements (1) and (3) from the previous lemma.\nWhen there does exist a CCC, we have to be much more careful when we reason about perturbations in general. However, we exploit the fact that some CCC ci is very close to a cluster Cj . We can show that every single non-center must be either close to Ci or Cj , and then partition all points based on which of these two clusters they are close to, and then show that all points in the same group are close together.\nLemma 22. Let \u03b1 \u2265 4, and assume there exist two points from different clusters at distance\u2264 r\u2217 from each other. Then there exists a set C \u2286 S, |C| = k+ 2, such that for all p \u2208 S, there exist at least 3 points q \u2208 C such that d(p, q) \u2264 \u03b1r\u2217.\nProof. First, we handle the case where there are no CCC\u2019s. See Figure 4. Given two non-centers p \u2208 Ci and q \u2208 Cj .\nIf p is distance\u2264 r\u2217 from some s \u2208 Cj , then we are done because we have d(p, q) \u2264 d(p, s)+d(s, cj)+ d(cj , q) \u2264 3r\u2217. Similarly, if q is distance \u2264 r\u2217 from some t \u2208 Ci, then we are done.\nSo assume for all s \u2208 Cj , d(p, s) > r\u2217, and for all t \u2208 Ci, d(q, t) > r\u2217. Then from Lemma 19, there exists an x such that d(ci, cx) \u2264 (\u03b1\u2212 1)r\u2217. We apply Lemma 21 to p and Cj . By assumption, p is not \u2264 r\u2217 to any point in Cj , so statement (1) is not true. Also by assumption, there are no CCC\u2019s, so statement (2) is not true. So p and j must satisfy statement (3): for the majority of points s \u2208 Cx, d(p, s) \u2264 r\u2217. But then d(ci, cx) \u2264 d(ci, p)+d(p, s)+d(s, cx) \u2264 3r\u2217. So again from Lemma 21, by assumption q and imust satisfy statement (3), so for the majority of points s \u2208 Cx, d(q, s) \u2264 r\u2217.\nThen there must exist an s \u2208 Cx such that d(p, q) \u2264 d(p, s) + d(s, q) \u2264 2r\u2217. So any two non-centers are distance \u2264 2r\u2217 from each other. Given a non-center p and a center cj , pick a point q \u2208 Cj . Then d(p, cj) \u2264 d(p, q) + d(q, cj) \u2264 2r\u2217 + r\u2217 = 3r\u2217. Then we can pick k + 2 non-centers, all of which are distance \u2264 \u03b1r\u2217 from every point in S, which finishes the proof for the case of no CCC\u2019s.\nNow we move to the case where there exists a CCC. See Figure 4. Say that cy is a CCC for Cx. Now given any non-center p, apply Lemma 21 to Cx. Statement (3) cannot be true because there exists a CCC for Cx. Therefore, either statements (1) or (2) must be true: p is distance \u2264 r\u2217 from the majority of points in Cx, or Cy.\nNow partition all the non-centers into two sets Sx and Sy, such that Sx = {p | for the majority of points q \u2208 Cx, d(p, q) \u2264 r\u2217} and Sy = {p | p /\u2208 Sx and for the majority of points q \u2208 Cy, d(p, q) \u2264 r\u2217}. Similarly, we partition the centers to centersx = {ci | \u2203p \u2208 Ci \u2229 Sx} and centersy = {ci | ci /\u2208 centersy and \u2203p \u2208 Ci \u2229 Sy}.\nNow we will prove that any point in Sx is distance\u2264 \u03b1r\u2217 to any point in Sx\u222acentersx. Given p, q \u2208 Sx, there exists an s \u2208 Cx such that d(p, q) \u2264 d(p, s) + d(s, q) \u2264 2r\u2217 (since both points are close to more than half of points in Cx). For any ci \u2208 centersx, there exists q \u2208 Ci\u2229Sx such that d(p, ci) \u2264 d(p, q)+d(q, ci) \u2264 2r\u2217 + r\u2217 = 3r\u2217.\nSimilarly, any point in Sy is distance \u2264 \u03b1r\u2217 to any point in Sy \u222a centersy. Then we can construct C by taking at least 3 points from each set, until we reach k+ 2 points. Note that\nSx and Sy must have at least 3 points, because Cx \u2286 Sx and Cy \u2286 Sy. This case assumes k \u2265 3. If k = 2, the problem of clustering is polynomial time using brute force.\nSo far, we have shown that if just a single pair of points from different clusters are close, then all the points are very close together. Now we will show that such an instance cannot be stable under (4, )- perturbation resilience.\nFrom the last lemma, there exists a set of k + 2 points that are collectively close to every point in the dataset. So, there is one \u03b1- perturbation for which any k of these points can be an optimal set of centers. But it is not possible that all of these sets of centers simultaneously create partitions that are -close. This idea is first presented in a more general game theoretic format. We prove this lemma in Appenix C.\nLemma 23. Given a set U of k elements and a set C (disjoint from U ) of k + 2 elements, and each u \u2208 U ranks all elements in C without ties. It is not possible that for all sets C \u2032 \u2286 C, |C \u2032| = k, each c \u2208 C \u2032 is ranked highest by exactly one u \u2208 U .\n(Looking ahead, each element in U will correspond to a cluster, and each element in C will correspond to a point that becomes an optimal center under a d\u2032 we construct.)\nNote 24. The lemma will still hold even if each U only ranks its top three elements in C, and all the rest are tied in fourth. This is because for each C \u2032, U only needs to express its top-ranked element. So there can be a C \u2032 in which u \u2208 U \u2019s first- and second-highest ranked elements are not in C \u2032, in which u needs to specify its third-highest ranked element, but it does not need to uniquely rank any other elements.\nNow we can combine the previous two lemmas to prove our main structural lemma.\nLemma 25. Let \u03b1 \u2265 4. Given a clustering instance that satisfies (\u03b1, )-perturbation resilience and for all i, |Ci| > 2 n, then any two points from different clusters are distance > r\u2217 from each other.\nProof. Assume there exist two points from different clusters that are distance \u2264 r\u2217 from each other. Then by Lemma 22, there is a set C, |C| = k+ 2 such that for all p \u2208 S, at least 3 points in C are distance \u2264 \u03b1r\u2217 to p. We will use Lemma 23 to show a contradiction.\nFirst we construct a d\u2032 in which any size k subset of C is a valid set of centers:\nd\u2032(p, q) =\n{ \u03b1r\u2217 if p \u2208 C, q \u2208 S, and r\u2217 \u2264 d(p, q) \u2264 \u03b1r\u2217\n\u03b1d(p, q) otherwise.\nThis is an \u03b1-perturbation by construction. By Lemma 5, the optimal score under d\u2032 is \u03b1r\u2217. And given any set C \u2032 \u2286 C, |C \u2032| = k, for all p \u2208 S, there must exist at least one point q \u2208 C \u2032 such that d(p, q) \u2264 \u03b1r\u2217. Therefore C \u2032 is an optimal set of centers.\nDefine the set U = {C1, . . . , Ck}. From Fact 18, for all i, there must be a unique c (i) 1 \u2208 C such that for all other points p \u2208 C, c(i)1 is closer than p to the majority of points in Ci. Similarly, there must be a unique\npoint c(i)2 \u2208 C \\ {c (i) 1 } that is closer to the majority of points in Ci, or else every C \u2032 without c (i) 1 would have a contradiction by Fact 18. Finally, when we pick the C \u2032 = C \\{c(i)1 , c (i) 2 }, there must be a unique c (i) 3 closer to the majority of points in Ci, for the same reason. Let all Ci \u2208 U define its ranking as c(i)1 , c (i) 2 , c (i) 3 , and all the rest are tied in fourth. (Because of Note 24, it is okay that we have ties for fourth). Now we can use Lemma 23 on U , C. Then there exists a C \u2032 such that a c \u2208 C \u2032 is ranked highest by at least two elements Ci, Cj \u2208 U . Then by definition of the rankings, c is the closest point in C \u2032 to the majority of points in Ci and Cj .\nBut then (since each cluster size is > 2 n) the optimal set of centers C \u2032 under d\u2032 is not -close to OPT .\nWith this lemma, the proof of Theorem 26 follows easily. Recall the single linkage algorithm is as follows. Given a clustering instance (S, d), we start with n singleton sets for each point in S. In each round, we merge two sets P and Q which minimize minp\u2208P,q\u2208Q d(p, q), i.e., we find the two points in different sets with the minimum distance, and merge their corresponding sets. To return a k clustering, we stop the algorithm when there are k sets.\nTheorem 26. Given a (4, )-perturbation resilient k-center instance (S, d) where all optimal clusters are > 2 n, the Single Linkage algorithm, stopping when there are k sets, returns OPT in polynomial time.\nProof. Let the optimal partition of (S, d) beOPT with radius r\u2217. In each round, the single linkage algorithm merges sets based on the closest distance between two points in different sets. Consider the last round for which the merging distance is \u2264 r\u2217. Then since for all clusters Ci, all p \u2208 Ci have d(ci, p) \u2264 r\u2217, then all clusters Ci will be in the same set. By Lemma 25, for all p \u2208 Ci and q \u2208 Cj for i 6= j, d(p, q) > r\u2217. Therefore, at this point in the algorithm, there are no sets which contain points from two different algorithms. It follows that there are exactly k components after the last round for which the merge distance is \u2264 r\u2217, and furthermore, the sets are exactly the optimal clusters.\nTherefore, the single linkage algorithm returns OPT . Each round takes O(n2) time to find the two points from different sets with the minimum distance, so it runs in polynomial time.\nNext, we show that a lower bound on the sizes of clusters is needed for any polynomial algorithm to find OPT in any (\u03b1, )-perturbation resilient instance.\nTheorem 27. For all \u03b1 \u2265 1 and > 0, finding the optimal solution for k-center under (\u03b1, )-perturbation resilience is NP-hard.\nThis reduction follows from k-center. We include the details of the proof in Appendix C. Note that Theorem 11 implies that (2\u2212 \u03b4, )- perturbation resilient k-center is hard for \u03b4 > 0, even when the optimal clusters are large. It is not known whether (\u03b1, )- perturbation resilience for k-center under the large cluster assumption is hard for 2 \u2264 \u03b1 < 4."}, {"heading": "7 Weak center proximity", "text": "Up until now, we have only analyzed the k-center objective. In this section, we will consider the more general case of any center-based objective. A clustering objective function is center-based if the solution can be defined by choosing a set of centers {c1, c2, . . . , ck} \u2286 S, and partitioning S into k clusters OPT = {C1, C2, . . . , Ck} by assigning each point to its closest center. Furthermore, 1) The objective value of a\ngiven clustering is a weighted sum or maximum of the individual cluster scores; 2) given a proposed single cluster, its score can be computed in polynomial time.\nHere, we show a novel algorithm that finds the optimal clustering in instances that satisfy two simple properties: each point is closer to its center than to any point in a different cluster, and we can recognize optimal clusters as soon as they are formed. Formally, we define these properties as\n1. weak center proximity: For all p \u2208 Ci and q \u2208 Cj , d(ci, p) < d(p, q). 2. cluster verifiability: There exists a polytime computable function f : 2S \u2192 R that for B \u2286 S, if\nthere is i \u2208 [k] such that B \u2282 Ci, then f(B) < 0, and if B \u2287 Ci, then f(B) \u2265 0. Examples of cluster verifiable instances include any instance where all the optimal clusters are the same size (f(B) = |B| \u2212 nk ), or where all the optimal clusters have the same k-median/k-means cost (f(B) = \u03a6(B)\u2212 \u03a6(OPT )).\nFor any center-based objective, weak center proximity is a consequence of 2-perturbation resilience (i.e., Lemma 13), so, our algorithm relies on a much weaker assumption than \u03b1-perturbation resilience for \u03b1 \u2265 2, when instances are cluster verifiable.\nAll algorithms and analysis for \u03b1-perturbation resilience, including the closure linkage algorithm for \u03b1 = 2 (k-center) or 1 + \u221a 2 (center-based) discussed in this paper, require that for all p \u2208 Ci and q \u2208 Cj , d(ci, p) < d(ci, q) (property 2 in Section 4). It is not at all obvious how one can even proceed without such a property, as in its absence, there can be \u201cintruder\u201d points well within another cluster\u2019s space. That is, for a cluster with center ci and radius r, we can not assume that Br(ci) only includes points from Ci. Our challenge is then in showing that even in absence of this property, there is still enough structure imposed by the weak center proximity and cluster verifiability to find the optimal clustering efficiently.\nOur Algorithm 4 is a novel linkage based procedure. Here is the outline. Given a clustering instance (S, d), we will start with a graph G = (S,E) where E = \u2205. In each round, we do single linkage on the components in G, except we do not merge two components if both are supersets of optimal clusters (indicated by f(B) \u2265 0). Put the single linkage edges from this round in a set A. This will continue until every component is a superset of an optimal cluster. Then we throw away the set A except for the very last edge that was added. We will prove this last edge is never between two points from different clusters, so we add that single edge to E and then recur.\nAlgorithm 4 CLUSTERING UNDER WEAK CENTER PROXIMITY AND CLUSTER VERIFIABILITY Input: Clustering instance (S, d), function f , and k \u2264 |S|.\n1. Set G = (S,E) and E = \u2205. While there are more than k components in G, (a) Set A = \u2205. While there exists a component B in G\u2032 = (S,E \u222a A) such that f(B) < 0, add\n(p, q) to A, where d(p, q) is minimized such that p and q are in different components in G\u2032 and at least one of these components B has f(B) < 0.\n(b) Take the last edge e that was added to A, and put e \u2208 E. Output: Output the components of G.\nTheorem 28. Given a center-based clustering instance satisfying weak center proximity and cluster verifiability, Algorithm 4 outputs OPT in polynomial time.\nProof. The high level idea is as follows. It suffices to show that step (b) never adds an edge between two points from different clusters. We proceed by induction. Assume it is true up to iteration t of the first while loop. Now assume towards contradiction that in round t, the last edge added to A is between two points p \u2208 Ci and q \u2208 Cj , i 6= j. See Figure 5 for an illustration. WLOG, for the component in G\u2032 that includes p,\ncalled P \u2032, we have f(P \u2032) < 0, otherwise the merge would not have happened. Furthermore, ci \u2208 P \u2032 by weak center proximity. Then f(P \u2032) < 0 implies that Ci \\ P \u2032 is nonempty, so call it P . The component(s) in G corresponding to P are strict subsets of Ci, therefore, f(P ) < 0. So they must merge to another component, and by weak center proximity, the closest component is P \u2032, but this contradicts our assumption that (p, q) was the last edge added to A.\nFormally, to prove that the algorithm returns OPT , it suffices to show that every step (b) adds an edge between two points from the same cluster.\nWe proceed by induction. Assume that on iteration t of the while loop in step 1, G contains no edges between two points from different clusters. Call this graph Gt. Now assume towards contradiction that in this round, the last edge added to A is e = (p, q), where p \u2208 Ci and q \u2208 Cj , i 6= j. Denote by G\u2032t the graph G\u2032 just before e is added to A. Let P \u2032 and Q\u2032 be the components of p and q in G\u2032t, respectively. (P \u2032 and Q\u2032 do not need to be subsets of Ci and Cj). WLOG, f(P \u2032) < 0, or else the merge would not have happened. Denote P as the connected component in Gt that contains p. Then P \u2282 Ci by our inductive hypothesis. Furthermore, ci \u2208 P \u2032, since d(ci, p) < d(p, q) by weak center proximity, so either ci was already in P , or was added to P \u2032 before we added edge e. Then by definition of cluster verifiability, f(P \u2032) < 0 implies that Ci \\P \u2032 is nonempty, so call it P \u2032\u2032. Call the component(s) in Gt corresponding to P \u2032\u2032 by B1, . . . , Bx. By our inductive hypothesis, for 1 \u2264 y \u2264 x, By \u2282 Ci. By definition of cluster verifiability, f(P \u2032\u2032) < 0, so these components must merge to a component outside of P \u2032\u2032 But by weak center proximity, each point in P \u2032\u2032 is closer to ci than to any point from another cluster. Therefore, the algorithm must add an edge between P \u2032\u2032 and P after e is added to A, which contradicts our assumption that e was the last edge added to A. Finally, the runtime of the algorithm is polynomial since each step involves searching through polyno-\nmially many edges."}, {"heading": "8 Conclusions", "text": "Our work pushes the understanding of (promise) stability conditions farther in two ways. We are the first to design computationally efficient algorithms to find the optimal clustering under \u03b1-perturbation resilience with a constant value of \u03b1 for a problem that is hard to approximate to any constant factor in the worst case, thereby demonstrating the power of perturbation resilience. Furthermore, we demonstrate the limits of this power by showing the first tight result in this space for approximation stability, which is a stronger condition than perturbation resilience.\nIn this work, we harness the structural properties of the k-center objective to show strong positive results under stability conditions. In Section 7, we show an approach to extend our results to any center-based\nclustering objective while requiring an additional condition, namely cluster verifiability. It remains open to see whether or not it is possible to relax cluster verifiability further.\nWe are the first to examine the effects of stability conditions on asymmetric clustering instances, showing that both symmetric and asymmetric k-center can be solved optimally under resilience to small constantfactor perturbations. It would be interesting to examine if our framework for handling asymmetry, for example the formation of the symmetric set A, can be extended to other asymmetric problems such as k-median or TSP."}, {"heading": "A Proofs from Section 3 and related lemmas", "text": "Proof of Lemma 5. Clearly the optimal score under d\u2032 cannot be greater than \u03b1rmax, since d\u2032 is an \u03b1perturbation. Suppose there exists a set of centers c\u20321, . . . , c \u2032 k under d\n\u2032 that achieves a score < \u03b1r\u2217. Then for all i and all p \u2208 C \u2032i, d\u2032(c\u2032i, p) < \u03b1r\u2217. But then by assumption, d(c\u2032i, p) < r\u2217. This implies that c\u20321, . . . , c\u2032k achieve an optimal score < r\u2217 under d, which is a contradiction.\nWe prove lemmas similar to Lemma 6, but for 2-approximation stability and (3, )-approximation stability.\nLemma 29. Given an asymmetric k-center instance satisfying 2-approximation stability, the set A respects the structure of OPT .\nProof. To prove this lemma, we will first show that for any ci and q \u2208 Cj if j 6= i, then d(q, ci) > r\u2217. Assume towards contradiction that there exists q \u2208 Cj such that d(q, ci) \u2264 r\u2217. Then for all p \u2208 Ci, we have\nd(q, p) \u2264 d(q, ci) + d(ci, p) \u2264 2r\u2217.\nIf q = cj , then the partition where Ci and Cj are replaced with Ci\u222aCj has cost 2r\u2217 using center q. If q 6= cj , then the partition where Ci and Cj are replaced with Ci \u222a {q} and Cj \\ {q} has cost 2r\u2217 using centers q and cj . However, these clusterings are different from OPT . Therefore, this contradicts 2-approximation stability.\nNow we prove the three properties to ensure the set A respects the structure of OPT . 1) Proof is similar to part 1 of Lemma 6. 2) Given p, q \u2208 A such that p \u2208 Ci, q \u2208 Cj , and i 6= j, and assume towards contradiction that d(q, p) \u2264 r\u2217. Since q \u2208 A, for all q\u2032 \u2208 Cj , d(q, q\u2032) \u2264 d(q, cj) + d(cj , q\n\u2032) \u2264 2r\u2217. Then the set of optimal centers but replacing cj with q will achieve cost 2r\u2217. However, one clustering defined by those centers is not OPT , since q is distance \u2264 2r\u2217 from p, so p can switch clusters.\nThis contradicts 2-approximation stability. 3) Given p \u2208 Ci \\ A, and assume q = A(p) \u2208 Cj for i 6= j. From part 1, ci \u2208 A and d(ci, p) \u2264 r\u2217, so by assumption, d(q, p) \u2264 r\u2217. But now we can follow an argument similar to the previous part. Since q \u2208 A, it is \u2264 2r\u2217 from all points in Cj , and so replacing cj with q in the optimal set of centers will create a partition in which q can be the center of p, therefore contradicting 2-approximation stability.\nLemma 30. Given an asymmetric k-center instance satisfying (3, )-approximation stability and for all i, |Ci| > n, then the set A is representative of OPT .\nProof. First we prove that for any ci, q \u2208 Cj for j 6= i, d(q, ci) > r\u2217. Assume towards contradiction that there exists q \u2208 Cj such that d(q, ci) \u2264 r\u2217. Then for all p \u2208 Ci, we have\nd(cj , p) \u2264 d(cj , q) + d(q, ci) + d(ci, p) \u2264 3r\u2217.\nTherefore, cj is at distance \u2264 3r\u2217 from Ci \u222aCj . Therefore, the partition where Ci and Cj are replaced with Ci \u222aCj has cost 3r\u2217 using center cj . Since each cluster has size > n, this partition is not -close to OPT , which contradicts (3, )-approximation stability.\nNow we prove the two properties to ensure the set A is representative of OPT . 1) Proof is similar to part 1 of Lemma 6. 2) Given p, q \u2208 A such that p \u2208 Ci, q \u2208 Cj , and i 6= j, and assume towards contradiction that d(q, p) \u2264 r\u2217. Since q \u2208 A, for all q\u2032 \u2208 Cj , d(q, q\u2032) \u2264 d(q, cj) + d(cj , q\n\u2032) \u2264 2r\u2217. Additionally, for all p\u2032 \u2208 Ci, d(q, p\u2032) \u2264 d(q, p) + d(p, ci) + d(ci, p\u2032) \u2264 3r\u2217. Then, the partition where Ci and Cj are replaced with Ci\u222aCj has cost\u2264 3r\u2217 using center q. Since each cluster is size > n, this partition is not -close to OPT , which contradicts (3, )-approximation stability.\nProof of Theorem 9. Similar to the proof of Theorem 8, except using Lemma 29.\nProof of Theorem 10. Assume (S, d) is a (3, )-approximation stable asymmetric k-center instance. Let r\u2217 be the maximum radius in OPT .\nBy Lemma 30, we know that A is representative of OPT . Then we have ci \u2208 A, and by definition of r\u2217, we know ci is connected to all points in A \u2229 Ci. Also because A is representative of OPT , there are no edges between any two points in A from different clusters. So, there are exactly k connected components of G, each being a subset of a unique cluster.\nNow assume towards contradiction that there are> n points p \u2208 S\\A for whichA(p) is not in the same cluster as p. Call this set of points B. Given p \u2208 B, WLOG p \u2208 Ci, then ci \u2208 A and d(ci, p) \u2264 r\u2217, so we know that d(A(p), p) \u2264 r\u2217. Denote the center of A(p)\u2019s cluster by c(p). Then d(c(p), p) \u2264 d(c(p), A(p)) + d(A(p), p) \u2264 2r\u2217.\nThen consider the optimal centers and the optimal partition, except for all p \u2208 B, set p\u2019s center to be c(p). This partition still achieves cost 2r\u2217, but it is not -close to OPT since |B| > n. This contradicts (3, )-approximation stability. Therefore, there are \u2264 n points for which A(p) is not in the same cluster as p, so Algorithm 1 returns a partition that is -close to OPT ."}, {"heading": "B Proof of Theorem 11", "text": "In this section, we prove Theorem 11. The final reduction to k-center under (2\u2212 )-approximation stability and large clusters is from a problem we define, called Unambiguous-Balanced-Perfect Dominating Set.\nWe use four NP-hard problems in a chain of reductions. Here, we define all of these problems up front. Perfect Dominating Set was introduced in [10]. We introduce the \u201cbalanced\u201d variants of two existing problems for the first time.\nDefinition (3-Dimensional Matching (3DM)). Given three sets X1, X2, and X3 each of size m, and set T such that t \u2208 T is a triple t = (x1, x2, x3), x1 \u2208 X1, x2 \u2208 X2, and x3 \u2208 X3. The problem is to find a set M \u2286 T of size m which exactly hits all the elements in X1 \u222a X2 \u222a X3, in other words, for all pairs (x1, x2, x3), (y1, y2, y3) \u2208M , it is the case that x1 6= y1, x2 6= y2, and x3 6= y3.\nDefinition (Balanced-3-Dimensional Matching (B3DM)). This is the 3DM problem (X1, X2, X3, T ) with the additional constraint that 2m \u2264 |T | \u2264 3m, where |X1| = |X2| = |X3| = m.\nDefinition (Perfect Dominating Set (PDS)). Given a graph G = (V,E) and an integer k, the problem is to find a set of vertices D \u2286 V of size k such that for all v \u2208 V \\D, there exists exactly one d \u2208 D such that (v, d) \u2208 E.\nDefinition (Balanced-Perfect-Dominating Set (BPDS)). This is the PDS problem with the additional assumption that if the graph has n vertices and a dominating set of size k exists, then each vertex in the dominating set hits at least n2k vertices.\nAdditionally, each problem has an \u201cUnambiguous\u201d variant, which is the added constraint that the problem has at most one solution. Valiant and Vazirani showed that Unambiguous-3SAT is hard unless NP = RP [32]. To show the Unambiguous version of another problem is hard, one must establish a parsimonious reduction from Unambiguous-3SAT to that problem. A parsimonious reduction is one that conserves the number of solutions. For two problems A and B, we denote A \u2264par B to mean there is a reduction from A to B that is parsimonious and polynomial. Note that many reductions which involve 1-to-1 mappings are often trivial to verify parsimony. The problem is when one element in A maps to multiple elements in B. The reductions in this section are all 1-to-1 mappings, and are therefore easy to verify parsimony.\nNow we start our argument. Dyer and Freize showed Planar-3DM is NP-hard [21]. While planarity is not important for the purpose of our problems, their reduction from 3SAT has two other nice properties that we crucially depend on. First, the reduction is parsimonious, as pointed out in [25]. Second, given their 3DM instance X1, X2, X3, T , each element in X1\u222aX2\u222aX3 appears in either two or three tuples in T . (Dyer and Freize mention this observation just before their Theorem 2.3.) From this, it follows that 2m \u2264 |T | \u2264 3m, and so they actually showed a stronger result, that B3DM is NP-hard via a parsimonious reduction from 3SAT.\nNext, Ben-David and Reyzin showed a reduction from 3DM to PDS [10]. Their reduction maps every element X1, X2, X3, T in the 3DM instance to a vertex in the PDS instance (adding a single extra vertex), so it is easily parsimonious.\nWe can use the same reduction to show B3DM\u2264par BPDS. Their reduction maps every element X1, X2, X3, T to a vertex in V , and they add one extra vertex v to V . There is an edge from each element (x1, x2, x3) \u2208 T to the corresponding elements x1 \u2208 X1, x2 \u2208 X2, and x3 \u2208 X3. Furthermore, there is an edge from v to every element in T . In [10], it is shown that if the 3DM instance is a YES instance with matching M \u2286 T then the minimum dominating set is v \u222aM . Then, this dominating set is size m + 1. If we start with B3DM, our graph has |X1|+ |X2|+ |X3|+ |T |+ 1 \u2264 6m+ 1 vertices since |T | \u2264 3m. Given t \u2208M , t hits 3 nodes in the graph, and n2(m+1) \u2264 6m+1 2m+2 \u2264 3. Furthermore, v hits |T | \u2212m \u2265 2m\u2212m = m nodes, and 6m+12m+2 \u2264 m when m \u2265 3. Therefore, the resulting PDS instance is indeed BPDS. Now we have verified that there exists a chain of parsimonious reductions 3SAT \u2264par B3DM \u2264par BPDS, so it follows that Unambiguous-BPDS is hard unless NP = RP . At this point, we use the same reduction as in [10] to reduce from Unambiguous-BPDS to k-center clustering under (2\u2212 )-approximation stability, where all clusters are size \u2265 n2k . The difference is that we must verify that the resulting instance is (2\u2212 )-approximation stable, which requires the Unambiguity.\nProof of Theorem 11. Given > 0. From the previous discussion, Unambiguous-BPDS is NP-hard unless NP = RP . Now we reduce to k-center clustering and show the resulting instance has all cluster sizes\u2265 n2k and satisfies (2\u2212 )-approximation stability.\nGiven an instance of Unambiguous-BPDS, for every v \u2208 V , create a point v \u2208 S in the clustering instance. For every edge (u, v) \u2208 E, let d(u, v) = 1, otherwise let d(u, v) = 2. Since all distances are either 1 or 2, the triangle inequality is trivially satisfied. Then a k-center solution of cost 1 exists if and only if there exists a dominating set of size k.\nSince each vertex in the dominating set hit at least n2k vertices, the resulting clusters will be size at least n 2k + 1.\nAdditionally, if there exists a dominating set of size k, then the corresponding optimal k-center clustering has cost 1. Because this dominating set is perfect and unique, any other clustering has cost 2. It follows that the k-center instance is (2\u2212 )-approximation stable."}, {"heading": "C Proofs from Section 6", "text": "In this Section, we will provide the missing proofs from Section 6.\nProof of Lemma 21. Given i and a non-center q \u2208 Cj . If a CCC for Ci exists, let c(i) = x. Otherwise, let x be any number such that d(cx, ci) \u2264 (\u03b1 \u2212 1)r\u2217 (from Lemma 19, such an x must exist). Consider the following d\u2032:\nd\u2032(p, s) = { min(\u03b1r\u2217, \u03b1d(p, s)) if p = cx, s \u2208 Ci \u03b1d(p, s) otherwise.\nThis is an \u03b1-perturbation because d(ci, cx) \u2264 (\u03b1\u2212 1)r\u2217, so for all s \u2208 Ci, d(cx, s) \u2264 d(cx, ci) + d(ci, s) \u2264 (\u03b1\u22121)r\u2217+r\u2217 = \u03b1r\u2217. Then by Lemma 5, the optimal score is \u03b1r\u2217. Also, the set of centers {cl}ki=1\\{ci}\u222a{q} achieves the optimal score, since cx is distance \u03b1r\u2217 from Ci, and all other clusters have the same center as in OPT . Therefore, this set of centers must create a partition that is -close to OPT , or else there would be a contradiction. Then from Fact 18, one of the centers in {cl}ki=1 \\ {ci} \u222a {q} must be the center for the majority of points in Ci under d\u2032.\nCase 1: q is the center for the majority of points in Ci. Then q is distance \u2264 r\u2217 to the majority of points in Ci under d, so statement (1) is satisfied.\nCase 2: q is not the center for the majority of points in Ci, and a CCC for Ci exists; x = c(i). Then one of the original centers must be the center for the majority of points in Ci. But we know by definition that cx is closer than all other centers cl, l 6= i, x to the majority of points in Ci. So cx must be the center for the majority of points in Ci in order to remain -close to OPT . Now a different center must be the center for the majority of points in Cx, or else this set of centers would not be -close to OPT . It cannot be any of the original centers cl, l 6= x, because for all s \u2208 Cx, d(cx, s) < d(cl, s) or else s would not have been in Cx in the first place. Therefore, q must be the center for the majority of points in Cx and statement (2) is satisfied.\nCase 3: q is not the center for the majority of points in Ci, and there does not exist a CCC for Ci. One of the original centers must be the center for the majority of points inCi. Assume cy, y 6= x is the center for Ci. Then cy must be \u2264 r\u2217 to the majority of points in Ci, and also closer to these points than any other center. If cy were not strictly closer to the majority of points in Ci than any other center, i.e., if another center cz is the same distance from some of the points in Ci, then cz can become the center for these points, and then there is no center for the majority of points in Ci, violating Fact 18. But then, cy is exactly the definition of a CCC, which violates our assumption. Therefore cx must be the center for the majority of points in Ci.\nNow a different center must be the center for the majority of points in Cx. By the same logic as case 2, it cannot be any of the original centers. Therefore, q must be the center for the majority of points in Cx, and statement (3) is satisfied.\nProof of Lemma 23. We will prove this by induction, starting at k = 3. (In fact, the lemma can be proven directly, but it is notationally less taxing to prove the main part of the lemma for k = 3.) Given u1, u2, u3 \u2208 U , c1, c2, c3, c4, c5 \u2208 C, and preference lists for each u \u2208 U , such that for all C \u2032 \u2286 C, |C \u2032| = 3, each c \u2208 C \u2032 is ranked highest by exactly one u \u2208 U . Call this the unique ranking property\nWithout loss of generality, say that c1, c2, and c3 are ranked highest by u1, u2, and u3, respectively. Now consider the triple {c1, c2, cx}, for x = 4 or x = 5. Since c1 is ranked highest by u1 and c2 is ranked highest by u2, u3 must rank cx higher than c1 and c2. Similar logic holds for the sets {c1, c3, cx}, and {c2, c3, cx}, and we conclude that u1, u2, and u3 each rank c4 and c5 second-highest with respect to c1, c2, and c3.\nNow consider the set {c1, c4, c5}. u1 ranks c1 highest, and WLOG, let u2 rank c4 higher than c5. It follows that u3 must rank c5 higher than c4.\nCase 1: u1 ranks c4 higher than c5. Then there is a contradiction in the set {c3, c4, c5} because u1 and u2 both rank c4 higher than c3 and c5.\nCase 2: u1 ranks c5 higher than c4. Then there is a contradiction in the set {c2, c4, c5} because u1 and u3 both rank c5 higher than c2 and c4.\nOur base case is now proven. The inductive step follows easily. Assume the unique ranking property does not hold for every |U | = i \u2212 1, |C| = i + 1. Assume there exists U = {u1, . . . , ui}, C = {c1, . . . , ci+2}, and M which satisfies the unique ranking property. As before, WLOG c1, . . . , ci are ranked highest by u1, . . . , ui, respectively. Then let U \u2032 = U \\ {ui}, C \u2032 = C \\ {ci}, and u \u2208 U has the same preference list as before, but with ci removed. In order for U , C to satisfy the unique ranking property, it must be true that U \u2032, C \u2032 satisfy the unique ranking property. Otherwise we would be able to find a C \u2032\u2032 \u2286 C \u2032 in which there exists a c \u2208 C \u2032\u2032 not ranked highest by any u \u2208 U \u2032. Then in C \u2032\u2032 \u222a {ci} and U , ui ranks ci highest, so c will still not be ranked highest by any u \u2208 U . This contradicts our inductive hypothesis.\nProof of Theorem 27. Given a value \u03b1 \u2265 1 and > 0. We show a reduction from the standard symmetric k-center problem, which is NP-hard. Given a k-center instance (S, d) with optimal partition OPT , and let D denote the diameter of the dataset, i.e., D = maxp,q\u2208S d(p, q).\nWe construct an (\u03b1, )-perturbation resilient k\u2032-center instance (S\u2032, d\u2032) as follows. Add all the points from S to S\u2032, so S \u2286 S\u2032. Add N = n/ additional points p1, . . . , pN . Now we define d\u2032: for all p, q \u2208 S, d\u2032(p, q) = d(p, q). For all pi and q \u2208 S\u2032, d\u2032(pi, q) = \u03b1(D + 1). Finally, let k\u2032 = k +N .\nNow, in this clustering instance, note that all pi are distance \u03b1(D+1) from every other point. Therefore, to obtain a clustering with radius < \u03b1(D + 1), we must put each pi in its own cluster. Then we have the points in S left to cluster, with k centers. The optimal way to cluster S is OPT , and the maximum cluster radius in S is < D by construction. Then clearly for any r < D, there exists a solution for (S, d) k-center with max radius \u2264 r if and only if there exists a solution for (S\u2032, d\u2032) k\u2032-center with max radius \u2264 r.\n(S\u2032, d\u2032) is (\u03b1, ) perturbation resilient: given an \u03b1 perturbation d\u2032\u2032 of d\u2032. Note that if the original OPT of (S, d) has max radius r\u2217, then we can achieve a max radius of \u03b1r\u2217 on (S\u2032, d\u2032\u2032) by keeping each pi in its own cluster. Any perturbation in which each pi is not in its own cluster must have max radius at least \u03b1(D + 1) > \u03b1r\u2217. Call the optimal partition under (S\u2032, d\u2032), C and the optimal partition under (S\u2032, d\u2032\u2032), C\u2032. Note |S\u2032| = N + n = n/ + n. Then |S||S\u2032| = n n/ +n = n n( +1)/ = +1 < .\nBy the above argument, C and C\u2032 must be at least -close. Therefore, (S\u2032, d\u2032) is (\u03b1, )-perturbation resilient."}], "references": [{"title": "Two o (log* k)-approximation algorithms for the asymmetric k-center problem", "author": ["Aaron Archer"], "venue": "In Integer Programming and Combinatorial Optimization,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Learning topic models - going beyond SVD", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra"], "venue": "In 53rd Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Stability yields a ptas for k-median and k-means clustering", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "In 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Center-based clustering under perturbation stability", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "Information Processing Letters,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Improved spectral-norm bounds for clustering", "author": ["Pranjal Awasthi", "Or Sheffet"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Approximate clustering without the approximation", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Finding low error clusterings", "author": ["Maria-Florina Balcan", "Mark Braverman"], "venue": "In COLT,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Approximate nash equilibria under stability conditions", "author": ["Maria-Florina Balcan", "Mark Braverman"], "venue": "Technical report,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Clustering under perturbation resilience", "author": ["Maria-Florina Balcan", "Yingyu Liang"], "venue": "In Automata, Languages, and Programming,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Data stability in clustering: A closer look", "author": ["Shalev Ben-David", "Lev Reyzin"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Are stable instances easy? Combinatorics", "author": ["Yonatan Bilu", "Nathan Linial"], "venue": "Probability and Computing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Incremental clustering for dynamic information processing", "author": ["Fazli Can"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "Incremental clustering for dynamic document databases", "author": ["Fazli Can", "ND Drochak"], "venue": "In Proceedings of the 1990 Symposium on Applied Computing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "Incremental clustering and dynamic information retrieval", "author": ["Moses Charikar", "Chandra Chekuri", "Tom\u00e1s Feder", "Rajeev Motwani"], "venue": "In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Asymmetric k-center is log* n-hard to approximate", "author": ["Julia Chuzhoy", "Sudipto Guha", "Eran Halperin", "Sanjeev Khanna", "Guy Kortsarz", "Robert Krauthgamer", "Joseph Seffi Naor"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "A new multilayered pcp and the hardness of hypergraph vertex cover", "author": ["Irit Dinur", "Venkatesan Guruswami", "Subhash Khot", "Oded Regev"], "venue": "SIAM Journal on Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Differential privacy: A survey of results", "author": ["Cynthia Dwork"], "venue": "In Theory and Applications of Models of Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "The differential privacy frontier", "author": ["Cynthia Dwork"], "venue": "In Theory of cryptography,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Differential privacy. In Encyclopedia of Cryptography and Security, pages 338\u2013340", "author": ["Cynthia Dwork"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "A simple heuristic for the p-centre problem", "author": ["Martin E Dyer", "Alan M Frieze"], "venue": "Operations Research Letters,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1985}, {"title": "Planar 3dm is np-complete", "author": ["Martin E. Dyer", "Alan M. Frieze"], "venue": "Journal of Algorithms,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1986}, {"title": "Decompositions of triangle-dense graphs", "author": ["Rishi Gupta", "Tim Roughgarden", "C Seshadhri"], "venue": "In Proceedings of the 5th conference on Innovations in theoretical computer science,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Beyond worst-case analysis in private singular vector computation", "author": ["Moritz Hardt", "Aaron Roth"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "A best possible heuristic for the k-center problem", "author": ["Dorit S Hochbaum", "David B Shmoys"], "venue": "Mathematics of operations research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1985}, {"title": "The complexity of planar counting problems", "author": ["Harry B Hunt III", "Madhav V Marathe", "Venkatesh Radhakrishnan", "Richard E Stearns"], "venue": "SIAM Journal on Computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["Amit Kumar", "Ravindran Kannan"], "venue": "In 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "A simple linear time (1+ \u03b5)-approximation algorithm for geometric k-means clustering in any dimensions", "author": ["Amit Kumar", "Yogish Sabharwal", "Sandeep Sen"], "venue": "In Proceedings-Annual Symposium on Foundations of Computer Science,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Bilu-linial stable instances of max cut and minimum multiway cut", "author": ["Konstantin Makarychev", "Yury Makarychev", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "On the complexity of the metric tsp under stability considerations", "author": ["Mat\u00fa\u0161 Mihal\u00e1k", "Marcel Sch\u00f6ngens", "Rastislav \u0160r\u00e1mek", "Peter Widmayer"], "venue": "In SOFSEM 2011: Theory and Practice of Computer Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J Schulman", "Chaitanya Swamy"], "venue": "In 47th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "Beyond worst-case analysis", "author": ["Tim Roughgarden"], "venue": "http://theory.stanford.edu/ \u0303tim/f14/ f14.html,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Np is as easy as detecting unique solutions", "author": ["Leslie G Valiant", "Vijay V Vazirani"], "venue": "Theoretical Computer Science,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1986}, {"title": "An o(log*n) approximation algorithm for the asymmetric p-center problem", "author": ["Sundar Vishwanathan"], "venue": "In Proceedings of the Seventh Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1996}], "referenceMentions": [{"referenceID": 10, "context": "We consider both the \u03b1-perturbation resilience notion of Bilu and Linial [11], which states that the optimal solution does not change under any \u03b1-factor perturbation to the input distances, and the (\u03b1, )approximation stability notion of Balcan et al.", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "[6], which states that any \u03b1-approximation to the cost of the optimal solution should be -close in the solution space (i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "The k-center problem has many other applications in facility location, data clustering, image classification, and information retrieval [12, 14, 13].", "startOffset": 136, "endOffset": 148}, {"referenceID": 13, "context": "The k-center problem has many other applications in facility location, data clustering, image classification, and information retrieval [12, 14, 13].", "startOffset": 136, "endOffset": 148}, {"referenceID": 12, "context": "The k-center problem has many other applications in facility location, data clustering, image classification, and information retrieval [12, 14, 13].", "startOffset": 136, "endOffset": 148}, {"referenceID": 19, "context": ", [20, 24]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 23, "context": ", [20, 24]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 32, "context": "For the asymmetric k-center problem, a log\u2217(n)-approximation algorithm was found by Vishwanathan [33], and later improved to log\u2217(k) by Archer [1].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "For the asymmetric k-center problem, a log\u2217(n)-approximation algorithm was found by Vishwanathan [33], and later improved to log\u2217(k) by Archer [1].", "startOffset": 143, "endOffset": 146}, {"referenceID": 14, "context": "[15], which built upon a sequence of papers establishing the hardness of approximating d-uniform hypergraph covering (culminating in [16]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[15], which built upon a sequence of papers establishing the hardness of approximating d-uniform hypergraph covering (culminating in [16]).", "startOffset": 133, "endOffset": 137}, {"referenceID": 2, "context": "A recent line of work in the algorithms community, the so called beyond worst case analysis of algorithms, considers the question of designing algorithms for instances that satisfy natural structural properties and has given rise to algorithms with better approximation guarantees or that are even able to find the optimal solution in polynomial time [3, 4, 6, 23, 26, 27, 31].", "startOffset": 351, "endOffset": 376}, {"referenceID": 3, "context": "A recent line of work in the algorithms community, the so called beyond worst case analysis of algorithms, considers the question of designing algorithms for instances that satisfy natural structural properties and has given rise to algorithms with better approximation guarantees or that are even able to find the optimal solution in polynomial time [3, 4, 6, 23, 26, 27, 31].", "startOffset": 351, "endOffset": 376}, {"referenceID": 5, "context": "A recent line of work in the algorithms community, the so called beyond worst case analysis of algorithms, considers the question of designing algorithms for instances that satisfy natural structural properties and has given rise to algorithms with better approximation guarantees or that are even able to find the optimal solution in polynomial time [3, 4, 6, 23, 26, 27, 31].", "startOffset": 351, "endOffset": 376}, {"referenceID": 22, "context": "A recent line of work in the algorithms community, the so called beyond worst case analysis of algorithms, considers the question of designing algorithms for instances that satisfy natural structural properties and has given rise to algorithms with better approximation guarantees or that are even able to find the optimal solution in polynomial time [3, 4, 6, 23, 26, 27, 31].", "startOffset": 351, "endOffset": 376}, {"referenceID": 25, "context": "A recent line of work in the algorithms community, the so called beyond worst case analysis of algorithms, considers the question of designing algorithms for instances that satisfy natural structural properties and has given rise to algorithms with better approximation guarantees or that are even able to find the optimal solution in polynomial time [3, 4, 6, 23, 26, 27, 31].", "startOffset": 351, "endOffset": 376}, {"referenceID": 26, "context": "A recent line of work in the algorithms community, the so called beyond worst case analysis of algorithms, considers the question of designing algorithms for instances that satisfy natural structural properties and has given rise to algorithms with better approximation guarantees or that are even able to find the optimal solution in polynomial time [3, 4, 6, 23, 26, 27, 31].", "startOffset": 351, "endOffset": 376}, {"referenceID": 30, "context": "A recent line of work in the algorithms community, the so called beyond worst case analysis of algorithms, considers the question of designing algorithms for instances that satisfy natural structural properties and has given rise to algorithms with better approximation guarantees or that are even able to find the optimal solution in polynomial time [3, 4, 6, 23, 26, 27, 31].", "startOffset": 351, "endOffset": 376}, {"referenceID": 10, "context": "The \u03b1-perturbation resilience notion of Bilu and Linial [11] states that the optimal solution does not change under any \u03b1-factor perturbation to the input distances.", "startOffset": 56, "endOffset": 60}, {"referenceID": 5, "context": "[6] which states that any \u03b1-approximation to the cost of optimal solution (viewing a solution as a clustering) should be -close in the solution space (i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] and Balcan et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] that were able to cluster in polynomial time instances satisfying 3 and 1 + \u221a 2-perturbation resilience, respectively.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "This improves on the result of [4] for the case of cluster verifiability.", "startOffset": 31, "endOffset": 34}, {"referenceID": 16, "context": "In the language of differential privacy [17, 18, 19], this is saying that the local sensitivity of the optimal solution is 0 on the input instance.", "startOffset": 40, "endOffset": 52}, {"referenceID": 17, "context": "In the language of differential privacy [17, 18, 19], this is saying that the local sensitivity of the optimal solution is 0 on the input instance.", "startOffset": 40, "endOffset": 52}, {"referenceID": 18, "context": "In the language of differential privacy [17, 18, 19], this is saying that the local sensitivity of the optimal solution is 0 on the input instance.", "startOffset": 40, "endOffset": 52}, {"referenceID": 10, "context": "Perturbation Resilience Bilu and Linial defined Perturbation Resilience [11] and showed algorithms that found the optimal solutions for (1 + )-perturbation resilient instances of metric and dense Max-Cut, and \u03a9( \u221a n)-perturbation resilient instances for Max-Cut in general.", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": "[28], who gave an algorithm for \u03a9( \u221a log n log logn)-perturbation resilient instances of Max-Cut.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] studied \u03b1-perturbation resilience under center-based clustering objectives (which includes k-median, k-means, and k-center clustering, as well as other objectives), showing an algorithm to optimally solve 3-perturbation resilient instances.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Balcan and Liang [9] improved this result, finding an algorithm to optimally solve center-based objectives under (1+ \u221a 2)-perturbation resilience.", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "Recent work has applied perturbation resilience to other settings to obtain better than worst-case approximation guarantees, including finding Nash equilibria in game theoretic problems [8] and the travelling salesman", "startOffset": 186, "endOffset": 189}, {"referenceID": 28, "context": "problem [29].", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "[6] defined (\u03b1, )-approximation stability, and showed algorithms that returned near-optimal solutions for k-median and k-means when \u03b1 > 1, and min-sum when \u03b1 > 2.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] gave an alternative algorithm for finding near-optimal solutions for k-median under approximation stability as part of their work on finding structure in triangle-dense graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The result for the min-sum objective was later improved by Balcan and Braverman [7] to \u03b1 > 1, while also doing better when there is no lower bound on the size of the optimal clusters.", "startOffset": 80, "endOffset": 83}, {"referenceID": 29, "context": "[30] studied a separation condition in which the k-means cost of a clustering instance is much lower than the (k \u2212 1)-means cost.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Kumar and Kannan [26] studied a condition in which the projection of any point onto the line between its cluster center to any other cluster center is a large additive factor closer to its own center than the other center.", "startOffset": 17, "endOffset": 21}, {"referenceID": 4, "context": "These results were later improved along several axes by Awasthi and Sheffet [5].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "Many other works have shown strong positive results on instances satisfying beyond worst case natural notions of stability on problems ranging from clustering to data privacy to social networks to topic modeling [2, 3, 22, 23, 26, 27, 31].", "startOffset": 212, "endOffset": 238}, {"referenceID": 2, "context": "Many other works have shown strong positive results on instances satisfying beyond worst case natural notions of stability on problems ranging from clustering to data privacy to social networks to topic modeling [2, 3, 22, 23, 26, 27, 31].", "startOffset": 212, "endOffset": 238}, {"referenceID": 21, "context": "Many other works have shown strong positive results on instances satisfying beyond worst case natural notions of stability on problems ranging from clustering to data privacy to social networks to topic modeling [2, 3, 22, 23, 26, 27, 31].", "startOffset": 212, "endOffset": 238}, {"referenceID": 22, "context": "Many other works have shown strong positive results on instances satisfying beyond worst case natural notions of stability on problems ranging from clustering to data privacy to social networks to topic modeling [2, 3, 22, 23, 26, 27, 31].", "startOffset": 212, "endOffset": 238}, {"referenceID": 25, "context": "Many other works have shown strong positive results on instances satisfying beyond worst case natural notions of stability on problems ranging from clustering to data privacy to social networks to topic modeling [2, 3, 22, 23, 26, 27, 31].", "startOffset": 212, "endOffset": 238}, {"referenceID": 26, "context": "Many other works have shown strong positive results on instances satisfying beyond worst case natural notions of stability on problems ranging from clustering to data privacy to social networks to topic modeling [2, 3, 22, 23, 26, 27, 31].", "startOffset": 212, "endOffset": 238}, {"referenceID": 30, "context": "Many other works have shown strong positive results on instances satisfying beyond worst case natural notions of stability on problems ranging from clustering to data privacy to social networks to topic modeling [2, 3, 22, 23, 26, 27, 31].", "startOffset": 212, "endOffset": 238}, {"referenceID": 10, "context": "We consider two notions of stability, perturbation resilience introduced by Bilu & Linial [11] and approximation stability introduced by Balcan et al.", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "In Section 7, we study center-based objectives [9], a more general class of clustering functions which includes objective functions such as k-center, k-median, and k-means.", "startOffset": 47, "endOffset": 50}, {"referenceID": 14, "context": "As an example, while there is a simple 2-approximation for symmetric k-center, it is NP-hard to approximate asymmetric k-center to a factor better than log\u2217(n) [15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 9, "context": "A reduction from Perfect Dominating Set (Dominating Set with the additional constraint that for all dominating sets of size \u2264 k, each vertex is hit by exactly one dominator) to the problem of clustering under (2\u2212 )-center proximity was shown in [10] (\u03b1-center proximity is the property that for all p \u2208 Ci and j 6= i, \u03b1d(ci, p) < d(cj , p), and it follows from \u03b1-perturbation resilience).", "startOffset": 245, "endOffset": 249}, {"referenceID": 8, "context": "We build on the algorithm and analysis originally introduced by Balcan and Liang [9] for finding the optimal clustering for (1+ \u221a 2)-perturbation resilient instances of any center-based objective.", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": "To move beyond the 1 + \u221a 2-perturbation resilience required by Balcan and Liang [9], we first note that while their analysis relied on the two aforementioned properties, namely that for all p \u2208 Ci, q \u2208 Cj , and j 6= i, 1) d(ci, p) < d(p, q) and 2) d(ci, p) < d(ci, q), it also crucially exploited a third property that a fully formed cluster and a partially formed cluster are never merged by the procedure.", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "However, the bottleneck of Balcan and Liang [9] analysis is in satisfying properties 2 and 3.", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "[4], shows that property 1 transfers easily to the case of 2-perturbation resilience.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "This property follows directly from \u03b1-perturbation resilience [4].", "startOffset": 62, "endOffset": 65}], "year": 2017, "abstractText": "The k-center problem is a canonical and long-studied facility location and clustering problem with many applications in both its symmetric and asymmetric forms. Both versions of the problem have tight approximation factors on worst case instances: a 2-approximation for symmetric k-center and a log\u2217(k)approximation for the asymmetric version. Therefore to improve on these ratios, one must go beyond the worst case. In this work, we take this approach and provide strong positive results both for the asymmetric and symmetric k-center problems under two very natural input stability (promise) conditions. We consider both the \u03b1-perturbation resilience notion of Bilu and Linial [11], which states that the optimal solution does not change under any \u03b1-factor perturbation to the input distances, and the (\u03b1, )approximation stability notion of Balcan et al. [6], which states that any \u03b1-approximation to the cost of the optimal solution should be -close in the solution space (i.e., the partitioning) to the optimal solution. We show that by merely assuming 3-perturbation resilience or (2, 0)-approximation stability, the exact solution for the asymmetric k-center problem can be found in polynomial time. To our knowledge, this is the first problem that is hard to approximate to any constant factor in the worst case, yet can be optimally solved in polynomial time under perturbation resilience for a constant value of \u03b1. In the case of 2approximation stability, we prove our result is tight by showing k-center under (2 \u2212 )-approximation stability is hard unless NP = RP . For the case of symmetric k-center, we give an efficient algorithm to cluster 2-perturbation resilient instances. Our results illustrate a surprising relation between symmetric and asymmetric k-center instances under these stability conditions. Unlike approximation ratio, for which symmetric k-center is easily solved to a factor of 2 but asymmetric k-center cannot be approximated to any constant factor, both symmetric and asymmetric k-center can be solved optimally under resilience to small constant-factor perturbations. \u2217Authors\u2019 addresses: {ninamf,nhaghtal,crwhite}@cs.cmu.edu. ar X iv :1 50 5. 03 92 4v 1 [ cs .D S] 1 4 M ay 2 01 5", "creator": "LaTeX with hyperref package"}}}