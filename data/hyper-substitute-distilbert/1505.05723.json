{"id": "1505.05723", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2015", "title": "On the relation between accuracy and fairness in binary classification", "abstract": "independent study highlighted the increase in accuracy - fairness tradeoff in binary classification. we argue improved comparison of non - discriminatory classifiers needs to account the overall rates of positive interpretation, while controversies about performance may be avoided, versus averages among discrimination supporting naive baselines surrounding the same analysis vary among previous rates of default predictions. reviewers provide practical recommendations for sound values of non - linear classifiers, and present my brief draft proposed experimental analysis of link between truth and non - discrimination.", "histories": [["v1", "Thu, 21 May 2015 13:20:06 GMT  (49kb)", "http://arxiv.org/abs/1505.05723v1", "Accepted for presentation to the 2nd workshop on Fairness, Accountability, and Transparency in Machine Learning (this http URL)"]], "COMMENTS": "Accepted for presentation to the 2nd workshop on Fairness, Accountability, and Transparency in Machine Learning (this http URL)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["indre zliobaite"], "accepted": false, "id": "1505.05723"}, "pdf": {"name": "1505.05723.pdf", "metadata": {"source": "META", "title": "On the relation between accuracy and fairness in binary classification", "authors": ["Indr\u0117 \u017dliobait\u0117"], "emails": ["INDRE.ZLIOBAITE@AALTO.FI"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 5.\n05 72\n3v 1\n[ cs\n.L G\n] 2\n1 M\nay 2\n01 5"}, {"heading": "1. Introduction", "text": "Discrimination-aware machine learning is an emerging research area, which studies how to make predictive models free from discrimination, when historical data, on which they are built, may be biased, incomplete, or even contain past discriminatory decisions. Research assumes that the protected grounds, against which discrimination is forbidden, are given by legislation. The goal for machine learning is to develop algorithmic techniques for incorporating those non-discriminatory constraints into predictive models.\nA number of studies in discrimination-aware machine learning and data mining (Pedreschi et al., 2009; Kamiran et al., 2010; Calders & Verwer, 2010) focus on achieving equal acceptance rates (proportions of positive decisions) for favored and protected groups of individuals in binary classification. Forcing acceptance rates to be equal without taking into account other characteristics of individuals can be seen as an affirmative action, which introduces positive discrimination promoting the protected community. This may be desired for legal and political reasons.\nWe revisit this popular scenario of discrimination aware\nThe 2nd Workshop on Fairness, Accountability, and Transparency in Machine Learning, Lille, France. Copyright 2015 by the author.\nmachine learning, and identify some pitfalls to avoid when comparing the performance of such classifiers, that is, a comparison may be misleading if the proportions of positive predictions of the classifiers are different. We provide methodological recommendations for sound comparison, and present a brief theoretical and empirical analysis of tradeoffs between accuracy and non-discrimination."}, {"heading": "2. Problem setting and assumptions", "text": "Given a dataset that contains discrimination the goal is to build a classifier that would be as accurate as possible, and obey non-discrimination constraints. For example, a model could decide upon granting a loan given demographic information and financial standing, and considering ethnicity of an applicant (native, foreign) as the protected ground. We assume that the values of the target variable (labels) in the historical dataset are objectively correct, e.g. whether the loan has been repaid or not. For discrimination to happen the target variable needs to be polar, that is, one outcome (accept) should be preferred over the other (reject).\nLet X denote a set of input variables (e.g. salary, assets), s denote the protected characteristic (e.g. ethnicity: native (w) or foreign (b)), and y denote the target variable (e.g. loan decision: accept (+) or reject (\u2212)). A classifier maps X to y, that is, y\u0302 = f(X). Even though s is not among the input variables, some variables in X may be correlated with s (e.g. social security payment history may be shorter for foreigners, because they have arrived recently), and, as a result, classifier f may capture the protected characteristics, and induce indirect discrimination in decision making.\nLet discrimination be measured as the difference in rates of acceptance: d = p(+|w) \u2212 p(+|b). Suppose that discrimination in the historical dataset is d0 = \u03b4, the desired discrimination in the classifier output is d\u22c6, the proportion of favored individuals in the data is p(w) = \u03b1, the prior probability of acceptance in the data is p(+) = \u03c00, and the rate of acceptance in the classifier output is pf(+) = \u03c0.\nMany classifiers produce probability scores (such as Naive Bayes or logistic regression). Typically, a probability score can be computed for non-probabilistic classifiers as well (such as kNN, SVM, decision trees). Individuals scoring\nabove a threshold, which by default is typically 0.5, will get a positive decision. Considering available resources a decision maker can choose a different threshold. Suppose that the objective is to keep discrimination at the desired level d\u22c6 (typically zero), and at the same time maximize the prediction accuracy. Effectively, by choosing the threshold a decision maker chooses the acceptance rate \u03c0."}, {"heading": "3. Accuracy and fairness", "text": "The performance of discrimination-aware classifiers is typically compared by plotting discrimination vs. accuracy. An attempt to remove discrimination can easily produce classifiers with different acceptance rates \u03c0 from those in the original dataset, especially when using off-the-shelve classifier implementations (e.g. WEKA1), which simply round the numerical probability scores without any constraints on the positive output rates.\nOur main message is that evaluation of nondiscriminatory classifiers must take into account rates of acceptance, otherwise classifier performance is not comparable, because changing the acceptance rate changes baseline accuracy and baseline discrimination.\nA small experiment with a benchmark dataset (Adult from UCI2 repository) illustrates the situation. The target variable describes whether a person has high income or low. The protected characteristic (gender) is not among the inputs. We randomly split the dataset into two halves: training and testing. We train a logistic regression (similar results have been obtained with Naive Bayes and decision tree J48) on a train set, output class probability scores for the test set, and vary the classification threshold from 0 to 1, which changes the acceptance rate \u03c0. We also plot the accuracy of a random classifier that does not use any inputs, but randomly decides upon the outcome given the probability of acceptance \u03c0. Figure 1 presents the results.\nFrom the left plot we see that the more extreme the acceptance rate is (either all reject, or all accept), the closer the performance of an intelligent classifier (logistic regres-\n1http://www.cs.waikato.ac.nz/ml/weka/ 2http://archive.ics.uci.edu/ml/\nsion) is to that of a random classifier, which assigns labels at random. Therefore, better observed accuracy does not necessarily mean better classification ability, if the acceptance rates of the two classifiers are different. In order to be able to compare such classifiers we could normalize the accuracy with respect to \u03c0. Therefore, we suggest using for comparison a normalized accuracy, such as Cohen\u2019s Kappa (Cohen, 1960), which indicates by how much a classifier in question is better than a random classifier:\n\u03ba = A\u2212R\n1\u2212R , (1)\nwhere A is the accuracy of the classifier in question, and R is the accuracy of a random classifier, in our case R = \u03c00\u03c0+(1\u2212 \u03c00)(1\u2212 \u03c0). Note, that \u03ba \u2208 [0, 1], where 1 means the ideal accuracy, and 0 indicates a random result3.\nIn the right plot we see how discrimination varies with different acceptance rates. There is no discrimination if everybody is accepted, or nobody is accepted, and the closer the acceptance rate \u03c0 gets to these extremes, the smaller is d. This is not due to a better fairness of the classifier, because the classifier is exactly the same, and its output is the same, just the classification threshold varies. We would like to assess the fairness of the classifier, therefore, similarly to the accuracy, we need to normalize the result with respect to \u03c0.\nWe propose to normalize d by the maximum possible dmax at each \u03c0. Discrimination would be at its maximum if a classifier ranks candidates in such a way that first everyone from the favored community is accepted, and only then candidates from the protected community start to be accepted4. In such a case the maximum discrimination is\ndmax = min\n(\n\u03c0 \u03b1 , 1\u2212 \u03c0 1\u2212 \u03b1\n)\n, (2)\nwhere \u03b1 is the proportion of the favored community individuals in the data, and \u03c0 is the acceptance rate.\nWe propose to normalize the discrimination measure by the maximum possible discrimination.\n\u03b4 = p(+|w)\u2212 p(+|b)\ndmax , (3)\nwhere dmax given in Eq. (2) is the maximum possible discrimination at a given acceptance rate. The maximum\n3One could consider other accuracy measures for imbalanced data, such as F-score. We prefer Cohen\u2019s Kappa, since F-score does not behave consistently at the extreme acceptance rates, and, therefore, is more difficult to interpret. F-score of a classifier that accepts everybody would be equal to \u03c00, which varies depending on the dataset, while Kappa always gives 1 in this case.\n4It can be compared to a (supposedly fictional) evacuation procedure from the Titanic. Passengers are put in a queue, where all the first class passengers have a priority over third class passengers. Then as many passengers are evacuated, as there are boats.\nvalue of \u03b4 is 1, which means the worst possible discrimination, where the favored community has a complete priority, \u03b4 = 0 means no discrimination where people from the favored and protected communities fully mix in the queue. \u03b4 can be negative, indicating a reverse discrimination.\nFigure 2 plots normalized accuracy \u03ba and normalized discrimination \u03b4 of the logistic regression in our experiment. Large part of discrimination appears to be flat and closely in line with the discrimination in the data. The results now make sense, since the classifier in the experiment does not have any mechanisms for discrimination removal. At the extreme ends, where everybody is accepted, or everybody is rejected, intuitively, there is no discrimination, and the normalized measure correctly shows no discrimination."}, {"heading": "4. Baselines and tradeoffs", "text": "It has been observed (Kamiran et al., 2010) that, assuming the labels in data are correct, discrimination removal comes at a cost \u2013 it reduces prediction accuracy. The authors have found given no constraints on the acceptance rates, that the maximum possible accuracy decreases linearly with reducing difference in rates of acceptance. We revisit the problem of accuracy-fairness tradeoff to see if the normalized measures would show similar relations.\nAn oracle is a fictional baseline classifier that has the maximum possible intelligence (as if it knows the true labels), and strives to satisfy non-discrimination constraints. A random classifier is the opposite, it does not use any intelligence. For each individual a random classifier makes a random prediction with the probability of acceptance \u03c0.\nThe accuracy of the oracle will be A0 = 1, kappa will be \u03ba0 = 1, the discrimination would be as in the data d0 and \u03b40. The random classifier defines the other baseline of performance with A = \u03c00\u03c0+ (1\u2212 \u03c00)(1\u2212 \u03c0), \u03ba = 0, and d = \u03b4 = 0. With \u03c0 = 0 (or \u03c0 = 1) the random classifier turns into the majority class classifier.\nSuppose, a decision maker aims at removing all discrimination such that d\u22c6 = 0 and \u03b4\u22c6 = 0. As suggested in (Kamiran et al., 2010), the oracle would either reduce the\nacceptance rate for the favored community (if \u03b1 \u2264 0.5), or increase the acceptance rate for the protected community (if \u03b1 > 0.5). The resulting decrease in classification accuracy would be linearly proportional to the discrimination in the data (A0 \u2212A) = min (\u03b1, (1 \u2212 \u03b1)) (d0 \u2212 d).\nWe find that if the rate of acceptance is to be fixed, that is \u03c0 = \u03c00, then the normalized accuracy of the oracle decreases linearly with decrease in normalized discrimination\n(\u03ba0 \u2212 \u03ba) = min\n(\n\u03b1 \u03c00 , 1\u2212 \u03b1 1\u2212 \u03c00\n)\n(\u03b40 \u2212 \u03b4). (4)\nIf the rate of acceptance does not need to be fixed, the optimal strategy is still the same \u2013 either to reduce acceptance for the favored community (\u201ddecrease males\u201d), or to increase acceptance for the protected community (\u201dincrease females\u201d), but the choice now depends not only on \u03b1, but also on \u03c00 and \u03b4\u22c6. We do not have a closed form solution at the moment, but Figure 3 presents simulated results of the oracle classifier on the benchmark dataset (Adult). \u201dChange both\u201d is the solution where the acceptance rate is kept the same as in the original data. These experiments show the maximum possible accuracy, given the discrimination constraints. We can see that when using the normalized measures for accuracy and discrimination the upper bounds remain linear."}, {"heading": "5. Interesting cases", "text": "We wrap up our study with an experiment to illustrate the difference between the raw and normalized measures when comparing non-discriminatory classifiers.\nThe experiment compares the performance of three classifiers (logistic regression, Naive Bayes and decision tree J48 from WEKA) trained using three different strategies: including the protected characteristic among classifier inputs, excluding the protected characteristic from classifier inputs, and excluding the protected characteristic from classifier inputs plus massaging the labels of the training data. Massaging is perhaps the simplest discrimination removal strategy, it has been introduced in (Kamiran & Calders, 2009). Training labels are converted from binary to numeric using a ranker function, we use a logistic regression\nfit on the same training data. A number of lowest ranked males who have a positive label are changed to negative, and the same number of highest ranked females, who have a negative label, are changed to positive such that the positive rate remains the same as in the original data, but the discrimination is zero. Then a classifier is learned on this modified training data. Testing data is not modified. Table 1 presents the results measured on the testing data.\nWe can make several interesting observations. First, all classifiers tend to output lower acceptance rates than that in the original data. At the same time, if the protected characteristic is used, the discrimination measure d may show a decrease in the nominal discrimination as compared to the original data, but the normalized discrimination \u03b4 by all three classifiers is even higher than in the data. Apparently, a classifier learned on discriminatory data without any protective measures amplifies discrimination.\nRemoving the protected characteristic (no s) indicates little improvement in discrimination. This is due to, so called, redlining effect. A number of features in the data are correlated with the protected characteristic, therefore, discrimination is still captured, and, in cases of logistic regression and decision tree, is still higher than in the original dataset.\nInterestingly, massaging strategy outputs higher acceptance rates than removing the protected characteristic. The acceptance rates of massaging are closer to the positive rates in the original data, and discrimination is lower, as expected. This suggests, that when discrimination is present in the training data, but usage of the protected characteristic is not allowed, classifiers tend to decrease the acceptance rate, which may show better nominal discrimination figures, but the real underlying discrimination (measured by normalized \u03b4) remains.\nFinally, Figure 4 presents normalized accuracies and discriminations at different acceptance rates. Overall we can see that massaging does remove some of discrimination, but at many acceptance rates the removal is not very precise, and sometimes even overshoots introducing a reverse\ndiscrimination. This calls for a revision of the massaging, and possibly other discrimination removal techniques, taking into consideration possibility of different acceptance rates and normalized measures of discrimination."}, {"heading": "6. Conclusion", "text": "Evaluation of non-discriminatory classifiers needs to take into account positive output rates, otherwise the comparison may be misleading and conclusions about comparative performance may be invalid.\nWe have introduced a normalization factor for discrimination measure, considering the maximum possible discrimination at a given acceptance rate. The maximum discrimination is present when the protected individuals start to be accepted only after everybody from the favored community is accepted.\nAcceptance rates may be constrained by resources, and not freely available to choose for decision makes. If the acceptance rate in the data and in the classifier outputs is fixed, then classifiers are comparable in terms of A and d, otherwise they need to be compared in terms of \u03ba and \u03b4."}], "references": [{"title": "Three naive bayes approaches for discrimination-free classification", "author": ["Calders", "Toon", "Verwer", "Sicco"], "venue": "Data Min. Knowl. Discov.,", "citeRegEx": "Calders et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Calders et al\\.", "year": 2010}, {"title": "A coefficient of agreement for nominal scales", "author": ["Cohen", "Jacob"], "venue": "Educational and Psychological Measurement,", "citeRegEx": "Cohen and Jacob.,? \\Q1960\\E", "shortCiteRegEx": "Cohen and Jacob.", "year": 1960}, {"title": "Classification without discrimination", "author": ["Kamiran", "Faisal", "Calders", "Toon"], "venue": "In Proc. of the 2nd IC4 conf. on Computer, Control and Communication,", "citeRegEx": "Kamiran et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kamiran et al\\.", "year": 2009}, {"title": "Discrimination aware decision tree learning", "author": ["Kamiran", "Faisal", "Calders", "Toon", "Pechenizkiy", "Mykola"], "venue": "In Proc. of the 2010 IEEE International Conference on Data Mining,", "citeRegEx": "Kamiran et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kamiran et al\\.", "year": 2010}, {"title": "Measuring discrimination in socially-sensitive decision records", "author": ["Pedreschi", "Dino", "Ruggieri", "Salvatore", "Turini", "Franco"], "venue": "In Proc. of the SIAM Int. Conf. on Data Mining,", "citeRegEx": "Pedreschi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pedreschi et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "A number of studies in discrimination-aware machine learning and data mining (Pedreschi et al., 2009; Kamiran et al., 2010; Calders & Verwer, 2010) focus on achieving equal acceptance rates (proportions of positive decisions) for favored and protected groups of individuals in binary classification.", "startOffset": 77, "endOffset": 147}, {"referenceID": 3, "context": "A number of studies in discrimination-aware machine learning and data mining (Pedreschi et al., 2009; Kamiran et al., 2010; Calders & Verwer, 2010) focus on achieving equal acceptance rates (proportions of positive decisions) for favored and protected groups of individuals in binary classification.", "startOffset": 77, "endOffset": 147}, {"referenceID": 3, "context": "It has been observed (Kamiran et al., 2010) that, assuming the labels in data are correct, discrimination removal comes at a cost \u2013 it reduces prediction accuracy.", "startOffset": 21, "endOffset": 43}, {"referenceID": 3, "context": "As suggested in (Kamiran et al., 2010), the oracle would either reduce the 0 0.", "startOffset": 16, "endOffset": 38}], "year": 2015, "abstractText": "Our study revisits the problem of accuracyfairness tradeoff in binary classification. We argue that comparison of non-discriminatory classifiers needs to account for different rates of positive predictions, otherwise conclusions about performance may be misleading, because accuracy and discrimination of naive baselines on the same dataset vary with different rates of positive predictions. We provide methodological recommendations for sound comparison of nondiscriminatory classifiers, and present a brief theoretical and empirical analysis of tradeoffs between accuracy and non-discrimination.", "creator": "LaTeX with hyperref package"}}}