{"id": "1505.07428", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2015", "title": "Training a Convolutional Neural Network for Appearance-Invariant Place Recognition", "abstract": "place - another one one the most researched uses exploring computer vision, enabling humans voiced a key part in mobile robotics and autonomous space applications for performing loop closure constrained visual character search. globally, the difficulty of recognizing random revisited typically increases with this impairment caused, especially often, by directional or illumination variations, basically hinders successful long - term application of such representations in fragmented environments. in this paper we develop a locality mapping code ( cnn ), using a the first time with the purpose of recognizing revisited locations under severe geographic scenarios, which maps images to large dual dimensional layer where euclidean distances reveal place dissimilarity. in applications of such network to learn the desired invariances, organizers communicate it with triplets / images filtered from clusters which present an central element in visual appearance. the triplets are selected in such way only two samples project from the right location and the missing one approximately taken from a focusing sensor. astronomers guide these system through extensive reflection, where we study relative performance than state - of - art calculations in a number separated geometric models.", "histories": [["v1", "Wed, 27 May 2015 18:21:54 GMT  (4919kb,D)", "http://arxiv.org/abs/1505.07428v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["ruben gomez-ojeda", "manuel lopez-antequera", "nicolai petkov", "javier gonzalez-jimenez"], "accepted": false, "id": "1505.07428"}, "pdf": {"name": "1505.07428.pdf", "metadata": {"source": "CRF", "title": "Training a Convolutional Neural Network for Appearance-Invariant Place Recognition", "authors": ["Ruben Gomez-Ojeda", "Manuel Lopez-Antequera", "Nicolai Petkov", "Javier Gonzalez-Jimenez"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "The process of identifying images that belong to the same location, usually known as place recognition, is still an open problem in computer vision. Place recognition is a key part in mobile robotics and autonomous driving applications, such as vision-based simultaneous localization and mapping (SLAM) systems, where revisiting a location introduces important information which can be employed in the tasks of localization [20] and loop closure [15]. It also can be applied in augmented reality applications, where the user obtains information about important places, monuments or texts from a single image taken with a smartphone camera. The difficulties induced by changes in the scenario,\nviewpoint, illumination or weather conditions makes place recognition a much more difficult task than one may intuitively think (see Figure 1). Traditionally, place recognition has focused on scenarios without major appearance changes. In that context, most methods employ bags of visual words inspired by [24] and [17]. Bag-of-words (BoW) approaches have proven to work quickly and effectively in static scenes, but they have several drawbacks. They usually rely on traditional keypoint descriptors, such as SIFT [11], SURF [1], or BRIEF [3], which describe the local appearance of individual patches, limiting their descriptive power with respect to whole image methods, as observed by [13]. Their performance in challenging environments strongly depends on the invariance of those descriptors to perceptual changes. Convolutional neural networks (CNNs) are gaining importance in most classification tasks [9]. When used as generic feature generators, they often outperform the state-of-art algorithms even for tasks different to classification [19]. However, their use in place recognition is limited to the exploitation of generic features extracted from the internal layers of pre-trained CNNs [4][25].\nIn this paper, we propose a novel approach to place\nar X\niv :1\n50 5.\n07 42\n8v 1\n[ cs\n.C V\n] 2\n7 M\nay 2\nrecognition capable of detecting revisited places under extreme changes in weather, illumination, or external conditions. In contrast to previous algorithms which rely on visual descriptors, our algorithm works with the complete image, reducing unnecessary errors induced by posterior feature matching processes by providing a better estimate of place similarity. For that purpose, we have trained a CNN for the task of recognizing revisited places. To the best of our knowledge, this is the first CNN specifically trained to perform place recognition as opposed to using generic features extracted from networks trained for other tasks. We demonstrate that place recognition can be better resolved by discriminatively training a network for such a problem, since visual cues that are relevant for object classification may not be optimal for place recognition. Moreover, we claim that place recognition can be performed with a smaller network than those employed for object recognition. We contribute to the state of the art with a CNN:\n\u25e6 Capable of recognizing revisited places under challenging appearance changes of the scene, including seasonal, time of day and outdoor/indoor changes.\n\u25e6 Suitable for any long term, real time place recognition tasks which are often necessary in mobile robotics and autonomous navigation.\nWe demonstrate these claims with extensive experimentation in several challenging datasets, where we compare our proposal with two state-of-art algorithms: DBoW2 [14], and a generic network as in [18]. Experiments show the better performance of our method, which recognizes previously visited locations under severe appearance changes with a higher rate of success than the state-of-art algorithms, with an inferior computational burden than previous CNNbased methods on datasets where appearance changes are severe."}, {"heading": "2. Related Work", "text": "As mentioned above, visual place recognition has been object of research under the field of SLAM, often as a key part of the localization and loop closing modules. One of the first SLAM techniques which introduced BoW in this context was FAB-MAP [5], where a probabilistic approach to place recognition based on the local appearance of each location was proposed. They also deal with perceptual aliasing in the environment by introducing a generative model which implements some logic reasoning to discard false positives caused by this phenomena. However, the use of SURF features and the employment of the generative model increases the computational burden. This was tackled in [6] with DBoW2, where for the first time they introduced the use bags of binary words obtained from BRIEF descriptors, reducing in more than an order of magnitude\nthe time employed in the feature extraction process. The use of BRIEF, which is not rotation or scale invariant, limits the recognition task to scenes taken from the same viewpoint in planar trajectories. An improved version of this algorithm has been recently published in [14], where the authors build a urban dictionary based on ORB [21] which yields a better performance in popular datasets.\nA common problem to previous techniques is their poor behavior in place recognition under different illumination conditions and poorly textured environments, and also their limited invariance to scale and viewpoint. In [10], the authors deal with that by building a vocabulary tree that employs straight lines in combination with the MSLD descriptor [27], which increases the robustness against changes in weather conditions. However, the evaluation sequences do not include strong perceptual changes, thus the system may not be suitable to long-term operations in changing environments. This problem was tackled by Neubert et al. in [16], where they propose a place recognition algorithm capable of working across seasons. They argue that seasonal changes in the scene are predictable, and propose a superpixel-based algorithm (SP-APC) which is able to predict those changes and then recognize the scene, with a prediction process based on a dictionary that learns from training data how the appearance of the scene changes over the year. On the other hand, the algorithm is only tested with the Nordland dataset [26], which shows extreme seasonal changes, and hence it will not predict gradual changes in the environment. A different strategy works on local sequences instead of estimating the best single location, with the proposal of Milford and Wyeth as one of the most relevant contributions [13]. They propose SeqSLAM, a post-processing technique that recognizes sequences of locations previously visited, under challenging perceptual changes. Their approach estimates the best match by taking into account not only the single location, but also imposing coherence with the surrounding sequence. Under this assumption, they obtain a good performance by only applying a local contrast enhancement to the input images (downsampled from the original datasets), and then comparing the normalized images by processing the sum of absolute differences (SAD) between them. However, this procedure has several drawbacks. It only works with local and consistent sequences, which makes it impractical for applications that work with isolated images. It also may fail with big changes of viewpoint and rotation, and also suffers image aliasing since its viewpoint invariance is only due to extreme downscaling of the input images. Recently, another group of techniques has irrupted with promising results, motivated by the outstanding performance achieved by CNNs as generic feature generators in several classification tasks [19]. In this context, a recent work is [25], where the authors employ a pre-trained network named OverFeat [23], which was the\nwinner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 [22]. They study the use of the intermediate representations learned by the CNN as image features valuable for place recognition even under challenging appearance changes, with promising results."}, {"heading": "3. Methodology", "text": "To solve the task of detecting if an image belongs to a previously visited place, we propose to train a Convolutional Neural Network to embed images in a low dimensional space where Euclidean distance represents location dissimilarity. Our solution is inspired by works in content-based image retrieval, however, our network is trained to produce a feature vector invariant to drastic appearance changes in the scene such as seasonal changes. To achieve this, we train the network using labeled datasets which present the same locations under different illumination, point of view or weather conditions. We apply a training technique similar to [28], where the network presented with triplets of images, formed by a query image xi, an image from the same location, xj , and an image of a different location, xk. In the following we describe the architecture and the training process of the proposed network."}, {"heading": "3.1. Architecture of the CNN", "text": "In view of the difficulties in training a convolutional neural network from scratch using a relatively small specialized dataset, we take the approach of modifying a pre-trained network. In particular, we resort to the reference CaffeNet network [8], which mirrors the architecture of Krizhevsky et al. [9], from which we only keep the first four convolutional layers, replacing the rest with a single fully connected layer which is our descriptor output (see Figure 2). Since we discard all the fully connected layers, we are not constrained to the original input size of 227\u00d7 227 pixels and instead work with a smaller input of 160\u00d7 120."}, {"heading": "3.2. Description of the Cost Function", "text": "In a nutshell, the network maps aM\u00d7N\u00d7C input image to a descriptor vector of length D, which corresponds to the\nactivations of the output layer of the CNN, i.e.:\nh : RM\u00d7N\u00d7C 7\u2212\u2192 RD\nx 7\u2212\u2192 h(x) (1)\nbeing h(x) the descriptor of the image x, whose Euclidean distances to other descriptors must be representative of location dissimilarity. In order to achieve this behavior, the network parameters \u03c9\u2217 are obtained by minimizing the following objective function\n\u03c9\u2217 = argmin \u03c9\n{ L+ \u03bb \u2016\u03c9\u201622 } (2)\nwhere the second term represents a regularization over the parameters of the network \u03c9, and the first term L is the sum of the cost functions over all the triplets, that can be expressed as\nL = \u2211\n(xi,xj ,xk)\u2208\u03c4\nC(xi, xj , xk) (3)\nwith C being the cost function for each triplet of images. The cost function employed is similar to that in [28], and can be expressed as:\nC(xi, xj , xk) = max { 0, 1\u2212 \u2016h(xi)\u2212 h(xk)\u20162\n\u03b2 + \u2016h(xi)\u2212 h(xj)\u20162\n} (4)\nThis cost function is satisfiable when the distance of the dissimilar pair is larger than the distance of the similar pair by at least a margin \u03b2, producing zero cost. This means that dissimilar descriptors will not continue to be separated indefinitely in the descriptor space during training. On the contrary, triplets not satisfying this condition will produce costs that the training process will aim to reduce by updating the weights of the CNN accordingly."}, {"heading": "3.3. Training the CNN", "text": "To achieve the desired invariances in the representation produced by the network, triplets must be chosen as to provide relevant visual cues (see Figure 3 for an example). We train the network using a mixture of triplets from several datasets, which are detailed in the following sections, to\nimprove invariance to lighting, weather and point of view changes. The network is trained using the Caffe library [8], modified to include the previously described cost function.\nAs previously explained, the weights of the four convolutional layers are fine-tuned from the CaffeNet reference network, an implementation of [9], whereas the final fully connected layer is new. We scale the learning rate of the pre-trained layers by a factor of 1/1000 and fix the global learning rate at 0.001. The margin \u03b2 is set to 1 and the regularization constant \u03bb to 0.0005. We train for a 40.000 iterations, for a total of 1.2 million triplets, using portions fo the KITTI, Nordland, and Alderley datasets, which are described in the following sections."}, {"heading": "3.3.1 KITTI Dataset", "text": "The odometry benchmark from the KITTI dataset [7] is comprised of 11 training sequences with accurate ground truth of the trajectory, and 10 test sequences without ground truth for evaluation. Both the training and the test sequences are stereo frames extracted from urban environments in daylight conditions. We select triplets in order to increase the robustness of the network to changes in viewpoint by choosing the similar pair in a wide variety of relative poses. We also check that the different pairs do not belong to the same place by employing the ground truth location (since loop closures exist in the sequences) Figure 3 depicts a triplet extracted from the KITTI dataset."}, {"heading": "3.3.2 Alderley Dataset", "text": "We have also trained the network with the Alderley dataset [13], which contains severe changes in illumination and weather conditions. This dataset is formed by two sequences of 8 km along the suburb of Alderley in Brisbane (Australia). The first one was recorded during a clear morning, while the second one was collected in a stormy night with low visibility (see Figure 4). In order to achieve robustness to the aforementioned changes, during training we provide the network with challenging triplets that combine images from both sequences (we have used the first 10k frames from the day sequence and their matches from the night sequence for the training, while reserving the rest for experimentation)."}, {"heading": "3.3.3 Nordland Dataset", "text": "The Nordland dataset [26], extracted from the TV documentary \u201cNordlandsbanen - Minutt for Minutt\u201d produced by the Norwegian Broadcasting Corporation NRK consists of a 728 km long train journey connecting the cities of Trondheim and Bod\u00f8 in Norway. The sequence was recorded once in each season, and hence it contains challenging appearance changes, as Figure 1 shows. Additionally, it provides different weather conditions due to the large length of the dataset (the sequences are 10 hour long approximately). We generate triplets by providing two images from the same place in different seasons, and an image from another location in any season (we check that frames are actually from different places using the included GPS ground truth)."}, {"heading": "4. Experimental Evaluation", "text": "In order to validate the proposed network, we perform a series of experiments where we compare the behavior of\nour system with two state-of-art techniques in place recognition: DBoW2 [14], and a feature vector extracted from an internal layer of a neural network trained for object classification as in [25]. The actual implementations used are the official distribution of ORB-SLAM [14], and the CaffeNet [8] implementation of [9], which we simply name as CaffeNet in this work. The resolutions of the input images are 160 \u00d7 120 in our proposal, 227 \u00d7 227 in CaffeNet , and the native resolution of each dataset in DBoW2. In the following, we first describe the methodology employed for the comparison, then we present a number of experiments with datasets from several environments, under different appearance changes. Finally, we also compare the computational cost of the algorithms and their feasibility for place recognition tasks, such as loop closure modules in visual SLAM algorithms."}, {"heading": "4.1. On Comparing Confusion Matrices", "text": "The key element of a place recognition system is the estimation of the similarity between the compared images. For that purpose, we calculate a descriptor h(xi) for each input image xi, and then we estimate the similarity with other images by comparing the Euclidean distance from their descriptors. A common measurement widely employed in place recognition collects each distance (or score) in a confusion matrix, where the rows and the columns express the database and the query sequence, respectively, that is M(i, j) = \u2016h(xi)\u2212 h(xj)\u20162. In our case, a normalized confusion matrixM\u2217 can be defined as follows:\nM\u2217(i, j) = M(i, j) max{M(i, j)}\n(5)\nwhose terms include the normalized Euclidean distance between the descriptors associated to the i and j images from each sequence. For the methods with which we compare our proposal, the confusion matrices include the proposed normalized scores for each methodology, which are:\n\u25e6 DBoW2 [14]: the proposed score is already normalized, but their approach associates high scores to similar images, thus we estimate the complementary matrix before the comparison.\n\u25e6 CaffeNet [25]: we extract the convolutional layers outputs conv4, which present the best results for the tested datasets, and compare them using Euclidean distance as they propose.\nPlace recognition methods for loop closure generally employ post-processing techniques to find good matches which actually represent the same location in the confusion matrix, usually by looking for sequences of similar frames [12] [18]. Any method that generates a confusion matrix can benefit from such post-processing techniques, including\nours. For this reason, we perform our experimental comparisons on the \u201craw\u201d confusion matrix. A problem of using confusion matrices to compare the performance of different methods, is that it is quite difficult to establish a indicator of the quality of a confusion matrix, since there is no ground truth measurement of the place similarity between any two images. To overcome this issue, we perform a comparison based on synchronized sequences which do not present any loop closures, since in those cases the ground truth pair is placed on the diagonal of the confusion matrix. In order to generate a quality measurement of a confusion matrix, we start by only keeping its k smallest values. Then we plot the ratio of points that fall within the diagonal with respect to d, which is defined as the maximum distance to the diagonal to consider a point as an inlier (see Figure 7)."}, {"heading": "4.2. KITTI Dataset", "text": "First, we compare the performance of the state-of-art algorithms with our proposal by processing the test sequences from the KITTI dataset [7], which has a resolution of 1241 \u00d7 376. Figure 5 depicts the confusion matrices obtained with the sequence KITTI-11 by comparing images from the left and right cameras, where we can observe a good performance of all methods. We also notice that both DBoW2 and CaffeNet present a thin diagonal, and they do not show any good matches outside the diagonal. In contrast, the confusion matrix obtained with our approach presents a thicker diagonal, and also zones with low values which correspond to parts of the sequence where the car is either stopped or circulating with low speed. It implies that our approach is more robust to changes in point of view, and hence, is a more versatile option for place recognition tasks which may not require the camera to be in the exact same location. Nevertheless, it is quite difficult to extract quantitative conclusions with the observation of these charts. Instead, Figure 6 depicts the 10 best matches for each input image. While both CaffeNet and our approach exhibit a good performance, with low dispersion around the diagonal, DBoW2 presents a considerable amount of outliers during the whole sequence. This is quantified in Figure 7, where we can observe that both CNN-based methods yield better results than DBoW2, while we observe a slightly superior performance of CaffeNet against our approach. However, it is worth considering that the features extracted from CaffeNet are 64k-dimensional, whereas ours are much smaller, of 128 elements. This is of importance for sustainable long running place recognition as will be discussed in Section 4.6."}, {"heading": "4.3. Ma\u0301laga Urban Dataset", "text": "We also evaluate the performance of the techniques with the Ma\u0301laga Urban Dataset [2], which contains frames obtained from a stereo camera, with a resolution of 1024\u00d7768,\nand data acquired from five laser scanners during a 37 km sequence in Ma\u0301laga (Spain) with cloudy weather and direct sunlight in several parts of the sequence. As can be observed in Figure 8, the urban structure presented by this dataset is quite different than the one in the KITTI sequences, which makes it a challenging environment since none of the methods have been trained with this dataset. Figure 9 depicts the performance of the three compared methods when tested on the Ma\u0301laga-10 (we employ the left sequence as database and the right one as query). While the CNN-based methods perform well, with a small superiority of CaffeNet, DBoW2 has a poor behavior, with a high ratio of outliers. This proves that both CNN-based approaches are capable of recognizing images from multiple environments (even when they have not been trained with similar\nimages), which makes them an interesting choice for lifelong applications, while DBoW2 approach lacks this capability."}, {"heading": "4.4. Nordland Dataset", "text": "As mentioned above, the Nordland Dataset [26] includes sequences with 1920\u00d7 1080 resolution from the same perspective during the four seasons of the year, which leads to severe changes in the appearance of the environment. For these experiments, we have employed the last hour of the dataset, which was not used for training, and removed the segments which include either tunnels or stations. Figure 10 depicts the performance curves of the three approaches by comparing the most challenging sequence pair, summer and winter (other seasonal combinations yield similar results).\nWe observe the better performance of our proposal against CaffeNet , which presents considerably less inliers than our approach for all the diagonal widths, for both k = 5 and k = 10, which is logical since neither CaffeNet or DBoW2 have been trained with the purpose of being robust to those appearance changes."}, {"heading": "4.5. Alderley Dataset", "text": "We also have tested the robustness to challenging changes in weather and lightning conditions of the three\nmethods, by processing the last 5k frames from the day sequence and their matches from the night sequence of the Alderley dataset (which has a resolution of 640\u00d7260). Figure 11 shows the outperformance of our proposal against CaffeNet and DBoW2, with a better ratio of inliers against diagonal width in all cases. However, it can be noticed that a low ratio is obtained by all three approaches, since it is a highly challenging dataset. Hence, the use of a postprocessing technique based on sequentiality would be unavoidable to obtain a system with a reasonable performance in similar scenarios."}, {"heading": "4.6. Performance", "text": "Finally, we examine the computational performance in several aspects, which are presented in Table 1. Our tests run on an Intel Core i7-3770, while our GPU tests also rely on an NVidia GeForce GTX 790. First, we measure the time required to process a single image. In both CNN-based methods, the value includes loading the image and performing a forward pass to obtain the feature vector. In the case of DBoW2 [15], we measure the time required to compute the bag-of-words histogram. Since the input image resolu-\ntion for DBoW2 is variable depending on the dataset, we have included the minimum and maximum average times from all the sequences. The results indicate that DBoW2 is less demanding than both CNN-based methods and that ours is three times faster than using the reference CaffeNet network. We then measure the size of the descriptor, which is relevant since the computational cost of calculating the confusion matrix (which is required for any loop closure system) increases with it. The length of the word histogram of DBoW2 is variable in the official implementation, and can be as long as the dictionary size (32k elements). In our experiments, the length of the histogram varied from 200\nto 500 elements on average, increasing in datasets where it performs well. On this matter, our method clearly outperforms both CaffeNet and DBoW2 with a smaller, fixed length descriptor of 128 elements."}, {"heading": "5. Conclusions", "text": "We have trained a convolutional neural network to perform place recognition under heavy appearance changes due to weather, seasons and perspective. The network embeds images in a 128-dimensional space where samples from similar locations are separated by small Euclidean distances. The network was trained using triplets of images from datasets where weather, lighting and point of view changes were present, in order to allow the network to learn invariances to these changes. The proposed network outperforms the state-of-art methods for place recognition in several challenging datasets, providing superior robustness to viewpoint and weather conditions changes. The small size of the resulting vector makes our system suitable for applications where long-term operation is required."}], "references": [{"title": "SURF: Speeded Up Robust Features BT - Computer VisionECCV", "author": ["H. Bay", "T. Tuytelaars", "L. Gool"], "venue": "Computer VisionECCV 2006,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "The M\u00e1laga urban dataset: High-rate stereo and LiDAR in a realistic urban scenario", "author": ["J.-L. Blanco-Claraco", "F.-A. Moreno-Due\u00f1as", "J. Gonz\u00e1lez-Jim\u00e9nez"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "BRIEF: Binary robust independent elementary features. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture", "author": ["M. Calonder", "V. Lepetit", "C. Strecha", "P. Fua"], "venue": "Notes in Bioinformatics),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Convolutional Neural Network-based Place Recognition", "author": ["Z. Chen", "O. Lam", "A. Jacobson", "M. Milford"], "venue": "arXiv preprint arXiv:1411.1509,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance", "author": ["M. Cummins", "P. Newman"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Bags of binary words for fast place recognition in image sequences. Robotics", "author": ["D. Galvez-Lopez", "J.D. Tardos"], "venue": "IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Are we ready for autonomous driving? The KITTI vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances In Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Outdoor place recognition in urban environments using straight lines", "author": ["J.H. Lee", "S. Lee", "G. Zhang", "J. Lim", "W.K. Chung", "I.H. Suh"], "venue": "Robotics and Automation (ICRA),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Distinctive image features from scaleinvariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Vision-based place recognition: how low can you go", "author": ["M. Milford"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "SeqSLAM: Visual routebased navigation for sunny summer days and stormy winter nights", "author": ["M.J. Milford", "G.F. Wyeth"], "venue": "Proceedings - IEEE International Conference on Robotics and Automation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "ORB- SLAM: a Versatile and Accurate Monocular SLAM System", "author": ["R. Mur-Artal", "J.M.M. Montiel", "J.D. Tardos"], "venue": "arXiv preprint arXiv:1502.00956,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Fast Relocalisation and Loop Closing in Keyframe-Based SLAM", "author": ["R. Mur-Artal", "J.D. Tard\u00f3s"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Superpixelbased appearance change prediction for long-term navigation across seasons", "author": ["P. Neubert", "N. Sunderhauf", "P. Protzel"], "venue": "European Conference on Mobile Robots, ECMR 2013 - Conference Proceedings,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Scalable recognition with a vocabulary tree", "author": ["D. Nist\u00e9r", "H. Stew\u00e9nius"], "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "All-environment visual place recognition with smart", "author": ["E. Pepperell", "P.I. Corke", "M.J. Milford"], "venue": "Robotics and Automation (ICRA),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Appearancebased indoor localization: A comparison of patch descriptor performance", "author": ["J. Rivera-Rubio", "I. Alexiou", "A.A. Bharath"], "venue": "Pattern Recognition Letters,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "ORB: an efficient alternative to SIFT or SURF", "author": ["E. Rublee", "V. Rabaud", "K. Konolige", "G. Bradski"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "arXiv preprint arXiv:1409.0575,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Video Google: a text retrieval approach to object matching in videos", "author": ["J. Sivic", "a. Zisserman"], "venue": "Proceedings Ninth IEEE International Conference on Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "On the Performance of ConvNet Features for Place Recognition", "author": ["N. Sunderhauf", "F. Dayoub", "S. Sareh", "U. Ben", "M. Michael"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Are we there yet? challenging seqslam on a 3000 km journey across all four seasons", "author": ["N. S\u00fcnderhauf", "P. Neubert", "P. Protzel"], "venue": "Proc. of Workshop on Long-Term Autonomy, IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "MSLD: A robust descriptor for line matching", "author": ["Z. Wang", "F. Wu", "Z. Hu"], "venue": "Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Learning descriptors for object recognition and 3d pose estimation", "author": ["P. Wohlhart", "V. Lepetit"], "venue": "arXiv preprint arXiv:1502.05908,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Place recognition is a key part in mobile robotics and autonomous driving applications, such as vision-based simultaneous localization and mapping (SLAM) systems, where revisiting a location introduces important information which can be employed in the tasks of localization [20] and loop closure [15].", "startOffset": 275, "endOffset": 279}, {"referenceID": 14, "context": "Place recognition is a key part in mobile robotics and autonomous driving applications, such as vision-based simultaneous localization and mapping (SLAM) systems, where revisiting a location introduces important information which can be employed in the tasks of localization [20] and loop closure [15].", "startOffset": 297, "endOffset": 301}, {"referenceID": 25, "context": "Frames extracted from the Nordland dataset [26] that belong to the same place in winter, spring, summer and fall.", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "In that context, most methods employ bags of visual words inspired by [24] and [17].", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "In that context, most methods employ bags of visual words inspired by [24] and [17].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "They usually rely on traditional keypoint descriptors, such as SIFT [11], SURF [1], or BRIEF [3], which describe the local appearance of individual patches, limiting their descriptive power with respect to whole image methods, as observed by [13].", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "They usually rely on traditional keypoint descriptors, such as SIFT [11], SURF [1], or BRIEF [3], which describe the local appearance of individual patches, limiting their descriptive power with respect to whole image methods, as observed by [13].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "They usually rely on traditional keypoint descriptors, such as SIFT [11], SURF [1], or BRIEF [3], which describe the local appearance of individual patches, limiting their descriptive power with respect to whole image methods, as observed by [13].", "startOffset": 93, "endOffset": 96}, {"referenceID": 12, "context": "They usually rely on traditional keypoint descriptors, such as SIFT [11], SURF [1], or BRIEF [3], which describe the local appearance of individual patches, limiting their descriptive power with respect to whole image methods, as observed by [13].", "startOffset": 242, "endOffset": 246}, {"referenceID": 8, "context": "Convolutional neural networks (CNNs) are gaining importance in most classification tasks [9].", "startOffset": 89, "endOffset": 92}, {"referenceID": 18, "context": "When used as generic feature generators, they often outperform the state-of-art algorithms even for tasks different to classification [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 3, "context": "However, their use in place recognition is limited to the exploitation of generic features extracted from the internal layers of pre-trained CNNs [4][25].", "startOffset": 146, "endOffset": 149}, {"referenceID": 24, "context": "However, their use in place recognition is limited to the exploitation of generic features extracted from the internal layers of pre-trained CNNs [4][25].", "startOffset": 149, "endOffset": 153}, {"referenceID": 13, "context": "We demonstrate these claims with extensive experimentation in several challenging datasets, where we compare our proposal with two state-of-art algorithms: DBoW2 [14], and a generic network as in [18].", "startOffset": 162, "endOffset": 166}, {"referenceID": 17, "context": "We demonstrate these claims with extensive experimentation in several challenging datasets, where we compare our proposal with two state-of-art algorithms: DBoW2 [14], and a generic network as in [18].", "startOffset": 196, "endOffset": 200}, {"referenceID": 4, "context": "One of the first SLAM techniques which introduced BoW in this context was FAB-MAP [5], where a probabilistic approach to place recognition based on the local appearance of each location was proposed.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "This was tackled in [6] with DBoW2, where for the first time they introduced the use bags of binary words obtained from BRIEF descriptors, reducing in more than an order of magnitude the time employed in the feature extraction process.", "startOffset": 20, "endOffset": 23}, {"referenceID": 13, "context": "An improved version of this algorithm has been recently published in [14], where the authors build a urban dictionary based on ORB [21] which yields a better performance in popular datasets.", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "An improved version of this algorithm has been recently published in [14], where the authors build a urban dictionary based on ORB [21] which yields a better performance in popular datasets.", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "In [10], the authors deal with that by building a vocabulary tree that employs straight lines in combination with the MSLD descriptor [27], which increases the robustness against changes in weather conditions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In [10], the authors deal with that by building a vocabulary tree that employs straight lines in combination with the MSLD descriptor [27], which increases the robustness against changes in weather conditions.", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "in [16], where they propose a place recognition algorithm capable of working across seasons.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "On the other hand, the algorithm is only tested with the Nordland dataset [26], which shows extreme seasonal changes, and hence it will not predict gradual changes in the environment.", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "A different strategy works on local sequences instead of estimating the best single location, with the proposal of Milford and Wyeth as one of the most relevant contributions [13].", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "Recently, another group of techniques has irrupted with promising results, motivated by the outstanding performance achieved by CNNs as generic feature generators in several classification tasks [19].", "startOffset": 195, "endOffset": 199}, {"referenceID": 24, "context": "In this context, a recent work is [25], where the authors employ a pre-trained network named OverFeat [23], which was the", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "In this context, a recent work is [25], where the authors employ a pre-trained network named OverFeat [23], which was the", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "N is a local contrast normalization operation acting across channels as applied in [9].", "startOffset": 83, "endOffset": 86}, {"referenceID": 21, "context": "winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 27, "context": "We apply a training technique similar to [28], where the network presented with triplets of images, formed by a query image xi, an image from the same location, xj , and an image of a different location, xk.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "In particular, we resort to the reference CaffeNet network [8], which mirrors the architecture of Krizhevsky et al.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "[9], from which we only keep the first four convolutional layers, replacing the rest with a single fully connected layer which is our descriptor output (see Figure 2).", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "The cost function employed is similar to that in [28], and can be expressed as:", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "Training triplet extracted from the KITTI dataset [7], where large viewpoint invariances are present.", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "The network is trained using the Caffe library [8], modified to include the previously described cost function.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "As previously explained, the weights of the four convolutional layers are fine-tuned from the CaffeNet reference network, an implementation of [9], whereas the final fully connected layer is new.", "startOffset": 143, "endOffset": 146}, {"referenceID": 6, "context": "The odometry benchmark from the KITTI dataset [7] is comprised of 11 training sequences with accurate ground truth of the trajectory, and 10 test sequences without ground truth for evaluation.", "startOffset": 46, "endOffset": 49}, {"referenceID": 12, "context": "We have also trained the network with the Alderley dataset [13], which contains severe changes in illumination and weather conditions.", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "The Nordland dataset [26], extracted from the TV documentary \u201cNordlandsbanen - Minutt for Minutt\u201d produced by the Norwegian Broadcasting Corporation NRK consists of a 728 km long train journey connecting the cities of Trondheim and Bod\u00f8 in Norway.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "Frames extracted from the Alderley dataset [13], where drastic illumination changes are present.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "our system with two state-of-art techniques in place recognition: DBoW2 [14], and a feature vector extracted from an internal layer of a neural network trained for object classification as in [25].", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "our system with two state-of-art techniques in place recognition: DBoW2 [14], and a feature vector extracted from an internal layer of a neural network trained for object classification as in [25].", "startOffset": 192, "endOffset": 196}, {"referenceID": 13, "context": "The actual implementations used are the official distribution of ORB-SLAM [14], and the CaffeNet [8] implementation of [9], which we simply name as CaffeNet in this work.", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "The actual implementations used are the official distribution of ORB-SLAM [14], and the CaffeNet [8] implementation of [9], which we simply name as CaffeNet in this work.", "startOffset": 97, "endOffset": 100}, {"referenceID": 8, "context": "The actual implementations used are the official distribution of ORB-SLAM [14], and the CaffeNet [8] implementation of [9], which we simply name as CaffeNet in this work.", "startOffset": 119, "endOffset": 122}, {"referenceID": 13, "context": "\u25e6 DBoW2 [14]: the proposed score is already normalized, but their approach associates high scores to similar images, thus we estimate the complementary matrix before the comparison.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "\u25e6 CaffeNet [25]: we extract the convolutional layers outputs conv4, which present the best results for the tested datasets, and compare them using Euclidean distance as they propose.", "startOffset": 11, "endOffset": 15}, {"referenceID": 11, "context": "Place recognition methods for loop closure generally employ post-processing techniques to find good matches which actually represent the same location in the confusion matrix, usually by looking for sequences of similar frames [12] [18].", "startOffset": 227, "endOffset": 231}, {"referenceID": 17, "context": "Place recognition methods for loop closure generally employ post-processing techniques to find good matches which actually represent the same location in the confusion matrix, usually by looking for sequences of similar frames [12] [18].", "startOffset": 232, "endOffset": 236}, {"referenceID": 6, "context": "First, we compare the performance of the state-of-art algorithms with our proposal by processing the test sequences from the KITTI dataset [7], which has a resolution of 1241 \u00d7 376.", "startOffset": 139, "endOffset": 142}, {"referenceID": 1, "context": "We also evaluate the performance of the techniques with the M\u00e1laga Urban Dataset [2], which contains frames obtained from a stereo camera, with a resolution of 1024\u00d7768,", "startOffset": 81, "endOffset": 84}, {"referenceID": 25, "context": "As mentioned above, the Nordland Dataset [26] includes sequences with 1920\u00d7 1080 resolution from the same perspective during the four seasons of the year, which leads to severe changes in the appearance of the environment.", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "Frame extracted from the M\u00e1laga Urban Dataset [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 14, "context": "In the case of DBoW2 [15], we measure the time required to compute the bag-of-words histogram.", "startOffset": 21, "endOffset": 25}], "year": 2015, "abstractText": "Place recognition is one of the most challenging problems in computer vision, and has become a key part in mobile robotics and autonomous driving applications for performing loop closure in visual SLAM systems. Moreover, the difficulty of recognizing a revisited location increases with appearance changes caused, for instance, by weather or illumination variations, which hinders the long-term application of such algorithms in real environments. In this paper we present a convolutional neural network (CNN), trained for the first time with the purpose of recognizing revisited locations under severe appearance changes, which maps images to a low dimensional space where Euclidean distances represent place dissimilarity. In order for the network to learn the desired invariances, we train it with triplets of images selected from datasets which present a challenging variability in visual appearance. The triplets are selected in such way that two samples are from the same location and the third one is taken from a different place. We validate our system through extensive experimentation, where we demonstrate better performance than state-of-art algorithms in a number of popular datasets.", "creator": "LaTeX with hyperref package"}}}