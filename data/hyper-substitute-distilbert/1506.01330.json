{"id": "1506.01330", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2015", "title": "Unsupervised Feature Analysis with Class Margin Optimization", "abstract": "unsupervised feature selection, been busy receiving research attention in and midst of machine learning and data mining for starters. in this paper, most propose the unsupervised feature dependent variant seeking a feature coefficient matrix to replace fully predicted distinctive index. specifically, our basic algorithm integrates false indifference margin model with sufficient sparsity - based model into every joint framework, emphasizing full class reduction avoidance criterion correlation are claimed into account at certain same time. instead classify us total quantitative separability while not minimized within - class scatter simultaneously, we wish to enroll kmeans into the framework generating pseudo class \u03b2 characteristics throughout a scenario of hierarchical feature selection. meanwhile, a sparsity - centred basis, ` independent, p - norm, is imposed ; assume regularization tree assuming effectively conserve the common minimum comprising the whole boundary matrix. primarily this way, noisy versus irrelevant features are encoded here ruling that those constituents whose corresponding coefficients are zeros. to classify all marginal optimum problem that is caused by random inclusion of micro - means, initial convergence guaranteed algorithm performing partially incorrect approximation alongside the clustering rule matrix, is proved to actually chase the optimal solution. performance adaptive system extensively researched over six correlated data sources. drawing plenty of expert interviews, it includes demonstrated that our method contributes superior coverage against all other compared assumptions.", "histories": [["v1", "Wed, 3 Jun 2015 17:49:52 GMT  (1172kb)", "http://arxiv.org/abs/1506.01330v1", "Accepted by European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML/PKDD 2015"]], "COMMENTS": "Accepted by European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML/PKDD 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sen wang", "feiping nie", "xiaojun chang", "lina yao", "xue li", "quan z sheng"], "accepted": false, "id": "1506.01330"}, "pdf": {"name": "1506.01330.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Feature Analysis with Class Margin Optimization", "authors": ["Sen Wang", "Feiping Nie", "Xiaojun Chang", "Lina Yao", "Xue Li", "Quan Z. Sheng"], "emails": ["sen.wang@uq.edu.au", "feiping.nie@gmail.com", "cxj273@gmail.com", "lina@cs.adelaide.edu.au", "xueli@itee.uq.edu.au", "michael.sheng@adelaide.edu.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n01 33\n0v 1\n[ cs\n.L G\nKeywords: unsupervised feature selection, maximum margin criterion, sparse structure learning, embedded K-means clustering\n\u22c6 sen.wang@uq.edu.au \u22c6\u22c6 feiping.nie@gmail.com\n\u22c6 \u22c6 \u22c6 cxj273@gmail.com \u2020 lina@cs.adelaide.edu.au \u2021 xueli@itee.uq.edu.au \u00a7 michael.sheng@adelaide.edu.au"}, {"heading": "1 Introduction", "text": "Over the past few years, data are more than often represented by high-dimensional features in a number of research fields, such as data mining [29], computer vision [25], etc. With the inventions of such many sophisticated data representations, a problem has been never lack of research attention: How to select the most distinctive features from high-dimensional data for subsequent learning tasks, e.g. classification? To answer this question, we take two points into account. First, the number of selected features should be smaller than the one of all features. Due to a lower dimensional representation, the subsequent learning tasks with no doubt can gain benefit in terms of efficiency [35]. Second, the selected features should have more discriminant power than the original all features. Many previous works have proven that removing those noisy and irrelevant features can improve discriminant power in most cases. In light of advantages of feature selection, different new algorithms have been flourished with various types of applications recently.\nAccording to the types of supervision, feature selection can be generally divided into three categories, i.e. supervised, semi-supervised, and unsupervised feature selection algorithms. Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31]. Label information of training data points is utilized to guide the supervised feature selection methods to seek distinctive subsets of features with different search strategies, i.e. complete search, heuristic search, and non-deterministic search. In the real world, class information is quite limited, resulting in the development of semi-supervised feature selection methods [30,9,3,4], in which both labeled and unlabeled data are utilized.\nIn unsupervised scenarios, feature selection is more challenging, since there is no class information to use for selecting features. In the literature, unsupervised feature selection can be roughly categorized into three groups, i.e. filter, wrapper, and embedded methods. Filter-based unsupervised feature selection methods rank features according to some intrinsic properties of data. Then those features with higher scores are selected for the further learning tasks. The selection is independent to the consequent process. For example, He et al. [10] assume that data from the same class are often close to each other and use the locality preserving power of data, also termed as Laplacian Score, to evaluate importance degrees of features. In [34], a unified framework has been proposed for both supervised and unsupervised feature selection schemes using a spectral graph. Tabakhi et al. [26] have proposed an unsupervised feature selection method to select the optimal feature subset in an iterative algorithm, which is based on ant colony optimization. Wrapper-based methods as a more sophisticated way wrap learning algorithms to yield learned results that will be used to select distinctive subsets of features. In [17], for instance, the authors have developed a model that selects relevant features using two backward stepwise selection algorithms without prior knowledges of features. Normally, wrapper-based methods have better performance than filter-based methods, since they use learning algorithms. Unfortunately, the disadvantage is that the computation of wrapper methods is\nmore expensive. Embedded methods are seeking a trade-off between them by integrating feature selection and clustering together into a joint framework. Because clustering algorithms are able to provide pseudo labels which can reflect the intrinsic information of data, some works [1,16,20,28] incorporate different clustering algorithms in objective functions to select features.\nMost of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.g. graph Laplacian, to reflect intrinsic relationships among data, labeled and unlabeled. When the number of data is extremely large, the computational burden of constructing a graph Laplacian is significantly heavy. Meanwhile, some traditional feature selection algorithms [10,7] neglect correlations among features. The distinctive features are individually selected according to the importance of each feature rather than taking correlations among features into account. Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33]. It has proven that discovering feature correlation is beneficial to feature selection.\nIn this paper, we propose a graph-free method to select features by combining Maximum Margin Criterion with feature correlation mining into a joint framework. Specifically, the method, on one hand, aims to learn a feature coefficient matrix which linearly combines features to maximize the class margins. With the increase of the separability of the entire transformed data by maximizing the total scatter, the proposed method also expects distances between data points within the same class to be minimized after the linear transformation by the coefficient matrix. Since there is no class information can be borrowed from, K-means clustering is jointly embedded in the framework to provide pseudo labels. Inspired by recent feature selection works using sparsity-based model on the regularization term [4], on the other hand, the proposed algorithm learns sparse structural information of the coefficient matrix, with the goal of reducing noisy and irrelevant features by removing those features whose coefficients are zeros. The main contributions of this paper can be summarized as follows:\n\u2013 The proposed method makes efforts to maximize class margins in a framework, where simultaneously considers the separability of the transformed data and distances between the transformed data within the same class. Besides, a sparsity-based regularization model is jointly applied on the feature coefficient matrix to analyze correlations among features in an iterative algorithm;\n\u2013 K-means clustering is embedded into the framework generating cluster labels, which can be used as pseudo labels. Both maximizing class margins and learning sparse structures can benefit from generated pseudo labels during each iteration;\n\u2013 Because the performance of K-means is dominated by the initialization, we propose a strategy to avoid our algorithm rapidly converge to a local optimum, which is largely ignored by most of existing approaches using K-means clustering. Theoretical proof of convergence is also provided.\n\u2013 We have conducted extensive experiments over six benchmark datasets. The experimental results show that our method has better performance than all the compared unsupervised algorithms.\nThe rest of this paper is organized as follows: Notations and definitions that are used throughout the entire paper will be given in section 2. Our method will be elaborated in section 3, followed by proposing its optimization with an algorithm to guarantee the convergence property in section 4. In section 5, extensive experimental results are reported with related analysis. Lastly, the conclusion of this paper will be given in section 6."}, {"heading": "2 Notations and Definitions", "text": "To give a better understanding of the proposed method, notations and definitions which are used throughout this paper are summarized in this section. Matrices and vectors are written as boldface uppercase letters and boldface lowercase letters, respectively. Given a data set denoted as X = [x1, . . . ,xn] \u2208 R\nd\u00d7n, where n is the number of training data and d is the feature dimension. The mean of data is denoted as x\u0304. The feature coefficient matrix, W \u2208 Rd\u00d7d \u2032\n, linearly combines data features as W TX, d\u2032 is the feature dimension after the linear transformation. Given a cluster centroid matrix for the transformed data, G = [g1, . . . , gc] \u2208 R\nd\u2032\u00d7c, its cluster indicator of transformed xi is represented as ui = [ui1, . . . , uic]. c is the number of centroids. If transformed xi belongs to the j-th cluster, uij = 1, j = 1, . . . , c. Otherwise, uij = 0. Correspondingly, the cluster indicator matrix is U = [uT1 , . . . ,u T n ]\nT \u2208 Rn\u00d7c. For an arbitrary matrix M \u2208 Rr\u00d7l, its \u21132,p-norm is defined as:\n\u2016M\u20162,p =\n\n \nr \u2211\ni=1\n\n\nl \u2211\nj=1\nM2ij\n\n\np 2 \n \n1 p\n(1)\nThe i-th row of M is represented by M i. The between-class, within-class and total scatter matrices of data are respectively defined as:\nSb =\nc \u2211\ni=1\nni(x\u0304i \u2212 x\u0304)(x\u0304i \u2212 x\u0304) T ,\nSw = c \u2211\ni=1\nni \u2211\nj=1\n(xj \u2212 x\u0304i)(xj \u2212 x\u0304i) T ,\nSt =\nn \u2211\ni=1\n(xi \u2212 x\u0304)(xi \u2212 x\u0304) T\n(2)\nwhere ni is the number of data for the c-th class. St = Sw+Sb. Other notations and definitions will be explained when they are in use."}, {"heading": "3 Proposed Method", "text": "We now introduce our proposed method for unsupervised feature selection. To exploit distinctive features, an intuitive way is to find a linear transformation matrix which can project the data into a new space where the original data are more separable. PCA is the most popular approach to analyze the separability of features. PCA aims to seek directions on which transformed data have max variances. In other words, PCA is to maximize the separability of linearly trans-\nformed data by maximizing the covariance: max W\nn \u2211\ni=1\n(W T (xi\u2212x\u0304)) T (W T (xi\u2212x\u0304)).\nWithout losing the generality, we assume the data has zero mean, i.e. x\u0304 = 0. Recall the definition of total scatter of data, PCA is equivalent to maximize the total scatter of data. However, if only total scatter is considered as a separability measure, the within-class scatter might be also geometrically maximized with the maximization of the total scatter. This is not helpful to distinctive feature discovery. The representative model, LDA, solves this problem by maximizing\nFisher criterion: max W WTSbW W TSwW . However, LDA and its variants require class information to construct between-class and within-class scatter matrices [2], which is not suitable for unsupervised feature selection. Before we give the objective that can solve the aforementioned problem, we first look at a supervised feature selection framework:\nmax W\nn \u2211\ni=1\n(W Txi) T (W Txi)\u2212 \u03b1\nc \u2211\ni=1\nni \u2211\nj=1\n(W T (xj \u2212 x\u0304i)) T (W T (xj \u2212 x\u0304i))\u2212 \u03b2\u2126(W )\ns.t. W TW = I, (3) where \u03b1 and \u03b2 are regularization parameters. In this framework, the first term is to maximize the total scatter, while the second term is to minimize the withinclass scatter. The third part is a sparsity-based regularization term which controls the sparsity of W . This model is quite similar with the classical LDA-based methods. Due to there is no class information in the unsupervised scenario, we need virtual labels to minimize the distances between data within the same class while maximize the total separability at the same time. To achieve this goal, we apply K-means clustering in our framework to replace the ground truth by generating cluster indicators of data. Given c centroids G = [g1, . . . , gc] \u2208 R\nd\u2032\u00d7c, the objective function of the traditional K-means algorithm aims to minimize the following function:\nc \u2211\ni=1\n\u2211\nyj\u2208Yi\n(yj \u2212 gi) T (yj \u2212 gi)\n=\nn \u2211\ni=1\n(yi \u2212Gu T i ) T (yi \u2212Gu T i ),\n(4)\nwhere yi = W Txi. Note that K-means is used to assign cluster labels, which are used as pseudo labels, to minimize the within-class scatter after the linear\ntransformation by W . Then, we can substitute (4) into (3):\nmax W\nn \u2211\ni=1\n(W Txi) T (W Txi)\u2212 \u03b1\nn \u2211\ni=1\n(W Txi \u2212Gu T i ) T (W Txi \u2212Gu T i )\u2212 \u03b2\u2126(W )\ns.t. W TW = I, (5) As mentioned above, the sparsity-based regularization term has been widely used to find out correlated structures among features. The motivation behind this is to exploit sparse structures of the feature coefficient matrix. By imposing the sparse constraint, some of the rows of the feature coefficient matrix shrink to zeros. Those features corresponding to non-zero coefficients are selected as the distinctive subset of features. In this way, noisy and redundant features can be removed. This sparsity-based regularization has been applied in various problems. Inspired by the \u201dshrinking to zero\u201d idea, we utilize a sparsity model to uncover the common structures shared by features. To achieve that goal, we propose to minimize the \u21132,p-norm of the coefficient matrix, \u2016W \u20162,p, (0 < p < 2). From the definition of \u2016W \u20162,p in (1), outliers or negative impact of the irrelevant wi\u2019s are suppressed by minimizing the \u21132,p-norm. Note that p is a parameter that controls the degree of correlated structures among features. The lower p is, the more shared structures among are expected to exploit. After a number of optimization steps, the optimal feature coefficient matrix, W , can be obtained. Thus, we impose the \u21132,p-norm on the regularization term and re-write the objective function in a matrix representation as follows:\nmax W ,G,U\nTr(W TStW )\u2212 \u03b1\u2016W TX \u2212GUT \u20162F \u2212 \u03b2\u2016W \u20162,p\ns.t. W TW = I, (6)\nwhere U is an indicator matrix. Tr(\u00b7) is trace operator, while \u2016 \u00b7 \u20162F is the Frobenius norm of a matrix. Our proposed method integrates the Maximum Margin Criterion and sparse regularization into a joint framework. Embedding K-means into the framework not only minimizes the distances between withinclass data while maximizing total data separability, but also provides cluster labels. The cluster centroids generated by K-means can further guide the sparse structure learning on the feature coefficient matrix in each iterative step of our solution, which will be explained in the next section. We name this method for the unsupervised feature analysis with class margin optimization as UFCM."}, {"heading": "4 Optimization", "text": "In this section, we present our solution to the objective function in (6). Since the \u21132,p-norm is used to exploit sparse structures, the objective function cannot be solved in a closed form. Meanwhile, the objective function is not jointly convex with respect to three variables, i.e. W,G,U . Thus, we propose to solve the problem as follows.\nWe define a diagonal matrix D whose diagonal entries are defined as:\nDii = 1\n2 p \u2016wi\u20162\u2212p2\n. (7)\nThe objective function in (6) is equivalent to:\nmax W ,G,U\nTr(W TStW )\u2212 \u03b1\u2016W TX \u2212GUT \u20162F \u2212 \u03b2Tr(W TDW )\ns.t. W TW = I (8)\nWe propose to optimize the objective function in two steps in each iteration as follows:\n(1) Fix W,G and optimize U : When W is fixed, the first and third terms can be viewed as constants. While the second term can be viewed as the objective function of K-means, assigning cluster labels to each data. Also, the cluster centroid matrix G = [g1, . . . , gc] is also fixed, the optimal U is:\nUij =\n{\n1, j = argmin k \u2016W Txi \u2212 gk\u2016 2 F , 0, Otherwise. (9)\nThis is equivalent to perform K-means on the transformed data, W TX, which means the solution is unique.\n(2) Fix U and optimize W,G: After fixing the indicator matrix, U , we set the derivative of Equation (8)\nwith respect to G equal to 0:\n\u2212\u03b1 \u2202Tr(W TX \u2212GUT )T (W TX \u2212GUT )\n\u2202G = 0\n\u21d2 \u22122\u03b1W TXU + 2\u03b1GUTU = 0\n\u21d2 G = W TXU(UTU)\u22121\n(10)\nSubstituting Equation (10) into Equation (8), we have:\nTr(W TStW )\u2212 \u03b1\u2016W TX \u2212W TXU(UTU)\u22121UT \u20162F \u2212 \u03b2Tr(W TDW )\n= \u03b1Tr ( (W TXU(UTU)\u22121UT \u2212W TX)(W TX \u2212W TXU(UTU)\u22121UT )T )\n+Tr(W TStW )\u2212 \u03b2Tr(W TDW )\n= \u03b1Tr ( W TXU(UTU)\u22121UTXTW \u2212W TXXTW )\n+Tr(W TStW )\u2212 \u03b2Tr(W TDW )\n= Tr[W T (St + \u03b1XU(U TU)\u22121UTXT \u2212 \u03b1XXT \u2212 \u03b2D)W ]\n(11) Thus, the objective function becomes:\nmax W\nTr[W T (St + \u03b1XU(U TU)\u22121UTXT \u2212 \u03b1XXT \u2212 \u03b2D)W ]\ns.t. W TW = I (12)\nAlgorithm 1 Unsupervised Feature Analysis with Class Margin Optimization.\nInput: Data matrix X = [x1, . . . ,xn] \u2208 R d\u00d7n and parameters \u03b1 and \u03b2. Output: Feature coefficient matrix W and cluster indicator matrix U . 1: Initialize W by PCA on X ; 2: Initialize U by K-means on W TX ; 3: repeat 4: Compute D according to (7); 5: Update U according to (14); 6: Update W by eigen-decomposition of (13); 7: Update G according to (10); 8: until Convergence\nThe objective function can be then solved by performing eigen-decomposition of the following formula:\nSt + \u03b1XU(U TU)\u22121UTXT \u2212 \u03b1XXT \u2212 \u03b2D (13)\nThe optimal W can be determined by choosing d\u2032 eigenvectors corresponding to d\u2032 largest eigenvalues, d\u2032 \u2264 d. Our proposed method can be solved by above steps in an iterative algorithm. Each step can obtain the corresponding optimum. As the cluster indicator matrix U is initialized by K-means, the performance of our algorithm is determined by the initialization of K-means. To alleviate the local optimum problem, an update strategy for U is demanded. Generally speaking, we randomly initialize U a number of times and make comparisons according to the second term in Equation (6). Then we choose how to update the indicator matrix. Specifically, the optimal U\u2217i and W \u2217 i has been derived in the i-th iteration. In the (i + 1)-th iteration, we first randomly initialize U r times (r = 10 in our experiment) and combine the derivedU\u2217i in the i-th iteration as an updating candidate set: U\u0303i+1 = [U 0 i+1,U 1 i+1, . . . ,U r i+1], U 0 i+1 = U \u2217 i . According to \u2016W TX \u2212GUT \u20162F , the candidate, which yields the smallest value, is chosen to update U\u2217i+1:\nU\u2217i+1 = U\u0303 j i+1, j = argmin j \u2016W TX \u2212G(U\u0303 ji+1) T \u20162F (14)\nwhere j is the index of candidate set, j = 0, 1, . . . , r. In this way, we compare the derived cluster indicator matrix with r randomly initialized counterparts to alleviate the local optimum problem. We summarize the solution in Algorithm 1 which outputs the learned feature coefficient matrix W to select distinctive features.\nFrom Algorithm 1, it can be seen that the most computational operation is the eigen-decomposition in Equation (13). The computational complexity is O(d3). If the dimensionality of the data, d, is very high, dimensionality reduction is desirable. To analyze the convergence of our proposed method, the following proposition and its proof are given.\nProposition 1. Algorithm 1 monotonically increases the objective function in Equation (6) until convergence.\nProof. Assuming that, in the i-th iteration, the transformation matrix W and cluster centroid matrix G have been derived as Wi and Gi. In the (i + 1)-th iteration step, we use Wi and Gi to update Ui+1 according to the updating strategy in (14). We can have the following inequality:\nTr(W Ti StWi)\u2212 \u03b1\u2016W T i X \u2212GiU T i \u2016 2 F \u2212 \u03b2\u2016Wi\u20162,p\n\u2264Tr(W Ti StWi)\u2212 \u03b1\u2016W T i X \u2212GiU T i+1\u2016 2 F \u2212 \u03b2\u2016Wi\u20162,p\n(15)\nSimilarly, when Ui+1 is fixed to optimize W and G in the (i + 1)-th iteration, the following inequality can be obtained according to Equation (12):\nTr(W Ti StWi)\u2212 \u03b1\u2016W T i X \u2212GiU T i+1\u2016 2 F \u2212 \u03b2\u2016Wi\u20162,p\n\u2264Tr(W Ti+1StWi+1)\u2212 \u03b1\u2016W T i+1X \u2212Gi+1U T i+1\u2016 2 F \u2212 \u03b2\u2016Wi+1\u20162,p\n(16)\nAfter combining Equation (15) and (16) together, it indicates that the proposed algorithm will monotonically increase the objective function in each iteration. It is worth noting that the algorithm is alleviating the local optimum problem raised by random initializations of K-means, rather than completely solving it. However, our algorithm can avoid to rapidly converge to a local optimum and may converge to the global optimal solution."}, {"heading": "5 Experiments", "text": "In this section, experimental results will be presented together with related analysis. We compare our method with seven approaches over six benchmark datasets. Besides, we also conduct experiments to evaluate performance variations in different aspects. They are including the impact of different selected feature numbers, the validation of feature correlation analysis, and parameter sensitivity analysis. Lastly, the convergence demonstration is shown."}, {"heading": "5.1 Experiment Setup", "text": "In the experiments, we have compared our method with seven approaches as follows:\n\u2013 All Features: All original variables are preserved as the baseline in the experiments. \u2013 Max Variance: Features are ranked according to the variance magnitude of each feature in a descending order. The highest ranked features are selected. \u2013 Spectral Feature Selection (SPEC) [34]: This method employs a unified framework to select features one by one based on spectral graph theory. \u2013 Multi-Cluster Feature Selection (MCFS) [1]: This unsupervised approach selects those features who make the multi-cluster structure of the data preserved best. Features are selected using spectral regression with the \u21131-norm regularization.\n\u2013 Robust Unsupervised Feature Selection (RUFS) [20]: RUFS jointly performs robust label learning and robust feature learning. To achieve this, robust orthogonal nonnegative matrix factorization is applied to learn labels while the \u21132,1-norm minimization is simultaneously utilized to learn the features. \u2013 Nonnegative Discriminative Feature Selection (NDFS) [16]: NDFS exploits local discriminative information and feature correlations simultaneously. Besides, the manifold structure information is also considered jointly. \u2013 Laplacian Score (LapScore) [10]: This method learns and selects distinctive features by evaluating their powers of locality preserving, which is also called Laplacian Score.\nAll the parameters (if any) are tuned in the range of {10\u22123, 10\u22121, 101, 103} for each algorithm mentioned above and the best results are reported. The size of the neighborhood is set to 5 for any algorithm based on spectral clustering. The number of random initializations required in the update strategy in (14), is set at 10 in the experiment. To measure the performance, two metrics have been used: Clustering Accuracy (ACC) and Normalized Mutual Information (NMI).\nFor a data point xi, its ground truth label is denoted as pi and its clustering label that is produced from a clustering algorithm, is represented as qi. Then, ACC metric over a data set with n data points is defined as follows:\nACC =\n\u2211n\ni=1 \u03b4(pi,map(qi))\nn , (17)\nwhere \u03b4(x, y) = 1 if x = y and \u03b4(x, y) = 0 otherwise. map(x) is the best mapping function which permutes clustering labels to match the ground truth labels using the Kuhn-Munkres algorithm. A larger ACC means better performance.\nAccording to the definition in [24], NMI is defined as:\nNMI =\n\u2211c\nl=1\n\u2211c h=1 tl,hlog( n\u00d7tl,h\ntl t\u0303h )\n\u221a\n( \u2211c l=1 tllog tl n )( \u2211c h=1 t\u0303hlog t\u0303h n ) , (18)\nwhere tl is the number of data points in the l-th cluster, 1 \u2264 l \u2264 c, which is generated by a clustering algorithm. While t\u0303h denotes the number of data points in the h-th ground truth cluster. tl,h is the number of data points which are in the intersection of the l-th and h-th clusters. Similarly, a larger NMI means better performance.\nThe performance evaluations are performed over six benchmark datasets as follows:\n\u2013 COIL20 [18]: It contains 1,440 gray-scale images of 20 objects (72 images per object) under various poses. The objects are rotated through 360 degrees and taken at the interval of 5 degrees. \u2013 MNIST [15]: It is a large-scale dataset of handwritten digits, which has been widely used as a test bed in data mining. The dataset contains 60,000 training images and 10,000 testing images. In this paper, we use its subclass\nThe pixel value in data is used as the feature. Details of data sets that are used in this paper are summarized in Table 1."}, {"heading": "5.2 Experimental Results", "text": "To compare the performance of our proposed algorithm with others, we repeatedly perform the test five times and report the average performance results (ACC and NMI ) with standard deviations in Tables 2 and 3. It is observed that our proposed method consistently achieves better performance than all other compared approaches across all the data sets. Besides, it is worth noting that our method is superior to those state-of-the-art counterparts that rely on a graph Laplacian (SPEC, RUFS, NDFS, LapScore).\nWe study how the number of selected features can affect the performance by conducting an experiment whose results are shown in Figure 1. From the figure,\nperformance variations with respect to the number of selected features using the proposed algorithm over three data sets, including COIL20, MNIST, and USPS, have been illustrated. We only adopt ACC as the metric. Some observations can be obtained: 1) When the number of selected features is small, e.g. 500 on each data set, the accuracy value is relatively small. 2) With the increase of selected features, performance can peak at a certain point. For example, the performance of our algorithm peaks at 0.7475 on COIL20 when the number of selected features increases to 800. Similarly, 0.6392 (800 selected features) and 0.7813 (600 selected features) are observed on MNIST and USPS, respectively. 3) When all features are in use, the performance is worse than the best. Similar trends can be also observed on the other data sets. It is concluded that our algorithm is able to select distinctive features.\nTo demonstrate exploiting feature correlation is beneficial to the performance, we conduct an experiment in which parameters \u03b1 and p are both fixed at 1. \u03b2 varies in a range of [0, 10\u22123, 10\u22122, 10\u22121, 1, 101, 102, 103]. The performance variation results with respect to different \u03b2s are plotted in Figure 2. The experiment is conducted over three data sets, i.e. COIL20, MNIST, and USPS. From the results, we can observe that the performance is relatively low, when there is no correlation exploiting in the framework, i.e. \u03b2 = 0. The performance always peaks at a certain point when a proper degree of sparsity is imposed to the regularization term. For example, the performance is only 0.6993 when \u03b2 = 0 on COIL20. The performance increases to 0.7285 when \u03b2 = 101. Similar observations are also obtained on the other data sets. We can conclude that sparse structure learning on feature coefficient matrix contributes to the performance of our unsupervised feature selection method."}, {"heading": "5.3 Studies on Parameter Sensitivity and Convergence", "text": "There are three parameters in our algorithms, which are denoted as \u03b1, \u03b2 and p in (6). \u03b1 and \u03b2 are two regularization parameters while p controls the degree of sparsity. To investigate the sensitivity of the parameters, we conduct an experiment to study how they exert influences on performance. Firstly, we fix \u03b2 = 10\u22121 and derive the performance variations under different combinations of \u03b1s and ps in Figure 3. Secondly, \u03b1 is fixed at 10\u22121. The performance variation results with respect to different \u03b2s and ps are shown in Figure 4. Both \u03b1 and \u03b2 vary in a range of [10\u22123, 10\u22121, 101, 101]. While p changes in [0.5, 1.0, 1.5]. We only take ACC as the metric.\nTo validate that our algorithm will monotonically increase the objective function value in (6), we conduct an experiment to demonstrate this fact. In this\nexperiment, all parameters (\u03b1, \u03b2, and p) in (6) are fixed at 1. The objective function values and corresponding iteration numbers are drawn in Figure 5. We take COIL20, MNIST, and USPS as examples. Similar observations can be also obtained on the other data sets. From the figure, it can be seen that our algorithm converges to the optimum, usually within eight iteration steps, over three data sets. We can then conclude that the proposed method is efficient and effective."}, {"heading": "6 Conclusion", "text": "In this paper, an unsupervised feature selection approach has been proposed by using the MaximumMargin Criterion and the sparsity-based model. More specifically, the proposed method seeks to maximize the total scatter on one hand. On the other hand, the within-class scatter is simultaneously considered to minimize. Since there is no label information in an unsupervised scenario, K-means clustering is embedded into the framework jointly. Advantages can be summarized as twofold: First, pseudo labels generated by K-means clustering is beneficial to maximizing class margins in each iteration step. Second, pseudo labels can guide the sparsity-based model to exploit sparse structures of the feature coefficient matrix. Noisy and uncorrelated features can be therefore removed. Since the objective function is non-convex for all variables, we have proposed an algorithm with a guaranteed convergence property. To avoid to rapidly converge to a local optimum which is caused by K-means, we have applied an updating strategy to alleviate the problem. In this way, our proposed method might converge to the global optimum. Extensive experimental results have shown that our method has superior performance against all other compared approaches over six benchmark data sets."}], "references": [{"title": "Unsupervised feature selection for multi-cluster data", "author": ["D. Cai", "C. Zhang", "X. He"], "venue": "ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD). pp. 333\u2013342. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Compound rank-k projections for bilinear analysis", "author": ["X. Chang", "F. Nie", "S. Wang", "Y. Yang"], "venue": "IEEE Trans. Neural Netw. Learning Syst.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "A convex formulation for semi-supervised multi-label feature selection", "author": ["X. Chang", "F. Nie", "Y. Yang", "H. Huang"], "venue": "AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised feature analysis for multimedia annotation by mining label correlation", "author": ["X. Chang", "H. Shen", "S. Wang", "J. Liu", "X. Li"], "venue": "Advances in Knowledge Discovery and Data Mining, pp. 74\u201385. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic concept discovery for large-scale zero-shot event detection", "author": ["X. Chang", "Y. Yang", "A.G. Hauptmann", "E.P. Xing", "Y. Yu"], "venue": "IJCAI", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiple graph unsupervised feature selection", "author": ["X. Du", "Y. Yan", "P. Pan", "G. Long", "L. Zhao"], "venue": "Signal Processing", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "John Wiley & Sons", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 23(6), 643\u2013 660", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Semisupervised feature selection via spline regression for video semantic recognition", "author": ["Y. Han", "Y. Yang", "Y. Yan", "Z. Ma", "N. Sebe", "X. Zhou"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "Advances in Neural Information Processing Systems (NIPS). pp. 507\u2013514", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Joint embedding learning and sparse regression: A framework for unsupervised feature selection", "author": ["C. Hou", "F. Nie", "X. Li", "D. Yi", "Y. Wu"], "venue": "IEEE T. Cybernetics 44(6), 793\u2013804", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 16(5), 550\u2013554", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "A practical approach to feature selection", "author": ["K. Kira", "L.A. Rendell"], "venue": "International Workshop on Machine Learning. pp. 249\u2013256", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "Estimating attributes: analysis and extensions of relief", "author": ["I. Kononenko"], "venue": "Machine Learning: ECML-94. pp. 171\u2013182. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11), 2278\u20132324", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Unsupervised feature selection using nonnegative spectral analysis", "author": ["Z. Li", "Y. Yang", "J. Liu", "X. Zhou", "H. Lu"], "venue": "AAAI Conference on Artificial Intelligence (AAAI). pp. 1026\u20131032", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Variable selection for clustering with gaussian mixture models", "author": ["C. Maugis", "G. Celeux", "M.L. Martin-Magniette"], "venue": "Biometrics 65(3), 701\u2013709", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Columbia object image library (coil-20)", "author": ["S.A. Nene", "S.K. Nayar", "H Murase"], "venue": "Tech. rep., Technical Report CUCS-005-96", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient and robust feature selection via joint l2, 1-norms minimization", "author": ["F. Nie", "H. Huang", "X. Cai", "C.H.Q. Ding"], "venue": "NIPS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust unsupervised feature selection", "author": ["M. Qian", "C. Zhai"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI). pp. 1621\u20131627. AAAI Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Theoretical comparison between the gini index and information gain criteria", "author": ["L.E. Raileanu", "K. Stoffel"], "venue": "Annals of Mathematics and Artificial Intelligence 41(1), 77\u201393", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F.S. Samaria", "A.C. Harter"], "venue": "IEEE Workshop on Applications of Computer Vision. pp. 138\u2013142. IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Multi-task support vector machines for feature selection with shared knowledge discovery", "author": ["Sen Wang", "Xiaojun Chang", "X.L.Q.Z.S.W.C."], "venue": "Signal Processing", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "Journal of Machine Learning Research (JMLR) 3, 583\u2013617", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep learning face representation from predicting 10,000 classes", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1891\u20131898. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "An unsupervised feature selection algorithm based on ant colony optimization", "author": ["S. Tabakhi", "P. Moradi", "F. Akhlaghian"], "venue": "Engineering Applications of Artificial Intelligence 32, 112\u2013123", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised feature selection via unified trace ratio formulation and k-means clustering (TRACK)", "author": ["D. Wang", "F. Nie", "H. Huang"], "venue": "ECML/PKDD", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Embedded unsupervised feature selection", "author": ["S. Wang", "J. Tang", "H. Liu"], "venue": "AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Data mining with big data", "author": ["X. Wu", "X. Zhu", "G.Q. Wu", "W. Ding"], "venue": "IEEE Transactions on Knowledge and Data Engineering (TKDE) 26(1), 97\u2013107", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative semi-supervised feature selection via manifold regularization", "author": ["Z. Xu", "I. King", "M.T. Lyu", "R. Jin"], "venue": "IEEE Transactions on Neural Networks 21(7), 1033\u20131047", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Feature selection for multimedia analysis by sharing information among multiple tasks", "author": ["Y. Yang", "Z. Ma", "A.G. Hauptmann", "N. Sebe"], "venue": "IEEE Transactions on Multimedia 15(3), 661\u2013669", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "l2, 1-norm regularized discriminative feature selection for unsupervised learning", "author": ["Y. Yang", "H.T. Shen", "Z. Ma", "Z. Huang", "X. Zhou"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI). vol. 22, p. 1589. Citeseer", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval", "author": ["Y. Yang", "Y. Zhuang", "F. Wu", "Y. Pan"], "venue": "IEEE Transactions on Multimedia 10(3), 437\u2013446", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Z. Zhao", "H. Liu"], "venue": "International Conference on Machine Learning. pp. 1151\u20131157. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Self-taught dimensionality reduction on the high-dimensional small-sized data", "author": ["X. Zhu", "Z. Huang", "Y. Yang", "H.T. Shen", "C. Xu", "J. Luo"], "venue": "Pattern Recognition 46(1), 215\u2013229", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": "Over the past few years, data are more than often represented by high-dimensional features in a number of research fields, such as data mining [29], computer vision [25], etc.", "startOffset": 143, "endOffset": 147}, {"referenceID": 24, "context": "Over the past few years, data are more than often represented by high-dimensional features in a number of research fields, such as data mining [29], computer vision [25], etc.", "startOffset": 165, "endOffset": 169}, {"referenceID": 34, "context": "Due to a lower dimensional representation, the subsequent learning tasks with no doubt can gain benefit in terms of efficiency [35].", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 76, "endOffset": 79}, {"referenceID": 12, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 142, "endOffset": 146}, {"referenceID": 22, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 152, "endOffset": 159}, {"referenceID": 30, "context": "Representative supervised feature selection algorithms include Fisher score [7], Relief[13] and its extension, ReliefF [14], information gain [21], etc [23,31].", "startOffset": 152, "endOffset": 159}, {"referenceID": 29, "context": "In the real world, class information is quite limited, resulting in the development of semi-supervised feature selection methods [30,9,3,4], in which both labeled and unlabeled data are utilized.", "startOffset": 129, "endOffset": 139}, {"referenceID": 8, "context": "In the real world, class information is quite limited, resulting in the development of semi-supervised feature selection methods [30,9,3,4], in which both labeled and unlabeled data are utilized.", "startOffset": 129, "endOffset": 139}, {"referenceID": 2, "context": "In the real world, class information is quite limited, resulting in the development of semi-supervised feature selection methods [30,9,3,4], in which both labeled and unlabeled data are utilized.", "startOffset": 129, "endOffset": 139}, {"referenceID": 3, "context": "In the real world, class information is quite limited, resulting in the development of semi-supervised feature selection methods [30,9,3,4], in which both labeled and unlabeled data are utilized.", "startOffset": 129, "endOffset": 139}, {"referenceID": 9, "context": "[10] assume that data from the same class are often close to each other and use the locality preserving power of data, also termed as Laplacian Score, to evaluate importance degrees of features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "In [34], a unified framework has been proposed for both supervised and unsupervised feature selection schemes using a spectral graph.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "[26] have proposed an unsupervised feature selection method to select the optimal feature subset in an iterative algorithm, which is based on ant colony optimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In [17], for instance, the authors have developed a model that selects relevant features using two backward stepwise selection algorithms without prior knowledges of features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Because clustering algorithms are able to provide pseudo labels which can reflect the intrinsic information of data, some works [1,16,20,28] incorporate different clustering algorithms in objective functions to select features.", "startOffset": 128, "endOffset": 140}, {"referenceID": 15, "context": "Because clustering algorithms are able to provide pseudo labels which can reflect the intrinsic information of data, some works [1,16,20,28] incorporate different clustering algorithms in objective functions to select features.", "startOffset": 128, "endOffset": 140}, {"referenceID": 19, "context": "Because clustering algorithms are able to provide pseudo labels which can reflect the intrinsic information of data, some works [1,16,20,28] incorporate different clustering algorithms in objective functions to select features.", "startOffset": 128, "endOffset": 140}, {"referenceID": 27, "context": "Because clustering algorithms are able to provide pseudo labels which can reflect the intrinsic information of data, some works [1,16,20,28] incorporate different clustering algorithms in objective functions to select features.", "startOffset": 128, "endOffset": 140}, {"referenceID": 9, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 33, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 19, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 26, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 15, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 10, "context": "Most of the existing unsupervised feature selection methods [10,34,20,27,16,11] rely on a graph, e.", "startOffset": 60, "endOffset": 79}, {"referenceID": 9, "context": "Meanwhile, some traditional feature selection algorithms [10,7] neglect correlations among features.", "startOffset": 57, "endOffset": 63}, {"referenceID": 6, "context": "Meanwhile, some traditional feature selection algorithms [10,7] neglect correlations among features.", "startOffset": 57, "endOffset": 63}, {"referenceID": 31, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 19, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 4, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 5, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 18, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 32, "context": "Recently, exploiting feature correlations has attracted much research attention [32,20,5,6,19,33].", "startOffset": 80, "endOffset": 97}, {"referenceID": 3, "context": "Inspired by recent feature selection works using sparsity-based model on the regularization term [4], on the other hand, the proposed algorithm learns sparse structural information of the coefficient matrix, with the goal of reducing noisy and irrelevant features by removing those features whose coefficients are zeros.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "However, LDA and its variants require class information to construct between-class and within-class scatter matrices [2], which is not suitable for unsupervised feature selection.", "startOffset": 117, "endOffset": 120}, {"referenceID": 33, "context": "\u2013 Spectral Feature Selection (SPEC) [34]: This method employs a unified framework to select features one by one based on spectral graph theory.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "\u2013 Multi-Cluster Feature Selection (MCFS) [1]: This unsupervised approach selects those features who make the multi-cluster structure of the data preserved best.", "startOffset": 41, "endOffset": 44}, {"referenceID": 19, "context": "\u2013 Robust Unsupervised Feature Selection (RUFS) [20]: RUFS jointly performs robust label learning and robust feature learning.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "\u2013 Nonnegative Discriminative Feature Selection (NDFS) [16]: NDFS exploits local discriminative information and feature correlations simultaneously.", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "\u2013 Laplacian Score (LapScore) [10]: This method learns and selects distinctive features by evaluating their powers of locality preserving, which is also called Laplacian Score.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "According to the definition in [24], NMI is defined as:", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "The performance evaluations are performed over six benchmark datasets as follows: \u2013 COIL20 [18]: It contains 1,440 gray-scale images of 20 objects (72 images per object) under various poses.", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "\u2013 MNIST [15]: It is a large-scale dataset of handwritten digits, which has been widely used as a test bed in data mining.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "\u2013 ORL [22]: This data set which is used as a benchmark for face recognition, consists of 40 different subjects with 10 images each.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "\u2013 USPS [12]: This dataset collects 9,298 images of handwritten digits (0-9) from envelops by the U.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "\u2013 YaleB [8]: It consists of 2,414 frontal face images of 38 subjects.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 0, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "\u03b2 varies in a range of [0, 10, 10, 10, 1, 10, 10, 10].", "startOffset": 23, "endOffset": 53}, {"referenceID": 9, "context": "Both \u03b1 and \u03b2 vary in a range of [10, 10, 10, 10].", "startOffset": 32, "endOffset": 48}, {"referenceID": 9, "context": "Both \u03b1 and \u03b2 vary in a range of [10, 10, 10, 10].", "startOffset": 32, "endOffset": 48}, {"referenceID": 9, "context": "Both \u03b1 and \u03b2 vary in a range of [10, 10, 10, 10].", "startOffset": 32, "endOffset": 48}, {"referenceID": 9, "context": "Both \u03b1 and \u03b2 vary in a range of [10, 10, 10, 10].", "startOffset": 32, "endOffset": 48}], "year": 2015, "abstractText": "Unsupervised feature selection has been always attracting research attention in the communities of machine learning and data mining for decades. In this paper, we propose an unsupervised feature selection method seeking a feature coefficient matrix to select the most distinctive features. Specifically, our proposed algorithm integrates the Maximum Margin Criterion with a sparsity-based model into a joint framework, where the class margin and feature correlation are taken into account at the same time. To maximize the total data separability while preserving minimized within-class scatter simultaneously, we propose to embed Kmeans into the framework generating pseudo class label information in a scenario of unsupervised feature selection. Meanwhile, a sparsity-based model, l2,p-norm, is imposed to the regularization term to effectively discover the sparse structures of the feature coefficient matrix. In this way, noisy and irrelevant features are removed by ruling out those features whose corresponding coefficients are zeros. To alleviate the local optimum problem that is caused by random initializations of K-means, a convergence guaranteed algorithm with an updating strategy for the clustering indicator matrix, is proposed to iteratively chase the optimal solution. Performance evaluation is extensively conducted over six benchmark data sets. From plenty of experimental results, it is demonstrated that our method has superior performance against all other compared approaches.", "creator": "LaTeX with hyperref package"}}}