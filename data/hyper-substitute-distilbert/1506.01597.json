{"id": "1506.01597", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2015", "title": "Abstractive Multi-Document Summarization via Phrase Selection and Merging", "abstract": "updates include improved abstraction - based large - document summarization framework that thoroughly construct recognizable sentences including developing brighter fine - tasting syntactic units than sentences, namely, noun / verb parts. presented using graphical abstraction - based approaches, our method frequently constructs descriptive pool including terms and facts projected by reflections from the input documents. with new outputs being displayed by encoding and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the phrase construction constraints. we employ integer linear optimization protocols conducting query sequencing. filtering across input programming to receive simple global model assignment or a summary. experimental results on the benchmark data set yearbook 2014 predicted that our framework outperforms the other - of - the - art models including automated pyramid evaluation approach, adequately achieves reasonably well available on manual linguistic quality analysis.", "histories": [["v1", "Thu, 4 Jun 2015 14:04:10 GMT  (65kb,D)", "https://arxiv.org/abs/1506.01597v1", "11 pages, 1 figures, accepted as a full paper at ACL 2015"], ["v2", "Fri, 5 Jun 2015 15:02:46 GMT  (65kb,D)", "http://arxiv.org/abs/1506.01597v2", "11 pages, 1 figure, accepted as a full paper at ACL 2015"]], "COMMENTS": "11 pages, 1 figures, accepted as a full paper at ACL 2015", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["lidong bing", "piji li", "yi liao", "wai lam", "weiwei guo", "rebecca j passonneau"], "accepted": true, "id": "1506.01597"}, "pdf": {"name": "1506.01597.pdf", "metadata": {"source": "CRF", "title": "Abstractive Multi-Document Summarization via Phrase Selection and Merging\u2217", "authors": ["Lidong Bing", "Piji Li", "Yi Liao", "Wai Lam", "Weiwei Guo", "Rebecca J. Passonneau"], "emails": ["lbing@cs.cmu.edu,", "wlam}@se.cuhk.edu.hk", "wguo@yahoo-inc.com,", "becky@ccls.columbia.edu"], "sections": [{"heading": "1 Introduction", "text": "Existing multi-document summarization (MDS) methods fall in three categories: extraction-based, compression-based and abstraction-based. Most\n\u2217 The work described in this paper is substantially supported by grants from the Research and Development Grant of Huawei Technologies Co. Ltd (YB2013090068/TH138232) and the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 413510 and 14203414). The work was done when Weiwei Guo was in Columbia University\nsummarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence.\nIn fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner.\nIn this paper, we propose an abstractive MDS framework that can construct new sentences by\nar X\niv :1\n50 6.\n01 59\n7v 2\n[ cs\n.C L\n] 5\nJ un\n2 01\n5\nexploring more fine-grained syntactic units than sentences, namely, noun/verb phrases (NPs/VPs). This idea is based on two observations. First, the major constituent phrases loosely correspond to the concepts and facts. After reading a set of documents describing the same topic or event, a person digests these documents as key concepts and facts in his/her mind, such as \u201can armed man\u201d and \u201cwalked into an Amish school\u201d from Figure 1. Second, a summary writer re-organizes the key concepts and facts to form new sentences for the summary. Accordingly, our proposed framework has two major components corresponding to the above observations. The first component creates a pool of concepts and facts represented by NPs and VPs from the input documents. A salience score is computed for each phrase by exploiting redundancy of the document content in a global manner. The second component constructs new sentences by selecting and merging phrases based on their salience scores, and ensures the validity of new sentences using a integer linear optimization model.\nThe contribution of this paper is two folds. (1) We extract NPs/VPs from constituency trees to represent key concepts/facts, and merge them to construct new sentences, which allows more summary content units (SCUs) (Nenkova and Passonneau, 2004) to be included in a sentence by breaking the original sentence boundaries. (2) The designed optimization framework for addressing the problem is unique and effective. Our optimization algorithm simultaneously selects and merges a set of phrases that maximize the number of cov-\nered SCUs in a summary. Meanwhile, since the basic unit is phrases, we design compatibility relations among NPs and VPs, as well as other optimization constraints, to ensure that the generated sentences contain correct facts. Compared with the sentence fusion approaches that compute salience scores of sentence clusters, our proposed framework explores a more fine-grained textual unit (i.e., phrases), and maximizes the salience of selected phrases in a global manner."}, {"heading": "2 Description of Our Framework", "text": "We first introduce how to extract NPs and VPs from constituency trees, and subsequently calculate salience scores for them. Then we formulate the sentence generation task as an optimization problem, and design constraints. In the end, we perform several post-processing steps to improve the order and the readability of the generated sentences."}, {"heading": "2.1 Phrase Salience Calculation", "text": "The first component decomposes the sentences in documents into a set of noun phrases (NPs) derived from the subject parts of a constituency tree and a set of verb-object phrases (VPs), representing potential key concepts and key facts, respectively. These phrases will serve as the basic elements for sentence generation.\nWe employ Stanford parser (Klein and Manning, 2003) to obtain a constituency tree for each input sentence. After that, we extract NPs and VPs from the tree as follows: (1) The NPs and VPs that are the direct children of the sentence node (repre-\nsented by the S node) are extracted. (2) VPs (NPs) in a path on which all the nodes are VPs (NPs) are also recursively extracted and regarded as having the same parent node S. Recursive operation in the second step will only be carried out in two levels since the phrases in the lower levels may not be able to convey a complete fact. Take the tree in Figure 1 as an example, the corresponding sentence is decomposed into phrases \u201cAn armed man\u201d, \u201cwalked into an Amish school, sent the boys outside and tied up and shot the girls, killing three of them\u201d, \u201cwalked into an Amish school\u201d, \u201csent the boys outside\u201d, and \u201ctied up and shot the girls, killing three of them\u201d. 1 Because of the recursive operation, the extracted phrases may have overlaps. Later, we will show how to avoid such overlapping in phrase selection.\nA salience score is calculated for each phrase to indicate its importance. Different types of salience can be incorporated in our framework, such as position-based method (Yih et al., 2007), statistical feature based method (Woodsend and Lapata, 2012), concept-based method (Li et al., 2011), etc. One key characteristic of our approach is that the considered basic units are phrases instead of sentences. Such finer granularity leaves more room for better global salience score by potentially covering more distinct facts. In our implementation, we adopt a concept-based weight incorporating the position information. The concept set is designated to be the union set of unigrams, bigrams, and named entities in the documents. We remove stopwords and perform lemmatization before extracting unigrams and bigrams. The position-based term frequency is used in the concept weighting scheme. When counting the frequency, each occurrence of a concept in an input document is weighted with the paragraph position. The weight larger than 1 is given to the concept occurrences in the first few paragraphs. Specifically, the weight of the first paragraph is B and the weight decreases as the position of the paragraph increases from the beginning of the doc-\n1 We only consider the recursive operation for a VP with more than one parallel sub-VPs, such as the highest VP in Figure 1. The sub-VPs following modal, link or auxiliary verbs are not extracted as individual VPs. In addition, we also extract the clauses functioning as subjects of sentences as NPs, such as \u201cthat clause\u201d. Note that we also mention such clauses as \u201cnoun phrase\u201d although their syntactic labels could be \u201cSBAR\u201d or \u201cS\u201d.\nument. The weighting function is:\nH(p) =\n{ \u03c1p \u2217B if p < \u2212(logB/ log \u03c1)\n1 otherwise ,\n(1) where p is the position of the paragraph starting from 0, from beginning of the document, and \u03c1 is a positive constant and smaller than 1. Then, the salience of a phrase is calculated as the summed weights of its concepts."}, {"heading": "2.2 New Sentence Construction Model", "text": "The construction of new sentences is formulated as an optimization problem which is able to simultaneously generate a group of sentences. Each new sentence is composed of one NP and at least one VP, where the NP and VPs may come from different source sentences. In the process of new sentence generation, the compatibility relation between NP and VP and a variety of summarization requirements are jointly considered."}, {"heading": "2.2.1 Compatibility Relation", "text": "Compatibility relation is designed to indicate whether an NP and a VP can be used to form a new sentence. For example, the NP \u201cPolice\u201d from another sentence should not be the subject of the VP \u201csent the boys outside\u201d extracted from Figure 1. We use some heuristics to find compatibility, and then expand the compatibility relation to more phrases by extracting coreference.\nTo find coreference NPs (different mentions for the same entity), we first conduct coreference resolution for each document with Stanford coreference resolution package (Lee et al., 2013). We adopt those resolution rules that are able to achieve high quality and address our need for summarization. In particular, Sieve 1, 2, 3, 4, 5, 9, and 10 in the package are used. A set of clusters are obtained and each cluster contains the mentions that refer to the same entity in a document. The clusters from different documents in the same topic are merged by matching the named entities. After merging, the mentions that are not NPs extracted in the phrase extraction step are removed in each cluster. Two NPs in the same cluster are determined as alternative of each other.\nTo find alternative VPs, Jaccard Index is employed as the similarity measure. Specifically, each VP is represented as a set of its concepts and the index value is calculated for each pair of VPs. If the value is larger than a threshold, the two VPs are determined as alternative of each other.\nWe then define an indicator matrix \u0393|N||V|, in which \u0393[i, j] = 1 if an NP Ni and a VP Vj come from the same node S in the constituency tree, otherwise, \u0393[i, j] = 0. Let N\u0303i and V\u0303i represent the alternative phrases of Ni and Vi as described above. The compatibility matrix \u0393\u0303|N||V| is defined as follows:\n\u0393\u0303[p, q] =  1 if Np \u2208 N\u0303i \u2227 \u0393[i, q] = 1 1 if Vq \u2208 V\u0303j \u2227 \u0393[p, j] = 1 1 if \u0393[p, q] = 1 0 otherwise\n(2)\nwhere \u0393\u0303[p, q] = 1 means Np and Vq are compatible/permitted for constructing a new sentence. \u0393\u0303 is the final compatibility matrix that we use in the optimization. The first case of Equation 2 implies that if Np and Ni are coreferent, Np can replace Ni and serve as the subject of Ni\u2019s VP (i.e., Vq). The second case implies that if Vq is very similar to Vj , Vq can be concatenated to Vj\u2019s NP (i.e.,Np)."}, {"heading": "2.2.2 Phrase-based Content Optimization", "text": "The overall objective function of our optimization formulation to select NPs and VPs is defined as:\nmax{ \u2211 i \u03b1iS N i \u2212 \u2211 i<j \u03b1ij(S N i + S N j )R N ij\n+ \u2211 i \u03b2iS V i \u2212 \u2211 i<j \u03b2ij(S V i + S V j )R V ij},\n(3)\nwhere \u03b1i and \u03b2i are selection indicators for the NP Ni and the VP Vi, respectively. SNi and S V i are the salience scores of Ni and Vi. \u03b1ij and \u03b2ij are co-occurrence indicators of pairs (Ni, Nj) and (Vi, Vj). RNij and R V ij are the similarity of pairs (Ni, Nj) and (Vi, Vj). IfNi andNj are coreferent, RNij = 1. Otherwise, the similarity is calculated with the above Jaccard Index based method. The notations are summarized in Table 1.\nSpecifically, we maximize the salience score of the selected NPs and VPs as indicated by the first and the third terms in Equation 3, and penalize the selection of similar NP pairs and similar VP pairs as indicated by the second and the fourth terms. Meanwhile, the phrase selection is governed by a set of constraints so that the selected phrases can generate valid sentences. The constraints will be explained in details in Section 2.2.3.\nOne characteristic of our objective function is that NPs and VPs are treated differently, i.e., there\nare different selection/penalty terms for NP and VP. Such design enables us to avoid the false penalty between an NP and a VP. For example, the algorithm produces two sentences: the first sentence is \u201cthe gunman shot ...\u201d with an NP \u201cthe gunman\u201d, and the other sentence has a VP \u201cconfirmed the gunman died\u201d. Obviously, we should not penalize the redundancy between them, because mentioning the gunman is necessary in both sentences."}, {"heading": "2.2.3 Sentence Generation Constraints", "text": "To summarize the related sentences in the documents, human writers usually merge the important facts in different VPs about the same entity into a single sentence, and omit the trivial facts. Also, the same entity is likely to be described by coreferent NPs. Therefore, in our approach, only one NP is selected and employed as the subject of the newly generated sentence, which is then concatenated with the merged facts (i.e., VPs). If the compatibility entry \u0393\u0303[i, j] for Ni and Vj is 1, we define a sentence generation indicator \u03b3\u0303ij to indicate whether both Ni and Vj are selected to construct a new sentence in the summary.\nWe design the following groups of constraints to realize our aim of phrase selection and new sentence construction. The objective function and constraints are linear, therefore the problem can be solved by existing Integer Linear Programming (ILP) solvers such as simplex algorithm (Dantzig and Thapa, 1997). NP validity. To maintain the consistency between the selection indicator \u03b1 and the compatibility entry \u0393\u0303 for NP Ni, we introduce two constraints as follows:\n\u2200i, j, \u03b1i \u2265 \u03b3\u0303ij ; \u2200i, \u2211 j \u03b3\u0303ij \u2265 \u03b1i. (4)\nThese two constraints work together to ensure the valid assignment of \u03b1 according to the compatibility entry \u0393\u0303. VP legality. Similarly, the following requirement guarantees the consistency between the selection indicator \u03b2 and the compatibility entry \u0393\u0303 for selected VP Vi:\n\u2200j, \u2211 i \u03b3\u0303ij = \u03b2j . (5)\nThe above two constraints jointly ensure that the selected NPs and VPs are able to form new summary sentences according to the values of sentence generation indicators. Not i-within-i. Two phrases in the same path of a constituency tree cannot be chosen at the same time:\nif \u2203Vk Vj , then \u03b2k + \u03b2j \u2264 1, if \u2203Nk Nj , then \u03b1k + \u03b1j \u2264 1.\n(6)\nFor example, \u201cwalked into an Amish school, sent the boys outside and tied up and shot the girls, killing three of them\u201d and \u201cwalked into an Amish school\u201d cannot be both incorporated in the summary, because of the obvious redundancy. Phrase co-occurrence. These constraints control the co-occurrence relation of NPs or VPs. For NPs, we introduce three constraints:\n\u03b1ij \u2212 \u03b1i \u2264 0, (7) \u03b1ij \u2212 \u03b1j \u2264 0, (8)\n\u03b1i + \u03b1j \u2212 \u03b1ij \u2264 1. (9)\nConstraints 7 to 9 ensure a valid solution of NP selection. The first two constraints state that if the units Ni and Nj co-occur in the summary (i.e., \u03b1ij = 1), then we have to include them individually (i.e., \u03b1i = 1 and \u03b1j = 1). The third constraint is the inverse of the first two. Similarly, the constraints for VPs are as follows:\n\u03b2ij \u2212 \u03b2i \u2264 0, (10) \u03b2ij \u2212 \u03b2j \u2264 0, (11)\n\u03b2i + \u03b2j \u2212 \u03b2ij \u2264 1. (12)\nSentence number. In abstractive summarization, we do not prefer to generate many short sentences. This is controlled by:\u2211\ni\n\u03b1i \u2264 K, (13)\nwhere K is the maximum number of sentences.\nShort sentence avoidance. We do not select the VPs from very short sentences because a short sentence normally cannot convey a complete key fact (Woodsend and Lapata, 2012).\nif l(S) < M,Vi \u2208 S, then \u03b2i = 0, (14)\nwhere M is the threshold of the sentence length. Pronoun avoidance. We exclude the NPs that are pronouns from being selected as the subject of the new sentences. As previously observed (Woodsend and Lapata, 2012), pronouns are normally not used by human summary writers. It is because the summary is short and the narration relation of sentences is relatively simple so that pronouns are not needed. Moreover, in automatic summary, pronouns will cause ambiguity in the summary, especially when the sentence order is automatically determined. Therefore, we model the constraint as:\nif Ni is pronoun, then \u03b1i = 0. (15)\nLength constraint. The overall length of the selected NPs and VPs is no larger than a limit L:\u2211\ni {l(Ni) \u2217 \u03b1i}+ \u2211 j {l(Vj) \u2217 \u03b2j} \u2264 L, (16)\nwhere l() is the word-based length of a phrase."}, {"heading": "2.3 Postprocessing", "text": "Recall that we require that one NP and at least one VP compose a sentence. Thus, we form a raw sentence with a selected NP as the subject followed by the corresponding selected VPs that are indicated by sentence generation indicator \u03b3\u0303ij having the value 1. The VPs in a summary sentence are ordered according to their natural order if they come from the same document. Otherwise, they are ordered according to the timestamps of the corresponding documents. After that, if the total length is smaller than L, we add conjunctions such as \u201cand\u201d and \u201cthen\u201d to concatenate the VPs for improving the readability of the newly generated sentences. The pseudo-timestamp of a sentence is defined as the earliest timestamp of its VPs and the sentences are ordered based on their pseudo-timestamps."}, {"heading": "2.4 Relation to Existing MDS Approaches", "text": "Many existing extraction-based and compressionbased MDS approaches could be regarded as special cases under our framework: (1) To simulate\nextraction-based summarization, we just need to constrain that the highest NP and the highest VP from the same sentence are selected simultaneously. In addition, no NPs and VPs in lower levels can be selected. Thus, the output only contains the original sentences of the source documents. (2) To simulate compression-based summarization, we can adapt our framework to conduct sentence selection and sentence compression in a joint manner. Specifically, we only need to restrict that the NP and VPs of a summary sentence must come from the same original sentence."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Experimental Setup", "text": "The data set of traditional summarization task in Text Analysis Conference (TAC) 2011 is used to evaluate the performance of our approach. This data set is the latest one and it contains 44 topics. Each topic falls into one of 5 predefined event categories and contains 10 related news documents. There are four writers to write model summaries for each topic.\nThe data set of traditional summarization task in TAC 2010 is employed as the development/tuning data set. This data set contains 46 topics from the same predefined categories. Each topic also has 10 documents and 4 model summaries.\nBased on the tuning set, the key parameters of our model are set as follows. The constants B and \u03c1 in the weighting function are set to 6 and 0.5 repectively. The similarity threshold in obtaining the alternative VPs is 0.75. We did not observe significant difference between cosine similarity and Jaccard Index.\nWe mainly evaluate the system by pyramid evaluation. To gain a comprehensive understanding, we also evaluate by ROUGE evaluation and manual linguistic quality evaluation."}, {"heading": "3.2 Results with Pyramid Evaluation", "text": "The pyramid evaluation metric (Nenkova and Passonneau, 2004) involves semantic matching of summary content units (SCUs) so as to recognize alternate realizations of the same meaning. Different weights are assigned to SCUs based on their frequency in model summaries. A weighted inventory of SCUs named a pyramid is created, which constitutes a resource for investigating alternate realizations of the same meaning. Such property makes pyramid method more suitable to evalu-\nate summaries. Another widely used evaluation metric is ROUGE (Lin and Hovy, 2003) and it evaluates summaries from word overlapping perspective. Because of the strict string matching, it ignores the semantic content units and performs better when larger sets of model summaries are available. In contrast to ROUGE, pyramid scoring is robust with as few as four model summaries (Nenkova and Passonneau, 2004). Therefore, in recent summarization evaluation workshops such as TAC, the pyramid is used as the major metric.\nSince manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different from those of TAC, we employ the automated version of pyramid proposed in (Passonneau et al., 2013). The automated pyramid scoring procedure relies on distributional semantics to assign SCUs to a target summary. Specifically, all n-grams within sentence bounds are extracted, and converted into 100 dimension latent topical vectors via a weighted matrix factorization model (Guo and Diab, 2012). Similarly, the contributors and the label of an SCU are transformed into 100 dimensional vector representations. An SCU is assigned to a summary if there exists an n-gram such that the similarity score between the SCU low dimensional vector and the n-gram low dimensional vector exceeds a threshold. Passonneau et al. (2013) showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores, yielding more accurate pyramid scores than string matching based automated methods (Harnly et al., 2005). In this paper, we adopt the same setting as in (Passonneau et al., 2013): a 100 dimension matrix factorization model is learned on a domain independent corpus, which is drawn from sense definitions of WordNet and Wiktionary2, and Brown corpus. We exper-\n2http://en.wiktionary.org/\niment with 2 threshold values, i.e., 0.6 and 0.65, similar to those used in (Passonneau et al., 2013).\nThe top three systems in TAC 2011 evaluated with manual pyramid score were System 22 (Li et al., 2011), 43, and 17 (Ng et al., 2011). Table 2 shows the comparison with them under the automated pyramid evaluation. Our method achieves the best results in both thresholds, which means that our method is able to find more semantic content units (SCUs) than the state-of-the-art system in TAC 2011. In addition, paired t-test (with p < 0.01) comparing our model with the best system in TAC 2011, i.e., System 22, shows that the performance of our model is significantly better. It is worth noting that the three systems used additional external linguistic resources: System 22 used a Wikipedia corpus for providing domain knowledge, System 17 and 43 defined some categoryspecific features. Without any domain adaption, our framework can still achieve encouraging performance.\nWe calculate Pearson\u2019s correlation to measure how well the automatic pyramid approximates the manual pyramid scores for 50 system submissions in TAC 2011. The values are 0.91 and 0.93 for thresholds 0.6 and 0.65 respectively. It demonstrates that the automated pyramid is reliable to differentiate the performance of different methods."}, {"heading": "3.3 Results with ROUGE Evaluation", "text": "As mentioned above, we favor the pyramid evaluation over the ROUGE score because it can measure the summary quality beyond simply string matching. Here, we also provide ROUGE score for our reference. ROUGE-1.5.5 package3 is employed with the same parameters as in TAC. The results are summarized in Table 3. Our performance is slightly better than System 22, and it is not as good as System 43 and 17. The reason is that System 43 and 17 used category-specific features and trained the feature weights with the category information\n3http://www.berouge.com/Pages/default.aspx\nin TAC 2010 data. These features help them select better category-specific content for the summary. However, the usability of such features depends on the availability of predefined categories in the summarization task, as well as the availability of training data with the same predefined categories for estimating feature weights. Therefore, the adaptability of these methods is limited to some extent. In contrast, our framework does not define any category-specific feature and only uses TAC 2010 data to tune the parameters for general summarization purpose."}, {"heading": "3.4 Linguistic Quality Evaluation", "text": "The linguistic quality of summaries is evaluated using the five linguistic quality questions on grammaticality (Q1), non-redundancy (Q2), referential clarity (Q3), focus (Q4), and coherence (Q5) in Document Understanding Conferences (DUC). A Likert scale with five levels is employed with 5 being very good with 1 being very poor. A summary was blindly evaluated by three assessors on each question. System 22 performed better than System 43 and 17 in TAC 2011 on the evaluation of readability, which is an aggregation of the above questions. Considering the intensive labor force of manual assessment, we only conduct comparison with System 22.\nThe results are given in Table 4. On average, the two systems perform very closely. System 22 is an extraction-based method that picks the original sentences, hence it achieves higher score in Q1 grammaticality, while our approach has some new sentences with grammar mistakes, which is a common problem for abstractive methods and deserves more future research effort. For Q4 focus, our score is higher than System 22, which reveals that our summary sentences are relatively more cohesive. The score of Q3 referential clarity shows that the referential relation is basically clear in our summaries, even when new sentences are automatically generated. In general, ignoring the grammaticality scores, our system still performs better than System 22. Specifically, the average scores of our system and System 22 on the last four questions are 3.37 and 3.33 respectively."}, {"heading": "4 Qualitative Results", "text": ""}, {"heading": "4.1 Analysis of Summary Sentence Type", "text": "There are three types of sentences in the summaries generated by our framework, namely, new\nsentences, compressed sentences, and original sentences. A new sentence is constructed by merging the phrases from different original sentences. A compressed sentence is generated by deleting phrases from an original sentence. An original sentence in the summary is directly extracted from the input documents.\nThe percentage of different types of sentences in our summaries is calculated. About 33% of the summary sentences are newly constructed. This demonstrates that our framework has good capability of merging phrases from the original sentences so as to convey more information in compacted summaries. In addition, about 44% of the summary sentences are generated by compression. It shows a unique characteristic of our framework: sentence construction and sentence compression are conducted in a unified model."}, {"heading": "4.2 Case Study", "text": "Table 5 shows the summary of the first topic, i.e., \u201cAmish Shooting\u201d, by our framework. The summary sentence ID and the sentence type are given in the form of \u201c[summary sentence ID: sentence type]\u201d. Each selected phrase and the original sentence ID where the phrase originated are given in the form of \u201c{selected phrase (original sentence ID)}\u201d. There are three compressed sentences with IDs 1, 2, and 4, one new sentence with ID 3, and two original sentences with IDs 5 and 6.\nThe new sentence is constructed from the following original sentences in which the extracted NPs and VPs are indicated with colored parentheses:\n(84): On Monday morning, (NP Charles Carl Roberts IV) (VP (VP entered the West Nickel Mines Amish School in Lancaster County) and (VP shot 10 girls), (VP killing five)). (85): (NPRoberts) (VP killed himself as police stormed the building). (150): (NP Roberts) (VP left what they described as rambling notes for his family).\n[1:C] {An armed man (25)} {walked into an Amish school (25)} {tied up and shot the girls, killing three of them. (25)} [2:C] {A man who laid siege to a one-room Amish schoolhouse (64)} {told his wife shortly before opening fire that he had molested two young girls who were his relatives decades ago (64)} {was tormented by dreams of molesting again. (64)} [3:N] {Charles Carl Roberts IV (84)} {killed himself as police stormed the building (85)} {left what they described as rambling notes for his family. (150)} [4:C] {The gunman (145)} {was not Amish (145)} {had not attended the school. (145)} [5:O] {The shootings (148)} {occurred about 10:45 a.m.(148)} [6:O] {Police (149)} {could offer no explanation for the killings. (149)}\nTable 5: The summary of \u201cAmish Shooting\u201d topic.\nThe NPs of these sentences are coreferent so that some of their VPs are merged and concatenated with one NP, i.e., \u201cCharles Carl Roberts IV\u201d.\nThe summary sentences with IDs 1, 2, and 4 are compressions from the following original sentences respectively:\n(25): (NPAn armed man) (VP(VPwalked into an Amish school), (VP sent the boys outside) and (VP tied up and shot the girls, killing three of them)), (NP authorities) (VP said). (64): (NP(NP A man)who laid siege to a one-room Amish schoolhouse),(VP killing five girls),(VP(VP told his wife shortly before opening fire that he had molested two young girls who were his relatives decades ago)and(VP was tormented by \u201cdreams of molesting again\u201d)),(NP authorities)(VP said Tue). (145): According to media reports, (NP the gunman) (VP(VP was not Amish) and (VP had not attended the school)).\nSome uncritical information is excluded from the summary sentences, such as \u201csent the boys outside\u201d, \u201cauthorities said\u201d, etc. In addition, the VP \u201ckilling five girls\u201d of the original sentence with ID 64 is also excluded since it has significant redundancy with the summary sentence with ID 1."}, {"heading": "5 Related Work", "text": "Existing multi-document summarization (MDS) works can be classified into three categories:\nextraction-based approaches, compression-based approaches, and abstraction-based approaches.\nExtraction-based approaches are the most studied of the three. Early studies mainly followed a greedy strategy in sentence selection (C\u0327elikyilmaz and Hakkani-Tu\u0308r, 2011; Goldstein et al., 2000; Wan et al., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases.\nCompression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences.\nOn the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest\nand Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014).\nRecently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; Ka\u030ageba\u0308ck et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks."}, {"heading": "6 Conclusions and Future Work", "text": "We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely, noun phrases and verb phrases. The designed optimization framework operates on the summary level so that more complementary semantic content units can be incorporated. The phrase selection and merging is done simultaneously to achieve global optimal. Meanwhile, the constructed sentences should satisfy the constraints related to summarization requirements such as NP/VP compatibility. Experimental results on TAC 2011 summarization data set show that our framework outperforms the top systems in TAC 2011 under the pyramid metric. For future work, one aspect is to enhance the grammar quality of the generated new sentences and compressed sentences. Another aspect is to improve time efficiency of our framework, and its major bottleneck is the time consuming ILP optimzation."}], "references": [{"title": "Fast and robust compressive summarization with dual decomposition and multitask learning", "author": ["Almeida", "Martins2013] Miguel Almeida", "Andre Martins"], "venue": "In ACL,", "citeRegEx": "Almeida et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Almeida et al\\.", "year": 2013}, {"title": "Sentence fusion for multidocument news summarization", "author": ["Barzilay", "McKeown2005] Regina Barzilay", "Kathleen R. McKeown"], "venue": null, "citeRegEx": "Barzilay et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2005}, {"title": "Query-chain focused summarization", "author": ["Baumel et al.2014] Tal Baumel", "Raphael Cohen", "Michael Elhadad"], "venue": "In ACL,", "citeRegEx": "Baumel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baumel et al\\.", "year": 2014}, {"title": "Jointly learning to extract and compress", "author": ["Dan Gillick", "Dan Klein"], "venue": "In HLT,", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2011}, {"title": "Ranking with recursive neural networks and its application to multidocument summarization", "author": ["Cao et al.2015] Ziqiang Cao", "Furu Wei", "Li Dong", "Sujian Li", "Ming Zhou"], "venue": null, "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Concept-based classification for multi-document summarization", "author": ["\u00c7elikyilmaz", "Dilek Hakkani-T\u00fcr"], "venue": "In ICASSP,", "citeRegEx": "\u00c7elikyilmaz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "\u00c7elikyilmaz et al\\.", "year": 2011}, {"title": "Towards robust abstractive multi-document summarization: A caseframe analysis of centrality and domain", "author": ["Cheung", "Penn2013] Jackie Chi Kit Cheung", "Gerald Penn"], "venue": "In ACL,", "citeRegEx": "Cheung et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cheung et al\\.", "year": 2013}, {"title": "Hierarchical summarization: Scaling up multidocument summarization", "author": ["Stephen Soderland", "Gagan Bansal", "Mausam"], "venue": "In ACL,", "citeRegEx": "Christensen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Christensen et al\\.", "year": 2014}, {"title": "Linear Programming 1: Introduction", "author": ["Dantzig", "Thapa1997] George B. Dantzig", "Mukund N. Thapa"], "venue": null, "citeRegEx": "Dantzig et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dantzig et al\\.", "year": 1997}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network. arXiv preprint arXiv:1406.3830", "author": ["Denil et al.2014] Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas"], "venue": null, "citeRegEx": "Denil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["Erkan", "Radev2004] G\u00fcnes Erkan", "Dragomir R. Radev"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Erkan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Erkan et al\\.", "year": 2004}, {"title": "A formal model for information selection in multi-sentence text extraction", "author": ["Filatova", "Vasileios Hatzivassiloglou"], "venue": "In COLING", "citeRegEx": "Filatova et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Filatova et al\\.", "year": 2004}, {"title": "Sentence fusion via dependency graph compression", "author": ["Filippova", "Strube2008] Katja Filippova", "Michael Strube"], "venue": "In EMNLP,", "citeRegEx": "Filippova et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2008}, {"title": "Multi-sentence compression: Finding shortest paths in word graphs", "author": ["Katja Filippova"], "venue": "In COLING,", "citeRegEx": "Filippova.,? \\Q2010\\E", "shortCiteRegEx": "Filippova.", "year": 2010}, {"title": "Opinosis: A graphbased approach to abstractive summarization of highly redundant opinions", "author": ["ChengXiang Zhai", "Jiawei Han"], "venue": "In COLING,", "citeRegEx": "Ganesan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganesan et al\\.", "year": 2010}, {"title": "Framework for abstractive summarization using text-to-text generation", "author": ["Genest", "Guy Lapalme"], "venue": "In MTTG,", "citeRegEx": "Genest et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Genest et al\\.", "year": 2011}, {"title": "Fully abstractive approach to guided summarization", "author": ["Genest", "Guy Lapalme"], "venue": "In ACL,", "citeRegEx": "Genest et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Genest et al\\.", "year": 2012}, {"title": "A scalable global model for summarization", "author": ["Gillick", "Favre2009] Dan Gillick", "Benoit Favre"], "venue": "In Workshop on ILP for NLP,", "citeRegEx": "Gillick et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2009}, {"title": "The icsi summarization system at tac", "author": ["Gillick et al.2007] Dan Gillick", "Benoit Favre", "Dilek Hakkani-t\u00fcr"], "venue": "In Proc. of Text Understanding Conference", "citeRegEx": "Gillick et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2007}, {"title": "Multi-document summarization by sentence extraction", "author": ["Vibhu Mittal", "Jaime Carbonell", "Mark Kantrowitz"], "venue": "In NAACL-ANLP-AutoSum,", "citeRegEx": "Goldstein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Goldstein et al\\.", "year": 2000}, {"title": "Modeling sentences in the latent space", "author": ["Guo", "Diab2012] Weiwei Guo", "Mona Diab"], "venue": "In ACL,", "citeRegEx": "Guo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2012}, {"title": "Using topic themes for multi-document summarization", "author": ["Harabagiu", "Lacatusu2010] Sanda Harabagiu", "Finley Lacatusu"], "venue": "ACM Trans. Inf. Syst.,", "citeRegEx": "Harabagiu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Harabagiu et al\\.", "year": 2010}, {"title": "Automation of summary evaluation by the pyramid method", "author": ["Harnly et al.2005] Aaron Harnly", "Ani Nenkova", "Rebecca Passonneau", "Owen Rambow"], "venue": "RANLP", "citeRegEx": "Harnly et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Harnly et al\\.", "year": 2005}, {"title": "Cut and paste based text summarization", "author": ["Jing", "McKeown2000] Hongyan Jing", "Kathleen R. McKeown"], "venue": "In NAACL,", "citeRegEx": "Jing et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Jing et al\\.", "year": 2000}, {"title": "Extractive summarization using continuous vector space models", "author": ["Olof Mogren", "Nina Tahmasebi", "Devdatt Dubhashi"], "venue": "In CVSC@EACL,", "citeRegEx": "K\u00e5geb\u00e4ck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "K\u00e5geb\u00e4ck et al\\.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Manning2003] Dan Klein", "Christopher D. Manning"], "venue": "In ACL,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Statistics-based summarization - step one: Sentence compression", "author": ["Knight", "Marcu2000] Kevin Knight", "Daniel Marcu"], "venue": "In AAAI-IAAI,", "citeRegEx": "Knight et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Knight et al\\.", "year": 2000}, {"title": "Deterministic coreference resolution based on entity-centric", "author": ["Lee et al.2013] Heeyoung Lee", "Angel Chang", "Yves Peirsman", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky"], "venue": "precision-ranked rules. Comput. Linguist.,", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Pkutm participation in tac2011", "author": ["Li et al.2011] Huiying Li", "Yue Hu", "Zeyuan Li", "Xiaojun Wan", "Jianguo Xiao"], "venue": "In Proceedings of TAC", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Reader-aware multi-document summarization via sparse coding", "author": ["Li et al.2015] Piji Li", "Lidong Bing", "Wai Lam", "Hang Li", "Yi Liao"], "venue": "In IJCAI", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Multi-document summarization via budgeted maximization of submodular functions", "author": ["Lin", "Bilmes2010] Hui Lin", "Jeff Bilmes"], "venue": "In HLT,", "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["Lin", "Bilmes2012] Hui Lin", "Jeff Bilmes"], "venue": "In UAI,", "citeRegEx": "Lin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Automatic evaluation of summaries using ngram co-occurrence statistics", "author": ["Lin", "Hovy2003] Chin-Yew Lin", "Eduard Hovy"], "venue": "In NAACL,", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Improving summarization performance by sentence compression: a pilot study", "author": ["Chin-Yew Lin"], "venue": "In Proceedings of the sixth international workshop on Information retrieval with Asian languages-Volume", "citeRegEx": "Lin.,? \\Q2003\\E", "shortCiteRegEx": "Lin.", "year": 2003}, {"title": "Query-oriented multi-document summarization via unsupervised deep learning", "author": ["Liu et al.2012] Yan Liu", "Sheng-hua Zhong", "Wenjie Li"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Summarization with a joint model for sentence extraction and compression", "author": ["Martins", "Smith2009] Andr\u00e9 F.T. Martins", "Noah A. Smith"], "venue": "In Workshop on ILP for NLP,", "citeRegEx": "Martins et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "A study of global inference algorithms in multi-document summarization", "author": ["Ryan McDonald"], "venue": "In ECIR,", "citeRegEx": "McDonald.,? \\Q2007\\E", "shortCiteRegEx": "McDonald.", "year": 2007}, {"title": "Abstractive summarization of spoken and written conversations based on phrasal queries", "author": ["Mehdad et al.2014] Yashar Mehdad", "Giuseppe Carenini", "Raymond T. Ng"], "venue": "In ACL,", "citeRegEx": "Mehdad et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mehdad et al\\.", "year": 2014}, {"title": "Evaluating content selection in summarization: The pyramid method", "author": ["Nenkova", "Passonneau2004] Ani Nenkova", "Rebecca J. Passonneau"], "venue": "In HLT-NAACL,", "citeRegEx": "Nenkova et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2004}, {"title": "Entity-driven rewrite for multi-document summarization", "author": ["Ani Nenkova"], "venue": "In Third International Joint Conference on Natural Language Processing,", "citeRegEx": "Nenkova.,? \\Q2008\\E", "shortCiteRegEx": "Nenkova.", "year": 2008}, {"title": "Swing: Exploiting category-specific information for guided summarization", "author": ["Ng et al.2011] Jun-Ping Ng", "Praveen Bysani", "Ziheng Lin", "Min yen Kan", "Chew lim Tan"], "venue": "Proceedings of TAC", "citeRegEx": "Ng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2011}, {"title": "Exploiting timelines to enhance multi-document summarization", "author": ["Ng et al.2014] Jun-Ping Ng", "Yan Chen", "Min-Yen Kan", "Zhoujun Li"], "venue": "In ACL,", "citeRegEx": "Ng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2014}, {"title": "Automated pyramid scoring of summaries using distributional semantics", "author": ["Emily Chen", "Weiwei Guo", "Dolores Perin"], "venue": "ACL", "citeRegEx": "Passonneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Passonneau et al\\.", "year": 2013}, {"title": "Modelling events through memory-based, open-ie patterns for abstractive summarization", "author": ["Marco Cornolti", "Enrique Alfonseca", "Katja Filippova"], "venue": "In ACL,", "citeRegEx": "Pighin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pighin et al\\.", "year": 2014}, {"title": "Information status distinctions and referring expressions: An empirical study of references to people in news summaries", "author": ["Ani Nenkova", "Kathleen McKeown"], "venue": null, "citeRegEx": "Siddharthan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Siddharthan et al\\.", "year": 2011}, {"title": "Large-margin learning of submodular summarization models", "author": ["Sipos et al.2012] Ruben Sipos", "Pannaga Shivaswamy", "Thorsten Joachims"], "venue": "In EACL,", "citeRegEx": "Sipos et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sipos et al\\.", "year": 2012}, {"title": "Ctsum: Extracting more certain summaries for news articles", "author": ["Wan", "Zhang2014] Xiaojun Wan", "Jianmin Zhang"], "venue": "In SIGIR,", "citeRegEx": "Wan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2014}, {"title": "Manifold-ranking based topicfocused multi-document summarization", "author": ["Wan et al.2007] Xiaojun Wan", "Jianwu Yang", "Jianguo Xiao"], "venue": "In IJCAI,", "citeRegEx": "Wan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2007}, {"title": "Automatic generation of story highlights", "author": ["Woodsend", "Lapata2010] Kristian Woodsend", "Mirella Lapata"], "venue": "In ACL,", "citeRegEx": "Woodsend et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2010}, {"title": "Multiple aspect summarization using integer linear programming", "author": ["Woodsend", "Lapata2012] Kristian Woodsend", "Mirella Lapata"], "venue": "In EMNLPCoNLL,", "citeRegEx": "Woodsend et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2012}, {"title": "Evolutionary timeline summarization: A balanced optimization framework via iterative substitution", "author": ["Yan et al.2011] Rui Yan", "Xiaojun Wan", "Jahna Otterbacher", "Liang Kong", "Xiaoming Li", "Yan Zhang"], "venue": "In SIGIR,", "citeRegEx": "Yan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2011}, {"title": "Multidocument summarization by maximizing informative content-words", "author": ["Yih et al.2007] Wen-tau Yih", "Joshua Goodman", "Lucy Vanderwende", "Hisami Suzuki"], "venue": "In IJCAI,", "citeRegEx": "Yih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2007}, {"title": "Sentence compression as a component of a multi-document summarization system", "author": ["Zajic et al.2006] David M. Zajic", "Bonnie J. Dorr", "Jimmy Lin", "Richard Schwartz"], "venue": "In DUC at NLT/NAACL", "citeRegEx": "Zajic et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zajic et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 47, "context": "The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007).", "startOffset": 208, "endOffset": 249}, {"referenceID": 33, "context": "To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method.", "startOffset": 103, "endOffset": 205}, {"referenceID": 52, "context": "To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method.", "startOffset": 103, "endOffset": 205}, {"referenceID": 29, "context": "To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method.", "startOffset": 103, "endOffset": 205}, {"referenceID": 13, "context": "One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010).", "startOffset": 98, "endOffset": 143}, {"referenceID": 51, "context": "Different types of salience can be incorporated in our framework, such as position-based method (Yih et al., 2007), statistical feature based method (Woodsend and Lapata, 2012), concept-based method (Li et al.", "startOffset": 96, "endOffset": 114}, {"referenceID": 28, "context": ", 2007), statistical feature based method (Woodsend and Lapata, 2012), concept-based method (Li et al., 2011), etc.", "startOffset": 92, "endOffset": 109}, {"referenceID": 27, "context": "To find coreference NPs (different mentions for the same entity), we first conduct coreference resolution for each document with Stanford coreference resolution package (Lee et al., 2013).", "startOffset": 169, "endOffset": 187}, {"referenceID": 42, "context": "Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different from those of TAC, we employ the automated version of pyramid proposed in (Passonneau et al., 2013).", "startOffset": 237, "endOffset": 262}, {"referenceID": 22, "context": "(2013) showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores, yielding more accurate pyramid scores than string matching based automated methods (Harnly et al., 2005).", "startOffset": 219, "endOffset": 240}, {"referenceID": 42, "context": "In this paper, we adopt the same setting as in (Passonneau et al., 2013): a 100 dimension matrix factorization model is learned on a domain independent corpus, which is drawn from sense definitions of WordNet and Wiktionary2, and Brown corpus.", "startOffset": 47, "endOffset": 72}, {"referenceID": 41, "context": "Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different from those of TAC, we employ the automated version of pyramid proposed in (Passonneau et al., 2013). The automated pyramid scoring procedure relies on distributional semantics to assign SCUs to a target summary. Specifically, all n-grams within sentence bounds are extracted, and converted into 100 dimension latent topical vectors via a weighted matrix factorization model (Guo and Diab, 2012). Similarly, the contributors and the label of an SCU are transformed into 100 dimensional vector representations. An SCU is assigned to a summary if there exists an n-gram such that the similarity score between the SCU low dimensional vector and the n-gram low dimensional vector exceeds a threshold. Passonneau et al. (2013) showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores, yielding more accurate pyramid scores than string matching based automated methods (Harnly et al.", "startOffset": 238, "endOffset": 884}, {"referenceID": 42, "context": "65, similar to those used in (Passonneau et al., 2013).", "startOffset": 29, "endOffset": 54}, {"referenceID": 28, "context": "The top three systems in TAC 2011 evaluated with manual pyramid score were System 22 (Li et al., 2011), 43, and 17 (Ng et al.", "startOffset": 85, "endOffset": 102}, {"referenceID": 40, "context": ", 2011), 43, and 17 (Ng et al., 2011).", "startOffset": 20, "endOffset": 37}, {"referenceID": 19, "context": "Early studies mainly followed a greedy strategy in sentence selection (\u00c7elikyilmaz and Hakkani-T\u00fcr, 2011; Goldstein et al., 2000; Wan et al., 2007).", "startOffset": 70, "endOffset": 147}, {"referenceID": 47, "context": "Early studies mainly followed a greedy strategy in sentence selection (\u00c7elikyilmaz and Hakkani-T\u00fcr, 2011; Goldstein et al., 2000; Wan et al., 2007).", "startOffset": 70, "endOffset": 147}, {"referenceID": 36, "context": "Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012).", "startOffset": 103, "endOffset": 260}, {"referenceID": 51, "context": "Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012).", "startOffset": 103, "endOffset": 260}, {"referenceID": 18, "context": "Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012).", "startOffset": 103, "endOffset": 260}, {"referenceID": 45, "context": "Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012).", "startOffset": 103, "endOffset": 260}, {"referenceID": 33, "context": "As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009).", "startOffset": 93, "endOffset": 149}, {"referenceID": 52, "context": "As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009).", "startOffset": 93, "endOffset": 149}, {"referenceID": 3, "context": "Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015).", "startOffset": 104, "endOffset": 231}, {"referenceID": 29, "context": "Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015).", "startOffset": 104, "endOffset": 231}, {"referenceID": 39, "context": "Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011).", "startOffset": 151, "endOffset": 192}, {"referenceID": 44, "context": "Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011).", "startOffset": 151, "endOffset": 192}, {"referenceID": 14, "context": "Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014).", "startOffset": 105, "endOffset": 148}, {"referenceID": 37, "context": "Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014).", "startOffset": 105, "endOffset": 148}, {"referenceID": 41, "context": "Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011).", "startOffset": 86, "endOffset": 142}, {"referenceID": 50, "context": "Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011).", "startOffset": 86, "endOffset": 142}, {"referenceID": 2, "context": "Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al.", "startOffset": 188, "endOffset": 209}, {"referenceID": 7, "context": ", 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014).", "startOffset": 97, "endOffset": 123}, {"referenceID": 43, "context": "A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014).", "startOffset": 132, "endOffset": 153}, {"referenceID": 34, "context": "Moreover, some works (Liu et al., 2012; K\u00e5geb\u00e4ck et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks.", "startOffset": 21, "endOffset": 100}, {"referenceID": 24, "context": "Moreover, some works (Liu et al., 2012; K\u00e5geb\u00e4ck et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks.", "startOffset": 21, "endOffset": 100}, {"referenceID": 9, "context": "Moreover, some works (Liu et al., 2012; K\u00e5geb\u00e4ck et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks.", "startOffset": 21, "endOffset": 100}, {"referenceID": 4, "context": "Moreover, some works (Liu et al., 2012; K\u00e5geb\u00e4ck et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks.", "startOffset": 21, "endOffset": 100}], "year": 2015, "abstractText": "ive Multi-Document Summarization via Phrase Selection and Merging\u2217 Lidong Bing\u00a7 Piji Li Yi Liao Wai Lam Weiwei Guo\u2020 Rebecca J. Passonneau\u2021 \u00a7Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA USA Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong \u2020Yahoo Labs, Sunnyvale, CA, USA \u2021Center for Computational Learning Systems, Columbia University, New York, NY, USA \u00a7lbing@cs.cmu.edu, {pjli, yliao, wlam}@se.cuhk.edu.hk \u2020wguo@yahoo-inc.com, \u2021becky@ccls.columbia.edu", "creator": "LaTeX with hyperref package"}}}