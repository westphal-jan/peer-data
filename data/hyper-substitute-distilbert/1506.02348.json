{"id": "1506.02348", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Convergence Rates of Active Learning for Maximum Likelihood Estimation", "abstract": "universal active learner is currently a class of models, albeit similar set of particular examples, given this ability to interactively transmit tests against a subset across these examples ; the goal of its robot is \" learn a lesson in the class that fits the data well.", "histories": [["v1", "Mon, 8 Jun 2015 04:05:43 GMT  (16kb)", "http://arxiv.org/abs/1506.02348v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["kamalika chaudhuri", "sham m kakade", "praneeth netrapalli", "sujay sanghavi"], "accepted": true, "id": "1506.02348"}, "pdf": {"name": "1506.02348.pdf", "metadata": {"source": "CRF", "title": "Convergence Rates of Active Learning for Maximum Likelihood Estimation", "authors": ["Kamalika Chaudhuri", "Sham Kakade", "Praneeth Netrapalli", "Sujay Sanghavi"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n02 34\n8v 1\n[ cs\nPrevious theoretical work has rigorously characterized label complexity of active learning, but most of this work has focused on the PAC or the agnostic PAC model. In this paper, we shift our attention to a more general setting \u2013 maximum likelihood estimation. Provided certain conditions hold on the model class, we provide a two-stage active learning algorithm for this problem. The conditions we require are fairly general, and cover the widely popular class of Generalized Linear Models, which in turn, include models for binary and multi-class classification, regression, and conditional random fields.\nWe provide an upper bound on the label requirement of our algorithm, and a lower bound that matches it up to lower order terms. Our analysis shows that unlike binary classification in the realizable case, just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation. On the empirical side, the recent work in [11] and [12] (on active linear and logistic regression) shows the promise of this approach."}, {"heading": "1 Introduction", "text": "In active learning, we are given a sample space X , a label space Y, a class of models that map X to Y, and a large set U of unlabelled samples. The goal of the learner is to learn a model in the class with small target error while interactively querying the labels of as few of the unlabelled samples as possible.\nMost theoretical work on active learning has focussed on the PAC or the agnostic PAC model, where the goal is to learn binary classifiers that belong to a particular hypothesis class [2, 13, 9, 6, 3, 4, 22], and there has been only a handful of exceptions [19, 8, 20]. In this paper, we shift our attention to a more general setting \u2013 maximum likelihood estimation (MLE), where Pr(Y |X) is described by a model \u03b8 belonging to a model class \u0398. We show that when data is generated by a model in this class, we can do active learning provided the model class \u0398 has the following simple property: the Fisher information matrix for any model \u03b8 \u2208 \u0398 at any (x, y) depends only on x and \u03b8. This condition is satisfied in a number of widely applicable model classes, such as Linear Regression and Generalized Linear Models (GLMs), which in turn includes models for Multiclass Classification and Conditional\nRandom Fields. Consequently, we can provide active learning algorithms for maximum likelihood estimation in all these model classes.\nThe standard solution to active MLE estimation in the statistics literature is to select samples for label query by optimizing a class of summary statistics of the asymptotic covariance matrix of the estimator [5]. The literature, however, does not provide any guidance towards which summary statistic should be used, or any analysis of the solution quality when a finite number of labels or samples are available. There has also been some recent work in the machine learning community [11, 12, 19] on this problem; but these works focus on simple special cases (such as linear regression [19, 11] or logistic regression [12]), and only [19] involves a consistency and finite sample analysis.\nIn this work, we consider the problem in its full generality, with the goal of minimizing the expected log-likelihood error over the unlabelled data. We provide a two-stage active learning algorithm for this problem. In the first stage, our algorithm queries the labels of a small number of random samples from the data distribution in order to construct a crude estimate \u03b81 of the optimal parameter \u03b8\u2217. In the second stage, we select a set of samples for label query by optimizing a summary statistic of the covariance matrix of the estimator at \u03b81; however, unlike the experimental design work, our choice of statistic is directly motivated by our goal of minimizing the expected log-likelihood error, which guides us towards the right objective.\nWe provide a finite sample analysis of our algorithm when some regularity conditions hold and when the negative log likelihood function is convex. Our analysis is still fairly general, and applies to Generalized Linear Models, for example. We match our upper bound with a corresponding lower bound, which shows that the convergence rate of our algorithm is optimal (except for lower order terms); the finite sample convergence rate of any algorithm that uses (perhaps multiple rounds of) sample selection and maximum likelihood estimation is either the same or higher than that of our algorithm. This implies that unlike what is observed in learning binary classifiers, a single round of interaction is sufficient to achieve near-optimal log likelihood error for ML estimation."}, {"heading": "1.1 Related Work", "text": "Previous theoretical work on active learning has focussed on learning a classifier belonging to a hypothesis class H in the PAC model. Both the realizable and non-realizable cases have been considered. In the realizable case, a line of work [6, 18] has looked at a generalization of binary search; while their algorithms enjoy low label complexity, this style of algorithms is inconsistent in the presence of noise. The two main styles of algorithms for the non-realizable case are disagreementbased active learning [2, 9, 4], and margin or confidence-based active learning [3, 22]. While active learning in the realizable case has been shown to achieve an exponential improvement in label complexity over passive learning [2, 6, 13], in the agnostic case, the gains are more modest (sometimes a constant factor) [13, 9, 7]. Moreover, lower bounds [14] show that the label requirement of any agnostic active learning algorithm is always at least \u2126(\u03bd2/\u01eb2), where \u03bd is the error of the best hypothesis in the class, and \u01eb is the target error. In contrast, our setting is much more general than binary classification, and includes regression, multi-class classification and certain kinds of conditional random fields that are not covered by previous work.\n[19] provides an active learning algorithm for linear regression problem under model mismatch. Their algorithm attempts to learn the location of the mismatch by fitting increasingly refined partitions of the domain, and then uses this information to reweight the examples. If the partition is highly refined, then the computational complexity of the resulting algorithm may be exponential in the dimension of the data domain. In contrast, our algorithm applies to a more general setting, and while we do not address model mismatch, our algorithm has polynomial time complexity. [1] provides an active learning algorithm for Generalized Linear Models in an online selective sampling setting; however, unlike ours, their input is a stream of unlabelled examples, and at each step, they need to decide whether the label of the current example should be queried.\nOur work is also related to the classical statistical work on optimal experiment design, which\nmostly considers maximum likelihood estimation [5]. For uni-variate estimation, they suggest selecting samples to maximize the Fisher information which corresponds to minimizing the variance of the regression coefficient. When \u03b8 is multi-variate, the Fisher information is a matrix; in this case, there are multiple notions of optimal design which correspond to maximizing different parameters of the Fisher information matrix. For example, D-optimality maximizes the determinant, and A-optimality maximizes the trace of the Fisher information. In contrast with this work, we directly optimize the expected log-likelihood over the unlabelled data which guides us to the appropriate objective function; moreover, we provide consistency and finite sample guarantees.\nFinally, on the empirical side, [12] and [11] derive algorithms similar to ours for logistic and linear regression based on projected gradient descent. Notably, these works provide promising empirical evidence for this approach to active learning; however, no consistency guarantees or convergence rates are provided (the rates presented in these works are not stated in terms of the sample size). In contrast, our algorithm applies more generally, and we provide consistency guarantees and convergence rates. Moreover, unlike [12], our logistic regression algorithm uses a single extra round of interaction, and our results illustrate that a single round is sufficient to achieve a convergence rate that is optimal except for lower order terms."}, {"heading": "2 The Model", "text": "We begin with some notation. We are given a pool U = {x1, . . . , xn} of n unlabelled examples drawn from some instance space X , and the ability to interactively query labels belonging to a label space Y of m of these examples. In addition, we are given a family of models M = {p(y|x, \u03b8), \u03b8 \u2208 \u0398} parameterized by \u03b8 \u2208 \u0398 \u2286 Rd. We assume that there exists an unknown parameter \u03b8\u2217 \u2208 \u0398 such that querying the label of an xi \u2208 U generates a yi drawn from the distribution p(y|xi, \u03b8\u2217). We also abuse notation and use U to denote the uniform distribution over the examples in U .\nWe consider the fixed-design (or transductive) setting, where our goal is to minimize the error on the fixed set of points U . For any x \u2208 X , y \u2208 Y and \u03b8 \u2208 \u0398, we define the negative log-likelihood function L(y|x, \u03b8) as: L(y|x, \u03b8) = \u2212 log p(y|x, \u03b8) Our goal is to find a \u03b8\u0302 to minimize LU (\u03b8\u0302), where\nLU (\u03b8) = EX\u223cU,Y \u223cp(Y |X,\u03b8\u2217)[L(Y |X, \u03b8)]\nby interactively querying labels for a subset of U of size m, where we allow label queries with replacement i.e., the label of an example may be queried multiple times.\nAn additional quantity of interest to us is the Fisher information matrix, or the Hessian of the negative log-likelihood L(y|x, \u03b8) function, which determines the convergence rate. For our active learning procedure to work correctly, we require the following condition.\nCondition 1. For any x \u2208 X , y \u2208 Y, \u03b8 \u2208 \u0398, the Fisher information \u2202 2L(y|x,\u03b8)\n\u2202\u03b82 is a function of only x and \u03b8 (and does not depend on y.)\nCondition 1 is satisfied by a number of models of practical interest; examples include linear regression and generalized linear models. Section 5.1 provides a brief derivation of Condition 1 for generalized linear models.\nFor any x, y and \u03b8, we use I(x, \u03b8) to denote the Hessian \u2202 2L(y|x,\u03b8)\n\u2202\u03b82 ; observe that by Assumption 1, this is just a function of x and \u03b8. Let \u0393 be any distribution over the unlabelled samples in U ; for any \u03b8 \u2208 \u0398, we use:\nI\u0393(\u03b8) = EX\u223c\u0393[I(X, \u03b8)]\nAlgorithm 1 ActiveSetSelect Input: Samples xi, for i = 1, \u00b7 \u00b7 \u00b7 , n 1: Draw m1 samples u.a.r from U , and query their labels to get S1. 2: Use S1 to solve the MLE problem:\n\u03b81 = argmin\u03b8\u2208\u0398 \u2211\n(xi,yi)\u2208S1\nL(yi|xi, \u03b8)\n3: Solve the following SDP (refer Lemma 3):\na\u2217 = argminaTr ( S\u22121IU (\u03b81) ) s.t.    S = \u2211 i aiI(xi, \u03b81) 0 \u2264 ai \u2264 1\u2211\ni ai = m2\n4: Draw m2 examples using probability \u0393 = \u03b1\u03931 + (1 \u2212 \u03b1)U where the distribution \u03931 = a \u2217 i\nm2 and\n\u03b1 = 1\u2212m\u22121/62 . Query their labels to get S2. 5: Use S2 to solve the MLE problem:\n\u03b82 = argmin\u03b8\u2208\u0398 \u2211\n(xi,yi)\u2208S2\nL(yi|xi, \u03b8)\nOutput: \u03b82"}, {"heading": "3 Algorithm", "text": "The main idea behind our algorithm is to sample xi from a well-designed distribution \u0393 over U , query the labels of these samples and perform ML estimation over them. To ensure good performance, \u0393 should be chosen carefully, and our choice of \u0393 is motivated by Lemma 1. Suppose the labels yi are generated according to: yi \u223c p(y|xi, \u03b8\u2217). Lemma 1 states that the expected loglikelihood error of the ML estimate with respect to m samples from \u0393 in this case is essentially Tr ( I\u0393(\u03b8 \u2217)\u22121IU (\u03b8\u2217) ) /m.\nThis suggests selecting \u0393 as the distribution \u0393\u2217 that minimizes Tr ( I\u0393\u2217(\u03b8 \u2217)\u22121IU (\u03b8\u2217) ) . Unfortunately, we cannot do this as \u03b8\u2217 is unknown. We resolve this problem through a two stage algorithm; in the first stage, we use a small number m1 of samples to construct a coarse estimate \u03b81 of \u03b8\n\u2217 (Steps 1-2). In the second stage, we calculate a distribution \u03931 which minimizes Tr ( I\u03931(\u03b81) \u22121IU (\u03b81) ) and draw samples from (a slight modification of) this distribution for a finer estimation of \u03b8\u2217 (Steps 3-5). The distribution \u03931 is modified slightly to \u0393\u0304 (in Step 4) to ensure that I\u0393\u0304(\u03b8\n\u2217) is well conditioned with respect to IU (\u03b8\n\u2217). The algorithm is formally presented in Algorithm 1. Finally, note that Steps 1-2 are necessary because IU and I\u0393 are functions of \u03b8. In certain special cases such as linear regression, IU and I\u0393 are independent of \u03b8. In those cases, Steps 1-2 are unnecessary, and we may skip directly to Step 3."}, {"heading": "4 Performance Guarantees", "text": "The following regularity conditions are essentially a quantified version of the standard Local Asymptotic Normality (LAN) conditions for studying maximum likelihood estimation (see [16, 21]).\nAssumption 1. (Regularity conditions for LAN)\n1. Smoothness: The first three derivatives of L(y|x, \u03b8) exist in all interior points of \u0398 \u2286 Rd. 2. Compactness: \u0398 is compact and \u03b8\u2217 is an interior point of \u0398.\n3. Strong Convexity: IU (\u03b8 \u2217) = 1n \u2211n i=1 I (xi, \u03b8\n\u2217) is positive definite with smallest singular value \u03c3min > 0.\n4. Lipschitz continuity: There exists a neighborhood B of \u03b8\u2217 and a constant L3 such that for all x \u2208 U , I(x, \u03b8) is L3-Lipschitz in this neighborhood.\u2225\u2225\u2225IU (\u03b8\u2217)\u22121/2 (I (x, \u03b8) \u2212 I (x, \u03b8\u2032)) IU (\u03b8\u2217)\u22121/2\n\u2225\u2225\u2225 2 \u2264 L3 \u2016\u03b8 \u2212 \u03b8\u2032\u2016IU (\u03b8\u2217) ,\nfor every \u03b8, \u03b8\u2032 \u2208 B. 5. Concentration at \u03b8\u2217: For any x \u2208 U and y, we have (with probability one),\n\u2016\u2207L(y|x, \u03b8\u2217)\u2016IU (\u03b8\u2217)\u22121 \u2264 L1, and \u2225\u2225\u2225IU (\u03b8\u2217)\u22121/2I (x, \u03b8\u2217) IU (\u03b8\u2217)\u22121/2 \u2225\u2225\u2225 2 \u2264 L2.\n6. Boundedness: max(x,y) sup\u03b8\u2208\u0398 |L(x, y|\u03b8)| \u2264 R. In addition to the above, we need one extra condition which is essentially a pointwise self concordance. This condition is satisfied by a vast class of models, including the generalized linear models.\nAssumption 2. Point-wise self concordance:\n\u2212L4 \u2016\u03b8 \u2212 \u03b8\u2217\u20162 I (x, \u03b8\u2217) I (x, \u03b8)\u2212 I (x, \u03b8\u2217) L4 \u2016\u03b8 \u2212 \u03b8\u2217\u20162 I (x, \u03b8\u2217) . Definition 1. [Optimal Sampling Distribution \u0393\u2217] We define the optimal sampling distribution \u0393\u2217 over the points in U as the distribution \u0393\u2217 = (\u03b3\u22171 , . . . , \u03b3 \u2217 n) for which \u03b3 \u2217 i \u2265 0, \u2211 i \u03b3 \u2217 i = 1, and\nTr ( I\u0393\u2217(\u03b8 \u2217)\u22121IU (\u03b8\u2217) ) is as small as possible.\nDefinition 1 is motivated by Lemma 1, which indicates that under some mild regularity conditions, a ML estimate calculated on samples drawn from \u0393\u2217 will provide the best convergence rates (including the right constant factor) for the expected log-likelihood error.\nWe now present the main result of our paper. The proof of the following theorem and all the supporting lemmas will be presented in Appendix A.\nTheorem 1. Suppose the regularity conditions in Assumptions 1 and 2 hold. Let \u03b2 \u2265 10, and the number of samples used in step (1) be m1 > O ( max ( L2 log 2 d, L21 ( L23 + 1 \u03c3min ) log2 d, diameter(\u0398) Tr(IU (\u03b8\u2217)\u22121) , \u03b22L2 4 \u03b4 Tr ( IU (\u03b8 \u2217)\u22121 ))) . Then with probability \u2265 1 \u2212 \u03b4, the expected log likelihood error of the estimate \u03b82 of Algorithm 1 is bounded as:\nE [LU (\u03b82)]\u2212 LU (\u03b8\u2217) \u2264 ( 1 + 2\n\u03b2 \u2212 1\n)4 (1 + \u01eb\u0303m2)Tr ( I\u0393\u2217(\u03b8 \u2217)\u22121IU (\u03b8 \u2217) ) 1 m2 + R m22 , (1)\nwhere \u0393\u2217 is the optimal sampling distribution in Definition 1 and \u01eb\u0303m2 = O (( L1L3 + \u221a L2 ) \u221a log dm2\nm 1/6 2\n) .\nMoreover, for any sampling distribution \u0393 satisfying I\u0393(\u03b8 \u2217) cIU (\u03b8\u2217) and label constraint of m2, we have the following lower bound on the expected log likelihood error for ML estimate:\nE [ LU (\u03b8\u0302\u0393) ] \u2212 LU (\u03b8\u2217) \u2265 (1\u2212 \u01ebm2)Tr ( I\u0393(\u03b8 \u2217)\u22121IU (\u03b8 \u2217) ) 1 m2 \u2212 L 2 1 cm22 , (2)\nwhere \u01ebm2 def = \u01eb\u0303m2 c2m\n1/3 2\n.\nRemark 1. (Restricting to Maximum Likelihood Estimation) Our restriction to maximum likelihood estimators is minor, as this is close to minimax optimal (see [15]). Minor improvements with certain kinds of estimators, such as the James-Stein estimator, are possible."}, {"heading": "4.1 Discussions", "text": "Several remarks about Theorem 1 are in order. The high probability bound in Theorem 1 is with respect to the samples drawn in S1; provided these samples are representative (which happens with probability \u2265 1 \u2212 \u03b4), the output \u03b82 of Algorithm 1 will satisfy (1). Additionally, Theorem 1 assumes that the labels are sampled with replacement; in other words, we can query the label of a point xi multiple times. Removing this assumption is an avenue for future work.\nSecond, the highest order term in both (1) and (2) is Tr ( I\u0393\u2217(\u03b8 \u2217)\u22121IU (\u03b8\u2217) ) /m. The terms\ninvolving \u01ebm2 and \u01eb\u0303m2 are lower order as both \u01ebm2 and \u01eb\u0303m2 are o(1). Moreover, if \u03b2 = \u03c9(1), then the term involving \u03b2 in (1) is of a lower order as well. Observe that \u03b2 also measures the tradeoff between m1 and m2, and as long as \u03b2 = o( \u221a m2), m1 is also of a lower order than m2. Thus, provided \u03b2 is \u03c9(1) and o( \u221a m2), the convergence rate of our algorithm is optimal except for lower order terms.\nFinally, the lower bound (2) applies to distributions \u0393 for which I\u0393(\u03b8 \u2217) \u2265 cIU (\u03b8\u2217), where c occurs in the lower order terms of the bound. This constraint is not very restrictive, and does not affect the asymptotic rate. Observe that IU (\u03b8\n\u2217) is full rank. If I\u0393(\u03b8\u2217) is not full rank, then the expected log likelihood error of the ML estimate with respect to \u0393 will not be consistent, and thus such a \u0393 will never achieve the optimal rate. If I\u0393(\u03b8\n\u2217) is full rank, then there always exists a c for which I\u0393(\u03b8\n\u2217) \u2265 cIU (\u03b8\u2217). Thus (2) essentially states that for distributions \u0393 where I\u0393(\u03b8\u2217) is close to being rank-deficient, the asymptotic convergence rate of O(Tr ( I\u0393(\u03b8 \u2217)\u22121IU (\u03b8\u2217) ) /m2) is achieved at larger values of m2."}, {"heading": "4.2 Proof Outline", "text": "Our main result relies on the following three steps."}, {"heading": "4.2.1 Bounding the Log-likelihood Error", "text": "First, we characterize the log likelihood error (wrt U) of the empirical risk minimizer (ERM) estimate\nobtained using a sampling distribution \u0393. Concretely, let \u0393 be a distribution on U . Let \u03b8\u0302\u0393 be the ERM estimate using the distribution \u0393:\n\u03b8\u0302\u0393 = argmin\u03b8\u2208\u0398 1\nm2\nm2\u2211\ni=1\nL(Yi|Xi, \u03b8), (3)\nwhere Xi \u223c \u0393 and Yi \u223c p(y|Xi, \u03b8\u2217). The core of our analysis is Lemma 1, which shows a precise estimate of the log likelihood error E [ LU ( \u03b8\u0302\u0393 ) \u2212 LU (\u03b8\u2217) ] .\nLemma 1. Suppose L satisfies the regularity conditions in Assumptions 1 and 2. Let \u0393 be a distribution on U and \u03b8\u0302\u0393 be the ERM estimate (3) using m2 labeled examples. Suppose further that I\u0393(\u03b8\n\u2217) cIU (\u03b8\u2217) for some constant c < 1. Then, for any p \u2265 2 and m2 large enough (depending on p), we have:\n(1\u2212 \u01ebm2) \u03c42 m2 \u2212 L 2 1\ncm p/2 2\n\u2264 E [ LU ( \u03b8\u0302\u0393 ) \u2212 LU (\u03b8\u2217) ] \u2264 (1 + \u01ebm2) \u03c42\nm2 +\nR\nmp2 ,\nwhere \u01ebm2 = O ( 1 c2 ( L1L3 + \u221a L2 )\u221a p log dm2\nm2\n) and \u03c42 def = Tr ( I\u0393(\u03b8 \u2217)\u22121IU (\u03b8\u2217) ) ."}, {"heading": "4.2.2 Approximating \u03b8\u2217", "text": "Lemma 1 motivates sampling from the optimal sampling distribution \u0393\u2217 that minimizes Tr ( I\u0393\u2217(\u03b8 \u2217)\u22121IU (\u03b8\u2217) ) . However, this quantity depends on \u03b8\u2217, which we do not know. To resolve this issue, our algorithm first queries the labels of a small fraction of points (m1) and solves a ML estimation problem to obtain a coarse estimate \u03b81 of \u03b8\n\u2217. How close should \u03b81 be to \u03b8\n\u2217? Our analysis indicates that it is sufficient for \u03b81 to be close enough that for any x, I(x, \u03b81) is a constant factor spectral approximation to I(x, \u03b8\n\u2217); the number of samples needed to achieve this is analyzed in Lemma 2.\nLemma 2. Suppose L satisfies the regularity conditions in Assumptions 1 and 2. If the number of samples used in the first step\nm1 > O  max  L2 log2 d, L21 ( L23 + 1\n\u03c3min\n) log2 d, diameter(\u0398)\nTr ( IU (\u03b8\u2217)\n\u22121 ) , \u03b2 2L24 \u03b4\nTr ( IU (\u03b8 \u2217)\u22121 )     ,\nthen, we have:\n\u2212 1 \u03b2 I (x, \u03b8\u2217) I (x, \u03b81)\u2212 I (x, \u03b8\u2217)\n1 \u03b2 I (x, \u03b8\u2217) \u2200 x \u2208 X\nwith probability greater than 1\u2212 \u03b4."}, {"heading": "4.2.3 Computing \u03931", "text": "Third, we are left with the task of obtaining a distribution \u03931 that minimizes the log likelihood error. We now pose this optimization problem as an SDP.\nFrom Lemmas 1 and 2, it is clear that we should aim to obtain a sampling distribution \u0393 = ( aim2 :\ni \u2208 [n]) minimizing Tr ( I\u0393(\u03b81) \u22121 IU (\u03b81) ) . Let IU (\u03b81) = \u2211 j \u03c3jvjvj \u22a4 be the singular value decomposition (svd) of IU (\u03b81). Since Tr ( I\u0393(\u03b81) \u22121 IU (\u03b81) ) = \u2211d j=1 \u03c3jvj \u22a4I\u0393(\u03b81) \u22121 vj , this is equivalent to solving:\nmin a,c\nd\u2211\nj=1\n\u03c3jcj s.t.   \nS = \u2211\ni aiI(xi, \u03b81) vj\n\u22a4S\u22121vj \u2264 cj ai \u2208 [0, 1]\u2211 i ai = m2.\n(4)\nAmong the above constraints, the constraint vj \u22a4S\u22121vj \u2264 cj seems problematic. However, Schur\ncomplement formula tells us that:\n[ cj vj \u22a4\nvj S\n] 0 \u21d4 S 0 and vj\u22a4S\u22121vj \u2264 cj . In our case, we\nknow that S 0, since it is a sum of positive semi definite matrices. The above argument proves the following lemma.\nLemma 3. The following two optimization programs are equivalent:\nmina Tr ( S\u22121IU (\u03b81) )\ns.t. S = \u2211\ni aiI(xi, \u03b81) ai \u2208 [0, 1]\u2211 i ai = m2.\n\u2261\nmina,c \u2211d\nj=1 \u03c3jcj s.t. S = \u2211 i aiI(xi, \u03b81)[\ncj vj \u22a4 vj S\n] 0\nai \u2208 [0, 1]\u2211 i ai = m2,\nwhere IU (\u03b81) = \u2211 j \u03c3jvjvj \u22a4 denotes the svd of IU (\u03b81)."}, {"heading": "5 Illustrative Examples", "text": "We next present some examples that illustrate Theorem 1. We begin by showing that Condition 1 is satisfied by the popular class of Generalized Linear Models."}, {"heading": "5.1 Derivations for Generalized Linear Models", "text": "A generalized linear model is specified by three parameters \u2013 a linear model, a sufficient statistic, and a member of the exponential family. Let \u03b7 be a linear model: \u03b7 = \u03b8\u22a4X . Then, in a Generalized Linear Model (GLM), Y is drawn from an exponential family distribution with parameter \u03b7. Specifically, p(Y = y|\u03b7) = e\u03b7\u22a4t(y)\u2212A(\u03b7), where t(\u00b7) is the sufficient statistic and A(\u00b7) is the log-partition function. From properties of the exponential family, the log-likelihood is written as log p(y|\u03b7) = \u03b7\u22a4t(y)\u2212A(\u03b7). If we take \u03b7 = \u03b8\u22a4x, and take the derivative with respect to \u03b8, we have: \u2202 log p(y|\u03b8,x)\n\u2202\u03b8 = xt(y) \u2212 xA\u2032(\u03b8\u22a4x). Taking derivatives again gives us \u22022 log p(y|\u03b8,x) \u2202\u03b82 = \u2212xx\u22a4A\u2032\u2032(\u03b8\u22a4x), which is independent of y."}, {"heading": "5.2 Specific Examples", "text": "We next present three illustrative examples of problems that our algorithm may be applied to.\nLinear Regression. Our first example is linear regression. In this case, x \u2208 Rd and Y \u2208 R are generated according to the distribution: Y = \u03b8\u22a4\u2217 X+\u03b7, where \u03b7 is a noise variable drawn fromN (0, 1). In this case, the negative loglikelihood function is: L(y|x, \u03b8) = (y \u2212 \u03b8\u22a4x)2, and the corresponding Fisher information matrix I(x, \u03b8) is given as: I(x, \u03b8) = xx\u22a4. Observe that in this (very special) case, the Fisher information matrix does not depend on \u03b8; as a result we can eliminate the first two steps of the algorithm, and proceed directly to step 3. If \u03a3 = 1n \u2211 i xixi\n\u22a4 is the covariance matrix of U , then Theorem 1 tells us that we need to query labels from a distribution \u0393\u2217 with covariance matrix \u039b such that Tr ( \u039b\u22121\u03a3 ) is minimized.\nWe illustrate the advantages of active learning through a simple example. Suppose U is the unlabelled distribution:\nxi = { e1 w.p. 1\u2212 d\u22121d2 , ej w.p. 1 d2 for j \u2208 {2, \u00b7 \u00b7 \u00b7 , d} ,\nwhere ej is the standard unit vector in the j th direction. The covariance matrix \u03a3 of U is a diagonal matrix with \u03a311 = 1 \u2212 d\u22121d2 and \u03a3jj = 1d2 for j \u2265 2. For passive learning over U , we query labels of examples drawn from U which gives us a convergence rate of\nTr(\u03a3\u22121\u03a3) m = d m . On the other hand,\nactive learning chooses to sample examples from the distribution \u0393\u2217 such that\nxi = { e1 w.p. \u223c 1\u2212 d\u221212d , ej w.p. \u223c 12d for j \u2208 {2, \u00b7 \u00b7 \u00b7 , d} ,\nwhere \u223c indicates that the probabilities hold upto O (\n1 d2\n) . This has a diagonal covariance ma-\ntrix \u039b such that \u039b11 \u223c 1 \u2212 d\u221212d and \u039bjj \u223c 12d for j \u2265 2, and convergence rate of Tr(\u039b\u22121\u03a3)\nm \u223c 1 m ( 2d d+1 \u00b7 ( 1\u2212 d\u22121d2 ) + (d\u2212 1) \u00b7 2d \u00b7 1d2 ) \u2264 4m , which does not grow with d!\nLogistic Regression. Our second example is logistic regression for binary classification. In this case, x \u2208 Rd, Y \u2208 {\u22121, 1} and the negative log-likelihood function is: L(y|x, \u03b8) = log(1 + e\u2212y\u03b8\u22a4x), and the corresponding Fisher information I(x, \u03b8) is given as: I(x, \u03b8) = e \u03b8\u22a4x\n(1+e\u03b8\u22a4x)2 \u00b7 xx\u22a4.\nFor illustration, suppose \u2016\u03b8\u2217\u20162 and \u2016x\u20162 are bounded by a constant and the covariance matrix \u03a3 is sandwiched between two multiples of identity in the PSD ordering i.e., cdI \u03a3 Cd I for\nsome constants c and C. Then the regularity assumptions 1 and 2 are satisfied for constant values of L1, L2, L3 and L4. In this case, Theorem 1 states that choosing m1 to be \u03c9 ( Tr ( IU (\u03b8 \u2217)\u22121 )) = \u03c9 (d)\ngives us the optimal convergence rate of (1 + o(1)) Tr(I\u0393\u2217 (\u03b8\u2217)\u22121IU (\u03b8\u2217))\nm2 .\nMultinomial Logistic Regression. Our third example is multinomial logistic regression for multi-class classification. In this case, Y \u2208 1, . . . ,K, x \u2208 Rd, and the parameter matrix \u03b8 \u2208 R (K\u22121)\u00d7d. The negative log-likelihood function is written as: L(y|x, \u03b8) = \u2212\u03b8\u22a4y x+log(1+ \u2211K\u22121 k=1 e \u03b8\u22a4k x), if y 6= K, and L(y = k|x, \u03b8) = log(1+\u2211K\u22121k=1 e\u03b8 \u22a4\nk x) otherwise. The corresponding Fisher information matrix is a (K\u22121)d\u00d7 (K\u22121)d matrix, which is obtained as follows. Let F be the (K\u22121)\u00d7 (K\u22121) matrix with:\nFii = e\u03b8\n\u22a4 i x(1 + \u2211\nk 6=i e \u03b8\u22a4k x)\n(1 + \u2211\nk e \u03b8\u22a4k x)2\n, Fij = \u2212 e\u03b8\n\u22a4 i x+\u03b8 \u22a4 j x\n(1 + \u2211\nk e \u03b8\u22a4k x)2\nThen, I(x, \u03b8) = F \u2297 xx\u22a4. Similar to the example in the logistic regression case, suppose\n\u2225\u2225\u03b8\u2217y \u2225\u2225 2 and \u2016x\u20162 are bounded\nby a constant and the covariance matrix \u03a3 satisfies cdI \u03a3 Cd I for some constants c and C. Since F \u2217 = diag (p\u2217i ) \u2212 p\u2217p\u2217\u22a4, where p\u2217i = P (y = i|x, \u03b8\u2217), the boundedness of \u2225\u2225\u03b8\u2217y \u2225\u2225 2 and \u2016x\u20162 implies that c\u0303I F \u2217 C\u0303I for some constants c\u0303 and C\u0303 (depending on K). This means that cc\u0303 d I I(x, \u03b8\u2217) CC\u0303d I and so the regularity assumptions 1 and 2 are satisfied with L1, L2, L3 and L4 being constants. Theorem 1 again tells us that using \u03c9(d) samples in the first step gives us the optimal convergence rate of maximum likelihood error."}, {"heading": "6 Conclusion", "text": "In this paper, we provide an active learning algorithm for maximum likelihood estimation which provably achieves the optimal convergence rate (upto lower order terms) and uses only two rounds of interaction. Our algorithm applies in a very general setting, which includes Generalized Linear Models.\nThere are several avenues of future work. Our algorithm involves solving an SDP which is computationally expensive; an open question is whether there is a more efficient, perhaps greedy, algorithm that achieves the same rate. A second open question is whether it is possible to remove the with replacement sampling assumption. A final question is what happens if IU (\u03b8\n\u2217) has a high condition number. In this case, our algorithm will require a large number of samples in the first stage; an open question is whether we can use a more sophisticated procedure in the first stage to reduce the label requirement."}, {"heading": "Acknowledgements", "text": "KC thanks NSF under IIS 1162851 for research support. Part of this work was done when KC was visiting Microsoft Research New England."}, {"heading": "A Proofs", "text": "In order to prove Lemma 1, we use the following result which is a modification of [10]. In particular, the following lemma is a generalization of Theorem 5.1 from [10], and its proof (omitted here) follows from generalizing the proof of that theorem.\nLemma 4. Suppose \u03c81, \u00b7 \u00b7 \u00b7 , \u03c8n : Rd \u2192 R are random functions drawn iid from a distribution. Let P = E [\u03c8i] and Q : R d \u2192 R be another function. Let\n\u03b8\u0302 = argmin\u03b8\u2208S \u2211\ni\n\u03c8i(\u03b8), and \u03b8 \u2217 = argmin\u03b8\u2208SP (\u03b8).\nAssume:\n1. (Convexity of \u03c8): Assume that \u03c8 is convex (with probability one),\n2. (Smoothness of \u03c8): Assume that \u03c8 is smooth in the following sense: the first, second and third derivatives exist at all interior points of S (with probability one),\n3. (Regularity conditions): Suppose\n(a) S is compact, (b) \u03b8\u2217 is an interior point of S, (c) \u22072P (\u03b8\u2217) is positive definite (and hence invertible), (d) \u2207Q(\u03b8\u2217) = 0, (e) There exists a neighborhood B of \u03b8\u2217 and a constant L\u03033 such that (with probability one),\n\u22072\u03c8(\u03b8) and \u22072Q(\u03b8) are L\u03033 Lipschitz, namely \u2225\u2225\u2225 ( \u22072P (\u03b8\u2217) )\u22121/2 (\u22072\u03c8(\u03b8)\u2212\u22072\u03c8(\u03b8\u2032) ) ( \u22072P (\u03b8\u2217) )\u22121/2\u2225\u2225\u2225\n2 \u2264 L\u03033 \u2016\u03b8 \u2212 \u03b8\u2032\u2016\u22072P (\u03b8\u2217) , and\n\u2225\u2225\u2225 ( \u22072Q(\u03b8\u2217) )\u22121/2 (\u22072Q(\u03b8)\u2212\u22072Q(\u03b8\u2032) ) ( \u22072Q(\u03b8\u2217) )\u22121/2\u2225\u2225\u2225\n2 \u2264 L\u03033 \u2016\u03b8 \u2212 \u03b8\u2032\u2016\u22072P (\u03b8\u2217) ,\nfor \u03b8, \u03b8\u2032 \u2208 B, 4. (Concentration at \u03b8\u2217) Suppose \u2016\u2207\u03c8(\u03b8\u2217)\u2016\u22072P (\u03b8\u2217)\u22121 \u2264 L\u03031 and\n\u2225\u2225\u2225 ( \u22072P (\u03b8\u2217) )\u22121/2 \u22072\u03c8(\u03b8\u2217) ( \u22072P (\u03b8\u2217) )\u22121/2\u2225\u2225\u2225 2 \u2264 L\u03032\nhold with probability one.\nChoose p \u2265 2 and define\n\u01ebn def = c\u0303(L\u03031L\u03033 + \u221a L\u03032) \u221a p log dn\nn ,\nwhere c\u0303 is an appropriately chosen constant. Let c\u0303\u2032 be another appropriately chosen constant. If n is large enough so that \u221a\np log dn n \u2264 c\u0303\u2032 min { 1\u221a L\u03032 , 1 L\u03031L\u03033 , diameter(B) L\u03031 } , then:\n(1\u2212 \u01ebn) \u03c42 n \u2212 L\u03031\n2\nnp/2 \u2264 E\n[ Q(\u03b8\u0302)\u2212Q(\u03b8\u2217) ] \u2264 (1 + \u01ebn) \u03c42\nn + max\u03b8\u2208S Q(\u03b8)\u2212Q(\u03b8\u2217) np ,\nwhere\n\u03c42 def =\n1\nn2 Tr\n   \u2211\ni,j\nE [ \u2207\u03c8i(\u03b8\u2217)\u2207\u03c8j(\u03b8\u2217)\u22a4 ]  P (\u03b8\u2217)\u22121Q(\u03b8\u2217)P (\u03b8\u2217)\u22121   .\nThe following lemma is a fundamental result relating the variance of the gradient of the log likelihood to Fisher information matrix for a large class of probability distributions [17].\nLemma 5. Suppose L satisfies the regularity conditionsin Assumptions 1 and 2. Then, for any example x, we have:\nEp(y|x,\u03b8\u2217) [ \u2207L(Y |x, \u03b8\u2217)\u2207L(Y |x, \u03b8\u2217)\u22a4 ] = \u22072Ix(\u03b8\u2217).\nWe now prove Lemma 1.\n(Proof of Lemma 1). We first define\n\u03c8i(\u03b8) = L (Y |X, \u03b8) ,\nwhere X \u223c \u0393 and Y \u223c p(Y |X, \u03b8\u2217) for i = 1, \u00b7 \u00b7 \u00b7 ,m2 and Q(\u03b8) def= LU (\u03b8). Using the notation of Lemma 4, this means that\n\u22072P (\u03b8\u2217) = I\u0393(\u03b8\u2217) and \u22072Q(\u03b8\u2217) = IU (\u03b8\u2217).\nUsing the regularity conditions from Section 4 and the hypothesis that I\u0393(\u03b8 \u2217) cIU (\u03b8\u2217), we see\nthat this satisfies the hypothesis of Lemma 4 with constants\n(L\u03031, L\u03032, L\u03033) = (L1/ \u221a c, L2/c, L3/c 3/2)\nWe now apply Lemma 4 to conclude that for large enough m2, we have:\n(1\u2212 \u01ebm2)\u03c42/m2 \u2212 L21\ncm p/2 2\n\u2264 E [ LU ( \u03b8\u0302 ) \u2212 LU (\u03b8\u2217) ] \u2264 (1 + \u01ebm2)\u03c42/m2 + R\nmp2 ,\nwhere\n\u01ebm2 = O (( L\u03031L\u03033 + \u221a L\u03032 )\u221a p log dm2\nm2\n) = O ( 1\nc2\n( L1L3 + \u221a L2 )\u221ap log dm2 m2 ) and\n\u03c42 def = Tr ( E [ \u2207P\u0302 (\u03b8\u2217)\u2207P\u0302 (\u03b8\u2217)\u22a4 ] I\u0393(\u03b8 \u2217)\u22121IU (\u03b8 \u2217)I\u0393(\u03b8 \u2217)\u22121 ) = Tr ( I\u0393(\u03b8 \u2217)\u22121IU (\u03b8 \u2217) ) ,\nusing Lemma 5 in the last step.\nWe now prove Lemma 2.\n(Proof of Lemma 2). Define\n\u03c8i(\u03b8) def = L (Y |X, \u03b8) ,\nwhere X \u223c U and Y \u223c p(Y |X, \u03b8\u2217) for i = 1, \u00b7 \u00b7 \u00b7 ,m1 and Q(\u03b8) def= \u2016\u03b8 \u2212 \u03b8\u2217\u201622. Using the regularity conditions from Section 4, we see that this satisfies the hypothesis of Lemma 4 with constants\n(L\u03031, L\u03032, L\u03033) = (L1, L2,max ( L3,\n1\u221a \u03c3min\n) ))\nWe now apply Lemma 4 to conclude that\nE [ \u2016\u03b81 \u2212 \u03b8\u2217\u201622 ] \u2264 (1 + \u01ebm1)\u03c42/m1 + diameter(\u0398)\nm21 ,\nwhere \u01ebm1 = O ( (L1 max ( L3, 1\u221a \u03c3min ) + \u221a L2) \u221a log dm1 m1 ) , and\n\u03c42 def = Tr ( E [ \u2207L\u0302U (\u03b8\u2217)\u2207L\u0302U (\u03b8\u2217) \u22a4] IU (\u03b8 \u2217)\u22122 ) = Tr ( IU (\u03b8 \u2217)\u22121 ) ,\nusing Lemma 5 in the last step. By the choice of m1, we have that\nE [ \u2016\u03b81 \u2212 \u03b8\u2217\u201622 ] \u2264 2\u03c42/m1.\nMarkov\u2019s inequality then tells us that with probability at least 1\u2212 \u03b4, we have:\n\u2016\u03b81 \u2212 \u03b8\u2217\u201622 \u2264 2\u03c42 \u03b4m1 \u2264 1 \u03b22L24 .\nUsing Assumption 2 on point-wise self concordancy of I(x, \u03b8) now finishes the proof.\n(Proof of Theorem 1). The proof is a careful combination of Lemmas 1, 2 and 3. Lower Bound: For any \u0393 that satisfies I\u0393(\u03b8 \u2217) cIU (\u03b8\u2217), we can apply Lemma 1 to write:\nE [ LU ( \u03b8\u0302\u0393 ) \u2212 LU (\u03b8\u2217) ] \u2265 (1\u2212 \u01ebm2)\nTr ( I\u0393(\u03b8 \u2217)\u22121IU (\u03b8\u2217) )\nm2 \u2212 L\n2 1\ncm22 .\nThe lower bound follows. Upper Bound: We begin by showing that if Assumptions 1 and 2 are satisfied, then, from Lemma 2, we have that with probability \u2265 1\u2212 \u03b4, it holds that:\n\u03b2 \u2212 1 \u03b2 I(x, \u03b8\u2217) I(x, \u03b81) \u03b2 + 1 \u03b2 I(x, \u03b8\u2217) \u2200 x \u2208 U\nwith probability \u2265 1\u2212 \u03b4. This means that the following hold for distributions \u03931, \u0393\u2217 and U :\n\u03b2 \u2212 1 \u03b2 I\u03931(\u03b8 \u2217) I\u03931(\u03b81) \u03b2 + 1 \u03b2 I\u03931(\u03b8 \u2217), (5) \u03b2 \u2212 1 \u03b2 I\u0393\u2217(\u03b8 \u2217) I\u0393\u2217(\u03b81) \u03b2 + 1 \u03b2 I\u0393\u2217(\u03b8 \u2217), and (6)\n\u03b2 \u2212 1 \u03b2 IU (\u03b8 \u2217) IU (\u03b81) \u03b2 + 1 \u03b2 IU (\u03b8 \u2217). (7)\nSince \u0393 = \u03b1\u03931+(1\u2212\u03b1)U , we have that I\u0393(\u03b8\u2217) \u03b1I\u03931(\u03b8\u2217) which further implies that I\u0393(\u03b8\u2217)\u22121 1 \u03b1I\u03931(\u03b8 \u2217)\u22121. Similarly, since I\u0393(\u03b8 \u2217) (1 \u2212 \u03b1)IU (\u03b8\u2217), we can apply Lemma 1 on \u0393 to get:\nE [LU (\u03b82)\u2212 LU (\u03b8\u2217)] \u2264 (1 + \u01eb\u0302m2) Tr\n( I\u0393(\u03b8 \u2217)\u22121IU (\u03b8\u2217) )\nm2 +\nR m22 \u2264 1 \u03b1 (1 + \u01eb\u0302m2)\nTr ( I\u03931(\u03b8 \u2217)\u22121IU (\u03b8\u2217) )\nm2 +\nR\nm22\n\u2264 (1 + \u01eb\u0303m2) Tr\n( I\u03931(\u03b8 \u2217)\u22121IU (\u03b8\u2217) )\nm2 +\nR\nm22 ,\nwhere \u01eb\u0302m2 , \u01eb\u0303m2 = O ( 1 (1\u2212\u03b1)2 ( L1L3 + \u221a L2 )\u221a log dm2\nm2\n) = O (( L1L3 + \u221a L2 ) \u221a log dm2\nm 1/6 2\n) .\nFrom (5) and (7), the right hand side is at most:\n(1 + \u01eb\u0303m2)( \u03b2 + 1 \u03b2 \u2212 1) 2Tr\n( I\u03931(\u03b81) \u22121IU (\u03b81) )\nm2 +\nR\nm22\nBy definition of \u03931, this is at most:\n(1 + \u01eb\u0303m2)( \u03b2 + 1 \u03b2 \u2212 1) 2Tr\n( I\u0393\u2217(\u03b81) \u22121IU (\u03b81) )\nm2 +\nR\nm22\nFinally, applying (6) and (7), we get that this is at most:\n(1 + \u01eb\u0303m2)( \u03b2 + 1 \u03b2 \u2212 1) 4Tr\n( I\u0393\u2217(\u03b8 \u2217)\u22121IU (\u03b8\u2217) )\nm2 +\nR\nm22\nThe upper bound follows."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "An active learner is given a class of models, a large set of unlabeled examples, and the ability<lb>to interactively query labels of a subset of these examples; the goal of the learner is to learn a<lb>model in the class that fits the data well.<lb>Previous theoretical work has rigorously characterized label complexity of active learning,<lb>but most of this work has focused on the PAC or the agnostic PAC model. In this paper,<lb>we shift our attention to a more general setting \u2013 maximum likelihood estimation. Provided<lb>certain conditions hold on the model class, we provide a two-stage active learning algorithm<lb>for this problem. The conditions we require are fairly general, and cover the widely popular<lb>class of Generalized Linear Models, which in turn, include models for binary and multi-class<lb>classification, regression, and conditional random fields.<lb>We provide an upper bound on the label requirement of our algorithm, and a lower bound<lb>that matches it up to lower order terms. Our analysis shows that unlike binary classification in<lb>the realizable case, just a single extra round of interaction is sufficient to achieve near-optimal<lb>performance in maximum likelihood estimation. On the empirical side, the recent work in [11]<lb>and [12] (on active linear and logistic regression) shows the promise of this approach.", "creator": "LaTeX with hyperref package"}}}