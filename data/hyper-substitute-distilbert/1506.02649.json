{"id": "1506.02649", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Faster SGD Using Sketched Conditioning", "abstract": "too build better graphical method for warming up stochastic smoothing strategies promoting sketching cycles, nor have discovered a powerful tool helping accelerating models for linear linear modeling. we revisit the notion of conditioning for numerical reverse - order methods and suggest the use of sketching methods included constructing a cheap conditioner that attains a corresponding speedup with modification to the stochastic homogeneous dynamics ( sgd ) condition. under our theoretical guarantees assume convexity, readers discuss experimental applicability / approximation method to efficient neural compression, and experimentally demonstrate elastic merits.", "histories": [["v1", "Mon, 8 Jun 2015 15:08:37 GMT  (32kb)", "http://arxiv.org/abs/1506.02649v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["alon gonen", "shai shalev-shwartz"], "accepted": false, "id": "1506.02649"}, "pdf": {"name": "1506.02649.pdf", "metadata": {"source": "CRF", "title": "Faster SGD Using Sketched Conditioning", "authors": ["Alon Gonen", "Shai Shalev-Shwartz"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n02 64\n9v 1\n[ cs\n.N A\n] 8\nJ un"}, {"heading": "1 Introduction", "text": "We consider empirical loss minimization problems of the form:\nmin W\u2208Rp\u00d7n\nL(W ) := 1\nm\nm \u2211\ni=1\n\u2113yi(Wxi) , (1)\nwhere for every i, xi is an n-dimensional vector and \u2113yi is a loss function from R p to R. For example, in multiclass categorization with the logistic-loss, we have that \u2113yi(a) = log (\n\u2211p j=1 exp(aj \u2212 ayi)\n)\n. Later in the paper we will generalize the discussion to the\ncase in which W is the weight matrix of an intermediate layer of a deep neural network. We consider the large data regime, in which m is large. A popular algorithm for this case is Stochastic Gradient Descent (SGD). The basic idea is to initialize W1 to be some matrix, and at each time t to draw an index i \u2208 {1, . . . ,m} uniformly at random from the training sequence S = ((x1, y1), . . . , (xm, ym)), and then update Wt+1 based on the gradient of \u2113yi(Wxi) at W . When performing this update we would like to decrease the value of \u2113yi(Wxi) while bearing in mind that we only look on a single example, and therefore we should not change W too much. This can be formalized by an update of the form\nWt+1 = argmin W\u2208Rp\u00d7n\n1\n2\u03b7 D(W,Wt) + \u2113yi(Wxi) ,\n\u2217School of Computer Science, The Hebrew University, Jerusalem, Isreal \u2020School of Computer Science, The Hebrew University, Jerusalem, Isreal\nwhere D(\u00b7, \u00b7) is some distance measure between matrices and \u03b7, the learning rate, controls the tradeoff between the desire to minimize the function and the desire to stay close to Wt. Since we keep W close to Wt, we can further simplify things by using the first-order approximation of \u2113yi around Wtxi,\n\u2113yi(Wxi) \u2248 \u2113yi(Wtxi) + \u3008W \u2212Wt , (\u2207\u2113yi(Wtxi))x\u22a4i \u3009 ,\nwhere \u2207\u2113yi(Wtxi) \u2208 Rp is the (sub)gradient of \u2113yi at the p-dimensional vector Wtxi (as a column vector), and for two matricesA,B we use the notation \u3008A,B\u3009 = \u2211i,j Ai,jBi,j . Hence, the update becomes\nWt+1 = argmin W\u2208Rp\u00d7n\n1\n2\u03b7 D(W,Wt) + \u2113yi(Wtxi) + \u3008W \u2212Wt , (\u2207\u2113yi(Wtxi))x\u22a4i \u3009 (2)\nEquation (2) defines a family of algorithms, where different instances are derived by specifying the distance measure D. The simplest choice of D is the squared Frobenius norm regularizer, namely,\nD(W,Wt) = \u2016W \u2212Wt\u20162F = \u3008W \u2212Wt,W \u2212Wt\u3009 .\nIt is easy to verify that for this choice of D, the update given in Equation (2) becomes\nWt+1 = Wt \u2212 \u03b7(\u2207\u2113yi(Wtxi))x\u22a4i ,\nwhich is exactly the update rule of SGD. Note that the Frobenius norm distance measure can be rewritten as\nD(W,Wt) = \u3008I , (W \u2212Wt)\u22a4(W \u2212Wt)\u3009\nIn this paper, we consider the family of distance measures of the form\nDA(W,Wt) = \u3008A , (W \u2212Wt)\u22a4(W \u2212Wt)\u3009\nwhere A is a positive definite matrix. For every such choice of A, the update given in Equation (2) becomes\nWt+1 = Wt \u2212 \u03b7(\u2207\u2113yi(Wtxi))(A\u22121xi)\u22a4 . (3)\nWe refer to the matrix A as a conditioning matrix (for a reason that will become clear shortly) and call the resulting algorithm Conditioned SGD.\nHow should we choose the conditioning matrix A? There are two considerations. First, we would like to choose A so that the algorithm will converge to a solution of Equation (1) as fast as possible. Second, we would like that it will be easy to compute both the matrix A and the update rule given in Equation (3).\nWe start with the first consideration. Naturally, the convergence of the Conditioned SGD algorithm depends on the specific problem at hand. However, we can rely on convergence bounds and picks A that minimizes these bounds. Concretely, assuming that each \u2113yi is convex and \u03c1-Lipschitz, denote C = 1 m \u2211m i=1 xix \u22a4 i the correlation\nmatrix of the data, and let W \u2217 be an optimum of Equation (1), then the sub-optimality of the Conditioned SGD algorithm after performing T iterations is upper bounded by\n1\n2\u03b7T DA(W\n\u22c6,W1) + \u03b7\u03c12\n2 tr(A\u22121C) .\nWe still cannot minimize this bound w.r.t. A as we do not know the value of W \u22c6. So, we further upper bound DA(W \u22c6,W1) by considering two possible bounds. Denoting the spectral norm and the trace norm by \u2016 \u00b7 \u2016sp and \u2016 \u00b7 \u2016tr, respectively, we have\n1. DA(W \u22c6,W1) \u2264 \u2016A\u2016sp \u2016(W \u22c6 \u2212W1)\u22a4(W \u22c6 \u2212W1)\u2016tr 2. DA(W \u22c6,W1) \u2264 \u2016A\u2016tr \u2016(W \u22c6 \u2212W1)\u22a4(W \u22c6 \u2212W1)\u2016sp\nInterestingly, for the first possibility above, the optimal A becomes A = I , corresponding to the vanilla SGD algorithm. However, for the second possibility, we show that the optimal A becomes A = C1/2. The ratio between the required number of iterations to achieve \u01eb sub-optimality is\n# iterations for A = I # iterations for A = C1/2 = \u2016(W \u22c6 \u2212W1)\u22a4(W \u22c6 \u2212W1)\u2016tr \u2016C\u2016tr \u2016(W \u22c6 \u2212W1)\u22a4(W \u22c6 \u2212W1)\u2016sp \u2016C1/2\u20162tr\nThe above ratio is always between 1/n and min{n, p}. We argue that in many typical cases the ratio will be greater than 1, meaning that the conditioner A = C1/2 will lead to faster convergence. For example, suppose that the norm of each row of W \u22c6 is order of 1, but the rows are not correlated. Let us also choose W1 = 0 and assume that p = \u0398(n). Then, \u2016(W\n\u22c6\u2212W1)\u22a4(W\u22c6\u2212W1)\u2016tr \u2016(W\u22c6\u2212W1)\u22a4(W\u22c6\u2212W1)\u2016sp is order of n. On the other hand, if the\neigenvalues of C decay fast, then \u2016C\u2016tr\u2016C1/2\u20162tr \u2248 1. Therefore, in such scenarios, using the conditioner A = C1/2 will lead to a factor of n less iterations relatively to vanilla SGD.\nGetting back to the question of how to choose A, the second consideration that we have mentioned is the time required to compute A\u22121 and to apply the update rule given in Equation (3). As we will show later, the time required to compute A\u22121 is less of an issue relatively to the time of applying the update rule at each iteration, so we focus on the latter.\nObserve that the time required to apply Equation (3) is order of (p+n)n. Therefore, if p \u2248 n then we have no significant overhead in applying the conditioner relatively to applying vanilla SGD. If p \u226a n, then the update time is dominated by the time required to compute A\u22121xi. To decrease this time, we propose to use A of the form QBQ\u22a4 + a(I \u2212 QQ\u22a4), where Q \u2208 Rn\u00d7k has orthonormal columns, B \u2208 Rk\u00d7k is invertible and k \u226a n. We use linear sketching techniques (see [23]) for constructing this conditioner efficiently, and therefore we refer to the resulting algorithm as Sketched Conditioned SGD (SCSGD). Intuitively, the sketched conditioner is a combination of the two conditioners A = I and A = C1/2, where the matrix QBQ\u22a4 captures the top eigenvalues of C and the matrix a(I \u2212 QQ\u22a4) deals with the smaller eigenvalues of C. We show that if the eigenvalues of C decay fast enough then SCSGD enjoys similar speedup to the full conditioner A = C1/2. The advantage of using the sketched\nconditioner is that the time required to apply Equation (3) becomes (p+k)n. Therefore, if p \u2265 k then the runtime per iteration of SCSGD and the runtime per iteration of the vanilla SGD are of the same order of magnitude.\nThe rest of the paper is organized as follows. In the next subsection we survey some related work. In Section 2 we describe in detail our conditioning method. Finally, in Section 3 we discuss variants of the method that are applicable to deep learning problems and report some preliminary experiments showing the merits of conditioning for deep learning problems. Due to the lack of space, proofs are omitted and can be found in Appendix A."}, {"heading": "1.1 Related work", "text": "Conditioning is a well established technique in optimization aiming at choosing an \u201cappropriate\u201d coordinate system for the optimization process. For twice differentiable objectives, maybe the most well known approach is Newton\u2019s method which dynamically changes the coordinate system according to the Hessian of the objective around the current solution. There are several problems with utilizing the Hessian. First, in our case, the Hessian matrix is of size (pn)\u00d7 (pn). Hence, it is computationally expensive to compute and invert it. Second, even for convex problems, the Hessian matrix might be meaningless. For example, for linear regression with the absolute loss the Hessian matrix is the zero matrix almost everywhere. Third, when the number of training examples is very large, stochastic methods are preferable and it is not clear how to adapt Newton method to the stochastic case. The crux of the problem is that while it is easy to construct an unbiased, low variance, estimate of the gradient, based on a single example, it is not clear how to construct a good estimate of the Newton\u2019s direction based on a small mini-batch of examples.\nMany approaches have been proposed for speeding up Newton\u2019s method. For example, the R{\u00b7} operator technique [14, 22, 10, 9]. However, these methods are not applicable for the stochastic setting. An obvious way to decrease the storage and computational cost is to only consider the diagonal elements of the Hessian (see [3]). Schraudolph [18] proposed an adaptation of the L-BFGS approach to the online setting, in which at each iteration, the estimation of the inverse of the Hessian is computed based on only the last few noisy gradients. Naturally, this yields a low rank approximation. In [5], the two aforementioned approaches are combined to yield the SGD-QN algorithm. In the same paper, an analysis of second order SGD is described, but with A being always the Hessian matrix at the optimum (which is of course not known). There are various other approximations, see for example [16, 4, 21, 15].\nTo tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian. A somewhat related approach is Amari\u2019s natural gradient descent [1, 2]. See the discussion in [13]. To the best of our knowledge, these methods come with no theoretical guarantees.\nThe aforementioned approaches change the conditioner at each iteration of the algorithm. A general treatment of this approach is described in [11][Section 1.3.1] under the name \u201cVariable Metric\u201d. Maybe the most relevant approach is the Adagrad algorithm [6], which was originally proposed for the online learning setting but can be easily adapted to the stochastic optimization setting. In our notation, the AdaGrad al-\ngorithm uses a (pn) \u00d7 (pn) conditioning matrix that changes along time and has the form,At = \u03b4I+ 1t \u2211t i=1 \u2207t\u2207\u22a4t , where\u2207t = vec(\u2207\u2113yi(Wtxi))x\u22a4i ). There are several advantages of our method relatively to AdaGrad. First, the convergence bound we obtain is better than the convergence bound of AdaGrad. Specifically, while both bounds have the sane dependence on \u2016C1/2\u20162tr, our bound depends on \u2016W \u2217\u20162sp while AdaGrad depends on \u2016W \u2217\u20162F . As we discussed before, there may be a gap of p between \u2016W \u2217\u20162sp and \u2016W \u2217\u20162F . More critically, when using a full conditioner, the storage complexity of our conditioner is n2, while the storage complexity of AdaGrad is (np)2. In addition, the time complexity of applying the update rule is (p+ n)n for our conditioner versus (np)2 for AdaGrad. For this reason, most practical application of AdaGrad relies on a diagonal approximation of At. In contrast, we can use a full conditioner in many practical cases, and even when n is large our sketched conditioner can be applied without a significant increase in the complexity relatively to vanilla SGD. Finally, because we derive our algorithm for the stochastic case (as opposed to the adversarial online optimization setting), and because we bound the component \u2207\u2113y(Wtx) using the Lipschitzness of \u2113y , the conditioner we use is the constant C\u22121/2 along the entire run of the optimization process, and should only be calculated once. In contrast, AdaGrad replaces the conditioner in every iteration."}, {"heading": "2 Conditioning and Sketched Conditioning", "text": "As mentioned previously, the algorithms we consider start with an initial matrix W1 and at each iteration update the matrix according to Equation (3). The following lemma provides an upper bound on the expected sub-optimality of any algorithm of this form.\nLemma 1. Fix a positive definite matrix A \u2208 Rn\u00d7n. Let W \u22c6 be the minimizer of Equation (1), let \u03c3 \u2208 R be such that \u03c3 \u2265 \u2016W \u22c6\u2016sp and denote C = 1m \u2211m i=1 xix \u22a4 i . Assume that for every i, \u2113yi is convex and \u03c1-Lipschitz. Then, if we apply the update rule given in Equation (3) using the conditioner A and denote W\u0304 = 1T \u2211T t=1 Wt, then\nE[L(W\u0304 )\u2212 L(W \u22c6)] \u2264 1 2\u03b7T tr(AW \u22c6\u22a4W \u22c6) + \u03b7\u03c12 2 E [ tr(A\u22121C) ]\n\u2264 \u03c3 2\n2\u03b7T tr(A) +\n\u03b7\u03c12\n2 E [ tr(A\u22121C) ] .\nIn particular, for \u03b7 = \u03c3/(\u03c1 \u221a T ), we obtain\nE[L(W\u0304 )\u2212 L(W \u22c6)] \u2264 \u03c3\u03c1\u221a T (tr(A) + tr(A\u22121C)) .\nThe proof of the above lemma can be obtained by replacing the standard inner product with the inner product induced by A. For completeness, we provide a proof in Appendix A.\nIn Appendix A we show that the conditioner which minimizes the bound given in the above Lemma is A = C1/2. This yields:\nTheorem 1. Following the notation of Lemma 1, assume that we run the meta-algorithm with A = C1/2, then\nE[L(W\u0304 )\u2212 L(W \u22c6)] \u2264 \u03c3\u03c1\u221a T \u00b7 tr(C1/2) ."}, {"heading": "2.1 Sketched Conditioning", "text": "Let k < n and assume that rank(C) \u2265 k. Consider the following family of conditioners:\nA = {A = QBQ\u22a4 + a(I \u2212QQ\u22a4) : Q \u2208 Rn\u00d7k, Q\u22a4Q = I, B \u227b 0 \u2208 Rk\u00d7k, a > 0} (4) Before proceeding, we show that the conditioners in A are indeed positive definite, and give a formula for their inverse.\nLemma 2. Let A = QBQ\u22a4+ a(I\u2212QQ\u22a4) \u2208 A. Then, A \u227b 0 and its inverse is given by\nA\u22121 = QB\u22121Q\u22a4 + a\u22121(I \u2212QQ\u22a4) .\nInformally, every conditioner A \u2208 A is a combination of a low rank conditioner and the identity conditioner. The most appealing property of these conditioners is that we can compute A\u22121x in time O(nk) and therefore the time complexity of calculating the update given in Equation (3) is O(n(p+ k)).\nIn the next subsections we focus on instances of A which are induced by an approximate best rank-k approximation of C. However, for now, we give an analysis for any choice of A \u2208 A.\nTheorem 2. Following the notation of Lemma 1, let A \u2208 A and denote C\u0303 = Q\u22a4CQ. Then, if a = \u221a\ntr(C)\u2212tr(C\u0303) n\u2212k , we have\nE[L(W\u0304 )\u2212L(W \u22c6)] \u2264 \u03c3\u03c1 2 \u221a T \u00b7 ( tr(B) + tr(B\u22121C\u0303) + 2 \u221a (n\u2212 k)(tr(C)\u2212 tr(C\u0303)) ) ."}, {"heading": "2.2 Low-rank conditioning via exact low-rank approximation", "text": "Maybe the most straightforward approach of defining Q and B is by taking the leading eigenvectors of C. Concretely, let C = UDU\u22a4 be the eigenvalue decomposition of C and denote the diagonal elements of D by \u03bb1 \u2265 . . . \u2265 \u03bbn \u2265 0. Recall that for any k \u2264 n, the best rank-k approximation of C is given by Ck = UkDkU\u22a4k , where Uk \u2208 Rn\u00d7k consists of the first k columns of U and Dk is the first k \u00d7 k sub-matrix of D. Denote C\u0303 = Q\u22a4CQ and consider the conditioner A\u0303 which is determined from Equation (4) by setting Q = Uk, B = C\u03031/2, and a as in Theorem 2.\nTheorem 3. Let Q = Uk, B = C\u03031/2 and a as in Theorem 2, and consider the conditioner given in Equation (4). Then,\nE[L(W\u0304 )\u2212 L(W \u22c6)] \u2264 \u03c3\u03c1\u221a T \u00b7 ( tr(C 1/2 k ) + \u221a (n\u2212 k)(tr(C)\u2212 tr(Ck)) ) .\nIn particular, if \u221a (n\u2212 k)(tr(C) \u2212 tr(Ck)) = O(tr(C1/2)), then the obtained bound is of the same order as the bound in Theorem 1.\nWe refer to the condition \u221a (n\u2212 k)(tr(C) \u2212 tr(Ck)) = O(tr(C1/2)) as a fast spectral decay property of the matrix C."}, {"heading": "2.3 Low-rank conditioning via sketching", "text": "The conditioner defined in the previous subsection requires the exact calculation of the matrix C and its eigenvalue decomposition. In this section we describe a faster technique for calculating a sketched conditioner. Before formally describing the sketching technique, let us try to explain the intuition behind it. Figure 1 depicts a set of 1000 (blue) random points in the plane. Suppose that we represent this sequence by a matrix X \u2208 R2\u00d71000. Now we draw a vector \u03c9 \u2208 R1000\u00d71 whose coordinates are N (0, 1) i.i.d. random variables and consider the vector z = X\u03c9. The vector z is simply a random combination of these points. As we can see, z coincides with the strongest direction of the data. More generally, the idea of sketching is that if we take a matrix X \u2208 Rn\u00d7m and multiply it from the right by random matrix \u2126 \u2208 Rm\u00d7r, then with high probability, we preserve the strongest directions of the column space of X . The above intuition is formalized by the following result, which follows from [17] by setting \u01eb = 11.\nLemma 3. Let X \u2208 Rn\u00d7m. Let r = \u0398(k) and let \u2126 \u2208 Rm\u00d7r be a random matrix whose elements are i.i.d. N (0, 1/r) random variables. Let P \u2208 Rn\u00d7r be a matrix whose columns form an orthonormal basis of the column space of X\u2126, let U \u2208 Rr\u00d7k be a matrix whose columns are the top k eigenvectors of the matrix (P\u22a4X)(P\u22a4X)\u22a4, and let Q = PU \u2208 Rn\u00d7k. Then,\nE\u2016QQ\u22a4X \u2212X\u2016F \u2264 2\u2016X \u2212Xk\u2016F . (5)\nLet X \u2208 Rn\u00d7m be a matrix whose columns are the vectors x1, . . . , xm. Based on Lemma 3, we produce a matrix Q \u2208 Rn\u00d7k which satisfies the inequality E[\u2016QQ\u22a4X\u2212\n1See also [23][Lemmas 4.1,4.2]. In particular, the elements of \u2126 can be drawn either according to be i.i.d. N (0, 1/r) or zero-mean \u00b11 random variables. Also, the bounds on the lower dimension in [23] are better in (additive) factor k log k.\nX\u2016F ] \u2264 2\u2016X \u2212Xk\u2016F . Let C\u0303 = Q\u22a4CQ. Our sketched conditioner is determined by the matrix Q and the matrix B = C\u03031/2. As we show in Algorithm 1, we can compute a factored form of the inverse of the conditioner, A\u0303\u22121, in time O(mnk). We turn to\nAlgorithm 1 Sketched Conditioning: Preprocessing\nInput: X \u2208 Rn,m , Parameters: k < n, r \u2208 \u0398(k) Output: Q, B\u22121, a\u22121 that determines a conditioner according to Equation (4) Sample each element of \u2126 \u2208 Rm\u00d7r i.i.d. from N (0, r\u22121) Compute Z = X\u2126 # in time O(mnr) [P,\u223c] = QR(Z) # in time O(r2n) Compute Y = P\u22a4X # in time O(mnr) Compute the SVD: Y = U \u2032\u03a3\u2032V \u2032\u22a4 # in time O(mr2) Compute Q = PU \u2032k # in time O(nrk) Compute C\u0303 = Q\u22a4CQ # in time O(mkn) Compute B\u22121 = C\u0303\u22121/2 # in time O(k3) Compute a\u22121 = \u221a\nn\u2212k tr(C)\u2212tr(C\u0303) # in time O(mn+ k)\ndiscuss the performance of this conditioner. Relying on Lemma 3, we start by relating the trace of C\u0303 = Q\u22a4CQ to the trace of C.\nLemma 4. We have tr(C)\u2212 tr(C\u0303) \u2264 4(tr(C)\u2212 tr(Ck)).\nThe next lemma holds for any choice of Q \u2208 Rn\u00d7k with orthonormal columns.\nLemma 5. Assume that C is of rank at least k. Let Q \u2208 Rn\u00d7k with orthonormal columns and define C\u0303 = Q\u22a4CQ\u22a4, B = C\u03031/2. Then, tr(B) = tr(B\u22121C\u0303) = O(tr(C 1/2 k )).\nCombining the last two lemmas with Theorem 2, we conclude:\nTheorem 4. Consider running SCSGD with the conditioner given in Algorithm 1. Then,\nE[L(W\u0304 )\u2212 L(W \u22c6)] \u2264 O ( \u03c3\u03c1\u221a T \u00b7 ( tr(C 1/2 k ) + \u221a (n\u2212 k)(tr(C)\u2212 tr(Ck)) ) ) .\nIn particular, if the fast spectral decay property holds, i.e., \u221a (n\u2212 k)(tr(C)\u2212 tr(Ck)) = O(tr(C1/2)), then the obtained bound is of the same order as the bound in Theorem 1."}, {"heading": "3 Experiments with Deep Learning", "text": "While our theoretical guarantees were derived for convex problems, the conditioning technique can be adapted for deep learning problems, as we outline below.\nA feedforward deep neural network is a function f that can be written as a composition f = f1 \u25e6 f2 \u25e6 . . . \u25e6 fq, where each fi is called a layer function. Some of the layer functions are predefined, while other layer functions are parameterized by\nweights matrices. Training of a network amounts to optimizing w.r.t. the weights matrices. The most popular layer function with weights is the affine layer (a.k.a. \u201cfully connected\u201d layer). This layer performs the transformation y = Wx+b, where x \u2208 Rn, W \u2208 Rp,n, and b \u2208 Rp. The network is usually trained based on variants of stochastic gradient descent, where the gradient of the objective w.r.t. W is calculated based on the backpropagation algorithm, and has the form \u03b4x\u22a4, where \u03b4 \u2208 Rp.\nTo apply conditioning to an affine layer, instead of the vanilla SGD update W = W \u2212\u03b7\u03b4x\u22a4, we can apply a conditioned update of the form W = W \u2212\u03b7\u03b4(A\u22121x)\u22a4. To calculate A we could go over the entire training data and calculate C = 1m \u2211m i=1 xix \u22a4 i . However, unlike the convex case, now the vectors xi are not constant but depends on weights of previous layers. Therefore, we initialize C = I and update it according to the update rule C = (1 \u2212 \u03bd)C + \u03bdxix\u22a4i . for some \u03bd \u2208 (0, 1). From time to time, we replace the conditioner to be A = C1/2 for the current value of A. In our experiments, we updated the conditioning matrix after each 50s iterations. Note that the process of calculating A = C1/2 can be performed in a different thread, in parallel to the main stochastic gradient descent process, and therefore it causes no slowdown to the main stochastic gradient descent process.\nThe same technique can be applied to convolutional layers (that also have weights), because it is possible to write a convolutional layer as a composition of a transformation called \u201cIm2Col\u201d and a vanilla affine layer. Besides these changes, the rest of the algorithm is the same as in the convex case.\nBelow we describe two experiments in which we have applied conditioning technique to a popular variant of stochastic gradient descent. In particular, we used stochastic gradient descent with a mini-batch of size 64, a learning rate of \u03b7t = 0.01(1 + 0.0001t)\u22123/4, and with Nesterov momentum with parameter 0.9, as described in [20]. To initialize the weights we used the so-called Xavier method, namely, chose each element ofW at random according to a uniform distribution over [\u2212a, a], with a = \u221a\n3/n. We chose these parameters because they are the default in the popular Caffe library (http://caffe.berkeleyvision.org), without attempting to tune them. We conducted experiments with the MNIST dataset [8] and with the Street View House Numbers (SVHN) dataset [12].\nMNIST: We used a variant of the LeNet architecture [7]. The input is images of 28 \u00d7 28 pixels. We apply the following layer functions: Convolution with kernel size of 5 \u00d7 5, without padding, and with 20 output channels. Max-pooling with kernel size of 2 \u00d7 2. Again, a convolutional and pooling layers with the same kernel sizes, this time with 50 output channels. Finally, an affine layer with 500 output channels, followed by a ReLU layer and another affine layer with 10 output channels that forms the prediction. In short, the architecture is: conv 5x5x20, maxpool 2x2, conv 5x5x50, maxpool 2x2, affine 500, relu, affine 10.\nFor training, we used the multiclass log loss function. Figure 2 and Figure 3 show the training and the test errors both w.r.t. the multiclass log loss function and the zeroone loss (where the x-axis corresponds to the number of iterations). In both graphs, we can see that SCSGD enjoys a much faster convergence rate.\nSVHN: In this experiment we used a much smaller network. The input is images of size 32\u00d732 pixels. Using the same terminology as above, the architecture is now: conv 5x5x8, relu, conv 5x5x16, maxpool 2x2, conv 5x5x16, maxpool 2x2, affine 32, relu, affine 32, relu, affine 10, avgpool 4x4. The results are summarized in the graphs of Figure 4 and Figure 5. We again see a superior convergence rate of SCSGD relatively to SGD."}, {"heading": "Acknowledgments", "text": "This work is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."}, {"heading": "A Proofs Omitted from The Text", "text": "Proof. (of Lemma 1) Using the notation from Section 1, denote\n\u2206t = 1\n2 DA(W\n\u22c6,Wt)\u2212 1\n2 DA(W\n\u22c6,Wt+1) .\nAs in the standard proof of SGD (for the Lipschitz case), we consider the progress of Wt towards W \u22c6. Recall that Wt+1 = Wt \u2212 \u03b7\u2207\u2113yit (Wtxit)x\u22a4itA\u22121, where it \u2208 [m] is the random index that is drawn at time t. For simplicity, denote the p \u00d7 n matrix \u2207\u2113yit (Wtxit)x\u22a4it by Gt. Thus, Wt+1 \u2212Wt = GtA\u22121. By standard algebraic manipulations we have\n\u2206t = 1 2 tr(A(W \u22c6 \u2212Wt+1)\u22a4(W \u22c6 \u2212Wt+1))\u2212 1 2 tr(A(W \u22c6 \u2212Wt)\u22a4(W \u22c6 \u2212Wt))\n= tr((W \u22c6 \u2212Wt+1)A(Wt+1 \u2212Wt)\u22a4) + 1 2 tr((Wt+1 \u2212Wt)\u22a4A(Wt+1 \u2212Wt)) = tr((W \u22c6 \u2212Wt+1)A(\u2212\u03b7GtA\u22121)\u22a4) + 1\n2 tr((\u03b7GtA\n\u22121)A(\u03b7GtA \u22121)\u22a4)\n= \u03b7 \u00b7 tr((Wt+1 \u2212W \u22c6)G\u22a4t ) + \u03b72\n2 tr(GtA\n\u22121G\u22a4t )\n= \u03b7 \u00b7 tr(G\u22a4t (Wt+1 \u2212W \u22c6)) + \u03b72\n2 tr(GtA\n\u22121G\u22a4t )\n= \u03b7 \u00b7 tr(G\u22a4t (Wt \u2212W \u22c6)) + \u03b7 \u00b7 tr(G\u22a4t (Wt+1 \u2212Wt)) + \u03b72\n2 tr(GtA\n\u22121G\u22a4t )\n= \u03b7 \u00b7 tr(G\u22a4t (Wt \u2212W \u22c6))\u2212 \u03b72 \u00b7 tr(GtA\u22121G\u22a4t ) + \u03b72\n2 tr(GtA\n\u22121G\u22a4t )\n= \u03b7 \u00b7 tr(G\u22a4t (Wt \u2212W \u22c6))\u2212 \u03b72\n2 tr(GtA\n\u22121G\u22a4t )\n\u2265 \u03b7\u3008Gt,W \u22c6 \u2212Wt+1\u3009 \u2212 \u03c12\u03b72\n2 tr(x\u22a4itA \u22121xit) .\nwhere in the last inequality we used the fact that the loss function is \u03c1-Lipschitz. Summing over t and dividing by \u03b7 we obtain\nT \u2211\nt=1\n\u3008Gt,Wt \u2212W \u22c6\u3009 \u2264 1 2\u03b7 tr(A(W \u22c6 \u2212W1)\u22a4(W \u22c6 \u2212W1)) +\n\u03b7\u03c12\n2 tr(A\u22121\n\u2211\nt\nxitx \u22a4 it) .\nRecall that W1 = 0. Note that the expected value of Gt is the gradient of L at Wt and the expected value of xitx \u22a4 it is C. Taking expectation over the choice of it for all t, dividing by T and relying on the fact that L(Wt)\u2212 L(W \u22c6) \u2264 \u3008\u2207L(Wt),Wt \u2212W \u22c6\u3009, we obtain\nL(W\u0304 )\u2212 L(W \u22c6) \u2264 1 2\u03b7T tr(AW \u22c6\u22a4W \u22c6) + \u03b7\u03c12 2 tr(A\u22121C)\nProof. (of Theorem 1) For simplicity, we assume that C has full rank. If this is not the case, one can add a tiny amount of noise to the instances to make sure that C is of full rank.\nWe would like to optimize tr(A) + tr(A\u22121C) over all positive definite matrics. Since every matrix A \u227b 0 can be written as A = \u03c4M , where M \u227b 0, tr(M) = 1 and \u03c4 = tr(A), an equivalent objective is given by\nmin \u03c4>0 min M\u227b0:\ntr(M)=1\n\u03c32\n2\u03b7T \u03c4 +\n\u03b7\u03c12\n2\u03c4 tr(M\u22121C) . (6)\nThe following lemma characterizes the optimizer.\nLemma 6. Let C \u227b 0. Then,\nmin M\u227b0:\ntr(M)\u22641\ntr(M\u22121C) = (tr(C1/2))2 ,\nand the minimum is attained by M\u22c6 = (tr(C1/2))\u22121 \u00b7 C1/2. Straightforward optimization over \u03c4 yields the value \u03c4 = tr(C1/2). Subtituitng \u03c4 and M in Equation (6) and applying Lemma 1, we conclude the proof of Theorem 1.\nProof. (of Lemma 6) First, it can be seen that M\u22c6 is feasible and attains the claimed minimal value. We complete the proof by showing the following inequality for any feasible A: tr(M\u22121C) \u2265 (tr(C1/2))2 . We claim the following analogue of Fan\u2019s inequality: For any symmetric matrix M \u2208 R\nn\u00d7n, tr(M\u22121C) \u2265 \u3008\u03bb\u2191(M\u22121), \u03bb\u2193(C)\u3009 = \u3008(\u03bb\u2193(M))\u22121, \u03bb\u2193(C)\u3009 ,\nwhere \u2193 and \u2191 are used to represent decreasing and increasing orders, respectively, and for a vector x = (x1, . . . , xn) with positive components, x\u22121 = (1/x1, . . . , 1/xn). The equality is clear so we proceed by proving the inequality. Let M be a n \u00d7 n symmetric matrix. Assume that C =\n\u2211n i=1 \u03bbiuiu \u22a4 i and M = \u2211n i=1 \u00b5iviv \u22a4 i are the\nspectral decompositions of C and M , respectively. Letting \u03b1i,j = \u3008ui, vj\u3009, we have\ntr(M\u22121C) = \u2211\ni,j\n\u03b12i,j\u03bbi/\u00b5j .\nNote that since both v1, . . . , vn and u1, . . . , un form orthonormal bases, the matrix Z \u2208 Rn\u00d7n whose (i, j)-th element is \u03b12i,j is doubly stochastic. So, we have\ntr(M\u22121C) = \u03bb\u22a4Z\u00b5\u22121 .\nViewing the right side as a function of Z , we can apply Birkhoff\u2019s theorem and conclude that the minimum is obtained by a permutation matrix. The claimed inequality follows.\nThus, we next consider the objective\nmin \u00b5\u2208E\nn \u2211\ni=1\n\u03bbi/\u00b5i ,\nwhere E = {\u00b5 \u2208 Rn+ : \u2211n i=1 \u00b5i \u2264 1}. The corresponding Lagrangian2 is\nL(\u00b5;\u03b1) =\nn \u2211\ni=1\n\u03bbi/\u00b5i \u2212 n \u2211\ni=1\n\u03b1i\u00b5i + \u03b1n+1(\nn \u2211\ni=1\n\u00b5i \u2212 1) .\nNext, we compare the differential to zero and rearrange, to obtain\n(\u03bbi/\u00b5 2 i ) n i=1 = (\u03b1n+1 \u2212 \u03b1i)ni=1 .\nBy complementary slackness, \u03b11 = . . . = \u03b1n = 0. Thus,\n\u00b52i = c\u03bb 1/2 i ,\nfor some c > 0. The constraint \u2211 i=1 \u00b5i \u2264 1 implies that c = \u2211 i=1 \u03bb 1/2 i . Substituting the minimizer in the objective, we conclude the proof.\nProof. (of Lemma 2) Since B = EE\u22a4 for some matrix E, it follows that QBQ\u22a4 = QE(QE)\u22a4, thus it is positive semidefinite. The matrix a(I \u2212QQ\u22a4) is clearly positive semidefinite. It remains to show that A is invertible and thus it is positive definite. We have\nAA\u22121 = (QBQ\u22a4 + a(I \u2212QQ\u22a4))(QB\u22121Q\u22a4 + 1 a (I \u2212QQ\u22a4))\n= QBQ\u22a4QB\u22121Q\u22a4 + (I \u2212QQ\u22a4)(I \u2212QQ\u22a4) + 0 + 0 = QQ\u22a4 + I \u2212QQ\u22a4 = I .\nProof. (of Theorem 2) Recall that\nA = QBQ\u22a4 + a(I \u2212QQ\u22a4) .\nAccording to Lemma 1, we need to show that\ntr(A) + tr(A\u22121C) \u2264 tr(B) + tr(B\u22121C\u0303) + 2 \u221a (n\u2212 k)(tr(C)\u2212 tr(C\u0303)) .\nSince the trace is invariant to cyclic permutations, we have\ntr(A) = tr(QBQ\u22a4) + a \u00b7 tr(I \u2212QQ\u22a4) = tr(Q\u22a4QB) + a(n\u2212 k) = tr(B) + a(n\u2212 k) .\n2The strict inequalities \u00b5i > 0 are not allowed, but we can replace them with weak inequalities and let f(\u00b5) = \u221e for any \u00b5 whose one of its components is not greater than zero\nUsing Lemma 2, we obtain\ntr(A\u22121C) = tr(QB\u22121Q\u22a4C) + a\u22121 \u00b7 tr((I \u2212QQ\u22a4)C) = tr(B\u22121Q\u22a4CQ) + a\u22121(tr(C) \u2212 tr(QQ\u22a4C)) = tr(B\u22121C\u0303) + a\u22121(tr(C)\u2212 tr(C\u0303)) .\nSubtituting a = \u221a\ntr(C)\u2212tr(C\u0303) n\u2212k , we complete the proof.\nProof. (of Theorem 3) Note that\nC\u0303 = Q\u22a4CQ = U\u22a4k UDU \u22a4Uk = UkDkUk) = Ck .\nB = C\u03031/2 = C 1/2 k .\nB\u22121C\u0303 = C\u0303\u22121/2C\u0303 = C\u03031/2 = C1/2k .\nInvoking Theorem 2, we obtain the desired bound.\nProof. (of Lemma 4) With a alight abuse of notation, we consider the decomposition C = Ck + Cn\u2212k (here, Cn\u2212k corresponds to the last n\u2212 k eigenvalues rather than to the first n\u2212 k eigenvalues). We need to show that\ntr(C\u0303) \u2265 tr(Ck)\u2212 3tr(Cd\u2212k) .\nLet X\u0304 = 1\u221a m X , where X \u2208 Rn\u00d7m is the matrix whose columns are x1, . . . , xm. Note that C = X\u0304X\u0304\u22a4. Also, since Q satisfies Equation (5) w.r.t. X , it satisfies the same inequality w.r.t. X\u0304 . Let X\u0304 = U\u03a3V \u22a4 be the SVD of X . Note that the same matrix U participates in the SVD (EVD) of the matrix C = X\u0304X\u0304\u22a4, i.e., we have C = UDU\u22a4, where D = \u03a32. Recall that the best rank-k approximation of X\u0304 is UkU \u22a4 k X\u0304 = Uk\u03a3kV \u22a4 k . By assumption,\n\u2016QQ\u22a4X\u0304 \u2212 X\u0304\u20162F \u2264 4\u2016UkU\u22a4k X\u0304 \u2212 X\u0304\u20162F (7)\nNote that\n\u2016X\u0304 \u2212QQ\u22a4X\u0304\u20162F = tr(X\u0304\u22a4X\u0304) + tr(X\u0304\u22a4QQ\u22a4QQ\u22a4X\u0304)\u2212 2tr(X\u0304\u22a4QQ\u22a4X\u0304) = tr(C)\u2212 tr(QQ\u22a4C) = tr(C) \u2212 tr(C\u0303) .\nSimilarly,\n\u2016UkU\u22a4k X\u0304 \u2212 X\u0304\u20162F = tr(C)\u2212 tr(UkU\u22a4k C) = tr(C) \u2212 tr(Ck) .\nThus, Equation (7) implies that\ntr(C)\u2212 tr(C\u0303) \u2264 4(tr(C) \u2212 tr(Ck)) .\nHence, tr(C\u0303) \u2265 4tr(Ck)\u2212 3tr(C) = tr(Ck)\u2212 3tr(Cd\u2212k) .\nProof. (of Lemma 5) First, we note that B = B\u22121C\u0303 = C\u03031/2. Thus, we need to show that tr(C\u03031/2) = O(tr(C1/2k )). Second, we observe that for every positive scalar b, we have\ntr(C\u03031/2) = O(tr(C 1/2 k )) \u21d4 tr(bC\u03031/2) = O(tr(bC 1/2 k )) .\nDenote the k top eigenvalues of C and C\u0303 by \u03bb1, . . . , \u03bbk and \u03bb\u03031, . . . , \u03bb\u0303k , respectively. According to the above observation, we may assume w.l.o.g. that \u03bbi \u2265 1 for all i \u2208 [k] (simply consider b = \u03bb\u22121k ).\nLet X\u0304 = U\u03a3V \u22a4 be the SVD of X\u0304 , where X\u0304 = (1/ \u221a m)X . Since UkU\u22a4k X\u0304 is the\nbest rank-k approximation of X\u0304 , we have\n\u2016UkU\u22a4k X\u0304 \u2212 X\u0304\u20162F \u2264 \u2016QQ\u22a4X\u0304 \u2212 X\u0304\u20162F\nfor all Q \u2208 Rn\u00d7k with orthonormal columns. As in the proof of Lemma 4, this implies that tr(Ck) \u2265 tr(C\u0303) . Therefore,\ntr(C\u03031/2)\u2212 tr(C1/2k ) = k \u2211\ni=1\n(\n\u221a\n\u03bb\u0303i \u2212 \u221a \u03bbi) =\nk \u2211\ni=1\n\u03bb\u0303i \u2212 \u03bbi \u221a\n\u03bb\u0303i + \u221a \u03bbi\n\u2264 k \u2211\ni=1\n\u03bb\u0303i \u2212 \u03bbi\n= tr(C\u0303)\u2212 tr(Ck)) \u2264 0\nwhere the first inequality follows from the assumption that \u03bbi \u2265 1 for all i \u2208 [k]."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Shun-Ichi Amari"], "venue": "Neural computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Adaptive method of realizing natural gradient learning for multilayer perceptrons", "author": ["Shun-Ichi Amari", "Hyeyoung Park", "Kenji Fukumizu"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Improving the convergence of back-propagation learning with second order methods", "author": ["Sue Becker", "Yann Le Cun"], "venue": "In Proceedings of the 1988 connectionist models summer school,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Yoshua Bengio"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Sgd-qn: Careful quasinewton stochastic gradient descent", "author": ["Antoine Bordes", "L\u00e9on Bottou", "Patrick Gallinari"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Deep learning via hessian-free optimization", "author": ["James Martens"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Exact calculation of the product of the hessian matrix of feedforward network error functions and a vector in 0 (n) time", "author": ["Martin F M\u00f8ller"], "venue": "DAIMI Report Series,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Revisiting natural gradient for deep networks", "author": ["Razvan Pascanu", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1301.3584,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Fast exact multiplication by the hessian", "author": ["Barak A Pearlmutter"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["Nicolas L Roux", "Pierre-Antoine Manzagol", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Partial bfgs update and efficient step-length calculation for three-layer neural networks", "author": ["Kazumi Saito", "Ryohei Nakano"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tamas Sarlos"], "venue": "In Foundations of Computer Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["Nicol Schraudolph", "Jin Yu", "Simon G\u00fcnter"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["Nicol N Schraudolph"], "venue": "Neural computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Krylov subspace descent for deep learning", "author": ["Oriol Vinyals", "Daniel Povey"], "venue": "arXiv preprint arXiv:1111.4259,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Backpropagation: Past and future", "author": ["Paul J Werbos"], "venue": "In Neural Networks,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1988}], "referenceMentions": [{"referenceID": 13, "context": "For example, the R{\u00b7} operator technique [14, 22, 10, 9].", "startOffset": 41, "endOffset": 56}, {"referenceID": 21, "context": "For example, the R{\u00b7} operator technique [14, 22, 10, 9].", "startOffset": 41, "endOffset": 56}, {"referenceID": 9, "context": "For example, the R{\u00b7} operator technique [14, 22, 10, 9].", "startOffset": 41, "endOffset": 56}, {"referenceID": 8, "context": "For example, the R{\u00b7} operator technique [14, 22, 10, 9].", "startOffset": 41, "endOffset": 56}, {"referenceID": 2, "context": "An obvious way to decrease the storage and computational cost is to only consider the diagonal elements of the Hessian (see [3]).", "startOffset": 124, "endOffset": 127}, {"referenceID": 17, "context": "Schraudolph [18] proposed an adaptation of the L-BFGS approach to the online setting, in which at each iteration, the estimation of the inverse of the Hessian is computed based on only the last few noisy gradients.", "startOffset": 12, "endOffset": 16}, {"referenceID": 4, "context": "In [5], the two aforementioned approaches are combined to yield the SGD-QN algorithm.", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "There are various other approximations, see for example [16, 4, 21, 15].", "startOffset": 56, "endOffset": 71}, {"referenceID": 3, "context": "There are various other approximations, see for example [16, 4, 21, 15].", "startOffset": 56, "endOffset": 71}, {"referenceID": 20, "context": "There are various other approximations, see for example [16, 4, 21, 15].", "startOffset": 56, "endOffset": 71}, {"referenceID": 14, "context": "There are various other approximations, see for example [16, 4, 21, 15].", "startOffset": 56, "endOffset": 71}, {"referenceID": 18, "context": "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.", "startOffset": 46, "endOffset": 61}, {"referenceID": 8, "context": "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.", "startOffset": 46, "endOffset": 61}, {"referenceID": 20, "context": "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.", "startOffset": 46, "endOffset": 61}, {"referenceID": 12, "context": "To tackle the second problem, several methods [19, 9, 21, 13] rely on different variants of the Gauss-Newton approximation of the Hessian.", "startOffset": 46, "endOffset": 61}, {"referenceID": 0, "context": "A somewhat related approach is Amari\u2019s natural gradient descent [1, 2].", "startOffset": 64, "endOffset": 70}, {"referenceID": 1, "context": "A somewhat related approach is Amari\u2019s natural gradient descent [1, 2].", "startOffset": 64, "endOffset": 70}, {"referenceID": 12, "context": "See the discussion in [13].", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "A general treatment of this approach is described in [11][Section 1.", "startOffset": 53, "endOffset": 57}, {"referenceID": 5, "context": "Maybe the most relevant approach is the Adagrad algorithm [6], which was originally proposed for the online learning setting but can be easily adapted to the stochastic optimization setting.", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": "The above intuition is formalized by the following result, which follows from [17] by setting \u01eb = 11.", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "9, as described in [20].", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "We conducted experiments with the MNIST dataset [8] and with the Street View House Numbers (SVHN) dataset [12].", "startOffset": 48, "endOffset": 51}, {"referenceID": 11, "context": "We conducted experiments with the MNIST dataset [8] and with the Street View House Numbers (SVHN) dataset [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "MNIST: We used a variant of the LeNet architecture [7].", "startOffset": 51, "endOffset": 54}], "year": 2015, "abstractText": "We propose a novel method for speeding up stochastic optimization algorithms via sketching methods, which recently became a powerful tool for accelerating algorithms for numerical linear algebra. We revisit the method of conditioning for accelerating first-order methods and suggest the use of sketching methods for constructing a cheap conditioner that attains a significant speedup with respect to the Stochastic Gradient Descent (SGD) algorithm. While our theoretical guarantees assume convexity, we discuss the applicability of our method to deep neural networks, and experimentally demonstrate its merits.", "creator": "LaTeX with hyperref package"}}}