{"id": "1506.03487", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "From Paraphrase Database to Compositional Paraphrase Model and Back", "abstract": "posterior query database ( ppdb ; ganitkevitch et al., 2013 ) is vastly extensive semantic corpus, consisting of rich corpus of phrase pairs with ( heuristic ) relationship estimates. moreover, he reads little relevant how it can affect estimate used, due which relatively uncertain nature shifting the confidences and remaining necessarily poor coverage. we adapt models to generate the phrase intersection individually computed ppdb to assist parametric semantic models supporting score paraphrase markers more comfortably than the ppdb's correlation scores while simultaneously evaluating its connectivity. researchers allow progressively improved phrase intersections locally low as logical recall embeddings. moreover, we introduce significantly new, horizontally annotated datasets to specify two - string length models. using our paraphrase model trained knowledge loops, infants obtain worst - of - be - press results including standard word structure bigram similarity tasks and achieving strong result on our new modified phrase consistency tasks.", "histories": [["v1", "Wed, 10 Jun 2015 21:29:28 GMT  (35kb)", "https://arxiv.org/abs/1506.03487v1", "2015 TACL paper to Appear. Submitted 1/2015. Accepted 2/2015. Published 6/2015"], ["v2", "Wed, 26 Aug 2015 21:18:00 GMT  (36kb)", "http://arxiv.org/abs/1506.03487v2", "2015 TACL paper updated with an appendix describing new 300 dimensional embeddings. Submitted 1/2015. Accepted 2/2015. Published 6/2015"]], "COMMENTS": "2015 TACL paper to Appear. Submitted 1/2015. Accepted 2/2015. Published 6/2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john wieting", "mohit bansal", "kevin gimpel", "karen livescu", "dan roth"], "accepted": true, "id": "1506.03487"}, "pdf": {"name": "1506.03487.pdf", "metadata": {"source": "CRF", "title": "From Paraphrase Database to Compositional Paraphrase Model and Back", "authors": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu", "Dan Roth"], "emails": ["wieting2@illinois.edu", "danr@illinois.edu", "mbansal@ttic.edu", "kgimpel@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n03 48\n7v 2\n[ cs\n.C L\n] 2\n6 A\nug 2\n01 5"}, {"heading": "1 Introduction", "text": "Paraphrase detection3 is the task of analyzing two segments of text and determining if they have the same meaning despite differences in structure and wording. It is useful for a variety of NLP tasks like question answering (Rinaldi et al., 2003; Fader et al., 2013), semantic parsing (Berant and Liang, 2014), textual entail-\n1We release our datasets, code, and trained models on the authors\u2019 websites.\n2This version differs from the previous one with the inclusion of Appendix A, which contains details about new higher dimensional embeddings we have released. These embeddings achieve human-level performance on SL999 and WS353.\n3See Androutsopoulos and Malakasiotis (2010) for a survey on approaches for detecting paraphrases.\nment (Bosma and Callison-Burch, 2007), and machine translation (Marton et al., 2009).\nOne component of many such systems is a paraphrase table containing pairs of text snippets, usually automatically generated, that have the same meaning. The most recent work in this area is the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), a collection of confidence-rated paraphrases created using the pivoting technique of Bannard and Callison-Burch (2005) over large parallel corpora. The PPDB is a massive resource, containing 220 million paraphrase pairs. It captures many short paraphrases that would be difficult to obtain using any other resource. For example, the pair {we must do our utmost, we must make every effort} has little lexical overlap but is present in PPDB. The PPDB has recently been used for monolingual alignment (Yao et al., 2013), for predicting sentence similarity (Bjerva et al., 2014), and to improve the coverage of FrameNet (Rastogi and Van Durme, 2014).\nThough already effective for multiple NLP tasks, we note some drawbacks of PPDB. The first is lack of coverage: to use the PPDB to compare two phrases, both must be in the database. The second is that PPDB is a nonparametric paraphrase model; the number of parameters (phrase pairs) grows with the size of the dataset used to build it. In practice, it can become unwieldy to work with as the size of the database increases. A third concern is that the confidence estimates in PPDB are a heuristic combination of features, and their quality is unclear.\nWe address these issues in this work by introducing ways to use PPDB to construct parametric paraphrase models. First we show that initial skip-gram word vectors (Mikolov et al., 2013a) can be fine-tuned for the paraphrase task by training on word pairs from PPDB. We call them PARA-\nGRAM word vectors. We find additive composition of PARAGRAM vectors to be a simple but effective way to embed phrases for short-phrase paraphrase tasks. We find improved performance by training a recursive neural network (RNN; Socher et al., 2010) directly on phrase pairs from PPDB.\nWe show that our resulting word and phrase representations are effective on a wide variety of tasks, including two new datasets that we introduce. The first, Annotated-PPDB, contains pairs from PPDB that were scored by human annotators. It can be used to evaluate paraphrase models for short phrases. We use it to show that the phrase embeddings produced by our methods are significantly more indicative of paraphrasability than the original heuristic scoring used by Ganitkevitch et al. (2013). Thus we use the power of PPDB to improve its contents.\nOur second dataset, ML-Paraphrase, is a reannotation of the bigram similarity corpus from Mitchell and Lapata (2010). The task was originally developed to measure semantic similarity of bigrams, but some annotations are not congruent with the functional similarity central to paraphrase relationships. Our re-annotation can be used to assess paraphrasing capability of bigram compositional models. In summary, we make the following contributions:\nProvide new PARAGRAM word vectors, learned using PPDB, that achieve state-of-the-art performance on the SimLex-999 lexical similarity task (Hill et al., 2014b) and lead to improved performance in sentiment analysis.\nProvide ways to use PPDB to embed phrases. We compare additive and RNN composition of PARAGRAM vectors. Both can improve PPDB by reranking the paraphrases in PPDB to improve correlations with human judgments. They can be used as concise parameterizations of PPDB, thereby vastly increasing its coverage. We also perform a qualitative analysis of the differences between additive and RNN composition.\nIntroduce two new datasets. The first contains PPDB phrase pairs and evaluates how well models can measure the quality of short paraphrases. The second is a new annotation of the bigram similarity task in Mitchell and Lapata (2010) that makes it suitable for evaluating bigram paraphrases.\nWe release the new datasets, complete with annotation instructions and raw annotations, as well as our code and the trained models.4"}, {"heading": "2 Related Work", "text": "There is a vast literature on representing words as vectors. The intuition of most methods to create these vectors (or embeddings) is that similar words have similar contexts (Firth, 1957). Earlier models made use of latent semantic analysis (LSA) (Deerwester et al., 1990). Recently, more sophisticated neural models, work originating with (Bengio et al., 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014). These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014).\nPhrase representations can be created from word vectors using compositional models. Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices.\nMore recent work has shown that the extremely efficient neural embeddings of Mikolov et al. (2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al., 2013b). Hashimoto et al. (2014) introduced an alternative word embedding and compositional model based on predicate-argument structures that does well on two simple composition tasks, including the one introduced by Mitchell and Lapata (2010).\nAn alternative approach to composition, used by Socher et al. (2011), is to train a recursive neural network (RNN) whose structure is defined by a binarized parse tree. In particular, they trained their RNN as an unsupervised autoencoder. The RNN captures the latent structure of composition. Recent work has shown that this model struggles in tasks in-\n4available on the authors\u2019 websites\nvolving compositionality (Blacoe and Lapata, 2012; Hashimoto et al., 2014).5 However, we found success using RNNs in a supervised setting, similar to Socher et al. (2014), who used RNNs to learn representations for image descriptions. The objective function we used in this work was motivated by their multimodal objective function for learning joint image-sentence representations.\nLastly, the PPDB has been used along with other resources to learn word embeddings for several tasks, including semantic similarity, language modeling, predicting human judgments, and classification (Yu and Dredze, 2014; Faruqui et al., 2015). Concurrently with our work, it has also been used to construct paraphrase models for short phrases (Yu and Dredze, 2015)."}, {"heading": "3 New Paraphrase Datasets", "text": "We created two novel datasets: (1) AnnotatedPPDB, a subset of phrase pairs from PPDB which are annotated according to how strongly they represent a paraphrase relationship, and (2) MLParaphrase, a re-annotation of the bigram similarity dataset from Mitchell and Lapata (2010), again annotated for strength of paraphrase relationship."}, {"heading": "3.1 Annotated-PPDB", "text": "Our motivation for creating Annotated-PPDB was to establish a way to evaluate compositional paraphrase models on short phrases. Most existing paraphrase tasks focus on words, like SimLex-999 (Hill et al., 2014b), or entire sentences, such as the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). To our knowledge, there are no datasets that focus on the paraphrasability of short phrases. Thus, we created Annotated-PPDB so that researchers can focus on local compositional phenomena and measure the performance of models directly\u2014avoiding the need to do so indirectly in a sentence-level task. Models that have strong performance on Annotated-PPDB can be used to provide more accurate confidence scores for the paraphrases in the PPDB as well as reduce the need for large paraphrase tables altogether.\n5We also replicated this approach and found training to be time-consuming even using low-dimensional word vectors.\nAnnotated-PPDB was created in a multi-step process (outlined below) involving various automatic filtering steps followed by crowdsourced human annotation. One of the aims for our dataset was to collect a variety of paraphrase types\u2014we wanted to include pairs that were non-trivial to recognize as well as those with a range of similarity and length. We focused on phrase pairs with limited lexical overlap to avoid including those with only trivial differences.\nWe started with candidate phrases extracted from the first 10M pairs in the XXL version of the PPDB and then executed the following steps.6 Filter phrases for quality: Only those phrases whose tokens were in our vocabulary were retained.7 Next, all duplicate paraphrase pairs were removed; in PPDB, these are distinct pairs that contain the same two phrases with the order swapped. Filter by lexical overlap: Next, we calculated the word overlap score in each phrase pair and then retained only those pairs that had a score of less than 0.5. By word overlap score, we mean the fraction of tokens in the smaller of the phrases with Levenshtein distance \u2264 1 to a token in the larger of the phrases. This was done to exclude less interesting phrase pairs like \u3008my dad had, my father had\u3009 or \u3008ballistic missiles, of ballistic missiles\u3009 that only differ in a synonym or the addition of a single word. Select range of paraphrasabilities: To balance our dataset with both clear paraphrases and erroneous pairs in PPDB, we sampled 5,000 examples from ten chunks of the first 10M initial phrase pairs where a chunk is defined as 1M phrase pairs. Select range of phrase lengths: We then selected 1,500 phrases from each 5000-example sample that encompassed a wide range of phrase lengths. To do this, we first binned the phrase pairs by their effective size. Let n1 be the number of tokens of length greater than one character in the first phrase and n2 the same for the second phrase. Then the effective size is defined as max(n1, n2). The bins contained pairs of effective size of 3, 4, and 5 or more, and 500\n6Note that the confidence scores for phrase pairs in PPDB are based on a weighted combination of features with weights determined heuristically. The confidence scores were used to place the phrase pairs into their respective sets (S, M, L, XL, XXL, etc.), where each larger set subsumes all smaller ones.\n7Throughout, our vocabulary is defined as the most common 100K word types in English Wikipedia, following tokenization and lowercasing (see \u00a75).\npairs were selected from each bin. This gave us a total of 15,000 phrase pairs.\nPrune to 3,000: 3,000 phrase pairs were then selected randomly from the 15,000 remaining pairs to form an initial dataset, Annotated-PPDB-3K. The phrases were selected so that every phrase in the dataset was unique.\nAnnotate with Mechanical Turk: The dataset was then rated on a scale from 1-5 using Amazon Mechanical Turk, where a score of 5 denoted phrases that are equivalent in a large number of contexts, 3 meant that the phrases had some overlap in meaning, and 1 indicated that the phrases were dissimilar or contradictory in some way (e.g., can not adopt and is able to accept).\nWe only permitted workers whose location was in the United States and who had done at least 1,000 HITS with a 99% acceptance rate. Each example was labeled by 5 annotators and their scores were averaged to produce the final rating. Table 1 shows some statistics of the data. Overall, the annotated data had a mean deviation (MD)8 of 0.80. Table 1 shows that overall, workers found the phrases to be of high quality, as more than two-thirds of the pairs had an average score of at least 3. Also from the Table, we can see that workers had stronger agreement on very low and very high quality pairs and were less certain in the middle of the range.\nPrune to 1,260: To create our final dataset, Annotated-PPDB, we selected 1,260 phrase pairs from the 3,000 annotations. We did this by first binning the phrases into 3 categories: those with scores in the interval [1, 2.5), those with scores in the interval [2.5, 3.5], and those with scores in the interval (3.5, 5]. We took the 420 phrase pairs with the lowest MD in each bin, as these have the most agreement about their label, to form Annotated-PPDB.\nThese 1,260 examples were then randomly split into a development set of 260 examples and a test set of 1,000 examples. The development set had an MD of 0.61 and the test set had an MD of 0.60, indicating the final dataset had pairs of higher agreement than the initial 3,000.\n8MD is similar to standard deviation, but uses absolute value instead of squared value and thus is both more intuitive and less sensitive to outliers."}, {"heading": "3.2 ML-Paraphrase", "text": "Our second newly-annotated dataset, ML-Paraphrase, is based on the bigram similarity task originally introduced by Mitchell and Lapata (2010); we refer to the original annotations as the ML dataset.\nThe ML dataset consists of human similarity ratings for three types of bigrams: adjective-noun (JN), noun-noun (NN), and verb-noun (VN). Through manual inspection, we found that the annotations were not consistent with the notion of similarity central to paraphrase tasks. For instance, television set and television programme were the highest rated phrases in the NN section (based on average annotator score). Similarly, one of the highest ranked JN pairs was older man and elderly woman. This indicates that the annotations reflect topical similarity in addition to capturing functional or definitional similarity.\nTherefore, we had the data re-annotated by two authors of this paper who are native English speakers.9 The bigrams were labeled on a scale from 1- 5 where 5 denotes phrases that are equivalent in a large number of contexts, 3 indicates the phrases are roughly equivalent in a narrow set of contexts, and 1 means the phrases are not at all equivalent in any context. Following annotation, we collapsed the rating scale by merging 4s and 5s together and 1s and 2s together.\nStatistics for the data are shown in Table 2. We show inter-annotator Spearman \u03c1 and Cohen\u2019s \u03ba in columns 2 and 3, indicating substantial agreement on the JN and VN portions but only moderate agreement on NN. In fact, when evaluating our NN anno-\n9We tried using Mechanical Turk here, but due to such short phrases, with few having the paraphrase relationship, workers did not perform well on the task.\ntations against those from the original ML data (column 4), we find \u03c1 to be 0.38, well below the average human correlation of 0.49 (final column) reported by Mitchell and Lapata and also surpassed by pointwise multiplication (Mitchell and Lapata, 2010). This suggests that the original NN portion, more so than the others, favored a notion of similarity more related to association than paraphrase."}, {"heading": "4 Paraphrase Models", "text": "We now present parametric paraphrase models and discuss training. Our goal is to embed phrases into a low-dimensional space such that cosine similarity in the space corresponds to the strength of the paraphrase relationship between phrases.\nWe use a recursive neural network (RNN) similar to that used by Socher et al. (2014). We first use a constituent parser to obtain a binarized parse of a phrase. For phrase p, we compute its vector g(p) through recursive computation on the parse. That is, if phrase p is the yield of a parent node in a parse tree, and phrases c1 and c2 are the yields of its two child nodes, we define g(p) recursively as follows:\ng(p) = f(W [g(c1); g(c2)] + b)\nwhere f is an element-wise activation function (tanh), [g(c1); g(c2)] \u2208 R2n is the concatenation of the child vectors, W \u2208 Rn\u00d72n is the composition matrix, b \u2208 Rn is the offset, and n is the dimensionality of the word embeddings. If node p has no children (i.e., it is a single token), we define g(p) = W (p) w , where Ww is the word embedding matrix in which particular word vectors are indexed using superscripts. The trainable parameters of the model are W , b, and Ww."}, {"heading": "4.1 Objective Functions", "text": "We now present objective functions for training on pairs extracted from PPDB. The training data consists of (possibly noisy) pairs taken directly from the original PPDB. In subsequent sections, we discuss how we extract training pairs for particular tasks.\nWe assume our training data consists of a set X of phrase pairs \u3008x1, x2\u3009, where x1 and x2 are assumed to be paraphrases. To learn the model parameters (W, b,Ww), we minimize our objective function over the data using AdaGrad (Duchi et al., 2011) with mini-batches. The objective function follows:\nmin W,b,Ww\n1\n|X|\n(\n\u2211\n\u3008x1,x2\u3009\u2208X\nmax(0, \u03b4 \u2212 g(x1) \u00b7 g(x2) + g(x1) \u00b7 g(t1))\n+ max(0, \u03b4 \u2212 g(x1) \u00b7 g(x2) + g(x2) \u00b7 g(t2))\n)\n+ \u03bbW (\u2016W\u2016 2 + \u2016b\u20162) + \u03bbWw \u2016Wwinitial \u2212Ww\u2016 2\n(1)\nwhere \u03bbW and \u03bbWw are regularization parameters, Wwinitial is the initial word embedding matrix, \u03b4 is the margin (set to 1 in all of our experiments), and t1 and t2 are carefully-selected negative examples taken from a mini-batch during optimization.\nThe intuition for this objective is that we want the two phrases to be more similar to each other (g(x1) \u00b7 g(x2)) than either is to their respective negative examples t1 and t2, by a margin of at least \u03b4.\nSelecting Negative Examples To select t1 and t2 in Eq. 1, we simply chose the most similar phrase in the mini-batch (other than those in the given phrase pair). E.g., for choosing t1 for a given \u3008x1, x2\u3009:\nt1 = argmax t:\u3008t,\u00b7\u3009\u2208Xb\\{\u3008x1,x2\u3009} g(x1) \u00b7 g(t)\nwhere Xb \u2286 X is the current mini-batch. That is, we want to choose a negative example ti that is similar to xi according to the current model parameters. The downside of this approach is that we may occasionally choose a phrase ti that is actually a true paraphrase of xi. We also tried a strategy in which we selected the least similar phrase that would trigger an update (i.e., g(ti) \u00b7g(xi) > g(x1) \u00b7g(x2)\u2212\u03b4), but we found the simpler strategy above to work better and used it for all experiments reported below.\nDiscussion The objective in Eq. 1 is similar to one used by Socher et al. (2014), but with several differences. Their objective compared text and projected images. They also did not update the underlying word embeddings; we do so here, and in a way such that they are penalized from deviating from their initialization. Also for a given \u3008x1, x2\u3009, they do not select a single t1 and t2 as we do, but use the entire training set, which can be very expensive with a large training dataset.\nWe also experimented with a simpler objective that sought to directly minimize the squared L2norm between g(x1) and g(x2) in each pair, along with the same regularization terms as in Eq. 1. One problem with this objective function is that the global minimum is 0 and is achieved simply by driving the parameters to 0. We obtained much better results using the objective in Eq. 1.\nTraining Word Paraphrase Models To train just word vectors on word paraphrase pairs (again from PPDB), we used the same objective function as above, but simply dropped the composition terms. This gave us an objective that bears some similarity to the skip-gram objective with negative sampling in word2vec (Mikolov et al., 2013a). Both seek to maximize the dot products of certain word pairs while minimizing the dot products of others. This objective function is:\nmin Ww\n1\n|X|\n(\n\u2211\n\u3008x1,x2\u3009\u2208X\nmax(0, \u03b4 \u2212W (x1)w \u00b7W (x2) w\n+W (x1)w \u00b7W (t1) w ) + max(0, \u03b4 \u2212W (x1) w \u00b7W (x2) w +\nW (x2)w \u00b7W (t2) w )\n)\n+ \u03bbWw \u2016Wwinitial \u2212Ww\u2016 2 (2)\nIt is like Eq. 1 except with word vectors replacing the RNN composition function and with the regularization terms on the W and b removed.\nWe further found we could improve this model by incorporating constraints. From our training pairs, for a given word w, we assembled all other words that were paired with it in PPDB and all of their lemmas. These were then used as constraints during the pairing process: a word t could only be paired with w if it was not in its list of assembled words."}, {"heading": "5 Experiments \u2013 Word Paraphrasing", "text": "We first present experiments on learning lexical paraphrasability. We train on word pairs from PPDB and evaluate on the SimLex-999 dataset (Hill et al., 2014b), achieving the best results reported to date."}, {"heading": "5.1 Training Procedure", "text": "To learn word vectors that reflect paraphrasability, we optimized Eq. 2. There are many tunable hyperparameters with this objective, so to make training tractable we fixed the initial learning rates for the word embeddings to 0.5 and the margin \u03b4 to 1. Then we did a coarse grid search over a parameter space for \u03bbWw and the mini-batch size. We considered \u03bbWw values in {10\n\u22122, 10\u22123, ..., 10\u22127, 0} and minibatch sizes in {100, 250, 500, 1000}. We trained for 20 epochs for each set of hyperparameters using AdaGrad (Duchi et al., 2011).\nFor all experiments, we initialized our word vectors with skip-gram vectors trained using word2vec (Mikolov et al., 2013a). The vectors were trained on English Wikipedia (tokenized and lowercased, yielding 1.8B tokens).10 We used a window size of 5 and a minimum count cut-off of 60, producing vectors for approximately 270K word types. We retained vectors for only the 100K most frequent words, averaging the rest to obtain a single vector for unknown words. We will refer to this set of the 100K most frequent words as our vocabulary."}, {"heading": "5.2 Extracting Training Data", "text": "For training, we extracted word pairs from the lexical XL section of PPDB. We used the XL data for all experiments, including those for phrases. We used XL instead of XXL because XL has better quality overall while still being large enough so that we could be selective in choosing training pairs. There are a total of 548,085 pairs. We removed 174,766 that either contained numerical digits or words not in our vocabulary. We then removed 260,425 redundant pairs, leaving us with a final training set of 112,894 word pairs.\n10We used the December 2, 2013 snapshot."}, {"heading": "5.3 Tuning and Evaluation", "text": "Hyperparameters were tuned using the wordsim-353 (WS353) dataset (Finkelstein et al., 2001), specifically its similarity (WS-S) and relatedness (WSR) partitions (Agirre et al., 2009). In particular, we tuned to maximize 2\u00d7WS-S correlation minus the WS-R correlation. The idea was to reward vectors with high similarity and relatively low relatedness, in order to target the paraphrase relationship.\nAfter tuning, we evaluated the best hyperparameters on the SimLex-999 (SL999) dataset (Hill et al., 2014b). We chose SL999 as our primary test set as it most closely evaluates the paraphrase relationship. Even though WS-S is a close approximation to this relationship, it does not include pairs that are merely associated and assigned low scores, which SL999 does (see discussion in Hill et al., 2014b).\nNote that for all experiments we used cosine similarity as our similarity metric and evaluated the statistical significance of dependent correlations using the one-tailed method of (Steiger, 1980)."}, {"heading": "5.4 Results", "text": "Table 3 shows results on SL999 when improving the initial word vectors by training on word pairs from PPDB, both with and without constraints. The \u201cPARAGRAM WS\u201d rows show results when tuning to maximize 2\u00d7WS-S \u2212 WS-R. We also show results for strong skip-gram baselines and the best results from the literature, including the state-of-the-art results from Hill et al. (2014a) as well as the inter-\nannotator agreement from Hill et al. (2014b).11\nThe table illustrates that, by training on PPDB, we can surpass the previous best correlations on SL999 by 4-6% absolute, achieving the best results reported to date. We also find that we can train low-dimensional word vectors that exceed the performance of much larger vectors. This is very useful as using large vectors can increase both time and memory consumption in NLP applications.\nTo generate word vectors to use for downstream applications, we chose hyperparameters so as to maximize performance on SL999.12 These word vectors, which we refer to as PARAGRAM vectors, had a \u03c1 of 0.57 on SL999. We use them as initial word vectors for the remainder of the paper."}, {"heading": "5.5 Sentiment Analysis", "text": "As an extrinsic evaluation of our PARAGRAM word vectors, we used them in a convolutional neural network (CNN) for sentiment analysis. We used the simple CNN from Kim (2014) and the binary sentence-level sentiment analysis task from Socher et al. (2013). We used the standard data splits, removing examples with a neutral rating. We trained on all constituents in the training set while only using full sentences from development and test, giving us train/development/test sizes of 67,349/872/1,821.\nThe CNN uses m-gram filters, each of which is an m\u00d7n vector. The CNN computes the inner product between an m-gram filter and each m-gram in an example, retaining the maximum match (so-called \u201cmax-pooling\u201d). The score of the match is a single dimension in a feature vector for the example, which is then associated with a weight in a linear classifier used to predict positive or negative sentiment.\nWhile Kim (2014) used m-gram filters of several lengths, we only used unigram filters. We also fixed the word vectors during learning (called \u201cstatic\u201d by Kim). After learning, the unigram filters correspond to locations in the fixed word vector space. The learned classifier weights represent how strongly each location corresponds to positive or negative sentiment. We expect this static CNN to\n11Hill et al. (2014a) did not report the dimensionality of the vectors that led to their state-of-the-art results.\n12We did not use constraints during training.\nbe more effective if the word vector space separates positive and negative sentiment.\nIn our experiments, we compared baseline skipgram embeddings to our PARAGRAM vectors. We used AdaGrad learning rate of 0.1, mini-batches of size 10, and a dropout rate of 0.5. We used 200 unigram filters and rectified linear units as the activation (applied to the filter output + filter bias). We trained for 30 epochs, predicting labels on the development set after each set of 3,000 examples. We recorded the highest development accuracy and used those parameters to predict labels on the test set.\nResults are shown in Table 4. We see improvements over the baselines when using PARAGRAM vectors, even exceeding the performance of higherdimensional skip-gram vectors."}, {"heading": "6 Experiments \u2013 Compositional Paraphrasing", "text": "In this section, we describe experiments on a variety of compositional phrase-based paraphrasing tasks. We start with the simplest case of bigrams, and then proceed to short phrases. For all tasks, we again train on appropriate data from PPDB and test on various evaluation datasets, including our two novel datasets (Annotated-PPDB and ML-Paraphrase)."}, {"heading": "6.1 Training Procedure", "text": "We trained our models by optimizing Eq. 1 using AdaGrad (Duchi et al., 2011). We fixed the initial learning rates to 0.5 for the word embeddings and 0.05 for the composition parameters, and the margin to 1. Then we did a coarse grid search over a parameter space for \u03bbWw , \u03bbW , and mini-batch size.\nFor \u03bbWw , our search space again consisted of {10\u22122, 10\u22123, ..., 10\u22127, 0}, for \u03bbW it was {10\u22121, 10\u22122, 10\u22123, 0}, and we explored batch sizes of {100, 250, 500, 1000, 2000}. When initializing with PARAGRAM vectors, the search space for \u03bbWw was shifted upwards to be\n{10, 1, 10\u22121 , 10\u22123, ..., 10\u22126} to reflect our increased confidence in the initial vectors. We trained only for 5 epochs for each set of parameters. For baselines, we used the same initial skip-gram vectors as in Section 5."}, {"heading": "6.2 Evaluation and Baselines", "text": "For all experiments, we again used cosine similarity as our similarity metric and evaluated the statistical significance using the method of (Steiger, 1980).\nA baseline used in all compositional experiments is vector addition of skip-gram (or PARAGRAM) word vectors. Unlike explicit word vectors, where point-wise multiplication acts as a conjunction of features and performs well on composition tasks (Mitchell and Lapata, 2008), using addition with skip-gram vectors (Mikolov et al., 2013b) gives better performance than multiplication."}, {"heading": "6.3 Bigram Paraphrasability", "text": "To evaluate our ability to paraphrase bigrams, we consider the original bigram similarity task from Mitchell and Lapata (2010) as well as our newlyannotated version of it: ML-Paraphrase.\nExtracting Training Data Training data for these tasks was extracted from the XL portion of PPDB. The bigram similarity task from Mitchell and Lapata (2010) contains three types of bigrams: adjective-noun (JN), noun-noun (NN), and verb-noun (VN). We aimed to collect pairs from PPDB that mirrored these three types of bigrams.\nWe found parsing to be unreliable on such short segments of text, so we used a POS tagger (Manning et al., 2014) to tag the tokens in each phrase. We then used the word alignments in PPDB to extract bigrams for training. For JN and NN, we extracted pairs containing aligned, adjacent tokens in the two phrases with the appropriate partof-speech tag. Thus we extracted pairs like \u3008easy job, simple task\u3009 for the JN section and \u3008town meeting, town council\u3009 for the NN section. We used a different strategy for extracting training data for the VN subset: we took aligned VN tokens and took the closest noun after the verb. This was done to approximate the direct object that would have been ideally extracted with a dependency parse. An example from this section is \u3008achieve goal, achieve aim\u3009.\nWe removed phrase pairs that (1) contained words not in our vocabulary, (2) were redundant with others, (3) contained brackets, or (4) had Levenshtein distance \u2264 1. The final criterion helps to ensure that we train on phrase pairs with non-trivial differences. The final training data consisted of 133,997 JN pairs, 62,640 VN pairs and 35,601 NN pairs.\nBaselines In addition to RNN models, we report baselines that use vector addition as the composition function, both with our skip-gram embeddings and PARAGRAM embeddings from Section 5.\nWe also compare to several results from prior work. When doing so, we took their best correlations for each data subset. That is, the JN and NN results from Mitchell and Lapata (2010) use their multiplicative model and the VN results use their dilation model. From Hashimoto et al. (2014) we used their PAS-CLBLM Addl and PAS-CLBLM Addnl models. We note that their vector dimensionalities are larger than ours, using n = 2000 and 50 respectively.\nResults Results are shown in Table 5. We report results on the test portion of the original Mitchell and Lapata (2010) dataset (ML) as well as the entirety of our newly-annotated dataset (MLParaphrase). RNN results on ML were tuned on the respective development sections and RNN results on ML-Paraphrase were tuned on the entire ML dataset.\nOur RNN model outperforms results from the literature on most sections in both datasets and its average correlations are among the highest.13 The one\n13The results obtained here differ from those reported in Hashimoto et al. (2014) as we scored their vectors with a newer Python implementation of Spearman \u03c1 that handles ties (Hashimoto, P.C.).\nsubset of the data that posed difficulty was the NN section of the ML dataset. We suspect this is due to the reasons discussed in Section 3.2; for our MLParaphrase dataset, by contrast, we do see gains on the NN section.\nWe also outperform the strong baseline of adding 1000-dimensional skip-gram embeddings, a model with 40 times the number of parameters, on our MLParaphrase dataset. This baseline had correlations of 0.45, 0.43, and 0.47 on the JN, NN, and VN partitions, with an average of 0.45\u2014below the average \u03c1 of the RNN (0.52) and even the {PARAGRAM, +} model (0.46).\nInterestingly, the type of vectors used to initialize the RNN has a significant effect on performance. If we initialize using the 25-dimensional skip-gram vectors, the average \u03c1 on ML-Paraphrase drops to 0.43, below even the {PARAGRAM, +} model."}, {"heading": "6.4 Phrase Paraphrasability", "text": "In this section we show that by training a model based on filtered phrase pairs in PPDB, we can actually distinguish between quality paraphrases and poor paraphrases in PPDB better than the original heuristic scoring scheme from Ganitkevitch et al. (2013).\nExtracting Training Data As before, training data was extracted from the XL section of PPDB. Similar to the procedure to create our AnnotatedPPDB dataset, phrases were filtered such that only those with a word overlap score of less than 0.5 were kept. We also removed redundant phrases and phrases that contained tokens not in our vocabulary. The phrases were then binned according to their effective size and 20,000 examples were selected from\nbins of effective sizes of 3, 4, and more than 5, creating a training set of 60,000 examples. Care was taken to ensure that none of our training pairs was also present in our development and test sets.\nBaselines We compare our models with strong lexical baselines. The first, strict word overlap, is the percentage of words in the smaller phrase that are also in the larger phrase. We also include a version where the words are lemmatized prior to the calculation.\nWe also train a support vector regression model (epsilon-SVR) (Chang and Lin, 2011) on the 33 features that are included for each phrase pair in PPDB. We scaled the features such that each lies in the interval [\u22121, 1] and tuned the parameters using 5-fold cross validation on our dev set.14 We then trained on the entire dev set after finding the best performing C and \u01eb combination and evaluated on the test set of Annotated-PPDB.\nResults We evaluated on our Annotated-PPDB dataset described in \u00a73.1. Table 6 shows the Spearman correlations on the 1000-example test set. RNN models were tuned on the development set of 260 examples. All other methods had no hyperparameters and therefore required no tuning.\nWe note that the confidence estimates from Ganitkevitch et al. (2013) reach a \u03c1 of 0.25 on the test set, similar to the results of strict overlap. While 25-dimensional skip-gram embeddings only reach 0.20, we can improve this to 0.32 by fine-tuning them using PPDB (thereby obtaining our PARA-\n14We tuned both parameters over {2\u221210, 2\u22129, ..., 210}.\nGRAM vectors). By using the PARAGRAM vectors to initialize the RNN, we reach a correlation of 0.40, which is better than the PPDB confidence estimates by 15% absolute.\nWe again consider addition of 1000-dimensional skip-gram embeddings as a baseline, and they continue to perform strongly (\u03c1 = 0.37). The RNN initialized with PARAGRAM vectors does reach a higher \u03c1 (0.40), but the difference is not statistically significant (p = 0.16). Thus we can achieve similarlystrong results with far fewer parameters.\nThis task also illustrates the importance of initializing our RNN model with appropriate word embeddings. An RNN initialized with skip-gram vectors has a modest \u03c1 of 0.22, well below the \u03c1 of the RNN initialized with PARAGRAM vectors. Clearly, initialization is important when optimizing non-convex objectives like ours, but it is noteworthy that our best results came from first improving the word vectors and then learning the composition model, rather than jointly learning both from scratch."}, {"heading": "7 Qualitative Analysis", "text": "We performed a qualitative analysis to uncover sources of error and determine differences between adding PARAGRAM vectors and using an RNN initialized with them. To do so, we took the output of both systems on Annotated-PPDB and mapped their cosine similarities to the interval [1, 5]. We then computed their absolute error as compared to the gold ratings.\nTable 7 shows how the average of these absolute errors changes with the magnitude of the gold ratings. The RNN performs better (has lower average absolute error) for less similar pairs. Vector addition only does better on the most similar pairs. This is presumably because the most positive pairs have high word overlap and so can be represented effectively with a simpler model.\nTo further investigate the differences between these models, we removed those pairs with gold scores in [2, 4], in order to focus on pairs with extreme scores. We identified two factors that distinguished the performance between the two models: length ratio and the amount of lexical overlap. We did not find evidence that non-compositional phrases, such as idioms, were a source of error as these were not found in ML-Paraphrase and only appear rarely in Annotated-PPDB.\nWe define length ratio as simply the number of tokens in the smaller phrase divided by the number of tokens in the larger phrase. Overlap ratio is the number of equivalent tokens in the phrase pair divided by the number of tokens in the smaller of the two phrases. Equivalent tokens are defined as tokens that are either exact matches or are paired up in the lexical portion of PPDB used to train the PARAGRAM vectors.\nTable 9 shows how the performance of the models changes under different values of length ratio and overlap ratio.15 The values in this table are the percentage changes in absolute error when using the RNN over the PARAGRAM vector addition model. So negative values indicate superior performance by the RNN.\nA few trends emerge from this table. One is that as the length ratio increases (i.e., the phrase pairs are closer in length), addition surpasses the RNN for positive examples. For negative examples, the trend is reversed. The same trend appears for over-\n15The bin delimiters were chosen to be uniform over the range of output values of the length ratio ([0.4,1] with one outlier data point removed) and overlap ratio ([0,1]).\nlap ratio. Examples from Annotated-PPDB illustrating these trends on positive examples are shown in Table 8.\nWhen considering both positive and negative examples (\u201cBoth\u201d), we see that the RNN excels on the most difficult examples (large differences in phrase length and less lexical overlap). For easier examples, the two fare similarly overall (-2.0 to 0.0% change), but the RNN does much better on negative examples. This aligns with the intuition that addition should perform well when two paraphrastic phrases have high lexical overlap and similar length. But when they are not paraphrases, simple addition is misled and the RNN\u2019s learned composition function better captures the relationship. This may suggest new architectures for modeling compositionality differently depending on differences in length and amount of overlap."}, {"heading": "8 Conclusion", "text": "We have shown how to leverage PPDB to learn state-of-the-art word embeddings and compositional models for paraphrase tasks. Since PPDB was created automatically from parallel corpora, our models are also built automatically. Only small amounts of annotated data are used to tune hyperparameters.\nWe also introduced two new datasets to evaluate compositional models of short paraphrases, filling a gap in the NLP community, as currently there are no datasets created for this purpose. Successful models on these datasets can then be used to extend the coverage of, or provide an alternative to, PPDB.\nThere remains a great deal of work to be done in developing new composition models, whether with new network architectures or distance functions. In this work, we based our composition function on constituent parse trees, but this may not be the best approach\u2014especially for short phrases. Dependency syntax may be a better alternative (Socher et al., 2014). Besides improving composition, another direction to explore is how to use models for short phrases in sentence-level paraphrase recognition and other downstream tasks."}, {"heading": "Appendix A", "text": "Increasing the dimension of word embeddings or training them on more data can have a significant positive impact on many tasks\u2014both at the word level and on downstream tasks. We scaled\nup our original 25-dimensional PARAGRAM embeddings and modified our training procedure slightly in order to produce two sets of 300-dimensional PARAGRAM vectors.16 The vectors outperform our original 25-dimensional PARAGRAM vectors on all tasks and achieve human-level performance on SL999 and WS353. Moreover, when simply using vector addition as a compositional model, they are both on par with the RNN models we trained specifically for each task. These results can be seen in Tables 10, 11, and 12.\nThe main modification was to use higherdimensional initial embeddings, in our case the pretrained 300-dimensional GloVe embeddings.17 Since PPDB only contains lowercased words, we extracted only one GloVe vector per word type (regardless of case) by taking the first occurrence of each word in the vocabulary. This is the vector for the most common casing of the word, and was used as\n16Both PARAGRAM300,WS353 and PARAGRAM300,SL999 vectors can be found on the authors\u2019 websites.\n17We used the GloVe vectors trained on 840 billion tokens of Common Crawl data, available at http://nlp.stanford.edu/projects/glove/\nthe word\u2019s single initial vector in our experiments. This reduced the vocabulary from the original 2.2 million types to 1.7 million.\nSmaller changes included replacing dot product with cosine similarity in Equation 2 and a change to the negative sampling procedure. We experimented with three approaches: MAX sampling discussed in Section 4.1, RAND sampling which is random sampling from the batch, and a 50/50 mixture of MAX sampling and RAND sampling.\nFor training data, we selected all word pairs in the lexical portion of PPDB XL that were in our vocabulary, removing redundancies. This resulted in 169,591 pairs for training. We trained our models for 10 epochs and tuned hyperparameters (batch size, \u03bbWw , \u03b4, and sampling method) in two ways: maximum correlation on WS353 (PARAGRAM300,WS353) and maximum correlation on SL999 (PARAGRAM300,SL999).18 We report results for both sets of embeddings in Tables 10, 11, and 12, and make both available to the community in the hope that they may be useful for other downstream tasks."}, {"heading": "Acknowledgements", "text": "We thank the editor and the anonymous reviewers as well as Juri Ganitkevitch, Weiran Wang, and Kazuma Hashimoto for their valuable comments and technical assistance. We also thank Chris Callison-Burch, Dipanjan Das, Kuzman Ganchev, Ellie Pavlick, Slav Petrov, Owen Rambow, David Sontag, Oscar Ta\u0308ckstro\u0308m, Kapil Thadani, Lyle Ungar, Benjamin Van Durme, and Mo Yu for helpful conversations. This research was supported by a Google Faculty Research Award to Mohit Bansal, Karen Livescu, and Kevin Gimpel, the Multimodal Information Access & Synthesis Center at UIUC, part of CCICADA, a DHS Science and Technology Center of Excellence, and by DARPA under agreement number FA8750-13-2-0008. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government.\n18Note that if we use the approach in Section 5.3 in which we tune to maximize 2\u00d7WS-S correlation minus the WS-R correlation, the SL999 \u03c1 is 0.640, still higher than any other reported result to the best of our knowledge."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Con-", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "A survey of paraphrasing and textual entailment methods", "author": ["Ion Androutsopoulos", "Prodromos Malakasiotis."], "venue": "Journal of Artificial Intelligence Research, pages 135\u2013187.", "citeRegEx": "Androutsopoulos and Malakasiotis.,? 2010", "shortCiteRegEx": "Androutsopoulos and Malakasiotis.", "year": 2010}, {"title": "Paraphrasing with bilingual parallel corpora", "author": ["Colin Bannard", "Chris Callison-Burch."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 597\u2013604. Association for Computational Linguistics.", "citeRegEx": "Bannard and Callison.Burch.,? 2005", "shortCiteRegEx": "Bannard and Callison.Burch.", "year": 2005}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang."], "venue": "Proceedings of ACL.", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["Johannes Bjerva", "Johan Bos", "Rob van der Goot", "Malvina Nissim."], "venue": "SemEval 2014, page 642.", "citeRegEx": "Bjerva et al\\.,? 2014", "shortCiteRegEx": "Bjerva et al\\.", "year": 2014}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learn-", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Paraphrase substitution for recognizing textual entailment", "author": ["Wauter Bosma", "Chris Callison-Burch."], "venue": "Proceedings of the 7th International Conference on Cross-Language Evaluation Forum: Evaluation of Multilingual and Multi-modal Information Re-", "citeRegEx": "Bosma and Callison.Burch.,? 2007", "shortCiteRegEx": "Bosma and Callison.Burch.", "year": 2007}, {"title": "Libsvm: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin."], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.", "citeRegEx": "Chang and Lin.,? 2011", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Indexing by latent semantic analysis", "author": ["Scott C. Deerwester", "Susan T Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman."], "venue": "JAsIs, 41(6):391\u2013407.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Bill Dolan", "Chris Quirk", "Chris Brockett."], "venue": "Proceedings of Coling 2004, pages 350\u2013356, Geneva, Switzerland, Aug 23\u2013Aug 27. COLING.", "citeRegEx": "Dolan et al\\.,? 2004", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "J. Mach. Learn. Res., 12:2121\u20132159, July.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1608\u20131618, Sofia, Bul-", "citeRegEx": "Fader et al\\.,? 2013", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web, pages 406\u2013414. ACM.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "A Synopsis of Linguistic Theory, 19301955", "author": ["J.R. Firth"], "venue": null, "citeRegEx": "Firth.,? \\Q1957\\E", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Ppdb: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "HLT-NAACL, pages 758\u2013764. The Association for Computational Linguistics.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Jointly learning word representations and composition functions using predicate-argument structures", "author": ["Kazuma Hashimoto", "Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural", "citeRegEx": "Hashimoto et al\\.,? 2014", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "CoRR, abs/1408.3456.", "citeRegEx": "Hill et al\\.,? 2014b", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Improved statistical machine translation using monolingually-derived paraphrases", "author": ["Yuval Marton", "Chris Callison-Burch", "Philip Resnik."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 381\u2013390, Singapore, Au-", "citeRegEx": "Marton et al\\.,? 2009", "shortCiteRegEx": "Marton et al\\.", "year": 2009}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "ACL, pages 236\u2013 244. Citeseer.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131439.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Monolingual machine translation for paraphrase generation", "author": ["Chris Quirk", "Chris Brockett", "William Dolan."], "venue": "Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 142\u2013149, Barcelona, Spain, July. Association for Computational Linguis-", "citeRegEx": "Quirk et al\\.,? 2004", "shortCiteRegEx": "Quirk et al\\.", "year": 2004}, {"title": "Augmenting FrameNet via PPDB", "author": ["Pushpendre Rastogi", "Benjamin Van Durme."], "venue": "Proceedings of the Second Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 1\u20135, Baltimore, Maryland, USA, June. Association for Compu-", "citeRegEx": "Rastogi and Durme.,? 2014", "shortCiteRegEx": "Rastogi and Durme.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Potts."], "venue": "Pro-", "citeRegEx": "Potts.,? 2013", "shortCiteRegEx": "Potts.", "year": 2013}, {"title": "Tests for comparing elements of a correlation matrix", "author": ["James H Steiger."], "venue": "Psychological Bulletin,", "citeRegEx": "Steiger.,? 1980", "shortCiteRegEx": "Steiger.", "year": 1980}, {"title": "Learning composition models for phrase embeddings", "author": ["Mo Yu", "Mark Dredze."], "venue": "Transactions of", "citeRegEx": "Yu and Dredze.,? 2015", "shortCiteRegEx": "Yu and Dredze.", "year": 2015}, {"title": "Estimating linear models for compositional", "author": ["Francesca Fallucchi", "Suresh Manandhar"], "venue": null, "citeRegEx": "Fallucchi and Manandhar.,? \\Q2010\\E", "shortCiteRegEx": "Fallucchi and Manandhar.", "year": 2010}], "referenceMentions": [{"referenceID": 18, "context": "The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive semantic resource, consisting of a list of phrase pairs with (heuristic) confidence estimates.", "startOffset": 24, "endOffset": 57}, {"referenceID": 14, "context": "a variety of NLP tasks like question answering (Rinaldi et al., 2003; Fader et al., 2013), semantic parsing (Berant and Liang, 2014), textual entail-", "startOffset": 47, "endOffset": 89}, {"referenceID": 6, "context": ", 2013), semantic parsing (Berant and Liang, 2014), textual entail-", "startOffset": 26, "endOffset": 50}, {"referenceID": 9, "context": "ment (Bosma and Callison-Burch, 2007), and machine translation (Marton et al.", "startOffset": 5, "endOffset": 37}, {"referenceID": 23, "context": "ment (Bosma and Callison-Burch, 2007), and machine translation (Marton et al., 2009).", "startOffset": 63, "endOffset": 84}, {"referenceID": 1, "context": "See Androutsopoulos and Malakasiotis (2010) for a survey on approaches for detecting paraphrases.", "startOffset": 4, "endOffset": 44}, {"referenceID": 18, "context": "The most recent work in this area is the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), a collection of confidence-rated paraphrases created using the pivoting technique of Bannard and Callison-Burch (2005) over large parallel corpora.", "startOffset": 61, "endOffset": 94}, {"referenceID": 7, "context": ", 2013), for predicting sentence similarity (Bjerva et al., 2014), and to improve the coverage of FrameNet (Rastogi and Van Durme, 2014).", "startOffset": 44, "endOffset": 65}, {"referenceID": 2, "context": ", 2013), a collection of confidence-rated paraphrases created using the pivoting technique of Bannard and Callison-Burch (2005) over large parallel corpora.", "startOffset": 94, "endOffset": 128}, {"referenceID": 24, "context": "First we show that initial skip-gram word vectors (Mikolov et al., 2013a) can", "startOffset": 50, "endOffset": 73}, {"referenceID": 18, "context": "We use it to show that the phrase embeddings produced by our methods are significantly more indicative of paraphrasability than the original heuristic scoring used by Ganitkevitch et al. (2013). Thus we use the power of PPDB to improve its contents.", "startOffset": 167, "endOffset": 194}, {"referenceID": 26, "context": "Our second dataset, ML-Paraphrase, is a reannotation of the bigram similarity corpus from Mitchell and Lapata (2010). The task was originally developed to measure semantic similarity of bigrams, but some annotations are not congruent with the functional similarity central to paraphrase relationships.", "startOffset": 90, "endOffset": 117}, {"referenceID": 20, "context": "Provide new PARAGRAM word vectors, learned using PPDB, that achieve state-of-the-art performance on the SimLex-999 lexical similarity task (Hill et al., 2014b) and lead to improved performance in sentiment analysis.", "startOffset": 139, "endOffset": 159}, {"referenceID": 26, "context": "The second is a new annotation of the bigram similarity task in Mitchell and Lapata (2010) that makes it", "startOffset": 64, "endOffset": 91}, {"referenceID": 17, "context": "The intuition of most methods to create these vectors (or embeddings) is that similar words have similar contexts (Firth, 1957).", "startOffset": 114, "endOffset": 127}, {"referenceID": 11, "context": "Earlier models made use of latent semantic analysis (LSA) (Deerwester et al., 1990).", "startOffset": 58, "endOffset": 83}, {"referenceID": 5, "context": "Recently, more sophisticated neural models, work originating with (Bengio et al., 2003), have been gaining popularity (Mikolov et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 24, "context": ", 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 38, "endOffset": 86}, {"referenceID": 28, "context": ", 2003), have been gaining popularity (Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 38, "endOffset": 86}, {"referenceID": 3, "context": "These embeddings are now being used in new ways as they are being tailored to specific downstream tasks (Bansal et al., 2014).", "startOffset": 104, "endOffset": 125}, {"referenceID": 7, "context": "Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well.", "startOffset": 95, "endOffset": 120}, {"referenceID": 7, "context": "Simple but effective compositional models were studied by Mitchell and Lapata (2008; 2010) and Blacoe and Lapata (2012). They compared a variety of binary operations on word vectors and found that simple point-wise multiplication of explicit vector representations performed very well. Other works like Zanzotto et al. (2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices.", "startOffset": 95, "endOffset": 326}, {"referenceID": 4, "context": "(2010) and Baroni and Zamparelli (2010) also explored composition using models based on operations of vectors and matrices.", "startOffset": 11, "endOffset": 40}, {"referenceID": 25, "context": "(2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al., 2013b).", "startOffset": 78, "endOffset": 101}, {"referenceID": 23, "context": "More recent work has shown that the extremely efficient neural embeddings of Mikolov et al. (2013a) also do well on compositional tasks simply by adding the word vectors (Mikolov et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 19, "context": "Hashimoto et al. (2014)", "startOffset": 0, "endOffset": 24}, {"referenceID": 26, "context": "introduced an alternative word embedding and compositional model based on predicate-argument structures that does well on two simple composition tasks, including the one introduced by Mitchell and Lapata (2010).", "startOffset": 184, "endOffset": 211}, {"referenceID": 8, "context": "volving compositionality (Blacoe and Lapata, 2012; Hashimoto et al., 2014).", "startOffset": 25, "endOffset": 74}, {"referenceID": 19, "context": "volving compositionality (Blacoe and Lapata, 2012; Hashimoto et al., 2014).", "startOffset": 25, "endOffset": 74}, {"referenceID": 8, "context": "volving compositionality (Blacoe and Lapata, 2012; Hashimoto et al., 2014).5 However, we found success using RNNs in a supervised setting, similar to Socher et al. (2014), who used RNNs to learn representations for image descriptions.", "startOffset": 26, "endOffset": 171}, {"referenceID": 15, "context": "Lastly, the PPDB has been used along with other resources to learn word embeddings for several tasks, including semantic similarity, language modeling, predicting human judgments, and classification (Yu and Dredze, 2014; Faruqui et al., 2015).", "startOffset": 199, "endOffset": 242}, {"referenceID": 33, "context": "Concurrently with our work, it has also been used to construct paraphrase models for short phrases (Yu and Dredze, 2015).", "startOffset": 99, "endOffset": 120}, {"referenceID": 26, "context": "We created two novel datasets: (1) AnnotatedPPDB, a subset of phrase pairs from PPDB which are annotated according to how strongly they represent a paraphrase relationship, and (2) MLParaphrase, a re-annotation of the bigram similarity dataset from Mitchell and Lapata (2010), again annotated for strength of paraphrase relationship.", "startOffset": 249, "endOffset": 276}, {"referenceID": 20, "context": "Most existing paraphrase tasks focus on words, like SimLex-999 (Hill et al., 2014b), or entire sentences, such as the Microsoft Research Paraphrase Corpus (Dolan et al.", "startOffset": 63, "endOffset": 83}, {"referenceID": 12, "context": ", 2014b), or entire sentences, such as the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004).", "startOffset": 80, "endOffset": 120}, {"referenceID": 29, "context": ", 2014b), or entire sentences, such as the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004).", "startOffset": 80, "endOffset": 120}, {"referenceID": 26, "context": "Our second newly-annotated dataset, ML-Paraphrase, is based on the bigram similarity task originally introduced by Mitchell and Lapata (2010); we refer to the original annotations as the ML dataset.", "startOffset": 115, "endOffset": 142}, {"referenceID": 27, "context": "49 (final column) reported by Mitchell and Lapata and also surpassed by pointwise multiplication (Mitchell and Lapata, 2010).", "startOffset": 97, "endOffset": 124}, {"referenceID": 13, "context": "To learn the model parameters (W, b,Ww), we minimize our objective function over the data using AdaGrad (Duchi et al., 2011) with mini-batches.", "startOffset": 104, "endOffset": 124}, {"referenceID": 24, "context": "This gave us an objective that bears some similarity to the skip-gram objective with negative sampling in word2vec (Mikolov et al., 2013a).", "startOffset": 115, "endOffset": 138}, {"referenceID": 20, "context": "We train on word pairs from PPDB and evaluate on the SimLex-999 dataset (Hill et al., 2014b), achieving the best results reported to date.", "startOffset": 72, "endOffset": 92}, {"referenceID": 13, "context": "We trained for 20 epochs for each set of hyperparameters using AdaGrad (Duchi et al., 2011).", "startOffset": 71, "endOffset": 91}, {"referenceID": 24, "context": "For all experiments, we initialized our word vectors with skip-gram vectors trained using word2vec (Mikolov et al., 2013a).", "startOffset": 99, "endOffset": 122}, {"referenceID": 20, "context": "58 Hill et al. (2014b) 200 0.", "startOffset": 3, "endOffset": 23}, {"referenceID": 20, "context": "58 Hill et al. (2014b) 200 0.446 Hill et al. (2014a) - 0.", "startOffset": 3, "endOffset": 53}, {"referenceID": 16, "context": "Hyperparameters were tuned using the wordsim-353 (WS353) dataset (Finkelstein et al., 2001), specifically its similarity (WS-S) and relatedness (WSR) partitions (Agirre et al.", "startOffset": 65, "endOffset": 91}, {"referenceID": 0, "context": ", 2001), specifically its similarity (WS-S) and relatedness (WSR) partitions (Agirre et al., 2009).", "startOffset": 77, "endOffset": 98}, {"referenceID": 20, "context": "After tuning, we evaluated the best hyperparameters on the SimLex-999 (SL999) dataset (Hill et al., 2014b).", "startOffset": 86, "endOffset": 106}, {"referenceID": 32, "context": "the one-tailed method of (Steiger, 1980).", "startOffset": 25, "endOffset": 40}, {"referenceID": 20, "context": "We also show results for strong skip-gram baselines and the best results from the literature, including the state-of-the-art results from Hill et al. (2014a) as well as the interannotator agreement from Hill et al.", "startOffset": 138, "endOffset": 158}, {"referenceID": 20, "context": "We also show results for strong skip-gram baselines and the best results from the literature, including the state-of-the-art results from Hill et al. (2014a) as well as the interannotator agreement from Hill et al. (2014b).11", "startOffset": 138, "endOffset": 223}, {"referenceID": 21, "context": "We used the simple CNN from Kim (2014) and the binary sentence-level sentiment analysis task from Socher et al.", "startOffset": 28, "endOffset": 39}, {"referenceID": 21, "context": "We used the simple CNN from Kim (2014) and the binary sentence-level sentiment analysis task from Socher et al. (2013). We used the standard data splits, removing examples with a neutral rating.", "startOffset": 28, "endOffset": 119}, {"referenceID": 21, "context": "While Kim (2014) used m-gram filters of several lengths, we only used unigram filters.", "startOffset": 6, "endOffset": 17}, {"referenceID": 13, "context": "1 using AdaGrad (Duchi et al., 2011).", "startOffset": 16, "endOffset": 36}, {"referenceID": 32, "context": "For all experiments, we again used cosine similarity as our similarity metric and evaluated the statistical significance using the method of (Steiger, 1980).", "startOffset": 141, "endOffset": 156}, {"referenceID": 26, "context": "Unlike explicit word vectors, where point-wise multiplication acts as a conjunction of features and performs well on composition tasks (Mitchell and Lapata, 2008), using addition with skip-gram vectors (Mikolov et al.", "startOffset": 135, "endOffset": 162}, {"referenceID": 25, "context": "Unlike explicit word vectors, where point-wise multiplication acts as a conjunction of features and performs well on composition tasks (Mitchell and Lapata, 2008), using addition with skip-gram vectors (Mikolov et al., 2013b) gives better performance than multiplication.", "startOffset": 202, "endOffset": 225}, {"referenceID": 26, "context": "To evaluate our ability to paraphrase bigrams, we consider the original bigram similarity task from Mitchell and Lapata (2010) as well as our newlyannotated version of it: ML-Paraphrase.", "startOffset": 100, "endOffset": 127}, {"referenceID": 26, "context": "The bigram similarity task from Mitchell and Lapata (2010) contains three types of bigrams: adjective-noun (JN), noun-noun (NN), and verb-noun (VN).", "startOffset": 32, "endOffset": 59}, {"referenceID": 22, "context": "ger (Manning et al., 2014) to tag the tokens in each phrase.", "startOffset": 4, "endOffset": 26}, {"referenceID": 26, "context": "Model Mitchell and Lapata (2010) Bigrams ML-Paraphrase word vectors n comp.", "startOffset": 6, "endOffset": 33}, {"referenceID": 25, "context": "Table 5: Results on the test section of the bigram similarity task of Mitchell and Lapata (2010) and our newly annotated version (ML-Paraphrase).", "startOffset": 70, "endOffset": 97}, {"referenceID": 19, "context": "05) over the skip-gram model, \u2020 statistically significant over the {PARAGRAM, +} model, and \u2021 statistically significant over Hashimoto et al. (2014).", "startOffset": 125, "endOffset": 149}, {"referenceID": 25, "context": "That is, the JN and NN results from Mitchell and Lapata (2010) use their multiplicative model and the VN results use their dilation model.", "startOffset": 36, "endOffset": 63}, {"referenceID": 19, "context": "From Hashimoto et al. (2014) we used their PAS-CLBLM Addl and PAS-CLBLM Addnl models.", "startOffset": 5, "endOffset": 29}, {"referenceID": 26, "context": "We report results on the test portion of the original Mitchell and Lapata (2010) dataset (ML) as well as the entirety of our newly-annotated dataset (MLParaphrase).", "startOffset": 54, "endOffset": 81}, {"referenceID": 19, "context": "The results obtained here differ from those reported in Hashimoto et al. (2014) as we scored their vectors with a newer Python implementation of Spearman \u03c1 that handles ties (Hashimoto, P.", "startOffset": 56, "endOffset": 80}, {"referenceID": 18, "context": "model based on filtered phrase pairs in PPDB, we can actually distinguish between quality paraphrases and poor paraphrases in PPDB better than the original heuristic scoring scheme from Ganitkevitch et al. (2013).", "startOffset": 186, "endOffset": 213}, {"referenceID": 10, "context": "We also train a support vector regression model (epsilon-SVR) (Chang and Lin, 2011) on the 33 features that are included for each phrase pair in PPDB.", "startOffset": 62, "endOffset": 83}, {"referenceID": 20, "context": "611, which is easily beaten by automatic methods (Hill et al., 2014b).", "startOffset": 49, "endOffset": 69}, {"referenceID": 26, "context": "Model Mitchell and Lapata (2010) Bigrams ML-Paraphrase word vectors n comp.", "startOffset": 6, "endOffset": 33}], "year": 2015, "abstractText": "The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive semantic resource, consisting of a list of phrase pairs with (heuristic) confidence estimates. However, it is still unclear how it can best be used, due to the heuristic nature of the confidences and its necessarily incomplete coverage. We propose models to leverage the phrase pairs from the PPDB to build parametric paraphrase models that score paraphrase pairs more accurately than the PPDB\u2019s internal scores while simultaneously improving its coverage. They allow for learning phrase embeddings as well as improved word embeddings. Moreover, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models. Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.1,2", "creator": "LaTeX with hyperref package"}}}