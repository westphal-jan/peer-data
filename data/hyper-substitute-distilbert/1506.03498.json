{"id": "1506.03498", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "Matrix Completion from Fewer Entries: Spectral Detectability and Rank Estimation", "abstract": "algorithm completion counting the rank matrices from few intervals is termed measure with immediate practical purposes. we consider between two aspects of this problem : detectability, i. e. the ability of secure high rank $ mt $ select from the fewest possible random computation, satisfying performance besides achieving small reconstruction predictions. hamilton propose a spectral algorithm for these two tasks called macbeth ( for matrix improvement through the bethe model ). the same is recovered as the number of negative eigenvalues of the main hessian matrix,... generates corresponding error initially distributed to intermediate condition / final realization of dummy ensemble and the particular matrix and the revealed reconstruction. we recognize analytical performance in a random matrix setting all results from the statistical structures in the primitive neural circuits, and describe simultaneous addition that macbeth efficiently calculating whatever rank $ r $ represents a large $ n \\ times > $ \u2190 e $ kc ( r ) r \\ sqrt { nm } $ {, rn $ c ( r ) $ gives a constant matching to $ rf $. we efficiently predict we corresponding root - root - square error test will show then macbeth compares favorably to already existing approaches.", "histories": [["v1", "Wed, 10 Jun 2015 22:46:02 GMT  (170kb,D)", "https://arxiv.org/abs/1506.03498v1", null], ["v2", "Tue, 30 Jun 2015 17:15:13 GMT  (432kb,D)", "http://arxiv.org/abs/1506.03498v2", null], ["v3", "Thu, 28 Jan 2016 10:16:56 GMT  (432kb,D)", "http://arxiv.org/abs/1506.03498v3", "NIPS Conference 2015"]], "reviews": [], "SUBJECTS": "cond-mat.dis-nn cs.LG stat.ML", "authors": ["alaa saade", "florent krzakala", "lenka zdeborov\u00e1"], "accepted": true, "id": "1506.03498"}, "pdf": {"name": "1506.03498.pdf", "metadata": {"source": "CRF", "title": "Matrix Completion from Fewer Entries: Spectral Detectability and Rank Estimation", "authors": ["A. Saade", "F. Krzakala", "L. Zdeborov\u00e1"], "emails": [], "sections": [{"heading": null, "text": "Matrix Completion from Fewer Entries: Spectral Detectability and Rank Estimation\nA. Saade1, F. Krzakala1,2,3 and L. Zdeborov\u00e14 1 Laboratoire de Physique Statistique, UMR 8550 CNRS, Department of Physics,\n\u00c9cole Normale Sup\u00e9rieure and PSL Research University, Rue Lhomond, 75005 Paris, France 2 Sorbonne Universit\u00e9s, UPMC Univ Paris 06, UMR 8550, LPS, F-75005, Paris, France\n3 ESPCI and CNRS UMR 7083 Gulliver, 10 rue Vauquelin,Paris 75005 4 Institut de Physique Th\u00e9orique, CEA Saclay and URA 2306, CNRS, 91191 Gif-sur-Yvette, France\n(Dated: January 29, 2016)\nThe completion of low rank matrices from few entries is a task with many practical applications. We consider here two aspects of this problem: detectability, i.e. the ability to estimate the rank r reliably from the fewest possible random entries, and performance in achieving small reconstruction error. We propose a spectral algorithm for these two tasks called MaCBetH (for Matrix Completion with the Bethe Hessian). The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries. We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank r of a large n \u00d7m matrix from C(r)r \u221a nm entries, where C(r) is a constant depicted in Fig. 2. We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.\nI. INTRODUCTION\nMatrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank r. This problem has witnessed a burst of activity, see e.g. [1\u20133], motivated by many applications such as collaborative filtering [1], quantum tomography [4] in physics, or the analysis of a covariance matrix [1].A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise. The most widely considered question in this setting is how many entries need to be revealed such that the matrix can be completed exactly in a computationally efficient way [1, 3]. While our present paper assumes the same model, the main questions we investigate are different.\nThe first question we address in this paper is detectability, i.e. how many random entries do we need to reveal in order to be able to estimate the rank r reliably. This question is motivated by the more generic problem of detecting structure (in our case, low rank) hidden in partially observed data. It is reasonable to expect the existence of a region where exact completion is hard or even impossible yet the rank estimation is tractable. A second question we address is what is the minimum achievable root-mean-square error (RMSE) in estimating the unknown elements of the matrix. In practice, even if exact reconstruction is not possible, having a procedure that provides a very small RMSE might be quite sufficient.\nIn this paper we propose an algorithm called MaCBetH that gives the best known empirical performance for the two tasks above when the rank r is small. The rank in our algorithm is estimated as the number of negative eigenvalues of an associated Bethe Hessian matrix [5, 6], and the corresponding eigenvectors are used as an initial condition for the local optimization of a cost function commonly considered in matrix completion (see e.g. [3]). In particular, in the random matrix setting, we show that MaCBetH detects the rank of a large n \u00d7 m matrix from C(r)r \u221a nm entries, where C(r) is a small constant, see Fig. 2, and C(r) \u2192 1 as r \u2192 \u221e. The corresponding RMSE is evaluated empirically, and in the regime close to C(r)r \u221a nm, it compares very favorably to existing approaches, in particular to OptSpace [3].\nThis contribution is organized as follows. First, in Sec. II we define the problem and present generally our approach in the context of existing works. In Sec. III we describe our algorithm and motivate its construction via a spectral relaxation of the Hopfield model of neural network. Next, in Sec. IV we show how the performance of the proposed spectral method can be analyzed using, in parts, results from spin glass theory and phase transitions, and rigorous results on the spectral density of large random matrices. Finally, in Sec. V we present numerical simulations that demonstrate the efficiency of MaCBetH.\nar X\niv :1\n50 6.\n03 49\n8v 3\n[ co\nnd -m\nat .d\nis -n\nn] 2\n8 Ja\nn 20\n16\n2"}, {"heading": "II. PROBLEM DEFINITION AND RELATION TO OTHER WORKS", "text": "LetMtrue be a rank-r matrix such that\nMtrue = XY \u2020 , (1)\nwhere X \u2208 Rn\u00d7r and Y \u2208 Rm\u00d7r are two (unknown) tall matrices. We observe only a small fraction of the elements of Mtrue, chosen uniformly at random. We call E the subset of observed entries, andM the (sparse) matrix supported on E whose nonzero elements are the revealed entries ofMtrue. The aim is to reconstruct the rank r matrixMtrue = XY \u2020 given M. An important parameter which controls the difficulty of the problem is = |E|/ \u221a nm. In the case of a square matrixM, this is the average number of revealed entries per line or column. In our numerical examples and theoretical justifications we shall generate the low rank matrixMtrue = XY \u2020, using tall matrices X and Y with iid Gaussian elements, we call this the random matrix setting. The MaCBetH algorithm is, however, non-parametric and does not use any prior knowledge about X and Y . The analysis we perform applies to the limit n\u2192\u221e while m/n = \u03b1 = O(1) and r = O(1).\nThe matrix completion problem was popularized in [1] who proposed nuclear norm minimization as a convex relaxation of the problem. The algorithmic complexity of the associated semidefinite programming is, however, O(n2m2). A low complexity procedure to solve the problem was later proposed by [7] and is based on singular value decomposition (SVD). A considerable step towards theoretical understanding of matrix completion from few entries was made in [3] who proved that with the use of trimming the performance of SVD-based matrix completion can be improved and a RMSE proportional to \u221a nr/|E| can be achieved. The algorithm of [3] is referred to as OptSpace, and empirically it achieves state-of-the-art RMSE in the regime of very few revealed entries. OptSpace proceeds in three steps [3]. First, one trims the observed matrix M by setting to zero all rows (resp. columns) with more revealed entries than twice the average number of revealed entries per row (resp. per column). Second, a singular value decompositions is performed on the matrix and only the first r components are kept. When the rank r is unknown it is estimated as the index for which the ratio between two consecutive singular values has a minimum. Third, a local minimization of the discrepancy between the observed entries and a low-rank estimate is performed. The initial condition for this minimization is given by the first r left and right singular vectors from the second step.\nIn this work we improve upon OptSpace by replacing the first two steps by a different spectral procedure that detects the rank and provides a better initial condition for the discrepancy minimization. Our method leverages on recent progress made in the task of detecting communities in the stochastic block model [5, 8] with spectral methods. Both in community detection and matrix completion, traditional spectral methods fail in the very sparse regime due to the existence of spurious large eigenvalues (or singular values) corresponding to localized eigenvectors [3, 8]. The authors of [5, 8, 9] showed that using the non-backtracking matrix or the closely related Bethe Hessian as a basis for the spectral method in community detection provides reliable rank estimation and better inference performance. The present paper provides an analogous improvement for the matrix completion problem. In particular, we shall analyze the algorithm using tools from spin glass theory in statistical mechanics, and show that there exists a phase transition between a phase where it is able to detect the rank, and a phase where it is unable to do so."}, {"heading": "III. ALGORITHM AND MOTIVATION", "text": ""}, {"heading": "A. The MacBetH algorithm", "text": "A standard approach to the completion problem (see e.g. [3]) is to minimize the cost function\nmin X,Y \u2211 (ij)\u2208E [Mij \u2212 (XY \u2020)ij ]2 (2)\nover X \u2208 Rn\u00d7r and Y \u2208 Rm\u00d7r. This function is non-convex, and global optimization is hard. One therefore resorts to a local optimization technique with a careful choice of the initial conditions X0, Y0. In our method, given the matrix M, we consider a weighted bipartite undirected graph with adjacency matrix A \u2208 R(n+m)\u00d7(n+m)\nA = ( 0 M MT 0 ) . (3)\n3 We will refer to the graph thus defined as G. We now define the Bethe Hessian matrix H(\u03b2) \u2208 R(n+m)\u00d7(n+m) to be the matrix with elements\nHij(\u03b2) =\n( 1 +\n\u2211 k\u2208\u2202i sinh2 \u03b2Aik\n) \u03b4ij \u2212 1\n2 sinh(2\u03b2Aij) , (4)\nwhere \u03b2 is a parameter that we will fix to a well-defined value \u03b2SG depending on the data, and \u2202i stands for the neighbors of i in the graph G. The MaCBetH algorithm that is the main subject of this paper is then, given the matrix A, which we assume to be centered:\nAlgorithm (MaCBetH)\n1. Numerically solve for the value of \u03b2\u0302SG such that\nF (\u03b2\u0302SG) \u2261 1\u221a nm \u2211 (i,j)\u2208E tanh2(\u03b2\u0302SGMij) = 1 . (5)\n2. Build the Bethe Hessian H(\u03b2\u0302SG) following eq. (4).\n3. Compute all its negative eigenvalues \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbr\u0302 and corresponding eigenvectors v1, \u00b7 \u00b7 \u00b7 , vr\u0302. r\u0302 is our estimate for the rank r. Set X0 (resp. Y0) to be the first n lines (resp. the last m lines) of the matrix [v1 v2 \u00b7 \u00b7 \u00b7 vr\u0302].\n4. Perform local optimization of the cost function (2) with rank r\u0302 and initial condition X0, Y0.\nThe function F , in the first step, being an increasing function of \u03b2, \u03b2\u0302SG can be found efficiently, e.g. by dichotomy. Alternatively, \u03b2\u0302SG in step 1 can be tuned in such a way that the number of negative eigenvalues of the Bethe Hessian is the largest possible. In step 2 we could also use the non-backtracking matrix weighted by tanh\u03b2Mij , it was shown in [5] that the spectrum of the Bethe Hessian and the non-backtracking matrix are closely related. In the next section, we will motivate and analyze this algorithm (in the setting whereMtrue was generated from elements-wise random X and Y ) and show that in this case MaCBetH is able to infer the rank whenever > c. Fig. 1 illustrates the spectral properties of the Bethe Hessian that justify this algorithm: the spectrum is composed of a few informative negative eigenvalues, well separated from the bulk (which remains positive). This algorithm is computationally efficient as it is based on the eigenvalue decomposition of a sparse, symmetric matrix."}, {"heading": "B. Motivation from a Hopfield model", "text": "We shall now motivate the construction of the MaCBetH algorithm from a graphical model perspective and a spectral relaxation. Given the observed matrix M from the previous section, we consider the following graphical model\nP ({s}, {t}) = 1 Z exp \u03b2 \u2211 (i,j)\u2208E Mijsitj  , (6) where the {si}1\u2264i\u2264n and {tj}1\u2264j\u2264m are binary variables, and \u03b2 is a parameter controlling the strength of the interactions. This model is a (generalized) Hebbian Hopfield model [10] on a bipartite sparse graph. To study it, we can use the standard Bethe approximation which is widely believed to be exact for such problems on large random graphs [11, 12]. In this approximation the means E(si),E(tj) and moments E(sitj) of each variable are approximated by the parameters bi, cj and \u03beij that minimize the so-called Bethe free energy FBethe({bi}, {cj}, {\u03beij}) that reads\nFBethe({bi}, {cj}, {\u03beij}) = \u2212 \u2211\n(i,j)\u2208E\nMij\u03beij + \u2211\n(i,j)\u2208E \u2211 si,tj \u03b7 (1 + bisi + cjtj + \u03beijsitj 4 )\n+ n\u2211 i=1 (1\u2212 di) \u2211 si \u03b7 (1 + bisi 2 ) + m\u2211 j=1 (1\u2212 dj) \u2211 tj \u03b7 (1 + cjtj 2 ) , (7)\nwhere \u03b7(x) := x lnx, and di, dj are the degrees of nodes i and j in the graph G.\n4 \u03bb0 5 10 15 20 25 \u03c1 (\u03bb ) 0 0.04 0.08 0.12 0.16 0.2 \u03b2 = 0.25Direct diag BP \u03bb 0 1 2 3 4 5 6 7 8 \u03c1 (\u03bb ) 0 0.1 0.2 0.3 0.4 0.5 \u03b2 = 0.12824Direct diag BP \u03bb -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 \u03c1 (\u03bb ) 0 0.2 0.4 0.6 0.8 1 1.2 1.4 \u03b2 = 0.05Direct diag BP \u03bb 0.2 0.4 0.6 0.8 1 1.2 \u03c1 (\u03bb ) 0 1 2 3 4 5 6 7 \u03b2 = 0.01Direct diag BP 0.7 0.8 0.9 0 0.9 1.8 0 0.25 0.5 0 0.15 0.3 0 0.6 1.2 0 0.03 0.06 -0.5 0 0.5 0 0.09 0.18\nFIG. 1: Spectral density of the Bethe Hessian for various values of the parameter \u03b2. The red dots are the result of the direct diagonalisation of the Bethe Hessian for a rank r = 5 and n = m = 104 matrix, with = 15 revealed entries per row on average. The black curves are the solutions to the recursion (18) computed with belief propagation on a graph of size 105. We isolated the 5 smallest eigenvalues, represented as small bars for convenience, and the inset is a zoom around these smallest eigenvalues. For \u03b2 small enough (top plots), the Bethe Hessian is positive definite, signaling that the paramagnetic state (8) is a local minimum of the Bethe free energy. As \u03b2 increases, the spectrum is shifted towards the negative region and has 5 negative eigenvalues at the approximate value of \u03b2\u0302SG = 0.12824 (to be compared to \u03b2R = 0.0832 for this case) evaluated by our algorithm (lower left plot). These eigenvalues, corresponding to the retrieval states (9), become positive and eventually merge in the bulk as \u03b2 is further increased (lower right plot), while the bulk of uninformative eigenvalues remains at all values of \u03b2 in the positive region.\nNeural networks models such as eq. (6) have been extensively studied over the last decades (see e.g. [12\u201316] and references therein) and the phenomenology, that we shall review briefly here, is well known. In particular, for \u03b2 small enough, the global minimum of the Bethe free energy corresponds to the so-called paramagnetic state\n\u2200i, j, bi = cj = 0, \u03beij = tanh (\u03b2Mij). (8)\nAs we increase \u03b2, above a certain value \u03b2R, the model enters a retrieval phase, where the free energy has local minima correlated with the factors X and Y . There are r local minima, called retrieval states ({bli}, {clj}, {\u03belij}) indexed by l = 1, \u00b7 \u00b7 \u00b7 , r such that, in the large n,m limit,\n\u2200l = 1 \u00b7 \u00b7 \u00b7 r, 1 n n\u2211 i=1 Xi,lb l i > 0, 1 m m\u2211 j=1 Yj,lc l j > 0 . (9)\nThese retrieval states are therefore well-suited as initial conditions for the local optimization of eq. (2), and we expect their number to tell us the correct rank. Increasing \u03b2 above a critical value \u03b2SG the system eventually enters a spin glass phase, marked by the appearance of many spurious minima.\nIt would be tempting to continue the Bethe approach and to derive the belief propagation equations, but we shall here instead consider a simpler spectral relaxation of the problem, following the same strategy as used in [5, 6] for graph clustering. First, we use the fact that the paramagnetic state (8) is always a stationary point of the Bethe free energy, for any value of \u03b2 [17, 18]. In order to detect the retrieval states, we thus study its stability by looking for negative eigenvalues of the Hessian of the Bethe free energy evaluated at the paramagnetic state (8). At this point, the elements of the Hessian involving one derivative with respect to \u03beij vanish, while the block involving two such derivatives is a diagonal positive definite matrix [5, 17]. The remaining part is the matrix called Bethe Hessian in [5],\n5 and eigenvectors corresponding to its negative eigenvalues are thus expected to give an approximation of the retrieval states (9). The picture exposed in this section is summarized in figure 1. This motivates the use of the MaCBetH algorithm.\nNote that a similar approach was used in [16] to detect the retrieval states of a Hopfield model using the weighted non-backtracking matrix [8], which linearizes the belief propagation equations rather than the Bethe free energy, resulting in a larger, non-symmetric matrix. The Bethe Hessian, while mathematically closely related, is also simpler to handle in practice."}, {"heading": "IV. ANALYSIS OF PERFORMANCE IN DETECTION", "text": "We now show how the performance of MaCBetH can be analyzed, and the spectral properties of the matrix characterized using both tools from statistical mechanics and rigorous arguments."}, {"heading": "A. Analysis of the phase transition", "text": "We start by investigating the phase transition above which our spectral method will detect the correct rank. Let xp = (x l p)1\u2264l\u2264r, yp = (y l p)1\u2264l\u2264r be random vectors with the same empirical distribution as the lines of X and Y respectively. Using the statistical mechanics correspondence between the negative eigenvalues of the Bethe Hessian and the appearance of phase transitions in model (6), we can compute the values \u03b2R and \u03b2SG where instabilities towards, respectively, the retrieval states and the spurious glassy states, arise. We have repeated the computations of [13\u201316] in the case of model (6), using the cavity method [12]. We refer the reader interested in the technical details of the statistical mechanics approach to neural networks to [14\u201316].\nFollowing a standard computation for locating phase transitions in the Bethe approximation (see e.g. [12, 19]), the stability of the paramagnetic state (8) towards these two phases can be monitored in terms of the two following parameters:\n\u03bb(\u03b2) = lim s\u2192\u221e E [ s\u220f p=1 tanh2 ( \u03b2 r\u2211 l=1 xlpy l p ) tanh2 ( \u03b2 r\u2211 l=1 xlp+1y l p )] 1 2s , (10)\n\u00b5(\u03b2) = lim s\u2192\u221e E [ s\u220f p=1 tanh ( \u03b2|x1py1p|+ \u03b2 r\u2211 l=2 xlpy l p ) tanh ( \u03b2|x1p+1y1p|+ \u03b2 r\u2211 l=2 xlp+1y l p )] 1 2s , (11)\nwhere the expectation is over the distribution of the vectors xp, yp. The parameter \u03bb(\u03b2) controls the sensitivity of the paramagnetic solution to random noise, while \u00b5(\u03b2) measures its sensitivity to a perturbation in the direction of a retrieval state. \u03b2SG and \u03b2R are defined implicitly as \u03bb(\u03b2SG) = 1 and \u00b5(\u03b2R) = 1, i.e. the value beyond which the perturbation diverges. The existence of a retrieval phase is equivalent to the condition \u03b2SG > \u03b2R, so that there exists a range of values of \u03b2 where the retrieval states exist, but not the spurious ones. If this condition is met, by setting \u03b2 = \u03b2SG in our algorithm, we ensure the presence of meaningful negative eigenvalues of the Bethe Hessian.\nWe define the critical value of = c such that \u03b2SG > \u03b2R if and only if > c. In general, there is no closed-form formula for this critical value, which is defined implicitly in terms of the functions \u03bb and \u00b5. We thus computed c numerically using a population dynamics algorithm [12] and the results for C(r) = c/r are presented on Figure 2. Quite remarkably, with the definition = |E|/ \u221a nm, the critical value c does not depend on the ratio m/n, only on the rank r. In the limit of large and r it is possible to obtain a simple closed-form formula. In this case the observed entries of the matrix become jointly Gaussian distributed, and uncorrelated, and therefore independent. Expression (10) then simplifies to\n\u03bb(\u03b2) \u223c r\u2192\u221e\nE [ tanh2 ( \u03b2 r\u2211 l=1 xlyl )] . (12)\nWe stress that the MaCBetH algorithm uses an empirical estimator (5) of this quantity to compute an approximation \u03b2\u0302SG of \u03b2SG purely from the revealed entries.\nIn the large r, regime, both \u03b2SG, \u03b2R decay to 0, so that we can further approximate\n1 = \u03bb(\u03b2SG) \u223c r, \u2192\u221e r\u03b22SGE[x2]E[y2] , (13)\n1 = \u00b5(\u03b2R) \u223c r, \u2192\u221e\n\u03b2R \u221a E[x2]E[y2] , (14)\n6 r 5 10 15 20 25 C (r ) 0.9 1 1.1 1.2 1.3 1.4 1.5 C(r) C(r \u2192 \u221e) 1 + 0.812 r\u22123/4\nFIG. 2: Location of the critical value as a function of the rank r. MaCBetH is able to estimate the correct rank from |E| > C(r)r \u221a nm known entries. We used a population dynamics algorithm with a population of size 1\u00d7 106 to compute the functions \u03bb and \u00b5 from (10,11). The dotted line is a fit suggesting that C(r)\u2212 1 = O(r\u22123/4).\nso that we reach the simple asymptotic expression, in the large , r limit, that c = r, or equivalently C(r) = 1. It is interesting to note that this result was obtained as the detectability threshold in completion of rank r = O(n) matrices from O(n2) entries in the Bayes optimal setting in [20]. Notice, however, that exact completion in the setting of [20] is only possible for > r(m+ n)/ \u221a nm: clearly detection and exact completion are different phenomena."}, {"heading": "B. Computation of the spectral density", "text": "In this section, we show how the spectral density of the Bethe Hessian can be computed analytically on tree-like graphs such as those generated by picking uniformly at random the observed entries of the matrix XY \u2020. The spectral density is defined as\n\u03bd(\u03bb) = lim n,m\u2192\u221e\n1\nn+m n+m\u2211 i=1 \u03b4(\u03bb\u2212 \u03bbi) , (15)\nwhere the \u03bbi\u2019s are the eigenvalues of the Bethe Hessian. Using again the cavity method, It can be shown [21] that the spectral density (in which potential delta peaks have been removed) is given by\n\u03bd(\u03bb) = lim n,m\u2192\u221e\n1\n\u03c0(n+m) n+m\u2211 i=1 Im\u2206i(\u03bb) , (16)\nwhere the \u2206i are complex variables living on the vertices of the graph G, which are given by:\n\u2206i = ( \u2212 \u03bb+ 1 + \u2211 k\u2208\u2202i sinh2 \u03b2Aik \u2212 \u2211 l\u2208\u2202i 1 4 sinh2(2\u03b2Ail)\u2206l\u2192i )\u22121 , (17)\nwhere \u2202i is the set of neighbors of i. The \u2206i\u2192j are the (linearly stable) solution of the following belief propagation recursion:\n\u2206i\u2192j = ( \u2212 \u03bb+ 1 + \u2211 k\u2208\u2202i sinh2 \u03b2Aik \u2212 \u2211 l\u2208\u2202i\\j 1 4 sinh2(2\u03b2Ail)\u2206l\u2192i )\u22121 . (18)\n7 \u03f5 2 3 4 5 6 7 8 9 10 M ea n in fe rr ed ra n k 0 0.5 1 1.5 2 2.5 3 Rank 3 n = m = 500 n = m = 2000 n = m = 8000 n = m = 16000 Transition \u03f5c \u03f5 9 10 11 12 13 14 15 16 17 18 19 0 1 2 3 4 5 6 7 8 9 10 Rank 10\nFIG. 3: Mean inferred rank as a function of , for different sizes. Each point is averaged over 100 samples of matrices XY \u2020 of size n\u00d7m, with the entries of X,Y drawn from a Gaussian distribution of mean 0 and variance 1. The theoretical transition is computed with a population dynamics algorithm (see section IVA). The finite size effects are considerable but consistent with the asymptotic prediction.\nThe ingredients to derive this formula are to turn the computation of the spectral density into a marginalization problem for a graphical model on the graph G, and then write the belief propagation equations to solve it. Quite remarkably, it has been shown [22] that this approach leads to an asymptotically exact (and rigorous) description of the spectral density on Erd\u0151s-R\u00e9nyi random graphs, which are locally tree-like in the limit where n,m\u2192\u221e. We can again solve equation (18) numerically using the belief propagation algorithm. The results are shown on Fig. 1: the bulk of the spectrum is always positive.\nWe now demonstrate that for any value of \u03b2 < \u03b2SG, there exists an open set around \u03bb = 0 where the spectral density vanishes. This justifies independently or choice for the parameter \u03b2. The proof follows [5] and begins by noticing that \u2206i\u2192j = cosh\u22122(\u03b2Aij) is a fixed point of the recursion (18) for \u03bb = 0. Since this fixed point is real, the corresponding spectral density is 0. Now consider a small perturbation \u03b4ij of this solution such that \u2206i\u2192j = cosh \u22122(\u03b2Aij)(1 + cosh \u22122(\u03b2Aij)\u03b4ij). The linearized version of (18) writes \u03b4i\u2192j = \u2211 l\u2208\u2202i\\j tanh\n2(\u03b2Ail)\u03b4i\u2192l . The linear operator thus defined is a weighted version of the non-backtracking matrix of [8]. Its spectral radius is given by \u03c1 = \u03bb(\u03b2), where \u03bb is defined in 10. In particular, for \u03b2 < \u03b2SG, \u03c1 < 1, so that a straightforward application [5] of the implicit function theorem allows to show that there exists a neighborhood U of 0 such that for any \u03bb \u2208 U , there exists a real, linearly stable fixed point of (18), yielding a spectral density equal to 0."}, {"heading": "V. NUMERICAL TESTS", "text": "The algorithm was implemented in Julia [23], using the NLopt optimization package [24] for the minimization of the discrepancy (2). A matlab demo using the implementation of the limited-memory BFGS algorithm of [25] is also available. Both demos can be downloaded from [26].\nFigure 3 illustrates the ability of the Bethe Hessian to infer the rank above the critical value c in the limit of large size n,m (see section IVA). In Figure 4, we demonstrate the suitability of the eigenvectors of the Bethe Hessian as starting point for the minimization of the cost function (2). We compare the final RMSE achieved on the reconstructed matrix XY \u2020 with 4 other initializations of the optimization, including the largest singular vectors of the trimmed matrix M [3]. MaCBetH systematically outperforms all the other choices of initial conditions. Remarkably, the performance achieved by MaCBetH with the inferred rank is essentially the same as the one achieved with an oracle rank. By contrast, estimating the correct rank from the (trimmed) SVD is more challenging. We note that for the choice of parameters we consider, trimming had a negligible effect. Along the same lines, OptSpace [3] uses a different\n8 10 20 30 40 50 P (R M S E < 10 \u2212 1 ) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Rank 3 Macbeth OR Tr-SVD OR Random OR Macbeth IR Tr-SVD IR\n\u03f5 10 20 30 40 50\nP (R\nM S E <\n10 \u2212 8 ) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n10 20 30 40 50 60 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1 Rank 10\n\u03f5 10 20 30 40 50 60 0\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nFIG. 4: RMSE as a function of the number of revealed entries per row : comparison between different initializations for the optimization of the cost function (2). The top row shows the probability that the achieved RMSE is smaller than 10\u22121, while the bottom row shows the probability that the final RMSE is smaller than 10\u22128. The probabilities were estimated as the frequency of success over 100 samples of matrices XY \u2020 of size 10000\u00d7 10000, with the entries of X,Y drawn from a Gaussian distribution of mean 0 and variance 1. All methods optimize the cost function (2) using a limited-memory BFGS algorithm [27] part of NLopt [24], starting from different initial conditions. The maximum number of iterations was set to 1000. The initial conditions compared are MaCBetH with oracle rank (MaCBetH OR) or inferred rank (MaCBetH IR), SVD of the observed matrixM after trimming, with oracle rank (Tr-SVD OR), or inferred rank (Tr-SVD IR, note that this is equivalent to OptSpace [3] in this regime), and random initial conditions with oracle rank (Random OR). For the Tr-SVD IR method, we inferred the rank from the SVD by looking for an index for which the ratio between two consecutive eigenvalues is minimized, as suggested in [28].\nminimization procedure, but from our tests we could not see any difference in performance due to that."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we have presented MaCBetH, an algorithm for matrix completion that is efficient for two distinct, complementary, tasks: (i) it has the ability to estimate a finite rank r reliably from fewer random entries than other existing approaches, and (ii) it gives lower root-mean-square reconstruction errors than its competitors. The algorithm is built around the Bethe Hessian matrix and leverages both on recent progresses in the construction of efficient spectral methods for clustering of sparse networks [5, 8, 9], and on the OptSpace approach [3] for matrix completion. Demos in Julia and matlab are available for download [26].\nThe method presented here offers a number of possible future directions, including replacing the minimization of the cost function by a message-passing type algorithm, the use of different neural network models, or a more theoretical direction involving the computation of information theoretically optimal transitions for detectability.\n9"}, {"heading": "Acknowledgment", "text": "The research leading to these results has received funding from the European Research Council under the European Union\u2019s 7th Framework Programme (FP/2007-2013/ERC Grant Agreement 307087-SPARCS).\n[1] E. J. Cand\u00e8s and B. Recht, \u201cExact matrix completion via convex optimization,\u201d Foundations of Computational mathematics, vol. 9, no. 6, pp. 717\u2013772, 2009. [2] E. J. Cand\u00e8s and T. Tao, \u201cThe power of convex relaxation: Near-optimal matrix completion,\u201d Information Theory, IEEE Transactions on, vol. 56, no. 5, pp. 2053\u20132080, 2010. [3] R. H. Keshavan, A. Montanari, and S. Oh, \u201cMatrix completion from a few entries,\u201d Information Theory, IEEE Transactions on, vol. 56, no. 6, pp. 2980\u20132998, 2010. [4] D. Gross, Y.-K. Liu, S. T. Flammia, S. Becker, and J. Eisert, \u201cQuantum state tomography via compressed sensing,\u201d Physical review letters, vol. 105, no. 15, p. 150401, 2010. [5] A. Saade, F. Krzakala, and L. Zdeborov\u00e1, \u201cSpectral clustering of graphs with the bethe hessian,\u201d in Advances in Neural Information Processing Systems, 2014, pp. 406\u2013414. [6] A. Saade, F. Krzakala, M. Lelarge, and L. Zdeborov\u00e1, \u201cSpectral detection in the censored block model,\u201d IEEE International Symposium on Information Theory (ISIT2015), to appear, 2015. [7] J.-F. Cai, E. J. Cand\u00e8s, and Z. Shen, \u201cA singular value thresholding algorithm for matrix completion,\u201d SIAM Journal on Optimization, vol. 20, no. 4, pp. 1956\u20131982, 2010. [8] F. Krzakala, C. Moore, E. Mossel, J. Neeman, A. Sly, L. Zdeborov\u00e1, and P. Zhang, \u201cSpectral redemption in clustering sparse networks,\u201d Proc. Natl. Acad. Sci., vol. 110, no. 52, pp. 20 935\u201320 940, 2013. [9] C. Bordenave, M. Lelarge, and L. Massouli\u00e9, \u201cNon-backtracking spectrum of random graphs: community detection and non-regular ramanujan graphs,\u201d 2015, arXiv:1501.06087. [10] J. J. Hopfield, \u201cNeural networks and physical systems with emergent collective computational abilities,\u201d Proc. Nat. Acad. Sci., vol. 79, no. 8, pp. 2554\u20132558, 1982. [11] J. S. Yedidia, W. T. Freeman, and Y. Weiss, \u201cBethe free energy, kikuchi approximations, and belief propagation algorithms,\u201d Advances in neural information processing systems, vol. 13, 2001. [12] M. Mezard and A. Montanari, Information, Physics, and Computation. Oxford University Press, 2009. [13] D. J. Amit, H. Gutfreund, and H. Sompolinsky, \u201cSpin-glass models of neural networks,\u201d Physical Review A, vol. 32, no. 2,\np. 1007, 1985. [14] B. Wemmenhove and A. Coolen, \u201cFinite connectivity attractor neural networks,\u201d Journal of Physics A: Mathematical and\nGeneral, vol. 36, no. 37, p. 9617, 2003. [15] I. P. Castillo and N. Skantzos, \u201cThe little\u2013hopfield model on a sparse random graph,\u201d Journal of Physics A: Mathematical\nand General, vol. 37, no. 39, p. 9087, 2004. [16] P. Zhang, \u201cNonbacktracking operator for the ising model and its applications in systems with multiple states,\u201d Physical\nReview E, vol. 91, no. 4, p. 042120, 2015. [17] J. M. Mooij and H. J. Kappen, \u201cValidity estimates for loopy belief propagation on binary real-world networks.\u201d in Advances\nin Neural Information Processing Systems, 2004, pp. 945\u2013952. [18] F. Ricci-Tersenghi, \u201cThe bethe approximation for solving the inverse ising problem: a comparison with other inference\nmethods,\u201d J. Stat. Mech.: Th. and Exp., p. P08015, 2012. [19] L. Zdeborov\u00e1, \u201cStatistical physics of hard optimization problems,\u201d acta physica slovaca, vol. 59, no. 3, pp. 169\u2013303, 2009. [20] Y. Kabashima, F. Krzakala, M. M\u00e9zard, A. Sakata, and L. Zdeborov\u00e1, \u201cPhase transitions and sample complexity in\nbayes-optimal matrix factorization,\u201d 2014, arXiv:1402.1298. [21] T. Rogers, I. P. Castillo, R. K\u00fchn, and K. Takeda, \u201cCavity approach to the spectral density of sparse symmetric random\nmatrices,\u201d Phys. Rev. E, vol. 78, no. 3, p. 031116, 2008. [22] C. Bordenave and M. Lelarge, \u201cResolvent of large random graphs,\u201d Random Structures and Algorithms, vol. 37, no. 3, pp.\n332\u2013352, 2010. [23] J. Bezanson, A. Edelman, S. Karpinski, and V. B. Shah, \u201cJulia: A fresh approach to numerical computing,\u201d 2014,\narXiv:1411.1607. [24] S. G. Johnson, \u201cThe nlopt nonlinear-optimization package,\u201d 2014. [25] M. Schmidt, \u201cminfunc: unconstrained differentiable multivariate optimization in matlab,\u201d http://www.cs.ubc.ca/\n~schmidtm/Software/minFunc.html , 2005. [26] The matlab (http://github.com/alaa-saade/macbeth_matlab) and the Julia (http://github.com/alaa-saade/macbeth_\njulia) implementaions are distributed on github. They are also linked on their author\u2019s webpage http://alaasaade.wordpress. com/ and are listed in the SPHINX group software page as well ( http://www.lps.ens.fr/~krzakala/WASP.html ). [27] D. C. Liu and J. Nocedal, \u201cOn the limited memory bfgs method for large scale optimization,\u201d Mathematical programming, vol. 45, no. 1-3, pp. 503\u2013528, 1989. [28] R. H. Keshavan, A. Montanari, and S. Oh, \u201cLow-rank matrix completion with noisy observations: a quantitative comparison,\u201d in 47th Annual Allerton Conference on Communication, Control, and Computing, 2009, pp. 1216\u20131222."}], "references": [{"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational mathematics, vol. 9, no. 6, pp. 717\u2013772, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "Information Theory, IEEE Transactions on, vol. 56, no. 5, pp. 2053\u20132080, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "Information Theory, IEEE Transactions on, vol. 56, no. 6, pp. 2980\u20132998, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Quantum state tomography via compressed sensing", "author": ["D. Gross", "Y.-K. Liu", "S.T. Flammia", "S. Becker", "J. Eisert"], "venue": "Physical review letters, vol. 105, no. 15, p. 150401, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Spectral clustering of graphs with the bethe hessian", "author": ["A. Saade", "F. Krzakala", "L. Zdeborov\u00e1"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 406\u2013414.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral detection in the censored block model", "author": ["A. Saade", "F. Krzakala", "M. Lelarge", "L. Zdeborov\u00e1"], "venue": "IEEE International Symposium on Information Theory (ISIT2015), to appear, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization, vol. 20, no. 4, pp. 1956\u20131982, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1956}, {"title": "Spectral redemption in clustering sparse networks", "author": ["F. Krzakala", "C. Moore", "E. Mossel", "J. Neeman", "A. Sly", "L. Zdeborov\u00e1", "P. Zhang"], "venue": "Proc. Natl. Acad. Sci., vol. 110, no. 52, pp. 20 935\u201320 940, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Non-backtracking spectrum of random graphs: community detection and non-regular ramanujan graphs", "author": ["C. Bordenave", "M. Lelarge", "L. Massouli\u00e9"], "venue": "2015, arXiv:1501.06087.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proc. Nat. Acad. Sci., vol. 79, no. 8, pp. 2554\u20132558, 1982.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1982}, {"title": "Bethe free energy, kikuchi approximations, and belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Advances in neural information processing systems, vol. 13, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Information, Physics, and Computation", "author": ["M. Mezard", "A. Montanari"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Spin-glass models of neural networks", "author": ["D.J. Amit", "H. Gutfreund", "H. Sompolinsky"], "venue": "Physical Review A, vol. 32, no. 2, p. 1007, 1985.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite connectivity attractor neural networks", "author": ["B. Wemmenhove", "A. Coolen"], "venue": "Journal of Physics A: Mathematical and General, vol. 36, no. 37, p. 9617, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "The little\u2013hopfield model on a sparse random graph", "author": ["I.P. Castillo", "N. Skantzos"], "venue": "Journal of Physics A: Mathematical and General, vol. 37, no. 39, p. 9087, 2004.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Nonbacktracking operator for the ising model and its applications in systems with multiple states", "author": ["P. Zhang"], "venue": "Physical Review E, vol. 91, no. 4, p. 042120, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Validity estimates for loopy belief propagation on binary real-world networks.", "author": ["J.M. Mooij", "H.J. Kappen"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "The bethe approximation for solving the inverse ising problem: a comparison with other inference methods", "author": ["F. Ricci-Tersenghi"], "venue": "J. Stat. Mech.: Th. and Exp., p. P08015, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical physics of hard optimization problems", "author": ["L. Zdeborov\u00e1"], "venue": "acta physica slovaca, vol. 59, no. 3, pp. 169\u2013303, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Phase transitions and sample complexity in bayes-optimal matrix factorization", "author": ["Y. Kabashima", "F. Krzakala", "M. M\u00e9zard", "A. Sakata", "L. Zdeborov\u00e1"], "venue": "2014, arXiv:1402.1298.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Cavity approach to the spectral density of sparse symmetric random matrices", "author": ["T. Rogers", "I.P. Castillo", "R. K\u00fchn", "K. Takeda"], "venue": "Phys. Rev. E, vol. 78, no. 3, p. 031116, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Resolvent of large random graphs", "author": ["C. Bordenave", "M. Lelarge"], "venue": "Random Structures and Algorithms, vol. 37, no. 3, pp. 332\u2013352, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Julia: A fresh approach to numerical computing", "author": ["J. Bezanson", "A. Edelman", "S. Karpinski", "V.B. Shah"], "venue": "2014, arXiv:1411.1607.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "The nlopt nonlinear-optimization package", "author": ["S.G. Johnson"], "venue": "2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "minfunc: unconstrained differentiable multivariate optimization in matlab", "author": ["M. Schmidt"], "venue": "http://www.cs.ubc.ca/ ~schmidtm/Software/minFunc.html , 2005.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming, vol. 45, no. 1-3, pp. 503\u2013528, 1989.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1989}, {"title": "Low-rank matrix completion with noisy observations: a quantitative comparison", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "47th Annual Allerton Conference on Communication, Control, and Computing, 2009, pp. 1216\u20131222.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "[1\u20133], motivated by many applications such as collaborative filtering [1], quantum tomography [4] in physics, or the analysis of a covariance matrix [1].", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[1\u20133], motivated by many applications such as collaborative filtering [1], quantum tomography [4] in physics, or the analysis of a covariance matrix [1].", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[1\u20133], motivated by many applications such as collaborative filtering [1], quantum tomography [4] in physics, or the analysis of a covariance matrix [1].", "startOffset": 0, "endOffset": 5}, {"referenceID": 0, "context": "[1\u20133], motivated by many applications such as collaborative filtering [1], quantum tomography [4] in physics, or the analysis of a covariance matrix [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "[1\u20133], motivated by many applications such as collaborative filtering [1], quantum tomography [4] in physics, or the analysis of a covariance matrix [1].", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "[1\u20133], motivated by many applications such as collaborative filtering [1], quantum tomography [4] in physics, or the analysis of a covariance matrix [1].", "startOffset": 149, "endOffset": 152}, {"referenceID": 0, "context": "The most widely considered question in this setting is how many entries need to be revealed such that the matrix can be completed exactly in a computationally efficient way [1, 3].", "startOffset": 173, "endOffset": 179}, {"referenceID": 2, "context": "The most widely considered question in this setting is how many entries need to be revealed such that the matrix can be completed exactly in a computationally efficient way [1, 3].", "startOffset": 173, "endOffset": 179}, {"referenceID": 4, "context": "The rank in our algorithm is estimated as the number of negative eigenvalues of an associated Bethe Hessian matrix [5, 6], and the corresponding eigenvectors are used as an initial condition for the local optimization of a cost function commonly considered in matrix completion (see e.", "startOffset": 115, "endOffset": 121}, {"referenceID": 5, "context": "The rank in our algorithm is estimated as the number of negative eigenvalues of an associated Bethe Hessian matrix [5, 6], and the corresponding eigenvectors are used as an initial condition for the local optimization of a cost function commonly considered in matrix completion (see e.", "startOffset": 115, "endOffset": 121}, {"referenceID": 2, "context": "[3]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The corresponding RMSE is evaluated empirically, and in the regime close to C(r)r \u221a nm, it compares very favorably to existing approaches, in particular to OptSpace [3].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "The matrix completion problem was popularized in [1] who proposed nuclear norm minimization as a convex relaxation of the problem.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "A low complexity procedure to solve the problem was later proposed by [7] and is based on singular value decomposition (SVD).", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "A considerable step towards theoretical understanding of matrix completion from few entries was made in [3] who proved that with the use of trimming the performance of SVD-based matrix completion can be improved and a RMSE proportional to \u221a nr/|E| can be achieved.", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "The algorithm of [3] is referred to as OptSpace, and empirically it achieves state-of-the-art RMSE in the regime of very few revealed entries.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "OptSpace proceeds in three steps [3].", "startOffset": 33, "endOffset": 36}, {"referenceID": 4, "context": "Our method leverages on recent progress made in the task of detecting communities in the stochastic block model [5, 8] with spectral methods.", "startOffset": 112, "endOffset": 118}, {"referenceID": 7, "context": "Our method leverages on recent progress made in the task of detecting communities in the stochastic block model [5, 8] with spectral methods.", "startOffset": 112, "endOffset": 118}, {"referenceID": 2, "context": "Both in community detection and matrix completion, traditional spectral methods fail in the very sparse regime due to the existence of spurious large eigenvalues (or singular values) corresponding to localized eigenvectors [3, 8].", "startOffset": 223, "endOffset": 229}, {"referenceID": 7, "context": "Both in community detection and matrix completion, traditional spectral methods fail in the very sparse regime due to the existence of spurious large eigenvalues (or singular values) corresponding to localized eigenvectors [3, 8].", "startOffset": 223, "endOffset": 229}, {"referenceID": 4, "context": "The authors of [5, 8, 9] showed that using the non-backtracking matrix or the closely related Bethe Hessian as a basis for the spectral method in community detection provides reliable rank estimation and better inference performance.", "startOffset": 15, "endOffset": 24}, {"referenceID": 7, "context": "The authors of [5, 8, 9] showed that using the non-backtracking matrix or the closely related Bethe Hessian as a basis for the spectral method in community detection provides reliable rank estimation and better inference performance.", "startOffset": 15, "endOffset": 24}, {"referenceID": 8, "context": "The authors of [5, 8, 9] showed that using the non-backtracking matrix or the closely related Bethe Hessian as a basis for the spectral method in community detection provides reliable rank estimation and better inference performance.", "startOffset": 15, "endOffset": 24}, {"referenceID": 2, "context": "[3]) is to minimize the cost function", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In step 2 we could also use the non-backtracking matrix weighted by tanh\u03b2Mij , it was shown in [5] that the spectrum of the Bethe Hessian and the non-backtracking matrix are closely related.", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "This model is a (generalized) Hebbian Hopfield model [10] on a bipartite sparse graph.", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "To study it, we can use the standard Bethe approximation which is widely believed to be exact for such problems on large random graphs [11, 12].", "startOffset": 135, "endOffset": 143}, {"referenceID": 11, "context": "To study it, we can use the standard Bethe approximation which is widely believed to be exact for such problems on large random graphs [11, 12].", "startOffset": 135, "endOffset": 143}, {"referenceID": 11, "context": "[12\u201316] and references therein) and the phenomenology, that we shall review briefly here, is well known.", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[12\u201316] and references therein) and the phenomenology, that we shall review briefly here, is well known.", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "[12\u201316] and references therein) and the phenomenology, that we shall review briefly here, is well known.", "startOffset": 0, "endOffset": 7}, {"referenceID": 14, "context": "[12\u201316] and references therein) and the phenomenology, that we shall review briefly here, is well known.", "startOffset": 0, "endOffset": 7}, {"referenceID": 15, "context": "[12\u201316] and references therein) and the phenomenology, that we shall review briefly here, is well known.", "startOffset": 0, "endOffset": 7}, {"referenceID": 4, "context": "It would be tempting to continue the Bethe approach and to derive the belief propagation equations, but we shall here instead consider a simpler spectral relaxation of the problem, following the same strategy as used in [5, 6] for graph clustering.", "startOffset": 220, "endOffset": 226}, {"referenceID": 5, "context": "It would be tempting to continue the Bethe approach and to derive the belief propagation equations, but we shall here instead consider a simpler spectral relaxation of the problem, following the same strategy as used in [5, 6] for graph clustering.", "startOffset": 220, "endOffset": 226}, {"referenceID": 16, "context": "First, we use the fact that the paramagnetic state (8) is always a stationary point of the Bethe free energy, for any value of \u03b2 [17, 18].", "startOffset": 129, "endOffset": 137}, {"referenceID": 17, "context": "First, we use the fact that the paramagnetic state (8) is always a stationary point of the Bethe free energy, for any value of \u03b2 [17, 18].", "startOffset": 129, "endOffset": 137}, {"referenceID": 4, "context": "At this point, the elements of the Hessian involving one derivative with respect to \u03beij vanish, while the block involving two such derivatives is a diagonal positive definite matrix [5, 17].", "startOffset": 182, "endOffset": 189}, {"referenceID": 16, "context": "At this point, the elements of the Hessian involving one derivative with respect to \u03beij vanish, while the block involving two such derivatives is a diagonal positive definite matrix [5, 17].", "startOffset": 182, "endOffset": 189}, {"referenceID": 4, "context": "The remaining part is the matrix called Bethe Hessian in [5],", "startOffset": 57, "endOffset": 60}, {"referenceID": 15, "context": "Note that a similar approach was used in [16] to detect the retrieval states of a Hopfield model using the weighted non-backtracking matrix [8], which linearizes the belief propagation equations rather than the Bethe free energy, resulting in a larger, non-symmetric matrix.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "Note that a similar approach was used in [16] to detect the retrieval states of a Hopfield model using the weighted non-backtracking matrix [8], which linearizes the belief propagation equations rather than the Bethe free energy, resulting in a larger, non-symmetric matrix.", "startOffset": 140, "endOffset": 143}, {"referenceID": 12, "context": "We have repeated the computations of [13\u201316] in the case of model (6), using the cavity method [12].", "startOffset": 37, "endOffset": 44}, {"referenceID": 13, "context": "We have repeated the computations of [13\u201316] in the case of model (6), using the cavity method [12].", "startOffset": 37, "endOffset": 44}, {"referenceID": 14, "context": "We have repeated the computations of [13\u201316] in the case of model (6), using the cavity method [12].", "startOffset": 37, "endOffset": 44}, {"referenceID": 15, "context": "We have repeated the computations of [13\u201316] in the case of model (6), using the cavity method [12].", "startOffset": 37, "endOffset": 44}, {"referenceID": 11, "context": "We have repeated the computations of [13\u201316] in the case of model (6), using the cavity method [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "We refer the reader interested in the technical details of the statistical mechanics approach to neural networks to [14\u201316].", "startOffset": 116, "endOffset": 123}, {"referenceID": 14, "context": "We refer the reader interested in the technical details of the statistical mechanics approach to neural networks to [14\u201316].", "startOffset": 116, "endOffset": 123}, {"referenceID": 15, "context": "We refer the reader interested in the technical details of the statistical mechanics approach to neural networks to [14\u201316].", "startOffset": 116, "endOffset": 123}, {"referenceID": 11, "context": "[12, 19]), the stability of the paramagnetic state (8) towards these two phases can be monitored in terms of the two following parameters:", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[12, 19]), the stability of the paramagnetic state (8) towards these two phases can be monitored in terms of the two following parameters:", "startOffset": 0, "endOffset": 8}, {"referenceID": 11, "context": "We thus computed c numerically using a population dynamics algorithm [12] and the results for C(r) = c/r are presented on Figure 2.", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "It is interesting to note that this result was obtained as the detectability threshold in completion of rank r = O(n) matrices from O(n) entries in the Bayes optimal setting in [20].", "startOffset": 177, "endOffset": 181}, {"referenceID": 19, "context": "Notice, however, that exact completion in the setting of [20] is only possible for > r(m+ n)/ \u221a nm: clearly detection and exact completion are different phenomena.", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "Using again the cavity method, It can be shown [21] that the spectral density (in which potential delta peaks have been removed) is given by", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "Quite remarkably, it has been shown [22] that this approach leads to an asymptotically exact (and rigorous) description of the spectral density on Erd\u0151s-R\u00e9nyi random graphs, which are locally tree-like in the limit where n,m\u2192\u221e.", "startOffset": 36, "endOffset": 40}, {"referenceID": 4, "context": "The proof follows [5] and begins by noticing that \u2206i\u2192j = cosh(\u03b2Aij) is a fixed point of the recursion (18) for \u03bb = 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "The linear operator thus defined is a weighted version of the non-backtracking matrix of [8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "In particular, for \u03b2 < \u03b2SG, \u03c1 < 1, so that a straightforward application [5] of the implicit function theorem allows to show that there exists a neighborhood U of 0 such that for any \u03bb \u2208 U , there exists a real, linearly stable fixed point of (18), yielding a spectral density equal to 0.", "startOffset": 73, "endOffset": 76}, {"referenceID": 22, "context": "The algorithm was implemented in Julia [23], using the NLopt optimization package [24] for the minimization of the discrepancy (2).", "startOffset": 39, "endOffset": 43}, {"referenceID": 23, "context": "The algorithm was implemented in Julia [23], using the NLopt optimization package [24] for the minimization of the discrepancy (2).", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "A matlab demo using the implementation of the limited-memory BFGS algorithm of [25] is also available.", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "We compare the final RMSE achieved on the reconstructed matrix XY \u2020 with 4 other initializations of the optimization, including the largest singular vectors of the trimmed matrix M [3].", "startOffset": 181, "endOffset": 184}, {"referenceID": 2, "context": "Along the same lines, OptSpace [3] uses a different", "startOffset": 31, "endOffset": 34}, {"referenceID": 25, "context": "All methods optimize the cost function (2) using a limited-memory BFGS algorithm [27] part of NLopt [24], starting from different initial conditions.", "startOffset": 81, "endOffset": 85}, {"referenceID": 23, "context": "All methods optimize the cost function (2) using a limited-memory BFGS algorithm [27] part of NLopt [24], starting from different initial conditions.", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "The initial conditions compared are MaCBetH with oracle rank (MaCBetH OR) or inferred rank (MaCBetH IR), SVD of the observed matrixM after trimming, with oracle rank (Tr-SVD OR), or inferred rank (Tr-SVD IR, note that this is equivalent to OptSpace [3] in this regime), and random initial conditions with oracle rank (Random OR).", "startOffset": 249, "endOffset": 252}, {"referenceID": 26, "context": "For the Tr-SVD IR method, we inferred the rank from the SVD by looking for an index for which the ratio between two consecutive eigenvalues is minimized, as suggested in [28].", "startOffset": 170, "endOffset": 174}, {"referenceID": 4, "context": "The algorithm is built around the Bethe Hessian matrix and leverages both on recent progresses in the construction of efficient spectral methods for clustering of sparse networks [5, 8, 9], and on the OptSpace approach [3] for matrix completion.", "startOffset": 179, "endOffset": 188}, {"referenceID": 7, "context": "The algorithm is built around the Bethe Hessian matrix and leverages both on recent progresses in the construction of efficient spectral methods for clustering of sparse networks [5, 8, 9], and on the OptSpace approach [3] for matrix completion.", "startOffset": 179, "endOffset": 188}, {"referenceID": 8, "context": "The algorithm is built around the Bethe Hessian matrix and leverages both on recent progresses in the construction of efficient spectral methods for clustering of sparse networks [5, 8, 9], and on the OptSpace approach [3] for matrix completion.", "startOffset": 179, "endOffset": 188}, {"referenceID": 2, "context": "The algorithm is built around the Bethe Hessian matrix and leverages both on recent progresses in the construction of efficient spectral methods for clustering of sparse networks [5, 8, 9], and on the OptSpace approach [3] for matrix completion.", "startOffset": 219, "endOffset": 222}, {"referenceID": 0, "context": "[1] E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] R.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "A. Saade, F. Krzakala and L. Zdeborov\u00e1 1 Laboratoire de Physique Statistique, UMR 8550 CNRS, Department of Physics, \u00c9cole Normale Sup\u00e9rieure and PSL Research University, Rue Lhomond, 75005 Paris, France 2 Sorbonne Universit\u00e9s, UPMC Univ Paris 06, UMR 8550, LPS, F-75005, Paris, France 3 ESPCI and CNRS UMR 7083 Gulliver, 10 rue Vauquelin,Paris 75005 4 Institut de Physique Th\u00e9orique, CEA Saclay and URA 2306, CNRS, 91191 Gif-sur-Yvette, France (Dated: January 29, 2016)", "creator": "LaTeX with hyperref package"}}}