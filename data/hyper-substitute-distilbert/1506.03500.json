{"id": "1506.03500", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image Generation", "abstract": "we introduce language - driven path generation, his task of generating an algorithm comprising partially specified contents assuming automatic database embedding, m. g., given initial word embedding of terms, if generate to natural image of its grasshopper. we implement a spectral method based on two mapping functions. the first takes as input a sample text ( as cf, it. g., by the word2vec library ) and falls visually towards a high - level object disk ( e. on., optical space constructed by evaluation of code top protocols for a single temporal network ). the second function convert raw abstract picture representation to clear space, by order you generate the target image. several equivalent capabilities include indicating the underlying system seeks images that capture general visual facts under the objects encoded in any document embedding, regarded as syntax domain typical environment, and are sufficient also discriminate between general categories alongside objects.", "histories": [["v1", "Wed, 10 Jun 2015 22:57:20 GMT  (2248kb,D)", "http://arxiv.org/abs/1506.03500v1", null], ["v2", "Mon, 23 Nov 2015 16:36:48 GMT  (2248kb,D)", "http://arxiv.org/abs/1506.03500v2", "A 6-page version to appear at the Multimodal Machine Learning NIPS 2015 Workshop"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["angeliki lazaridou", "dat tien nguyen", "raffaella bernardi", "marco baroni"], "accepted": false, "id": "1506.03500"}, "pdf": {"name": "1506.03500.pdf", "metadata": {"source": "CRF", "title": "Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image Generation", "authors": ["Angeliki Lazaridou", "Dat Tien Nguyen", "Raffaella Bernardi", "Marco Baroni"], "emails": ["firstname.lastname@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "Imagination, creating new images in the mind, is a fundamental capability of humans, studies of which date back to Plato\u2019s ideas about memory and perception. Through imagery, we form mental images, picture-like representations in our mind, that encode and extend our perceptual and linguistic experience of the world. Recent work in neuroscience attempts to generate reconstructions of these mental images, as encoded in vector-based representations of fMRI patterns [15]. In this work, we take the first steps towards implementing the same paradigm in a computational setup, by generating images that reflect the imagery of distributed word representations.\nWe introduce language-driven image generation, the task of visualizing the contents of a linguistic message, as encoded in word embeddings, by generating a real image. Language-driven image generation can serve as evaluation tool providing intuitive visualization of what computational representations of word meaning encode. More ambitiously, effective language-driven image generation could complement image search and retrieval, producing images for words that are not associated to images in a certain collection, either for sparsity, or due to their inherent properties (e.g., artists and psychologists might be interested in images of abstract or novel words). In this work, we focus on generating images for distributed representations encoding the meaning of single words. However, given recent advances in compositional distributed semantics [20] that produce embeddings for arbitrarily long linguistic units, we also see our contribution as the first step towards generating images depicting the meaning of phrases (e.g., blue car) and sentences. After all, language-driven image generation can be seen as the symmetric goal of recent research (e.g., [7, 8]) that introduced effective methods to generate linguistic descriptions of the contents of a given image.\nTo perform language-driven image generation, we combine various recent strands of research. Tools such as word2vec [12] and Glove [16] have been shown to produce extremely high-quality vectorbased word embeddings. At the same time, in computer vision, images are effectively represented\nar X\niv :1\n50 6.\n03 50\n0v 1\n[ cs\n.C V\n] 1\n0 Ju\nn 20\napp lian ce cont aine r furn itur\ne/la rge\ntool\ninst rum\nent\ngarm ent tool wea pon toy spor ts veh icle buil ding stru ctur e food fruit\n/veg etab\nle\nnatu ral o\nbjec t\nplan t bird fish inse ct rept\nile/a mph\nibia n\nmam mal\nMan-made Organic Animals\nA B C D E F\nH I J\nG\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nFigure 1: Generated images of 10 concepts per category for 20 basic categories, grouped my macrocategory. See supplementary materials for the answer key.\nby vectors of abstract visual features, such as those extracted by Convolutional Neural Networks (CNNs) [9]. Consequently, the problem of translating between linguistic and visual representations has been coached in terms of learning a cross-modal mapping function between vector spaces [4, 19]. Finally, recent work in computer vision, motivated by the desire to achieve a better understanding of what the layers of CNNs and other deep architectures have really learned, has proposed feature inversion techniques that map a representation in abstract visual feature space (e.g., from the top layer of a CNN) back onto pixel space, to produce a real image [24, 10].\nOur language-driven image generation system takes a word embedding as input (e.g., the word2vec vector for grasshopper), projects it with a cross-modal function onto visual space (e.g., onto a representation in the space defined by a CNN layer), and then applies feature inversion to it (using the method HOGgles method of [22]) to generate an actual image (cell A18 in Figure 3). We test our system in a rigorous zero-shot setup, in which words and images of tested concepts are neither used to train cross-modal mapping, nor employed to induce the feature inversion function. So, for example, our system mapped grasshopper onto visual and then pixel space without having ever been exposed to grasshopper pictures.\nFigure 3 illustrates our results (\u201canswer key\u201d for the figure provided as supplementary material). While it is difficult to discriminate among similar objects based on these images, the figure shows that our language-driven image generation method already captures the broad gist of different domains (food looks like food, animals are blobs in a natural environment, and so on)."}, {"heading": "2 Language-driven image generation", "text": ""}, {"heading": "2.1 From word to visual vectors", "text": "Up to now, feature inversion algorithms [10, 22, 24] have been applied to visual representations directly extracted from images (hence the \u201cinversion\u201d name). We aim instead at generating an image conveying the semantics of a concept as encoded in a word representation. Thus, we need a way to \u201ctranslate\u201d the word representation into a visual representation, i.e., a representation laying on the visual space that conveys the corresponding visual semantics of the word.\nCross-modal mapping has been first introduced in the context of zero-shot learning as a way to address the manual annotation bottleneck in domains where other vector-based representations (e.g.,\nimages or brain signals) must be associated to word labels [13, 19]. This is achieved by using training data to learn a mapping function from vectors in the domain of interest to vector representations of word labels. In our case, we are interested in the general ability of cross-modal mapping to translate a representation between different spaces, and specifically from a word to a visual feature space.\nThe mapping is performed by inducing a function f : Rd1 \u2192 Rd2 from data points (wi, vi), where wi \u2208 Rd1 is a word representation and vi \u2208 Rd2 the corresponding visual representation. The mapping function can then be applied to any given word vector wj to obtain its projection v\u0302j = f(wj) onto visual space. Following previous work [13, 4], we assume that the mapping is linear. To estimate its parameters M \u2208 Rd1\u00d7d2, given word vectors W paired with visual vectors V, we use Elastic-Net-penalized least squares regression, that linearly combines the L1 and L2 weight penalties of Lasso and Ridge regularization:\nM\u0302 = argmin M\u2208Rd1\u00d7d2\n\u2016WM\u2212V\u2016F + \u03bb1\u2016M\u20161 + \u03bb2\u2016M\u2016F (1)\nBy modifying the weights of the L1 and L2 penalties, \u03bb1 and \u03bb2, we can derive different regression methods. Specifically, we experiment with plain regression (\u03bb1 = 0, \u03bb2 = 0), ridge regression (\u03bb1 = 0, \u03bb2 6= 0), lasso regression (\u03bb1 6= 0 and \u03bb2 = 0) and symmetric elastic net (\u03bb1 = \u03bb2, \u03bb1 6= 0)."}, {"heading": "2.2 From visual vectors to images", "text": "Convolutional Neural Networks have recently surpassed human performance on object recognition [17]. Nevertheless, these models exhibit \u201cintriguing properties\u201d, that are somewhat surprising given their state-of-the-art performance [21], prompting an effort to reach a deeper understanding of how they really work. Given that these models consist of millions of parameters, there is ongoing research on feature inversion of different CNN layers to attain an intuitive visualization of what each of them learned.\nSeveral methods have been proposed for inverting CNN visual features, however, the exact nature of the task imposes certain constraints on the inversion method. For example, the original work of Zeiler and Fergus [24] cannot be straightforwardly adapted to our task of generating images from word embeddings, since their DeConvNet method requires information related to the activations of the network in several layers. In this work, we adopt the framework of Vondrick et al. [22] that casts the problem of inversion as paired dictionary learning.1\nSpecifically, given an image x0 \u2208 RD and its visual representation y = \u03c6(x0) \u2208 Rd, the goal is to find an image x\u2217 that minimizes the reconstruction error:\nx\u2217 = argmin x\u2208RD \u2016\u03c6(x)\u2212 y\u201622 (2)\nGiven that there are no guarantees regarding the convexity of \u03c6, both images and visual representations are approximated by paired, over-complete bases, U \u2208 RD\u00d7K and V \u2208 Rd\u00d7K , respectively. Enforcing U and V to have paired representations through shared coefficients \u03b1 \u2208 RK , i.e., x0 = U\u03b1 and y = V \u03b1, allows the feature inversion to be done by estimating such coefficients \u03b1 that minimize the reconstruction error. Practically, the algorithm proceeds by finding U , V and \u03b1 through a standard sparse coding method. For learning the parameters, the algorithm is presented with training data of the form (xi, yi), where xi is an image patch and yi the corresponding visual vector associated with that patch."}, {"heading": "3 Experimental Setup", "text": ""}, {"heading": "3.1 Materials", "text": "Dreamed Concepts We refer to the words we generate images for as dreamed concepts. The dreamed word set comes from the concepts studied by McRae et al. [11], in the context of property\n1Originally, the HOGgles method of [22] was introduced for visualizing HOG features. However, the method does not make feature-specific assumptions and it has also recently been used to invert CNN features [23].\nnorm generation. This set contains 541 base-level concrete concepts (e.g., cat, apple, car etc.) that span across 20 general and broad categories (e.g., animal, fruit/vegetable, vehicle etc). For the purposes of the current experiments, 69 McRae concepts were excluded (either because of high ambiguity or for technical reasons), resulting in 472 dreamed words we test on.\nSeen Concepts We refer to the set of words associated to real pictures that are used for training purposes as seen concepts. The real picture set contains approximately 480K images extracted from ImageNet [3] representing 5K distinct concepts. The seen concepts are used for training the crossmodal mapping. Importantly, the dreamed and seen concept sets do emphnot overlap.\nWord Representations For all seen and dreamed concepts, we build 300-dimensional word vectors with the word2vec toolkit,2 choosing the CBOW method.3 CBOW, which learns to predict a target word from the ones surrounding it, produces state-of-the-art results in many linguistic tasks [1]. Word vectors are induced from a language corpus (e.g., Wikipedia) of 2.8 billion words.4\nVisual Representations The visual representations, for the set of 480K seen concept images, are extracted with the pre-trained CNN model of [9] through the Caffe toolkit [6]. CNNs trained on natural images learn a hierarchy of increasingly more abstract properties: the features in the bottom layers resemble Gabor filters, while features in the top layers capture more abstract properties of the dataset or tasks the CNN is trained for (see [24]) (e.g., the topmost layer captures a distribution over training labels). In this work, we experiment with feature representations extracted from two levels, pool-5, extracted from the 5th layer (6x6x256=9216 dimensions), and fc-7, extracted from the 7th layer (1x4096 dimensions). pool-5 is an intermediate pooling layer that should capture object commonalities. fc-7 is a fully-connected layer just below the topmost one, and as such it is expected to capture high-level discriminative features of different object classes.\nSince each seen concept is associated with many images, we experiment with two ways to derive a unique visual representation. Inspired from categorization schemes in cognitive science [14], we will refer to them as the prototype and exemplar methods. The prototype visual vector of a concept is constructed by averaging the visual representations (either pool-5 or fc-7) of images tagged in ImageNet with the concept. The averaging method should smooth out noise and emphasize invariances in images associated to a concept. On the other hand, the constructed prototype does not correspond to an actual depiction of the concept. The exemplar visual vector, on the other hand, is a single visual vector that is picked as a good representative of the set, as it is the one with the highest average cosine similarity to all other vectors extracted from images labeled with the same concept."}, {"heading": "3.2 Model selection and parameter estimation", "text": "Visual feature type and concept representations In order to determine the optimal visual feature type (between pool-5 and fc-7) and concept representation method (between prototype and exemplar), we set up a human study through CrowdFlower.5 For 50 randomly chosen test concepts, we generate 4 images, each obtained by inverting the visual vector computed by combining a feature type with a concept representation method, e.g., for pool-5+prototype, we generate an image by inverting the visual vector resulting from averaging the pool-5 feature vectors extracted from images labeled with the test concept (details on our implementation of feature inversion below). Participants are then asked to judge which of the 4 images is more likely to denote the test concept. For each test concept, we collect 20 judgments. Overall, participants showed a strong significant preference for the images generated from inverting pool-5 feature vectors (28/50), and in particular for those that were generated from pool-5 by inverting feature vectors constructed with the exemplar protocol (18/50).6 The following experiments were thus carried out using the pool-5+exemplar visual space.\n2https://code.google.com/p/word2vec/ 3Other hyperparameters, adopted without tuning, include a context window size of 5 words to either side of the target, setting the sub-sampling option to 1e-05 and estimating the probability of target words by negative sampling, drawing 10 samples from the noise distribution [12].\n4Corpus sources: http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk 5http://www.crowdflower.com/ 6Throughout this paper, statistical significance is assessed with two-tailed exact binomial tests with thresh-\nold \u03b1 < 0.05, corrected for multiple comparisons with the false discovery rate method.\nCross-modal mapping To learn the mapping M of Equation 1, we use 5K training pairs (wc,vc) = {wc \u2208 R300,vc \u2208 R9216}, where wc is the word vector and vc is the visual vector for the (seen) concept c, based on pool-5 features and exemplar representation. Specifically, we estimate the weights M by training the 4 regression methods described in Section 2.1 above, cross-validating the values of \u03bb1 and \u03bb2 on the training data. Model selection is performed by conducting a human study on the language-driven image generation task. For the same test of 50 concepts as above, we obtain estimates of their visual vectors v\u0302 by mapping their word vectors into visual space through the different mapping functions M. We then generate an image by inverting the visual features v\u0302. Participants are again asked to judge which of the 4 images is more likely to denote the test concept. For each concept we collected 20 judgments. Participants showed a preference for plain regression (9/50 significant tests in favor of this model), which we adopt in rest of the paper.\nFeature inversion Training data for feature inversion (Section 2.2 above) are created by using the PASCAL VOC 2011 dataset, that contains 15K images of 20 distinct objects. Note that the 20 PASCAL objects are not part of our dreamed concepts, and thus the feature inversion is performed in a zero-shot way (the inversion will be asked to generate an image for a concept that it has never encountered before). In order to increase the size of the training data, from each image we derived several image patches xi associated with different parts of the image and paired them with their equivalent visual representations yi. Both paired dictionary learning and feature inversion are conducted using the HOGgles software [22] with default hyperparameters.7"}, {"heading": "4 Experiments", "text": "Figure 3 provides a snapshot of our results; we randomly picked 10 dreamed concepts from each of the 20 McRae categories, and we show the image we generated for them from the corresponding word embeddings, as described in Section 2. We stress again that the images of dreamed concepts were never used in any step of the pipeline, neither to train cross-modal mapping, nor to train feature inversion, so they are genuinely generated in a zero-shot manner, by leveraging their linguistic associations to seen concepts.\nNot surprisingly, the images we generate are not as clear as those one would get by retrieving existing images. However, we see in the figure that concepts belonging to different categories are clearly distinguished, with the exception of food and fruit/vegetable (columns 12 and 13), that look very much the same (on the other hand, fruit and vegetable are also food, and word vectors extracted from corpora will likely emphasize this \u201cfunctional\u201d role of theirs).\nWe next present a series of user studies providing quantitative and qualitative insights into the information that subjects can extract from the visual properties of the generated images."}, {"heading": "4.1 Experiment 1: Correct word vs. random confounder", "text": "The first experiment is a sanity check, evaluating whether the visual properties in the generated images are informative enough for subjects to guess the correct label against a random alternative.\nExperiment description Participants are presented with the generated image of a dreamed concept and are asked to judge if it is more likely to denote the correct word or a confounder randomly picked from the seen word set. Given that the confounder is a randomly picked item, the task is relatively easy. However, both confounders and dreamed concepts are concrete, basic-level concepts, so they are sometimes related just by chance. Moreover, the confounders were used to train the mapping and inversion functions, which could have introduced a systematic bias in their favour. We test the 472 dreamed concepts, collecting 20 ratings for each via CrowdFlower. Word order is randomized both across and within trials (the same setup is used in the following experiments, with image order also randomized).\nResults Participants show a consistent preference for the correct word (dreamed concept) (median proportion of votes in favor: 75%). Preference for the correct word is significantly different from chance in 211/472 cases. Participants expressed a significant preference for the confounder in 10\n7https://github.com/CSAILVision/ihog\ncases only, and in the majority of those, dreamed concepts and their confounders shared similar properties, e.g., cape-tabletop (both made of textile), zebra-baboon (both mammals), oak-boathouse (existing in similar natural environments).\nThe experiment confirms that our method can generally capture at least those visual properties of dreamed concepts that can distinguish them from visually dissimilar random items."}, {"heading": "4.2 Experiment 2: Correct image vs. image of similar concept", "text": "The second experiment ascertains to what extent subjects can pick the right generated image for a dreamed concept over a closely related alternative.\nExperiment description For each dreamed concept, we pick as confounder the closest semantic neighbor according to the subject-based conceptual distance statistics provided by McRae et al. [11]. In 379/472 cases, the confounder belongs to the category of the dreamed concept; hence, distinguishing the two concepts is quite challenging (e.g., mandarin vs. pumpkin). Participants were presented with the images generated from the dreamed concept and the confounder, and they were asked which of the two images is more likely to denote the dreamed concept.\nResults Results and examples are provided in Table 1. In the vast majority of cases (409/472) the participants did not show a significant preference for either the correct image or the confounder. This shows that the current image generation pipeline does not capture, yet, fine-grained properties that would allow within-category discrimination. Still, within the subset of 63 cases for which subjects did express a significant preferences, we observe a clear trend in favour of the correct image (41 vs. 22). Color and environment seem to be the fine-grained properties that determined many of the subjects\u2019 right or wrong choices within this subset. Of the 63 pairs, 14 involve concepts from different categories, and 49 same-category pairs. Of the former, in 11/14 the preference was for the right image. In 2 of the 3 wrong cases, the dreamed concept vs. intruder pairs have similar color (emerald vs. parsley, bowl vs. dish), while neither concept has a typical discriminative color in the third case (thermometer vs. marble). Even in the challenging same-category group, 30/49 pairs display the right preference. In particular, subjects distinguished objects that typically have different colors (e.g., flamingo vs. partridge), or live in different environments (e.g., turtle vs. tortoise). In the remaining 19 within-category cases in which the confounder was preferred, color seems again to play a crucial role in the confusion (e.g., alligator vs. crocodile, asparagus vs. spinach).\nWe next ran a follow-up experiment to find out to what extent the lack of precision of our algorithm should be attributed to noise in image generation from abstract visual features, independently of the linguistic origin of the signal. For these purposes, we replaced the visual feature vector produced by cross-modal mapping with the \u201cgold-standard\u201d visual vector for each dreamed/confounder concept (e.g., instead of mapping the partridge word vector onto visual space, we generated a partridge image by inverting a pool-5+exemplar vector directly extracted from a set of images labeled with this word obtained from ImageNet). We repeated the Experiment 2 setup using the images generated in this way. In this case, the number of pairs for which no significant preference emerged was 75.4% (356/472), in 17.6% (83/472) of the cases there was a significant preference for the correct image, and in 7% (33/472) for the confounder. The results in this setting are better than when visual features are derived from word representations, but not dramatically so. Since feature inversion is an active area of research in computer vision, we can thus expect that the quality of language-driven image generation will greatly improve simply in virtue of general advances in image generation methods."}, {"heading": "4.3 Experiment 3: Judging macro-categories of objects", "text": "The previous experiments have shown that our language-driven image generation system visualizes properties that are salient and relevant enough to distinguish unrelated concepts (Experiment 1) but not closely related ones (Experiment 2). The last experiment takes high-level category structure explicitly into account in the design.\nExperiment description We group the McRae categories into three macro-categories, namely ANIMAL vs. ORGANIC vs. MAN-MADE, that are widely recognized in cognitive science as fundamental and unambiguous [11]. Participants are given a generated image and are asked to pick the macro-category that best describes the object in it.\nResults Again, the number of images for which participants\u2019 preferences are not significant is high: 28% of the ORGANIC images, 47% of the MAN-MADE images and 56% of the ANIMAL images. However, when participants do show significant preference, in the large majority of cases it is in favor of the correct macro-category: this is so for 98% of the ORGANIC images (70.5% of total), 90% of the MAN-MADE images (48% of total), and 59% of the ANIMAL ones (25.7% of total). Table 2 reports the confusion matrix across the macro-categories. Confusions arise where one would expect them: both MAN-MADE and ANIMAL images are more often confused with ORGANIC things than with each other.\nAgain, color (either of the object itself or of the environment) is the leading property, distinguishing objects among the three macro-categories. As Figure 3 shows, orange, green and a darker mixture of colors characterize ORGANIC things, ANIMALs, and MAN-MADE objects respectively. Images that do not typically have these colors are harder to be recognized. For instance, the few mistakes for ORGANIC images belong to the natural object category (e.g., rocks); all the other categories within this macro-category are in the vast majority of the cases judged correctly. In the MAN-MADE macrocategory (Figure 2, left), the images of buildings are those more easily recognizable; as one can see in Figure 3 those images share the same pattern: two horizontal layers (land/dark and sky/blue) with a vertical structure cutting across them (the building itself). Similarly, vehicles display two layers with a small horizontal structure crossing them, and they are almost always correctly classified. Finally, within the ANIMAL macro-category (Figure 2, right), birds and fish are more often misclassified than other animals , with their typical environment probably playing a role in the confusion."}, {"heading": "5 Discussion", "text": "We introduced the new task of generating pictures visualizing the semantic content of linguistic expressions as encoded in word embeddings, proposing more specifically a method we dubbed language-driven image generation.\nThe current system seems capable to visualize the typical color of object classes and aspects of their characteristic environment. Interestingly, vector-based word representations are notoriously bad at capturing color [2], and we do not expect them to be much better at characterizing environments, so our results suggest that, already in its current form, our system could also be used to enrich word representations, by highlighting aspects of concepts that are not salient in language but are probably learned by similarity-based generalization from the cross-modal mapping training examples. In this sense, language-driven image generation is more than a simple word embedding evaluation tool. At the same time, our system completely ignores visual properties related to shape. Shapes are not often expressed by linguistic means (although we all recognize the typical \u201cgestalt\u201d of, say, a mammal, it is very difficult to describe it in words), but in the same way in which we can capture color and environment, better visual representations or feature inversion methods might lead us in the future to associate, by means of images, typical shapes to shape-blind linguistic representations.\nCurrently we approach language-based image generation as a two-step process. Inspired from recent work in caption generation that conditions word production on visual vectors, we plan to explore an end-to-end model that conditions the generation process on information encoded in the word embeddings of the word/phrase that we wish to produce an image for, building upon classic generative models of image generation [18, 5]."}, {"heading": "A Answer Keys to Figure 1", "text": "We provide the concept names of the word embeddings used to generate the images of Figure 1 (we provide again Figure 1 in this document to facilitate the readers). Due to lack of space, we split the concept names into 3 tables, Table 1-3, where each table provides the concept names of the word embeddings used to generate the MAN-MADE, ORGANIC and ANIMAL images respectively.\napp lian ce cont aine r furn itur\ne/la rge\ntool\ninst rum\nent\ngarm ent tool wea pon toy spor ts veh icle buil ding stru ctur e food fruit\n/veg etab\nle\nnatu ral o\nbjec t\nplan t bird fish inse ct rept\nile/a mph\nibia n\nmam mal\nMan-made Organic Animals\nA B C D E F\nH I J\nG\n11"}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of ACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Distributional semantics in Technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam Khanh Tran"], "venue": "In Proceedings of ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Lia-Ji Li", "Li Fei-Fei"], "venue": "In Proceedings of CVPR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Proceedings of NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard Zemel"], "venue": "In Proceedings of the NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Understanding deep image representations by inverting them", "author": ["Aravindh Mahendran", "Andrea Vedaldi"], "venue": "In Proceedings of CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Semantic feature production norms for a large set of living and nonliving things", "author": ["Ken McRae", "George Cree", "Mark Seidenberg", "Chris McNorgan"], "venue": "Behavior Research Methods,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Predicting human brain activity associated with the meanings of nouns", "author": ["Tom Mitchell", "Svetlana Shinkareva", "Andrew Carlson", "Kai-Min Chang", "Vincente Malave", "Robert Mason", "Marcel Just"], "venue": "Science, 320:1191\u20131195,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "The Big Book of Concepts", "author": ["Gregory Murphy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Reconstructing visual experiences from brain activity evoked by natural movies", "author": ["Shinji Nishimoto", "An T Vu", "Thomas Naselaris", "Yuval Benjamini", "Bin Yu", "Jack L Gallant"], "venue": "Current Biology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "Proceedings of AISTATS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Richard Socher", "Milind Ganjoo", "Christopher Manning", "Andrew Ng"], "venue": "In Proceedings of NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher Manning", "Andrew Ng", "Christopher Potts"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "HOGgles: Visualizing Object Detection Features", "author": ["C. Vondrick", "A. Khosla", "T. Malisiewicz", "A. Torralba"], "venue": "Proceedings of ICCV,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Acquiring visual classifiers from human imagination", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Aude Oliva", "Antonio Torralba"], "venue": "arXiv preprint arXiv:1410.4627,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Recent work in neuroscience attempts to generate reconstructions of these mental images, as encoded in vector-based representations of fMRI patterns [15].", "startOffset": 149, "endOffset": 153}, {"referenceID": 17, "context": "However, given recent advances in compositional distributed semantics [20] that produce embeddings for arbitrarily long linguistic units, we also see our contribution as the first step towards generating images depicting the meaning of phrases (e.", "startOffset": 70, "endOffset": 74}, {"referenceID": 6, "context": ", [7, 8]) that introduced effective methods to generate linguistic descriptions of the contents of a given image.", "startOffset": 2, "endOffset": 8}, {"referenceID": 7, "context": ", [7, 8]) that introduced effective methods to generate linguistic descriptions of the contents of a given image.", "startOffset": 2, "endOffset": 8}, {"referenceID": 10, "context": "Tools such as word2vec [12] and Glove [16] have been shown to produce extremely high-quality vectorbased word embeddings.", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "Tools such as word2vec [12] and Glove [16] have been shown to produce extremely high-quality vectorbased word embeddings.", "startOffset": 38, "endOffset": 42}, {"referenceID": 3, "context": "Consequently, the problem of translating between linguistic and visual representations has been coached in terms of learning a cross-modal mapping function between vector spaces [4, 19].", "startOffset": 178, "endOffset": 185}, {"referenceID": 16, "context": "Consequently, the problem of translating between linguistic and visual representations has been coached in terms of learning a cross-modal mapping function between vector spaces [4, 19].", "startOffset": 178, "endOffset": 185}, {"referenceID": 8, "context": ", from the top layer of a CNN) back onto pixel space, to produce a real image [24, 10].", "startOffset": 78, "endOffset": 86}, {"referenceID": 19, "context": ", onto a representation in the space defined by a CNN layer), and then applies feature inversion to it (using the method HOGgles method of [22]) to generate an actual image (cell A18 in Figure 3).", "startOffset": 139, "endOffset": 143}, {"referenceID": 8, "context": "Up to now, feature inversion algorithms [10, 22, 24] have been applied to visual representations directly extracted from images (hence the \u201cinversion\u201d name).", "startOffset": 40, "endOffset": 52}, {"referenceID": 19, "context": "Up to now, feature inversion algorithms [10, 22, 24] have been applied to visual representations directly extracted from images (hence the \u201cinversion\u201d name).", "startOffset": 40, "endOffset": 52}, {"referenceID": 11, "context": "images or brain signals) must be associated to word labels [13, 19].", "startOffset": 59, "endOffset": 67}, {"referenceID": 16, "context": "images or brain signals) must be associated to word labels [13, 19].", "startOffset": 59, "endOffset": 67}, {"referenceID": 11, "context": "Following previous work [13, 4], we assume that the mapping is linear.", "startOffset": 24, "endOffset": 31}, {"referenceID": 3, "context": "Following previous work [13, 4], we assume that the mapping is linear.", "startOffset": 24, "endOffset": 31}, {"referenceID": 18, "context": "Nevertheless, these models exhibit \u201cintriguing properties\u201d, that are somewhat surprising given their state-of-the-art performance [21], prompting an effort to reach a deeper understanding of how they really work.", "startOffset": 130, "endOffset": 134}, {"referenceID": 19, "context": "[22] that casts the problem of inversion as paired dictionary learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11], in the context of property", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Originally, the HOGgles method of [22] was introduced for visualizing HOG features.", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "However, the method does not make feature-specific assumptions and it has also recently been used to invert CNN features [23].", "startOffset": 121, "endOffset": 125}, {"referenceID": 2, "context": "The real picture set contains approximately 480K images extracted from ImageNet [3] representing 5K distinct concepts.", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "3 CBOW, which learns to predict a target word from the ones surrounding it, produces state-of-the-art results in many linguistic tasks [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Visual Representations The visual representations, for the set of 480K seen concept images, are extracted with the pre-trained CNN model of [9] through the Caffe toolkit [6].", "startOffset": 170, "endOffset": 173}, {"referenceID": 12, "context": "Inspired from categorization schemes in cognitive science [14], we will refer to them as the prototype and exemplar methods.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "com/p/word2vec/ Other hyperparameters, adopted without tuning, include a context window size of 5 words to either side of the target, setting the sub-sampling option to 1e-05 and estimating the probability of target words by negative sampling, drawing 10 samples from the noise distribution [12].", "startOffset": 291, "endOffset": 295}, {"referenceID": 19, "context": "Both paired dictionary learning and feature inversion are conducted using the HOGgles software [22] with default hyperparameters.", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "MAN-MADE, that are widely recognized in cognitive science as fundamental and unambiguous [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "Interestingly, vector-based word representations are notoriously bad at capturing color [2], and we do not expect them to be much better at characterizing environments, so our results suggest that, already in its current form, our system could also be used to enrich word representations, by highlighting aspects of concepts that are not salient in language but are probably learned by similarity-based generalization from the cross-modal mapping training examples.", "startOffset": 88, "endOffset": 91}, {"referenceID": 15, "context": "Inspired from recent work in caption generation that conditions word production on visual vectors, we plan to explore an end-to-end model that conditions the generation process on information encoded in the word embeddings of the word/phrase that we wish to produce an image for, building upon classic generative models of image generation [18, 5].", "startOffset": 340, "endOffset": 347}, {"referenceID": 4, "context": "Inspired from recent work in caption generation that conditions word production on visual vectors, we plan to explore an end-to-end model that conditions the generation process on information encoded in the word embeddings of the word/phrase that we wish to produce an image for, building upon classic generative models of image generation [18, 5].", "startOffset": 340, "endOffset": 347}], "year": 2017, "abstractText": "We introduce language-driven image generation, the task of generating an image visualizing the semantic contents of a word embedding, e.g., given the word embedding of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it onto a high-level visual space (e.g., the space defined by one of the top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general visual properties of the concepts encoded in the word embedding, such as color or typical environment, and are sufficient to discriminate between general categories of objects.", "creator": "LaTeX with hyperref package"}}}