{"id": "1506.05055", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2015", "title": "Numeric Input Relations for Relational Learning with Applications to Community Structure Analysis", "abstract": "excellent work in hybrid frame of computational relational learning ( stat ) approach focussed approaching discrete data, equally though a few approaches for hybrid srl models have long proposed that specify numerical and discrete variables. in balanced model we distinguish finite random variables primarily using a defined distribution normally processed by the model defines numerical input profiles that are only used for comparison the distribution of digital response variables. all show feasible numerical symmetric relations can very easily be used along the global bayesian network approximation, unless then existing analytical and learning methods attribute only cosmetic disruption to they applied in this generalized setting. the network framework replaces natural relational estimates of classical probabilistic explanations for categorical analytics. we demonstrate significant usefulness over rbn models with posterior relations relations by several examples.", "histories": [["v1", "Tue, 16 Jun 2015 18:18:06 GMT  (605kb)", "http://arxiv.org/abs/1506.05055v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jiuchuan jiang", "manfred jaeger"], "accepted": false, "id": "1506.05055"}, "pdf": {"name": "1506.05055.pdf", "metadata": {"source": "CRF", "title": "Numeric Input Relations for Relational Learning with Applications to Community Structure Analysis", "authors": ["Jiuchuan Jiang", "Manfred Jaeger"], "emails": ["jiangjiuchuan@163.com", "jaeger@cs.aau.dk"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n05 05\n5v 1\n[ cs\n.L G\n] 1\n6 Ju\nIn particular, we use the augmented RBN framework to define probabilistic models for multi-relational (social) networks in which the probability of a link between two nodes depends on numeric latent feature vectors associated with the nodes. A generic learning procedure can be used to obtain a maximum-likelihood fit of model parameters and latent feature values for a variety of models that can be expressed in the high-level RBN representation. Specifically, we propose a model that allows us to interpret learned latent feature values as community centrality degrees by which we can identify nodes that are central for one community, that are hubs between communities, or that are isolated nodes. In a multi-relational setting, the model also provides a characterization of how different relations are associated with each community."}, {"heading": "1. Introduction", "text": "Statistical-relational learning (SRL) models have mostly been developed for discrete data (see [7, 5] for general overviews). An important reason for this lies in the fact that inference for hybrid models combining discrete and continuous variables quickly lead to inference problems that consist of integration problems for which no closed-form solutions are available. Among the relatively few proposals for SRL frameworks with continuous variables are hybrid Markov Logic Networks [21], hybrid ProbLog [9], and hybrid dependency networks [17]. In these works the complexity of the inference problem is addressed by focussing on approximate, sampling based methods [21, 17], or by imposing significant restrictions on the models, so that the required integration tasks for exact inference become solvable [9].\nIn the first part of this paper we first take a closer look at the semantic and statistical roles that continuous variables can play in a probabilistic relational model. We arrive at\na main distinction between numeric input relations, and numeric probabilistic relations, and we argue that for many modeling and learning problems involving numeric data, only numeric input relations are needed. We then proceed to show how numeric input relations can be integrated into the Relational Bayesian Network (RBN) language [12], with little or no cost in terms of algorithmic developments or computational complexity.\nThe second part of the paper demonstrates by several examples and applications the usefulness of modeling with numeric input relations, and the feasibility of the associated learning problems. First, a synthetic environmental modeling example shows how RBNs with numeric input relations support natural and interpretable models that provide a relational extension for traditional statistical models (Section 4.2).\nWe then turn to community detection in (social) networks as our main application. Utilizing a general SRL modeling language allows us to encode a variety of probabilistic network models on a single platform with a single generic inference and learning engine. We use RBNs with numeric input relations to encode probabilistic models with continuous latent features representing community structure. The SRL framework makes it easy to develop models for multi-relational networks (a.k.a multiplex or multi-layer networks), where nodes are connected by more than one type of link. In such networks, it will usually no longer be possible to reduce community structure detection to a form of graph partitioning [8], because different relations may define a multitude of different, overlapping, and partly conflicting community structures. We therefore propose a latent feature model that allows us to identify a number of communities with no restrictions on how communities are related in terms of inclusion or disjointness. Furthermore, for each community we obtain a characterization of how they are defined in terms of the given network relations, and we are able to define a probabilistic significance measure that ranks the detected communities in terms of their explanatory value. Last but not least, we obtain for each node in the network, and each community, a community centrality degree (ccd). Unlike most previously defined soft or fuzzy community membership degrees, these ccd values are not normalized to sum up to one over the different communities. They thereby allow us, for example, to identify influential hub nodes [22] between communities (nodes with high centrality degree for multiple communities)."}, {"heading": "2. SRL Models and Numeric Relations", "text": "A SRL model defines a probability distribution over relational structures. A model can be instantiated over different input domains, which may consist only of a set of objects, or, more generally, a set of objects together with a set of known input relations. Thus, a SRL model defines conditional probability distributions\nP (IRprob | IRin,D,\u03b8) (1)\nwhere D ranges over a class of domains (usually the class of all finite sets), IRin ranges over interpretations over D of a set of input relations Rin, and IRprob ranges over interpretations of a set of probabilistic (or output) relations Rprob. Interpretations IRin and IRprob are given as value assignment to ground atoms r(d) (d \u2208 Darity(r)). In the discrete case, each relation r has an associated finite range of possible values. The distinction between input and probabilistic relation need not be explicitly defined in a given model. Input\nrelations can also be seen as probabilistic relations that are fully instantiated as evidence in a given inference or learning problem. Markov Logic Networks (MLNs) [18] are one prominent framework in which there is only such an implicit distinction between input and probabilistic relations."}, {"heading": "2.1 Hybrid SRL Models", "text": "Hybrid SRL models allow the introduction of numeric relations, so that atoms r(d) become real-valued variables. Based on (1), one can distinguish numerical input and numerical probabilistic relations. To obtain a clearer view of the implications of this distinction, consider a purely continuous, classical linear regression model:\nP (Y | X, \u03b1,\u03b2, \u03c3) \u2248 \u03b1+X \u00b7 \u03b2 +N(0, \u03c32) (2)\nThis model contains three different types of numerical variates: Y , the response variable, is a random variable with a Gaussian distribution. X, the predictor variables may be random variables themselves, or they may be non-probabilistic inputs whose values have to be instantiated before inferences about Y can be made. Finally, \u03b1,\u03b2, \u03c3 are parameters of the model. The functional specification (2) is completely symmetric for the predictor variables X and the parameters \u03b2. The conceptual difference between the two only becomes apparent when one considers repeated random samples Y1, . . . , Yn. These samples would usually be drawn with varying values X1, . . . ,Xn for the predictor variables, whereas the parameters \u03b2 remain constant.\nIn SRL, data does not usually consist of iid samples, and one learns from a single observed pair IRin, IRprob. The distinction we can make in (2) between X and \u03b2, therefore, is no longer supported. That means that in (1) numeric input atoms r(d) can be equally interpreted as predictor variables, or as object-specific parameters. Neither of these views requires to define r(d) as a random variable with an associated probability distribution: as long as (1) is used purely as a conditional model, no prior distribution for numeric input relations is needed. Such a model will not define (posterior) probability distributions for numerical atoms, but still support maximum likelihood inference for the numerical atoms, which depending on the interpretation of the input relations can be seen as MPE inference for unobserved predictor variables, or as estimation of object-specific parameters.\nThe clear focus of Hybrid ProbLog [9] is to introduce numeric probabilistic relations. The language provides constructs to explicitly define distributions of numeric atoms as Gaussian with specified mean and standard deviation, for example.\nThe nature of Hybrid MLNs [21] is a little less clear-cut, due to the only implicit distinction between input and output relations 1. Hybrid MLNs extend standard MLNs by numeric properties (which we can identify with numeric relations in our terminology) from which weighted numeric features can be constructed. Examples of weighted features that can be included in a hybrid MLN then are\ndistance(X,Y ) w1 (3)\n\u2212(length(Z)\u2212 1.5)2 w2 (4)\n1. For the following discussion we assume that the reader is familiar with MLNs\nA ground instance of a weighted feature contributes a weight to a possible world x (i.e. an interpretation of all discrete and numerical relations over a given domain) that is equal to the value of the ground feature multiplied with the weight of the feature. The probability of x then is given by 1/ZeW (x), where W is the sum of weights from all groundings of all features, and Z a normalization constant [21]. This definition, however, requires that a finite normalization constant Z can be found, which means that \u222b\neW (x) must be finite, where the integral represents integration over all numeric properties, and summation over all discrete relations. This normalization is not possible, for example, for an MLN only consisting of the weighted feature (3) with w1 = 1.0. No probability distribution for the distance property then is defined. For an MLN consisting of (4), on the other hand, normalization is possible, and a Gaussian distribution for the ground length atoms is defined. When a hybrid MLN contains numeric properties that prevent normalization, then no probabilistic inference for these properties is possible, and they either have to be instantiated to perform probabilistic inference for other properties and relations, or one has to use the model for MPE inference tasks. In summary, hybrid MLNs support numeric probabilistic relations under the condition that a finite normalization constant can be computed; otherwise they support numeric input relations for which no distribution is defined."}, {"heading": "3. Numerical Input Relations in RBNs", "text": ""}, {"heading": "3.1 Modeling", "text": "The RBN language is based on probability formulas that define the probability P (r(a) = true) for ground relational atoms r(a). The language of probability formulas is defined by a parsimonious grammar that is based on the two main constructs of convex combinations and combination functions. The following are two examples of the convex combination construct. To improve the readability and understandability of the formulas, we here use a modification of the original very compact syntax of [12], and write convex combinations in the form of \u201cwif-then-else\u201d statements (\u201cwif\u201d stands for \u201cweighted-if\u201d).\nP(heads(T) = true) \u2190 wif fair(T ) then 0.5 else 0.7 (5) P(cancer(X) = true) \u2190 wif 0.3 then genetic predisposition(X) else 0.1 (6)\nFormula (5) defines the probability of a coin-toss to come up heads as 0.5 if a fair coin is tossed, and 0.7 otherwise. Here the formula in the wif-clause is an ordinary Boolean condition. In formula (6) the wif-clause is a numerical mixture coefficient. The formula thereby defines the probability that X has cancer as a mixture of a contribution coming from a genetic predisposition (mixture weight 0.3), and a base rate of 0.1 (mixture weight 1-0.3).\nGenerally a formula wifA then B else C is evaluated over a concrete input domain to a probability value val(wif A then B else C) as val(A)\u00b7val(B)+(1\u2212val(A))\u00b7val(C). There are two features in this modeling approach that make an integration of numerical input relations extremely easy: first, logical input relations already are interpreted numerically: for example, val(fair(T )) is defined as 0 or 1, depending on whether fair(T ) is false or true. Second, according to the grammar of probability formulas, numerical constants and\nlogical atoms are just different base cases for probability (sub-) formulas, which can be used interchangeably in the construction of more complex formulas.\nThe generalization from Boolean to numerical relations, thus, is almost trivial: one can just allow relational atoms r(a) to evaluate to real values val(r(a)) in any range [min,max], where \u2212\u221e \u2264 min \u2264 max \u2264 \u221e depend on the intended meaning of r. The only additional modification one has to make is to ensure that in the end probability formulas defining the probability for a Boolean response variable return values in the interval [0, 1]. We do this by using the RBN combination function construct, which generally take multisets of (probability) values as inputs, and return a single probability value. In particular, we can define logistic regression as a RBN combination function. An example of a RBN model with numeric input relations and logistic regression combination function then is:\nP(cancer(A)=true) \u2190 COMBINE intensity(R) WITH l-reg FORALL R WHERE exposed (A,R)\n(7)\nAgain we here use a slightly more verbose version of the combination function syntax than the original one. Formula (7) defines for a person A the probability of getting cancer using the logistic regression function applied to the set of all intensity values of radiation sources R that A was exposed to. Thus, assuming that the numerical attribute intensity, and the logical relation exposed are known, the probability P(cancer(A)) evaluates to exp(S)/(1 + exp(S)), where S = \u2211\nR:exposed(A,R) intensity(R)."}, {"heading": "3.2 Inference and Learning", "text": "Probabilistic inference for RBNs with numerical input relations is no different from inference in a purely Boolean setting. All inference approaches that have previously been used for RBNs (i.e., compilation to Bayesian networks or arithmetic circuits, importance sampling) can still be used without modifications.\nFor learning the values of numerical relations, we use a slightly generalized version of the gradient graph that was introduced in [13] for parameter learning in RBNs. The resulting likelihood graph data structure is illustrated in Figure 1. The likelihood graph is a computational structure related to arithmetic circuits. Each node of the graph represents a function of the inputs in the bottom layer of the graph: model parameters (e), values of ground atoms in the numerical input relations (f), and truth values of ground probabilistic atoms that are unobserved in the data (g).\nThe topmost layer of nodes in the graph corresponds to ground probabilistic atoms that are instantiated in the data (a), or that are unobserved and need to be marginalized out for the computation of the likelihood (b) (there is a one-to-one correspondence between the nodes in (b) and (g)). The function associated with the ground atom nodes of this layer is the probability of the atom, given the current parameter settings, and instantiations of the ground atoms in (g).\nThe nodes in the intermediate layers (c) represent sub-formulas of the probability formulas for the ground atoms in (a) and (b). Finally, the root node represents the product over all nodes in (a) and (b), and thus represents the likelihood of the joint configuration\nof probabilistic atoms consisting of the observed values for (a), and the current setting at the nodes (g) for the atoms in (b).\nThe likelihood graph is constructed in a top-down manner by a recursive decomposition of the probability formulas. In this decomposition also sub-formulas will be encountered that have a constant value, and do not depend on any parameters or unobserved atoms. These sub-formulas are not represented explicitly by nodes in the graph and are not decomposed further. Their constant value is directly assimilated into the function computation at their parent nodes.\nThe likelihood graph supports computation of the likelihood values and the gradient of the likelihood function with respect to the numerical parameters (e) and (f). These computation are linear in the number of edges of the graph. Based on these elementary computations, the likelihood graph can be used for parameter learning via gradient ascend, marginalization over unobserved atoms via MCMC sampling (mainly used when learning from incomplete data), and MAP inference for unobserved probabilistic atoms. For parameter learning, we perform multiple restarts of gradient ascend with random initializations for the nodes (e), (f), (g)."}, {"heading": "4. Examples: Standard Logistic Regression", "text": "We now demonstrate the usefulness of RBN models with numerical input relations for practical modeling and learning problems, and the feasibility of learning via likelihood graph based gradient ascent. In this section we present examples that demonstrate the use of the logistic regression model in our relational framework for constructing models that closely follow conventional and interpretable statistical modeling approaches."}, {"heading": "4.1 Propositional: Cancer Data", "text": "In a first experiment we test whether standard \u201cpropositional\u201d logistic regression is properly embedded in our relational framework. For this we use a very small dataset containing data\non 27 cancer patients that was originally introduced in [16], and which is often used as a standard example for logistic regression. We use a simplified version of the dataset given in [1, Table 5.10], which contains a single numerical predictor variable LI, and a binary response variable indicating whether the cancer is in remission after treatment. A standard logistic regression model for predicting remission is represented by the probability formula\nP(remission(A)=true) \u2190 COMBINE \u03b1+ \u03b2 \u00b7 LI(A) WITH l-reg. (8)\nThe combination function construct in this formula is somewhat degenerate, since it here effects no combination over a multiset of values, and simply reduces to the application of the logistic regression function to the single number \u03b1+ \u03b2 \u00b7 LI(A).\nFigure 2 shows the probability of the response variable as a function of the predictor variable for the parameters \u03b1, \u03b2 learned from the RBN encoding (8), and for the parameters given in [1] (which were fitted using the SAS statistics toolbox). Clearly, our gradient ascent approach using the likelihood graph here yields results that are compatible with standard approaches to logistic regression."}, {"heading": "4.2 Relational: Water Network", "text": "In this section we consider a toy model for the propagation of pollution in a river network. This example demonstrates the ability to integrate into our relational modeling framework standard logistic response models based on meaningful and interpretable predictor relations.\nInput domains for this model consist of measuring stations in a river network that measure whether the river is polluted or not. Stations are related by the Boolean upstream relation, denoting that one station is directly upstream of another (i.e., without any other stations in-between). For any pair of stations in the upstream relation, there also is a numerical relation invdistance containing the inverse of the distance between the two stations.\nThe outermost wif-then-else construct in lines 1,2,10 of the model of Table 1 defines the polluted attributed as a mixture of two factors: first, there is a base probability of (1 \u2212 0.6) \u00b7 0.2 = 0.08 for pollution to occur regardless of pollution already being measured upstream. Second, lines 2.-9. contain a propagation model of pollution that is measured at one or several upstream stations. This probability sub-formula computes the expression\nGeneralizing from the baseline example of Section 4.1 we investigate whether the parameters of the model can still be identified from independent samples of the polluted attribute. To this end we sample N independent joint instantiations of the polluted attribute for the 12 measuring stations of the domain in Figure 3 with parameters \u03b1 = \u22123 and \u03b2 = 2, and the values of the invdistance relation as shown in Figure 3. All experiments are performed using 20 random restarts. In the first experiment the values of the invdistance relation are fixed at their true values of Figure 3, and we only learn the values of \u03b1, \u03b2. Figure 4 (a) shows the learned values for increasing sample sizes N = 20, 50, 200, 500. Clearly, quite accurate estimates for \u03b1, \u03b2 are already obtained from relatively small sample sizes.\nIn a second experiment, \u03b1, \u03b2 are fixed at their true values, and the values of the invdistance relation are learned. Figure 4 (b) shows the convergence of the estimates for the invdistance values of five different pairs of neighboring measuring stations. Again, the true values are consistently learned. The required sample size is much larger than for \u03b1, \u03b2, because a single sample only contains relevant information for the estimation of invdistance(Si, Sj) when polluted(Si) is true in that sample.\nIn a third experiment, both \u03b1, \u03b2 and the invdistance relations are learned. In this setup no convergence to the true values can be expected, since the parameters are not jointly identifiable: for any given setting of the parameters \u03b1, \u03b2, invdistance(), one obtains equiva-\nlent solutions in the form \u03b1, \u03bb \u00b7 \u03b2, invdistance/\u03bb. For this reason, we compare the products \u03b2 \u00b7 invdistance() for both the parameters in the generating and learned model. Figure 4 (c) shows these products for the same pairs of stations as in (b). The convergence here shows that even if the exact values of the parameters cannot be learned, a probabilistically equivalent model is learned (the learned value of the parameter \u03b1 also converges to the true value -3.0).\nTable 2 shows the size of the likelihood graph, the time for construction, and the average time per restart for the gradient ascent optimization. For different values of the sample size N , these numbers are given for the case where we only learn the relation invdistance() (top entry in each cell of the table), and the case where we learn \u03b1, \u03b2 and invdistance() (bottom entry). The likelihood graph is significantly larger when also learning \u03b1, \u03b2, because here more sub-formulas of the instantiated model depend on unknown parameters, and therefore can not be pruned in the construction."}, {"heading": "5. Application: Community Structure in Multi-Relational Networks", "text": "We will now apply learning of numeric input relations in RBNs for community structure analysis in multi-relational networks. Figure 5 shows a small network with 6 individuals connected by two different types of (undirected) links. Considering only the green (solid) link relation, one would identify {1, 3, 5} and {2, 4, 6} as communities, whereas the red (dashed) link relation points to communities {1, 2} and {3, 4, 5, 6}. Moreover, the community structure {1, 3, 5}, {2, 4, 6}, would indicate that the red links are representing an antagonistic relationship that is more likely to exist in between communities, than within communities. Considering both links simultaneously, and assuming both are positive indicators of communities, one may also consider {3,5} and {4,6} as the most clearly defined communities, to which 1 and 2 are more loosely connected.\nThis tiny example illustrates how multiple relations can lead to a rather complex community landscape, with multiple possible views and interpretations. Even though multirelational networks occur naturally, research on community detection has very much focused on the single-relational case. Proposals for dealing with multi-relational networks often consist of reductions to the single-relational setting, either by aggregating all relations into a single weighted relations [4, 15], or by aggregating results from community detection performed for each relation separately [2].\nIn Section 5.2 we will propose a latent feature model that takes all relations as input in a non-aggregated form, and returns multiple communities along with a characterization of how the different communities are correlated with the relations.\nFigure 6 shows a single-relational network with a relatively clear two-community structure. The two nodes 3 and 4, however, are perfectly ambiguous with regard to their community membership. Most existing soft clustering methods would give both nodes equal membership degrees of 0.5 for both communities. However, clearly it is desirable to be able to distinguish node 3, which is well connected to both communities, and which for information diffusion purposes would be the most influential node in the network [6], from node 4, which is completely isolated. Nodes 1 and 2 are both very strongly associated with the community on the left. However, instead of assigning a membership degree close to 1.0 to both of them, it will be more informative to assign a higher membership degree to node 2 than to node 1, so that the membership degree also reflects the centrality of the nodes for the communities. In our model, the learned values of latent numeric relations can be interpreted as community centrality degrees, that reflect the degree of connectivity of a node with all communities."}, {"heading": "5.1 Community Centrality Degrees", "text": "In this section we first consider the single-relational case to explore models for learning community membership degrees that satisfy the desiderata outlined in the preceding section. We introduce a numerical binary relation u(V,C), whose arguments are a node V , and a community C. The relation u is constrained to be non-negative. We can then define the following probabilistic model:\nP (link(V,W )) =\n{\n0 V = W eS/(1 + eS) V 6= W (9)\nwhere\nS = \u03b1+ \u2211\nC:community(C)\nu(V,C) \u00b7 u(W,C) (10)\nand \u03b1 is a real-valued constant (the intercept, in the language of log-linear models). This model is quite straightforward, and closely related to other models for link prediction (e.g. for recommender systems) in which the affinity of objects to be connected by a link is measured by the inner product of latent feature vectors associated with the objects. We note that in contrast to structurally similar probabilistic latent semantic models [11] the variables u(V,C), u(W,C) have no semantics as (conditional) probabilities, and (9),(10) is not a mixture model with the communities as hidden mixture components. The model is readily encoded as a RBN. The observed links in a network with node set N then define the likelihood function\nL(\u03b1, u) = \u220f\nV,W\u2208N :link(V,W)=true\nP (link(V,W )) \u220f\nV,W\u2208N :link(V,W)=false\n(1 \u2212 P (link(V,W ))) (11)\nThe generic learning method described in Section 3.2 can be used to fit the model parameters \u03b1 \u222a {u(V,C) | V,C : node(V ), cluster(C)}.\nWe applied this model to the well known Zachary Karate Club network depicted in Figure 7 (a), where the node colors represent the known \u201cground truth\u201d communities in this network [24]. Figure 7 (b) illustrates the learned u-values when the model is instantiated for 2 communities C1, C2. Nodes V are plotted in 2-dimensional space according to their u(V,C1), u(V,C2) values. Node colors still represent the ground truth. Some individual nodes, and groups of nodes, are marked correspondingly in Figure 7 (a) and (b). We first observe that the nodes 1 and 34 with maximal u(\u00b7, C1) and u(\u00b7, C2)-values are central nodes of their respective communities. In contrast, the node groups A and B are well-connected with their own communities, but separated from the other community. C is a large group of nodes with u(\u00b7, C1) and u(\u00b7, C2)-values of similar magnitude. All nodes in this group can be seen as potential hub nodes between the two communities, but node 3 with the highest sum u(\u00b7, C1) + u(\u00b7, C2) is most clearly identified as a well-connected hub between the communities.\nTwo further observations are worthwhile making: the fact that some u-values of zero have been learned indicates that allowing negative u-values could lead to higher likelihood scores. However, for the purpose of interpretability of the results, imposing the non-negativity constraint for u still seems beneficial. Second, for nodes that are pairwise structurally indistinguishable (all nodes other than node 27 in group B) identical u-values were learned. This, obviously, is highly desirable, and supports both the adequacy of the probabilistic model, and the effectiveness of the optimization procedure (which, starting from random initial values, could be feared to get stuck in local optima with non-identical values).\nWe compare the results obtained with model (9),(10) with a slight modification of the distance model proposed in [10]. This model is given by (9) in conjunction with\nS = \u03b1+ \u2211\nc:community(C)\n(u(V,C)\u2212 u(W,C))2. (12)\nThus, the log-odds of the link probability now depend on the squared Euclidean distance between the latent feature vectors. This model, too, is readily encoded by a RBN, and the learned u-values are visualized in Figure 7 (c). The positions of the nodes in the latent space here are not interpretable as community centrality degrees, and (in line with the motivation given by the authors for this model) rather are suitable as node-coordinates for graph visualization and plotting. The model (9),(12) also achieves a much lower log-likelihood of -245 than the model (9),(10), which achieves a log-likelihood of -157. The baseline model that does not contain any latent feature vectors u, and only the \u03b1 parameter is fitted (i.e., a fitted Erdo\u0308s-Re\u0301nyi model), achieves a log-likelihood score of -452.\nThe likelihood graphs for the models (9),(10) and (9),(12) contain 5679 and 10235 nodes, respectively. The construction times for the graphs are around 0.1s and 0.8s, respectively. The times per restart of the learning procedure was around 31s for both models. The increased computation time per gradient computation in the larger graph for the second model was offset by a smaller number of iterations required until convergence. The obtained results are quite robust: solutions with very similar likelihood scores and structures as the ones shown in Figure 7 are usually obtained as the highest-scoring solutions within 3-5 restarts."}, {"heading": "5.2 Communities in Multi-Relational Networks", "text": "We now generalize the model (9),(10) to multi-relational networks. For a network containing K relations linki (i = 1, . . . ,K), we introduce K new numeric attributes ti(C) on the cluster objects of the domain. The values of the ti are unconstrained. The intention is that ti(C) measures whether the existence of links of type i is positively (ti > 0) or negatively (ti < 0) correlated with membership in cluster C. We now define the probability P (linki(V,W )) using (9) in conjunction with\nSi = \u03b1i + \u2211\nc:community(C)\nu(V,C) \u00b7 u(W,C) \u00b7 ti(C). (13)\nGiven an observed network with K different link relations, we have to fit the model parameters {\u03b1i | i = 1, . . . ,K} \u222a {u(V,C) | V,C : node(V ), cluster(C)} \u222a {ti(C) | C, i : cluster(C), i = 1, . . . ,K}. This model is clearly not identifiable: for a given parameterization, multiplying all ti-values with a factor c > 0, and dividing all u-values by \u221a c leads to an equivalent parameterization. Absolute numeric values of the fitted parameters are therefore not significant, but relative magnitudes of values can still identify community structure.\nWe apply the model to the multi-relational wiring room network [3] depicted in Figure 8. The network consists of 14 nodes connected by 5 distinct relations. For better visibility, the relations here are displayed in two groups. Out of the 5 relations, 3 represent positive relationships, 1 is antagonistic, and 1 (\u201carguments about opening a window\u201d) potentially ambivalent. Relation 5 is directed, the others undirected. The coloring of the nodes represent a community structure found in [3] for this network.\nUsing 4 clusters, we learn u(\u00b7, Ci)-values for the 14 nodes as shown in Figure 9 , and ti(\u00b7)-values for the 4 communities as shown in Figure 10. The values u(\u00b7, C1) (light blue in Figure 9) identify nodes 9,10,11 as central nodes of community C1, to which 8 and 14 also are strongly associated. This very much coincides with the original green community. According to the ti(C1)-values of Figure 10, membership in this community is most positively associated with relations 2 and 3, and to a lesser extent 1 and 5. Relation 4 is clearly negatively associated with this community. Similarly, there is a good correspondence between community C3, and the original yellow community. According to the ti-values, this community is most clearly associated with relations 1 and 3. Community C2 considers relation 4 as strongly positive, and thereby provides a non-standard view on the community structure of this network, with nodes 7,8 the centers of this community. Finally, community C4 also considers relations 4 as positive, but unlike for C2, there is a negative association with relation 2.\nThe likelihood graph here contained 7361 nodes. The reported result is the best obtained in 10 random restarts of the learning procedure, where one restart took about 1 minute to compute."}, {"heading": "5.3 Community Significance Measure", "text": "It is highly desirable that a method not only returns the requested number of communities, but also provides a measure of the significance, or validity, of each community. On the basis of our probabilistic model, we obtain such a measure in terms of the explanatory value that a community provides for the observed network structure, where explanatory value is formalized by the likelihood gain obtained by including community information into the model.\nSpecifically, to measure the explanatory value of community Ck defined by the u(\u00b7, Ck)values of all nodes, we consider the model given by (9) in conjunction\nSi = \u03b1i + u(V,Ck) \u00b7 u(W,Ck) \u00b7 ti(Ck). (14)\nIn this model, we now keep the previously learned u(\u00b7, Ck) values fixed, and re-learn the parameters {\u03b1i | i = 1, . . . ,K} \u222a {ti(Ck) | i = 1, . . . ,K}. As a baseline we take the Erdo\u0308s-Re\u0301nyi log-likelihood obtained when only fitting the \u03b1i-values in a model without communities. We then define the likelihood gain obtained from community Ck as the loglikelihood obtained by (14), minus the Erdo\u0308s-Re\u0301nyi baseline.\nFor the communities identified for the Wiring Room network we obtain the following likelihood gain values: C3 : 71.4, C1 : 62.0, C2 : 39.5, C4 : 14.4. Thus, the ranking obtained by the likelihood gain scores reflects quite well the intuitive evaluation of the communities in terms of interpretability."}, {"heading": "5.4 Incomplete Network Data", "text": "An important benefit of using probabilistic models for network analysis is the ability to handle incomplete information: for the likelihood function (11) it is not required that for every pair of nodes V,W the true/false status of the link relation is known. Unlike many other graph partitioning and community detection methods, probabilistic approaches can therefore easily handle incomplete graph data, where link(V,W) atoms can also have an unknown status.\nApart from dealing with such potentially 3-valued graph data, we can also exploit this robustness of the likelihood function to improve scalability to larger networks by sub-sampling the false-link data. Assuming complete network data, the number of factors in (11), and hence the number of nodes in the likelihood graph, is quadratic in the number of nodes of the network. Since networks tend to be sparse, the number of true links are usually greatly outnumbered by the false links, and one may expect that the community structure is already well identified by the true links, and a random sub-sample of the false links.\nTo investigate the effects of learning from randomly sub-sampled data, we consider a multi-relational social network described in [19]. This network, which we call the Aarhus network, contains 61 nodes and 5 different relations. We apply our model (9),(13) with 5 communities to data consisting of all the true links (of all 5 relations), and a random sub-sample of q% (q = 100, 50, 20, 10, 5) of the false links. Having learned \u03b1i, ti and u parameters from sub-sampled data, we fix the learned ti and u parameters, and re-learn the \u03b1i parameters using the full data. In conjunction with these adjusted \u03b1i parameters, we evaluate the likelihood score of the learned ti and u parameters on the complete data. In this manner we can assess how well the ti,u parameters learned from sub-sampled data fit the complete data (since the learned \u03b1i essentially reflect estimates for the densities of the ri, these cannot fit the complete data when learned from a sub-sample in which false links are under-represented).\nFigure 11 shows the best log-likelihood score achieved in 20 restarts each for the different percentages of sampled false links. Surprisingly, the likelihood score first even improves when data is sub-sampled. A possible explanation for this can be a higher variance of the scores achieved in different restarts for the larger datasets, and the best out of 20 restarts being further from a global optimum. Figure 11 also shows the time per restart for the different data sets. These times follow very closely the number of atoms in the data.\nWe next try to determine how closely the communities identified from the sub-sampled data resemble the communities found from the full data. For this we consider as a reference the communities found from the 100% data in an extended sequence of 38 restarts, where the best solution then obtained a likelihood score of -3174. Let uref(\u00b7, Ck) be the u-values for community Ck in this reference solution, and u\nq(\u00b7, Ck) the u-values learned within 20 restarts from the q% data (q = 100, 50, 20, 10, 5). For all q, we compute Pearson\u2019s correlation between the vectors uref(\u00b7, Ck) and uq(\u00b7, Ck) for k = 1, . . . , 5, and then (manually) re-index the communities in uq to obtain the best pairwise matches (according to Pearson\u2019s correlation) between the uref(\u00b7, Ck) and uq(\u00b7, Ck). Figure 12 shows a coarse heat-map visualization of all Pearson correlations after the re-indexing. Rows in this figure correspond to the reference communities uref(\u00b7, Ck) (k = 1, . . . , 5). The 5 main columns correspond to the communities uq(\u00b7, Ck). An element at row k and column k\u2032 consists of 5 colored rectangles, representing the correlation between uref(\u00b7, Ck) and uq(\u00b7, Ck\u2032), for q = 100, 50, 20, 10, 5 (in this order). Dark red stands for a correlation > 0.7, medium red for > 0.5, and yellow for > 0.3. The result shows that reference communities 1 and 4 are usually also identified from sub-sampled data. These two communities are also the ones which are identified as the most significant ones according to our significance measure, which evaluates to 292, 183, 130, 330, 147 for communities 1,2,3,4,5, respectively."}, {"heading": "5.5 Related Work", "text": "Probabilistic latent feature models for social networks (in the single-relational setting) have been proposed in [10]. The focus there, however, is more on obtaining interpretable, visual embeddings of the nodes in latent space, than on community analysis.\nTo apply SRL modeling tools for node clustering in social network analysis has already been suggested in [20]. Clusters here consist of nodes with similar properties, however, not of connected communities of nodes. In [23] a nonparametric Bayesian model with discrete latent variables is proposed, that induces a hard partitioning of the nodes. That model is formulated for multi-relational networks, but only applied to single-relational ones in [23].\nSimilarly, an RBN model with discrete latent variables for standard partitioning-based community detection was presented in [14]."}, {"heading": "6. Conclusion", "text": "We have identified two distinct ways in which support for numerical data can be added to statistical relational models: as numerical probabilistic relations with an associated distribution model, or as numeric input relations, which can also be understood as object-specific model parameters.\nWe have extended the RBN framework to allow for numeric input relations. Such an extension is particularly well supported by RBNs, because here relational (logical) atoms always have been treated syntactically and semantically as interchangeable with numeric parameters, and only minimal adjustments to the language and its inference and learning algorithms are required. By also introducing a logistic regression combination function, we obtain a framework that supports standard modelling techniques for categorical data in a relational setting, where both models and learned parameters are interpretable. We here have focused on logistic regression for conditioning binary response variables on numeric inputs, but other models could be integrated by adding additional combination functions to the language. The only requirement is that the combination functions are differentiable.\nThe second part of the paper applies the extended RBNs to develop new models for community structure analysis in social networks. Specifically, we address the challenges of communities in multi-relational networks, and of assigning community centrality degrees for node-community pairs. Unlike most kinds of community membership degrees that are obtained by existing soft clustering methods, these ccd \u2019s are not fractional membership assignments, but measures for how well a node is connected with each community. At the same time, when applied to multi-relational networks, the proposed model provides an explanation of how communities relate to different relations, and a validity measure that for the significance of each detected community.\nThe RBN modeling tool provides a platform on which one can easily implement different network models for community detection, and which are all supported by a single generic learning algorithm. Like for all general purpose modeling and inference tools, this generality comes at the price that for any particular model more efficient inference and learning techniques could probably be developed by dedicated implementations that can incorporate numerous problem-specific optimizations. Thus, should at any point in time more \u201cindustrial strength\u201d applications be desired with the community structure model we proposed, then a new model-specific implementation may be needed.\nWith the network models we have investigated in this paper we have stayed close to established models, and only scratched the surface of the modeling capabilities provided within the RBN language. One line of future work is to integrate these structural network models with dynamic models for information diffusion within the networks.\nThe computational bottleneck in our implementation at this point is the size of the likelihood graph. We have already shown that to some extent this problem can be reduced by sub-sampling the false edges of the network. Other techniques we are currently exploring are optimization strategies in which in an iterative manner the likelihood function is only partially optimized based on smaller, partial likelihood graphs. Such iterative partial\noptimizations can either follow a block gradient descent strategy, in which only subsets of parameters are optimized in each iteration, stochastic gradient descent strategies, in which only the likelihood function of a part of the data is optimized, or a combination of both these approaches. The challenge is to develop generic strategies that are widely applicable to a broad range of models, and that do not require the user to perform model-specific tuning of the learning strategy in each case.\nIn principle, it would also be quite straightforward to add models for numeric probabilistic relations to the RBN framework. The language of probability formulas can directly be used also to define mean and variance of a Gaussian distribution (for example), and thereby define Gaussians that are in complex ways conditioned on continuous and categorical predictors. However, this will come at the price of loosing the tools for exact inference, and one would have to rely on sampling-based inference methods.\nAll results presented in this paper are obtained using an updated version of the Primula implementation of RBNs2, which will become available with then next system release."}], "references": [{"title": "Categorical Data Analysis", "author": ["A. Agresti"], "venue": "Wiley", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Abacus: frequent pattern mining-based community discovery in multidimensional networks", "author": ["Michele Berlingerio", "Fabio Pinelli", "Francesco Calabrese"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "An algorithm for clustering relational data with applications to social network analysis and comparison with multidimensional scaling", "author": ["Ronald L Breiger", "Scott A Boorman", "Phipps Arabie"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1975}, {"title": "Mining hidden community in heterogeneous social networks", "author": ["Deng Cai", "Zheng Shao", "Xiaofei He", "Xifeng Yan", "Jiawei Han"], "venue": "In Proceedings of the 3rd international workshop on Link discovery,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "editors", "author": ["L. De Raedt", "P. Frasconi", "K. Kersting", "S.H. Muggleton"], "venue": "Probabilistic Inductive Logic Programming, volume 4911 of Lecture Notes in Artificial Intelligence. Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Mining the network value of customers", "author": ["Pedro Domingos", "Matt Richardson"], "venue": "In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "editors", "author": ["L. Getoor", "B. Taskar"], "venue": "Introduction to Statistical Relational Learning. MIT Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Community structure in social and biological networks", "author": ["Michelle Girvan", "Mark EJ Newman"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Extending problog with continuous distributions", "author": ["B. Gutmann", "M. Jaeger", "L. De Raedt"], "venue": "Proceedings of the 20th Int. Conf. on Inductive Logic Programming (ILP), volume 6489 of LNCS, pages 76\u201391. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Latent space approaches to social network analysis", "author": ["Peter D Hoff", "Adrian E Raftery", "Mark S Handcock"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Latent semantic models for collaborative filtering", "author": ["Thomas Hofmann"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Relational bayesian networks", "author": ["M. Jaeger"], "venue": "Dan Geiger and Prakash Pundalik Shenoy, editors, Proceedings of the 13th Conference of Uncertainty in Artificial Intelligence (UAI-13), pages 266\u2013273, Providence, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Parameter learning for relational Bayesian networks", "author": ["M. Jaeger"], "venue": "Proceedings of the 24th International Conference on Machine Learning (ICML)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Community detection for multiplex social networks based on relational bayesian networks", "author": ["J. Jiang", "M. Jaeger"], "venue": "Proc. of the 21st Int. Symposium on Methodologies for Intelligent Systems (ISMIS)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A computer program for linear logistic regression analysis", "author": ["Elisa T Lee"], "venue": "Computer programs in biomedicine,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1974}, {"title": "Learning relational dependency networks in hybrid domains", "author": ["I. Ravkic", "J. Ramon", "J. Davis"], "venue": "Machine Learning", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning, 62(1- 2):107 \u2013 136", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards effective visual analytics on multiplex and multilayer networks", "author": ["Luca Rossi", "Matteo Magnani"], "venue": "Chaos, Solitons & Fractals,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Probabilistic classification and clustering in relational data", "author": ["Benjamin Taskar", "Eran Segal", "Daphne Koller"], "venue": "In Proc. of 17th International Joint Conference on Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Hybrid markov logic networks", "author": ["J. Wang", "P. Domingos"], "venue": "Proceedings of the Twenty-Third National Conference on Artificial Intelligence", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Scan: a structural clustering algorithm for networks", "author": ["Xiaowei Xu", "Nurcan Yuruk", "Zhidan Feng", "Thomas AJ Schweiger"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Nonparametric relational learning for social network analysis", "author": ["Zhao Xu", "Volker Tresp", "Shipeng Yu", "Kai Yu"], "venue": "In KDD 2008 Workshop on Social Network Mining and Analysis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "An information flow modelfor conflict and fission in small groups1", "author": ["W Zachary"], "venue": "Journal of anthropological research, 33(4):452\u2013473", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1977}], "referenceMentions": [{"referenceID": 6, "context": "Introduction Statistical-relational learning (SRL) models have mostly been developed for discrete data (see [7, 5] for general overviews).", "startOffset": 108, "endOffset": 114}, {"referenceID": 4, "context": "Introduction Statistical-relational learning (SRL) models have mostly been developed for discrete data (see [7, 5] for general overviews).", "startOffset": 108, "endOffset": 114}, {"referenceID": 19, "context": "Among the relatively few proposals for SRL frameworks with continuous variables are hybrid Markov Logic Networks [21], hybrid ProbLog [9], and hybrid dependency networks [17].", "startOffset": 113, "endOffset": 117}, {"referenceID": 8, "context": "Among the relatively few proposals for SRL frameworks with continuous variables are hybrid Markov Logic Networks [21], hybrid ProbLog [9], and hybrid dependency networks [17].", "startOffset": 134, "endOffset": 137}, {"referenceID": 15, "context": "Among the relatively few proposals for SRL frameworks with continuous variables are hybrid Markov Logic Networks [21], hybrid ProbLog [9], and hybrid dependency networks [17].", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "In these works the complexity of the inference problem is addressed by focussing on approximate, sampling based methods [21, 17], or by imposing significant restrictions on the models, so that the required integration tasks for exact inference become solvable [9].", "startOffset": 120, "endOffset": 128}, {"referenceID": 15, "context": "In these works the complexity of the inference problem is addressed by focussing on approximate, sampling based methods [21, 17], or by imposing significant restrictions on the models, so that the required integration tasks for exact inference become solvable [9].", "startOffset": 120, "endOffset": 128}, {"referenceID": 8, "context": "In these works the complexity of the inference problem is addressed by focussing on approximate, sampling based methods [21, 17], or by imposing significant restrictions on the models, so that the required integration tasks for exact inference become solvable [9].", "startOffset": 260, "endOffset": 263}, {"referenceID": 11, "context": "We then proceed to show how numeric input relations can be integrated into the Relational Bayesian Network (RBN) language [12], with little or no cost in terms of algorithmic developments or computational complexity.", "startOffset": 122, "endOffset": 126}, {"referenceID": 7, "context": "In such networks, it will usually no longer be possible to reduce community structure detection to a form of graph partitioning [8], because different relations may define a multitude of different, overlapping, and partly conflicting community structures.", "startOffset": 128, "endOffset": 131}, {"referenceID": 20, "context": "They thereby allow us, for example, to identify influential hub nodes [22] between communities (nodes with high centrality degree for multiple communities).", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "Markov Logic Networks (MLNs) [18] are one prominent framework in which there is only such an implicit distinction between input and probabilistic relations.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "The clear focus of Hybrid ProbLog [9] is to introduce numeric probabilistic relations.", "startOffset": 34, "endOffset": 37}, {"referenceID": 19, "context": "The nature of Hybrid MLNs [21] is a little less clear-cut, due to the only implicit distinction between input and output relations 1.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "The probability of x then is given by 1/Ze (x), where W is the sum of weights from all groundings of all features, and Z a normalization constant [21].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "To improve the readability and understandability of the formulas, we here use a modification of the original very compact syntax of [12], and write convex combinations in the form of \u201cwif-then-else\u201d statements (\u201cwif\u201d stands for \u201cweighted-if\u201d).", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "The only additional modification one has to make is to ensure that in the end probability formulas defining the probability for a Boolean response variable return values in the interval [0, 1].", "startOffset": 186, "endOffset": 192}, {"referenceID": 12, "context": "For learning the values of numerical relations, we use a slightly generalized version of the gradient graph that was introduced in [13] for parameter learning in RBNs.", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "on 27 cancer patients that was originally introduced in [16], and which is often used as a standard example for logistic regression.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "Figure 2 shows the probability of the response variable as a function of the predictor variable for the parameters \u03b1, \u03b2 learned from the RBN encoding (8), and for the parameters given in [1] (which were fitted using the SAS statistics toolbox).", "startOffset": 187, "endOffset": 190}, {"referenceID": 3, "context": "Proposals for dealing with multi-relational networks often consist of reductions to the single-relational setting, either by aggregating all relations into a single weighted relations [4, 15], or by aggregating results from community detection performed for each relation separately [2].", "startOffset": 184, "endOffset": 191}, {"referenceID": 1, "context": "Proposals for dealing with multi-relational networks often consist of reductions to the single-relational setting, either by aggregating all relations into a single weighted relations [4, 15], or by aggregating results from community detection performed for each relation separately [2].", "startOffset": 283, "endOffset": 286}, {"referenceID": 5, "context": "However, clearly it is desirable to be able to distinguish node 3, which is well connected to both communities, and which for information diffusion purposes would be the most influential node in the network [6], from node 4, which is completely isolated.", "startOffset": 207, "endOffset": 210}, {"referenceID": 10, "context": "We note that in contrast to structurally similar probabilistic latent semantic models [11] the variables u(V,C), u(W,C) have no semantics as (conditional) probabilities, and (9),(10) is not a mixture model with the communities as hidden mixture components.", "startOffset": 86, "endOffset": 90}, {"referenceID": 22, "context": "We applied this model to the well known Zachary Karate Club network depicted in Figure 7 (a), where the node colors represent the known \u201cground truth\u201d communities in this network [24].", "startOffset": 179, "endOffset": 183}, {"referenceID": 9, "context": "We compare the results obtained with model (9),(10) with a slight modification of the distance model proposed in [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "We apply the model to the multi-relational wiring room network [3] depicted in Figure 8.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "The coloring of the nodes represent a community structure found in [3] for this network.", "startOffset": 67, "endOffset": 70}, {"referenceID": 17, "context": "To investigate the effects of learning from randomly sub-sampled data, we consider a multi-relational social network described in [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 9, "context": "5 Related Work Probabilistic latent feature models for social networks (in the single-relational setting) have been proposed in [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 18, "context": "To apply SRL modeling tools for node clustering in social network analysis has already been suggested in [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "In [23] a nonparametric Bayesian model with discrete latent variables is proposed, that induces a hard partitioning of the nodes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "That model is formulated for multi-relational networks, but only applied to single-relational ones in [23].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Similarly, an RBN model with discrete latent variables for standard partitioning-based community detection was presented in [14].", "startOffset": 124, "endOffset": 128}], "year": 2014, "abstractText": "Most work in the area of statistical relational learning (SRL) is focussed on discrete data, even though a few approaches for hybrid SRL models have been proposed that combine numerical and discrete variables. In this paper we distinguish numerical random variables for which a probability distribution is defined by the model from numerical input variables that are only used for conditioning the distribution of discrete response variables. We show how numerical input relations can very easily be used in the Relational Bayesian Network framework, and that existing inference and learning methods need only minor adjustments to be applied in this generalized setting. The resulting framework provides natural relational extensions of classical probabilistic models for categorical data. We demonstrate the usefulness of RBN models with numeric input relations by several examples. In particular, we use the augmented RBN framework to define probabilistic models for multi-relational (social) networks in which the probability of a link between two nodes depends on numeric latent feature vectors associated with the nodes. A generic learning procedure can be used to obtain a maximum-likelihood fit of model parameters and latent feature values for a variety of models that can be expressed in the high-level RBN representation. Specifically, we propose a model that allows us to interpret learned latent feature values as community centrality degrees by which we can identify nodes that are central for one community, that are hubs between communities, or that are isolated nodes. In a multi-relational setting, the model also provides a characterization of how different relations are associated with each community.", "creator": "gnuplot 4.4 patchlevel 3"}}}