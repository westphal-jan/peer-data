{"id": "1506.06155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique Splits", "abstract": "we undertake a weighted algorithm method solving multivariate sliding threshold functions as split agents of choice trees to create improved random line classifiers. primary algorithm induction models resort to transparent and exhaustive procedures ; analyze beneficial univariate choice properties. unlike 2d, either method computes a cumulative combination graph voting scores at each node, and introduces possible parameters of variable linear combination ( oblique ) select codes instead adopting the variant of latent variable resolution distributions. we develop a convex - model upper bound evaluating the classification or construct a one - level server tree, and produce the bound continuous stochastic coefficient scaling at select internal area of proxy tree. distributions reaching up to 1000 continuously optimized oblique ( co2 ) decision trees are created, which significantly differentiate marginal systems with univariate splits and generic techniques for constructing proxy trees. experimental results are reported on self - entity classification benchmarks documenting compared null faces in the wild ( lfw ) dataset.", "histories": [["v1", "Fri, 19 Jun 2015 20:42:47 GMT  (2002kb,D)", "https://arxiv.org/abs/1506.06155v1", null], ["v2", "Wed, 24 Jun 2015 21:23:43 GMT  (2003kb,D)", "http://arxiv.org/abs/1506.06155v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["mohammad norouzi", "maxwell d collins", "david j fleet", "pushmeet kohli"], "accepted": false, "id": "1506.06155"}, "pdf": {"name": "1506.06155.pdf", "metadata": {"source": "CRF", "title": "CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique Splits", "authors": ["Mohammad Norouzi", "Maxwell D. Collins", "David J. Fleet", "Pushmeet Kohli"], "emails": ["fleet}@cs.toronto.edu", "mcollins@cs.wisc.edu", "pkohli@microsoft.com"], "sections": [{"heading": null, "text": "Index Terms\u2014decision trees, random forests, oblique splits, ramp loss\nF"}, {"heading": "1 INTRODUCTION", "text": "Decision trees [6], [24] and random forests [5], [13] have a long, successful history in machine learning, in part due to their computational efficiency and their applicability to large-scale classification and regression tasks (e.g., see [7], [11]). A case in point is the Microsoft Kinect, where multiple decision trees are learned on millions of training exemplars to enable real time human pose estimation from depth images [27]. The standard algorithm for decision tree induction grows a tree one node at a time, greedily and recursively. The building block of this procedure is an optimization at each internal node of the tree, which divides the training data at that node into two subsets according to a splitting criterion, such as Gini impurity index in CART [6], or information gain in C4.5 [25]. This corresponds to optimizing a binary decision stump, or a one-level decision tree, at each internal node. Most tree-based methods exploit univariate (axis-aligned) split functions, which compare one feature dimension to a threshold. Optimizing univariate decision stumps is straightforward because one can exhaustively enumerate all plausible thresholds for each feature, and thereby select the best parameters according to the split criterion. Conversely, univariate split functions have limited discriminative power.\nWe investigate the use of a more general and powerful family of split functions, namely, linear-combination (a.k.a., oblique) splits. Such split functions comprise a multivariate linear projection of the features followed by binary quantization. Clearly, exhaustive search with linear hyperplanes is not feasible, and based on our preliminary experiments, random sampling yields poor results. Further,\n\u2022 M. Norouzi and D. J. Fleet are with the Department of Computer Science, University of Toronto. Email: {norouzi, fleet}@cs.toronto.edu \u2022 M. D. Collins is with the Department of Computer Science, University of Wisconsin-Madison. Email: mcollins@cs.wisc.edu \u2022 P. Kohli is with Microsoft Research, Cambridge, UK. Email: pkohli@microsoft.com\ntypical splitting criteria for a one-level decision tree (decision stump) are discontinuous, since small changes in split parameters may change the assignment of data to branches of the tree. As a consequence, split parameters are not readily amenable to numerical optimization, so oblique split functions have not been used widely with tree-based methods.\nThis paper advocates a new building block for learning decision trees, i.e., an algorithm for continuous optimization of oblique decision stumps. To this end, we introduce a continuous upper bound on the empirical loss associated with a decision stump. This upper bound resembles a ramp loss, and accommodates any convex loss that is useful for multi-class classification, regression, or structured prediction [22]. As explained below, the bound is the difference of two convex terms, the optimization of which is effectively accomplished using the Convex-Concave Procedure of [33]. The proposed bound resembles the bound used for learning binary hash functions [20].\nSome previous work has also considered improving the classification accuracy of decision trees by using oblique split functions. For example, Murthy et al. [19] proposed a method called OC1, which yields some performance gains over CART and C4.5. Nevertheless, individual decision trees are rarely sufficiently powerful for many classification and regression tasks. Indeed, the power of tree-based methods often arises from diversity among the trees within a forest. Not surprisingly, a key question with optimized decision trees concerns the loss of diversity that occurs with optimization, and hence a reduction in the effectiveness of forests of such trees. The random forest of Breiman seems to achieve a good balance between optimization and randomness.\nOur experimental results suggest that one can effectively optimize oblique split functions, and the loss of diversity associated with such optimized decision trees can be mitigated. In particular, it is found that when the decision stump optimization is initialized with random forest\u2019s split func-\nar X\niv :1\n50 6.\n06 15\n5v 2\n[ cs\n.L G\n] 2\n4 Ju\nn 20\n15\n2 tions, one can indeed construct a forest of non-correlated decision trees. We effectively take advantage of the underlying non-convex optimization problem, for which a diverse set of initial states for the optimizer yields a set of different split functions. Like random forests, the resulting algorithm achieves very good performance gains as the number of trees in the ensemble increases.\nWe assess the effectiveness of our tree construction algorithm by generating up to 1000 decision trees on nine classification benchmarks. Our algorithm, called CO2 forest, outperforms random forest on all of the datasets. It is also shown to outperform a baseline of OC1 trees. As a largescale experiment, we consider the task of segmenting faces from the Labeled Faces in the Wild (LFW) dataset [14]. Again, our results confirm that CO2 forest outperforms other baselines."}, {"heading": "2 RELATED WORK", "text": "Breiman et al. [6] proposed a version of CART that employs linear combination splits, known as CART-linearcombination (CART-LC). Murthy et al. [12], [19] proposed OC1, a refinement of CART-LC that uses random restarts and random perturbations to escape local minima. The main idea behind both algorithms is to use coordinate descent to optimize the parameters of the oblique splits one dimension at a time. Keeping all of the weights corresponding to an oblique decision stump fixed except one, for each datum they compute the critical value of the missing weight at which the datum switches its assignment to the branches. Then, one can sort these critical values to find the optimal value of each weight (with other weights fixed). By performing multiple passes over the dimensions and the data, oblique splits with small empirical loss can be found.\nBy contrast, our algorithm updates all the weights simultaneously using gradient descent. While the aforementioned algorithms focus mainly on optimizing the splitting criterion to minimize tree size, there is little promise of improved generalization. Here, by adopting a formulation based on the latent variable SVM [32], our algorithm provides a natural means of regularizing the oblique split stumps, thereby improving the generalization power of the trees.\nThe hierarchical mixture of experts (HME) [16] uses soft splits rather than hard binary decisions to capture situations where the transition from low to high response is gradual. The empirical loss associated with HME is a smooth function of the unknown parameters and hence numerical optimization is feasible. The main drawback of HME concerns inference. That is, multiple paths along the tree should be explored during inference, which reduces the efficiency of the classifier.\nOur work builds upon random forest [5]. Random forest combines bootstrap aggregating (bagging) [4] and the random selection of features [13] to construct an ensemble of non-correlated decision trees. The method is used widely for classification and regression tasks, and research still investigates its theoretical characteristics [8]. Building on random forest, we also grow each tree using a bootstrapped version of the training dataset. The main difference is the way the split functions are selected. Training random forest is generally faster than using our optimized oblique trees, and\nbecause random forest uses univariate splits, classification with the same number of trees is often faster. Nevertheless, we often achieve similar accuracy with many fewer trees, and depending on the application, the gain in classification performance is clearly worth the computational overhead.\nThere also exist boosting based techniques for creating ensembles of decision trees [10], [34]. A key benefit of random forest over boosting is that it allows for faster training as the decision trees can be trained in parallel. In our experiments we usually train 30 trees in parallel on a multicore machine. Nevertheless, it is interesting to combine boosting techniques with our oblique trees, and we leave this to future work.\nMenze et al. [18] also consider a variant of oblique random forest. At each internal node they find an optimal split function using either ridge regression or linear discriminant analysis. Like other previous work [3], [30], the technique of [18] is only conveniently applicable to binary classification tasks. A big challenge in a multi-class setting is solving the combinatorial assignment of labels to the two leaves. In contrast to [18], our technique is more general, and allows for optimization of multi-class classification and regression loss functions.\nRota Bulo\u0301 & Kontschieder [26] recently proposed the use of multi-layer neural nets as split functions at internal nodes. While extremely powerful, the resulting decision trees lose their computational simplicity during training and testing. Further, it may be difficult to produce the required diversity among trees in a forest. This paper explores the middle ground, with a simple, yet effective class of linear multi-variate split functions. That said, note that the formulation of the upper bound used to optimize empirical loss in this paper can be extended to optimize other non-linear split functions, including neural nets (e.g., [21])."}, {"heading": "3 PRELIMINARIES", "text": "For ease of exposition, this paper is focused on binary classification trees, with m internal (split) nodes, and m+ 1 leaf (terminal) nodes.1 An input, x \u2208 Rp, is directed from the root of the tree down through internal nodes to a leaf node, which specifies a distribution over k class labels.\nEach internal node, indexed by i \u2208 {1, . . . ,m}, performs a binary test by evaluating a node-specific split function ti(x) : Rp \u2192 {\u22121,+1}. If ti(x) evaluates to \u22121, then x is directed to the left child of node i. Otherwise, x is directed to the right child. And so on down the tree. Each split function ti(\u00b7), parametrized by a weight vector wi, is assumed to be a linear threshold function of the form ti(x) = sgn(wiTx). We incorporate an offset parameter to obtain split functions of the form sgn(wiTx\u2212bi) by using homogeneous coordinates (i.e., by appending a constant \u201c\u22121\u201d to the end of the input feature vector).\nEach leaf node, indexed by j \u2208 {0, . . . ,m}, specifies a conditional probability distribution over class labels, l \u2208 {1, . . . , k}, denoted p(y = l | j). These distributions are parameterized in terms of a vector of unnormalized predic-\n1. In a binary tree the number of leaves is always one more than the number of internal (non-leaf) nodes.\n3 tive log-probabilities, denoted \u03b8j \u2208 Rk, and a conventional softmax function; i.e.,\np(y = l | j) = exp\n{ \u03b8j[l] }\u2211k \u03b1=1 exp { \u03b8j[\u03b1]\n} , (1) where v[\u03b1] denotes the \u03b1th element of vector v.\nThe parameters of the tree comprise the m internal weight vectors, each of dimension p + 1, and the m + 1 vectors of unnormalized log-probabilities, one for each leaf node, i.e., {wi}mi=1 and {\u03b8j}mj=0. Given a dataset of inputoutput pairs, D \u2261 {xz, yz}nz=1, where yz \u2208 {1, . . . , k} is the ground truth class label associated with input xz \u2208 Rp, we wish to find a joint configuration of oblique splits {wi}mi=1 and leaf parameters {\u03b8j}mj=0 that minimize some measure of misclassification loss on the training set. Joint optimization of the split functions and leaf parameters according to a global objective is, however, known to be extremely challenging [15] due to the discrete and sequential nature of the decisions within the tree.\nTo cope with the discontinuous objective caused by discrete split functions, we propose a smooth upper bound on the empirical loss, with which one can effectively learn a diverse collection of trees with oblique split functions. We apply this approach to the optimization of split functions of internal nodes within the context of a top-down greedy induction procedure, one in which each internal node is treated as an independent one-level decision stump. The split functions of the tree are optimized one node at a time, in a greedy fashion as one traverses the tree, breadth first, from the root downward. The procedure terminates when a desired tree depth is reached, or when some other stopping criterion is met. While we focus here on the optimization of a single stump, the formulation can be generalized to optimize entire trees."}, {"heading": "4 CONTINUOUS OPTIMIZATION OF OBLIQUE (CO2) DECISION STUMPS", "text": "A binary decision stump is parameterized by a weight vector w, and two vectors of unnormalized log-probabilities for the two leaf nodes, \u03b80 and \u03b81. The stump\u2019s loss function comprises two terms, one for each leaf, denoted `(\u03b80, y) and `(\u03b81, y), where ` : Rk \u00d7 {1, . . . , k} \u2192 R+. They measure the discrepancy between the label y and the distributions parameterized by \u03b80 and \u03b81. The binary test at the root of the stump acts as a gating function to select a leaf, and hence its associated loss. The empirical loss for the stump, i.e., the sum of the loss over the training set D, is defined as\nL (w,\u03b80,1;D) =\u2211 (x,y)\u2208D 1(wTx < 0) `(\u03b80, y) + 1(w Tx \u2265 0) `(\u03b81, y) , (2)\nwhere 1(\u00b7) is the usual indicator function. Given the softmax model of Eq. (1), the log loss takes the form\n`log(\u03b8, y) = \u2212\u03b8[y] + log {\u2211k\n\u03b1=1 exp\n{ \u03b8[\u03b1] }} . (3)\nRegarding this formulation, we note that the parameters which minimize Eq. (2) with the log loss, `log, are those that maximize information gain. One can prove this with\nstraightforward algebraic manipulation of Eq. (2), recognizing that the \u03b80 and \u03b81 that minimize Eq. (2), given any w, are the empirical class log-probabilities at the leaves.\nWe also note that the framework outlined below accommodates other loss functions that are convex in \u03b8. For instance, for regression tasks where y \u2208 Rk, one can use squared loss,\n`sqr(\u03b8,y) = \u2016\u03b8 \u2212 y\u201622 . (4)\nAs mentioned already above, it is also important to note that empirical loss, L(w,\u03b80,1;D), is a discontinuous function of w. As a consequence, optimization of L with respect to w is very challenging. Our approach, outlined in detail below, is to instead optimize a continuous upper bound on empirical loss. This bound is closely related to formulations of binary SVM and logistic regression classification. In the case of binary classification, the assignment of class labels to each side of the hyperplane, i.e., the parameters \u03b80 and \u03b81, are pre-specified. In contrast, a decision stump with a large numbers of labels entails joint optimization of both the assignment of the labels to the leaves and the hyperplane parameters."}, {"heading": "4.1 Upper Bound on Empirical Loss", "text": "The upper bound on loss that we employ, given an inputoutput pair (x, y), has the following form:\n1(wTx < 0) `(\u03b80, y) + 1(w Tx \u2265 0) `(\u03b81, y) \u2264 max ( \u2212wTx+`(\u03b80, y) , wTx+ `(\u03b81, y) ) \u2212 |wTx| , (5)\nwhere |wTx| denotes the absolute value of wTx. To verify the bound, first suppose that wTx < 0. In this case, it is straightforward to show that the inequality reduces to\n`(\u03b80, y) \u2264 max ( `(\u03b80, y) , 2w Tx+ `(\u03b81, y) ) , (6)\nwhich holds trivially. Conversely, when wTx \u2265 0 the inequality reduces to\n`(\u03b81, y) \u2264 max ( \u2212 2wTx+ `(\u03b80, y) , `(\u03b81, y) ) , (7)\nwhich is straightforward to validate. Hence the inequality in Eq. (5) holds.\nInterestinly, while empirical loss in Eq. (2) is invariant to \u2016w\u2016, the bound in Eq. (6) is not. That is, for any real scalar a > 0, sgn(awTx) does not change with a, and hence L(w,\u03b80,1) = L(aw,\u03b80,1). Thus, while the loss on the LHS of Eq. (5) is scale-invariant, the upper bound on the RHS of Eq. (5) does depend on \u2016w\u2016. Indeed, like the soft-margin binary SVM formulation, and margin rescaling formulations of structural SVM [31], the norm of w affects the interplay between the upper bound and empirical loss. In particular, as the scale of w increases, the upper bound becomes tighter and its optimization becomes more similar to a direct loss minimization.\nMore precisely, the upper bound becomes tighter as \u2016w\u2016 increases. This is evident from the following inequality, which holds for any real scalar a > 1:\nmax ( \u2212wTx+`(\u03b80, y), wTx+`(\u03b81, y) ) \u2212 |wTx| \u2265\nmax ( \u2212awTx+`(\u03b80, y), awTx+`(\u03b81, y) ) \u2212 a|wTx| . (8)\n4 To verify the bound, as above, consider the sign of wTx. When wTx < 0, inequality in Eq. (8) is equivalent to\nmax ( `(\u03b80, y) , 2w Tx+ `(\u03b81, y) ) \u2265\nmax ( `(\u03b80, y) , 2 aw Tx+ `(\u03b81, y) ) .\nConversely, when wTx \u2265 0, Eq. (8) is equivalent to max ( \u2212 2wTx+`(\u03b80, y) , `(\u03b81, y) ) \u2265\nmax ( \u2212 2 awTx+ `(\u03b80, y) , `(\u03b81, y) ) .\nThus, as \u2016w\u2016 increases the bound becomes tighter. In the limit, as \u2016w\u2016 becomes large, the loss terms `(\u03b80, y) and `(\u03b81, y) become negligible compared to the terms \u2212wTx and wTx, in which case the RHS of Eq. (5) equals its LHS, except when wTx \u2248 0. Hence, for large \u2016w\u2016, not only the bound gets tight, but also it becomes less smooth and more difficult to optimize in our nonconvex setting.\nFrom the derivation above, and through experiments below, we observe that when \u2016w\u2016 is constrained, optimization converges to better solutions that exhibit better generalization. Summing over the bounds for the training pairs, and restricting \u2016w\u2016, we obtain the surrogate objective we aim to optimize to find the decision stump parameters:\nminimize L\u2032 (w,\u03b80,1;D, \u03bd) such that \u2016w\u20162 \u2264 \u03bd ,\n(9)\nwhere \u03bd \u2208 R+ is a regularization parameter, and L\u2032 is the surrogate objective, i.e., the upper bound,\nL\u2032 (w,\u03b80,1;D, \u03bd) \u2261\u2211 (x,y)\u2208D max ( \u2212wTx+`(\u03b80, y),wTx+`(\u03b81, y) ) \u2212 |wTx| .\n(10) For all values of \u03bd, we have that L\u2032(w,\u03b80,1;D, \u03bd) \u2265 L(w,\u03b80,1;D). We find a suitable \u03bd via cross-validation. Instead of using the typical Lagrange form for regularization, we employed hard constraints with similar behavior."}, {"heading": "4.2 Convex-Concave Optimization", "text": "Minimizing the surrogate objective in Eq. (10) entails nonconvex optimization. While still challenging, it is important that L\u2032 (w,\u03b80,1;D, \u03bd) is better behaved than empirical loss. It is piecewise smooth and convex-concave in w, and the constraint on w defines a convex set. As a consequence, gradient-based optimization is applicable, although the surrogate objective is non-differentiable at isolated points. The objective also depends on the leaf parameters, \u03b80 and \u03b81, but only through the loss terms `, which we constrained to be convex in \u03b8. Therefore, for a fixed w, it follows that L\u2032 (w,\u03b80,1; D, \u03bd) is convex in \u03b80 and \u03b81.\nThe convex-concave nature of the surrogate objective allows us to use difference of convex (DC) programming, or the Convex-Concave Procedure (CCCP) [33], a method for minimizing objective functions expressed as sum of a convex and a concave term. The CCCP has been employed by Felzenszwalb et al. [9] and Yu & Joachims [32] to optimize latent variable SVM models that employ a similar convex-concave surrogate objective.\nThe Convex-Concave Procedure is an iterative method. At each iteration the concave term (\u2212|wTx| in our case)\nAlgorithm 1 The convex-concave procedure for Continuous Optimization of Oblique (CO2) decision stumps that minimizes Eq. (9) to estimate (w, \u03b80, \u03b81) given a training dataset D, and a hyper-parameter \u03bd that constrains the norm of w\n.\n1: Initialize w by a random univariate split 2: Estimate \u03b80, and \u03b81 based on w and D 3: while surrogate objective has not converged do 4: w(old) \u2190 w 5: for t = 1 to \u03c4 do 6: sample a pair (x, y) at random from D 7: s\u2190 sgn(w(old)x) 8: if \u2212wTx+ `(\u03b80, y) \u2265 wTx+ `(\u03b81, y) then 9: w\u2190 w + \u03b7(1 + s)x\n10: \u03b80 \u2190 \u03b80 \u2212 \u03b7 \u2202`(\u03b80, y)/\u2202\u03b8 11: else 12: w\u2190 w \u2212 \u03b7(1\u2212 s)x 13: \u03b81 \u2190 \u03b81 \u2212 \u03b7 \u2202`(\u03b81, y)/\u2202\u03b8 14: end if 15: if \u2016w\u201622 > \u03bd then 16: w\u2190 \u221a \u03bd \u00b7w/\u2016w\u20162 17: end if 18: end for 19: end while\nis replaced with its tangent plane at the current parameter estimate, to formulate a convex subproblem. The parameters are updated with those that minimize the convex subproblem, and then the tangent plane is updated. Let w(old) denote the estimate for w from the previous CCCP iteration. In the next iteration w(old), \u03b80, and \u03b81 are updated minimizing\u2211\n(x,y)\u2208D\n( max ( \u2212wTx+`(\u03b80, y),wTx+`(\u03b81, y) ) \u2212 sgn(w(old) T x)wTx ) ,\n(11)\nsuch that \u2016w\u20162 \u2264 \u03bd. Note that w(old) is constant during optimization of this CCCP subproblem. In that case, the second term within the sum over training data in Eq. (11) just defines a hyperplane in the space of w. The other (first) term within the sum entails maximization of a function that is convex in w, \u03b80 and \u03b81, since the maximum of two convex functions is convex. As a consequence, the objective of Eq. (11) is convex.\nWe use stochastic subgradient descent to minimize Eq. (11). After each subgradient update, w is projected back into the feasible region. For efficiency, we do not wait for complete convergence of the convex subproblem within CCCP. Instead, w(old) is updated after a fixed number of epochs (denoted \u03c4 ) over the training dataset. The pseudocode for the optimization procedure is outlined in Alg 1.\nIn practice, we implement Alg 1 with several small modifications. Instead of estimating the gradients based on a single data point, we use mini-batches of 100 elements, and average their gradients. We also use a momentum term of 0.9 to converge more quickly. Finally, although a constant learning rate \u03b7 is used in Alg 1, we instead track the value of the surrogate objective, and when it oscillates for more than a number of iterations we reduce the learning rate.\n5"}, {"heading": "5 IMPLEMENTATION AND EXPERIMENTAL DETAILS", "text": "In all tree construction methods considered here, we grow each decision tree as deep as possible, until we reach a pure leaf. We exploit the bagging ensemble learning algorithm [4] to create the forest such that each tree is built by using a new data set sampled uniformly with replacement from the original dataset. In Random Forest, for finding each univariate split function we only consider a candidate set of size q of random feature dimensions, where q is the only hyper-parameter in our random forest implementation. We set the parameter q by growing a forest of 1000 trees and testing them on a hold-out validation set of size 20% of the training set. Let p denote the dimensionality of the feature descriptors. We choose q from the candidate set of {p0.5, p0.6, p0.7, p0.8, p0.9} to accelerate validation. Some previous work suggests the use of q = \u221a p as a heuristic [11], which is included in the candidate set. We use an OC1 implementation provided by the authors [1]. We slightly modified the code to allow trees to grow to their fullest extent, removing hard-coded limits on tree depth and minimum examples for computing splits. We also modified the initialization of OC1 optimization to match our initialization for CO2, whereby an optimal axisaligned split on a subsampling of q possible features is used. Interestingly, we observed that both changes improve OC1\u2019s performance when building ensembles of multiple trees, OC1 Forest. We use the default values provided by the authors for OC1 hyperparameters.\nCO2 Forest has three hyper-parameters, namely, the regularization parameter \u03bd, the initial learning rate \u03b7, and q, the size of feature candidate set of which the best is selected to initialize the CO2 optimization. Ideally, one may consider using different regularizer parameters for different internal nodes of the tree, since the number of available training data decreases as one descends the tree. However, we use the same regularizer and learning rate for all of the nodes to keep hyper-parameter tuning simple. We set q as selected by the random forest validation above. We perform a grid search over \u03bd and \u03b7 to select the best hyper-parameters."}, {"heading": "6 EXPERIMENTS", "text": "Before presenting the classification results, we investigate the impact of the hyper-parameter \u03bd on our oblique decision trees. Fig. 1 depicts training and validation error rates for the MNIST dataset for different values of \u03bd \u2208 {0.1, 1, 10, 100} and different tree depths. One can see that as the tree depth increases, training error rate decreases monotonically. However, validation error rate saturates at a certain depth, e.g., a depth of 10 for MNIST. Growing the trees deeper beyond this point, either has no impact, or slightly hurts the performance. From the plots it appears that \u03bd = 10 exhibits the best training and validation error rates. The difference between different values of \u03bd seems to be larger for validation error.\nAs shown above in Eq. (8), as \u03bd increases the upper bound becomes tighter. Thus, one might suspect that larger \u03bd implies a better optimum and better training error rates. However, increasing \u03bd not only tightens the bound, but also makes the objective less smooth and harder to optimize. For MNIST, at \u03bd = 10 there appears to be a reasonable balance\nbetween the tightness of the bound and the smoothness of the objective. The hyper-parameter \u03bd also acts as a regularizer, contributing to the large gap in the validation error rates. For completeness, we also include baseline results with univariate decision trees. Clearly, the CO2 trees reach the same training error rates as the baseline but at a smaller depth. As seen from the validation error rates, the CO2 trees achieve better generalization too.\nClassification results for tree ensembles are generally much better than a single tree. Here, we compare our Continuously Optimized Oblique (CO2) decision forest with random forest [5] and OC1 forest, forest built using OC1 [19]. Results for random forest are obtained with the implementation of the scikit-learn package [23]. Both of the baselines use information gain as the splitting criterion for learning decision stumps. We do not directly compare with other types of classifiers, as our research concerns tree-based techniques. Nevertheless, the reported results are often competitive with the state-of-the-art."}, {"heading": "6.1 UCI multi-class benchmarks", "text": "We conduct experiments on nine UCI multi-class benchmarks, namely, SatImage, USPS, Pendigits, Letter, Protein, Connect4, MNIST, SensIT, Covertype. Table 1 provides a summary of the datasets, including the number of training and test points, the number of class labels, and the feature dimensionality. We use the training and test splits set by previous work, except for Connect4 and Covertype. More details about the datasets, including references to the corresponding publications can be found at the LIBSVM dataset repository page [2].\nTest error rates for random forest, OC1 Forest, and CO2 Forest with different numbers of trees (10, 30, 1000) are reported in Table 1. OC1 results are not presented on some datasets, as the derivative-free coordinate descent method used does not scale to large or high-dimensional datasets, e.g., requiring more than 24 hours to train a single tree on MNIST. CO2 Forest consistently outperforms random forest and OC1 Forest on all of the datasets. In some cases, i.e., Covertype, and SatImage, the improvement is small, but in four of the datasets CO2 Forest with only 10 trees outperforms random forest with 1000 trees.\nFor all methods, there is a large performance gain when the number of trees is increased from 10 to 30. The marginal gain from 30 to 1000 trees is less significant, but still notable. Finally, we also plot test error curves as a function of log number of trees in Fig. 2. CO2 Forest outperforms random forest and OC1 by a large margin and in most cases the marginal gain persists across different number of trees. For some datasets, OC1 Forest outperforms random forest, but it consistently underperforms CO2 Forest.\nFor pre-processing, the datasets are scaled so that either the feature dimensions are in the range of [0, 1], or they have a zero mean and a unit standard deviation. For CO2 Forest, we select \u03bd from the set {0.1, 1, 4, 10, 43, 100}, and \u03b7 from the set {.03, .01, .003}. A validation of 30 decision trees is performed over 18 entries of the grid of (\u03bd, \u03b7)."}, {"heading": "6.2 Labeled Faces in the Wild (LFW)", "text": "As a large-scale experiment, we consider the task of segmenting face parts based on the Labeled Faces in the Wild\n6 5 10 15 20 0 0.02 0.04 0.06 0.08 0.1\nTree depth\nTr ai\nni ng\nE rr\nor\n\u03bd = 0.1 \u03bd = 1 \u03bd = 100 \u03bd = 10\nbaseline\n5 10 15 20 0.04\n0.06\n0.08\n0.1\n0.12\n0.14\nTree depth\nV al\nid at\nio n\nE rr\nor\nFig. 1. The impact of hyper-parameter \u03bd on MNIST training and validation error rates for CO2 decision trees. The dashed baseline represents univariate decision trees with no pruning.\nTest error (%) with different number of trees Dataset Information Random Forest OC1 Forest CO2 Forest\nName #Train #Test #Class Dim 10 30 1000 10 30 1000 10 30 1000 SatImage 4, 435 2, 000 6 36 10.1 9.4 8.9 10.1 9.9 9.5 9.6 9.1 8.9\nUSPS 7, 291 2, 007 10 256 9.0 7.2 6.4 7.1 7.1 6.8 5.8 5.9 5.5 Pendigits 7, 494 3, 498 10 16 3.9 3.3 3.5 3.2 2.2 2.3 1.8 1.7 1.7\nLetter 15, 000 5, 000 26 16 6.6 4.7 3.7 7.6 5.0 3.8 3.2 2.3 1.8 Protein 17, 766 6, 621 3 357 39.9 35.5 30.9 39.1 34.6 30.8 33.8 31.2 30.3 Connect4\u2217 55, 000 12, 557 3 126 18.9 17.4 16.2 N/A 17.1 15.7 14.7 MNIST 60, 000 10, 000 10 784 4.5 3.5 2.8 N/A 2.5 2.0 1.9 SensIT 78, 823 19, 705 3 100 15.5 14.0 13.4 N/A 14.1 13.0 12.5 Covertype\u2217 500, 000 81, 012 7 54 3.2 2.8 2.6 N/A 3.1 2.7 2.6\nTABLE 1 Test error rates for forests with different number of trees on multi-class classification benchmarks. First few columns provide dataset information. Test error rates (%) for random forest, OC1 Forest, and CO2 Forest with 10, 30, and 1000 trees are reported. For datasets marked with a star \u201c\u2217\u201d (i.e., Connect4 & Covertype) we use our own training and test splits. As the number of training data points and feature dimensionality increase, OC1 becomes prohibitively slow, so this method is not applicable to the datasets with high-dimensional data or large training sets.\n(LFW) dataset [14]. We seek to label image pixels that belong to one of the following 7 face parts: lower face, nose, mouth, and the left and right eyes and eyebrows. These parts should be differentiated from the background, which provides a total of 8 class labels. To address this task, decision trees are trained on 31 \u00d7 31 image sub-windows to predict the label of the center pixel. Each 31 \u00d7 31 window with three RGB channels is vectorized to create an input in R2883. We ignore part labels for a 15-pixel border around each image at both training and test time.\nTo train each tree, we subsample 256,000 sub-windows from training images. We then normalize the pixels of each window to be of unit norm and variance across the training set. The same transformation is applied to input windows at test time using the normalization parameters calculated on the training set. To correct for the class label imbalance, like [17], we subsample training windows so that each label has an equal number of training examples. At test time, we reweight the class label probabilities given by the inverse of the factor that each label was undersampled or oversampled during training.\nOther than the random forest baseline, we also train decision trees and forests using split functions that compare two features (or \u201cprobes\u201d), where the choice of features comes from finding the optimal pair of features out of\na large number of sampled pairs. This method produces decision forests analogous to [27]. We call this baseline Twoprobe Forest. The same technique can be used to generate split functions with several features, but we found that using only two features produces the best accuracy on the validation set.\nBecause of the class label imbalance in LFW, classification accuracy is a poor measure of the segmentation quality. A more informative performance measure, also used in the PASCAL VOC challenge, is the class-average Jaccard score. We report Jaccard scores for the baselines vs. CO2 Forest in Table 2. It is clear that Two-probe Forest outperforms random forest, and CO2 Forest outperforms both of the baselines considerably. The superiority of CO2 Forest is consistent in Fig. 4, where Jaccard scores are depicted for forests with fixed tree depths, and forests with different number of trees. The Jaccard score is calculated for each class label, against all of the other classes, as 100 \u00b7 tp/(tp + fp + fn). The average of this quantity over classes is reported here. The test set comprises 250 randomly chosen images.\nWe use the Jaccard score to select the CO2 hyperparameters \u03bd and \u03b7. We perform grid search over \u03b7 \u2208 {10\u22125, 10\u22124, 3 \u00b7 10\u22124, 6 \u00b7 10\u22124, 0.001, 0.003} and \u03bd \u2208 {0.1, 1, 4, 10, 43, 100}. We compare the scores for 16 trees on a held-out validation set of 100 images. The choice of\n7 1 10 100 1,000 0.08 0.09 0.1 0.11 0.12 0.13 Satimage Random Forest OC1 Forest CO2 Forest 1 10 100 1,000 0.05 0.06 0.07 0.08 0.09 USPS Random Forest OC1 Forest CO2 Forest 1 10 100 1,000 0.01 0.02 0.03 0.04 0.05 0.06 Pendigits Random Forest OC1 Forest CO2 Forest\n1 10 100 1,000\n0.02\n0.03\n0.04\n0.05\n0.06\nLetter\nRandom Forest OC1 Forest CO2 Forest\n1 10 100 1,000\n0.3\n0.32\n0.34\n0.36\n0.38\n0.4\nProtein\nRandom Forest OC1 Forest CO2 Forest\n1 10 100 1,000 0.14\n0.15\n0.16\n0.17\n0.18\n0.19\nConnect4\nRandom Forest CO2 Forest\n1 10 100 1,000\n0.02\n0.03\n0.04\n0.05\n0.06\nMNIST\nRandom Forest CO2 Forest\n1 10 100 1,000\n0.12\n0.13\n0.14\n0.15\n0.16\nSensIT\nRandom Forest CO2 Forest\n1 10 100 1,000\n0.026\n0.028\n0.030\n0.032\n0.034\nCovertype\nRandom Forest CO2 Forest\nFig. 2. Test error curves for Random Forest and OC1 Forest vs. CO2 Forest as a function of (log) number of trees on the multi-class classification benchmarks. On the last four datasets, OC1 implementation is prohibitively slow, hence not applicable.\n\u03b7 = 10\u22124 and \u03bd = 1 achieves the highest validation Jaccard score of 41.87, and are used in the final experiments.\nWe note that some other tree-like structures [28] and more sophisticated Computer Vision systems built for face segmentation [29] achieve better segmentation accuracy on LFW. However, our models use only raw pixel values, and our goal was to compare CO2 Forest against forest baselines."}, {"heading": "7 CONCLUSION", "text": "We present Continuously Optimized Oblique (CO2) Forest, a new variant of random forest that uses oblique split functions. Even though the information gain criterion used for inducing decision trees is discontinuous and hard to optimize, we propose a continuous upper bound on the\ninformation gain objective. We leverage this bound to optimize oblique decision tree ensembles, which achieve a large improvement on classification benchmarks over a random forest baseline and previous methods of constructing oblique decision trees. In contrast to OC1 trees, our method scales to problems with high-dimensional inputs and large training sets, which are commonplace in Computer Vision and Machine Learning. Our framework is straightforward to generalize to other tasks, such as regression or structured prediction, as the upper bound is general and applies to any form of convex loss function.\n8 Input Ground truth Axis-aligned Two-probe CO2 Input Ground truth Axis-aligned Two-probe CO2\nREFERENCES\n[1] http://ccb.jhu.edu/software/oc1/oc1.tar.gz. [2] http://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/datasets/\nmulticlass.html. [3] K. P. Bennett and J. A. Blue. A support vector machine approach\nto decision trees. IJCNN, 1998. [4] L. Breiman. Bagging predictors. Machine learning, 24(2), 1996. [5] L. Breiman. Random forests. Machine Learning, 45(1):5\u201332, 2001. [6] L. Breiman, J. Friedman, R. A. Olshen, and C. J. Stone. Classification\nand regression trees. Chapman & Hall/CRC, 1984. [7] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and\nMedical Image Analysis. Springer, 2013. [8] M. Denil, D. Matheson, and N. De Freitas. Narrowing the gap:\nRandom forests in theory and in practice. ICML, 2014. [9] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan.\nObject detection with discriminatively trained part-based models. IEEE Trans. PAMI, pages 1627\u20131645, 2010. [10] J. H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics, pages 1189\u20131232, 2001. [11] T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning (Ed. 2). Springer, 2009. [12] D. Heath, S. Kasif, and S. Salzberg. Induction of oblique decision trees. IJCAI, 1993. [13] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. PAMI, pages 832\u2013844, 1998. [14] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, 2007. [15] L. Hyafil and R. L. Rivest. Constructing optimal binary decision trees is NP-complete. Information Processing Letters, 5(1):15\u201317, 1976. [16] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Comput., 6(2):181\u2013214, 1994. [17] P. Kontschieder, P. Kohli, J. Shotton, and A. Criminisi. GeoF: Geodesic forests for learning coupled predictors. CVPR, 2013. [18] B. H. Menze, B. M. Kelm, D. N. Splitthoff, U. Koethe, and F. A. Hamprecht. On oblique random forests. In Machine Learning and Knowledge Discovery in Databases, pages 453\u2013469. 2011. [19] S. K. Murthy, S. Kasif, and S. Salzberg. A system for induction of oblique decision trees. Journal of Artificial Intelligence Research, 1994.\n[20] M. Norouzi and D. J. Fleet. Minimal Loss Hashing for Compact Binary Codes. ICML, 2011. [21] M. Norouzi, D. J. Fleet, and R. Salakhutdinov. Hamming Distance Metric Learning. NIPS, 2012. [22] S. Nowozin, C. Rother, S. Bagon, T. Sharp, B. Yao, and P. Kohli. Decision tree fields. ICCV, 2011. [23] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. JMLR, pages 2825\u20132830, 2011. [24] J. R. Quinlan. Induction of decision trees. Machine learning, 1986. [25] J. R. Quinlan. C4.5: programs for machine learning. Elsevier, 1993. [26] S. Rota Bulo\u0301 and P. Kontschieder. Neural decision forests for\nsemantic image labelling. CVPR, 2014. [27] J. Shotton, R. Girshick, A. Fitzgibbon, T. Sharp, M. Cook, M. Finoc-\nchio, R. Moore, P. Kohli, A. Criminisi, A. Kipman, and A. Blake. Efficient human pose estimation from single depth images. IEEE Trans. PAMI, 2013. [28] J. Shotton, T. Sharp, P. Kohli, S. Nowozin, J. Winn, and A. Criminisi. Decision jungles: Compact and rich models for classification. In NIPS, pages 234\u2013242. 2013. [29] B. M. Smith, L. Zhang, J. Brandt, Z. Lin, and J. Yang. Exemplarbased face parsing. In CVPR, pages 3484\u20133491, 2013. [30] R. Tibshirani and T. Hastie. Margin trees for high-dimensional classification. JMLR, 8, 2007. [31] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. ICML, 2004. [32] C.-N. J. Yu and T. Joachims. Learning structural SVMs with latent variables. ICML, 2009. [33] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Comput., pages 915\u2013936, 2003. [34] J. Zhu, H. Zou, S. Rosset, and T. Hastie. Multi-class adaboost. Statistics and Its Interface, 2009."}], "references": [{"title": "A support vector machine approach to decision trees", "author": ["K.P. Bennett", "J.A. Blue"], "venue": "IJCNN", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning, 24(2)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1):5\u201332", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": "Chapman & Hall/CRC", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1984}, {"title": "Decision Forests for Computer Vision and Medical Image Analysis", "author": ["A. Criminisi", "J. Shotton"], "venue": "Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Narrowing the gap: Random forests in theory and in practice", "author": ["M. Denil", "D. Matheson", "N. De Freitas"], "venue": "ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Trans. PAMI, pages 1627\u20131645", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics, pages 1189\u20131232", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "The elements of statistical learning (Ed", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "2). Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Induction of oblique decision trees", "author": ["D. Heath", "S. Kasif", "S. Salzberg"], "venue": "IJCAI", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "The random subspace method for constructing decision forests", "author": ["T.K. Ho"], "venue": "IEEE Trans. PAMI, pages 832\u2013844", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["G.B. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller"], "venue": "Technical Report 07-49, University of Massachusetts, Amherst", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Constructing optimal binary decision trees is NP-complete", "author": ["L. Hyafil", "R.L. Rivest"], "venue": "Information Processing Letters, 5(1):15\u201317", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1976}, {"title": "Hierarchical mixtures of experts and the EM algorithm", "author": ["M.I. Jordan", "R.A. Jacobs"], "venue": "Neural Comput., 6(2):181\u2013214", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "GeoF: Geodesic forests for learning coupled predictors", "author": ["P. Kontschieder", "P. Kohli", "J. Shotton", "A. Criminisi"], "venue": "CVPR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "On oblique random forests", "author": ["B.H. Menze", "B.M. Kelm", "D.N. Splitthoff", "U. Koethe", "F.A. Hamprecht"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "A system for induction of oblique decision trees", "author": ["S.K. Murthy", "S. Kasif", "S. Salzberg"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "Minimal Loss Hashing for Compact Binary Codes", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Hamming Distance Metric Learning", "author": ["M. Norouzi", "D.J. Fleet", "R. Salakhutdinov"], "venue": "NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Decision tree fields", "author": ["S. Nowozin", "C. Rother", "S. Bagon", "T. Sharp", "B. Yao", "P. Kohli"], "venue": "ICCV", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "JMLR, pages 2825\u20132830", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine learning", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1986}, {"title": "Neural decision forests for semantic image labelling", "author": ["S. Rota Bul\u00f3", "P. Kontschieder"], "venue": "CVPR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient human pose estimation from single depth images", "author": ["J. Shotton", "R. Girshick", "A. Fitzgibbon", "T. Sharp", "M. Cook", "M. Finocchio", "R. Moore", "P. Kohli", "A. Criminisi", "A. Kipman", "A. Blake"], "venue": "IEEE Trans. PAMI", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Decision jungles: Compact and rich models for classification", "author": ["J. Shotton", "T. Sharp", "P. Kohli", "S. Nowozin", "J. Winn", "A. Criminisi"], "venue": "In NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Exemplarbased face parsing", "author": ["B.M. Smith", "L. Zhang", "J. Brandt", "Z. Lin", "J. Yang"], "venue": "CVPR, pages 3484\u20133491", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Margin trees for high-dimensional classification", "author": ["R. Tibshirani", "T. Hastie"], "venue": "JMLR, 8", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "ICML", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning structural SVMs with latent variables", "author": ["C.-N.J. Yu", "T. Joachims"], "venue": "ICML", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Comput., pages 915\u2013936", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Multi-class adaboost", "author": ["J. Zhu", "H. Zou", "S. Rosset", "T. Hastie"], "venue": "Statistics and Its Interface", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "Decision trees [6], [24] and random forests [5], [13] have a long, successful history in machine learning, in part due to their computational efficiency and their applicability to large-scale classification and regression tasks (e.", "startOffset": 15, "endOffset": 18}, {"referenceID": 21, "context": "Decision trees [6], [24] and random forests [5], [13] have a long, successful history in machine learning, in part due to their computational efficiency and their applicability to large-scale classification and regression tasks (e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "Decision trees [6], [24] and random forests [5], [13] have a long, successful history in machine learning, in part due to their computational efficiency and their applicability to large-scale classification and regression tasks (e.", "startOffset": 44, "endOffset": 47}, {"referenceID": 10, "context": "Decision trees [6], [24] and random forests [5], [13] have a long, successful history in machine learning, in part due to their computational efficiency and their applicability to large-scale classification and regression tasks (e.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": ", see [7], [11]).", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": ", see [7], [11]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "A case in point is the Microsoft Kinect, where multiple decision trees are learned on millions of training exemplars to enable real time human pose estimation from depth images [27].", "startOffset": 177, "endOffset": 181}, {"referenceID": 3, "context": "The building block of this procedure is an optimization at each internal node of the tree, which divides the training data at that node into two subsets according to a splitting criterion, such as Gini impurity index in CART [6], or information gain in C4.", "startOffset": 225, "endOffset": 228}, {"referenceID": 19, "context": "This upper bound resembles a ramp loss, and accommodates any convex loss that is useful for multi-class classification, regression, or structured prediction [22].", "startOffset": 157, "endOffset": 161}, {"referenceID": 29, "context": "As explained below, the bound is the difference of two convex terms, the optimization of which is effectively accomplished using the Convex-Concave Procedure of [33].", "startOffset": 161, "endOffset": 165}, {"referenceID": 17, "context": "The proposed bound resembles the bound used for learning binary hash functions [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "[19] proposed a method called OC1, which yields some performance gains over CART and C4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "As a largescale experiment, we consider the task of segmenting faces from the Labeled Faces in the Wild (LFW) dataset [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 3, "context": "[6] proposed a version of CART that employs linear combination splits, known as CART-linearcombination (CART-LC).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[12], [19] proposed OC1, a refinement of CART-LC that uses random restarts and random perturbations to escape local minima.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[12], [19] proposed OC1, a refinement of CART-LC that uses random restarts and random perturbations to escape local minima.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "Here, by adopting a formulation based on the latent variable SVM [32], our algorithm provides a natural means of regularizing the oblique split stumps, thereby improving the generalization power of the trees.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "The hierarchical mixture of experts (HME) [16] uses soft splits rather than hard binary decisions to capture situations where the transition from low to high response is gradual.", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": "Our work builds upon random forest [5].", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "Random forest combines bootstrap aggregating (bagging) [4] and the random selection of features [13] to construct an ensemble of non-correlated decision trees.", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "Random forest combines bootstrap aggregating (bagging) [4] and the random selection of features [13] to construct an ensemble of non-correlated decision trees.", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "The method is used widely for classification and regression tasks, and research still investigates its theoretical characteristics [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "There also exist boosting based techniques for creating ensembles of decision trees [10], [34].", "startOffset": 84, "endOffset": 88}, {"referenceID": 30, "context": "There also exist boosting based techniques for creating ensembles of decision trees [10], [34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "[18] also consider a variant of oblique random forest.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Like other previous work [3], [30], the technique of [18] is only conveniently applicable to binary classification tasks.", "startOffset": 25, "endOffset": 28}, {"referenceID": 26, "context": "Like other previous work [3], [30], the technique of [18] is only conveniently applicable to binary classification tasks.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Like other previous work [3], [30], the technique of [18] is only conveniently applicable to binary classification tasks.", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "In contrast to [18], our technique is more general, and allows for optimization of multi-class classification and regression loss functions.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "Rota Bul\u00f3 & Kontschieder [26] recently proposed the use of multi-layer neural nets as split functions at internal nodes.", "startOffset": 25, "endOffset": 29}, {"referenceID": 18, "context": ", [21]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "Joint optimization of the split functions and leaf parameters according to a global objective is, however, known to be extremely challenging [15] due to the discrete and sequential nature of the decisions within the tree.", "startOffset": 141, "endOffset": 145}, {"referenceID": 27, "context": "Indeed, like the soft-margin binary SVM formulation, and margin rescaling formulations of structural SVM [31], the norm of w affects the interplay between the upper bound and empirical loss.", "startOffset": 105, "endOffset": 109}, {"referenceID": 29, "context": "The convex-concave nature of the surrogate objective allows us to use difference of convex (DC) programming, or the Convex-Concave Procedure (CCCP) [33], a method for minimizing objective functions expressed as sum of a convex and a concave term.", "startOffset": 148, "endOffset": 152}, {"referenceID": 6, "context": "[9] and Yu & Joachims [32] to optimize latent variable SVM models that employ a similar convex-concave surrogate objective.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[9] and Yu & Joachims [32] to optimize latent variable SVM models that employ a similar convex-concave surrogate objective.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "We exploit the bagging ensemble learning algorithm [4] to create the forest such that each tree is built by using a new data set sampled uniformly with replacement from the original dataset.", "startOffset": 51, "endOffset": 54}, {"referenceID": 8, "context": "Some previous work suggests the use of q = \u221a p as a heuristic [11], which is included in the candidate set.", "startOffset": 62, "endOffset": 66}, {"referenceID": 2, "context": "Here, we compare our Continuously Optimized Oblique (CO2) decision forest with random forest [5] and OC1 forest, forest built using OC1 [19].", "startOffset": 93, "endOffset": 96}, {"referenceID": 16, "context": "Here, we compare our Continuously Optimized Oblique (CO2) decision forest with random forest [5] and OC1 forest, forest built using OC1 [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 20, "context": "Results for random forest are obtained with the implementation of the scikit-learn package [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "(LFW) dataset [14].", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "To correct for the class label imbalance, like [17], we subsample training windows so that each label has an equal number of training examples.", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "This method produces decision forests analogous to [27].", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "We note that some other tree-like structures [28] and more sophisticated Computer Vision systems built for face segmentation [29] achieve better segmentation accuracy on LFW.", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "We note that some other tree-like structures [28] and more sophisticated Computer Vision systems built for face segmentation [29] achieve better segmentation accuracy on LFW.", "startOffset": 125, "endOffset": 129}], "year": 2015, "abstractText": "We propose a novel algorithm for optimizing multivariate linear threshold functions as split functions of decision trees to create improved Random Forest classifiers. Standard tree induction methods resort to sampling and exhaustive search to find good univariate split functions. In contrast, our method computes a linear combination of the features at each node, and optimizes the parameters of the linear combination (oblique) split functions by adopting a variant of latent variable SVM formulation. We develop a convex-concave upper bound on the classification loss for a one-level decision tree, and optimize the bound by stochastic gradient descent at each internal node of the tree. Forests of up to 1000 Continuously Optimized Oblique (CO2) decision trees are created, which significantly outperform Random Forest with univariate splits and previous techniques for constructing oblique trees. Experimental results are reported on multi-class classification benchmarks and on Labeled Faces in the Wild (LFW) dataset.", "creator": "LaTeX with hyperref package"}}}