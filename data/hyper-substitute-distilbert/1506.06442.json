{"id": "1506.06442", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2015", "title": "A Deep Memory-based Architecture for Sequence-to-Sequence Learning", "abstract": "we trace neural transformation machine ( scratch ), a digital architecture spanning sequence - among - table learning, ct identifies certain task through a generalized labeled speech transformations from the representation of arbitrary given transcript ( e. g., a scripted pronunciation ) to the final reconstructed sequence ( e. t., phonetic to english ). inspired by our pure paper turing technology [ 8 ], brain compute the intermediate representations into stacked bits of artifacts, and use script - write operations on computer memories to realize generic numerical representations composing those representations. those transformations merely designed in advance to proper translations are learned from data. through layer - out - layer transformations, human can measure algebraic relations simply allowing applications specified as machine translation but those languages. the architecture can be trained with normal back - propagation on remote texts, and system learning can be rapidly accomplished up to a large complexity. ntram is broad enough to subsume the state - in - ] - art logic translation found in [ 25 ] to its complexity case, hp significantly improves upon the model indicating better current application. remarkably, sen, being exclusively neural network - analytic, can achieve performance akin to the discrete phrase - based machine translation system ( ml ) with reasonably small vocabulary and even modest parameter specification.", "histories": [["v1", "Mon, 22 Jun 2015 02:12:54 GMT  (369kb,D)", "http://arxiv.org/abs/1506.06442v1", "12 pages"], ["v2", "Wed, 18 Nov 2015 13:55:44 GMT  (810kb,D)", "http://arxiv.org/abs/1506.06442v2", "12 pages, Under review as a conference paper at ICLR 2016"], ["v3", "Thu, 19 Nov 2015 14:23:34 GMT  (874kb,D)", "http://arxiv.org/abs/1506.06442v3", "12 pages, Under review as a conference paper at ICLR 2016"], ["v4", "Thu, 7 Jan 2016 08:14:08 GMT  (901kb,D)", "http://arxiv.org/abs/1506.06442v4", "13 pages, Under review as a conference paper at ICLR 2016"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["fandong meng", "zhengdong lu", "zhaopeng tu", "hang li", "qun liu"], "accepted": false, "id": "1506.06442"}, "pdf": {"name": "1506.06442.pdf", "metadata": {"source": "CRF", "title": "Neural Transformation Machine: A New Architecture for Sequence-to-Sequence Learning", "authors": ["Fandong Meng", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Qun Liu"], "emails": ["liuqun}@ict.ac.cn", "HangLi.HL}@huawei.com"], "sections": [{"heading": "1 Introduction", "text": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21]. Recently, there has been significant progress in development of technologies for the task using purely neural network-based models. Without loss of generality, we take machine translation as example in this paper. Previous efforts on neural machine translation generally fall into two categories:\n\u2022 Encoder-Decoder: models of this type first summarize the source sentence into a fixedlength vector by the encoder, typically implemented with a recurrent neural network (RNN) or a convolutional neural network (CNN), and then unfold the vector into the target sentence by the decoder, typically implemented with a RNN [1, 4, 19];\n\u2217The work is done when the first author worked as intern at Noahs Ark Lab, Huawei Technologies.\nar X\niv :1\n50 6.\n06 44\n2v 1\n[ cs\n.C L\n] 2\n2 Ju\n\u2022 Automatic Alignment: with RNNsearch [2] as representative, it represents the source sentence as a sequence of vectors after a processing step (e.g., through a bi-directional RNN [17]), and then simultaneously conducts dynamic alignment through a gating neural network and generation of the target sentence through another RNN.\nEmpirical comparison between the two schools of methods indicates that the automatic alignment approach is more efficient than the encoder-decoder approach: it can achieve comparable results with far less parameters and training instances [11]. This superiority in efficiency comes mainly from the mechanism of dynamic alignment, which avoids the need to represent the entire source sentence with a fixed-length vector [19].\nThe dynamic alignment mechanism [2] is intrinsically related to the content-based addressing on an external memory in the recently proposed Neural Turing Machines (NTM) [8]. Inspired by both works, we propose a novel deep architecture, named Neural Transformation Machine (NTram), for sequence-to-sequence learning. NTram carries out the task through a series of non-linear transformations from the input sequence, to different levels of intermediate representations, and eventually to the final output sequence. Similar to the notion of memory in [8], NTram stores the series of intermediate representations with stacked layers of memories, and conducts transformations between those representations with read-write operations on the memories. Through layer-by-layer stacking of such transformations, NTram generalizes the notion of inter-layer nonlinear mapping in neural networks, and therefore introduces a powerful new deep architecture tailored for sequence-to-sequence learning. NTram naturally takes RNNsearch [2] as a special case with a relatively shallow architecture. More importantly NTram accommodates many alternative deeper architectures, offering more modeling capability, and is empirically superior to RNNsearch on machine translation tasks."}, {"heading": "2 Read-Write as a Nonlinear Transformation", "text": "We start with discussing read-write operations between two pieces of memory as a generalized form of nonlinear transformation. As illustrated in Figure 1, this transformation consists of memories of two layers (R-memory and W - memory), read-heads, a write-head, and a controller. Basically, the controller sequentially operates the read-heads to get the values from R-memory (\u201creading\u201d), which are then sent to the write-head for modifying the values at specific locations in W -memory (\u201cwriting\u201d). These basic components are more formally defined below, following those in Neural Turing Machines (NTM) [8], with however important modifications for the nesting architecture, implementation efficiency and description simplicity.\n\u2022 Memory: a memory is generally defined as a matrix with potentially infinite size, while here we limit ourselves to pre-determined (pre-claimed) N \u00d7 d matrix, with N memory locations and d values in each location. In our implementation of NTram, N is always instance-dependent and is pre-determined by the algorithm. In a NTram system, we have memories of different layers, which in general have different d.\n\u2022 Controller: a controller operates the read-heads and write-head, with discussion on its mechanism put off to Section 2.1. For the simplicity in modeling, NTram is only allowed to read from memory of lower layer and write to memory of higher layer. As another constraint, reading memory can only be performed after the writing to it completes, following a similar convention as in NTM [8].\n\u2022 Read-heads: a read-head gets the values from the corresponding memory, following the instructions of the controller, which also influences the controller in feeding the state-machine (see Section 2.1). NTram allows multiple read-heads for one controller, with potentially different addressing strategies (see Section 2.2).\n\u2022 Write-head: a write-head simply takes the instruction from controller and modifies the values at specific locations."}, {"heading": "2.1 Controller", "text": "The core to the controller is a state machine, implemented as a Long Short-Term Memory RNN (LSTM) [10], with state at time t denoted as st. With st, the controller determines the reading and writing at time t, while the return of reading in turn takes part in updating the state. For simplicity, only one reading and writing is allowed at one time step, but more than one read-heads are allowed (see Section 3.1 for an example). Now suppose for one particular instance (index omitted for notational simplicity), the system reads from the R-memory (Mr, with Nr units) and writes to W -memory (denoted Mw, with Nw units)\nR-memory: Mr = {xr1 , xr2 , \u00b7 \u00b7 \u00b7 , xrNr}, W -memory: M w = {xw1 , xw2 , \u00b7 \u00b7 \u00b7 , xwNw},\nwith xrn \u2208 Rdr and xwn \u2208 Rdw . The main equations for controllers are then\nRead vector: rt = Fr(Mr, st; \u0398r) Write vector: vt = Fw(st; \u0398w)\nState update: st+1 = Fdyn(st, rt; \u0398dyn)\nwhere Fdyn(\u00b7), Fr(\u00b7) and Fw(\u00b7) are respectively the operators for dynamics, reading and writing1, parameterized by \u0398dyn, \u0398r, and \u0398w.\nIn the remainder of this section, we will discuss several relatively simple yet effective choices of reading and writing strategies."}, {"heading": "2.2 Addressing for Reading", "text": "Location-based Addressing With location-based addressing (L-addressing), the reading is simply rt = x r t . Notice that with L-addressing, the state machine automatically runs on a clock determined by the spatial structure of R-memory. Following this clock, the write-head operates the same number of times. One important variant, as suggested in [2, 19], is to go through R-memory backwards after the forward reading pass, where the state machine (LSTM) has the same structure but parameterized differently.\n1Note that our definition of writing is slightly different from that in [8].\nContent-based Addressing With a content-based addressing (C-addressing), the return at t is\nrt = Fr(Mr, st; \u0398r) = Nr\u2211 n=1 g\u0303(st,x r n; \u0398r)x r n, and g\u0303(st,x r n; \u0398r) = g(st,x r n; \u0398r)\u2211Nr n\u2032=1 g(st,x r n\u2032 ; \u0398r) ,\nwhere g(st,x r n; \u0398r), implemented as a deep neural network (DNN), gives un-normalized \u201caffliation\u201d score for unit xrn in R-memory. It is strongly related to the automatic alignment mechanism first introduced in [2] and general attention models discussed in computer vision [9]. One particular advantage of the content-based addressing is that it provides a way to do global reordering of the sequence, therefore introduces great flexibility in learning the representation.\nHybrid Addressing: With hybrid addressing (H-addressing) for reading, we essentially use two read-heads (can be easily extended to more), one with L-addressing and the other with C-addressing. At each time t, the controller simply concatenates the return of both read-heads as the final return:\nrt = [x r t , Nr\u2211 n=1 g(st,x r n; \u0398r)x r n].\nIt is worth noting that with H-addressing, the tempo of the state machine will be determined by the L-addressing read-head, and therefore creates W -memory of the same number of locations in writing. As shown later, H-addressing can be readily extended to allow readheads to work on different memories."}, {"heading": "2.3 Addressing for Writing", "text": "Location-based Addressing With location-based addressing (L-addressing), the writing is simple. At any time t, only the tth location in W -memory is updated: xWt = vt def = Fw(st; \u0398w), which will be kept unchanged afterwards.\nContent-based Addressing In a way similar to C-addressing for reading, the location for units to be written is determined through a gating network g(st,x w n,t; \u0398w), where the values in W -memory at time t is given by\n\u2206n,t = g\u0303(st,x w n,t; \u0398w)Fw(st; \u0398w), x w n,t = (1\u2212 \u03b1)xwn,t\u22121 + \u03b1\u2206n,t, n = 1, 2, \u00b7 \u00b7 \u00b7 , Nw,\nwhere xwn,t stands for the values of the n th location in W -memory at time t, \u03b1 is the forgetting factor (similarly defined as in [8]), g\u0303 is the normalized weight (with unnormalized score implemented also with a DNN) given to the nth location at time t."}, {"heading": "2.4 Read-Write as a Nonlinear Transformation", "text": "Clearly the read-write defined above in this section transforms the representation in R-memory to the representation in W -memory, with the spatial structure2 embedded in a way shaped\n2This spatial structure is to encode the temporal structure of input and output sequences.\njointly by design (in specifying the strategy and the implementation details) and the later supervised learning (in tuning the parameters). We argue that the memory offers more representational flexibility for encoding sequences with complicated structures, and the transformation introduced above provides more modeling flexibility for sequence-to-sequence learning.\nAs the most \u201cconventional\u201d special case, if we use L-addressing for both reading and writing, we actually get the familiar structure in units found in RNN with stacked layers [16]. Indeed, as illustrated in the figure right to the text, this read-write strategy will invoke a relatively local dependency based on the original spatial order in R-memory. It is not hard to show that we can recover some deep RNN model in [16] after stacking layers of read-write operations like this.\nThe C-addressing, however, be it for reading and writing, offers a means for major reordering of the cells, while H-addressing can add into it the spatial structure of the lower layer memory. Memory with designed inner structure gives more representational flexibility for sequences than a fixed-length vector, especially when coupled with appropriate reading strategy in composing the memory of next layer in a deep architecture as shown later. On the other hand, the learned memory-based representation is in general less universal than the fixed-length representation, since they typically needs a particular reading strategy to decode the information.\nIn this paper, we consider four types of transformations induced by combinations of the read and write addressing strategies, listed pictorially in Figure 2. Notice that 1) we only include one combination with C-addressing for writing since it is computationally expensive to optimize when combined with a C-addressing reading (see Section 3.2 for some analysis) , and 2) for one particular read-write strategy there are still a fair amount of implementation details to be specified, which are omitted due to the space limit. One can easily design different read/write strategies, for example a particular way of H-addressing for writing."}, {"heading": "3 NTram: Stacking Them Together", "text": "We stack the transformations together to form a deep architecture for sequence-to-sequence learning (named NTram), in a way analogous to the layers in DNNs. The aim of NTram is to learn the representation of sequence better suited to the task (e.g., machine translation) through layer-by-layer transformations. Just as in DNN, we expect that stacking relatively simple transformations can greatly enhances the expressing power and the efficiency of NTram, especially in handling translation between languages with vastly different syntactical structures (e.g., Chinese and English).\nAs illustrated in Figure 3 (left panel), the stacking is straightforward: we can just apply a transformation on top of another, with the W -memory in lower layer being the R-memory of upper layer. Based on the memory layers stacked in this manner, we can define the entire deep architecture of NTram, with diagram in Figure 3 (right panel). Basically, it starts with symbol sequence (Layer-0), then moves to sequence of word embeddings (Layer-1), through layers of transformation to reach the final intermeidate layer (Layer-L), which will be read by the output layer. The operations in output layer, relying on another LSTM to generating the target sequence, are similar to a memory read-write, with however the following two differences:\n\u2022 it predicts the symbols for the target sequence, and takes the \u201cguess\u201d as part of the input to update the state of the generating LSTM, while in a memory read-write, there is no information flow from higher layers to lower layers;\n\u2022 since the target sequence is in general of different length as the top-layer memory, it is limited to take only pure C-addressing reading, and relies on the built-in mechanism of the generating LSTM to stop (e.g., after generating a End-of-Sentence token).\nMemory of different layers could be equipped with different read-write strategies, and even for the same strategy, the configurations and learned parameters are in general different. This is in contrast to DNNs, for which the transformations of different layers are more homogeneous (mostly linear transforms with nonlinear activation function). As shown by our empirical study (Section 5), a sensible architecture design in combining the nonlinear transformations can greatly affect the performance of the model, on which however little is known and future research is needed."}, {"heading": "3.1 Cross-Layer Reading", "text": "In addition to the generic read-write strategies in Section 2.4, we also introduce the crosslayer reading into NTram for more modeling flexibility. In other words, for writing in any Layer-`, NTram allows reading from any layers lower than `, instead of just Layer-`\u22121. More specifically, we consider the following two cases.\nMemory-Bundle: A Memory-Bundle, as shown in Figure 4 (left panel), concatenates the units of two aligned memories in reading, regardless of the addressing strategy. Formally, the nth location in the bundle of memory Layer-`\u2032 and Layer-`\u2032\u2032 would be\nx(` \u2032+`\u2032\u2032) n = [(x `\u2032 n ) >, (x` \u2032\u2032 n ) >]>.\nSince it requires strict alignment between the memories to put together, Memory-Bundle is usually on layers created with spatial structure of same origin (see Section 4 for examples).\nShort-Cut Unlike Memory-Bundle, Short-Cut allows reading from layers with potentially different inner structures by using multiple read-heads, as shown in Figure 4 (right panel). For example, one can use a C-addressing read-head on memory Layer-`\u2032 and a Laddressing read-head on Layer-`\u2032\u2032 for the writing to memory Layer-` with ` > `\u2032, `\u2032\u2032."}, {"heading": "3.2 Optimization", "text": "For any designed architecture, the parameters to be optimized include {\u0398dyn,\u0398r,\u0398w} for each controller, the parameters for the LSTM in the output layer, and the word-embeddings. Since the reading from each memory can only be done after the writing on it completes, the \u201cfeed-forward\u201d process can be described in two scales: 1) the flow from memory of lower layer to memory of higher layer, and 2) the forming of a memory at each layer controlled by the corresponding state machine. Accordingly in optimization, the flow of \u201ccorrection signal\u201d also propagates at two scales:\n\u2022 On the \u201ccross-layer\u201d scale: the signal starts with the output layer and propagates from higher layers to lower layers, until Layer-1 for the tuning of word embedding;\n\u2022 On the \u201cwithin-layer\u201d scale: the signal back-propagates through time (BPTT) controlled by the corresponding state-machine (LSTM). In optimization, there is a correction for each reading or writing on each location in a memory, making the C-addressing more expensive than L-addressing for it in general involves all locations in the memory at each time t.\nThe optimization can be done via the standard back-propagation (BP) aiming to maximize the likelihood of the target sequence. In practice, we use the standard stochastic gradient descent (SGD [14]) and mini-batch (size 80) with learning rate controlled by AdaDelta [22]."}, {"heading": "4 NTram: Some Special Cases", "text": "We discuss four representative special cases of NTram: Arc-I, II, III and IV, as novel deep architectures for machine translation. We also show that RNNsearch [2] can be described in the framework in NTram as a relatively shallow case.\nArc-I The first proposal, including two variants (Arc-Ihyb and Arc-Iloc), is designed to demonstrate the effect of C-addressing in intermediate memory layers, with diagram shown in the figure right to the text. It employs a Laddressing reading from memory Layer-1 (the embedding layer), and L-addressing writing to Layer-2. After that, Arc-Ihyb writes to Layer-3 (L-addressing) based on its H-addressing reading (two read-heads) on Layer-2, while Arc-Iloc uses L-addressing to read from Layer-2. Once Layer-3 is formed, it is then put together with Layer-2 for a Memory-Bundle, from which the output layer reads (C-addressing) for predicting the target sequence.\nArc-II As a variant of Arc-Ihyb, this architecture is designed to investigate the effect of H-addressing reading from different layers of memory (or Short-Cut in Section 3.1). It uses the same strategy as Arc-Ihyb in generating memory Layer-1 and 2, but differs in generating Layer-3, where Arc-II uses C-addressing reading on Layer-2 but Laddressing reading on Layer-1. Once Layer-3 is formed, it is then put together with Layer-2 as a Memory-Bundle, which is then read by the output layer for predicting the target sequence.\nArc-III We intend to use this design to study a deeper architecture and more complicated addressing strategy. Arc-III follows the same way to generate Layer-1, Layer-2 and Layer-3 as Arc-II. After that it uses two read-heads combined with a L-addressing write to generate Layer-4, where the two read-heads consist of a L-addressing readhead on Layer-1 and a C-addressing read-head on the memory bundle of Layer-2 and Layer-3. After the generation of Layer-4, it puts Layer-2, 3 and 4 together for a bigger Memory-Bundle to the output layer. Arc-III, with 4 intermediate layers, is the deepest among the four special cases.\nArc-IV This proposal is designed to study the efficacy of C-addressing for writing in forming intermediate representation. It employs a L-addressing reading from memory Layer-1 and L-addressing writing to Layer-2. After that, it uses a L-addressing reading on Layer-2 to write to Layer3 with C-addressing. For C-addressing writing to Layer3, all locations in Layer-3 are initially set to be a linear transformation of that in Layer-2, where this linear transformation is also subject to optimization. Once Layer-3 is formed, it is then bundled with Layer-2 for the reading (C-addressing) of the output layer."}, {"heading": "4.1 Special Cases: RNNsearch", "text": "Interestingly, RNNsearch [2], the seminal work of neural translation model with automatic alignment, can be viewed as a special case of NTram with shallow architecture. More specifically, as illustrated in the figure right to text, it employs L-addressing reading on memory Layer-1 (the embedding layer), and L-addressing writing to Layer-2, which then is read (C-addressing) by the output layer for generating the target sequence. Essentially, Layer-2 is the only intermediate layer created by nontrivial read-write operations."}, {"heading": "5 Experiments", "text": "We report our empirical study on applying NTram to Chinese-to-English translation, and compare it against state-of-the-art NMT models and statistical machine translation model."}, {"heading": "5.1 Setup", "text": "Dataset and Evaluation Metrics: Our training data consist of 1.25M sentence pairs extracted from LDC corpora.3 The bilingual training data contain 27.9 million Chinese words and 34.5 million English words. We choose NIST MT Evaluation test set 2002 (MT02) as our development set, NIST MT Evaluation test sets 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets. The numbers of sentences in NIST MT02, MT03, MT04 and MT05 are 878, 919, 1788 and 1082 respectively. We use the case-insensitive 4-gram NIST BLEU score4 as our evaluation metric, and sign-test [6] as statistical significance test.\nPreprocessing: We perform word segmentation for Chinese with the Stanford NLP toolkit5, and English tokenization with the tokenizer from Moses6. In training of the neural networks,\n3The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.\n4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 5http://nlp.stanford.edu/software/segmenter.shtml 6http://www.statmt.org/moses/\nwe limit the source and target vocabularies to the most frequent 16K words in Chinese and English, covering approximately 95.8% and 98.3% of the two corpora respectively. All the out-of-vocabulary words are mapped to a special token UNK."}, {"heading": "5.2 Comparisons to Other Models", "text": "We compare our method with the following models:\n\u2022 Moses: We take the open source phrase-based translation system Moses [12] (with default configuration) as the comparison system of conventional SMT. The word alignments are obtained with GIZA++ [15] on the corpora in both directions, using the \u201cgrow-diag-final-and\u201d balance strategy [13]. We adopt SRI Language Modeling Toolkit [18] to train a 4-gram language model with modified Kneser-Ney smoothing on the target portion of training data. For Moses, we use all the vocabulary of training data.\n\u2022 RNNsearch: We also compare NTram against the automatic alignment model proposed in [2]. We use the default setting as in [2], denoted as RNNsearch (default), as well as the optimal re-scaling of the model (on sizes of both embedding and hidden layers, with about 50% more parameters than the default setting) in terms of the best testing performance, denoted as RNNsearch (best). We use the RNNsearch as the NMT baseline, for it represents the state-of-the-art neural machine translation methods with a small vocabulary and modest parameter size (30M\u223c50M).\nFor a fair comparison to RNNsearch, 1) the output layer in all NTram architectures are implemented as gated-RNN in [2] as a variant of LSTM [5], and 2) all the NTram architectures are designed to have the same embedding size as in RNNsearch (default) with parameter size less or comparable to RNNsearch (best)."}, {"heading": "5.3 Results", "text": "The main results of different models are given in Table 1. RNNsearch (best) is about 1.7 point behind Moses in BLEU7 on average, which is consistent with the observations made by other\n7The reported Moses does not include any language model trained with a separate monolingual corpus.\nauthors on different machine translation tasks [2, 11]. Remarkably, some sensible designs of NTram (e.g., Arc-II) can already achieve performance comparable to Moses, with only 42M parameters, while RNNsearch (best) has 46M parameters.\nClearly most of the NTram architectures (Arc-Ihyb, Arc-II and Arc-III) yield performances better than the NMT baselines. Among them, Arc-II outperforms the best setting of NMT baseline RNNsearch (best) by about 1.5 BLEU on average with less parameters. This suggests that the deeper architectures in NTram help to capture the transformation of representations essential to the machine translation."}, {"heading": "5.4 Discussion", "text": "Further comparison between Arc-Ihyb and Arc-Iloc (similar parameter sizes) suggests that C-addressing reading plays an important role in learning a powerful transformation between intermediate representations, necessary for translation between language pairs with vastly different syntactical structures. This conjecture is further verified by the good performances of Arc-II and Arc-III, both of which have C-addressing read-heads in their intermediate memory layers.\nClearly the BLEU scores of Arc-IV are considerably lower than the other architectures, while close analysis shows that the translation results are usually shorter than those of Arc-I\u223cArc-III for about 15%. One possibility is that our particular implementation of C-addressing for writing in Arc-IV (Section 4) is hard to optimize, which might need to be \u201cguided\u201d by another write-head or some smart initialization. This will be one thing for our future exploration.\nAs another observation, cross-layer reading almost always helps. The performances of Arc-I, II, III and IV unanimously drop after removing the Memory-Bundle and ShortCut (results omitted here), even after the broadening of memory units to keep the parameter size unchanged.\nWe have also observed some failure cases in the architecture designing for NTram, most notably when we have only single read-head with C-addressing in writing to the memory that is the sole layer going to the next stage. Comparison of this design with H-addressing (results omitted here) suggests that another read-head with L-addressing can prevent the transformation from going astray by adding the tempo from a memory with a clearer temporal structure."}, {"heading": "6 Conclusion", "text": "We propose NTram, a novel architecture for sequence-to-sequence learning, which is stimulated by the recent work of Neural Turing Machine[8] and Neural Machine Translation [2]. NTram builds its deep architecture for processing sequence data on the basis of a series of transformations induced by the read-write operations on a stack of memories. This new architecture significantly improves the expressing power of models in sequence-to-sequence learning, which is verified by our empirical study on benchmark machine translation tasks."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks", "author": ["M. Auli", "M. Galley", "C. Quirk", "G. Zweig"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of ICLR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1"], "venue": "In Proceedings of ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.2007,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst. Moses"], "venue": "In Proceedings of ACL on interactive poster and demonstration sessions,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "In Proceedings of NAACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "In Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of ICLR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "In Proceedings of ICSLP,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "gencnn: A convolutional architecture for word sequence prediction", "author": ["M. Wang", "Z. Lu", "H. Li", "W. Jiang", "Q. Liu"], "venue": "arXiv preprint arXiv:1503.05034,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "Inspired by the recent Neural Turing Machines [8], we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations of those representations.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "NTram is broad enough to subsume the state-of-the-art neural translation model in [2] as its special case, while significantly improves upon the model with its deeper architecture.", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 148, "endOffset": 155}, {"referenceID": 18, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 148, "endOffset": 155}, {"referenceID": 6, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 180, "endOffset": 187}, {"referenceID": 19, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 180, "endOffset": 187}, {"referenceID": 2, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 211, "endOffset": 218}, {"referenceID": 20, "context": "Sequence-to-sequence learning is a fundamental problem in natural language processing, with many important applications such as machine translation [2, 19], part-of-speech tagging [7, 20] and dependency parsing [3, 21].", "startOffset": 211, "endOffset": 218}, {"referenceID": 0, "context": "\u2022 Encoder-Decoder: models of this type first summarize the source sentence into a fixedlength vector by the encoder, typically implemented with a recurrent neural network (RNN) or a convolutional neural network (CNN), and then unfold the vector into the target sentence by the decoder, typically implemented with a RNN [1, 4, 19]; \u2217The work is done when the first author worked as intern at Noahs Ark Lab, Huawei Technologies.", "startOffset": 319, "endOffset": 329}, {"referenceID": 3, "context": "\u2022 Encoder-Decoder: models of this type first summarize the source sentence into a fixedlength vector by the encoder, typically implemented with a recurrent neural network (RNN) or a convolutional neural network (CNN), and then unfold the vector into the target sentence by the decoder, typically implemented with a RNN [1, 4, 19]; \u2217The work is done when the first author worked as intern at Noahs Ark Lab, Huawei Technologies.", "startOffset": 319, "endOffset": 329}, {"referenceID": 18, "context": "\u2022 Encoder-Decoder: models of this type first summarize the source sentence into a fixedlength vector by the encoder, typically implemented with a recurrent neural network (RNN) or a convolutional neural network (CNN), and then unfold the vector into the target sentence by the decoder, typically implemented with a RNN [1, 4, 19]; \u2217The work is done when the first author worked as intern at Noahs Ark Lab, Huawei Technologies.", "startOffset": 319, "endOffset": 329}, {"referenceID": 1, "context": "\u2022 Automatic Alignment: with RNNsearch [2] as representative, it represents the source sentence as a sequence of vectors after a processing step (e.", "startOffset": 38, "endOffset": 41}, {"referenceID": 16, "context": ", through a bi-directional RNN [17]), and then simultaneously conducts dynamic alignment through a gating neural network and generation of the target sentence through another RNN.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "Empirical comparison between the two schools of methods indicates that the automatic alignment approach is more efficient than the encoder-decoder approach: it can achieve comparable results with far less parameters and training instances [11].", "startOffset": 239, "endOffset": 243}, {"referenceID": 18, "context": "This superiority in efficiency comes mainly from the mechanism of dynamic alignment, which avoids the need to represent the entire source sentence with a fixed-length vector [19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 1, "context": "The dynamic alignment mechanism [2] is intrinsically related to the content-based addressing on an external memory in the recently proposed Neural Turing Machines (NTM) [8].", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "The dynamic alignment mechanism [2] is intrinsically related to the content-based addressing on an external memory in the recently proposed Neural Turing Machines (NTM) [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 7, "context": "Similar to the notion of memory in [8], NTram stores the series of intermediate representations with stacked layers of memories, and conducts transformations between those representations with read-write operations on the memories.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "NTram naturally takes RNNsearch [2] as a special case with a relatively shallow architecture.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "These basic components are more formally defined below, following those in Neural Turing Machines (NTM) [8], with however important modifications for the nesting architecture, implementation efficiency and description simplicity.", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "As another constraint, reading memory can only be performed after the writing to it completes, following a similar convention as in NTM [8].", "startOffset": 136, "endOffset": 139}, {"referenceID": 9, "context": "The core to the controller is a state machine, implemented as a Long Short-Term Memory RNN (LSTM) [10], with state at time t denoted as st.", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "One important variant, as suggested in [2, 19], is to go through R-memory backwards after the forward reading pass, where the state machine (LSTM) has the same structure but parameterized differently.", "startOffset": 39, "endOffset": 46}, {"referenceID": 18, "context": "One important variant, as suggested in [2, 19], is to go through R-memory backwards after the forward reading pass, where the state machine (LSTM) has the same structure but parameterized differently.", "startOffset": 39, "endOffset": 46}, {"referenceID": 7, "context": "Note that our definition of writing is slightly different from that in [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "It is strongly related to the automatic alignment mechanism first introduced in [2] and general attention models discussed in computer vision [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "It is strongly related to the automatic alignment mechanism first introduced in [2] and general attention models discussed in computer vision [9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 7, "context": "where xn,t stands for the values of the n th location in W -memory at time t, \u03b1 is the forgetting factor (similarly defined as in [8]), g\u0303 is the normalized weight (with unnormalized score implemented also with a DNN) given to the nth location at time t.", "startOffset": 130, "endOffset": 133}, {"referenceID": 15, "context": "As the most \u201cconventional\u201d special case, if we use L-addressing for both reading and writing, we actually get the familiar structure in units found in RNN with stacked layers [16].", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "It is not hard to show that we can recover some deep RNN model in [16] after stacking layers of read-write operations like this.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "In practice, we use the standard stochastic gradient descent (SGD [14]) and mini-batch (size 80) with learning rate controlled by AdaDelta [22].", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "In practice, we use the standard stochastic gradient descent (SGD [14]) and mini-batch (size 80) with learning rate controlled by AdaDelta [22].", "startOffset": 139, "endOffset": 143}, {"referenceID": 1, "context": "We also show that RNNsearch [2] can be described in the framework in NTram as a relatively shallow case.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "1 Special Cases: RNNsearch Interestingly, RNNsearch [2], the seminal work of neural translation model with automatic alignment, can be viewed as a special case of NTram with shallow architecture.", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "We use the case-insensitive 4-gram NIST BLEU score4 as our evaluation metric, and sign-test [6] as statistical significance test.", "startOffset": 92, "endOffset": 95}, {"referenceID": 11, "context": "\u2022 Moses: We take the open source phrase-based translation system Moses [12] (with default configuration) as the comparison system of conventional SMT.", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "The word alignments are obtained with GIZA++ [15] on the corpora in both directions, using the \u201cgrow-diag-final-and\u201d balance strategy [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "The word alignments are obtained with GIZA++ [15] on the corpora in both directions, using the \u201cgrow-diag-final-and\u201d balance strategy [13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "We adopt SRI Language Modeling Toolkit [18] to train a 4-gram language model with modified Kneser-Ney smoothing on the target portion of training data.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "\u2022 RNNsearch: We also compare NTram against the automatic alignment model proposed in [2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "We use the default setting as in [2], denoted as RNNsearch (default), as well as the optimal re-scaling of the model (on sizes of both embedding and hidden layers, with about 50% more parameters than the default setting) in terms of the best testing performance, denoted as RNNsearch (best).", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "For a fair comparison to RNNsearch, 1) the output layer in all NTram architectures are implemented as gated-RNN in [2] as a variant of LSTM [5], and 2) all the NTram architectures are designed to have the same embedding size as in RNNsearch (default) with parameter size less or comparable to RNNsearch (best).", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "For a fair comparison to RNNsearch, 1) the output layer in all NTram architectures are implemented as gated-RNN in [2] as a variant of LSTM [5], and 2) all the NTram architectures are designed to have the same embedding size as in RNNsearch (default) with parameter size less or comparable to RNNsearch (best).", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "authors on different machine translation tasks [2, 11].", "startOffset": 47, "endOffset": 54}, {"referenceID": 10, "context": "authors on different machine translation tasks [2, 11].", "startOffset": 47, "endOffset": 54}, {"referenceID": 7, "context": "We propose NTram, a novel architecture for sequence-to-sequence learning, which is stimulated by the recent work of Neural Turing Machine[8] and Neural Machine Translation [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "We propose NTram, a novel architecture for sequence-to-sequence learning, which is stimulated by the recent work of Neural Turing Machine[8] and Neural Machine Translation [2].", "startOffset": 172, "endOffset": 175}], "year": 2015, "abstractText": "We propose Neural Transformation Machine (NTram), a novel architecture for sequenceto-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recent Neural Turing Machines [8], we store the intermediate representations in stacked layers of memories, and use read-write operations on the memories to realize the nonlinear transformations of those representations. Those transformations are designed in advance but the parameters are learned from data. Through layer-by-layer transformations, NTram can model complicated relations necessary for applications such as machine translation between distant languages. The architecture can be trained with normal back-propagation on parallel texts, and the learning can be easily scaled up to a large corpus. NTram is broad enough to subsume the state-of-the-art neural translation model in [2] as its special case, while significantly improves upon the model with its deeper architecture. Remarkably, NTram, being purely neural network-based, can achieve performance comparable to the traditional phrase-based machine translation system (Moses) with a small vocabulary and a modest parameter size.", "creator": "LaTeX with hyperref package"}}}