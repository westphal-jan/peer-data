{"id": "1506.06579", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2015", "title": "Understanding Neural Networks Through Deep Visualization", "abstract": "recent years researchers produced great advances targeting brain behaviors, deep neural operations ( dnns ), including software clusters atop artificial auditory knowledge networks ( cnn ) while analyze natural sounds. simultaneously, our understanding of how these models arise, particularly what computations may perform at intermediate layers, has buried behind. researchers in physical field will be effectively accelerated by the development of adaptive tools among visualizing and interpreting neural programming. we introduce two practical tools here. nsa first gains another tool when realizes the activations constructed on previous partition of oracle mobile application as none possesses different image or concept ( e. g. visible single webcam stream ). ibm have calculated that looking at live activations patterns change in structure regarding user input ultimately build valuable equations governing how convnets work. the spatial perspective enables visualizing features at each layer of a cluster via regularized optimization in image representations. because previous versions of this idea take less recognizable detail, here. introduce several new regularization methods that combine to present qualitatively clearer, more interpretable visualizations. both areas are capturing text and apply roughly integrating specifically - trained architecture with neural setup.", "histories": [["v1", "Mon, 22 Jun 2015 12:57:15 GMT  (8509kb,D)", "http://arxiv.org/abs/1506.06579v1", "12 pages. To appear at ICML Deep Learning Workshop 2015"]], "COMMENTS": "12 pages. To appear at ICML Deep Learning Workshop 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jason yosinski", "jeff clune", "anh nguyen", "thomas fuchs", "hod lipson"], "accepted": false, "id": "1506.06579"}, "pdf": {"name": "1506.06579.pdf", "metadata": {"source": "META", "title": "Understanding Neural Networks Through Deep Visualization", "authors": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson"], "emails": ["YOSINSKI@CS.CORNELL.EDU", "JEFFCLUNE@UWYO.EDU", "ANGUYEN8@UWYO.EDU", "FUCHS@CALTECH.EDU", "HOD.LIPSON@CORNELL.EDU"], "sections": [{"heading": null, "text": "Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pretrained convnet with minimal setup.\nPublished in the Deep Learning Workshop, 31 st International Conference on Machine Learning, Lille, France, 2015. Copyright 2015 by the author(s)."}, {"heading": "1. Introduction", "text": "The last several years have produced tremendous progress in training powerful, deep neural network models that are approaching and even surpassing human abilities on a variety of challenging machine learning tasks (Taigman et al., 2014; Schroff et al., 2015; Hannun et al., 2014). A flagship example is training deep, convolutional neural networks (CNNs) with supervised learning to classify natural images (Krizhevsky et al., 2012). That area has benefitted from the combined effects of faster computing (e.g. GPUs), better training techniques (e.g. dropout (Hinton et al., 2012)), better activation units (e.g. rectified linear units (Glorot et al., 2011)), and larger labeled datasets (Deng et al., 2009; Lin et al., 2014).\nWhile there has thus been considerable improvements in our knowledge of how to create high-performing architectures and learning algorithms, our understanding of how these large neural models operate has lagged behind. Neural networks have long been known as \u201cblack boxes\u201d because it is difficult to understand exactly how any particular, trained neural network functions due to the large number of interacting, non-linear parts. Large modern neural networks are even harder to study because of their size; for example, understanding the widely-used AlexNet DNN involves making sense of the values taken by the 60 million trained network parameters. Understanding what is learned is interesting in its own right, but it is also one key way of further improving models: the intuitions provided by understanding the current generation of models should suggest ways to make them better. For example, the deconvolutional technique for visualizing the features learned by the hidden units of DNNs suggested an architectural change of smaller convolutional filters that led to ar X iv :1\n50 6.\n06 57\n9v 1\n[ cs\n.C V\n] 2\nstate of the art performance on the ImageNet benchmark in 2013 (Zeiler & Fergus, 2013).\nWe also note that tools that enable understanding will especially benefit the vast numbers of newcomers to deep learning, who would like to take advantage of off-the-shelf software packages \u2014 like Theano (Bergstra et al., 2010), Pylearn2 (Goodfellow et al., 2013), Caffe (Jia et al., 2014), and Torch (Collobert et al., 2011) \u2014 in new domains, but who may not have any intuition for why their models work (or do not). Experts can also benefit as they iterate ideas for new models or when they are searching for good hyperparameters. We thus believe that both experts and newcomers will benefit from tools that provide intuitions about the inner workings of DNNs. This paper provides two such tools, both of which are open source so that scientists and practitioners can integrate them with their own DNNs to better understand them.\nThe first tool is software that interactively plots the activations produced on each layer of a trained DNN for userprovided images or video. Static images afford a slow, detailed investigation of a particular input, whereas video input highlights the DNNs changing responses to dynamic input. At present, the videos are processed live from a user\u2019s computer camera, which is especially helpful because users can move different items around the field of view, occlude and combine them, and perform other manipulations to actively learn how different features in the network respond.\nThe second tool we introduce enables better visualization of the learned features computed by individual neurons at every layer of a DNN. Seeing what features have been learned is important both to understand how current DNNs work and to fuel intuitions for how to improve them.\nAttempting to understand what computations are performed at each layer in DNNs is an increasingly popular direction of research. One approach is to study each layer as a group and investigate the type of computation performed by the set of neurons on a layer as a whole (Yosinski et al., 2014; Mahendran & Vedaldi, 2014). This approach is informative because the neurons in a layer interact with each other to pass information to higher layers, and thus each neuron\u2019s contribution to the entire function performed by the DNN depends on that neuron\u2019s context in the layer.\nAnother approach is to try to interpret the function computed by each individual neuron. Past studies in this vein roughly divide into two different camps: dataset-centric and network-centric. The former requires both a trained DNN and running data through that network; the latter requires only the trained network itself. One dataset-centric approach is to display images from the training or test set that cause high or low activations for individual units. Another is the deconvolution method of Zeiler & Fer-\ngus (2013), which highlights the portions of a particular image that are responsible for the firing of each neural unit.\nNetwork-centric approaches investigate a network directly without any data from a dataset. For example, Erhan et al. (2009) synthesized images that cause high activations for particular units. Starting with some initial input x = x0, the activation ai(x) caused at some unit i by this input is computed, and then steps are taken in input space along the gradient \u2202ai(x)/\u2202x to synthesize inputs that cause higher and higher activations of unit i, eventually terminating at some x\u2217 which is deemed to be a preferred input stimulus for the unit in question. In the case where the input space is an image, x\u2217 can be displayed directly for interpretation. Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et al., 2013; Nguyen et al., 2014) or lower activations (Szegedy et al., 2013) for output units.\nThese gradient-based approaches are attractive in their simplicity, but the optimization process tends to produce images that do not greatly resemble natural images. Instead, they are composed of a collection of \u201chacks\u201d that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure (Simonyan et al., 2013; Nguyen et al., 2014; Szegedy et al., 2013; Goodfellow et al., 2014). The fact that activations may be effected by such hacks is better understood thanks to several recent studies. Specifically, it has been shown that such hacks may be applied to correctly classified images to cause them to be misclassified even via imperceptibly small changes (Szegedy et al., 2013), that such hacks can be found even without the gradient information to produce unrecognizable \u201cfooling examples\u201d (Nguyen et al., 2014), and that the abundance of non-natural looking images that cause extreme activations can be explained by the locally linear behavior of neural nets (Goodfellow et al., 2014).\nWith such strong evidence that optimizing images to cause high activations produces unrecognizable images, is there any hope of using such methods to obtain useful visualizations? It turns out there is, if one is able to appropriately regularize the optimization. Simonyan et al. (2013) showed that slightly discernible images for the final layers of a convnet could be produced withL2-regularization. Mahendran and Vedaldi (2014) also showed the importance of incorporating natural-image priors in the optimization process when producing images that mimic an entire-layer\u2019s firing pattern produced by a specific input image. We build on these works and contribute three additional forms of regularization that, when combined, produce more recognizable, optimization-based samples than previous methods. Because the optimization is stochastic, by starting at different random initial images, we can produce a set of opti-\n2\n3\nmized images whose variance provides information about the invariances learned by the unit.\nTo summarize, this paper makes the following two contributions:\n1. We describe and release a software tool that provides a live, interactive visualization of every neuron in a trained convnet as it responds to a user-provided image or video. The tool displays forward activation values, preferred stimuli via gradient ascent, top images for each unit from the training set, deconv highlighting (Zeiler & Fergus, 2013) of top images, and backward diffs computed via backprop or deconv starting from arbitrary units. The combined effect of these complementary visualizations promotes a greater understanding of what a neuron computes than any single method on its own. We also describe a few insights we have gained from using this tool. (Section 2).\n2. We extend past efforts to visualize preferred activation patterns in input space by adding several new types of regularization, which produce what we believe are the most interpretable images for large convnets so far (Section 3).\nBoth of our tools are released as open source and are available at http://yosinski.com/deepvis. While the tools could be adapted to integrate with any DNN software framework, they work out of the box with the popular Caffe DNN software package (Jia et al., 2014). Users may run visualizations with their own Caffe DNN or our pretrained DNN, which comes with pre-computed images optimized to activate each neuron in this trained network. Our pre-trained network is nearly identical to the \u201cAlexNet\u201d architecture (Krizhevsky et al., 2012), but with local reponse normalization layers after pooling layers following (Jia et al., 2014). It was trained with the Caffe framework on the ImageNet 2012 dataset (Deng et al., 2009)."}, {"heading": "2. Visualizing Live Convnet Activations", "text": "Our first visualization method is straightforward: plotting the activation values for the neurons in each layer of a convnet in response to an image or video. In fully connected neural networks, the order of the units is irrelevant, so plots of these vectors are not spatially informative. However, in convolutional networks, filters are applied in a way that respects the underlying geometry of the input; in the case of 2D images, filters are applied in a 2D convolution over the two spatial dimensions of the image. This convolution produces activations on subsequent layers that are, for each channel, also arranged spatially.\nFigure 1 shows examples of this type of plot for the conv5\nlayer. The conv5 layer has size 256\u00d713\u00d713, which we depict as 256 separate 13\u00d713 grayscale images. Each of the 256 small images contains activations in the same spatial x-y spatial layout as the input data, and the 256 images are simply and arbitrarily tiled into a 16\u00d716 grid in row-major order. Figure 2 shows a zoomed in view of one particular channel, conv5151, that responds to human and animal faces. All layers can be viewed in the software tool, including pooling and normalization layers. Visualizing these layers provides intuitions about their effects and functions.\nAlthough this visualization is simple to implement, we find it informative because all data flowing through the network can be visualized. There is nothing mysterious happening behind the scenes. Because this convnet contains only a single path from input to output, every layer is a bottleneck through which all information must pass en-route to a classification decision. The layer sizes are all small enough that any one layer can easily fit on a computer screen.1 So far, we have gleaned several surprising intuitions from using the tool:\n\u2022 One of the most interesting conclusions so far has been that representations on some layers seem to be surprisingly local. Instead of finding distributed representations on all layers, we see, for example, detectors for text, flowers, fruit, and faces on conv4 and conv5. These conclusions can be drawn either from the live visualization or the optimized images (or, best, by using both in concert) and suggest several directions for future research (discussed in Section 4).\n\u2022 When using direct file input to classify photos from Flickr or Google Images, classifications are often correct and highly confident (softmax probability for correct class near 1). On the other hand, when using input from a webcam, predictions often cannot be correct because no items from the training set are shown in the image. The training set\u2019s 1000 classes, though numerous, do not cover most common household objects. Thus, when shown a typical webcam view of a person with no ImageNet classes present, the output has no single high probability, as is expected. Surprisingly, however, this probability vector is noisy and varies significantly in response to tiny changes in the input, often changing merely in response to the noise from the webcam. We might have instead expected unchanging and low confidence predictions for a given scene when no object the network has been trained to classify is present. Plotting the fully connected layers (fc6 and fc7) also reveals a similar sensitivity to small input changes.\n1The layer with the most activations is conv1 which, when tiled, is only 550x550 before adding padding.\n4\n\u2022 Although the last three layers are sensitive to small input changes, much of the lower layer computation is more robust. For example, when visualizing the\nconv5 layer, one can find many invariant detectors for faces, shoulders, text, etc. by moving oneself or objects in front of the camera. Even though the 1000 classes contain no explicitly labeled faces or text, the network learns to identify these concepts simply because they represent useful partial information for making a later classification decision. One face detector, denoted conv5151 (channel number 151 on conv5), is shown in Figure 2 activating for human and lion faces and in Figure 1 activating for a cat face. Zhou et al. (2014) recently observed a similar effect where convnets trained only to recognize different scene types \u2014 playgrounds, restaurant patios, living rooms, etc. \u2014 learn object detectors (e.g. for chairs, books, and sofas) on intermediate layers.\nThe reader is encouraged to try this visualization tool out for him or herself. The code, together with pre-trained models and images synthesized by gradient ascent, can be downloaded at http://yosinski.com/deepvis."}, {"heading": "3. Visualizing via Regularized Optimization", "text": "The second contribution of this work is introducing several regularization methods to bias images found via optimization toward more visually interpretable examples. While each of these regularization methods helps on its own, in combination they are even more effective. We found useful combinations via a random hyperparameter search, as discussed below.\nFormally, consider an image x \u2208 RC\u00d7H\u00d7W , where C = 3 color channels and the height (H) and width (W ) are both 227 pixels. When this image is presented to a neural network, it causes an activation ai(x) for some unit i, where for simplicity i is an index that runs over all units on all layers. We also define a parameterized regularization function R\u03b8(x) that penalizes images in various ways.\nOur network was trained on ImageNet by first subtracting the per-pixel mean of examples in ImageNet before inputting training examples to the network. Thus, the direct input to the network, x, can be thought of as a zero-centered input. We may pose the optimization problem as finding an image x\u2217 where\nx\u2217 = argmax x (ai(x)\u2212R\u03b8(x)) (1)\nIn practice, we use a slightly different formulation. Because we search for x\u2217 by starting at some x0 and taking gradient steps, we instead define the regularization via an operator r\u03b8(\u00b7) that maps x to a slightly more regularized version of itself. This latter definition is strictly more expressive, allowing regularization operators r\u03b8 that are not\n5\nthe gradient of any R\u03b8. This method is easy to implement within a gradient descent framework by simply alternating between taking a step toward the gradient of ai(x) and taking a step in the direction given by r\u03b8. With a gradient descent step size of \u03b7, a single step in this process applies the update:\nx\u2190 r\u03b8 ( x+ \u03b7\n\u2202ai \u2202x\n) (2)\nWe investigated the following four regularizations. All are designed to overcome different pathologies commonly encountered by gradient descent without regularization.\nL2 decay: A common regularization, L2 decay penalizes large values and is implemented as r\u03b8(x) = (1\u2212\u03b8decay)\u00b7x. L2 decay tends to prevent a small number of extreme pixel values from dominating the example image. Such extreme single-pixel values neither occur naturally with great frequency nor are useful for visualization. L2 decay was also used by Simonyan et al. (2013).\nGaussian blur: Producing images via gradient ascent tends to produce examples with high frequency information (see Supplementary Section S1 for a possible reason). While these images cause high activations, they are neither realistic nor interpretable (Nguyen et al., 2014). A useful regularization is thus to penalize high frequency information. We implement this as a Gaussian blur step r\u03b8(x) = GaussianBlur(x, \u03b8b width). Convolving with a blur kernel is more computationally expensive than the other regularization methods, so we added another hyperparameter \u03b8b every to allow, for example, blurring every several optimization steps instead of every step. Blurring an image multiple times with a small width Gaussian kernel is equivalent to blurring once with a larger width kernel, and the effect will be similar even if the image changes slightly during the optimization process. This technique thus lowers computational costs without limiting the expressiveness of the regularization. Mahendran & Vedaldi (2014) used a penalty with a similar effect to blurring, called total variation, in their work reconstructing images from layer codes.\nClipping pixels with small norm: The first two regularizations suppress high amplitude and high frequency information, so after applying both, we are left with an x\u2217 that contains somewhat small, somewhat smooth values. However, x\u2217 will still tend to contain non-zero pixel values everywhere. Even if some pixels in x\u2217 show the primary object or type of input causing the unit under consideration to activate, the gradient with respect to all other pixels in x\u2217 will still generally be non-zero, so these pixels will also shift to show some pattern as well, contributing in whatever small way they can to ultimately raise the chosen unit\u2019s activation. We wish to bias the search away from such behavior\nand instead show only the main object, letting other regions be exactly zero if they are not needed. We implement this bias using an r\u03b8(x) that computes the norm of each pixel (over red, green, and blue channels) and then sets any pixels with small norm to zero. The threshold for the norm, \u03b8n pct, is specified as a percentile of all pixel norms in x.\nClipping pixels with small contribution: Instead of clipping pixels with small norms, we can try something slightly smarter and clip pixels with small contributions to the activation. One way of computing a pixel\u2019s contribution to an activation is to measure how much the activation increases or decreases when the pixel is set to zero; that is, to compute the contribution as |ai(x)\u2212 ai(x\u2212j)|, where x\u2212j is x but with the jth pixel set to zero. This approach is straightforward but prohibitively slow, requiring a forward pass for every pixel. Instead, we approximate this process by linearizing ai(x) around x, in which case the contribution of each dimension of x can be estimated as the elementwise product of x and the gradient. We then sum over all three channels and take the absolute value, computing | \u2211 c x \u25e6 \u2207xai(x)|. We use the absolute value to find pixels with small contribution in either direction, positive or negative. While we could choose to keep the pixel transitions where setting the pixel to zero would result in a large activation increase, these shifts are already handled by gradient ascent, and here we prefer to clip only the pixels that are deemed not to matter, not to take large gradient steps outside the region where the linear approximation is most valid. We define this r\u03b8(x) as the operation that sets pixels with contribution under the \u03b8c pct percentile to zero.\nIf the above regularization methods are applied individually, they are somewhat effective at producing more interpretable images; Figure 3 shows the effects of each individual hyperparameter. However, preliminary experiments uncovered that their combined effect produces better visualizations. To pick a reasonable set of hyperparameters for all methods at once, we ran a random hyperparameter search of 300 possible combinations and settled on four that complement each other well. The four selected combinations are listed in Table 1 and optimized images using each are shown for the \u201cGorilla\u201d class output unit in Figure 4. Of the four, some show high frequency information, others low frequency; some contain dense pixel data, and others contain only sparse outlines of important regions. We found the version in the lower-left quadrant to be the best single set of hyperparameters, but often greater intuition can be gleaned by considering all four at once. Figure 5 shows the optimization results computed for a selection of units on all layers. A single image for every filter of all five convolutional layers is shown in Supplementary Figure S1. Nine images for each filter of all layers, including each of the 1000 ImageNet output classes, can be viewed at http://yosinski.com/deepvis.\n6\n8"}, {"heading": "4. Discussion and Conclusion", "text": "We have introduced two visual tools for aiding in the interpretation of trained neural nets. Intuition gained from these tools may prompt ideas for improved methods and future research. Here we discuss several such ideas.\nThe interactive tool reveals that representations on later convolutional layers tend to be somewhat local, where channels correspond to specific, natural parts (e.g. wheels, faces) instead of being dimensions in a completely distributed code. That said, not all features correspond to natural parts, raising the possibility of a different decomposition of the world than humans might expect. These visualizations suggest that further study into the exact nature of learned representations \u2014 whether they are local to a single channel or distributed across several \u2014 is likely to be interesting (see Zhou et al. (2014) for work in this direction). The locality of the representation also suggests that during transfer learning, when new models are trained atop the conv4 or conv5 representations, a bias toward sparse connectivity could be helpful because it may be necessary to combine only a few features from these layers to create important features at higher layers.\nThe second tool \u2014 new regularizations that enable improved, interpretable, optimized visualizations of learned features \u2014 will help researchers and practitioners understand, debug, and improve their models. The visualizations also reveal a new twist in an ongoing story. Previous stud-\nies have shown that discriminative networks can easily be fooled or hacked by the addition of certain structured noise in image space (Szegedy et al., 2013; Nguyen et al., 2014). An oft-cited reason for this property is that discriminative training leads networks to ignore non-discriminative information in their input, e.g. learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs. For this reason it has been seen as a hopeless endeavor to create a generative model in which one randomly samples an x from a broad distribution on the space of all possible images and then iteratively transforms x into a recognizable image by moving it to a region that satisfies both a prior p(x) and posterior p(y|x) for some class label y. Past attempts have largely supported this view by producing unrealistic images using this method (Nguyen et al., 2014; Simonyan et al., 2013).\nHowever, the results presented here suggest an alternate possibility: the previously used priors may simply have been too weak (see Section S1 for one hypothesis of why a strong p(x) model is needed). With the careful design or learning of a p(x) model that biases toward realism, one may be able to harness the large number of parameters present in a discriminately learned p(y|x) model to generate realistic images by enforcing probability under both models simultaneously. Even with the simple, hand-coded p(x) models we use in this paper as regularizers, complex dependencies between distant pixels already arise (cf. the beetles with structure spanning over 100 pixels in Figure 4). This implies that the discriminative parameters also contain significant \u201cgenerative\u201d structure from the training dataset; that is, the parameters encode not only the jaguar\u2019s spots, but to some extent also its four legs. With better, learned probabilistic models over the input and activations of higher layers, much more structure may be apparent. Work by Dai et al. (2015) shows some interesting results in this direction. While the images generated in this paper are far from being photo-realistic, they do suggest that\n9\ntransferring discriminatively trained parameters to generative models \u2014 opposite the direction of the usual unsupervised pretraining approach \u2014 may be a fruitful area for further investigation."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the NASA Space Technology Research Fellowship (JY) for funding, Wendy Shang, Yoshua Bengio, Brian Cheung, and Andrej Karpathy for helpful discussions, and Freckles the cat for her feline countenance."}, {"heading": "S1. Why are gradient optimized images dominated by high frequencies?", "text": "In the main text we mentioned that images produced by gradient ascent to maximize the activations of neurons in convolutional networks tend to be dominated by high frequency information (cf. the left column of Figure 3). One hypothesis for why this occurs centers around the differing statistics of the activations of channels in a convnet. The conv1 layer consists of blobs of color and oriented Gabor edge filters of varying frequencies. The average activation values (after the rectifier) of the edge filters vary across filters, with low frequency filters generally having much higher average activation values than high frequency filters. In one experiment we observed that the average activation values of the five lowest frequency edge filters was 90 versus an average for the five highest frequency filters of 5.4, a difference of a factor of 17 (manuscript in preparation)2,3. The activation values for blobs of color generally fall in the middle of the range. This phenomenon likely arises for reasons related to the 1/f power spectrum of natural images in which low spatial frequencies tend to contain higher energy than high spatial frequencies (Torralba & Oliva, 2003).\nNow consider the connections from the conv1 filters to a single unit on conv2. In order to merge information from both low frequency and high frequency conv1 filters, the connection weights from high frequency conv1 units may generally have to be larger than connections from low frequency conv1 units in order to allow both signals to affect the conv2 unit\u2019s activation similarly. If this is the case, then due to the larger multipliers, the activation of this particular conv2 unit is affected more by small changes in the activations of high frequency filters than low frequency filters.\n2Li, Yosinski, Clune, Song, Hopcroft, Lipson. 2015. How similar are features learned by different deep neural networks? In preparation.\n3Activation values are averaged over the ImageNet validation set, over all spatial positions, over the channels with the five {highest, lowest} frequencies, and over four separately trained networks.\nSeen in the other direction: when gradient information is passed from higher layers to lower layers during backprop, the partial derivative arriving at this conv2 unit (a scalar) will be passed backward and multiplied by larger values when destined for high frequency conv1 filters than low frequency filters. Thus, following the gradient in pixel space may tend to produce an overabundance of high frequency changes instead of low frequency changes.\nThe above discussion focuses on the differing statistics of edge filters in conv1, but note that activation statistics on subsequent layers also vary across each layer.4 This may produce a similar (though more subtle to observe) effect in which rare higher layer features are also overrepresented compared to more common higher layer features.\nOf course, this hypothesis is only one tentative explanation for why high frequency information dominates the gradient. It relies on the assumption that the average activation of a unit is a representative statistic of the whole distribution of activations for that unit. In our observation this has been the case, with most units having similar, albeit scaled, distributions. However, more study is needed before a definitive conclusion can be reached."}, {"heading": "S2. Conv Layer Montages", "text": "One example optimized image using the hyperparameter settings from the third row of Table 1 for every filter of all five convolutional layers is shown in Figure S1.\n4We have observed that statistics vary on higher layers, but in a different manner: most channels on these layers have similar average activations, with most of the variance across channels being dominated by a small number of channels with unusually small or unusually large averages (Li, Yosinski, Clune, Song, Hopcroft, Lipson. 2015. How similar are features learned by different deep neural networks? In preparation.)\nconv5\nconv3 conv4\nconv2\nconv1\nFigure S1. One optimized, preferred image for every channel of all five convolutional layers. These images were produced with the hyperparameter combinations from the third row of Table 1. Best viewed electronically, zoomed in.\n12"}], "references": [{"title": "Torch7: A matlab-like environment for machine learning", "author": ["Collobert", "Ronan", "Kavukcuoglu", "Koray", "Farabet", "Cl\u00e9ment"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Generative modeling of convolutional neural networks", "author": ["Dai", "Jifeng", "Lu", "Yang", "Wu", "Ying Nian"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Visualizing higher-layer features of a deep", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Deep sparse rectifier networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Pylearn2: a machine learning research", "author": ["Goodfellow", "Ian J", "Warde-Farley", "David", "Lamblin", "Pascal", "Dumoulin", "Vincent", "Mirza", "Mehdi", "Pascanu", "Razvan", "Bergstra", "James", "Bastien", "Fr\u00e9d\u00e9ric", "Bengio", "Yoshua"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Explaining and Harnessing Adversarial Examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "ArXiv e-prints,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep Speech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": null, "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoff"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Microsoft COCO: common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C. Lawrence"], "venue": "CoRR, abs/1405.0312,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Understanding Deep Image Representations by Inverting Them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "ArXiv e-prints,", "citeRegEx": "Mahendran and Vedaldi,? \\Q2014\\E", "shortCiteRegEx": "Mahendran and Vedaldi", "year": 2014}, {"title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images", "author": ["Nguyen", "Anh", "Yosinski", "Jason", "Clune", "Jeff"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "FaceNet: A Unified Embedding for Face Recognition and Clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": null, "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian J", "Fergus", "Rob"], "venue": "CoRR, abs/1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Taigman", "Yaniv", "Yang", "Ming", "Ranzato", "Marc\u2019Aurelio", "Wolf", "Lior"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Taigman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2014}, {"title": "Statistics of natural image categories. Network: computation in neural systems", "author": ["Torralba", "Antonio", "Oliva", "Aude"], "venue": null, "citeRegEx": "Torralba et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2003}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1311.2901,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}, {"title": "Object detectors emerge in deep scene cnns", "author": ["Zhou", "Bolei", "Khosla", "Aditya", "Lapedriza", "\u00c0gata", "Oliva", "Aude", "Torralba", "Antonio"], "venue": "CoRR, abs/1412.6856,", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "How similar are features learned by different deep neural networks? In preparation", "author": ["Li", "Yosinski", "Clune", "Song", "Hopcroft", "Lipson"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "How similar are features learned by different deep neural networks", "author": ["Lipson"], "venue": null, "citeRegEx": "2015.,? \\Q2015\\E", "shortCiteRegEx": "2015.", "year": 2015}], "referenceMentions": [{"referenceID": 17, "context": "The last several years have produced tremendous progress in training powerful, deep neural network models that are approaching and even surpassing human abilities on a variety of challenging machine learning tasks (Taigman et al., 2014; Schroff et al., 2015; Hannun et al., 2014).", "startOffset": 214, "endOffset": 279}, {"referenceID": 14, "context": "The last several years have produced tremendous progress in training powerful, deep neural network models that are approaching and even surpassing human abilities on a variety of challenging machine learning tasks (Taigman et al., 2014; Schroff et al., 2015; Hannun et al., 2014).", "startOffset": 214, "endOffset": 279}, {"referenceID": 7, "context": "The last several years have produced tremendous progress in training powerful, deep neural network models that are approaching and even surpassing human abilities on a variety of challenging machine learning tasks (Taigman et al., 2014; Schroff et al., 2015; Hannun et al., 2014).", "startOffset": 214, "endOffset": 279}, {"referenceID": 10, "context": "A flagship example is training deep, convolutional neural networks (CNNs) with supervised learning to classify natural images (Krizhevsky et al., 2012).", "startOffset": 126, "endOffset": 151}, {"referenceID": 8, "context": "dropout (Hinton et al., 2012)), better activation units (e.", "startOffset": 8, "endOffset": 29}, {"referenceID": 4, "context": "rectified linear units (Glorot et al., 2011)), and larger labeled datasets (Deng et al.", "startOffset": 23, "endOffset": 44}, {"referenceID": 2, "context": ", 2011)), and larger labeled datasets (Deng et al., 2009; Lin et al., 2014).", "startOffset": 38, "endOffset": 75}, {"referenceID": 11, "context": ", 2011)), and larger labeled datasets (Deng et al., 2009; Lin et al., 2014).", "startOffset": 38, "endOffset": 75}, {"referenceID": 5, "context": ", 2010), Pylearn2 (Goodfellow et al., 2013), Caffe (Jia et al.", "startOffset": 18, "endOffset": 43}, {"referenceID": 9, "context": ", 2013), Caffe (Jia et al., 2014), and Torch (Collobert et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 0, "context": ", 2014), and Torch (Collobert et al., 2011) \u2014 in new domains, but who may not have any intuition for why their models work (or do not).", "startOffset": 19, "endOffset": 43}, {"referenceID": 3, "context": "For example, Erhan et al. (2009) synthesized images that cause high activations for particular units.", "startOffset": 13, "endOffset": 33}, {"referenceID": 13, "context": "Others have followed suit, using the gradient to find images that cause higher activations (Simonyan et al., 2013; Nguyen et al., 2014) or lower activations (Szegedy et al.", "startOffset": 91, "endOffset": 135}, {"referenceID": 16, "context": ", 2014) or lower activations (Szegedy et al., 2013) for output units.", "startOffset": 29, "endOffset": 51}, {"referenceID": 13, "context": "Instead, they are composed of a collection of \u201chacks\u201d that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure (Simonyan et al., 2013; Nguyen et al., 2014; Szegedy et al., 2013; Goodfellow et al., 2014).", "startOffset": 213, "endOffset": 304}, {"referenceID": 16, "context": "Instead, they are composed of a collection of \u201chacks\u201d that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure (Simonyan et al., 2013; Nguyen et al., 2014; Szegedy et al., 2013; Goodfellow et al., 2014).", "startOffset": 213, "endOffset": 304}, {"referenceID": 6, "context": "Instead, they are composed of a collection of \u201chacks\u201d that happen to cause high (or low) activations: extreme pixel values, structured high frequency patterns, and copies of common motifs without global structure (Simonyan et al., 2013; Nguyen et al., 2014; Szegedy et al., 2013; Goodfellow et al., 2014).", "startOffset": 213, "endOffset": 304}, {"referenceID": 16, "context": "Specifically, it has been shown that such hacks may be applied to correctly classified images to cause them to be misclassified even via imperceptibly small changes (Szegedy et al., 2013), that such hacks can be found even without the gradient information to produce unrecognizable \u201cfooling examples\u201d (Nguyen et al.", "startOffset": 165, "endOffset": 187}, {"referenceID": 13, "context": ", 2013), that such hacks can be found even without the gradient information to produce unrecognizable \u201cfooling examples\u201d (Nguyen et al., 2014), and that the abundance of non-natural looking images that cause extreme activations can be explained by the locally linear behavior of neural nets (Goodfellow et al.", "startOffset": 121, "endOffset": 142}, {"referenceID": 6, "context": ", 2014), and that the abundance of non-natural looking images that cause extreme activations can be explained by the locally linear behavior of neural nets (Goodfellow et al., 2014).", "startOffset": 156, "endOffset": 181}, {"referenceID": 14, "context": "Simonyan et al. (2013) showed that slightly discernible images for the final layers of a convnet could be produced withL2-regularization.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "Mahendran and Vedaldi (2014) also showed the importance of incorporating natural-image priors in the optimization process when producing images that mimic an entire-layer\u2019s firing pattern produced by a specific input image.", "startOffset": 0, "endOffset": 29}, {"referenceID": 9, "context": "While the tools could be adapted to integrate with any DNN software framework, they work out of the box with the popular Caffe DNN software package (Jia et al., 2014).", "startOffset": 148, "endOffset": 166}, {"referenceID": 10, "context": "Our pre-trained network is nearly identical to the \u201cAlexNet\u201d architecture (Krizhevsky et al., 2012), but with local reponse normalization layers after pooling layers following (Jia et al.", "startOffset": 74, "endOffset": 99}, {"referenceID": 9, "context": ", 2012), but with local reponse normalization layers after pooling layers following (Jia et al., 2014).", "startOffset": 84, "endOffset": 102}, {"referenceID": 2, "context": "It was trained with the Caffe framework on the ImageNet 2012 dataset (Deng et al., 2009).", "startOffset": 69, "endOffset": 88}, {"referenceID": 20, "context": "Zhou et al. (2014) recently observed a similar effect where convnets trained only to recognize different scene types \u2014 playgrounds, restaurant patios, living rooms, etc.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "L2 decay was also used by Simonyan et al. (2013).", "startOffset": 26, "endOffset": 49}, {"referenceID": 13, "context": "While these images cause high activations, they are neither realistic nor interpretable (Nguyen et al., 2014).", "startOffset": 88, "endOffset": 109}, {"referenceID": 20, "context": "These visualizations suggest that further study into the exact nature of learned representations \u2014 whether they are local to a single channel or distributed across several \u2014 is likely to be interesting (see Zhou et al. (2014) for work in this direction).", "startOffset": 207, "endOffset": 226}, {"referenceID": 16, "context": "ies have shown that discriminative networks can easily be fooled or hacked by the addition of certain structured noise in image space (Szegedy et al., 2013; Nguyen et al., 2014).", "startOffset": 134, "endOffset": 177}, {"referenceID": 13, "context": "ies have shown that discriminative networks can easily be fooled or hacked by the addition of certain structured noise in image space (Szegedy et al., 2013; Nguyen et al., 2014).", "startOffset": 134, "endOffset": 177}, {"referenceID": 13, "context": "Past attempts have largely supported this view by producing unrealistic images using this method (Nguyen et al., 2014; Simonyan et al., 2013).", "startOffset": 97, "endOffset": 141}, {"referenceID": 1, "context": "Work by Dai et al. (2015) shows some interesting results in this direction.", "startOffset": 8, "endOffset": 26}], "year": 2015, "abstractText": "Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pretrained convnet with minimal setup. Published in the Deep Learning Workshop, 31 st International Conference on Machine Learning, Lille, France, 2015. Copyright 2015 by the author(s).", "creator": "LaTeX with hyperref package"}}}