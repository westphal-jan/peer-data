{"id": "1506.07947", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2015", "title": "Collaboratively Learning Preferences from Ordinal Data", "abstract": "in professions such as recommendation matching and revenue filtering, it is important properly predict preferences on items they have not been seen amongst expert user or predict outcomes of comparisons among those that have also been processed. a popular discrete choice model of multinomial logit model captures parameter structure of the smallest variance with strict low - rank matrix. thus order to predict the results, candidate has to learn the underlying relations into noisy observations and the low - rank matrix, collected by revealed preferences in the forms of ordinal data. a convex system now learn such a program is sometimes practice a strict relaxation of nuclear norm statistics. we present different convex hardness approach in two sources of interest : weighted ranking theorem bundled rule modeling. in both interests, we show that our weak mask does minimax stationary. we prove an averaging pass conditional nash selection error with spatial precision, here derived a matching preference - theoretic descent bound.", "histories": [["v1", "Fri, 26 Jun 2015 03:06:27 GMT  (50kb,D)", "http://arxiv.org/abs/1506.07947v1", "38 pages 2 figures"]], "COMMENTS": "38 pages 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT stat.ML", "authors": ["sewoong oh", "kiran koshy thekumparampil", "jiaming xu"], "accepted": true, "id": "1506.07947"}, "pdf": {"name": "1506.07947.pdf", "metadata": {"source": "CRF", "title": "Collaboratively Learning Preferences from Ordinal Data", "authors": ["Sewoong Oh", "Kiran K. Thekumparampil", "Jiaming Xu"], "emails": ["swoh@illinois.edu", "thekump2@illinois.edu", "jxu18@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "In many applications such as recommendation systems and revenue management, it is important to predict preferences on items that have not been seen by a user or predict outcomes of comparisons among those that have never been compared. Predicting such hidden preferences would be hopeless without further assumptions on the structure of the preference. Motivated by the success of matrix factorization models on collaborative filtering applications, we model hidden preferences with lowrank matrices to collaboratively learn preference matrices from ordinal data. In this paper, we consider the following two concrete scenarios:\n\u2022 Collaborative ranking. Consider an online market that collects each user\u2019s preference as a ranking over a subset of items that are \u2018seen\u2019 by the user. Such data can be obtained by directly asking to compare some items, or by indirectly tracking online activities on which items are viewed, how much time is spent on the page, or how the user rated the items. In order to make personalized recommendations, we want (a) a model that captures how users who preferred similar items are also likely to have similar preferences on unseen items; and (b) to predict which items the user might prefer, by learning such models from ordinal data.\n\u2217Department of Industrial and Enterprise Systems Engineering, University of Illinois at Urbana-Champaign, email: swoh@illinois.edu \u2020Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, e-mail: thekump2@illinois.edu \u2021Statistics Department, University of Pennsylvania, e-mail: jxu18@illinois.edu\nar X\niv :1\n50 6.\n07 94\n7v 1\n[ cs\n.L G\n] 2\n6 Ju\nn 20\n\u2022 Bundled choice modeling. Discrete choice models describe how a user makes decisions on what to purchase. Typical choice models assume the willingness to buy an item is independent of what else the user bought. In many cases, however, we make \u2018bundled\u2019 purchases: we buy particular ingredients together for one recipe or we buy two connecting flights. One choice (the first flight) has a significant impact on the other (the connecting flight). In order to optimize the assortment (which flight schedules to offer) for maximum expected revenue, it is crucial to accurately predict the willingness of the consumers to purchase, based on past history. We consider a case where there are two types of products (e.g. jeans and shirts), and want (a) a model that captures such interacting preferences for pairs of items, one from each category; and (b) to predict the consumer\u2019s choice probabilities on pairs of items, by learning such models from past purchase history.\nWe use a discrete choice model known as MultiNomial Logit (MNL) model (described in Section 2.1) to represent the preferences. In collaborative ranking context, MNL uses a low-rank matrix to represent the hidden preferences of the users. Each row corresponds to a user\u2019s preference over all the items, and when presented with a subset of items the user provides a ranking over those items, which is a noisy version of the hidden true preference. the low-rank assumption naturally captures the similarities among users and items, by representing each on a low-dimensional space. In bundled choice modeling context, the low-rank matrix now represents how pairs of items are matched. Each row corresponds to an item from the first category and each column corresponds to an item from the second category. An entry in the matrix represents how much the pair is preferred by a randomly chosen user from a pool of users. Notice that in this case we do not model individual preferences, but the preference of the whole population. The purchase history of the population is the record of which pair was chosen among a subsets of items that were presented, which is again a noisy version of the hidden true preference. The low-rank assumption captures the similarities and dis-similarities among the items in the same category and the interactions across categories.\nContribution. A natural approach to learn such a low-rank model, from noisy observations, is to solve a convex relaxation of nuclear norm minimization (described in Section 2.2). We present such an approach for learning the MNL model from ordinal data, in two contexts: collaborative ranking and bundled choice modeling. In both cases, we analyze the sample complexity of the algorithm, and provide an upper bound on the resulting error with finite samples. We prove minimax-optimality of our approach by providing a matching information-theoretic lower bound (up to a poly-logarithmic factor). Technically, we utilize the Random Utility Model (RUM) interpretation (outlined in Section 2.1) of the MNL model to prove both the upper bound and the fundamental limit, which could be of interest to analyzing more general class of RUMs.\nRelated work. In the context of collaborative ranking, MNL models have been proposed to model partial rankings from a pool of users. Existing work is limited to the case when each user provides pair-wise comparisons [1, 2]. [2] proposes solving a convex relaxation of maximizing the likelihood over matrices with bounded nuclear norm. It is shown that this approach achieves statistically optimal generalization error rate. Our analysis techniques are inspired by [1], which proposed the convex relaxation similar to ours, but when the users provide only pair-wise comparisons. For pairwise comparisons, our main result in Theorem 3 matches those of [1], but our result is more general in the sense that we analyze more general sampling models beyond pairwise comparisons. In general, \u201ccollaborative ranking\u201d has been used typically to refer to the problem of learning personal rankings when the data is ratings on items (as opposed to ordinal data). Matrix factorization approaches have been widely applied in practice [3, 4], but no theoretical guarantees\nare known. The remainder of the paper is organized as follows. In Section 2, we present the MNL model and propose a convex relaxation for learning the model, in the context of collaborative ranking. We provide theoretical guarantees for collaborative ranking in Section 3. In Section 4, we present the problem statement for bundled choice modeling, and analyze a similar convex relaxation approach. Notations. We use |||A|||Fand |||A|||\u221eto denote the Frobenius norm and the `\u221e norm, |||A|||nuc =\u2211 i \u03c3i(A) to denote the nuclear norm where \u03c3i(A) denote the i-th singular value, and |||A|||2 = \u03c31(A)\nfor the spectral norm. We use \u3008\u3008u, v\u3009\u3009 = \u2211\ni uivi and \u2016u\u2016 to denote the inner product and the Euclidean norm. All ones vector is denoted by 1 and I(A) is the indicator function of the event A. The set of the fist N integers are denoted by [N ] = {1, . . . , N}."}, {"heading": "2 Model and Algorithm", "text": "In this section, we present a discrete choice modeling for collaborative ranking, and propose an inference algorithm for learning the model from ordinal data."}, {"heading": "2.1 MultiNomial Logit (MNL) model for comparative judgment", "text": "In collaborative ranking, we want to model how people who have similar preferences on a subset of items are likely to have similar tastes on other items as well. When users provide ratings, as in collaborative filtering applications, matrix factorization models are widely used since the low-rank structure captures the similarities between users. When users provide ordered preferences, we use a discrete choice model known as MultiNomial Logit (MNL) model that has a similar low-rank structure that captures the similarities between users and items.\nLet \u0398\u2217 be the d1\u00d7d2 dimensional matrix capturing the preference of d1users on d2 items, where the rows and columns correspond to users and items, respectively. Typically, \u0398\u2217 is assumed to be low-rank, having a rank r that is much smaller than the dimensions. However, in the following we allow a more general setting where \u0398\u2217 might be only approximately low rank. When a user i is presented with a set of alternatives Si \u2286 [d2], she reveals her preferences as a ranked list over those items. To simplify the notations we assume all users compare the same number k of items, but the analysis naturally generalizes to the case when the size might differ from a user to a user. Let vi,` \u2208 Si denote the (random) `-th best choice of user i. Each user gives a ranking, independent of other users\u2019 rankings, from\nP {vi,1, . . . , vi,k} = k\u220f `=1 e \u0398\u2217i,vi,`\u2211 j\u2208Si,` e \u0398\u2217i,j , (1)\nwhere with Si,` \u2261 Si \\ {vi,1, . . . , vi,`\u22121}. For a single user i, the i-th row of \u0398\u2217 represents the underlying preference vector of the user, and the more preferred items are more likely to be ranked higher. The probabilistic nature of the model captures the noise in the users revealed preferences.\nThe random utility model (RUM), pioneered by [5, 6, 7], describes the choices of users as manifestations of the underlying utilities. The MNL models is a special case of RUM where each decision maker and each alternative are represented by a r-dimensional feature vectors ui and vj respectively, such that \u0398\u2217ij = \u3008\u3008ui, vj\u3009\u3009, resulting in a low-rank matrix. When presented with a set of\nalternatives Si, the decision maker i ranks the alternatives according to their random utility drawn from\nUij = \u3008\u3008ui, vj\u3009\u3009+ \u03beij , (2)\nfor item j, where \u03beij follow the standard Gumbel distribution. Intuitively, this provides a justification for the MNL model as modeling the decision makers as rational being, seeking to maximize utility. Technically, this RUM interpretation plays a crucial role in our analysis, in proving restricted strong convexity in Appendix A.4 and also in proving fundamental limit in Appendix C.\nThere are a few cases where the Maximum Likelihood (ML) estimation for RUM is tractable. One notable example is the Plackett-Luce (PL) model, which is a special case of the MNL model where \u0398\u2217 is rank-one and all users have the same features. PL model has been widely applied in econometrics [8], analyzing elections [9], and machine learning [10]. Efficient inference algorithms has been proposed [11, 12, 13], and the sample complexity has been analyzed for the MLE [14] and for the Rank Centrality [15]. Although PL is quite restrictive, in the sense that it assumes all users share the same features, little is known about inference in RUMs beyond PL. Recently, to overcome such a restriction, mixed PL models have been studied, where \u0398\u2217 is rank-r but there are only r classes of users and all users in the same class have the same features. Efficient inference algorithms with provable guarantees have been proposed by applying recent advances in tensor decomposition methods [16, 17], directly clustering the users [18, 19], or using sampling methods [20]. However, this mixture PL is still restrictive, and both clustering and tensor based approaches rely heavily on the fact that the distribution is a \u201cmixture\u201d and require additional incoherence assumptions on \u0398\u2217. For more general models, efficient inference algorithms have been proposed [21] but no performance guarantee is known for finite samples. Although the MLE for the general MNL model in (1) is intractable, we provide a polynomial-time inference algorithm with provable guarantees."}, {"heading": "2.2 Nuclear norm minimization", "text": "Assuming \u0398\u2217 is well approximated by a low-rank matrix, we estimate \u0398\u2217 by solving the following convex relaxation given the observed preference in the form of ranked lists {(vi,1, . . . , vi,k)}i\u2208[d1].\n\u0398\u0302 \u2208 arg min \u0398\u2208\u2126 L(\u0398) + \u03bb|||\u0398|||nuc, (3)\nwhere the (negative) log likelihood function according to (1) is\nL(\u0398) = \u2212 1 k d1 d1\u2211 i=1 k\u2211 `=1 \u3008\u3008\u0398, eieTvi,`\u3009\u3009 \u2212 log \u2211 j\u2208Si,` exp ( \u3008\u3008\u0398, eieTj \u3009\u3009 ) , (4) with Si = {vi,1, . . . , vi,k} and Si,` \u2261 Si \\ {vi,1, . . . , vi,`\u22121}, and appropriately chosen set \u2126 defined in (7). Since nuclear norm is a tight convex surrogate for the rank, the above optimization searches for a low-rank solution that maximizes the likelihood. Nuclear norm minimization has been widely used in rank minimization problems [22], but provable guarantees typically exists only for quadratic loss function L(\u0398) [23, 24]. Our analysis extends such analysis techniques to identify the conditions under which restricted strong convexity is satisfied for a convex loss function that is not quadratic.\n3 Collaborative ranking from k-wise comparisons\nWe first provide background on the MNL model, and then present main results on the performance guarantees. Notice that the distribution (1) is independent of shifting each row of \u0398\u2217 by a constant. Hence, there is an equivalent class of \u0398\u2217 that gives the same distributions for the ranked lists:\n[\u0398\u2217] = {A \u2208 Rd1\u00d7d2 | A = \u0398\u2217 + u1T for some u \u2208 Rd1} . (5)\nSince we can only estimate \u0398\u2217 up to this equivalent class, we search for the one whose rows sum to zero, i.e. \u2211 j\u2208[d2] \u0398 \u2217 i,j = 0 for all i \u2208 [d1]. Let \u03b1 \u2261 maxi,j1,j2 |\u0398\u2217ij1 \u2212\u0398 \u2217 ij2 | denote the dynamic range of the underlying \u0398\u2217, such that when k items are compared, we always have\n1 k e\u2212\u03b1 \u2264 P {vi,1 = j} \u2264 1 k e\u03b1 , (6)\nfor all j \u2208 Si, all Si \u2286 [d2] satisfying |Si| = k and all i \u2208 [d1]. We do not make any assumptions on \u03b1 other than that \u03b1 = O(1) with respect to d1 and d2. The purpose of defining the dynamic range in this way is that we seek to characterize how the error scales with \u03b1. Given this definition, we solve the optimization in (3) over\n\u2126\u03b1 = { A \u2208 Rd1\u00d7d2 \u2223\u2223 |||A|||\u221e \u2264 \u03b1, and \u2200i \u2208 [d1] we have \u2211 j\u2208[d2] Aij = 0 } . (7)\nWhile in practice we do not require the `\u221e norm constraint, we need it for the analysis. For a related problem of matrix completion, where the loss L(\u03b8) is quadratic, either a similar condition on `\u221e norm is required or a different condition on incoherence is required."}, {"heading": "3.1 Performance guarantee", "text": "We provide an upper bound on the resulting error of our convex relaxation, when a multi-set of items Si presented to user i is drawn uniformly at random with replacement. Precisely, for a given k, Si = {ji,1, . . . , ji,k} where ji,`\u2019s are independently drawn uniformly at random over the d2 items. Further, if an item is sampled more than once, i.e. if there exists ji,`1 = ji,`2 for some i and `1 6= `2, then we assume that the user treats these two items as if they are two distinct items with the same MNL weights \u0398\u2217i,ji,`1 = \u0398\u2217i,ji,`2 .The resulting preference is therefore always over k items (with possibly multiple copies of the same item), and distributed according to (1). For example, if k = 3, it is possible to have Si = {ji,1 = 1, ji,2 = 1, ji,3 = 2}, in which case the resulting ranking can be (vi,1 = ji,1, vi,2 = ji,3, vi,3 = ji,2) with probability (e \u0398\u2217i,1)/(2 e\u0398 \u2217 i,1 + e\u0398 \u2217 i,2) \u00d7 (e\u0398 \u2217 i,2)/(e\u0398 \u2217 i,1 + e\u0398 \u2217 i,2). Such sampling with replacement is necessary for the analysis, where we require independence in the choice of the items in Si in order to apply the symmetrization technique (e.g. [25]) to bound the expectation of the deviation (cf. Appendix A.4). Similar sampling assumptions have been made in existing analyses on learning low-rank models from noisy observations, e.g. [24]. Let d \u2261 (d1 + d2)/2, and let \u03c3j(\u0398\u2217) denote the j-th singular value of the matrix \u0398\u2217. Define\n\u03bb0 \u2261 e2\u03b1 \u221a d1 log d+ d2(log d)2\nk d21 d2 .\nTheorem 1. Under the described sampling model, assume 24 \u2264 k \u2264 min{d21, (d21 +d22)/(2d1)} log d, and \u03bb \u2208 [32\u03bb0, c0\u03bb0] with any constant c0 = O(1) larger than 32. Then, solving the optimization (3) achieves\n1\nd1d2\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 F \u2264 288 \u221a 2 e4\u03b1c0\u03bb0 \u221a r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F + 288e4\u03b1c0\u03bb0 min{d1,d2}\u2211 j=r+1 \u03c3j(\u0398 \u2217) , (8)\nfor any r \u2208 {1, . . . ,min{d1, d2}} with probability at least 1\u2212 2d\u22123 where d = (d1 + d2)/2.\nA proof is provided in Appendix A. The above bound shows a natural splitting of the error into two terms, one corresponding to the estimation error for the rank-r component and the second one corresponding to the approximation error for how well one can approximate \u0398\u2217 with a rank-r matrix. This bound holds for all values of r and one could potentially optimize over r. We show such results in the following corollaries.\nCorollary 3.1 (Exact low-rank matrices). Suppose \u0398\u2217 has rank at most r. Under the hypotheses of Theorem 1, solving the optimization (3) with the choice of the regularization parameter \u03bb \u2208 [32\u03bb0, c0\u03bb0] achieves with probability at least 1\u2212 2d\u22123,\n1\u221a d1d2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F \u2264 288 \u221a 2e6\u03b1c0\n\u221a r(d1 log d+ d2(log d)2)\nk d1 . (9)\nThe number of entries is d1d2 and we rescale the Frobenius norm error appropriately by 1/ \u221a d1d2. When \u0398\u2217 is a rank-r matrix, then the degrees of freedom in representing \u0398\u2217 is r(d1 + d2) \u2212 r2 = O(r(d1 + d2)). The above theorem shows that the total number of samples, which is (k d1), needs to scale as O(rd1(log d) + rd2(log d)\n2) in order to achieve an arbitrarily small error. This is only poly-logarithmic factor larger than the degrees of freedom. In Section 3.2, we provide a lower bound on the error directly, that matches the upper bound up to a logarithmic factor.\nThe dependence on the dynamic range \u03b1, however, is sub-optimal. It is expected that the error increases with \u03b1, since the \u0398\u2217 scales as \u03b1, but the exponential dependence in the bound seems to be a weakness of the analysis, as seen from numerical experiments in the right panel of Figure 1. Although the error increase with \u03b1, numerical experiments suggests that it only increases at most linearly. However, tightening the scaling with respect to \u03b1 is a challenging problem, and such sub-optimal dependence is also present in existing literature for learning even simpler models, such as the Bradley-Terry model [15] or the Plackett-Luce model [14], which are special cases of the MNL model studied in this paper. A practical issue in achieving the above rate is the choice of \u03bb, since the dynamic range \u03b1 is not known in advance. Figure 1 illustrates that the error is not sensitive to the choice of \u03bb for a wide range.\nAnother issue is that the underlying matrix might not be exactly low rank. It is more realistic to assume that it is approximately low rank. Following [24] we formalize this notion with \u201c`q-ball\u201d of matrices defined as\nBq(\u03c1q) \u2261 {\u0398 \u2208 Rd1\u00d7d2 | \u2211\nj\u2208[min{d1,d2}]\n|\u03c3j(\u0398\u2217)|q \u2264 \u03c1q} . (10)\nWhen q = 0, this is a set of rank-\u03c10 matrices. For q \u2208 (0, 1], this is set of matrices whose singular values decay relatively fast. Optimizing the choice of r in Theorem 1, we get the following result.\nCorollary 3.2 (Approximately low-rank matrices). Suppose \u0398\u2217 \u2208 Bq(\u03c1q) for some q \u2208 (0, 1] and \u03c1q > 0. Under the hypotheses of Theorem 1, solving the optimization (3) with the choice of the regularization parameter \u03bb \u2208 [32\u03bb0, c0\u03bb0] achieves with probability at least 1\u2212 2d\u22123,\n1\u221a d1d2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F \u2264 2 \u221a \u03c1q\u221a d1d2 288\u221a2c0e6\u03b1 \u221a d1d2(d1 log d+ d2(log d)2) k d1  2\u2212q 2 . (11)\nThis is a strict generalization of Corollary 3.1. For q = 0 and \u03c10 = r, this recovers the exact low-rank estimation bound up to a factor of two. For approximate low-rank matrices in an `q-ball, we lose in the error exponent, which reduces from one to (2 \u2212 q)/2. A proof of this Corollary is provided in Appendix B.\nThe left panel of Figure 1 confirms the scaling of the error rate as predicted by Corollary 3.1. The lines merge to a single line when the sample size is rescaled appropriately. We make a choice of \u03bb = (1/2) \u221a (log d)/(kd2), This choice is independent of \u03b1 and is smaller than proposed in Theorem 1. We generate random rank-r matrices of dimension d \u00d7 d, where \u0398\u2217 = UV T with U \u2208 Rd\u00d7r and V \u2208 Rd\u00d7r entries generated i.i.d from uniform distribution over [0, 1]. Then the row-mean is subtracted form each row, and then the whole matrix is scaled such that the largest entry is \u03b1 = 5. The root mean squared error (RMSE) is plotted where RMSE = (1/d)|||\u0398\u2217 \u2212 \u0398\u0302|||F. We implement and solve the convex optimization (3) using proximal gradient descent method as analyzed in [26]. The right panel in Figure 1 illustrates that the actual error is insensitive to the choice of \u03bb for a broad range of \u03bb \u2208 [ \u221a (log d)/(kd2), 28 \u221a (log d)/(kd2)], after which it increases with \u03bb."}, {"heading": "3.2 Information-theoretic lower bound for low-rank matrices", "text": "For a polynomial-time algorithm of convex relaxation, we gave in the previous section a bound on the achievable error. We next compare this to the fundamental limit of this problem, by giving\na lower bound on the achievable error by any algorithm (efficient or not). A simple parameter counting argument indicates that it requires the number of samples to scale as the degrees of freedom i.e., kd1 \u221d r(d1 + d2), to estimate a d1 \u00d7 d2 dimensional matrix of rank r. We construct an appropriate packing over the set of low-rank matrices with bounded entries in \u2126\u03b1 defined as (7), and show that no algorithm can accurately estimate the true matrix with high probability using the generalized Fano\u2019s inequality. This provides a constructive argument to lower bound the minimax error rate, which in turn establishes that the bounds in Theorem 1 is sharp up to a logarithmic factor, and proves no other algorithm can significantly improve over the nuclear norm minimization.\nTheorem 2. Suppose \u0398\u2217 has rank r. Under the described sampling model, for large enough d1 and d2 \u2265 d1, there is a universal numerical constant c > 0 such that\ninf \u0398\u0302 sup \u0398\u2217\u2208\u2126\u03b1\nE [ 1\u221a\nd1d2\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F ] \u2265 c min { \u03b1e\u2212\u03b1 \u221a r d2 k d1 , \u03b1d2\u221a d1d2 log d } , (12)\nwhere the infimum is taken over all measurable functions over the observed ranked lists {(vi,1, . . . , vi,k)}i\u2208[d1].\nA proof of this theorem is provided in Appendix C. The term of primary interest in this bound is the first one, which shows the scaling of the (rescaled) minimax rate as \u221a r(d1 + d2)/(kd1) (when d2 \u2265 d1), and matches the upper bound in (8). It is the dominant term in the bound whenever the number of samples is larger than the degrees of freedom by a logarithmic factor, i.e., kd1 > r(d1 + d2) log d, ignoring the dependence on \u03b1. This is a typical regime of interest, where the sample size is comparable to the latent dimension of the problem. In this regime, Theorem 2 establishes that the upper bound in Theorem 1 is minimax-optimal up to a logarithmic factor in the dimension d."}, {"heading": "4 Choice modeling for bundled purchase history", "text": "In this section, we use the MNL model to study another scenario of practical interest: choice modeling from bundled purchase history. In this setting, we assume that we have bundled purchase history data from n users. Precisely, there are two categories of interest with d1 and d2 alternatives in each category respectively. For example, there are d1 tooth pastes to choose from and d2 tooth brushes to choose from. For the i-th user, a subset Si \u2286 [d1] of alternatives from the first category is presented along with a subset Ti \u2286 [d2] of alternatives from the second category. We use k1 and k2 to denote the number of alternatives presented to a single user, i.e. k1 = |Si| and k2 = |Ti|, and we assume that the number of alternatives presented to each user is fixed, to simplify notations. Given these sets of alternatives, each user makes a \u2018bundled\u2019 purchase and we use (ui, vi) to denote the bundled pair of alternatives (e.g. a tooth brush and a tooth paste) purchased by the i-th user. Each user makes a choice of the best alternative, independent of other users\u2019s choices, according to the MNL model as\nP {(ui, vi) = (j1, j2)} = e \u0398\u2217j1,j2\u2211 j\u20321\u2208Si,j\u20322\u2208Ti e \u0398\u2217 j\u20321,j \u2032 2 , (13)\nfor all j1 \u2208 Si and j2 \u2208 Ti. The distribution (13) is independent of shifting all the values of \u0398\u2217 by a constant. Hence, there is an equivalent class of \u0398\u2217 that gives the same distribution for the choices: [\u0398\u2217] \u2261 {A \u2208 Rd1\u00d7d2 |A = \u0398\u2217 + c11T for some c \u2208 R} . Since we can only estimate \u0398\u2217 up to this equivalent class, we search for the one that sum to zero, i.e. \u2211 j1\u2208[d1],j2\u2208[d2] \u0398 \u2217 j1,j2\n= 0. Let \u03b1 = maxj1,j\u20321\u2208[d1],j2,j\u20322\u2208[d2] |\u0398 \u2217 j1,j2 \u2212\u0398\u2217j\u20321,j\u20322 |, denote the dynamic range of the underlying \u0398 \u2217, such that when k1 \u00d7 k2 alternatives are presented, we always have\n1\nk1k2 e\u2212\u03b1 \u2264 P {(ui, vi) = (j1, j2)} \u2264\n1\nk1k2 e\u03b1 , (14)\nfor all (j1, j2) \u2208 Si \u00d7 Ti and for all Si \u2286 [d1] and Ti \u2286 [d2] such that |Si| = k1 and |Ti| = k2. We do not make any assumptions on \u03b1 other than that \u03b1 = O(1) with respect to d1 and d2. Assuming \u0398\u2217 is well approximate by a low-rank matrix, we solve the following convex relaxation, given the observed bundled purchase history {(ui, vi, Si, Ti)}i\u2208[n]:\n\u0398\u0302 \u2208 arg min \u0398\u2208\u2126\u2032\u03b1 L(\u0398) + \u03bb|||\u0398|||nuc , (15)\nwhere the (negative) log likelihood function according to (13) is\nL(\u0398) = \u2212 1 n n\u2211 i=1 \u3008\u3008\u0398, euieTvi\u3009\u3009 \u2212 log  \u2211 j1\u2208Si,j2\u2208Ti exp ( \u3008\u3008\u0398, ej1eTj2\u3009\u3009 ) , and (16) \u2126\u2032\u03b1 \u2261 { A \u2208 Rd1\u00d7d2\n\u2223\u2223 |||A|||\u221e \u2264 \u03b1, and \u2211 j1\u2208[d1],j2\u2208[d2] Aj1,j2 = 0 } . (17)\nCompared to collaborative ranking, (a) rows and columns of \u0398\u2217 correspond to an alternative from the first and second category, respectively; (b) each sample corresponds to the purchase choice of a user which follow the MNL model with \u0398\u2217; (c) each person is presented subsets Si and Ti of items from each category; (d) each sampled data represents the most preferred bundled pair of alternatives."}, {"heading": "4.1 Performance guarantee", "text": "We provide an upper bound on the error achieved by our convex relaxation, when the multi-set of alternatives Si from the first category and Ti from the second category are drawn uniformly at random with replacement from [d1] and [d2] respectively. Precisely, for given k1 and k2, we let Si = {j(i)1,1, . . . , j (i) 1,k1 } and Ti = {j(i)2,1, . . . , j (i) 2,k2 }, where j(i)1,`\u2019s and j (i) 2,`\u2019s are independently drawn uniformly at random over the d1 and d2 alternatives, respectively. Similar to the previous section, this sampling with replacement is necessary for the analysis. Define\n\u03bb1 =\n\u221a e2\u03b1 max{d1, d2} log d\nn d1 d2 . (18)\nTheorem 3. Under the described sampling model, assume 16e2\u03b1 min{d1, d2} log d \u2264 n \u2264 min{d5, k1k2 max{d21, d22}} log d, and \u03bb \u2208 [8\u03bb1, c1\u03bb1] with any constant c1 = O(1) larger than max{8, 128/ \u221a min{k1, k2}}. Then,\nsolving the optimization (15) achieves\n1\nd1d2\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 F \u2264 48 \u221a 2 e2\u03b1c1\u03bb1 \u221a r \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F + 48e2\u03b1c1\u03bb1 min{d1,d2}\u2211 j=r+1 \u03c3j(\u0398 \u2217) , (19)\nfor any r \u2208 {1, . . . ,min{d1, d2}} with probability at least 1\u2212 2d\u22123 where d = (d1 + d2)/2.\nA proof is provided in Appendix D. Optimizing over r gives the following corollaries.\nCorollary 4.1 (Exact low-rank matrices). Suppose \u0398\u2217 has rank at most r. Under the hypotheses of Theorem 3, solving the optimization (15) with the choice of the regularization parameter \u03bb \u2208 [8\u03bb1, c1\u03bb1] achieves with probability at least 1\u2212 2d\u22123,\n1\u221a d1d2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F \u2264 48 \u221a 2e3\u03b1c1 \u221a r(d1 + d2) log d n . (20)\nThis corollary shows that the number of samples n needs to scale as O(r(d1 +d2) log d) in order to achieve an arbitrarily small error. This is only a logarithmic factor larger than the degrees of freedom. We provide a fundamental lower bound on the error, that matches the upper bound up to a logarithmic factor. For approximately low-rank matrices in an `1-ball as defined in (10), we show an upper bound on the error, whose error exponent reduces from one to (2\u2212 q)/2.\nCorollary 4.2 (Approximately low-rank matrices). Suppose \u0398\u2217 \u2208 Bq(\u03c1q) for some q \u2208 (0, 1] and \u03c1q > 0. Under the hypotheses of Theorem 3, solving the optimization (15) with the choice of the regularization parameter \u03bb \u2208 [8\u03bb1, c1\u03bb1] achieves with probability at least 1\u2212 2d\u22123,\n1\u221a d1d2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F \u2264 2 \u221a \u03c1q\u221a d1d2 ( 48 \u221a 2c1e 3\u03b1 \u221a d1d2(d1 + d2) log d n ) 2\u2212q 2 . (21)\nSince the proof is almost identical to the proof of Corollary 3.2 in Appendix B, we omit it.\nTheorem 4. Suppose \u0398\u2217 has rank r. Under the described sampling model, there is a universal constant c > 0 such that\ninf \u0398\u0302 sup \u0398\u2217\u2208\u2126\u03b1\nE [ 1\u221a\nd1d2\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F ] \u2265 c min {\u221a e\u22125\u03b1 r (d1 + d2) n , \u03b1(d1 + d2)\u221a d1d2 log d } , (22)\nwhere the infimum is taken over all measurable functions over the observed purchase history {(ui, vi, Si, Ti)}i\u2208[n].\nA proof is provided in Appendix E.1. The first term is the dominant term, and when the sample size is comparable to the latent dimension of the problem, this theorem establishes that Theorem 3 is minimax optimal up to a logarithmic factor."}, {"heading": "5 Discussion", "text": "We list remaining challenges for future research. (a) Nuclear norm minimization, while polynomialtime, is still slow. We want first-order methods that are efficient with provable guarantees. The main challenge is providing a good initialization to start such non-convex approaches. (b) For simpler models, such as the PL model, more general sampling over a graph has been studied. We want analytical results for more general sampling."}, {"heading": "Appendix", "text": ""}, {"heading": "A Proof of Theorem 1", "text": "We first introduce some additional notations used in the proof. Recall that L(\u0398) is the log likelihood function. Let \u2207L(\u0398) \u2208 Rd1\u00d7d2 denote its gradient such that \u2207ijL(\u0398) = \u2202L(\u0398)\u2202\u0398ij . Let \u2207 2L(\u0398) \u2208 Rd1d2\u00d7d1d2 denote its Hessian matrix such that \u22072ij,i\u2032j\u2032L(\u0398) = \u22022L(\u0398) \u2202\u0398ij\u2202\u0398i\u2032j\u2032 . By the definition of L(\u0398) in (4), we have\n\u2207L(\u0398\u2217) = \u2212 1 k d1 d1\u2211 i=1 k\u2211 `=1 ei(evi,` \u2212 pi,`) T , (23)\nwhere pi,` denotes the conditional choice probability at `-th position. Precisely, pi,` = \u2211\nj\u2208Si,` pj|(i,`)ej where pj|(i,`) is the probability that item j is chosen at `-th position from the top by the user i condi-\ntioned on the top `\u22121 choices such that pj|(i,`) \u2261 P {vi,` = j|vi,1, . . . , vi,`\u22121, Si} = e\u0398 \u2217 ij/( \u2211 j\u2032\u2208Si,` e \u0398ij\u2032 ) and Si,` \u2261 Si \\ {vi,1, . . . , vi,`\u22121}, where Si is the set of alternatives presented to the i-th user and vi,` is the item ranked at the `-th position by the user i. Notice that for i 6= i\u2032, \u2202\n2L(\u0398) \u2202\u0398ij\u2202\u0398i\u2032j\u2032 = 0 and\nthe Hessian is\n\u22022L(\u0398) \u2202\u0398ij\u2202\u0398ij\u2032 = 1 k d1 k\u2211 `=1 I ( j \u2208 Si,` )\u2202pj|(i,`) \u2202\u0398ij\u2032\n= 1\nk d1 k\u2211 `=1 I ( j, j\u2032 \u2208 Si,` ) ( pj|(i,`)I(j = j\u2032)\u2212 pj|(i,`)pj\u2032|(i,`) ) . (24)\nThis Hessian matrix is a block-diagonal matrix \u22072L(\u0398) = diag(H(1)(\u0398), . . . ,H(d1)(\u0398)) with\nH(i)(\u0398) = 1\nk d1 k\u2211 `=1 ( diag(pi,`)\u2212 pi,`pTi,` ) . (25)\nLet \u2206 = \u0398\u2217\u2212\u0398\u0302 where \u0398\u0302 is the optimal solution of the convex program in (3). We first introduce three key technical lemmas. The first lemma follows from Lemma 1 of [24], and shows that \u2206 is approximately low-rank.\nLemma A.1. If \u03bb \u2265 2|||\u2207L(\u0398\u2217)|||2, then we have\n|||\u2206|||nuc \u2264 4 \u221a 2r|||\u2206|||F + 4 min{d1,d2}\u2211 j=\u03c1+1 \u03c3j(\u0398 \u2217) , (26)\nfor all \u03c1 \u2208 [min{d1, d2}].\nThe following lemma provides a bound on the gradient using the concentration of measure for sum of independent random matrices [27].\nLemma A.2. For any positive constant c \u2265 1 and log d \u2265 4(1 + c)/9, with probability at least 1\u2212 2d\u2212c,\n\u2016\u2207L(\u0398\u2217)\u20162 \u2264\n\u221a 4(1 + c) log d\nk d21 max\n{\u221a d1/d2 , e 2\u03b1 \u221a 4(1 + c) log d } . (27)\nSince we are typically interested in the regime where the number of samples is much smaller than the dimension d1 \u00d7 d2 of the problem, the Hessian is typically not positive definite. However, when we restrict our attention to the vectorized \u2206 with relatively small nuclear norm, then we can prove restricted strong convexity, which gives the following bound.\nLemma A.3 (Restricted Strong Convexity for collaborative ranking). Fix any \u0398 \u2208 \u2126\u03b1 and assume 24 \u2264 k \u2264 min{d21, (d21 + d22)/(2d1)} log d. Under the random sampling model of the alternatives {ji`}i\u2208[d1],`\u2208[k] and the random outcome of the comparisons described in section 1, with probability larger than 1\u2212 2d\u2212218,\nVec(\u2206)T \u22072L(\u0398) Vec(\u2206) \u2265 e \u22124\u03b1\n24 d1d2 |||\u2206|||2F , (28)\nfor all \u2206 in A where A = { \u2206 \u2208 Rd1\u00d7d2 \u2223\u2223 |||\u2206|||\u221e \u2264 2\u03b1 , \u2211\nj\u2208[d2]\n\u2206ij = 0 for all i \u2208 [d1] and |||\u2206|||2F \u2265 \u00b5|||\u2206|||nuc } . (29)\nwith\n\u00b5 \u2261 210 e2\u03b1 \u03b1d2\n\u221a d1 log d\nk min{d1, d2} . (30)\nBuilding on these lemmas, the proof of Theorem 1 is divided into the following two cases. In both cases, we will show that\n|||\u2206|||2F \u2264 72 e 4\u03b1c0\u03bb0 d1d2 |||\u2206|||nuc , (31)\nwith high probability. Applying Lemma A.1 proves the desired theorem. We are left to show Eq. (31) holds.\nCase 1: Suppose |||\u2206|||2F \u2265 \u00b5 |||\u2206|||nuc. With \u2206 = \u0398\u2217 \u2212 \u0398\u0302, the Taylor expansion yields\nL(\u0398\u0302) = L(\u0398\u2217)\u2212 \u3008\u3008\u2207L(\u0398\u2217),\u2206\u3009\u3009+ 1 2 Vec(\u2206)\u22072L(\u0398)VecT (\u2206), (32)\nwhere \u0398 = a\u0398\u0302 + (1\u2212 a)\u0398\u2217 for some a \u2208 [0, 1]. It follows from Lemma A.3 that with probability at least 1\u2212 2d\u2212218 ,\nL(\u0398\u0302)\u2212 L(\u0398\u2217) \u2265 \u2212\u3008\u3008\u2207L(\u0398\u2217),\u2206\u3009\u3009+ e \u22124\u03b1\n48 d1 d2 |||\u2206|||2F\n\u2265 \u2212|||\u2207L(\u0398\u2217)|||2|||\u2206|||nuc + e\u22124\u03b1\n48 d1 d2 |||\u2206|||2F .\nFrom the definition of \u0398\u0302 as an optimal solution of the minimization, we have L(\u0398\u0302)\u2212 L(\u0398\u2217) \u2264 \u03bb ( |||\u0398\u2217|||nuc \u2212 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 nuc ) \u2264 \u03bb|||\u2206|||nuc .\nBy the assumption, we choose \u03bb \u2265 32\u03bb0. In view of Lemma A.2, this implies that \u03bb \u2265 2|||\u2207L(\u0398\u2217)|||2 with probability at least 1\u2212 2d\u22123. It follows that with probability at least 1\u2212 2d\u22123 \u2212 2d\u2212218 ,\ne\u22124\u03b1\n48d1d2 |||\u2206|||2F \u2264\n( \u03bb+ |||\u2207L(\u0398\u2217)|||2 ) |||\u2206|||nuc \u2264 3\u03bb\n2 |||\u2206|||nuc .\nBy our assumption on \u03bb \u2264 c0\u03bb0, this proves the desired bound in Eq. (31) Case 2: Suppose |||\u2206|||2F \u2264 \u00b5 |||\u2206|||nuc. By the definition of \u00b5 and the fact that c0 \u2265 32, it follows that \u00b5 \u2264 72 e4\u03b1c0\u03bb0 d1d2, and we get the same bound as in Eq. (31)."}, {"heading": "A.1 Proof of Lemma A.1", "text": "Denote the singular value decomposition of \u0398\u2217 by \u0398\u2217 = U\u03a3V T , where U \u2208 Rd1\u00d7d1 and V \u2208 Rd2\u00d7d2 are orthogonal matrices. For a given r \u2208 [min{d1, d2}], Let Ur = [u1, . . . , ur] and Vr = [v1, . . . , vr], where ui \u2208 Rd1\u00d71 and vi \u2208 Rd2\u00d71 are the left and right singular vectors corresponding to the i-th largest singular value, respectively. Define T to be the subspace spanned by all matrices in Rd1\u00d7d2 of the form UrA\nT or BV Tr for any A \u2208 Rd2\u00d7r or B \u2208 Rd1\u00d7r, respectively. The orthogonal projection of any matrix M \u2208 Rd1\u00d7d2 onto the space T is given by PT (M) = UrUTr M +MVrV Tr \u2212UrUTr MVrV Tr . The projection of M onto the complement space T\u22a5 is PT\u22a5(M) = (I \u2212 UrUTr )M(I \u2212 VrV Tr ). The subspace T and the respective projections onto T and T\u22a5 play crucial a role in the analysis of nuclear norm minimization, since they define the sub-gradient of the nuclear norm at \u0398\u2217. We refer to [23] for more detailed treatment of this topic.\nLet \u2206\u2032 = PT (\u2206) and \u2206\u2032\u2032 = PT\u22a5(\u2206). Notice that PT (\u0398\u2217) = Ur\u03a3rV Tr , where \u03a3r \u2208 Rr\u00d7r is the diagonal matrix formed by the top r singular values. Since PT (\u0398\u2217) and \u2206\u2032\u2032 have row and column spaces that are orthogonal, it follows from Lemma 2.3 in [22] that\u2223\u2223\u2223\u2223\u2223\u2223PT (\u0398\u2217)\u2212\u2206\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u2223nuc = |||PT (\u0398\u2217)|||nuc + \u2223\u2223\u2223\u2223\u2223\u2223\u2206\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u2223nuc . Hence, in view of the triangle inequality,\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 nuc = \u2223\u2223\u2223\u2223\u2223\u2223PT (\u0398\u2217) + PT\u22a5(\u0398\u2217)\u2212\u2206\u2032 \u2212\u2206\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u2223nuc\n\u2265 \u2223\u2223\u2223\u2223\u2223\u2223PT (\u0398\u2217)\u2212\u2206\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u2223nuc \u2212 \u2223\u2223\u2223\u2223\u2223\u2223PT\u22a5(\u0398\u2217)\u2212\u2206\u2032\u2223\u2223\u2223\u2223\u2223\u2223nuc\n= |||PT (\u0398\u2217)|||nuc + \u2223\u2223\u2223\u2223\u2223\u2223\u2206\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u2223 nuc \u2212 \u2223\u2223\u2223\u2223\u2223\u2223PT\u22a5(\u0398\u2217)\u2212\u2206\u2032\u2223\u2223\u2223\u2223\u2223\u2223nuc\n\u2265 |||PT (\u0398\u2217)|||nuc + \u2223\u2223\u2223\u2223\u2223\u2223\u2206\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u2223 nuc \u2212 |||PT\u22a5(\u0398\u2217)|||nuc \u2212 \u2223\u2223\u2223\u2223\u2223\u2223\u2206\u2032\u2223\u2223\u2223\u2223\u2223\u2223 nuc\n= |||\u0398\u2217|||nuc + \u2223\u2223\u2223\u2223\u2223\u2223\u2206\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u2223 nuc \u2212 2|||PT\u22a5(\u0398\u2217)|||nuc \u2212 \u2223\u2223\u2223\u2223\u2223\u2223\u2206\u2032\u2223\u2223\u2223\u2223\u2223\u2223 nuc . (33)\nBecause \u0398\u0302 is an optimal solution, we have\n\u03bb (\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\nnuc \u2212 |||\u0398\u2217|||nuc\n) \u2264 L(\u0398\u2217)\u2212 L(\u0398\u0302) (a) \u2264 \u3008\u3008\u2206,\u2207L(\u0398\u2217)\u3009\u3009 (b)\n\u2264 |||\u2206|||nuc|||\u2207L(\u0398 \u2217)|||2 \u2264\n\u03bb 2 |||\u2206|||nuc,\n(34)\nwhere (a) holds due to the concavity of L; (b) follows from the Cauchy-Schwarz inequality; the last inequality holds due to the assumption that \u03bb \u2265 2|||\u2207L(\u0398\u2217)|||2. Combining (33) and (34) yields\n2 (\u2223\u2223\u2223\u2223\u2223\u2223\u2206\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u2223\nnuc \u2212 2|||PT\u22a5(\u0398\u2217)|||nuc \u2212 \u2223\u2223\u2223\u2223\u2223\u2223\u2206\u2032\u2223\u2223\u2223\u2223\u2223\u2223 nuc ) \u2264 |||\u2206|||nuc \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2206\u2032\u2223\u2223\u2223\u2223\u2223\u2223 nuc + \u2223\u2223\u2223\u2223\u2223\u2223\u2206\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u2223 nuc .\nThus |||\u2206\u2032\u2032|||nuc \u2264 3|||\u2206\u2032|||nuc + 4|||PT\u22a5(\u0398\u2217)|||nuc. By triangle inequality,\n|||\u2206|||nuc \u2264 4 \u2223\u2223\u2223\u2223\u2223\u2223\u2206\u2032\u2223\u2223\u2223\u2223\u2223\u2223 nuc + 4|||PT\u22a5(\u0398\u2217)|||nuc .\nNotice that \u2206\u2032 = UrU T r \u2206 + (I \u2212 UrUTr )\u2206VrV Tr . Both UrUTr \u2206 and (I \u2212 UrUTr )\u2206VrV Tr have rank\nat most r. Thus \u2206\u2032 has rank at most 2r. Hence, |||\u2206\u2032|||nuc \u2264 \u221a 2r|||\u2206\u2032|||F \u2264 \u221a\n2r|||\u2206|||F. Then the theorem follows because |||PT\u22a5(\u0398\u2217)|||nuc = \u2211min{d1,d2} j=r+1 \u03c3j(\u0398 \u2217)."}, {"heading": "A.2 Proof of Lemma A.2", "text": "Define Xi = \u2212ei \u2211k\n`=1(evi,` \u2212 pi,`)T such that \u2207L(\u0398\u2217) = 1 k d1 \u2211d1 i=1Xi, which is a sum of d1 inde-\npendent random matrices. Note that since pi,` has (k + 1 \u2212 `) non-zero entries, each bounded in absolute value by e2\u03b1/(k + 1\u2212 `), we have the following bound deterministically:\n|||Xi|||2 = \u2225\u2225\u2225 k\u2211 `=1 ( evi,` \u2212 pi,` )\u2225\u2225\u2225 \u2264 \u221a k +\nk\u2211 `=1 \u2225\u2225\u2225pi,`\u2225\u2225\u2225 \u2264 \u221a k + e2\u03b1\nk\u2211 `=1 1\u221a k + 1\u2212 `\n\u2264 \u221a k + e2\u03b12( \u221a k + 1\u2212 1) \u2264 3e2\u03b1 \u221a k ,\nand \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2211 i E [ XiX T i ]\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2 \u2264 9e4\u03b1k \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 d1\u2211 i=1 E [ eie T i ]\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2 = 9e4\u03b1k|||Id1\u00d7d1 |||2 = 9e 4\u03b1k,\nand\nd1\u2211 i=1 E [ XTi Xi ] = d1\u2211 i=1 k\u2211 `,`\u2032=1 E [ (ei,` \u2212 pi,`)(evi,`\u2032 \u2212 pi,`\u2032) T ]\n= d1\u2211 i=1 k\u2211 `=1 E [ (ei,` \u2212 pi,`)(evi,` \u2212 pi,`) T ]\n= d1\u2211 i=1 k\u2211 `=1 ( E [ evi,`e T vi,` ] \u2212 E [ pi,`p T i,` ])\nd1\u2211 i=1 k\u2211 `=1 E [ evi,`e T vi,` ]\n= d1\u2211 i=1 k d2 Id2\u00d7d2 .\nTherefore, \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 d1\u2211 i=1 E [ XTi Xi ]\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2 \u2264 kd1 d2 .\nBy matrix Bernstein inequality [27],\nP ( |||\u2207L(\u0398\u2217)|||2 > t ) \u2264 (d1 + d2) exp ( \u2212k2 d21 t2/2 (d1k/min{d2, d1/(9e4\u03b1)}) + (3e2\u03b1k3/2d1t/3) ) ,\nwhich gives the desired tail probability of 2d\u2212c for the choice of\nt = max\n{\u221a 4(1 + c) log d\nk d1 min{d2, d1/(9e4\u03b1)} ,\n4(1 + c)e2\u03b1 log d\nk1/2 d1\n}\n=\n\u221a 4(1 + c) log d\nk1/2 d1 max\n{\u221a d1/d2 , e 2\u03b1 \u221a 4(1 + c) log d } ,\nwhere the last equality holds due to the assumption that log d \u2265 4(1 + c)/9."}, {"heading": "A.3 Proof of Lemma A.3", "text": "Recall that the Hessian matrix is a block-diagonal matrix with the i-th block H(i)(\u0398) given by (25). We use the following remark from [14] to bound the Hessian.\nRemark A.4. [14, Claim 1] Given \u03b8 \u2208 Rr, let p be the column probability vector with pi = e\u03b8i/(e\u03b81 + \u00b7 \u00b7 \u00b7+ e\u03b8\u03c1) for each i \u2208 [\u03c1] and for any positive integer \u03c1. If |\u03b8i| \u2264 \u03b1, for all i \u2208 [\u03c1], then\ne2\u03b1 ( diag(p)\u2212 ppT ) 1\n\u03c1 diag(1)\u2212 1 \u03c12 11T .\nBy letting 1Si,` = \u2211 j\u2208Si,` ej and applying the above claim, we have\ne2\u03b1H(i)(\u0398) 1 k d1 k\u2211 `=1 ( 1 k \u2212 `+ 1 diag(1Si,`)\u2212\n1\n(k \u2212 `+ 1)2 1Si,`1\nT Si,`\n)\n= 1\n2 k d1 k\u2211 `=1\n1 (k \u2212 `+ 1)2 \u2211\nj,j\u2032\u2208Si,`\n(ej \u2212 ej\u2032)(ej \u2212 ej\u2032)T\n1 2 k3 d1 k\u2211 `=1 \u2211 j,j\u2032\u2208Si,` (ej \u2212 ej\u2032)(ej \u2212 ej\u2032)T .\nHence,\nVec(\u2206)\u22072L(\u0398)VecT (\u2206) = d1\u2211 i=1 (\u2206T ei) TH(i)(\u0398)(\u2206T ei)\n\u2265 e \u22122\u03b1\n2 k3 d1 d1\u2211 i=1 k\u2211 `=1 \u2211 j,j\u2032\u2208Si,` \u2223\u2223\u2223\u2223\u2223\u2223eTi \u2206(ej \u2212 ej\u2032)\u2223\u2223\u2223\u2223\u2223\u222322. By changing the order of the summation, we get that\nk\u2211 `=1 \u2211 j,j\u2032\u2208Si,` \u2223\u2223\u2223\u2223\u2223\u2223eTi \u2206(ej \u2212 ej\u2032)\u2223\u2223\u2223\u2223\u2223\u222322 = k\u2211 `,`\u2032=1 \u3008\u3008\u2206, ei,ji,` \u2212 ei,ji,`\u2032 \u3009\u3009 2 k\u2211 `\u2032\u2032=1 I ( \u03c3i(ji,`\u2032\u2032 ) \u2264 min{\u03c3i(ji,`), \u03c3i(ji,`\u2032)} ) .\nDefine\n\u03c7i,`,`\u2032,`\u2032\u2032 \u2261 I ( \u03c3i(ji,`\u2032\u2032 ) \u2264 min{\u03c3i(ji,`), \u03c3i(ji,`\u2032)} ) , (35)\nand let\nH(\u2206) \u2261 e \u22122\u03b1\n2 k3 d1 d1\u2211 i=1 k\u2211 `,`\u2032=1 \u3008\u3008\u2206, ei,ji,` \u2212 ei,ji,`\u2032 \u3009\u3009 2 k\u2211 `\u2032\u2032=1 \u03c7i,`,`\u2032,`\u2032\u2032 .\nThen we have VecT (\u2206)\u22072L(\u0398)Vec(\u2206) \u2265 H(\u2206). To prove the theorem, it suffices to bound H(\u2206) from the below. First, we prove a lower bound on the expectation E[H(\u2206)]. Notice that for ` 6= `\u2032, the conditional expectation of \u03c7i,`,`\u2032,`\u2032\u2032 \u2019s, given the set of alternatives presented to user i is\nE [ k\u2211 `\u2032\u2032=1 \u03c7i,`,`\u2032,`\u2032\u2032 \u2223\u2223 ji,1, . . . , ji,k] = 1 + \u2211 `\u2032\u2032 6=`,`\u2032\nexp(\u03b8i,ji,`\u2032\u2032 )\nexp(\u03b8i,ji,`\u2032\u2032 ) + exp(\u03b8i,ji,`\u2032 ) + exp(\u03b8i,ji,`)\n\u2265 1 + k \u2212 2 1 + 2e2\u03b1 \u2265 k 3e2\u03b1 .\nThen,\nE[H(\u2206)] = e\u22122\u03b1\n2 k3 d1 \u2211 i,`,`\u2032 E [ \u3008\u3008\u2206, ei,ji,` \u2212 ei,ji,`\u2032 \u3009\u3009 2E [ k\u2211 `\u2032\u2032=1 \u03c7i,`,`\u2032,`\u2032\u2032 \u2223\u2223 ji,1, . . . , ji,k]]\n\u2265 e \u22124\u03b1\n6 k2 d1 d1\u2211 i=1 \u2211 `,`\u2032\u2208[k] E [ \u3008\u3008\u2206, ei,ji,` \u2212 ei,ji,`\u2032 \u3009\u3009 2 ]\n= e\u22124\u03b1\n6 k2 d1 d1\u2211 i=1 \u2211 6\u0300=`\u2032\u2208[k]  2 d2 d2\u2211 j=1 \u22062ij \u2212 2 d22 d2\u2211 j,j\u2032=1 \u2206ij\u2206ij\u2032  =\ne\u22124\u03b1(k \u2212 1) 3 k d1 d2 |||\u2206|||2F , (36)\nwhere the last equality holds because \u2211\nj\u2208[d2] \u2206ij = 0 for \u2206 \u2208 \u21262\u03b1 and for all i \u2208 [d1]. We are left to prove that H(\u2206) cannot deviate from its mean too much. Suppose there exists a \u2206 \u2208 A such that Eq. (28) is violated, i.e. H(\u2206) < (e\u22124\u03b1/(24 d1d2))|||\u2206|||2F. We will show this happens with a small probability. From Eq. (36), we get that for k \u2265 24,\nE[H(\u2206)]\u2212H(\u2206) \u2265 (7k \u2212 8) 24k\ne\u22124\u03b1 d1 d2 |||\u2206|||2F\n\u2265 (20/3) e \u22124\u03b1\n24 d1d2 |||\u2206|||2F . (37)\nWe use a peeling argument as in [24, Lemma 3], [28] to upper bound the probability that Eq. (37) is true. We first construct the following family of subsets to cover A such that A \u2286 \u22c3\u221e `=1 S`.\nRecall \u00b5 = 210e2\u03b1\u03b1d2 \u221a\n(d1 log d)/(kmin{d1, d2}), define in (30). Notice that since for any \u2206 \u2208 A, |||\u2206|||2F \u2265 \u00b5|||\u2206|||nuc \u2265 \u00b5|||\u2206|||F, it follows that |||\u2206|||F \u2265 \u00b5. Then, we can cover A with the family of sets\nS` = { \u2206 \u2208 Rd1\u00d7d2 \u2223\u2223\u2223 |||\u2206|||\u221e \u2264 2\u03b1 , \u03b2`\u22121\u00b5 \u2264 |||\u2206|||F \u2264 \u03b2`\u00b5 , \u2211\nj\u2208[d2]\n\u2206ij = 0 for all i \u2208 [d1], and |||\u2206|||nuc \u2264 \u03b2 2`\u00b5 } ,\nwhere \u03b2 = \u221a\n10/9 and for ` \u2208 {1, 2, 3, . . .}. This implies that when there exists a \u2206 \u2208 A such that (37) holds, then there exists an ` \u2208 Z+ such that \u2206 \u2208 S` and\nE[H(\u2206)]\u2212H(\u2206) \u2265 (20/3) e \u22124\u03b1\n24 d1d2 \u03b22(`\u22121)\u00b52\n\u2265 e \u22124\u03b1\n4 d1d2 \u03b22`\u00b52 . (38)\nApplying the union bound over ` \u2208 Z+, we get from (37) and (38) that\nP { \u2203\u2206 \u2208 A , H(\u2206) < e \u22124\u03b1\n24 d1d2 |||\u2206|||2F\n} \u2264 \u221e\u2211 `=1 P { sup \u2206\u2208S` ( E[H(\u2206)]\u2212H(\u2206) ) > e\u22124\u03b1 4 d1d2 (\u03b2`\u00b5)2 }\n\u2264 \u221e\u2211 `=1 P\n{ sup\n\u2206\u2208B(\u03b2`\u00b5)\n( E[H(\u2206)]\u2212H(\u2206) ) > e\u22124\u03b1\n4 d1d2 (\u03b2`\u00b5)2\n} , (39)\nwhere we define a new set B(D) such that S` \u2286 B(\u03b2`\u00b5):\nB(D) = { \u2206 \u2208 Rd1\u00d7d2 \u2223\u2223 \u2016\u2206\u2016\u221e \u2264 2\u03b1, |||\u2206|||F \u2264 D, \u2211\nj\u2208[d2]\n\u2206ij = 0 for all i \u2208 [d1], \u00b5|||\u2206|||nuc \u2264 D 2 } .\n(40)\nThe following key lemma provides the upper bound on this probability.\nLemma A.5. For (16 min{d1, d2} log d)/(3d1) \u2264 k \u2264 d21 log d,\nP { sup\n\u2206\u2208B(D)\n( E[H(\u2206)]\u2212H(\u2206) ) \u2265 e \u22124\u03b1\n4d1d2 D2\n} \u2264 exp { \u2212 e \u22124\u03b1 kD4\n219\u03b14d1d22\n} . (41)\nLet \u03b7 = exp ( \u2212 e \u22124\u03b14k(\u03b2\u22121.002)\u00b54\n219\u03b14d1d22\n) . Applying the tail bound to (39), we get\nP { \u2203\u2206 \u2208 A , H(\u2206) < e \u22124\u03b1\n24 d1d2 |||\u2206|||2F\n} \u2264 \u221e\u2211 `=1 exp { \u2212 e \u22124\u03b1k(\u03b2`\u00b5)4 219\u03b14d1d22 } (a)\n\u2264 \u221e\u2211 `=1 exp { \u2212 e \u22124\u03b14k`(\u03b2 \u2212 1.002)\u00b54 219\u03b14d1d22 } \u2264 \u03b7\n1\u2212 \u03b7 ,\nwhere (a) holds because \u03b2x \u2265 x log \u03b2 \u2265 x(\u03b2\u22121.002) for the choice of \u03b2 = \u221a\n10/9. By the definition of \u00b5,\n\u03b7 = exp { \u2212 2 23 e4\u03b1d22d1(log d) 2(\u03b2 \u2212 1.002) k(min{d1, d2})2 } \u2264 exp{\u2212 218 log d} ,\nwhere the last inequality follows from the assumption that k \u2264 max{d1, d22/d1} log d = (d22d1 log d)/(min{d1, d2})2, and \u03b2 \u2212 1.002 \u2265 2\u22125. Since for d \u2265 2, exp{\u2212218 log d} \u2264 1/2 and thus \u03b7 \u2264 1/2, the lemma follows by assembling the last two displayed inequalities."}, {"heading": "A.4 Proof of Lemma A.5", "text": "Recall that\nH(\u2206) = e\u22122\u03b1\n2 k3 d1 d1\u2211 i=1 k\u2211 `,`\u2032=1 \u3008\u3008\u2206, ei,ji,` \u2212 ei,ji,`\u2032 \u3009\u3009 2 k\u2211 `\u2032\u2032=1 \u03c7i,`,`\u2032,`\u2032\u2032 ,\nwith \u03c7i,`,`\u2032,`\u2032\u2032 = I ( \u03c3i(ji,`\u2032\u2032 ) \u2264 min{\u03c3i(ji,`), \u03c3i(ji,`\u2032)} ) . Let Z = sup\u2206\u2208B(D) E[H(\u2206)] \u2212H(\u2206) be the worst-case random deviation of H(\u2206) form its mean. We prove an upper bound on Z by showing that Z \u2212 E[Z] \u2264 e\u22124\u03b1D2/(64d1d2) with high probability, and E[Z] \u2264 9e\u22124\u03b1D2/(40d1d2). This proves the desired claim in Lemma A.5.\nTo prove the concentration of Z, we utilize the random utility model (RUM) theoretic interpretation of the MNL model. The random variable Z depends on the random choice of alternatives\n{ji,`}i\u2208[d1],`\u2208[k] and the random k-wise ranking outcomes {\u03c3i}i\u2208[d1]. The random utility theory, pioneered by [5, 6, 7], tells us that the k-wise ranking from the MNL model has the same distribution as first drawing independent (unobserved) utilities ui,`\u2019s of the item ji,` for user i according to the standard Gumbel Cumulative Distribution Function (CDF) F (c \u2212 \u0398i,ji,`) with F (c) = e\u2212e \u2212c\n, and then ranking the k items for user i according to their respective utilities. Given this definition of the MNL model, we have \u03c7i,`,`\u2032,`\u2032\u2032 = I ( ui,`\u2032\u2032 \u2265 max{ui,`, ui,`\u2032} ) . Thus Z is a function of independent choices of the items and their (unobserved) utilities, i.e. Z = f({(ji,`, ui,`)}i\u2208[d1],`\u2208[k]). Let xi,` = (ji,`, ui,`) and write H(\u2206) as H(\u2206, {xi,`}i\u2208[d1],`\u2208[k]). This allows us to bound the difference and apply McDiarmid\u2019s tail bound. Note that for any i \u2208 [d1], ` \u2208 [k], x1,1, . . . , xd1,k, and x\u2032i,`,\u2223\u2223 f(x1,1, . . . , xi,`, . . . , xd1,k )\u2212 f(x1,1, . . . , x\u2032i,`, . . . , xd1,k ) \u2223\u2223 = \u2223\u2223 sup\n\u2206\u2208B(D) (E [H(\u2206)]\u2212H(\u2206, x1,1, . . . , xi,`, . . . , xd1,k))\u2212 sup \u2206\u2208B(D)\n( E [H(\u2206)]\u2212H(\u2206, x1,1, . . . , x\u2032i,`, . . . , xd1,k) ) \u2223\u2223 \u2264 sup\n\u2206\u2208B(D) \u2223\u2223H(\u2206, x1,1, . . . , xi,`, . . . , xd1,k)\u2212H(\u2206, x1,1, . . . , x\u2032i,`, . . . , xd1,k)\u2223\u2223 (a)\n\u2264 e \u22122\u03b1\n2 k3 d1 sup \u2206\u2208B(D) { 2 \u2211 `\u2032\u2208[k] \u3008\u3008\u2206, ei,ji,` \u2212 ei,ji,`\u2032 \u3009\u3009 2 k\u2211 `\u2032\u2032=1 \u03c7i,`,`\u2032,`\u2032\u2032 + \u2211 `\u2032,`\u2032\u2032\u2208[k] \u3008\u3008\u2206, ei,ji,`\u2032 \u2212 ei,ji,`\u2032\u2032 \u3009\u3009 2\u03c7i,`\u2032,`\u2032\u2032,` } (b)\n\u2264 8\u03b1 2e\u22122\u03b1\nk3 d1\n{ 2 \u2211 `\u2032\u2208[k]\\{`} k\u2211 `\u2032\u2032=1 \u03c7i,`,`\u2032,`\u2032\u2032 + \u2211 `\u2032,`\u2032\u2032\u2208[k],`\u2032 6=`\u2032\u2032, \u03c7i,`\u2032,`\u2032\u2032,` } \u2264 16\u03b1 2e\u22122\u03b1\nk d1 ,\nwhere (a) follows because for a fixed i and `, the random variable xi,` = (ji,`, ui,`) can appear in three terms, i.e. \u2211 `\u2032,`\u2032\u2032\u3008\u3008\u2206, ei,ji,` \u2212 ei,ji,`\u2032 \u3009\u3009 2\u03c7i,`,`\u2032,`\u2032\u2032 + \u2211\n`\u2032,`\u2032\u2032\u3008\u3008\u2206, ei,ji,`\u2032 \u2212 ei,ji,`\u3009\u3009 2\u03c7i,`\u2032,`,`\u2032\u2032 +\u2211\n`\u2032,`\u2032\u2032\u3008\u3008\u2206, ei,ji,`\u2032 \u2212 ei,ji,`\u2032\u2032 \u3009\u3009 2\u03c7i,`\u2032,`\u2032\u2032,`, and (b) follows because |\u2206ij | \u2264 2\u03b1 for all i, j since \u2206 \u2208 B(D). The last inequality follows because in the worst case, \u2211 `\u2032\u2208[k]\\{`} \u2211k\n`\u2032\u2032=1 \u03c7i,`,`\u2032,`\u2032\u2032 \u2264 k(k \u2212 1)/2 and\u2211 `\u2032,`\u2032\u2032\u2208[k],`\u2032 6=`\u2032\u2032 \u03c7i,`\u2032,`\u2032\u2032,` \u2264 k(k \u2212 1). This holds with equality if \u03c3i(ji,`) = k and \u03c3i(ji,`) = 1, respectively. By bounded differences inequality, we have\nP {Z \u2212 E [Z] \u2265 t} \u2264 exp ( \u2212 k 2 d21 t 2\n27 \u03b14e\u22124\u03b1d1k\n) ,\nIt follows that for the choice of t = e\u22124\u03b1D2/(64d1d2),\nP { Z \u2212 E [Z] \u2265 e \u22124\u03b1D2\n64d1d2\n} \u2264 exp ( \u2212 e \u22124\u03b1kD4\n219\u03b14d1d22\n) .\nWe are left to prove the upper bound on E[Z] using symmetrization and contraction. Define random variables\nYi,`,`\u2032,`\u2032\u2032(\u2206) \u2261 (\u2206i,ji,` \u2212\u2206i,ji,`\u2032 ) 2\u03c7i,`,`\u2032,`\u2032\u2032 , (42)\nwhere the randomness is in the choice of alternatives ji,`, ji,`\u2032 , and ji,`\u2032\u2032 , and the outcome of the comparisons of those three alternatives.\nThe main challenge in applying the symmetrization to \u2211\n`,`\u2032,`\u2032\u2032\u2208[k] Yi,`,`\u2032,`\u2032\u2032(\u2206) is that we need to partition the summation over the set [k]\u00d7[k]\u00d7[k] into subsets of independent random variables, such that we can apply the standard symmetrization argument. to this end, we prove in the following lemma a a generalization of the well-known problem of scheduling a round robin tournament to a tournament of matches involving three teams each. No teams are present in more than one triple in a single round, and we want to minimize the number of rounds to cover all combination of triples are matched. For example, when there are k = 6 teams, there is a simple construction of such a tournament: T1 = {(1, 2, 3), (4, 5, 6)}, T2 = {1, 2, 4), (3, 5, 6)}, T3 = {(1, 2, 5), (3, 4, 6)}, T4 = {(1, 2, 6), (3, 4, 5)}, T5 = {(1, 3, 4), (2, 5, 6)}, T6 = {(1, 3, 5), (2, 4, 6)}, T7 = {(1, 3, 6), (2, 4, 5)}, T8 = {(1, 4, 5), (2, 3, 6)}, T9 = {(1, 4, 6), (2, 3, 5)}, T10 = {(1, 5, 6), (2, 3, 4)}. This is a perfect scheduling of a tournament with three teams in each match. For a general k, the following lemma provides a construction with O(k2) rounds.\nLemma A.6. There exists a partition (T1, . . . , TN ) of [k]\u00d7 [k]\u00d7 [k] for some N \u2264 24k2 such that Ta\u2019s are disjoint subsets of [k] \u00d7 [k] \u00d7 [k], \u22c3 a\u2208[N ] Ta = [k] \u00d7 [k] \u00d7 [k], |Ta| \u2264 bk/3c and for any a \u2208 [N ] the set of random variables in Ta satisfy\n{Yi,`,`\u2032,`\u2032\u2032}i\u2208[d1],(`,`\u2032,`\u2032\u2032)\u2208Ta are mutually independent .\nNow, we are ready to partition the summation.\nE [ Z ] = e\u22122\u03b1 2 k3 d1 E [ sup \u2206\u2208B(D) \u2211 i\u2208[d1] \u2211 `,`\u2032,`\u2032\u2032\u2208[k] { E[Yi,`,`\u2032,`\u2032\u2032(\u2206)]\u2212 Yi,`,`\u2032,`\u2032\u2032(\u2206) }] = e\u22122\u03b1\n2 k3 d1 E [ sup \u2206\u2208B(D) \u2211 i\u2208[d1] \u2211 a\u2208[N ] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta { E[Yi,`,`\u2032,`\u2032\u2032(\u2206)]\u2212 Yi,`,`\u2032,`\u2032\u2032(\u2206) }] \u2264 e \u22122\u03b1\n2 k3 d1 \u2211 a\u2208[N ] E [ sup \u2206\u2208B(D) \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta { E[Yi,`,`\u2032,`\u2032\u2032(\u2206)]\u2212 Yi,`,`\u2032,`\u2032\u2032(\u2206) }] \u2264 e \u22122\u03b1\nk3 d1 \u2211 a\u2208[N ] E [ sup \u2206\u2208B(D) \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta \u03bei,`,`\u2032,`\u2032\u2032Yi,`,`\u2032,`\u2032\u2032(\u2206) ]\n= e\u22122\u03b1\nk3 d1 \u2211 a\u2208[N ] E [ sup \u2206\u2208B(D) \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta \u03bei,`,`\u2032,`\u2032\u2032(\u2206i,ji,` \u2212\u2206i,ji,`\u2032 ) 2\u03c7i,`,`\u2032,`\u2032\u2032 ] , (43)\nwhere the first inequality follows from the fact that sum of the supremum if no less than the supremum of the sum, and the second inequality follows from standard symmetrization argument applied to independent random variables {Yi,`,`\u2032,`\u2032\u2032(\u2206)}i\u2208[d1],(`,`\u2032,`\u2032\u2032)\u2208Ta with i.i.d. Rademacher random variables \u03bei,`,`\u2032,`\u2032\u2032 \u2019s. Since (\u2206i,ji,` \u2212\u2206i,ji,`\u2032 )\n2\u03c7i,`,`\u2032,`\u2032\u2032 \u2264 4\u03b1|\u2206i,ji,` \u2212\u2206i,ji,`\u2032 |\u03c7i,`,`\u2032,`\u2032\u2032 , we have by the Ledoux-Talagrand contraction inequality that\nE [\nsup \u2206\u2208B(D) \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta \u03bei,`,`\u2032,`\u2032\u2032(\u2206i,ji,` \u2212\u2206i,ji,`\u2032 ) 2\u03c7i,`,`\u2032,`\u2032\u2032 ] \u2264 8\u03b1E [ sup\n\u2206\u2208B(D) \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta \u03bei,`,`\u2032,`\u2032\u2032 \u03c7i,`,`\u2032,`\u2032\u2032 \u3008\u3008\u2206, ei(eji,` \u2212 eji,`\u2032 ) T \u3009\u3009 ]\n(44)\nApplying Ho\u0308lder\u2019s inequality, we get that\u2223\u2223\u2223 \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta \u03bei,`,`\u2032,`\u2032\u2032 \u03c7i,`,`\u2032,`\u2032\u2032 \u3008\u3008\u2206, ei(eji,` \u2212 eji,`\u2032 ) T \u3009\u3009 \u2223\u2223\u2223\n\u2264 |||\u2206|||nuc \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta \u03bei,`,`\u2032,`\u2032\u2032 \u03c7i,`,`\u2032,`\u2032\u2032 ( ei(eji,` \u2212 eji,`\u2032 ) T )\u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2 . (45)\nWe are left to prove that the expected value of the right-hand side of the above inequality is bounded by C|||\u2206|||nuc \u221a kd1 log d/min{d1, d2} for some numerical constant C. For i \u2208 [d1] and\n(`, `\u2032, `\u2032\u2032) \u2208 Ta, let Wi,`,`\u2032,`\u2032\u2032 = \u03bei,`,`\u2032,`\u2032\u2032 \u03c7i,`,`\u2032,`\u2032\u2032 ( ei(eji,` \u2212 eji,`\u2032 ) T )\nbe independent zero-mean random matrices, such that\u2223\u2223\u2223\u2223\u2223\u2223Wi,`,`\u2032,`\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u22232 = \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u03bei,`,`\u2032,`\u2032\u2032 \u03c7i,`,`\u2032,`\u2032\u2032 (ei(eji,` \u2212 eji,`\u2032 )T )\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 \u2264 \u221a2 , almost surely, and\nE[Wi,`,`\u2032,`\u2032\u2032W Ti,`,`\u2032,`\u2032\u2032 ] = E[ ( ei(eji,` \u2212 eji,`\u2032 ) T (eji,` \u2212 eji,`\u2032 )e T i ) \u03c7i,`,`\u2032,`\u2032\u2032 ]\n= 2E [ \u03c7i,`,`\u2032,`\u2032\u2032 ] eie T i\n2eieTi ,\nand\nE[W Ti,`,`\u2032,`\u2032\u2032Wi,`,`\u2032,`\u2032\u2032 ] = E[ ( (eji,` \u2212 eji,`\u2032 )e T i ei(eji,` \u2212 eji,`\u2032 ) T ) \u03c7i,`,`\u2032,`\u2032\u2032 ]\nE[(eji,` \u2212 eji,`\u2032 )e T i ei(eji,` \u2212 eji,`\u2032 ) T ]\n= 2\nd2 Id2\u00d7d2 \u2212\n2\nd22 11T .\nThis gives\n\u03c32 = max  \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta E[Wi,`,`\u2032,`\u2032\u2032W Ti,`,`\u2032,`\u2032\u2032 ] \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2 , \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta E[W Ti,`,`\u2032,`\u2032\u2032Wi,`,`\u2032,`\u2032\u2032 ] \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2  \u2264 max { 2|Ta| ,\n2d1|Ta| d2\n} =\n2d1|Ta| min{d1, d2} \u2264 2d1k 3 min{d1, d2} ,\nsince we have designed Ta\u2019s such that |Ta| \u2264 k/3. Applying matrix Bernstein inequality [27] yields the tail bound\nP  \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta Wi,`,`\u2032,`\u2032\u2032 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2 \u2265 t  \u2264 (d1 + d2) exp( \u2212t2/2\u03c32 +\u221a2t/3 ) .\nChoosing t = max {\u221a 32kd1 log d/(3 min{d1, d2}), (16 \u221a 2/3) log d }\n, we obtain with probability at least 1\u2212 2d\u22123,\u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta Wi,`,`\u2032,`\u2032\u2032 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 max {\u221a 32kd1 log d 3 min{d1, d2} , 16 \u221a 2 log d 3 } .\nIt follows from the fact \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211i\u2208[d1]\u2211(`,`\u2032,`\u2032\u2032)\u2208TaWi,`,`\u2032,`\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 \u2264\u2211i,(`,`\u2032,`\u2032\u2032) \u2223\u2223\u2223\u2223\u2223\u2223Wi,`,`\u2032,`\u2032\u2032\u2223\u2223\u2223\u2223\u2223\u22232 \u2264 \u221a2d1k/3 that\nE \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208[d1] \u2211 (`,`\u2032,`\u2032\u2032)\u2208Ta Wi,`,`\u2032,`\u2032\u2032 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2  \u2264 max{\u221a 32kd1 log d 3 min{d1, d2} , 16 \u221a 2 log d 3 } + 2 \u221a 2d1k 3d3\n\u2264 2\n\u221a 32kd1 log d\n3 min{d1, d2} ,\nwhere the last inequality follows from the assumption that (16 min{d1, d2} log d)/(3d1) \u2264 k \u2264 d21 log d. Substituting this in the RHS of Eq. (45), and then together with Eqs. (44) and (43), this gives the following desired bound:\nE[Z] \u2264 \u2211 a\u2208[N ] sup \u2206\u2208B(D) 16\u03b1e\u22122\u03b1 k3 d1\n\u221a 32kd1 log d\n3 min{d1, d2} |||\u2206|||nuc\n\u2264 \u2211 a\u2208[N ]\ne\u22124\u03b1 \u221a 2\n16 \u221a 3k2 d1 d2\n( 210e2\u03b1\u03b1d2\n\u221a d1 log d\nkmin{d1, d2} ) \ufe38 \ufe37\ufe37 \ufe38\n=\u00b5\n|||\u2206|||nuc\n\u2264 9e \u22124\u03b1D2\n40d1d2 ,\nwhere the last inequality holds because N \u2264 4k2 and \u00b5|||\u2206|||nuc \u2264 D2."}, {"heading": "A.5 Proof of Lemma A.6", "text": "Recall that Yi,`,`\u2032,`\u2032\u2032(\u2206) = (\u2206i,ji,` \u2212 \u2206i,ji,`\u2032 ) 2\u03c7i,`,`\u2032,`\u2032\u2032 , as defined in (42). From the random utility model (RUM) interpretation of the MNL model presented in Section 1, it is not difficult to show that Yi,`,`\u2032,`\u2032\u2032 and Yi,\u02dc\u0300,\u02dc\u0300\u2032,\u02dc\u0300\u2032\u2032 are mutually independent if the two triples (`, `\n\u2032, `\u2032\u2032) and (\u02dc\u0300, \u02dc\u0300\u2032, \u02dc\u0300\u2032\u2032) do not overlap, i.e., no index is present in both triples.\nNow, borrowing the terminologies from round robin tournaments, we construct a schedule for a tournament with k teams where each match involve three teams. Let Ta,b denote a set of triples playing at the same round, indexed by two integers a \u2208 {3, . . . , 2k \u2212 3} and b \u2208 {5, . . . , 2k \u2212 1}. Hence, there are total N = (2k \u2212 5)2 rounds.\nEach round (a, b) consists of disjoint triples and is defined as Ta,b \u2261 { (`, `\u2032, `\u2032\u2032) \u2208 [k]\u00d7 [k]\u00d7 [k] | ` < `\u2032 < `\u2032\u2032, `+ `\u2032 = a, and `\u2032 + `\u2032\u2032 = b } .\nWe need to prove that (a) there is no missing triple; and (b) no team plays twice in a single round. First, for any ordered triple (`, `\u2032, `\u2032\u2032), there exists a \u2208 {3, . . . , 2k\u22123} and b \u2208 {5, . . . , 2k\u22121} such that `+ `\u2032 = a and `\u2032 + `\u2032\u2032 = b. This proves that all ordered triples are covered by the above construction. Next, given a pair (a, b), no two triples in Ta,b can share the same team. Suppose there exists two distinct ordered triples (`, `\u2032, `\u2032\u2032) and (\u02dc\u0300, \u02dc\u0300\u2032, \u02dc\u0300\u2032\u2032) both in Ta,b, and one of the triples are shared. Then, from the two equations ` + `\u2032 = \u02dc\u0300+ \u02dc\u0300\u2032 = a and `\u2032 + `\u2032\u2032 = \u02dc\u0300\u2032 + \u02dc\u0300\u2032\u2032 = b, it follows that all three indices must be the same, which is a contradiction. This proves the desired claim for ordered triples.\nOne caveat is that we wanted to cover the whole [k]\u00d7 [k]\u00d7 [k], and not just the ordered triples. In the above construction, for example, a triple (3, 2, 1) does not appear. This can be resolved by simply taking all Ta,b\u2019s from the above construction, and make 6 copies of each round, and permuting all the triples in each copy according to the same permutation over {1, 2, 3}. This increases the total rounds to N = 6(2k \u2212 5)2 \u2264 24k2. Note that |Ta,b| \u2264 bk/3c since no item can be in more than one triple."}, {"heading": "B Proof of estimating approximate low-rank matrices in Corollary", "text": "3.2\nWe follow closely the proof of a similar corollary in [24]. First fix a threshold \u03c4 > 0, and set r = max{j|\u03c3j(\u0398\u2217) > \u03c4}. With this choice of r, we have\nmin{d1,d2}\u2211 j=r+1 \u03c3j(\u0398 \u2217) = \u03c4 min{d1,d2}\u2211 j=r+1 \u03c3j(\u0398 \u2217) \u03c4 \u2264 \u03c4 min{d1,d2}\u2211 j=r+1 (\u03c3j(\u0398\u2217) \u03c4 )q \u2264 \u03c41\u2212q\u03c1q .\nAlso, since r\u03c4 q \u2264 \u2211r\nj=1 \u03c3j(\u0398 \u2217)q \u2264 \u03c1q, it follows that \u221a r \u2264 \u221a\u03c1a\u03c4\u2212q/2. Using these bounds, Eq. (8)\nis now \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 F \u2264 288 \u221a 2c0e\n4\u03b1d1d2\u03bb0\ufe38 \ufe37\ufe37 \ufe38 =A\n(\u221a \u03c1q\u03c4 \u2212q/2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F + \u03c41\u2212q\u03c1q ) .\nWith the choice of \u03c4 = A, it follows after some algebra that\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F \u2264 2\u221a\u03c1qA(2\u2212q)/2 ."}, {"heading": "C Proof of the information-theoretic lower bound in Theorem 2", "text": "The proof uses information-theoretic methods which reduces the estimation problem to a multiway hypothesis testing problem. to prove a lower bound on the expected error, it suffices to prove\nsup \u0398\u2217\u2208\u2126\u03b1\nP {\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232\nF \u2265 \u03b4\n2\n4\n} \u2265 1\n2 . (46)\nTo prove the above claim, we follow the standard recipe of constructing a packing in \u2126\u03b1. Consider a family {\u0398(1), . . . ,\u0398(M(\u03b4)} of d1\u00d7d2 dimensional matrices contained in \u2126\u03b1 satisfying \u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1) \u2212\u0398(`2)\u2223\u2223\u2223\u2223\u2223\u2223 F \u2265 \u03b4 for all `1, `2,\u2208 [M(\u03b4)]. We will use M to refer to M(\u03b4) for simplify the notation. Suppose we draw an index L \u2208 [M(\u03b4)] uniformly at random, and we are given direct observations \u03c3i as per MNL model with \u0398\u2217 = \u0398(L) on a randomly chosen set of k items Si for each user i \u2208 [d1]. It follows from triangular inequality that\nsup \u0398\u2217\u2208\u2126\u03b1\nP {\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232\nF \u2265 \u03b4\n2\n4\n} \u2265 P { L\u0302 6= L } , (47)\nwhere L\u0302 is the resulting best estimate of the multiway hypothesis testing on L. The generalized Fano\u2019s inequality gives\nP { L\u0302 6= L|S(1), . . . , S(d1) } \u2265 1\u2212 I(L\u0302;L) + log 2\nlogM (48) \u2265 1\u2212 ( M 2 )\u22121\u2211 `1,`2\u2208[M ]DKL(\u0398\n(`1)\u2016\u0398(`2)) + log 2 logM , (49)\nwhere DKL(\u0398 (`1)\u2016\u0398(`2)) denotes the Kullback-Leibler divergence between the distributions of the partial rankings P { \u03c31, . . . , \u03c3d1 |\u0398(`1), S(1), . . . , S(d1) } and P { \u03c31, . . . , \u03c3d1 |\u0398(`2), S(1), . . . , S(d1) } . The second inequality follows from a standard technique, which we repeat here for completeness. Let \u03a3 = {\u03c31, . . . , \u03c3d1} denote the observed outcome of comparisons. Since L\u2013\u0398(L)\u2013\u03a3\u2013L\u0302 form a Markov chain, the data processing inequality gives I(L\u0302;L) \u2264 I(\u03a3;L). For simplicity, we drop the conditioning on the set of alternatives {S(1), . . . , S(d1)}, and and let p(\u00b7) denotes joint, marginal, and conditional distribution of respective random variables. It follows that\nI(\u03a3;L) = \u2211\n`\u2208[M ],\u03a3\np(\u03a3|`) 1 M log p(`,\u03a3) p(`)p(\u03a3)\n= 1\nM \u2211 `\u2208[M ] \u2211 \u03a3 p(\u03a3|`) log p(\u03a3|`)1 M \u2211 `\u2032 p(\u03a3|`\u2032)\n\u2264 1 M2 \u2211 `,`\u2032\u2208[M ] \u2211 \u03a3 p(\u03a3|`) log p(\u03a3|`) p(\u03a3|`\u2032)\n= 1\nM2 \u2211 `,`\u2032\u2208[M ] DKL(\u0398 (`1)\u2016\u0398(`2)) , (50)\nwhere the first inequality follows from Jensen\u2019s inequality. To compute the KL-divergence, recall that from the RUM interpretation of the MNL model (see Section 1), one can generate sample rankings \u03a3 by drawing random variables with exponential distributions with mean e\u0398 \u2217 ij \u2019s. Precisely, let X(`) = [X (`) ij ]i\u2208[d1],j\u2208Si denote the set of random variables, where X (`) ij is drawn from the exponential distribution with mean e\u2212\u0398 (`) ij . The MNL ranking follows by ordering the alternatives in each Si according to this {X(`)ij }j\u2208Si by ranking the smaller ones on the top. This forms a Markov chain L\u2013X(L)\u2013\u03a3, and the standard data processing inequality gives\nDKL(\u0398 (`1)\u2016\u0398(`2)) \u2264 DKL(X(`1)\u2016X(`2)) (51) = \u2211 i\u2208[d1] \u2211 j\u2208Si { e\u0398 (`1) ij \u2212\u0398 (`2) ij \u2212 (\u0398(`1)ij \u2212\u0398 (`2) ij )\u2212 1 } (52)\n\u2264 e 2\u03b1\n4\u03b12 \u2211 i\u2208[d1] \u2211 j\u2208Si (\u0398 (`1) ij \u2212\u0398 (`2) ij ) 2 , (53)\nwhere the last inequality follows from the fact that ex\u2212x\u22121 \u2264 (e2\u03b1/(4\u03b12))x2 for any x \u2208 [\u22122\u03b1, 2\u03b1]. Taking expectation over the randomly chosen set of alternatives,\nES(1),...,S(d1)[DKL(\u0398 (`1)\u2016\u0398(`2))] \u2264 e\n2\u03b1 k\n4\u03b12 d2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1) \u2212\u0398(`2)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 F . (54)\nCombined with (49), we get that P { L\u0302 6= L } = ES(1),...,S(d1)[P { L\u0302 6= L|S(1), . . . , S(d1) } ] (55)\n\u2265 1\u2212 ( M 2 )\u22121\u2211 `1,`2\u2208[M ](e 2\u03b1k/(4\u03b12d2)) \u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1) \u2212\u0398(`2)\u2223\u2223\u2223\u2223\u2223\u22232 F + log 2\nlogM , (56)\nThe remainder of the proof relies on the following probabilistic packing.\nLemma C.1. Let d2 \u2265 d1 \u2265 607 be positive integers. Then for each r \u2208 {1, . . . , d1}, and for any positive \u03b4 > 0 there exists a family of d1 \u00d7 d2 dimensional matrices {\u0398(1), . . . ,\u0398(M(\u03b4))} with cardinality M(\u03b4) = b(1/4) exp(rd2/576)c such that each matrix is rank r and the following bounds hold: \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398(`)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\nF \u2264 \u03b4 , for all ` \u2208 [M ] (57)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1) \u2212\u0398(`2)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\nF \u2265 \u03b4 , for all `1, `2 \u2208 [M ] (58) \u0398(`) \u2208 \u2126\u03b1\u0303 , for all ` \u2208 [M ] , (59)\nwith \u03b1\u0303 = (8\u03b4/d2) \u221a 2 log d for d = (d1 + d2)/2.\nSuppose \u03b4 \u2264 \u03b1d2/(8 \u221a 2 log d) such that the matrices in the packing set are entry-wise bounded by \u03b1, then the above lemma implies that \u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1) \u2212\u0398(`2)\u2223\u2223\u2223\u2223\u2223\u22232\nF \u2264 4\u03b42, which gives\nP { L\u0302 6= L } \u2265 1\u2212 e2\u03b1k\u03b42 \u03b12d2 + log 2\nrd 576 \u2212 2 log 2\n\u2265 1 2 ,\nwhere the last inequality holds for \u03b42 \u2264 (\u03b12d2/(e2\u03b1k))((rd/1152)\u22122 log 2). If we assume rd \u2265 3195 for simplicity, this bound on \u03b4 can be simplified to \u03b4 \u2264 \u03b1e\u2212\u03b1 \u221a r d2 d/(2304 k). Together with (46) and (47), this proves that for all \u03b4 \u2264 min{\u03b1d2/(8 \u221a 2 log d), \u03b1e\u2212\u03b1 \u221a r d2 d/(2304 k)},\ninf \u0398\u0302 sup \u0398\u2217\u2208\u2126\u03b1\nE [ \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\nF\n] \u2265 \u03b4\n4 .\nChoosing \u03b4 appropriately to maximize the right-hand side finishes the proof of the desired claim."}, {"heading": "C.1 Proof of Lemma C.1", "text": "Following the construction in [24], we use probabilistic method to prove the existence of the desired family. We will show that the following procedure succeeds in producing the desired family with probability at least half, which proves its existence. Let d = (d1 + d2)/2, and suppose d2 \u2265 d1 without loss of generality. For the choice of M \u2032 = erd2/576, and for each ` \u2208 [M \u2032], generate a rank-r matrix \u0398(`) \u2208 Rd1\u00d7d2 as follows:\n\u0398(`) = \u03b4\u221a rd2\nU(V (`))T ( Id2\u00d7d2 \u2212 1\nd2 11T\n) , (60)\nwhere U \u2208 Rd1\u00d7r is a random orthogonal basis such that UTU = Ir\u00d7r and V (`) \u2208 Rd2\u00d7r is a random matrix with each entry V\n(`) ij \u2208 {\u22121,+1} chosen independently and uniformly at random.\nBy construction, notice that \u2223\u2223\u2223\u2223\u2223\u2223\u0398(`)\u2223\u2223\u2223\u2223\u2223\u2223\nF = (\u03b4/\n\u221a rd2) \u2223\u2223\u2223\u2223\u2223\u2223(V (`))T (I\u2212 (1/d2)11T )\u2223\u2223\u2223\u2223\u2223\u2223F \u2264 \u03b4, since \u2223\u2223\u2223\u2223\u2223\u2223V (`)\u2223\u2223\u2223\u2223\u2223\u2223F =\u221a rd2 and (I\u2212 (1/d2)11T ) is a projection which can only decrease the norm.\nNow, consider \u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1) \u2212\u0398(`2)\u2223\u2223\u2223\u2223\u2223\u22232\nF = (\u03b42/(rd2)) \u2223\u2223\u2223\u2223\u2223\u2223(I\u2212 (1/d2)11T )(V (`1) \u2212 V (`2))\u2223\u2223\u2223\u2223\u2223\u22232F \u2261 f(V (`1), V (`2)) which is a function over 2rd2 i.i.d. random Rademacher variables V (`1) and V (`2) which define \u0398(`1) and \u0398(`2) respectively. Since f is Lipschitz in the following sense, we can apply McDiarmid\u2019s concentration inequality. For all (V (`1), V (`2)) and (V\u0303 (`1), V\u0303 (`2)) that differ in only one variable, say V\u0303 (`1) = V (`1) + 2eij , for some standard basis matrix eij , we have\u2223\u2223f(V (`1), V (`2))\u2212 f(V\u0303 (`1), V\u0303 (`2))\u2223\u2223 =\u2223\u2223\u2223\u2223\u2223 \u03b42r d2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223(I\u2212 1d211T )(V (`1) \u2212 V (`2)) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232\nF\n\u2212 \u03b4 2\nr d2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223(I\u2212 1d211T )(V (`1) \u2212 V (`2) + 2eij) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232\nF \u2223\u2223\u2223\u2223\u2223 (61) = \u2223\u2223\u2223\u2223\u2223 \u03b42r d2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232(I\u2212 1d211T )eij \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 F + \u03b42 r d2 \u3008\u3008(I\u2212 1 d2 11T )(V (`1) \u2212 V (`2)), 2eij\u3009\u3009\n\u2223\u2223\u2223\u2223\u2223 (62) \u2264 4 \u03b4 2\nr d2 +\n\u03b4\nr d2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223(I\u2212 1d211T )(V (`1) \u2212 V (`2)) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u221e |||2eij |||1 (63)\n\u2264 12 \u03b4 2\nr d2 , (64)\nwhere we used the fact that (I\u2212 1d211 T )(V (`1) \u2212 V (`2)) is entry-wise bounded by four. The expectation E[f(V (`1), V (`2))] is\n\u03b42\nr d2 E [\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223(I\u2212 1d211T )(V (`1) \u2212 V (`2)) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232\nF\n] = 2\u03b42\nr d2 E [\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223(I\u2212 1d211T )V (`1) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232\nF\n] (65)\n= 2\u03b42 r d2 E [\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223V (`1)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 F ] \u2212 2\u03b4 2 r d22 E [ \u20161TV (`1)\u20162 ] (66) = 2 \u03b42 (d2 \u2212 1)\nd2 . (67)\nApplying McDiarmid\u2019s inequality with bounded difference 12\u03b42/(rd2), we get that\nP { f(V (`1), V (`2)) \u2264 2\u03b42(1\u2212 1/d2)\u2212 t } \u2264 exp { \u2212 t\n2 r d2 144 \u03b44\n} , (68)\nSince there are less than (M \u2032)2 pairs of (`1, `2), setting t = (1 \u2212 2/d2)\u03b42 and applying the union bound gives\nP {\nmin `1,`2\u2208[M \u2032] \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1) \u2212\u0398(`2)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 F \u2265 \u03b42 } \u2265 1\u2212 exp { \u2212 r d2 144 ( 1\u2212 2 d2 )2 + 2 logM \u2032 } \u2265 7 8 , (69)\nwhere we used M \u2032 = exp{rd2/576} and d2 \u2265 607. We are left to prove that \u0398(`)\u2019s are in \u2126(8\u03b4/d2) \u221a 2 log d2 as defined in (7). Since we removed the mean such that \u0398(`)1 = 0 by construction, we only need to show that the maximum entry is bounded by (8\u03b4/d2) \u221a 2 log d2. We first prove an upper bound in (71) for a fixed ` \u2208 [M \u2032], and use this to show that there exists a large enough subset of matrices satisfying this bound. From (125),\nconsider (UV T )ij = \u3008\u3008ui, vj\u3009\u3009, where ui \u2208 Rr is the first r entries of a random vector drawn uniformly from the d2-dimensional sphere, and vj \u2208 Rr is drawn uniformly at random from {\u22121,+1}r with \u2016vj\u2016 = \u221a r. Using Levy\u2019s theorem for concentration on the sphere [29], we have\nP {|\u3008\u3008ui, vj\u3009\u3009| \u2265 t} \u2264 2 exp { \u2212 d2 t 2\n8 r\n} . (70)\nNotice that by the definition (125), maxi,j |\u0398(`)ij | \u2264 (2\u03b4/ \u221a rd2) maxi,j |\u3008\u3008ui, vj\u3009\u3009|. Setting t = \u221a (32r/d2) log d2 and taking the union bound over all d1d2 indices, we get\nP {\nmax i,j |\u0398(`)ij | \u2264\n2\u03b4 \u221a\n32 log d2 d2\n} \u2265 1\u2212 2d1d2 exp { \u2212 4 log d2 } \u2265 1\n2 , (71)\nfor a fixed ` \u2208 [M \u2032]. Consider the event that there exists a subset S \u2282 [M \u2032] of cardinality M = (1/4)M \u2032 with the same bound on maximum entry, then from (71) we get\nP { \u2203S \u2282 [M \u2032] such that \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398(`)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u221e \u2264 2\u03b4 \u221a 32 log d2 d2 for all ` \u2208 S } \u2265 M \u2032\u2211 m=M ( M \u2032 m )(1 2 )m , (72)\nwhich is larger than half for our choice of M < M \u2032/2."}, {"heading": "D Proof of Theorem 3", "text": "We use similar notations and techniques as the proof of Theorem 1 in Appendix A. From the definition of L(\u0398) in Eq. (17), we have for the true parameter \u0398\u2217, the gradient evaluated at the true parameter is\n\u2207L(\u0398\u2217) = \u2212 1 n n\u2211 i=1 (euie T vi \u2212 pi) , (73)\nwhere pi denotes the conditional probability of the MNL choice for the i-th sample. Precisely, pi =\u2211 j1\u2208Si \u2211 j2\u2208Ti pj1,j2|Si,Tiej1e T j2 where pj1,j2|Si,Ti is the probability that the pair of items (j1, j2) is chosen at the i-th sample such that pj1,j2|Si,Ti \u2261 P {(ui, vi) = (j1, j2)|Si, Ti} = e \u0398\u2217j1,j2/( \u2211 j\u20321\u2208Si,j\u20322\u2208Ti e \u0398\u2217 j\u20321,j \u2032 2 ), where (ui, vi) is the pair of items selected by the i-th user among the set of pairs of alternatives Si \u00d7 Ti. The Hessian can be computed as\n\u22022L(\u0398) \u2202\u0398j1,j2 \u2202\u0398j\u20321,j\u20322 = 1 n n\u2211 i=1 I ( (j1, j2) \u2208 Si \u00d7 Ti )\u2202pj1,j2|Si,Ti \u2202\u0398j\u20321,j\u20322\n(74)\n= 1\nn n\u2211 i=1 I ( (j1, j2), (j \u2032 1, j \u2032 2) \u2208 Si \u00d7 Ti ) ( pj1,j2|Si,TiI((j1, j2) = (j \u2032 1, j \u2032 2))\u2212 pj1,j2|Si,Tipj\u20321,j\u20322|Si,Ti ) , (75)\nWe use \u22072L(\u0398) \u2208 Rd1d2\u00d7d1d2 to denote this Hessian. Let \u2206 = \u0398\u2217 \u2212 \u0398\u0302 where \u0398\u0302 is an optimal solution to the convex optimization in (15). We introduce the following key technical lemmas.\nLemma A.1 Eq. (26) The following lemma provides a bound on the gradient using the concentration of measure for\nsum of independent random matrices [27].\nLemma D.1. For any positive constant c \u2265 1 and n \u2265 (4(1 + c)e2\u03b1d1d2 log d)/max{d1, d2}, with probability at least 1\u2212 2d\u2212c,\n|||\u2207L(\u0398\u2217)|||2 \u2264\n\u221a 4(1 + c)e2\u03b1 max{d1, d2} log d\nd1 d2 n . (76)\nSince we are typically interested in the regime where the number of samples is much smaller than the dimension d1 \u00d7 d2 of the problem, the Hessian is typically not positive definite. However, when we restrict our attention to the vectorized \u2206 with relatively small nuclear norm, then we can prove restricted strong convexity, which gives the following bound.\nLemma D.2 (Restricted Strong Convexity for bundled choice modeling). Fix any \u0398 \u2208 \u2126\u2032\u03b1 and assume (min{d1, d2}/min{k1, k2}) log d \u2264 n \u2264 min{d5 log d, k1k2 max{d21, d22} log d}. Under the random sampling model of the alternatives {jia}i\u2208[n],a\u2208[k1] from the first set of items [d1], {jib}i\u2208[n],b\u2208[k1] from the second set of items [d2] and the random outcome of the comparisons described in section 1, with probability larger than 1\u2212 2d\u2212225,\nVec(\u2206)T \u22072L(\u0398) Vec(\u2206) \u2265 e \u22122\u03b1\n8 d1 d2 |||\u2206|||2F , (77)\nfor all \u2206 in A\u2032 where\nA\u2032 = { \u2206 \u2208 Rd1\u00d7d2 \u2223\u2223 |||\u2206|||\u221e \u2264 2\u03b1 , \u2211\nj1\u2208[d1],j2\u2208[d2]\n\u2206j1j2 = 0 and |||\u2206||| 2 F \u2265 \u00b5\u0303|||\u2206|||nuc\n} . (78)\nwith\n\u00b5\u0303 \u2261 210 \u03b1d1d2\n\u221a log d\nn min{d1, d2} min{k1, k2} . (79)\nBuilding on these lemmas, the proof of Theorem 3 is divided into the following two cases. In both cases, we will show that\n|||\u2206|||2F \u2264 12 e 2\u03b1c1\u03bb1 d1d2 |||\u2206|||nuc , (80)\nwith high probability. Applying Lemma A.1 proves the desired theorem. We are left to show Eq. (80) holds.\nCase 1: Suppose |||\u2206|||2F \u2265 \u00b5\u0303 |||\u2206|||nuc. With \u2206 = \u0398\u2217 \u2212 \u0398\u0302, the Taylor expansion yields\nL(\u0398\u0302) = L(\u0398\u2217)\u2212 \u3008\u3008\u2207L(\u0398\u2217),\u2206\u3009\u3009+ 1 2 Vec(\u2206)\u22072L(\u0398)VecT (\u2206), (81)\nwhere \u0398 = a\u0398\u0302 + (1\u2212 a)\u0398\u2217 for some a \u2208 [0, 1]. It follows from Lemma D.2 that with probability at least 1\u2212 2d\u2212225 ,\nL(\u0398\u0302)\u2212 L(\u0398\u2217) \u2265 \u2212|||\u2207L(\u0398\u2217)|||2|||\u2206|||nuc + e\u22122\u03b1\n8 d1 d2 |||\u2206|||2F .\nFrom the definition of \u0398\u0302 as an optimal solution of the minimization, we have L(\u0398\u0302)\u2212 L(\u0398\u2217) \u2264 \u03bb ( |||\u0398\u2217|||nuc \u2212 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 nuc ) \u2264 \u03bb|||\u2206|||nuc .\nBy the assumption, we choose \u03bb \u2265 8\u03bb1. In view of Lemma D.1, this implies that \u03bb \u2265 2|||\u2207L(\u0398\u2217)|||2 with probability at least 1\u2212 2d\u22123. It follows that with probability at least 1\u2212 2d\u22123 \u2212 2d\u2212225 ,\ne\u22122\u03b1\n8d1d2 |||\u2206|||2F \u2264\n( \u03bb+ |||\u2207L(\u0398\u2217)|||2 ) |||\u2206|||nuc \u2264 3\u03bb\n2 |||\u2206|||nuc .\nBy our assumption on \u03bb \u2264 c1\u03bb1, this proves the desired bound in Eq. (80) Case 2: Suppose |||\u2206|||2F \u2264 \u00b5\u0303 |||\u2206|||nuc. By the definition of \u00b5 and the fact that c1 \u2265 128/ \u221a min{k1, k2}, it follows that \u00b5\u0303 \u2264 12 e2\u03b1c1\u03bb1 d1d2, and we get the same bound as in Eq. (80)."}, {"heading": "D.1 Proof of Lemma D.1", "text": "Define Xi = \u2212(euieTvi \u2212 pi) such that \u2207L(\u0398 \u2217) = (1/n) \u2211n i=1Xi, which is a sum of n independent random matrices. Note that since pi is entry-wise bounded by e 2\u03b1/(k1k2),\n|||Xi|||2 \u2264 1 + e2\u03b1\u221a k1k2 ,\nand\nn\u2211 i=1 E[XiXTi ] = n\u2211 i=1 (E[euie T ui ]\u2212 pip T i ) (82)\nn\u2211 i=1 E[euie T ui ] (83) e 2\u03b1 n\nd1 Id1\u00d7d1 , (84)\nwhere the last inequality follows from the fact that for any given Si, ui will be chosen with probability at most e2\u03b1/k1, if it is in the set Si which happens with probability k1/d1. Therefore,\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 E[XiXTi ] \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2 \u2264 e 2\u03b1 n d1 . (85)\nSimilarly, \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 E[XTi Xi] \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2 \u2264 e 2\u03b1 n d2 . (86)\nApplying matrix Bernstein inequality [27], we get P {|||\u2207L(\u0398\u2217)|||2 > t} \u2264 (d1 + d2) exp { \u2212n2t2/2\n(e2\u03b1nmax{d1, d2}/(d1d2)) + ((1 + (e2\u03b1/ \u221a k1k2))nt/3)\n} ,(87)\nwhich gives the desired tail probability of 2d\u2212c for the choice of\nt = max {\u221a4(1 + c)e2\u03b1 max{d1, d2} log d\nd1d2n ,\n4(1 + c)(1 + e 2\u03b1\n\u221a k1k2 ) log d\n3n } = \u221a 4(1 + c)e2\u03b1 max{d1, d2} log d\nd1d2n ,\nwhere the last equality follows from the assumption that n \u2265 (4(1 + c)e2\u03b1d1d2 log d)/max{d1, d2}."}, {"heading": "D.2 Proof of Lemma D.2", "text": "Thee quadratic form of the Hessian defined in (75) can be lower bounded by\nVec(\u2206)T \u22072L(\u0398) Vec(\u2206) \u2265 e \u22122\u03b1\n2 k21 k 2 2 n n\u2211 i=1 \u2211 j1,j\u20321\u2208Si \u2211 j2,j\u20322\u2208Ti ( \u2206j1,j2 \u2212\u2206j\u20321,j\u20322 )2 \ufe38 \ufe37\ufe37 \ufe38\n\u2261H\u0303(\u2206)\n, (88)\nwhich follows from Remark A.4. To lower bound H\u0303(\u2206), we first compute the mean:\nE[H\u0303(\u2206)] = e\u22122\u03b1\n2 k21 k 2 2 n n\u2211 i=1 E [ \u2211 j1,j\u20321\u2208Si \u2211 j2,j\u20322\u2208Ti ( \u2206j1,j2 \u2212\u2206j\u20321,j\u20322 )2] (89)\n= e\u22122\u03b1\nd1 d2 |||\u2206|||2F , (90)\nwhere we used the fact that E[ \u2211 j1\u2208Si,j2\u2208Ti \u2206j1,j2 ] = (k1k2/(d1d2)) \u2211 j\u20321\u2208[d1],j\u20322\u2208[d2] \u2206j\u20321,j\u20322 = 0 for \u2206 \u2208 \u2126\u20322\u03b1 in (17). We now prove that H\u0303(\u2206) does not deviate from its mean too much. Suppose there exists a \u2206 \u2208 A\u2032 defined in (78) such that Eq. (77) is violated, i.e. H\u0303(\u2206) < (e\u22122\u03b1/(8k1k2d1d2))|||\u2206|||2F. In this case,\nE[H\u0303(\u2206)]\u2212 H\u0303(\u2206) \u2265 7 e \u22122\u03b1\n8d1d2 |||\u2206|||2F . (91)\nWe will show that this happens with a small probability. We use the same peeling argument as in Appendix A with\nS \u2032` = { \u2206 \u2208 Rd1\u00d7d2 | |||\u2206|||\u221e \u2264 2\u03b1, \u03b2 `\u22121\u00b5\u0303 \u2264 |||\u2206|||F \u2264 \u03b2 `\u00b5\u0303, \u2211\nj1\u2208[d1],j2\u2208[d2]\n\u2206j1,j2 = 0, and |||\u2206|||nuc \u2264 \u03b2 2`\u00b5\u0303 } ,\nwhere \u03b2 = \u221a\n10/9 and for ` \u2208 {1, 2, 3, . . .}, and \u00b5\u0303 is defined in (79). By the peeling argument, there exists an ` \u2208 Z+ such that \u2206 \u2208 S \u2032` and\nE[H\u0303(\u2206)]\u2212 H\u0303(\u2206) \u2265 7 e \u22122\u03b1\n8d1d2 \u03b22`\u22122(\u00b5\u0303)2 \u2265 7 e\n\u22122\u03b1\n9 d1d2 \u03b22`(\u00b5\u0303)2 . (92)\nApplying the union bound over ` \u2208 Z+,\nP { \u2203\u2206 \u2208 A\u2032 , H\u0303(\u2206) < e \u22122\u03b1\n8 d1 d2 |||\u2206|||2F\n} \u2264 \u221e\u2211 `=1 P { sup \u2206\u2208S\u2032` ( E[H\u0303(\u2206)]\u2212 H\u0303(\u2206) ) > 7 e\u22122\u03b1 9d1d2 (\u03b2`\u00b5\u0303)2 }\n\u2264 \u221e\u2211 `=1 P\n{ sup\n\u2206\u2208B\u2032(\u03b2`\u00b5\u0303)\n( E[H\u0303(\u2206)]\u2212 H\u0303(\u2206) ) > 7e\u22122\u03b1\n9d1d2 (\u03b2`\u00b5\u0303)2\n} , (93)\nwhere we define the set B\u2032(D) such that S \u2032` \u2286 B\u2032(\u03b2`\u00b5\u0303):\nB\u2032(D) = { \u2206 \u2208 Rd1\u00d7d2 \u2223\u2223 \u2016\u2206\u2016\u221e \u2264 2\u03b1, |||\u2206|||F \u2264 D, \u2211\nj1\u2208[d1],j2\u2208[d2]\n\u2206j1j2 = 0, \u00b5\u0303|||\u2206|||nuc \u2264 D 2 } . (94)\nThe following key lemma provides the upper bound on this probability.\nLemma D.3. For (min{d1, d2}/min{k1, k2}) log d \u2264 n \u2264 d5 log d,\nP { sup\n\u2206\u2208B\u2032(D)\n( E[H\u0303(\u2206)]\u2212 H\u0303(\u2206) ) \u2265 e \u22122\u03b1D2\n2d1d2\n} \u2264 exp { \u2212 n min{k 2 1, k 2 2} k1k2D4\n210\u03b14d21d 2 2\n} . (95)\nLet \u03b7 = exp ( \u2212nk1k2 min{k 2 1 ,k 2 2}(\u03b2\u22121.002)(\u00b5\u0303)4\n210\u03b14d21d 2 2\n) . Applying the tail bound to (93), we get\nP { \u2203\u2206 \u2208 A\u2032 , H\u0303(\u2206) < e \u22122\u03b1\n8 d1d2 |||\u2206|||2F\n} \u2264 \u221e\u2211 `=1 exp { \u2212 nk1k2 min{k 2 1, k 2 2} (\u03b2`\u00b5\u0303)4 210\u03b14d21d 2 2 } (a)\n\u2264 \u221e\u2211 `=1 exp { \u2212 nk1k2 min{k 2 1, k 2 2}`(\u03b2 \u2212 1.002)(\u00b5\u0303)4 210\u03b14d21d 2 2 } \u2264 \u03b7\n1\u2212 \u03b7 ,\nwhere (a) holds because \u03b2x \u2265 x log \u03b2 \u2265 x(\u03b2\u22121.002) for the choice of \u03b2 = \u221a\n10/9. By the definition of \u00b5\u0303,\n\u03b7 = exp { \u2212 2\n30 k1k2 max{d22, d21}(log d)2(\u03b2 \u2212 1.002) n\n} \u2264 exp{\u2212 225 log d} ,\nwhere the last inequality follows from the assumption that n \u2264 k1k2 max{d21, d22} log d, and \u03b2 \u2212 1.002 \u2265 2\u22125. Since for d \u2265 2, exp{\u2212225 log d} \u2264 1/2 and thus \u03b7 \u2264 1/2, the lemma follows by assembling the last two displayed inequalities."}, {"heading": "D.3 Proof of Lemma D.3", "text": "Let Z \u2261 sup\u2206\u2208B\u2032(D) E[H\u0303(\u2206)] \u2212 H\u0303(\u2206) and consider the tail bound using McDiarmid\u2019s inequality. Note that Z has a bounded difference of (8\u03b12e\u22122\u03b1 max{k1, k2})/(k21k22n) when one of the k1k2n independent random variables are changed, which gives\nP {Z \u2212 E[Z] \u2265 t} \u2264 exp ( \u2212 k 4 1k 4 2n 2t2\n64\u03b14e\u22124\u03b1 max{k21, k22}k1k2n\n) . (96)\nWith the choice of t = D2/(4e2\u03b1 d1d2), this gives\nP { Z \u2212 E[Z] \u2265 e \u22122\u03b1 4d1d2 D2 } \u2264 exp ( \u2212 k 3 1k 3 2nD 4 210\u03b14d21d 2 2 max{k21, k22} ) . (97)\nWe first construct a partition of the space similar to Lemma A.6. Let\nk\u0303 \u2261 min{k1, k2} . (98)\nLemma D.4. There exists a partition (T1, . . . , TN ) of {[k1] \u00d7 [k2]} \u00d7 {[k1] \u00d7 [k2]} for some N \u2264 2k21k 2 2/k\u0303 such that T`\u2019s are disjoint subsets, \u22c3 `\u2208[N ] T` = {[k1] \u00d7 [k2]} \u00d7 {[k1] \u00d7 [k2]}, |T`| \u2264 k\u0303 and for any ` \u2208 [N ] the set of random variables in T` satisfy\n{(\u2206ji,a,ji,b \u2212\u2206ji,a\u2032 ,ji,b\u2032 ) 2}i\u2208[n],((a,b),(a\u2032,b\u2032))\u2208T` are mutually independent .\nwhere ji,a for i \u2208 [n] and a \u2208 [k1] denote the a-th chosen item to be included in the set Si.\nNow we prove an upper bound on E[Z] using the symmetrization technique. Recall that ji,a is independently and uniformly chosen from [d1] for i \u2208 [n] and a \u2208 [k1]. Similarly, ji,b is independently and uniformly chosen from [d1] for i \u2208 [n] and b \u2208 [k2].\nE[Z] = e\u22122\u03b1\n2 k21 k 2 2 n\nE  sup \u2206\u2208B\u2032(D) n\u2211 i=1 \u2211 a,a\u2032\u2208[k1] \u2211 b,b\u2032\u2208[k2] E [( \u2206ji,a,ji,b \u2212\u2206ji,a\u2032 ,ji,b\u2032 )2]\u2212 (\u2206ji,a,ji,b \u2212\u2206ji,a\u2032 ,ji,b\u2032)2 (99) \u2264 e \u22122\u03b1\n2 k21 k 2 2 n \u2211 `\u2208[N ] E  sup \u2206\u2208B\u2032(D) n\u2211 i=1 \u2211 (j1,j2,j\u20321,j \u2032 2)\u2208T` E [( \u2206j1,j2 \u2212\u2206j\u20321,j\u20322 )2]\u2212 (\u2206j1,j2 \u2212\u2206j\u20321,j\u20322)2  (100) \u2264 e \u22122\u03b1\nk21 k 2 2 n \u2211 `\u2208[N ] E  sup \u2206\u2208B\u2032(D) n\u2211 i=1 \u2211 (j1,j2,j\u20321,j \u2032 2)\u2208T` \u03bei,j1,j2,j\u20321,j\u20322 ( \u2206j1,j2 \u2212\u2206j\u20321,j\u20322 )2 , (101) where the first inequality follows for the fact that the supremum of the sum is smaller than the sum of supremum, and the second inequality follows from standard symmetrization with i.i.d. Rademacher random variables \u03bei,j1,j2,j\u20321,j\u20322 \u2019s. It follows from Ledoux-Talagrand contraction inequality that\nE  sup \u2206\u2208B\u2032(D) n\u2211 i=1 \u2211 (j1,j2,j\u20321,j \u2032 2)\u2208T` \u03bei,j1,j2,j\u20321,j\u20322 ( \u2206j1,j2 \u2212\u2206j\u20321,j\u20322 )2 (102) \u2264 8\u03b1E  sup \u2206\u2208B\u2032(D) n\u2211 i=1 \u2211 (j1,j2,j\u20321,j \u2032 2)\u2208T` \u03bei,j1,j2,j\u20321,j\u20322 ( \u2206j1,j2 \u2212\u2206j\u20321,j\u20322\n) (103) \u2264 8\u03b1E  sup \u2206\u2208B\u2032(D) |||\u2206|||nuc \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u2211 (j1,j2,j\u20321,j \u2032 2)\u2208T` \u03bei,j1,j2,j\u20321,j\u20322 ( ej1,j2 \u2212 ej\u20321,j\u20322 )\u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2\n (104) \u2264 8\u03b1D 2\n\u00b5\u0303 E \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u2211 (j1,j2,j\u20321,j \u2032 2)\u2208T` \u03bei,j1,j2,j\u20321,j\u20322 ( ej1,j2 \u2212 ej\u20321,j\u20322 )\u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2  , (105)\nwhere the second inequality follows for the Ho\u0308lder\u2019s inequality and the last inequality follows from \u00b5\u0303|||\u2206|||nuc \u2264 D2 for all \u2206 \u2208 B\u2032(D). To bound the expected spectral norm of the random matrix, we use matrix Bernstein\u2019s inequality. Note that \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u03bei,j1,j2,j\u20321,j\u20322c\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 \u2264 \u221a2 almost surely, E[(ej1,j2\u2212ej\u20321,j\u20322)(ej1,j2\u2212ej\u20321,j\u20322) T ] (2/d1)Id1\u00d7d1 , and E[(ej1,j2\u2212ej\u20321,j\u20322) T (ej1,j2\u2212ej\u20321,j\u20322)] (2/d2)Id2\u00d7d2 . It follows that \u03c32 = 2n|T`|/min{d1, d2}, where |T`| \u2264 min{k1, k2}. It follows that\nP  \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u2211 (j1,j2,j\u20321,j \u2032 2)\u2208T` \u03bei,j1,j2,j\u20321,j\u20322 ( ej1,j2 \u2212 ej\u20321,j\u20322 )\u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2 > t  \u2264 (d1 + d2) exp{ \u2212t2/22nmin{k1,k2} min{d1,d2} + \u221a 2t 3 } ,\nChoosing t = max{ \u221a 64n(min{k1, k2}/min{d1, d2}) log d, (16 \u221a\n2/3) log d}, we obtain a bound on the spectral norm of t with probability at least 1\u22122d\u22127. From the fact that \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211ni=1\u2211(j1,j2,j\u20321,j\u20322)\u2208T` \u03bei,j1,j2,j\u20321,j\u20322(ej1,j2 \u2212 ej\u20321,j\u20322)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 \u2264 (n/ \u221a 2) min{k1, k2}, it follows that\nE \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u2211 (j1,j2,j\u20321,j \u2032 2)\u2208T` \u03bei,j1,j2,j\u20321,j\u20322 ( ej1,j2 \u2212 ej\u20321,j\u20322 )\u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2  (106) \u2264 max {\u221a64n min{k1, k2} log d min{d1, d2} , (16 \u221a 2/3) log d } + 2nmin{k1, k2}\u221a 2d7 (107)\n\u2264\n\u221a 66n min{k1, k2} log d\nmin{d1, d2} (108)\nwhich follows form the assumption that nmin{k1, k2} \u2265 min{d1, d2} log d and n \u2264 d5 log d. Substituting this bound in (101), and (105), we get that\nE[Z] \u2264 16e \u22122\u03b1\u03b1D2\n\u00b5\u0303\n\u221a 66 log d\nnmin{k1, k2}min{d1, d2} (109)\n\u2264 e \u22122\u03b1D2\n4 d1d2 . (110)"}, {"heading": "E Proof of the information-theoretic lower bound in Theorem 4", "text": "This proof follow closely the proof of Theorem 2 in Appendix C. We apply the generalized Fano\u2019s inequality in the same way to get Eq. (49)\nP { L\u0302 6= L } \u2265 1\u2212 ( M 2 )\u22121\u2211 `1,`2\u2208[M ]DKL(\u0398\n(`1)\u2016\u0398(`2)) + log 2 logM , (111)\nThe main challenge in this case is that we can no longer directly apply the RUM interpretation to compete DKL(\u0398\n(`1)\u2016\u0398(`2)). This will result in over estimating the KL-divergence, because this approach does not take into account that we only take the top winner, out of those k1k2 alternatives. Instead, we compute the divergence directly, and provide an appropriate bound. Let the set of k1\nrows and k2 columns chosen in one of the n sampling be S \u2282 [d1] and T \u2282 [d2] respectively. Then,\nDKL(\u0398 (`1)\u2016\u0398(`2)) (a)= n(\nd1 k1 )( d2 k2 )\u2211 S,T \u2211 i\u2208S j\u2208T e\u0398 (`1) ij\u2211\ni\u2032\u2208S j\u2032\u2208T\ne \u0398\n(`1) i\u2032j\u2032 log\ne \u0398 (`1) ij \u2211 i\u2032\u2208S j\u2032\u2208T e \u0398 (`2) i\u2032j\u2032\ne\u0398 (`2) ij \u2211\ni\u2032\u2208S j\u2032\u2208T\ne \u0398\n(`1) i\u2032j\u2032\n (112)\n(b) \u2264 n( d1 k1 )( d2 k2 )\u2211 S,T \u2211 i,j e2\u0398 (`1) ij \u2211 i\u2032,j\u2032 e \u0398 (`2) i\u2032j\u2032 \u2212 e\u0398 (`1) ij +\u0398 (`2) ij \u2211 i\u2032,j\u2032 e \u0398 (`1) i\u2032j\u2032\ne\u0398 (`2) ij (\u2211 i\u2032,j\u2032 e \u0398 (`1) i\u2032j\u2032\n)2  (113)\n(c) \u2264 ne 2\u03b1\nk21k 2 2 ( d1 k1 )( d2 k2 )\u2211 S,T \u2211 i,j e2\u0398(`1)ij \u2212\u0398(`2)ij \u2211 i\u2032,j\u2032 e \u0398 (`2) i\u2032j\u2032 \u2212 e\u0398 (`1) ij \u2211 i\u2032,j\u2032 e \u0398 (`1) i\u2032j\u2032  (114)\n= ne2\u03b1\nk21k 2 2 ( d1 k1 )( d2 k2 )\u2211 S,T \u2211 i\u2032,j\u2032 e \u0398 (`2) i\u2032j\u2032 \u2211 i,j ( e\u0398 (`1) ij \u2212 e\u0398 (`2) ij )2 e\u0398 (`2) ij \u2212 (\u2211 i,j (e\u0398 (`1) ij \u2212 e\u0398 (`2) ij ) )2 (115)\n(d) \u2264 ne 4\u03b1 k1k2 ( d1 k1 )( d2 k2 )\u2211 S,T \u2211 i,j ( e\u0398 (`1) ij \u2212 e\u0398 (`2) ij )2 (116)\n(e) \u2264 ne 5\u03b1 k1k2 ( d1 k1 )( d2 k2 )\u2211 S,T \u2211 i,j ( \u0398 (`1) ij \u2212\u0398 (`2) ij )2 (117)\n(f) =\nne5\u03b1\nd1d2\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1)ij \u2212\u0398(`2)ij \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 F\n(118)\n(119)\nHere (a) is by definition of KL-distance and the fact that S, T are chosen uniformly from all possible such sets and (b) is due to the fact that log(x) \u2264 x\u22121 with x = (e\u0398 (`1) ij \u2211\ni\u2032\u2208S,j\u2032\u2208T e \u0398\n(`2) i\u2032j\u2032 )/(e\u0398 (`2) ij \u2211\ni\u2032\u2208S,j\u2032\u2208T e \u0398\n(`1) i\u2032j\u2032 ).\nThe constants at (c) is due to the fact that each element of \u0398(`1) is upper bounded by \u03b1 and lower bounded by \u2212\u03b1. We can get (d) by removing the second term which is always negative, and using the bond of \u03b1. (e) is obtained because ex where \u2212\u03b1 \u2264 x \u2264 \u03b1 is Lipschitz continuous with Lipschitz constant e\u03b1. At last (f) is obtained by simple counting of the occurrences of each ij. Thus we have,\nP { L\u0302 6= L } \u2265 1\u2212\n( M 2 )\u22121\u2211 `1,`2\u2208[M ] ne5\u03b1 d1d2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398(`2)ij \u2212\u0398(`2)ij \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232 F + log 2\nlogM , (120)\nThe remainder of the proof relies on the following probabilistic packing.\nLemma E.1. Let d2 \u2265 d1 be sufficiently large positive integers. Then for each r \u2208 {1, . . . , d1}, and for any positive \u03b4 > 0 there exists a family of d1\u00d7d2 dimensional matrices {\u0398(1), . . . ,\u0398(M(\u03b4))} with cardinality M(\u03b4) = b(1/4) exp(rd2/576)c such that each matrix is rank r and the following bounds\nhold: \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398(`)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F \u2264 \u03b4 , for all ` \u2208 [M ] (121)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1) \u2212\u0398(`2)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\nF \u2265 1 2 \u03b4 , for all `1, `2 \u2208 [M ] (122) \u0398(`) \u2208 \u2126\u2032\u03b1\u0303 , for all ` \u2208 [M ] , (123)\nwith \u03b1\u0303 = (8\u03b4/d2) \u221a 2 log d for d = (d1 + d2)/2.\nSuppose \u03b4 \u2264 \u03b1d2/(8 \u221a 2 log d) such that the matrices in the packing set are entry-wise bounded\nby \u03b1, then the above lemma E.1 implies that \u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1) \u2212\u0398(`2)\u2223\u2223\u2223\u2223\u2223\u22232\nF \u2264 4\u03b42, which gives\nP { L\u0302 6= L } \u2265 1\u2212 e5\u03b1n4\u03b42 d1d2 + log 2\nrd2 576 \u2212 2 log 2\n\u2265 1 2 , (124)\nwhere the last inequality holds for \u03b42 \u2264 (rd1d22/(1152e5\u03b1n)) and assuming rd2 \u2265 1600. Together with (124) and (122), this proves that for all \u03b4 \u2264 min{\u03b1d2/(8 \u221a 2 log d), rd1d 2 2/(1152e 5\u03b1n)},\ninf \u0398\u0302 sup \u0398\u2217\u2208\u2126\u03b1\nE [ \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398\u0302\u2212\u0398\u2217\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\nF\n] \u2265 \u03b4/4 .\nChoosing \u03b4 appropriately to maximize the right-hand side finishes the proof of the desired claim. Also by symmetry, we can apply the same argument to get similar bound with d1 and d2 interchanged."}, {"heading": "E.1 Proof of Lemma E.1", "text": "We show that the following procedure succeeds in producing the desired family with probability at least half, which proves its existence. Let d = (d1 + d2)/2, and suppose d2 \u2265 d1 without loss of generality. For the choice of M \u2032 = erd2/576, and for each ` \u2208 [M \u2032], generate a rank-r matrix \u0398(`) \u2208 Rd1\u00d7d2 as follows:\n\u0398(`) = \u03b4\u221a rd2\nU(V (`))T ( Id2\u00d7d2 \u2212 1TU(V (`))T1\nd1d2 11T\n) , (125)\nwhere U \u2208 Rd1\u00d7r is a random orthogonal basis such that UTU = Ir\u00d7r and V (`) \u2208 Rd2\u00d7r is a random matrix with each entry V\n(`) ij \u2208 {\u22121,+1} chosen independently and uniformly at random. By construction, notice that \u2223\u2223\u2223\u2223\u2223\u2223\u0398(`)\u2223\u2223\u2223\u2223\u2223\u2223\nF \u2264 (\u03b4/\n\u221a rd2) \u2223\u2223\u2223\u2223\u2223\u2223U(V (`))T \u2223\u2223\u2223\u2223\u2223\u2223 F\n= \u03b4. Now, by triangular inequality, we have\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1) \u2212\u0398(`2)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\nF \u2265 \u03b4\u221a\nrd2\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223U(V (`1) \u2212 V (`2))T \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F \u2212 \u03b4 |1 TU(V (`1) \u2212 V (`2))T1| d1d2 \u221a rd2 \u2223\u2223\u2223\u2223\u2223\u222311T \u2223\u2223\u2223\u2223\u2223\u2223 F\n\u2265 \u03b4\u221a rd2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223V (`1) \u2212 V (`2)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F\ufe38 \ufe37\ufe37 \ufe38\nA\n\u2212 \u03b4\u221a r d1 d22 ( |1TU(V (`1))T1|\ufe38 \ufe37\ufe37 \ufe38\nB\n+|1TU(V (`2))T1| ) .\nWe will prove that the first term is bounded by A \u2265 \u221a rd2 with probability at least 7/8 for all M \u2032 matrices, and we will show that we can find M matrices such that the second term is bounded\nby B \u2264 8 \u221a\n2rd2 log(32r) log(32d) with probability at least 7/8. Together, this proves that with probability at least 3/4, there exists M matrices such that\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u0398(`1) \u2212\u0398(`2)\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 F \u2265 \u03b4 ( 1\u2212\n\u221a 27 log(32r) log(32d)\nd1d2\n) \u2265 1\n2 \u03b4 ,\nfor all `1, `2 \u2208 [M ] and for sufficiently large d1 and d2. Applying similar McDiarmid\u2019s inequality as Eq. (69) in Appendix C, it follows that A2 \u2265 rd2 with probability at least 7/8 for M \u2032 = erd2/576 and a sufficiently large d2. To prove a bound on B, we will show that for a given `,\nP { |1TU(V (`))T1| \u2264 8 \u221a 2rd2 log(32r) log(32d) } \u2265 7\n8 . (126)\nThen using the similar technique as in (72), it follows that we can find M = (1/4)M \u2032 matrices all satisfying this bound and also the bound on the max-entry in (127). We are left to prove (126). We apply a series of concentration inequalities. Let H1 be the event that {|\u3008\u3008V (`)i ,1\u3009\u3009| \u2264\u221a 2d2 log(32r) for all i \u2208 [r]}. Then, applying the standard Hoeffding\u2019s inequality, we get that P {H1} \u2265 15/16, where V (`)i is the i-th column of V (`). We next change the variables and represent 1TU as \u221a d1u\nT U\u0303 , where u is drawn uniformly at random from the unit sphere and U\u0303 is a r dimensional subspace drawn uniformly at random. By symmetry, \u221a d1u\nT U\u0303 have the same distribution as 1TU . Let H2 be the event that {|\u3008\u3008U\u0303i, (V (`))T1\u3009\u3009| \u2264 \u221a 16r(d2/d1) log(32r) log(32d) for all i \u2208 [d1]}, where U\u0303i is the i-th row of U\u0303 . Then, applying Levy\u2019s theorem for concentration on the sphere [29], we have P {H2|H1} \u2265 15/16. Finally, let H3 be the event that {| \u221a d1\u3008\u3008u, U\u0303(V (`))T \u3009\u30091| \u2264\n8 \u221a\n2rd2 log(32r) log(32d)}. Then, again applying Levy\u2019s concentration, we get P {H3|H1, H2} \u2265 15/16. Collecting all three concentration inequalities, we get that with probability at least 13/16, |1TU(V (`))T1| \u2264 8 \u221a 2rd2 log(32r) log(32d), which proves Eq. (126).\nWe are left to prove that \u0398(`)\u2019s are in \u2126(8\u03b4/d2) \u221a 2 log d2 as defined in (17). Similar to Eq. (71),\napplying Levy\u2019s concentration gives P {\nmax i,j |\u0398(`)ij | \u2264\n2\u03b4 \u221a\n32 log d2 d2\n} \u2265 1\u2212 2 exp { \u2212 2 log d2 } \u2265 1\n2 , (127)\nfor a fixed ` \u2208 [M \u2032]. Then using the similar technique as in (72), it follows that there exists M = (1/4)M \u2032 matrices all satisfying this bound and also the bound on B in Eq. (126)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>In applications such as recommendation systems and revenue management, it is important to<lb>predict preferences on items that have not been seen by a user or predict outcomes of comparisons<lb>among those that have never been compared. A popular discrete choice model of multinomial<lb>logit model captures the structure of the hidden preferences with a low-rank matrix. In order to<lb>predict the preferences, we want to learn the underlying model from noisy observations of the<lb>low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural<lb>approach to learn such a model is to solve a convex relaxation of nuclear norm minimization.<lb>We present the convex relaxation approach in two contexts of interest: collaborative ranking<lb>and bundled choice modeling. In both cases, we show that the convex relaxation is minimax<lb>optimal. We prove an upper bound on the resulting error with finite samples, and provide a<lb>matching information-theoretic lower bound.", "creator": "LaTeX with hyperref package"}}}