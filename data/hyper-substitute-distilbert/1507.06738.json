{"id": "1507.06738", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jul-2015", "title": "Linear Contextual Bandits with Knapsacks", "abstract": "but declare the linear texture mapping feasible without strictly convex constraints and hierarchical concave objective function. in particular round, the outcome restriction pulling an integer is a vector, that defines jointly governing the fields below that arm. the stability constraints require smooth distribution of these vectors actively grow in a certain anatomical object. operation anomaly is a concave obstacle determining this average vector. survival requirement turns out to be a common generalization of traditional intrinsic scaling filters ( lincontextual ) [ auer 2003 ], when exhibiting overlapping actions along convex criminals ( instrumental ) [ agrawal, devanur 1931 ], and the online stochastic semantic programming ( tagged ) problem [ agrawal, devanur 1921 ]. we present algorithms with otherwise - existent regret bounds throughout concurrent iteration. intrinsic bounds compare favorably to results on the unstructured alternative : outlaw problem [ agrawal e as. 2015, lamb et al. 2014 ] where boundary relation gives the consequences and the outcomes could be interpreted, indeed the theory still succeeds considering other fixed set of policies.", "histories": [["v1", "Fri, 24 Jul 2015 04:24:22 GMT  (36kb)", "https://arxiv.org/abs/1507.06738v1", null], ["v2", "Sat, 9 Jul 2016 06:29:22 GMT  (37kb)", "http://arxiv.org/abs/1507.06738v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["shipra agrawal", "nikhil r devanur"], "accepted": true, "id": "1507.06738"}, "pdf": {"name": "1507.06738.pdf", "metadata": {"source": "CRF", "title": "Linear Contextual Bandits with Knapsacks", "authors": ["Shipra Agrawal", "Nikhil R. Devanur"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 7.\n06 73\n8v 2\n[ cs\n.L G\n] 9\nJ ul\n2 01\n6"}, {"heading": "1 Introduction", "text": "In the contextual bandit problem [7, 13, 21, 2], the decision maker observes a sequence of contexts (or features). In every round she needs to pull one out of K arms, after observing the context for that round. The outcome of pulling an arm may be used along with the contexts to decide future arms. Contextual bandit problems have found many useful applications such as online recommendation systems, online advertising, and clinical trials, where the decision in every round needs to be customized to the features of the user being served. The linear contextual bandit problem [1, 7, 16] is a special case of the contextual bandit problem, where the outcome is linear in the feature vector encoding the context. As pointed by [2], contextual bandit problems represent a natural half-way point between supervised learning and reinforcement learning: the use of features to encode contexts and the models for the relation between these feature vectors and the outcome are often inherited from supervised learning, while managing the exploration-exploitation tradeoff is necessary to ensure good performance in reinforcement learning. The linear contextual bandit problem can thus be thought of as a midway between the linear regression model of supervised learning, and reinforcement learning.\nRecently, there has been a significant interest in introducing multiple \u201cglobal constraints\u201d in the standard bandit setting [10, 3, 11, 5]. Such constraints are crucial for many important real-world applications. For example, in clinical trials, the treatment plans may be constrained by the total availability of medical facilities, drugs and other resources. In online advertising, there are budget constraints that restrict the number of times an ad is shown. Other applications include dynamic pricing, dynamic procurement, crowdsourcing, etc.; see [10, 3] for many such examples.\nIn this paper, we consider linear contextual bandit with knapsacks (henceforth, linCBwK) problem. In this problem, the context vectors are generated i.i.d. in every round from some unknown distribution, and on picking an arm, a reward and a consumption vector is observed, which depend linearly on the context vector. The aim of the decision maker is to maximize a total reward while ensuring the the total consumption of every resource remains withing a given budget. Below, we give a more precise definition of this problem.\n\u2217Columbia University. sa3305@columbia.edu. \u2020Microsoft Research. nikdev@microsoft.com.\nWe use the following notational convention throughout: vectors are denoted by bold face lower case letters, while matrices are denoted by regular face upper case letters. Other quantities such as sets, scalars, etc. may be of either case, but never bold faced. All vectors are column vectors, i.e., a vector in n dimensions is treated as an n\u00d7 1 matrix. The transpose of matrix A is A\u22a4.\nDefinition 1 (linCBwK). There are K \u201carms\u201d, which we identify with the set [K]. The algorithm is initially given as input a budget B \u2208 R+. In every round t, the algorithm first observes context xt(a) \u2208 [0, 1]m for every arm a, and then chooses an arm at \u2208 [K], and finally observes a reward rt(at) \u2208 [0, 1] and a ddimensional consumption vector vt(at) \u2208 [0, 1]d. The algorithm has a \u201cno-op\u201d option, which is to pick none of the arms and get 0 reward and 0 consumption. The goal of the algorithm is to pick arms such that the total reward\n\u2211T t=1 rt(at) is maximized, while ensuring that the total consumption does not exceed budget, i.e.,\n\u2211\nt vt(at) \u2264 B1. We make the following stochastic assumption for context, reward, consumption vectors. In every round t, the tuple {xt(a), rt(a),vt(a)}Ka=1 is generated from an unknown distribution D, independent of everything in previous rounds. Also, there exists an unknown vector \u00b5\u2217 \u2208 [0, 1]m and matrix W\u2217 \u2208 [0, 1]m\u00d7d such that for every arm a, given contexts xt(a), and history Ht\u22121 before time t,\nE[rt(a)|xt(a), Ht\u22121] = \u00b5\u22a4\u2217 xt(a), E[vt(a)|xt(a), Ht\u22121] = W\u22a4\u2217 xt(a). (1)\nFor succinctness, we will denote the tuple of contexts for K arms at time t as matrix Xt \u2208 [0, 1]m\u00d7K, with xt(a) being the a\nth column of this matrix. Similarly, rewards are represented as vector rt \u2208 [0, 1]K, and consumption vectors are represented as matrix Vt \u2208 [0, 1]d\u00d7K.\nAs we discuss later in the text, the assumption in equation (1) forms the primary distinction between our linear contextual bandit setting and the general contextual bandit setting considered in [5]. Exploiting this linearity assumption will allow us to generate regret bounds which do not depend on number of arms K, rendering it to be especially useful when number of arms is large. Some examples include recommendation systems with large number of products (e.g., retail products, travel packages, ad creatives, sponsored facebook posts). Another advantage over using general contextual bandit setting of [5] is that we don\u2019t need an oracle access to a certain optimization problem, which is required to solve an NP-Hard problem in this case. (See Section 1.1 for a more detailed discusssion.)\nWe compare the performance of an algorithm to that of the optimal adaptive policy that knows the distribution D and the parameters (\u00b5\u2217,W\u2217), and can take into account the history upto that point as well as the current context to decide (possibly with randomization) which arm to pull at time t. However, it is easier to work with an upper bound on this, which is the optimal expected reward of a static policy that is required to satisfy the constraints only in expectation. This technique has been used in several related problems and is standard by now [19, 10].\nDefinition 2 (Optimal Static Policy). Consider any policy that is context dependent but non-adaptive: for a policy \u03c0, let \u03c0(X) \u2208 \u2206K+1 (the unit simplex) denote the probability distribution over arms played (plus no-op) when the context is X \u2208 X . Define r(\u03c0) and v(\u03c0) to be the expected reward and consumption vector of policy \u03c0, respectively, i.e.\nr(\u03c0) := E(X,r,V )\u223cD[r\u03c0(X)] = EX\u223cD[\u00b5 \u22a4 \u2217 X\u03c0(X)]. (2) v(\u03c0) := E(X,r,V )\u223cD[V \u03c0(X)] = EX\u223cD[W \u22a4 \u2217 X\u03c0(X)]. (3)\nLet \u03c0\u2217 := argmax\u03c0 T r(\u03c0) such that T v(\u03c0) \u2264 B1 (4)\nbe the optimal static policy. Note that since no-op is allowed, a feasible policy always exists. We denote the value of this optimal static policy by OPT := T r(\u03c0\u2217).\nFollowing lemma proves that OPT upper bounds the value of optimal adaptive policy. The proof is in Appendix B.\nLemma 1. Let OPT denote the value of optimal adaptive policy that knows the distribution D and parameters \u00b5\u2217,W\u2217, We show that there exists a static policy \u03c0 \u2217 such that T r(\u03c0\u2217) \u2265 OPT, and Tv(\u03c0\u2217) \u2264 B.\nDefinition 3 (Regret). Let at be the arm played at time t by the algorithm. Then, regret is defined as\nregret(T ) := OPT\u2212 T \u2211\nt=1\nrt(at)"}, {"heading": "1.1 Main results", "text": "Our main result is an algorithm with near-optimal regret bound for linCBwK .\nTheorem 1. There is an algorithm for linCBwK such that if B > mT 3/4, then with probability at least 1\u2212 \u03b4,\nregret(T ) = O ( (OPTB + 1)m \u221a ln(dT/\u03b4) ln(T )T ) .\nRelation to general contextual bandits. There have been recent papers [5, 11] that solve problems similar to linCBwK but for general contextual bandits. Here the relation between contexts and outcome vectors is arbitrary and the algorithms compete with an arbitrary fixed set of context dependent policies \u03a0 accessible via an optimization oracle, with regret bounds being O (\n(OPTB + 1) \u221a\nKT log(dT |\u03a0|/\u03b4) ) . These\napproaches could potentially be applied to the linear setting using a set \u03a0 of linear context dependent policies. Comparing their bounds with ours, in our results, essentially a \u221a\nK log(|\u03a0|) factor is replaced by a factor of m. Most importantly, we have no dependence on K,1 which enables us to consider problems with large action spaces. In any case, both K and log(|\u03a0|) are at least m, so their bounds are no smaller.\nFurther, suppose that we want to use their result with the set of linear policies, i.e., policies of the form\narg max a\u2208[K]\n{xt(a)\u22a4\u03b8},\nfor some fixed \u03b8 \u2208 \u211cm. Then, their algorithms would require access to an \u201cArg-Max Oracle\u201d that can find the best such policy (maximizing total reward) for a given set of contexts and rewards (no resource consumption). We show that infact the optimization problem underlying such an \u201cArg-Max Oracle\u201d problem is NP-Hard, making such an approach computationally expensive. (Proof is in Appendix C.)\nThe only downside to our results is that we need the budget B to be \u2126(mT 3/4). Getting similar bounds for budgets as small as B = \u0398(m \u221a T ) is an interesting open problem. (This also indicates that this is indeed a harder problem than all the special cases.) Near-optimality of regret bounds. In [17], it was shown that for the linear contextual bandits problem, no online algorithm can achieve a regret bound better than \u2126(m \u221a T ). In fact, they prove this lower bound for linear contextual bandits with static contexts. Since that problem is a special case of the linCBwK problem with d = 1, this shows that the dependence on m and T in the above regret bound is optimal upto log factors. For general contextual bandits with resource constraints, the bounds of [5, 11] are near optimal.\nRelation to BwK [3] and OSPP [4]. It is easy to see that the linCBwK problem is a generalization of the linear contextual bandits problem [1, 7, 16]. There, the outcome is scalar and the goal is to simply maximize the sum of these. Remarkably, the linCBwK problem also turns out to be a common generalization of bandits with knapsacks (BwK) problem considered in [10, 3], and the online stochastic packing problem (OSPP) studied by [18, 6, 22, 19, 4]. In both BwK and OSPP, the outcome of every round t is a reward rt and a vector vt and the goal of the algorithm is to maximize \u2211T t=1 rt while ensuring that \u2211T t=1 vt \u2264 B1. The problems differ in how these rewards and vectors are picked. In the OSPP problem, in every round t, the algorithm may pick any reward,vector pair from a given set At of d + 1-dimensional vectors. The set\n1Similar to the regret bounds for linear contextual bandits [1, 7, 16].\nAt is drawn i.i.d. from an unknown distribution over sets of vectors. This corresponds to the special case of linCBwK , where m = d + 1 and the context xt(a) itself is equal to (rt(a),vt(a). In the BwK problem, there is a fixed set of arms, and for each arm there is an unknown distribution over reward,vector pairs. The algorithm picks an arm and a reward,vector pair is drawn from the corresponding distribution for that arm. This corresponds to the special case of linCBwK , where m = K and the context Xt = I, the identity matrix, for all t.\nWe use techniques from all three special cases: our algorithms follow the primal-dual paradigm using an online learning algorithm to search the dual space, that was established in [3]. In order to deal with linear contexts, we use techniques from [1, 7, 16] to estimate the weight matrix W\u2217, and define \u201coptimistic estimates\u201d of W\u2217. We also use the technique of combining the objective and the constraints using a certain tradeoff parameter and that was introduced in [4]. Further new difficulties arise, such as in estimating the optimum value from the first few rounds, a task that follows from standard techniques in each of the special cases but is very challenging here. We develop a new way of exploration that uses the linear structure, so that one can evaluate all possible choices that could have led to an optimum solution on the historic sample. This technique might be of independent interest in estimating optimum values. One can see that the problem is indeed more than the sum of its parts, from the fact that we get the optimal bound for linCBwK only when B \u2265 \u2126\u0303(mT 3/4), unlike either special case for which the optimal bound holds for all B (but is meaningful only for B = \u2126\u0303(m \u221a T )).\nThe approach in [3] (for BwK) extends to the case of \u201cstatic\u201d contexts,2 where each arm has a context that doesn\u2019t change over time. The OSPP of [4] is not a special case of linCBwK with static contexts; this is one indication of the additional difficulty of dynamic over static contexts.\nOther related work. Budget constraints in a bandit setting has recieved considerable attention, but most of the early work focussed on special cases such as a single budget constraint in the regular (non-contextual) setting [20, 23, 26, 29, 35, 36]. Recently, [38] showed an O( \u221a T ) regret in the linear contextual setting with a single budget constraint, when costs depend only on contexts and not arms. Budget constraints that arise in particular applications such as online advertising [14, 31], dynamic pricing [8, 12] and crowdsourcing [9, 33, 34] have also been considered. There has also been a long line of work studying special cases of the OSCP problem [18, 19, 22, 6, 28, 24, 37, 30, 27, 15].\nDue to space constraints, we have eliminated many proofs from the main text. All the missing proofs are in the appendix."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Confidence Ellipsoid", "text": "Consider a stochastic process which in each round t, generates a pair of observations (rt,yt), such that rt is an unknown linear function of yt plus some 0-mean bounded noise, i.e., rt = \u00b5 \u22a4 \u2217 yt + \u03b7t, where yt,\u00b5\u2217 \u2208 Rm, |\u03b7t| \u2264 2R, and E[\u03b7t|y1, r1, . . . ,yt\u22121, rt\u22121,yt] = 0. At any time t, a high confidence estimate of the unknown vector \u00b5\u2217 can be obtained by building a\n\u201cConfidence Ellipsoid\u201d around the \u21132-regularized least-square estimate \u00b5\u0302t constructed from the observations made so far. This technique is common in prior work on linear contextual bandits (e.g., in [7, 16, 1]). For any regularization parameter \u03bb > 0, let\nMt := \u03bbI + \u2211t\u22121 i=1 yiy \u22a4 i , and \u00b5\u0302t := M \u22121 t \u2211t\u22121 i=1 yiri.\nThe following result from [1] shows that \u00b5\u2217 lies with high probability in an ellipsoid with center \u00b5\u0302t. For any positive semi-definite (PSD) matrix M, define the M -norm as \u2016\u00b5\u2016M := \u221a\n\u00b5\u22a4M\u00b5. The confidence ellipsoid at time t is defined as\nCt := { \u00b5 \u2208 Rm : \u2016\u00b5\u2212 \u00b5\u0302t\u2016Mt \u2264 R \u221a m ln ((1+tm/\u03bb)/\u03b4) + \u221a \u03bbm } .\n2It was incorrectly claimed in [3] that the approach can be extended to dynamic contexts without much modifications.\nLemma 2 (Theorem 2 of [1]). If \u2200 t, \u2016\u00b5\u2217\u20162 \u2264 \u221a m and \u2016yt\u20162 \u2264 \u221a m, then with prob. 1\u2212 \u03b4, \u00b5\u2217 \u2208 Ct.\nAnother useful observation about this construction is stated below. It first appeared as Lemma 11 of [7], and was also proved as Lemma 3 in [16].\nLemma 3 (Lemma 11 of [7]). \u2211T t=1 \u2016yt\u2016M\u22121t \u2264 \u221a mT ln(T ).\nAs a corollary of the above two lemmas, we obtain a bound on the total error in the estimate provided by \u201cany point\u201d from the confidence ellipsoid. (Proof is in Appendix D.)\nCorollary 1. For t = 1, . . . , T , let \u00b5\u0303t \u2208 Ct be a point in the confidence ellipsoid, with \u03bb = 1, 2R = 1. Then, with probability 1\u2212 \u03b4,\n\u2211T t=1 |\u00b5\u0303\u22a4t yt \u2212 \u00b5\u22a4\u2217 yt| \u2264 2m \u221a T ln ((1+Tm)/\u03b4) ln(T )."}, {"heading": "2.2 Online Learning", "text": "The online convex optimization (OCO ) problem considers a T round game played between a learner and an adversary, where in round t, the learner chooses a \u03b8t \u2208 \u2126, and then the adversary picks a concave function gt(\u03b8t) : \u2126 \u2192 R. The learner\u2019s choice \u03b8t may only depend on learner\u2019s and adversary\u2019s choices in previous rounds. The goal of the learner is to minimize regret defined as the difference between the learner\u2019s objective value and the value of the best single choice on hindsight:\nR(T ) := max\u03b8\u2208\u2126 \u2211T t=1 gt(\u03b8)\u2212 \u2211T t=1 gt(\u03b8t).\nIn particular, we will use linear reward functions with values in [\u22121, 1], and domain \u2126 is the unit simplex in d+ 1 dimensions. The algorithm online mirror descent (OMD ) has very fast per step update rules, and provides the following regret guarantees for this setting.\nLemma 4. [32] The online mirror-descent algorithm for the OCO problem achieves regret\nR(T ) = O( \u221a log(d)T ).\nWe actually need the domain to be\n\u2126 = {\u03b8 : \u2016\u03b8\u20161 \u2264 1, \u03b8 \u2265 0} .\nThis is a special case of a unit simplex in d+ 1 dimensions, by letting the rewards on one of the dimensions always be zero. For the rest of the paper, we assume that the OMD algorithm is using this domain."}, {"heading": "3 Algorithm", "text": ""}, {"heading": "3.1 Optimistic estimates of unknown parameters", "text": "Let at denote the arm played by the algorithm at time t. In the beginning of every round, we use the outcomes and contexts from previous rounds to construct a confidence ellipsoid for \u00b5\u2217 and every column of W\u2217. The construction of confidence ellipsoid for \u00b5\u2217 follows directly from the techniques in Section 2.1 with yt = xt(at) and rt being reward at time t. To construct a confidence ellipsoid for a column j of W\u2217, we use the techniques in Section 2.1 while substituting yt = xt(at) and rt = vt(at)j for every j.\nAs in Section 2.1, let Mt := I + \u2211t\u22121 i=1 xi(ai)xi(ai) \u22a4, and construct the regularized least squares estimate\nfor \u00b5\u2217,W\u2217, respectively, as\n\u00b5\u0302t := M \u22121 t \u2211t\u22121 i=1 xi(ai)ri(ai) \u22a4 (5)\nW\u0302t := M \u22121 t \u2211t\u22121 i=1 xi(ai)vi(ai) \u22a4. (6)\nDefine confidence ellipsoid for parameter \u00b5\u2217 as\nCt,0 := { \u00b5 \u2208 Rm : \u2016\u00b5\u2212 \u00b5\u0302\u2016Mt \u2264 \u221a m ln ((d+tmd)/\u03b4) + \u221a m } ,\nand optimistic estimate of \u00b5\u2217 for every arm a as:\n\u00b5\u0303t(a) := argmax\u00b5\u2208Ct,0 xt(a) \u22a4\u00b5. (7)\nLet wj denote the j th column of a matrix W . We define a confidence ellipsoid for each column j, as\nCt,j := { w \u2208 Rm : \u2016w\u2212 w\u0302tj\u2016Mt \u2264 \u221a m ln ((d+tmd)/\u03b4) + \u221a m } ,\nand denote by Gt, the Cartesian product of all these ellipsoids: Gt := {W \u2208 Rm\u00d7d : wj \u2208 Ct,j}. Note that Lemma 2 implies W\u2217 \u2208 Gt with probability 1 \u2212 \u03b4. Now, given a vector \u03b8t \u2208 Rd, we define the optimistic estimate of weight matrix at time t w.r.t. \u03b8t, for every arm a \u2208 [K], as :\nW\u0303t(a) := argminW\u2208Gt xt(a) \u22a4W\u03b8t. (8)\nIntuitively, for reward we want an upper confidence bound and for consumption we want a lower confidence bound as an optimistic estimate. This intuition aligns with the above definitions, where the maximizer was used in case of reward and a minimizer was used for consumption. The utility and precise meaning of \u03b8t will become clearer when we describe the algorithm and present regret analysis.\nUsing the definition of \u00b5\u0303t, W\u0303t, along with the results in Lemma 2 and Corollary 1 about confidence ellipsoids, the following can be derived.\nCorollary 2. With probability 1\u2212 \u03b4, for any sequence of \u03b81, \u03b82, . . . , \u03b8T , 1. xt(a) \u22a4\u00b5\u0303t(a) \u2265 xt(a)\u22a4\u00b5\u0303, for all arms a \u2208 [K], for all time t.\n2. xt(a) \u22a4W\u0303t(a)\u03b8t \u2264 xt(a)\u22a4W\u2217\u03b8t, for all arms a \u2208 [K], for all time t.\n3. |\u2211Tt=1(\u00b5\u0303t(at)\u2212 \u00b5\u2217)\u22a4xt(at)| \u2264 ( 2m \u221a T ln ((1+tm)/\u03b4) ln(T ) ) .\n4. \u2016\u2211Tt=1(W\u0303t(at)\u2212W\u2217)\u22a4xt(at)\u2016 \u2264 \u20161d\u2016 ( 2m \u221a T ln ((d+tmd)/\u03b4) ln(T ) ) .\nEssentially, the first two claims ensure that we have optimistic estimates, and the last two claims ensure that the estimates quickly converge to the true parameters."}, {"heading": "3.2 The core algorithm", "text": "In this section, we present an algorithm, and analysis, under the assumption that a certain parameter Z is given. Later, we show how to use the first T0 rounds to estimate Z, and also bound the additional regret due to these T0 rounds. We define Z now.\nAssumption 1. Assume we are given Z such that OPTB \u2264 Z \u2264 O(OPTB + 1).\nThe algorithm constructs estimates \u00b5\u0302t and W\u0302t as in Section 3.1. It also runs the OMD algorithm for an instance of the online learning problem, over the unit simplex. The vector played by the online learning algorithm in time step t is \u03b8t. After observing the context, the optimistic estimates for each arm are then constructed using \u03b8t, as defined in (7) and (8). Intuitively, \u03b8t is used here as a multiplier to combine different columns of the weight matrix, to get an optimistic weight vector for every arm. An adjusted estimated reward for arm a is then defined by using Z to combine optimistic estimate of reward with optimistic estimate of consumption, as (xt(a)\n\u22a4\u00b5\u0303t(a))\u2212Z(xt(a)\u22a4W\u0303t(a)\u03b8t). The algorithm chooses the arm which appears to be the best according to adjusted estimated reward. After observing the resulting reward and consumption vectors, the estimates are updated. The online learning algorithm is advanced by one step, by defining the profit vector to be vt(at)\u2212 BT 1. The algorithm ends either after T time steps or as soon as the total consumption exceeds the budget along some dimension.\nAlgorithm 1 Algorithm for linCBwK , with given Z\nInitialize \u03b81 as per the OCO algorithm. Initialize Z such that OPTB \u2264 Z \u2264 O(OPTB + 1). for all t = 1, ..., T do Observe Xt. For every a \u2208 [K], compute \u00b5\u0303t(a) and W\u0303t(a) as per (7) and (8) respectively. Play the arm at := argmaxa\u2208[K] xt(a)\n\u22a4(\u00b5\u0303t(a)\u2212 ZW\u0303t(a)\u03b8t). Observe rt(at) and vt(at). If for some j = 1..d, \u2211\nt\u2032\u2264t vt\u2032(at\u2032) \u00b7 ej \u2265 B then EXIT. Use xt(at), rt(at) and vt(at) to obtain \u00b5\u0302t+1, W\u0302t+1 and Gt+1. Update \u03b8t+1 as per the OCO algorithm with gt(\u03b8t) := \u03b8t \u00b7 ( vt(at)\u2212 BT 1 )\n. end for\nTheorem 2. Given a Z as per Assumption 1, Algorithm 1 achieves the following bounds, given that R(T ) is the regret of the OCO algorithm, with probability 1\u2212 \u03b4:\nregret(T ) \u2264 O ( (OPTB + 1)m \u221a T ln(dT/\u03b4) ln(T ) ) .\n(Proof Sketch) We provide a sketch of the proof here, with the full proof in Appendix E. Let \u03c4 be the stopping time of the algorithm. The proof is in 3 steps:\nStep 1: Since E[vt(at)|Xt, at, Ht\u22121] = W\u22a4\u2217 xt(at), we apply Azuma-Hoeffding to get that with high probability \u2225 \u2225\n\u2211\u03c4 t=1 vt(at)\u2212W\u22a4\u2217 xt(at)\n\u2225 \u2225 \u221e is small. Similarly, a lower bound on the sum of \u00b5 \u22a4 \u2217 xt(at) is sufficient.\nStep 2: From Corollary 2, with high probability, we can bound \u2225 \u2225\n\u2225 \u2211T t=1(W\u2217 \u2212 W\u0303t(at))\u22a4xt(at)\n\u2225 \u2225 \u2225\n\u221e . It is\ntherefore sufficient to work with the sum of the vectors W\u0303t(at) \u22a4xt(at), and similarly \u00b5\u0303t(at) \u22a4xt(at).\nStep 3: The proof is completed by showing the desired bound on OPT\u2212 \u2211\u03c4 t=1 \u00b5\u0303t(at) \u22a4xt(at). This part is similar to the online stochastic packing problem; if the actual reward and consumption vectors were \u00b5\u0303t(at)\n\u22a4xt(at) and W\u0303t(at)\u22a4xt(at), then it would be exactly like that problem. We adapt techniques from [4]: use the OCO algorithm and the Z parameter to combine constraints into the objective. If a dimension is being consumed too fast, then the multiplier for that dimension should increase, making the algorithm to pick arms that are not likely to consume too much along this dimension."}, {"heading": "3.3 Algorithm with Z computation", "text": "In this section, we present a modification of Algorithm 1 which computes the required parameter Z and therefore does not need to be provided with a Z as input, as assumed previously in Assumption 1. The algorithm computes Z using the observations from first T0 rounds. Once Z is computed, the algorithm from the previous section can be run for the remaining time steps. However, it needs to be modified slightly to take into account the budget consumed during the first T0 rounds. We handle this by using a smaller budget B\u2032 = B \u2212 T0 in the computations for remaining rounds. The modified algorithm is given below.\nAlgorithm 2 Algorithm for linCBwK , with Z computation\nInputs: B, T0, B \u2032 = B \u2212 T0 Using observations from first T0 rounds, compute Z such that OPT B\u2032 \u2264 Z \u2264 O(OPTB\u2032 + 1). Run Algorithm 1 for T \u2212 T0 rounds and budget B\u2032.\nNext, we provide details of the first T0 rounds, and choice of T0. We provide a method that takes advantage of the linear structure of the problem, and explores in the mdimensional space of contexts and weight vectors to obtain bounds independent of K. We use the following procedure. In every round t = 1, . . . , T0, after observing Xt, let pt \u2208 \u2206[K] be\npt := arg max p\u2208\u2206[K] \u2016Xtp\u2016M\u22121t , (9)\nwhere Mt := I + \u2211t\u22121 i=1(Xipi)(Xipi) \u22a4. (10)\nSelect arm at = a with probability pt(a). In fact, since Mt is a PSD matrix, due to convexity of the function \u2016Xtp\u20162M\u22121t , it is the same as playing at = argmaxa\u2208[K] \u2016xt(a)\u2016M\u22121t . Construct estimates \u00b5\u0302, W\u0302t of \u00b5\u2217,W\u2217 at time t as\n\u00b5\u0302t := M \u22121 t \u2211t\u22121 i=1(Xipi)ri(ai), W\u0302t := M \u22121 t \u2211t\u22121 i=1(Xipi)vi(ai) \u22a4.\nAnd, for some value of \u03b3 defined later, obtain an estimate \u02c6OPT \u03b3 of OPT as:\n\u02c6OPT \u03b3 := max\u03c0\nT T0 \u2211T0 i=1 \u00b5\u0302 \u22a4 i Xi\u03c0(Xi)\nsuch that TT0 \u2211T0 i=1 W\u0302 \u22a4 i Xi\u03c0(Xi) \u2264 B + \u03b3.\n(11)\nFor an intuition about the choice of arm in (9), observe from the discussion in Section 2.1 that every column w\u2217j of W\u2217 is guaranteed to lie inside the confidence ellipsoid centered at column w\u0302tj of W\u0302t, namely the ellipsoid, \u2016w \u2212 w\u0302tj\u20162Mt \u2264 4m ln(Tm/\u03b4). Note that this ellipsoid has principle axes as eigenvectors of Mt, and the length of semi-principle axes is given by inverse eigenvalues of Mt. Therefore, by maximizing \u2016Xtp\u2016M\u22121t we are choosing the context closest to the direction of the longest principal axes of the confidence ellipsoid, i.e. in the direction of maximum uncertainty. Intuitively, this corresponds to pure exploration: by making an observation in the direction where uncertainty is large we can reduce the uncertainty in our estimate most effectively.\nA more algebraic explanation is as follows. For a good estimation of OPT by \u02c6OPT \u03b3 , we want the\nestimates W\u0302t and W\u2217 (and, \u00b5\u0302 and \u00b5\u2217) to be close enough so that \u2016 \u2211T0 t=1(W\u0302t \u2212 W\u0302\u2217)\u22a4Xt\u03c0(Xt)\u2016\u221e (and, |\u2211T0t=1(\u00b5\u0302t \u2212 \u00b5\u2217)\u22a4Xt\u03c0(Xt)|) is small for all policies \u03c0, and in particular for sample optimal policies. Now, using Cauchy-Schwartz these are bounded by\n\u2211T0 t=1 \u2016\u00b5\u0302t \u2212 \u00b5\u2217\u2016Mt\u2016Xt\u03c0(Xt))\u2016M\u22121t , and \u2211T0\nt=1 \u2016W\u0302t \u2212W\u2217\u2016Mt\u2016Xt\u03c0(Xt))\u2016M\u22121t ,\nwhere we define \u2016W\u2016M , the M -norm of matrix W to be the max of column-wise M -norms. Using Lemma 2, the term \u2016\u00b5\u0302t \u2212 \u00b5\u2217\u2016Mt is bounded by 2 \u221a m ln(T0m/\u03b4) , and \u2016W\u0302t \u2212W\u2217\u2016Mt is bounded by 2 \u221a m ln(T0md/\u03b4), with probability 1 \u2212 \u03b4. Lemma 3 bounds the second term \u2211T0t=1 \u2016Xt\u03c0(Xt)\u2016M\u22121t but only when \u03c0 is the played policy. This is where we use that the played policy pt was chosen to maximize \u2016Xtpt\u2016M\u22121t , so that \u2211T0\nt=1 \u2016Xt\u03c0(Xt)\u2016M\u22121t \u2264 \u2211T0 t=1 \u2016Xtpt\u2016M\u22121t and the bound \u2211T0 t=1 \u2016Xtpt\u2016M\u22121t \u2264 \u221a mT0 ln(T0) given by Lemma\n3 actually bounds \u2211T0 t=1 \u2016Xt\u03c0(Xt)\u2016M\u22121t for all \u03c0. Combining, we get a bound of 2m \u221a T0ln(T0) ln(T0d/\u03b4) on deviations \u2016\u2211T0t=1(W\u0302t \u2212 W\u0302\u2217)\u22a4Xt\u03c0(Xt)\u2016\u221e and | \u2211T0\nt=1(\u00b5\u0302t \u2212 \u00b5\u2217)\u22a4Xt\u03c0(Xt)| for all \u03c0. We prove the following lemma.\nLemma 5. For \u03b3 = (\nT T0\n)\n2m \u221a T0ln(T0) ln(T0d/\u03b4), with probability 1\u2212O(\u03b4),\nOPT\u2212 2\u03b3 \u2264 \u02c6OPT2\u03b3 \u2264 OPT+ 9\u03b3(OPTB + 1).\nCorollary 3. Set Z = ( \u02c6OPT\n2\u03b3 +2\u03b3) B + 1, with above value of \u03b3. Then, with probability 1\u2212O(\u03b4), OPT\nB + 1 \u2264 Z \u2264 (1 + 11\u03b3 B )( OPT B + 1).\nCorollary 3 implies that as long as B \u2265 \u03b3, i.e., B \u2265 \u2126\u0303( mT\u221a T0 ), Z is a constant factor approximation of\nOPT B + 1 \u2265 Z\u2217, therefore Theorem 2 should provide an O\u0303\n( (OPTB + 1)m \u221a T ) regret bound. However, this\nbound does not account for the budget consumed in the first T0 rounds. Considering that (at most) T0 amount can be consumed from the budget in the first T0 rounds, we have an additional regret of OPT B T0. Further, since we have B\u2032 = B \u2212 T0 budget for remaining T \u2212 T0 rounds, we need a Z that satisfies the required assumption for B\u2032 instead of B (i.e., we need OPTB\u2032 \u2264 Z \u2264 O(1) ( OPT B\u2032 + 1 )\n). If B \u2265 2T0, then, B\u2032 \u2265 B/2, and using 2 times the Z computed in Corollary 3 would satisfy the required assumption.\nTogether, these observations give Theorem 3.\nTheorem 3. Using Algorithm 2 with T0 such that B > max{2T0,mT/ \u221a T0}, and twice the Z given by Corollary 3, we get a high probability regret bound of\nO\u0303 ( ( OPT\nB + 1 )\n( T0 +m \u221a T )) .\nIn particular, using T0 = \u221a T , and assuming B > mT 3/4 gives a regret bound of\nO\u0303 ( ( OPT\nB + 1 )\nm \u221a T ) ."}, {"heading": "A Concentration Inequalities", "text": "Lemma 6 (Azuma-Hoeffding inequality). If a super-martingale (Yt; t \u2265 0), corresponding to filtration Ft, satisfies |Yt \u2212 Yt\u22121| \u2264 ct for some constant ct, for all t = 1, . . . , T , then for any a \u2265 0,\nPr(YT \u2212 Y0 \u2265 a) \u2264 e \u2212 a2 2 \u2211T t=1 c2 t ."}, {"heading": "B Benchmark", "text": "Proof of Lemma 1. For an instantiation \u03c9 = (Xt, Vt) T t=1 of the sequence of inputs, let vector p \u2217 t (\u03c9) \u2208 \u2206K+1 denote the distribution over actions (plus no-op) taken by the optimal adaptive policy at time t. Then,\nOPT = E\u03c9\u223cDT [ \u2211T t=1 r \u22a4 t p \u2217 t (\u03c9)] (12)\nAlso, since this is a feasible policy,\nE\u03c9\u223cDT [ T \u2211\nt=1\nV \u22a4t p \u2217 t (\u03c9)] \u2264 B1 (13)\nConstruct a static context dependent policy \u03c0\u2217 as follows: for any X \u2208 [0, 1]m\u00d7K , define\n\u03c0\u2217(X) := 1\nT\nT \u2211\nt=1\nE\u03c9[p \u2217 t (\u03c9)|Xt = X ].\nIntuitively, \u03c0\u2217(X)a denotes (in hindsight) the probability that the optimal adaptive policy takes an action a when presented with a context X , averaged over all time steps. Now, by definition of r(\u03c0),v(\u03c0), from above definition of \u03c0\u2217, and (12), (13),\nT r(\u03c0\u2217) = TEX\u223cD[\u00b5 \u22a4 \u2217 X\u03c0 \u2217(X)] = E\u03c9[ \u2211T t=1 Vtp \u2217 t (\u03c9)] = OPT,\nTv(\u03c0\u2217) = TEX\u223cD[W \u22a4 \u2217 X\u03c0 \u2217(X)] = E\u03c9 [ \u2211T t=1 Vtp \u2217 t (\u03c9)] \u2264 B1,"}, {"heading": "C Hardness of linear AMO", "text": "In this section we show that finding the best linear policy is NP-Hard. The input to the problem is, for each t \u2208 [T ], and each arm a \u2208 [K], a context xt(a) \u2208 [0, 1]m, and a reward rt(a) \u2208 [\u22121, 1]. The output is a vector \u03b8 \u2208 \u211cm that maximizes \u2211t rt(at) where\nat = arg max a\u2208[K]\n{xt(a)\u22a4\u03b8}.\nWe give a reduction from the problem of learning halfspaces with noise [25]. The input to this problem is for some integer n, for each i \u2208 [n], a vector zi \u2208 [0, 1]m, and yi \u2208 {\u22121,+1}. The output is a vector \u03b8 \u2208 \u211cm that maximizes\nn \u2211\ni=1\nsign(z\u22a4i \u03b8)yi.\nGiven an instance of the problem of learning halfspaces with noise, construct an instance of the linear AMO as follows. The time horizon T = n, and the number of arms K = 2. For each t \u2208 [T ], the context of\nthe first arm, xt(1) = zt, and its reward rt(1) = yt. The context of the second arm, xt(2) = 0, the all zeroes vector, and the reward rt(2) is also 0.\nThe total reward of a linear policy w.r.t a vector \u03b8 for this instance is\n|{i : sign(z\u22a4i \u03b8) = 1, yi = 1}| \u2212 |{i : sign(z\u22a4i \u03b8) = 1, yi = \u22121}|.\nIt is easy to see that this is an affine transformation of the objective for the problem of learning halfspaces with noise."}, {"heading": "D Confidence ellipsoids", "text": "Proof of Corollary 1. The following holds with probability 1\u2212 \u03b4. T \u2211\nt=1\n|\u00b5\u0303\u22a4t xt \u2212 \u00b5\u22a4\u2217 xt| \u2264 T \u2211\nt=1\n\u2016\u00b5\u0303t \u2212 \u00b5\u2217\u2016Mt\u2016xt\u2016M\u22121t\n\u2264 ( \u221a m ln ( 1 + tm\n\u03b4\n) + \u221a m\n)\n\u221a\nmT ln(T ).\nThe inequality in the first line is a matrix-norm version of Cauchy-Schwartz (Lemma 7). The inequality in the second line is due to Lemmas 2 and 3. The lemma follows from multiplying out the two factors in the second line.\nLemma 7. For any positive definite matrix M \u2208 Rn\u00d7n and any two vectors a,b \u2208 Rn, |a\u22a4b| \u2264 \u2016a\u2016M\u2016b\u2016M\u22121 .\nProof. Since M is positive definite, there exists a matrix M1/2 such that M = M1/2M \u22a4 1/2. Further, M \u22121 = M\u22a4\u22121/2M\u22121/2 where M\u22121/2 = M \u22121 1/2.\n\u2016a\u22a4M1/2\u20162 = a\u22a4M1/2M\u22a41/2a = a\u22a4Ma = \u2016a\u20162M .\nSimilarly, \u2016M\u22121/2b\u20162 = \u2016b\u20162M\u22121 . Now applying Cauchy-Schwartz, we get that\n|a\u22a4b| = |a\u22a4M1/2M\u22121/2b| \u2264 \u2016a\u22a4M1/2\u2016\u2016M\u22121/2b\u2016 = \u2016a\u2016M\u2016b\u2016M\u22121 .\nProof of Corollary 2. Here, the first claim follows simply from definition of W\u0303t(a) and the observation that with probability 1 \u2212 \u03b4, W \u2217 \u2208 Gt. To obtain the second claim, apply Corollary 1 with \u00b5\u2217 = w\u2217j ,yt = xt(at), \u00b5\u0303t = [W\u0303t(at)]j (the j th column of W\u0303t(at)), to bound | \u2211 t([W\u0303t(at)]j\u2212w\u2217j)\u22a4xt(at)| \u2264 \u2211\nt |([W\u0303t(at)]j\u2212 w\u2217j)\u22a4xt(at)| for every j, and then take the norm."}, {"heading": "E Appendix for Section 3.2", "text": "Proof of Theorem 2: We will use R\u2032 to denote the main term in the regret bound.\nR\u2032(T ) := O ( m \u221a ln(mdT/\u03b4) ln(T )T )\nLet \u03c4 be the stopping time of the algorithm. Let Ht\u22121 be the history of plays and observations before time t, i.e. Ht\u22121 := {\u03b8\u03c4 , X\u03c4 , a\u03c4 , r\u03c4 (a\u03c4 ),v\u03c4 (a\u03c4 ), \u03c4 = 1, . . . , t\u2212 1}. Note that Ht\u22121 determines \u03b8t, \u00b5\u0302t, W\u0302t,Gt, but it does not determine Xt, at, W\u0303t (since at and W\u0303t(a) depend on the context Xt at time t). The proof is in 3 steps:\nStep 1: Since E[vt(at)|Xt, at, Ht\u22121] = W\u22a4\u2217 xt(at), we apply Azuma-Hoeffding to get that with probability 1\u2212 \u03b4,\n\u2225 \u2225 \u2211\u03c4 t=1 vt(at)\u2212W\u22a4\u2217 xt(at) \u2225 \u2225 \u221e \u2264 R \u2032(T ). (14)\nSimilarly, a lower bound on the sum of \u00b5\u22a4\u2217 xt(at) is sufficient.\nStep 2: From Corollary 2, with probability 1\u2212 \u03b4, \u2225\n\u2225 \u2225 \u2211T t=1(W\u2217 \u2212 W\u0303t(at))\u22a4xt(at)\n\u2225 \u2225 \u2225\n\u221e \u2264 R\u2032(T ). (15)\nIt is therefore sufficient to bound the sum of the vectors W\u0303t(at) \u22a4xt(at), and similarly for \u00b5\u0303t(at) \u22a4xt(at). We use the shorthand notation of r\u0303t := \u00b5t(at) \u22a4xt(at), r\u0303sum := \u2211\u03c4 t=1 r\u0303t, v\u0303t := W\u0303t(at) \u22a4xt(at) and v\u0303sum := \u2211\u03c4 t=1 v\u0303t for the rest of this proof.\nStep 3: The proof is completed by showing that\nE[r\u0303sum] \u2265 OPT\u2212 ZR\u2032(T ).\nLemma 8. \u03c4 \u2211\nt=1\nE[r\u0303t|Ht\u22121] \u2265 \u03c4\nT OPT+ Z\n\u03c4 \u2211\nt=1\n\u03b8t \u00b7 E[v\u0303t \u2212 1 B\nT |Ht\u22121]\nProof. Let r\u2217t := \u00b5t(at) \u22a4Xt\u03c0\u2217(Xt) and v\u0302\u2217t := W\u0303t(at) \u22a4Xt\u03c0\u2217(Xt). By Corollary 2, with probability 1\u2212 \u03b4, we have that TEXt [r \u2217 t |Ht\u22121] \u2265 OPT, and EXt [v\u0302\u2217t |Ht\u22121] \u2264 BT 1. By the choice made by the algorithm,\nr\u0303t \u2212 Z(\u03b8t \u00b7 v\u0303t) \u2265 r\u2217t \u2212 Z(\u03b8t \u00b7 v\u0302\u2217t ) EXt [r\u0303t \u2212 Z(\u03b8t \u00b7 v\u0303t)|Ht\u22121] \u2265 EXt [r\u0303t|Ht\u22121]\u2212 Z(\u03b8t \u00b7 E[v\u0303t|Ht\u22121])\n\u2265 1 T OPT\u2212 Z\u03b8t \u00b7 B1 T\nSumming above inequality for t = 1 to \u03c4 gives the lemma statement.\nLemma 9. \u03c4 \u2211\nt=1\n\u03b8t \u00b7 (v\u0303t \u2212 B T 1) \u2265 B \u2212 \u03c4B T \u2212R\u2032(T ).\nProof. Recall that gt(\u03b8t) = \u03b8t \u00b7 ( v\u0303t \u2212 BT 1 ) , therefore the LHS in the required inequality is \u2211\u03c4\nt=1 gt(\u03b8t). Let \u03b8\u2217 := argmax||\u03b8||1\u22641,\u03b8\u22650 \u2211\u03c4 t=1 gt(\u03b8). We use the regret definition for the OCO algorithm to get that \u2211\u03c4 t=1 gt(\u03b8t) \u2265 \u2211\u03c4 t=1 gt(\u03b8 \u2217)\u2212R(T ). Note that fromt the regret bound given in Lemma 4, R(T ) \u2264 R\u2032(T ).\nCase 1: \u03c4 < T . This means that \u2211\u03c4 t=1(vt(at) \u00b7 ej) \u2265 B for some j. Then from (14) and (15), it must be that\n\u2211\u03c4 t=1(v\u0303t \u00b7 ej) \u2265 B \u2212R\u2032(T ) so that \u2211\u03c4 t=1 gt(\u03b8 \u2217) \u2265\u2211\u03c4t=1 gt(ej) \u2265 B \u2212 \u03c4BT \u2212R\u2032(T ).\nCase 2: \u03c4 = T . In this case, B \u2212 \u03c4T B = 0 = \u2211\u03c4 t=1 gt(0) \u2264 \u2211\u03c4 t=1 gt(\u03b8 \u2217), which completes the proof of the lemma.\nNow, we are ready to prove Theorem 2, which states that Algorithm 1 achieves a regret of ZR\u2032(T ). Proof of Theorem 2. Substituting the inequality from Lemma 9 in Lemma 8, we get\n\u03c4 \u2211\nt=1\nE[r\u0303t|Ht\u22121] \u2265 \u03c4\nT OPT+ ZB\n( 1\u2212 \u03c4 T ) \u2212 ZR\u2032(T )\nAlso, Z \u2265 OPTB . Substituting in above,\nE[r\u0303sum] =\n\u03c4 \u2211\nt=1\nE[r\u0303t|Ht\u22121] \u2265 \u03c4 T OPT+OPT(1\u2212 \u03c4 T )\u2212 ZR(T )\n\u2265 OPT\u2212 ZR\u2032(T )\nFrom Steps 1 and 2, this implies a lower bound on E[ \u2211\u03c4\nt=1 rt(at)]. The proof is now completed by using Azuma-Hoeffding to bound the actual total reward with high probability."}, {"heading": "F Appendix for Section 3.3", "text": "Proof of Lemma 5. Let us define an \u201cintermediate sample optimal\u201d as:\nOPT \u03b3 := maxq\nT T0 \u2211T0 i=1 \u00b5 \u22a4 \u2217 Xi\u03c0(Xi)])\nsuch that TT0 \u2211T0 i=1 W \u22a4 \u2217 Xi\u03c0(Xi) \u2264 B + \u03b3\n(16)\nAbove sample optimal knows the parameters \u00b5\u2217,W\u2217, the error comes only from approximating the expected value over context distribution by average over the observed contexts. We do not actually compute OPT \u03b3 , but will use it for the convenience of proof exposition. The proof involves two steps.\nStep 1: Bound |OPT\u03b3 \u2212OPT|.\nStep 2: Bound | \u02c6OPT2\u03b3 \u2212OPT\u03b3 |\nStep 1 bound can be borrowed from the work on Online Stochastic Convex Programming in [4]: since \u00b5\u2217,W\n\u2217 is known, so there is effectively full information before making the decision, i.e., consider the vectors [\u00b5\u22a4\u2217 xt(a),W \u22a4 \u2217 xt(a)] as outcome vectors which can be observed for all arms a before choosing the distribution over arms to be played at time t, therefore, the setting in [4] applies. In fact, \u02c6OPT \u03b3 as defined by Equation (F.10) in [4] when At = {[\u00b5\u22a4\u2217 xt(a),W\u22a4\u2217 xt(a)], a \u2208 [K]}, f identity, and S = {v\u22121 \u2264 BT }, is same as 1T times OPT \u03b3 defined here. And using Lemma F.4 and Lemma F.6 in [4] (using L = 1, Z\u2217 = OPT/B), we obtain that for any \u03b3 \u2265 (\nT T0\n)\n2m \u221a T0ln(T0) ln(T0d/\u03b4), with probability 1\u2212O(\u03b4),\nOPT\u2212 \u03b3 \u2264 OPT\u03b3 \u2264 OPT+ 2\u03b3(OPT B + 1). (17)\nFor Step 2, we show that with probability 1\u2212 \u03b4, for all \u03c0, \u03b3 \u2265 (\nT T0\n)\n2m \u221a\nT0ln(T0) ln(T0d/\u03b4)\n| T0 \u2211\ni=1\n(\u00b5\u0302i \u2212 \u00b5\u2217)\u22a4Xi\u03c0(Xi)| \u2264 \u03b3 (18)\n\u2016 T T0\nT0 \u2211\ni=1\n(W\u0302i \u2212W\u2217)\u22a4Xi\u03c0(Xi)\u2016\u221e \u2264 \u03b3 (19)\nThis is sufficient to prove both lower and upper bound on \u02c6OPT 2\u03b3 for \u03b3 \u2265 (\nT T0\n)\n2m \u221a\nT0ln(T0) ln(T0d/\u03b4). For\nlower bound, we can simply use (19) for optimal policy for OPT \u03b3 , denoted by \u03c0\u0304. This implies that (because of relaxation of distance constraint by \u03b3) \u03c0\u0304 is a feasible primal solution for \u02c6OPT 2\u03b3 , and therefore using (17) and (18),\n\u02c6OPT 2\u03b3 + \u03b3 \u2265 OPT\u03b3 \u2265 OPT\u2212 \u03b3.\nFor the upper bound, we can use (19) for the optimal policy \u03c0\u0302 for \u02c6OPT 2\u03b3 . Then, using (17) and (18),\n\u02c6OPT 2\u03b3 \u2264 OPT3\u03b3 + \u03b3 \u2264 OPT+ 6\u03b3(OPT\nB + 1) + \u03b3.\nCombining, this proves the desired lemma statement:\nOPT\u2212 2\u03b3 \u2264 \u02c6OPT2\u03b3 \u2264 OPT+ 7\u03b3(OPT B + 1) (20)\nWhat remains is to proof the claim in (18) and (19). We show the proof for (19), the proof for (18) is similar. Observe that for any \u03c0,\n\u2016 T0 \u2211\nt=1\n(W\u0302t \u2212W\u2217)\u22a4Xt\u03c0(Xt)\u2016\u221e \u2264 T0 \u2211\nt=1\n\u2016(W\u0302t \u2212W\u2217)\u22a4Xt\u03c0(Xt)\u2016\u221e\n\u2264 T0 \u2211\nt=1\n\u2016W\u0302t \u2212W\u2217\u2016Mt\u2016Xt\u03c0(Xt)\u2016M\u22121t\nwhere \u2016W\u0302t \u2212W\u2217\u2016Mt = maxj \u2016w\u0302tj \u2212w\u2217j\u2016Mt . Now, applying Lemma 2 to every column w\u0302tj of W\u0302t, we have that with probability 1\u2212 \u03b4 for all t,\n\u2016W\u0302t \u2212W\u2217\u2016Mt \u2264 2 \u221a m log(td/\u03b4) \u2264 2 \u221a m log(T0d/\u03b4)\nAnd, by choice of pt \u2016Xt\u03c0(Xt)\u2016M\u22121t \u2264 \u2016Xtpt\u2016M\u22121t .\nAlso, by Lemma 3, T0 \u2211\nt=1\n\u2016Xtpt\u2016M\u22121t \u2264 \u221a mT0 ln(T0)\nTherefore, substituting,\n\u2016 T0 \u2211\nt=1\n(W\u0302t \u2212W\u2217)\u22a4Xt\u03c0(Xt)\u2016\u221e \u2264 (2 \u221a m log(T0d/\u03b4))\nT0 \u2211\nt=1\n\u2016Xtpt\u2016M\u22121t\n\u2264 (2 \u221a m log(T0d/\u03b4)) \u221a mT0 ln(T0) \u2264 T0 T \u03b3"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We consider the linear contextual bandit problem with resource consumption, in addition to reward<lb>generation. In each round, the outcome of pulling an arm is a reward as well as a vector of resource<lb>consumptions. The expected values of these outcomes depend linearly on the context of that arm. The<lb>budget/capacity constraints require that the total consumption doesn\u2019t exceed the budget for each re-<lb>source. The objective is once again to maximize the total reward. This problem turns out to be a<lb>common generalization of classic linear contextual bandits (linContextual) [7, 16, 1], bandits with knap-<lb>sacks (BwK) [3, 10], and the online stochastic packing problem (OSPP) [4, 19]. We present algorithms<lb>with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the<lb>unstructured version of the problem [5, 11] where the relation between the contexts and the outcomes<lb>could be arbitrary, but the algorithm only competes against a fixed set of policies accessible through<lb>an optimization oracle. We combine techniques from the work on linContextual, BwK and OSPP in a<lb>nontrivial manner while also tackling new difficulties that are not present in any of these special cases.", "creator": "LaTeX with hyperref package"}}}