{"id": "1508.05154", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2015", "title": "Posterior calibration and exploratory analysis for natural language processing models", "abstract": "many models in query language study define probabilistic distributions towards linguistic aspects. analysis show whether ( 1 ) the real knowledge a person's truth distribution data ultimately should avoid directly evaluated, as to method simulations determine certain empirical frequencies, and ( 2 ) nlp uncertainty can effectively projected not fundamentally represent biological components, but also involves exploratory significance points, such a difference when others trust and not trust the expected representation. we present a solution to test calibration, will apply measurements when compare the two or several commonly used models. we also contribute a coreference survey algorithm that essentially investigate confidence intervals for a political event extraction task.", "histories": [["v1", "Fri, 21 Aug 2015 00:25:51 GMT  (126kb,D)", "https://arxiv.org/abs/1508.05154v1", "12 pages (including supplementary information) in EMNLP 2015"], ["v2", "Wed, 2 Sep 2015 17:26:24 GMT  (126kb,D)", "http://arxiv.org/abs/1508.05154v2", "15 pages (including supplementary information), proceedings of EMNLP 2015"]], "COMMENTS": "12 pages (including supplementary information) in EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["khanh nguyen", "brendan o'connor"], "accepted": true, "id": "1508.05154"}, "pdf": {"name": "1508.05154.pdf", "metadata": {"source": "CRF", "title": "Posterior calibration and exploratory analysis for natural language processing models", "authors": ["Khanh Nguyen", "Brendan O\u2019Connor"], "emails": ["kxnguyen@cs.umd.edu", "brenocon@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "Natural language processing systems are imperfect. Decades of research have yielded analyzers that mis-identify named entities, mis-attach syntactic relations, and mis-recognize noun phrase coreference anywhere from 10-40% of the time. But these systems are accurate enough so that their outputs can be used as soft, if noisy, indicators of language meaning for use in downstream analysis, such as systems that perform question answering, machine translation, event extraction, and narrative analysis (McCord et al., 2012; Gimpel and Smith, 2008; Miwa et al., 2010; Bamman et al., 2013).\nTo understand the performance of an analyzer, researchers and practitioners typically measure the accuracy of individual labels or edges among a single predicted output structure y, such as a most-probable tagging or entity clustering argmaxy P (y|x) (conditional on text data x).\n1This is the extended version of a paper published in Proceedings of EMNLP 2015. This version includes acknowledgments and an appendix. For all materials, see: http: //brenocon.com/nlpcalib/\nBut a probabilistic model gives a probability distribution over many other output structures that have smaller predicted probabilities; a line of work has sought to control cascading pipeline errors by passing on multiple structures from earlier stages of analysis, by propagating prediction uncertainty through multiple samples (Finkel et al., 2006), K-best lists (Venugopal et al., 2008; Toutanova et al., 2008), or explicitly diverse lists (Gimpel et al., 2013); often the goal is to marginalize over structures to calculate and minimize an expected loss function, as in minimum Bayes risk decoding (Goodman, 1996; Kumar and Byrne, 2004), or to perform joint inference between early and later stages of NLP analysis (e.g. Singh et al., 2013; Durrett and Klein, 2014).\nThese approaches should work better when the posterior probabilities of the predicted linguistic structures reflect actual probabilities of the structures or aspects of the structures. For example, say a model is overconfident: it places too much probability mass in the top prediction, and not enough in the rest. Then there will be little benefit to using the lower probability structures, since in the training or inference objectives they will be incorrectly outweighed by the top prediction (or in a sampling approach, they will be systematically undersampled and thus have too-low frequencies). If we only evaluate models based on their top predictions or on downstream tasks, it is difficult to diagnose this issue.\nInstead, we propose to directly evaluate the calibration of a model\u2019s posterior prediction distribution. A perfectly calibrated model knows how often it\u2019s right or wrong; when it predicts an event with 80% confidence, the event empirically turns out to be true 80% of the time. While perfect accuracy for NLP models remains an unsolved challenge, perfect calibration is a more achievable goal, since a model that has imperfect accuracy could, in principle, be perfectly calibrated. In this paper, we develop a method to empirically analyze calibration that is appropriate for NLP models (\u00a73)\nar X\niv :1\n50 8.\n05 15\n4v 2\n[ cs\n.C L\n] 2\nS ep\n2 01\n5\nand use it to analyze common generative and discriminative models for tagging and classification (\u00a74).\nFurthermore, if a model\u2019s probabilities are meaningful, that would justify using its probability distributions for any downstream purpose, including exploratory analysis on unlabeled data. In \u00a76 we introduce a representative corpus exploration problem, identifying temporal event trends in international politics, with a method that is dependent on coreference resolution. We develop a coreference sampling algorithm (\u00a75.2) which projects uncertainty into the event extraction, inducing a posterior distribution over event frequencies. Sometimes the event trends have very high posterior variance (large confidence intervals),2 reflecting when the NLP system genuinely does not know the correct semantic extraction. This highlights an important use of a calibrated model: being able to tell a user when the model\u2019s predictions are likely to be incorrect, or at least, not giving a user a false sense of certainty from an erroneous NLP analysis."}, {"heading": "2 Definition of calibration", "text": "Consider a binary probabilistic prediction problem, which consists of binary labels and probabilistic predictions for them. Each instance has a ground-truth label y \u2208 {0, 1}, which is used for evaluation. The prediction problem is to generate a predicted probability or prediction strength q \u2208 [0, 1]. Typically, we use some form of a probabilistic model to accomplish this task, where q represents the model\u2019s posterior probability3 of the instance having a positive label (y = 1).\nLet S = {(q1, y1), (q2, y2), \u00b7 \u00b7 \u00b7 (qN , yN )} be the set of prediction-label pairs produced by the model. Many metrics assess the overall quality of how well the predicted probabilities match the data, such as the familiar cross entropy (negative average log-likelihood),\nL`(~y, ~q) = 1\nN \u2211 i yi log 1 qi + (1\u2212 yi) log 1 1\u2212 qi\nor mean squared error, also known as the Brier score when y is binary (Brier, 1950),\nL2(~y, ~q) = 1\nN \u2211 i (yi \u2212 qi)2\n2We use the terms confidence interval and credible interval interchangeably in this work; the latter term is debatably more correct, though less widely familiar.\n3Whether q comes from a Bayesian posterior or not is irrelevant to the analysis in this section. All that matters is that predictions are numbers q \u2208 [0, 1].\nBoth tend to attain better (lower) values when q is near 1 when y = 1, and near 0 when y = 0; and they achieve a perfect value of 0 when all qi = yi.4\nLet P(y, q) be the joint empirical distribution over labels and predictions. Under this notation, L2 = Eq,y[y \u2212 q]2. Consider the factorization\nP(y, q) = P(y | q) P(q)\nwhere P(y | q) denotes the label empirical frequency, conditional on a prediction strength (Murphy and Winkler, 1987).5 Applying this factorization to the Brier score leads to the calibrationrefinement decomposition (DeGroot and Fienberg, 1983), in terms of expectations with respect to the prediction strength distribution P(q):\nL2 = Eq[q \u2212 pq]2\ufe38 \ufe37\ufe37 \ufe38 Calibration MSE + Eq[pq(1\u2212 pq)]\ufe38 \ufe37\ufe37 \ufe38 Refinement\n(1)\nwhere we denote pq \u2261 P(y = 1 | q) for brevity. Here, calibration measures to what extent a model\u2019s probabilistic predictions match their corresponding empirical frequencies. Perfect calibration is achieved when P(y = 1 | q) = q for all q; intuitively, if you aggregate all instances where a model predicted q, they should have y = 1 at q percent of the time. We define the magnitude of miscalibration using root mean squared error: Definition 1 (RMS calibration error).\nCalibErr = \u221a Eq[q \u2212 P(y = 1 | q)]2\nThe second term of Eq 1 refers to refinement, which reflects to what extent the model is able to separate different labels (in terms of the conditional Gini entropy pq(1 \u2212 pq)). If the prediction strengths tend to cluster around 0 or 1, the refinement score tends to be lower. The calibrationrefinement breakdown offers a useful perspective on the accuracy of a model posterior. This paper focuses on calibration.\nThere are several other ways to break down squared error, log-likelihood, and other probabilistic scoring rules.6 We use the Brier-based calibration error in this work, since unlike cross-entropy\n4These two loss functions are instances of proper scoring rules (Gneiting and Raftery, 2007; Bro\u0308cker, 2009).\n5 We alternatively refer to this as label frequency or empirical frequency. The P probabilities can be thought of as frequencies from the hypothetical population the data and predictions are drawn from. P probabilities are, definitionally speaking, completely separate from a probabilistic model that might be used to generate q predictions.\n6They all include a notion of calibration corresponding to a Bregman divergence (Bro\u0308cker, 2009); for example, crossentropy can be broken down such that KL divergence is the measure of miscalibration.\nAlgorithm 1 Estimate calibration error using adaptive binning. Input: A set of N prediction-label pairs {(q1, y1), (q2, y2), \u00b7 \u00b7 \u00b7 , (qN , yN )}. Output: Calibration error. Parameter: Target bin size \u03b2. Step 1: Sort pairs by prediction values qk in ascending order.\nStep 2: For each, assign bin label bk = \u230a k\u22121 \u03b2 \u230b + 1. Step 3: Define each bin Bi as the set of indices of pairs that have the same bin label. If the last bin has size less than \u03b2, merge it with the second-to-last bin (if one exists). Let {B1, B2, \u00b7 \u00b7 \u00b7 , BT } be the set of bins. Step 4: Calculate empirical and predicted probabilities per bin:\np\u0302i = 1 |Bi| \u2211 k\u2208Bi yk and q\u0302i = 1 |Bi| \u2211 k\u2208Bi qk\nStep 5: Calculate the calibration error as the root mean squared error per bin, weighted by bin size in case they are not uniformly sized:\nCalibErr = \u221a\u221a\u221a\u221a 1 N T\u2211 i=1 |Bi|(q\u0302i \u2212 p\u0302i)2\nit does not tend toward infinity when near probability 0; we hypothesize this could be an issue since both p and q are subject to estimation error."}, {"heading": "3 Empirical calibration analysis", "text": "From a test set of labeled data, we can analyze model calibration both in terms of the calibration error, as well as visualizing the calibration curve of label frequency versus predicted strength. However, computing the label frequencies P(y = 1|q) requires an infinite amount of data. Thus approximation methods are required to perform calibration analysis."}, {"heading": "3.1 Adaptive binning procedure", "text": "Previous studies that assess calibration in supervised machine learning models (Niculescu-Mizil and Caruana, 2005; Bennett, 2000) calculate label frequencies by dividing the prediction space into deciles or other evenly spaced bins\u2014e.g. q \u2208 [0, 0.1), q \u2208 [0.1, 0.2), etc.\u2014and then calculating the empirical label frequency in each bin. This procedure may be thought of as using a form of nonparametric regression (specifically, a regressogram; Tukey 1961) to estimate the function f(q) = P(y = 1 | q) from observed data points. But models in natural language processing give very skewed distributions of confidence scores q (many are near 0 or 1), so this procedure performs poorly, having much more variable estimates near\nAlgorithm 2 Estimate calibration error\u2019s confidence interval by sampling. Input: A set of N prediction-label pairs {(q1, y1), (q2, y2), \u00b7 \u00b7 \u00b7 , (qN , yN )}. Output: Calibration error with a 95% confidence interval. Parameter: Number of samples, S. Step 1: Calculate {p\u03021, p\u03022, \u00b7 \u00b7 \u00b7 , p\u0302T } from step 4 of Algorithm 1. Step 2: Draw S samples. For each s = 1..S,\n\u2022 For each bin i = 1..T , draw p\u0302(s)i \u223c N ( p\u0302i, \u03c3\u0302 2 i ) , where\n\u03c3\u03022i = p\u0302i(1 \u2212 p\u0302i)/|Bi|. If necessary clip to [0, 1]: p\u0302 (s) i := min(1,max(0, p\u0302 (s) i ))\n\u2022 Calculate the sample\u2019s CalibErr from using the pairs (q\u0302i, p\u0302 (s) i ) as per Step 5 of Algorithm 1.\nStep 3: Calculate the 95% confidence interval for the calibration error as:\nCalibErravg \u00b1 1.96 s\u0302error where CalibErravg and s\u0302error are the mean and the standard deviation, respectively, of the CalibErrs calculated from the samples.\nthe middle of the q distribution (Figure 1). We propose adaptive binning as an alternative. Instead of dividing the interval [0, 1] into fixed-width bins, adaptive binning defines the bins such that there are an equal number of points in each, after which the same averaging procedure is used. This method naturally gives wider bins to area with fewer data points (areas that require more smoothing), and ensures that these areas have roughly similar standard errors as those near the boundaries, since for a bin with \u03b2 number of points and empirical frequency p, the standard error is estimated by \u221a p(1\u2212 p)/\u03b2, which is bounded above by 0.5/ \u221a \u03b2. Algorithm 1 describes the procedure for estimating calibration error using adaptive binning, which can be applied to any probabilistic model that predicts posterior probabilities."}, {"heading": "3.2 Confidence interval estimation", "text": "Especially when the test set is small, estimating calibration error may be subject to error, due to uncertainty in the label frequency estimates. Since how to estimate confidence bands for nonparametric regression is an unsolved problem (Wasserman, 2006), we resort to a simple method based on the binning. We construct a binomial normal approximation for the label frequency estimate in each bin, and simulate from it; every simulation across all bins is used to construct a calibration error; these simulated calibration errors are collected to construct a normal approximation for the calibra-\ntion error estimate. Since we use bin sizes of at least \u03b2 \u2265 200 in our experiments, the central limit theorem justifies these approximations. We report all calibration errors along with their 95% confidence intervals calculated by Algorithm 2.7"}, {"heading": "3.3 Visualizing calibration", "text": "In order to better understand a model\u2019s calibration properties, we plot the pairs (p\u03021, q\u03021), (p\u03022, q\u03022), \u00b7 \u00b7 \u00b7 , (p\u0302T , q\u0302T ) obtained from the adaptive binning procedure to visualize the calibration curve of the model\u2014this visualization is known as a calibration or reliability plot. It provides finer grained insight into the calibration behavior in different prediction ranges. A perfectly calibrated curve would coincide with the y = x diagonal line. When the curve lies above the diagonal, the model is underconfident (q < pq); and when it is below the diagonal, the model is overconfident (q > pq).\nAn advantage of plotting a curve estimated from fixed-size bins, instead of fixed-width bins, is that the distribution of the points hints at the refinement aspect of the model\u2019s performance. If the points\u2019 positions tend to cluster in the bottom-left and topright corners, that implies the model is making more refined predictions."}, {"heading": "4 Calibration for classification and tagging models", "text": "Using the method described in \u00a73, we assess the quality of posterior predictions of several classification and tagging models. In all of our exper-\n7A major unsolved issue is how to fairly select the bin size. If it is too large, the curve is oversmoothed and calibration looks better than it should be; if it is too small, calibration looks worse than it should be. Bandwidth selection and cross-validation techniques may better address this problem in future work. In the meantime, visualizations of calibration curves help inform the reader of the resolution of a particular analysis\u2014if the bins are far apart, the data is sparse, and the specific details of the curve are not known in those regions.\niments, we set the target bin size in Algorithm 1 to be 5,000 and the number of samples in Algorithm 2 to be 10,000."}, {"heading": "4.1 Naive Bayes and logistic regression", "text": ""}, {"heading": "4.1.1 Introduction", "text": "Previous work on Naive Bayes has found its probabilities to have calibration issues, in part due to its incorrect conditional independence assumptions (Niculescu-Mizil and Caruana, 2005; Bennett, 2000; Domingos and Pazzani, 1997). Since logistic regression has the same log-linear representational capacity (Ng and Jordan, 2002) but does not suffer from the independence assumptions, we select it for comparison, hypothesizing it may have better calibration.\nWe analyze a binary classification task of Twitter sentiment analysis from emoticons. We collect a dataset consisting of tweets identified by the Twitter API as English, collected from 2014 to 2015, with the \u201cemoticon trick\u201d (Read, 2005; Lin and Kolcz, 2012) to label tweets that contain at least one occurrence of the smiley emoticon \u201c:)\u201d as \u201chappy\u201d (y = 1) and others as y = 0. The smiley emoticons are deleted in positive examples. We sampled three sets of tweets (subsampled from the Decahose/Gardenhose stream of public tweets) with Jan-Apr 2014 for training, May-Dec 2014 for development, and Jan-Apr 2015 for testing. Each set contains 105 tweets, split between an equal number of positive and negative instances. We use binary features based on unigrams extracted from the twokenize.py8 tokenization. We use the scikit-learn (Pedregosa et al., 2011) implementations of Bernoulli Naive Bayes and L2-regularized logistic regression. The models\u2019 hyperparameters (Naive Bayes\u2019 smoothing paramter and logistic regression\u2019s regularization strength) are chosen to\n8https://github.com/myleott/ ark-twokenize-py\nmaximize the F-1 score on the development set."}, {"heading": "4.1.2 Results", "text": "Naive Bayes attains a slightly higher F-1 score (NB 73.8% vs. LR 72.9%), but logistic regression has much lower calibration error: less than half as much RMSE (NB 0.105 vs. LR 0.041; Figure 2). Both models have a tendency to be underconfident in the lower prediction range and overconfident in the higher range, but the tendency is more pronounced for Naive Bayes."}, {"heading": "4.2 Hidden Markov models and conditional random fields", "text": ""}, {"heading": "4.2.1 Introduction", "text": "Hidden Markov models (HMM) and linear chain conditional random fields (CRF) are another commonly used pair of analogous generative and discriminative models. They both define a posterior over tag sequences P (y|x), which we apply to part-of-speech tagging.\nWe can analyze these models in the binary calibration framework (\u00a72-3) by looking at marginal distribution of binary-valued outcomes of parts of the predicted structures. Specifically, we examine calibration of predicted probabilities of individual tokens\u2019 tags (\u00a74.2.2), and of pairs of consecutive tags (\u00a74.2.3). These quantities are calculated with the forward-backward algorithm.\nTo prepare a POS tagging dataset, we extract Wall Street Journal articles from the English CoNLL-2011 coreference shared task dataset from Ontonotes (Pradhan et al., 2011), using the CoNLL-2011 splits for training, development and testing. This results in 11,772 sentences for training, 1,632 for development, and 1,382 for testing, over a set of 47 possible tags.\nWe train an HMM with Dirichlet MAP using one pseudocount for every transition and word emission. For the CRF, we use the L2regularized L-BFGS algorithm implemented in\nCRFsuite (Okazaki, 2007). We compare an HMM to a CRF that only uses basic transition (tag-tag) and emission (tag-word) features, so that it does not have an advantage due to more features. In order to compare models with similar task performance, we train the CRF with only 3000 sentences from the training set, which yields the same accuracy as the HMM (about 88.7% on the test set). In each case, the model\u2019s hyperparameters (the CRF\u2019s L2 regularizer, the HMM\u2019s pseudocount) are selected by maximizing accuracy on the development set."}, {"heading": "4.2.2 Predicting single-word tags", "text": "In this experiment, we measure miscalibration of the two models on predicting tags of single words. First, for each tag type, we produce a set of 33,306 prediction-label pairs (for every token); we then concatenate them across the tags for calibration analysis. Figure 3 shows that the two models exhibit distinct calibration patterns. The HMM tends to be very underconfident whereas the CRF is overconfident, and the CRF has a lower (better) overall calibration error.\nWe also examine the calibration errors of the individual POS tags (Figure 4(a)). We find that CRF is significantly better calibrated than HMM in most but not all categories (39 out of 47). For example, they are about equally calibrated on predicting the NN tag. The calibration gap between the two models also differs among the tags."}, {"heading": "4.2.3 Predicting two-consecutive-word tags", "text": "There is no reason to restrict ourselves to model predictions of single words; these models define marginal distributions over larger textual units. Next we examine the calibration of posterior predictions of tag pairs on two consecutive words in the test set. The same analysis may be important for, say, phrase extraction or other chunking/parsing tasks.\n0.000\n0.025\n0.050\n0.075\nNN IN NN P DT JJ Ave rag\ne ( 5)\nAve rag\ne ( all)\nLabel\nC al\nib E\nrr\nHMM CRF\n(a)\nWe report results for the top 5 and 100 most frequent tag pairs (Figure 4(b)). We observe a similar pattern as seen from the experiment on single tags: the CRF is generally better calibrated than the HMM, but the HMM does achieve better calibration errors in 29 out of 100 categories.\nThese tagging experiments illustrate that, depending on the application, different models can exhibit different levels of calibration."}, {"heading": "5 Coreference resolution", "text": "We examine a third model, a probabilistic model for within-document noun phrase coreference, which has an efficient sampling-based inference procedure. In this section we introduce it and analyze its calibration, in preparation for the next section where we use it for exploratory data analysis."}, {"heading": "5.1 Antecedent selection model", "text": "We use the Berkeley coreference resolution system (Durrett and Klein, 2013), which was originally presented as a CRF; we give it an equivalent a series of independent logistic regressions (see appendix for details). The primary component of this model is a locally-normalized log-linear distribution over clusterings of noun phrases, each cluster denoting an entity. The model takes a fixed input of N mentions (noun phrases), indexed by i in their positional order in the document. It posits that every mention i has a latent antecedent selection decision, ai \u2208 {1, . . . , i\u2212 1, NEW}, denoting\nwhich previous mention it attaches to, or NEW if it is starting a new entity that has not yet been seen at a previous position in the text. Such a mentionmention attachment indicates coreference, while the final entity clustering includes more links implied through transitivity. The model\u2019s generative process is:\nDefinition 2 (Antencedent coreference model and sampling algorithm).\n\u2022 For i = 1..N , sample ai \u223c 1Zi exp(w Tf(i, ai, x)) \u2022 Calculate the entity clusters as e := CC(a), the connected components of the antecedent graph having edges (i, ai) for i where ai 6= NEW.\nHere x denotes all information in the document that is conditioned on for log-linear features f . e = {e1, ...eM} denotes the entity clusters, where each element is a set of mentions. There areM entity clusters corresponding to the number of connected components in a. The model defines a joint distribution over antecedent decisions P (a|x) =\u220f\ni P (ai|x); it also defines a joint distribution over entity clusterings P (e|x), where the probability of an e is the sum of the probabilities of all a vectors that could give rise to it. In a manner similar to a distance-dependent Chinese restaurant process (Blei and Frazier, 2011), it is non-parametric in the sense that the number of clusters M is not fixed in advance."}, {"heading": "5.2 Sampling-based inference", "text": "For both calibration analysis and exploratory applications, we need to analyze the posterior distribution over entity clusterings. This distribution is a complex mathematical object; an attractive approach to analyze it is to draw samples from this distribution, then analyze the samples.\nThis antecedent-based model admits a very straightforward procedure to draw independent e samples, by stepping through Def. 2: independently sample each ai then calculate the connected components of the resulting antecedent graph. By construction, this procedure samples from the joint distribution of e (even though we never compute the probability of any single clustering e).\nUnlike approximate sampling approaches, such as Markov chain Monte Carlo methods used in other coreference work to sample e (Haghighi and Klein, 2007), here there are no questions about burn-in or autocorrelation (Kass et al., 1998). Every sample is independent and very fast to\ncompute\u2014only slightly slower than calculating the MAP assignment (due to the exp and normalization for each ai). We implement this algorithm by modifying the publicly available implementation from Durrett and Klein.9"}, {"heading": "5.3 Calibration analysis", "text": "We consider the following inference query: for a randomly chosen pair of mentions, are they coreferent? Even if the model\u2019s accuracy is comparatively low, it may be the case that it is correctly calibrated\u2014if it thinks there should be great variability in entity clusterings, it may be uncertain whether a pair of mentions should belong together.\nLet `ij be 1 if the mentions i and j are predicted to be coreferent, and 0 otherwise. Annotated data defines a gold-standard `(g)ij value for every pair i, j. Any probability distribution over e defines a marginal Bernoulli distribution for every proposition `ij , marginalizing out e:\nP (`ij = 1 | x) = \u2211 e 1{(i, j) \u2208 e}P (e | x) (2)\nwhere (i, j) \u2208 e is true iff there is an entity in e that contains both i and j.\nIn a traditional coreference evaluation of the best-prediction entity clustering, the model assigns 1 or 0 to every `ij and the pairwise precision and recall can be computed by comparing them to the corresponding `(g)ij . Here, we instead compare the qij \u2261 P (`ij = 1 | x, e) prediction strengths against `(g)ij empirical frequencies to assess pairwise calibration, with the same binary calibration analysis tools developed in \u00a73 by aggregating pairs with similar qij values. Each qij is computed by averaging over 1,000 samples, simply taking the fraction of samples where the pair (i, j) is coreferent.\n9Berkeley Coreference Resolution System, version 1.1: http://nlp.cs.berkeley.edu/projects/ coref.shtml\nWe perform this analysis on the development section of the English CoNLL-2011 data (404 documents). Using the sampling inference method discussed in \u00a75.2, we compute 4.3 millions prediction-label pairs and measure their calibration error. Our result shows that the model produces very well-calibrated predictions with less than 1% CalibErr (Figure 5), though slightly overconfident on middle to high-valued predictions. The calibration error indicates that it is the most calibrated model we examine within this paper. This result suggests we might be able to trust its level of uncertainty."}, {"heading": "6 Uncertainty in Entity-based Exploratory Analysis", "text": ""}, {"heading": "6.1 Entity-syntactic event aggregation", "text": "We demonstrate one important use of calibration analysis: to ensure the usefulness of propagating uncertainty from coreference resolution into a system for exploring unannotated text. Accuracy cannot be calculated since there are no labels; but if the system is calibrated, we postulate that uncertainty information can help users understand the underlying reliability of aggregated extractions and isolate predictions that are more likely to contain errors.\nWe illustrate with an event analysis application to count the number of \u201ccountry attack events\u201d: for a particular country of the world, how many news articles describe an entity affiliated with that country as the agent of an attack, and how does this number change over time? This is a simplified version of a problem where such systems have been built and used for political science analysis (Schrodt et al., 1994; Schrodt, 2012; Leetaru and Schrodt, 2013; Boschee et al., 2013; O\u2019Connor et al., 2013). A coreference component can improve extraction coverage in cases such as \u201cRussian troops were sighted . . . and they attacked . . . \u201d\nWe use the coreference system examined in \u00a75 for this analysis. To propagate coreference uncertainty, we re-run event extraction on multiple coreference samples generated from the algorithm described in \u00a75.2, inducing a posterior distribution over the event counts. To isolate the effects of coreference, we use a very simple syntactic dependency system to identify affiliations and events. Assume the availability of dependency parses for a document d, a coreference resolution e, and a lexicon of country names, which contains a small set of words w(c) for each country c; for example, w(FRA) = {france, french}. The binary function\nf(c, e;xd) assesses whether an entity e is affiliated with country c and is described as the agent of an attack, based on document text and parses xd; f returns true iff both:10\n\u2022 There exists a mention i \u2208 e described as country c: either its head word is in w(c) (e.g. \u201cAmericans\u201d), or its head word has an nmod or amod modifier in w(c) (e.g. \u201cAmerican forces\u201d, \u201cpresident of the U.S.\u201d); and there is only one unique country c among the mentions in the entity.\n\u2022 There exists a mention j \u2208 e which is the nsubj or agent argument to the verb \u201cattack\u201d (e.g. \u201cthey attacked\u201d, \u201cthe forces attacked\u201d, \u201cattacked by them\u201d).\nFor a given c, we first calculate a binary variable for whether there is at least one entity fulfilling f in a particular document,\na(d, c, ed) = \u2228 e\u2208ed f(c, e;xd) (3)\nand second, the number of such documents in d(t), the set of New York Times articles published in a given time period t,\nn(t, c, ed(t)) = \u2211\nd\u2208d(t)\na(d, c, ed) (4)\nThese quantities are both random variables, since they depend on e; thus we are interested in the posterior distribution of n, marginalizing out e,\nP (n(t, c, ed(t)) | xd(t)) (5)\nIf our coreference model was highly certain (only one structure, or a small number of similar structures, had most of the probability mass in the space of all possible structures), each document would have an a posterior near either 0 or 1, and their sum in Eq. 5 would have a narrow distribution. But if the model is uncertain, the distribution will be wider. Because of the transitive closure, the probability of a is potentially more complex than the single antecedent linking probability between two mentions\u2014the affiliation and attack information can propagate through a long coreference chain."}, {"heading": "6.2 Results", "text": "We tag and parse a 193,403 article subset of the Annotated New York Times LDC corpus (Sandhaus, 2008), which includes articles about world\n10Syntactic relations are Universal Dependencies (de Marneffe et al., 2014); more details for the extraction rules are in the appendix.\nnews from the years 1987 to 2007 (details in appendix). For each article, we run the coreference system to predict 100 samples, and evaluate f on every entity in every sample.11 The quantity of interest is the number of articles mentioning attacks in a 3-month period (quarter), for a given country. Figure 6 illustrates the mean and 95% posterior credible intervals for each quarter. The posterior mean m is calculated as the mean of the samples, and the interval is the normal approximation m\u00b1 1.96 s, where s is the standard deviation among samples for that country and time period.\nUncertainty information helps us understand whether a difference between data points is real. In the plots of Figure 6, if we had used a 1-best coreference resolution, only a single line would be shown, with no assessment of uncertainty. This is problematic in cases when the model genuinely does not know the correct answer. For example, the 1993-1996 period of the USA plot (Figure 6, top) shows the posterior mean fluctuating from 1 to 5 documents; but when credible intervals are taken into consideration, we see that model does not know whether the differences are real, or were caused by coreference noise.\nA similar case is highlighted at the bottom plot of Figure 6. Here we compare the event counts for Yugoslavia and NATO, which were engaged in a conflict in 1999. Did the New York Times devote more attention to the attacks by one particular side? To a 1-best system, the answer would be yes. But the posterior intervals for the two countries\u2019 event counts in mid-1999 heavily overlap, indicating that the coreference system introduces too much uncertainty to obtain a conclusive answer for this question. Note that calibration of the coreference model is important for the credible intervals to be useful; for example, if the model was badly calibrated by being overconfident (too much probability over a small set of similar structures), these intervals would be too narrow, leading to incorrect interpretations of the event dynamics.\nVisualizing this uncertainty gives richer information for a potential user of an NLP-based system, compared to simply drawing a line based on a single 1-best prediction. It preserves the genuine uncertainty due to ambiguities the system was unable to resolve. This highlights an alternative use of Finkel et al. (2006)\u2019s approach of sampling multiple NLP pipeline components, which in that work was used to perform joint inference. Instead\n11We obtained similar results using only 10 samples. We also obtained similar results with a different query function, the total number of entities, across documents, that fulfill f .\nof focusing on improving an NLP pipeline, we can pass uncertainty on to exploratory purposes, and try to highlight to a user where the NLP system may be wrong, or where it can only imprecisely specify a quantity of interest.\nFinally, calibration can help error analysis. For a calibrated model, the more uncertain a prediction is, the more likely it is to be erroneous. While coreference errors comprise only one part of event extraction errors (alongside issues in parse quality, factivity, semantic roles, etc.), we can look at highly uncertain event predictions to understand the nature of coreference errors relative to our task. We manually analyzed documents with a 50% probability to contain an \u201cattack\u201ding countryaffiliated entity, and found difficult coreference cases.\nIn one article from late 1990, an \u201cattack\u201d event for IRQ is extracted from the sentence \u201cBut some political leaders said that they feared that Mr. Hussein might attack Saudi Arabia\u201d. The mention \u201cMr. Hussein\u201d is classified as IRQ only when it is coreferent with a previous mention \u201cPresident Saddam Hussein of Iraq\u201d; this occurs only 50% of the time, since in some posterior samples the coreference system split apart these two \u201cHussein\u201d mentions. This particular document is additionally difficult, since it includes the names of more than 10 countries (e.g. United States, Saudi Arabia, Egypt), and some of the Hussein mentions are even clustered with presidents of other countries (such as \u201cPresident Bush\u201d), presumably because they share the \u201cpresident\u201d title. These types of errors are a major issue for a political analysis task; further analysis could assess their prevalence and how to address them in future work."}, {"heading": "7 Conclusion", "text": "In this work, we argue that the calibration of posterior predictions is a desirable property of probabilistic NLP models, and that it can be directly evaluated. We also demonstrate a use case of having calibrated uncertainty: its propagation into downstream exploratory analysis.\nOur posterior simulation approach for exploratory and error analysis relates to posterior predictive checking (Gelman et al., 2013), which analyzes a posterior to test model assumptions; Mimno and Blei (2011) apply it to a topic model.\nOne avenue of future work is to investigate more effective nonparametric regression methods to better estimate and visualize calibration error, such as Gaussian processes or bootstrapped kernel\ndensity estimation. Another important question is: what types of inferences are facilitated by correct calibration? Intuitively, we think that overconfidence will lead to overly narrow confidence intervals; but in what sense are confidence intervals \u201cgood\u201d when calibration is perfect? Also, does calibration help joint inference in NLP pipelines? It may also assist calculations that rely on expectations, such as inference methods like minimum Bayes risk decoding, or learning methods like EM, since calibrated predictions imply that calculated expectations are statistically unbiased (though the implications of this fact may be subtle). Finally, it may be interesting to pursue recalibration methods, which readjust a non-calibrated model\u2019s predictions to be calibrated; recalibration methods have been developed for binary (Platt, 1999; Niculescu-Mizil and Caruana, 2005) and multiclass (Zadrozny and Elkan, 2002) classification settings, but we are unaware of methods appropriate for the highly structured outputs typical in linguistic analysis. Another approach might be to directly constrain CalibErr = 0 during training, or try to reduce it as a training-time risk minimization or cost objective (Smith and Eisner, 2006; Gimpel and Smith, 2010; Stoyanov et al., 2011; Bru\u0308mmer and Doddington, 2013).\nCalibration is an interesting and important property of NLP models. Further work is necessary to address these and many other questions."}, {"heading": "Acknowledgments", "text": "Thanks to Erik Learned-Miller, Benjamin Marlin, Craig Greenberg, Phan-Minh Nguyen, Caitlin Cellier and the CMU ARK Lab for discussion and comments, and to the anonymous reviewers (especially R3) for helpful suggestions."}, {"heading": "1 Sampling a deterministic function of a random variable", "text": "In several places in this paper, we define probability distributions over deterministic functions of a random variable, and sample from them by applying the deterministic function to samples of the random variable. This should be valid by construction, but we supply the following argument for further justification. X is a random variable and g(x) is a deterministic function which takes a value of X as its input. Since g depends on a random variable, g(X) is a random variable as well. The distribution for g(X), or aspects of it (such as a PMF or independent samples from it) can be calculated by marginalizing out X with a Monte Carlo approximation. Assuming g has discrete outputs (as is the case for the event counting function n, or connected components function CC), we examine the probability mass function:\npmf(h) \u2261 P (g(X) = h) (6) = \u2211 x P (g(x) = h | x) P (x) (7)\n= \u2211 x 1{g(x) = h}P (x) (8) \u2248 1 S \u2211 x\u223cP (X) 1{g(x) = h} (9)\nEq. 8 holds because g(x) is a deterministic function, and Eq. 9 is a Monte Carlo approximation that uses S samples from P (x).\nThis implies that a set of g values calculated on x samples, {g(x(s)) : x(s) \u223c P (x)}, should constitute a sample from the distribution P (g(X)); in our event analysis section we usually call this the \u201cposterior\u201d distribution of g(X) (the n(t, c) function there). In our setting, we do not directly use the PMF calculation above; instead, we construct normal approximations to the probability distribution g(X).\nWe use this technique in several places. For the calibration error confidence interval, the calibration error is a deterministic function of the uncertain empirical label frequencies pi; there, we propagate posterior uncertainty from a normal approximation to the Bernoulli parameter\u2019s posterior (the pi distribution under the central limit theorem) through simulation. In the coreference model, the\nconnected components function is a deterministic function of the antecedent vector; thus repeatedly calculating e(s) := CC(a(s)) yields samples of entity clusterings from their posterior. For the event analysis, the counting function n(t, c, ed(t)) is a function of the entity samples, and thus can be recalculated on each\u2014this is a multiple step deterministic pipeline, which postprocesses simulated random variables.\nAs in other Monte Carlo-based inference techniques (as applied to both Bayesian and frequentist (e.g. bootstrapping) inference), the mean and standard deviation of samples drawn from the distribution constitute the mean and standard deviation of the desired posterior distribution, subject to Monte Carlo error due to the finite number of samples, which by the central limit theorem shrinks at a rate of 1/ \u221a S. The Monte Carlo standard error for estimating the mean is \u03c3/ \u221a S where \u03c3 is the standard deviation. So with 100 samples, the Monte Carlo standard error for the mean is \u221a 100 = 10 times smaller than standard deviation. Thus in the time series graphs, which are based on S = 100 samples, the posterior mean (dark line) has Monte Carlo uncertainty that is 10 times smaller than the vertical gray area (95% CI) around it."}, {"heading": "2 Normalization in the coreference model", "text": "Durrett and Klein (2013) present their model as a globally normalized, but fully factorized, CRF:\nP (a|x) = 1 Z \u220f i exp(wTf(i, ai, x))\nSince the factor function decomposes independently for each random variable ai, their probabilities are actually independent, and can be rewritten with local normalization,\nP (a|x) = \u220f i 1 Zi exp(wTf(i, ai, x))\nThis interpretation justifies the use of independent sampling to draw samples of the joint posterior."}, {"heading": "3 Event analysis: Corpus selection, country affiliation, and parsing", "text": "Articles are filtered to yield a dataset about world news. In the New York Times Annotated Corpus, every article is tagged with a large set of labels. We include articles that contain a category whose label starts with the string Top/News/World, and exclude articles with any category matching the\nregex /(Sports|Opinion), and whose text body contains a mention of at least one country name.\nCountry names are taken from the dictionary country igos.txt based on previous work (http: //brenocon.com/irevents/). Country name matching is case insensitive and uses light stemming: when trying to match a word against the lexicon, if a match is not found, it backs off to stripping the last and last two characters. (This is usually unnecessary since the dictionary contains modifier forms.)\nPOS, NER, and constituent and dependency parses are produced with Stanford CoreNLP 3.5.2 with default settings except for one change, to use its shift-reduce constituent parser (for convenience of processing speed). We treat tags and parses as fixed and leave their uncertainty propagation for future work.\nWhen formulating the extraction rules, we examined frequencies of all syntactic dependencies within country-affiliated entities, in order to help find reasonably high-coverage syntactic relations for the \u201cattack\u201d rule."}, {"heading": "4 Event time series graphs", "text": "The following pages contain posterior time series graphs for 20 countries, as described in the section on coreference-based event aggregation, in order of decreasing total event frequency. As in the main paper, the blue line indicates the posterior mean, and the gray region indicates 95% posterior credibility intervals, with count aggregation at the monthly level. The titles are ISO3 country codes.\n1990 1995 2000 2005\n0 10\n20 30\nUSA\n0 10\n20 30\n1990 1995 2000 2005\n0 10\n20 30\n40\nIRQ\n0 10\n20 30\n40\n1990 1995 2000 2005\n0 5\n10 20\nISR\n0 5\n10 20\n1990 1995 2000 2005\n0 2\n4 6\n8\nGBR\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 2\n4 6\n8\nPSE\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 10\n20 30\nIRN\n0 10\n20 30\n1990 1995 2000 2005\n0 5\n10 20\nSRB\n0 5\n10 20\n1990 1995 2000 2005\n0 2\n4 6\n8\nRUS\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 2\n4 6\n8\nCHN\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 2\n4 6\n8\nFRA\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 2\n4 6\n8\nDEU\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 2\n4 6\n8\nIND\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 2\n4 6\n8 AFG\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 5\n10 15\nIGONAT\n0 5\n10 15\n1990 1995 2000 2005\n0 2\n4 6\n8\nJPN\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 2\n4 6\n8\nKWT\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 2\n4 6\n8\nPAK\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 2\n4 6\n8\nHRV\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 2\n4 6\n8\nLBN\n0 2\n4 6\n8\n1990 1995 2000 2005\n0 2\n4 6\n8\nSYR\n0 2\n4 6\n8"}], "references": [{"title": "Learning latent personas of film characters", "author": ["David Bamman", "Brendan O\u2019Connor", "Noah A. Smith"], "venue": "In Proceedings of ACL,", "citeRegEx": "Bamman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bamman et al\\.", "year": 2013}, {"title": "Assessing the calibration of naive Bayes\u2019 posterior estimates", "author": ["Paul N. Bennett"], "venue": "Technical report,", "citeRegEx": "Bennett.,? \\Q2000\\E", "shortCiteRegEx": "Bennett.", "year": 2000}, {"title": "Distance dependent Chinese restaurant processes", "author": ["David M. Blei", "Peter I. Frazier"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Blei and Frazier.,? \\Q2011\\E", "shortCiteRegEx": "Blei and Frazier.", "year": 2011}, {"title": "Automatic extraction of events from open source text for predictive forecasting", "author": ["Elizabeth Boschee", "Premkumar Natarajan", "Ralph Weischedel"], "venue": "Handbook of Computational Approaches to Counterterrorism,", "citeRegEx": "Boschee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boschee et al\\.", "year": 2013}, {"title": "Verification of forecasts expressed in terms of probability", "author": ["Glenn W. Brier"], "venue": "Monthly weather review,", "citeRegEx": "Brier.,? \\Q1950\\E", "shortCiteRegEx": "Brier.", "year": 1950}, {"title": "Reliability, sufficiency, and the decomposition of proper scores", "author": ["Jochen Br\u00f6cker"], "venue": "Quarterly Journal of the Royal Meteorological Society,", "citeRegEx": "Br\u00f6cker.,? \\Q2009\\E", "shortCiteRegEx": "Br\u00f6cker.", "year": 2009}, {"title": "Likelihood-ratio calibration using priorweighted proper scoring rules", "author": ["Niko Br\u00fcmmer", "George Doddington"], "venue": "arXiv preprint arXiv:1307.7981,", "citeRegEx": "Br\u00fcmmer and Doddington.,? \\Q2013\\E", "shortCiteRegEx": "Br\u00fcmmer and Doddington.", "year": 2013}, {"title": "The comparison and evaluation of forecasters", "author": ["Morris H. DeGroot", "Stephen E. Fienberg"], "venue": "The statistician,", "citeRegEx": "DeGroot and Fienberg.,? \\Q1983\\E", "shortCiteRegEx": "DeGroot and Fienberg.", "year": 1983}, {"title": "On the optimality of the simple Bayesian classifier under zero-one loss", "author": ["Pedro Domingos", "Michael Pazzani"], "venue": "Machine learning,", "citeRegEx": "Domingos and Pazzani.,? \\Q1997\\E", "shortCiteRegEx": "Domingos and Pazzani.", "year": 1997}, {"title": "Easy victories and uphill battles in coreference resolution", "author": ["Greg Durrett", "Dan Klein"], "venue": "In EMNLP, pages 1971\u20131982,", "citeRegEx": "Durrett and Klein.,? \\Q2013\\E", "shortCiteRegEx": "Durrett and Klein.", "year": 2013}, {"title": "A joint model for entity analysis: Coreference", "author": ["Greg Durrett", "Dan Klein"], "venue": "typing, and linking. Transactions of the Association for Computational Linguistics,", "citeRegEx": "Durrett and Klein.,? \\Q2014\\E", "shortCiteRegEx": "Durrett and Klein.", "year": 2014}, {"title": "Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines", "author": ["Jenny Rose Finkel", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods", "citeRegEx": "Finkel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2006}, {"title": "Bayesian data analysis", "author": ["Andrew Gelman", "John B. Carlin", "Hal S. Stern", "David B. Dunson", "Aki Vehtari", "Donald B. Rubin"], "venue": "Chapman and Hall/CRC,", "citeRegEx": "Gelman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 2013}, {"title": "Rich sourceside context for statistical machine translation", "author": ["Kevin Gimpel", "Noah A. Smith"], "venue": "In Proceedings of the Third Workshop on Statistical Machine Translation,", "citeRegEx": "Gimpel and Smith.,? \\Q2008\\E", "shortCiteRegEx": "Gimpel and Smith.", "year": 2008}, {"title": "Softmaxmargin CRFs: Training log-linear models with cost functions", "author": ["Kevin Gimpel", "Noah A. Smith"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Gimpel and Smith.,? \\Q2010\\E", "shortCiteRegEx": "Gimpel and Smith.", "year": 2010}, {"title": "A systematic exploration of diversity in machine translation", "author": ["Kevin Gimpel", "Dhruv Batra", "Chris Dyer", "Gregory Shakhnarovich"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Gimpel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["Tilmann Gneiting", "Adrian E. Raftery"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gneiting and Raftery.,? \\Q2007\\E", "shortCiteRegEx": "Gneiting and Raftery.", "year": 2007}, {"title": "Parsing algorithms and metrics", "author": ["Joshua Goodman"], "venue": "Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Goodman.,? \\Q1996\\E", "shortCiteRegEx": "Goodman.", "year": 1996}, {"title": "Markov chain Monte Carlo in practice: a roundtable discussion", "author": ["Robert E. Kass", "Bradley P. Carlin", "Andrew Gelman", "Radford M. Neal"], "venue": "The American Statistician,", "citeRegEx": "Kass et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kass et al\\.", "year": 1998}, {"title": "Minimum Bayes-risk decoding for statistical machine translation", "author": ["Shankar Kumar", "William Byrne"], "venue": "HLT-NAACL 2004: Main Proceedings,", "citeRegEx": "Kumar and Byrne.,? \\Q2004\\E", "shortCiteRegEx": "Kumar and Byrne.", "year": 2004}, {"title": "GDELT: Global data on events, location, and tone", "author": ["Kalev Leetaru", "Philip A. Schrodt"], "venue": "In ISA Annual Convention,", "citeRegEx": "Leetaru and Schrodt.,? \\Q1979\\E", "shortCiteRegEx": "Leetaru and Schrodt.", "year": 1979}, {"title": "Large-scale machine learning at Twitter", "author": ["Jimmy Lin", "Alek Kolcz"], "venue": "In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "Lin and Kolcz.,? \\Q2012\\E", "shortCiteRegEx": "Lin and Kolcz.", "year": 2012}, {"title": "Deep parsing in Watson", "author": ["Michael C. McCord", "J. William Murdock", "Branimir K. Boguraev"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "McCord et al\\.,? \\Q2012\\E", "shortCiteRegEx": "McCord et al\\.", "year": 2012}, {"title": "Bayesian checking for topic models", "author": ["David Mimno", "David Blei"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mimno and Blei.,? \\Q2011\\E", "shortCiteRegEx": "Mimno and Blei.", "year": 2011}, {"title": "A general framework for forecast verification", "author": ["Allan H. Murphy", "Robert L. Winkler"], "venue": "Monthly Weather Review,", "citeRegEx": "Murphy and Winkler.,? \\Q1987\\E", "shortCiteRegEx": "Murphy and Winkler.", "year": 1987}, {"title": "Predicting good probabilities with supervised learning", "author": ["Alexandru Niculescu-Mizil", "Rich Caruana"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Niculescu.Mizil and Caruana.,? \\Q2005\\E", "shortCiteRegEx": "Niculescu.Mizil and Caruana.", "year": 2005}, {"title": "Learning to extract international relations from political context", "author": ["Brendan O\u2019Connor", "Brandon Stewart", "Noah A. Smith"], "venue": "In Proceedings of ACL,", "citeRegEx": "O.Connor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2013}, {"title": "Crfsuite: a fast implementation of conditional random fields (CRFs)", "author": ["Naoaki Okazaki"], "venue": null, "citeRegEx": "Okazaki.,? \\Q2007\\E", "shortCiteRegEx": "Okazaki.", "year": 2007}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in large margin classifiers", "author": ["John Platt"], "venue": null, "citeRegEx": "Platt.,? \\Q2000\\E", "shortCiteRegEx": "Platt.", "year": 2000}, {"title": "CoNLL-2011 shared task: Modeling unrestricted coreference in Ontonotes", "author": ["Sameer Pradhan", "Lance Ramshaw", "Mitchell Marcus", "Martha Palmer", "Ralph Weischedel", "Nianwen Xue"], "venue": "In Proceedings of the Fifteenth Conference", "citeRegEx": "Pradhan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2011}, {"title": "Using emoticons to reduce dependency in machine learning techniques for sentiment classification", "author": ["Jonathon Read"], "venue": "In Proceedings of the ACL Student Research Workshop,", "citeRegEx": "Read.,? \\Q2005\\E", "shortCiteRegEx": "Read.", "year": 2005}, {"title": "The New York Times Annotated Corpus", "author": ["Evan Sandhaus"], "venue": "Linguistic Data Consortium,", "citeRegEx": "Sandhaus.,? \\Q2008\\E", "shortCiteRegEx": "Sandhaus.", "year": 2008}, {"title": "Precedents, progress, and prospects in political event data", "author": ["Philip A. Schrodt"], "venue": "International Interactions,", "citeRegEx": "Schrodt.,? \\Q2012\\E", "shortCiteRegEx": "Schrodt.", "year": 2012}, {"title": "Joint inference of entities, relations, and coreference", "author": ["Sameer Singh", "Sebastian Riedel", "Brian Martin", "Jiaping Zheng", "Andrew McCallum"], "venue": "In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction,", "citeRegEx": "Singh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2013}, {"title": "Minimum risk annealing for training log-linear models", "author": ["David A. Smith", "Jason Eisner"], "venue": "In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,", "citeRegEx": "Smith and Eisner.,? \\Q2006\\E", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}, {"title": "A global joint model for semantic role labeling", "author": ["Kristina Toutanova", "Aria Haghighi", "Christopher D. Manning"], "venue": "Computational Linguistics,", "citeRegEx": "Toutanova et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2008}, {"title": "Wider pipelines: Nbest alignments and parses in MT training", "author": ["Ashish Venugopal", "Andreas Zollmann", "Noah A. Smith", "Stephan Vogel"], "venue": "In Proceedings of AMTA,", "citeRegEx": "Venugopal et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Venugopal et al\\.", "year": 2008}, {"title": "All of nonparametric statistics", "author": ["Larry Wasserman"], "venue": "Springer Science & Business Media,", "citeRegEx": "Wasserman.,? \\Q2006\\E", "shortCiteRegEx": "Wasserman.", "year": 2006}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["Bianca Zadrozny", "Charles Elkan"], "venue": "In Proceedings of KDD,", "citeRegEx": "Zadrozny and Elkan.,? \\Q2002\\E", "shortCiteRegEx": "Zadrozny and Elkan.", "year": 2002}], "referenceMentions": [{"referenceID": 22, "context": "But these systems are accurate enough so that their outputs can be used as soft, if noisy, indicators of language meaning for use in downstream analysis, such as systems that perform question answering, machine translation, event extraction, and narrative analysis (McCord et al., 2012; Gimpel and Smith, 2008; Miwa et al., 2010; Bamman et al., 2013).", "startOffset": 265, "endOffset": 350}, {"referenceID": 13, "context": "But these systems are accurate enough so that their outputs can be used as soft, if noisy, indicators of language meaning for use in downstream analysis, such as systems that perform question answering, machine translation, event extraction, and narrative analysis (McCord et al., 2012; Gimpel and Smith, 2008; Miwa et al., 2010; Bamman et al., 2013).", "startOffset": 265, "endOffset": 350}, {"referenceID": 0, "context": "But these systems are accurate enough so that their outputs can be used as soft, if noisy, indicators of language meaning for use in downstream analysis, such as systems that perform question answering, machine translation, event extraction, and narrative analysis (McCord et al., 2012; Gimpel and Smith, 2008; Miwa et al., 2010; Bamman et al., 2013).", "startOffset": 265, "endOffset": 350}, {"referenceID": 11, "context": "com/nlpcalib/ But a probabilistic model gives a probability distribution over many other output structures that have smaller predicted probabilities; a line of work has sought to control cascading pipeline errors by passing on multiple structures from earlier stages of analysis, by propagating prediction uncertainty through multiple samples (Finkel et al., 2006), K-best lists (Venugopal et al.", "startOffset": 343, "endOffset": 364}, {"referenceID": 36, "context": ", 2006), K-best lists (Venugopal et al., 2008; Toutanova et al., 2008), or explicitly diverse lists (Gimpel et al.", "startOffset": 22, "endOffset": 70}, {"referenceID": 35, "context": ", 2006), K-best lists (Venugopal et al., 2008; Toutanova et al., 2008), or explicitly diverse lists (Gimpel et al.", "startOffset": 22, "endOffset": 70}, {"referenceID": 15, "context": ", 2008), or explicitly diverse lists (Gimpel et al., 2013); often the goal is to marginalize over structures to calculate and minimize an expected loss function, as in minimum Bayes risk decoding (Goodman, 1996; Kumar and Byrne, 2004), or to perform joint inference between early and later stages of NLP analysis (e.", "startOffset": 37, "endOffset": 58}, {"referenceID": 17, "context": ", 2013); often the goal is to marginalize over structures to calculate and minimize an expected loss function, as in minimum Bayes risk decoding (Goodman, 1996; Kumar and Byrne, 2004), or to perform joint inference between early and later stages of NLP analysis (e.", "startOffset": 145, "endOffset": 183}, {"referenceID": 19, "context": ", 2013); often the goal is to marginalize over structures to calculate and minimize an expected loss function, as in minimum Bayes risk decoding (Goodman, 1996; Kumar and Byrne, 2004), or to perform joint inference between early and later stages of NLP analysis (e.", "startOffset": 145, "endOffset": 183}, {"referenceID": 10, "context": ", 2013); often the goal is to marginalize over structures to calculate and minimize an expected loss function, as in minimum Bayes risk decoding (Goodman, 1996; Kumar and Byrne, 2004), or to perform joint inference between early and later stages of NLP analysis (e.g. Singh et al., 2013; Durrett and Klein, 2014).", "startOffset": 262, "endOffset": 312}, {"referenceID": 4, "context": "or mean squared error, also known as the Brier score when y is binary (Brier, 1950),", "startOffset": 70, "endOffset": 83}, {"referenceID": 24, "context": "where P(y | q) denotes the label empirical frequency, conditional on a prediction strength (Murphy and Winkler, 1987).", "startOffset": 91, "endOffset": 117}, {"referenceID": 7, "context": "5 Applying this factorization to the Brier score leads to the calibrationrefinement decomposition (DeGroot and Fienberg, 1983), in terms of expectations with respect to the prediction strength distribution P(q):", "startOffset": 98, "endOffset": 126}, {"referenceID": 16, "context": "These two loss functions are instances of proper scoring rules (Gneiting and Raftery, 2007; Br\u00f6cker, 2009).", "startOffset": 63, "endOffset": 106}, {"referenceID": 5, "context": "These two loss functions are instances of proper scoring rules (Gneiting and Raftery, 2007; Br\u00f6cker, 2009).", "startOffset": 63, "endOffset": 106}, {"referenceID": 5, "context": "They all include a notion of calibration corresponding to a Bregman divergence (Br\u00f6cker, 2009); for example, crossentropy can be broken down such that KL divergence is the measure of miscalibration.", "startOffset": 79, "endOffset": 94}, {"referenceID": 25, "context": "Previous studies that assess calibration in supervised machine learning models (Niculescu-Mizil and Caruana, 2005; Bennett, 2000) calculate label frequencies by dividing the prediction space into deciles or other evenly spaced bins\u2014e.", "startOffset": 79, "endOffset": 129}, {"referenceID": 1, "context": "Previous studies that assess calibration in supervised machine learning models (Niculescu-Mizil and Caruana, 2005; Bennett, 2000) calculate label frequencies by dividing the prediction space into deciles or other evenly spaced bins\u2014e.", "startOffset": 79, "endOffset": 129}, {"referenceID": 37, "context": "Since how to estimate confidence bands for nonparametric regression is an unsolved problem (Wasserman, 2006), we resort to a simple method based on the binning.", "startOffset": 91, "endOffset": 108}, {"referenceID": 25, "context": "Previous work on Naive Bayes has found its probabilities to have calibration issues, in part due to its incorrect conditional independence assumptions (Niculescu-Mizil and Caruana, 2005; Bennett, 2000; Domingos and Pazzani, 1997).", "startOffset": 151, "endOffset": 229}, {"referenceID": 1, "context": "Previous work on Naive Bayes has found its probabilities to have calibration issues, in part due to its incorrect conditional independence assumptions (Niculescu-Mizil and Caruana, 2005; Bennett, 2000; Domingos and Pazzani, 1997).", "startOffset": 151, "endOffset": 229}, {"referenceID": 8, "context": "Previous work on Naive Bayes has found its probabilities to have calibration issues, in part due to its incorrect conditional independence assumptions (Niculescu-Mizil and Caruana, 2005; Bennett, 2000; Domingos and Pazzani, 1997).", "startOffset": 151, "endOffset": 229}, {"referenceID": 30, "context": "We collect a dataset consisting of tweets identified by the Twitter API as English, collected from 2014 to 2015, with the \u201cemoticon trick\u201d (Read, 2005; Lin and Kolcz, 2012) to label tweets that contain at least one occurrence of the smiley emoticon \u201c:)\u201d as \u201chappy\u201d (y = 1) and others as y = 0.", "startOffset": 139, "endOffset": 172}, {"referenceID": 21, "context": "We collect a dataset consisting of tweets identified by the Twitter API as English, collected from 2014 to 2015, with the \u201cemoticon trick\u201d (Read, 2005; Lin and Kolcz, 2012) to label tweets that contain at least one occurrence of the smiley emoticon \u201c:)\u201d as \u201chappy\u201d (y = 1) and others as y = 0.", "startOffset": 139, "endOffset": 172}, {"referenceID": 29, "context": "To prepare a POS tagging dataset, we extract Wall Street Journal articles from the English CoNLL-2011 coreference shared task dataset from Ontonotes (Pradhan et al., 2011), using the CoNLL-2011 splits for training, development and testing.", "startOffset": 149, "endOffset": 171}, {"referenceID": 27, "context": "CRFsuite (Okazaki, 2007).", "startOffset": 9, "endOffset": 24}, {"referenceID": 9, "context": "We use the Berkeley coreference resolution system (Durrett and Klein, 2013), which was originally presented as a CRF; we give it an equivalent a series of independent logistic regressions (see appendix for details).", "startOffset": 50, "endOffset": 75}, {"referenceID": 2, "context": "In a manner similar to a distance-dependent Chinese restaurant process (Blei and Frazier, 2011), it is non-parametric in the sense that the number of clusters M is not fixed in advance.", "startOffset": 71, "endOffset": 95}, {"referenceID": 18, "context": "Unlike approximate sampling approaches, such as Markov chain Monte Carlo methods used in other coreference work to sample e (Haghighi and Klein, 2007), here there are no questions about burn-in or autocorrelation (Kass et al., 1998).", "startOffset": 213, "endOffset": 232}, {"referenceID": 32, "context": "We illustrate with an event analysis application to count the number of \u201ccountry attack events\u201d: for a particular country of the world, how many news articles describe an entity affiliated with that country as the agent of an attack, and how does this number change over time? This is a simplified version of a problem where such systems have been built and used for political science analysis (Schrodt et al., 1994; Schrodt, 2012; Leetaru and Schrodt, 2013; Boschee et al., 2013; O\u2019Connor et al., 2013).", "startOffset": 394, "endOffset": 503}, {"referenceID": 3, "context": "We illustrate with an event analysis application to count the number of \u201ccountry attack events\u201d: for a particular country of the world, how many news articles describe an entity affiliated with that country as the agent of an attack, and how does this number change over time? This is a simplified version of a problem where such systems have been built and used for political science analysis (Schrodt et al., 1994; Schrodt, 2012; Leetaru and Schrodt, 2013; Boschee et al., 2013; O\u2019Connor et al., 2013).", "startOffset": 394, "endOffset": 503}, {"referenceID": 26, "context": "We illustrate with an event analysis application to count the number of \u201ccountry attack events\u201d: for a particular country of the world, how many news articles describe an entity affiliated with that country as the agent of an attack, and how does this number change over time? This is a simplified version of a problem where such systems have been built and used for political science analysis (Schrodt et al., 1994; Schrodt, 2012; Leetaru and Schrodt, 2013; Boschee et al., 2013; O\u2019Connor et al., 2013).", "startOffset": 394, "endOffset": 503}, {"referenceID": 31, "context": "We tag and parse a 193,403 article subset of the Annotated New York Times LDC corpus (Sandhaus, 2008), which includes articles about world", "startOffset": 85, "endOffset": 101}, {"referenceID": 11, "context": "This highlights an alternative use of Finkel et al. (2006)\u2019s approach of sampling multiple NLP pipeline components, which in that work was used to perform joint inference.", "startOffset": 38, "endOffset": 59}, {"referenceID": 12, "context": "Our posterior simulation approach for exploratory and error analysis relates to posterior predictive checking (Gelman et al., 2013), which analyzes a posterior to test model assumptions; Mimno and Blei (2011) apply it to a topic model.", "startOffset": 110, "endOffset": 131}, {"referenceID": 12, "context": "Our posterior simulation approach for exploratory and error analysis relates to posterior predictive checking (Gelman et al., 2013), which analyzes a posterior to test model assumptions; Mimno and Blei (2011) apply it to a topic model.", "startOffset": 111, "endOffset": 209}, {"referenceID": 25, "context": "Finally, it may be interesting to pursue recalibration methods, which readjust a non-calibrated model\u2019s predictions to be calibrated; recalibration methods have been developed for binary (Platt, 1999; Niculescu-Mizil and Caruana, 2005) and multiclass (Zadrozny and Elkan, 2002) classification settings, but we are unaware of methods appropriate for the highly structured outputs typical in linguistic analysis.", "startOffset": 187, "endOffset": 235}, {"referenceID": 38, "context": "Finally, it may be interesting to pursue recalibration methods, which readjust a non-calibrated model\u2019s predictions to be calibrated; recalibration methods have been developed for binary (Platt, 1999; Niculescu-Mizil and Caruana, 2005) and multiclass (Zadrozny and Elkan, 2002) classification settings, but we are unaware of methods appropriate for the highly structured outputs typical in linguistic analysis.", "startOffset": 251, "endOffset": 277}, {"referenceID": 34, "context": "Another approach might be to directly constrain CalibErr = 0 during training, or try to reduce it as a training-time risk minimization or cost objective (Smith and Eisner, 2006; Gimpel and Smith, 2010; Stoyanov et al., 2011; Br\u00fcmmer and Doddington, 2013).", "startOffset": 153, "endOffset": 254}, {"referenceID": 14, "context": "Another approach might be to directly constrain CalibErr = 0 during training, or try to reduce it as a training-time risk minimization or cost objective (Smith and Eisner, 2006; Gimpel and Smith, 2010; Stoyanov et al., 2011; Br\u00fcmmer and Doddington, 2013).", "startOffset": 153, "endOffset": 254}, {"referenceID": 6, "context": "Another approach might be to directly constrain CalibErr = 0 during training, or try to reduce it as a training-time risk minimization or cost objective (Smith and Eisner, 2006; Gimpel and Smith, 2010; Stoyanov et al., 2011; Br\u00fcmmer and Doddington, 2013).", "startOffset": 153, "endOffset": 254}], "year": 2015, "abstractText": "Many models in natural language processing define probabilistic distributions over linguistic structures. We argue that (1) the quality of a model\u2019s posterior distribution can and should be directly evaluated, as to whether probabilities correspond to empirical frequencies; and (2) NLP uncertainty can be projected not only to pipeline components, but also to exploratory data analysis, telling a user when to trust and not trust the NLP analysis. We present a method to analyze calibration, and apply it to compare the miscalibration of several commonly used models. We also contribute a coreference sampling algorithm that can create confidence intervals for a political event extraction task.1", "creator": "LaTeX with hyperref package"}}}