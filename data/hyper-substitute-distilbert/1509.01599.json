{"id": "1509.01599", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2015", "title": "Better Document-level Sentiment Analysis from RST Discourse Parsing", "abstract": "significance structure is the organizational link between surface features and document - volume aggregation, analyzed as sentiment polarity. we show that the viewpoint analyses build on truth structure theory ( rst ) parsers can compare document - level sentiment indices, via insights onto local information channel front narrative representation. first, clearly show while reweighting discourse units according and their position ; semantic context relation creates the perceived structure shall promise substantial improvements on lexicon - based sentiment analysis. thence, we investigate a powerful neural network over the rst map, that offers significant progress over classification - based methods.", "histories": [["v1", "Fri, 4 Sep 2015 20:28:12 GMT  (34kb,D)", "http://arxiv.org/abs/1509.01599v1", "Published at Empirical Methods in Natural Language Processing (EMNLP 2015)"], ["v2", "Fri, 11 Sep 2015 15:41:53 GMT  (34kb,D)", "http://arxiv.org/abs/1509.01599v2", "Published at Empirical Methods in Natural Language Processing (EMNLP 2015)"]], "COMMENTS": "Published at Empirical Methods in Natural Language Processing (EMNLP 2015)", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["parminder bhatia", "yangfeng ji", "jacob eisenstein"], "accepted": true, "id": "1509.01599"}, "pdf": {"name": "1509.01599.pdf", "metadata": {"source": "CRF", "title": "Better Document-level Sentiment Analysis from RST Discourse Parsing\u2217", "authors": ["Parminder Bhatia"], "emails": ["parminder.bhatia243@gmail.com,", "jiyfeng@gatech.edu", "jacobe@gatech.edu"], "sections": [{"heading": null, "text": ""}, {"heading": "1 Introduction", "text": "Sentiment analysis and opinion mining are among the most widely-used applications of language technology, impacting both industry and a variety of other academic disciplines (Feldman, 2013; Liu, 2012; Pang and Lee, 2008). Yet sentiment analysis is still dominated by bag-of-words approaches, and attempts to include additional linguistic context typically stop at the sentence level (Socher et al., 2013). Since document-level opinion mining inherently involves multi-sentence texts, it seems that analysis of document-level structure should have a role to play.\nA classic example of the potential relevance of discourse to sentiment analysis is shown in Figure 1. In this review of the film The Last Samurai, the positive sentiment words far outnumber the\n:::::::: negative ::::::::: sentiment words. But the discourse structure \u2014 indicated here with Rhetorical Structure Theory (RST; Mann and Thompson, 1988) \u2014\n\u2217Code is available at https://github.com/ parry2403/R2N2\nR\nCONCESSION\nJUSTIFY\n1A CONJUNCTION\nELABORATION\n1B 1C\n1D\nJUSTIFY\n1E CONJUNCTION 1F 1G\n1H\n[It could have been a great movie]1A [It does have beautiful scenery,]1B [some of the best since Lord of the Rings.]1C [The acting is well done,]1D [and I really liked the son of the leader of the Samurai.]1E [He was a likable chap,]1F [and I\n:::: hated to see him die.]1G [But,\nother than all that, this movie is :::::: nothing more than hidden ::::: rip-offs.]1H\nFigure 1: Example adapted from Voll and Taboada (2007).\nclearly favors the final sentence, whose polarity is negative. This example is illustrative in more than one way: it was originally identified by Voll and Taboada (2007), who found that manuallyannotated RST parse trees improved lexiconbased sentiment analysis, but that automaticallygenerated parses from the SPADE parser (Soricut and Marcu, 2003), which was then state-of-the-art, did not.\nSince this time, RST discourse parsing has improved considerably, with the best systems now yielding 5-10% greater raw accuracy than SPADE, depending on the metric. The time is therefore right to reconsider the effectiveness of RST for document-level sentiment analysis. In this paper, we present two different ways of combining RST discourse parses with sentiment analysis. The methods are both relatively simple, and\nar X\niv :1\n50 9.\n01 59\n9v 1\n[ cs\n.C L\n] 4\nS ep\n2 01\n5\ncan be used in combination with an \u201coff the shelf\u201d discourse parser. We consider the following two architectures:\n\u2022 Reweighting the contribution of each discourse unit, based on its position in a dependency-like representation of the discourse structure. Such weights can be defined using a simple function, or learned from a small of data.\n\u2022 Recursively propagating sentiment up through the RST parse, in an architecture inspired by recursive neural networks (Smolensky, 1990; Socher et al., 2011).\nBoth architectures can be used in combination with either a lexicon-based sentiment analyzer, or a trained classifier. Indeed, for users whose starting point is a lexicon-based approach, a simple RST-based reweighting function can offer significant improvements. For those who are willing to train a sentiment classifier, the recursive model yields further gains."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Rhetorical Structure Theory", "text": "RST is a compositional model of discourse structure, in which elementary discourse units (EDUs) are combined intro progressively larger discourse units, ultimately covering the entire document. Discourse relations may involve a nucleus and a satellite, or they may be multinuclear. In the example in Figure 1, the unit 1C is the satellite of a relationship with its nucleus 1B; together they form a larger discourse unit, which is involved in a multinuclear CONJUNCTION relation.\nThe nuclearity structure of RST trees suggests a natural approach to evaluating the importance of segments of text: satellites tend to be less important, and nucleii tend to be more important (Marcu, 1999). This idea has been leveraged extensively in document summarization (Gerani et al., 2014; Uze\u0302da et al., 2010; Yoshida et al., 2014), and was the inspiration for Voll and Taboada (2007), who examined intra-sentential relations, eliminating all words except those in the top-most nucleus within each sentence. More recent work focuses on reweighting each discourse unit depending on the relations in which it participates (Heerschop et al., 2011; Hogenboom et al.,\n2015). We consider such an approach, and compare it with a compositional method, in which sentiment polarity is propagated up the discourse tree.\nMarcu (1997) provides the seminal work on automatic RST parsing, but there has been a recent spike of interest in this task, with contemporary approaches employing discriminative learning (Hernault et al., 2010), rich features (Feng and Hirst, 2012), structured prediction (Joty et al., 2015), and representation learning (Ji and Eisenstein, 2014; Li et al., 2014). With many strong systems to choose from, we employ the publiclyavailable DPLP parser (Ji and Eisenstein, 2014),1. To our knowledge, this system currently gives the best F-measure on relation identification, the most difficult subtask of RST parsing. DPLP is a shiftreduce parser (Sagae, 2009), and its time complexity is linear in the length of the document."}, {"heading": "2.2 Sentiment analysis", "text": "There is a huge literature on sentiment analysis (Pang and Lee, 2008; Liu, 2012), with particular interest in determining the overall sentiment polarity (positive or negative) of a document. Bagof-words models are widely used for this task, as they offer accuracy that is often very competitive with more complex approaches. Given labeled data, supervised learning can be applied to obtain sentiment weights for each word. However, the effectiveness of supervised sentiment analysis depends on having training data in the same domain as the target, and this is not always possible. Moreover, in social science applications, the desired labels may not correspond directly to positive or negative sentiment, but may focus on other categories, such as politeness (Danescu-NiculescuMizil et al., 2013), narrative frames (Jurafsky et al., 2014), or a multidimensional spectrum of emotions (Kim et al., 2012). In these cases, labeled documents may not be available, so users often employ a simpler method: counting matches against lists of words associated with each category. Such lists may be built manually from introspection, as in LIWC (Tausczik and Pennebaker, 2010) and the General Inquirer (Stone, 1966). Alternatively, they may be induced by bootstrapping from a seed set of words (Hatzivassiloglou and McKeown, 1997; Taboada et al., 2011). While lexicon-based methods may be less accurate than supervised classifiers, they are easier to apply to\n1https://github.com/jiyfeng/DPLP\nnew domains and problem settings. Our proposed approach can be used in combination with either method for sentiment analysis, and in principle, could be directly applied to other document-level categories, such as politeness."}, {"heading": "2.3 Datasets", "text": "We evaluate on two review datasets. In both cases, the goal is to correctly classify the opinion polarity as positive or negative. The first dataset is comprised of 2000 movie reviews, gathered by Pang and Lee (2004). We perform ten-fold crossvalidation on this data. The second dataset is larger, consisting of 50,000 movie reviews, gathered by Socher et al. (2013), with a predefined 50/50 split into training and test sets. Documents are scored on a 1-10 scale, and we treat scores \u2264 4 as negative,\u2265 7 as positive, and ignore scores of 5-6 as neutral \u2014 although in principle nothing prevents extension of our approaches to more than two sentiment classes."}, {"heading": "3 Discourse depth reweighting", "text": "Our first approach to incorporating discourse information into sentiment analysis is based on quantifying the importance of each unit of text in terms of its discourse depth. To do this, we employ the dependency-based discourse tree (DEPDT) formulation from prior work on summarization (Hirao et al., 2013). The DEP-DT formalism converts the constituent-like RST tree into a directed graph over elementary discourse units (EDUs), in a process that is a close analogue of the transformation of a headed syntactic constituent parse to a syntactic dependency graph (Ku\u0308bler et al., 2009).\nThe DEP-DT representation of the discourse in Figure 1 in shown in Figure 2. The graph is constructed by propagating \u201chead\u201d information up the RST tree; if the elementary discourse unit ei is the satellite in a discourse relation headed by ej , then\nthere is an edge from ej to ei. Thus, the \u201cdepth\u201d of each EDU is the number of times in which it is embedded in the satellite of a discourse relation. The exact algorithm for constructing DEP-DTs is given by Hirao et al. (2013).\nGiven this representation, we construct a simple linear function for weighting the contribution of the EDU at depth di:\n\u03bbi = max(0.5, 1\u2212 di/6). (1)\nThus, at di = 0, we have \u03bbi = 1, and at di \u2265 3, we have \u03bbi = 0.5. Now assume each elementary discourse unit contributes a prediction \u03c8i = \u03b8>wi, where wi is the bag-of-words vector, and \u03b8 is a vector of weights, which may be either learned or specified by a sentiment lexicon. Then the overall prediction for a document is given by,\n\u03a8 = \u2211\ni\n\u03bbi(\u03b8 >wi) = \u03b8>(\n\u2211\ni\n\u03bbiwi). (2)\nEvaluation We apply this approach in combination with both lexicon-based and classificationbased sentiment analysis. We use the lexicon of Wilson et al. (2005), and set \u03b8j = 1 for words marked \u201cpositive\u201d, and \u03b8j = \u22121 for words marked negative. For classification-based analysis, we set \u03b8 equal to the weights obtained by training a logistic regression classifier, tuning the regularization coefficient on held-out data.\nResults are shown in Table 1. As seen in the comparison between lines B1 and D1, discourse depth weighting offers substantial improvements over the bag-of-words approach for lexiconbased sentiment analysis, with raw improvements of 4\u22125%. Given the simplicity of this approach \u2014 which requires only a sentiment lexicon and a discourse parser \u2014 we strongly recommend the application of discourse depth weighting for lexiconbased sentiment analysis at the document level. However, the improvements for the classificationbased models are considerably smaller, less than 1% in both datasets."}, {"heading": "4 Rhetorical Recursive Neural Networks", "text": "Discourse-depth reweighting offers significant improvements for lexicon-based sentiment analysis, but the improvements over the more accurate classification-based method are meager. We therefore turn to a data-driven approach for combining sentiment analysis with rhetorical structure theory, based on recursive neural networks (Socher et al.,\n2011). The idea is simple: recursively propagate sentiment scores up the RST tree, until the root of the document is reached. For nucleus-satellite discourse relations, we have:\n\u03a8i = tanh(K (ri) n \u03a8n(i) +K (ri) s \u03a8s(i)), (3)\nwhere i indexes a discourse unit composed from relation ri, n(i) indicates its nucleus, and s(i) indicates its satellite. Returning to the example in Figure 1, the sentiment score for the discourse unit obtained by combining 1B and 1C is obtained from tanh(K(elaboration)n \u03a81B + K (elaboration) s \u03a81C). Similarly, for multinuclear relations, we have,\n\u03a8i = tanh( \u2211\nj\u2208n(i) K(ri)n \u03a8j). (4)\nIn the base case, each elementary discourse unit\u2019s sentiment is constructed from its bag-of-words, \u03a8i = \u03b8\n>wi. Because the structure of each document is different, the network architecture varies in each example; nonetheless, the parameters can be reused across all instances.\nThis approach, which we call a Rhetorical Recursive Neural Network (R2N2), is reminiscent of the compositional model proposed by Socher et al. (2013), where composition is over the constituents of the syntactic parse of a sentence, rather than the units of a discourse. However, a crucial difference is that in R2N2s, the elements \u03a8 and K are scalars: we do not attempt to learn a latent distributed representation of the sub-document units. This is because discourse units typically comprise multiple words, so that accurate analysis of the sentiment for elementary discourse units is not so difficult as accurate analysis of individual words.\nThe scores for individual discourse units can be computed from a bag-of-words classifier, or, in future work, from a more complex model such as a recursive or recurrent neural network.\nWhile this neural network structure captures the idea of compositionality over the RST tree, the most deeply embedded discourse units can be heavily down-weighted by the recursive composition (assuming Ks < Kn): in the most extreme case of a right-branching or left-branching structure, the recursive operator may be appliedN times to the most deeply embedded EDU. In contrast, discourse depth reweighting applies a uniform weight of 0.5 to all discourse units with depth \u2265 3. In the spirit of this approach, we add an additional component to the network architecture, capturing the bag-of-words for the entire document. Thus, at the root node we have:\n\u03a8doc = \u03b3\u03b8 >(\n\u2211\ni\nwi) + \u03a8rst-root, (5)\nwith \u03a8rst-root defined recursively from Equations 3 and 4, \u03b8 indicating the vector of per-word weights, and the scalar \u03b3 controlling the tradeoff between these two components.\nLearning R2N2 is trained by backpropagating from a hinge loss objective; assuming yt \u2208 {\u22121, 1} for each document t, we have the loss Lt = (1 \u2212 yt\u03a8doc,t)+. From this loss, we use backpropagation through structure to obtain gradients on the parameters (Goller and Kuchler, 1996). Training is performed using stochastic gradient descent. For simplicity, we follow Zirn et al. (2011) and focus on the distinction between contrastive and non-contrastive relations. The set of contrastive relations includes CONTRAST, COMPARISON, ANTITHESIS, ANTITHESIS-E, CONSEQUENCE-S, CONCESSION, and PROBLEM-SOLUTION.\nEvaluation Results for this approach are shown in lines R1 and R2 of Table 1. Even without distinguishing between discourse relations, we get an improvement of more than 3% accuracy on the Stanford data, and 0.5% on the smaller Pang & Lee dataset. Adding sensitivity to discourse relations (distinguishingK(r) for contrastive and noncontrastive relations) offers further improvements on the Pang & Lee data, outperforming the baseline classifier (D2) by 1.3%.\nThe accuracy of discourse relation detection is only 60% for even the best systems (Ji and Eisen-\nstein, 2014), which may help to explain why relations do not offer a more substantial boost. An anonymous reviewer recommended evaluating on gold RST parse trees to determine the extent to which improvements in RST parsing might transfer to downstream document analysis. Such an evaluation would seem to require a large corpus of texts with both gold RST parse trees and sentiment polarity labels; the SFU Review Corpus (Taboada, 2008) of 30 review texts offers a starting point, but is probably too small to train a competitive sentiment analysis system."}, {"heading": "5 Related Work", "text": "Section 2 mentions some especially relevant prior work. Other efforts to incorporate RST into sentiment analysis have often focused on intrasentential discourse relations (Heerschop et al., 2011; Zhou et al., 2011; Chenlo et al., 2014), rather than relations over the entire document. Wang et al. (2012) address sentiment analysis in Chinese. Lacking a discourse parser, they focus on explicit connectives, using a strategy that is related to our discourse depth reweighting. Wang and Wu (2013) use manually-annotated discourse parses in combination with a sentiment lexicon, which is automatically updated based on the discourse structure. Zirn et al. (2011) use an RST parser in a Markov Logic Network, with the goal of making polarity predictions at the sub-sentence level, rather than improving document-level prediction. None of the prior work considers the sort of recurrent compositional model presented here.\nAn alternative to RST is to incorporate \u201cshallow\u201d discourse structure, such as the relations from the Penn Discourse Treebank (PDTB). PDTB relations were shown to improve sentencelevel sentiment analysis by Somasundaran et al. (2009), and were incorporated in a model of sentiment flow by Wachsmuth et al. (2014). PDTB relations are often signaled with explicit discourse connectives, and these may be used as a feature (Trivedi and Eisenstein, 2013; Lazaridou et al., 2013) or as posterior constraints (Yang and Cardie, 2014). This prior work on discourse relations within sentences and between adjacent sentences can be viewed as complementary to our focus on higher-level discourse relations across the entire document.\nThere are unfortunately few possibilities for direct comparison of our approach against prior\nwork. Heerschop et al. (2011) and Wachsmuth et al. (2014) also employ the Pang and Lee (2004) dataset, but neither of their results are directly comparable: Heerschop et al. (2011) exclude documents that SPADE fails to parse, and Wachsmuth et al. (2014) evaluates only on individual sentences rather than entire documents. The only possible direct comparison is with very recent work from Hogenboom et al. (2015), who employ a weighting scheme that is similar to the approach described in Section 3. They evaluate on the Pang and Lee data, and consider only lexicon-based sentiment analysis, obtaining document-level accuracies between 65% (for the baseline) and 72% (for their best discourse-augmented system). Table 1 shows that fully supervised methods give much stronger performance on this dataset, with accuracies more than 10% higher."}, {"heading": "6 Conclusion", "text": "Sentiment polarity analysis has typically relied on a \u201cpreponderance of evidence\u201d strategy, hoping that the words or sentences representing the overall polarity will outweigh those representing counterpoints or rhetorical concessions. However, with the availability of off-the-shelf RST discourse parsers, it is now easy to include documentlevel structure in sentiment analysis. We show that a simple reweighting approach offers robust advantages in lexicon-based sentiment analysis, and that a recursive neural network can substantially outperform a bag-of-words classifier. Future work will focus on combining models of discourse structure with richer models at the sentence level.\nAcknowledgments Thanks to the anonymous reviewers for their helpful suggestions on how to improve the paper.\nThe following members of the Georgia Tech Computational\nLinguistics Laboratory offered feedback throughout the re-\nsearch process: Naman Goyal, Vinodh Krishan, Umashanthi\nPavalanathan, Ana Smith, Yijie Wang, and Yi Yang. Several\nclass projects in Georgia Tech CS 4650/7650 took alternative\napproaches to the application of discourse parsing to senti-\nment analysis, which was informative to this work; thanks\nparticularly to Julia Cochran, Rohit Pathak, Pavan Kumar\nRamnath, and Bharadwaj Tanikella. This research was sup-\nported by a Google Faculty Research Award, by the National\nInstitutes of Health under award number R01GM112697-01,\nand by the Air Force Office of Scientific Research. The con-\ntent is solely the responsibility of the authors and does not\nnecessarily represent the official views of these sponsors."}], "references": [{"title": "Rhetorical structure theory for polarity estimation: An experimental study", "author": ["Jos\u00e9 M Chenlo", "Alexander Hogenboom", "David E Losada."], "venue": "Data & Knowledge Engineering.", "citeRegEx": "Chenlo et al\\.,? 2014", "shortCiteRegEx": "Chenlo et al\\.", "year": 2014}, {"title": "A computational approach to politeness with application to social factors", "author": ["Cristian Danescu-Niculescu-Mizil", "Moritz Sudhof", "Dan Jurafsky", "Jure Leskovec", "Christopher Potts."], "venue": "Proceedings of the Association for Computational Linguistics (ACL),", "citeRegEx": "Danescu.Niculescu.Mizil et al\\.,? 2013", "shortCiteRegEx": "Danescu.Niculescu.Mizil et al\\.", "year": 2013}, {"title": "Techniques and applications for sentiment analysis", "author": ["Ronen Feldman."], "venue": "Communications of the ACM, 56(4):82\u201389.", "citeRegEx": "Feldman.,? 2013", "shortCiteRegEx": "Feldman.", "year": 2013}, {"title": "Text-level Discourse Parsing with Rich Linguistic Features", "author": ["Vanessa Wei Feng", "Graeme Hirst."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), Jeju, Korea.", "citeRegEx": "Feng and Hirst.,? 2012", "shortCiteRegEx": "Feng and Hirst.", "year": 2012}, {"title": "Abstractive summarization of product reviews using discourse structure", "author": ["Shima Gerani", "Yashar Mehdad", "Giuseppe Carenini", "Raymond T Ng", "Bita Nejat."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), Baltimore, MD.", "citeRegEx": "Gerani et al\\.,? 2014", "shortCiteRegEx": "Gerani et al\\.", "year": 2014}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler."], "venue": "Neural Networks, IEEE International Conference on, pages 347\u2013352. IEEE.", "citeRegEx": "Goller and Kuchler.,? 1996", "shortCiteRegEx": "Goller and Kuchler.", "year": 1996}, {"title": "Predicting the semantic orientation of adjectives", "author": ["Vasileios Hatzivassiloglou", "Kathleen R. McKeown."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), pages 174\u2013181, Madrid, Spain.", "citeRegEx": "Hatzivassiloglou and McKeown.,? 1997", "shortCiteRegEx": "Hatzivassiloglou and McKeown.", "year": 1997}, {"title": "Polarity analysis of texts using discourse structure", "author": ["Bas Heerschop", "Frank Goossen", "Alexander Hogenboom", "Flavius Frasincar", "Uzay Kaymak", "Franciska de Jong."], "venue": "Proceedings of the 20th ACM international conference on Information and knowl-", "citeRegEx": "Heerschop et al\\.,? 2011", "shortCiteRegEx": "Heerschop et al\\.", "year": 2011}, {"title": "HILDA: A Discourse Parser Using Support Vector Machine Classification", "author": ["Hugo Hernault", "Helmut Prendinger", "David A. duVerle", "Mitsuru Ishizuka."], "venue": "Dialogue and Discourse, 1(3):1\u201333.", "citeRegEx": "Hernault et al\\.,? 2010", "shortCiteRegEx": "Hernault et al\\.", "year": 2010}, {"title": "Single-document summarization as a tree knapsack problem", "author": ["Tsutomu Hirao", "Yasuhisa Yoshida", "Masaaki Nishino", "Norihito Yasuda", "Masaaki Nagata."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pages", "citeRegEx": "Hirao et al\\.,? 2013", "shortCiteRegEx": "Hirao et al\\.", "year": 2013}, {"title": "Using rhetorical structure in sentiment analysis", "author": ["Alexander Hogenboom", "Flavius Frasincar", "Franciska de Jong", "Uzay Kaymak."], "venue": "Communications of the ACM, 58(7):69\u201377.", "citeRegEx": "Hogenboom et al\\.,? 2015", "shortCiteRegEx": "Hogenboom et al\\.", "year": 2015}, {"title": "Representation learning for text-level discourse parsing", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), Baltimore, MD.", "citeRegEx": "Ji and Eisenstein.,? 2014", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2014}, {"title": "CODRA: A novel discriminative framework for rhetorical analysis", "author": ["Shafiq Joty", "Giuseppe Carenini", "Raymond Ng."], "venue": "Computational Linguistics, 41(3).", "citeRegEx": "Joty et al\\.,? 2015", "shortCiteRegEx": "Joty et al\\.", "year": 2015}, {"title": "Narrative framing of consumer sentiment in online restaurant reviews", "author": ["Dan Jurafsky", "Victor Chahuneau", "Bryan R Routledge", "Noah A Smith."], "venue": "First Monday, 19(4).", "citeRegEx": "Jurafsky et al\\.,? 2014", "shortCiteRegEx": "Jurafsky et al\\.", "year": 2014}, {"title": "Do you feel what i feel? social aspects of emotions in twitter conversations", "author": ["Suin Kim", "JinYeong Bak", "Alice Haeyun Oh."], "venue": "Proceedings of the International Conference on Web and Social Media (ICWSM), Menlo Park, California. AAAI Publica-", "citeRegEx": "Kim et al\\.,? 2012", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "Dependency parsing", "author": ["Sandra K\u00fcbler", "Ryan McDonald", "Joakim Nivre."], "venue": "Synthesis Lectures on Human Language Technologies, 1(1):1\u2013127.", "citeRegEx": "K\u00fcbler et al\\.,? 2009", "shortCiteRegEx": "K\u00fcbler et al\\.", "year": 2009}, {"title": "A bayesian model for joint unsupervised induction of sentiment, aspect and discourse representations", "author": ["Angeliki Lazaridou", "Ivan Titov", "Caroline Sporleder."], "venue": "Proceedings of the Association for Computational Linguistics (ACL),", "citeRegEx": "Lazaridou et al\\.,? 2013", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Recursive deep models for discourse parsing", "author": ["Jiwei Li", "Rumeng Li", "Eduard Hovy."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Sentiment analysis and opinion mining", "author": ["Bing Liu."], "venue": "Synthesis Lectures on Human Language Technologies, 5(1):1\u2013167.", "citeRegEx": "Liu.,? 2012", "shortCiteRegEx": "Liu.", "year": 2012}, {"title": "Discourse structures for text generation", "author": ["William Mann."], "venue": "Proceedings of the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computational Linguistics, pages 367\u2013375. Association for Com-", "citeRegEx": "Mann.,? 1984", "shortCiteRegEx": "Mann.", "year": 1984}, {"title": "The rhetorical parsing of natural language texts", "author": ["Daniel Marcu."], "venue": "Proceedings of the European Chapter of the Association for Computational Linguistics (EACL), pages 96\u2013103.", "citeRegEx": "Marcu.,? 1997", "shortCiteRegEx": "Marcu.", "year": 1997}, {"title": "The automatic construction of large-scale corpora for summarization research", "author": ["Daniel Marcu."], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 137\u2013144. ACM.", "citeRegEx": "Marcu.,? 1999", "shortCiteRegEx": "Marcu.", "year": 1999}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), pages 271\u2013278.", "citeRegEx": "Pang and Lee.,? 2004", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Opinion mining and sentiment analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Foundations and trends in information retrieval, 2(1-2):1\u2013135.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Analysis of Discourse Structure with Syntactic Dependencies and Data-Driven Shift-Reduce Parsing", "author": ["Kenji Sagae."], "venue": "Proceedings of the 11th International Conference on Parsing Technologies (IWPT\u201909), pages 81\u201384, Paris, France, October.", "citeRegEx": "Sagae.,? 2009", "shortCiteRegEx": "Sagae.", "year": 2009}, {"title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems", "author": ["Paul Smolensky."], "venue": "Artificial intelligence, 46(1):159\u2013216.", "citeRegEx": "Smolensky.,? 1990", "shortCiteRegEx": "Smolensky.", "year": 1990}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["Richard Socher", "Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the International Conference on Machine Learning (ICML), Seattle,", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of Empirical Methods for", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification", "author": ["Swapna Somasundaran", "Galileo Namata", "Janyce Wiebe", "Lise Getoor."], "venue": "Proceedings of Empirical Methods for Natural", "citeRegEx": "Somasundaran et al\\.,? 2009", "shortCiteRegEx": "Somasundaran et al\\.", "year": 2009}, {"title": "Sentence level discourse parsing using syntactic and lexical information", "author": ["Radu Soricut", "Daniel Marcu."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 149\u2013156.", "citeRegEx": "Soricut and Marcu.,? 2003", "shortCiteRegEx": "Soricut and Marcu.", "year": 2003}, {"title": "The General Inquirer: A Computer Approach to Content Analysis", "author": ["Philip J. Stone."], "venue": "The MIT Press.", "citeRegEx": "Stone.,? 1966", "shortCiteRegEx": "Stone.", "year": 1966}, {"title": "Lexiconbased methods for sentiment analysis", "author": ["Maite Taboada", "Julian Brooke", "Milan Tofiloski", "Kimberly Voll", "Manfred Stede."], "venue": "Computational linguistics, 37(2):267\u2013307.", "citeRegEx": "Taboada et al\\.,? 2011", "shortCiteRegEx": "Taboada et al\\.", "year": 2011}, {"title": "SFU review corpus", "author": ["Maite Taboada."], "venue": "http://www.sfu.ca/ \u0303mtaboada/ research/SFU_Review_Corpus.html.", "citeRegEx": "Taboada.,? 2008", "shortCiteRegEx": "Taboada.", "year": 2008}, {"title": "The psychological meaning of words: LIWC and computerized text analysis methods", "author": ["Yla R Tausczik", "James W Pennebaker."], "venue": "Journal of Language and Social Psychology, 29(1):24\u201354.", "citeRegEx": "Tausczik and Pennebaker.,? 2010", "shortCiteRegEx": "Tausczik and Pennebaker.", "year": 2010}, {"title": "Discourse connectors for latent subjectivity in sentiment analysis", "author": ["Rakshit Trivedi", "Jacob Eisenstein."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 808\u2013813, Stroudsburg, Pennsylva-", "citeRegEx": "Trivedi and Eisenstein.,? 2013", "shortCiteRegEx": "Trivedi and Eisenstein.", "year": 2013}, {"title": "A comprehensive comparative evaluation of rst-based summarization methods", "author": ["Vin\u0131\u0301cius Rodrigues Uz\u00eada", "Thiago Alexandre Salgueiro Pardo", "Maria Das Gra\u00e7as Volpe Nunes"], "venue": "ACM Transactions on Speech and Language Processing", "citeRegEx": "Uz\u00eada et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Uz\u00eada et al\\.", "year": 2010}, {"title": "Not all words are created equal: Extracting semantic orientation as a function of adjective relevance", "author": ["Kimberly Voll", "Maite Taboada."], "venue": "Proceedings of Australian Conference on Artificial Intelligence.", "citeRegEx": "Voll and Taboada.,? 2007", "shortCiteRegEx": "Voll and Taboada.", "year": 2007}, {"title": "Modeling review argumentation for robust sentiment analysis", "author": ["Henning Wachsmuth", "Martin Trenkmann", "Benno Stein", "Gregor Engels."], "venue": "Proceedings of the International Conference on Computational Linguistics (COLING).", "citeRegEx": "Wachsmuth et al\\.,? 2014", "shortCiteRegEx": "Wachsmuth et al\\.", "year": 2014}, {"title": "Exploiting hierarchical discourse structure for review sentiment analysis", "author": ["Fei Wang", "Yunfang Wu."], "venue": "Proceedings of the Conference on Asian Language Processing (IALP), pages 121\u2013124.", "citeRegEx": "Wang and Wu.,? 2013", "shortCiteRegEx": "Wang and Wu.", "year": 2013}, {"title": "Exploiting discourse relations for sentiment analysis", "author": ["Fei Wang", "Yunfang Wu", "Likun Qiu."], "venue": "Proceedings of the International Conference on Computational Linguistics (COLING), pages 1311\u2013 1320, Mumbai, India.", "citeRegEx": "Wang et al\\.,? 2012", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Recognizing contextual polarity in phraselevel sentiment analysis", "author": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), pages 347\u2013354.", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Context-aware learning for sentence-level sentiment analysis with posterior regularization", "author": ["Bishan Yang", "Claire Cardie."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), Baltimore, MD.", "citeRegEx": "Yang and Cardie.,? 2014", "shortCiteRegEx": "Yang and Cardie.", "year": 2014}, {"title": "Dependency-based Discourse Parser for Single-Document Summarization", "author": ["Yasuhisa Yoshida", "Jun Suzuki", "Tsutomu Hirao", "Masaaki Nagata."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).", "citeRegEx": "Yoshida et al\\.,? 2014", "shortCiteRegEx": "Yoshida et al\\.", "year": 2014}, {"title": "Unsupervised discovery of discourse relations for eliminating intrasentence polarity ambiguities", "author": ["Lanjun Zhou", "Binyang Li", "Wei Gao", "Zhongyu Wei", "Kam-Fai Wong."], "venue": "Proceedings of Empirical Methods for Natural Language Process-", "citeRegEx": "Zhou et al\\.,? 2011", "shortCiteRegEx": "Zhou et al\\.", "year": 2011}, {"title": "Fine-grained sentiment analysis with structural features", "author": ["C\u00e4cilia Zirn", "Mathias Niepert", "Heiner Stuckenschmidt", "Michael Strube."], "venue": "Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP), pages 336\u2013344, Chi-", "citeRegEx": "Zirn et al\\.,? 2011", "shortCiteRegEx": "Zirn et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Sentiment analysis and opinion mining are among the most widely-used applications of language technology, impacting both industry and a variety of other academic disciplines (Feldman, 2013; Liu, 2012; Pang and Lee, 2008).", "startOffset": 174, "endOffset": 220}, {"referenceID": 18, "context": "Sentiment analysis and opinion mining are among the most widely-used applications of language technology, impacting both industry and a variety of other academic disciplines (Feldman, 2013; Liu, 2012; Pang and Lee, 2008).", "startOffset": 174, "endOffset": 220}, {"referenceID": 23, "context": "Sentiment analysis and opinion mining are among the most widely-used applications of language technology, impacting both industry and a variety of other academic disciplines (Feldman, 2013; Liu, 2012; Pang and Lee, 2008).", "startOffset": 174, "endOffset": 220}, {"referenceID": 27, "context": "Yet sentiment analysis is still dominated by bag-of-words approaches, and attempts to include additional linguistic context typically stop at the sentence level (Socher et al., 2013).", "startOffset": 161, "endOffset": 182}, {"referenceID": 32, "context": "Figure 1: Example adapted from Voll and Taboada (2007).", "startOffset": 40, "endOffset": 55}, {"referenceID": 29, "context": "This example is illustrative in more than one way: it was originally identified by Voll and Taboada (2007), who found that manuallyannotated RST parse trees improved lexiconbased sentiment analysis, but that automaticallygenerated parses from the SPADE parser (Soricut and Marcu, 2003), which was then state-of-the-art, did not.", "startOffset": 260, "endOffset": 285}, {"referenceID": 29, "context": "This example is illustrative in more than one way: it was originally identified by Voll and Taboada (2007), who found that manuallyannotated RST parse trees improved lexiconbased sentiment analysis, but that automaticallygenerated parses from the SPADE parser (Soricut and Marcu, 2003), which was then state-of-the-art, did not.", "startOffset": 92, "endOffset": 107}, {"referenceID": 25, "context": "\u2022 Recursively propagating sentiment up through the RST parse, in an architecture inspired by recursive neural networks (Smolensky, 1990; Socher et al., 2011).", "startOffset": 119, "endOffset": 157}, {"referenceID": 26, "context": "\u2022 Recursively propagating sentiment up through the RST parse, in an architecture inspired by recursive neural networks (Smolensky, 1990; Socher et al., 2011).", "startOffset": 119, "endOffset": 157}, {"referenceID": 21, "context": "The nuclearity structure of RST trees suggests a natural approach to evaluating the importance of segments of text: satellites tend to be less important, and nucleii tend to be more important (Marcu, 1999).", "startOffset": 192, "endOffset": 205}, {"referenceID": 7, "context": "More recent work focuses on reweighting each discourse unit depending on the relations in which it participates (Heerschop et al., 2011; Hogenboom et al., 2015).", "startOffset": 112, "endOffset": 160}, {"referenceID": 10, "context": "More recent work focuses on reweighting each discourse unit depending on the relations in which it participates (Heerschop et al., 2011; Hogenboom et al., 2015).", "startOffset": 112, "endOffset": 160}, {"referenceID": 30, "context": ", 2014), and was the inspiration for Voll and Taboada (2007), who examined intra-sentential relations, eliminating all words except those in the top-most nucleus within each sentence.", "startOffset": 46, "endOffset": 61}, {"referenceID": 8, "context": "Marcu (1997) provides the seminal work on automatic RST parsing, but there has been a recent spike of interest in this task, with contemporary approaches employing discriminative learning (Hernault et al., 2010), rich features (Feng and Hirst, 2012), structured prediction (Joty et al.", "startOffset": 188, "endOffset": 211}, {"referenceID": 3, "context": ", 2010), rich features (Feng and Hirst, 2012), structured prediction (Joty et al.", "startOffset": 23, "endOffset": 45}, {"referenceID": 12, "context": ", 2010), rich features (Feng and Hirst, 2012), structured prediction (Joty et al., 2015), and representation learning (Ji and Eisenstein, 2014; Li et al.", "startOffset": 69, "endOffset": 88}, {"referenceID": 11, "context": ", 2015), and representation learning (Ji and Eisenstein, 2014; Li et al., 2014).", "startOffset": 37, "endOffset": 79}, {"referenceID": 17, "context": ", 2015), and representation learning (Ji and Eisenstein, 2014; Li et al., 2014).", "startOffset": 37, "endOffset": 79}, {"referenceID": 11, "context": "With many strong systems to choose from, we employ the publiclyavailable DPLP parser (Ji and Eisenstein, 2014),1.", "startOffset": 85, "endOffset": 110}, {"referenceID": 24, "context": "DPLP is a shiftreduce parser (Sagae, 2009), and its time complexity is linear in the length of the document.", "startOffset": 29, "endOffset": 42}, {"referenceID": 23, "context": "There is a huge literature on sentiment analysis (Pang and Lee, 2008; Liu, 2012), with particular interest in determining the overall sentiment polarity (positive or negative) of a document.", "startOffset": 49, "endOffset": 80}, {"referenceID": 18, "context": "There is a huge literature on sentiment analysis (Pang and Lee, 2008; Liu, 2012), with particular interest in determining the overall sentiment polarity (positive or negative) of a document.", "startOffset": 49, "endOffset": 80}, {"referenceID": 13, "context": ", 2013), narrative frames (Jurafsky et al., 2014), or a multidimensional spectrum of emotions (Kim et al.", "startOffset": 26, "endOffset": 49}, {"referenceID": 14, "context": ", 2014), or a multidimensional spectrum of emotions (Kim et al., 2012).", "startOffset": 52, "endOffset": 70}, {"referenceID": 33, "context": "Such lists may be built manually from introspection, as in LIWC (Tausczik and Pennebaker, 2010) and the General Inquirer (Stone, 1966).", "startOffset": 64, "endOffset": 95}, {"referenceID": 30, "context": "Such lists may be built manually from introspection, as in LIWC (Tausczik and Pennebaker, 2010) and the General Inquirer (Stone, 1966).", "startOffset": 121, "endOffset": 134}, {"referenceID": 6, "context": "Alternatively, they may be induced by bootstrapping from a seed set of words (Hatzivassiloglou and McKeown, 1997; Taboada et al., 2011).", "startOffset": 77, "endOffset": 135}, {"referenceID": 31, "context": "Alternatively, they may be induced by bootstrapping from a seed set of words (Hatzivassiloglou and McKeown, 1997; Taboada et al., 2011).", "startOffset": 77, "endOffset": 135}, {"referenceID": 22, "context": "The first dataset is comprised of 2000 movie reviews, gathered by Pang and Lee (2004). We perform ten-fold crossvalidation on this data.", "startOffset": 66, "endOffset": 86}, {"referenceID": 22, "context": "The first dataset is comprised of 2000 movie reviews, gathered by Pang and Lee (2004). We perform ten-fold crossvalidation on this data. The second dataset is larger, consisting of 50,000 movie reviews, gathered by Socher et al. (2013), with a predefined 50/50 split into training and test sets.", "startOffset": 66, "endOffset": 236}, {"referenceID": 9, "context": "To do this, we employ the dependency-based discourse tree (DEPDT) formulation from prior work on summarization (Hirao et al., 2013).", "startOffset": 111, "endOffset": 131}, {"referenceID": 15, "context": "The DEP-DT formalism converts the constituent-like RST tree into a directed graph over elementary discourse units (EDUs), in a process that is a close analogue of the transformation of a headed syntactic constituent parse to a syntactic dependency graph (K\u00fcbler et al., 2009).", "startOffset": 254, "endOffset": 275}, {"referenceID": 9, "context": "The exact algorithm for constructing DEP-DTs is given by Hirao et al. (2013). Given this representation, we construct a simple linear function for weighting the contribution of the EDU at depth di:", "startOffset": 57, "endOffset": 77}, {"referenceID": 40, "context": "We use the lexicon of Wilson et al. (2005), and set \u03b8j = 1 for words marked \u201cpositive\u201d, and \u03b8j = \u22121 for words marked negative.", "startOffset": 22, "endOffset": 43}, {"referenceID": 22, "context": "Table 1: Sentiment classification accuracies on two movie review datasets (Pang and Lee, 2004; Socher et al., 2013), described in Section 2.", "startOffset": 74, "endOffset": 115}, {"referenceID": 27, "context": "Table 1: Sentiment classification accuracies on two movie review datasets (Pang and Lee, 2004; Socher et al., 2013), described in Section 2.", "startOffset": 74, "endOffset": 115}, {"referenceID": 26, "context": "This approach, which we call a Rhetorical Recursive Neural Network (R2N2), is reminiscent of the compositional model proposed by Socher et al. (2013), where composition is over the constituents of the syntactic parse of a sentence, rather than the units of a discourse.", "startOffset": 129, "endOffset": 150}, {"referenceID": 5, "context": "From this loss, we use backpropagation through structure to obtain gradients on the parameters (Goller and Kuchler, 1996).", "startOffset": 95, "endOffset": 121}, {"referenceID": 5, "context": "From this loss, we use backpropagation through structure to obtain gradients on the parameters (Goller and Kuchler, 1996). Training is performed using stochastic gradient descent. For simplicity, we follow Zirn et al. (2011) and focus on the distinction between contrastive and non-contrastive relations.", "startOffset": 96, "endOffset": 225}, {"referenceID": 32, "context": "Such an evaluation would seem to require a large corpus of texts with both gold RST parse trees and sentiment polarity labels; the SFU Review Corpus (Taboada, 2008) of 30 review texts offers a starting point, but is probably too small to train a competitive sentiment analysis system.", "startOffset": 149, "endOffset": 164}, {"referenceID": 7, "context": "Other efforts to incorporate RST into sentiment analysis have often focused on intrasentential discourse relations (Heerschop et al., 2011; Zhou et al., 2011; Chenlo et al., 2014), rather than relations over the entire document.", "startOffset": 115, "endOffset": 179}, {"referenceID": 43, "context": "Other efforts to incorporate RST into sentiment analysis have often focused on intrasentential discourse relations (Heerschop et al., 2011; Zhou et al., 2011; Chenlo et al., 2014), rather than relations over the entire document.", "startOffset": 115, "endOffset": 179}, {"referenceID": 0, "context": "Other efforts to incorporate RST into sentiment analysis have often focused on intrasentential discourse relations (Heerschop et al., 2011; Zhou et al., 2011; Chenlo et al., 2014), rather than relations over the entire document.", "startOffset": 115, "endOffset": 179}, {"referenceID": 0, "context": ", 2011; Chenlo et al., 2014), rather than relations over the entire document. Wang et al. (2012) address sentiment analysis in Chinese.", "startOffset": 8, "endOffset": 97}, {"referenceID": 0, "context": ", 2011; Chenlo et al., 2014), rather than relations over the entire document. Wang et al. (2012) address sentiment analysis in Chinese. Lacking a discourse parser, they focus on explicit connectives, using a strategy that is related to our discourse depth reweighting. Wang and Wu (2013) use manually-annotated discourse parses in combination with a sentiment lexicon, which is automatically updated based on the discourse structure.", "startOffset": 8, "endOffset": 288}, {"referenceID": 0, "context": ", 2011; Chenlo et al., 2014), rather than relations over the entire document. Wang et al. (2012) address sentiment analysis in Chinese. Lacking a discourse parser, they focus on explicit connectives, using a strategy that is related to our discourse depth reweighting. Wang and Wu (2013) use manually-annotated discourse parses in combination with a sentiment lexicon, which is automatically updated based on the discourse structure. Zirn et al. (2011) use an RST parser in a Markov Logic Network, with the goal of making polarity predictions at the sub-sentence level, rather than improving document-level prediction.", "startOffset": 8, "endOffset": 453}, {"referenceID": 34, "context": "PDTB relations are often signaled with explicit discourse connectives, and these may be used as a feature (Trivedi and Eisenstein, 2013; Lazaridou et al., 2013) or as posterior constraints (Yang and Cardie, 2014).", "startOffset": 106, "endOffset": 160}, {"referenceID": 16, "context": "PDTB relations are often signaled with explicit discourse connectives, and these may be used as a feature (Trivedi and Eisenstein, 2013; Lazaridou et al., 2013) or as posterior constraints (Yang and Cardie, 2014).", "startOffset": 106, "endOffset": 160}, {"referenceID": 41, "context": ", 2013) or as posterior constraints (Yang and Cardie, 2014).", "startOffset": 36, "endOffset": 59}, {"referenceID": 27, "context": "PDTB relations were shown to improve sentencelevel sentiment analysis by Somasundaran et al. (2009), and were incorporated in a model of sentiment flow by Wachsmuth et al.", "startOffset": 73, "endOffset": 100}, {"referenceID": 27, "context": "PDTB relations were shown to improve sentencelevel sentiment analysis by Somasundaran et al. (2009), and were incorporated in a model of sentiment flow by Wachsmuth et al. (2014). PDTB relations are often signaled with explicit discourse connectives, and these may be used as a feature (Trivedi and Eisenstein, 2013; Lazaridou et al.", "startOffset": 73, "endOffset": 179}, {"referenceID": 7, "context": "Heerschop et al. (2011) and Wachsmuth et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "Heerschop et al. (2011) and Wachsmuth et al. (2014) also employ the Pang and Lee (2004) dataset, but neither of their results are directly comparable: Heerschop et al.", "startOffset": 0, "endOffset": 52}, {"referenceID": 7, "context": "Heerschop et al. (2011) and Wachsmuth et al. (2014) also employ the Pang and Lee (2004) dataset, but neither of their results are directly comparable: Heerschop et al.", "startOffset": 0, "endOffset": 88}, {"referenceID": 7, "context": "Heerschop et al. (2011) and Wachsmuth et al. (2014) also employ the Pang and Lee (2004) dataset, but neither of their results are directly comparable: Heerschop et al. (2011) exclude documents that SPADE fails to parse, and Wachsmuth et al.", "startOffset": 0, "endOffset": 175}, {"referenceID": 7, "context": "Heerschop et al. (2011) and Wachsmuth et al. (2014) also employ the Pang and Lee (2004) dataset, but neither of their results are directly comparable: Heerschop et al. (2011) exclude documents that SPADE fails to parse, and Wachsmuth et al. (2014) evaluates only on individual sentences rather than entire documents.", "startOffset": 0, "endOffset": 248}, {"referenceID": 7, "context": "Heerschop et al. (2011) and Wachsmuth et al. (2014) also employ the Pang and Lee (2004) dataset, but neither of their results are directly comparable: Heerschop et al. (2011) exclude documents that SPADE fails to parse, and Wachsmuth et al. (2014) evaluates only on individual sentences rather than entire documents. The only possible direct comparison is with very recent work from Hogenboom et al. (2015), who employ a weighting scheme that is similar to the approach described in Section 3.", "startOffset": 0, "endOffset": 407}], "year": 2017, "abstractText": "Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that reweighting discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers significant improvements over classificationbased methods.", "creator": "LaTeX with hyperref package"}}}