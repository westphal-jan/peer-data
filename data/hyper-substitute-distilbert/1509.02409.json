{"id": "1509.02409", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2015", "title": "Data-selective Transfer Learning for Multi-Domain Speech Recognition", "abstract": "negative data in function of acoustic designers for automatic discourse recognition has been reported in several contexts such light emotion change or speaker genetics. this paper proposes having better technique way overcome pathogen aggregation by efficient simulation of speech molecules for acoustic model training. here data is plotted on screening unto a specific target. a reliable, based all likelihood ratios strategies used does determine how acoustically similar because training step is assigned a target selection chamber. the approach, evaluated against a grey - circle data set, including speech from radio and tv broadcasts, telephone conversations, meetings, lectures and read rooms. experiments demonstrate sometimes the screening technique intentionally finds relevant data because risks temporal transfer. variables on static 6 - - digit test session generate a relative improvement of 45 % with data on over - all data. plp based models, and 2 % about dnn accuracy.", "histories": [["v1", "Tue, 8 Sep 2015 15:20:12 GMT  (71kb,D)", "http://arxiv.org/abs/1509.02409v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.SD", "authors": ["mortaza doulaty", "oscar saz", "thomas hain"], "accepted": false, "id": "1509.02409"}, "pdf": {"name": "1509.02409.pdf", "metadata": {"source": "CRF", "title": "Data\u2013selective Transfer Learning for Multi\u2013Domain Speech Recognition", "authors": ["Mortaza Doulaty", "Oscar Saz", "Thomas Hain"], "emails": ["t.hain}@sheffield.ac.uk"], "sections": [{"heading": null, "text": "matic speech recognition has been reported in several contexts such as domain change or speaker characteristics. This paper proposes a novel technique to overcome negative transfer by efficient selection of speech data for acoustic model training. Here data is chosen on relevance for a specific target. A submodular function based on likelihood ratios is used to determine how acoustically similar each training utterance is to a target test set. The approach is evaluated on a wide\u2013domain data set, covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech. Experiments demonstrate that the proposed technique both finds relevant data and limits negative transfer. Results on a 6\u2013hour test set show a relative improvement of 4% with data selection over using all data in PLP based models, and 2% with DNN features. Index Terms: data selection, transfer learning, negative transfer, speech recognition"}, {"heading": "1. Introduction", "text": "As Automatic Speech Recognition (ASR) systems improve their accuracy, new applications and domains become the target of research. Automatic transcription of speech with unknown origin is a challenging task, which is related to access to so\u2013called \u201cfound data\u201d, such as media and historical audio archives. For this to be feasible, ASR has to produce an accurate output for whichever the conditions contained in the target data (e.g. interviews, distant recordings, telephone conversations, etc). Training acoustic models for an unknown domain, e.g. YouTube recordings, can be infeasible if the origin of the target speech can not be properly assessed, and the loss of accuracy can be large due to wrong modelling decisions. Another option is to train an acoustic model on a large amount of data from multiple domains, although this is not guaranteed to give the most optimal results.\nMaximum Likelihood Estimation (MLE) of Gaussian Mixture Model (GMM) parameters of a Hidden Markov Model (HMM) is still a standard approach to train acoustic models in ASR, either with perceptually\u2013based features like Perceptual Linear Prediction (PLP) features [1], or with Deep Neural Network (DNN) based features [2] in tandem configuration. However, MLE has two well\u2013known requirements: first, model correctness is assumed; and second the amount of training data is required to be infinite [3]. None of the above are valid in standard situations in ASR, although systems are sometimes trained with many years of speech data (e.g [4]). However, adding more data does not guarantee that the performance of the system will improve, and even if it does, the gains become smaller and smaller [5]. A further effect, negative transfer, is found in several examples, which indicates that knowledge acquired for a task can have a negative performance effect in another task\n[6]. As a result, being able to select informative training data remains an important task.\nThis paper studies positive and negative transfer in ASR in a multi\u2013domain scenario. The work proposes to use submodular functions based on acoustic similarity between the target test set and training data, in which positive transfer will be exploited to improve performance across domains, while reducing the impact of negative transfer at the same time. Submodular functions have been successfully used before to select data in semisupervised training and active learning for ASR tasks [7, 8]. However, here we show that these can also be used to select acoustically matching data in an un\u2013supervised manner.\nThis paper is structured as follows: Section 2 provides a review of data selection techniques for ASR, and Section 3 introduces the proposed approach for data selection. Section 4 describes the experimental setup, followed by results and analysis in Section 5. The final Section 6 summarises and concludes the paper."}, {"heading": "2. Data selection for ASR", "text": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13]. Here the objective is, given a large pool of training data, to find a subset of data such that a model set trained with that data will achieve comparable performance to a model set trained with all the data. This line of work is related to active learning, where the aim is to select a subset for manual transcription with the least budget [14, 15, 16], and with unsupervised and semi\u2013 supervised learning techniques, where the overall objective is to select a subset of the training set with the most reliable available transcripts [17, 18, 19].\nTwo techniques are typically used for selecting data: uncertainty sampling [20], where the scores from an existing model are used to choose or reject data; and query by committee [21], where votes of distinctly trained models are used [7]. For uncertainty sampling two types of scores have been explored. Confidence scores are used to select data with the most reliable transcriptions, as in semi\u2013supervised training [17, 4], or to select data for manual transcription in active learning [15, 14]. Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].\nThe use of submodular functions has been proposed to tackle the effect of the diminishing returns, when adding more data to a training set [8, 5, 7]. A submodular function is defined as any function f : 2\u2126 \u2192 R that fulfils\nf(S) + f(T ) \u2265 f(S \u222a T ) + f(S \u2229 T ), \u2200S, T \u2286 \u2126 (1)\nWith submodular functions the problem of data selection turns into a submodular maximisation problem, where the ob-\nar X\niv :1\n50 9.\n02 40\n9v 1\n[ cs\n.L G\n] 8\nS ep\n2 01\n5\njective is to find a subset S from the complete training set \u2126 so that any new subset T added to S will not increase the value of the submodular function f :\nargmax S\u2286\u2126\n{f(S)|f(S \u222a T ) < f(S), T \u2286 \u2126 \\ S} (2)\nFinding S is an NP\u2013hard problem [22, 8] and greedy solutions are proposed where the subset S is increased iteratively by the item s \u2208 \u2126 that maximises the value of f when added to S as in Equation 3.\ns = argmax s\u2208\u2126\\S\n{f(S \u222a {s})} (3)\nThe set S is obtained when either the optimal S is found (f(S) > f(S \u222a {s})), or a budget N is reached (|S| \u2264 N ).\nIf the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7]\nSeveral functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7]."}, {"heading": "3. Likelihood ratio data selection", "text": "To decide whether data bears resemblance to a training set, one can opt for a classification approach that identifies an item to be suitable or not. Here we make use of the Likelihood Ratio (LR) between a GMM trained on the target data (\u0398tgt), and a GMM trained on the complete training set (\u0398\u2126). The total LR of an utterance in the training set LR(O),O \u2208 \u2126 of length T frames is defined as the geometric mean of the frame\u2013based LR values of the target data model \u0398tgt and the background model \u0398\u2126, assuming frame independence.\nLR(O) = 1 T T\u2211 t=1 p(Ot|\u0398tgt) p(Ot|\u0398\u2126)\n(4)\nOne can define a modular function [22] based on the accumulated LRs of all utterances included in a subset S \u2286 \u2126 in the following form:\nfLR(S) = \u2211 O\u2208S ( LR(O) ) . (5)\nModular functions are a special case of submodular functions [22] where the greater than or equal sign in Equation 1 changes to the equal sign. This way, the proposed function fLR is submodular as well. And since all of the values for LR are non\u2013negative, and therefore any sum of these numbers, as constituted by the function f , the function is necessarily monotonic with expanding sets (A \u2286 B \u2286 \u2126, f(A) \u2264 f(B)). If a submodular function is non\u2013decreasing and normalised (f(\u2205) = 0), then the greedy solution obtained by Equation 3 is no worse than the optimal value by a constant fraction (1\u2212 1/e) [23]. Thus the subset S (greedy solution) can be used as the training set. The stopping criterion for adding more data to this subset S is based on a \u201cbudget\u201d, in the form of a maximum amount of hours of speech to be used."}, {"heading": "4. Experimental setup", "text": "To evaluate the proposed approach in a multi\u2013domain ASR task, a data set combining 6 different types of data was chosen from the following sources:\n\u2022 Radio (RD): BBC Radio4 broadcasts on February 2009. \u2022 Television (TV): Broadcasts from BBC on May 2008. \u2022 Telephone speech (CT): From the Fisher corpus1 [25]. \u2022 Meetings (MT): From AMI [26] and ICSI [27] corpora. \u2022 Lectures (TK): From TedTalks [28]. \u2022 Read speech (RS): From the WSJCAM0 corpus [29].\nA subset of 10h from each domain was selected to form the training set (60h in total), and 1h from each domain was used for testing (6h in total). The selection of the domains aims to cover the most common and distinctive types of audio recordings used in ASR tasks.\nTwo types of acoustic features were used: first, 13 PLP features plus first and second derivatives for a total of 39\u2013 dimensional feature vectors; and second, a 65\u2013dimensional feature vector concatenating the 39 PLP features and 26 bottleneck (BN) features extracted from a 4\u2013hidden\u2013layer DNN trained on the full 60 hours of data. 31 adjacent frames (15 frames to the left and 15 frames to the right) of 23 dimensional log Mel filter bank features were concatenated to form a 713\u2013dimensional super vector; Discrete Cosine Transform (DCT) was applied to this super vector to de\u2013correlate and compress it to 368 dimensions and then fed into the neural network. The network was trained on 4,000 triphone state targets and the 26 dimensional bottleneck layer was placed before the output layer. The objective function used for training was frame\u2013level cross\u2013entropy and the optimisation was performed with stochastic gradient descent using the backpropagation algorithm. DNN training was performed with the TNet toolkit [30] and more details can be found at [31]. For both types of features, MLE\u2013based GMM\u2013 HMM models were trained using HTK [32] with 5\u2013state crossword triphones and 16 gaussians per state. The language model was based on a 50,000\u2013word vocabulary and was trained by combination of component language models for each of the 6 domains. The interpolation weights were tuned using an independent development set."}, {"heading": "4.1. Baseline results", "text": "Table 1 presents results using both types of acoustic features. These results show the large variety in performance among domains, from 17\u201318% for read speech and radio broadcasts to 51% for television broadcasts. The use of DNN front\u2013ends provides a 25% relative improvement in performance against PLP features; which is consistent across domains and follows results previously seen in the literature [33]."}, {"heading": "5. Results", "text": "An initial set of experiments was conducted to identify and measure negative transfer in ASR tasks, and an evaluation of the proposed data selection technique was performed.\n1All of the telephone speech data was up\u2013sampled to 16 kHz to match the sampling rate of the rest of the data."}, {"heading": "5.1. Evaluation of negative transfer", "text": "Six different domain\u2013dependent MLE models were trained from the 10 hours of training data for each domain (in all of the experiments PLP features were used, unless stated otherwise). Each of these models was then used to decode the complete test set. The results in Table 2 show that in\u2013domain results (when the train and test data match based on manually labelled domains) are not greatly different from those obtained with a model trained on 60\u2013hour training set. Instead, cross\u2013domain scores (train and test are mismatched) result in considerable performance decreases everywhere.\nA second set of experiments was performed with models trained on 20 hours of data, using data from every possible pair of domains, for a total of 30 new acoustic models. Figure 1 shows the results in terms of relative improvement and degradation over the results of the 10\u2013hour in\u2013domain models. The rows of Figure 1 represent the testing domain and the columns represent the domain that was added in training to the data of the domain of the row. Positive values (blue squares) mark the existence of positive transfer, such as adding TV data to Radio data (7% improvement) or adding Radio data to Lecture data (4% improvement). But negative values (red squares) mark negative transfer, like adding Telephone data to Read speech (16% loss) or adding Read speech to Lecture data (5% loss).\nThese results showed that positive and negative transfer occurred across domains, possibly due to similarities and differences in speech styles, acoustic channels and background conditions. However a rule\u2013based optimisation of the best model for each target domain would require a complex and error\u2013prone\nprocess. The next experiments aimed to evaluate how an automatic selection of training could exploit positive transfer, while restricting negative transfer."}, {"heading": "5.2. Data selection based on budget", "text": "The data selection technique proposed in Section 3 was evaluated next. For each of the six target test domains, Gaussian Mixture Models (GMM) with 512 mixtures were trained (\u0398tgt1:6 ), and a background 512\u2013mixture GMM (\u0398\u2126) was trained from the complete training set of 60 hours. These GMMs were used to calculate the LR value for each training utterance (LR(O)) in order select the training data according to the acoustic similarity.\nThe first evaluation was performed using data selection based on budget. Five possible budgets of 10, 20, 30, 40 and 50 hours were designed for each test domain and the respective training data was chosen using the fLR(S) submodular function. Figure 2 shows relative improvement for each domain and budget against the results with the 60\u2013hour model. The graphs show that all domains improve performance as the budget increases until a certain limit is reached, then negative transfer decreases the performance, converging to the WER achieved with the 60\u2013hour trained model.\nIn order to observe which types of data were selected for each domain with the different budgets, Figure 3 presents the percentage of training data selected for each test domain with a 10\u2013hour budget. While the majority of the data was chosen\nfrom the same domain, some cross\u2013domain data was also selected, indicating positive transfer between domains. This occurred, for instance, with TV and Read speech data towards Radio data; and Lecture data towards TV data."}, {"heading": "5.3. Automatic decision on budget", "text": "An issue that can arise with the evaluated budget\u2013based proposal is the fact that a decision on a budget has to be made, and as the results in Figure 2 suggest, the optimal budget varies across different domains. A method for deciding a budget for a given target domain was proposed by selecting only utterances whose likelihood\u2013ratio is above a threshold defined as the mean of the highest\u2013weighted mixture of a GMM fitted to the distribution of likelihood ratios. The use of the mixture with the highest weight avoids the influence of outliers in the distribution of the LR values.\nThe experiments with an automatic budget decistion were performed for both types of features, PLP and PLP+BN. Table 3 presents the results for these experiments and compares them to the outcome of data selection based on a 30\u2013hour budget, which was the best fixed budget from Figure 2. The results showed that the use of an automatically derived threshold improved the results obtained with a fixed budget for both types of features, indicating that the proposed method could estimate the right amount of data to select for each target domain.\nThe amount of data selected for each domain is presented in Table 4. This Table shows how Read speech and Conversational Telephone speech are the ones which benefited from a lower amount of training data (20 hours or less), while the rest of the domains preferred more data (from 30 to 40 hours). These values were consistent with the patterns of positive and negative transfer observed in Figure 2."}, {"heading": "6. Conclusion", "text": "In this paper, the effect of positive and negative transfer across widely diverse domains in ASR was explored. We confirmed that the use of more data in MLE\u2013based acoustic models does not always provide increases in performance. A submodular function based on Likelihood Ratio was proposed and used to perform an informed and efficient selection of data for different target test sets. The evaluation of selection techniques based on budget and on automatic budget decision has achieved gains of 4% over a 60\u2013hour MLE model for PLP features and 2% for PLP+BN features.\nPrevious works have shown that data selection techniques can result in data sets biased towards specific groups of phones or triphones [19]. A phonetic analysis of the data sets given by the likelihood ratio function used in this paper did not show any bias on phones in these data sets. The 60\u2013hour training data used in this work was well balanced phonetically which limited the risk of phonetic biases in the selected data. In situations where the original training data might present less well distributed phonetic content, the proposed function should be complemented by a function that takes into account the resulting phone distribution of the data.\nFuture work should explore similar data selection techniques for other training criteria besides MLE. The presented methods are based on LR and hence well\u2013suited for MLE, but other submodular functions will be required to cater for needs given by discriminative objective functions such as Minimum Phone Error training. Further work should also investigate data selection techniques for datasets larger than the one studied here, and in completely mismatched conditions and using different features that better describe the background\u2019s acoustic characteristics [34].\nThe technique presented in this paper can be used for building targeted models for \u201cfound speech data\u201d. The ability of using very diverse data sets to transcribe newly found sets of speech recorded in unknown conditions is especially necessary to deal with this type of data. Other tasks, such as the automatic transcription of multi\u2013genre media archives might also potentially benefit from the advances achieved in this work."}, {"heading": "7. Acknowledgements", "text": "This work was supported by the EPSRC Programme Grant EP/I031022/1 Natural Speech Technology (NST)."}, {"heading": "8. Data Access Statement", "text": "The speech data used in this paper was obtained from the following sources: Fisher Corpus (LDC catalogue number LDC2004T19), ICSI Meetings corpus (LDC catalogue number LDC2004S02), WSJCAM0 (LDC catalogue number LDC95S24), AMI corpus (DOI number 10.1007/11677482 3), TedTalks data (freely available as part of the IWSLT evaluations), BBC Radio and TV data (this data was distributed to the NST project\u2019s partners with an agreement with BBC R&D and not publicly available yet).\nThe specific file lists used for training and testing in the experiments in this paper, as well as result files can be downloaded from http://mini.dcs.shef.ac.uk/ publications/papers/is15-doulaty."}, {"heading": "9. References", "text": "[1] H. Hermansky, \u201cPerceptual linear predictive (PLP) analysis of\nspeech,\u201d The Journal of the Acoustical Society of America, vol. 87, no. 4, pp. 1738\u20131752, 1990.\n[2] F. Grezl, M. Karafia\u0301t, S. Konta\u0301r, and J. Cernocky\u0300, \u201cProbabilistic and bottle-neck features for LVCSR of meetings.\u201d in Proceedings of ICASSP, Hawaii, USA, 2007, pp. 757\u2013760.\n[3] X. Huang, A. Acero, and H. Hon, Spoken language processing. Prentice Hall: Englewood Cliffs, 2001.\n[4] O. Kapralova, J. Alex, E. Weinstein, P. Moreno, and O. Siohan, \u201cA big data approach to acoustic model training corpus selection,\u201d in Proceedings of Interspeech, Singapore, 2014, pp. 2083\u20132087.\n[5] K. Wei, Y. Liu, K. Kirchhoff, and J. Bilmes, \u201cUnsupervised submodular subset selection for speech data,\u201d in Proceedings of ICASSP, Florence, Italy, 2014.\n[6] M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G. Dietterich, \u201cTo transfer or not to transfer,\u201d in NIPS 2005 Workshop on Transfer Learning, vol. 898, 2005.\n[7] H. Lin and J. Bilmes, \u201cHow to select a good training-data subset for transcription: Submodular active selection for sequences,\u201d in Proceedings of Interspeech, Brighton, UK, 2009.\n[8] K. Wei, Y. Liu, K. Kirchhoff, C. Bartels, and J. Bilmes, \u201cSubmodular subset selection for large-scale speech training data,\u201d in Proceedings of ICASSP, Florence, Italy, 2014.\n[9] Y. Wu, R. Zhang, and A. Rudnicky, \u201cData selection for speech recognition,\u201d in Proceedings of ASRU, Kyoto, Japan, 2007, pp. 562\u2013565.\n[10] R. Zhang and A. Rudnicky, \u201cA new data selection approach for semi-supervised acoustic modeling,\u201d in Proceedings of ICASSP, Toulouse, France, 2006.\n[11] A. Nagroski, L. Boves, and H. Steeneken, \u201cIn search of optimal data selection for training of automatic speech recognition systems,\u201d in Proceedings of ASRU, St. Thomas, US Virgin Islands, 2003, pp. 67\u201372.\n[12] E. Gouvea and M. H. Davel, \u201cKullback-Leibler divergence-based ASR training data selection,\u201d in Proceedings of Interspeech, Florence, Italy, 2011, pp. 2297\u20132300.\n[13] O. Siohan and M. Bacchiani, \u201ciVector-based acoustic data selection.\u201d in Proceedings of Interspeech, Lyon, France, 2013, pp. 657\u2013 661.\n[14] G. Riccardi and D. Hakkani-Tu\u0308r, \u201cActive and unsupervised learning for automatic speech recognition.\u201d in proceedings of Interspeech, Geneva, Switzerland, 2003.\n[15] G. Tur, R. Schapire, and D. Hakkani-Tu\u0308r, \u201cActive learning for spoken language understanding,\u201d in Proceedings of ICASSP, Hong Kong, 2003.\n[16] B. Settles, \u201cActive learning literature survey,\u201d University of Wisconsin, Madison, WI, USA, Tech. Rep., 2010.\n[17] F. Wessel and H. Ney, \u201cUnsupervised training of acoustic models for large vocabulary continuous speech recognition,\u201d IEEE Transactions on Speech and Audio Processing, vol. 13, no. 1, pp. 23\u201331, 2005.\n[18] P. Lanchantin, P. J. Bell, M. J. Gales, T. Hain, X. Liu, Y. Long, J. Quinnell, S. Renals, O. Saz, and M. S. Seigel, \u201cAutomatic transcription of multi-genre media archives,\u201d in Proceedings of SLAM Workshop, Marseille, France, 2013.\n[19] O. Siohan, \u201cTraining data selection based on context-dependent state matching,\u201d in Proceedings of ICASSP, Florence, Italy, 2014, pp. 3316\u20133319.\n[20] X. Zhu, \u201cSemi-supervised learning literature survey,\u201d University of Wisconsin, Madison, WI, USA, Tech. Rep., 2005.\n[21] H. S. Seung, M. Opper, and H. Sompolinsky, \u201cQuery by committee,\u201d in Proceedings of COLT Workshop, Pittsburgh, PA, USA, 1992, pp. 287\u2013294.\n[22] A. Krause and D. Golovin, \u201cSubmodular function maximization,\u201d Tractability: Practical Approaches to Hard Problems, 2014.\n[23] G. Nemhauser, L. Wolsey, and M. Fisher, \u201cAn analysis of approximations for maximizing submodular set functions I,\u201d Mathematical Programming, vol. 14, no. 1, pp. 265\u2013294, 1978.\n[24] K. Wei, Y. Liu, K. Kirchhoff, and J. Bilmes, \u201cUsing document summarization techniques for speech data subset selection.\u201d in Proceedings of HLT-NAACL, Atlanta, USA, 2013, pp. 721\u2013726.\n[25] C. Cieri, D. Miller, and K. Walker, \u201cThe Fisher corpus: A resource for the next generations of speech-to-text.\u201d in Proceedings of LREC, Lisbon, Portugal, 2004, pp. 69\u201371.\n[26] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, W. Karaiskos, Vasilis Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner, \u201cThe AMI meeting corpus: A preannouncement,\u201d in Proceedings of MLMI, Bethesda, USA, 2006, pp. 28\u201339.\n[27] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters, \u201cThe ICSI meeting corpus,\u201d in Proceedings of ICASSP, Hong Kong, 2003.\n[28] R. W. N. Ng, M. Doulaty, R. Doddipatla, O. Saz, M. Hasan, T. Hain, W. Aziz, K. Shaf, and L. Specia, \u201cThe USFD spoken language translation system for IWSLT 2014,\u201d Lake Tahoe, USA, 2014.\n[29] T. Robinson, J. Fransen, D. Pye, J. Foote, and S. Renals, \u201cWSJCAM0: A british english speech corpus for large vocabulary continuous speech recognition,\u201d in Proceedings of ICASSP, Detroit, USA, 1995.\n[30] K. Vesely, L. Burget, and F. Grezl, \u201cParallel training of neural networks for speech recognition,\u201d in Proceedings of Interspeech, Makuhari, Japan, 2010.\n[31] Y. Liu, P. Zhang, and T. Hain, \u201cUsing neural network front-ends on far field multiple microphones based speech recognition,\u201d in Proceedings of ICASSP, Florence, Italy, 2014.\n[32] S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw, X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey et al., \u201cThe HTK book (for HTK version 3.4),\u201d Cambridge university engineering department, vol. 2, no. 2, 2006.\n[33] Z.-J. Yan, Q. Huo, and J. Xu, \u201cA scalable approach to using dnnderived features in gmm-hmm based acoustic modeling for lvcsr.\u201d in Proceedings of Interspeech, Lyon, France, 2013, pp. 104\u2013108.\n[34] O. Saz, M. Doulaty, and T. Hain, \u201cBackground\u2013tracking acoustic features for genre identification of broadcast shows,\u201d Lake Tahoe NV, USA, 2014."}], "references": [{"title": "Perceptual linear predictive (PLP) analysis of speech", "author": ["H. Hermansky"], "venue": "The Journal of the Acoustical Society of America, vol. 87, no. 4, pp. 1738\u20131752, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Cernock\u1ef3, \u201cProbabilistic and bottle-neck features for LVCSR of meetings.", "author": ["F. Grezl", "M. Karafi\u00e1t", "S. Kont\u00e1r"], "venue": "Proceedings of ICASSP, Hawaii,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "A big data approach to acoustic model training corpus selection", "author": ["O. Kapralova", "J. Alex", "E. Weinstein", "P. Moreno", "O. Siohan"], "venue": "Proceedings of Interspeech, Singapore, 2014, pp. 2083\u20132087.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised submodular subset selection for speech data", "author": ["K. Wei", "Y. Liu", "K. Kirchhoff", "J. Bilmes"], "venue": "Proceedings of ICASSP, Florence, Italy, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "To transfer or not to transfer", "author": ["M.T. Rosenstein", "Z. Marx", "L.P. Kaelbling", "T.G. Dietterich"], "venue": "NIPS 2005 Workshop on Transfer Learning, vol. 898, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "How to select a good training-data subset for transcription: Submodular active selection for sequences", "author": ["H. Lin", "J. Bilmes"], "venue": "Proceedings of Interspeech, Brighton, UK, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Submodular subset selection for large-scale speech training data", "author": ["K. Wei", "Y. Liu", "K. Kirchhoff", "C. Bartels", "J. Bilmes"], "venue": "Proceedings of ICASSP, Florence, Italy, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Data selection for speech recognition", "author": ["Y. Wu", "R. Zhang", "A. Rudnicky"], "venue": "Proceedings of ASRU, Kyoto, Japan, 2007, pp. 562\u2013565.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "A new data selection approach for semi-supervised acoustic modeling", "author": ["R. Zhang", "A. Rudnicky"], "venue": "Proceedings of ICASSP, Toulouse, France, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "In search of optimal data selection for training of automatic speech recognition systems", "author": ["A. Nagroski", "L. Boves", "H. Steeneken"], "venue": "Proceedings of ASRU, St. Thomas, US Virgin Islands, 2003, pp. 67\u201372.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Kullback-Leibler divergence-based ASR training data selection", "author": ["E. Gouvea", "M.H. Davel"], "venue": "Proceedings of Interspeech, Florence, Italy, 2011, pp. 2297\u20132300.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "iVector-based acoustic data selection.", "author": ["O. Siohan", "M. Bacchiani"], "venue": "Proceedings of Interspeech, Lyon, France,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Active learning for spoken language understanding", "author": ["G. Tur", "R. Schapire", "D. Hakkani-T\u00fcr"], "venue": "Proceedings of ICASSP, Hong Kong, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin, Madison, WI, USA, Tech. Rep., 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised training of acoustic models for large vocabulary continuous speech recognition", "author": ["F. Wessel", "H. Ney"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 13, no. 1, pp. 23\u201331, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic transcription of multi-genre media archives", "author": ["P. Lanchantin", "P.J. Bell", "M.J. Gales", "T. Hain", "X. Liu", "Y. Long", "J. Quinnell", "S. Renals", "O. Saz", "M.S. Seigel"], "venue": "Proceedings of SLAM Workshop, Marseille, France, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Training data selection based on context-dependent state matching", "author": ["O. Siohan"], "venue": "Proceedings of ICASSP, Florence, Italy, 2014, pp. 3316\u20133319.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "University of Wisconsin, Madison, WI, USA, Tech. Rep., 2005.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Query by committee", "author": ["H.S. Seung", "M. Opper", "H. Sompolinsky"], "venue": "Proceedings of COLT Workshop, Pittsburgh, PA, USA, 1992, pp. 287\u2013294.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1992}, {"title": "Submodular function maximization", "author": ["A. Krause", "D. Golovin"], "venue": "Tractability: Practical Approaches to Hard Problems, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "An analysis of approximations for maximizing submodular set functions I", "author": ["G. Nemhauser", "L. Wolsey", "M. Fisher"], "venue": "Mathematical Programming, vol. 14, no. 1, pp. 265\u2013294, 1978.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1978}, {"title": "Using document summarization techniques for speech data subset selection.", "author": ["K. Wei", "Y. Liu", "K. Kirchhoff", "J. Bilmes"], "venue": "Proceedings of HLT-NAACL, Atlanta,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "The Fisher corpus: A resource for the next generations of speech-to-text.", "author": ["C. Cieri", "D. Miller", "K. Walker"], "venue": "Proceedings of LREC, Lisbon, Portugal,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "The AMI meeting corpus: A preannouncement", "author": ["J. Carletta", "S. Ashby", "S. Bourban", "M. Flynn", "M. Guillemot", "T. Hain", "J. Kadlec", "W. Karaiskos", "Vasilis Kraaij", "M. Kronenthal", "G. Lathoud", "M. Lincoln", "A. Lisowska", "I. McCowan", "W. Post", "D. Reidsma", "P. Wellner"], "venue": "Proceedings of MLMI, Bethesda, USA, 2006, pp. 28\u201339.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "The ICSI meeting corpus", "author": ["A. Janin", "D. Baron", "J. Edwards", "D. Ellis", "D. Gelbart", "N. Morgan", "B. Peskin", "T. Pfau", "E. Shriberg", "A. Stolcke", "C. Wooters"], "venue": "Proceedings of ICASSP, Hong Kong, 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "The USFD spoken language translation system for IWSLT 2014", "author": ["R.W.N. Ng", "M. Doulaty", "R. Doddipatla", "O. Saz", "M. Hasan", "T. Hain", "W. Aziz", "K. Shaf", "L. Specia"], "venue": "Lake Tahoe, USA, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "WSJ- CAM0: A british english speech corpus for large vocabulary continuous speech recognition", "author": ["T. Robinson", "J. Fransen", "D. Pye", "J. Foote", "S. Renals"], "venue": "Proceedings of ICASSP, Detroit, USA, 1995.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1995}, {"title": "Parallel training of neural networks for speech recognition", "author": ["K. Vesely", "L. Burget", "F. Grezl"], "venue": "Proceedings of Interspeech, Makuhari, Japan, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Using neural network front-ends on far field multiple microphones based speech recognition", "author": ["Y. Liu", "P. Zhang", "T. Hain"], "venue": "Proceedings of ICASSP, Florence, Italy, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "The HTK book (for HTK version 3.4)", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey"], "venue": "Cambridge university engineering department, vol. 2, no. 2, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "A scalable approach to using dnnderived features in gmm-hmm based acoustic modeling for lvcsr.", "author": ["Z.-J. Yan", "Q. Huo", "J. Xu"], "venue": "Proceedings of Interspeech,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Background\u2013tracking acoustic features for genre identification of broadcast shows", "author": ["O. Saz", "M. Doulaty", "T. Hain"], "venue": "Lake Tahoe NV, USA, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Maximum Likelihood Estimation (MLE) of Gaussian Mixture Model (GMM) parameters of a Hidden Markov Model (HMM) is still a standard approach to train acoustic models in ASR, either with perceptually\u2013based features like Perceptual Linear Prediction (PLP) features [1], or with Deep Neural Network (DNN) based features [2] in tandem configuration.", "startOffset": 261, "endOffset": 264}, {"referenceID": 1, "context": "Maximum Likelihood Estimation (MLE) of Gaussian Mixture Model (GMM) parameters of a Hidden Markov Model (HMM) is still a standard approach to train acoustic models in ASR, either with perceptually\u2013based features like Perceptual Linear Prediction (PLP) features [1], or with Deep Neural Network (DNN) based features [2] in tandem configuration.", "startOffset": 315, "endOffset": 318}, {"referenceID": 2, "context": "g [4]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "However, adding more data does not guarantee that the performance of the system will improve, and even if it does, the gains become smaller and smaller [5].", "startOffset": 152, "endOffset": 155}, {"referenceID": 4, "context": "A further effect, negative transfer, is found in several examples, which indicates that knowledge acquired for a task can have a negative performance effect in another task [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "Submodular functions have been successfully used before to select data in semisupervised training and active learning for ASR tasks [7, 8].", "startOffset": 132, "endOffset": 138}, {"referenceID": 6, "context": "Submodular functions have been successfully used before to select data in semisupervised training and active learning for ASR tasks [7, 8].", "startOffset": 132, "endOffset": 138}, {"referenceID": 3, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 6, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 7, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 5, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 8, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 9, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 2, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 10, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 11, "context": "Data selection for ASR has mostly been studied for minimal representative data selection [5, 8, 9, 7, 10, 11, 4, 12, 13].", "startOffset": 89, "endOffset": 120}, {"referenceID": 12, "context": "This line of work is related to active learning, where the aim is to select a subset for manual transcription with the least budget [14, 15, 16], and with unsupervised and semi\u2013 supervised learning techniques, where the overall objective is to select a subset of the training set with the most reliable available transcripts [17, 18, 19].", "startOffset": 132, "endOffset": 144}, {"referenceID": 13, "context": "This line of work is related to active learning, where the aim is to select a subset for manual transcription with the least budget [14, 15, 16], and with unsupervised and semi\u2013 supervised learning techniques, where the overall objective is to select a subset of the training set with the most reliable available transcripts [17, 18, 19].", "startOffset": 132, "endOffset": 144}, {"referenceID": 14, "context": "This line of work is related to active learning, where the aim is to select a subset for manual transcription with the least budget [14, 15, 16], and with unsupervised and semi\u2013 supervised learning techniques, where the overall objective is to select a subset of the training set with the most reliable available transcripts [17, 18, 19].", "startOffset": 325, "endOffset": 337}, {"referenceID": 15, "context": "This line of work is related to active learning, where the aim is to select a subset for manual transcription with the least budget [14, 15, 16], and with unsupervised and semi\u2013 supervised learning techniques, where the overall objective is to select a subset of the training set with the most reliable available transcripts [17, 18, 19].", "startOffset": 325, "endOffset": 337}, {"referenceID": 16, "context": "This line of work is related to active learning, where the aim is to select a subset for manual transcription with the least budget [14, 15, 16], and with unsupervised and semi\u2013 supervised learning techniques, where the overall objective is to select a subset of the training set with the most reliable available transcripts [17, 18, 19].", "startOffset": 325, "endOffset": 337}, {"referenceID": 17, "context": "Two techniques are typically used for selecting data: uncertainty sampling [20], where the scores from an existing model are used to choose or reject data; and query by committee [21], where votes of distinctly trained models are used [7].", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "Two techniques are typically used for selecting data: uncertainty sampling [20], where the scores from an existing model are used to choose or reject data; and query by committee [21], where votes of distinctly trained models are used [7].", "startOffset": 179, "endOffset": 183}, {"referenceID": 5, "context": "Two techniques are typically used for selecting data: uncertainty sampling [20], where the scores from an existing model are used to choose or reject data; and query by committee [21], where votes of distinctly trained models are used [7].", "startOffset": 235, "endOffset": 238}, {"referenceID": 14, "context": "Confidence scores are used to select data with the most reliable transcriptions, as in semi\u2013supervised training [17, 4], or to select data for manual transcription in active learning [15, 14].", "startOffset": 112, "endOffset": 119}, {"referenceID": 2, "context": "Confidence scores are used to select data with the most reliable transcriptions, as in semi\u2013supervised training [17, 4], or to select data for manual transcription in active learning [15, 14].", "startOffset": 112, "endOffset": 119}, {"referenceID": 12, "context": "Confidence scores are used to select data with the most reliable transcriptions, as in semi\u2013supervised training [17, 4], or to select data for manual transcription in active learning [15, 14].", "startOffset": 183, "endOffset": 191}, {"referenceID": 5, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 156, "endOffset": 166}, {"referenceID": 8, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 156, "endOffset": 166}, {"referenceID": 7, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 156, "endOffset": 166}, {"referenceID": 10, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 216, "endOffset": 228}, {"referenceID": 11, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 216, "endOffset": 228}, {"referenceID": 16, "context": "Entropy\u2013based methods aim to pick data that, for instance, fits a uniform distribution of target units (phonemes, words, etc), resulting in maximum entropy [7, 10, 9] or having a similar distribution to a target set [12, 13, 19].", "startOffset": 216, "endOffset": 228}, {"referenceID": 6, "context": "The use of submodular functions has been proposed to tackle the effect of the diminishing returns, when adding more data to a training set [8, 5, 7].", "startOffset": 139, "endOffset": 148}, {"referenceID": 3, "context": "The use of submodular functions has been proposed to tackle the effect of the diminishing returns, when adding more data to a training set [8, 5, 7].", "startOffset": 139, "endOffset": 148}, {"referenceID": 5, "context": "The use of submodular functions has been proposed to tackle the effect of the diminishing returns, when adding more data to a training set [8, 5, 7].", "startOffset": 139, "endOffset": 148}, {"referenceID": 19, "context": "Finding S is an NP\u2013hard problem [22, 8] and greedy solutions are proposed where the subset S is increased iteratively by the item s \u2208 \u03a9 that maximises the value of f when added to S as in Equation 3.", "startOffset": 32, "endOffset": 39}, {"referenceID": 6, "context": "Finding S is an NP\u2013hard problem [22, 8] and greedy solutions are proposed where the subset S is increased iteratively by the item s \u2208 \u03a9 that maximises the value of f when added to S as in Equation 3.", "startOffset": 32, "endOffset": 39}, {"referenceID": 20, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 151, "endOffset": 162}, {"referenceID": 19, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 151, "endOffset": 162}, {"referenceID": 5, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 151, "endOffset": 162}, {"referenceID": 21, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 323, "endOffset": 330}, {"referenceID": 6, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 323, "endOffset": 330}, {"referenceID": 3, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 359, "endOffset": 362}, {"referenceID": 5, "context": "If the function f is a normalised monotone submodular function, then the simple greedy algorithm provides a good approximation of the optimal solution [23, 22, 7] Several functions f can be found in the literature to perform data selection for ASR tasks, including facility location functions, saturated coverage functions [24, 8], diversity reward functions [5] or graph cut functions [7].", "startOffset": 386, "endOffset": 389}, {"referenceID": 19, "context": "One can define a modular function [22] based on the accumulated LRs of all utterances included in a subset S \u2286 \u03a9 in the following form:", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "Modular functions are a special case of submodular functions [22] where the greater than or equal sign in Equation 1 changes to the equal sign.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "If a submodular function is non\u2013decreasing and normalised (f(\u2205) = 0), then the greedy solution obtained by Equation 3 is no worse than the optimal value by a constant fraction (1\u2212 1/e) [23].", "startOffset": 185, "endOffset": 189}, {"referenceID": 22, "context": "\u2022 Telephone speech (CT): From the Fisher corpus [25].", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "\u2022 Meetings (MT): From AMI [26] and ICSI [27] corpora.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "\u2022 Meetings (MT): From AMI [26] and ICSI [27] corpora.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "\u2022 Lectures (TK): From TedTalks [28].", "startOffset": 31, "endOffset": 35}, {"referenceID": 26, "context": "\u2022 Read speech (RS): From the WSJCAM0 corpus [29].", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "DNN training was performed with the TNet toolkit [30] and more details can be found at [31].", "startOffset": 49, "endOffset": 53}, {"referenceID": 28, "context": "DNN training was performed with the TNet toolkit [30] and more details can be found at [31].", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "For both types of features, MLE\u2013based GMM\u2013 HMM models were trained using HTK [32] with 5\u2013state crossword triphones and 16 gaussians per state.", "startOffset": 77, "endOffset": 81}, {"referenceID": 30, "context": "The use of DNN front\u2013ends provides a 25% relative improvement in performance against PLP features; which is consistent across domains and follows results previously seen in the literature [33].", "startOffset": 188, "endOffset": 192}, {"referenceID": 16, "context": "Previous works have shown that data selection techniques can result in data sets biased towards specific groups of phones or triphones [19].", "startOffset": 135, "endOffset": 139}, {"referenceID": 31, "context": "Further work should also investigate data selection techniques for datasets larger than the one studied here, and in completely mismatched conditions and using different features that better describe the background\u2019s acoustic characteristics [34].", "startOffset": 242, "endOffset": 246}], "year": 2015, "abstractText": "Negative transfer in training of acoustic models for automatic speech recognition has been reported in several contexts such as domain change or speaker characteristics. This paper proposes a novel technique to overcome negative transfer by efficient selection of speech data for acoustic model training. Here data is chosen on relevance for a specific target. A submodular function based on likelihood ratios is used to determine how acoustically similar each training utterance is to a target test set. The approach is evaluated on a wide\u2013domain data set, covering speech from radio and TV broadcasts, telephone conversations, meetings, lectures and read speech. Experiments demonstrate that the proposed technique both finds relevant data and limits negative transfer. Results on a 6\u2013hour test set show a relative improvement of 4% with data selection over using all data in PLP based models, and 2% with DNN features.", "creator": "LaTeX with hyperref package"}}}