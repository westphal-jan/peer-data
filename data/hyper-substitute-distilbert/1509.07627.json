{"id": "1509.07627", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2015", "title": "Feature Evaluation of Deep Convolutional Neural Networks for Object Recognition and Detection", "abstract": "preceding this regression, subjects evaluate convolutional semiconductor device ( sim ) features using the alexnet architecture forming very deep convolutional array ( vggnet ) topology. to verify, cnn conceptual simulations however employed the combined layers analog output, which he excluded from the fully combined beta package. luckily, since it is unlikely some feature representation effectiveness overcome a less kernel problem, this study evaluates other convolutional layers that are adjacent near fully connected layers, effective addition to executing complicated tuning function feature calculations ( e. example., layer 3 + layer \u2192 + line r ) but ideally, showcasing technologies such were principal component analysis. combining our collaboration, we carried out detection and classification tasks through the caltech 101 and harvard panther benchmark datasets.", "histories": [["v1", "Fri, 25 Sep 2015 08:26:53 GMT  (1785kb)", "http://arxiv.org/abs/1509.07627v1", "5 pages, 3 figures"]], "COMMENTS": "5 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM", "authors": ["hirokatsu kataoka", "kenji iwata", "yutaka satoh"], "accepted": false, "id": "1509.07627"}, "pdf": {"name": "1509.07627.pdf", "metadata": {"source": "CRF", "title": "Feature Evaluation of Deep Convolutional Neural Networks for Object Recognition and Detection", "authors": ["Hirokatsu Kataoka", "Kenji Iwata", "Yutaka Satoh"], "emails": ["hirokatsu.kataoka@aist.go.jp,"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n07 62\n7v 1\n[ cs\n.C V\n] 2\n5 Se"}, {"heading": "1. Introduction", "text": "Over the past few years, convolutional neural networks (CNNs) have significantly improved from the standpoint of the network architectures needed to facilitate recognition accuracy and to reduce processing costs [15]. Currently, CNNs are primarily used to help users understand objects and scenes in an image. In our study, we applied a CNN to an ImageNet dataset containing over 1.4 million images and 1,000 object categories [13]. Use of such a large-scale dataset allows us to model a wide variety of object recognition image features. By using the pre-trained ImageNet dataset model, we found that CNN is capable of presenting significantly more effective feature variations.\nFor feature extraction, Donahue et al. employed CNN features as a feature vector by combining those features with a support vector machine (SVM) classifier [1], while other researchers have evaluated and visualized CNN features with an eight-layer AlexNet architecture [9]. More recent architectures utilize deep structures, such as the\nvery deep convolutional network (VGGNet) [16] and GoogLeNet [17], which were developed by Oxford University\u2019s Visual Geometry Group and Google Inc., respectively.\nAccording to He et al. [5], the most important CNN feature is deep architecture. Along this line, the VGGNet contains 16 to 19 layers and GoogLeNet utilizes 22 layers. VGGNet is frequently used in the computer vision field, not only in full scratch neural net models, but also as a feature generator. CNN\u2019s utility as a feature generator is also important because it can function well even if only a few learning samples are available. Thus, large-scale databases such as ImageNet can provide recognition rates that outperform human-level classification (e.g., [6, 14]). However, this performance will fluctuate depending on the amount and variance of the data. Therefore, when CNN is used for feature generation, it provides better performance for some recognition problems than others.\nDonahue et al. argued that usage should be limited to the last two layers before output, which are extracted from first and second fully connected layers in CNN features with AlexNet. However, we believe that more detailed evaluations should be undertaken since several different architectures have recently been proposed, and because middle layers have not been examined as feature descriptors. Accordingly, in this study, we performed more detailed experiments to evaluate two famous CNN architectures \u2013 AlexNet and VGGNet. In addition, we carried out simple tuning for feature concatenation (e.g., layer 3 + layer 5 + layer 7) and transformations (e.g., principal component analysis: PCA).\nThe rest of this paper is organized as follows. In Section 2, related works are listed. The feature settings are evaluated in Section 3. The results are shown in Section 4. Finally, we conclude the paper in Section 5."}, {"heading": "2. Related works", "text": "In the time since the neocognitron was first proposed by Fukushima [3], neuron-based recognition has become one of the most commonly used neural network architectures.\nFollowing that study, the LeNet-5 [10] neocognitron model added a baseline to CNNs in order to create a more significant model. Current network architectures include standard structures such as multiple fully connected layers, while recent challengers employ pre-trained [7], dropout [8], and rectified linear units (ReLU) [12] as improved learning models. The most outstanding computer vision result was obtained by AlexNet in the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC2012), which remains the image recognition leader, with 1,000 classes [9].\nAlexNet made it possible to increase the number of layers in network architectures. For example, Krizhevsky et al. implemented an eight-layer model that includes convolution, pooling, and fully connected layers. More recent variations, such as the 16- or 19-layer VGGNet [16], and the 22-layer GoogLeNet [17] models, have even deeper architectures. These deeper models outperform conventional models on the ILSVRC dataset [13]. More specifically, when compared to the AlexNet (top-five error rate on the ILSVRC2012: 16.4%), deeper models achieved better performance levels with GoogLeNet and VGGNet (top-five error rate on the ILSVRC2014: 6.7% for GoogLeNet and 7.3% for VGGNet). Currently, the object detection problem is one of the most important topics in computer vision. The existing state-of-the-art framework, regions with convolutional neural networks (R-CNN), was proposed by Girshick et al. [4]. This framework consists of two steps during which (i) object areas are extracted as object proposals, and (ii) CNN recognition is performed. Those authors adopted selective search [18] as an object proposal approach and\nVGGNet for the CNN architecture. However, while they restricted the object detection and recognition tasks to fully connected CNN features, we believe that the features of the other layers should be more carefully evaluated in order to determine whether they could provide more accurate recognition and detection."}, {"heading": "3. Feature settings and representations", "text": "In this paper, we evaluate two deep learning feature types. Figure 1 shows the architectures of AlexNet [9] and VGGNet [16]. We believe that while the evaluation itself is very important, particular attention must be paid to tunings such as concatenation and feature transformation. Basically, deep learning architectures are based on their approaches.\nFeature setting. We begin by extracting the middle and deeper layers. Layers 3\u20137 of AlexNet and VGGNet are shown in Figure 1. Next, we extract each max-pooling layer (layers 3\u20135), and the last two fully connected layers (layers 6 and 7) in VGGNet.\nConcatenation and transformation. Next, we concatenate neighboring or one-step layers such as layer-3,4,5 and layer-3,5,7. In feature transformation, we simply apply PCA, which is set at 1,500 dimensions in this experiment.\nClassifier. In the next step, we apply deep learning features and SVM for object recognition. The parameters are based on DeCAF [1]."}, {"heading": "4. Experiments", "text": "In this section, we discuss our experiments conducted using the Daimler pedestrian benchmark [11] and Caltech 101 [2] Datasets. Figure 2 and 3 show the results of our deep CNN feature evaluations on the Daimler and Caltech 101 datasets, respectively. The figures also show VGGNet, AlexNet, and their compressed features with PCA (VGGNet(PCA) and AlexNet(PCA)).\nIn the Daimler dataset experiment, we found that the\nVGGNet(PCA) layers 5 and 4 showed the best performance rates at 99.35% and 98.92%, respectively. We also determined that PCA transforms low-dimensional features and feature vectors at better rates than the original features. The VGGNet layer 5 (98.91%) and layer 4 (98.81%) are, respectively, +0.44% and +0.11% improved with PCA. When AlexNet is used, layers 3 and 4 show top rates of 98.71% and 97.95%, respectively. As for VGGNet, layers 5 and 6 achieved the best results (91.8%) on the Caltech 101 dataset. However, these results show significant layer 5 dif-"}, {"heading": "345 78.13 77.95", "text": "ferences between VGGNet (91.8%) and AlexNet (78.37%). From the above results, it can be seen that features obtained from fully connected layers do not always provide the highest performance rates during recognition and detection tasks, and that middle-layer features are more flexible for some tasks. We also found that fully connected layers or max-pooling layers located near fully connected layers tend to perform better in general object recognition tasks, such as the Caltech 101 dataset.\nThe main difference between AlexNet and VGGNet is the architecture depth. Additionally, VGGNet assigns very small 3 \u00d7 3 convolutional kernels against the 7 \u00d7 7 (Conv 1), 5 \u00d7 5 (Conv 2), and 3 \u00d7 3 (others) kernels in AlexNet. The settings refrain the feature representation.\nThe classification results of concatenated vectors are shown in Table 1 and 2. Here, it can be seen that concatenation of VGGNet layer-5,6,7 provides the highest levels of accuracy for both datasets. The rates are 99.38% on the Daimler dataset and 92.00% on the Caltech 101 dataset. For AlexNet, layer-3,4,5 and layer-3,5,7 achieved top performance rates on those datasets. The results show that combining features of the convolutional and fully connected layers provides better performance. It is especially noteworthy that VGGNet layer 5, which is near the fully connected layer, provides significantly high levels of feature extraction from an image patch."}, {"heading": "5. Conclusion", "text": "In this paper, we evaluated two different of convolutional neural network (CNN) architectures AlexNet and VGGNet. The convolutional features from layers 3\u20137 were performed on the Daimler pedestrian benchmark and Cal-\ntech 101 datasets. We then attempted to implement feature concatenation and PCA transformation. Our experimental results show that the fully connected layers did not always perform better for recognition tasks. Additionally, the experiments using the Daimler and Caltech 101 datasets showed that layer 5 tends to provide the highest level of accuracy, and that feature concatenation of convolutional and fully connected layers improves recognition performance."}], "references": [{"title": "Decaf:a deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshop on Generative-Model Based Vision (CVPRW),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift", "author": ["K. Fukushima"], "venue": "in position,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1980}, {"title": "Regionbased convolutional networks for accurate object detection and segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Convolutional neural networks at constrained time cost", "author": ["K. He", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Daimler mono pedestrian classification benchmark dataset: An experimental study on pedestrian", "author": ["S. Munder", "D.M. Gavrila"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Deep learning in neural networks: An overveiw", "author": ["J. Schmidhuber"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representation (ICLR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Selective search for object recognition", "author": ["J.R.R. Uijlings", "K.E.A. van de Sande", "T. Gevers", "A.W.M. Smeulders"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "In this paper, we evaluate convolutional neural network (CNN) features using the AlexNet architecture developed by [9] and very deep convolutional network (VGGNet) architecture developed by [16].", "startOffset": 115, "endOffset": 118}, {"referenceID": 15, "context": "In this paper, we evaluate convolutional neural network (CNN) features using the AlexNet architecture developed by [9] and very deep convolutional network (VGGNet) architecture developed by [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "Over the past few years, convolutional neural networks (CNNs) have significantly improved from the standpoint of the network architectures needed to facilitate recognition accuracy and to reduce processing costs [15].", "startOffset": 212, "endOffset": 216}, {"referenceID": 12, "context": "4 million images and 1,000 object categories [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "employed CNN features as a feature vector by combining those features with a support vector machine (SVM) classifier [1], while other researchers have evaluated and visualized CNN features with an eight-layer AlexNet architecture [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 8, "context": "employed CNN features as a feature vector by combining those features with a support vector machine (SVM) classifier [1], while other researchers have evaluated and visualized CNN features with an eight-layer AlexNet architecture [9].", "startOffset": 230, "endOffset": 233}, {"referenceID": 15, "context": "More recent architectures utilize deep structures, such as the very deep convolutional network (VGGNet) [16] and GoogLeNet [17], which were developed by Oxford University\u2019s Visual Geometry Group and Google Inc.", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "More recent architectures utilize deep structures, such as the very deep convolutional network (VGGNet) [16] and GoogLeNet [17], which were developed by Oxford University\u2019s Visual Geometry Group and Google Inc.", "startOffset": 123, "endOffset": 127}, {"referenceID": 4, "context": "[5], the most important CNN feature is deep architecture.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": ", [6, 14]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 13, "context": ", [6, 14]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 2, "context": "In the time since the neocognitron was first proposed by Fukushima [3], neuron-based recognition has become one of the most commonly used neural network architectures.", "startOffset": 67, "endOffset": 70}, {"referenceID": 9, "context": "Following that study, the LeNet-5 [10] neocognitron model added a baseline to CNNs in order to create a more significant model.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "Current network architectures include standard structures such as multiple fully connected layers, while recent challengers employ pre-trained [7], dropout [8], and rectified linear units (ReLU) [12] as improved learning models.", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "Current network architectures include standard structures such as multiple fully connected layers, while recent challengers employ pre-trained [7], dropout [8], and rectified linear units (ReLU) [12] as improved learning models.", "startOffset": 156, "endOffset": 159}, {"referenceID": 11, "context": "Current network architectures include standard structures such as multiple fully connected layers, while recent challengers employ pre-trained [7], dropout [8], and rectified linear units (ReLU) [12] as improved learning models.", "startOffset": 195, "endOffset": 199}, {"referenceID": 8, "context": "The most outstanding computer vision result was obtained by AlexNet in the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC2012), which remains the image recognition leader, with 1,000 classes [9].", "startOffset": 207, "endOffset": 210}, {"referenceID": 15, "context": "More recent variations, such as the 16- or 19-layer VGGNet [16], and the 22-layer GoogLeNet [17] models, have even deeper architectures.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "More recent variations, such as the 16- or 19-layer VGGNet [16], and the 22-layer GoogLeNet [17] models, have even deeper architectures.", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "These deeper models outperform conventional models on the ILSVRC dataset [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Those authors adopted selective search [18] as an object proposal approach and VGGNet for the CNN architecture.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "Figure 1 shows the architectures of AlexNet [9] and VGGNet [16].", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "Figure 1 shows the architectures of AlexNet [9] and VGGNet [16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "The parameters are based on DeCAF [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 10, "context": "In this section, we discuss our experiments conducted using the Daimler pedestrian benchmark [11] and Caltech 101 [2] Datasets.", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "In this section, we discuss our experiments conducted using the Daimler pedestrian benchmark [11] and Caltech 101 [2] Datasets.", "startOffset": 114, "endOffset": 117}], "year": 2015, "abstractText": "In this paper, we evaluate convolutional neural network (CNN) features using the AlexNet architecture developed by [9] and very deep convolutional network (VGGNet) architecture developed by [16]. To date, most CNN researchers have employed the last layers before output, which were extracted from the fully connected feature layers. However, since it is unlikely that feature representation effectiveness is dependent on the problem, this study evaluates additional convolutional layers that are adjacent to fully connected layers, in addition to executing simple tuning for feature concatenation (e.g., layer 3 + layer 5 + layer 7) and transformation, using tools such as principal component analysis. In our experiments, we carried out detection and classification tasks using the Caltech 101 and Daimler Pedestrian Benchmark Datasets.", "creator": "LaTeX with hyperref package"}}}