{"id": "1510.01032", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2015", "title": "Deep convolutional acoustic word embeddings using word-pair side information", "abstract": "recent studies have introduced revisiting noisy words as several basic functional unit in gesture recognition encoding query applications, instead preferring metric units. such whole - domain interference maps rely on a basis that maps a variable - length speech inventory leaving a vector describing a infinite - dimensional space ; the resulting acoustic computation blocks need to compensate for accurate conversions between selected word types, directly onto an null space. we compare several old and new approaches from a translation orientation task. our best analogy uses side information resembling the form of known language pairs to train approximately siamese convolutional constraint input ( stem ) : a lattice of tied stimuli that form two auditory segments provide input and produce complex embeddings, trained with a binary search whereby separates differently - word pairs and different - root strings giving reasonable margin. a word classifier filter performs similarly, but requires physically stronger supervision. both cases of treatments yield considerable costs over making best yet relevant reports on the word discrimination task.", "histories": [["v1", "Mon, 5 Oct 2015 05:25:32 GMT  (353kb,D)", "https://arxiv.org/abs/1510.01032v1", "5 pages, 3 figures"], ["v2", "Fri, 8 Jan 2016 14:54:44 GMT  (353kb,D)", "http://arxiv.org/abs/1510.01032v2", "5 pages, 3 figures; added reference, acknowledgement and link to code"]], "COMMENTS": "5 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["herman kamper", "weiran wang", "karen livescu"], "accepted": false, "id": "1510.01032"}, "pdf": {"name": "1510.01032.pdf", "metadata": {"source": "CRF", "title": "DEEP CONVOLUTIONAL ACOUSTIC WORD EMBEDDINGS USING WORD-PAIR SIDE INFORMATION", "authors": ["Herman Kamper", "Weiran Wang", "Karen Livescu"], "emails": ["h.kamper@sms.ed.ac.uk,", "weiranwang@ttic.edu,", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "Index Terms\u2014 Acoustic word embeddings, segmental acoustic models, fixed-dimensional representations, query-by-example search."}, {"heading": "1. INTRODUCTION", "text": "Most current speech processing systems rely on a deep architecture to classify speech frames into subword units (often phone states). This approach still relies on frame-level independence assumptions as well as a pronunciation lexicon for breaking up words into their subword constituents. As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.\nSome of the earliest speech recognition systems were based on template-based whole-word modelling [8]. This idea has been revisited in modern template-based automatic speech recognition (ASR) systems [1, 2], as well as modern speech indexing applications such as query-by-example search [9, 10]. These systems typically use dynamic time warping (DTW) to quantify the similarity of phone or word segments of variable length. Recent work has also considered frame-level embeddings which map acoustic features to a new frame-level representation that is tailored to word discrimination when combined with DTW [11\u201313]. DTW, however, has known inadequacies [14] and is quadratic-time in the duration of the segments.\nLevin et al. [3] proposed a segmental approach where an arbitrarylength speech segment is embedded in a fixed-dimensional space such that segments of the same word type have similar embeddings. Segments can then be compared by simply calculating a distance in the embedding space, a linear time operation in the embedding dimensionality. Several approaches were developed in [3], and in [15] these were successfully applied in a query-by-example search system.\nThis research was supported by NSF grant IIS-1321015. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency. HK is funded by a Commonwealth Scholarship.\nBengio and Heigold [4] similarly used whole-word fixeddimensional representations in a segmental ASR lattice rescoring system. Their acoustic embeddings are obtained from a convolutional neural network (CNN), trained with a combination of a word classification and a ranking loss. When combining the hypotheses of the baseline system with the embedding-based scores, ASR performance was improved. A similar approach was followed in [5], where long short-term memory (LSTM) networks were used to obtain wholeword embeddings for a query-by-example search task. Finally, Maas et al. [6] trained a regression CNN that reconstructs a semantic word embedding from acoustic speech input; these features were used in a segmental conditional random field ASR system.\nIn this paper we compare several CNN-based approaches to each other and to the best approach of Levin et al. [3], on a word discrimination task. This task has been used in several other studies [11,12,16] to assess the accuracy of acoustic embedding approaches without the need to train a complete recognition or search system. Building on ideas from earlier CNN-based approaches, we propose new networks that make use of weaker supervision in the form of known word pairs. The approach is based on Siamese networks: tied networks that take in pairs of input vectors and minimize or maximize a distance depending on whether a pair comes from the same or different classes [17]. We show that a Siamese CNN trained with with a hinge-like contrastive loss function outperforms the best approach of Levin et al. [3], and performs similarly to a word classifier CNN, despite the weaker form of supervision. By reducing the Siamese CNN embedding dimensionality with a post-processing linear discriminant analysis, we also obtain a more compact embedding that maintains best performance."}, {"heading": "2. ACOUSTIC WORD EMBEDDING APPROACHES", "text": "For speech applications using fixed-dimensional representations of whole words, it is desirable to find a mapping such that word segments of the same type are close in the embedding space while those of different types are far from each other. Formally, we use the notation Y = y1:T to denote a vector time series, where each yt \u2208 Rb is a b-dimensional frame-level feature vector (e.g. MFCCs). An acoustic word embedding approach is a function f(Y ) that maps arbitrarylength time series Y into a fixed-dimensional space Rd; if Y1 and Y2 are two word segments, the distance between the vectors f(Y1) and f(Y2) should indicate whether they are of the same word type or not.\nTypical embedding approaches use a training set of known word segments Ytrain = {Yi}Ntraini=1 to learn f . Different degrees of supervision can be assumed, ranging from unsupervised, where the only knowledge of Ytrain is that it contains unidentified word segments, to supervised, where the word label for each segment is known. Below we review previous work (Sections 2.1 and 2.2) and then present our own approaches which use weak supervision in the form of known word pairs (Section 2.3), and can also additionally use word labels to find lower-dimensional but still accurate embeddings (Section 2.4).\nar X\niv :1\n51 0.\n01 03\n2v 2\n[ cs\n.C L\n] 8\nJ an\n2 01\n6"}, {"heading": "2.1. Reference vector methods", "text": "Several embedding approaches were proposed and compared in Levin et al. [3] based on the idea of using a reference vector to construct the mapping f . For a target speech segment, a reference vector consists of the DTW alignment cost to every exemplar in a reference set Yref \u2286 Ytrain. Applying dimensionality reduction to the reference vector yields the desired embedding in Rd. The intuition is that the content of a speech segment should be characterized well through its similarity to other segments. Such embeddings have subsequently been used for keyword search [15] and unsupervised term discovery [18]. One drawback is the need to compute a large number of DTW alignments. Several dimensionality reduction approaches were considered in [3], and in Section 3.3 we compare to their best overall approach which uses a combination of Laplacian eigenmaps (a nonlinear graph embedding approach) and linear discriminant analysis."}, {"heading": "2.2. Word classification CNN", "text": "For the whole-word speech recognition system in [4], Bengio and Heigold proposed that, when word labels Wtrain = {wi}Ntraini=1 are available for the training segments Ytrain, a supervised neural network can simply be trained to predict the word class (type) given the speech as input. The softmax prediction layer of such a neural network then gives a fixed-dimensional representation in Rd, where d here is the number of distinct word types (the vocabulary size). During testing, some inputs may correspond to unseen words, but even in these cases the softmax layer gives a fixed-dimensional distributional representation of the input in terms of seen word types.\nA standard feed-forward neural network classifier, however, requires fixed-dimensional input. A simple solution was used in [4]: All word segments are padded to the same length, given by the maximum duration of a word segment in Ytrain. Instead of using fully-connected layers, convolutional and pooling layers are used to alleviate the effect of the padding. A convolutional neural network (CNN) such as this is shown in Figure 1(a). Our implementation uses mean-normalized MFCCs which are zero-padded to npad frames. One-dimensional convolution is performed only over time, covering a number of frames and all features, and is followed by max-pooling. These layers can be repeated. A number of fully-connected layers is used next, which feeds into the final softmax layer. Formally, the whole CNN defines a mapping function f : Yi \u2208 Rb\u00d7npad \u2192 xi \u2208 Rd, that takes input Yi, obtained by padding the variable-length b-dimensional vector time series Yi, and produces the acoustic embedding xi.\nInstead of using the representation from the word classification CNN directly, Bengio and Heigold [4] used a paired network with a ranking loss to map acoustic word embeddings into a common space\nwith orthography-based word embeddings obtained by also mapping the word labelsWtrain into a lower-dimensional space. This was done to make it possible to use the classifier outputs in a particular lattice rescoring architecture, which requires scores for lattice arcs. The evaluation framework we use (Section 3.1) is designed to be decoupled from a recognition architecture, and we can therefore use the distributional representation from the classifier CNN directly. An investigation of whether embeddings of word labels can be additionally used to improve acoustic word embeddings is left for future work."}, {"heading": "2.3. Word similarity Siamese CNNs", "text": "If the labelsWtrain for the training set Ytrain are not known, a weaker form of supervision that has also been used [11\u201313, 19] is the knowledge that pairs of word segments in Ytrain share the same unknown word type: Strain = {(m,n) : (Ym, Yn) are of the same type}. This type of side information is appealing since it is often easier to obtain in low-resource settings, for example by using an unsupervised term discovery system [20, 21] to find unidentified matching word pairs.\nSuch paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26]. Many of these studies use Siamese networks, a term used since the early 1990s to describe a pair of networks with tied parameters which is trained to optimize a distance function between representations of two data instances [17]. To train these networks it is sometimes assumed that pairs not in Strain belong to different types; we also make this assumption here.\nFigure 1(b) illustrates how we apply this idea to obtain acoustic word embeddings. The two sides of our Siamese network take padded inputs Y1 and Y2. For the two sides we use CNNs similar to that of the word classification CNN. But instead of terminating in a softmax layer, the final fully connected layer on each side gives the desired acoustic embedding. In initial experiments on development data, we considered several loss functions, and here we focus only on the most successful ones. We found that losses based on cosine similarity outperformed Euclidean-based losses, and in particular the coscos2 loss from [27] gave the best performance of the losses in [17, 27]:\nlcos cos2(x1,x2) =\n{ 1\u2212cos(x1,x2)\n2 if same\ncos2(x1,x2) if different (1)\nThis loss pushes the angle between embeddings of the same type to be zero, while embeddings of different types are pushed to be orthogonal.\nIn discrimination tasks, the decision of whether two data instances are of the same type is not based on their absolute distance, but rather their relative distance compared to other pairs. This motivates a margin-based (hinge) loss, similar to that of [24, 25]:\nlcos hinge = max {0,m+ dcos(x1,x2)\u2212 dcos(x1,x3)} (2)\nwhere dcos(x1,x2) = 1\u2212cos(x1,x2)2 is the cosine distance between x1 and x2, and m is a margin parameter. Here, x1 and x2 are always of the same type while x1 and x3 are of different types. This loss is therefore at a minimum when all embeddings x1 and x2 of the same type are more similar by a margin m than embeddings x1 and x3 of different types. The margin also gives some leeway to the model.\nAlthough Siamese networks have been used widely, to our knowledge this is the first work which uses Siamese networks (in particular Siamese CNNs) to obtain acoustic word embeddings from speech."}, {"heading": "2.4. Controlling embedding dimensionality", "text": "We aim to learn word embeddings that are both discriminative and compact (low-dimensional). The desired dimensionality may be guided by both computational and data constraints, and we may wish to be able to adjust it. For word classification networks (Section 2.2), the output dimensionality is given by the vocabulary size. In our experiments (next section), we explore adjusting the dimensionality by inserting an additional linear bottleneck layer before the final softmax, with the number of units corresponding to the desired final dimensionality. In Siamese networks (Section 2.3) the final dimensionality can be directly tuned. If we have access to word labelsWtrain in addition to word pairs Strain, we can also perform additional dimensionlity reduction on the Siamese CNN outputs using a supervised technique; in our experiments we use linear discriminant analysis (LDA)."}, {"heading": "3. EXPERIMENTS", "text": ""}, {"heading": "3.1. Evaluation and experimental setup", "text": "Ultimately we would like to evaluate the different acoustic embedding approaches for downstream speech recognition and search tasks. However, we do not want to be tied to a specific recognition architecture, and we would like to quickly compare many embedding approaches. We therefore use a word discrimination task developed for this purpose [16]; in the same-different task, we are given a pair of acoustic segments, each corresponding to a word, and we must decide whether the segments are examples of the same or different words.\nThis task can be approached in a number of ways, but typically it is done either via a DTW score between segments (when using frameby-frame embeddings), or via a Euclidean or cosine distance between vectors (when embedding complete segments). In our evaluation, after training a model on Ytrain, the acoustic word embeddings of a disjoint test set Ytest are computed. For every word pair in this set, the cosine distance1 is calculated between their embeddings. Two words can then be classified as being of the same or different type based on some threshold, and a precision-recall curve is obtained by varying the threshold. To evaluate embeddings across different operating points, the area under the precision-recall curve is calculated to yield the final evaluation metric, referred to as the average precision (AP).\nWe use data from the Switchboard corpus of English conversational telephone speech. Data is parameterized as Mel-frequency cepstral coefficients (MFCCs) with first and second order derivatives, yielding 39-dimensional feature vectors. Cepstral mean and variance normalization (CMVN) is applied per conversation side. For the training set Ytrain we use the set of about 10k word tokens from [11,12]; it consists of word segments of at least 5 characters and 0.5 seconds in duration extracted from a forced alignment of the transcriptions, and comprises about 105 minutes of speech. For the Siamsese CNNs, this set results in about 100k word segment pairs for Strain. For testing, we use the 11k-token set Ytest from [3, 11, 12], making the results from\n1We also tried Euclidean distance, but as in [3,12,13], cosine worked better.\nthese studies directly comparable to the results obtained here.2 This set was extracted from a portion of Switchboard distinct from Ytrain. Similarly, we extracted an 11k-token development set.\nAs mentioned in Section 1, recent studies [11, 12] have also been using frame-level embedding approaches in combination with DTW to perform the same-different task. These approaches map the original features to a new frame-level representation that is tailored to word discrimination. We compare our results to that of [11], which uses posteriograms over a partitioned universal background model (UBM), as well as [12], which uses a correspondence autoencoder."}, {"heading": "3.2. Network architectures", "text": "We used the Theano [28] toolkit to implement the CNN-based models of Sections 2.2 and 2.3.3 Models are trained using ADADELTA [29], an adaptive learning rate stochastic optimization method that adapts over time based on an accumulation of past gradients; we set the momentum hyper-parameter to \u03c1 = 0.9 and the precision parameter to = 10\u22126. Input speech segments are padded to npad = 200 frames (2 s), which corresponds to the longest word segment in Ytrain. The architectures of the CNNs were optimized separately on the development data for each network type, resulting in the following structures: \u2022 Word classifier CNN: 1-D convolution with 96 filters over 9\nframes; ReLU; max pooling over 3 units; 1-D convolution with 96 filters over 8 units; ReLU; max pooling over 3 units; 1024-unit fully-connected ReLU; softmax layer over 1061 word types. \u2022 Word similarity Siamese CNN: two convolutional and max pooling layers as above; 2048-unit fully-connected ReLU; 1024-unit fully-connected linear linear; terminates in loss l(x1,x2). For the word classifier CNN, we only train on words in Ytrain that occur at least three times; this gives a subset of 87% of all tokens with 1061 unique word types. This minimum count was tuned on the development set. To see the effect of the convolutions, we also train a word classifier deep neural network (DNN) using two 2048-unit fully-connected ReLU layers and a 1061-unit softmax layer. For the Siamese CNN using lcos hinge, we use a margin m = 0.15 (tuned on the development set). If we had used ReLUs in the final layer in the Siamese CNNs, the angles between embeddings would be restricted to [0, \u03c0/2]; we therefore use a final linear layer. All weights are initialized randomly; we run all models with five different initializations and report average performance and standard deviations."}, {"heading": "3.3. Results", "text": "Table 1 shows AP performance on the test set from previous studies (models 1 to 4) as well as our newly implemented models (5 to 11).\nThe first three models perform word discrimination using DTW on frame-level embeddings of word segments; model 1 works directly on acoustic features, while models 2 and 3 work on features optimized for word discrimination. Model 3 yields the best previously reported result on this task. Model 4 is the best acoustic word embedding approach from [3] (Section 2.1), representing the best previous result for an approach that produces embeddings of whole word segments.\nModels 5 to 11 are the neural network-based approaches. The effect of using the convolutional layers is evident from the large improvement in AP of model 6 over model 5. Both of these models are trained on the word type labelsWtrain, which is also the type of supervision used for model 4, making the improvement of model\n2In [3], a slightly different training set was used. Nevertheless, the size of their training set is comparable to the set used here.\n3CNN code: https://github.com/kamperh/couscous. Complete recipe: https://github.com/kamperh/recipe_swbd_wordembeds.\n6 over model 4 noteworthy.4 The dimensionality of the acoustic embeddings of model 6, however, is much larger than that of model 4. We therefore also trained a version of model 6 where the embedding is obtained from a linear bottleneck layer inserted just before the final softmax layer. The lower-dimensional embeddings from this approach (model 7) still improves on model 4 by a sizable margin.\nOf the Siamese CNNs, the model with the lcos hinge loss (model 9, Section 2.3) outperforms its lcos cos2 counterpart, and yields a large improvement over model 4, which was the previous best acoustic embedding approach. It also gives similar performance to the word classification CNN (model 6), even though the pair-wise side information Strain used for model 9 is a weaker form of supervision than the fully labelled supervisionWtrain used in model 6. When reducing the embedding dimensionality to 50 (model 10), AP is still higher than any of the d = 50 competitors. Model 9\u2019s improvement over model 3 is also interesting since the former does not use any DTW alignment information. Finally, model 11 shows that LDA on the output of model 9 does not yield any improvement, but does produce a much smaller embedding without loss in performance. This model uses exactly the same word class supervisionWtrain as models 6 and 7."}, {"heading": "3.4. Further discussion and analysis", "text": "Although the structures of models 8 and 9 are identical, the model using lcos hinge significantly outperforms its counterpart using lcos cos2 . This is in line with the fact that a loss like lcos hinge, which optimizes embeddings based on relative distances between positive and negative pairs, is much more closely aligned with the discrimination task than a loss like lcos cos2 , which looks at distances of word pairs in isolation (without regard to their distances relative to other pairs). The lcos hinge loss also allows more freedom in the model since it does not penalize same-word pairs (x1,x2) if they are already more similar by the margin m than the corresponding different-word pairs (x1,x3).\nThe closer match between the same-different task and the training loss lcos hinge could also explain the improvements over the DTW-based model 3 (both using exactly the same supervision Strain); this latter model aims to learn better features at the local frame level, but does\n4For model 6, embeddings are taken from the final softmax output. We also experimented with embeddings from the softmax layer but before applying the exponential normalization; this gave worse development results.\nso without regard to the (relative) similarities of complete segments. Figure 2 shows AP on the development set when varying the target dimensionalities of the different CNN-based approaches (see Section 2.4). The lcos hinge Siamese CNN outperforms all the other models (apart from the post-processed LDA model) at all operating points, and gives stable performance over a range of dimensionalities (300 and onwards). The word classifier CNN does much worse in this case (compared to the result of model 6), perhaps since embeddings here are not taken from the final layer, which is explicitly optimized for word classification, but from an intermediate layer. In contrast, for the Siamese CNNs, embeddings are always obtained directly from the layer that is optimized in the target loss. The figure also shows that when word labelsWtrain are available, compact embeddings can be obtained by performing LDA on top of the Siamese CNN representation, without loss in performance; this can prove to be important for downstream tasks which might require smaller embeddings.\nHere, a relatively small set of labelled word examples Ytrain is used to train the word classifier networks (as also done in the studies we compare to). In contrast, by using pairs of words and relative comparisons between them, a much larger set Strain is used for training Siamese networks. This type of paired supervision is ideal for generalizing to unseen word types, and is often easier to obtain in low-resource settings (see Section 2.3). While frame-level feature learning (model 3) can also use the larger pair-wise training set Strain, such approaches need to be coupled with DTW, which is limiting."}, {"heading": "4. CONCLUSION", "text": "We studied several acoustic word embedding approaches based on convolutional neural networks (CNNs); these networks take a wholeword speech segment as input and produce a fixed-dimensional vector. Our best new approach is a Siamese CNN that uses a hinge-based loss function to minimize the distance between word pairs of the same type relative to the distance between pairs of different types. On the same-different word discrimination task, this approach yields an average precision (AP) of 0.549, an improvement over the best previously published results on this task with whole-word embeddings (0.365 AP) and DTW with learned frame features (0.469 AP). A word classifier CNN performs similarly (0.532 AP) to the Siamese CNN, but requires much stronger labelled supervision, and performs worse at smaller dimensionalties. Future work will consider sequence models (e.g. RNNs, LSTMs), and will apply these embeddings to downstream tasks such as term discovery, speech recognition, and search."}, {"heading": "5. REFERENCES", "text": "[1] M. De Wachter, M. Matton, K. Demuynck, P. Wambacq, R. Cools, and D. Van Compernolle, \u201cTemplate-based continuous speech recognition,\u201d IEEE Trans. Audio, Speech, Language Process., vol. 15, no. 4, pp. 1377\u20131390, 2007.\n[2] G. Heigold, P. Nguyen, M. Weintraub, and V. Vanhoucke, \u201cInvestigations on exemplar-based features for speech recognition towards thousands of hours of unsupervised, noisy data,\u201d in Proc. ICASSP, 2012.\n[3] K. Levin, K. Henry, A. Jansen, and K. Livescu, \u201cFixeddimensional acoustic embeddings of variable-length segments in low-resource settings,\u201d in Proc. ASRU, 2013.\n[4] S. Bengio and G. Heigold, \u201cWord embeddings for speech recognition,\u201d in Proc. Interspeech, 2014.\n[5] G. Chen, C. Parada, and T. N. Sainath, \u201cQuery-by-example keyword spotting using long short-term memory networks,\u201d in Proc. ICASSP, 2015.\n[6] A. L. Maas, S. D. Miller, T. M. O\u2019neil, A. Y. Ng, and P. Nguyen, \u201cWord-level acoustic modeling with convolutional vector regression,\u201d in Proc. ICML Workshop Representation Learn., 2012.\n[7] O. J. Ra\u0308sa\u0308nen, \u201cGenerating hyperdimensional distributed representations from continuous-valued multivariate sensory input,\u201d in Proc. CogSci, 2015.\n[8] C. S. Myers and L. R. Rabiner, \u201cConnected digit recognition using a level-building DTW algorithm,\u201d IEEE Trans. Acoust., Speech, Signal Process., vol. 29, no. 3, pp. 351\u2013363, 1981.\n[9] Y. Zhang and J. R. Glass, \u201cUnsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams,\u201d in Proc. ASRU, 2009.\n[10] Y. Zhang, R. Salakhutdinov, H.-A. Chang, and J. R. Glass, \u201cResource configurable spoken query detection using deep Boltzmann machines,\u201d in Proc. ICASSP, 2012.\n[11] A. Jansen, S. Thomas, and H. Hermansky, \u201cWeak top-down constraints for unsupervised acoustic model training,\u201d in Proc. ICASSP, 2013.\n[12] H. Kamper, M. Elsner, A. Jansen, and S. J. Goldwater, \u201cUnsupervised neural network based feature extraction using weak top-down constraints,\u201d in Proc. ICASSP, 2015.\n[13] R. Thiollie\u0300re, E. Dunbar, G. Synnaeve, M. Versteegh, and E. Dupoux, \u201cA hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling,\u201d in Proc. Interspeech, 2015.\n[14] L. R. Rabiner, A. E. Rosenberg, and S. E. Levinson, \u201cConsiderations in dynamic time warping algorithms for discrete word recognition,\u201d IEEE Trans. Acoust., Speech, Signal Process., vol. 26, no. 6, pp. 575\u2013582, 1978.\n[15] K. Levin, A. Jansen, and B. Van Durme, \u201cSegmental acoustic indexing for zero resource keyword search,\u201d in Proc. ICASSP, 2015.\n[16] M. A. Carlin, S. Thomas, A. Jansen, and H. Hermansky, \u201cRapid evaluation of speech representations for spoken term discovery,\u201d in Proc. Interspeech, 2011.\n[17] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore, E. Sa\u0308ckinger, and R. Shah, \u201cSignature verification using a \u2018Siamese\u2019 time delay neural network,\u201d Int. J. Pattern Rec., vol. 7, no. 4, pp. 669\u2013688, 1993.\n[18] H. Kamper, A. Jansen, and S. Goldwater, \u201cFully unsupervised small-vocabulary speech recognition using a segmental Bayesian model,\u201d in Proc. Interspeech, 2015.\n[19] D. Renshaw, H. Kamper, A. Jansen, and S. J. Goldwater, \u201cA comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge,\u201d in Proc. Interspeech, 2015.\n[20] A. S. Park and J. R. Glass, \u201cUnsupervised pattern discovery in speech,\u201d IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.\n[21] A. Jansen and B. Van Durme, \u201cEfficient spoken term discovery using randomized algorithms,\u201d in Proc. ASRU, 2011.\n[22] L. Badino, C. Canevari, L. Fadiga, and G. Metta, \u201cAn autoencoder based approach to unsupervised learning of subword units,\u201d in Proc. ICASSP, 2014.\n[23] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, \u201cLearning deep structured semantic models for web search using clickthrough data,\u201d in Proc. CIMK, 2013.\n[24] T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEfficient estimation of word representations in vector space,\u201d arXiv preprint arXiv:1301.3781, 2013.\n[25] J. Wieting, M. Bansal, K. Gimpel, and K. Livescu, \u201cFrom paraphrase database to compositional paraphrase model and back,\u201d Trans. ACL, vol. 3, pp. 345\u2013358, 2015.\n[26] R. Hadsell, S. Chopra, and Y. LeCun, \u201cDimensionality reduction by learning an invariant mapping,\u201d in Proc. CVPR, 2006.\n[27] G. Synnaeve, T. Schatz, and E. Dupoux, \u201cPhonetics embedding learning with side information,\u201d in Proc. SLT, 2014.\n[28] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio, \u201cTheano: a CPU and GPU math expression compiler,\u201d in Proc. SciPy, 2010.\n[29] M. D. Zeiler, \u201cADADELTA: An adaptive learning rate method,\u201d arXiv preprint arXiv:1212.5701, 2012."}], "references": [{"title": "Template-based continuous speech recognition", "author": ["M. De Wachter", "M. Matton", "K. Demuynck", "P. Wambacq", "R. Cools", "D. Van Compernolle"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 15, no. 4, pp. 1377\u20131390, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Investigations on exemplar-based features for speech recognition towards thousands of hours of unsupervised, noisy data", "author": ["G. Heigold", "P. Nguyen", "M. Weintraub", "V. Vanhoucke"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Fixeddimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["K. Levin", "K. Henry", "A. Jansen", "K. Livescu"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Word embeddings for speech recognition", "author": ["S. Bengio", "G. Heigold"], "venue": "Proc. Interspeech, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Query-by-example keyword spotting using long short-term memory networks", "author": ["G. Chen", "C. Parada", "T.N. Sainath"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Word-level acoustic modeling with convolutional vector regression", "author": ["A.L. Maas", "S.D. Miller", "T.M. O\u2019neil", "A.Y. Ng", "P. Nguyen"], "venue": "Proc. ICML Workshop Representation Learn., 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating hyperdimensional distributed representations from continuous-valued multivariate sensory input", "author": ["O.J. R\u00e4s\u00e4nen"], "venue": "Proc. CogSci, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Connected digit recognition using a level-building DTW algorithm", "author": ["C.S. Myers", "L.R. Rabiner"], "venue": "IEEE Trans. Acoust., Speech, Signal Process., vol. 29, no. 3, pp. 351\u2013363, 1981.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1981}, {"title": "Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Resource configurable spoken query detection using deep Boltzmann machines", "author": ["Y. Zhang", "R. Salakhutdinov", "H.-A. Chang", "J.R. Glass"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Weak top-down constraints for unsupervised acoustic model training", "author": ["A. Jansen", "S. Thomas", "H. Hermansky"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised neural network based feature extraction using weak top-down constraints", "author": ["H. Kamper", "M. Elsner", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling", "author": ["R. Thiolli\u00e8re", "E. Dunbar", "G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Considerations in dynamic time warping algorithms for discrete word recognition", "author": ["L.R. Rabiner", "A.E. Rosenberg", "S.E. Levinson"], "venue": "IEEE Trans. Acoust., Speech, Signal Process., vol. 26, no. 6, pp. 575\u2013582, 1978.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1978}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Rapid evaluation of speech representations for spoken term discovery", "author": ["M.A. Carlin", "S. Thomas", "A. Jansen", "H. Hermansky"], "venue": "Proc. Interspeech, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Signature verification using a \u2018Siamese\u2019 time delay neural network", "author": ["J. Bromley", "J.W. Bentz", "L. Bottou", "I. Guyon", "Y. LeCun", "C. Moore", "E. S\u00e4ckinger", "R. Shah"], "venue": "Int. J. Pattern Rec., vol. 7, no. 4, pp. 669\u2013688, 1993.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "Fully unsupervised small-vocabulary speech recognition using a segmental Bayesian model", "author": ["H. Kamper", "A. Jansen", "S. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge", "author": ["D. Renshaw", "H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "An autoencoder based approach to unsupervised learning of subword units", "author": ["L. Badino", "C. Canevari", "L. Fadiga", "G. Metta"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "Proc. CIMK, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["J. Wieting", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "Trans. ACL, vol. 3, pp. 345\u2013358, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "Proc. CVPR, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Phonetics embedding learning with side information", "author": ["G. Synnaeve", "T. Schatz", "E. Dupoux"], "venue": "Proc. SLT, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proc. SciPy, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 1, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 2, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 3, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 4, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 5, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 6, "context": "As an alternative, some researchers [1\u20137] have started to reconsider using whole words as the basic modelling unit.", "startOffset": 36, "endOffset": 41}, {"referenceID": 7, "context": "Some of the earliest speech recognition systems were based on template-based whole-word modelling [8].", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "This idea has been revisited in modern template-based automatic speech recognition (ASR) systems [1, 2], as well as modern speech indexing applications such as query-by-example search [9, 10].", "startOffset": 97, "endOffset": 103}, {"referenceID": 1, "context": "This idea has been revisited in modern template-based automatic speech recognition (ASR) systems [1, 2], as well as modern speech indexing applications such as query-by-example search [9, 10].", "startOffset": 97, "endOffset": 103}, {"referenceID": 8, "context": "This idea has been revisited in modern template-based automatic speech recognition (ASR) systems [1, 2], as well as modern speech indexing applications such as query-by-example search [9, 10].", "startOffset": 184, "endOffset": 191}, {"referenceID": 9, "context": "This idea has been revisited in modern template-based automatic speech recognition (ASR) systems [1, 2], as well as modern speech indexing applications such as query-by-example search [9, 10].", "startOffset": 184, "endOffset": 191}, {"referenceID": 10, "context": "Recent work has also considered frame-level embeddings which map acoustic features to a new frame-level representation that is tailored to word discrimination when combined with DTW [11\u201313].", "startOffset": 182, "endOffset": 189}, {"referenceID": 11, "context": "Recent work has also considered frame-level embeddings which map acoustic features to a new frame-level representation that is tailored to word discrimination when combined with DTW [11\u201313].", "startOffset": 182, "endOffset": 189}, {"referenceID": 12, "context": "Recent work has also considered frame-level embeddings which map acoustic features to a new frame-level representation that is tailored to word discrimination when combined with DTW [11\u201313].", "startOffset": 182, "endOffset": 189}, {"referenceID": 13, "context": "DTW, however, has known inadequacies [14] and is quadratic-time in the duration of the segments.", "startOffset": 37, "endOffset": 41}, {"referenceID": 2, "context": "[3] proposed a segmental approach where an arbitrarylength speech segment is embedded in a fixed-dimensional space such that segments of the same word type have similar embeddings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Several approaches were developed in [3], and in [15] these were successfully applied in a query-by-example search system.", "startOffset": 37, "endOffset": 40}, {"referenceID": 14, "context": "Several approaches were developed in [3], and in [15] these were successfully applied in a query-by-example search system.", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": "Bengio and Heigold [4] similarly used whole-word fixeddimensional representations in a segmental ASR lattice rescoring system.", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "A similar approach was followed in [5], where long short-term memory (LSTM) networks were used to obtain wholeword embeddings for a query-by-example search task.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "[6] trained a regression CNN that reconstructs a semantic word embedding from acoustic speech input; these features were used in a segmental conditional random field ASR system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3], on a word discrimination task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "This task has been used in several other studies [11,12,16] to assess the accuracy of acoustic embedding approaches without the need to train a complete recognition or search system.", "startOffset": 49, "endOffset": 59}, {"referenceID": 11, "context": "This task has been used in several other studies [11,12,16] to assess the accuracy of acoustic embedding approaches without the need to train a complete recognition or search system.", "startOffset": 49, "endOffset": 59}, {"referenceID": 15, "context": "This task has been used in several other studies [11,12,16] to assess the accuracy of acoustic embedding approaches without the need to train a complete recognition or search system.", "startOffset": 49, "endOffset": 59}, {"referenceID": 16, "context": "The approach is based on Siamese networks: tied networks that take in pairs of input vectors and minimize or maximize a distance depending on whether a pair comes from the same or different classes [17].", "startOffset": 198, "endOffset": 202}, {"referenceID": 2, "context": "[3], and performs similarly to a word classifier CNN, despite the weaker form of supervision.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] based on the idea of using a reference vector to construct the mapping f .", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Such embeddings have subsequently been used for keyword search [15] and unsupervised term discovery [18].", "startOffset": 63, "endOffset": 67}, {"referenceID": 17, "context": "Such embeddings have subsequently been used for keyword search [15] and unsupervised term discovery [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "Several dimensionality reduction approaches were considered in [3], and in Section 3.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "For the whole-word speech recognition system in [4], Bengio and", "startOffset": 48, "endOffset": 51}, {"referenceID": 3, "context": "A simple solution was used in [4]: All word segments are padded to the same length, given by the maximum duration of a word segment in Ytrain.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "Instead of using the representation from the word classification CNN directly, Bengio and Heigold [4] used a paired network with a ranking loss to map acoustic word embeddings into a common space with orthography-based word embeddings obtained by also mapping the word labelsWtrain into a lower-dimensional space.", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "If the labelsWtrain for the training set Ytrain are not known, a weaker form of supervision that has also been used [11\u201313, 19] is the knowledge that pairs of word segments in Ytrain share the same unknown word type: Strain = {(m,n) : (Ym, Yn) are of the same type}.", "startOffset": 116, "endOffset": 127}, {"referenceID": 11, "context": "If the labelsWtrain for the training set Ytrain are not known, a weaker form of supervision that has also been used [11\u201313, 19] is the knowledge that pairs of word segments in Ytrain share the same unknown word type: Strain = {(m,n) : (Ym, Yn) are of the same type}.", "startOffset": 116, "endOffset": 127}, {"referenceID": 12, "context": "If the labelsWtrain for the training set Ytrain are not known, a weaker form of supervision that has also been used [11\u201313, 19] is the knowledge that pairs of word segments in Ytrain share the same unknown word type: Strain = {(m,n) : (Ym, Yn) are of the same type}.", "startOffset": 116, "endOffset": 127}, {"referenceID": 18, "context": "If the labelsWtrain for the training set Ytrain are not known, a weaker form of supervision that has also been used [11\u201313, 19] is the knowledge that pairs of word segments in Ytrain share the same unknown word type: Strain = {(m,n) : (Ym, Yn) are of the same type}.", "startOffset": 116, "endOffset": 127}, {"referenceID": 19, "context": "This type of side information is appealing since it is often easier to obtain in low-resource settings, for example by using an unsupervised term discovery system [20, 21] to find unidentified matching word pairs.", "startOffset": 163, "endOffset": 171}, {"referenceID": 20, "context": "This type of side information is appealing since it is often easier to obtain in low-resource settings, for example by using an unsupervised term discovery system [20, 21] to find unidentified matching word pairs.", "startOffset": 163, "endOffset": 171}, {"referenceID": 12, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 101, "endOffset": 109}, {"referenceID": 21, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 101, "endOffset": 109}, {"referenceID": 22, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 136, "endOffset": 143}, {"referenceID": 23, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 136, "endOffset": 143}, {"referenceID": 24, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 136, "endOffset": 143}, {"referenceID": 25, "context": "Such paired supervision has been used for several problems and domains, including phonetic discovery [13, 22], semantic word embeddings [23\u201325] and vision applications [26].", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "Many of these studies use Siamese networks, a term used since the early 1990s to describe a pair of networks with tied parameters which is trained to optimize a distance function between representations of two data instances [17].", "startOffset": 225, "endOffset": 229}, {"referenceID": 26, "context": "We found that losses based on cosine similarity outperformed Euclidean-based losses, and in particular the coscos loss from [27] gave the best performance of the losses in [17, 27]:", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "We found that losses based on cosine similarity outperformed Euclidean-based losses, and in particular the coscos loss from [27] gave the best performance of the losses in [17, 27]:", "startOffset": 172, "endOffset": 180}, {"referenceID": 26, "context": "We found that losses based on cosine similarity outperformed Euclidean-based losses, and in particular the coscos loss from [27] gave the best performance of the losses in [17, 27]:", "startOffset": 172, "endOffset": 180}, {"referenceID": 23, "context": "This motivates a margin-based (hinge) loss, similar to that of [24, 25]:", "startOffset": 63, "endOffset": 71}, {"referenceID": 24, "context": "This motivates a margin-based (hinge) loss, similar to that of [24, 25]:", "startOffset": 63, "endOffset": 71}, {"referenceID": 15, "context": "We therefore use a word discrimination task developed for this purpose [16]; in the same-different task, we are given a pair of acoustic segments, each corresponding to a word, and we must decide whether the segments are examples of the same or different words.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "For the training set Ytrain we use the set of about 10k word tokens from [11,12]; it consists of word segments of at least 5 characters and 0.", "startOffset": 73, "endOffset": 80}, {"referenceID": 11, "context": "For the training set Ytrain we use the set of about 10k word tokens from [11,12]; it consists of word segments of at least 5 characters and 0.", "startOffset": 73, "endOffset": 80}, {"referenceID": 2, "context": "For testing, we use the 11k-token set Ytest from [3, 11, 12], making the results from", "startOffset": 49, "endOffset": 60}, {"referenceID": 10, "context": "For testing, we use the 11k-token set Ytest from [3, 11, 12], making the results from", "startOffset": 49, "endOffset": 60}, {"referenceID": 11, "context": "For testing, we use the 11k-token set Ytest from [3, 11, 12], making the results from", "startOffset": 49, "endOffset": 60}, {"referenceID": 2, "context": "1We also tried Euclidean distance, but as in [3,12,13], cosine worked better.", "startOffset": 45, "endOffset": 54}, {"referenceID": 11, "context": "1We also tried Euclidean distance, but as in [3,12,13], cosine worked better.", "startOffset": 45, "endOffset": 54}, {"referenceID": 12, "context": "1We also tried Euclidean distance, but as in [3,12,13], cosine worked better.", "startOffset": 45, "endOffset": 54}, {"referenceID": 10, "context": "As mentioned in Section 1, recent studies [11, 12] have also been using frame-level embedding approaches in combination with DTW to perform the same-different task.", "startOffset": 42, "endOffset": 50}, {"referenceID": 11, "context": "As mentioned in Section 1, recent studies [11, 12] have also been using frame-level embedding approaches in combination with DTW to perform the same-different task.", "startOffset": 42, "endOffset": 50}, {"referenceID": 10, "context": "We compare our results to that of [11], which uses posteriograms over a partitioned universal background model (UBM), as well as [12], which uses a correspondence autoencoder.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "We compare our results to that of [11], which uses posteriograms over a partitioned universal background model (UBM), as well as [12], which uses a correspondence autoencoder.", "startOffset": 129, "endOffset": 133}, {"referenceID": 27, "context": "We used the Theano [28] toolkit to implement the CNN-based models of Sections 2.", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "Models are trained using ADADELTA [29], an adaptive learning rate stochastic optimization method that adapts over time based on an accumulation of past gradients; we set the momentum hyper-parameter to \u03c1 = 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 2, "context": "Model 4 is the best acoustic word embedding approach from [3] (Section 2.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "2In [3], a slightly different training set was used.", "startOffset": 4, "endOffset": 7}, {"referenceID": 10, "context": "2 Best partitioned UBM [11] 100 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "3 Correspondence autoencoder [12] 100 0.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "4 Reference vector approach [3] 50 0.", "startOffset": 28, "endOffset": 31}], "year": 2016, "abstractText": "Recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units. Such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types, directly in the embedding space. We compare several old and new approaches in a word discrimination task. Our best approach uses side information in the form of known word pairs to train a Siamese convolutional neural network (CNN): a pair of tied networks that take two speech segments as input and produce their embeddings, trained with a hinge loss that separates same-word pairs and different-word pairs by some margin. A word classifier CNN performs similarly, but requires much stronger supervision. Both types of CNNs yield large improvements over the best previously published results on the word discrimination task.", "creator": "LaTeX with hyperref package"}}}