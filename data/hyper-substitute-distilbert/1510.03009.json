{"id": "1510.03009", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2015", "title": "Neural Networks with Few Multiplications", "abstract": "facilitating more deep learning analytic iteration continues notoriously for ineffective. since most of modern investment in efficient engine activity is typically built on floating point optimization, we investigate an approach to training that eliminates mathematical obstacles carrying most of machinery. our example consists of explicit demands : providing we accurately binarize coefficients to translate signals involved in computing hidden states to sign changes. but, while back - balancing point derivatives, in addition to separating the weights, we quantize texture representations at each layer to convert storing remaining multiplications reflecting time shifts. distinct properties regarding contemporary popular datasets ( ml, db, svhn ) show that this approach so only acts not hurt training results but can result in hopefully better difficulty than standard low gradient descent optimization, paving the challenge to fast, hardware - bound training of neural learners.", "histories": [["v1", "Sun, 11 Oct 2015 04:32:39 GMT  (85kb,D)", "http://arxiv.org/abs/1510.03009v1", "7 pages, 3 figures"], ["v2", "Mon, 9 Nov 2015 20:16:10 GMT  (111kb,D)", "http://arxiv.org/abs/1510.03009v2", "Submitted to ICLR 2016, 8 pages, 3 figures; More descriptions, add pseudo-code, and more experiments"], ["v3", "Fri, 26 Feb 2016 05:24:30 GMT  (111kb,D)", "http://arxiv.org/abs/1510.03009v3", "Published as a conference paper at ICLR 2016. 9 pages, 3 figures"]], "COMMENTS": "7 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["zhouhan lin", "matthieu courbariaux", "roland memisevic", "yoshua bengio"], "accepted": true, "id": "1510.03009"}, "pdf": {"name": "1510.03009.pdf", "metadata": {"source": "CRF", "title": "Neural Networks with Few Multiplications", "authors": ["Zhouhan Lin", "Matthieu Courbariaux"], "emails": ["zhouhan.lin@umontreal.ca", "matthieu.courbariaux@polymtl.ca", "roland.memisevic@umontreal.ca", "yoshua.umontreal@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Training deep neural networks has long been computational demanding and time consuming. For some state-of-the-art architectures, it can take weeks to get models trained [Krizhevsky et al., 2012]. Another problem is that the demand for memory can be huge. For example, many commmon models in speech recognition or machine translation need 12 Gigabytes or more of storage [Gulcehre et al., 2015]. To deal with these issues it is common to train deep neural networks by resorting to GPU or CPU clusters and to well designed parallelization strategies [Le, 2013].\nMost of the computation performed in training a neural network are floating point multiplications. In this paper, we focus on eliminating most of these multiplications to reduce computation. Based on our previous work [Courbariaux et al., 2015], which eliminates multiplications in computing hidden representations by binarizing weights, our method deals with both hidden state computations and backward weight updates. Our approach has 2 components. In the forward pass, weights are stochastically binarized using an approach we call binary connect or ternary connect, and for backpropagation of errors, we propose a new approach which we call quantized back propagation that converts multiplications into bit-shifts."}, {"heading": "2 Related work", "text": "Several approaches have been proposed in the past to simplify computations in neural networks. Some of them try to restrict weight values to be an integer power of two, thus to reduce all the multiplications to be binary shifts [Kwan and Tang, 1993] [Marchesi et al., 1993]. In this way, multipli-\nar X\niv :1\n51 0.\n03 00\n9v 1\n[ cs\n.L G\n] 1\n1 O\ncations are eliminated in both training and testing time. The disadvantage is that model performance can be severely reduced, and convergence of training can no longer be guaranteed.\nAnother approach, [Kim and Paris, 2015], introduces a completely Boolean network, which simplifies the test time computation at an acceptable performance hit. The approach still requires a real-valued, full resolution training phase, however, so the benefits of reducing computations does not apply to training. Similarly, [Machado et al., 2015] manage to get acceptable accuracy on sparse representation classification by replacing all floating-point multiplications by integer shifts. Bitstream networks [Burge et al., 1999] also provides a way of binarizing neural network connections, by substituting weight connections with logical gates.\nThere are some other techniques, which focus on reducing the training complexity. For instance, instead of reducing the resolution of weights, [Simard and Graf, 1994] quantizes states, learning rates, and gradients to powers of two. This approach manages to eliminate multiplications with negligible performance reduction."}, {"heading": "3 Binary and ternary connect", "text": ""}, {"heading": "3.1 Binary connect revisited", "text": "In [Courbariaux et al., 2015], we introduced a weight binarization technique which removes multiplications in the forward pass. We summarize this approach in this subsection, and introduce an extenion to it in the next.\nConsider a neural network layer with N input and M output units. The forward computation is y = h(Wx + b) where W and b are weights and biases, respectively, h is the activation function, and x and y are the layer\u2019s inputs and outputs. If we choose ReLU as h, there will be no multiplications in computing the activation function, thus all multiplications reside in the matrix product Wx. For each input vector x, NM floating point multiplications are needed.\nBinary connect eliminates these multiplications by stochastically sampling weights to be \u22121 or 1. Full resolution weights w\u0304 are kept in memory as reference, and each time when y is needed, we sample a stochastic weight matrix W according to w\u0304. For each element of the sampled matrix W , the probability of getting a 1 is proportional to how \u201dclose\u201d its corresponding entry in w\u0304 is to 1. i.e.,\nP (Wij = 1) = w\u0304ij + 1\n2 ; P (Wij = \u22121) = 1 \u2212 P (Wij = 1) (1)\nIt is necessary to add some edge constraints to w\u0304. To ensure that P (Wij = 1) lies in a reasonable range, values in w\u0304 are forced to be a real value in the interval [-1, 1]. If during the updates any of its value grows beyond that interval, we set it to be its corresponding edge values \u22121 or 1. That way floating point multiplications become sign changes.\nA remaining question concerns the use of multiplications in the random number generator involved in the sampling process. Sampling an integer has to be faster than multiplication for the algorithm to be worth it. Fortunately, efficiently generating random numbers has been studied in [Jeavons et al., 1994] [van Daalen et al., 1993]. Also, it is possible to get random numbers according to real random processes, like CPU temperatures, etc. We are not going into the details of random number generation as this is not the focus of this paper."}, {"heading": "3.2 Ternary connect", "text": "The binary connect introduced in the former subsection allows weights to be \u22121 or 1. However, in a trained neural network, it is common to observe that many learned weights are zero or close to zero. Although the stochastic sampling process would allow the mean value of sampled weights to be zero, this suggests that it may be beneficial to explicitly allow weights to be zero.\nTo allow weights to be zero, some adjustments are needed for Eq. 1. We split the interval of [-1, 1], within which the full resolution weight value w\u0304ij lies, into two sub-intervals: [\u22121, 0] and (0, 1]. If a weight value w\u0304ij drops into one of them, we sample w\u0304ij to be the two edge values of that interval, according to their distance from w\u0304ij , i.e., if w\u0304ij > 0:\nP (Wij = 1) = w\u0304ij ; P (Wij = 0) = 1 \u2212 w\u0304ij (2)\nand if w\u0304ij <= 0:\nP (Wij = \u22121) = \u2212w\u0304ij ; P (Wij = 0) = 1 + w\u0304ij (3)\nLike binary connect, ternary connect also eliminates all multiplications in the forward pass."}, {"heading": "4 Quantized back propagation", "text": "In the former section we described how multiplications can be eliminated from the forward pass. In this section, we propose a way to eliminate multiplications from the backward pass.\nSuppose the i-th layer of the network has N input and M output units, and consider an error signal \u03b4 propagating downward from its output. The updates for weights and biases would be the outer product of the layer\u2019s input and the error signal:\n\u2206W = \u03b7\u03b4 \u25e6 h \u2032 (Wx + b)xT (4)\n\u2206b = \u03b7\u03b4 \u25e6 h \u2032 (Wx + b) (5)\nwhere \u03b7 is the learning rate, and x the input to the layer. While propagating through the layers, the error signal \u03b4 needs to be updated, too. Its update taking into account the next layer below takes the form:\n\u03b4 = WT \u03b4 \u25e6 h \u2032 (Wx + b) (6)\nThere are 3 terms that appear repeatedly in Eqs. 4 to 6: \u03b4, h \u2032 (Wx + b) and x. The latter two terms introduce matrix outer products. To eliminate multiplications, we can quantize one of them to be an integer power of 2, so that multiplications involving that term become binary shifts. The expression h \u2032 (Wx + b) contains downflowing gradients, which are largely determined by the cost function and network parameters, thus it is hard to bound its values. However, bounding the values is essential for quantization because we need to supply a fixed number of bits for each sampled value, and if that value varies too much, we will need too many bits for the exponent. This, in turn, will result in the need for more bits to store the sampled value and unnecessarily increase the required amount of computation.\nWhile h \u2032 (Wx + b) is not a good choice for quantization, x is a better choice, because it is the hidden representation at each layer, and we know roughly the distribution of each layer\u2019s activation.\nOur approach is therefore to eliminate multiplications in Eq. 4 by quantizing each entry in x to an integer power of 2. That way the outer product in Eq. 4 becomes a series of bit shifts. Experimentally, we find that allowing a maximum of 3 to 4 bits of shift is sufficient to make the network work well. This means that 3 bits are already enough to quantize x. As the float32 format has 24 bits of mantissa, shifting (to the left or right) by 3 to 4 bits is completely tolerable. We refer to this approach of back propagation as \u201dquantized back propagation.\u201d\nIf we choose ReLU as the activation function and use binary (ternary) connect to sampleW , computing the term h \u2032 (Wx + b) involves no multiplications at all. In addition, quantized back propagation eliminates the multiplications in the outer product in Eq. 4. The only place where multiplications remain is the element-wise product. From Eqs. 4 to 6, we can see that 6 \u00d7M multiplications are needed for all computations.\nLike in the forward pass, most of the multiplications are used in the weight updates. Compared with standard back propagation, which would need 2MN + 6M multiplications, the amount of multiplications left is negligible in quantized back propagation. Our experiments in Section 5 show that this way of dramatically decreasing multiplications does not necessarily entail a loss in performance."}, {"heading": "5 Experiments", "text": "We tried our approach on both fully connected networks and convolutional networks. Our implementation uses Theano [Bastien et al., 2012]. We experimented with 3 datasets: MNIST, CIFAR10, and SVHN. In the following subsection we show the performance that these multiplier-light neural networks can achieve. In the subsequent subsections we study some of their properties, such as convergence and robustness, in more detail."}, {"heading": "5.1 General performance", "text": "We tested different variations of our approach, and compare the results with [Courbariaux et al., 2015] and full precision training (Table 1). All models are trained with stochastic gradient descent (SGD) without momentum. We use batch normalization for all the models to accelerate learning. At training time, binary (ternary) connect and quantized back propagation are used, while at test time, we use the learned full resolution weights for the forward propagation. For each dataset, all hyper-parameters are set to the same values for the different methods, except that the learning rate is adapted independently for each one."}, {"heading": "5.1.1 MNIST", "text": "The MNIST dataset [LeCun et al., 1998] has 50000 images for training and 10000 for testing. All images are grey value images of size 28 \u00d7 28 pixels, falling into 10 classes corresponding to the 10 digits. The model we use is a fully connected network with 4 layers: 784-1024-1024-1024-10. At the last layer we use the hinge loss as the cost. The training set is separated into two parts, one of which is the training set with 40000 images and the other the validation set with 10000 images. Our experimental results show that despite removing most multiplications, our approach yields a comparable (in fact, even slightly higher) performance than full resolution training. The performance improvement is likely due to the regularization effect implied by the stochastic sampling."}, {"heading": "5.1.2 CIFAR10", "text": "CIFAR10 [Krizhevsky and Hinton, 2009] contains images of size 32 \u00d7 32 RGB pixels. Like for MNIST, we split the dataset into 40000, 10000, and 10000 training-, validation-, and test-cases, respetively. We apply our approach in a convolutional network for this dataset. The network has 6 convolution/pooling layers, 1 fully connected layer and 1 classification layer. We use the hinge loss for training. Similar to what we observed in the fully connected network, binary (ternary) connect and quantized back propagation yield a slightly higher performance than ordinary SGD."}, {"heading": "5.1.3 SVHN", "text": "The Street View House Numbers (SVHN) dataset [Netzer et al., 2011] contains RGB images of house numbers. It contains more than 600,000 images in its extended training set, and roughly 26,000 images in its test set. We remove 6,000 images from the training set for validation. We use 7 layers of convolution/pooling, 1 fully connected layer, and 1 classification layer. The performances we get is consistent with our results on CIFAR10. Again, it slightly improves over ordinary SGD by using binary (ternary) connect and quantized back propagation."}, {"heading": "5.2 Convergence", "text": "Taking the convolutional networks on CIFAR10 as a test-bed, we now study the learning behaviour in more detail. Figure 1 shows the performance of the model in terms of test set errors during training. The figure shows that binarization makes the network converge slower than ordinary SGD, but yields a better optimum after the algorithm converges. Compared with binary connect (red line), adding quantization in the error propagation (yellow line) doesn\u2019t hurt the model accuracy at all. Moreover, having ternary connect combined with quantized back propagation (green line) surpasses all the other three approaches."}, {"heading": "5.3 The effect of bit clipping", "text": "In Section 4 we mentioned that quantization will be limited by the number of bits we use. The maximum number of bits to shift determines the amount of memory needed, but it also determines in what range a single weight update can vary. Figure 2 shows the model performance as a function\nof the maximum allowed bit shifts. These experiments are conducted on the MNIST dataset, with the aforementioned fully connected model. For each case of bit clipping, we repeat the experiment for 10 times with different initial random instantiations.\nThe figure shows that the approach is not very sensible to the number of bits used. The maximum allowed shift in the figure varies from 2 bits to 10 bits, and the performance remains roughly the same. Even by restricting bit shifts to 2, the model can still learn successfully. The fact that the performance is not very sensitive to the maximum of allowed bit shifts suggests that we do not need to redefine the number of bits used for quantizing x for different tasks, which would be an important practical advantage.\nThe x to be quantized is not necessarily distributed symmetrically around 2. For example, Figure 3 shows the distribution of x at each layer in the middle of training. The maximum amount of shift to the left does not need to be the same as that on the right. A more efficient way is to use different values for the maximum left shift and the maximum right shift. Bearing that in mind, we set it to 3 bits maximum to the right and 4 bits to the left."}, {"heading": "6 Conclusion and future work", "text": "We proposed a way to eliminate most of the floating point multiplications used during training a feedforward neural network. This could make it possible to dramatically accelerate the training of neural networks by using dedicated hardware implementations. A somewhat surprising fact is that instead of damaging prediction accuracy the approach tends improve it, which is probably due to the regularization effect it entails. Directions for future work include exploring actual implementations of this approach (for example, using FPGA), seeking more efficient ways of binarization, and the extension to recurrent neural networks."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bastien et al", "F. 2012] Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Pulsed neural networks. chapter Stochastic Bit-stream Neural Networks, pages 337\u2013352", "author": ["Burge et al", "P.S. 1999] Burge", "M.R. van Daalen", "B.J.P. Rising", "J.S. Shawe-Taylor"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1999\\E", "shortCiteRegEx": "al. et al\\.", "year": 1999}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Courbariaux et al", "M. 2015] Courbariaux", "Y. Bengio", "David", "J.-P"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Gulcehre et al", "C. 2015] Gulcehre", "O. Firat", "K. Xu", "K. Cho", "L. Barrault", "Lin", "H.-C", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1503.03535", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Generating binary sequences for stochastic computing", "author": ["Jeavons et al", "P. 1994] Jeavons", "D.A. Cohen", "J. Shawe-Taylor"], "venue": "Information Theory, IEEE Transactions", "citeRegEx": "al. et al\\.,? \\Q1994\\E", "shortCiteRegEx": "al. et al\\.", "year": 1994}, {"title": "Bitwise neural networks", "author": ["Kim", "Paris", "M. 2015] Kim", "S. Paris"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Hinton", "A. 2009] Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105", "author": ["Krizhevsky et al", "A. 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Multiplierless multilayer feedforward neural network design suitable for continuous input-output mapping", "author": ["Kwan", "Tang", "H.K. 1993] Kwan", "C. Tang"], "venue": null, "citeRegEx": "Kwan et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Kwan et al\\.", "year": 1993}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["V. Q"], "venue": "[Le,", "citeRegEx": "Q.,? \\Q2013\\E", "shortCiteRegEx": "Q.", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al", "Y. 1998] LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "Computational cost reduction in learned transform classifications. arXiv preprint arXiv:1504.06779", "author": ["Machado et al", "E.L. 2015] Machado", "C.J. Miosso", "R. von Borries", "M. Coutinho", "Berger", "P. d. A", "T. Marques", "R.P. Jacobi"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Fast neural networks without multipliers", "author": ["Marchesi et al", "M. 1993] Marchesi", "G. Orlandi", "F. Piazza", "A. Uncini"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "al. et al\\.,? \\Q1993\\E", "shortCiteRegEx": "al. et al\\.", "year": 1993}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer et al", "Y. 2011] Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Backpropagation without multiplication", "author": ["Simard", "Graf", "P.Y. 1994] Simard", "H.P. Graf"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Simard et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Simard et al\\.", "year": 1994}, {"title": "Device for generating binary sequences for stochastic computing", "author": ["van Daalen et al", "M. 1993] van Daalen", "P. Jeavons", "J. Shawe-Taylor", "D. Cohen"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1993\\E", "shortCiteRegEx": "al. et al\\.", "year": 1993}], "referenceMentions": [], "year": 2015, "abstractText": "For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardwarefriendly training of neural networks.", "creator": "LaTeX with hyperref package"}}}