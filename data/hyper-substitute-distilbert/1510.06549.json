{"id": "1510.06549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Oct-2015", "title": "Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling", "abstract": "interface is an administrator storing data, themes, and other content, and then require tools will analyze and interpret discourse, tools to turn that content into information and knowledge. topic modeling tools gradually developed to solve these problems. topic models grow when lda [ kim et. al. 2003 ] allow inference patterns into data without getting sorted automatically. correctly analyzing patterns, article patterns are called descriptive. among statistical initiatives throughout lda, authors succeeding them can simultaneously analyze all groups of literature and identify topic similarities. recently, the introduction integrating complex topic modeling ( spdp ) [ chen n. al. 1966 ] performs one better resolving many topic simulations against a continuous setting.", "histories": [["v1", "Thu, 22 Oct 2015 09:40:54 GMT  (4984kb,D)", "http://arxiv.org/abs/1510.06549v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.DC cs.LG", "authors": ["aaron q li"], "accepted": false, "id": "1510.06549"}, "pdf": {"name": "1510.06549.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling\nby\nAaron(Qiaochu) Li\nA thesis submitted in partial satisfaction of the\nrequirements for the degree of\nBachelor of Science(Advanced)(Honours)\nin\nResearch School of Computer Science\nof\nAustralian National University\nCommittee in charge:\nDoctor Wray Buntine, Supervisor Doctor Scott Sanner, Co-supervisor\nSemester 2 2012 ar X\niv :1\n51 0.\n06 54\n9v 1\n[ cs\n.C L\n] 2\n2 O\nct 2\n01 5\nMulti-GPU Distributed Parallel Bayesian Differential Topic Modelling\nCopyright 2012 by\nAaron(Qiaochu) Li\ni Abstract\nMulti-GPU Distributed Parallel Bayesian Differential Topic Modelling\nby\nAaron(Qiaochu) Li Bachelor of Science(Advanced)(Honours)\nResearch School of Computer Science Australian National University\nDoctor Wray Buntine, Supervisor Doctor Scott Sanner, Co-supervisor\nThere is an explosion of data, documents, and other content, and people require tools to analyze and interpret these, tools to turn the content into information and knowledge. Topic modelling have been developed to solve these problems. Bayesian topic models such as Latent Dirichlet Allocation (LDA) [1] allow salient patterns in large collection of documents to be extracted and analyzed automatically. When analyzing texts, these patterns are called topics, represented as a distribution of words. Although numerous extensions of LDA have been created in academia in the last decade to address many problems, few of them can reliablily analyze multiple groups of documents and extract the similarities and differences in topics across these groups. Recently, the introduction of techniques for differential topic modelling, namely the Shadow Poisson Dirichlet Process model (SPDP) [2] performs uniformly better than many existing topic models in a discriminative setting.\nThere is also a need to improve the running speed of algorithms for topic models. While some effort has been made for distributed algorithms, there is no work currently done using graphical processing units (GPU). Note the GPU framework has already become the most cost-efficient and popular parallel platform for many research and industry problems.\nIn this thesis, I propose and implement a scalable multi-GPU distributed parallel framework which approximates SPDP, called MGPU-DP-SPDP, and a version running on a single GPU, Improved-GPU-SPDP. Through experiments, I have shown ImprovedGPU-SPDP improved the running speed of SPDP by about 50 times while being almost as accurate as SPDP, with only one single cheap laptop GPU. Furthermore, I have shown the speed improvement of MGPU-DP-SPDP is sublinearly scalable when multiple GPUs are used, while keeping the accuracy fairly comparable to SPDP. Therefore, on a mediumsized GPU cluster, the speed improvement could potentially reach a factor of a thousand.\nNote SPDP is just a representative of perhaps another hundred other extensions of LDA. Although my algorithm is implemented to work with SPDP, it is designed to be a general framework that can be extended to work with other LDA extensions and improve\nii\ntheir speed, with only a small amount of modification. The speed-up on smaller collections, typically gained as a result of an exploratory query to a search engine (i.e., 1000s of documents rather than 100,000s), means that these more complex LDA extensions could now be done in real-time, thus opening up a new way of using these LDA models in industry.\niii\nContents\nList of Figures vi\nList of Tables viii"}, {"heading": "1 Introduction 1", "text": "1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Introduction to Topic Modelling . . . . . . . . . . . . . . . . . . . . . . . . 3 1.3 Introduction to Graphical Processing Units (GPUs) . . . . . . . . . . . . . 4 1.4 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"}, {"heading": "2 Background 7", "text": "2.1 Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3 Existing Bayesian Topic Models . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.3.1 Latent Dirichlet Allocation (LDA) . . . . . . . . . . . . . . . . . . . 9 2.3.2 Pitman-Yor Topic Modelling (PYTM) . . . . . . . . . . . . . . . . 10 2.3.3 Hierarchical Pitman-Yor Topic Modelling (HPYTM) . . . . . . . . 14 2.3.4 Differential Topic Modelling Using Shadow Poisson Dirichlet Process 16\n2.4 Model Derivation and Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . 20 2.4.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.4.2 Latent Dirichlet Allocation (LDA) . . . . . . . . . . . . . . . . . . . 21 2.4.3 Pitman-Yor Topic Modelling (PYTM) . . . . . . . . . . . . . . . . 23\n2.4.3.1 Poisson Dirichlet Process . . . . . . . . . . . . . . . . . . 23 2.4.3.2 Predictive Probability and Inference . . . . . . . . . . . . 24\n2.4.4 Hierarchical Pitman-Yor Topic Modelling (HPYTM) . . . . . . . . 25 2.4.5 Shared Topic Modelling Using Shadow Poisson Dirichlet Process . . 25"}, {"heading": "3 Problems and Solutions 33", "text": "3.1 SPDP Computational Time Analysis . . . . . . . . . . . . . . . . . . . . . 34\n3.1.1 Theoretical Running Time Analysis . . . . . . . . . . . . . . . . . . 34 3.1.2 Practical Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.1.3 Parallelization Issues . . . . . . . . . . . . . . . . . . . . . . . . . . 35\niv\n3.2 The Goal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.2.1 Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n3.2.2 Topic Performance Measure . . . . . . . . . . . . . . . . . . . . . . 36\n3.2.2.1 PMI-Score Based on Wikipedia Corpus . . . . . . . . . . . 37\n3.2.2.2 Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.2.2.3 Topic Quality and Intepretability . . . . . . . . . . . . . . 38\n3.3 The Innovation: Speeding Up SPDP . . . . . . . . . . . . . . . . . . . . . 39\n3.3.1 Distributed Parallelization Proposals . . . . . . . . . . . . . . . . . 39\n3.3.1.1 Basic Parallelism: Over Topics and Word-Associations . . 39\n3.3.1.2 Parallel Word Sampling . . . . . . . . . . . . . . . . . . . 40\n3.3.1.3 Parallelism With Improved Accuracy: Word Order Rearrangement . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.3.1.4 Traditional Distributed Model: Dividing Documents . . . 43\n3.3.2 All-in-one: Putting Everything Together . . . . . . . . . . . . . . . 44\n3.4 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.4.1 Parallelization Framework Comparisons . . . . . . . . . . . . . . . . 48\n3.4.1.1 CPU v.s. GPU . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.4.1.2 OpenCL v.s CUDA . . . . . . . . . . . . . . . . . . . . . . 51\n3.4.1.3 Hardware and Architectures . . . . . . . . . . . . . . . . . 54\n3.4.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57"}, {"heading": "4 Experiments 60", "text": "4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n4.2.1 Basic Parallelization Over Topics . . . . . . . . . . . . . . . . . . . 71\n4.2.2 Parallelization Over Words . . . . . . . . . . . . . . . . . . . . . . . 71\n4.2.3 Effect of Re-ordering Words . . . . . . . . . . . . . . . . . . . . . . 79\n4.2.4 Multi-GPU Distributed Parallelism . . . . . . . . . . . . . . . . . . 85\n4.2.5 Multi-GPU Distributed Parallelism: Effect of Duplicating Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n4.2.6 Hellinger Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . 95"}, {"heading": "5 Conclusion 98", "text": "5.1 Conclusions and Key Contributions . . . . . . . . . . . . . . . . . . . . . . 99\n5.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.2.1 Automated Optimization . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.2.2 Scalability And Robustness of Multi-GPU Sampling . . . . . . . . . 100\n5.2.3 Full SPDP Parallelization With Non-identity Transformation Matrices100\n5.2.4 Large Scale Implementation . . . . . . . . . . . . . . . . . . . . . . 101\n5.2.5 Adapting the Three-level Distributed Parallel Framework to Other LDA Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nv Bibliography 102\nvi\nList of Figures\n2.3.1 LDA graphical model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3.2 Word frequency (y-axis in log scale) v.s Word frequency ranking (x-axis in log scale) in Wikipedia (extracted from Wikipedia [3] under LGPL license) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.3.3 PYTM graphical model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.3.4 PYTM graphical model breakdown. . . . . . . . . . . . . . . . . . . . . . . 13\n2.3.5 HPYTM graphical model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.3.6 HPYTM graphical model breakdown. . . . . . . . . . . . . . . . . . . . . . 16\n2.3.7 SPDP graphical model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.3.1 SPDP Multi-GPU Distributed Parallel Sampling Proposal . . . . . . . . . 47\n3.4.1 OpenCL Framework (from [4]) . . . . . . . . . . . . . . . . . . . . . . . . . 52\n3.4.2 AMD HD5870 \u201cCypress\u201d Architecture (from [5]) . . . . . . . . . . . . . . . 54\n3.4.3 NVIDIA GTX480 \u201cFermi\u201d Architecture (from [5]) . . . . . . . . . . . . . 55\n4.2.1 SPDP perplexity through Gibbs iterations (logarithm scaled) with the dataset \u201cRedState v.s DailyKos\u201d . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.2.2 SPDP perplexity through Gibbs iterations (logarithm scaled) with the dataset \u201cReuters Disasters\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.2.3 SPDP perplexity through Gibbs iterations (logarithm scaled) with the dataset \u201cReuters Disasters\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.2.4 GPU-SPDP perplexity v.s Original SPDP with the dataset \u201cRedState v.s DailyKos\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n4.2.5 GPU-SPDP perplexity v.s Original SPDP with the dataset \u201cReuters Disasters\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n4.2.6 GPU-SPDP perplexity v.s Original SPDP with the dataset \u201cInternational Political News\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n4.2.7 Improved GPU-SPDP perplexity with the dataset \u201cRedState v.s DailyKos\u201d 80\n4.2.8 Improved GPU-SPDP perplexity with the dataset \u201cRedState v.s DailyKos\u201d 80\nvii\n4.2.9 Improved GPU-SPDP perplexity with the dataset \u201cRedState v.s DailyKos\u201d 81 4.2.10 MGPU-DP-SPDP perplexity convergence with the dataset \u201cRedState v.s DailyKos\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 4.2.11 MGPU-DP-SPDP perplexity convergence with the dataset \u201cReuters Disasters\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 4.2.12 MGPU-DP-SPDP perplexity convergence with the dataset \u201cInternational Political News\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 4.2.13 MGPU-DP-SPDP perplexity convergence with the duplicated dataset \u201cRedState v.s DailyKos\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 4.2.14 Hellinger distance between the original SPDP and Improved-GPU-SPDP with the dataset \u201cReuters Disasters\u201d . . . . . . . . . . . . . . . . . . . . . 96 4.2.15 Hellinger distance between the original SPDP and MGPU-DP-SPDP with the dataset \u201cReuters Disasters\u201d . . . . . . . . . . . . . . . . . . . . . . . . 97\nviii\nList of Tables\n3.1 CPU v.s GPU comparison (extracted from [6]) . . . . . . . . . . . . . . . . 49\n3.2 Intel CPU v.s NVIDIA GPU comparison (from [7]) . . . . . . . . . . . . . 50\n3.3 Intel CPU v.s NVIDIA GPU memory bandwidth comparison (from [7]) . 50\n3.4 GTX480 v.s. HD5870 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n4.1 Experiment Hardware Platform Comparison . . . . . . . . . . . . . . . . . 63\n4.2 SPDP running time and perplexity with the dataset \u201cRedState v.s DailyKos\u201d 64\n4.3 SPDP topic quality progression on topic 11 with the dataset \u201cRedState v.s DailyKos\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n4.4 SPDP running time for different number of topics on \u201cRedState v.s DailyKos\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.5 SPDP running time with the datasets \u201cReuters Disaster\u201d and \u201cInternational Political News\u201d (K = 32) . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.6 SPDP topic quality with dataset \u201cRedState v.s DailyKos\u201d . . . . . . . . . 68\n4.7 SPDP topic quality with the dataset \u201cReuters Disasters\u201d . . . . . . . . . . 69\n4.8 SPDP topic quality with the dataset \u201cInternational Political News\u201d. . . . 70\n4.9 SPDP basic parallelism result with the dataset \u201cRedState v.s DailyKos\u201d . 71\n4.10 GPU-SPDP running time with the dataset \u201cRedState v.s DailyKos\u201d . . . 72\n4.11 GPU-SPDP running time with the datasets \u201cReuters Disasters\u201d and \u201cInternational Political News\u201d when K = 32 . . . . . . . . . . . . . . . . . . . 72\n4.12 GPU-SPDP topic quality with the dataset \u201cRedState v.s DailyKos\u201d . . . 76\n4.13 GPU-SPDP topic quality with the dataset \u201cReuters Disasters\u201d . . . . . . 77\n4.14 GPU-SPDP topic quality with the dataset \u201cInternational Political News\u201d 78\n4.15 Improved-GPU-SPDP topic quality with the dataset \u201cRedState v.s DailyKos\u201d 82\n4.16 Improved-GPU-SPDP topic quality with the dataset \u201cReuters Disasters\u201d 83\n4.17 Improved-GPU-SPDP topic quality with the dataset \u201cInternational Political News\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n4.18 MGPU-DP-SPDP Running Time with the dataset \u201cRedState v.s DailyKos\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nix\n4.19 MGPU-DP-SPDP Running Time with the dataset \u201cReuters Disasters\u201d and \u201cInternational Political News\u201d when K = 32 . . . . . . . . . . . . . . 86 4.20 MGPU-DP-SPDP topic quality with the dataset \u201cRedState v.s DailyKos\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 4.21 MGPU-DP-SPDP topic quality with the dataset \u201cReuters Disasters\u201d . . . 90 4.22 MGPU-DP-SPDP topic quality with the dataset \u201cInternational Political News\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 4.23 MGPU-DP-SPDP topic quality with the duplicated dataset \u201cRedState\nv.s DailyKos\u201d . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\nx"}, {"heading": "Acknowledgments", "text": "I would like to thank Dr. Wray Buntine and Dr. Scott Sanner for their guidance and support throughout the year. The research is a journey exploring the unknown. Their advices and experiences have saved me many times through the journey when I was about to give up, and guided me to the correct path when I thought it was impossible to go ahead. In particular, I would like to thank them for sharing their knowledge for many things, encouraging me to challenge myself with things that others have never done before, inspiring me with their frontier research results, and spending a large amount of their time to help me refine this thesis.\nI would also like to thank Changyou Chen, who patiently explained to me the background knowledge of the field, and generously shared his expertise in differential topic models. Without Changyou\u2019s tremendous amount of work previously done on differential topic models, I would not be able to deliver the research with this many interesting results.\nFinally, I would like to thank ANU and NICTA, who brought researchers together, created and maintained the environment for researchers to work on things they love to do.\n1 Chapter 1\nIntroduction\n2"}, {"heading": "1.1 Overview", "text": "There is an explosion of data, documents, and other content, and people require tools to analyze and interpret these, tools to turn the content into information and knowledge. Topic modelling is a research area that has been developed for exploratory information access, and can be applied to documents in particular to support the task of understanding content. Bayesian topic models such as Latent Dirichlet Allocation (LDA) [1] allow salient patterns in large collection of documents to be extracted and analyzed automatically. When analyzing texts, these patterns are called topics, represented as a distribution of words.\nAs research effort in topic models are getting slowly adapted to industry practice, there is a need to improve the running speed of the algorithms. In particular, in exploratory data analysis, it is usually the case that topic modelling is done in an interactive environment, where fast response is important. When data analysis is provided as a service, it is also important to do the computation cost-efficiently. While some effort has been made for distributed algorithms, there is no work currently done using graphical processing units (GPU). Note the GPU framework has already become the most cost-efficient and popular parallel platform for many research and industry problems.\nAlthough numerous extensions of LDA has been created in academia in the last decade to address many problems, few of them can reliablily analyze multiple groups of documents and extract the similarities and differences in topics across these groups. This problem is seen when businesses want to do comparative analysis, or when political analysts want to understand opinions across different political groups. Recently, the introduction of techniques for differential topic modelling, namely the Shadow Poisson Dirichlet Process model (SPDP) [2] performs uniformly better than many existing topic models in a discriminative setting.\nIn this thesis, I propose and implement a scalable multi-GPU distributed parallel framework which approximates SPDP, called MGPU-DP-SPDP, and a version running on a single GPU, Improved-GPU-SPDP. Through experiments, I have shown Improved-GPUSPDP improved the running speed of SPDP by about 50 times, with only one single cheap laptop GPU. Furthermore, I have shown the speed improvement of MGPU-DP-SPDP is sublinearly scalable when multiple GPUs are used. Therefore, on a medium-sized GPU cluster, the speed improvement could potentially reach thousands of times. My experiments have shown when a single GPU is used, Improved-GPU-SPDP is almost as accurate as SPDP, as measured by perplexity and intepretability; when multiple GPUs are used, MGPU-DP-SPDP is fairly comparable.\n3 Note SPDP is a representative of perhaps another hundred LDA extensions. Although my algorithm is implemented to work with SPDP, it is designed to be a general framework that can be extended to work with other LDA extensions and improve their speed, with only a small amount of modification. The speed up on smaller collections, typically gained as a result of an exploratory query to a search engine (i.e., 1000s of documents rather than 100,000s), means that these more complex LDA extensions could now be done in realtime, thus opening up a new way of using these LDA models in industry."}, {"heading": "1.2 Introduction to Topic Modelling", "text": "Today, the amount of information available to us is far greater than our capacity to process the information. The explosion of information has led to the rise of a new research area: Information Access. People need tools to organize, search, summarize, and understand information, tools to turn information into knowledge [8].\nTechniques such as topic modelling were invented to address these issues. Topic models uncover the underlying patterns in a collection of documents through analyzing the semantic content. Bayesian topic models are a class of topic models that assume a document contains multiple patterns to different extents, represented as Bayesian latent variables. When analyzing text, these patterns are represented as a distribution of words, called \u201ctopics\u201d. For example, (\u201cJava\u201d 0.25, \u201cC++\u201d 0.3, \u201cC\u201d 0.1, \u201cPython\u201d 0.1, \u201ccomputer\u201d 0.15, \u201cscience\u201d 0.1) can be interpreted as the topic \u201cprogramming languages\u201d.\nAt present, most search engines and document analysis tools uses keywords and relationships between documents as fundamental metrics. While these tools work reasonably well in searching for specific terms and have been shown to be able to find popular documents with respect to public opinion, they cannot explore the underlying patterns and topics among these documents, that someone may want to do in an exploratory analysis. By comparison, topic models provide insightful analysis on the topics contained by documents through statistical methods, represented in probability distributions over topics and words. Today, there are many proposed applications of topic modelling to different industries:\n\u2022 Finance, media, governmental: Public sentiment analysis [9]\n\u2022 Social network companies: Content based social network recommendation systems [10, 11]\n\u2022 Media companies: Trend analysis[12], traditional media bias detection [13]\n\u2022 Military: Differential discovery on text collections [2], author detection[14]\n\u2022 Enterprises, consumers: Search engine and document processing [15]\n4 \u2022 And many others...\nHere is an intuitive explanation of how Bayesian topic models work: It is a known fact that a human can quickly skim through a document, summarize the main topics, and provide a few words to describe each topic. Humans achieve this by memorizing words appeared in this document, and implicitly compare the relative frequency of words that have appeared so far. This process that can be imitated by computers with a few conditions: The input data, which is a collection of documents, only contain documents that are short enough for humans to skim through, but also long enough to contain multiple topics. The process is begun with feeding integer labeled words and documents to our program, which contains one topic model. Topic models make a few assumptions on how topics and words are generated by humans, make guesses on the underlying topics, then observe and count the words being fed to the program. Based on the observations, topic models adjust initial guesses on underlying topics, then make a more accurate estimate. The process re-iterates, until the topic models determine that the estimate of underlying topics are accurate enough. Then, the result is read out and presented for human to analyze."}, {"heading": "1.3 Introduction to Graphical Processing Units (GPUs)", "text": "A few years ago the graphical processing unit (GPU) was only considered a dedicated device to render images, or to convert digital images to analog signals for monitors. Most of them are used in high-end personal computers for gaming, by film companies to create animations and special effects, or by large organizations to visualize their data. Through the last few years, people have discovered the potential computing power of GPUs. Pioneers have developed programming frameworks that allow programmers to transfer computing instructions to GPU, to utilize the huge arithmetic computing power inside GPU that hasn\u2019t been properly exploited before ([16]).\nThese days the GPU has already become the one of the most adopted parallel computing devices for many research and industry problems due to its superior performance, cost-efficiency, and enegery-efficiency for massive parallelism. According to the top 500 supercomputer list in June 2012 [17], governments and private insititutions have already invested a massive amount of resources to create supercomputers with multi-GPU architecture. As of today, many companies and public research organizations have specialized teams in high performance computing dedicated to develop massive GPU parallel algorithms for their existing applications.\nLeaders of many industries have started to favor GPU computing as opposed to old-school CPU computing. In the financial industry, industry leaders such as Goldman Sachs and Morgan Stanley invested large amount of money into building GPU-based computing\n5 infrastructures, to run simulations of portofolio, financial market, and many other applications. In the research project Square-Kilometre-Array project ran by CSIRO [18], Australia, GPU is the crucial element to get peta-bytes of data per second processed. Millions of developers and computer technology hobbists around the world are also involved in GPU computing. Bitcoin is the world-first Peer-to-Peer decentralized virtual currency [19], with more than 10 million US dollar equivalent of transactions being processed every month. The network is secured by its users running cryptography algorithm (SHA256) on the network. Before 2011, the mainstream is to run the cryptography algorithm on CPU. The trend shifted completely in a short 2 months after the first GPU version encryption algorithm was developed. Today, among millions of Bitcoin users, almost no one runs the cryptography algorithm on the CPU anymore."}, {"heading": "1.4 Motivation", "text": "Since LDA was published in the last decade, hundreds of extensions have been made for many purposes, appearing in conferences such as ICML, NIPS, KDD, SIGIR, ICCV, and others. Many use sophisticated non-parametric statistics, and can be considerably slower than standard LDA. The problem is, the algorithms are all too slow in practice. When analyzing millions of documents, supercomputers are needed to get the result in a reasonable amount of time. When analyzing a small collection of documents, the running speed and the cost efficiency can still make a decisive distinction in many real world situations. For example, when the analysis tool is provided as a service, it is not feasible to use an expensive computing resource. Furthermore, people prefer to get the result as quickly as possible, rather than wait in a queue for the analysis to be scheduled on supercomputers then wait for hours or days to get the result back. When the analysis is done in an interactive environment, people expect to get fast response and immediate feedback.\nBecause of these cost-efficiency and running speed issues, many research results introduced at the beginning of this chapter cannot be feasibly implemented and used in the real world. Researchers and industries need solutions that are fast, scalable, cost-effective, and generalizable.\nAn example of an LDA extension is SPDP (introduced in 2.3.4), designed for differential text analysis. SPDP is a complex extension of LDA which has longer running time than most LDA extensions. In this thesis, I intend to use SPDP as a representative to implement and find a distributed parallel approximation framework, which should be both conceptually applied to another hundred LDA extensions, and practically implemented with small amount of modification. To address the cost-efficiency issue, I focus on the modern multi-GPU architecture, which has been increasingly popular among both industries and researchers in the last few years. My goal is to find a way to significantly improve\n6 the running speed and cost-efficiency of SPDP, as a representative of other extensions of LDA, under contemporary hardware architecture and framework, so that research results can get truly applied and adopted in industry and the real world, and provide a more efficient tool for researchers.\n7 Chapter 2\nBackground\n8"}, {"heading": "2.1 Scope", "text": "We assume the reader has graduate level background knowledge in general areas of computer science, statistics, and some related mathematics. In addition, to limit the length of our background chapter, we expect the reader to be familiar with standard machine learning, computer architecture, distributed and parallel computing, micro-processors, Bayesian statistics, parameter estimation, statistical inference, and Bayesian graphical models (see [20]). The Wikipedia, for instance, gives a good coverage of these areas. Moreover, we expect familiarities with basic topic models such as Latent Dirichlet Allocation (LDA) (see [1, 21]) and common performance measure such as perplexity (see section 3.2.2.2), as we only provide short explanations for these.\nWhile non-Bayesian topic models such as Probabilistic Latent Semantic Analysis (PLSA) [22] do exist, the focus of the topic modelling research field had been mostly shifted to Bayesian topic models since the advent of Latent Dirichlet Allocation (LDA) given their superior theoretical basis and good performance. Therefore, we restrict the scope of our thesis to Bayesian topic models only, and use LDA as a starting point for discussion."}, {"heading": "2.2 Notation", "text": "Unless otherwise explicitly stated, all random variables are discrete variables, all variables are non-negative real numbers or integers, and all probability distributions are discrete distributions. All Bayesian graphical models such as figure 2.3.1 use plate notation, where each rectangle represent repeating entities, number of repetition and range of repeating variables are specified in the bottom right corner."}, {"heading": "2.3 Existing Bayesian Topic Models", "text": "In this section I will give a brief overview of some existing models which are relevant to my research. Technical details such as model derivation, technical definitions, effects of hyper-parameter, predictive probability, and inference algorithm will be left to the next section.\nA mathematical definition and a conceptional description are given on the following models, ordered by their simplicity and the dates they are created:\n\u2022 Latent Dirichlet Allocation (LDA) [1]\n\u2022 Pitman-Yor Topic Modelling (PYTM) [23]\n\u2022 Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23]\n9 \u2022 Differential Topic Modelling using Shadow Poisson Dirichlet Process (SPDP) [2]\nMost topic models are unigram (i.e 1-gram) models - they only keep the number of occurrence of words appeared in documents, and completely ignore the order of words. In other areas of linguistic research, N-gram models are more popular than unigram models. They consider every N consecutive words as a block while ignore the order of blocks.\nThe reason that n-gram models are not commonly used in topic model research is, although n-gram topic models contains richer semantic information, it is often very difficult to find a large enough collection of documents that contain multiple occurrence of each n-gram block, hence posing severe difficulty to allow the topic model to extract any statistically meaningful information. On the other hand, most topic modelling algorithms have running time complexity growing linearly or quadratically with the size of vocabulary. As n-gram models extend the vocabulary size to n-th power of original vocabulary size, the efficiency of topic modelling with n-gram becomes an efficiency issue.\nAll models introduced in this section are unigram models. Semantic structures contained in words and sentence are completely ignored."}, {"heading": "2.3.1 Latent Dirichlet Allocation (LDA)", "text": "Latent Dirichlet Allocation (LDA) [1] is one of the most popular models used in topic modelling because of its simplicity. The graphical model is shown in Figure 2.3.1. The generation process is as follows.\n~\u03c6k \u223c Dirichlet(~\u03b2) \u2200k = 1...K ~\u03b8m \u223c Dirichlet(~\u03b1) \u2200m = 1...M zm,l \u223cMulti(~\u03b8m) \u2200m = 1...M, l = 1...Lm wm,l \u223cMulti(~\u03c6zm,l) \u2200m = 1...M, l = 1...Lm\nHere M be the total number of documents, Lm is total number of words in document m, subscript m denotes a document index, k denotes a topic index. The roles of other variables are:\n\u2022 ~\u03b8m : Topic proportion, a vector used as parameter of multinomial distribution for document m. It determines the likelihood of each topic in document m.\n\u2022 ~\u03b1: A constant hyper-parameter vector of dimension equal to number of topics. It serves as a prior for the Dirichlet distribution to determine the likelihood of topic proportions to be generated for document m.\n10\n~\u03b8m\nMulti\nzm,l\nMulti ~\u03c6k\nwm,l\nDir\nDir\n~\u03b1\n~\u03b2\nl \u2208 [1, Lm]\nm \u2208 [1,M ]\nk \u2208 [1,K]\nFigure 2.3.1: LDA graphical model\n\u2022 ~\u03c6k: Mixture component, a vector used as parameter of multinomial distribution for topic t. It determines the likelihood of each word in topic k.\n\u2022 ~\u03b2: A constant hyper-parameter vector of dimension equal to size of vocabulary, similar to ~\u03b1. It serves as a prior for the Dirichlet distribution to determine the likelihood of mixture components to be generated for topic.\nIntuitively, the process is as follows: First, ~\u03b8m and ~\u03c6k are randomly generated for document m and topic k according to parameters ~\u03b1 and ~\u03b2. Then, for each word index (range from 1 to Lm) in each document m, a topic zm,l = k is drawn from a bag of topics (represented as integers from 1 to K, where K is the number of topics), where the probability of each topic corresponds to value of each dimension in ~\u03b8m. After the topic k is drawn, the word wm,l is drawn from the vocabulary, where the probability for each word corresponds to each dimension in ~\u03c6k."}, {"heading": "2.3.2 Pitman-Yor Topic Modelling (PYTM)", "text": "Pitman-Yor Topic Modelling (PYTM) [23] made a few improvements over LDA, hence achieved significantly better performance when measured in perplexity. The PYTM assumes in each document the words are sequentially drawn from a distribution generated by a Poisson Dirichlet Process (PDP)[24] (also known as Pitman-Yor Process[25]). Topics\n11\nFigure 2.3.2: Word frequency (y-axis in log scale) v.s Word frequency ranking (x-axis in log scale) in Wikipedia (extracted from Wikipedia [3] under LGPL license)\n12\nare not drawn before words - instead, the generation processes of a word and a topic are mixed together. The PDP ensures words generated in each document follow the properties of a power-law, which states words already appearing before are exponentially more likely to appear again. The justification is based on Zipf\u2019s law developed in linguistics research, which states that in large text corpora, the number of times a word appear in the corpora is approximately inversely proportional to its rank. Figure 2.3.2 shows the plot of word frequency in Wikipedia as of 27 November, 2008 [3] . Each line represents a trend line fit to Zipf\u2019s law with some parameters.\nBecause the clustering effect in the PDP different from LDA, the number of unique words and rare words generated by PDP in each document is significantly higher than the results from LDA. In contrast LDA only generates i.i.d words from a multinomial distribution, hence is unable to capture the properties of a power-law [23].\nFigure 2.3.3 shows the graphical model of PYTM. Figure 2.3.3 shows a more precise illustration that breaks down each PDP generation step. The generation process of PYTM is given as follows. In LDA we generate zm,l and wm,l according to H(z, w|~\u03b8m,\u03a6), which is a bivariate distribution equivalent to z \u223c ~\u03b8m, w \u223c ~\u03c6z, and \u03a6 is the matrix representation of the collection of ~\u03c6k for all k = 1...K. In PYTM we modify the process and take a variant of H(z, w|~\u03b8m,\u03a6), then sample zm,l and wm,l from the variant.\n~\u03c6k \u223c Dirichlet(~\u03b2) \u2200k = 1...K ~\u03b8m \u223c Dirichlet(~\u03b1) \u2200m = 1...M Gm \u223c PDP (\u03b3, d,H(\u00b7, \u00b7|~\u03b8m,\u03a6)) \u2200m = 1...M\n(zm,l, wm,l) \u223c Gm \u2200m = 1...M, l = 1...Lm\nNote that as \u03b3 approaches infinity, Gm approaches H, so the process collapsed into regular LDA.\nBelow is another way of representing this process, similar to what is introduced in [23]. It breaks down the sampling part to Chinese Restaurant Process. For each word wm,l with index l in each document m:\n13\n~\u03b8m\nPY (\u03b3, d,H(\u00b7, \u00b7|~\u03b8m,\u03a6)) ~\u03c6k\nGm\nzm,l\nwm,l\nDir\nDir\n~\u03b1\n~\u03b2\nl \u2208 [1, Lm]\nm \u2208 [1,M ]\nk \u2208 [1,K]\nFigure 2.3.3: PYTM graphical model\n~\u03b8m\nMulti\nxm,l zm,j\nTable-Lookup Discrete ~\u03c6k\nwm,l vm,j\nDir\nDir\nPDP-Table-Draw\n\u03b3\nd\n~\u03b1\n~\u03b2\nl \u2208 [1, Lm] j \u2208 [1, Jm]\nm \u2208 [1,M ]\nk \u2208 [1,K]\nFigure 2.3.4: PYTM graphical model breakdown\n14\nrm,l \u223c Bernoulli( \u03b3 + dJm \u03b3 + l )\nxm,l = { xm,sfor some 1 \u2264 s \u2264 l \u2212 1, each with probability nm,s\u2212\u03b3d+l\u22121 (rm,l = 0) Jm + 1 (rm,l = 1)\nzm,l =\n{ zm,s (rm,l = 0)\nDraw from Multi(~\u03b8m) (rm,l = 1)\nvm,xm,l =\n{ unchanged (rm,l = 0)\nDraw from Discrete(~\u03c6zm,l) (rm,l = 1)\nwm,l = vm,xm,l\nwhere: \u03b3 is the concentration parameter of PDP, d is the discount parameter of PDP. K is total number of topics, M is total number of documents. Jm is number of distinct words so far in document m. Through out the process there are two types of generated words: either (1) when rm,l = 0, reuse a topic k or word already generated from ~\u03b8m or ~\u03c6k before, or (2) when rm,l = 1, a new topic zm,l and a new word wm,l is generated from ~\u03b8m and ~\u03c6k directly (which can be same as a topic or word generated before, or completely new). xm,l and vm,xm,l are defined as: If we put type (2) words into a sequence {vxm,l}m for each document m, and label them consecutively, xm,l is then the label for each word wm,l. Since type (1) word only reuses word that already appeared before, they share the same label with same type (2) word. New topic zm,l is only generated for type (2) word.\nThe generation process of PYTM can also be described by a much simpler Chinese Restaurant Process (CRP) analogy of Poisson Dirichlet Process. We will give an overview of Poisson Dirichlet Process and the CRP analogy in the next section."}, {"heading": "2.3.3 Hierarchical Pitman-Yor Topic Modelling (HPYTM)", "text": "The Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23] made one extension to PYTM by assuming the power-law phenomenon not only exists in each document but also within each topic. PDP word generation is now document-topic specific instead of only document-specific as it is in PYTM. In this setup, new words (type (2) words) are no longer drawn from Discrete(~\u03c6zm,l), instead they are drawn from a distribution generated by PDP for a specific topic. The Hierarchical Bayesian Language Model [26] replaces some parts of PYTM still inheriting some features of LDA with a more complicated structure, as illustrated in Figure 2.3.3 and 2.3.6. Note ~\u03b2 and ~\u03c6 in PYTM have been replaced by a two-tier hierarchical model.\n15\n~\u03b8m \u03b3k dk \u03c60\nPY (\u03b3, d,H(\u00b7, \u00b7|~\u03b8m,\u03a6)) ~\u03c6k\nGm\nzm,l\nwm,l\nDir\nPDP\nGamma Beta PDP\n~\u03b1 a\u03b3 b\u03b3 ad bd \u03b30 d0 U\nl \u2208 [1, Lm]\nm \u2208 [1,M ]\nk \u2208 [1,K]\nFigure 2.3.5: HPYTM graphical model\nThe break-down generation process is same as the generation process in PYTM, except for vm,xm,l :\nvm,xm,l =\n{ unchanged (rm,l = 0)\nDraw from Discrete(~\u03c6zm,l) (rm,l = 1)\nAnd in addition:\ndk \u223c Beta(ad, bd) \u2200k = 1, ..., K \u03b3k \u223c Gamma(a\u03b3, b\u03b3) \u2200k = 1, ..., K ~\u03c6k \u223c PDP (\u03b3k, dk, ~\u03c60)\u2200k = 1, ..., K ~\u03c60 \u223c PDP (\u03b30, d0, U)\nWhere ad, bd, a\u03b3, b\u03b3, \u03b30, d0 are all hyper-parameters, U is a discrete uniform distribution (\u2200w : p(w) = 1/V ).\n16\n~\u03b8m\nMulti\nxm,l zm,j \u03b3k dk \u03c60\nTable-Lookup Discrete ~\u03c6k\nwm,l vm,j\nDir\nPDP\nGamma Beta PDPPDP-Table-Draw\n\u03b3\nd\na\u03b3 b\u03b3 ad bd \u03b30 d0 U\n~\u03b1\nl \u2208 [1, Lm] j \u2208 [1, Jm]\nm \u2208 [1,M ]\nk \u2208 [1,K]\nFigure 2.3.6: HPYTM graphical model breakdown"}, {"heading": "2.3.4 Differential Topic Modelling Using Shadow Poisson Dirich-", "text": "let Process\nThis differential model [2] addresses the problem of comparing multiple groups of documents. Differential topic models extend standard topic models by giving the model the ability to find similarities and differences in topics among multiple groups of documents. In this setup, the input documents are organized into multiple groups sharing the same vocabulary. Topics are shared across all groups but each group has their own representation of each topic. In addition, each group is allowed to have their unique topics.\nIn standard topic modelling, the sources of documents are not differentiated. In other words, all documents are assumed to be in the same group. This assumption simplifies the mathematical formulation of the topic model and the predictive probabilities, but with such assumption in place, topic models are unable to extract information on the variation in popularity of topics and words among multiple sources of documents. The differential information is important because it allows us to analyze subtle differences in opinions and perspectives across multiple collection of documents.\nHere is an example to illustrate the power of differential topic models: consider a situation where we need to analyze articles gathered from two media outlets, one from Israel and one from Palestine. Apparently, Israeli editors are more likely to use the word \u201cterrorism\u201d\n17\nto describe the Israeli-Palestinian conflict, because of some extreme measures used by some Palestine. In contrast, Palestinians editors are likely to use \u201caggression\u201d to describe this topic, because they see Israeli as invaders to their homeland. When standard topic models are applied to both groups of documents individually, there is no guarantee that the same topic can be extracted from both groups. When standard topic models are applied to the whole collection of two groups of documents, they are more likely to mix Israeli-Palestinian issues into one topic, hence unable to provide differential analysis. Differential topic models are able to find both the shared topics among both groups, and different descriptions to such topics should they exist.\nCombining the essence of all models introduced above and results from other mathematics research and topic modelling research, especially the theoretical results in [24], the improved PDP table-configuration sampler in [27], and the Hierarchical Dirichlet Process model [28], the Differential Topic Model with Shadow Poisson Dirichlet Process (SPDP) is born. This model outperforms many existing models in differential topic modelling context when the performance is measured in perplexity.\nAlthough the superiority of this model has already been demonstrated in experiments, [2] does not provide an in depth explanation of the intuition of the model and derivation of the model. Since the rest of the thesis is entirely based on this model, in the following discussions I give an step-by-step explanation of this model.\nSimilar to the structure in HPYTM, each word-topic distribution is assumed to be generated from a base distribution ~\u03c60k. Assume the total vocabulary size is V . Each group is attached with a V \u00d7 V transformation matrix P i that transforms the shared base distribution ~\u03c60k, represents the similarity between each pair of words, such that the sum of each row or column in P i is 1. As a consequence, different transformation matrices introduce different word correlations for each group, so when words are generated, each group produces slightly different words for a common topic. The graphical model is given in 2.3.7. The generation process is as follows. The technical details of this model are left to next section, Model Derivation and Gibbs Sampler.\n~\u03c60k \u223c Dirichlet(~\u03b2) \u2200k = 1, ..., K ~\u03c6ik \u223c PDP (\u03b3k, dk, P i~\u03c60k) \u2200k = 1, ..., K, \u2200i = 1, ..., I ~\u03b8di \u223c Dirichlet(~\u03b1i) \u2200i = 1, ..., I, \u2200d = 1, ..., Di zlid \u223c Discrete(~\u03b8di ) \u2200i = 1, ..., I, \u2200d = 1, ..., Di, \u2200l = 1, ..., Li,d wlid \u223c Discrete(~\u03c6izlid) \u2200i = 1, ..., I, \u2200d = 1, ..., Di, \u2200l = 1, ..., Li,d\n18\n~\u03b8i,d\nMulti\nzli,d\nDiscrete ~\u03c6ik\n~\u03c60k\nwli,d\nDir\nPDP Dir\n~\u03b1\n~\u03b2\n\u03b3k\ndk\nPi\nl \u2208 [1, Li,d]\nd \u2208 [1, Di]\nk \u2208 [1,K]\ni \u2208 [1, I]\nk \u2208 [1,K]\nFigure 2.3.7: SPDP graphical model\nWhere I is number of groups, P i is the transformation matrix for group i. Di is number of documents in group i. Li,d is document length of document d in group i.\nThe differential topic model SPDP is one particular representative of perhaps another hundred extensions of LDA. When combined with other algorithms and models, it has even more potential applications in practice, For instance, in many real world situations, group labels on collections of document are not accurately given. Many blogs and articles are shared around many websites on the Internet, without providing any reliable label of originality, category, or perspective. The majority of documents on the Internet (including social network messages such as tweets) are not tagged. Furthermore, tags do not always provide accurate information. Multiple documents sharing the same tag may belong to different categories or be written in different perspectives, hence do not necessarily belong to the same group. For example, a document tagged with \u201cmachine learning\u201d could be in \u201creinforcement learning\u201d, \u201ctopic modelling\u201d, or other categories; a document tagged with \u201cpolitics\u201d could be either \u201cleft\u201d or \u201cright\u201d depending on its perspective. It is desirable to have an algorithm that automatically classifies documents into different categories and perspectives, free from human influence and judgements. Because the differential topic model SPDP provide a fundamental framework based on groups, it is a very suitable\n19\ncandidate for this task. For example, a simple and naive solution is to iteratively use the topic probabilities and word probabilities across multiple groups generated by SPDP as features for machine learning algorithms such as Expectation Maximization (EM) to classify documents belonging to multiple unknown groups.\n20"}, {"heading": "2.4 Model Derivation and Gibbs Sampler", "text": "All topic models mentioned in last section share some similarities in their derivations and inference processes, because fundamentally all of them are extensions of LDA. The collapsed Gibbs sampler is most frequently chosen by authors of above models to make inference on the multivariate latent variables. Although different models have different latent variables, for simplicity, we denote all of them by one latent parameter ~\u03b8 \u2208 \u0398latent. The goal is to make inference on the latent parameter (which include topic information) by estimating the Bayesian posterior of latent parameters (for example in LDA, word distributions \u03c6k, and topic distributions \u03b8m) given some data, according to Bayes\u2019 rule:\np(~\u03b8|data) = p( ~\u03b8, data)\np(data)\nSimply applying this formula often gives an intractable probability distribution for p(~\u03b8|data). To see this, consider the simplest LDA model. To infer the latent topic assignment variable ~z:\np(~z|~w) = p(~z, ~w) p(~w) = \u220fL l=1 p(zl, wl)\u220fL\nl=1 \u2211K k=1 p(zl = k, wl)\nThere are KL terms in the denominator, making the probability mathematically intractable.\nHowever, if we are given a large number of samples from this probability distribution, we may find a number of ways to estimate the latent variables ~\u03b8. Suppose we have samples ~x1, ..~xn from p(~\u03b8|data). The easiest way to estimate it is to simply calculate the average occurrence of each possible outcome:\np\u0302(~\u03b8|data) = 1 n n\u2211 i=1 \u03b4~\u03b8(~xi)\n21\nWhere ~xi are the observed samples. The collapsed Gibbs sampler is used for this computation. The collapsed Gibbs sampler is an iterative sampler that generate random samples of a joint multivariate probability distribution p(~x) given p(xi|~x\u2212i) is known. Here ~x\u2212i denotes ~x with element xi deleted. The collapsed Gibbs sampler requires a large number iterations for convergence. It starts with an arbitrary initial value for each xi, sample each element xi from p(xi|~x\u2212i) in each iteration, and update the value of xi as soon as it is sampled. The probability of samples ~x drawn in each iteration will almost surely converge to p(~\u03b8|data). The proof can be found in most advanced statistics textbooks, such as [29].\nIn our settings, usually the quantity p(\u03b8i|\u03b8\u2212i, data) can be easily deduced by computing:\np(\u03b8i|\u03b8\u2212i, data) = p(~\u03b8, data)\np(\u03b8\u2212i, data) (2.4.1)\nIn topic modelling this is often referred as the predictive probability. To make our method work, we need a neat way to compute the predictive probability which is derived from the the joint distribution of the latent parameter and the data. In the rest of this section, we will show the derivation process of joint distribution and predictive probability for most models we described in last section.\nFor most of these models, we have written a step by step derivation. For the basic LDA model, we only explain the important steps and provided references to existing publications, to help readers find more detailed explanation."}, {"heading": "2.4.1 Notation", "text": "Across this section, we use V to denote the size of the vocabulary, K to denote the total number of topics, M to denote total number of documents, nmk:m,k to denote number of times topic k observed in document m, where the first part of subscript is a label to distinguish it from other counting variables that may also be named with n, and nkw:k,w to denote number of times word w associated with topic k. We use nkw:k,. to denote the sum\nover dotted variables in the subscript, for example, nkw:k,. = \u2211V w=1 nkw:k,w, and similarly\nnmk:m,. = \u2211K k=1 nmk:m,k."}, {"heading": "2.4.2 Latent Dirichlet Allocation (LDA)", "text": "The joint distribution p(~\u03b8, data) of LDA can be represented as\n22\np(~w, ~z|~\u03b1, ~\u03b2) = p(~w|~z, ~\u03b2)p(~z|~\u03b1) (2.4.2)\nSince ~z is not dependent on ~\u03b2. The first term p(~w|~z, ~\u03b2) in 2.4.2 can be derived as:\np(~w|~z, ~\u03b2) = \u02c6 ~\u03c6 p(~w|~z, ~\u03c6)p(~\u03c6|~\u03b2)d~\u03c6\n= \u02c6 ~\u03c6 K\u220f k=1 1 \u2206(~\u03b2) V\u220f w=1 \u03c6 nk,w+\u03b2w\u22121 k,w d ~\u03c6\n= K\u220f k=1 \u2206(~\u03b2 + ~nk) \u2206(~\u03b2) (2.4.3)\nWhere nk,w is a counting variable representing number of terms in all documents that have been assigned to topic k and word w. \u2206(~x) is the Dirichlet delta function, the normalizing constant, as defined in [21]:\n\u2206(~x) =\n\u220fdim ~x k=1 \u0393(xk)\n\u0393( \u220fdim ~x\nk=1 (xk))\nSimilarly, the second term p(~z|~\u03b1) in 2.4.2 can be derived as:\np(~z|~\u03b1) = \u02c6 ~\u03b8 p(~z|, ~\u03b8)p(~\u03b8|~\u03b1)d~\u03b8\n= \u02c6 ~\u03b8 M\u220f d=1 1 \u2206(~\u03b1) K\u220f k=1 \u03b8 nm,k+\u03b1k\u22121 m,k d ~\u03b8\n= M\u220f k=1 \u2206(~\u03b1 + ~nm) \u2206(~\u03b2) (2.4.4)\nWhere nm,k is a counting variable representing number of terms in document m that have been assigned with topic k. Putting equations 2.4.22.4.32.4.4 together:\n23\np(~w, ~z|~\u03b1, ~\u03b2) = p(~w|~z, ~\u03b2)p(~z|~\u03b1)\n= K\u220f k=1 \u2206(~\u03b2 + ~nk) \u2206(~\u03b2) M\u220f k=1 \u2206(~\u03b1 + ~nm) \u2206(~\u03b2) (2.4.5)\nSubstitutes 2.4.5 into 2.4.1 :\np(zi = k|~w, ~z\u2212i, ~\u03b1, ~\u03b2) = p(~w, ~z)\np(~w, ~z\u2212i)\n= p(~w|~z)\np(~w\u2212i, ~z\u2212i)\np(~z)\np(wi)\n\u221d \u2206( ~\u03b2 + ~nk)\n\u2206(~\u03b2 + ~nk,\u2212i)\n\u2206(~\u03b1 + ~nm)\n\u2206(~\u03b1 + ~nm,\u2212i) \u221d \u0393(nk,w + \u03b2w)\u0393( \u2211V w=1(nk,w,\u2212i + \u03b2w))\n\u0393(nk,w,\u2212i + \u03b2w)\u0393( \u2211V w=1(nk,w + \u03b2w))\n\u0393(nm,k + \u03b1k)\u0393( \u2211K k=1(nm,k,\u2212i + \u03b1k))\n\u0393(nm,k,\u2212i + \u03b1k)\u0393( \u2211K k=1(nm,k + \u03b1k))\n\u221d nk,w,\u2212i + \u03b2w\u2211V w=1(nk,w,\u2212i + \u03b2w) nm,k,\u2212i + \u03b1k\u2211K k=1(nm,k,\u2212i + \u03b1k)\u2212 1\nWhere a variable with subscript \u2212i denotes the value of such variable with element i (word i) is removed. Above formula gives the proportional predictive probability for the collapsed Gibbs sampling. For more detailed explanation on LDA, readers should refer to [1]."}, {"heading": "2.4.3 Pitman-Yor Topic Modelling (PYTM)", "text": ""}, {"heading": "2.4.3.1 Poisson Dirichlet Process", "text": "We first give an overview of the Poisson Dirichlet Process (PDP) as it is the foundation of PYTM. As mentioned in the last section, PDP generates a probability distribution from a base distribution H(.), concentration parameter \u03b3, and discount parameter d. The process is denoted by PDP (\u03b3, d,H(.)). The Chinese Restaurant analogy is as follows: In a strange Chinese restaurant which has infinite number of tables, each table serves only one dish, and only when at least one customer is sitting on that table. Waiters will arrange each incoming customer to either sit in a table served with dish j, share it among other people who are already sitting there, or lead the customer to an empty table and\n24\nimmediately serve a dish. Waiters keep a record of the number of distinct dishes 1, ..., J served to customers, the number of customers served with dish j, denoted by nj, and the total number of customers, denoted by N . They use the following procedure to make seating arrangements for each incoming customer:\n\u2022 Take the customer to an empty table with probability \u03b3+dJ \u03b3+N , and serve some dish j\ndrawn from H(.)\n\u2022 Otherwise, take the customer to some table already serving dish j, with probability nj\u2212d \u03b3+N in total for all these tables\nSamples drawn from the distribution generated by the PDP can be understood as the dishes served to each customer. In the PYTM model (Figure 2.3.3 ), a restaurant is created for each document. Each word in the document is a customer coming to the restaurant. Each different word in the vocabulary is an unique type of dish. xm,l records which table the customer sat in. vm,s records the dish being served at table s in restaurant m. zm,s records the topic associated with table s in restaurant m. The observed words wm,l = vm,xm,l are the samples drawn from the distribution generated from PDP.\nThe formal definition of the Poisson Dirichlet Process is a sequence of draws from the base distribution H(.) coupled with probability weighting vector ~p drawn from Poisson Dirichlet Distribution as stated in [30]. A detailed Bayesian analysis of Poisson Dirichlet Process is given in [24] by Buntine and Hutter. Because their works are highly technical, far above the level of this thesis, and they are not directly related to topic modelling, we only use only some of their results and only show the derivation when necessary.\nFollowing the analogy we can immediately get the predictive probability of which dish would be served to an incoming customer:\np(xi = j|x1, x2, ..., xi\u22121, \u03b3, d,H(.)) = nj \u2212 d \u03b3 +N + \u03b3 + dJ \u03b3 +N H(j)"}, {"heading": "2.4.3.2 Predictive Probability and Inference", "text": "Predictive probability can be derived by removing a word, similar to LDA. Here we need both topic predictive probability and word predictive probability to proceed with inference, because when we remove a word, we add it back later, and we need to reconsider the sitting arrangement when add it back. Luckily the word predictive probability is directly given by CRP analogy because in above definition each word drawn is dependent on all previous words drawn :\np(wm,l = j|W ,Z,X) = nmw:m,j \u2212 d \u03b3 + nmw:m. + \u03b3 + dJm \u03b3 + nmw:m. K\u2211 k=1 nm,k + \u03b1m nm + \u03b1m:. nkw:k,j + \u03b2k nkw:k,. + \u03b2k:. (2.4.6)\n25\nWhere nmj:m,j is number of times word w appeared in document m (not including wm,l), nmw:m.is number of words observed so far in document m without wm,l, W ,Z,X are words, topics, sitting arrangements not including wm,l or anything associated with wm,l. H(.) is replaced by the summation term, which is borrowed from LDA word predictive probability as for this part they share the same word generation procedure (see figure 2.3.4 and the breakdown generation illustration.)\nSimilarly, the topic predictive probability is given by\np(zm,l = k|W ,Z,X, wm,l = j, xm,l = s) = nmk:m,k + \u03b1m nmk:m,. + \u03b1m:. nkw:k,j + \u03b2k nkw:k,. + \u03b2k:.\nas derived in LDA."}, {"heading": "2.4.4 Hierarchical Pitman-Yor Topic Modelling (HPYTM)", "text": "The derivation process is almost same as described in PYTM, except probabilities are computed recursively. We will skip this section as the detail is not particularly related to SPDP. Readers should refer to [23] if they are interested in the details."}, {"heading": "2.4.5 Shared Topic Modelling Using Shadow Poisson Dirichlet", "text": "Process\nAs mentioned in the last section the Shadow Poisson Dirichlet Process (SPDP) is in fact a Poisson Dirichlet Process coupled with linearly transformed base measure. One important property we used to derive the predictive probability in LDA is Dirichlet distribution is conjugate to Discrete (categorical, or multinomial) distribution. The same property is used in PYTM and its extension HPYTM as they use LDA as a foundation. In this model the same method does not apply because the transformed base measure is no longer conjugate to a Discrete (categorical, or multinomial) distribution.\nTo overcome this, first we introduce auxiliary variable ti,k,w, which we refer as multiplicity, represents number of tables served with dish w in restaurant i, k (group i, topic k). It is shown by Corollary 17 in [24] that in one \u201crestaurant\u201d:\np(~w,~t|\u03b3, d,H(.)) = (\u03b3|d)t. (d)n. J\u220f j=1 (H(w?j ) tjS nj tj ,\u03b3)\n26\nWhere J is the number distinct dishes, ~w is dish served to each customer and ~t is the multiplicity of each dish. (w?1, ..., w ? j ) is the sequence of distinct dishes. t. is the sum of multiplicities, equivalent to total number of non-empty tables. nj is number of customers having dish j, and n. is equivalent to total number of customers. (x|y)N and (x)N are Pochhammer symbol, where (x|y)N = \u220fN\u22121 n=0 (x + ny), and (x)N = (x|1)N . SNM,a is a generalized Sterling number, given by linear recursion SN+1M,a = S N M\u22121,a+(N\u2212Ma)SNM,a and SNM,a = 0 for M > N , S N 0,a = \u03b4N,0. Both generalized Sterling numbers and Pochhammer symbols can be computed and cached efficiently before the Gibbs sampling process starts.\nIn our settings H(w?j ) is replaced by the probability \u03c6 0 v. After transformation the base distribution becomes P i~\u03c60, and \u03c60w = \u2211 v pw,v\u03c6 0 v where pw,v denotes element of the matrix. Therefore:\np(~w,~t|\u03b3, d, ~\u03c60) = (\u03b3|d)t. (d)n. J\u220f j=1 S nj tj ,\u03b3( \u2211 v pw,v\u03c6 0 v) tj (2.4.7)\nIt is clear the summation term inside the product has to be simplified. Chen et al. introduced two solutions for this problem: blocked Gibbs sampling, and hybrid Gibbs sampling with variational method.\nThe hybrid Gibbs with variational method approximates the above equation by deriving an inequality for above equation with ~\u03c6 integrated out, then introduces another variable qw,v = pw,v\u03c60v\u2211 v\u2032 pw,v\u2032\u03c6 0 v\u2032 that can be substituted to the lower bound. By using Jensen\u2019s inequality and a Lagrange multiplier, it can be shown that qw,v maximize previously derived inequality after substitution. As the summation term is simplified, one then derive the sampling predictive probabilities as usual with equation 2.4.1. The latent variables such as word probability \u03c60k,v , can be computed by\n\u03c60k,v = e \u03c8(\u03b3v+\n\u2211 i \u2211 w q\ni k,w,vti,k,w)/ \u2211 v e\u03c8(\u03b3v+ \u2211 i \u2211 w q i k,w,vti,k,w) (2.4.8)\nwhere \u03c8 is the digamma function. However, experiements have shown this approximation is not very accurate, as error accumulates the performance of the algorithm degrades significantly. Therefore in the rest of this section we will be concentrating on the first method: blocked Gibbs sampling.\n27\nIn equation 2.4.7 suppose for each word w we have another auxilliary variable ~v which has dimension tw, the multiplicity of the word in one PDP process. We need ~v to separate the power term ( \u2211 v pw,v\u03c6 0 v) tj into this form ( \u2211 v1 pw,v1\u03c6 0 v)( \u2211 v2 pw,v2\u03c6 0 v2 )...( \u2211 vtw pw,vtw\u03c6 0 vtw\n), such that the terms inside each bracket is dependent on vi and its effect is marginalized out in 2.4.7. If we compute the joint probability as in equation 2.4.7 with some ~v we will get:\np(~w,~t, ~v|\u03b3, d, ~\u03c60) = (\u03b3|d)t. (d)n. J\u220f j=1 S nj tj ,\u03b3 tw\u220f s=1 pw,vs\u03c6 0 vs (2.4.9)\nWhich is much simpler than 2.4.7, simple enough to be efficiently computed in logarithm space.\nBecause of the dimensionality difference between ~w and ~v, to make use of the auxilliary variable ~v we need another auxilliary variable ~r called table indicator that has same dimension as ~w , constructed as follows: for each word (customer) wi,d,l we set ri,d,l = 1 if the word has created a new table (the customer is arranged to an empty table and served a new dish), otherwise we set ri,d,l = 0. For every word wi,d,l such that ri,d,l = 1 we associate the word with vi,k,w,t where t is the table index of the word. ~r coupled with ~t provides more information than ~t alone because ~r in a specific seating configuration to each word in the document and ~t disregards the information of which word is creator\nof the table. For each table configuration specified by ~t, there are \u220fV\nv=1 ( nv tv ) different\nconfigurations of ~r, thus\np(~w,~t|\u03b3, d, ~\u03c60) = V\u220f v=1 ( nv tv ) p(~r,~t|\u03b3, d, ~\u03c60) (2.4.10)\nNow we are ready to deduce a joint likelihood of these variables, which afterwards can be easily transformed into a predictive probability using 2.4.1 :\np(W,Z,V,R|~\u03b3, ~d, ~\u03b11:I , ~\u03b2,P) = \u02c6\n\u03a6\np(W,Z,V,R|~\u03c601:K , ~\u03b3, ~d, ~\u03b11:I ,P)p(~\u03c601:K |~\u03b2)d\u03a6\n= p(Z|~\u03b11:I) \u02c6\n\u03a6\np(W,V,R|Z, ~\u03c601:K , ~\u03b3, ~d,P)p(~\u03c601:K |~\u03b2)d\u03a6\n(2.4.11)\nWhere \u03a6 is the collection of ~\u03c601:K . Let \u0398 be the collection of ~\u03b81:I,1:D, then\n28\np(Z|~\u03b11:I) = \u02c6\n\u0398\np(Z|\u0398)p(\u0398|~\u03b11:I)d\u0398\n= \u02c6 \u0398 I\u220f i=1 D\u220f d=1\n1\nBetaK(~\u03b1i) K\u220f k=1 \u03b8 nidk:i,d,k i,d,k \u03b8 \u03b1i,k\u22121 i,d,k d\u0398\n= I\u220f i=1 D\u220f d=1 BetaK(~\u03b1i + ~nidk:i,d) BetaK(~\u03b1i)\nWhere ~nidk:i,d = (nidk:i,d,1, nidk:i,d,2, ..., nidk:i,d,K). Substitute equation 2.4.9 and 2.4.10 into the terms inside the integral in 2.4.11:\np(W,V,R|Z, ~\u03c601:K , ~\u03b3, ~d,P) = I\u220f i=1 K\u220f k=1\n(\u03b3|d)tikw:i,k,. (d)nikw:i,k,.( V\u220f\nw=1\n( nw tw )\u22121 S nikj:i,k,j tikw:i,k,j ,\u03b3k tikw:i,k,w\u220f s=1 pi,w,vs\u03c6 0 k,vs )\np(~\u03c601:K |~\u03b2) = K\u220f k=1\n1\nBeta(~\u03b2) V\u220f w=1 (\u03c60k,w) \u03b2v\u22121 (2.4.12)\nWith respect to \u03a6 the integral in 2.4.11 can be easily evaluated as a multinomial probability density function. To simplify the result more, we introduce an auxilliar statistic qi,k,w,v = \u2211tikw:i,k,w t=1 1vi,k,w,t=v , the number of tables associated (as defined by vikwt) with\none particular word v in a PDP process. By doing so, \u220ftikw:i,k,w\ns=1 pi,w,vs\u03c6 0 k,vs can be neatly rewritten as \u220fV\nv=1(pw,v\u03c6 0 k,v) qi,k,w,v , thus:\n29\n\u02c6 \u03a6 p(W,V,R|Z, ~\u03c601:K , ~\u03b3, ~d,P)p(~\u03c601:K |~\u03b2)d\u03a6\n= \u02c6 \u03a6 ( I\u220f i=1 K\u220f k=1 (\u03b3|d)tikw:i,k,. (d)nikw:i,k,. V\u220f w=1 ( nw tw )\u22121 S nikj:i,k,j tikw:i,k,j ,\u03b3k tikw:i,k,w\u220f s=1 pi,w,vs\u03c6 0 vs)\nK\u220f k=1\n1\nBeta(~\u03b2) V\u220f w=1 (\u03c60k,w) \u03b2w\u22121d\u03a6 (2.4.13)\n= \u02c6 \u03a6 K\u220f k=1\n1 Beta(~\u03b2) ( I\u220f i=1 (\u03b3|d)tikw:i,k,. (d)nikw:i,k,. V\u220f w=1 ( nw tw )\u22121 S nikj:i,k,j tikw:i,k,j ,\u03b3k\nV\u220f v=1 (\u03c60k,v) qi,k,w,v+\u03b2w\u22121(pw,v) qi,k,w,v)d\u03a6 (2.4.14)\n= K\u220f k=1\n1\nBeta(~\u03b2) ( ( I\u220f i=1 (\u03b3|d)tikw:i,k,. (d)nikw:i,k,. V\u220f w=1 ( nw tw )\u22121 S nikj:i,k,j tikw:i,k,j ,\u03b3k ) ( \u02c6\n\u03a6 I\u220f i=1 V\u220f w=1 V\u220f v=1 (\u03c60k,v) qi,k,w,v+\u03b2w\u22121(pw,v) qi,k,w,v)d\u03a6) ) (2.4.15)\nEvaluate the integral, we get\nK\u220f k=1 \u02c6 \u03a6 I\u220f i=1 V\u220f w=1 V\u220f v=1 (\u03c60k,v) qi,k,w,v+\u03b2w\u22121(pw,v) qi,k,w,v)d\u03a6\n=( I\u220f i=1 V\u220f w=1 V\u220f v=1 (pw,v) \u2211K k=1 qi,k,w,v)( K\u220f k=1 \u220fI i=1BetaV ( ~\u03b2 + \u2211 i \u2211 w ~qi,k,w) BetaV (~\u03b2) ) (2.4.16)\nWhere in 2.4.13 we simply substituted values from equation 2.4.12. In 2.4.14 we re-wrote the terms in qikwv. In 2.4.15 we rearranged the equation. In 2.4.16 we took out pw,v and evaluated the integral as multinomial probability density function.\nFinally, put all terms together:\n30\np(W,Z,V,R|~\u03b3, ~d, ~\u03b11:I , ~\u03b2,P)\n= I\u220f i=1 D\u220f d=1 BetaK(~\u03b1i + ~nidk:i,d)\nBetaK(~\u03b1i)( K\u220f k=1 ( I\u220f i=1 (\u03b3|d)tikw:i,k,. (d)nikw:i,k,. V\u220f w=1 ( nw tw )\u22121 S nikj:i,k,j tikw:i,k,j ,\u03b3k ) ) ( I\u220f\ni=1 V\u220f w=1 V\u220f v=1 (pw,v) \u2211K k=1 qi,k,w,v)( K\u220f k=1 \u220fI i=1BetaV ( ~\u03b2 + \u2211 i \u2211 w ~qi,k,w) BetaV (~\u03b2) )\nAnd simplify a bit:\np(W,Z,V,R|~\u03b3, ~d, ~\u03b11:I , ~\u03b2,P)\n=( I\u220f i=1 V\u220f w=1 V\u220f v=1 (pw,v) \u2211K k=1 qi,k,w,v)( I\u220f i=1 D\u220f d=1 BetaK(~\u03b1i + ~nidk:i,d) BetaK(~\u03b1i) )\n( K\u220f k=1 ( I\u220f i=1 (\u03b3|d)tikw:i,k,. (d)nikw:i,k,. BetaV (~\u03b2 + \u2211 i \u2211 w ~qi,k,w) ))\n( K\u220f k=1\n1\nBetaV (~\u03b2) I\u220f i=1 V\u220f w=1 ( nw tw )\u22121 S nikj:i,k,j tikw:i,k,j ,\u03b3k ) )\nAlthough the derivation of joint probability is a bit complicated and requires a significant amount of work, the inference steps are quite simple and efficient. In blocked Gibbs sampling, we sample multiple variables together in one step, one element each. In our settings the best choice is to sample one element each from z, r, v together: for each word w, if r = 0, we can simply ignore the value of v and compute the probability of (z = k, r = 0), otherwise we compute the probability of (z = k, r = 1, vw = v). Use the joint probability computed above and the formula 2.4.1:\np(wi,d,l = w, zi,d,l = k, ri,d,l = 0| the rest with word w removed)\n\u221d\u03b1i,k + nidk:i,d,k bk +mikw:i,k\u00b7 mikw:i,k,w \u2212 tikw:i,k,w + 1 mikw:i,k,w + 1\nS mikw:i,k,w+1 tikw:i,k,w,ak\nS mikw:i,k,w tikw:i,k,w,ak\n(2.4.17)\n31\nAlgorithm 2.1 SPDP Full Gibbs Sampling\n1: for all wi,d,l in all documents in all groups do 2: k = zi,d,l, w = wi,d,l 3: Sample ri,d,l \u223c Bernoulli( tikw:i,k,wmikw:i,k,w ) 4: Decrement nidk:i,d,k, mikw:i,k,w 5: if ri,d,l \u2261 1 then 6: Decrement ti,k,w 7: Sample t \u223c Uniform(1, ..., tikw:i,k,w + 1) 8: Remove t-th element from linked list ~vikw:i,k,w 9: Decrement qi,k,w,v\n10: end if 11: for each topic k in 1...K do 12: Compute following proportionalities for each v in 1...V: 13: zi,d,l = k, ri,d,l = 0:\n14: \u03b1i,k+nidk:i,d,k bk+mikw:i,k\u00b7 mikw:i,k,w\u2212tikw:i,k,w+1 mikw:i,k,w+1\nS mikw:i,k,w+1\ntikw:i,k,w,ak S mikw:i,k,w tikw:i,k,w,ak\n15: zi,d,l = k, ri,d,l = 1, vi,k,w,t = v:\n16: pi,w,v(\u03b1i,k+nidk:i,d,k) bk+aktikw:i,k,\u00b7 bk+mikw:i,k\u00b7 tikw:i,k,w+1 mikw:i,k,w+1\n\u03b3v+ \u2211\ni \u2211 w qi,k,w,v\u2211\nv\u2032 \u03b3v\u2032+ \u2211 i \u2211 w tikw:i,k,w S mikw:i,k,w+1 tikw:i,k,w+1,ak\nS mikw:i,k,w tikw:i,k,w,ak\n17: end for 18: Jointly sample zi,d,l, ri,d,l, and vi,k,w,t according to proportionalities above. 19: Increment nidk:i,d,k, mikw:i,k,w 20: if ri,d,l \u2261 1 then 21: Increment ti,k,w and qi,k,w,v 22: Add v to linked list ~vikw:i,k,w 23: end if 24: end for\np(wi,d,l = w, zi,d,l = k, ri,d,l = 1, vi,k,w,t = v| the rest with word w removed)\n\u221dpi,w,v(\u03b1i,k + nidk:i,d,k) bk + aktikw:i,k,\u00b7 bk +mikw:i,k\u00b7 tikw:i,k,w + 1 mikw:i,k,w + 1\n\u03b3v + \u2211\ni \u2211 w qi,k,w,v\u2211\nv\u2032 \u03b3v\u2032 + \u2211 i \u2211 w tikw:i,k,w S mikw:i,k,w+1 tikw:i,k,w+1,ak S mikw:i,k,w tikw:i,k,w,ak\n(2.4.18)\nThe algorithm is illustrated in Algorithm 2.1.\nAfter convergence, the probability of a topic in a document can be estimated in the same way as in LDA, by taking expectation of the Dirichlet distribution:\n32\n\u03b8\u0303i,d,k = nidk:i,d,k + \u03b1k\u2211K\nk=1(nidk:i,d,k + \u03b1k) (2.4.19)\nAnd the probability of a topic can be estimated by taking the weighted sum of \u03b8i,d,k with document length as weight. The probability of a word in a topic can be esitmated by using the approximation of 2.4.8 with qikwv:i,k,w,vtikw:i,k,w replaced by the exact count qikwv:i,k,w,v, and the posterier of PDP:\n\u03c6\u03030k,v = \u03b3v +\n\u2211 i \u2211 w qikwv:i,k,w,v\u2211\nv \u03b3v + \u2211 v \u2211 i \u2211 w qikwv:i,k,w,v\n(2.4.20)\n\u03c6\u0303ik,w = mikw:i,k,w \u2212 aktikw:i,k,w\nbk +mikw:i,k,\u00b7 + aktikw:i,k,\u00b7 bk +mikw:i,k,\u00b7 ( \u2211 v piw,v\u03c6\u0303 0 k,v) (2.4.21)\n33\nChapter 3\nProblems and Solutions\n34"}, {"heading": "3.1 SPDP Computational Time Analysis", "text": ""}, {"heading": "3.1.1 Theoretical Running Time Analysis", "text": "Assume there are N words in total across all documents in all groups. Let K represents the total number of topics to be extracted, and V represents total number of words in the vocabulary. Furthermore, assume Gibbs sampler only converges after T iterations. Algorithm 2.1 shows for each word, all topics and all possible word-associations must be sampled. Although Stirling numbers in the algorithm need to be computed recursively, it is possible to cache almost all of them before the algorithm begins, allowing constant-time retrieval complexity. To summarize, the theoretical worst case complexity of algorithm 2.1 is \u0398(TNKV ). As the transofrmation matrices P i define all possible word-associations to be sampled in the nested inner loop of algorithm 2.1, it is a significant influential factor of the total running time. To reduce the worst case complexity, the transformation matrices P i need to be sparse. Once it is guaranteed that there is no more than a small constant number of non-zero elements per row or column, the average running time of 2.1 can be lowered to \u0398(TNK)."}, {"heading": "3.1.2 Practical Issues", "text": "In practice, a small collection of documents contains over 2000 documents in each group, consisting of over 500000 words, and a vocabulary size over 20000. Number of topics we expect to retrieve from such a collection is ranging between approximately 30 to 100, and the collapsed Gibbs sampler requires about 2000 iterations to converge. Multiplying these numbers together, we can see that to analyze such a collection of documents may require at least a constant times 1015 cycles if transformation matrix P is not sparse.\nSuppose the matrix is sparse and each row or column contains at most 20 non-zero entries. Compared to non-sparse transformation matrix, the number of operations required is now reduced to a constant times over 1012 cycles. The constant factor, which accounts for memory instructions and arithmetic computations at line 14,16 in algorithm 2.1, could be very large. Complicated data structure could make the issue even worse, e.g. wordassociation linked lists vikw, the sparse matrices P\ni , and the sparse counts qikwv. Accessing or modifying them require frequent pointer chasing operations and a tradeoff between memory usage and access efficiency. If these variables are implemented as full-size arrays instead, the amount of memory being allocated would exceed a constant times V 2. In practice, we observed that the amount of memory required by full array implementation could reach tenth of gigabytes on a small document collection. This magnitude of memory consumption is far more than what is offered in consumer-grade computers, severely limits the scalability of this algorithm.\n35\nIn the experiments, 2000 Gibbs iterations over a small collection of documents took several hours to complete, even when the transformation matrices P i are set to identity matrices."}, {"heading": "3.1.3 Parallelization Issues", "text": "As explained in Chapter 1, algorithm 2.1 is under the framework of the collapsed Gibbs sampler. Mathematically, the collapsed Gibbs sampler requires all words to be sampled sequencially. The state of the collapsed Gibbs sampler (in our case they are the vector p(W,Z| the rest) at each step when a word is sampled) form a Markov chain. In other words, the probability distribution of topic and word-association for the sample of the current word is dependent on the sample generated for previous words. Sampling multiple words in parallel with old state information breaks the rule of dependency. Consequently, it is not mathematically guaranteed that the collapsed Gibbs sampler running in parallel would converge."}, {"heading": "3.2 The Goal", "text": "We are looking for a solution that not only addresses the above issues, but also satisfies a set of properties. It is preferred that the solution has flexible memory usage, is able to process large collection of documents without being slowed down by memory access; the solution should take advantage of parallelism, so it can be made scalable enough to be executed on multiple devices. The solution does not necessarily need to be exact. A good approximation which sacrifice a bit of accuracy but greatly improve running speed is good enough for practical purposes. To keep the balances between accuracy and speed, we also need a set of performance and quality requirements to measure the fitness of my approximation. They could include:\n\u2022 Scalability\n\u2022 Perplexity: a measure of fitness of the model to the data\n\u2022 Topic quality and intepretability: how good the topics are in terms of human understanding\nIn addition, it will also be discussed why the popular PMI-score is not an appropriate measure for performance in our problem. Note also since the goal is not to measure the quality of topics but to measure how well the approximation matches the original (sequential) SPDP, perplexity is adequate enough."}, {"heading": "3.2.1 Scalability", "text": "As the algorithm is expected to process hundreds to millions of documents in real world applications, it becomes necessary to design a memory access architecture that allows effi-\n36\ncient access to both dynamic count varaibles t, n,m, q, v, and static constant variables P i (transformation matrices), Snm (Stirling numbers). When the amount of memory required to store these variables becomes too large, these variables can no longer be physically stored in main memory. As Algorithm 2.1 accesses the variable n and the transformation matrices P i in a linear, consecutive pattern, they are the easiest ones to cache. In comparison, there is no common pattern in how words and topics should appear across a document, causing more or less random access to count variables t,m, q, v.\nA good memory access architecture should divide variables into multiple regions and multiple levels of hierarchy, putting the current demand to priority, and take historical access frequency into consideration. In a distributed or parallel system, the hierarchy can be global memory, device memory, local memory, constant memory, cache, and buffers. Should the algorithm be executed in a distributed or parallel manner, redundant copies are unavoidably created across multiple levels in the memory hierarchy.\nSuppose the current memory consumption of the algorithm is M and we are running a distributed or parallel version of this algorithm on S devices. A naive architecture that creates a redundant copy on every device require O(MS) amount of memory. This is problematic because M could exceed the total amount of memory available on each device. A good architecture should limit the amount of redundancy, ideally making the total memory consumption O(M), independent to S, or indepedent except for a few variables that only consume a small amount of memory."}, {"heading": "3.2.2 Topic Performance Measure", "text": "Perplexity and pointwise mutual information score (PMI score) based on Wikipedia corpus[31] are the two most popular measures used in the research field to judge the quality of generated topic models.\n\u2022 PMI score based on Wikipedia corpus calculates pointwise relevancy of top ten words in each topic with respect to frequency of co-occurence between corresponding words in the Wikipedia corpus.\n\u2022 Perplexity measures how well the generated model fits the test data.\nA good algorithm is expected to give results with high PMI score and low perplexity. In practice, we found in many situations the PMI score could be unreliable, as we will soon illustrate. We believe it is also important to manually check the intepretability of generated topics, and whether these topics make sense to humans. This is a time comsuming process highly subjective to human knowledge and intepretation, but this guarantees we get a sensible result.\n37"}, {"heading": "3.2.2.1 PMI-Score Based on Wikipedia Corpus", "text": "Out of many evaluation methods proposed in [31], the PMI-score based on the Wikipedia corpus is the consistent best performer with respect to intrinsic semantic quality of learned topics.\nIn [2] Chen et al. defined PMI-score as PMIScore(~w) = 1 45 \u2211 i<j PMI(wi, wj), where i, j \u2208 {1, ..., 10}, PMI(wi, wj) = log P (wi,wj)P (wi)P (wj) , and P (wi, wj) is defined as word wi and wj appears in the same 10 word window, P (wi) and P (wj) are the probability of occurrence of word wi, wj respectively, estimated from word frequency as in the April 2011 Wikipedia dump. Overall PMI-score is computed by summing PMI-score of top ten words in each topic over all topics.\nHowever, the PMI-score measure in SPDP is susceptable to the influence of transformation matrices P i. The entries of transformation matrices P i determine word-association, make associated words more likely to appear in the same topic. It is possible to manipulate entries of P i to artificially increase the PMI-score. Furthermore, there are many situations that the PMI-score cannot accurately judge the semantic relevancy between two words. When the collection of documents is focused on a specific area, very often there are technical phrases such as \u201cmachine learning\u201d, \u201cgroup theory\u201d, \u201ctopic quality\u201d appear everywhere across the documents. These phrases don\u2019t make sense to people who are not specialized in machine learning. They may appear very infrequently in Wikipedia except only in very few technical articles. In contrast individual terms \u201cmachine\u201d \u201clearning\u201d \u201cgroup\u201d \u201ctheory\u201d \u201ctopic\u201d \u201cquality\u201d may appear very frequently across everywhere in Wikipedia. Should we analyze a machine learning journal and produce a topic consists words such as \u201cmachine learning topic modelling score function ...\u201d, we would obtain a low PMI-score, indicating poor topic quality, which is apparently not the case.\nTherefore, we choose not to use PMI-score to measure the quality of the result."}, {"heading": "3.2.2.2 Perplexity", "text": "Perplexity represents a scaled likelihood of test data given the parameters trained by the training data. When measuring the quality of a topic model, perplexity is usually defined as:\nP (W| the rest) = I\u220f i=1 Di\u220f d=1 p(~wi,d| the rest)\u2212 1 N\n= exp(\u2212 \u2211I i=1 \u2211D d=1 log(p(~wi,d| the rest))\u2211I\ni=1 \u2211D d=1Ni,d\n)\n38\nWhere W represents all words among all test documents in all groups. Test documents are documents held out in each group during training phase. Di is the number of test documents in group i, N is total number of words, Ni,d is total number of words in test document d group i, I is total number of groups, ~wd is the words in test document d of group i. Similar to the definition in [21], we define p(~wi,d| the rest) as:\np(~wi,d| the rest) = Ni,d\u220f n=1 K\u2211 k=1 p(wn = t|zi,d,l = k)p(zi,d,l = k)\n= Nd\u220f n=1 K\u2211 k=1 (\u03c6\u0303ik,w\u03b8\u0303i,d,k) ni,d,w\nWhere Ni,d is length of test document i, d, and ni,d,w is number of times word w appears in test document i, d. The variables \u03c6\u0303ik,w, \u03b8\u0303i,d,k are as defined in Equations 2.4.19 and 2.4.20."}, {"heading": "3.2.2.3 Topic Quality and Intepretability", "text": "Good performance in perplexity and PMI score is not sufficient to indicate a good topic model. If the produced topics do not deliver coherent intepretable information to humans, they would not be useful in practice, even when they perform well in both PMI-score and perplexity.\nFor instance, the topic \u201cbarack obama apple iphone ipad health insurance\u201d could have high PMI score because some word-pairs in this topic have frequent appearences in the Wikipedia corpus ,but apparently this is not a good topic because it is a mixture of three topics. Similarly, low perplexity only shows that the topic model has good ability to predict words in test documents, which does not neccesarily mean the topics are of good quality. The problem is best illustrated with an example in american polital blog document collection, where a topic model simply puts highest weight into the most frequent words such as \u201cobama republican democrats said just\u201d, or simply computes word frequency and evenly spread word across all topics. The perplexity of such topic model can be even lower than good topic models but they do not give any useful topic information.\nIn differential topic modelling, we are also interested in the coherency of topics shared among different groups. One important distinction of SPDP is its ability to find subtle differences in topics shared among multiple groups. Rather than relying on a single measure produced by an automated algorithm, this ability is better to be judged by a human as it involves understanding the background knowledge and complicated semantic analysis.\n39"}, {"heading": "3.3 The Innovation: Speeding Up SPDP", "text": "In this section, I propose a number of ways to improve the running speed of SPDP. First, I give a discussion of a basic trivial parallelization on line 11-17 of Algorithm 2.1. I show that when the number of topics and the number of effective entries in the transformation matrices P i are small, such parallelization may raise significant threadcreation and synchronization overhead, contrary to what people would expect in the first place. I address the challenge that exact collapsed Gibbs sampling require words to be sequencially sampled, hence any parallelization may incur considerable risk in convergence and loss of accuracy. I give a discussion on possible ways to overcome this obstacle, then propose a parallel approximation that not only can be justified to work in theory, but also can be implemented and tested to work reasonably well in practice.\nIn addition to this, I also propose a method to significantly increase the accuracy of the approximation algorithm. After this, I introduce some existing state-of-art distributed and parallel models for LDA. I argue that the same model can be applied to SPDP after modification. Finally, I combine all these ideas together, and create an all-in-one multiGPU distributed parallel approximation of SPDP. I show that this all-in-one model not only works in theory, but also address the practical issues illustrated previously in the thesis.\nThroughout this section, I stick to one major principle: we are designing things for real world application. For this reason, I make notes on how these proposals can be adopted into multiple architectures, namely the conventional CPU architecture, and the novel GPU architecture, as the title of the thesis suggests. Of these two architectures, the primary one I focus on is commercially available consumer-grade GPU, especially the multi-GPU distributive architecture. I also give a brief discussion about the performance of the algorithms on traditional CPU architecture wherever it is applicable. Details on these two different architectures at framework and hardware level are left to section 3.4."}, {"heading": "3.3.1 Distributed Parallelization Proposals", "text": ""}, {"heading": "3.3.1.1 Basic Parallelism: Over Topics and Word-Associations", "text": "Line 11-17 of algorithm 2.1 contain only independent operations. Assuming sparse transformation matrices P i are used, K \u00d7 S threads can be issued in parallel to compute sampling probabilities of (k, v) pairs, where S is the maximum number of non-zero entries in each row and column of P i, k represents a topic, and v represents a word-association. In most systems (especially on a CPU architecture), thread creation is a very expensive operation. The cost of creating a thread can be greater than the benefit of having multiple threads computing these probabilities in parallel.\n40\nTypically the cost of creating a thread ranges from 105 to 107 CPU cycles, depending on system architecture. Regardless, the value is far greater than the number of cycles required to compute the probabilities for each (k, v) pair, which is in between 102 to 103 cycles. Unless the value of K \u00d7 S is in the order 104 and the system has an architecture that supports fast hardware thread creation, it is not a good idea to create threads inside the loop around line 11-17 of Algorithm 2.1.\nThe next thing worth trying is to have all threads created before the execution of the algorithm, and pre-allocate the threads to (k, v) pairs. Multiple synchronization points are required at different parts of the algorithm. Parts of the algorithm that cannot be parallelized (everything except line 11-17) must be designated to a single thread, while the rest of the threads are kept idle as they wait for this single thread to finish.\nThe effectiveness of this method is not as simple as it looks like. How much does it cost to synchronize all threads at multiple synchronization points? How many threads should be created so the algorithm can achieve best performance? The first question cannot be answered without doing experiments. The answer to the latter question is simple if we only consider an architecture based on CPUs - simply create as many as the CPU could support at hardware level. However, as we look further into GPU architecture, it can be quite complicated to determine the number of threads that should be created at this level to achieve best performance."}, {"heading": "3.3.1.2 Parallel Word Sampling", "text": "We mentioned in previous sections that any parallel word sampling breaks the mathematical rule required by the collapsed Gibbs sampler. The risk of divergence and loss of accuracy are not avoidable, but with appropriate parallelization methods, the loss can be minimized.\nLet us look into the convergence process of the collapsed Gibbs sampling in SPDP. In each iteration, the topic of a particular word is sampled based on the current counts with respect to all words except the word itself. Frequent topics and words have their counts accummulated quickly, while uncommon topics and words also lose their counts gradually. As the algorithm progresses through many iterations, changes are slowed down and counts are stablized, until they converge to a stationary state. This behavior is similar to many chaotic systems, where convergence is not dependent on initial values, and the final stationary state is not sensitive to small external change in positions of each body during early phase of convergence. This suggests that a relatively small error in counts probably does not matter much to the overall accuracy. The convergence of the collapsed Gibbs sampler is determined by statistics on words, not the dynamic topic assignments, so a small error in topic assignments should not affect the overall convergence.\n41\nSuppose there are N parallel threads sampling words on P processors in parallel. If all count varaibles are stored in global memory, and all threads are allowed to modify them directly, a rather large amount of inconsistency would be introduced, causing high risk of divergence and significantly loss in accuracy.\n1. Counts relevant to all words being sampled concurrently in P processors are removed at the beginning of Algorithm 2.1, provide incorrect information to all threads at the beginning of execution. The large discrepancy in count variables can be critical since the probability formulas in Algorithm 2.1 are computed based on the assumption that only one word is removed.\n2. A second level of inconsistency is introduced while multiple threads modify the same count variable at the same time, especially if modification happens while some threads are still in the process of computing topic and word-association probabilities.\n3. If spinlocks are used to prevent conflict in accessing the same count variable, a huge amount of execution time has to be wasted on waiting for memory access. Spinlocks could also introduce a large variance on execution time, adding large extra cost to synchronization operations, which can be fatal if P gets large.\nOn the other hand if a local copy of count variables is created for each thread, and only get synchronized after each sampling thread finishes execution, the lack of between-thread communication would inevitably cause count variables in all threads to be delayed by N steps. This is a critical issue when N is large, the collapsed Gibbs sampler cannot make use of any information that is too old.\nTo minimize the error while keeping execution speed as fast as possible, and the degree of parallelism as high as possible, we need a mechanism to keep count variables in each thread up-to-date as much as possible, with minimum amount of conflict in memory access, and smallest amount of global synchronization pointw. The solution has to be sought separately for GPU and CPU architectures. On a CPU architecture, the number of processors per device (machine) is typically small. All processors share a single global memory with very low access latency. Each processor has a large amount of cache, advanced instruction sets, and great arithmetic processing power. On the GPU architecture, global synchronization is not possible. Memory resource is scarce, access is differentiated into multiple levels in hierarchy. The number of processors on a GPU is very large but each processor is much slower and much less advanced than a CPU processor. Massive parallelization is effective under a GPU architecture, but not as effective on a CPU architecture. Memory resource is not much of an issue under a CPU architecture, but it has to be carefully measured under a GPU architecture.\nThe solution we propose is to create a minimum local copy of count variables while keeping global count variables updated at the same time. The process is as follows:\n42\n1. At the start of the algorithm, a thread is created for each word in all documents.\n2. At the beginning of each thread a local copy of count variables relevant to this word is created.\n3. Following this, the global count variables relevant to this word is immediately updated as in line 2-10 in Algorithm 2.1.\n4. After a new sample is drawn, the global count variable is immediately updated so any subsequent reading to this global counter is up-to-date.\nSince local count variables are created at approximately the same time across all threads before computing probabilities, the second level of inconsistency we mentioned previously no longer exists. Spinlocks or semaphores on global variables are totally optional. Error may accumulate if multiple threads modify the same count variable at the same time, but the chance of having multiple threads in the same batch accessing the same word and same topic is very low. Inconsistency between count variables and auxillary variables (e.g. sum of some count variable in some dimension) can be manually corrected regularly during the iteration.\nAccess to each variable is not accompanied with a lock, therefore it is safe to have the number of threads N far larger than number of available processors P , and have threads scheduled in batches to fit P processors. Not only subsequent batches can take advantage of updates in previous batches, multiple batches can also be pipelined on both CPU and GPU to take advantage of SIMD features on the processor or multiprocessors. Most parallel programming frameworks and hardware architectures can do this implicitly, if such features are supported.\nWhen pipelining is used, the time between creating local copy of count variables and updating global variables should be reduced to minimal, so the update could propagate faster. Because the only major operations between these two steps are to comput probabilities for each topic and word-association pair, an additional level of parallelization can be combined with the solution to minimize the delay.\nSimilar to what we discussed in Section 3.3.1.1, for each word we assign a group of at most K\u00d7S threads to compute the probabilities of topic and word-association pairs. We call this a workgroup. Denote the number of threads we use to compute probabilities of topic and word-association pairs by Q, which divides K \u00d7 S. Since we only have P processors, the total number of workgroups being executed concurrently on hardware is no more than dP\nQ e. As we discussed, it is safe to schedule more threads than number\nof available processors, which also implies it is safe to schedule more workgroups than number of available processors. Therefore, we can let the total number of workgroups\n43\nequal to total number of words across all documents in all groups, and create them all at the beginning of each Gibbs iteration. As it will be soon revealed in Section 3.4, this structure perfectly fits GPU architecture."}, {"heading": "3.3.1.3 Parallelism With Improved Accuracy: Word Order Rearrangement", "text": "When multiple words in the same document are sampled in parallel, the risk of conflicting memory access to count variable n is higher. To reduce the risk, we can change the order of picking words when we sample them. Since our model is unigram, we are allowed to sample words among all documents in whatever order we wish. At one time, we only want one word from each document sampled in parallel. Since the number of processors is limited, we can simply rearrange the order of words before scheduling them to processors. Words in the same document should be kept apart as much as possible, so at one time each batch of workgroups being concurrently processed mostly consists of only words from different documents.\nThis can be done as follows.\n1. Before execution of the algorithm, create an empty array A to store the words.\n2. Create a variable wind \u2190 0 to store the current word index.\n3. Create Dmax \u2190 max{Li,d \u2200i = 1..I, d = 1..Di}, representing the length of the longest document. Li,d is the length of document d in group i.\n4. Until wind \u2265 Dmax, (randomly) pick up word at index wind from each document d in each group i which Li,d > wind, and append to array A. Increment wind each time all documents and all groups are traversed.\nThis technique is applicable to both CPU and GPU architecture as it does not change anything inside original algorithm."}, {"heading": "3.3.1.4 Traditional Distributed Model: Dividing Documents", "text": "Many distributed models have already been proposed for LDA. The most related and influencial ones are AD-LDA (Approximate Distributed LDA) proposed by Newman et al. in [32], and an improved version proposed by Smola and Narayanamurthy in [33].\nNewman\u2019s model distributes D documents and counts related to these D documents into P processors (not necessarily on the same machine), and only synchronize count variables after each iteration of the collapsed Gibbs sampling. Newman argued this model is a good approximation because the sampling process on multiple processors barely touch the same word and same topic, hence error accumulated in count variables is insignificant.\n44\nSmola and Narayanamurthy\u2019s made an improvement over Newman\u2019s distributed framework. They proposed an architecture which assigns a dedicated processor to update and synchronize count variables globally and locally for each thread, so remaining threads can keep on with sampling and never get interrupted. Since updates are more frequent, count variables are more up-to-date, hence further reducing the amount of accumulated error.\nSPDP has a much more complicated structure than LDA. Effectively, LDA only use two types of count variable: nkw and ndk. In contrast, SPDP has n, t,m, v, q. Both types of count variable nkw and ndk in LDA can be accurately re-generated directly from topic assignments, whereas in SPDP count variables t, v, q cannot be re-generated or verified. Newman\u2019s distributed framework can be applied to SPDP, but keep count variables t, v, q approximately correct can be a challenge.\nTo enhance the model\u2019s ability to keep count variables approximately correct, two levels of error correction should be implemented. One level is inside each processor (or GPU), and the other is a global correction done without massive parallelism. Neither Newman nor Smola and Narayanamurthy discussed GPU architecture in their work. Rather than dividing documents to multiple processors, they can be divided to multiple GPUs under a GPU architecture. Smola and Narayanamurthy\u2019s improvement over Newman\u2019s model can only be applied if communication between GPUs is possible. However, the only two GPU programming frameworks available, namely OpenCL and CUDA, do not provide any method to achieve this. The OpenCL Specification [34] also explicitly stated that the behavior of modifying the content of one memory object while another device is accessing the same memory object is undefined. However, through experiments we found at the hardware level NVIDIA and AMD both support implicit weak synchronization across multiple GPUs, though such synchronization is explicitly declared \u201cunspecified\u201d in their official guide."}, {"heading": "3.3.2 All-in-one: Putting Everything Together", "text": "The previous ideas can be combined into a single three-layer distributed parallel model. We call the processor which initiates Algorithm 2.1 as the host. The combined model is as follows:\n1. At the beginning of each iteration of collapsed Gibbs sampling, rearrange words to maximize the distance between words in the same document, as illustrated in 3.3.1.3.\n2. Randomly divide all documents in all groups to G devices (a device can either be a machine, a processor, or a GPU). Distribute re-arranged words to corresponding devices. By so doing, each device should have approximately an equal amount of load.\n45\n3. For each device g, dispatch count variables relevant to words and documents assigned to g to the device.\n4. Sample words in parallel on each device with the proposed two-level framework described in Section 3.3.1.3. Depending on values of K (the number of topics) and S (the number of non-zero entries per row and column in transformation matrix P i), degree of parallelism should be tailored to hardware architecture and specification.\nUnder the CPU architecture, the degree of parallelism is based on the number of processors available and the maximum number of threads supported on each device. The process is basically as same as what is described in 3.3.1.3.\nUnder the GPU architecture, hardware vendors specify the number of computing units available on device, as well as the optimal workgroup size. The process described in Section 3.3.1.3 needs to be adjusted to fit these specifications. When GPU memory is insufficient to store all count variables and words at once, they have to be scheduled in multiple waves. This could be both beneficial and problematic. Each time a wave of words is sampled, count variables and auxillary variables can be validated and corrected efficiently on host, before the next wave is dispatched for sampling. On the other hand memory transfer between global memory on host and GPU memory on device has much higher latency than internal memory operations. The optimal number of waves can only be found through experiments and trial and error. The underlying pricinple is that the time spent on memory transfer must be far less than time spent on GPU kernel execution.\nAfter a wave of words is sampled on the device, new samples and new count variables are read back to host. As words are sampled in parallel, errors are inevitably accumulated in count variables, and they must be validated and corrected before doing anything else. An mentioned before, count variables n,m can be re-generated from topic assignments. If synchronization between devices is done in-place, other count variables can be simply read back from an arbitrary device, and auxillary variables can be corrected based on values of count variables. Otherwise, other count variables have to be corrected by adding up the differences between their original values and the new values returned from all devices. However, as we discussed in the last section, count variables can change gradually at early stages of collapsed Gibbs sampling. Under the distributed framework, all devices operate independently. The sum of all differences is an exaggeration of the actual amount of change in one variable. We are yet to find a well-justified formula to update count variables based on the differences, and in practice we found this method does not work well. Instead, through the experiments we rely on the implicit synchronization of GPU memory between multiple devices sharing the same context on the same platform. Although the official hardware vendor programming guide does not guarantee any update to one variable is immeidately synchronized to all devices, in practice this method works very well. We do not really need updates to be reflected to all devices, as long as the delay can be tolerated.\n46\nWhen implicit between-device synchronization is not reliable enough, it is possible to create multiple duplicates of the training data to mitigate the loss of accuracy. Random errors with respect to one word in particular document can be averaged out through its duplications, effectively prevented them to be accumulated toward one direction after a few iterations. This technique is also useful when the amount of data available is too small to be distributed to multiple devices. Nonetheless, duplication of the training data should be considered as a last resort only when the quality of the result is highly inaccurate, and the number of duplicates should be far smaller than number of devices. In practice, I did not encounter any situation which the training data have to be duplicated to get sensable result, though I found by duplicating the training data once, the quality of the result can always be slightly improved.\nFigure 3.3.1 illustrates the big picture of my proposal.\n47\n48"}, {"heading": "3.4 Implementation", "text": "Previously the issues of SPDP, along with the solutions, have been discussed. A generalized framework for speeding up has been created, a few distributed and parallel techniques have been proposed, and their fitness under both GPU and CPU architectures has been examined. This section is exclusively focused on GPU architecture and implmentation issues. I show that for my algorithm, GPU architecture has superior performance and superior cost-efficiency when compared to CPU architecture. A discussion is given on programming frameworks, namely OpenCL and CUDA, the two dominating GPU programming frameworks, and I argue that OpenCL is a better framework to start with. The architectural differences between two major GPU vendors, namely NVIDIA and AMD, are discussed, and a conclusion has been made that for my algorithm there is no evidence which one is better than another before any optimization. Practical issues such as memory constraints, memory transfer rate, clock cycle rate, work group size, and number of processors, are discussed in detail. An implementation is given at the end of the section, but it is not optimized for any particular type of GPU.\nFor simplicity it is assumed that across the whole section the transformation matrices P i are identity matrices. As a consequence, linked lists v are reduced to arrays storing the number of associations, as the only possible word-association is the word itself, and count variable q is always equal to the corresponding t count. The lowest level of parallelism, the topics and word-associations, has effectively reduced to two types of possibilities for each topic: topics with no word associations, and topics with identical word-association. This is done for two reasons:\n1. OpenCL is a subset of C99 language with a small number of extensions. Nonprimitive data structures like linked list are not easy to be natively implemented on GPU, and even harder to be implemented in a way to support parallel accesss. This is not the primary interest of this thesis, so we decide to leave this out for now. At the end of Chapter 4, a brief discussion is given on this issue.\n2. An effective error correction method for v, q counts when transformation matrices P i are not identity matrices has not been found. A few proposals have been made but none has been thoroughly tested and analyzed. To avoid confusing the readers, I decided not to present these partial solutions."}, {"heading": "3.4.1 Parallelization Framework Comparisons", "text": ""}, {"heading": "3.4.1.1 CPU v.s. GPU", "text": "The key winning factors for GPU over CPU are overall higher computing power and higher energy efficiency. This is a combined result from higher memory bandwidth and a higher number of simple processors at lower operating frequency. Table 3.1 shows a\n49\n50\ncomparison between a modern CPU and GPU, both designed by Advanced Micro Devices, Inc. (AMD), available on the consumer market at approximately the same price ( $100USD per piece, as of 30 September 2012). A comparison between NVIDIA GPU and Intel CPU is given in Table 3.2 and 3.3.\nThe tables show the key distinctions between the CPU and the GPU are core frequency, instruction latency, and the capacity of executing and scheduling large amount of hardware threads in parallel. The GPU has an overwhelming advantage in the last measurement, which compensentates for the minor deficiency in the first two measurements. Overall, the GPU is a much superior computing device for massive parallel tasks.\nIn my algorithm, the lowest parallel layer executes massive amount of threads in parallel (corresponding to line 11-17 of Algorithm 2.1), thus taking advantage of massive parallel execution and scheduling power of the GPU. The next parallel layer, where words are being sampled in parallel in multiple of workgroups, could take advantage of the large amount of computing units on the GPU. As the size of count variables related to a single word is relevatively small, the small but fast L2 cache of each computing unit perfectly fits the need of carrying single or multiple workgroups. In the upper most layer of the distributed parallel framework, where all documents and words are randomly distributed\n51\nto multiple GPU devices, the large memory bandwidth of the GPU becomes useful. We can dispatch as many words as possible to fill the global memory of each GPU, keeping the GPU as busy as possible, ensure the GPU kernel execution time is far greater than the time wasted on memory transfer. Multiple GPUs may finish kernel execution at different times, but data can be asynchronously read back to the CPU and the host memory for correction. The error correction process has a high accuracy requirement. Each topic assignment must be processed individually and many count variables must be incremented atomically, making the CPU the most suitable device for this part of the task.\nTo summarize, under the proposed three-level distributed parallelism, the combined use of the GPU and the CPU surely improve the efficiency of execution, given features of the GPU and the CPU are used appropriately."}, {"heading": "3.4.1.2 OpenCL v.s CUDA", "text": "The two dominating programming frameworks for general purpose GPU computing are CUDA, developed by NVIDIA, and OpenCL, developed by Khronous Group. Khronous Group is led by Apple Inc, with members including AMD, IBM, Intel, NVIDIA, and many other industry leaders. While CUDA is optimized for NVIDIA GPUs, it is a closed ecosystem controlled by NVIDIA which does not support any other device than NVIDIA GPU. In contrast, OpenCL is designed to be an open and portable framework which is supported by many types of devices including both CPU and GPU. At present, OpenCL is supported on AMD GPU, AMD CPU, Intel CPU, Intel GPU, NVIDIA GPU, Apple CPU, IBM PowerPC, and most ARM CPU. An OpenCL program could run on all above types of devices without any modification, as long as the program is not using any device-type specific extensions provided by specific vendors. These extensions are typically provided for optimization and debugging purposes.\nMany performance analyses have been done for comparing these two frameworks [35, 36]. Throughout these analysis I did not find any convincing evidence which shows one framework is better than another in terms of performance and usability. Therefore, I decided to choose OpenCL as the programming framework to implement my algorithm and conduct the experiments, because of its openness and portability.\nFigure 3.4.1 shows an overview of the OpenCL framework, taken from [4].\nOpenCL defines three types of memory on GPU: global memory, local memory, and private memory. Global memory is a shared memory region on GPU, accessible to all computing units and all threads. Local memory is located on each computing device, only accessible to the computing device itself, but can be accessed much faster than global memory. Private memory is small regions of memory only accessible by each thread itself, often\n52\nFigure 3.4.1: OpenCL Framework (from [4])\nimplemented as invidual registers on hardware. The details of memory structure is looked at in section 3.4.1.3, where it is shown that my algorithm fits well into this structure.\nA context, roughly speaking, is a shared information structure among devices in the same platform. Data belonging to the same context can be communicated and exchanged among multiple devices, but only devices in the same platform can share the same context. For example, AMD CPUs and AMD GPUs belong to the same platform so they can share the same context, while NVIDIA GPUs belong to another platform because they use a different structure to store information, hence they cannot share the same context with AMD devices. Intel CPUs, though implemented OpenCL, have a totally different structure to any other type of devices, thus cannot share the same context with either NVIDIA device or AMD device.\nA program is a piece of OpenCL code that can be compiled at run-time. OpenCL uses a subset of C99 language as fundamental programming framework, but it also allows programmers to use specific vendor extensions for optimization and debugging. For example, at present double precision floating point numbers are not officially supported by OpenCL specification. Khronos group made an extension for it, and programmers can call \u201c#pragma OPENCL EXTENSION cl khr fp64 : enable\u201d to enable this feature. AMD also have their own extension of double precision floating point numbers, which can be enabled by calling \u201c#pragma OPENCL EXTENSION cl amd fp64 : enable\u201d. More\n53\nexamples can be found in [6, 34].\nA program can define multiple kernel functions and multiple utility functions. Kernel functions have a specific keyword \u201c kernel\u201d as a signature, acting as entry points to the program to be executed on OpenCL devices. When the program compiles, OpenCL automatically optimize the program, at the same time changing all utility functions into inline functions.\nA program can define constant program-wide global variables, but cannot have nonconstant global variables. A program can allocate fixed size arrays, but not dynamic arrays similar to what is created by malloc() function in C. Varaibles in a program are allocated to private memory by default, but with keywords \u201c global\u201d or \u201c local\u201d, a variable can be allocated to device global memory or local memory, respectively.\nA kernel is an entry point at execution of the program. Multiple programs and multiple kernels are allowed for the same context, but a kernel is not allowed to call another kernel in the same program, as it must be scheduled on host. Each kernel may have a set of parameters that can be allocated to private memory, local memory, or global memory. Since kernel arguments are passed from the host, it is possible to allocate a dynamic array as kernel arguments. Besides C99 primitive types, a kernel argument can also be a struct, but the struct cannot contain pointers, where the only exception when the pointers represent fixed size arrays. The reason is it does not make sense pointing to a region of memory other than fixed size arrays given OpenCL devices have their own memory management. Similarly it doesn\u2019t make sense to have pointers as kernel arguments except when these pointers represent pre-allocated arrays.\nA memory object represents a region of memory residing on the device. From the perspective of the kernel, a memory object can be read-only, write-only, or allowing both reading and writing, depending on how the memory object is created on host. Instead of residing on device, memory object can also be a pointer mapping itself to a region in host memory, if the memory object is created with flag \u201cCL MEM USE HOST PTR\u201d.\nEach device has its own command queue to accept a sequence of command instructions from the host. Typical command instructions include launching kernel, reading memory object, writing memory object, mapping memory object to a buffer, and many others. When the host issue these commands, most of the time the programmer could choose to either block the execution of host code for synchronization or safety purposes, or not to block the execution of host code to allow more flexible and efficient scheduling. An optional notification can be sent back to the host when a command finishes execution on the kernel.\n54\nFigure 3.4.2: AMD HD5870 \u201cCypress\u201d Architecture (from [5])"}, {"heading": "3.4.1.3 Hardware and Architectures", "text": "Many types of GPUs are available today, but the two major ones, NVIDIA and AMD, who use slightly different architectures, have almost all the market share in both the consumer and the professional market. Programmers need to estimate the actual memory access pattern and level of parallelism required for their applications, to find the right type of GPU which suits their purposes. A siginificant amount of analysis has been done in academia and industry to compare the performance of these two types of GPUs, unanimously reaching the same conclusion that each type of GPU has their own favourable applications. For example, long time ago in the Bitcoin community [37] people have found AMD GPUs outperform NVIDIA GPUs for everything they need, whereas in the scientific community people are often in favour to NVIDIA GPUs because of better memory effiency, greater performance in double precision computation, and better cluster energy efficiency. A comperhensive academical study comparing a modern AMD GPU and NVIDIA GPU can be found in [38].\nFigure 3.4.2 and 3.4.3 shows the architecture differences between AMD GPUs and NVIDIA GPUs. This particular example compares AMD Radeon HD5870 \u201cCypress\u201d GPU and NVIDIA GeForce GTX480 \u201cFermi\u201d GPU. These two GPUs are released at around the same time in consumer market at approximately the same price, and they have been cho-\n55\nFigure 3.4.3: NVIDIA GTX480 \u201cFermi\u201d Architecture (from [5])\nsen quite often to benchmark AMD GPUs against NVIDIA GPUs for a period of time. In these figures, a warp scheduler is a thread dispatching and scheduling unit. A warp is a batch of simutaneous threads being dispatched and executed together in a Single Instruction, Multiple Data (SIMD) style. Table 3.4 shows their differences in a few key parameters. All processors of the Fermi GPU can operate at the shader frequency (see Table) except the schedulers and texture units.\nFrom the table and the figures it is not hard to see that Fermi GPU has more advanced multi-thread dispatching architecture, greater core frequency, larger memory, faster memory operations, and faster memory exchange speed with host memory, benefited from its multiple dedicated scheduler units, greater memory bandwidth, and greater memory bus width. In contrast Cypress provide greater maximum computing power measured in GFLOPs, higher number of processors (cores), higher number of threads, higher number of computing devices, and better energy efficiency.\nFermi\u2019s greater memory bandwidth is not particularly useful feature to the task at hand as long as we can keep the device busy by pushing maximum amount of data per iteration. However, in my proposed algorithm, the two-level parallelism involves considerable amount of global memory operations. Some features provided by Fermi, namely higher\n56\ncore frequency, higher memory frequency, and greater cache size, are greatly beneficial in this perspective. On the other hand if we could manage to reduce the amount of global memory access operations, and leverage the high throughput provided by AMD GPU to compensate its mild disadvantage in core frequency, AMD GPUs may perform much better than NVIDIA GPUs. However, achieving this may require complicated optimization at the code level for specific hardware.\n57"}, {"heading": "3.4.2 Implementation", "text": "Although an OpenCL kernel program must be written in C, the host program can be written in other languages. The official OpenCL specification use C++ for native host code language but wrappers have been already created for multiple languages including Java, Objective-C, Python, and many others. To achieve maximum portability without sacrifice in efficiency we have chosen Java for the implementation and the experiments. When properly implemented, Java has comparable efficiency to C and C++.\nAlgorithm 3.1 and 3.2 outlines the implementation pseudo code for my algorithm MGPUDP-SPDP (Multi-GPU Distributed Parallel SPDP). As mentioned as the beginning of this section, we have transformation matrices P i set to identity, which limits the number of word-associations to 1, and maximum number of threads needed at topic sampling level to number of topics times 2,. Typically, two times number of topics is far less than maximum numbers of threads in a local workgroup supported by a GPU device, hence for simplicity we assume this is the case in algorithm 3.2, and let each thread handle exactly one probability calculation. If number of topics times 2 is more than maximum numbers of threads in a local workgroup supported by a GPU device, the only thing needs to be done is to adjust the number of probability calculations one thread is required to handle.\nReaders who intend to implement the algorithm by themselves to reproduce the result should be aware that Line 22 and 24 of the pseudo code in Algorithm 3.2 should be computed in logarithm space to avoid overflow, especially when the required Stirling numbers can be an extremely large number at runtime. In my experiments it is observed in the original SPDP algorithm, a medium size document collection could require Stirling numbers as large as to the magnitude of e to the power of hundreds or thousands. The probability formulas provided in Equation 2.4.17, Equation 2.4.18, and Algorithm 2.1 are proportionalities. The result must be normalized before being used to generate a sample, and appropriate technique must be used to avoid overflow. One common way is to use the identity below, where pn denotes the probability and qn denotes corresponding proportionalities:\npn = exp( log qn \u2212 log max{q0, q1, q2, ...q2K}\u2211 n(log qn \u2212 log max{q0, q1, q2, ...q2K}) )\nThis implementation is not optimized. The number of threads in each workgroup is not fine-tuned to suit GPU\u2019s preference, and global memory operations are not reduced to increase its efficiency on AMD GPUs (see Section 3.4.1.3). An optimized implementation may further improve the running speed of my algorithm by many times, but this is not in the scope of this thesis. A basic version (which is used in the experiments) is given here to allow readers with any generic equipment able to reproduce the result.\n58\nAlgorithm 3.1 MGPU-DP-SPDP Host Code\n1: Initialize OpenCL. Read data from file. 2: Randomly initialize topic assignments z 3: Initialize counting variables m,n, t, q, v from z. 4: Run CPU SPDP algorithm for 1 iteration 5: Create shared device memory for common parametres 6: while Gibbs not converged do 7: Create shared device memory n,m, q, v 8: for all GPU g do 9: Create device specific memory for topic assignments zg from z, counts tg from t 10: Create kernel with shared parameters, including device specific memory, shared device memory, and common parametres 11: Enque execution of kernel 12: end for 13: Wait until all kernels have finished execution 14: for all GPU g do 15: Read back tg from g 16: Read back topic assignments zg from g 17: end for 18: Synchronize tg and merge into t 19: Re-generate counts from z, t 20: end while\n59\nAlgorithm 3.2 MGPU-DP-SPDP Kernel\n1: function ParallelSampleWord(Global Counts, Shared SPDP Parametres, Local Memory Variables ...) 2: Load shared parametres into private memory 3: K \u2190 number of topics 4: gid\u2190 Workgroup Group ID, lid\u2190 Workgroup Local ID 5: d \u2190 document ID, i \u2190 group ID, w \u2190 word ID of workgroup gid, k \u2190 topic\nassigned to word 6: if lid \u2264 2K and lid \u2261 0mod2 then 7: kid\u2190 lid/2 (topic represented by lid) 8: Copy global counts of topic kid word w group i into local memory 9: Copy global counts of topic kid document d group i into local memory\n10: Copy auxillary global counts, e.g sums, related to topic kid into local memory 11: Correct local counts copied by kid to ensure they are in valid range 12: Correct global counts copied by kid to ensure they are in valid range. 13: end if 14: Synchronize all threads in local workgroup 15: if lid \u2261 0 then 16: Decrement word counts in local memory in the same way as in non-parallel\nalgorithm 17: Decrement word counts in global memory atomically 18: end if 19: if lid \u2264 2K then 20: kid\u2190 lid/2 (topic represented by lid) 21: if lid \u2261 0mod2 then 22: p[lid]\u2190 probability for zgid = kid, rgid = 1, vi,k,w,t = w 23: else 24: p[lid]\u2190 probability for zgid = kid, rgid = 0: 25: end if 26: else 27: p[lid]\u2190 0 28: end if 29: Synchronize all threads in local workgroup 30: if lid \u2261 0 then 31: zgid \u2190 new topic assignment sampled from p 32: Increment counts in global memory atomically 33: end if 34: end function\n60\nChapter 4\nExperiments\n61"}, {"heading": "4.1 Experimental Setup", "text": "Three datasets 1 are used for the experiments to show the effectiveness of the three-level distributed-parallel framework introduced in Chapter 2. For all documents, words that do not carry any semantic meaning are discarded, such as \u201cI\u201d \u201cyou\u201d, \u201cshe\u201d, \u201che\u201d, \u201care\u201d, \u201cis\u201d, \u201cit\u201d, \u201cam\u201d, the so called stop words. Apostrophes, comma, and other symbols which do not carry semantic meanings are also removed, and words are tokenized into integers. We consider the plural form of a word is distinct from its singular form, as they may carry different meanings in some situations:\n1. Dataset \u201cRedState v.s DailyKos\u201d: A blog dataset that includes 2276 blogs from American political blogs RedState (www.redstate.com) and 2386 blogs from DailyKos (www.dailykos.com) to the end of 2011. Both blogs are focused on current political issues and government policies in the United States, and related international issues, but their political positions are opposing each other. It is well known that RedState is in association with Republican politicians, and DailyKos is an active supporter of Democratic Party in the United States. I train the algorithm with 2045 and 2146 blogs from RedState and DailyKos respectively, and evaluate the perplexity on the rest of the data. After words and symbols are processed and discarded, this yields a collection of documents that has vocabulary size 14724, average effective document length of 157 words in the RedState group, and average document length of 103 words in the DailyKos group.\n2. Dataset \u201cReuters Disaster\u201d: A news article dataset collected from worldwide disasters news articles between 1996 to 1997, written by journalists from different regions of the news agency \u201cReuters\u201d. Articles are divided into four regions: (1) Middle East and South East Asia (1508 articles), (2) European (1834 articles), (3) Asia Pacific (1580 articles), (4) America (mainly the United States, 2418 articles). For each group, 10% of the data is held out from training for computing the perplexity. After words and symbols are processed and discarded, this yields a collection of documents that has vocabulary size 51377, average effective document length of 128 words, 134 words, 132 words, and 151 words in group 1, 2, 3, 4 respectively.\n3. Dataset \u201cInternational Political News\u201d: A news article dataset collected from worldwide political news sources between 1996 to 1997, containing both government releases and private news agencies. Data is divided into three groups according to location where the news article is written: (1) European and Africa (4034 articles) (2) North America and South America (5042 articles) (3) Asia Pacific (6897 articles). For each group, 10% of the data is held out from training for computing perplexity. After words and symbols are processed and discarded, this yields a collection of documents that has vocabulary size 156080, average effective document length of 184 words, 177 words, and 151 words in group 1, 2, 3 respectively.\n1The Reuters RCV1 corpus, Volume 1: English Language, 1996-08-20 to 1997-08-19, [39].\n62\nThe three datasets differ significantly in size. Dataset 1,2,3 contain about 500000, 900000, and 3000000 words respectively. In addition, the vocabulary size of dataset 2 is about 3.5 times as large as dataset 1, and the vocabulary size of dataset 3 is about 10 times as large as dataset 1.\nI use the following parametres for SPDP: \u03b1 = 0.1 (Dirichlet hyper-parameter for words), \u03b3 = 0.1 (Dirichetlet hyper-parameter for topics), a = 0.7 (discount parameter for SPDP), b = 100 (concentration parameter for SPDP). These parameters are suggested as optimal parameters for blog data set by Chen [2], obtained by trial and error. It is also possible to learn the concentration parameters in the algorithm to improve the result, but these parameters are learned outside the regular collapsed Gibbs sampling cycle. Since learning these parameters is not related to the work in GPU speed improvement, these techniques are not discussed in this thesis.\nPerplexity will be evaluated based on the counts after 2000 Gibbs iterations. The common practice is to run less Gibbs iterations, and to take the average output every few iterations after a burn-in period to allow the counts to enter a relatively stable region. In my experiments, this method is not used. Instead, the program runs more iterations and reads out the output only once, because longer iterations leads to better convergence, eliminates the error accumulated by taking the average output from some states that may be far away from the converging state. I observed that my improved algorithm, even when it is only running on a single GPU in my test machine, could be 50 times faster than the original SPDP implementation. When running time is no longer a concern of SPDP algorithm, there is no reason for not choosing to spend a bit more time on training in exchange for a more accurate result.\nI have two machines: the first machine is a homemade GPU cluster configured with seven AMD HD5850 \u201cEvergreen\u201d GPUs, one AMD FX8150 \u201cBulldozer\u201d CPU, and 16GB Corsair DDR III 1667MHz RAM; the second machine is a high-end laptop workstation configured with one NVIDIA GTX460M \u201cFermi\u201d GPU, one Intel Core i7 740QM CPU, and 8GB Kingston 1333MHz RAM. The difference is in RAM frequency will not be an issue in benchmarking, since little computation is done on CPU, and the transmission delay between CPU and main memory is negligable. The AMD HD5850 is a reduced version of the HD5870 introduced in the last chapter, and NVIDIA GTX460M is a reduced version of the GTX480. They share the same architecture with their corresponding full versions, the differences are the number of cores, core clocks, memory rate, and memory frequency. Table 4.1 shows some key parameters of the four computing devices mentioned above, extracted from the vendor\u2019s official website.\nThroughout the experiments, I will use the amount of time elapsed for each Gibbs iteration (minus the amount of time spent on the debugging code, such as unnecessary memory\n63\ntransfer between the host and the GPUs) as the measure to compare the running time of different experiments. The reason that a profiler is not used to measure the amount of time allocated to processes and kernels is that it is also desirable to to take the amount of time spent on memory operations into account. I want to measure overall how fast my algorithm is compared to the original SPDP algorithm. A fine-tuned profiler is not necessary for the task because it is expected that the running time of MGPU-SPDP is only a small fraction of the running time of the original SPDP algorithm, and we only want to get some rough ratios."}, {"heading": "4.2 Experiments", "text": "The first experiment is performed on the original SPDP Java implementation. The result is used as a benchmark to compare with the results from the other experiments which are performed with different versions of MGPU-DP-SPDP that enable part or all features in the proposed three-level distributed parallelism.\nTable 4.2 and Figure 4.2.1 shows the perplexity of the original SPDP algorithm with dataset \u201cRedState v.s DailyKos\u201d on both machines where number of topics is set to be K = 32. The algorithm converges to approximately the same perplexity on both platforms at the same iterations, showing that although the algorithm is stochastic, the convergence speed is stable and platform independent, hence it is not necessary to test my algorithm on different devices which belong to the same class. Although the perplexity changes little\n64\nPlatform AMD FX8150 \u201cBulldozer\u201d Intel Core i7 740QM\nPerplexity after 1 iteration 3514 3514 Perplexity after 5 iterations 2910 3028 Perplexity after 10 iterations 2449 2412 Perplexity after 25 iterations 2042 2015 Perplexity after 100 iterations 1817 1819 Perplexity after 2000 iterations 1790 1788\nFigure 4.2.1: SPDP perplexity through Gibbs iterations (logarithm scaled) with the dataset \u201cRedState v.s DailyKos\u201d\n65\n66\nafter around 25 iterations, topic quality keeps getting better as the algorithm progresses, as shown in table 4.3. For each row in \u201cIterations\u201d column, two rows in \u201cTop Words\u201d column corresponds to different group of documents, the first row represents RedState group, second row represents DailyKos groups. Topics are shared across groups but each group has its own ranking of a particular topic.\nTable 4.4 shows the running time per iteration of the original SPDP algorithm with dataset \u201cRedState v.s DailyKos\u201d on both machines where the number of topics varies. The experiment performed on the Intel Core i7 740QM has slightly better running time due to the architectural advantage. The performance sharply dropped when the number of topics is increased to 256, due to fast expanding memory consumption (3GB) hence decreased CPU on-chip cache performance. The AMD FX8150 outperformed the Intel Core i7 740QM due to significantly larger level 2 cache (2\u00d74MB v.s. 4\u00d7256KB), and larger level 3 cache (8MB v.s 6MB).\nTable 4.5 shows the running time per iteration of original SPDP algorithm with the datasets \u201cReuters Disaster\u201d and \u201cInternational Political News\u201d. Figure 4.2.2 and 4.2.3 show the perplexity convergence of the original SPDP algorithm with these two datasets. Table 4.6, 4.7, and 4.8 show a few converged topics of original SPDP algorithm, on the dataset \u201cRedState v.s DailyKos\u201d, \u201cReuters Disaster\u201d, and \u201cInternational Political News\u201d respectively. The number of topics is set to 32 in these experiments.\n67\nFigure 4.2.2: SPDP perplexity through Gibbs iterations (logarithm scaled) with the dataset \u201cReuters Disasters\u201d\nFigure 4.2.3: SPDP perplexity through Gibbs iterations (logarithm scaled) with the dataset \u201cReuters Disasters\u201d\n68\n69\n70\n71\n4.2.1 Basic Parallelization Over Topics\nI implemented the proposal in Section 3.3.1.1, a basic parallelism over topics and wordassociations (recall that only one word can be associated since the transformation matrices P i are identity matrices). Threads are created before Gibbs sampling initiates, and synchronized after topic and word-association probabilities are computed. This experiment is only conducted on the Intel i7 740QM CPU and the NVIDIA GTX460M GPU since the results are sufficient to show that a single level of parallelism is not efficient.\nTable 4.9 shows the result on the dataset \u201cRedState v.s DailyKos\u201d. Just as we expected in Section 3.3.1.1, the cost of synchronization surpasses the benefit of parallelism. The original SPDP running on one thread is the consistent winner. The result shows that \u201ceasy\u201d parallelism such as this simply does not work - it does not improve the running speed of SPDP at all."}, {"heading": "4.2.2 Parallelization Over Words", "text": "I implemented the two-level parallel proposal in Section 3.3.1.2. The experiments are conducted on both AMD Radeon HD5850 and NVIDIA GTX460M GPUs, with varying number of topics. This version will be refered as GPU-SPDP from now on.\nTable 4.10 illustrates the running time of GPU-SPDP with the dataset \u201cRedState v.s DailyKos\u201d. Original SPDP best running times on CPU from Table 4.2 are provided as a single column for comparison. A massive speedup is obtained even though I did not optimize my code for the specific GPUs, as explained in Section 3.4.2. If the code is further optimized to device-specific parameters, we may expect a further improvement a few times better than the current result. With the implementation of Algorithm 3.2 only a fraction of the maximum local memory provided by both devices is used. Furthermore, the AMD HD5850, which has higher GFLOPs than the NVIDIA GTX460M, is reporting longer running time. This evidence shows that we have not used the full potential of these devices.\nNumber of Threads K = 32 K = 128 1 (CPU) 16.510 seconds 66.230 seconds 2 (CPU) 72.037 seconds 109.563 seconds 4 (CPU) 192.815 seconds 198.165 seconds 64 (CPU) > 600 seconds >600 seconds\n2\u00d7K (GPU) 260 seconds 334 seconds\nTable 4.9: SPDP basic parallelism result with the dataset \u201cRedState v.s DailyKos\u201d\n72\nTable 4.11 shows the running time on other datasets with the number of topics set to 32. Note this is not the optimal number of topics for current implementation to get the maximum speedup.\n73\nFigure 4.2.4, 4.2.5, and 4.2.6 show the comparisons between the perplexity of my algorithm and the perplexity of the original algorithm for all three datasets as number of iterations progresses, when number of topics is set to 32. It can be seen from the figures that my algorithm is only slightly worse than the original algorithm in terms of perplexity. Table 4.12, 4.13, and 4.14 show the sample outputs of a few topics produced by the algorithm after 2000 iterations for all three datasets. From the human intepretability point of view these topics possess high quality because an ordinary human reader could understand and make sense of the words in each topic.\nFor example, in the result of the experiment running on dataset \u201cRedState v.s DailyKos\u201d:\nTopic 26 shows RedState bloggers, who represent Republicans, are trying to completely avoid mentioning the issue that US government and CIA are torturing prisoners in Guantanamo detention camp who they believe are terrorists. Instead they redirect the topic to international issues in Pakistan and Israel, progress of peace negotiations in this area, and terrorism events around the globe.\nTopic 22 shows bloggers from both websites are addressing the issue of global financial crisis in approximately the same fashion. The subtle difference is DailyKos bloggers are looking at the issue from the government and public point of view (as seen from words \u201cpublic, matching, federal, etc.\u201d), whereas RedState bloggers are more focused on the market itself (as seen from words \u201crate, credit, securities, risk, bank, (wall) street\u201d). This is coherent to public knowledge that Democratic Party (that DailyKos represents) is running the government, and republicans are known to be close to Wall Street and big finance industry players.\nTopic 4 is focused on racial and gender equality issues. RedState bloggers seem to be quite interested by the fact that Barack Obama, the current US president and leader of Democratic Party, is an African American. On the other hand DailyKos bloggers cover a range of issues in this area, treating them as equally important. The name \u201cFerraro\u201d is mentioned because she is known to be a feminist who has strong opinion and great influence. She is the first female Vice Presidential candidate representing a major American political party. The city \u201cSan Francisco\u201d is mentioned because this is the major city for racial and gender equality campaigns and protests.\nTopic 12 is focused on political blogs and online media. It is very interesting that each of their website names appears as the top word in this topic.\n74\nFigure 4.2.4: GPU-SPDP perplexity v.s Original SPDP with the dataset \u201cRedState v.s DailyKos\u201d\nFigure 4.2.5: GPU-SPDP perplexity v.s Original SPDP with the dataset \u201cReuters Disasters\u201d\n75\nFigure 4.2.6: GPU-SPDP perplexity v.s Original SPDP with the dataset \u201cInternational Political News\u201d\nExamining the result closely, it can be noticed that some problems do exist in this version of approximation. The top topic (which is not shown here) has excessive probability in both groups, whereas the rest of the topics have much smaller probabilities compared to the result from original SPDP algorithm. This can be explained by the error accumulation due to parallel accessing same count variable through many iterations of approximation. A solution for this through word re-ordering is proposed in Section 3.3.1.3. In the next experiment, this is implemented in conjunction with this version of the approximation.\n76\n77\n78\n79"}, {"heading": "4.2.3 Effect of Re-ordering Words", "text": "As proposed in Section 3.3.1.3, it is expected that the amount of error can be reduced by rearranging the order of words before dispatching them to the GPU. In this experiment, this feature is added to the implementation, while everything else is as same as the previous experiment. Figure 4.2.7, 4.2.8, and 4.2.9 show the perplexity convergence of the improved version compared to the perplexity convergence result from GPU-SPDP and the original SPDP. Just as we did in the previous experiments, Table 4.15, 4.16, and 4.17 are included to show the quality of a few selected topics.\nThe perplexity is indeed reduced to a value almost as same as the original SPDP algorithm. The topic probabilities are in the correct range. The topic qualities are also slightly improved.\nThis means, now we have an algorithm which is almost as good as the original SPDP, but it runs about 30 to 50 times faster on a single consumer-grade cheap GPU which costs less than $150 USD, before any optimization, or using the power of multiple GPUs distribution framework as proposed in Section 3.3.2.\n80\nFigure 4.2.7: Improved GPU-SPDP perplexity with the dataset \u201cRedState v.s DailyKos\u201d\nFigure 4.2.8: Improved GPU-SPDP perplexity with the dataset \u201cRedState v.s DailyKos\u201d\n81\nFigure 4.2.9: Improved GPU-SPDP perplexity with the dataset \u201cRedState v.s DailyKos\u201d\n82\n83\n84\n85"}, {"heading": "4.2.4 Multi-GPU Distributed Parallelism", "text": "Finally, I implemented the ultimate algorithm MGPU-DP-SPDP, and tested it using the same configuration as used in previous experiments except all five Radeon HD5850 GPUs on the AMD machine are used this time.\nThe running time analysis is shown in Table 4.18. For the columns representing running time of the original SPDP and GPU-SPDP, we used data from Table 4.10. The speed improvement over GPU-SPDP is sublinear and sometimes much lower than the maximum value (500%) because of two reasons. First, for safety reasons we used atomic locks for incrementing and decrementing variables (i.e using OpenCL primitives). For some reasons, this is necessary for AMD GPUs when multiple GPUs are used, otherwise the program would crash. We believe atomic locks are not necessary, and the result could be much more impressive if the experiment is conducted on multiple professional NVIDIA GPUs, which do not have these problems. However, we do not have the equipment at our disposal. Second, the code is not optimized, and a substantial amount of operation is wasted on memory transfer. Nonetheless, the experiment is sufficient to show that MGPU-DP-SPDP is a scalable algorithm that could improve the running speed of GPU-SPDP dramatically, sublinear to the number of GPUs available to the machine.\n86\nThe convergence in perplexity is shown in Figure 4.2.10. The perplexity is not as good as the original algorithm, but it is fairly comparable, and words for each topic still look sensible. This is because some topic probabilities are off-scale, which is discussed in the next section.\n87\nFigure 4.2.10: MGPU-DP-SPDP perplexity convergence with the dataset \u201cRedState v.s DailyKos\u201d\nFigure 4.2.11: MGPU-DP-SPDP perplexity convergence with the dataset \u201cReuters Disasters\u201d\n88\nFigure 4.2.12: MGPU-DP-SPDP perplexity convergence with the dataset \u201cInternational Political News\u201d\nThe topic quality is shown in Table 4.20, 4.16, and 4.22. Occasionally strange words may appear, but overall the result is interesting, informative, and sensible.\n89\n90\n91\n92"}, {"heading": "4.2.5 Multi-GPU Distributed Parallelism: Effect of Duplicating", "text": "Training Data\nIt was found in one of my early implementations of MGPU-DP-SPDP (which has slightly worse error correction mechanism) that duplicating training data is sometimes crucial to get topic probabilities which do not look ridiculous. In my current version, duplicating the training data is no longer necessary. I found duplicating the data once could slightly improve the correctness of topic probabilities and the topic qualities particularly in a small document collection, which unsurprisingly results a better perplexity. Due to hardware limitations, I am unable to verify if more than one duplicate is required to improve the perplexity for a large number of GPUs, though I believe with good implementation of error correction and optimization, the need of improving the perplexity from duplicating the training data can be completely eliminated. Figure 4.2.13 shows the perplexity comparison after duplicating the data once for the \u201dRedState v.s DailyKos\u201d dataset. Table 4.23 shows the topic qualities.\n93\nFigure 4.2.13: MGPU-DP-SPDP perplexity convergence with the duplicated dataset \u201cRedState v.s DailyKos\u201d\n94\n95"}, {"heading": "4.2.6 Hellinger Distance", "text": "Hellinger distance [40] is a popular measure of the similarity between two topic models. Here two heatmaps in Figure 4.2.14 and 4.2.15 are provided to show the Hellinger distance between the original SPDP and Improved-GPU-SPDP, and the Hellinger distance between the original SPDP and MGPU-DP-SPDP, both with dataset \u201cReuters Disasters\u201d The xaxis is the topics as appeared in Improved-GPU-SPDP, and y-axis is the topics as appeared in the original SPDP, re-ordered to align with the topics in x-axis. Lower values mean better match in topics. The results shown here look almost as good as comparing two runs of the original SPDP.\n96\nFigure 4.2.14: Hellinger distance between the original SPDP and Improved-GPU-SPDP with the dataset \u201cReuters Disasters\u201d\n97\nFigure 4.2.15: Hellinger distance between the original SPDP and MGPU-DP-SPDP with the dataset \u201cReuters Disasters\u201d\n98\nChapter 5\nConclusion\n99"}, {"heading": "5.1 Conclusions and Key Contributions", "text": "I transformed the state-of-art algorithm SPDP into a distributed parallel approximation, where the running speed is substantially improved on a single GPU when only one GPU is used, and sublinearly scalable to number of GPUs available when multiple GPUs are used.\n\u2022 The single GPU algorithm improved the speed of the differential topic modelling algorithm SPDP by about 50 times on a single, cheap, medium range laptop GPU.\n\u2022 The multi-GPU algorithm MGPU-DP-SPDP leverages the latest modern multiGPU architecture, improves the running speed of SPDP and topic modelling on a homemade small GPU cluster, with very small sacrifice in topic quality, and only slightly worse or fairly comparable perplexity.\n\u2022 SPDP is a representative of perhaps another hundred extensions of LDA. My algorithm is designed in a generalized way, leaving mathematical derivations of SPDP intact. With little modification, the algorithm can be applied to other LDA extensions."}, {"heading": "5.2 Future Work", "text": "Although the experiments have been quite successful in my experiments, there are a few other improvements that could be done:\n\u2022 automated optimization tailored to GPU specifications and parameters;\n\u2022 scalability and robustness of multi-GPU sampling;\n\u2022 full SPDP parallelization with non-identity transformation matrices;\n\u2022 large scale implementation;\n\u2022 adapting the three-level distributed parallel framework to other LDA extensions."}, {"heading": "5.2.1 Automated Optimization", "text": "The OpenCL framework provides APIs for users to get information about the capacity and specification of available devices. It is possible to determine the preferred local workgroup size from these information and tailor my algorithm to maximize the performance. The number of threads per workgroup should be made as close to the preferred size as possible. Each thread should be allowed to compute multiple sample probabilities, and the value should be determined by d Nwords\nNthreads K(S+1) Tpreferred e, where Nwords is the number of words that can be sampled in parallel with at most C conflicts, Nthreads is the maximum number of\n100\nparallel hardware threads the device supports, K is number of topics, S is number of word associations (S = 1 if transformation matrix P is identity), Tpreferred is the preferred local workgroup size, C is a constant value to be determined. The formula simply states that we should sample as many as possible words per wave in parallel with at most C conflicts. A conflict is defined as multiple access to one count variable, which could happen when two workgroups represent the same word and topic, or same word and document, or same document and topic.\nTo have this idea implemented efficiently, kernel code must be generated dynamically and complicated change needs to be done in host code. With this properly implemented, we may expect a significant speed improvement to the algorithm due to better occupancy hence more efficient use of GPU device."}, {"heading": "5.2.2 Scalability And Robustness of Multi-GPU Sampling", "text": "The results of the experiments imply that the current error correction mechanism works well with word probabilities, but not as well with topic probabilities, hence increasing the perplexity. A better error correction mechanism is required for multi-GPU sampling. In addition, sending and reading data from multiple GPUs can be inefficient especially when the number of GPUs is large. It is possible to leave data on the GPUs, perform the error correction, random number generation, and count regeneration on GPUs. Complicated issues may arise, such as looking for an efficient way to generate random numbers and propagate to all GPU devices, and how not to make error while correcting error in parallel. We may also consider using the model introduced by Smola et al. in [33], having a dedicated GPU to update counts for all devices. However, the implementation of this model would be very complicated, and may require approximation as some features supported by the CPU and distributed networks are not supported by the GPU and OpenCL."}, {"heading": "5.2.3 Full SPDP Parallelization With Non-identity Transforma-", "text": "tion Matrices\nChen has showed in [2] that appropriate transformation matrices could uniformly improve the perplexity and the topic quality. For simplicity we implemented MGPU-DP-SPDP in Algorithm 3.1 and 3.2 assuming transformation matrices P i are identity matrices. As mentioned in Section 3.3.1.2, the difficulty in dealing with non-identity transformation matrices is in implementing a linked list v that can be modified in parallel with high performance. One easy solution is to store the modifications in a separate array, which is to be merged into linked list v after the GPU kernel finishes execution. However, because a massive number of words are scheduled to be sampled on the GPU, this may causes substantial delay in updating the linked list variable v and the count variable q, which may cost the algorithm more than the benefit of non-identity transformation matrices could\n101\nbring. Another approach is to allocate a fixed amount of global memory and local memory for each workgroup to store the linked list corresponding to the words they are assigned to, and merge the linked lists at the start of execution of each workgroup into the local memory. This allows any update to v and q to be reflected immediately, but consequently the algorithm may be slowed down substantially due to significantly increased amount of global memory access."}, {"heading": "5.2.4 Large Scale Implementation", "text": "With a GPU cluster supercomputer, my algorithm has the potential to improve the running speed of SPDP and topic modelling to an unparallel level of magnitude, potentially sublinear to number of GPUs times number of cores available on a single GPU. Take this medium sized GPU cluster supercomputer [41] in CSIRO for example, a GPU cluster with 64 Tesla S2050s containing 256 GPUs with 114688 streaming processors. Assuming each GPU can only improve the running speed of SPDP by 100 times (which is a highly conservative under-estimate, consider these GPUs are all the highest range professional scientific computing GPUs, and even my cheap laptop GPU is powerful enough to improve the running speed of SPDP by 54 times before optimization). Based on the evidence in my experiments it is reasonable to estimate that my algorithm running on this GPU cluster could speed up SPDP by thousands of times before optimization, if the document collection is large enough and a few issues faced in my experiments are solved through better implmentation. This value is much greater than the speedup achieved by Smola et al. did for LDA[33], which is the best speedup known so far to LDA. Of course, this is only an estimate, it may cause the algorithm to produce more error when the algorithm is running at a large scale, and it may take large amount of effort to make my algorithm scale to the level similar to CSIRO GPU cluster, since there could be many synchronization and memory issues."}, {"heading": "5.2.5 Adapting the Three-level Distributed Parallel Framework", "text": "to Other LDA Extensions\nAlthough my three-level distributed parallel framework is proposed and implemented for SPDP, it does not use many specific properties of SPDP. In particular, the mathematical derivation of SPDP is not modified. This enables the algorithm to be generalized and adapted to other LDA extensions, or LDA itself. It requires few modification to the framework, and a small amount of work for implementing error correction and other related parts. The framework is especially beneficial to those extensions with complex structures with many variables. Furthermore, the same approach may be taken to parallelize and approximate Markov chains and solve a wider range of problems.\n102"}], "references": [{"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Transformation Poisson-Dirichlet processes for differential topic modeling", "author": ["Wray Buntine"], "venue": "Technical report, ANU & NICTA,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Efficient Parallel Graph Exploration on Multi-Core CPU and GPU", "author": ["Sungpack Hong", "Tayo Oguntebi", "Kunle Olukotun"], "venue": "IEEE Computer Society,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Discovery in Text: Visualisation", "author": ["Wray Buntine"], "venue": "Topics and Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Joint sentiment/topic model for sentiment analysis", "author": ["Chenghua Lin", "Yulan He"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Dynamic Item Recommendation by Topic Modeling for Social Networks. In ITNG, pages 884\u2013889", "author": ["Sang Su Lee", "Tagyoung Chung", "Dennis McLeod"], "venue": "IEEE Computer Society,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Investigating topic models for social media user recommendation", "author": ["Marco Pennacchiotti", "Siva Gurumurthy"], "venue": "In Proceedings of the 20th international conference companion on World wide web,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Trend analysis model: trend consists of temporal words, topics, and timestamps", "author": ["Noriaki Kawamae"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Investigating bias in traditional media through social media", "author": ["Arjumand Younus", "Muhammad Atif Qureshi", "Suneel Kumar Kingrani"], "venue": "WWW (Companion Volume),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "The author-topic model for authors and documents", "author": ["Michal Rosen-Zvi", "Thomas Griffiths", "Mark Steyvers", "Padhraic Smyth"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Visualizing search results and document collections using topic maps", "author": ["David Newman", "Timothy Baldwin", "Lawrence Cavedon"], "venue": "J. Web Sem.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Parameter estimation for text analysis", "author": ["G. Heinrich"], "venue": "Technical report,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Probabilistic Latent Semantic Analysis", "author": ["Thomas Hofmann"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Topic models with power-law using Pitman-Yor process", "author": ["Issei Sato", "Hiroshi Nakagawa"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A Bayesian Review of the Poisson", "author": ["Wray L. Buntine", "Marcus Hutter"], "venue": "Dirichlet Process. CoRR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Gibbs Sampling Methods for Stick-Breaking Priors", "author": ["Hemant Ishwaran", "Lancelot F. James"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes", "author": ["Yee Whye Teh"], "venue": "ACL. The Association for Computer Linguistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Sampling Table Configurations for the Hierarchical Poisson-Dirichlet Process", "author": ["Changyou Chen", "Lan Du", "Wray Buntine"], "venue": "In Proceedings of the ECML/PKDD", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Hierarchical Dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Monte Carlo Statistical Methods", "author": ["C.P. Robert", "G. Casella"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator", "author": ["J. Pitman", "M. Yor"], "venue": "Annals of Probability,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1997}, {"title": "Automatic Evaluation of Topic Coherence", "author": ["David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin"], "venue": "In HLT-NAACL,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Distributed Algorithms for Topic Models", "author": ["David Newman", "Arthur U. Asuncion", "Padhraic Smyth", "Max Welling"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "An Architecture for Parallel", "author": ["Alexander J. Smola", "Shravan Narayanamurthy"], "venue": "Topic Models. PVLDB,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "A Performance Comparison of CUDA and OpenCL", "author": ["Kamran Karimi", "Neil G. Dickson", "Firas Hamze"], "venue": "CoRR, abs/1005.2581,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "A Comprehensive Performance Comparison of CUDA and OpenCL", "author": ["Jianbin Fang", "Ana Lucia Varbanescu", "Henk J. Sips"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Architecture Comparisons between Nvidia and ATI GPUs: Computation", "author": ["Ying Zhang", "Lu Peng", "Bin Li", "Jih-Kwon Peir", "Jianmin Chen"], "venue": "Parallelism and Data Communications,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "RCV1: A New Benchmark Collection for Text Categorization Research", "author": ["D.D. Lewis", "Y. Yand", "T.G. Rose", "F. Li"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2004}, {"title": "Modelling Sequential Text with an Adaptive Topic Model. In Empirical Methods in Natural Language Processing (EMNLP), page 9, Jeju /Korea", "author": ["Lan Du", "Wray Buntine", "Huidong Jin"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Bayesian topic models such as Latent Dirichlet Allocation (LDA) [1] allow salient patterns in large collection of documents to be extracted and analyzed automatically.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "Recently, the introduction of techniques for differential topic modelling, namely the Shadow Poisson Dirichlet Process model (SPDP) [2] performs uniformly better than many existing topic models in a discriminative setting.", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "s NVIDIA GPU comparison (from [7]) .", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "s NVIDIA GPU memory bandwidth comparison (from [7]) .", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "Bayesian topic models such as Latent Dirichlet Allocation (LDA) [1] allow salient patterns in large collection of documents to be extracted and analyzed automatically.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "Recently, the introduction of techniques for differential topic modelling, namely the Shadow Poisson Dirichlet Process model (SPDP) [2] performs uniformly better than many existing topic models in a discriminative setting.", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "People need tools to organize, search, summarize, and understand information, tools to turn information into knowledge [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 146, "endOffset": 149}, {"referenceID": 5, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 230, "endOffset": 238}, {"referenceID": 6, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 230, "endOffset": 238}, {"referenceID": 7, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 272, "endOffset": 276}, {"referenceID": 8, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 311, "endOffset": 315}, {"referenceID": 1, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 371, "endOffset": 374}, {"referenceID": 9, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 392, "endOffset": 396}, {"referenceID": 10, "context": "Today, there are many proposed applications of topic modelling to different industries: \u2022 Finance, media, governmental: Public sentiment analysis [9] \u2022 Social network companies: Content based social network recommendation systems [10, 11] \u2022 Media companies: Trend analysis[12], traditional media bias detection [13] \u2022 Military: Differential discovery on text collections [2], author detection[14] \u2022 Enterprises, consumers: Search engine and document processing [15]", "startOffset": 461, "endOffset": 465}, {"referenceID": 11, "context": "In addition, to limit the length of our background chapter, we expect the reader to be familiar with standard machine learning, computer architecture, distributed and parallel computing, micro-processors, Bayesian statistics, parameter estimation, statistical inference, and Bayesian graphical models (see [20]).", "startOffset": 306, "endOffset": 310}, {"referenceID": 0, "context": "Moreover, we expect familiarities with basic topic models such as Latent Dirichlet Allocation (LDA) (see [1, 21]) and common performance measure such as perplexity (see section 3.", "startOffset": 105, "endOffset": 112}, {"referenceID": 12, "context": "Moreover, we expect familiarities with basic topic models such as Latent Dirichlet Allocation (LDA) (see [1, 21]) and common performance measure such as perplexity (see section 3.", "startOffset": 105, "endOffset": 112}, {"referenceID": 13, "context": "While non-Bayesian topic models such as Probabilistic Latent Semantic Analysis (PLSA) [22] do exist, the focus of the topic modelling research field had been mostly shifted to Bayesian topic models since the advent of Latent Dirichlet Allocation (LDA) given their superior theoretical basis and good performance.", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "A mathematical definition and a conceptional description are given on the following models, ordered by their simplicity and the dates they are created: \u2022 Latent Dirichlet Allocation (LDA) [1] \u2022 Pitman-Yor Topic Modelling (PYTM) [23] \u2022 Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23]", "startOffset": 188, "endOffset": 191}, {"referenceID": 14, "context": "A mathematical definition and a conceptional description are given on the following models, ordered by their simplicity and the dates they are created: \u2022 Latent Dirichlet Allocation (LDA) [1] \u2022 Pitman-Yor Topic Modelling (PYTM) [23] \u2022 Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23]", "startOffset": 228, "endOffset": 232}, {"referenceID": 14, "context": "A mathematical definition and a conceptional description are given on the following models, ordered by their simplicity and the dates they are created: \u2022 Latent Dirichlet Allocation (LDA) [1] \u2022 Pitman-Yor Topic Modelling (PYTM) [23] \u2022 Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23]", "startOffset": 283, "endOffset": 287}, {"referenceID": 1, "context": "\u2022 Differential Topic Modelling using Shadow Poisson Dirichlet Process (SPDP) [2]", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "1 Latent Dirichlet Allocation (LDA) Latent Dirichlet Allocation (LDA) [1] is one of the most popular models used in topic modelling because of its simplicity.", "startOffset": 70, "endOffset": 73}, {"referenceID": 14, "context": "2 Pitman-Yor Topic Modelling (PYTM) Pitman-Yor Topic Modelling (PYTM) [23] made a few improvements over LDA, hence achieved significantly better performance when measured in perplexity.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "The PYTM assumes in each document the words are sequentially drawn from a distribution generated by a Poisson Dirichlet Process (PDP)[24] (also known as Pitman-Yor Process[25]).", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "The PYTM assumes in each document the words are sequentially drawn from a distribution generated by a Poisson Dirichlet Process (PDP)[24] (also known as Pitman-Yor Process[25]).", "startOffset": 171, "endOffset": 175}, {"referenceID": 14, "context": "d words from a multinomial distribution, hence is unable to capture the properties of a power-law [23].", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "Below is another way of representing this process, similar to what is introduced in [23].", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "The Hierarchical Pitman-Yor Topic Modelling (HPYTM) [23] made one extension to PYTM by assuming the power-law phenomenon not only exists in each document but also within each topic.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "The Hierarchical Bayesian Language Model [26] replaces some parts of PYTM still inheriting some features of LDA with a more complicated structure, as illustrated in Figure 2.", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "4 Differential Topic Modelling Using Shadow Poisson Dirichlet Process This differential model [2] addresses the problem of comparing multiple groups of documents.", "startOffset": 94, "endOffset": 97}, {"referenceID": 15, "context": "Combining the essence of all models introduced above and results from other mathematics research and topic modelling research, especially the theoretical results in [24], the improved PDP table-configuration sampler in [27], and the Hierarchical Dirichlet Process model [28], the Differential Topic Model with Shadow Poisson Dirichlet Process (SPDP) is born.", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "Combining the essence of all models introduced above and results from other mathematics research and topic modelling research, especially the theoretical results in [24], the improved PDP table-configuration sampler in [27], and the Hierarchical Dirichlet Process model [28], the Differential Topic Model with Shadow Poisson Dirichlet Process (SPDP) is born.", "startOffset": 219, "endOffset": 223}, {"referenceID": 19, "context": "Combining the essence of all models introduced above and results from other mathematics research and topic modelling research, especially the theoretical results in [24], the improved PDP table-configuration sampler in [27], and the Hierarchical Dirichlet Process model [28], the Differential Topic Model with Shadow Poisson Dirichlet Process (SPDP) is born.", "startOffset": 270, "endOffset": 274}, {"referenceID": 1, "context": "Although the superiority of this model has already been demonstrated in experiments, [2] does not provide an in depth explanation of the intuition of the model and derivation of the model.", "startOffset": 85, "endOffset": 88}, {"referenceID": 20, "context": "The proof can be found in most advanced statistics textbooks, such as [29].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "\u2206(~x) is the Dirichlet delta function, the normalizing constant, as defined in [21]:", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "For more detailed explanation on LDA, readers should refer to [1].", "startOffset": 62, "endOffset": 65}, {"referenceID": 21, "context": ") coupled with probability weighting vector ~ p drawn from Poisson Dirichlet Distribution as stated in [30].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "A detailed Bayesian analysis of Poisson Dirichlet Process is given in [24] by Buntine and Hutter.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "Readers should refer to [23] if they are interested in the details.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "It is shown by Corollary 17 in [24] that in one \u201crestaurant\u201d:", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "2 Topic Performance Measure Perplexity and pointwise mutual information score (PMI score) based on Wikipedia corpus[31] are the two most popular measures used in the research field to judge the quality of generated topic models.", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "1 PMI-Score Based on Wikipedia Corpus Out of many evaluation methods proposed in [31], the PMI-score based on the Wikipedia corpus is the consistent best performer with respect to intrinsic semantic quality of learned topics.", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "In [2] Chen et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "Similar to the definition in [21], we define p(~ wi,d| the rest) as:", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "in [32], and an improved version proposed by Smola and Narayanamurthy in [33].", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "in [32], and an improved version proposed by Smola and Narayanamurthy in [33].", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "s NVIDIA GPU comparison (from [7])", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "s NVIDIA GPU memory bandwidth comparison (from [7])", "startOffset": 47, "endOffset": 50}, {"referenceID": 25, "context": "Many performance analyses have been done for comparing these two frameworks [35, 36].", "startOffset": 76, "endOffset": 84}, {"referenceID": 26, "context": "Many performance analyses have been done for comparing these two frameworks [35, 36].", "startOffset": 76, "endOffset": 84}, {"referenceID": 27, "context": "A comperhensive academical study comparing a modern AMD GPU and NVIDIA GPU can be found in [38].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "The Reuters RCV1 corpus, Volume 1: English Language, 1996-08-20 to 1997-08-19, [39].", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "These parameters are suggested as optimal parameters for blog data set by Chen [2], obtained by trial and error.", "startOffset": 79, "endOffset": 82}, {"referenceID": 29, "context": "6 Hellinger Distance Hellinger distance [40] is a popular measure of the similarity between two topic models.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "in [33], having a dedicated GPU to update counts for all devices.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "3 Full SPDP Parallelization With Non-identity Transformation Matrices Chen has showed in [2] that appropriate transformation matrices could uniformly improve the perplexity and the topic quality.", "startOffset": 89, "endOffset": 92}, {"referenceID": 24, "context": "did for LDA[33], which is the best speedup known so far to LDA.", "startOffset": 11, "endOffset": 15}], "year": 2015, "abstractText": "Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling by Aaron(Qiaochu) Li Bachelor of Science(Advanced)(Honours) Research School of Computer Science Australian National University Doctor Wray Buntine, Supervisor Doctor Scott Sanner, Co-supervisor There is an explosion of data, documents, and other content, and people require tools to analyze and interpret these, tools to turn the content into information and knowledge. Topic modelling have been developed to solve these problems. Bayesian topic models such as Latent Dirichlet Allocation (LDA) [1] allow salient patterns in large collection of documents to be extracted and analyzed automatically. When analyzing texts, these patterns are called topics, represented as a distribution of words. Although numerous extensions of LDA have been created in academia in the last decade to address many problems, few of them can reliablily analyze multiple groups of documents and extract the similarities and differences in topics across these groups. Recently, the introduction of techniques for differential topic modelling, namely the Shadow Poisson Dirichlet Process model (SPDP) [2] performs uniformly better than many existing topic models in a discriminative setting. There is also a need to improve the running speed of algorithms for topic models. While some effort has been made for distributed algorithms, there is no work currently done using graphical processing units (GPU). Note the GPU framework has already become the most cost-efficient and popular parallel platform for many research and industry problems. In this thesis, I propose and implement a scalable multi-GPU distributed parallel framework which approximates SPDP, called MGPU-DP-SPDP, and a version running on a single GPU, Improved-GPU-SPDP. Through experiments, I have shown ImprovedGPU-SPDP improved the running speed of SPDP by about 50 times while being almost as accurate as SPDP, with only one single cheap laptop GPU. Furthermore, I have shown the speed improvement of MGPU-DP-SPDP is sublinearly scalable when multiple GPUs are used, while keeping the accuracy fairly comparable to SPDP. Therefore, on a mediumsized GPU cluster, the speed improvement could potentially reach a factor of a thousand. Note SPDP is just a representative of perhaps another hundred other extensions of LDA. Although my algorithm is implemented to work with SPDP, it is designed to be a general framework that can be extended to work with other LDA extensions and improve", "creator": "LaTeX with hyperref package"}}}