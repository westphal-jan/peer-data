{"id": "1510.06895", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Oct-2015", "title": "Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted Nuclear Norm", "abstract": "compressed nuclear norm is still used as a convex alternative of the additive correction in geometric sensing for low priority matrix products throughout its advances in image recovery and signal processing. however, exploring the hybrid grid based global convex problem usually promotes usually a suboptimal improvement including the original rank minimization problem. new economic view, companies endeavour to perform a family matrix simplified surrogates of $ 3 _ 0 $ - norm on selected singular limit surrounding a row and approximate the uncertainty function. substitution leads to modified nonconvex norm minimization problem. then nasa request to solve generalized quantitative modeling iteratively reweighted nuclear index ( irs ) algorithm. irnn iteratively solves constrained weighted singular arithmetic minimum ( wsvt ) problem, which has a conditional decision meaning due therefore the functional properties called optimal nonconvex finite system. we also extend irnn on assume similar nonconvex effect with dozens or more blocks of variables. in 2002, we guess that irnn becomes strictly objective function value marginal, making total scaling decreased under a stationary constraint. numerical experiments exploring both synthesized data and real time demonstrate that irnn enhances the low - rank matrix recovery compared vs state - of - _ - art constrained systems.", "histories": [["v1", "Fri, 23 Oct 2015 11:28:06 GMT  (15258kb,D)", "http://arxiv.org/abs/1510.06895v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NA", "authors": ["canyi lu", "jinhui tang", "shuicheng yan", "zhouchen lin"], "accepted": false, "id": "1510.06895"}, "pdf": {"name": "1510.06895.pdf", "metadata": {"source": "CRF", "title": "Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted Nuclear Norm", "authors": ["Canyi Lu", "Jinhui Tang", "Shuicheng Yan", "Zhouchen Lin"], "emails": ["canyilu@gmail.com;", "eleyans@nus.edu.sg).", "jinhuitang@mail.njust.edu.cn).", "zlin@pku.edu.cn)."], "sections": [{"heading": null, "text": "1 Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted Nuclear Norm\nCanyi Lu, Jinhui Tang, Senior Member, IEEE, Shuicheng Yan, Senior Member, IEEE, and Zhouchen Lin, Senior Member, IEEE\nThe nuclear norm is widely used as a convex surrogate of the rank function in compressive sensing for low rank matrix recovery with its applications in image recovery and signal processing. However, solving the nuclear norm based relaxed convex problem usually leads to a suboptimal solution of the original rank minimization problem. In this paper, we propose to perform a family of nonconvex surrogates of L0-norm on the singular values of a matrix to approximate the rank function. This leads to a nonconvex nonsmooth minimization problem. Then we propose to solve the problem by Iteratively Reweighted Nuclear Norm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular Value Thresholding (WSVT) problem, which has a closed form solution due to the special properties of the nonconvex surrogate functions. We also extend IRNN to solve the nonconvex problem with two or more blocks of variables. In theory, we prove that IRNN decreases the objective function value monotonically, and any limit point is a stationary point. Extensive experiments on both synthesized data and real images demonstrate that IRNN enhances the low-rank matrix recovery compared with state-of-the-art convex algorithms.\nIndex Terms\u2014Nonconvex low rank minimization, Iteratively reweighted nuclear norm algorithm\nI. INTRODUCTION\nBENEFITING from the success of Compressive Sensing(CS) [2], the sparse and low rank matrix structures have attracted considerable research interests from the computer vision and machine learning communities. There have been many applications which exploit these two structures. For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9]. Conventional CS recovery uses the L1-norm, i.e., \u2016x \u20161 =\u2211 i |xi|, as the surrogate of the L0-norm, i.e., \u2016x \u20160 = #{xi 6= 0}, and the resulting convex problem can be solved by fast first-order solvers [10], [11]. Though for certain problems, the L1-minimization is equivalent to the L0-minimization under certain incoherence conditions [12], the obtained solution by L1-minimization is usually suboptimal to the original L0minimization since the L1-norm is a loose approximation of the L0-norm. This motivates to approximate the L0-norm by nonconvex continuous surrogate functions. Many known\nC. Lu and S. Yan are with the Department of Electrical and Computer Engineering, National University of Singapore, Singapore (e-mail: canyilu@gmail.com; eleyans@nus.edu.sg).\nJ. Tang is with the School of Computer Science, Nanjing University of Science and Technology, China (e-mail: jinhuitang@mail.njust.edu.cn).\nZ. Lin is with the Key Laboratory of Machine Perception (MOE), School of EECS, Peking University, China (e-mail: zlin@pku.edu.cn).\nThis paper is an extended version of [1] published in CVPR 2014.\nnonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20]. We summarize their definitions in Table I and visualize them in Figure 1. Numerical studies [21], [22] have shown that the nonconvex sparse optimization usually outperforms convex models in the areas of signal recovery, error correction and image processing.\nThe low rank structure of a matrix is the sparsity defined on its singular values. A particularly interesting model is the low rank matrix recovery problem\nmin X\n\u03bbrank(X) + 1\n2 ||A(X)\u2212 b||2F , (1)\nwhere A is a linear mapping and \u03bb > 0. The above low rank minimization problem arises in many computer vision tasks such as multiple category classification [23], matrix completion [24], multi-task learning [25] and low-rank representation with squared loss for subspace segmentation [7]. Similar to the L0-minimization, the rank minimization problem (1) is also challenging to solve. Thus, the rank function is usually replaced by the convex nuclear norm, \u2016X \u2016\u2217 = \u2211 i \u03c3i(X), where \u03c3i(X)\u2019s denote the singular values of X. This leads to a relaxed convex formulation of (1):\nmin X\n\u03bb\u2016X \u2016\u2217 + 1\n2 ||A(X)\u2212 b||2F . (2)\nThe above convex problem can be efficiently solved by many known solvers [26], [27]. However, the obtained solution by solving (2) is usually suboptimal to (1) since the nuclear norm is also a loose approximation of the rank function. Such a phenomenon is similar to the difference between L1-norm\nar X\niv :1\n51 0.\n06 89\n5v 1\n[ cs\n.L G\n] 2\n3 O\nct 2\n01 5\nand L0-norm for sparse vector recovery. However, different from the nonconvex surrogates of L0-norm, the nonconvex rank surrogates and the optimization solvers have not been well studied before.\nIn this paper, to achieve a better approximation of the rank function, we extend the nonconvex surrogates of L0-norm shown in Table I onto the singular values of the matrix, and show how to solve the following general nonconvex nonsmooth low rank minimization problem [1]\nmin X\u2208Rm\u00d7n F (X) = m\u2211 i=1 g(\u03c3i(X)) + f(X), (3)\nwhere \u03c3i(X) denotes the i-th singular value of X \u2208 Rm\u00d7n (we assume that m \u2264 n in this work). The penalty function g and loss function f satisfy the following assumptions: A1 g : R+ \u2192 R+ is continuous, concave and monotonically increasing on [0,\u221e). It is possibly nonsmooth. A2 f : Rm\u00d7n \u2192 R+ is a smooth function of type C1,1, i.e.,\nthe gradient is Lipschitz continuous,\n||\u2207f(X)\u2212\u2207f(Y)||F \u2264 L(f)||X\u2212Y||F , (4)\nfor any X,Y \u2208 Rm\u00d7n, L(f) > 0 is called Lipschitz constant of \u2207f . f(X) is possibly nonconvex.\nNote that problem (3) is very general. All the nonconvex surrogates g of L0-norm in Table I satisfy the assumption A1. So \u2211m i=1 g(\u03c3i(X)) is the nonconvex surrogate of the rank function1. It is expected that it approximates the rank function better than the convex nuclear norm. To see this more intuitively, we show the balls of constant penalties for a symmetric 2\u00d72 matrix in Figure 2. For the loss function f in assumption A2, the most widely used one is the squared loss 1 2\u2016A(X)\u2212 b\u2016 2 F .\nThere are some related work which consider the nonconvex rank surrogates. But they are different from this work. The work [28], [29] extend the Lp-norm of a vector to the Schattenp norm (0 < p < 1) and use the iteratively reweighted least squares (IRLS) algorithm to solve the nonconvex rank minimization problem with affine constraint. IRLS is also applied for the unconstrained problem with the smoothed Schatten-p norm regularizer [30]. However, the obtained solution by IRLS\n1Note that the singular values of a matrix are always nonegative. So we only consider the nonconvex g definted on R+.\n3 may not be naturally of low rank, or it may require a lot of iterations to get a low rank solution. One may perform the singular value thresholding appropriately to achieve a low rank solution, but there has no theoretically sound rule to suggest a correct threshold. Another nonconvex rank surrogate is the truncated nuclear norm [31]. Their proposed alternating updating optimization algorithm may not be efficient due to double loops of iterations and cannot be applied to solve (3). The nonconvex low rank matrix completion problem considered in [32] is a special case of our problem (3). Our solver shown later for (3) is also much more general. The work [33] uses the nonconvex log-det heuristic in [34] for image recovery. But their augmented Lagrangian multiplier based solver lacks of the convergence guarantee. A possible method to solve (3) is the proximal gradient algorithm [35], which requires to compute the proximal mapping of the nonconvex function g. However, computing the proximal mapping requires solving a nonconvex problem exactly. To the best of our knowledge, without additional assumptions on g (e.g., the convexity of \u2207g [35]), there does not exist a general solver for computing the proximal mapping of the general nonconvex g in assumption A1.\nIn this work, we observe that all the existing nonconvex surrogates in Table I are concave and monotonically increasing on [0,\u221e). Thus their gradients (or supergradients at the nonsmooth points) are nonnegative and monotonically decreasing. Based on this key fact, we propose an Iteratively Reweighted Nuclear Norm (IRNN) algorithm to solve (3). It computes the proximal operator of the weighted nuclear norm, which has a closed form solution due to the nonnegative and monotonically decreasing supergradients. The cost is the same as the computing of singular value thresholding which is widely used in convex nuclear norm minimization. In theory, we prove that IRNN monotonically decreases the objective function value and any limit point is a stationary point.\nFurthermore, note that problem (3) contains only one block of variable. But there are also some work which aim at finding several low rank matrices simultaneously, e.g., [36]. So we further extend IRNN to solve the following problem with p \u2265 2 blocks of variables\nmin X F (X) = p\u2211 j=1 mj\u2211 i=1 gj(\u03c3i(Xj)) + f(X), (5)\nwhere X = {X1, \u00b7 \u00b7 \u00b7 ,Xp}, Xj \u2208 Rmj\u00d7nj (assume mj \u2264 nj), gj\u2019s satisfy the assumption A1, and \u2207f is Lipschitz continuous defined as follows.\nDefinition 1: Let f : Rn1\u00d7\u00b7 \u00b7 \u00b7\u00d7Rnp \u2192 R be differentiable. Then \u2207f is called Lipschitz continuous if there exist Li(f) > 0, i = 1, \u00b7 \u00b7 \u00b7 , n, such that\n|f(x)\u2212f(y)\u2212\u3008\u2207f(y),x\u2212y\u3009| \u2264 n\u2211 i=1 Li(f) 2 \u2016xi\u2212yi\u201622, (6)\nfor any x = [x1; \u00b7 \u00b7 \u00b7 ; xn] and y = [y1; \u00b7 \u00b7 \u00b7 ; yn] with xi,yi \u2208 Rni . We call Li(f)\u2019s as Lipschitz constants of \u2207f . Note that the Lipschitz continuity of the multivariable function is crucial for the extension of IRNN for (5). This definition is completely new and it is different from the one block variable\ncase defined in (4). For n = 1, (6) holds if (4) holds (Lemma 1.2.3 in [37]). This motivates the above definition. But note that (4) does not guarantee to hold based on (6). So the definition of the Lipschitz continuity of the multivariable function is different from (4). This makes the extension of IRNN for problem (5) nontrivial. A widely used function which satisfies (6) is f(x) = 12 \u2016 \u2211m i=1 Ai xi\u2212b\u2016 2 2 . Its Lipschitz constants are Li(f) = m\u2016Ai\u201622, i = 1, \u00b7 \u00b7 \u00b7 , n, where \u2016Ai\u20162 denotes the spectral norm of matrix Ai. This is easy to verified by using the property \u2016 \u2211m i=1 Ai(xi\u2212yi)\u2016 2 2 \u2264 m \u2016Ai(xi\u2212yi)\u2016 2 2 \u2264 m\u2016Ai\u201622\u2016xi\u2212yi \u201622, where yi\u2019s are of compatible size. In theory, we prove that IRNN for (5) also has the convergence guarantee. In practice, we propose a new nonconvex low rank tensor representation problem which is a special case of (5) for subspace clustering. The results demonstrate the effectiveness of nonconvex models over the convex counterpart.\nIn summary, the contributions of this paper are as follows. \u2022 Motivated from the nonconvex surrogates g of L0-norm\nin Table I, we propose to use a new family of nonconvex surrogates \u2211m i=1 g(\u03c3i(X)) to approximate the rank function. Then we propose the Iteratively Reweighted Nuclear Norm (IRNN) method to solve the nonconvex nonsmooth low rank minization problem (3). \u2022 We further extend IRNN to solve the nonconvex nonsmooth low rank minimization problem (5) with p \u2265 2 blocks of variables. Note that such an extension is nontrivial based on our new definition of Lipschitz continuity of the multivariable function in (6). In theory, we prove that IRNN converges with decreasing objective function values and any limit point is a stationary point. \u2022 For applications, we apply the nonconvex low rank models on image recovery and subspace clustering. Extensive experiments on both synthesized and real-world data well demonstrate the effectiveness of the nonconvex models.\nThe remainder of this paper is organized as follows: Section II presents the IRNN method for solving problem (3). Section III extends IRNN for solving problem (5) and provides the convergence analysis. The experimental results are presented in Section IV. Finally, we conclude this paper in Section V."}, {"heading": "II. NONCONVEX NONSMOOTH LOW-RANK MINIMIZATION", "text": "In this section, we show how to solve the general problem (3). Note that g in (3) is not necessarily smooth. An known example is the Capped L1 norm, see Figure 1. To handle the nonsmooth penalty g, we first introduce the concept of supergradient defined on the concave function.\nA. Supergradient of a Concave Function\nIf g is convex but nonsmooth, its subgradient u at x is defined as\ng(x) + \u3008u,y\u2212x\u3009 \u2264 g(y). (7)\nIf g is concave and differentiable at x, it is known that\ng(x) + \u3008\u2207g(x),y\u2212x\u3009 \u2265 g(y). (8)\nInspired by (8), we can define the supergradient of concave g at the nonsmooth point x [38].\n4  1 1 1g( ) T x v x x\n1x 2x\n 2 3 2g( ) T x v x x\n 2 2 2g( ) T x v x x\ng( )x\nFig. 3: Supergraidients of a concave function. v1 is a supergradient at x1, and v2 and v3 are supergradients at x2.\nDefinition 2: Let g : Rn \u2192 R be concave. A vector v is a supergradient of g at the point x \u2208 Rn if for every y \u2208 Rn, the following inequality holds\ng(x) + \u3008v,y\u2212x\u3009 \u2265 g(y). (9)\nThe supergradient at a nonsmooth point may not be unique. All supergradients of g at x are called the superdifferential of g at x. We denote the set of all the supergradients at x as \u2202g(x). If g is differentiable at x, then \u2207g(x) is the unique supergradient, i.e., \u2202g(x) = {\u2207g(x)}. Figure 3 illustrates the supergradients of a concave function at both differentiable and nondifferentiable points.\nFor concave g, \u2212g is convex, and vice versa. From this fact, we have the following relationship between the supergradient of g and the subgradient of \u2212g.\nLemma 1: Let g(x) be concave and h(x) = \u2212g(x). For any v \u2208 \u2202g(x), u = \u2212v \u2208 \u2202h(x), and vice versa.\nIt is trivial to prove the above fact by using (7) and (9). The relationship of the supergradient and subgradient shown in Lemma 1 is useful for exploring some properties of the supergradient. It is known that the subdiffierential of a convex function h is a monotone operator, i.e.,\n\u3008u\u2212 v,x\u2212y\u3009 \u2265 0, (10)\nfor any u \u2208 \u2202h(x), v \u2208 \u2202h(y). Now we show that the superdifferential of a concave function is an antimonotone operator.\nLemma 2: The superdifferential of a concave function g is an antimonotone operator, i.e.,\n\u3008u\u2212 v,x\u2212y\u3009 \u2264 0, (11)\nfor any u \u2208 \u2202g(x) and v \u2208 \u2202g(y). The above result can be easily proved by Lemma 1 and (10).\nThe antimonotone property of the supergradient of concave function in Lemma 2 is important in this work. Suppose that g : R\u2192 R satisfies the assumption A1, then (11) implies that\nu \u2265 v, for any u \u2208 \u2202g(x) and v \u2208 \u2202g(y), (12)\nwhen x \u2264 y. That is to say, the supergradient of g is monotonically decreasing on [0,\u221e). The supergradients of some usual concave functions are shown in Table I. We also visualize them in Figure 1. Note that for the Lp penalty, we further define that \u2202g(0) = +\u221e. This will not affect our algorithm and convergence analysis as shown later. The Capped L1 penalty is nonsmooth at \u03b8 = \u03b3 with its superdifferential \u2202g(\u03b3) = [0, \u03bb].\nB. Iteratively Reweighted Nuclear Norm Algorithm\nIn this subsection, based on the above concept of the supergradient of concave function, we show how to solve the general nonconvex and possibly nonsmooth problem (3). For the simplicity of notation, we denote \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3m as the singular values of X. The variable X in the k-th iteration is denoted as Xk and \u03c3ki = \u03c3i(X\nk) is the i-th singular value of Xk.\nIn assumption A1, g is concave on [0,\u221e). So, by the definition (9) of the supergradient, we have\ng(\u03c3i) \u2264 g(\u03c3ki ) + wki (\u03c3i \u2212 \u03c3ki ), (13)\nwhere\nwki \u2208 \u2202g(\u03c3ki ). (14)\nSince \u03c3k1 \u2265 \u03c3k2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3km \u2265 0, by the antimonotone property of supergradient (12), we have\n0 \u2264 wk1 \u2264 wk2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 wkm. (15)\nIn (15), the nonnegativeness of wki \u2019s is due to the monotonically increasing property of g in assumption A1. As we will see later, property (15) plays an important role for solving the subproblem of our proposed IRNN.\nMotivated by (13), we may use its right hand side as a surrogate of g(\u03c3i) in (3). Thus we may solve the following relaxed problem to update Xk+1:\nXk+1 = arg min X m\u2211 i=1 g(\u03c3ki ) + w k i (\u03c3i \u2212 \u03c3ki ) + f(X)\n= arg min X m\u2211 i=1 wki \u03c3i + f(X).\n(16)\nProblem (16) is a weighted nuclear norm regularized problem. The updating rule (16) can be regarded as an extension of the Iteratively Reweighted L1 (IRL1) algorithm [21] for the weighted L1-norm problem\nmin x m\u2211 i=1 wki |xi|+ l(x). (17)\nHowever, the weighted nuclear norm in (16) is nonconvex (it is convex if and only if wk1 \u2265 wk2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 wkm \u2265 0 [39]), while the weighted L1-norm in (17) is convex. For convex f in (16) and l in (17), solving the nonconvex problem (16) is much more challenging than the convex weighted L1-norm problem. In fact, it is not easier than solving the original problem (3).\nInstead of updating Xk+1 by solving (16), we linearize f(X) at Xk and add a proximal term:\nf(X) \u2248 f(Xk)+ \u3008\u2207f(Xk),X\u2212Xk\u3009+ \u00b5 2 ||X\u2212Xk||2F , (19)\nwhere \u00b5 > L(f). Such a choice of \u00b5 guarantees the convergence of our algorithm as shown later. Then we use the right hand sides of (13) and (19) as surrogates of g and f in (3),\n5 Algorithm 1 Solving problem (3) by IRNN Input: \u00b5 > L(f) - A Lipschitz constant of \u2207f . Initialize: k = 0, Xk, and wki , i = 1, \u00b7 \u00b7 \u00b7 ,m. Output: X\u2217. while not converge do\n1) Update Xk+1 by solving problem (20). 2) Update the weights wk+1i , i = 1, \u00b7 \u00b7 \u00b7 ,m, by\nwk+1i \u2208 \u2202g ( \u03c3i(X k+1) ) . (18)\nend while\nand update Xk+1 by solving\nXk+1 = arg min X m\u2211 i=1 g(\u03c3ki ) + w k i (\u03c3i \u2212 \u03c3ki )\n+ f(Xk) + \u3008\u2207f(Xk),X\u2212Xk\u3009+ \u00b5 2 ||X\u2212Xk||2F\n= arg min X m\u2211 i=1 wki \u03c3i + \u00b5 2 \u2225\u2225\u2225\u2225X\u2212 (Xk \u2212 1\u00b5\u2207f(Xk) )\u2225\u2225\u2225\u22252 F .\n(20)\nSolving (20) is equivalent to computing the proximity operator of the weighted nuclear norm. Due to (15), the solution to (20) has a closed form despite that it is nonconvex.\nLemma 3: [39, Theorem 2.3] For any \u03bb > 0, Y \u2208 Rm\u00d7n and 0 \u2264 w1 \u2264 w2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 ws (s = min(m,n)), a globally optimal solution to the following problem\nmin\u03bb s\u2211 i=1 wi\u03c3i(X) + 1 2 ||X\u2212Y||2F , (21)\nis given by the Weighted Singular Value Thresholding (WSVT)\nX\u2217 = US\u03bbw(\u03a3)V T , (22)\nwhere Y = U\u03a3V T is the SVD of Y, and S\u03bbw(\u03a3) = Diag{(\u03a3ii \u2212 \u03bbwi)+}. From Lemma 3, it can be seen that to solve (20) by using (22), (15) plays an important role and it holds for all g satisfying the assumption A1. If g(x) = x, then \u2211m i=1 g(\u03c3i) reduces to the convex nuclear norm \u2016X \u2016\u2217. In this case, wki = 1 for all i = 1, \u00b7 \u00b7 \u00b7 ,m. Then WSVT reduces to the conventional Singular Value Thresholding (SVT) [40], which is an important subroutine in convex low rank optimization. The updating rule (20) then reduces to the known proximal gradient method [10].\nAfter updating Xk+1 by solving (20), we then update the weights wk+1i \u2208 \u2202g ( \u03c3i(X k+1) )\n, i = 1, \u00b7 \u00b7 \u00b7 ,m. Iteratively updating Xk+1 and the weights corresponding to its singular values leads to the proposed Iteratively Reweighted Nuclear Norm (IRNN) algorithm. The whole procedure of IRNN is shown in Algorithm 1. If the Lipschitz constant L(f) is not known or computable, the backtracking rule can be used to estimate \u00b5 in each iteration [10].\nIt is worth mentioning that for the Lp penalty, if \u03c3ki = 0, then wki \u2208 \u2202g(\u03c3ki ) = {+\u221e}. By the updating rule of X k+1\nin (20), we have \u03c3k+1i = 0. This guarantees that the rank of the sequence {Xk} is nonincreasing.\nIn theory, we can prove that IRNN converges. Since IRNN is a special case of IRNN with Parallel Splitting (IRNN-PS) in Section III, so we only give the convergence results of IRNNPS later.\nAt the end of this section, we would like to remark some more differences between previous work and ours. \u2022 Our IRNN and IRNN-PS for nonconvex low rank\nminimization are different from previous iteratively reweighted solvers for nonconvex sparse minimization, e.g., [21], [30]. The key difference is that the weighted nuclear norm regularized problem is nonconvex while the weighted L1-norm regularized problem is convex. This makes the convergence analysis different. \u2022 Our IRNN and IRNN-PS utilize the common properties instead of specific ones of the nonconvex surrogates of L0-norm. This makes them much more general than many previous nonconvex low rank solvers, e.g., [22], [31], [33], which target for some special nonconvex problems."}, {"heading": "III. EXTENSIONS OF IRNN AND THE CONVERGENCE ANALYSIS", "text": "In this section, we extend IRNN to solve two types of problems which are more general than (3). The first one is to solve some similar problems as (3) but with more general nonconvex penalties. The second one is to solve problem (5) which has p \u2265 2 blocks of variables.\nA. IRNN for the Problems with More General Nonconvex Penalties\nIRNN can be extended to solve the following problem\nmin X m\u2211 i=1 gi(\u03c3i(X)) + f(X), (23)\nwhere gi\u2019s are concave and their supergradients satisfy 0 \u2264 v1 \u2264 v2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 vm for any vi \u2208 \u2202gi(\u03c3i(X)), i = 1, \u00b7 \u00b7 \u00b7 ,m. The truncated nuclear norm ||X ||r = \u2211m i=r+1 \u03c3i(X) [31] is an interesting example. Indeed, let\ngi(x) = { 0, i = 1, \u00b7 \u00b7 \u00b7 , r, x, i = r + 1, \u00b7 \u00b7 \u00b7 ,m.\n(24)\nThen ||X ||r = \u2211m i=1 gi(\u03c3i(X)) and its supergradients is\n\u2202gi(x) = { 0, i = 1, \u00b7 \u00b7 \u00b7 , r, 1, i = r + 1, \u00b7 \u00b7 \u00b7 ,m.\n(25)\nCompared with the alternating updating algorithm in [31], which require double loops, our IRNN will be more efficient and with stronger convergence guarantee.\nB. IRNN for the Multi-Blocks Problem (5)\nThe multi-blocks problem (5) also has some applications in computer vision. An example is the Latent Low Rank Representation (LatLRR) problem [36]\nmin L,R \u2016L\u2016\u2217 + \u2016R\u2016\u2217 +\n\u03bb 2 \u2016L X + X R\u2212X \u20162F . (26)\n6 Here we propose a more general Tensor Low Rank Representation (TLRR) as follows\nmin Pj\u2208Rmj\u00d7mj p\u2211 j=1 \u03bbj\u2016Pj\u2016\u2217 + 1 2 \u2225\u2225\u2225\u2225\u2225\u2225X \u2212 p\u2211 j=1 X \u00d7jPj \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n, (27)\nwhere X \u2208 Rm1\u00d7\u00b7\u00b7\u00b7\u00d7mp is an p-way tensor and X \u00d7jPj denotes the j-mode product [41]. TLRR is an extension of LRR [7] and LatLRR. It can also be applied for subspace clustering, see Section IV. If we replace \u2016Pj\u2016\u2217 in (26) as\u2211mj i=1 gj(\u03c3i(Pj)) with gj\u2019s satisfying the assumption A1, then we have the Nonconvex TLRR (NTLRR) model which is a special case of (5).\nNow we show how to solve (5). Similar to (20), we update Xj , j = 1, \u00b7 \u00b7 \u00b7 , p, by\nXk+1j = arg min Xj mj\u2211 i=1 wkji\u03c3i(Xj) + \u3008\u2207jf(X k),Xj \u2212Xkj \u3009\n+ \u00b5j 2 \u2016Xj \u2212Xkj \u20162F , (28)\nwhere \u00b5j > Li(f), the notation \u2207jf denotes the gradient of f w.r.t. Xj , and\nwkji \u2208 \u2202gj(\u03c3i(X k j )). (29)\nNote that (28) and (29) can be computed in parallel for j = 1, \u00b7 \u00b7 \u00b7 , p. So we call such a method as IRNN with Parallel Splitting (IRNN-PS).\nC. Convergence Analysis\nIn this section, we give the convergence analysis of IRNNPS for (5). For the simplicity of notation, we denote \u03c3kji = \u03c3i(X k j ) as the i-th singular value of Xj in the k-th iteration.\nTheorem 1: In problem (5), assume that gj\u2019s satisfies the assumption A1 and \u2207f is Lipschitz continuous. Then the sequence {Xk} generated by IRNN-PS satisfies the following properties:\n(1) F (Xk) is monotonically decreasing. Indeed,\nF (Xk)\u2212F (Xk+1) \u2265 p\u2211 j=1 \u00b5j \u2212 Lj(f) 2 ||Xkj\u2212X k+1 j ||2F \u2265 0;\n(2) lim k\u2192+\u221e\n(Xk \u2212Xk+1) = 0;\nProof. First, since Xk+1j is optimal to (28), we have\nm\u2211 i=1 wkji\u03c3 k+1 ji + \u3008\u2207jf(X k),Xk+1j \u2212X k j \u3009+ \u00b5j 2 ||Xk+1j \u2212X k j ||2F\n\u2264 m\u2211 i=1 wkji\u03c3 k ji + \u3008\u2207jf(X k),Xkj \u2212X k j \u3009+ \u00b5j 2 ||Xkj \u2212X k j ||2F .\nIt can be rewritten as\n\u3008\u2207jf(Xk),Xkj \u2212X k+1 j \u3009\n\u2265 \u2212 m\u2211 i=1 wkji(\u03c3 k ji \u2212 \u03c3k+1ji ) + \u00b5j 2 ||Xk \u2212Xk+1||2F .\nSecond, since \u2207f is Lipschitz continuous, by (6), we have\nf(Xk)\u2212 f(Xk+1)\n\u2265 p\u2211 j=1 ( \u3008\u2207jf(Xk),Xkj \u2212X k+1 j \u3009 \u2212 Lj(f) 2 ||Xkj \u2212X k+1 j ||2F ) .\nThird, by (29) and (9), we have\ngj(\u03c3 k ji)\u2212 gj(\u03c3k+1ji ) \u2265 w k ji(\u03c3 k ji \u2212 \u03c3k+1ji ).\nSumming the above three equations for all j and i leads to\nF (Xk)\u2212 F (Xk+1)\n= p\u2211 j=1 nj\u2211 i=1 ( gj(\u03c3 k ji)\u2212 g(\u03c3k+1ji ) ) + f(Xk)\u2212 f(Xk+1)\n\u2265 p\u2211 j=1 \u00b5j \u2212 Lj(f) 2 ||Xk+1j \u2212X k j ||2F \u2265 0.\nThus F (Xk) is monotonically decreasing. Summing the above inequality for k \u2265 1, we get\nF (X1) \u2265 p\u2211 j=1 \u00b5j \u2212 Lj(f) 2 +\u221e\u2211 k=1 ||Xk+1j \u2212X k j ||2F ,\nThis implies that lim k\u2192+\u221e (Xk \u2212Xk+1) = 0. Theorem 2: In problem (5), assume F (X) \u2192 +\u221e iff ||X ||F \u2192 +\u221e. Then any accumulation point X\u2217 of {Xk} generated by IRNN-PS is a stationary point to (5). Proof. Due to the above assumption, {Xk} is bounded. Thus there exists a matrix X\u2217 and a subsequence {Xkt} such that Xkt \u2192 X\u2217. Note that Xk\u2212Xk+1 \u2192 0 in Theorem 1, we have Xkj+1 \u2192 X\u2217. Thus \u03c3i(Xkt+1j ) \u2192 \u03c3i(X \u2217 j ) for j = 1, \u00b7 \u00b7 \u00b7 , p and i = 1, \u00b7 \u00b7 \u00b7 , nj . By Lemma 1, wktji \u2208 \u2202gj(\u03c3i(X kt j ))\nimplies that \u2212wktji \u2208 \u2202 ( \u2212gj(\u03c3i(Xktj )) ) . From the upper semi-continuous property of the subdifferential [42, Proposition 2.1.5], there exists \u2212w\u2217ji \u2208 \u2202 ( \u2212gj(\u03c3i(X\u2217j )) ) such that \u2212wktji \u2192 \u2212w\u2217ji. Again by Lemma 1, w\u2217ji \u2208 \u2202gj(\u03c3i(X \u2217 j )) and wktji \u2192 w\u2217ji. Denote h(Xj ,wj) = \u2211nj i=1 wji\u03c3i(Xj). Since X kt+1 j is optimal to (28), there exists Gkt+1j \u2208 \u2202h(X kt+1 j ,w kt j ), such that\nGkt+1j +\u2207jf(X kt) + \u00b5j(X kt+1 j \u2212X kt j ) = 0. (30)\nLet t \u2192 +\u221e in (30). Then there exists G\u2217j \u2208 \u2202h(X \u2217 j ,w \u2217 j ), such that 0 = G\u2217j +\u2207jf(X \u2217) \u2208 \u2202jF (X\u2217). (31)\nThus X\u2217 is a stationary point to (5)."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we present several experiments to demonstrate that the models with nonconvex rank surrogates outperform the ones with convex nuclear norm. We conduct three experiments. The first two aim to examine the convergence behavior of IRNN for the matrix completion problem [43] on both synthetic data and real images. The last experiment\nis tested on the tensor low rank representation problem (27) solved by IRNN-PS for face clustering.\nFor the first two experiments, we consider the nonconvex low rank matrix completion problem\nmin X m\u2211 i=1 g(\u03c3i(X)) + 1 2 ||P\u2126(X\u2212M)||2F , (32)\nwhere \u2126 is the set of indices of samples, and P\u2126 : Rm\u00d7n \u2192 Rm\u00d7n is a linear operator that keeps the entries in \u2126 unchanged and those outside \u2126 zeros. The gradient of squared loss function in (32) is Lipschitz continuous, with a Lipschitz constant L(f) = 1. We set \u00b5 = 1.1 in IRNN. For the choice of g, we use five nonconvex surrogates in Table I, including Lpnorm, SCAD, Logarithm, MCP and ETP. The other three nonconvex surrogates, including Capped L1, Geman and Laplace, are not used since we find that their recovery performances are very sensitive to the choices of \u03b3 and \u03bb in different cases. For the choice of \u03bb in g, we use a continuation technique to enhance the low rank matrix recovery. The initial value of \u03bb is set to a larger value \u03bb0, and dynamically decreased by \u03bb = \u03b7k\u03bb0 with \u03b7 < 1. It is stopped till reaching a predefined target \u03bbt. X is initialized as a zero matrix. For the choice of parameters (e.g., p and \u03b3) in g, we search them from a candidate set and use the one which obtains good performance in most cases.\nA. Low Rank Matrix Recovery on the Synthetic Data We first compare the low rank matrix recovery performances of nonconvex model (32) with the convex one by using nuclear\nnorm [9] on the synthetic data. We conduct two tasks. The first one is tested on the observed matrix M without noises, while the other one is tested on M with noises.\nFor the noise free case, we generate the rank r matrix M as ML MR, where ML \u2208 R150\u00d7r, and MR \u2208 Rr\u00d7150 are generated by the Matlab command randn. We randomly set 50% elements of M to be missing. The Augmented Lagrange Multiplier (ALM) [44] method is used to solve the noise free problem\nmin X ||X ||\u2217 s.t. P\u2126(X) = P\u2126(M). (33)\nThe default parameters of in the released codes2 of ALM are used. For problem (32), it is solved by IRNN with the parameters \u03bb0 = ||P\u2126(M)||\u221e, \u03bbt = 10\u22125\u03bb0 and \u03b7 = 0.7. The algorithm is stopped when ||P\u2126(X\u2212M)||F \u2264 10\u22125. The matrix recovery performance is evaluated by the Relative Error defined as\nRelative Error = ||X\u0302\u2212M ||F ||M ||F , (34)\nwhere X\u0302 is the recovered matrix by different algorithms. If the Relative Error is smaller than 10\u22123, then X\u0302 is regarded as a successful recovery of M. For each r, we repeat the experiments s = 100 times. Then we define the Frequency of Success = s\u0302s , where s\u0302 is the times of successful recovery. We also vary the underlying rank r of M from 20 to 33 for each algorithm. We show the frequency of success\n2Code: http://perception.csl.illinois.edu/matrix-rank/sample code.html.\n8 Image recovery by APGL lp\nImage recovery by APGL lp\nImage recovery by APGL\nImage recovery by APGL lp\nImage recovery by APGL lp\n(a) Original (b) Noisy image Image recovery by APGL (c) APGL lp (d) IRNN-Lp\nFig. 6: Comparison of image recovery on more images. (a) Original images. (b) Images with noises. Recovered images by (c) APGL and (d) IRNN-Lp. Best viewed in \u00d72 sized color pdf file.\nin Figure 4a. The legend IRNN-Lp in Figure 4a denotes the model (32) with Lp penalty solved by IRNN. It can be seen that IRNN for (32) with nonconvex rank surrogates significantly outperforms ALM for (33) with convex rank surrogate. This is because the nonconvex surrogates approximate the rank function much better than the convex nuclear norm. This also verifies that our IRNN achieves good solutions of (32), though its optimal solutions are in general not computable.\nFor the second task, we assume that the observed matrix M is noisy. It is generated by P\u2126(M) = P\u2126(ML MR)+0.1\u00d7randn. We compare IRNN for (32) with convex Accelerated Proximal Gradient with Line search (APGL)3 [24] which solves the noisy problem\nmin X\n\u03bb||X ||\u2217 + 1\n2 ||P\u2126(X)\u2212 P\u2126(M)||2F . (35)\nFor this task, we set \u03bb0 = 10||P\u2126(M)||\u221e and \u03bbt = 0.1\u03bb0 in IRNN. We run the experiments for 100 times and the underlying rank r is varying from 15 and 35. For each test, we compute the relative error in (34). Then we show the mean relative error over 100 tests in Figure 4c. Similar to\n3Code: http://www.math.nus.edu.sg/\u223cmattohkc/NNLS.html.\nthe noise free case, IRNN with nonconvex rank surrogates achieves much smaller recovery error than APGL for convex problem (35).\nIt is worth mentioning that though Logarithm seems to perform better than other nonconvex penalties for low rank matrix completion from Figure 4. It is still not clear which one is the best rank surrogate since the obtained solutions are not globally optimal. Answering this question is beyond the scope of this work.\nFigure 4b shows the running times of the compared methods. It can be seen that IRNN is slower than the convex ALM. This is due to the reinitialization of IRNN when using the continuation technique. Figure 4d plots the objective function values in each iterations of IRNN with different nonconvex penalties. As verified in theory, it can be seen that the values are decreasing.\nB. Application to Image Recovery\nIn this section, we apply the low rank matrix completion models (35) and (3) for image recovery. We follow the experimental settings in [31]. Here we consider two types of noises on the real images. The first one replaces 50% of pixels with random values (sample image (1) in Figure 5b). The other one adds some unrelated texts on the image (sample image (2) in Figure 5b). The goal is to remove the noises by using low rank matrix completion. Actually, the real images may not be of low-rank. But their top singular values dominate the main information. Thus, the image can be approximately recovered by a low-rank matrix. For the color image, there are three channels. Matrix completion is applied for each channel independently. We compare IRNN with some stateof-the-art methods on this task, including APGL, Low-Rank Matrix Fitting (LMaFit)4 [45] and Truncated Nuclear Norm Regularization (TNNR)5 [31]. For the obtained solution, we evaluate its quality by the Peak Signal-to-Noise Ratio (PSNR) and the relative error (34).\nFigure 5 (c)-(g) show the recovered images by different methods. It can be seen that our IRNN method for nonconvex models achieve much better recovery performance than APGL and LMaFit. The performances of low rank models (3) using different nonconvex surrogates are quite similar, so we only show the results by IRNN-Lp and IRNN-SCAD due to the limit of space. Some more results are shown in Figure 6. Figure 7 shows the PSNR values, relative errors and running time of different methods on all the tested images. It can be seen that IRNN with all the evaluated nonconvex functions achieves higher PSNR values and smaller relative error. This verifies that the nonconvex penalty functions are effective in this situation. The nonconvex truncated nuclear norm is close to our methods, but its running time is 3\u223c5 times of ours.\nC. Tensor Low-Rank Representation\nIn this section, we consider to use the Tensor Low-Rank Representation (TLRR) (27) for face clustering [46], [36].\n4Code: http://lmafit.blogs.rice.edu/. 5Code: https://sites.google.com/site/zjuyaohu/.\nProblem (27) can be solved by the Accelerated Proximal Gradient (APG) [10] method with the optimal convergence rate O(1/K2), where K is the number of iterations. The corresponding Nonconvex TLRR (NTLRR) related to (27) is\nmin Pj\u2208Rmj\u00d7mj p\u2211 j=1 mj\u2211 i=1 g(\u03c3i(Pj)) + 1 2 \u2225\u2225\u2225\u2225\u2225\u2225X \u2212 p\u2211 j=1 X \u00d7jPj \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n,\n(36) where we use the Logarithm function g in Table I, since we find it achieves the best performance in the previous experiments. Problem (36) has more than one block of variable, and thus it can be solved by IRNN-PS.\nIn this experiment, we use TLRR and NTLRR for face clustering. Assume that we are given m3 face images from k subjects with size m1 \u00d7m2. Then we can construct an 3- way tensor X \u2208 Rm1\u00d7m2\u00d7m3 . After solving (27) or (36), we follow the settings in [46] to construct the affinity matrix by W = (|P3 |+|PT3 |)/2. Finally, the Normalized Cuts (NCuts) [47] is applied based on W to segment the data into k groups.\nTwo challenging face databases, Extended Yale B [48] and UMIST6, are used for this test. Some sample face images are shown in Figure 8. Extended Yale B consists of 2,414 frontal face images of 38 subjects under various lighting, poses and illumination conditions. Each subject has 64 faces. We construct two clustering tasks based on the first 5 and 10 subjects face images of this database. The UMIST database contains 564 images of 20 subjects, each covering a range of poses from profile to frontal views. All the images in UMIST are used for clustering. For both databases, the images are resized into m1 \u00d7m2 = 28\u00d7 28.\n6http://www.cs.nyu.edu/\u223croweis/data.html.\nTable II shows the face clustering accuracies of NTLRR, compared with LRR, LatLRR and TLRR. The performances of LRR and LatLRR are consistent with previous work [46], [36]. Also, it can be seen that TLRR achieve better performance than LRR and LatLRR, since it exploits the inherent spatial structures among samples. More importantly, NTLRR futher improves TLRR. Such an improvement is similar to those in previous experiments, though the support in theory is still open."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "This work targeted for nonconvex low rank matrix recovery by applying the nonconvex surrogates of L0-norm on the singular values to approximate the rank function. We observed that all the existing nonconvex surrogates are concave and monotonically increasing on [0,\u221e). Then we proposed a general solver IRNN to solve the nonconvex nonsmooth low rank minimization problem (3). We also extend IRNN to solve problem (5) with multi-blocks of variables. In theory, we proved that any limit point is a stationary point. Experiments on both synthetic data and real data demonstrated that IRNN usually outperforms the state-of-the-art convex algorithms.\nThere are some interesting future work. First, it is still unclear which nonconvex surrogate is the best. It is possible to provide some support in theory under some conditions. Second, one may consider to use the alternating direction method of multiplier to solve the nonconvex problem with the affine constraint and to prove the convergence. Second, one may consider to solve the following problem by IRNN\nmin X m\u2211 i=1 g(h(\u03c3i(X))) + f(X), (37)\nwhen g(y) is concave and the following problem\nmin X\nwih(\u03c3i(X)) + ||X\u2212Y||2F , (38)\n10\ncan be cheaply solved. An interesting application of (37) is to extend the group sparsity on the singular values. By dividing the singular values into k groups, i.e., G1 = {1, \u00b7 \u00b7 \u00b7 , r1}, G2 = {r1 + 1, \u00b7 \u00b7 \u00b7 , r1 + r2 \u2212 1}, \u00b7 \u00b7 \u00b7 , Gk = { \u2211k\u22121 i ri +\n1, \u00b7 \u00b7 \u00b7 ,m}, where \u2211 i ri = m, we can define the group spar-\nsity on the singular values as ||X ||2,g = \u2211k i=1 g(||\u03c3Gi ||2). This is exactly the first term in (37) by letting h be the L2norm of a vector. g can be nonconvex functions satisfying the assumption A1 or specially the absolute convex function."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This research is supported by the Singapore National Research Foundation under its International Research Centre @Singapore Funding Initiative and administered by the IDM Programme Office. Z. Lin is supported by NSF of China (Grant nos. 61272341, 61231002, and 61121002) and MSRA."}], "references": [{"title": "Generalized nonconvex nonsmooth low-rank minimization", "author": ["Canyi Lu", "Jinhui Tang", "Shuicheng Yan", "Zhouchen Lin"], "venue": "CVPR. IEEE, 2014, pp. 4130\u20134137.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to compressive sampling", "author": ["Emmanuel J Cand\u00e8s", "Michael B Wakin"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 21\u201330, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "TPAMI, vol. 31, no. 2, pp. 210\u2013227, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Locality-constrained linear coding for image classification", "author": ["Jinjun Wang", "Jianchao Yang", "Kai Yu", "Fengjun Lv", "Thomas Huang", "Yihong Gong"], "venue": "CVPR. IEEE, 2010, pp. 3360\u20133367.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Image super-resolution via sparse representation", "author": ["Jianchao Yang", "John Wright", "Thomas S Huang", "Yi Ma"], "venue": "TIP, vol. 19, no. 11, pp. 2861\u20132873, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X.D. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM, vol. 58, no. 3, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["Guangcan Liu", "Zhouchen Lin", "Shuicheng Yan", "Ju Sun", "Yong Yu", "Yi Ma"], "venue": "TPAMI, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Correlation adaptive subspace segmentation by trace lasso", "author": ["Canyi Lu", "Jiashi Feng", "Zhouchen Lin", "Shuicheng Yan"], "venue": "ICCV. IEEE, 2013, pp. 1345\u20131352.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational mathematics, vol. 9, no. 6, pp. 717\u2013772, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Imaging Sciences, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast solution of-norm minimization problems when the solution may be sparse", "author": ["David L Donoho", "Yaakov Tsaig"], "venue": "IEEE Transactions on Information Theory, vol. 54, no. 11, pp. 4789\u20134812, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "For most large underdetermined systems of linear equations the minimal `1-norm solution is also the sparsest solution", "author": ["David L Donoho"], "venue": "Communications on Pure and Applied Mathematics, vol. 59, no. 6, pp. 797\u2013829, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "A statistical view of some chemometrics regression tools", "author": ["LLdiko Frank", "Jerome Friedman"], "venue": "Technometrics, 1993.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["Jianqing Fan", "Runze Li"], "venue": "Journal of the American Statistical Association, 2001.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast sparse regression and classification", "author": ["Jerome Friedman"], "venue": "International Journal of Forecasting, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["Cunhui Zhang"], "venue": "The Annals of Statistics, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis of multi-stage convex relaxation for sparse regularization", "author": ["Tong Zhang"], "venue": "JMLR, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "A feasible nonconvex relaxation approach to feature selection", "author": ["Cuixia Gao", "Naiyan Wang", "Qi Yu", "Zhihua Zhang"], "venue": "AAAI, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonlinear image recovery with half-quadratic regularization", "author": ["Donald Geman", "Chengda Yang"], "venue": "TIP, 1995.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Highly undersampled magnetic resonance image reconstruction via homotopic `0-minimization", "author": ["Joshua Trzasko", "Armando Manduca"], "venue": "TMI, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E. Cand\u00e8s", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier Analysis and Applications, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved iteratively reweighted least squares for unconstrained smoothed \\ell q minimization", "author": ["Ming-Jun Lai", "Yangyang Xu", "Wotao Yin"], "venue": "SIAM Journal on Numerical Analysis, vol. 51, no. 2, pp. 927\u2013957, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Uncovering shared structures in multiclass classification", "author": ["Yonatan Amit", "Michael Fink", "Nathan Srebro", "Shimon Ullman"], "venue": "ICML, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["Kimchuan Toh", "Sangwoon Yun"], "venue": "Pacific Journal of Optimization, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "Machine Learning, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["K. Toh", "S. Yun"], "venue": "Pacific Journal of Optimization, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends\u00ae in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Iterative reweighted algorithms for matrix rank minimization", "author": ["K. Mohan", "M. Fazel"], "venue": "JMLR, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Low-rank matrix recovery via iteratively reweighted least squares minimization", "author": ["Massimo Fornasier", "Holger Rauhut", "Rachel Ward"], "venue": "SIAM Journal on Optimization, vol. 21, no. 4, pp. 1614\u20131640, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "An unconstrained `q minimization with 0 < q \u2264 1 for sparse solution of underdetermined linear systems", "author": ["Ming-Jun Lai", "Jingyue Wang"], "venue": "SIAM Journal on Optimization, vol. 21, no. 1, pp. 82\u2013101, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast and accurate matrix completion via truncated nuclear norm regularization", "author": ["Yao Hu", "Debing Zhang", "Jieping Ye", "Xuelong Li", "Xiaofei He"], "venue": "TPAMI, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic low-rank matrix completion with adaptive spectral regularization algorithms", "author": ["Adrien Todeschini", "Fran\u00e7ois Caron", "Marie Chavent"], "venue": "NIPS, 2013, pp. 845\u2013853.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Compressive sensing via nonlocal low-rank regularization", "author": ["Weisheng Dong", "Guangming Shi", "Xin Li", "Yi Ma", "Feng Huang"], "venue": "TIP, vol. 23, no. 8, pp. 3618\u20133632, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance matrices", "author": ["Maryam Fazel", "Haitham Hindi", "Stephen P Boyd"], "venue": "American Control Conference. IEEE, 2003, vol. 3, pp. 2156\u20132162.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Generalized singular value thresholding", "author": ["Canyi Lu", "Changbo Zhu", "Chunyan Xu", "Shuicheng Yan", "Zhouchen Lin"], "venue": "AAAI, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Latent low-rank representation for subspace segmentation and feature extraction", "author": ["Guangcan Liu", "Shuicheng Yan"], "venue": "ICCV. IEEE, 2011, pp. 1615\u20131622.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Introductory lectures on convex optimization: A basic course, vol", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2004}, {"title": "The supergradient of a concave function", "author": ["KC Border"], "venue": "http://www.hss. caltech.edu/\u223ckcb/Notes/Supergrad.pdf, 2001, [Online].", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "Reduced rank regression via adaptive nuclear norm penalization", "author": ["Kun Chen", "Hongbo Dong", "Kungsik Chan"], "venue": "Biometrika, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Jianfeng Cai", "Emmanuel Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization, 2010.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Tensor decompositions and applications", "author": ["Tamara G Kolda", "Brett W Bader"], "venue": "SIAM Review, vol. 51, no. 3, pp. 455\u2013500, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonsmooth analysis and optimization", "author": ["Frank Clarke"], "venue": "Proceedings of the International Congress of Mathematicians, 1983.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1983}, {"title": "Matrix completion with noise", "author": ["E.J. Cand\u00e8s", "Y. Plan"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 925\u2013936, 2010.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "The augmented lagrange multiplier method for exact recovery of a corrupted low-rank matrices", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "UIUC Technical Report UILU-ENG-09-2215, Tech. Rep., 2009.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Solving a low-rank factorization model for matrix completion by a nonlinear successive overrelaxation algorithm", "author": ["Zaiwen Wen", "Wotao Yin", "Yin Zhang"], "venue": "Mathematical Programming Computation, 2012.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "ICML, 2010.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Normalized cuts and image segmentation", "author": ["J.B. Shi", "J. Malik"], "venue": "TPAMI, vol. 22, no. 8, pp. 888\u2013905, 2000.  11", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 1, "context": "INTRODUCTION BENEFITING from the success of Compressive Sensing (CS) [2], the sparse and low rank matrix structures have attracted considerable research interests from the computer vision and machine learning communities.", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 184, "endOffset": 187}, {"referenceID": 6, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 209, "endOffset": 212}, {"referenceID": 7, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 214, "endOffset": 217}, {"referenceID": 8, "context": "For instance, sparse coding has been widely used for face recognition [3], image classification [4] and super-resolution [5], while low rank models are applied for background modeling [6], motion segmentation [7], [8] and collaborative filtering [9].", "startOffset": 246, "endOffset": 249}, {"referenceID": 9, "context": ", \u2016x \u20160 = #{xi 6= 0}, and the resulting convex problem can be solved by fast first-order solvers [10], [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": ", \u2016x \u20160 = #{xi 6= 0}, and the resulting convex problem can be solved by fast first-order solvers [10], [11].", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "Though for certain problems, the L1-minimization is equivalent to the L0-minimization under certain incoherence conditions [12], the obtained solution by L1-minimization is usually suboptimal to the original L0minimization since the L1-norm is a loose approximation of the L0-norm.", "startOffset": 123, "endOffset": 127}, {"referenceID": 0, "context": "This paper is an extended version of [1] published in CVPR 2014.", "startOffset": 37, "endOffset": 40}, {"referenceID": 12, "context": "Lp [13] \u03bb\u03b8 p { +\u221e, if \u03b8 = 0, \u03bbp\u03b8p\u22121, if \u03b8 > 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "SCAD [14] \uf8f1\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3 \u03bb\u03b8, if \u03b8 \u2264 \u03bb, \u2212\u03b82+2\u03b3\u03bb\u03b8\u2212\u03bb2 2(\u03b3\u22121) , if \u03bb < \u03b8 \u2264 \u03b3\u03bb, \u03bb2(\u03b3+1) 2 , if \u03b8 > \u03b3\u03bb.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "Logarithm [15] \u03bb log(\u03b3+1) log(\u03b3\u03b8 + 1) \u03b3\u03bb (\u03b3\u03b8+1) log(\u03b3+1)", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "MCP [16] \uf8f2\uf8f3\u03bb\u03b8 \u2212 \u03b8 2 2\u03b3 , if \u03b8 < \u03b3\u03bb,", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "Capped L1 [17] { \u03bb\u03b8, if \u03b8 < \u03b3, \u03bb\u03b3, if \u03b8 \u2265 \u03b3.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "ETP [18] \u03bb 1\u2212exp(\u2212\u03b3) (1 \u2212 exp(\u2212\u03b3\u03b8)) \u03bb\u03b3 1\u2212exp(\u2212\u03b3) exp(\u2212\u03b3\u03b8) Geman [19] \u03bb\u03b8 \u03b8+\u03b3 \u03bb\u03b3 (\u03b8+\u03b3)2 Laplace [20] \u03bb(1 \u2212 exp(\u2212 \u03b8 \u03b3 )) \u03bb \u03b3 exp(\u2212 \u03b8 \u03b3 )", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "ETP [18] \u03bb 1\u2212exp(\u2212\u03b3) (1 \u2212 exp(\u2212\u03b3\u03b8)) \u03bb\u03b3 1\u2212exp(\u2212\u03b3) exp(\u2212\u03b3\u03b8) Geman [19] \u03bb\u03b8 \u03b8+\u03b3 \u03bb\u03b3 (\u03b8+\u03b3)2 Laplace [20] \u03bb(1 \u2212 exp(\u2212 \u03b8 \u03b3 )) \u03bb \u03b3 exp(\u2212 \u03b8 \u03b3 )", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "ETP [18] \u03bb 1\u2212exp(\u2212\u03b3) (1 \u2212 exp(\u2212\u03b3\u03b8)) \u03bb\u03b3 1\u2212exp(\u2212\u03b3) exp(\u2212\u03b3\u03b8) Geman [19] \u03bb\u03b8 \u03b8+\u03b3 \u03bb\u03b3 (\u03b8+\u03b3)2 Laplace [20] \u03bb(1 \u2212 exp(\u2212 \u03b8 \u03b3 )) \u03bb \u03b3 exp(\u2212 \u03b8 \u03b3 )", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 15, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 183, "endOffset": 187}, {"referenceID": 16, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 199, "endOffset": 203}, {"referenceID": 17, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 235, "endOffset": 239}, {"referenceID": 18, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 247, "endOffset": 251}, {"referenceID": 19, "context": "nonconvex surrogates of L0-norm have been proposed, including Lp-norm (0 < p < 1) [13], Smoothly Clipped Absolute Deviation (SCAD) [14], Logarithm [15], Minimax Concave Penalty (MCP) [16], Capped L1 [17], ExponentialType Penalty (ETP) [18], Geman [19] and Laplace [20].", "startOffset": 264, "endOffset": 268}, {"referenceID": 20, "context": "Numerical studies [21], [22] have shown that the nonconvex sparse optimization usually outperforms convex models in the areas of signal recovery, error correction and image processing.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "Numerical studies [21], [22] have shown that the nonconvex sparse optimization usually outperforms convex models in the areas of signal recovery, error correction and image processing.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "The above low rank minimization problem arises in many computer vision tasks such as multiple category classification [23], matrix completion [24], multi-task learning [25] and low-rank representation with squared loss for subspace segmentation [7].", "startOffset": 118, "endOffset": 122}, {"referenceID": 23, "context": "The above low rank minimization problem arises in many computer vision tasks such as multiple category classification [23], matrix completion [24], multi-task learning [25] and low-rank representation with squared loss for subspace segmentation [7].", "startOffset": 142, "endOffset": 146}, {"referenceID": 24, "context": "The above low rank minimization problem arises in many computer vision tasks such as multiple category classification [23], matrix completion [24], multi-task learning [25] and low-rank representation with squared loss for subspace segmentation [7].", "startOffset": 168, "endOffset": 172}, {"referenceID": 6, "context": "The above low rank minimization problem arises in many computer vision tasks such as multiple category classification [23], matrix completion [24], multi-task learning [25] and low-rank representation with squared loss for subspace segmentation [7].", "startOffset": 245, "endOffset": 248}, {"referenceID": 25, "context": "The above convex problem can be efficiently solved by many known solvers [26], [27].", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "The above convex problem can be efficiently solved by many known solvers [26], [27].", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "(a) Lp Penalty [13] 0 2 4 6 0 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "(b) SCAD Penalty [14] 0 2 4 6 0 1 2 3", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "(c) Logarithm Penalty [15] 0 2 4 6 0 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "(d) MCP Penalty [16]", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "(e) Capped L1 Penalty [17] 0 2 4 6 0 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "(f) ETP Penalty [18] 0 2 4 6 0 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "(g) Geman Penalty [19] 0 2 4 6 0 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "(h) Laplace Penalty [20]", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "In this paper, to achieve a better approximation of the rank function, we extend the nonconvex surrogates of L0-norm shown in Table I onto the singular values of the matrix, and show how to solve the following general nonconvex nonsmooth low rank minimization problem [1]", "startOffset": 268, "endOffset": 271}, {"referenceID": 27, "context": "The work [28], [29] extend the Lp-norm of a vector to the Schattenp norm (0 < p < 1) and use the iteratively reweighted least squares (IRLS) algorithm to solve the nonconvex rank minimization problem with affine constraint.", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "The work [28], [29] extend the Lp-norm of a vector to the Schattenp norm (0 < p < 1) and use the iteratively reweighted least squares (IRLS) algorithm to solve the nonconvex rank minimization problem with affine constraint.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "IRLS is also applied for the unconstrained problem with the smoothed Schatten-p norm regularizer [30].", "startOffset": 97, "endOffset": 101}, {"referenceID": 30, "context": "Another nonconvex rank surrogate is the truncated nuclear norm [31].", "startOffset": 63, "endOffset": 67}, {"referenceID": 31, "context": "The nonconvex low rank matrix completion problem considered in [32] is a special case of our problem (3).", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "The work [33] uses the nonconvex log-det heuristic in [34] for image recovery.", "startOffset": 9, "endOffset": 13}, {"referenceID": 33, "context": "The work [33] uses the nonconvex log-det heuristic in [34] for image recovery.", "startOffset": 54, "endOffset": 58}, {"referenceID": 34, "context": "A possible method to solve (3) is the proximal gradient algorithm [35], which requires to compute the proximal mapping of the nonconvex function g.", "startOffset": 66, "endOffset": 70}, {"referenceID": 34, "context": ", the convexity of \u2207g [35]), there does not exist a general solver for computing the proximal mapping of the general nonconvex g in assumption A1.", "startOffset": 22, "endOffset": 26}, {"referenceID": 35, "context": ", [36].", "startOffset": 2, "endOffset": 6}, {"referenceID": 36, "context": "3 in [37]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "Inspired by (8), we can define the supergradient of concave g at the nonsmooth point x [38].", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "The updating rule (16) can be regarded as an extension of the Iteratively Reweighted L1 (IRL1) algorithm [21] for the weighted L1-norm problem", "startOffset": 105, "endOffset": 109}, {"referenceID": 38, "context": "However, the weighted nuclear norm in (16) is nonconvex (it is convex if and only if w 1 \u2265 w 2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 w m \u2265 0 [39]), while the weighted L1-norm in (17) is convex.", "startOffset": 113, "endOffset": 117}, {"referenceID": 39, "context": "Then WSVT reduces to the conventional Singular Value Thresholding (SVT) [40], which is an important subroutine in convex low rank optimization.", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "The updating rule (20) then reduces to the known proximal gradient method [10].", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "If the Lipschitz constant L(f) is not known or computable, the backtracking rule can be used to estimate \u03bc in each iteration [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 20, "context": ", [21], [30].", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": ", [21], [30].", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": ", [22], [31], [33], which target for some special nonconvex problems.", "startOffset": 2, "endOffset": 6}, {"referenceID": 30, "context": ", [22], [31], [33], which target for some special nonconvex problems.", "startOffset": 8, "endOffset": 12}, {"referenceID": 32, "context": ", [22], [31], [33], which target for some special nonconvex problems.", "startOffset": 14, "endOffset": 18}, {"referenceID": 30, "context": "The truncated nuclear norm ||X ||r = \u2211m i=r+1 \u03c3i(X) [31] is an interesting example.", "startOffset": 52, "endOffset": 56}, {"referenceID": 30, "context": "Compared with the alternating updating algorithm in [31], which require double loops, our IRNN will be more efficient and with stronger convergence guarantee.", "startOffset": 52, "endOffset": 56}, {"referenceID": 35, "context": "An example is the Latent Low Rank Representation (LatLRR) problem [36]", "startOffset": 66, "endOffset": 70}, {"referenceID": 40, "context": "where X \u2208 Rm1\u00d7\u00b7\u00b7\u00b7\u00d7mp is an p-way tensor and X \u00d7jPj denotes the j-mode product [41].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "TLRR is an extension of LRR [7] and LatLRR.", "startOffset": 28, "endOffset": 31}, {"referenceID": 42, "context": "The first two aim to examine the convergence behavior of IRNN for the matrix completion problem [43] on both synthetic data and real images.", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": "Low Rank Matrix Recovery on the Synthetic Data We first compare the low rank matrix recovery performances of nonconvex model (32) with the convex one by using nuclear norm [9] on the synthetic data.", "startOffset": 172, "endOffset": 175}, {"referenceID": 43, "context": "The Augmented Lagrange Multiplier (ALM) [44] method is used to solve the noise free problem", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "We compare IRNN for (32) with convex Accelerated Proximal Gradient with Line search (APGL)3 [24] which solves the noisy problem", "startOffset": 92, "endOffset": 96}, {"referenceID": 30, "context": "We follow the experimental settings in [31].", "startOffset": 39, "endOffset": 43}, {"referenceID": 44, "context": "We compare IRNN with some stateof-the-art methods on this task, including APGL, Low-Rank Matrix Fitting (LMaFit)4 [45] and Truncated Nuclear Norm Regularization (TNNR)5 [31].", "startOffset": 114, "endOffset": 118}, {"referenceID": 30, "context": "We compare IRNN with some stateof-the-art methods on this task, including APGL, Low-Rank Matrix Fitting (LMaFit)4 [45] and Truncated Nuclear Norm Regularization (TNNR)5 [31].", "startOffset": 169, "endOffset": 173}, {"referenceID": 45, "context": "Tensor Low-Rank Representation In this section, we consider to use the Tensor Low-Rank Representation (TLRR) (27) for face clustering [46], [36].", "startOffset": 134, "endOffset": 138}, {"referenceID": 35, "context": "Tensor Low-Rank Representation In this section, we consider to use the Tensor Low-Rank Representation (TLRR) (27) for face clustering [46], [36].", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "Problem (27) can be solved by the Accelerated Proximal Gradient (APG) [10] method with the optimal convergence rate O(1/K), where K is the number of iterations.", "startOffset": 70, "endOffset": 74}, {"referenceID": 45, "context": "After solving (27) or (36), we follow the settings in [46] to construct the affinity matrix by W = (|P3 |+|P3 |)/2.", "startOffset": 54, "endOffset": 58}, {"referenceID": 46, "context": "Finally, the Normalized Cuts (NCuts) [47] is applied based on W to segment the data into k groups.", "startOffset": 37, "endOffset": 41}, {"referenceID": 45, "context": "The performances of LRR and LatLRR are consistent with previous work [46], [36].", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "The performances of LRR and LatLRR are consistent with previous work [46], [36].", "startOffset": 75, "endOffset": 79}], "year": 2015, "abstractText": "The nuclear norm is widely used as a convex surrogate of the rank function in compressive sensing for low rank matrix recovery with its applications in image recovery and signal processing. However, solving the nuclear norm based relaxed convex problem usually leads to a suboptimal solution of the original rank minimization problem. In this paper, we propose to perform a family of nonconvex surrogates of L0-norm on the singular values of a matrix to approximate the rank function. This leads to a nonconvex nonsmooth minimization problem. Then we propose to solve the problem by Iteratively Reweighted Nuclear Norm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular Value Thresholding (WSVT) problem, which has a closed form solution due to the special properties of the nonconvex surrogate functions. We also extend IRNN to solve the nonconvex problem with two or more blocks of variables. In theory, we prove that IRNN decreases the objective function value monotonically, and any limit point is a stationary point. Extensive experiments on both synthesized data and real images demonstrate that IRNN enhances the low-rank matrix recovery compared with state-of-the-art convex algorithms.", "creator": "LaTeX with hyperref package"}}}