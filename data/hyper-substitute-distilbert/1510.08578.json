{"id": "1510.08578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2015", "title": "My Reflections on the First Man vs. Machine No-Limit Texas Hold 'em Competition", "abstract": "the first ever filipino vs. adult no - fault based hold'em competition took person from april 24 - may 5th, 2015 at q'ok wonderland in pittsburgh, pa. discussing this article i present my thoughts using computer gaming process, agent branding, and modeling learned.", "histories": [["v1", "Thu, 29 Oct 2015 06:53:15 GMT  (31kb,D)", "http://arxiv.org/abs/1510.08578v1", null], ["v2", "Sat, 23 Jan 2016 20:41:52 GMT  (31kb,D)", "http://arxiv.org/abs/1510.08578v2", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.MA", "authors": ["sam ganzfried"], "accepted": false, "id": "1510.08578"}, "pdf": {"name": "1510.08578.pdf", "metadata": {"source": "CRF", "title": "My Reflections on the First Man vs. Machine No-Limit Texas Hold \u2019em Competition\u2217", "authors": ["Sam Ganzfried"], "emails": ["sam.ganzfried@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "The first ever human vs. computer no-limit Texas hold \u2019em competition took place from April 24\u2013May 8, 2015 at River\u2019s Casino in Pittsburgh, PA, organized by Carnegie Mellon University Professor Tuomas Sandholm. 20,000 hands of two-player no-limit Texas hold \u2019em were played between the computer program \u201cClaudico\u201d and four of the top human specialists in this variation of poker, Dong Kim, Jason Les, Bjorn Li, and Doug Polk (so 80,000 hands were played in total).1\nTo evaluate the performance, we used \u201cduplicate\u201d scoring, in which the same hands were played twice with the cards reversed to reduce the role of luck (and thereby the variance).2 Each human was given a partner, who played the identical hands against Claudico with the cards reversed. Polk was paired with Les, and Kim was paired with Li. The players played in two different rooms of the casino simultaneously, with one player from each of the pairings in each room (so that both players in each room had the same cards, while both players in the other room had the cards that Claudico had in the first room). In total, the humans ended up winning the match by 732,713 chips, which corresponds to a win rate of 9.16 big blinds\n\u2217The competition was organized by Professor Tuomas Sandholm, and the agent was created by Noam Brown, Sam Ganzfried, and Tuomas Sandholm. This article contains the author\u2019s personal thoughts on the event. Some of the work described in this article was performed while the author was a student at Carnegie Mellon University before the completion of his PhD. The article reflects the views of the author alone and not necessarily those of Carnegie Mellon University. The work done at Carnegie Mellon University was supported by the National Science Foundation under grants IIS-1320620, IIS-0964579, and CCF-1101668, as well as XSEDE computing resources provided by the Pittsburgh Supercomputing Center.\n1Doug Polk tweeted a list on 2/28/2015 ranking himself at number one, Kim number two, Li number three, and Les (according to speculation on his screenname) within the top ten, https://twitter.com/DougPolkPoker/status/ 571647246074163201. Several other players have also created lists placing Polk at number one (e.g., Nick Frame tweeted one on 9/28/2014, https://twitter.com/TCfromUB/status/516396810433486848). While these rankings are largely subjective, they are based on some objective factors; e.g., if player A beats player B over a significant sample of hands, or if player A is willing to play against player B but player B refuses to play against player A (i.e., by leaving the table when player A sits in against him), then these indicate an advantage of player A over player B. If one player contests the ranking and believes he is better than someone ranked higher, then a challenge can ensue (e.g., Kim and Frame played a challenge match in February 2015, https://www.pokerstars.com/en/blog/2015/dong-donger-kim-kyu-and-nicktcfromub-frame-on-their-unique-heads-up-challenge-up-challenge-154091.shtml).\n2For example, suppose human A has pocket aces and the computer has pocket kings, and A wins $5,000. This would indicate that the human outplayed the computer. However, suppose human B has the pocket kings against the computer\u2019s pocket aces in the identical situation and the computer wins $10,000. Then, taking both of these results into account, an improved estimator of performance would indicate that the computer outplayed the human, after the role of luck in the result was significantly reduced.\nar X\niv :1\n51 0.\n08 57\n8v 1\n[ cs\n.G T\n] 2\n9 O\nct 2\n01 5\nper 100 hands (BB/100),3 a common metric used to evaluate performance in poker. This was a relatively decisive win for the humans and was statistically significant at the 90% confidence level, though it was not statistically significant at the 95% level.4\nThe chips were just a placeholder to keep track of the score and did not represent real money; the humans were paid at the end from a prize pool of $100,000 which had been donated from River\u2019s Casino and Microsoft Research. The human with the smallest profit over the match received $10,000, while the other humans received $10,000 plus additional payoff in proportion to the profit above the lowest profit. Formally, let x1, x2, x3, x4 denote the profits of the four humans from highest to smallest, and let pi denote the corresponding payoffs. Then\nIf x1 > x4 (1)\np1 = $10, 000 + $60, 000 \u00b7 x1 \u2212 x4\nx1 + x2 + x3 \u2212 3x4 (2)\np2 = $10, 000 + $60, 000 \u00b7 x2 \u2212 x4\nx1 + x2 + x3 \u2212 3x4 (3)\np3 = $10, 000 + $60, 000 \u00b7 x3 \u2212 x4\nx1 + x2 + x3 \u2212 3x4 (4)\np4 = $10, 000 (5)\nElse (6)\np1 = p2 = p3 = p4 = $25, 000 (7)\nThis scheme ensured that all players received at least $10,000 and that payoffs were increasing in profit, giving each human a financial incentive to try their best individually.\nWhile this was the first man vs. machine competition for the no-limit variant of Texas hold \u2019em, there had been two prior competitions for the limit variant. In the limit variant all bets are of a fixed size, while in nolimit bets can be of any number of chips up to the amount remaining in a player\u2019s stack (the stacks are reset to a fixed amount of 200 big blinds at the start of each hand). Thus, the game tree for no-limit has a much larger branching factor and is significantly larger; there are 10165 nodes in the game tree for no-limit, while there are around 1017 nodes for limit [16]. In 2007 a program called Polaris that was created by researchers at the University of Alberta played four duplicate 500-hand matches against human professionals. The program won one match, tied one, and lost two, thus losing the match overall. In 2008 an improved version of Polaris competed against six human professionals in a second match, this time coming out victorious (three wins, two losses, and one tie). There have also been highly-publicized man vs. machine competitions for other games; for example, chess program Deep Blue lost to human expert Garry Kasparov in 1996 and beat him in 1997, and Jeopardy agent Watson defeated human champions in 2011.\nClaudico is Latin for \u201cI limp.\u201d Limping is the name of a specific play in poker. After the initial antes have been paid, the first player to act is the small blind and he has three available actions; fold (forfeit the pot), call (match the big blind by putting in 50 chips more), or raise by putting in additional chips beyond those needed to call (a raise can be any integral amount from 200 chips up to 20,000 chips in this situation). The second option of just calling is called \u201climping\u201d and has traditionally been viewed as a very weak play only made by bad players. In one popular book on strategy, Phil Gordon writes, \u201cLimping is for Losers. This is the most important fundamental in poker\u2014for every game, for every tournament, every stake: If you\n3The small blind (SB) and big blind (BB) correspond to initial investments, or \u201cantes\u201d of the players. In the match, the SB was 50 chips and the BB was 100 chips.\n4To put these results into some perspective, Dong Kim won the challenge described above against Nick Frame by 13.87 BB/100 (he won by $103,992 over 15,000 hands with blinds SB=$25, BB=$50), http://www.pokergurublog.com/ content/donger-kim-wins-heads-challenge-against-tcfromub, and Doug Polk defeated Ben Sulsky in another high-profile challenge match by 24.67 BB/100 (he won by $740,000 over 15,000 hands with blinds SB = $100, BB = $200), http://www.pokernews.com/news/2013/10/doug-polk-defeats-ben-sulsky-16618.htm.\nare the first player to voluntarily commit chips to the pot, open for a raise. Limping is inevitably a losing play. If you see a person at the table limping, you can be fairly sure he is a bad player. Bottom line: If your hand is worth playing, it is worth raising\u201d [13]. Claudico actually limps close to 10% of its hands, and based on discussion with the human players who did analysis it seems to have profited overall from the hands it limped. Claudico also makes several other plays that challenge conventional human poker strategy; for example it sometimes makes very small bets of 10% of the pot, and sometimes very large all-in bets for many times the pot (e.g., betting 20,000 into a pot of 500). By contrast, human players typically utilize a small number of bet sizes, usually between half pot and pot."}, {"heading": "2 Agent Architecture", "text": "Claudico was an improved version of an earlier agent called Tartanian7 that came in first place in the 2014 AAAI computer poker competition, beating each opposing agent with statistical significance. The architecture of that agent has been described in detail in a recent paper [3]. At a very high level, the design of the agent follows the three-step procedure depicted in Figure 1, which is the leading paradigm used by many of the strongest agents for large games.\nIn the first step, the original game is approximated by a smaller abstract game that hopefully retains much of the strategic structure of the initial game. The first abstractions for two-player Texas hold \u2019em were manually generated [2, 22], while current abstractions are computed algorithmically [9, 10, 12, 18, 23]. For smaller games, such as Rhode Island hold \u2019em, abstraction can be performed losslessly, and the abstract game is actually isomorphic to the full game [11]. However, for larger games, such as Texas hold \u2019em, we must be willing to incur some loss in the quality of the modeling approximation due to abstraction.\nThe second step is to compute an -equilibrium in the smaller abstracted game, using a custom iterative equilibrium-finding algorithm such as counterfactual regret minimization (CFR) [24] or a generalization of Nesterov\u2019s excessive gap technique [14].\nThe final step is to construct a strategy profile in the original game from the approximate equilibrium of the abstracted game by means of a reverse mapping procedure. When the action spaces of the original and abstracted games are identical, this step is often straightforward, since the equilibrium of the abstracted game can be played directly in the full game. However, even in this simplified setting often significant performance improvements can be obtained by applying a nontrivial reverse mapping. Several procedures have been shown to significantly improve performance that modify the action probabilities of the abstract equilibrium strategies by placing more weight on certain actions [3, 8]. These post-processing procedures\nare able to achieve robustness against limitations of the abstraction and equilibrium-finding phases of the paradigm.\nWhen the action spaces of the original and abstracted games differ, an additional procedure is needed to interpret actions taken by the opponent that are not allowed in the abstract game model. Such a procedure is called an action translation mapping. The typical approach for performing action translation is to map the opponent\u2019s action to a nearby action that is in the abstraction (perhaps probabilistically), and then respond as if the opponent had taken this action.\nAn additional crucial component of Claudico, that was not present in Tartanian7 due to a last-minute technical difficulty (thought a version of it was present in prior agent Tartanian6), is an approach for real-time computation of solutions in the part of the game tree that we have reached to a greater degree of accuracy than in the offline computation, called endgame solving, which is depicted in Figure 2 [7]. At a high level,\nendgame solving works by assuming both agents follow the precomputed approximate equilibrium strategies for the trunk portion of the game prior to the endgame; then the endgame induced by these trunk strategies is solved, using Bayes\u2019 rule to compute the input distributions of players\u2019 private information leading into the endgame. In general, such a procedure could produce a non-equilibrium strategy profile (even if the full game has a unique equilibrium and a single endgame); for example, in a sequential version of rockpaper-scissors where player 1 acts and then player 2 acts without observing the action taken by player 1, if we fix player 1 to follow his equilibrium strategy of randomizing equally among all three actions, then any strategy for player 2 is an equilibrium in the resulting endgame, because each one yields her expected payoff 0. In particular, the equilibrium solver could output the pure strategy Rock for her, which is clearly not an equilibrium of the full game. On the other hand, endgame solving is successful in other games; for example in a game where player 1 first selects an action ai and then an imperfect-information game Gi is played, we could simply solve the Gi corresponding to the action ai that is actually taken, provided that the Gi are independent and no information sets extend between several Gi. Furthermore, endgame solving has been previously demonstrated to improve performance empirically against strong computer programs in no-limit Texas hold \u2019em [7].\nWe used the endgame solver to compute our strategies in real time for the final betting round of each hand, called the river.5 Despite the theoretical limitation of the approach, Doug Polk related to me in personal communication after the competition ended that he thought the river strategy of Claudico using the endgame solver was the strongest part of the agent."}, {"heading": "2.1 Offline abstraction and equilibrium computation", "text": "Claudico\u2019s action abstraction was manually generated and consisted of sizes ranging from 0.1 pot in certain situations to all-in (wagering all of one\u2019s remaining chips). The information abstraction was computed using\n5There are (up to) four betting rounds in a hand of Texas hold \u2019em poker. First both players are dealt two private cards and there is an initial round called preflop. Then three public cards are dealt and there is the flop. Then there is one more additional public card on the turn, followed by one final public card in the river betting round.\na hierarchical algorithm that first clustered the three-card public flop boards into public buckets, then clustered the private information states for each postflop round (i.e., flop, turn, river) separately for each public bucket (no information abstraction was performed for the preflop round) [3]. This hierarchical abstraction algorithm allowed us to apply a new scalable distributed version of CFR [3]. We ran the equilibrium-finding algorithm for several months on Pittsburgh\u2019s Blacklight supercomputer using 961 cores (60 blades of 16 cores each, plus one core for the head blade, with each blade having 128 GB RAM)."}, {"heading": "2.2 Action translation", "text": "For the action translation mapping, we used the pseudo-harmonic mapping, which maps a bet x of the opponent to one of the nearest sizes in the abstraction A,B according to the following formula, where f(x) the probability that x is mapped to A [6]:\nf(x) = (B \u2212 x)(1 +A) (B \u2212A)(1 + x) .\nThis mapping was derived from analytical solutions of simplified poker games and has been demonstrated to outperform prior approaches in terms of exploitability in simplified games, as well as the best prior approach in terms of empirical performance against no-limit Texas hold \u2019em agents. The mapping also satisfies several axioms and theoretical properties that the best prior mappings do not satisfy, for example it is Lipschitz continuous in A and B, and therefore robust to small changes in the actions used in the action abstraction.\nAs an example to demonstrate the operation of the algorithm, suppose the opponent bets 100 into a pot of 500, and that the closest sizes in our abstraction are to \u201ccheck\u201d (i.e., bet 0) or to bet 0.25 pot: so A = 0 and B = 0.25. Plugging these in gives f(x) = 16 = 0.167. This is the probability we map his bet down to 0 and interpret it as a check. So we pick a random number in [0,1], and if it is above 16 we interpret the bet as 0.25 pot, and otherwise as a check."}, {"heading": "2.3 Post-processing", "text": "We used additional post-processing techniques to round the action probabilities that had been computed by the offline equilibrium-finding algorithm [8]. We used a generalization of the prior approach that applied a different rounding threshold for each betting round (i.e., action probabilities below the threshold were rounded to zero and then all probabilities were renormalized), with a more aggressive (i.e., larger) threshold used for the later betting rounds, since the equilibrium-finding algorithm obtains worse convergence for those rounds due to having fewer samples. We did not apply any post-processing for ourselves on the river when using the endgame solver, and assumed neither agent used any post-processing in the generation of the trunk strategies used as inputs to the endgame solver.6\n6It may seem somewhat strange that we applied post-processing for our own play, but assumed that no post-processing was applied for the trunk strategies entering the endgame, and that this may be problematic due to the mismatch between our own strategy and the model of it entering the endgame. We chose to do this because the endgame solving approach can be less robust if the input strategies have weight on only a small number of hands (as an extreme example, if all the weight was on one hand, then the endgame solver would assume that the other agent knew our exact hand, and the solution would require us to play extremely conservatively). The approach is much more robust if we include a small probability on many different hands before the postprocessing was applied. We believed that the gain in robustness outweighed the limitation of the mismatch (in addition to the reasons given above, we already expect there to be a mismatch between the input trunk strategy for the opponent, which is based off our offline equilibrium computation, and his own actual strategy, and thus we would not be removing this mismatch completely even if we eliminated it for our own strategy)."}, {"heading": "2.4 Endgame solving", "text": "The endgame solving algorithm consists of several steps [7]. First, the joint hand-strength input distributions are computed by applying Bayes\u2019 rule to the precomputed trunk strategies, utilizing a recently developed technique that requires only a linear number of lookups in the large strategy table (while the na\u0131\u0308ve approach requires a quadratic number of lookups and is impractical). Then the equity is computed for each hand, given these distributions.7 Then hands are bucketed separately for each player based on the computed equities for the given situation by applying an information abstraction algorithm. Finally an exact Nash equilibrium is computed in the game corresponding to this information abstraction and an action abstraction that had been precomputed for the specific pot and stack size of the current hand. All of this computation was done in real time during gameplay. To compute equilibria within the endgames, we used Gurobi\u2019s parallel linear program solver [15] to solve the sequence-form optimization formulation [19]."}, {"heading": "3 Problematic Hands", "text": "Several notable hands stood out during the course of the competition that highlighted weaknesses of the agent, which have been singled out in a thread that was devoted entirely to the competition on the most popular poker forum, the Two Plus Two Poker Forum.8\n1. In one hand, we had A4s (ace and four of the same suit) and folded preflop after we had put in over half of our stack (the human opponent had 99). This is regarded as a bad play, since we would only need to win around 25% of the time against the opponent\u2019s distribution for a call to be profitable at this point (we win about 33% of the time against the hand he had). The problem was that our translation mapping mapped the opponent\u2019s raise down to a smaller size, which caused us to look up a strategy for ourselves that had been computed thinking that the pot size was much smaller than we thought it was (we thought we had invested around 7,000 when we had actually invested close to 10,000\u2014recall that the starting stacks are 20,000). These translation issues can get magnified further as the hand develops if we think we have bet a percentage (e.g., 23 ) of the (correct) size of the pot, while the strategies we have precomputed assumed a different size of the pot.\n2. In another hand we had KT and folded to an all-in bet on the turn after putting in about 34 of our stack despite having top pair and a flush draw (there were three diamonds on the board and we had the king of diamonds; the opponent actually had A2 with the ace of diamonds, for a better flush draw but worse hand due to us having a pair already). The issue for this hand was that the human made a raise on the flop which was slightly below the smallest size we had in our abstraction in that situation, and we ended up mapping it down to just a call (it was just mapped down with around 3% probability in that situation, and so we ended up getting pretty \u201cunlucky\u201d that we mapped it in the \u201cwrong\u201d direction). This ended up causing us to think we had committed far fewer chips to the pot at that point than we actually had.\n7The equity of a hand against a distribution for the opponent is the probability of winning plus one half times the probability of tying.\n8The thread discussing the event has 232,252 views and 1,609 posts as of September 23, 2015, http: //forumserver.twoplustwo.com/29/news-views-gossip-sponsored-online-poker-report/ wcgrider-dong-kim-jason-les-bjorn-li-play-against-new-hu-bot-1526750/. Here are links to some of the posts in the thread that relate to the hands described: hand 1 http://forumserver.twoplustwo.com/ showpost.php?p=46888848&postcount=1275, hand 2 http://forumserver.twoplustwo.com/showpost. php?p=46802181&postcount=831, hand 3 http://forumserver.twoplustwo.com/showpost.php?p= 46773302&postcount=457. Note a minor clarification that Claudico invested closer to 75% than 80% of its stack in hand 2.\nThe problem in these hands was not due simply to a flaw in the action translation mapping, or even to a flaw in the action abstraction (though of course improvements to those would be very beneficial as well); even if we had used a different translation mapping and/or used different action sizes in the abstraction, we would still have potentially sizable gaps between certain sizes of the abstraction due to the fact that we can only select so many to keep the abstraction sufficiently small so that it can be solved within time and memory limits. That means that, given the current paradigm, we will necessarily have to map bets to sizes somewhat far away with some probability, which will cause our perception of the pot size to be incorrect, as these hands indicate. This is called the \u201coff-tree problem,\u201d which has received very little study thus far. Some agents, such as versions of the agent from the University of Alberta, attempt to mitigate this problem by specifically taking actions aimed to get us back on the tree (e.g., making a bet that we would not ordinarily make to correct for the pot size disparity). However, this is problematic too, as it requires us to take an undesirable action. The endgame solving approach provides a solution to this problem by inputting the correct pot size to the endgame solving algorithm, even if this differs from our perception of it at that point due to the opponent having taken an action outside of the action abstraction. In general, real-time endgame solving could correct for many misperceptions in game state information that have been accumulated along the course of game play; however, this would not apply to the preflop, flop, and turn rounds, where we are not using endgame solving. Thus it is necessary to explore additional approaches to this problem; improved algorithms for realtime computation for the earlier rounds is a potentially promising direction, and perhaps new approaches can also be developed for addressing the off-tree problem independently of endgame solving.\nWe went over the log files for these two specific hands with Doug Polk in person after the competition had ended, and he agreed that our plays in both hands were reasonable had the pot size been what our computed strategies perceived it to be at that point. Of course, we both agree that the hands were both major mistakes if you include the misperception of the pot size. Even though these were only low probability mistakes due to the randomization outcome selected by the translation mapping, these types of mistakes can become a significant liability in aggregate, particularly when playing against humans who are aware of them and actively trying to exploit them. Doug alluded to this point as well in an interview after the competition.9 Based on Doug\u2019s interview and subsequent conversations it seems that he views this as Claudico\u2019s biggest weakness, and it will be interesting to see what improvements can be found, and whether those can be exploited in turn by good countermeasures.\n3. In one other problematic hand, we made a large all-in bet (of around 19,000) into a relatively small pot of around 1700. There were three of a suit (spades) on the board, and we had a very weak hand without a fourth spade (so our bet was a \u201cbluff,\u201d hoping the opponent would fold a stronger hand). The problem is not that we made a large bet per se, or even that we did it with a very weak hand; extremely large bets are correct and part of equilibrium strategy in certain situations,10 and in such situations they must be made with some weak hands as bluffs to balance with the very strong \u201cvalue\u201d hands or else our strategy would be too predictable (if we never bluffed, then the opponent would just fold everything except his hands that beat half of our value hands, and then the bets with the\n9 http://www.highstakesdb.com/5793-exclusive-interview-with-no1-hunl-player-dougwcgrider-polk.aspx\n10As one example, Ankenman and Chen describe a game called the \u201cClairvoyance Game\u201d where player 1 is dealt a winning/losing hand with probability 1\n2 each, and is allowed to bet any amount up to initial stack n into a pot of 1; then player 2 can call or fold [1].\n(Player 1 knows whether he has a winning or losing hand, while player 2 does not know player 1\u2019s hand.) They analytically solve for the unique Nash equilibrium of the game, and it has player 1 betting all-in for n with his winning hand, and betting all-in with some probability with his losing hand, and checking with some probability (the probability is selected to make player 2 indifferent between calling and folding); player 2 then calls and folds with some probability (which is selected to make player 1 indifferent between \u201cbluffing\u201d and checking with his losing hand). This solution holds regardless of the stack size n; so even if n = 1, 000, 000, it would be optimal for player 1 to bet all-in for 1,000,000 to win a pot of 1 (a sketch of Ankenman and Chen\u2019s argument with the computed equilibrium strategies also appears in [6]). Thus, it is clear that at least in certain situations extremely large bets, both with strong and weak hands, are part of optimal strategies.\nbottom half of our value hands would be unprofitable). Thus, making large bets as bluffs is needed in certain situations. The problem is that certain hands are much better suited for them than others. For example, suppose the board was JsTs4sKcQh, and suppose we could have 3c2c (three and two of clubs) vs. 3s2c (three of spades and two of clubs). Both hands are extremely weak (they produce the worst possible five-card hand); however, if we have the 3s, it actually has a subtle and very significant benefit: it significantly reduces the probability that the opponent holds an extremely strong hand (e.g., an ace-high or king-high flush) because several of the hands that would constitute that strength would contain that card, e.g., As3s and Ks3s. Thus, this would make a much better choice for our hand to make a large bet with, since he is less likely to have a hand strong enough to call, making the bluff bet more effective. Our endgame-solving algorithm described in Section 2.4 takes this \u201ccard removal\u201d factor into account to an extent, since the equities are computed for each hand against the distribution the opponent could hold given that hand; however, this does not fully take into account the card removal effect. For example, the 3c2c and 3s2c hands would both have the lowest possible equity (it would be slightly above zero only because of possible ties), and would be necessarily grouped into the same bucket by our endgame information abstraction algorithm (the worst bucket) despite the fact that they have very different card removal properties.\nDoug Polk said that he thought the river strategy using the endgame solver overall was the strongest part of Claudico; however, he thought that utilizing the large betting sizes without properly accounting for card removal was actually a significant weakness, since we would be bluffing with non-optimal hands. We came to this conclusion ourselves as well during the competition, and for this reason decided to take out the large bets for ourselves from the endgame solver partway through the competition, since this issue is most problematic for those bet sizes (for smaller bet sizes, card removal is still important, but significantly less important since we are not just trying to \u201cblock\u201d the opponent from having a small number of extremely strong hands, since he will be calling with many more hands). Interestingly, Dong Kim told me after the competition that they had conducted analysis and we were actually profiting on the large bet sizes during the time we used them, despite the theoretical issue described above. I think everyone agrees that massive \u201coverbets\u201d are part of full optimal strategies, and likely underutilized by even the best human players. But card removal is also particularly important for these sizes, and I think for an agent to use them successfully an improved algorithm for dealing with blockers/card removal would need to be developed, though I am still quite curious how well we would have performed if we continued with those sizes included in the agent."}, {"heading": "4 Conclusion", "text": "It is one thing to evaluate a poker agent against other computer agents, who largely also play static approximations of equilibrium strategies; it is another to compete against the strongest human specialists, who will adapt and attempt to capitalize on even the smallest perceived weaknesses. This was the first time a no-limit Texas hold \u2019em agent has competed against human players of this caliber, and we really had no idea what to expect entering the competition, as previously all of our experiments had been against computer agents from the AAAI Annual Computer Poker Competition. We learned many valuable lessons that will be pivotal in developing improved agents going forward. We have highlighted the two most important avenues for future research. The first is to develop an improved approach for the \u201coff-tree\u201d problem where we make a mistake due to a misperception of the actual size of the pot after translating an action for the opponent that is not in our action abstraction. We have outlined promising agendas for attacking this problem, including improved action abstraction and translation algorithms, novel approaches for real-time computation that address the portion of the game prior to the final round, and entirely new approaches specifically geared at solving the off-tree problem independently of the other problems. And the second is to develop an improved approach for information abstraction that better accounts for card removal/\u201cblockers\u201d (i.e., that accounts for the fact\nthat us having certain cards in our hand modifies the probability of the opponent having certain hands). This issue is most problematic within the information abstraction algorithm for the endgame, where the card removal effect is most significant due to the distributions for us and the opponent being the most well defined (i.e., there is no more potential remaining in the hand due to uncertainty of public cards, and this relative certainty will likely cause the distributions to put positive weight on fewer hands), and it limits our ability to utilize large bet sizes, which have been demonstrated to be optimal in certain settings. Of course, it would be beneficial to develop an improved information abstraction algorithm that accomplishes this in the part of the game prior to the endgame as well.\nAt first glance it may appear that these issues are purely pragmatic and specific to poker. While one of the main goals is certainly to produce a poker agent that can beat the strongest humans in two-player no-limit Texas hold \u2019em, there are deeper theoretical questions related to each component of the agent that has been described. Endgame solving has been proven to have theoretical guarantees in certain games while it can lead to strategies with high exploitability in others (even if the full game has a single Nash equilibrium and just a single endgame is considered) [7]. It would be interesting to prove theoretical bounds on its performance on interesting game classes, perhaps classes that include variants of poker. Empirically the approach appears to be very successful on poker despite its lack of theoretical guarantees. Recently an approach has been developed for game decomposition that has theoretical guarantees [4], however from personal communication with the authors I have learned that the approach performs worse empirically than our approach that does not have a worst-case guarantee.\nThe main abstraction algorithms that have been successful in practice are heuristic and have no theoretical guarantees. It is extremely difficult to prove meaningful theoretical guarantees when performing such a large degree of abstraction, e.g., approximating a game with 10165 states by one with 1014 states. There has been some recent work done on abstraction algorithms with theoretical guarantees, though that work does not scale to games nearly as large as no-limit Texas hold \u2019em. One line of work performs lossless abstraction, that guarantees that the abstract game is exactly isomorphic to the original game [11]. This work has been applied to compute equilibrium strategies in Rhode Island hold \u2019em, a medium-sized (3.1 billion nodes) variant of poker. Recent work has also presented the first lossy abstraction algorithms with bounds on the solution quality [20]. However, the algorithms are based on integer programming formulations, and only scale to a tiny poker game with a 5-card deck. It would be very interesting to bridge this gap between heuristics that work well in practice for large games with no theoretical guarantees, and the approaches with theoretical guarantees that have more modest scalability.\nScalable algorithms for computing Nash equilibria have diverse applications, including cybersecurity (e.g., determining optimal thresholds to protect against phishing attacks), business (e.g., auctions and negotiations), national security (e.g., computing strategies for officers to protect airports), and medicine. For medicine, algorithms that were created in the course of research on poker [17] have been applied to compute robust policies for diabetes management [5]; recently it has been proposed that equilibrium-finding algorithms are applicable to the problem of treating diseases such as the HIV virus that can mutate adversarially [21].\nFor the pseudo-harmonic action translation mapping, in addition to showing that it outperforms the best prior approach in terms of exploitability in several games, we have also presented several axioms and theoretical properties that it satisfies; for example, it is Lipschitz continuous in A and B, and therefore robust to small changes in the actions used in the action abstraction [6]. Another mapping that has very high exploitability in several games also satisfies these axioms, and further investigation can lead to deeper theoretical understanding of this problem and potentially new improved approaches.\nEven the post-processing approaches, which appear to be purely heuristic, have interesting theoretical open questions. For example, it has been shown that purification (i.e., selecting the highest-probability action with probability 1) leads to an improved performance in uniform random 4\u00d7 4 matrix games using random 3 \u00d7 3 abstractions when playing against the Nash equilibrium of the full 4 \u00d7 4 game for the opponent [8].\nThese results were based off simulations that were statistically significant at the 95% confidence level, and it would be interesting to provide a formal proof. Furthermore, that paper provided a conjecture for the specific supports of the games for which the approach would improve or not change performance, which was also based on statistically-significant simulations. It would be interesting to prove this formally as well, and to generalize the results to games of arbitrary size. On a broader level, there is relatively little theoretical understanding for why the post-processing approaches\u2014which one would expect to make the strategies more predictable\u2014have been shown to be consistently successful. Surprisingly, the improvements in empirical performance do not necessarily come at the expense of worst-case exploitability, and a degree of thresholding has been demonstrated to actually reduce exploitability for a limit Texas hold \u2019em agent [8]."}], "references": [{"title": "The Mathematics of Poker", "author": ["Jerrod Ankenman", "Bill Chen"], "venue": "ConJelCo LLC,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Approximating game-theoretic optimal strategies for full-scale poker", "author": ["Darse Billings", "Neil Burch", "Aaron Davidson", "Robert Holte", "Jonathan Schaeffer", "Terence Schauenberg", "Duane Szafron"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Hierarchical abstraction, distributed equilibrium computation, and post-processing, with application to a champion no-limit Texas Hold\u2019em agent", "author": ["Noam Brown", "Sam Ganzfried", "Tuomas Sandholm"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multi-Agent Systems (AA- MAS),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Solving imperfect information games using decomposition", "author": ["Neil Burch", "Michael Johanson", "Michael Bowling"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Tractable objectives for robust policy optimization", "author": ["Katherine Chen", "Michael Bowling"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Action translation in extensive-form games with large action spaces: Axioms, paradoxes, and the pseudo-harmonic mapping", "author": ["Sam Ganzfried", "Tuomas Sandholm"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Endgame solving in large imperfect-information games", "author": ["Sam Ganzfried", "Tuomas Sandholm"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multi-Agent Systems (AA- MAS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Strategy purification and thresholding: Effective non-equilibrium approaches for playing large games", "author": ["Sam Ganzfried", "Tuomas Sandholm", "Kevin Waugh"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "A competitive Texas Hold\u2019em poker player via automated abstraction and real-time equilibrium computation", "author": ["Andrew Gilpin", "Tuomas Sandholm"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Better automated abstraction techniques for imperfect information games, with application to Texas Hold\u2019em poker", "author": ["Andrew Gilpin", "Tuomas Sandholm"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Lossless abstraction of imperfect information games", "author": ["Andrew Gilpin", "Tuomas Sandholm"], "venue": "Journal of the ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "A heads-up no-limit Texas Hold\u2019em poker player: Discretized betting models and automatically generated equilibrium-finding programs", "author": ["Andrew Gilpin", "Tuomas Sandholm", "Troels Bjerre S\u00f8rensen"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multi-Agent Systems (AA- MAS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Phil Gordon\u2019s Little Gold Book: Advanced Lessons for Mastering Poker 2.0", "author": ["Phil Gordon"], "venue": "Gallery Books,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Smoothing techniques for computing Nash equilibria of sequential games", "author": ["Samid Hoda", "Andrew Gilpin", "Javier Pe\u00f1a", "Tuomas Sandholm"], "venue": "Mathematics of Operations Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Measuring the size of large no-limit poker games", "author": ["Michael Johanson"], "venue": "Technical report, University of Alberta,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Finding optimal abstract strategies in extensive-form games", "author": ["Michael Johanson", "Nolan Bard", "Neil Burch", "Michael Bowling"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Evaluating state-space abstractions in extensive-form games", "author": ["Michael Johanson", "Neil Burch", "Richard Valenzano", "Michael Bowling"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Fast algorithms for finding randomized strategies in game trees", "author": ["Daphne Koller", "Nimrod Megiddo", "Bernhard von Stengel"], "venue": "In Proceedings of the 26th ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Extensive-form game abstraction with bounds", "author": ["Christian Kroer", "Tuomas Sandholm"], "venue": "In Proceedings of the ACM Conference on Economics and Computation (EC),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Steering evolution strategically: Computational game theory and opponent exploitation for treatment planning, drug design, and synthetic biology", "author": ["Tuomas Sandholm"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Abstraction methods for game theoretic poker", "author": ["Jiefu Shi", "Michael Littman"], "venue": "In CG \u201900: Revised Papers from the Second International Conference on Computers and Games,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "A practical use of imperfect recall", "author": ["Kevin Waugh", "Martin Zinkevich", "Michael Johanson", "Morgan Kan", "David Schnizlein", "Michael Bowling"], "venue": "In Proceedings of the Symposium on Abstraction, Reformulation and Approximation (SARA),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Regret minimization in games with incomplete information", "author": ["Martin Zinkevich", "Michael Bowling", "Michael Johanson", "Carmelo Piccione"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}], "referenceMentions": [{"referenceID": 14, "context": "Thus, the game tree for no-limit has a much larger branching factor and is significantly larger; there are 10165 nodes in the game tree for no-limit, while there are around 1017 nodes for limit [16].", "startOffset": 194, "endOffset": 198}, {"referenceID": 12, "context": "Bottom line: If your hand is worth playing, it is worth raising\u201d [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 2, "context": "The architecture of that agent has been described in detail in a recent paper [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "The first abstractions for two-player Texas hold \u2019em were manually generated [2, 22], while current abstractions are computed algorithmically [9, 10, 12, 18, 23].", "startOffset": 77, "endOffset": 84}, {"referenceID": 20, "context": "The first abstractions for two-player Texas hold \u2019em were manually generated [2, 22], while current abstractions are computed algorithmically [9, 10, 12, 18, 23].", "startOffset": 77, "endOffset": 84}, {"referenceID": 8, "context": "The first abstractions for two-player Texas hold \u2019em were manually generated [2, 22], while current abstractions are computed algorithmically [9, 10, 12, 18, 23].", "startOffset": 142, "endOffset": 161}, {"referenceID": 9, "context": "The first abstractions for two-player Texas hold \u2019em were manually generated [2, 22], while current abstractions are computed algorithmically [9, 10, 12, 18, 23].", "startOffset": 142, "endOffset": 161}, {"referenceID": 11, "context": "The first abstractions for two-player Texas hold \u2019em were manually generated [2, 22], while current abstractions are computed algorithmically [9, 10, 12, 18, 23].", "startOffset": 142, "endOffset": 161}, {"referenceID": 16, "context": "The first abstractions for two-player Texas hold \u2019em were manually generated [2, 22], while current abstractions are computed algorithmically [9, 10, 12, 18, 23].", "startOffset": 142, "endOffset": 161}, {"referenceID": 21, "context": "The first abstractions for two-player Texas hold \u2019em were manually generated [2, 22], while current abstractions are computed algorithmically [9, 10, 12, 18, 23].", "startOffset": 142, "endOffset": 161}, {"referenceID": 10, "context": "For smaller games, such as Rhode Island hold \u2019em, abstraction can be performed losslessly, and the abstract game is actually isomorphic to the full game [11].", "startOffset": 153, "endOffset": 157}, {"referenceID": 22, "context": "The second step is to compute an -equilibrium in the smaller abstracted game, using a custom iterative equilibrium-finding algorithm such as counterfactual regret minimization (CFR) [24] or a generalization of Nesterov\u2019s excessive gap technique [14].", "startOffset": 182, "endOffset": 186}, {"referenceID": 13, "context": "The second step is to compute an -equilibrium in the smaller abstracted game, using a custom iterative equilibrium-finding algorithm such as counterfactual regret minimization (CFR) [24] or a generalization of Nesterov\u2019s excessive gap technique [14].", "startOffset": 245, "endOffset": 249}, {"referenceID": 2, "context": "Several procedures have been shown to significantly improve performance that modify the action probabilities of the abstract equilibrium strategies by placing more weight on certain actions [3, 8].", "startOffset": 190, "endOffset": 196}, {"referenceID": 7, "context": "Several procedures have been shown to significantly improve performance that modify the action probabilities of the abstract equilibrium strategies by placing more weight on certain actions [3, 8].", "startOffset": 190, "endOffset": 196}, {"referenceID": 6, "context": "An additional crucial component of Claudico, that was not present in Tartanian7 due to a last-minute technical difficulty (thought a version of it was present in prior agent Tartanian6), is an approach for real-time computation of solutions in the part of the game tree that we have reached to a greater degree of accuracy than in the offline computation, called endgame solving, which is depicted in Figure 2 [7].", "startOffset": 410, "endOffset": 413}, {"referenceID": 6, "context": "Furthermore, endgame solving has been previously demonstrated to improve performance empirically against strong computer programs in no-limit Texas hold \u2019em [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 2, "context": ", flop, turn, river) separately for each public bucket (no information abstraction was performed for the preflop round) [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "This hierarchical abstraction algorithm allowed us to apply a new scalable distributed version of CFR [3].", "startOffset": 102, "endOffset": 105}, {"referenceID": 5, "context": "For the action translation mapping, we used the pseudo-harmonic mapping, which maps a bet x of the opponent to one of the nearest sizes in the abstraction A,B according to the following formula, where f(x) the probability that x is mapped to A [6]:", "startOffset": 244, "endOffset": 247}, {"referenceID": 0, "context": "So we pick a random number in [0,1], and if it is above 16 we interpret the bet as 0.", "startOffset": 30, "endOffset": 35}, {"referenceID": 7, "context": "We used additional post-processing techniques to round the action probabilities that had been computed by the offline equilibrium-finding algorithm [8].", "startOffset": 148, "endOffset": 151}, {"referenceID": 6, "context": "The endgame solving algorithm consists of several steps [7].", "startOffset": 56, "endOffset": 59}, {"referenceID": 17, "context": "To compute equilibria within the endgames, we used Gurobi\u2019s parallel linear program solver [15] to solve the sequence-form optimization formulation [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "aspx As one example, Ankenman and Chen describe a game called the \u201cClairvoyance Game\u201d where player 1 is dealt a winning/losing hand with probability 1 2 each, and is allowed to bet any amount up to initial stack n into a pot of 1; then player 2 can call or fold [1].", "startOffset": 262, "endOffset": 265}, {"referenceID": 5, "context": "This solution holds regardless of the stack size n; so even if n = 1, 000, 000, it would be optimal for player 1 to bet all-in for 1,000,000 to win a pot of 1 (a sketch of Ankenman and Chen\u2019s argument with the computed equilibrium strategies also appears in [6]).", "startOffset": 258, "endOffset": 261}, {"referenceID": 6, "context": "Endgame solving has been proven to have theoretical guarantees in certain games while it can lead to strategies with high exploitability in others (even if the full game has a single Nash equilibrium and just a single endgame is considered) [7].", "startOffset": 241, "endOffset": 244}, {"referenceID": 3, "context": "Recently an approach has been developed for game decomposition that has theoretical guarantees [4], however from personal communication with the authors I have learned that the approach performs worse empirically than our approach that does not have a worst-case guarantee.", "startOffset": 95, "endOffset": 98}, {"referenceID": 10, "context": "One line of work performs lossless abstraction, that guarantees that the abstract game is exactly isomorphic to the original game [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "Recent work has also presented the first lossy abstraction algorithms with bounds on the solution quality [20].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "For medicine, algorithms that were created in the course of research on poker [17] have been applied to compute robust policies for diabetes management [5]; recently it has been proposed that equilibrium-finding algorithms are applicable to the problem of treating diseases such as the HIV virus that can mutate adversarially [21].", "startOffset": 78, "endOffset": 82}, {"referenceID": 4, "context": "For medicine, algorithms that were created in the course of research on poker [17] have been applied to compute robust policies for diabetes management [5]; recently it has been proposed that equilibrium-finding algorithms are applicable to the problem of treating diseases such as the HIV virus that can mutate adversarially [21].", "startOffset": 152, "endOffset": 155}, {"referenceID": 19, "context": "For medicine, algorithms that were created in the course of research on poker [17] have been applied to compute robust policies for diabetes management [5]; recently it has been proposed that equilibrium-finding algorithms are applicable to the problem of treating diseases such as the HIV virus that can mutate adversarially [21].", "startOffset": 326, "endOffset": 330}, {"referenceID": 5, "context": "For the pseudo-harmonic action translation mapping, in addition to showing that it outperforms the best prior approach in terms of exploitability in several games, we have also presented several axioms and theoretical properties that it satisfies; for example, it is Lipschitz continuous in A and B, and therefore robust to small changes in the actions used in the action abstraction [6].", "startOffset": 384, "endOffset": 387}, {"referenceID": 7, "context": ", selecting the highest-probability action with probability 1) leads to an improved performance in uniform random 4\u00d7 4 matrix games using random 3 \u00d7 3 abstractions when playing against the Nash equilibrium of the full 4 \u00d7 4 game for the opponent [8].", "startOffset": 246, "endOffset": 249}, {"referenceID": 7, "context": "Surprisingly, the improvements in empirical performance do not necessarily come at the expense of worst-case exploitability, and a degree of thresholding has been demonstrated to actually reduce exploitability for a limit Texas hold \u2019em agent [8].", "startOffset": 243, "endOffset": 246}], "year": 2017, "abstractText": "The first ever human vs. computer no-limit Texas hold \u2019em competition took place from April 24\u2013 May 8, 2015 at River\u2019s Casino in Pittsburgh, PA. In this article I present my thoughts on the competition design, agent architecture, and lessons learned.", "creator": "TeX"}}}