{"id": "1511.00736", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2015", "title": "ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction based on Graph Embedding in Structural and Topological Space", "abstract": "studying the sequence factor rna is important before understanding 3d molecular evolution of organization. the location of scarce available protein structures has virtually become extremely unreliable. still, geographic determination of the function facing a protein ligand paints a debate, thus, a time consuming science. engineering difficulties are often due to shifting common role with spatial and graph structures in precision determination of protein functions in living resources. in 2005 paper, we introduces protnn, for general development on mesh flow prediction. given several unannotated protein structure and a set of sample data, protnn finds generally necessary corresponding annotated result solely on protein graph parameter graphs. protnn shows to the query protein the function with often highest number of votes relatively large set of $ 60 $ nearest neighbor query components, where $ 0 $ establishes a user defined parameter. software output demonstrates that protnn is hard to accurately classify mathematical datasets in an extremely arbitrary runtime compared to made - of - the - art approaches.", "histories": [["v1", "Mon, 2 Nov 2015 23:02:48 GMT  (971kb,D)", "http://arxiv.org/abs/1511.00736v1", null], ["v2", "Mon, 25 Jan 2016 01:55:45 GMT  (1369kb)", "http://arxiv.org/abs/1511.00736v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.SI", "authors": ["wajdi dhifli", "abdoulaye banir\\'e diallo"], "accepted": false, "id": "1511.00736"}, "pdf": {"name": "1511.00736.pdf", "metadata": {"source": "CRF", "title": "ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction based on Graph Embedding in Structural and Topological Space", "authors": ["Wajdi Dhifli", "Abdoulaye Banir\u00e9 Diallo"], "emails": ["dhifli.wajdi@courrier.uqam.ca,", "diallo.abdoulaye@uqam.ca"], "sections": [{"heading": null, "text": "Keywords: Protein structures, function prediction, graph classification"}, {"heading": "1 Introduction", "text": "Proteins are ubiquitous in the living cells. They play key roles in the functional and evolutionary machinery of species. Studying protein functions is paramount for understanding the molecular mechanisms of life. The advances in sequencing techniques provide high amounts of biological data including protein structures. In fact, the number of proteins in the Protein Data Bank (PDB) [1] has more than tripled over the last decade. Alternative databases such as SCOP [2] and CATH [3] are undergoing the same trend. However, the determination of the function of protein structures remains a difficult, costly, and time consuming task. Manual protein functional classification methods are no longer able to follow the rapid increase of data. Accurate computational and machine learning tools present an efficient alternative that could offer considerable boosting to meet the increasing load of data.\nProteins are composed of complex three-dimensional folding of long chains of amino acids. This spatial structure is an essential component in protein functionality and is thus subject to evolutionary pressures to optimize the inter-residue\nar X\niv :1\n51 1.\n00 73\n6v 1\n[ cs\n.L G\n] 2\nN ov\n2 01\n5\ncontacts that support it [4]. Existing computational methods for protein function prediction try to simulate biological phenomena that define the function of protein. The most conventional technique is to perform a similarity search between an unknown protein and a reference database of proteins with known functions. The query protein is assigned with the same functional class of the most similar (based on the sequence or the structure) reference protein. There exists several classification methods based on the protein sequence (e.g. Blast [5], ...); or the protein structure (e.g. Combinatorial Extention [6], Sheba [7], FatCat [8], ...). These methods rely on the assumption that protein sharing the most common sites are more likely to share functions. This classification strategy is based on the hypothesis that structurally similar proteins could share a common ancestor [9]. Another popular approach for protein functional classification is to look for relevant substructures (also so-called motifs) among proteins with known functions, then use them as features to identify the function of unknown proteins. Such motifs could be discriminative [10], representative [11], cohesive [4], etc. Each of the mentioned protein functional classification approaches suffers different drawbacks. Sequence-based classification do not incorporate spatial information of amino acids that are not contiguous in the primary structure but interconnected in 3D space. This makes them less efficient in predicting the function for structurally similar proteins with low sequence similarity (remote homologues). Both structure and substructure-based classification techniques do incorporate spatial information in function prediction which makes them more efficient than sequence-based classification. However, such consideration makes these methods subject to the \u201dno free lunch\u201d principle [12], where the gain in accuracy comes with an offset of computational cost. Hence, it is essential to find an efficient way to incorporate 3D structure information with low time complexity.\nIn this paper, we present ProtNN, a novel approach for function prediction of protein 3D structures. ProtNN incorporates protein 3D structure information via the combination of a rich set of structural and topological descriptors that guarantee an informative multi-view representation of the structures that considers spatial information through different dimensions. Such a representation transforms the complex protein 3D structure into an attribute vector of fixed size allowing computational efficiency. For classification, ProtNN assigns to a query protein the function with the highest number of votes across the set of its k most similar reference proteins, where k is a user defined parameter. Experimental evaluation shows that ProtNN is able to accurately classify different benchmark datasets with a gain of up to 47x of computational cost compared to gold standard approaches from the literature."}, {"heading": "2 Methods", "text": ""}, {"heading": "2.1 Graph Representation of Protein 3D Structures", "text": "A crucial step in computational studies of protein 3D structures is to look for a convenient representation of their spatial conformations. Graphs represent the most appropriate data structure to model the complex structure of protein. In\nthis context, a protein 3D structure can be seen as a set of elements (amino acids and atoms) that are interconnected through chemical interactions [4,9,11]. Figure 1 shows a real example of the human hemoglobin protein and its graph representation. The Figure shows clearly that the graph representation preserves the overall structure of the protein and its components.\nProtein Graph Model Let G be a graph consisting of a set of nodes V and edges E. L is a label function that associates a label l to each node in V . Each node of G represents an amino acid from the 3D structure, and is labeled with its corresponding amino acid type. Let \u2206 be a function that computes the euclidean distance between pairs of nodes \u2206(u, v),\u2200u, v \u2208 V , and \u03b4 a distance threshold. Each node in V is defined by its 3D coordinates in IR3, and both \u2206 and \u03b4 are expressed in angstroms (A\u030a). Two nodes u and v (\u2200u, v \u2208 V ) are linked by an edge e(u, v) \u2208 E, if the distance between their C\u03b1 atoms is below or equal to \u03b4. Formally, the adjacency matrix A of G is defined as follows:\nAu,v = { 1, if \u2206(C\u03b1u , C\u03b1v ) \u2264 \u03b4 0, otherwise\n(1)"}, {"heading": "2.2 Structural and Topological Embedding of Protein Graphs", "text": "Graph Embedding In ProtNN, each protein 3D structure is represented by a graph according to Equation 1. Then, each graph is embedded into a vector of structural and topological features under the assumption that structurally similar graphs should give similar structural and topological feature-vectors. In such manner, ProtNN guarantees accuracy and computational efficiency. It is worth noting that even though structurally similar graphs should have similar\ntopological properties, ProtNN similarity should not necessarily give the same results of structure matching (as in structural alignment). But it should enrich it, since ProtNN considers even hidden similarities (like graph density and energy) that are not considered in structural matching.\nStructural and Topological Attributes In ProtNN, the pairwise similarity between two protein graphs is measured by the distance between their vector representations. In order to avoid the loss of structural information in the embedding, and to guarantee ProtNN accuracy, we use a set of structural and topological attributes from the literature that have shown to be interesting and efficient in describing connected graphs [13,14]. In the following, we present the list of the used attributes, see the Appendix for formal definitions: (A1) number of nodes, (A2) number of edges, (A3) average degree, (A4) density, (A5) average clustering coefficient, (A6) average effective eccentricity, (A7) effective diameter, (A8) effective radius, (A9) closeness centrality, (A10) percentage of central nodes, (A11) percentage of end points, (A12) number of distinct eigenvalues, (A13) spectral radius, (A14) second largest eigenvalue, (A15) energy, (A16) neighborhood impurity, (A17) link impurity, and (A18) label entropy."}, {"heading": "2.3 ProtNN: Nearest Neighbor Protein Functional Classification", "text": "The general classification pipeline of ProtNN can be described as follows: first a preprocessing is performed on the reference protein database \u2126 in which a graph model GP is created for each reference protein P , \u2200P \u2208 \u2126, according to Equation 1. A structural and topological description vector VP is created for each graph model GP , by computing the corresponding values of each of the structural and topological attributes described in Section 2.2. The resulting matrix M\u2126 =\u22c3 VP , \u2200P \u2208 \u2126, represents the preprocessed reference database that is used for prediction in ProtNN. In order to guarantee an equal participation of all used attributes in the classification, a min-max normalization (xnormalized = x\u2212min max\u2212min , where x is an attribute value, min and max are the minimum and maximum values for the attribute vector) is applied on each attribute of M\u2126 independently such that no attribute will dominate in the prediction. It is also worth mentioning that for real world applications M\u2126 is only computed once, and can be incrementally updated with other attributes as well as newly added protein 3D structures with no need to recompute the attributes for the entire set. This guarantees a high flexibility and easy extension of ProtNN in real world application. The prediction step in ProtNN is described in Algorithm 1."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Datasets", "text": "We use six benchmark datasets of protein structures that have previously been used in [10,15,16,17]. Each dataset is composed of positive protein examples\nDataset SCOP ID Family name Pos. Neg.\nDS1 48623 Vertebrate phospholipase A2 29 29 DS2 52592 G-proteins 33 33 DS3 48942 C1 set domains 38 38 DS4 56437 C-type lectin domains 38 38 DS5 56251 Proteasome subunits 35 35 DS6 88854 Protein kinases, catalyc subunits 41 41\nthat are from a selected protein family, and negative protein examples that are randomly sampled from the PDB [1]. The selected positive protein families are Vertebrate phospholipase A2, G-protein family, C1-set domains, C-type lectin domains, and protein kinases, catalytic subunits. Table 1 summarizes the characteristics of the six datasets."}, {"heading": "3.2 Protocol and Settings", "text": "Experiments were performed on CentOS Linux workstations with Intel core i7 CPU at 3.40 GHz, and 16.00 GB of RAM. To transform protein into graph, we used a \u03b4 value of 7A\u030a. The evaluation measure is the classification accuracy, and the evaluation technique is Leave-One-Out (LOO) where each dataset is used to create N classification scenarios, where N is the number of proteins in the dataset. In each scenario, a reference protein is used as a query instance and the rest of the dataset is used as reference. The aim is to correctly predict the class of the query protein. The classification accuracy for each dataset is averaged over results for all the N evaluations."}, {"heading": "4 Results and Discussion", "text": ""}, {"heading": "4.1 ProtNN Classification Results", "text": "Results Using Different Distance Measures We study the effect of varying the distance measure on the classification accuracy of ProtNN. We fixed k=1, and we used nine different distance measures namely Euclidean, standardized Euclidean (std-euclidean), Cosine, Manhattan, Correlation, Minkovski, Chebyshev, Canberra, and Braycurtis. See [18] for a formal definition of these measures. Figure 2 shows the obtained results. Overall, varying the distance measure did not\nsignificantly affect the classification accuracy of ProtNN for the six datasets. Indeed, the standard deviation of the classification accuracy of ProtNN with each distance measure did not exceed 3% on the six datasets. A ranking based on the average classification accuracy over the six datasets suggests the following descending order: (1) Manhattan, (2) Braycurtis, (3) std-Euclidean, (4) Canberra, (5) Cosine, (6) Euclidean - Minkowski, (8) Correlation, (9) Chebyshev.\nResults Using Different Numbers of Nearest Neighbors In the following, we evaluate the classification accuracy of ProtNN on each of the six benchmark datasets using different numbers of nearest neighbors k \u2208 [1,10]. The same experiment is performed using each of the top-five distance measures. For simplicity, we plot the average value of classification accuracy for each value of k \u2208 [1,10] over the six datasets using each of the top-five measures. Figure 3 shows the obtained results. The number of considered nearest neighbors k has a clear effect on the accuracy. The obtained results suggest that the \u201doptimal\u201d value of k \u2208 {1,2}. The overall accuracy tendency shows that it decreases with higher values of k. This is due to the structural similarity that a query protein may share with other evolutionary close proteins exerting different functions. High values of k engender considering too many neighbors which may causes a misclassification.\nAnalysis of the Used Attributes In the following, we study the importance of the used attributes in order to identify which ones are the most informative. We\nfollow the Recursive Feature Elimination (RFE) approach [19] with ProtNN as the classifier. In RFE, one feature is removed at each iteration, where the remaining features are the ones that best enhance the classification accuracy. The pruning stops when no further enhancement is observed or no more features are left. The remaining features constitute the optimal subset for that context. In Table 2, we record the ranking of the used attributes in our experiments. For more generalization, RFE was performed on each of the six datasets using a combination of each of the top-five distance measures and each of the topfive values of k. The total number of experiments is 150. For each attribute, we count the total number of times it appears in the optimal subset of attributes. A score of total countnumber of experiments is assigned to each attribute according to its total count. It is clear that best subset of attributes depends on the dataset. The five most informative attributes are respectively: A15 (energy), A17 (link impurity), A12 (number of distinct eigenvalues), A16 (neighborhood impurity), and A13 (spectral radius). All spectral attributes showed to be very informative. Indeed, three of them (A15, A12, and A13) ranked in the top-five, and A14 (second largest eigenvalue) ranked in the top-ten (9th) with a score of 0.52 meaning that for more than half of all the experiments, all spectral attributes were selected in\nthe optimal subset of attributes. Unsurprisingly, A11 (percentage of end points) ranked last with a very low score. This is because proteins are dense molecules and thus very few nodes of their respective graphs will be end points (extremity amino acids in the primary structure with no spatial links). Label attributes also showed to be very informative. Indeed, A17, A16, and A18 ranked respectively 2nd, 4th, and 6th with scores of more than 0.61. This is due to the importance of the distribution of the types of amino acids and their interactions. Both have to follow a certain harmony in order to exert a particular function. A9 (closeness centrality), A5 (average clustering coefficient) and A8 (effective radius) ranked in the top-ten with scores of more than 0.5 (A8 scored 0.49 ' 0.5). However, all A1 (number of nodes), A2 (number of edges), A3 (average degree), A4 (density), A6 (average effective eccentricity), A7 (effective diameter), and A10 (percentage of central nodes) scored less than 0.5. This is because each of these attributes is represented by one of the top-ten attributes and thus presents a redundant information. A6 and A9 are both expressed based on all shortest paths of the graph. Both A7 and A8 are expressed based on A6. A10 is expressed based on A8 and thus on A6 too. A1, A2, A3, and A4 are all highly correlated to A5.\nComparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]). For sequence and structural alignment-based classification, we align each protein against all the rest of the dataset. We assign to the query protein the function of the reference protein with the best hit score. For the substructure-based approaches, all the selected approaches are mainly for mining discriminative subgraphs. LPGBCMP is used with maxvar = 1 and d = 0.25 for, respectively, feature consistency map building and overlapping. For all these approaches, the discovered substructures are considered as features for describing each example of the original data. The constructed description matrix is used for training in the classification. For our approach, we show the classification accuracy results of ProtNN with RFE using std-Euclidean distance. We also show the best results of ProtNN (denoted ProtNN*) with RFE using each of the top-five distance measures. We use k = 1 both for ProtNN and ProtNN*. Table 3 shows the obtained results.\nThe alignment-based approaches FatCat and Sheba outperformed CE, Blast, and all the subgraph-based approaches. Except CE, all the other approaches scored on average better than Blast. This shows that the spatial information constitutes an important asset for functional classification. For the subgraphbased approaches, D&D scored better than LPGBCMP and GAIA on all cases except with DS1 where GAIA scored best. On average, ProtNN* ranked first with the smallest distance between its results and the best obtained accuracies with each dataset. This is because ProtNN considers both structural information, and hidden topological properties that are omitted by the other approaches."}, {"heading": "4.2 Scalability and Runtime Analysis", "text": "In this section, we study the computational cost of ProtNN and FatCat, the most competitive approach. We analyze the variation of runtime for both approaches with higher numbers of protein 3D-structures ranging from 10 to 100 proteins with a step-size of 10. In Figure 4, we report the runtime results in seconds (left) and in log10-scale (right). A huge gap is clearly observed between the runtime of ProtNN and that of FatCat. The gap gets larger with higher numbers of proteins. Indeed, FatCat runtime took over 5570 seconds with the 100 proteins while ProtNN runtime did not exceed 118 seconds for the same set which means that our approach is 47x faster than FatCat on that experiment. The average runtime of graph transformation of ProtNN was 0.8 second and that of the computation of attributes was 0.6 second for each protein. The total runtime of similarity search and function prediction of ProtNN was only 0.1 on the set of 100 proteins. In real world applications, the graph transformation and attribute computation for the reference database are computed only once and can be updated with no need to recompute the existing values. This ensures computational efficiency and easy extension of our approach."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed ProtNN, a new fast and accurate approach for protein function prediction. We defined a graph transformation and embedding model that incorporates explicit as well as hidden structural and topological properties of the 3D-structure of proteins. We successfully implemented the proposed model and we experimentally showed that it allows to detect similarity and to predict the function of protein 3D-structures efficiently. Empirical results of our experiments showed that considering structural information constitutes a major asset for identifying the functions of proteins correctly. They also showed that the alignment-based classification as well as subgraph-based classification present very competitive approaches. Yet, as the number comparisons between pairs of proteins grows tremendously with the size of dataset, enormous computational costs would be the results of more detailed models. ProtNN showed that it is able to accurately classify multiple benchmark datasets from the literature with very low computational costs.\nIn future works, we aim to integrating more protein information in our model to further enhance the accuracy of our function prediction system. We also plan to apply our approach on large scale dataset that includes the entire PDB, which will takes ProtNN beyond theoretical proposition to become a reference bioinformatics tool for real world applications."}, {"heading": "Appendix: Structural and Topological Attributes", "text": "In the following is the list of structural and topological attributes used in ProtNN:\nA1- Number of nodes: The total number of nodes of the graph |V |. A2- Number of edges: The total number of edges of the graph |E|. A3- Average degree: The degree of a node u, denoted deg(u), is the number its\nadjacent nodes. The average degree of a graph G is the average all deg(u), \u2200u \u2208 G. Formally: deg(G) = 1|V | \u2211|V | i=1 deg(ui).\nA4- Density: The density of a graph G = (V,E) measures how many edges are in E compared to the maximum possible number of edges between the nodes\nin V . Formally: den(G) = 2|E|(|V |\u2217(|V |\u22121)) .\nA5- Average clustering coefficient: The clustering coefficient of a node u, denoted c(u), measures how complete the neighborhood of u is, c(u) = 2euku(ku\u22121) where ku is the number of neighbors of u and eu is the number of connected pairs of neighbors. The average clustering coefficient of a graph G, is given\nas the average value over all its nodes. Formally: C(G) = 1|V | \u2211|V | i=1 c(ui).\nA6- Average effective eccentricity: For a node u, the effective eccentricity represents the maximum length of the shortest paths between u and every other node v in G, e(u) = max{d(u, v) : v \u2208 V, u 6= v}. The average effective eccentricity is defined as Ae(G) = 1|V | \u2211|V | i=1 e(ui). A7- Effective diameter: It represents the maximum value of effective eccentricity over all nodes in the graph G, i.e., diam(G) = max{e(u) | u \u2208 V } where e(u) represents the effective eccentricity of u as defined above. A8- Effective radius: The effective radius represents the minimum value of effective eccentricity over all nodes of G, rad(G) = min{e(u) | u \u2208 V }. A9- Closeness centrality: The closeness centrality measures how fast information spreads from a given node to other reachable nodes in the graph. For a node u, it represents the reciprocal of the average shortest path length between u and every other reachable node in the graph G, Cc(u) =\n|V |\u22121\u2211 v\u2208{V \\u} d(u,v) where d(u, v) is the length of the shortest path between the nodes u and v. For G, we consider the average value of closeness centrality of all its nodes, Cc(G) = 1 |V | \u2211|V | i=1 ui.\nA10- Percentage of central nodes: It is the ratio of the number of central nodes from the number of nodes in the graph. A node u is central if the value of its eccentricity is equal to the effective radius of the graph, e(u) = rad(G). A11- Percentage of end points: It represents the ratio of the number of nodes with deg(u) = 1 from the total number of nodes of G. A12- Number of distinct eigenvalues: The adjacency matrix A of G has a set of eigenvalues. We count the number of distinct eigenvalues of A. A13- Spectral radius: Let h1,h2, ...,hm be the set of eigenvalues of the adjacency matrix A of G. The spectral radius of G, denoted \u03c1(G), represents the largest magnitude eigenvalue, i.e., \u03c1(G) = max(| hi |) where i \u2208 {1, ..,m}. A14- Second largest eigenvalue: The value of the second largest eigenvalue. A15- Energy: The energy of an adjacency matrix A of a graph G is defined as\nthe squared sum of the eigenvalues of A. Formally: E(G) = \u2211m i=1 h 2 i .\nA16- Neighborhood impurity: For a node u having a label L(u) and a neighborhood N(u), it is defined as ImpDeg(u) =| L(v) : v \u2208 N(u), L(u) 6= L(v) |. The neighborhood impurity of G is the average ImpDeg over all nodes. A17- Link impurity: An edge {u, v} is considered to be impure if L(u) 6= L(v). The link impurity of a graph G with k edges is defined as: |{u,v}\u2208E:L(u) 6=L(v)|k . A18- Label entropy: It measures the uncertainty of labels. For a graph G of k labels, it is defined as E(G) = \u2212 \u2211k i=1 p(li) log p(li), where li is the i th label."}], "references": [{"title": "The protein data bank", "author": ["H.M. Berman", "J.D. Westbrook", "Z. Feng", "G. Gilliland", "T.N. Bhat", "H. Weissig", "I.N. Shindyalov", "P.E. Bourne"], "venue": "Nucleic Acids Research 28(1)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Data growth and its impact on the scop database: new developments", "author": ["A. Andreeva", "D. Howorth", "J.M. Chandonia", "S.E. Brenner", "T.J.P. Hubbard", "C. Chothia", "A.G. Murzin"], "venue": "Nucleic Acids Research 36(1)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "CATH: comprehensive structural and functional annotations for genome sequences", "author": ["I. Sillitoe", "T.E. Lewis", "A.L. Cuff", "S. Das", "P. Ashford", "N.L. Dawson", "N. Furnham", "R.A. Laskowski", "D. Lee", "J.G. Lees", "S. Lehtinen", "R.A. Studer", "J.M. Thornton", "C.A. Orengo"], "venue": "Nucleic Acids Research 43(Database-Issue)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Mining the entire protein databank for frequent spatially cohesive amino acid patterns", "author": ["P. Meysman", "C. Zhou", "B. Cule", "B. Goethals", "K. Laukens"], "venue": "BioData Mining 8", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Basic local alignment search tool", "author": ["S. Altschul", "W. Gish", "W. Miller", "E. Myers", "D. Lipman"], "venue": "Journal of Molecular Biology 215", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Protein structure alignment by incremental combinatorial extension of the optimum path", "author": ["I.N. Shindyalov", "P.E. Bourne"], "venue": "Protein Engineering 11(9)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Protein structure alignment using environmental profiles", "author": ["J. Jung", "B. Lee"], "venue": "Protein Engineering 13", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Flexible structure alignment by chaining aligned fragment pairs allowing twists", "author": ["Y. Ye", "A. Godzik"], "venue": "Bioinformatics 19", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Protein function prediction via graph kernels", "author": ["K.M. Borgwardt", "C.S. Ong", "S. Sch\u00f6nauer", "S.V.N. Vishwanathan", "A.J. Smola", "H. Kriegel"], "venue": "Proceedings Thirteenth International Conference on Intelligent Systems for Molecular Biology 2005, Detroit, MI, USA, 25-29 June 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Graph classification: a diversified discriminative feature selection approach", "author": ["Y. Zhu", "J.X. Yu", "H. Cheng", "L. Qin"], "venue": "21st ACM International Conference on Information and Knowledge Management, ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Smoothing 3D protein structure motifs through graph mining and amino-acids similarities", "author": ["W. Dhifli", "R. Saidi", "E. Mephu Nguifo"], "venue": "Journal of Computational Biology 21(2)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "No free lunch theorems for optimization", "author": ["D. Wolpert", "W.G. Macready"], "venue": "IEEE Transactions on Evolutionary Computation 1(1)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Graphs over time: densification laws, shrinking diameters and possible explanations", "author": ["J. Leskovec", "J. Kleinberg", "C. Faloutsos"], "venue": "eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Effective graph classification based on topological and label attributes", "author": ["G. Li", "M. Semerci", "B. Yener", "M.J. Zaki"], "venue": "Statistical Analysis and Data Mining 5(4)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Boosting with structure information in the functional space: an application to graph classification", "author": ["H. Fei", "J. Huan"], "venue": "ACM knowledge discovery and data mining conference (KDD).", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Graph classification based on pattern co-occurrence", "author": ["N. Jin", "C. Young", "W. Wang"], "venue": "ACM International Conference on Information and Knowledge Management.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "GAIA: graph classification using evolutionary computation", "author": ["N. Jin", "C. Young", "W. Wang"], "venue": "Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning SciPy for Numerical and Scientific Computing - Second Edition", "author": ["Sergio J. Rojas G.", "Erik A Christensen", "F.J.B.S."], "venue": "Community experience distilled. Packt Publishing", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine Learning 46(1-3)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "In fact, the number of proteins in the Protein Data Bank (PDB) [1] has more than tripled over the last decade.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "Alternative databases such as SCOP [2] and CATH [3] are undergoing the same trend.", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "Alternative databases such as SCOP [2] and CATH [3] are undergoing the same trend.", "startOffset": 48, "endOffset": 51}, {"referenceID": 3, "context": "contacts that support it [4].", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "Blast [5], .", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Combinatorial Extention [6], Sheba [7], FatCat [8], .", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "Combinatorial Extention [6], Sheba [7], FatCat [8], .", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "Combinatorial Extention [6], Sheba [7], FatCat [8], .", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "This classification strategy is based on the hypothesis that structurally similar proteins could share a common ancestor [9].", "startOffset": 121, "endOffset": 124}, {"referenceID": 9, "context": "Such motifs could be discriminative [10], representative [11], cohesive [4], etc.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "Such motifs could be discriminative [10], representative [11], cohesive [4], etc.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Such motifs could be discriminative [10], representative [11], cohesive [4], etc.", "startOffset": 72, "endOffset": 75}, {"referenceID": 11, "context": "However, such consideration makes these methods subject to the \u201dno free lunch\u201d principle [12], where the gain in accuracy comes with an offset of computational cost.", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "this context, a protein 3D structure can be seen as a set of elements (amino acids and atoms) that are interconnected through chemical interactions [4,9,11].", "startOffset": 148, "endOffset": 156}, {"referenceID": 8, "context": "this context, a protein 3D structure can be seen as a set of elements (amino acids and atoms) that are interconnected through chemical interactions [4,9,11].", "startOffset": 148, "endOffset": 156}, {"referenceID": 10, "context": "this context, a protein 3D structure can be seen as a set of elements (amino acids and atoms) that are interconnected through chemical interactions [4,9,11].", "startOffset": 148, "endOffset": 156}, {"referenceID": 12, "context": "In order to avoid the loss of structural information in the embedding, and to guarantee ProtNN accuracy, we use a set of structural and topological attributes from the literature that have shown to be interesting and efficient in describing connected graphs [13,14].", "startOffset": 258, "endOffset": 265}, {"referenceID": 13, "context": "In order to avoid the loss of structural information in the embedding, and to guarantee ProtNN accuracy, we use a set of structural and topological attributes from the literature that have shown to be interesting and efficient in describing connected graphs [13,14].", "startOffset": 258, "endOffset": 265}, {"referenceID": 9, "context": "We use six benchmark datasets of protein structures that have previously been used in [10,15,16,17].", "startOffset": 86, "endOffset": 99}, {"referenceID": 14, "context": "We use six benchmark datasets of protein structures that have previously been used in [10,15,16,17].", "startOffset": 86, "endOffset": 99}, {"referenceID": 15, "context": "We use six benchmark datasets of protein structures that have previously been used in [10,15,16,17].", "startOffset": 86, "endOffset": 99}, {"referenceID": 16, "context": "We use six benchmark datasets of protein structures that have previously been used in [10,15,16,17].", "startOffset": 86, "endOffset": 99}, {"referenceID": 0, "context": "that are from a selected protein family, and negative protein examples that are randomly sampled from the PDB [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 17, "context": "See [18] for a formal definition of these measures.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "Results Using Different Numbers of Nearest Neighbors In the following, we evaluate the classification accuracy of ProtNN on each of the six benchmark datasets using different numbers of nearest neighbors k \u2208 [1,10].", "startOffset": 208, "endOffset": 214}, {"referenceID": 9, "context": "Results Using Different Numbers of Nearest Neighbors In the following, we evaluate the classification accuracy of ProtNN on each of the six benchmark datasets using different numbers of nearest neighbors k \u2208 [1,10].", "startOffset": 208, "endOffset": 214}, {"referenceID": 0, "context": "For simplicity, we plot the average value of classification accuracy for each value of k \u2208 [1,10] over the six datasets using each of the top-five measures.", "startOffset": 91, "endOffset": 97}, {"referenceID": 9, "context": "For simplicity, we plot the average value of classification accuracy for each value of k \u2208 [1,10] over the six datasets using each of the top-five measures.", "startOffset": 91, "endOffset": 97}, {"referenceID": 0, "context": "Tendancy of average accuracy of ProtNN for each value of k \u2208 [1,10] over the six datasets and using each of the top-five distance measures.", "startOffset": 61, "endOffset": 67}, {"referenceID": 9, "context": "Tendancy of average accuracy of ProtNN for each value of k \u2208 [1,10] over the six datasets and using each of the top-five distance measures.", "startOffset": 61, "endOffset": 67}, {"referenceID": 18, "context": "follow the Recursive Feature Elimination (RFE) approach [19] with ProtNN as the classifier.", "startOffset": 56, "endOffset": 60}, {"referenceID": 4, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 207, "endOffset": 210}, {"referenceID": 5, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 291, "endOffset": 294}, {"referenceID": 6, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 302, "endOffset": 305}, {"referenceID": 7, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 318, "endOffset": 321}, {"referenceID": 16, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 384, "endOffset": 388}, {"referenceID": 14, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 398, "endOffset": 402}, {"referenceID": 9, "context": "Comparison with Other Classification Techniques We compare our approach with multiple state-of-the-art approaches for protein function prediction namely: sequence alignment-based classification (using Blast [5]), structural alignment-based classification (using Combinatorial Extension (CE) [6], Sheba [7], and FatCat [8]), and substructure(subgraph)-based classification (using GAIA [17], LPGBCMP [15], and D&D [10]).", "startOffset": 412, "endOffset": 416}], "year": 2017, "abstractText": "Studying the function of proteins is important for understanding the molecular mechanisms of life. The number of publicly available protein structures has increasingly become extremely large. Still, the determination of the function of a protein structure remains a difficult, costly, and time consuming task. The difficulties are often due to the essential role of spatial and topological structures in the determination of protein functions in living cells. In this paper, we propose ProtNN, a novel approach for protein function prediction. Given an unannotated protein structure and a set of annotated proteins, ProtNN finds the nearest neighbor annotated structures based on protein graph pairwise similarities. ProtNN assigns to the query protein the function with the highest number of votes across the set of k nearest neighbor reference proteins, where k is a user defined parameter. Experimental evaluation demonstrates that ProtNN is able to accurately classify several datasets in an extremely fast runtime compared to state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}