{"id": "1511.02580", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "How far can we go without convolution: Improving fully-connected networks", "abstract": "we write ways whereby extend intrinsic performance of fully shared robots. tricks spin around two approaches have particular have a strong effect on robotics : linear bottleneck learning of unsupervised buddy - systems using autoencoders without hidden task cycles. there show that both approaches can reduce realized after average gradient coverage \u2014 reducing sensitivity reduces traction net. we show that a fully connected mesh did use approximately 70 % classification accuracy ; the model - decision cifar - 10 task, which outputs just higher than predicted current state - intervention - simulation - transition. by adding deformations to the training data, perfect fully connected network performs 78 % accuracy, which is just 10 % reflective of a decent convolutional network.", "histories": [["v1", "Mon, 9 Nov 2015 06:56:24 GMT  (493kb,D)", "http://arxiv.org/abs/1511.02580v1", "10 pages, 11 figures, submitted for ICLR 2016"]], "COMMENTS": "10 pages, 11 figures, submitted for ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["zhouhan lin", "roland memisevic", "kishore konda"], "accepted": false, "id": "1511.02580"}, "pdf": {"name": "1511.02580.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zhouhan Lin", "Roland Memisevic"], "emails": ["roland.memisevic}@umontreal.ca", "konda.kishorereddy@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Convolutional neural networks (CNN) have had huge successes since Krizhevsky et al. (2012), especially in computer vision applications. The main computational idea behind these, weight sharing, is unfortunately not biologically plausible, and it does not map nicely to simple, densely parallel hardware designs, which may one day yield lower-energy, and more efficient ways to run neural networks. The reason is that weight sharing requires long-range communication, for example, when distributing derivatives, during learning.\nIn this paper, we explore the performance that one can possibly achieve with a neural network without convolutions, in other words, a neural network learning on \u201cpermutation invariant\u201d classification tasks. We use these as a test-bed for studying alternative architectural design choices beside convolution, which are biologically plausible and potentially more hardware friendly. Studying these design choices is relevant also because many tasks and sub-tasks do not come with any local translation invariance, making them not amenable to convolution (an example being the hidden layer in a recurrent neural network).\nTwo architectural choices we found to have a strong impact on performance are (i) linear bottleneck layers and (ii) pre-training using autoencoders whose hidden units have no biases. Taken together, these two approaches allow us a network to achieve state-of-the-art performance on the permutationinvariant CIFAR-10 task 1, and by adding deformations (thus removing the permutation invariance requirement) it achieves performance close to the range of that achieved by CNNs."}, {"heading": "1.1 SPARSITY IN NEURAL NETWORKS", "text": "Both approaches can be viewed from the perspective of sparsity in a neural network. Sparsity is commonly considered as a desirable property in machine learning models, and it is often encouraged\n1An example implementation that generates the state-of-the-art on this task is available at https: //github.com/hantek/zlinnet\nar X\niv :1\n51 1.\n02 58\n0v 1\n[ cs\n.L G\n] 9\nN ov\nexplicitly as a regularizer. Unfortunately, in deep, multilayer neural networks sparsity comes at a price: in common activation functions, such as sigmoid or ReLU, zero (or almost zero) activations are attained at values where derivatives are zero, too. This prevents derivatives from flowing through inactive hidden units, and makes the optimization difficult. Stated in terms of the vanishing gradients problem (e.g., Hochreiter et al. (2001)) this means that for a ReLU activation many Jacobians are diagonal matrices containing many zeros along the diagonal.\nTo alleviate this problem, recently several activation functions were proposed, where zero derivatives do not, or are less likely to, occur. In the PReLU activation function (He et al., 2015), for example, the zero-derivative regime is replaced by a learned, and typically small-slope, linear activation. Another approach is the Maxout activation (Goodfellow et al., 2013), which is defined as the maximum of several linear activations. Accordingly, preventing zero-derivatives this way was shown to improve the optimization and the performance of multilayer networks. Unfortunately, these methods solve the problem by giving up on sparsity altogether, which raises the question whether sparsity is simply not as desirable as widely assumed or whether the benefit of the optimization outweigh any detrimental effect on sparsity. Sparsity also plays a crucial role in unsupervised learning (which can also be viewed as a way to help the optimization (Saxe et al., 2013)), where these activations have accordingly never been successfully applied.\nThis view of sparsity motivates us to revisit a simple, but as we shall show surprisingly effective, approach to retaining the benefits of sparsity without preventing gradient-flow. The idea is to interleave linear, low-dimensional layers with the sparse, high-dimensional layers containing ReLU or sigmoid activations. We show that this approach outperforms equivalent PReLU and Maxout networks on the fully supervised, permutation invariant CIFAR-10 task.\nA second detrimental side-effect of sparsity, discussed in more detail in Konda et al. (2014), is that for a ReLU or sigmoid unit to become sparse it typically learns strongly negative bias-terms. In other words, while sparse activations can be useful in terms of the learning objective (for example, by allowing the network to carve up space into small regions) it forces the network to utilize bias terms that are not optimal for the representation encoded in hidden layer activations. Konda et al. (2014), for example, suggest bias-terms equal to zero to be optimal and propose a way to train an autoencoder with zero-bias hidden units. We suggest in Section 3 that pre-training a network with these autoencoders may be viewed as a way to orthogonalize subsets of weights in the network, and show that this yields an additional strong performance improvement.\nWe shall discuss the motivation for linear bottleneck layers in the next section and pre-training using autoencoders in Section 3, followed by experimental results in Section 4."}, {"heading": "2 LINEAR BOTTLENECK LAYERS", "text": "One drawback of sparsity in a deep network is that it amounts to data scarcity: a weight whose post-synaptic unit is off 80% of the time will effectively get to see only 20% of the training data. In lower layers of a convolutional network, this problem is compensated by weight-sharing, which increases the effective number of training examples (patches in that case) per weight by a factor of several thousand. In a fully connected layer (more precisely, a layer without weight sharing) such compensation is not possible and the only way to counter this effect would be by increasing the training set size.\nSparsity is a common and stable effect in neural networks containing ReLU or sigmoid units, and it can be related to the fact the biases are driven to zero in response to regularization (eg., Konda et al. (2014)). Figure 1 (left) shows the sparsity level attained after training ReLU MLPs. 2 The plots show that the sparsity of the hidden presentation increases with the depth of the network and increase further in the presence of dropout.\n2Two MLPs (1000-2000-3000 units and 2000-2000-2000 units, respectively), trained for 501 epochs on CIFAR-10. Each network was trained once with dropout and once without. For the experiments with dropout, an input noise level of 0.2 and hidden noise level of 0.5 was used."}, {"heading": "2.1 LINEAR LAYERS AS DISTRIBUTION RESHAPING", "text": "It is well-known that different activation functions work best when the input is in a reasonable scale, which is the motivation for using pre-processing by mean subtraction and scaling, for example. Furthermore, incorporating them into hidden unit activation, can yield further performance improvements (Ioffe & Szegedy, 2015).\nHere, we focus on the distribution over the outputs of a unit. The output of a linear neuron Y with inputs ~X = (X1, X2, ..., Xi, XN ) is given by Y = \u2211N i wiXi + b, where wi is the entry in the weight vector corresponding to input node i, b is the bias of the linear neuron, and N denotes the input dimension. Assume the data fed into each input node Xi is independent and has a finite mean \u00b5i and variance \u03c32i . In the net input to a neuron, the mean and variance of each wiXi term are tuned by its corresponding weight values:\n\u00b5\u0302i = wi\u00b5i; \u03c3\u0302i 2 = w2i \u03c3 2 i (1)\nAccording to the Lyapunov theorem (DeGroot et al., 1986), if a sequence of random variables with finite mean and variance are independent but not necessarily identically distributed, the distribution of the sum:\nS = ( N\u2211\ni=1 wiXi \u2212 N\u2211 i=1 \u00b5\u0302i )( N\u2211 i=1 \u03c3\u0302i 2 )\u2212 12\n(2)\ntends to standard Gaussian distribution for N \u2212\u2192 \u221e. If we write Y in terms of S, that is, Y =(\u2211N i=1 \u03c3\u0302i 2 ) 1 2 S + b + \u2211N i=1 \u00b5\u0302i we see that Y approximates a Gaussian distribution whose mean\nis b + \u2211N i=1 \u00b5\u0302i and whose variance equals to \u2211N i=1 \u03c3\u0302i 2, when the input dimension N \u2212\u2192 \u221e. For mean-centered data, we have \u00b5\u0302i \u2248 0. Thus the actual mean value of Y is merely dominated by b in which case the p.d.f. of Y is:\npLin (y) \u2248 1\u221a\n2\u03c0 \u2211N i=1 \u03c3\u0302i 2 e \u2212 (y\u2212b)\n2 2 \u2211N i=1 \u03c3\u03022 i (3)\nThis form of asymptotic distribution holds regardless of the weight wi. That means, no matter how the network is trained, or even not trained, the asymptotic distribution of output of a linear unit tends to be Gaussian.\nSince the pre-hidden activation of ReLU is linear it tends to be Gaussian as well. The ReLU activation then simply sets all negative values to zero which yields:\npReLU (y) = {\u222b 0 \u2212\u221e pLin (y) dy \u00b7 \u03b4(0), y \u2264 0 pLin (y) , y > 0\n(4)\nwhere \u03b4(0) is the Dirac delta. The distribution has a delta spike at zero and a Gaussian tail at its positive end (Figure 2).\nSince the bias controls the intensity of the \u03b4(0) spike, it controls the sparsity of output representation (left plot). The observation that biases tend to zero motivated Konda et al. (2014) to introduce a \u201czero-bias\u201d activation function which uses a fixed threshold followed by linear activation. Typically while using zero-bias ReLU, a threshold of 1 is introduced during pre-training stage, and set back to zero while training its subsequent layers and fine-tuning. The distribution of pre-hidden activity of a zero-bias ReLU stretches equally on both sides of zero. As a result, for zero-bias ReLU activation half of the probability mass concentrates on a delta spike located at zero, as illustrated it Figure 2 (right). While batch normalization alone will push the mean and variance into the optimal range for a subsequent ReLU unit, it will not resolve the issue that the distribution is peaked at the negative bias. In fact, the estimate of mean and variance will suffer from the presence of a highly non-Gaussian distribution. Typical histograms over hidden unit activations for several activation functions are shown in Figure 3a."}, {"heading": "2.2 DERIVATIVES IN THE PRESENCE OF LINEAR LAYERS", "text": "We now consider the derivatives of a ReLU network. The activation of the i-th layer is given by: ~Hi+1 = R(wi ~Hi + ~bi), (5)\nwhere R() is the element-wise activation function. The back-propagated updates on wi will be: \u2206wi = \u03b4 \u25e6R \u2032 ( ~Hi+1 ) \u00b7 ~Hi (6)\nwhere \u03b4 stands for the down flowing error signal coming from the upper layer, \u25e6 stands for elementwise multiply, and \u00b7 is the product of a vertical vector and a horizontal vector. As discussed in Section 2.1, at least 50% (usually much more in practice due to negative biases) of the values in the representation Hi and Hi+1 are typically equal to zero (cf., Figure 1, left) making a large fraction of the entries of \u2206wi per training case zero. If we introduce a linear layer between two ReLU layers,\n~Hl = wl ~Hi + ~bl ~Hi+1 = R ( wi ~Hl + ~bi ) (7)\nwhere ~Hl stands for the output of linear layer, and wl, bl are the weights and biases in the linear layer, we obtain updates of the form:\n\u2206wl = \u03b4l \u00b7 ~Hi , \u2206wi = \u03b4 \u25e6R \u2032 ( ~Hi+1 ) \u00b7 ~Hl (8)\nwhere \u03b4l is the error signal passed to the linear layer, \u03b4l = \u03b4wi. Since the linear layer representation ~Hl is dense, both \u2206wl and \u2206wi become denser: only half of the values in these two update matrices are zeros. More importantly, there always exists a simple ReLU layer that is equivalent to a ReLU/linear combination because any linear layer can be absorbed into the weights of the ReLU layer: w = wiwl, b = wibl + bi (9) Then, the equivalent update on w becomes:\n\u2206w = \u2206wi\u2206wl + wi\u2206wl + \u2206wiwl (10)\nEven if half of the values in \u2206wi and \u2206wl are zero, their product is a dense matrix. The second and third term in Equation 10 are also dense, so with a linear bottleneck layer, we actually obtain a dense update in an equivalent ReLU layer."}, {"heading": "2.3 REDUCING PARAMETERS", "text": "Besides helping with back propagation, it is important to note that linear layers also reduce the total number of parameters. Suppose a linear layer with L units is inserted between two nonlinear layers with N units each. The total number of parameters would become 2NL+L+N . This is much less than the number of parameters that would result from directly connecting the two nonlinear layer, which would amount to N2 + N parameters. Convolutional network also reduces the number of parameters by convolution kernels. Note that for any trained convolutional network, we can always find a fully connected network with the same accuracy by expanding the convolutional kernels."}, {"heading": "3 PRE-TRAINING AND ZERO-BIAS ACTIVATIONS", "text": "It was suggested by Saxe et al. (2013) that the benefit of unsupervised pre-training of a network using RBMs or autoencoders may result in weight matrices that are closer to orthogonal and thus less affected by vanishing gradients problems. It is interesting to note, however, that the sparsityinducing negative biases yield reconstructions that are affine not linear and accordingly may not orthogonalize weights after all (Konda et al., 2014). This may be one of the reasons why the practical success of these pre-training schemes has been quite limited by comparison to fully supervised learning using back propagation.\nIt is also important to note that due to sparsity, the number of active units in a layer is often smaller than that the same number in the layer below, so the \u201ceffective\u201d weight matrix for a given input example is not a square matrix. Rather than simply orthogonal weight matrices, we should be looking for networks where hidden units which tend to be active on the same inputs have weights that tend to be orthogonal to one another. In other words, we should be looking for \u201corthogonal active paths\u201d through the network rather than overall orthogonal weight matrices.\nIn order to obtain hidden units with linear not affine encodings Konda et al. (2014) introduce \u201czerobias autoencoders\u201d (ZAE) whose activations are threshold-ed when pre-trained by minimizing the autoencoder reconstruction error, and whose thresholds are removed when the weights are used for classification or for initializing a feed forward network. As discussed in Konda et al. (2014) minimizing squared error under the linear encoding should encourage weights corresponding to hidden units that tend to be \u201con\u201d together to orthogonalize (because the orthogonal projection is what minimizes reconstruction error). Konda et al. (2014) reported decent classification performance in various supervised tasks, but found only a weak improvement over standard autoencoders when used to initialize a single-hidden-layer MLP.\nThe view of a zero-bias activation function as a way to orthogonalize weights suggests especially deep networks to profit from ZAE pre-training, and so we investigated the performance of ZAEpretrained networks with many hidden layers, as well as in conjunction with interleaved bottleneck layers (as discussed in Section 2). We found them to yield a separate, and significant, performance improvement in fully connected networks."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 CIFAR-10", "text": "The CIFAR-10 dataset is a subset of the 80 Million Tiny Images Torralba et al. (2008), and contains 10 balanced classes. It provides a training set with 50000 samples and a test set of 10000 samples. Each sample is a color image with 32\u00d7 32 RGB pixels. We first compare mixed models with linear bottleneck layers and ordinary networks. We compare two different activation functions: ReLU and zero-bias ReLU (Konda et al., 2014). Comparison are based on classification accuracy. All the experiments in this subsection share the same pre-processing pipeline and the same type of classifier. For pre-processing, the raw data is contrast normalized and centered to have zero mean, followed by PCA whitening, retaining 99% of the variance.\nWe trained 3 ReLU networks with different configurations, and compare their results with the ReLULin network 4000ReLU \u2212 1000Linear \u2212 4000ReLU, that interleaves a linear layer between two ReLU layers. All networks in Figure 4a are trained by supervised back propagation using stochastic gradient descent with 0.9 momentum. The ReLU-Lin network outperforms all the pure ReLU networks and reaches an accuracy of 56.84%. We also found that the ReLU-Lin network tends to be more stable than the pure ReLU networks: if we pre-train these networks layer-wise, then for the ReLU networks the majority of the units in higher layers become \u201cdead\u201d, whereas the ReLU-Lin network is still stable for unsupervised pre-training without dead units at the second ReLU layer.\nWe repeat the same experiment by substituting ReLU with ZAE (Figure 4b). The Z-Lin network is configured as 4000Z \u2212 1000Linear \u2212 4000Z (same as in Figure 1, but with only one Z-Lin pair). Since ZAE does not suffer from the problem of dead units that much as ReLU, all the networks are first layer-wise pre-trained in an autoencoder, and then fine-tuned with stochastic gradient descent. During pre-training the linear layer, we have 1.0 weight decay added to the cost. Similar to the case of ReLU, it is also observed that introducing linear bottleneck layers makes the stacked deeper model outperform its shallow counterpart. Having a linear inserted in the middle makes the Z-Lin model outperform all the other models, yielding an accuracy of 65.76%.\nWe also compare the bottleneck layer structure with other related modifications like PReLU (He et al., 2015) and Maxout (Goodfellow et al., 2013), the bottleneck structure performs better than both of them, if we apply it on the same network architecture. In comparison to batch normalization\n(Ioffe & Szegedy, 2015) (but on fully connected networks) on the other hand, their improvements are about the same."}, {"heading": "4.2 THE HIGGS DATASET", "text": "The HIGGS dataset has 11 million samples with 28 dimensions. The first 21 features are kinematic properties measured by particle detectors in a particle accelerator, and the remaining 7 features are functions of the first 21 features. Thus the dataset itself is permutation invariant. The task is to decide if or not a given sample corresponds to a Higgs Boson.\nWe tried both ReLU and ZAE with/without linear bottleneck layers on this dataset. Similar to before, for each model PCA whitening retaining 99% of the variance is used for pre-processing. (corresponding to 27 principle components). We use the same model size for four different models that we compare (zero-bias ReLU, Z-Lin, ReLU, and ReLU-Lin). The structure is 27\u2212800\u2212100\u2212 800 \u2212 100 \u2212 2 for all, so the models differ by using different activation functions. We train all the models using SGD with momentum, and tune learning rates individually. We do not use any pretraining. The results are shown in Figure 4c, which also confirm the effectiveness of the bottleneck layers, albeit not as pronounced as on the CIFAR-10 data. Also, zero-bias units do not yield an improvement here."}, {"heading": "4.3 REDUCING PARAMETERS", "text": "In this subsection we explore how the network\u2019s performance is affected by increasingly reducing the number of parameters, using bottlenecks of different sizes.\nWe define a network by stacking 3 Z-LIN pairs, plus a ZAE layer and a logistic regression classifier. Each ZAE layer has 4000 hidden units. We reduce the linear layer size from 1000 down to 100 units. Training involves dropout, pre-training and fine-tuning. The results are shown in Figure 5. We observe that even with a hidden layer of size 100, which has only 1/20 of the parameters, the network still works reasonably well and does not loose too much accuracy.\nA 7-layer fully connected network with 4000 units each layer has around 112 million parameters. With linear layers, the smallest model in Figure 5 has only around 2.44 million parameters. By comparison, a typical convolutional network yielding an accuracy higher than 80% on CIFAR10 would have around 3.5 million parameters."}, {"heading": "4.4 ACHIEVING STATE-OF-THE-ART ON PERMUTATION INVARIANT CIFAR-10", "text": "We compare the performance of the Z-Lin model with other published methods on the CIFAR-10. For our method, pre-processing steps are the same to Section 4.1. For the Z-Lin network, each ZAE layer 4000 hidden units, and each linear autoencoder has 1000 hidden units. We use logistic regression on top of the last ZAE layer for classification. Following Konda et al. (2014), the threshold of all ZAEs are fixed at 1.0 during pre-training, and set to 0 while training the subsequent layers and performing fine-tuning. As before, we subtract the mean value and normalize the activations to have a standard deviation of 1.0 in all layers.\nWe train the networks using stochastic gradient descent with 0.9 momentum and decreasing learning rate. The learning rate is set to 0.001 for the ZAEs and to 0.0001 for the linear autoencoder. Weight decay is used for the linear autoencoder and for the logistic regression layer. The latter is trained using nonlinear conjugate gradients. After pre-training, we use a tiny learning rate of 5 \u00d7 10\u22126 to fine-tune the whole network, which yields an overall accuracy of 65.7%. This already outperforms all previous published permutation invariant CIFAR-10 results, the next best-performing of which are 63.1% (Le et al., 2013), and 63.9% (Konda et al., 2014).\nBy adding dropout (Srivastava et al., 2014) during pre-training and fine-tuning, and using a very deep Z-LIN network (3 Z-LIN pairs, plus a ZAE layer and logistic regression classifier, i.e., 4000Z\u2212 1000Lin\u22124000Z\u22121000Lin\u22124000Z\u22121000Lin\u22124000Z\u221210) the performance improves to 69.62%, which exceeds the current state-of-the-art on permutation invariant CIFAR10 by a very large margin.\nIf we give up on permutation-invariance by using data augmentation (eg., Krizhevsky et al. (2012)) but retain the use of a fully connected network, the performance improves much further. Here, we add flipping, rotation, and shifting to the original data during training of a 4000Z \u2212 1000Lin \u2212 4000Z \u2212 10 Z-LIN network pushing the performance to 78.62%. This is a much higher accuracy on CIFAR-10 achieved by any fully connected network we are aware of, and it is only about 10% behind a well-trained convolutional network."}, {"heading": "5 RELATED WORK", "text": "The idea of a linear bottleneck layer is very old and has been used as early as the 1980\u2019s in the context of autoencoders (eg., Baldi & Hornik (1989)). More recently, Ba & Caruana (2014) used a linear bottleneck layer to factorize a single-layer network and showed that it helped speed up learning. An application of a linear bottleneck layer in the last layer of a neural network for dealing with highdimensional outputs is described in Sainath et al. (2013) and Xue et al. (2013). In contrast to our work, in none of these methods is the goal to alleviate vanishing gradients and deal with sparsity, and (accordingly) they use just a single bottleneck layer in the network.\nReshape distributions over activations using linear layers is also related to the recently introduced batch-normalization trick (Ioffe & Szegedy, 2015), in that it is also a way to adjust the distribution of inputs to a subsequent layer. In contrast to that work, linear bottleneck layers not only adjust the mean and variance of the inputs to the subsequent layer, but reshape the whole distribution."}, {"heading": "6 DISCUSSION", "text": "It is well known that a single-hidden-layer neural network can model any non-linear function under mild conditions (Funahashi, 1989; Cybenko, 1989). The intuition behind this observation is that the hidden layer carves up space into half-spaces, or \u201ctiles\u201d, and the subsequent linear layer composes the non-linear function by combining different linear regions to produce the output. It is interesting to note that this view may suggest using a pair of layers (a non-linear followed by a linear layer) to define the non-linear function, leading thus to interleaved linear/non-linear layers.\nThe practical usefulness of this result is limited, however, because to approximate any given function it would require an exponentially large number of hidden units. In practice, this is one motivation for using multilayer networks which compute a sequence of consequently more restricted, but tractable non-linear functions. Arguably, in the presence of enough training data and computational resources, wide hidden layers would still be preferable to narrow ones. However, in practice, wider hidden layers also entail more sparsity, which prevents the flow of derivatives. Also, as sparse activations propagate upwards through the network, they tend to proliferate, aggravating the problem in higher layers."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank the developers of Theano (Bastien et al., 2012). We acknowledge the support of the following agencies for research funding and computing support: Samsung, NSERC, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs and CIFAR."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["Ba", "Jimmy", "Caruana", "Rich"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural networks,", "citeRegEx": "Baldi and Hornik,? \\Q1989\\E", "shortCiteRegEx": "Baldi and Hornik", "year": 1989}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["Cybenko", "George"], "venue": "Mathematics of control, signals and systems,", "citeRegEx": "Cybenko and George.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko and George.", "year": 1989}, {"title": "On the approximate realization of continuous mappings by neural networks", "author": ["Funahashi", "Ken-Ichi"], "venue": "Neural networks,", "citeRegEx": "Funahashi and Ken.Ichi.,? \\Q1989\\E", "shortCiteRegEx": "Funahashi and Ken.Ichi.", "year": 1989}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Hochreiter", "Sepp", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "J\u00fcrgen"], "venue": "long-term dependencies,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Zero-bias autoencoders and the benefits of co-adapting", "author": ["Konda", "Kishore", "Memisevic", "Roland", "Krueger", "David"], "venue": "features. stat,", "citeRegEx": "Konda et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Konda et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fastfood - approximating kernel expansions in loglinear time", "author": ["Le", "Quoc", "Sarlos", "Tamas", "Smola", "Alex"], "venue": "In 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Sainath", "Tara N", "Kingsbury", "Brian", "Sindhwani", "Vikas", "Arisoy", "Ebru", "Ramabhadran", "Bhuvana"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Torralba", "Antonio", "Fergus", "Robert", "Freeman", "William T"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Torralba et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 1958}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Xue", "Jian", "Li", "Jinyu", "Gong", "Yifan"], "venue": null, "citeRegEx": "Xue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "Convolutional neural networks (CNN) have had huge successes since Krizhevsky et al. (2012), especially in computer vision applications.", "startOffset": 66, "endOffset": 91}, {"referenceID": 5, "context": "In the PReLU activation function (He et al., 2015), for example, the zero-derivative regime is replaced by a learned, and typically small-slope, linear activation.", "startOffset": 33, "endOffset": 50}, {"referenceID": 13, "context": "Sparsity also plays a crucial role in unsupervised learning (which can also be viewed as a way to help the optimization (Saxe et al., 2013)), where these activations have accordingly never been successfully applied.", "startOffset": 120, "endOffset": 139}, {"referenceID": 5, "context": ", Hochreiter et al. (2001)) this means that for a ReLU activation many Jacobians are diagonal matrices containing many zeros along the diagonal.", "startOffset": 2, "endOffset": 27}, {"referenceID": 5, "context": "In the PReLU activation function (He et al., 2015), for example, the zero-derivative regime is replaced by a learned, and typically small-slope, linear activation. Another approach is the Maxout activation (Goodfellow et al., 2013), which is defined as the maximum of several linear activations. Accordingly, preventing zero-derivatives this way was shown to improve the optimization and the performance of multilayer networks. Unfortunately, these methods solve the problem by giving up on sparsity altogether, which raises the question whether sparsity is simply not as desirable as widely assumed or whether the benefit of the optimization outweigh any detrimental effect on sparsity. Sparsity also plays a crucial role in unsupervised learning (which can also be viewed as a way to help the optimization (Saxe et al., 2013)), where these activations have accordingly never been successfully applied. This view of sparsity motivates us to revisit a simple, but as we shall show surprisingly effective, approach to retaining the benefits of sparsity without preventing gradient-flow. The idea is to interleave linear, low-dimensional layers with the sparse, high-dimensional layers containing ReLU or sigmoid activations. We show that this approach outperforms equivalent PReLU and Maxout networks on the fully supervised, permutation invariant CIFAR-10 task. A second detrimental side-effect of sparsity, discussed in more detail in Konda et al. (2014), is that for a ReLU or sigmoid unit to become sparse it typically learns strongly negative bias-terms.", "startOffset": 34, "endOffset": 1456}, {"referenceID": 5, "context": "In the PReLU activation function (He et al., 2015), for example, the zero-derivative regime is replaced by a learned, and typically small-slope, linear activation. Another approach is the Maxout activation (Goodfellow et al., 2013), which is defined as the maximum of several linear activations. Accordingly, preventing zero-derivatives this way was shown to improve the optimization and the performance of multilayer networks. Unfortunately, these methods solve the problem by giving up on sparsity altogether, which raises the question whether sparsity is simply not as desirable as widely assumed or whether the benefit of the optimization outweigh any detrimental effect on sparsity. Sparsity also plays a crucial role in unsupervised learning (which can also be viewed as a way to help the optimization (Saxe et al., 2013)), where these activations have accordingly never been successfully applied. This view of sparsity motivates us to revisit a simple, but as we shall show surprisingly effective, approach to retaining the benefits of sparsity without preventing gradient-flow. The idea is to interleave linear, low-dimensional layers with the sparse, high-dimensional layers containing ReLU or sigmoid activations. We show that this approach outperforms equivalent PReLU and Maxout networks on the fully supervised, permutation invariant CIFAR-10 task. A second detrimental side-effect of sparsity, discussed in more detail in Konda et al. (2014), is that for a ReLU or sigmoid unit to become sparse it typically learns strongly negative bias-terms. In other words, while sparse activations can be useful in terms of the learning objective (for example, by allowing the network to carve up space into small regions) it forces the network to utilize bias terms that are not optimal for the representation encoded in hidden layer activations. Konda et al. (2014), for example, suggest bias-terms equal to zero to be optimal and propose a way to train an autoencoder with zero-bias hidden units.", "startOffset": 34, "endOffset": 1870}, {"referenceID": 8, "context": ", Konda et al. (2014)).", "startOffset": 2, "endOffset": 22}, {"referenceID": 8, "context": "The observation that biases tend to zero motivated Konda et al. (2014) to introduce a \u201czero-bias\u201d activation function which uses a fixed threshold followed by linear activation.", "startOffset": 51, "endOffset": 71}, {"referenceID": 8, "context": "It is interesting to note, however, that the sparsityinducing negative biases yield reconstructions that are affine not linear and accordingly may not orthogonalize weights after all (Konda et al., 2014).", "startOffset": 183, "endOffset": 203}, {"referenceID": 12, "context": "It was suggested by Saxe et al. (2013) that the benefit of unsupervised pre-training of a network using RBMs or autoencoders may result in weight matrices that are closer to orthogonal and thus less affected by vanishing gradients problems.", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": "It is interesting to note, however, that the sparsityinducing negative biases yield reconstructions that are affine not linear and accordingly may not orthogonalize weights after all (Konda et al., 2014). This may be one of the reasons why the practical success of these pre-training schemes has been quite limited by comparison to fully supervised learning using back propagation. It is also important to note that due to sparsity, the number of active units in a layer is often smaller than that the same number in the layer below, so the \u201ceffective\u201d weight matrix for a given input example is not a square matrix. Rather than simply orthogonal weight matrices, we should be looking for networks where hidden units which tend to be active on the same inputs have weights that tend to be orthogonal to one another. In other words, we should be looking for \u201corthogonal active paths\u201d through the network rather than overall orthogonal weight matrices. In order to obtain hidden units with linear not affine encodings Konda et al. (2014) introduce \u201czerobias autoencoders\u201d (ZAE) whose activations are threshold-ed when pre-trained by minimizing the autoencoder reconstruction error, and whose thresholds are removed when the weights are used for classification or for initializing a feed forward network.", "startOffset": 184, "endOffset": 1036}, {"referenceID": 8, "context": "It is interesting to note, however, that the sparsityinducing negative biases yield reconstructions that are affine not linear and accordingly may not orthogonalize weights after all (Konda et al., 2014). This may be one of the reasons why the practical success of these pre-training schemes has been quite limited by comparison to fully supervised learning using back propagation. It is also important to note that due to sparsity, the number of active units in a layer is often smaller than that the same number in the layer below, so the \u201ceffective\u201d weight matrix for a given input example is not a square matrix. Rather than simply orthogonal weight matrices, we should be looking for networks where hidden units which tend to be active on the same inputs have weights that tend to be orthogonal to one another. In other words, we should be looking for \u201corthogonal active paths\u201d through the network rather than overall orthogonal weight matrices. In order to obtain hidden units with linear not affine encodings Konda et al. (2014) introduce \u201czerobias autoencoders\u201d (ZAE) whose activations are threshold-ed when pre-trained by minimizing the autoencoder reconstruction error, and whose thresholds are removed when the weights are used for classification or for initializing a feed forward network. As discussed in Konda et al. (2014) minimizing squared error under the linear encoding should encourage weights corresponding to hidden units that tend to be \u201con\u201d together to orthogonalize (because the orthogonal projection is what minimizes reconstruction error).", "startOffset": 184, "endOffset": 1338}, {"referenceID": 8, "context": "It is interesting to note, however, that the sparsityinducing negative biases yield reconstructions that are affine not linear and accordingly may not orthogonalize weights after all (Konda et al., 2014). This may be one of the reasons why the practical success of these pre-training schemes has been quite limited by comparison to fully supervised learning using back propagation. It is also important to note that due to sparsity, the number of active units in a layer is often smaller than that the same number in the layer below, so the \u201ceffective\u201d weight matrix for a given input example is not a square matrix. Rather than simply orthogonal weight matrices, we should be looking for networks where hidden units which tend to be active on the same inputs have weights that tend to be orthogonal to one another. In other words, we should be looking for \u201corthogonal active paths\u201d through the network rather than overall orthogonal weight matrices. In order to obtain hidden units with linear not affine encodings Konda et al. (2014) introduce \u201czerobias autoencoders\u201d (ZAE) whose activations are threshold-ed when pre-trained by minimizing the autoencoder reconstruction error, and whose thresholds are removed when the weights are used for classification or for initializing a feed forward network. As discussed in Konda et al. (2014) minimizing squared error under the linear encoding should encourage weights corresponding to hidden units that tend to be \u201con\u201d together to orthogonalize (because the orthogonal projection is what minimizes reconstruction error). Konda et al. (2014) reported decent classification performance in various supervised tasks, but found only a weak improvement over standard autoencoders when used to initialize a single-hidden-layer MLP.", "startOffset": 184, "endOffset": 1587}, {"referenceID": 8, "context": "We compare two different activation functions: ReLU and zero-bias ReLU (Konda et al., 2014).", "startOffset": 71, "endOffset": 91}, {"referenceID": 14, "context": "The CIFAR-10 dataset is a subset of the 80 Million Tiny Images Torralba et al. (2008), and contains 10 balanced classes.", "startOffset": 63, "endOffset": 86}, {"referenceID": 5, "context": "We also compare the bottleneck layer structure with other related modifications like PReLU (He et al., 2015) and Maxout (Goodfellow et al.", "startOffset": 91, "endOffset": 108}, {"referenceID": 8, "context": "Following Konda et al. (2014), the threshold of all ZAEs are fixed at 1.", "startOffset": 10, "endOffset": 30}, {"referenceID": 11, "context": "1% (Le et al., 2013), and 63.", "startOffset": 3, "endOffset": 20}, {"referenceID": 8, "context": "9% (Konda et al., 2014).", "startOffset": 3, "endOffset": 23}, {"referenceID": 8, "context": "9% (Konda et al., 2014). By adding dropout (Srivastava et al., 2014) during pre-training and fine-tuning, and using a very deep Z-LIN network (3 Z-LIN pairs, plus a ZAE layer and logistic regression classifier, i.e., 4000Z\u2212 1000Lin\u22124000Z\u22121000Lin\u22124000Z\u22121000Lin\u22124000Z\u221210) the performance improves to 69.62%, which exceeds the current state-of-the-art on permutation invariant CIFAR10 by a very large margin. If we give up on permutation-invariance by using data augmentation (eg., Krizhevsky et al. (2012)) but retain the use of a fully connected network, the performance improves much further.", "startOffset": 4, "endOffset": 504}, {"referenceID": 11, "context": "They are (from left to right): 1) Logistic Regression on whitened data; 2) Pure backprop on a 782-10000-10 network; 3) Pure backprop on a 78210000-10000-10 network; 4) RBM with 2 hidden layers of 10000 hidden units each, plus a logistic regression; 5) RBM with 10000 hiddens plus logistic regression; 6) \u201dFastfood FFT\u201d model (Le et al., 2013); 7) Zerobias autoencoder of 4000 hidden units with logistic regression (Konda et al.", "startOffset": 325, "endOffset": 342}, {"referenceID": 8, "context": ", 2013); 7) Zerobias autoencoder of 4000 hidden units with logistic regression (Konda et al., 2014); 8) 782-4000-1000-4000-10 Z-Lin network trained without dropout; 9) 782-4000-1000-4000-10004000-1000-4000-10 Z-Lin network, trained with dropout 10) Z-Lin network the same as (8) but trained with dropout and data augmentation; Results (1)-(5) are from Krizhevsky & Hinton (2009).", "startOffset": 79, "endOffset": 99}, {"referenceID": 8, "context": ", 2013); 7) Zerobias autoencoder of 4000 hidden units with logistic regression (Konda et al., 2014); 8) 782-4000-1000-4000-10 Z-Lin network trained without dropout; 9) 782-4000-1000-4000-10004000-1000-4000-10 Z-Lin network, trained with dropout 10) Z-Lin network the same as (8) but trained with dropout and data augmentation; Results (1)-(5) are from Krizhevsky & Hinton (2009). The final one is distinguished with a grey background because it uses data augmentation.", "startOffset": 80, "endOffset": 379}, {"referenceID": 12, "context": "An application of a linear bottleneck layer in the last layer of a neural network for dealing with highdimensional outputs is described in Sainath et al. (2013) and Xue et al.", "startOffset": 139, "endOffset": 161}, {"referenceID": 12, "context": "An application of a linear bottleneck layer in the last layer of a neural network for dealing with highdimensional outputs is described in Sainath et al. (2013) and Xue et al. (2013). In contrast to our work, in none of these methods is the goal to alleviate vanishing gradients and deal with sparsity, and (accordingly) they use just a single bottleneck layer in the network.", "startOffset": 139, "endOffset": 183}], "year": 2015, "abstractText": "We propose ways to improve the performance of fully connected networks. We found that two approaches in particular have a strong effect on performance: linear bottleneck layers and unsupervised pre-training using autoencoders without hidden unit biases. We show how both approaches can be related to improving gradient flow and reducing sparsity in the network. We show that a fully connected network can yield approximately 70% classification accuracy on the permutationinvariant CIFAR-10 task, which is much higher than the current state-of-the-art. By adding deformations to the training data, the fully connected network achieves 78% accuracy, which is just 10% short of a decent convolutional network.", "creator": "LaTeX with hyperref package"}}}