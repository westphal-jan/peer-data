{"id": "1511.02595", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "A New Relaxation Approach to Normalized Hypergraph Cut", "abstract": "normalized graph cut ( slice ) has become a serious research proposition due to finding wide limitations in a large variety of areas like le model implement very large scale integration ( vlsi ) approximation design. most of binary learning methods are based regarding discrete relationships ( solve ). alternatively, compact finite - generated systems similarity among the vertices ( objects ) occasionally create more complex labeled graphs, which are more made from hyperedges in hypergraphs. unfortunately, generalized segment driven ( map ) applications attracted slower and more innovation. solving nhc mechanisms cannot achieve computational performance in real applications. in chapter episode, we mentioned a strict relaxation approach, which as called log nhc ( rnhc ), to solve my nhc problem. < model asks \\ model differential optimization problem at the stiefel manifold. to solve this problem, also resort to the cayley metric tensor devise every continuous learning algorithm. experimental data on a graphs demonstrating large hypergraph curves exploring clustering and mesh in vlsi plots show that np can outperform natural curve - of - the - sphere methods.", "histories": [["v1", "Mon, 9 Nov 2015 08:30:03 GMT  (272kb,D)", "http://arxiv.org/abs/1511.02595v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["cong xie", "wu-jun li", "zhihua zhang"], "accepted": false, "id": "1511.02595"}, "pdf": {"name": "1511.02595.pdf", "metadata": {"source": "CRF", "title": "A New Relaxation Approach to Normalized Hypergraph Cut", "authors": ["Cong Xie", "Wu-Jun Li", "Zhihua Zhang"], "emails": ["xcgoner1108@gmail.com", "liwujun@nju.edu.cn", "zhang-zh@cs.sjtu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "The goal of graph cut (or called graph partitioning) [Wu and Leahy, 1993] is to divide the vertices (nodes) in a graph into several groups (clusters), making the number of edges across different clusters minimized while the number of edges within the clusters maximized. Besides the goal which should be achieved in graph cut, normalized graph cut (NGC) should also make the volumes of different clusters as balanced as possible by adopting some normalization techniques. In many real applications, NGC has been proved to achieve better performance than unnormalized graph cut [Shi and Malik, 2000; Ng et al., 2001; Gonzalez et al., 2012]. Thus, NGC is a popular theme due to its wide applications in a large variety of areas, including machine learning [Ng et al., 2001; Xie et al., 2014], parallel and distributed computation [Gonzalez et al., 2012; Jain et al., 2013; Chen et al., 2015], image segmentation [Shi and Malik, 2000], and social network analysis [Tang and Liu, 2010; Li and Schuurmans, 2011], and so on. For example, graph-based clustering methods such\nas spectral clustering [Ng et al., 2001] can be seen as NGC methods. In social network analysis, NGC has been widely used for community detection in social networks (graphs).\nMost of traditional NGC methods are based on pairwise relationships (similarities) [Shi and Malik, 2000; Ng et al., 2001]. However, the relationships between vertices (objects) can be more complex than pairwise in real-world applications. In particular, the objects may be grouped together according to different properties or topics. The groups can be viewed as relationships which are typically not pairwise. A good example in industrial domain is the very large scale integration (VLSI) circuit design [Hagen and Kahng, 1992]. The objects in the circuits are connected in groups via wire nets. Typically, these complex non-pairwise relationships can be represented as hyperedges in hypergraphs [Berge and Minieka, 1973]. More specifically, a hypergraph contains a set of vertices and a set of hyperedges, and a hyperedge is an edge that connects at least two vertices in the hypergraph. Note that an ordinary pairwise edge can be treated as a special hyperedge which connects exactly two vertices.\nLike NGC, normalized hypergraph cut (NHC) [Catalyurek and Aykanat, 1999; Zhou et al., 2006] tries to divide the vertices into several groups (clusters) by minimizing the number of hyperedges connecting nodes in different clusters and meanwhile maximizing the number of hyperedges within the clusters. Furthermore, some normalization techniques are also adopted in NHC methods. NHC has been widely used in many applications and attracted more and more attention. For example, NHC has been used to reduce the communication cost and balance the workload in parallel computation such as sparse matrix-vector multiplication [Catalyurek and Aykanat, 1999]. In fact, it is pointed out that hypergraphs can model the communication cost of parallel graph computing more precisely [Catalyurek and Aykanat, 1999]. In addition, the large scale machine learning framework GraphLab [Gonzalez et al., 2012] utilizes similar ideas for distributed graph computing. Due to its wide applications, a lot of algorithms have been proposed for the NHC problem [Bolla, 1993; Karypis et al., 1999; Catalyurek and Aykanat, 1999; Zhou et al., 2006; Ding and Yilmaz, 2008; Bulo\u0300 and Pelillo, 2009; Pu and Faltings, 2012; Anandkumar et al., 2014]. These algorithms can be divided into three main categories: heuristic approach, spectral approach, and tensor approach.\nThe \u201chMetis\u201d [Karypis et al., 1999] and \u201cParkway\u201d [Tri-\nar X\niv :1\n51 1.\n02 59\n5v 1\n[ cs\n.L G\n] 9\nN ov\n2 01\n5\nfunovic and Knottenbelt, 2004] are two famous tools to solve the NHC problem by using some heuristics. The basic idea is to first construct a sequence of successively coarser hypergraphs. Then the coarsest (smallest) hypergraph is cut (partitioned), based on which the cut (partitioning) of the original hypergraph is obtained by successively projecting and refining the cut results of the former coarser level to the next finer level of the hypergraph.\nSpectral approach [Bolla, 1993; Zhou et al., 2006; Agarwal et al., 2006; Rodr\u0131\u0301guez, 2009] is also used to solve the NHC problem. In [Bolla, 1993], the eigen-decomposition on an unnormalized Laplacian matrix is proposed for hypergraph cut by using \u201cclique expansion.\u201d More specifically, each hyperedge is replaced by a clique (a fully connected subgraph) and the hypergraph is converted to an ordinary graph based on which the Laplacian matrix is constructed. Zhou et al. [2006] also use \u201cclique expansion\u201d as that in [Bolla, 1993] to convert the hypergraph to an ordinary graph, and then adopt the wellknown spectral clustering methods [Ng et al., 2001] to perform clustering on the normalized Laplacian matrix. Agarwal et al. [2006] survey various Laplace like operators for hypergraphs and study the relationships between hypergraphs and ordinary graphs. In addition, some theoretical analysis for the hypergraph Laplacian matrix is also provided in [Rodr\u0131\u0301guez, 2009].\nRecent studies [Frieze and Kannan, 2008; Cooper and Dutle, 2012] suggest using an affinity tensor of order k to represent k-uniform hypergraphs, in which each hyperedge connects to k nodes exactly. Note that the ordinary graph is a 2-uniform hypergraph. Tensor decomposition of a highorder normalized Laplacian is then used to solve the NHC problem. Frieze and Kannan [2008] first use a 3-d matrix (order 3 tensor) to represent a 3-uniform hypergraph and find cliques. Cooper and Dutle [2012] generalize such an idea to any uniform hypergraphs and use the high-order Laplacian to partition the hypergraphs.\nThe existing methods mentioned above are limited in some aspects. More specifically, implementation of the heuristic approaches is simple. However, there does not exist any theoretical guarantee for those approaches. Although proved to be efficient, the spectral approach using \u201cclique expansion\u201d does not model the NHC precisely because it approximates the non-pairwise relationships via pairwise ones. Subsequently, some information of the original structure of the hypergraph may be lost due to the expansion. The tensor approach can only be used for uniform hypergraphs, but hypergraphs from real applications are typically not uniform.\nIn this paper, we propose a novel relaxation approach, which is called relaxed NHC (RNHC), to solve the NHC problem. The main contributions are briefly outlined as follows:\n\u2022 We propose a novel model to precisely formulate the NHC problem. Our model can preserve the original structure of general hypergraphs. Moreover, our model does not require the hypergraphs to be uniform.\n\u2022 Our formulation for the NHC yields a NP-hard problem. We propose a novel relaxation on the Stiefel manifold. And we efficiently solve the relaxed problem via design-\nthat RNHC can outperform the state-of-the-art methods. The remainder of this paper is organized as follows. We first introduce some basic definitions as well as the baseline algorithm in Section 2. The RNHC model and our algorithm are proposed in Section 3. In Section 4, we present the experimental results on several VLSI hypergraph benchmarks. Finally, we conclude this paper in Section 5."}, {"heading": "2 Preliminaries", "text": "In this section, we present the notation and problem definition of this paper. In addition, the spectral clustering approach [Zhou et al., 2006] is also briefly introduced."}, {"heading": "2.1 Notation", "text": "Let V = {v1, v2, . . . , vn} denote the set of n vertices (nodes), and E = {e1, e2, . . . , em} denote the set of m undirected hyperedges. A hyperedge e \u2208 E is a subset of V which might contain more than two vertices. Note that an ordinary edge is still a hyperedge which has exactly two vertices. A hypergraph is defined as H = (V,E) with vertex set V and hyperedge set E. A hypergraph H can be represented by an n\u00d7m matrix B with entries B(v, e) = 1 if v \u2208 e and 0 otherwise. We define an n \u00d7 n diagonal matrix D with diagonal entries D(v, v) = \u2211 e\u2208E B(v, e) which is the degree of vertex v. Some notations that will be used are listed in Table 1."}, {"heading": "2.2 Normalized Hypergraph Cut", "text": "Let each vertex v \u2208 V be uniquely assigned to a cluster c \u2208 C where C = {c1, c2, . . . , cp} denotes the p clusters. Then\neach hyperedge spans a set of different clusters. A p-way hypergraph cut is a sequence of disjoint subsets ci \u2286 V with \u222api=1ci = V . A min-cut problem is to find a hypergraph cut such that the number of hyperedges spanning different clusters is minimized.\nNow we formally define the normalized hypergraph cut (NHC) problem. We define the volume of cluster ci to be vol(ci) = \u2211 v\u2208ci D(v, v). A hyperedge gets one cut if its vertices span two different clusters. Given the set of clusters C, the cut of a hyperedge e is defined as\u2211\ni\u2208[p] \u2211 j\u2208[p],j 6=i 1{e\u2229ci 6=\u2205,e\u2229cj 6=\u2205},\ni.e., the number of pairs of different clusters that cut e. Then the cut-value of a hypergraph cut (denoted hcut(C)) is the total number of cuts of all the hyperedges in the hypergraph:\nhcut(C) = \u2211 e\u2208E \u2211 i\u2208[p] \u2211 j\u2208[p],j 6=i 1{e\u2229ci 6=\u2205,e\u2229cj 6=\u2205}.\nThe cut-value caused by a specific cluster ci is hcut(ci) = \u2211 e\u2208E \u2211 j\u2208[p],j 6=i 1{e\u2229ci 6=\u2205,e\u2229cj 6=\u2205}.\nThen, if we normalize the cut-value of each cluster by its volume, we can get the NHC value nhcut(C):\nnhcut(C) = \u2211 i\u2208[p]\n\u2211 e\u2208E \u2211 j\u2208[p],j 6=i 1{e\u2229ci 6=\u2205,e\u2229cj 6=\u2205}\nvol(ci) . (1)\nThe normalized hypergraph cut (NHC) problem is to find a hypergraph cut which can minimize the nhcut(C). Note that in nhcut(C) the weights of the hyperedges and vertices are assumed to be 1, which means that the hypergraph is unweighted. However, our model, learning algorithm and results of this paper can be easily extended to weighted hypergraphs, which is not the focus of this paper."}, {"heading": "2.3 Spectral Approach", "text": "The spectral approach [Zhou et al., 2006] approximates the NHC via \u201cclique expansion.\u201d Each hyperedge e is expanded to a fully connected subgraph with the same edge weight 1/ \u2223\u2223e\u2223\u2223. Then, the hypergraph is converted to a weighted ordinary graph. The NHC problem is solved by spectral clustering [Ng et al., 2001] on the expanded graph. That is, the p smallest eigenvalues will be calculated, and the corresponding eigenvectors will be treated as the new features for the vertices, which will be further clustered via the K-Means algorithm to get the final solution.\nIn the spectral approach, the number of cuts of a hyperedge is approximated by the edge cut of the corresponding clique, which is the number of edges across different clusters normalized by the total number of vertices in the clique. Note that any pair of vertices in a clique is connected by an edge. Thus, the hypergraph cut is approximated as\napprox hcut(C) = \u2211 i\u2208[p] \u2211 e\u2208E \u2223\u2223e \u2229 ci\u2223\u2223(\u2223\u2223e\u2223\u2223\u2212 \u2223\u2223e \u2229 ci\u2223\u2223)/\u2223\u2223e\u2223\u2223,\nand the corresponding NHC is approximated as\napprox nhcut(C) = \u2211 i\u2208[p] \u2211 e\u2208E \u2223\u2223e \u2229 ci\u2223\u2223(\u2223\u2223e\u2223\u2223\u2212 \u2223\u2223e \u2229 ci\u2223\u2223)/\u2223\u2223e\u2223\u2223 vol(ci) .\nAlthough such approximation sounds reasonable, it loses the original structure of the hypergraph and solving the NHC problem becomes indirectly."}, {"heading": "3 Methodology", "text": "In this section, we provide a novel model to formulate the NHC problem as well as a corresponding relaxed optimization problem. An efficient learning algorithm is then presented to solve it."}, {"heading": "3.1 NHC Formulation", "text": "We first rewrite (1) into a matrix form. The solution can be represented as an n\u00d7 p matrix X with entries X(v, c) = 1 if v \u2208 c and 0 otherwise, where v is a vertex and c is a cluster. Note that the columns of X are mutually orthogonal. Given the assignment of the vertices, the assignment of hyperedges is consequently obtained. A hyperedge occurs in a cluster if and only if at least one of its vertices occurs in the cluster. Thus, the corresponding assignment of hyperedges can be represented as a p \u00d7 m matrix S with entries S(c, e) = 1 if \u2203v \u2208 c such that e 3 v and S(c, e) = 0 otherwise, where e is a hyperedge. Note that S = sgn(XTB) where sgn() is the element-wise sign function. The ith row of S = [s1, s2, . . . , sp]T represents the hyperedges belonging to the corresponding cluster ci. Note that sTi sj represents the number of hyperedges between the two clusters ci and cj , namely, the cut associated with the two clusters. And the ithe diagonal element of the matrix XTDX corresponds to the volume of the ith cluster.\nThus, the NHC problem can be represented as:\nmin X\ntr(SST (1p1 T p \u2212 Ip)(XTDX)\u22121). (2)\nRecall that X is column orthogonal. To express such constraints, we normalize the columns of X to get X\u0304 such that X\u0304(v, c) = X(v, c)/ \u221a\u2211n i=1X(i, c). Thus, we have X\u0304\nT X\u0304 = Ip. We also let S\u0304 = sgn(X\u0304TB). To simplify the representation, we assume the degrees of the vertices are roughly the same. Then, we can rewrite the optimization problem in (2) as follows:\nmin X\u0304\ntr(S\u0304T (1p1 T p \u2212 Ip)S\u0304), s.t. X\u0304T X\u0304 = Ip. (3)"}, {"heading": "3.2 Relaxation", "text": "The problem in (3) is NP-hard, which is intractable to solve because of the quadratic form in the objective function and the occurrence of the sign function. To make the problem tractable, we will simplify the expression and use elementary matrix operations to obtain S and the corresponding S\u0304 before we relax the problem.\nFirst, we simplify the objective function. Note that the quadratic form in (3) can be rewritten in a summation form\u2211\ne\u2208E pe \u00d7 (pe \u2212 1), (4)\nwhere pe is the number of clusters that hyperedge e occurs, or pe = \u2211 c\u2208C S(c, e). Obviously, minimizing (4) is equivalent to minimize \u2211 e\u2208E pe, (5)\nwhich leads to a simpler expression of the problem:\nmin X\u0304 \u2211 ij S\u0304ij , s.t. X\u0304T X\u0304 = Ip. (6)\nNext, we simplify the expression of S. Recall the definition of S, that is, S(c, e) = 1 if and only if \u2203v \u2208 e such that v \u2208 c. In that case, each element of S can be expressed as S(c, e) = maxv\u2208eX(v, c). And the minimizer does not change if we substitute X with X\u0304.\nNow we further relax the problem. We simply relax X\u0304 to a n\u00d7p real matrix satisfying X\u0304T X\u0304 = Ip. To make the objective function differentiable, we replace the maximum function by the log-sum-exp function, which is a differentiable approximation of the maximum function. We denote the relaxed S\u0304 by S\u0302. Then, we have S\u0302(c, e) = 1\u03b1 ln {\u2211 v\u2208e exp[\u03b1X\u0304(v, c)] } , where \u03b1 is an enough large parameter. When \u03b1 gets larger, the approximation gets closer to the maximum function. Moreover, S\u0304 can be written in a matrix form:\nS\u0302 = 1 \u03b1 ln [ exp(\u03b1X\u0304)TB ] . (7)\nThen, we can relax the NHC problem as follows:\nmin X\u0304 \u2211 ij S\u0302ij , s.t. X\u0304T X\u0304 = Ip. (8)"}, {"heading": "3.3 Learning Algorithm", "text": "Now we devise an learning algorithm to solve the relaxed optimization problem in (8). Since the objective function is minimized under the orthogonality constraint, the corresponding feasible set Mpn = { X\u0304 \u2208 Rn\u00d7p\n\u2223\u2223X\u0304T X\u0304 = Ip} is called the Stiefel manifold. There are algorithms in [Edelman et al., 1998; Wen and Yin, 2013], which have been proposed to deal with such kinds of constraints. Note that the optimization problem with orthogonality constraints is non-convex, which means there is no guarantee to obtain a global minimizer. To find a local minimizer on the Stiefel manifold, we introduce the Cayley transformation [Wen and Yin, 2013] to devise the learning algorithm.\nGiven any feasible X\u0304 and the differentiable objective function f(X\u0304) = \u2211 ij S\u0302ij , where S\u0302 is defined in (7), we compute the gradient matrix with respect to X\u0304:\nG = { B [ (1p1 T m)./(X\u0303 TB) ]T} X\u0303, (9)\nwhere X\u0303 = exp(\u03b1X\u0304). Denote\nA = GXT \u2212XGT , (10) which is skew-symmetric. Then we have the Cayley transformation\nQ = (I + \u03c4\n2 A)\u22121(I +\n\u03c4 2 A). (11)\nAnd the new trial point starting from X\u0304 will be searched on the curve\nY(\u03c4) = QX\u0304. (12)\nNote that Y(0) = X\u0304, Y(\u03c4)TY(\u03c4) = X\u0304T X\u0304 for all \u03c4 \u2208 R, and Y(\u03c4) is smooth in \u03c4 . Furthermore, {Y(\u03c4)}\u03c4\u22650 is a descent path because dd\u03c4Y(0) equals the projection of (\u2212G) into the tangent space ofMpn at X\u0304.\nWith all the properties above, we can solve the relaxed problem by a gradient descent algorithm on the curve and discretize the solution via the K-Means clustering algorithm. We present the whole learning procedure in Algorithm 1.\nAlgorithm 1 A relaxed learning algorithm for the NHC problem (RNHC) Input: A hypergraph H and the corresponding matrix B;\nnumber of clusters p; maximal number of iterations T ; a stopping criterion ; the parameter \u03b1 of log-sum-exp function Output: A binary matrix X. 1: Initialization: pick an arbitrary orthogonal matrix X\u03040 \u2208\nRn\u00d7p, set t\u2190 0. 2: for t\u2190 {1, \u00b7 \u00b7 \u00b7 , T} do 3: Generate f(X\u0304t), G, and A according to (8), (9), (10). 4: Compute the step size \u03c4t by using line search along the path Y(\u03c4) defined by (12). 5: Set X\u0304t+1 \u2190 Y(\u03c4t). 6: if \u2016 \u2207f(X\u0304t+1) \u2016\u2264 then 7: Stop. 8: end if 9: end for\n10: X = K-Means(X\u0304t, p)."}, {"heading": "3.4 Complexity Analysis", "text": "We now study the time complexity and storage cost of our algorithm.\nTime Complexity We analyze the time complexity step by step. The flops for computing the objective function, the gradient matrix G and the corresponding skew-symmetric matrix A are O(mnp), O(mnp) and O(n2p), respectively. The computation of Q in (11) needs to compute the inverse of (I+ \u03c42A). According to the Sherman-Morrison-Woodbury formula, when p is much smaller than n/2, we only need to compute the inverse of a 2p\u00d72pmatrix. Thus, the computation of Y(\u03c4) needs 8np2 + O(p3). For a different \u03c4 , updating Y(\u03c4) needs 4np2 +O(p3). Note that we always have m \u2265 n.\nPutting all the above components together, assuming the number of iterations to be T and ignoring the K-Means step, the time complexity of our algorithm is O(T \u00d7mnp).\nIgnoring the K-Means step, the time complexity of the spectral approach [Zhou et al., 2006] is O(mnp) for computing the p largest singular values and singular vectors.\nAlthough the time complexity of our algorithm seems larger than the spectral approach [Zhou et al., 2006], the number of iterations T can be tuned by changing the stopping\ncriterion. Note that T is usually small enough, so it can be viewed as a constant, which yields the same time complexity O(mnp) as the spectral approach. Moreover, a smaller T may lead to a constant factor of reduction for the time complexity.\nStorage Cost Ignoring the K-Means step, the largest matrix constructed in our algorithm is S\u0302. So the storage cost is O(mp)."}, {"heading": "4 Experiment", "text": "In this section, empirical evaluation on real hypergraphs is used to verify the effectiveness of our algorithm. Our experiment is taken on a workstation with Intel E5-2650-v2 2.6GHz (2\u00d7 8 cores) and 128GB of DDR3 RAM."}, {"heading": "4.1 Datasets and Baselines", "text": "The hypergraphs used in our experiment are from a set of hypergraph benchmarks for clustering and partitioning in VLSI domain from the ISPD98 Circuit Benchmark Suite 1. There are totally 18 hypergraphs in the dataset. The 12 largest ones of them are chosen for our experiment. The information of the datasets is shown in Table 2.\nIn our experiment, we adopt the spectral approach [Zhou et al., 2006] mentioned in Section 2.3 as the baseline. Our method in Algorithm 1 is named as relaxed normalized hypergraph cut (RNHC). In all the experiments, we pick parameter \u03b1 = 100, the maximal number of iterations T = 1000, and the stopping criterion = 10\u22129 for our algorithm. The reason why we do not compare our algorithm with the heuristic approaches such as hMetis and Parkway is that we only focus on solving NHC problem via optimization in this paper. And the spectral approach is the algorithm most related to our work."}, {"heading": "4.2 Clustering Visualization", "text": "We visualize the clustering produced by the spectral approach and RNHC on ibm07 in Figure 1. In the figure, we illustrate\n1http://vlsicad.ucsd.edu/UCLAWeb/cheese/ ispd98.html\nthe matrix B defined in Section 2.1. Each row of B represents a vertex and each column represents a hyperedge. A non-blank point located at (x, y) in the figure implies that the yth vertex belongs to the xth hyperedge. The number of clusters is 3. Different colors indicate different clusters. The vertices are rearranged such that vertices in the same cluster will be grouped together. And the hyperedges in both Figures 1(a) and 1(b) are arranged in the same order. In a better clustering, there should be less overlapping columns (hyperedges) between clusters."}, {"heading": "4.3 Accuracy Comparison", "text": "We evaluate the number of clusters from 2 to 8. For each trial, both algorithms will be tested for 40 times and the best NHC value will be picked for comparison. The reason why we compare the best NHC value instead of the average performance is that both algorithms utilize the K-Means algorithm for final clustering, whose result depends on the starting point. Sometimes K-Means simply fails to obtain p clusters, which means that some of the clusters are empty. Moreover, K-Means occasionally produces extremely bad NHC values because of some bad starting point. Such failures may make the average performance meaningless. Furthermore, it is also hard to tell which trial fails and which one succeeds.\nWe test the objective value of NHC in (2) for each algorithm. The comparison of the objective value is shown in Figure 2. Note that a smaller objective value implies a better NHC. It can be seen that our algorithm produces a better objective value in most cases."}, {"heading": "4.4 Speed Comparison", "text": "The comparison of the time for clustering on the largest 4 datasets is shown in Figure 3. To guarantee fairness, all the experiments are carried out in a single thread by setting \u201cmaxNumCompThreads(1)\u201d in MATLAB. We can find that our RNHC algorithm is faster than the baseline in all cases."}, {"heading": "5 Conclusion", "text": "In this paper we have proposed a new model to formulate the normalized hypergraph cut problem. Furthermore, we have provided an effective approach to relax the new model, and developed an efficient learning algorithm to solve the relaxed hypergraph cut problem. Experimental results on real hypergraphs have shown that our algorithm can outperform the state-of-the-art approach. It is interesting to apply our approach to other practical problems, such as the graph par-\ntitioning problem in distributed computation, in the future work."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Normalized graph cut (NGC) has become a popular research topic due to its wide applications in a large variety of areas like machine learning and very large scale integration (VLSI) circuit design. Most of traditional NGC methods are based on pairwise relationships (similarities). However, in real-world applications relationships among the vertices (objects) may be more complex than pairwise, which are typically represented as hyperedges in hypergraphs. Thus, normalized hypergraph cut (NHC) has attracted more and more attention. Existing NHC methods cannot achieve satisfactory performance in real applications. In this paper, we propose a novel relaxation approach, which is called relaxed NHC (RNHC), to solve the NHC problem. Our model is defined as an optimization problem on the Stiefel manifold. To solve this problem, we resort to the Cayley transformation to devise a feasible learning algorithm. Experimental results on a set of large hypergraph benchmarks for clustering and partitioning in VLSI domain show that RNHC can outperform the state-of-the-art methods.", "creator": "TeX"}}}