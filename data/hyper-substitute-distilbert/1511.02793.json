{"id": "1511.02793", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Generating Images from Captions with Attention", "abstract": "instead anticipating the recent progress in artificial models, we derive a software the generates images from natural language descriptions. the proposed model that maintains patches incorporating texture texture, while attending to several grammatical words in actual description. after validation on microsoft coco, we illustrate her model with theoretical baseline image models on image generation and activation functions. guidelines demonstrate that ai researcher produces higher quality samples than procedural approaches and generates images with novel 3d compositions corresponding into previously specified captions accompanying the dataset.", "histories": [["v1", "Mon, 9 Nov 2015 18:18:53 GMT  (889kb,D)", "http://arxiv.org/abs/1511.02793v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Mon, 29 Feb 2016 17:56:29 GMT  (889kb,D)", "http://arxiv.org/abs/1511.02793v2", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["elman mansimov", "emilio parisotto", "jimmy lei ba", "ruslan salakhutdinov"], "accepted": true, "id": "1511.02793"}, "pdf": {"name": "1511.02793.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Elman Mansimov", "Emilio Parisotto", "Jimmy Lei"], "emails": ["emansim@cs.toronto.edu,", "eparisotto@cs.toronto.edu,", "rsalakhu@cs.toronto.edu,", "jimmy@psi.utoronto.ca"], "sections": [{"heading": null, "text": "Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset."}, {"heading": "1 INTRODUCTION", "text": "Statistical natural image modelling remains a fundamental problem in computer vision and image understanding. The challenging nature of this task has motivated recent approaches to exploit the inference and generative capabilities of deep neural networks. Previously studied deep generative models of images often defined distributions that were restricted to being either unconditioned or conditioned on classification labels. In real world applications, however, images rarely appear in isolation as they are often accompanied by unstructured textual descriptions, such as on web pages and in books. The additional information from these descriptions could be used to simplify the image modelling task. Moreover, learning generative models conditioned on text also allows a better understanding of the generalization performance of the model, as we can create textual descriptions of completely new scenes not seen at training time.\nThere are numerous ways to learn a generative model over both image and text modalities. One approach is to learn a generative model of text conditioned on images, known as caption generation (Kiros et al., 2014a; Karpathy & Li, 2015; Vinyals et al., 2015; Xu et al., 2015). These models take an image descriptor and generate unstructured texts using a recurrent decoder. In contrast, in this paper we explore models that condition in the opposite direction, i.e. taking textual descriptions as input and using them to generate relevant images. Generating high dimensional realistic images from their descriptions combines the two challenging components of language modelling and image generation, and can be considered to be more difficult than caption generation.\nIn this paper, we illustrate how sequential deep learning techniques can be used to build a conditional probabilistic model over natural image space effectively. By extending the Deep Recurrent Attention Writer (DRAW) (Gregor et al., 2015), our model iteratively draws patches on a canvas, while attending to the relevant words in the description. Overall, the main contributions of this work are the following: we introduce a conditional alignDRAW model, a generative model of images from captions using a soft attention mechanism. The images generated by our alignDRAW model are refined in a post-processing step by a deterministic Laplacian pyramid adversarial network (Denton et al., 2015). We further illustrate how our method, learned on Microsoft COCO (Lin et al., 2014), generalizes to captions describing novel scenes that are not seen in the dataset, such as \u201cA stop sign is flying in blue skies\u201d (see Fig. 1).\nar X\niv :1\n51 1.\n02 79\n3v 1\n[ cs\n.L G\n] 9\nN ov\n2 01\n5\nA stop sign is flying in blue skies. A herd of elephants flying in the blue skies. A toilet seat sits open in the grass field. A person skiing on sand clad vast desert.\nFigure 1: Examples of generated images based on captions that describe novel scene compositions that are highly unlikely to occur in real life. The captions describe a common object doing unusual things or set in a strange location."}, {"heading": "2 RELATED WORK", "text": "Deep Neural Networks have achieved significant success in various tasks such as image recognition (Krizhevsky et al., 2012), speech transcription (Graves et al., 2013), and machine translation (Bahdanau et al., 2015). While most of the recent success has been achieved by discriminative models, generative models have not yet enjoyed the same level of success. Most of the previous work in generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdinov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006). While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate derivatives of an intractable partition function (normalization constant), making it difficult to scale them to large datasets.\nKingma & Welling (2014) have introduced the Variational Auto-Encoder (VAE) which can be seen as a neural network with continuous latent variables. The encoder is used to approximate a posterior distribution and the decoder is used to stochastically reconstruct the data from latent variables. Gregor et al. (2015) further introduced the Deep Recurrent Attention Writer (DRAW), extending the VAE approach by incorporating a novel differentiable attention mechanism.\nGenerative Adversarial Networks (GANs) (Goodfellow et al., 2014) are another type of generative models that use noise-contrastive estimation (Gutmann & Hyva\u0308rinen, 2010) to avoid calculating an intractable partition function. The model consists of a generator that generates samples using a uniform distribution and a discriminator that discriminates between real and generated images. Recently, Denton et al. (2015) have scaled those models by training conditional GANs at each level of a Laplacian pyramid of images.\nWhile many of the previous approaches have focused on unconditional models or models conditioned on labels, in this paper we develop a generative model of images conditioned on captions."}, {"heading": "3 MODEL", "text": "Our proposed model defines a generative process of images conditioned on captions. In particular, captions are represented as a sequence of consecutive words and images are represented as a sequence of patches drawn on a canvas ct over time t = 1, ..., T . The model can be viewed as a part of the sequence-to-sequence framework (Sutskever et al., 2014; Cho et al., 2014; Srivastava et al., 2015)."}, {"heading": "3.1 LANGUAGE MODEL: THE BIDIRECTIONAL ATTENTION RNN", "text": "Let y be the input caption, represented as a sequence of 1-of-K encoded words y = (y1, y2, ..., yN ), whereK is the size of the vocabulary andN is the length of the sequence. We obtain the caption sentence representation by first transforming each word yi to an m-dimensional vector representation hlangi , i = 1, .., N using the Bidirectional RNN. In a Bidirectional RNN, the two LSTMs (Hochreiter & Schmidhuber, 1997) with forget gates (Gers et al., 2000) process the input sequence from both forward and backward directions. The Forward LSTM computes the sequence of forward hidden states [ \u2212\u2192 h lang1 , \u2212\u2192 h lang2 , ..., \u2212\u2192 h langN ] , whereas the Backward LSTM computes the sequence of backward hidden states [ \u2190\u2212 h lang1 , \u2190\u2212 h lang2 , ..., \u2190\u2212 h langN ]. These hidden states are then concatenated together into the sequence hlang = [hlang1 , h lang 2 , ..., h lang N ], with h lang i = [ \u2212\u2192 h langi , \u2190\u2212 h langi ], 1 \u2264 i \u2264 N ."}, {"heading": "3.2 IMAGE MODEL: THE CONDITIONAL DRAW NETWORK", "text": "To generate an image x conditioned on the caption information y, we extended the DRAW network (Gregor et al., 2015) to include caption representation hlang at each step, as shown in Fig. 2. The conditional DRAW network is a stochastic recurrent neural network that consists of a sequence of latent variables Zt \u2208 RD, t = 1, .., T , where the output is accumulated over all T time-steps. For simplicity in notation, the images x \u2208 Rh\u00d7w are assumed to have size h-by-w and only one color channel.\nUnlike the original DRAW network where latent variables are independent spherical Gaussians N (0, I), the latent variables in the proposed alignDRAW model have their mean and variance depend on the previous hidden states of the generative LSTM hgent\u22121, except for P (Z1) = N (0, I). Namely, the mean and variance of the prior distribution over Zt are parameterized by:\nP (Zt|Z1:t\u22121) = N ( \u00b5(hgent\u22121), \u03c3(h gen t\u22121) ) ,\n\u00b5(hgent\u22121) = tanh(W\u00b5h gen t\u22121), \u03c3(hgent\u22121) = exp ( tanh(W\u03c3h gen t\u22121) ) ,\nwhere W\u00b5 \u2208 RD\u00d7n, W\u03c3 \u2208 RD\u00d7n are the learned model parameters, and n is the dimensionality of hgent , the hidden state of the generative LSTM. Similar to (Bachman & Precup, 2015), we have observed that the model performance is improved by including dependencies between latent variables.\nFormally, an image is generated by iteratively computing the following set of equations for t = 1, ..., T (see Fig. 2), with hgen0 and c0 initialized to learned biases:\nzt \u223c P (Zt|Z1:t\u22121) = N ( \u00b5(hgent\u22121), \u03c3(h gen t\u22121) ) , (1)\nst = align(h gen t\u22121, h lang), (2) hgent = LSTM gen(hgent\u22121, [zt, st]), (3)\nct = ct\u22121 + write(h gen t ), (4) x\u0303 \u223c P (x |y, Z1:T ) = \u220f i P (xi |y, Z1:T ) = \u220f i Bern(\u03c3(cT,i)). (5)\nThe align function is used to compute the alignment between the input caption and intermediate image generative steps (Bahdanau et al., 2015). Given the caption representation from the language model, hlang = [hlang1 , h lang 2 , ..., h lang N ], the align operator outputs a dynamic sentence representation st at each step through a weighted sum using alignment probabilities \u03b1t1...N :\nst = align(h gen t\u22121, h lang) = \u03b1t1h lang 1 + \u03b1 t 2h lang 2 + ...+ \u03b1 t Nh lang N . (6)\nThe corresponding alignment probability \u03b1tk for the k th word in the caption is obtained using the caption representation hlang and the current hidden state of the generative model hgent\u22121:\n\u03b1tk = exp\n( v> tanh(Uhlangk +Wh gen t\u22121 + b) ) \u2211N i=1 exp ( v> tanh(Uhlangi +Wh gen t\u22121 + b)\n) , (7) where v \u2208 Rl, U \u2208 Rl\u00d7m,W \u2208 Rl\u00d7n and b \u2208 Rl are the learned model parameters of the alignment model.\nThe LSTM gen function of Eq. 3 is defined by the LSTM network with forget gates (Gers et al., 2000) at a single time-step. To generate the next hidden state hgent , the LSTM\ngen takes the previous hidden state hgent\u22121 and combines it with the input from both the latent sample zt and the sentence representation st.\nThe output of the LSTM gen function hgent is then passed through the write operator which is added to a cumulative canvas matrix ct \u2208 Rh\u00d7w (Eq. 4). The write operator produces two arrays of 1D Gaussian filter banks Fx(h gen t ) \u2208 Rh\u00d7p and Fy(h gen t ) \u2208 Rw\u00d7p whose filter locations and scales are computed from the generative LSTM hidden state hgent (same as defined in Gregor et al. (2015)). The Gaussian filter banks are then applied to the generated p-by-p image patch K(hgent ) \u2208 Rp\u00d7p, placing it onto the canvas:\n\u2206ct = ct \u2212 ct\u22121 = write(hgent ) = Fx(h gen t )K(h gen t )Fy(h gen t ) >. (8)\nFinally, each entry cT,i from the final canvas matrix cT is transformed using a sigmoid function \u03c3 to produce a conditional Bernoulli distribution with mean vector \u03c3(cT ) over the h\u00d7w image pixels x given the latent variables Z1:T and the input caption y1. In practice, when generating an image x, instead of sampling from the conditional Bernoulli distribution, we simply use the conditional mean x = \u03c3(cT )."}, {"heading": "3.3 LEARNING", "text": "The model is trained to maximize a variational lower bound L on the marginal likelihood of the correct image x given the input caption y:\nL = \u2211 Z Q(Z |x,y) logP (x |y, Z)\u2212DKL (Q(Z |x,y) \u2016P (Z |y)) \u2264 logP (x |y). (9) Similar to the DRAW model, the inference recurrent network produces an approximate posterior Q(Z1:T |x,y) via a read operator, which reads a patch from an input image x using two arrays of 1D Gaussian filters (inverse of write from section 3.2) at each time-step t. Specifically,\nx\u0302t = x\u2212 \u03c3(ct\u22121), (10) rt = read(xt, x\u0302t, h gen t\u22121), (11)\nhinfert = LSTM infer(hinfert\u22121 , [rt, h gen t\u22121]), (12) Q(Zt|x,y, Z1:t\u22121) = N ( \u00b5(hinfert ), \u03c3(h infer t ) ) , (13)\nwhere x\u0302 is the error image and hinfer0 is initialized to the learned bias b. Note that the inference LSTM infer takes as its input both the output of the read operator rt \u2208 Rp\u00d7p, which depends on the original input image x, and the previous state of the generative decoder hgent\u22121, which depends on the latent sample history z1:t\u22121 and dynamic sentence representation st\u22121 (see Eq. 3). Hence, the approximate posterior Q will depend on the input image x, the corresponding caption y, and the latent history Z1:t\u22121, except for the first step Q(Z1|x), which depends only on x. The terms in the variational lower bound Eq. 9 can be rearranged using the law of total expectation. Therefore, the variational bound L is calculated as follows:\nL =EQ(Z1:T |y,x) [ log p(x |y, Z1:T )\u2212 T\u2211 t=2 DKL (Q(Zt |Z1:t\u22121,y,x) \u2016P (Zt |Z1:t\u22121,y)) ] \u2212DKL (Q(Z1 |x) \u2016P (Z1)) . (14)\n1We also experimented with a conditional Gaussian observation model, but it worked worse compared to the Bernoulli model.\nA yellow school bus parked in a parking lot. A red school bus parked in a parking lot. A green school bus parked in a parking lot. A blue school bus parked in a parking lot.\nThe expectation can be approximated by L Monte Carlo samples z\u03031:T from Q(Z1:T |y,x):\nL \u2248 1 L L\u2211 l=1 [ log p(x |y, z\u0303l1:T )\u2212 T\u2211 t=2 DKL ( Q(Zt | z\u0303l1:t\u22121,y,x) \u2016P (Zt | z\u0303l1:t\u22121,y) )] \u2212DKL (Q(Z1 |x) \u2016P (Z1)) . (15)\nThe model can be trained using stochastic gradient descent. In all of our experiments, we used only a single sample fromQ(Z1:T |y,x) for parameter learning. Training details, hyperparameter settings, and the overall model architecture are specified in Appendix B. We will also release our code with pretrained models."}, {"heading": "3.4 GENERATING IMAGES FROM CAPTIONS", "text": "During the image generation step, we discard the inference network and instead sample from the prior distribution. Due to the blurriness of samples generated by the DRAW model, we perform an additional post processing step where we use an adversarial network trained on residuals of a Laplacian pyramid conditioned on the skipthought representation (Kiros et al., 2015) of the captions to sharpen the generated images, similar to (Denton et al., 2015). By fixing the prior of the adversarial generator to its mean, it gets treated as a deterministic neural network that allows us to define the conditional data term in Eq. 14 on the sharpened images and estimate the variational lower bound accordingly."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 MICROSOFT COCO", "text": "Microsoft COCO (Lin et al., 2014) is a large dataset containing 82,783 images, each annotated with at least 5 captions. The rich collection of images with a wide variety of styles, backgrounds and objects makes the task of learning a good generative model very challenging. For consistency with related work on caption generation, we used only the first five captions when training and evaluating our model. The images were resized to 32\u00d732 pixels for consistency with other tiny image datasets (Krizhevsky, 2009). In the following subsections, we analyzed both the qualitative and quantitative aspects of our model as well as compared its performance with that of other, related generative models.2 Appendix A further reports some additional experiments using the MNIST dataset."}, {"heading": "4.1.1 ANALYSIS OF GENERATED IMAGES", "text": "The main goal of this work is to learn a model that can understand the semantic meaning expressed in the textual descriptions of images, such as the properties of objects, the relationships between them, and then use that knowledge to generate relevant images. To examine the understanding of\n2To see more generated images, visit http://www.cs.toronto.edu/\u02dcemansim/cap2im.html\nA very large commercial plane flying in blue skies. A very large commercial plane flying in rainy skies. A herd of elephants walking across a dry grass field. A herd of elephants walking across a green grass field.\nFigure 4: Bottom: Examples of changing the background while keeping the caption fixed. Top: The respective nearest training images based on pixel-wise L2 distance. The nearest images from the training set also indicate that the model was not simply copying the patterns it observed during the learning phase.\nour model, we wrote a set of captions inspired by the COCO dataset and changed some words in the captions to see whether the model made the relevant changes in the generated samples.\nFirst, we explored whether the model understood one of the most basic properties of any object, the color. In Fig. 3, we generated images of school buses with four different colors: yellow, red, green and blue. Although, there are images of buses with different colors in the training set, all mentioned school buses are specifically colored yellow. Despite that, the model managed to generate images of an object that is visually reminiscent of a school bus that is painted with the specified color.\nApart from changing the colors of objects, we next examined whether changing the background of the scene described in a caption would result in the appropriate changes in the generated samples. The task of changing the background of an image is somewhat harder than just changing the color of an object because the model will have to make alterations over a wider visual area. Nevertheless, as shown in Fig. 4 changing the skies from blue to rainy in a caption as well as changing the grass type from dry to green in another caption resulted in the appropriate changes in the generated image.\nDespite a large number of ways of changing colors and backgrounds in descriptions, in general we found that the model made appropriate changes as long as some similar pattern was present in the training set. However, the model struggled when the visual difference between objects was very small, such as when the objects have the same general shape and color. In Fig. 3, we demonstrate that when we swap two objects that are both visually similar, for example cats and dogs, it is difficult to discriminate solely from the generated samples whether it is an image of a cat or dog, even though we might notice an animal-like shape. This highlights a limitation of the model in that it has difficulty modelling the fine-grained details of objects.\nAs a test of model generalization, we tried generating images corresponding to captions that describe scenarios that are highly unlikely to occur in real life. These captions describe a common object doing unusual things or set in a strange location, for example \u201cA toilet seat sits open in the grass field\u201d. Even though some of these scenarios may never occur in real life, it is very easy for humans to imagine the corresponding scene. Nevertheless, as you can see in Fig. 1, the model managed to generate reasonable images."}, {"heading": "4.1.2 ANALYSIS OF ATTENTION", "text": "After flipping sets of words in the captions, we further explored which words the model attended to when generating images. It turned out that during the generation step, the model mostly focused on the specific words (or nearby words) that carried the main semantic meaning expressed in the sentences. The attention values of words in sentences helped us interpret the reasons why the model made the changes it did when we flipped certain words. For example, in Fig. 5, top row, we can see that when we flipped the word \u201cdesert\u201d to \u201cforest\u201d, the attention over words in the sentence did not change drastically. This suggests that, in their respective sentences, the model looked at \u201cdesert\u201d and \u201cforest\u201d with relatively equal probability, and thus made the correct changes. In contrast, when we swap words \u201cbeach\u201d and \u201csun\u201d, we can see a drastic change between sentences in the probability distribution over words. By noting that the model completely ignores the word \u201csun\u201d in the second\nA rider on a blue motorcycle in the desert. A rider on a blue motorcycle in the forest. A surfer, a woman, and a child walk on the beach. A surfer, a woman, and a child walk on the sun.\nalignDRAW LAPGAN Conv-Deconv VAE Fully-Conn VAE\nFigure 5: Top: Examples of most attended words while changing the background in the caption. Bottom: Four different models displaying results from sampling caption A group of people walk on a beach with surf boards.\nsentence, we can therefore gain a more thorough understanding of why we see no visual differences between the images generated by each caption.\nWe also tried to analyze the way the model generated images. Unfortunately, we found that there was no significant connection between the patches drawn on canvas and the most attended words at particular time-steps."}, {"heading": "4.1.3 COMPARISON WITH OTHER MODELS", "text": "Quantitatively evaluating generative models remains a challenging task in itself as each method of evaluation suffers from its own specific drawbacks. Compared to reporting classification accuracies in discriminative models, the measures defining generative models are intractable most of the times and might not correctly define the real quality of the model. To get a better comparison between performances of different generative models, we report results on two different metrics as well as a qualitative comparison of different generative models.\nWe compared the performance of the proposed model to the DRAW model conditioned on captions without the align function (noalignDRAW) as well as the DRAW model conditioned on the skipthought vectors of (Kiros et al., 2015) (skipthoughtDRAW). All of the conditional DRAW models were trained with a binary cross-entropy cost function, i.e. they had Bernoulli conditional likelihoods. We also compared our model with Fully-Connected (Fully-Conn) and ConvolutionalDeconvolutional (Conv-Deconv) Variational Autoencoders which were trained with the least squares cost function. The LAPGAN model of (Denton et al., 2015) was trained on a two level Laplacian Pyramid with a GAN as a top layer generator and all stages were conditioned on the same skipthought vector.\nIn Fig. 5, bottom row, we generated several samples from the prior of each of the current state-ofthe-art generative models, conditioned on the caption \u201cA group of people walk on a beach with surf boards\u201d. While all of the samples look sharp, the images generated by LAPGAN look more noisy and it is harder to make out definite objects, whereas the images generated by variational models trained with least squares cost function have a watercolor effect on the images. We found that the quality of generated samples was similar among different variants of conditional DRAW models.\nAs for the quantitative comparison of different models, we first compare the performances of the model trained with variational methods. We rank the images in the test set conditioned on the captions based on the variational lower bound of the log-probabilities and then report the PrecisionRecall metric as an evaluation of the quality of the generative model (see Table 1.). Perhaps unsurprisingly, generative models did not perform well on the image retrieval task. To deal with the large computational complexity involved in looping through each test image, we create a shortlist of one hundred images including the correct one, based on the images having the closest Euclidean distance in the last fully-connected feature space of a VGG-like model (Simonyan & Zisserman, 2015) trained on the CIFAR dataset3 (Krizhevsky, 2009). Since there are \u201ceasy\u201d images for which\n3The architecture of the model is described here http://torch.ch/blog/2015/07/30/cifar. html. The shortlist of test images used for evaluation can be downloaded from http://www.cs. toronto.edu/\u02dcemansim/cap2im/test-nns.pkl.\nthe model assigns high log-probabilities independent of the query caption, we instead look at the ratio of the likelihood of the image conditioned on the sentence to the likelihood of the image conditioned on the mean sentence representation in the training set, following the retrieval protocol of (Kiros et al., 2014b). We also found that the lower bound on the test log-probabilities decreased for sharpened images, and that sharpening considerably hurt the retrieval results. Since sharpening changes the statistics of images, the estimated log-probabilities of image pixels is not necessarily a good metric. Some examples of generated images before and after sharpening are shown in Appendix C.\nInstead of calculating error per pixel, we turn to a smarter metric, the Structural Similarity Index (SSI) (Wang et al., 2004), which incorporates luminance and contrast masking into the error calculation. Strong inter-dependencies of closer pixels are also taken into account and the metric is calculated on small windows of the images. Due to independence property of test captions, we sampled fifty images from the prior of each generative model for every caption in the test set in order to calculate SSI. As you can see on Table 1, SSI scores achieved by variational models were higher compared to SSI score achieved by LAPGAN."}, {"heading": "5 DISCUSSION", "text": "In this paper, we demonstrated that the alignDRAW model, a combination of a recurrent variational autoencoder with an alignment model over words, succeeded in generating images that correspond to a given input caption. By extensively using attentional mechanisms, our model gained several advantages. Namely, the use of the visual attention mechanism allowed us to decompose the problem of image generation into a series of steps instead of a single forward pass, while the attention over words provided us an insight whenever our model failed to generate a relevant image. Additionally, our model generated images corresponding to captions which generalized beyond the training set, such as sentences describing novel scenarios which are highly unlikely to occur in real life.\nBecause the alignDRAW model tends to output slightly blurry samples, we augmented the model with a sharpening post-processing step in which GAN generated edges which were added to the alignDRAW samples. Unfortunately, this is not an ideal solution due to the fact that the whole model was not trained in an end-to-end fashion. Therefore a direction of future work would be to find methods that can bypass the separate post-processing step and output sharp images directly in an end-to-end manner.\nAcknowledgments: This work was supported by Samsung and IARPA, Raytheon BBN Contract No. D11PC20071. We would like to thank developers of Theano (Bastien et al., 2012), the authors of (Denton et al., 2015) for open sourcing their code, and Ryan Kiros and Nitish Srivastava for helpful discussions."}, {"heading": "APPENDIX A: MNIST WITH CAPTIONS", "text": "As an additional experiment, we trained our model on the MNIST dataset with artificial captions. Either one or two digits from the MNIST training dataset were placed on a 60 \u00d7 60 blank image. One digit was placed in one of the four (top-left, top-right, bottom-left or bottom-right) corners of the image. Two digits were either placed horizontally or vertically in non-overlapping fashion. The corresponding artificial captions specified the identity of each digit along with their relative positions, e.g. \u201cThe digit three is at the top of the digit one\u201d, or \u201cThe digit seven is at the bottom left of the image\u201d.\nThe generated images together with the attention alignments are displayed in Figure 6. The model correctly displayed the specified digits at the described positions and even managed to generalize reasonably well to the configurations that were never present during training. In the case of generating two digits, the model would dynamically attend to the digit in the caption it was drawing at that particular time-step. Similarly, in the setting where the caption specified only a single digit, the model would correctly attend to the digit in the caption during the whole generation process. In both cases, the model placed small attention values on the words describing the position of digits in the images."}, {"heading": "APPENDIX B: TRAINING DETAILS", "text": "HYPERPARAMETERS\nEach parameter in alignDRAW was initialized by sampling from a Gaussian distribution with mean 0 and standard deviation 0.01. The model was trained using RMSprop with an initial learning rate of 0.001. For the Microsoft COCO task, we trained our model for 18 epochs. The learning rate was reduced to 0.0001 after 11 epochs. For the MNIST with Captions task, the model was trained for 150 epochs and the learning rate was reduced to 0.0001 after 110 epochs. During each epoch, randomly created 10, 000 training samples were used for learning. The norm of the gradients was clipped at 10 during training to avoid the exploding gradients problem.\nWe used a vocabulary size of K = 25323 and K = 22 for the Microsoft COCO and MNIST with Captions datasets respectively. All capital letters in the words were converted to small letters as a preprocessing step. For all tasks, the hidden states \u2212\u2192 h langi and \u2190\u2212 h langi in the language model had 128 units. Hence the dimensionality of the concatenated state of the Bidirectional LSTM hlangi = [ \u2212\u2192 h langi , \u2190\u2212 h langi ] was 256. The parameters in the align operator (Eq. 7) had a dimensionality of l = 512, so that v \u2208 R512, U \u2208 R512\u00d7256, W \u2208 R512\u00d7ngen and b \u2208 R512. The architectural configurations of the alignDRAW models are shown in Table 2.\nThe GAN model used for sharpening had the same configuration as the 28 \u00d7 28 model trained by Denton et al. (2015) on the edge residuals of the CIFAR dataset. The configuration can be found at https://gist.github.com/soumith/e3f722173ea16c1ea0d9. The model was trained for 6 epochs.\nEVALUATION\nTable 3 shows the estimated variational lower bounds on the average train/validation/test logprobabilities. Note that the alignDRAW model does not suffer much from overfitting. The results substantially worsen after sharpening test images."}, {"heading": "APPENDIX C: EFFECT OF SHARPENING IMAGES.", "text": "Some examples of generated images before (top row) and after (bottom row) sharpening images using an adversarial network trained on residuals of a Laplacian pyramid conditioned on the skipthought vectors of the captions."}], "references": [{"title": "Data generation as sequential decision making", "author": ["Bachman", "Philip", "Precup", "Doina"], "venue": "In NIPS,", "citeRegEx": "Bachman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bachman et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "CoRR, abs/1211.5590,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "G\u00fcl\u00e7ehre", "\u00c7aglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Denton", "Emily L", "Chintala", "Soumith", "Szlam", "Arthur", "Fergus", "Robert"], "venue": "In NIPS,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers", "Felix", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "Neural Computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian J", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron C", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "Mohamed", "A.-r"], "venue": "In IEEE Workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Gutmann", "Michael", "Hyv\u00e4rinen", "Aapo"], "venue": "In AISTATS,", "citeRegEx": "Gutmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Li", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "In ICML,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Richard S"], "venue": "CoRR, abs/1411.2539,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Torralba", "Antonio", "Urtasun", "Raquel", "Fidler", "Sanja"], "venue": "In NIPS,", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Master\u2019s Thesis,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Microsoft COCO: Common objects in context", "author": ["T.Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Information processing in dynamical systems: foundations of harmony theory", "author": ["Smolensky", "Paul"], "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition,", "citeRegEx": "Smolensky and Paul.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky and Paul.", "year": 1986}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": "In ICML,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Wang", "Zhou", "Bovik", "Alan C", "Sheikh", "Hamid R", "Simoncelli", "Eero P"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "One approach is to learn a generative model of text conditioned on images, known as caption generation (Kiros et al., 2014a; Karpathy & Li, 2015; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 103, "endOffset": 184}, {"referenceID": 27, "context": "One approach is to learn a generative model of text conditioned on images, known as caption generation (Kiros et al., 2014a; Karpathy & Li, 2015; Vinyals et al., 2015; Xu et al., 2015).", "startOffset": 103, "endOffset": 184}, {"referenceID": 8, "context": "By extending the Deep Recurrent Attention Writer (DRAW) (Gregor et al., 2015), our model iteratively draws patches on a canvas, while attending to the relevant words in the description.", "startOffset": 56, "endOffset": 77}, {"referenceID": 4, "context": "The images generated by our alignDRAW model are refined in a post-processing step by a deterministic Laplacian pyramid adversarial network (Denton et al., 2015).", "startOffset": 139, "endOffset": 160}, {"referenceID": 19, "context": "We further illustrate how our method, learned on Microsoft COCO (Lin et al., 2014), generalizes to captions describing novel scenes that are not seen in the dataset, such as \u201cA stop sign is flying in blue skies\u201d (see Fig.", "startOffset": 64, "endOffset": 82}, {"referenceID": 18, "context": "Deep Neural Networks have achieved significant success in various tasks such as image recognition (Krizhevsky et al., 2012), speech transcription (Graves et al.", "startOffset": 98, "endOffset": 123}, {"referenceID": 7, "context": ", 2012), speech transcription (Graves et al., 2013), and machine translation (Bahdanau et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 1, "context": ", 2013), and machine translation (Bahdanau et al., 2015).", "startOffset": 33, "endOffset": 56}, {"referenceID": 10, "context": "Most of the previous work in generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdinov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006).", "startOffset": 166, "endOffset": 187}, {"referenceID": 6, "context": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are another type of generative models that use noise-contrastive estimation (Gutmann & Hyv\u00e4rinen, 2010) to avoid calculating an intractable partition function.", "startOffset": 39, "endOffset": 64}, {"referenceID": 1, "context": ", 2013), and machine translation (Bahdanau et al., 2015). While most of the recent success has been achieved by discriminative models, generative models have not yet enjoyed the same level of success. Most of the previous work in generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdinov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006). While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate derivatives of an intractable partition function (normalization constant), making it difficult to scale them to large datasets. Kingma & Welling (2014) have introduced the Variational Auto-Encoder (VAE) which can be seen as a neural network with continuous latent variables.", "startOffset": 34, "endOffset": 669}, {"referenceID": 1, "context": ", 2013), and machine translation (Bahdanau et al., 2015). While most of the recent success has been achieved by discriminative models, generative models have not yet enjoyed the same level of success. Most of the previous work in generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdinov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006). While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate derivatives of an intractable partition function (normalization constant), making it difficult to scale them to large datasets. Kingma & Welling (2014) have introduced the Variational Auto-Encoder (VAE) which can be seen as a neural network with continuous latent variables. The encoder is used to approximate a posterior distribution and the decoder is used to stochastically reconstruct the data from latent variables. Gregor et al. (2015) further introduced the Deep Recurrent Attention Writer (DRAW), extending the VAE approach by incorporating a novel differentiable attention mechanism.", "startOffset": 34, "endOffset": 959}, {"referenceID": 1, "context": ", 2013), and machine translation (Bahdanau et al., 2015). While most of the recent success has been achieved by discriminative models, generative models have not yet enjoyed the same level of success. Most of the previous work in generative models has focused on variants of Boltzmann Machines (Smolensky, 1986; Salakhutdinov & Hinton, 2009) and Deep Belief Networks (Hinton et al., 2006). While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate derivatives of an intractable partition function (normalization constant), making it difficult to scale them to large datasets. Kingma & Welling (2014) have introduced the Variational Auto-Encoder (VAE) which can be seen as a neural network with continuous latent variables. The encoder is used to approximate a posterior distribution and the decoder is used to stochastically reconstruct the data from latent variables. Gregor et al. (2015) further introduced the Deep Recurrent Attention Writer (DRAW), extending the VAE approach by incorporating a novel differentiable attention mechanism. Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are another type of generative models that use noise-contrastive estimation (Gutmann & Hyv\u00e4rinen, 2010) to avoid calculating an intractable partition function. The model consists of a generator that generates samples using a uniform distribution and a discriminator that discriminates between real and generated images. Recently, Denton et al. (2015) have scaled those models by training conditional GANs at each level of a Laplacian pyramid of images.", "startOffset": 34, "endOffset": 1526}, {"referenceID": 24, "context": "The model can be viewed as a part of the sequence-to-sequence framework (Sutskever et al., 2014; Cho et al., 2014; Srivastava et al., 2015).", "startOffset": 72, "endOffset": 139}, {"referenceID": 3, "context": "The model can be viewed as a part of the sequence-to-sequence framework (Sutskever et al., 2014; Cho et al., 2014; Srivastava et al., 2015).", "startOffset": 72, "endOffset": 139}, {"referenceID": 23, "context": "The model can be viewed as a part of the sequence-to-sequence framework (Sutskever et al., 2014; Cho et al., 2014; Srivastava et al., 2015).", "startOffset": 72, "endOffset": 139}, {"referenceID": 5, "context": "In a Bidirectional RNN, the two LSTMs (Hochreiter & Schmidhuber, 1997) with forget gates (Gers et al., 2000) process the input sequence from both forward and backward directions.", "startOffset": 89, "endOffset": 108}, {"referenceID": 8, "context": "2 IMAGE MODEL: THE CONDITIONAL DRAW NETWORK To generate an image x conditioned on the caption information y, we extended the DRAW network (Gregor et al., 2015) to include caption representation h at each step, as shown in Fig.", "startOffset": 138, "endOffset": 159}, {"referenceID": 1, "context": "The align function is used to compute the alignment between the input caption and intermediate image generative steps (Bahdanau et al., 2015).", "startOffset": 118, "endOffset": 141}, {"referenceID": 5, "context": "3 is defined by the LSTM network with forget gates (Gers et al., 2000) at a single time-step.", "startOffset": 51, "endOffset": 70}, {"referenceID": 5, "context": "3 is defined by the LSTM network with forget gates (Gers et al., 2000) at a single time-step. To generate the next hidden state h t , the LSTM gen takes the previous hidden state h t\u22121 and combines it with the input from both the latent sample zt and the sentence representation st. The output of the LSTM gen function h t is then passed through the write operator which is added to a cumulative canvas matrix ct \u2208 Rh\u00d7w (Eq. 4). The write operator produces two arrays of 1D Gaussian filter banks Fx(h gen t ) \u2208 Rh\u00d7p and Fy(h gen t ) \u2208 Rw\u00d7p whose filter locations and scales are computed from the generative LSTM hidden state h t (same as defined in Gregor et al. (2015)).", "startOffset": 52, "endOffset": 670}, {"referenceID": 16, "context": "Due to the blurriness of samples generated by the DRAW model, we perform an additional post processing step where we use an adversarial network trained on residuals of a Laplacian pyramid conditioned on the skipthought representation (Kiros et al., 2015) of the captions to sharpen the generated images, similar to (Denton et al.", "startOffset": 234, "endOffset": 254}, {"referenceID": 4, "context": ", 2015) of the captions to sharpen the generated images, similar to (Denton et al., 2015).", "startOffset": 68, "endOffset": 89}, {"referenceID": 19, "context": "1 MICROSOFT COCO Microsoft COCO (Lin et al., 2014) is a large dataset containing 82,783 images, each annotated with at least 5 captions.", "startOffset": 32, "endOffset": 50}, {"referenceID": 16, "context": "We compared the performance of the proposed model to the DRAW model conditioned on captions without the align function (noalignDRAW) as well as the DRAW model conditioned on the skipthought vectors of (Kiros et al., 2015) (skipthoughtDRAW).", "startOffset": 201, "endOffset": 221}, {"referenceID": 4, "context": "The LAPGAN model of (Denton et al., 2015) was trained on a two level Laplacian Pyramid with a GAN as a top layer generator and all stages were conditioned on the same skipthought vector.", "startOffset": 20, "endOffset": 41}, {"referenceID": 26, "context": "Instead of calculating error per pixel, we turn to a smarter metric, the Structural Similarity Index (SSI) (Wang et al., 2004), which incorporates luminance and contrast masking into the error calculation.", "startOffset": 107, "endOffset": 126}], "year": 2015, "abstractText": "Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.", "creator": "LaTeX with hyperref package"}}}