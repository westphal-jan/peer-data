{"id": "1511.04108", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "LSTM-based Deep Learning Models for Non-factoid Answer Selection", "abstract": "in this paper, we apply modeling general natural learning ( dl ) framework targeting detailed answer projection task, which happens not run on sufficiently defined features that linguistic tools. the basic assumption requires to build the embeddings of questions and answers based on moderately robust short - term recognition ( db ) models, and measure language closeness by cosine similarity. there further offer 2d basic for introducing simple halves. one is to define a comprehensive composite grammar whereby questions and answers before combining convolutional neural network with the derived framework, the other seeking to introduce a useful but efficient graphical tensor in order enable project meaningful answer representation according throughout the question stimuli. several variations accompanying simulation are provided. experimental demonstration on a public insurance - led premise is wherein neural extended models fully outperform two statistical - beyond - the - art data - human scenarios and that strong dl threshold.", "histories": [["v1", "Thu, 12 Nov 2015 22:01:54 GMT  (109kb,D)", "http://arxiv.org/abs/1511.04108v1", null], ["v2", "Wed, 18 Nov 2015 15:00:46 GMT  (199kb,D)", "http://arxiv.org/abs/1511.04108v2", "correct some typos"], ["v3", "Thu, 7 Jan 2016 17:56:29 GMT  (130kb,D)", "http://arxiv.org/abs/1511.04108v3", "added new experiments on TREC-QA"], ["v4", "Mon, 28 Mar 2016 04:12:45 GMT  (122kb,D)", "http://arxiv.org/abs/1511.04108v4", "added new experiments on TREC-QA"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ming tan", "cicero dos santos", "bing xiang", "bowen zhou"], "accepted": false, "id": "1511.04108"}, "pdf": {"name": "1511.04108.pdf", "metadata": {"source": "CRF", "title": "LSTM-BASED DEEP LEARNING MODELS FOR NON- FACTOID ANSWER SELECTION", "authors": ["Ming Tan", "Bing Xiang"], "emails": ["mingtan@us.ibm.com", "bingxia@us.ibm.com", "zhou@us.ibm.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The answer selection problem can be formulated as follows: Given a question q and an answer candidate pool {a1, a2, \u00b7 \u00b7 \u00b7 , as} for this question, we aim to search for the best answer candidate ak, where 1 \u2264 k \u2264 s. An answer is a token sequence with an arbitrary length, and a question can correspond to multiple ground-truth answers. In testing, the candidate answers for a question may not be observed in the training phase. Answer selection is one of the essential components in typical question answering (QA) systems. It is also a stand-alone task with applications in knowledge base construction and information extraction.\nThe nature of this problem is a mapping between a question and an answer. The major challenge is that the correct answer might not directly share keywords with the question. Instead, they may only be semantically related. Moreover, the answers are sometimes noisy and contain a large amount of unrelated information.\nRecently, deep learning models have obtained a significant success on various natural language processing tasks, such as semantic analysis (Tang et al., 2015), machine translation (Bahdanau et al., 2015) and text summarization (Rush et al., 2015).\nIn this paper, we propose a deep learning framework for answer selection which does not require any feature engineering, linguistic tools, or external resources. This framework is based on building bidirectional long short term memory (biLSTM) models on both questions and answers respectively, connecting with a pooling layer and utilizing a similarity metric to measure the matching degree. We improve this basic model from two perspectives. Firstly, a simple pooling layer may suffer from the incapability of keeping the local linguistic information. In order to obtain better embeddings for the questions and answers, we build a convolutional neural network (CNN) structure on top of biLSTM. Secondly, in order to better distinguish candidate answers according to the question, we introduce a simple but efficient attention model to this framework for the answer embedding generation according to the question context. We conduct the experiments based on InsuranceQA (Feng et al., 2015) 1, a newly-released non-factoid QA dataset from the insurance domain. The\n1git clone https://github.com/shuzi/insuranceQA.git\nar X\niv :1\n51 1.\n04 10\n8v 1\n[ cs\n.C L\n] 1\n2 N\nov 2\n01 5\nproposed models demonstrate a significant out-performance compared to two state-of-the-art nonDL baselines and a strong DL baseline based on CNN.\nThe rest of the paper is organized as follows: Section 2 describes the related work for answer selection; Section 3 provides the details of the proposed models; Experimental settings and results are discussed in section 4 and 5. Finally, we draw conclusions in section 6."}, {"heading": "2 RELATED WORK", "text": "Previous work of this area usually required feature engineering, linguistic tools, or external resources. For example, semantic features were constructed based on WordNet in (Yih et al., 2013). This model pairs semantically related words based on word semantic relations. In (Wang & Manning, 2010; Wang et al., 2007), the answer selection problem is transformed to a syntactical matching between the question/answer parse trees. Some work tried to fulfill the matching using minimal edit sequences between dependency parse trees (Heilman & Smith, 2010; Yao et al., 2013). Recently, discriminative tree-edit features extraction and engineering over parsing trees were automated in (Severyn & Moschitti, 2013).\nWhile these methods show effectiveness, they might suffer from the availability of those additional resources, the remarkable effort of feature engineering and the systematic complexity by introducing linguistic tools, such as parse trees and dependency trees.\nThere were prior methods using deep learning technologies for the answer selection task. The approaches for non-factoid question answering generally pursue the solution on the following directions: Firstly, the question and answer representations are learned and matched by certain similarity metrics (Feng et al., 2015; Yu et al., 2014). Secondly, a joint feature vector is constructed based on both the question and the answer, and then the task can be converted into a classification or learningto-rank problem (Wang & Nyberg, 2015). Finally, recently proposed models for textual generation can intrinsically be used for answer selection and generation (Bahdanau et al., 2015; Vinyals & Le, 2015).\nThe proposed framework belongs to the first category. There are two major differences between our approaches and the work in (Feng et al., 2015): (1) The architectures developed in (Feng et al., 2015) are only based on CNN, whereas our models are based on bidirectional LSTMs, which are more capable of exploiting long-range sequential context information. Moreover, we also integrate the CNN structures on the top of biLSTM for better performance. (2) Feng et al. (2015) tackle the question and answer independently, while one of our proposed structures developed an efficient attentive models to generate answer embeddings according to the question."}, {"heading": "3 APPROACH", "text": "In this section, we describe the proposed framework and its variations. We first introduce the general framework, which is to build bi-directional LSTM on both questions and their answer candidates, and then use the similarity metric to measure the distance of question answer pairs. In the following two subsections, we extend the basic model in two independent directions."}, {"heading": "3.1 BASIC MODEL: QA-LSTM", "text": "Long Short-Term Memory (LSTM): Recurrent Neural Networks (RNN) have been widely exploited to deal with variable-length sequence input. The long-distance history is stored in a recurrent hidden vector which is dependent on the immediate previous hidden vector. LSTM (Hochreiter & Schmidhuber, 1997) is one of the popular variations of RNN to mitigate the gradient vanish problem of RNN. Our LSTM implementation is similar to the one in (Graves et al., 2013) with minor modification. Given an input sequence x = {x(1),x(2), \u00b7 \u00b7 \u00b7 ,x(n)}, where x(t) is an E-dimension word vector in this paper. The hidden vector h(t) ( the size is H ) at the time step t is updated as follows.\nit = \u03c3(Wix(t) +Uih(t\u2212 1) + bi) (1)\nft = \u03c3(Wfx(t) +Ufh(t\u2212 1) + bf ) (2) ot = \u03c3(Wox(t) +Uoh(t\u2212 1) + bo) (3) C\u0303t = tanh(Wcx(t) +Uch(t\u2212 1) + bc) (4) Ct = it \u2217 C\u0303t + ft \u2217 Ct\u22121 (5) ht = ot \u2217 tanh(Ct) (6)\nIn the LSTM architecture, there are three gates (input i, forget f and output o), and a cell memory vector c. \u03c3 is the sigmoid function. The input gate can determine how incoming vectors xt alter the state of the memory cell. The output gate can allow the memory cell to have an effect on the outputs. Finally, the forget gate allows the cell to remember or forget its previous state. W \u2208 RH\u00d7E , U \u2208 RH\u00d7H and b \u2208 RH\u00d71 are the network parameters. Bidirectional Long Short-Term Memory (biLSTM): Single direction LSTMs suffer a weakness of not utilizing the contextual information from the future tokens. Bidirectional LSTM utilizes both the previous and future context by processing the sequence on two directions, and generate two independent sequences of LSTM output vectors. One processes the input sequence in the forward direction, while the other processes the input in the reverse direction. The output at each time step is the concatenation of the two output vectors from both directions, ie. ht = \u2212\u2192 ht \u2016 \u2190\u2212 ht .\nQA-LSTM: The basic model in this work is shown in Figure 1. BiLSTM generates distributed representations for both the question and answer independently, and then utilize cosine similarity to measure their distance. Following the same ranking loss in (Feng et al., 2015; Weston et al., 2014; Hu et al., 2014), we define the training objective as a hinge loss.\nL = max{0,M \u2212 cosine(q, a+) + cosine(q, a\u2212)} (7)\nwhere a+ is a ground truth answer, a\u2212 is an incorrect answer randomly chosen from the entire answer space, and M is constant margin. We treat any question with more than one ground truth as multiple training examples, each for one ground truth.\nThere are three simple ways to generate representations for questions and answers based on the word-level biLSTM outputs: (1) Average pooling; (2) max pooling; (3) the concatenation of the last vectors on both directions. The three strategies will be compared with the experimental performance in Section 5. Dropout operation is performed on the QA representations before cosine similarity matching.\nFinally, from preliminary experiments, we observe that the architectures, in which both question and answer sides share the same network parameters, is significantly better than the one that the question and answer sides own their own parameters separately, and converges much faster. As discussed in (Feng et al., 2015), this is reasonable, because for a shared layer network, the corresponding elements in question and answer vectors represent the same biLSTM outputs. While for the network with separate question and answer parameters, there is no such constraint and the model has doublesized parameters, making it difficult to learn for the optimizer."}, {"heading": "3.2 QA-LSTM/CNN", "text": "In the previous subsection, we generate the question and answer representations only by simple operations, such as max or mean pooling. In this subsection, we resort to a CNN structure built on the outputs of biLSTM, in order to give a more composite representation of questions and answers.\nThe structure of CNN in this work is similar to the one in (Feng et al., 2015), as shown in Figure 2. Unlike the traditional forward neural network, where each output is interactive with each input, the convolutional structure only imposes local interactions between the inputs within a filter size m.\nIn this work, for every window with the size of m in biLSTM output vectors, ie. Hm(t) = [h(t),h(t + 1), \u00b7 \u00b7 \u00b7 ,h(t + m \u2212 1)], where t is a certain time step, the convolutional filter F = [F(0) \u00b7 \u00b7 \u00b7F(m\u2212 1)] will generate one value as follows.\noF (t) = tanh\n[( m\u22121\u2211\ni=0\nh(t+ i)TF(i) ) +b ] (8)\nwhere b is a bias, and F and b are the parameters of this single filter.\nSame as typical CNNs, a max-k pooling layer is built on the top of the convolutional layer. Intuitively, we want to emphasize the top-k values from each convolutional filter. By k-MaxPooling, the maximum k values will be kept for one filter, which indicate the highest degree that a filter matches the input sequence.\nFinally, there are N parallel filters, with different parameter initialization, and the convolutional layer gets N -dimension output vectors. We get two output vectors with dimension of kN for the questions and answers respectively. In this work, k = 1. k > 1 did not show any obvious improvement in our early experiments. The intuition of this structure is, instead of evenly considering the lexical information of each token as the previous subsection, we emphasize on certain parts of the answer, such that QA-LSTM/CNN can more effectively differentiate the ground truths and incorrect answers."}, {"heading": "3.3 ATTENTION-BASED QA-LSTM", "text": "In the previous subsection, we described one extension from the basic model, which targets at providing more composite embeddings for questions and answers respectively. In this subsection, we investigate an extension from another perspective. Instead of generating QA representation independently, we leverage a simple attention model for the answer vector generation based on the knowledge of question.\nThe fixed width of hidden vectors becomes a bottleneck, when the bidirectional LSTM models must propagate dependencies over long distances over the questions and answers. An attention mech-\nanism are used to alleviate this weakness by dynamically aligning the more informative parts of answers to the questions. This strategy has been used in many other natural language processing tasks, such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), sentence summarization (Rush et al., 2015) and factoid question answering (Hermann et al., 2015; Sukhbaatar et al., 2015).\nInspired by the work in (Hermann et al., 2015), we develop a very simple but efficient attention on the basic model. Figure 3 shows the structure. Prior to the average or mean pooling, each biLSTM output vector will be multiplied by a softmax weight, which is determined by the question embedding from biLSTM.\nSpecifically, given the output vector of biLSTM on the answer side at time step t, ha(t), and the question embedding, oq , the updated vector h\u0303a(t) for each answer token are formulated below.\nma,q(t) = tanh(Wamha(t) +Wqmoq) (9)\nsa,q(t) \u221d exp(wTmsma,q(t)) (10) h\u0303a(t) = ha(t)sa,q(t) (11)\nwhere Wam, Wqm and wms are attention parameters. The major difference between this approach and the one in (Hermann et al., 2015) is that Hermann et al. (2015)\u2019s attentive reader emphasizes the informative part of supporting facts, and then uses a combined embedding of the query and the supporting facts to predict the factoid answers. In this work, we directly use the attention-based representations to measure the QA distances. Experiments show that the attention models lead to a substantial improvement from non-attentive QA-LSTM models, indicating the attention mechanism can more efficiently distinguish correct answers from incorrect ones according to the question text."}, {"heading": "4 EXPERIMENT SETTINGS", "text": "Having described a number of models in the previous section, we evaluate the proposed approaches on the insurance domain dataset, InsuranceQA, provided by Feng et al. (2015). The InsuranceQA dataset provides a training set, a validation set, and two test sets. We list the numbers of questions and answers of the dataset in Table 1. It suggests that a question may correspond to multiple answers. The questions are much shorter than answers. The average length of questions is 7, and the average length of answers is 94. This corpus contains 24981 unique answers in total. The answer pool for each question has 500 candidates, including the ground truth answers. The answer pools are included in the published dataset."}, {"heading": "4.1 BASELINES", "text": "For comparison, we report the performances of four baselines in Table 2: two state-of-the-art nonDL approaches and two variations of a strong DL approach based on CNN as follows.\nBad-of-word: The idf-weighted sum of word vectors for the question and for all of its answer candidates is used as a feature vector. Similar to this work, the candidates are re-ranked according the cosine similarity to a question.\nMetzler-Bendersky IR model: A state-of-the-art weighted dependency (WD) model, which employs a weighted combination of term-based and term proximity-based ranking features to score each candidate answer.\nArchitecture-II in (Feng et al., 2015): Instead of using LSTM, a CNN model is employed to learn a distributed vector representation of a given question and its answer candidates, and the answers are scored by cosine similarity with the question. No attention model is used in this baseline.\nArchitecture-II with Geometricmean of Euclidean and Sigmoid Dot product (GESD): GESD is used to measure the distance between the question and answers. This is the model which achieved the best performance in (Feng et al., 2015)."}, {"heading": "4.2 SETUP", "text": "The models in this work is implemented with Theano (Bastien et al., 2012) from scratch, and all experiments are processed in a GPU cluster. Each model is trained for 140 epochs. All models are converged after 140 epochs. We use the accuracy on validation set to locate the best epoch and best hyper-parameter settings for testing.\nThe word embedding is trained by word2vec (Mikolov et al., 2013), and the word vector size is 100. Word embeddings are also parameters and are optimized as well during the training. Stochastic Gradient Descent (SGD) is the optimization strategy. The learning rate in our experiments is 0.1. We tried different margin values, such as 0.05, 0.1 and 0.15, and finally fixed the margin as 0.1. We also tried to include l2 norm in the training objective. However, preliminary experiments show that regularization factors do not show any improvements. Also, the dimension of LSTM output\nvectors is 141 for one direction, such that biLSTM has a comparable number of parameters with a single-direction LSTM with 200 dimension.\nWe train our models in mini-batches (the batch size B is 100), and the maximum length L of questions and answers is 200. Any tokens out of this range will be discarded. Because the questions or answers within a mini-batch may have different lengths, we resort to a mask matrix M \u2208 RB\u00d7L to indicate the real length of each token sequence."}, {"heading": "5 RESULTS AND DISCUSSIONS", "text": "In this section, detailed analysis on experimental results are given. Table 3 summarizes the results of our models on InsuranceQA dataset.\nFrom Row (A) to (C), we list QA-LSTM without either CNN structure or attention model. They vary on how to utilize the biLSTM output vectors to form sentential embeddings for questions and answers in shown in section 3.1. We can observe that just concatenating of the last vectors from both direction (A) performs the worst. It is surprised to see using max-pooling (C) is much better than average pooling (B). The potential reason is that the max-pooling extracts more local values for each dimension, so that more local information can be reflected on the output embeddings.\nFrom Row (D) to (F), CNN layers are built on the top of the biLSTM with different filter numbers. We set the filter width m = 2, and we did not see better performance if we increase m to 3 or 4. Row (F) with 4000 filters gets the best validation accuracy, obtained a comparable performance with the best baseline (Row (D) in Table 2 ). Row F shared a highly analogous CNN structure with Architecture II in Feng et al. (2015), except that the later used a shallow hidden layer to transform the word embeddings into the input of CNN structure, while Row F take the output of biLSTM as CNN input.\nRow (G) and (H) corresponds to QA-LSTM with the attention model. (G) connects the output vectors of answers after attention with a max pooling layer, and (H) with an average pooling. In comparison to Model (C), Model (G) shows over 2% improvement on both validation and Test2 sets. With respect to the model with mean pooling layers (B), the improvement from attention is more remarkable. Model (H) is over 8% higher on all datasets compared to (B), and gets improvements from the best baseline by 3%, 2.8% and 1.2% on the validation, Test1 and Test2 sets, respectively. Compared to Architecture II in (Feng et al., 2015), which involved a large number of CNN filters, (H) model also has fewer parameters.\nAlso, we made a minor change on Model (F) to include the attention model described in section 3.3, as indicated in Row (I). Instead of directly using the output vectors of biLSTM as the input of CNN structure, the biLSTM outputs employ the attention according to Eq. 9-11 in section3.3, and oq is the average of question biLSTM output vectors, such that the question context can be used to evaluate the softmax weights of the input of CNN. Although compared to (F), it shows 1% improvement on all sets, we fail to see obvious improvements compared to Model (H). Although Model (I) achieves better number on Test2, but does not on validation and Test1. We assume that the effective attention might have vanished during the CNN operations. However, both model (H) and (I) substantially outperform all the baselines.\nFinally, we investigate the proposed models on how they perform with respect to long answers. We divide the questions of Test1 and Test2 sets into eleven buckets, according to the average length of their ground truths. In the table of Figure 4, we list the bucket levels and the number of questions which belong to each bucket, for example, Test1 has 165 questions, whose average ground truth lengths are 55 < L \u2264 60. We select models of (C), (F), (H) and (I) in Table 3 for comparison. Model (C) is without attention and sentential embeddings are formed only by max pooling. Model (F) utilizes CNN, while model (H) and (I) integrate attention. As shown in the left figure in Figure 4, (C) gets better or close performance compared to other models on buckets with shorter answers (\u2264 50, \u226455, \u226460). However, as the ground lengths increase, the gap between (C) and other models becomes more obvious. The similar phenomenon is also observed in the right figure for Test2. This suggests the effectiveness of the two extensions from the basic model of QA-LSTM, especially for long-answer questions."}, {"heading": "6 CONCLUSION", "text": "In this paper, we study the answer selection task by employing a bidirection-LSTM based deep learning framework. The proposed framework does not rely on feature engineering, linguistic tools or external resources, and can apply to any domain. We further extended the basic framework on two directions. Firstly, we combine a convolutional neural network into this framework, in order to give more composite representations for questions and ansewrs. Secondly, we integrate a simple but efficient attention mechanism in the generation of answer embeddings according to the question. Experimental results on a public insurance dataset show that both two extensions outperform a strong deep learning baseline, which is built only on CNN for QA embeddings, but without LSTM structure and attention included. In the future, we would like to further evaluate the proposed approaches described in this paper for different tasks, such as answer quality prediction in Community QA and recognizing textual entailment."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "Proceedings of International conference of learning representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Frederic", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Applying deep learning to answer selection: A study and an open task", "author": ["Feng", "Minwei", "Xiang", "Bing", "Glass", "Michael", "Wang", "Lidan", "Zhou", "Bowen"], "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),", "citeRegEx": "Feng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Tree edit models for recognizing textual entailments, paraphrases, and answers to questions", "author": ["Heilman", "Michael", "Smith", "Noah A"], "venue": "Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics (NAACL),", "citeRegEx": "Heilman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "Jurgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Hu", "Baotian", "Lu", "Zhengdong", "Li", "Hang", "Chen", "Qingcai"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A neural attention model for sentence summarization", "author": ["Rush", "Alexander", "Chopra", "Sumit", "Weston", "Jason"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Automatic feature engineering for answer selection and extraction", "author": ["Severyn", "Aliaksei", "Moschitti", "Alessandro"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Severyn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2013}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Tang", "Duyu", "Qin", "Bing", "Liu", "Ting"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["Vinyals", "Oriol", "Le", "Quoc V"], "venue": "Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Wang", "Di", "Nyberg", "Eric"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Probabilistic tree-edit models with structured latent variables for textual entailment and question answering", "author": ["Wang", "Mengqiu", "Manning", "Christopher"], "venue": "The Proceedings of the 23rd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "What is the jeopardy model? a quasisynchronous grammar for qa", "author": ["Wang", "Mengqiu", "Smith", "Noah", "Teruko", "Mitamura"], "venue": "The Proceedings of EMNLP-CoNLL,", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "tagspace: Semantic embeddings from hashtags", "author": ["Weston", "Jason", "Chopra", "Sumit", "Adams", "Keith"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Answer extraction as sequence tagging with tree edit distance", "author": ["Yao", "Xuchen", "Durme", "Benjamin", "Clark", "Peter"], "venue": "Proceedings of NAACL-HLT,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Yih", "Wen-tau", "Chang", "Ming-Wei", "Meek", "Christopher", "Pastusiak", "Andrzej"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguist (ACL),", "citeRegEx": "Yih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2013}, {"title": "Deep learning for answer sentence selection", "author": ["Yu", "Lei", "Hermann", "Karl M", "Blunsom", "Phil", "Pulman", "Stephen"], "venue": "NIPS Deep Learning Workshop,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Recently, deep learning models have obtained a significant success on various natural language processing tasks, such as semantic analysis (Tang et al., 2015), machine translation (Bahdanau et al.", "startOffset": 139, "endOffset": 158}, {"referenceID": 0, "context": ", 2015), machine translation (Bahdanau et al., 2015) and text summarization (Rush et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 9, "context": ", 2015) and text summarization (Rush et al., 2015).", "startOffset": 31, "endOffset": 50}, {"referenceID": 2, "context": "We conduct the experiments based on InsuranceQA (Feng et al., 2015) 1, a newly-released non-factoid QA dataset from the insurance domain.", "startOffset": 48, "endOffset": 67}, {"referenceID": 20, "context": "For example, semantic features were constructed based on WordNet in (Yih et al., 2013).", "startOffset": 68, "endOffset": 86}, {"referenceID": 17, "context": "In (Wang & Manning, 2010; Wang et al., 2007), the answer selection problem is transformed to a syntactical matching between the question/answer parse trees.", "startOffset": 3, "endOffset": 44}, {"referenceID": 19, "context": "Some work tried to fulfill the matching using minimal edit sequences between dependency parse trees (Heilman & Smith, 2010; Yao et al., 2013).", "startOffset": 100, "endOffset": 141}, {"referenceID": 2, "context": "The approaches for non-factoid question answering generally pursue the solution on the following directions: Firstly, the question and answer representations are learned and matched by certain similarity metrics (Feng et al., 2015; Yu et al., 2014).", "startOffset": 212, "endOffset": 248}, {"referenceID": 21, "context": "The approaches for non-factoid question answering generally pursue the solution on the following directions: Firstly, the question and answer representations are learned and matched by certain similarity metrics (Feng et al., 2015; Yu et al., 2014).", "startOffset": 212, "endOffset": 248}, {"referenceID": 0, "context": "Finally, recently proposed models for textual generation can intrinsically be used for answer selection and generation (Bahdanau et al., 2015; Vinyals & Le, 2015).", "startOffset": 119, "endOffset": 162}, {"referenceID": 2, "context": "There are two major differences between our approaches and the work in (Feng et al., 2015): (1) The architectures developed in (Feng et al.", "startOffset": 71, "endOffset": 90}, {"referenceID": 2, "context": ", 2015): (1) The architectures developed in (Feng et al., 2015) are only based on CNN, whereas our models are based on bidirectional LSTMs, which are more capable of exploiting long-range sequential context information.", "startOffset": 44, "endOffset": 63}, {"referenceID": 0, "context": "Finally, recently proposed models for textual generation can intrinsically be used for answer selection and generation (Bahdanau et al., 2015; Vinyals & Le, 2015). The proposed framework belongs to the first category. There are two major differences between our approaches and the work in (Feng et al., 2015): (1) The architectures developed in (Feng et al., 2015) are only based on CNN, whereas our models are based on bidirectional LSTMs, which are more capable of exploiting long-range sequential context information. Moreover, we also integrate the CNN structures on the top of biLSTM for better performance. (2) Feng et al. (2015) tackle the question and answer independently, while one of our proposed structures developed an efficient attentive models to generate answer embeddings according to the question.", "startOffset": 120, "endOffset": 636}, {"referenceID": 3, "context": "Our LSTM implementation is similar to the one in (Graves et al., 2013) with minor modification.", "startOffset": 49, "endOffset": 70}, {"referenceID": 2, "context": "Following the same ranking loss in (Feng et al., 2015; Weston et al., 2014; Hu et al., 2014), we define the training objective as a hinge loss.", "startOffset": 35, "endOffset": 92}, {"referenceID": 18, "context": "Following the same ranking loss in (Feng et al., 2015; Weston et al., 2014; Hu et al., 2014), we define the training objective as a hinge loss.", "startOffset": 35, "endOffset": 92}, {"referenceID": 7, "context": "Following the same ranking loss in (Feng et al., 2015; Weston et al., 2014; Hu et al., 2014), we define the training objective as a hinge loss.", "startOffset": 35, "endOffset": 92}, {"referenceID": 2, "context": "As discussed in (Feng et al., 2015), this is reasonable, because for a shared layer network, the corresponding elements in question and answer vectors represent the same biLSTM outputs.", "startOffset": 16, "endOffset": 35}, {"referenceID": 2, "context": "The structure of CNN in this work is similar to the one in (Feng et al., 2015), as shown in Figure 2.", "startOffset": 59, "endOffset": 78}, {"referenceID": 0, "context": "This strategy has been used in many other natural language processing tasks, such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), sentence summarization (Rush et al.", "startOffset": 105, "endOffset": 152}, {"referenceID": 12, "context": "This strategy has been used in many other natural language processing tasks, such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), sentence summarization (Rush et al.", "startOffset": 105, "endOffset": 152}, {"referenceID": 9, "context": ", 2014), sentence summarization (Rush et al., 2015) and factoid question answering (Hermann et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 5, "context": ", 2015) and factoid question answering (Hermann et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 39, "endOffset": 86}, {"referenceID": 11, "context": ", 2015) and factoid question answering (Hermann et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 39, "endOffset": 86}, {"referenceID": 5, "context": "Inspired by the work in (Hermann et al., 2015), we develop a very simple but efficient attention on the basic model.", "startOffset": 24, "endOffset": 46}, {"referenceID": 5, "context": "The major difference between this approach and the one in (Hermann et al., 2015) is that Hermann et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 5, "context": "The major difference between this approach and the one in (Hermann et al., 2015) is that Hermann et al. (2015)\u2019s attentive reader emphasizes the informative part of supporting facts, and then uses a combined embedding of the query and the supporting facts to predict the factoid answers.", "startOffset": 59, "endOffset": 111}, {"referenceID": 2, "context": "Having described a number of models in the previous section, we evaluate the proposed approaches on the insurance domain dataset, InsuranceQA, provided by Feng et al. (2015). The InsuranceQA dataset provides a training set, a validation set, and two test sets.", "startOffset": 155, "endOffset": 174}, {"referenceID": 2, "context": "Architecture-II in (Feng et al., 2015) 61.", "startOffset": 19, "endOffset": 38}, {"referenceID": 2, "context": "Architecture-II in (Feng et al., 2015): Instead of using LSTM, a CNN model is employed to learn a distributed vector representation of a given question and its answer candidates, and the answers are scored by cosine similarity with the question.", "startOffset": 19, "endOffset": 38}, {"referenceID": 2, "context": "This is the model which achieved the best performance in (Feng et al., 2015).", "startOffset": 57, "endOffset": 76}, {"referenceID": 1, "context": "The models in this work is implemented with Theano (Bastien et al., 2012) from scratch, and all experiments are processed in a GPU cluster.", "startOffset": 51, "endOffset": 73}, {"referenceID": 8, "context": "The word embedding is trained by word2vec (Mikolov et al., 2013), and the word vector size is 100.", "startOffset": 42, "endOffset": 64}, {"referenceID": 2, "context": "Compared to Architecture II in (Feng et al., 2015), which involved a large number of CNN filters, (H) model also has fewer parameters.", "startOffset": 31, "endOffset": 50}, {"referenceID": 2, "context": "Row F shared a highly analogous CNN structure with Architecture II in Feng et al. (2015), except that the later used a shallow hidden layer to transform the word embeddings into the input of CNN structure, while Row F take the output of biLSTM as CNN input.", "startOffset": 70, "endOffset": 89}], "year": 2017, "abstractText": "In this paper, we apply a general deep learning (DL) framework for the answer selection task, which does not depend on manually defined features or linguistic tools. The basic framework is to build the embeddings of questions and answers based on bidirectional long short-term memory (biLSTM) models, and measure their closeness by cosine similarity. We further extend this basic model in two directions. One is to define a more composite representation for questions and answers by combining convolutional neural network with the basic framework, the other is to utilize a simple but efficient attention mechanism in order to generate the answer representation according to the question context. Several variations of models are provided. Experimental results on a public insurance-domain dataset demonstrate that the extended models substantially outperform two state-of-theart non-DL baselines and a strong DL baseline.", "creator": "LaTeX with hyperref package"}}}