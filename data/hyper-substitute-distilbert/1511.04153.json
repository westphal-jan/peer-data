{"id": "1511.04153", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2015", "title": "Adaptive Affinity Matrix for Unsupervised Metric Learning", "abstract": "spectral theory is one out the most robust clustering schemes introducing superior capability at handle all adaptive clustering task. dynamic network clustering methods extend partial nonlinear hierarchy from specific data manifold to a subspace. only a dynamic concentration focuses exploring the resources dependence relationships which can come handled applying just unsupervised relational metric approach. despite practice, the task processing the affinity variable exhibits a tremendous dependency on resource manifold learning. while gradual success of affinity learning has been achieved in recent years, some examples such as noise threshold seemed already completely unknown. in systems perspective, we propose own popular definition, tandem adaptive affinity matrix ( aim ), to specify an adaptive affinity matrix and update a distance diagram computing the affinity. we assume the affinity matrix to be positive semidefinite with ability helps quantify arbitrary pairwise dissimilarity. our method proposes based on posing the domain of generalized function as a partial decomposition problem. we yield the affinity with both 2d primal solution, and subsequently widely - recognised persistence kernel. the provided matrix can be regarded as the optimal representation from spatial relationship on the manifold. extensive experiments on a number of real - world data pages illustrate the effectiveness the efficiency algorithm adaam.", "histories": [["v1", "Fri, 13 Nov 2015 03:59:14 GMT  (467kb,D)", "https://arxiv.org/abs/1511.04153v1", null], ["v2", "Sun, 11 Sep 2016 13:58:06 GMT  (55kb,D)", "http://arxiv.org/abs/1511.04153v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yaoyi li", "junxuan chen", "hongtao lu"], "accepted": false, "id": "1511.04153"}, "pdf": {"name": "1511.04153.pdf", "metadata": {"source": "CRF", "title": "ADAPTIVE AFFINITY MATRIX FOR UNSUPERVISED METRIC LEARNING", "authors": ["Yaoyi Li", "Junxuan Chen", "Yiru Zhao", "Hongtao Lu", "Jiao Tong"], "emails": ["htlu}@sjtu.edu.cn"], "sections": [{"heading": null, "text": "Index Terms\u2014 Affinity Learning, Feature Projection, Dimensionality Reduction, Spectral Clustering"}, {"heading": "1. INTRODUCTION", "text": "Spectral clustering methods which are based on eigendecomposition demonstrate splendid performance on many realworld challenge data sets. During the past decades, a series of spectral clustering methods have been proposed: Multidimensional Scaling (MDS) [1], Local Linear Embedding (LLE) [2], Isomap [3], Laplacian Eigenmaps [4] and variant of Spectral Clustering [5]. There are three shortages of spectral clustering methods mentioned above. First, these approaches only provide the embedding map of the training data. The out-of-sample extension is not straightforward. Second, The complexity of these approaches relies on the number of data points. Third, the performance of spectral clustering methods highly depend on the robustness of the affinity graph.\nMany important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral\n\u2217Corresponding author This paper is supported by NSFC (No. 61272247, 61533012, 61472075), the 863 National High Technology Research and Development Program of China (SS2015AA020501) and the Major Basic Research Program of Shanghai Science and Technology Committee (15JC1400103).\nclustering. Locality Preserving Projections (LPP) proposed in [7] introduces a linear projection obtained from Laplacian Eigenmaps. Their work provides a linear approximation of the embedding mapping, which reduces the time complexity and achieves out-of-sample extension straightforwardly. The linear embedding gives a metric learning perspective of the spectral clustering. Nie, Wang, and Huang proposed the Projected Clustering with Adaptive Neighbors (PCAN) in [14] where they regard the pairwise similarity as an extra variable to be solved in the optimization problem and they set a penalty of the rank of graph Laplacian to restrict specific connected components in the affinity matrix. With this framework, PCAN alternately update affinity matrix and projection. Although some affinity learning algorithms have been proposed in recent years, the technique of choosing an appropriate affinity matrix is still remained to be addressed.\nOur goal is to extract more adaptive similarity information with minimal extra time consumption for the linear approximation of spectral clustering. Such information will take the objective of locality preserving rather than only the distance between images into consideration. Inspired by the recent progress on scalable spectral clustering [10] and data similarity learning [14], we propose a novel approach dubbed Adaptive Affinity Matrix (AdaAM). Our affinity matrix is relatively dense and can capture both global and local information. Specifically, AdaAM decomposes the affinity graph into a product of two low-rank identical matrices. As the ideal case described in [5], if we assume the pairwise affinity in the same class are exceedingly similar, the affinity matrix may turn into a low-rank matrix. We optimize the decomposed matrix with the similar scheme of spectral clustering. The affinity graph obtained by optimization is used as an intermediate affinity matrix, firstly. With the combination of the intermediate affinity matrix and the k-NN affinity graph derived by the heat kernel, we figure out a final adaptive affinity matrix from a naive spectral clustering. We conduct the affinity graph with the data projection and apply LPP to this specific graph to learn a metric for clustering.\nWe illustrate the effective and efficiency of the proposed approach for clustering on image data sets. We show the advantage of AdaAM for challenging data sets by comparing our approach with k nearest neighborhood heat kernel (kNN)\nar X\niv :1\n51 1.\n04 15\n3v 2\n[ cs\n.C V\n] 1\n1 Se\np 20\n16\n[4] and some other state-of-the-art algorithms in Section 3. Our main contribution is that we integrate the affinity matrix learning into the framework of spectral clustering with the same paradigm, and we employ the low rank trick to make our approach more efficient."}, {"heading": "2. ADAPTIVE AFFINITY MATRIX", "text": ""}, {"heading": "2.1. Notation", "text": "In this paper, we write all matrices as uppercase (English or Greek alphabet) and vectors are written as lowercase. The vector with all elements one is denoted by 1. H is the centering matrix denoted by H = I \u2212 1n11\nT. The origin data matrix is denoted by X \u2208 Rn\u00d7d, where n is the number of the data points and d is the dimension of the data. X is assumed to be normalized with zero mean, i.e. X = HX . The denotation xi means the i-th data point vector. We also denote the linear projection by A and denote the metric matrix by M = ATA. Hence the Mahalanobis distance based on is dism(xi, xj) = (xi \u2212 xj)TM(xi \u2212 xj). The k-NN heat kernel matrix is denoted by W \u2208 Rn\u00d7n with\nwij = { exp(\u2212\u2016xi\u2212xj\u20162t ), xi \u2208 Nk(xj) or xj \u2208 Nk(xi) 0, otherwise\n(1) whereNk(x) is the set of k nearest neighbors of x. The corresponding Laplacian matrix is denoted by L = D\u2212W , where D is the diagonal matrix with dii = \u2211 j wij . We also denote both intermediate increment and final adaptive affinity matrix as \u2206, the corresponding diagonal weight matrix and Laplacian matrix as D\u2206 and L\u2206 = D\u2206 \u2212\u2206."}, {"heading": "2.2. Intermediate Affinity Matrix", "text": "We separate our algorithm into two parts, intermediate affinity matrix and final adaptive affinity matrix. In this section, we will introduce the first part. For the i-th data point xi, we connect any the data point xi to the data point xj with the similarity \u03b4ij . With the hope that small Euclidean distance between two data points leads to a large similarity, we aim to choose \u03b4ij to minimize the following objective function\nmin n\u2211 i,j \u2016xi \u2212 xj\u201622 \u03b4ij (2)\nunder appropriate constraints, where \u03b4ij is the ij-th element of the intermediate affinity matrix \u2206.\nDifferent from PCAN [14], we reformulate the equation with graph Laplacian,\nmin tr(XTL\u2206X) (3)\nunder some constraints.\nWith a straightforward thought we can decompose the Laplacian into two identical matrices, since the graph Laplacian is a positive semidefinite matrix in general. We show this thought is not appropriate in our framework as follows.\nIf we assume that\nL\u2206 = UU T (4)\nwhere U \u2208 Rn\u00d7s is a column orthogonal matrix with UTU = I . With the relaxing of the constraints, we finally need to solve the problem\nU = arg min UTU=I\ntr(XTUUTX)\n\u21d2 U = arg min UTU=I\ntr(UTXXTU) (5)\nIf we assume the product of matrix X to be K (i.e. K = XXT ), the Eq. (5) gives a simple form of the Laplacian Eigenmaps\nThis optimization problem can be solved by selecting eigenvectors of matrix K corresponding to several smallest eigenvalues. However, K is a low-rank matrix generally with d n and the eigenvectors of K minimizing the objective function in Eq. (5) is in the null space of X . Hence, the solution of above problem is not unique. Inspired by LSC [10] we assume the affinity matrix to be a positive semidefinite matrix and decompose it into the product of a matrix P \u2208 Rn\u00d7t with orthogonal columns and PT instead of decomposing the Laplacian matrix, where t is the expected rank of \u2206.\nTherefore we reformulate Eq. (3) as\nmin PTP=I\ntr(XTD\u2206X) + tr(X T (\u2212PPT )X) (6)\nwhere we abandon the properties that connected weight is non-negative and the graph Laplacian is positive semidefinite. The negative connected weights in \u2206 can be used to measure the dissimilarity between data points. We will show that the solution of this optimization problem makes D\u2206 equal to 0.\nFor the first part of Eq. (6), we can write it as\nmin n\u2211 i=1 \u2016xi\u201622 d\u2206ii s.t. PTP = I\nd\u2206ii = (PP T 1)i\n(7)\nLet z = (\u2016x1\u201622, \u2016x2\u201622, ..., \u2016xn\u201622)T . With a Lagrange multipliers \u03bb, the one dimensional situation of problem (7) can be rewritten as\nmin zT ppT 1\u2212 \u03bb(pT p\u2212 1) (8)\nFinally, the minimization problem (7) reduces to finding the eigenvector corresponding to the minimum eigenvalue of the problem 1zT p = \u03bbp. Because the matrix 1zT has rank\none, there is only one nonzero eigenvalue \u2211n\ni=1 \u2016xi\u201622, which implies \u03bb = 0. Hence, for the P satisfying problem (7) with arbitrary column number less than n, we have zTPPT 1 = 0. It is equivalent to\nn\u2211 i=1 \u2016xi\u201622 d\u2206ii = 0 (9)\nGenerally, in real-world data set, \u2016xi\u201623 6= 0 always holds, thus, the P minimizing the first part of the objective function (6) has the propertyD\u2206 = 0. Meanwhile the set of all P with the property D\u2206 = 0 is the solution of Eq. (7).\nThe matrix P , which minimizes the second part of the objective function (6), is given by the maximum eigenvalue to the eigen problem:\n(XXT )p = \u03bbp\u21d2 1XXT p = \u03bb1p (10)\nAs the dataX has zero mean, we have \u03bb1p = 1XXT p = 0. Therefore, for the maximum eigenvalue which is larger than 0, the corresponding eigenvector always satisfies 1p = 0. Let the minimum solution of the second part of problem (6) be P = (p1, p2, ..., pt). We have\n1TP = 0\u21d2 1TPPT = 0\u21d2 d\u2206ii = 0 (11)\nwhich means that the property D\u2206 = 0 holds for the optimal solution of the second part of (6) and the solution is in the set of the solution of Eq. (7). Therefore the solution of the second part of Eq. (6) can also optimize the object function (7) and the solution of the optimization problem (6) makes D\u2206 equal to 0. The objective function (6) can be reduced to\nP = arg max PTP=I\ntr(PTXXTP ) (12)\nwhich has the solution as singular value decomposition of X with complexity relies on d rather than n. We obtain the intermediate affinity matrix \u2206 = PPT from the distribution of origin data with similarity and dissimilarity information. The graph Laplacian of \u2206 is L\u2206 = D\u2206 \u2212\u2206 = \u2212\u2206.\nTo mitigate the impact of noise and rank reducing problem, we apply sparsification to \u2206. We will discuss the sparsification further in Section 2.4.\nAlgorithm 1 Adaptive Affinity Matrix Input:\nData points X \u2208 Rn\u00d7d; cluster number c; neighborhood size k; reduced dimension m;\nOutput: Mahalanobis metric M and linear projection A.\n1: Construct the k-NN heat kernelW , the corresponding diagonal weight matrix D and the Laplacian matrix L; 2: Compute the P with orthogonal columns according to Eq. (12) for the intermediate affinity matrix \u2206 = PPT ; 3: Get the linear projection matrix A according to Eq. (13); 4: Produce a new matrix P according to Eq. (16) for the\nfinal adaptive affinity matrix \u2206 = PPT ; 5: Get linear projectionA \u2208 Rm\u00d7d and Mahalanobis metric M = ATA by applying LPP to the affinity matrix \u2206+D;"}, {"heading": "2.3. Final Adaptive Affinity Matrix", "text": "In this section, we formulate a naive linear spectral clustering and provide the final adaptive affinity matrix.\nWith the intermediate affinity matrix \u2206, we can solve the following problem for a linear projection A:\na = arg min aT a=1\ntr(aTXT (L+ L\u2206)Xa) (13)\nwhere a is the one-dimension case of A and L + L\u2206 is the combination of the Laplacian of k-NN heat kernel and the intermediate affinity matrix. The projection vector a is given by the minimum eigenvalue of the eigen problem:\nXT (L\u2212\u2206)Xa = \u03bba (14)\nSubsequently, to compute L\u2206 of Eq. (13) given A, we rewrite the affinity optimization problem with the linear projection matrix A as we did in Eq. (6)\nP = arg min PTP=I\n( c+ tr(ATXTD\u2206XA)\n+ tr(ATXT (\u2212PPT )XA) ) (15)\nwhere we assume the final adaptive affinity matrix to be \u2206 = PPT and c = tr(ATXTLXA). The property D\u2206 = 0 still holds, because of the zero mean of XA. Therefore, Eq. (15) reduces to\nP = arg max PTP=I\ntr(PTXAATXTP ) (16)\nThis can be solved by singular value decomposition of matrix XA and taking the left-singular vectors which correspond to the largest singular values. We apply sparsification on the adaptive affinity matrix \u2206 = PPT obtained from Eq. (16) and attain the sparse affinity matrix.\nIntuitively, we can iterate Eq. (13) and Eq. (16) to minimize the value of objective function. However, as Fig. 1(a)\nshows, the adaptive affinity matrix with only once iteration performs well in practice and the continuing iterations show no remarkable outperformance.\nSince the weight of nodes in the graph plays an important role in some algorithms and methods based on Normalized Cuts [15] like LPP has the constraint relying on D\u2206. In our approach we have D\u2206 = 0, therefore we add the weight matrix D computed from the k-NN heat kernel to our affinity matrix. Finally, we replace the affinity matrix in LPP with the matrix \u2206 + D to get the linear projection A and the metric matrix M = ATA."}, {"heading": "2.4. Sparsification Strategy", "text": "From the optimization problem (12) and (16), we can observe that the matrices XXT and XAATXT are both low-rank matrix. Seeing that the solution of the optimization problem mentioned above is based on the singular value decomposition, this low-rank fact will result in that the column number of the solution P could be far less than the rank of XXT and XAATXT . This process will produce a low-rank affinity matrix which leads to a progressively rank decreasing in our approach. To prevent the rank decreasing happening, we implement sparsification in our approach. The sparsification strategy may mitigate the problem of noise edges as well.\nFig. 1 justifies our sparsification procedure by demonstrating the histogram of the magnitude of the final adaptive affinity matrix obtained from Eq. (16) without sparsification. We can observe that most elements of the affinity matrix concentrate in the range with small magnitude, and the sparsification procedure may reserve a portion of the affinity elements which are more representative.\nInspired by the thought of k-NN heat kernel, we sort all the elements of affinity matrix \u2206 by decreasing magnitude and only reserve the first t elements. We consider that the parameter t is better to be in inverse proportion to the number of clusters, in which case the average elements reserved for each cluster will be proportionate to the number of data points in each cluster. The t is selected by following equation:\nt = bn 2\n\u03b1c c (17)\nwhere b\u00b7c is the floor function, n2 is the number of elements in \u2206, c is the number of clusters and \u03b1 is a coefficient.\nWe set \u03b1 to be 2.5 for the first sparsification in the computation of the intermediate affinity matrix and set \u03b1 to be 5 for the second sparsification in the computation of the final adaptive affinity matrix. The \u03b1 is decided by a rough parameter search, and it gives a stable performance in most data sets.\nWe summarize our algorithm in Algorithm 1. We set reduced dimension m to be the same as the number of classes"}, {"heading": "3. EXPERIMENTS", "text": "In this section, we conduct several experiments to demonstrate the effectiveness and efficiency of the proposed approach AdaAM."}, {"heading": "3.1. Data Sets", "text": "We evaluate the proposed approach on five image data sets: UMIST The UMIST Face Database consists of 575 images of 20 individuals with 220\u00d7220 pixels [16]. We use the images resized to 40\u00d740 pixels in our experiments.\nCOIL20 A data set consists of 1,440 images of 20 objects with discarded background [17].\nUSPS The USPS handwritten digit database has 9,298 images of 10 digits with 16\u00d716 pixels [18].\nMNIST The MNIST database of handwritten digits has 70,000 images of 10 classes [19]. In our experiments, we select the first 10,000 images of this database.\nExYaleB The Extended Yale Face Database B consists of 2,414 cropped images with 38 individuals and around 64 images under different illuminations per individual [20].\nThe statistics of data sets are summarized in Tab. 2."}, {"heading": "3.2. Compared Algorithms", "text": "We compare our approach with the other affinity learning algorithms described in Section Related Work. We adopt LPP\nto the affinity matrices generated by these state-of-the-art approaches to obtain the distance metric.\nCon-kNN Cons-kNN Consensus k-NNs [12] with the aim of selecting robust neighborhoods.\nDN Dominant Neighborhoods proposed in [11]. ClustRF-Bi A spacial case of ClustRF-Strct [13], which is also proposed in [21, 22]. Due to the huge memory requirement of ClustRF-Strct on the data set with thousands instances, we implement this special case in our experiments.\nPCAN Projected Clustering with Adaptive Neighbors proposed in [14]. Because PCAN is an algorithm which can generate the linear projection and clusters simultaneously, we denote the method combining the projection of PCAN and k-Means as PCAN-kMeans and we also show the clustering result of PCAN in Tab. 1 for reference.\nWe also compare our approach with the k-NN heat kernel affinity matrix. We use k-NN to denote this typical approach."}, {"heading": "3.3. Parameter Selection and Experiment Details", "text": "Because there is no validation data set in unsupervised learning tasks, for more general case, we impose the same parameter selection criteria on all the algorithms in our experiments. We set the size of neighborhood to be k = Round(log2(n/c)), where n is the number of data instances and c is the number of classes. We also set the projected dimension, which is equal to the rank of metric matrix, to be the same as the number of classes [5]. All the other parameters in our approach are fixed in every experiment.\nWe denote 10 times of k-Means as a round and select the clustering result with the minimal within-cluster sum as the result of each round of k-Means. We apply 100 rounds\nk-Means to each algorithms for the evaluation of the performance (Tab. 1), 10 rounds k-Means for the experiment of the sensitivity to the neighborhood size k (Fig. 2) and one round k-Means for the experiment of execution time (Fig. 3)."}, {"heading": "3.4. Experiment Results", "text": "In the experiment of clustering accuracy, we evaluate the projection ability of AdaAM with other five algorithms on five benchmark data sets mentioned above. Tab. 1 gives the average and the maximal accuracy of 100 rounds k-Means of each model. From Tab. 1, we can observe that superiority of AdaAM on the task of the unsupervised metric learning. In most case, AdaAM performs much better than the other approaches. Our approach attains four best results of the average accuracy and five best maximal accuracy on five data sets. We can also observe that the proposed AdaAM decisively outperforms other five methods on ExYaleB data set. Different from the other data sets, the image data in ExYaleB are properly aligned and under different illumination. This difference makes some images more similar to the image in different class under the same illumination, which result in a high rank affinity matrix. Our approach is based on a low rank approximation of the optimal affinity matrix with the ability to handle such noises in the affinity matrix.\nSince the neighborhood size k selection criteria is fixed in the experiment of accuracy, which may cause the loss of the best performance, we show the trend of accuracy according to the size of neighborhood in Fig. 2. Fig. 2 shows that AdaAM attains the best result in most cases and the sensitivity to the size of neighborhood is better or comparable to the other models. Since our approach is based on the low rank approximation of the optimal affinity matrix, it requires more information from the pairwise similarity. Hence, for small k, baseline methods are sometimes better than our approach.\nFig. 3 illustrates the efficiency of AdaAM by the semilog graph of execution time with different number of data points selected from MNIST. It can be observed that our ap-\nproach is a inexpensive algorithm in practice with much lower time consumption to PCAN-kMeans, ClustRF-Bi and DN. We also show that AdaAM keeps approximately double time consumption to Cons-kNN with the much better performance."}, {"heading": "4. CONCLUSION", "text": "In this paper, we present a novel affinity learning approach for unsupervised metric learning, called Adaptive Affinity Matrix (AdaAM). In our new affinity learning model, the affinity matrix is learned from the same framework of spectral clustering. More specifically, we show that the affinity learning can be reduced to a singular value decomposition problem. With the affinity matrix learned, the distance metric can be derived by some off-the-shelf approaches based on the affinity graph like LPP. Extensive experiments on clustering image data sets demonstrate the superiority of the proposed method AdaAM."}, {"heading": "5. REFERENCES", "text": "[1] Trevor F Cox and Michael AA Cox, Multidimensional scaling, CRC Press, 2000.\n[2] Sam T Roweis and Lawrence K Saul, \u201cNonlinear dimensionality reduction by locally linear embedding,\u201d Science, vol. 290, no. 5500, pp. 2323\u20132326, 2000.\n[3] Joshua B Tenenbaum, Vin De Silva, and John C Langford, \u201cA global geometric framework for nonlinear dimensionality reduction,\u201d Science, vol. 290, no. 5500, pp. 2319\u20132323, 2000.\n[4] Mikhail Belkin and Partha Niyogi, \u201cLaplacian eigenmaps and spectral techniques for embedding and clustering.,\u201d in NIPS, 2001.\n[5] Andrew Y Ng, Michael I Jordan, Yair Weiss, et al., \u201cOn spectral clustering: Analysis and an algorithm,\u201d in NIPS, 2002.\n[6] Yoshua Bengio, Jean-Franc\u0327ois Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Le Roux, and Marie Ouimet, \u201cOut-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering,\u201d in NIPS, 2004.\n[7] Xiaofei He and Partha Niyogi, \u201cLocality preserving projections,\u201d in NIPS, 2004.\n[8] Charless Fowlkes, Serge Belongie, Fan Chung, and Jitendra Malik, \u201cSpectral grouping using the nystrom method,\u201d IEEE TPAMI, vol. 26, no. 2, pp. 214\u2013225, 2004.\n[9] Donghui Yan, Ling Huang, and Michael I Jordan, \u201cFast approximate spectral clustering,\u201d in ACM SIGKDD, 2009.\n[10] Xinlei Chen and Deng Cai, \u201cLarge scale spectral clustering with landmark-based representation.,\u201d in AAAI, 2011.\n[11] Massimiliano Pavan and Marcello Pelillo, \u201cDominant sets and pairwise clustering,\u201d IEEE TPAMI, vol. 29, no. 1, pp. 167\u2013172, 2007.\n[12] Vittal Premachandran and Ramakrishna Kakarala, \u201cConsensus of k-nns for robust neighborhood selection on graph-based manifolds,\u201d in CVPR, 2013.\n[13] Xiatian Zhu, Chen Change Loy, and Shaogang Gong, \u201cConstructing robust affinity graphs for spectral clustering,\u201d in CVPR, 2014.\n[14] Feiping Nie, Xiaoqian Wang, and Heng Huang, \u201cClustering and projected clustering with adaptive neighbors,\u201d in ACM SIGKDD, 2014.\n[15] Jianbo Shi and Jitendra Malik, \u201cNormalized cuts and image segmentation,\u201d IEEE TPAMI, vol. 22, no. 8, pp. 888\u2013905, 2000.\n[16] Daniel B Graham and Nigel M Allinson, \u201cCharacterising virtual eigensignatures for general purpose face recognition,\u201d in Face Recognition, pp. 446\u2013456. Springer, 1998.\n[17] Sameer A Nene, Shree K Nayar, Hiroshi Murase, et al., \u201cColumbia object image library (coil-20),\u201d Tech. Rep., Technical Report CUCS-005-96, 1996.\n[18] Jonathan J Hull, \u201cA database for handwritten text recognition research,\u201d IEEE TPAMI, vol. 16, no. 5, pp. 550\u2013 554, 1994.\n[19] Yann LeCun, Le\u0301on Bottou, Yoshua Bengio, and Patrick Haffner, \u201cGradient-based learning applied to document recognition,\u201d Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.\n[20] K.C. Lee, J. Ho, and D. Kriegman, \u201cAcquiring linear subspaces for face recognition under variable lighting,\u201d IEEE TPAMI, vol. 27, no. 5, pp. 684\u2013698, 2005.\n[21] Antonio Criminisi, Jamie Shotton, and Ender Konukoglu, Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning, Now Publishers Inc., 2012.\n[22] Yuru Pei, Tae-Kyun Kim, and Hongbin Zha, \u201cUnsupervised random forest manifold alignment for lipreading,\u201d in ICCV, 2013."}], "references": [{"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T Roweis", "Lawrence K Saul"], "venue": "Science, vol. 290, no. 5500, pp. 2323\u20132326, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B Tenenbaum", "Vin De Silva", "John C Langford"], "venue": "Science, vol. 290, no. 5500, pp. 2319\u20132323, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "NIPS, 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y Ng", "Michael I Jordan", "Yair Weiss"], "venue": "NIPS, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering", "author": ["Yoshua Bengio", "Jean-Fran\u00e7ois Paiement", "Pascal Vincent", "Olivier Delalleau", "Nicolas Le Roux", "Marie Ouimet"], "venue": "NIPS, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Locality preserving projections", "author": ["Xiaofei He", "Partha Niyogi"], "venue": "NIPS, 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Spectral grouping using the nystrom method", "author": ["Charless Fowlkes", "Serge Belongie", "Fan Chung", "Jitendra Malik"], "venue": "IEEE TPAMI, vol. 26, no. 2, pp. 214\u2013225, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast approximate spectral clustering", "author": ["Donghui Yan", "Ling Huang", "Michael I Jordan"], "venue": "ACM SIGKDD, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Large scale spectral clustering with landmark-based representation", "author": ["Xinlei Chen", "Deng Cai"], "venue": "AAAI, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Dominant sets and pairwise clustering", "author": ["Massimiliano Pavan", "Marcello Pelillo"], "venue": "IEEE TPAMI, vol. 29, no. 1, pp. 167\u2013172, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Consensus of k-nns for robust neighborhood selection on graph-based manifolds", "author": ["Vittal Premachandran", "Ramakrishna Kakarala"], "venue": "CVPR, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Constructing robust affinity graphs for spectral clustering", "author": ["Xiatian Zhu", "Chen Change Loy", "Shaogang Gong"], "venue": "CVPR, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustering and projected clustering with adaptive neighbors", "author": ["Feiping Nie", "Xiaoqian Wang", "Heng Huang"], "venue": "ACM SIGKDD, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "IEEE TPAMI, vol. 22, no. 8, pp. 888\u2013905, 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Characterising virtual eigensignatures for general purpose face recognition", "author": ["Daniel B Graham", "Nigel M Allinson"], "venue": "Face Recognition, pp. 446\u2013456. Springer, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Columbia object image library (coil-20)", "author": ["Sameer A Nene", "Shree K Nayar", "Hiroshi Murase"], "venue": "Tech. Rep., Technical Report CUCS-005-96, 1996.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "A database for handwritten text recognition research", "author": ["Jonathan J Hull"], "venue": "IEEE TPAMI, vol. 16, no. 5, pp. 550\u2013 554, 1994.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K.C. Lee", "J. Ho", "D. Kriegman"], "venue": "IEEE TPAMI, vol. 27, no. 5, pp. 684\u2013698, 2005.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning", "author": ["Antonio Criminisi", "Jamie Shotton", "Ender Konukoglu"], "venue": "Now Publishers Inc.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Unsupervised random forest manifold alignment for lipreading", "author": ["Yuru Pei", "Tae-Kyun Kim", "Hongbin Zha"], "venue": "ICCV, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "During the past decades, a series of spectral clustering methods have been proposed: Multidimensional Scaling (MDS) [1], Local Linear Embedding (LLE) [2], Isomap [3], Laplacian Eigenmaps [4] and variant of Spectral Clustering [5].", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "During the past decades, a series of spectral clustering methods have been proposed: Multidimensional Scaling (MDS) [1], Local Linear Embedding (LLE) [2], Isomap [3], Laplacian Eigenmaps [4] and variant of Spectral Clustering [5].", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "During the past decades, a series of spectral clustering methods have been proposed: Multidimensional Scaling (MDS) [1], Local Linear Embedding (LLE) [2], Isomap [3], Laplacian Eigenmaps [4] and variant of Spectral Clustering [5].", "startOffset": 187, "endOffset": 190}, {"referenceID": 3, "context": "During the past decades, a series of spectral clustering methods have been proposed: Multidimensional Scaling (MDS) [1], Local Linear Embedding (LLE) [2], Isomap [3], Laplacian Eigenmaps [4] and variant of Spectral Clustering [5].", "startOffset": 226, "endOffset": 229}, {"referenceID": 4, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 5, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 6, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 7, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 8, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 9, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 10, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 11, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 12, "context": "Many important progresses [6, 7, 8, 9, 10, 11, 12, 13, 14] have been made to mitigate the above issues of the spectral \u2217Corresponding author This paper is supported by NSFC (No.", "startOffset": 26, "endOffset": 58}, {"referenceID": 5, "context": "Locality Preserving Projections (LPP) proposed in [7] introduces a linear projection obtained from Laplacian Eigenmaps.", "startOffset": 50, "endOffset": 53}, {"referenceID": 12, "context": "Nie, Wang, and Huang proposed the Projected Clustering with Adaptive Neighbors (PCAN) in [14] where they regard the pairwise similarity as an extra variable to be solved in the optimization problem and they set a penalty of the rank of graph Laplacian to restrict specific connected components in the affinity matrix.", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "Inspired by the recent progress on scalable spectral clustering [10] and data similarity learning [14], we propose a novel approach dubbed Adaptive Affinity Matrix (AdaAM).", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "Inspired by the recent progress on scalable spectral clustering [10] and data similarity learning [14], we propose a novel approach dubbed Adaptive Affinity Matrix (AdaAM).", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "As the ideal case described in [5], if we assume the pairwise affinity in the same class are exceedingly similar, the affinity matrix may turn into a low-rank matrix.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "[4] and some other state-of-the-art algorithms in Section 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Different from PCAN [14], we reformulate the equation with graph Laplacian,", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "Inspired by LSC [10] we assume the affinity matrix to be a positive semidefinite matrix and decompose it into the product of a matrix P \u2208 Rn\u00d7t with orthogonal columns and P instead of decomposing the Laplacian matrix, where t is the expected rank of \u2206.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Since the weight of nodes in the graph plays an important role in some algorithms and methods based on Normalized Cuts [15] like LPP has the constraint relying on D\u2206.", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "We evaluate the proposed approach on five image data sets: UMIST The UMIST Face Database consists of 575 images of 20 individuals with 220\u00d7220 pixels [16].", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "COIL20 A data set consists of 1,440 images of 20 objects with discarded background [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "USPS The USPS handwritten digit database has 9,298 images of 10 digits with 16\u00d716 pixels [18].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "MNIST The MNIST database of handwritten digits has 70,000 images of 10 classes [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "ExYaleB The Extended Yale Face Database B consists of 2,414 cropped images with 38 individuals and around 64 images under different illuminations per individual [20].", "startOffset": 161, "endOffset": 165}, {"referenceID": 10, "context": "Con-kNN Cons-kNN Consensus k-NNs [12] with the aim of selecting robust neighborhoods.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "DN Dominant Neighborhoods proposed in [11].", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "ClustRF-Bi A spacial case of ClustRF-Strct [13], which is also proposed in [21, 22].", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "ClustRF-Bi A spacial case of ClustRF-Strct [13], which is also proposed in [21, 22].", "startOffset": 75, "endOffset": 83}, {"referenceID": 20, "context": "ClustRF-Bi A spacial case of ClustRF-Strct [13], which is also proposed in [21, 22].", "startOffset": 75, "endOffset": 83}, {"referenceID": 12, "context": "PCAN Projected Clustering with Adaptive Neighbors proposed in [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "We also set the projected dimension, which is equal to the rank of metric matrix, to be the same as the number of classes [5].", "startOffset": 122, "endOffset": 125}], "year": 2016, "abstractText": "Spectral clustering is one of the most popular clustering approaches with the capability to handle some challenging clustering problems. Only a little work of spectral clustering focuses on the explicit linear map which can be viewed as the distance metric learning. In practice, the selection of the affinity matrix exhibits a tremendous impact on the unsupervised learning. In this paper, we propose a novel method, dubbed Adaptive Affinity Matrix (AdaAM), to learn an adaptive affinity matrix and derive a distance metric. We assume the affinity matrix to be positive semidefinite with ability to quantify the pairwise dissimilarity. Our method is based on posing the optimization of objective function as a spectral decomposition problem. The provided matrix can be regarded as the optimal representation of pairwise relationship on the manifold. Extensive experiments on a number of image data sets show the effectiveness and efficiency of AdaAM.", "creator": "LaTeX with hyperref package"}}}