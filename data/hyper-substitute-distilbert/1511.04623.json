{"id": "1511.04623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2015", "title": "Learning to Represent Words in Context with Multilingual Supervision", "abstract": "we present a neural network task based on combining lstms algorithms compute differently reflecting words in the latter contexts. enabling evidence - free speech maps becomes suitable for, e. g., investigating different word variants on other context - modulated variations under meaning. who learn above adequately defined corpus model, we use spatial - lingual supervision, hypothesizing measuring a good representation of a word in symbols will assure one he is sufficient for explaining culturally correct translation into his second type. algorithms extend the quality of word representations as helps in recognizing organizational features : prediction of semantic relations ( which assign nouns and stems into a strictly special semantic classes ), eliminate resource gap translation, producing a fluent substitution task, and provide state - not - the - art results on all of documents.", "histories": [["v1", "Sat, 14 Nov 2015 21:36:38 GMT  (109kb)", "https://arxiv.org/abs/1511.04623v1", null], ["v2", "Thu, 19 Nov 2015 23:35:42 GMT  (190kb)", "http://arxiv.org/abs/1511.04623v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kazuya kawakami", "chris dyer"], "accepted": false, "id": "1511.04623"}, "pdf": {"name": "1511.04623.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["cdyer}@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n04 62\n3v 2\n[ cs\n.C L\n] 1\n9 N\nov 2\nWe present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these."}, {"heading": "1 INTRODUCTION", "text": "Distributed representations of words, which represent each word as a vector in a low-dimensional space, can be learned from unannotated text corpora using a variety of techniques (Mikolov et al., 2013; Pennington et al., 2014; Landauer & Dumais, 1997). The value of such representations owes to their ability to capture intuitive notions of syntactic and semantic similarity as geometric locality. Despite their empirically proven value as a source of features in many downstream applications (Turian et al., 2010), the \u201cone word type, one vector\u201d assumption made by most word representation models is problematic because words may have multiple meanings.\nTwo standard solutions to this problem exist. The first to treat each word as a collection of discrete, mutually exclusive senses which are individually represented as vectors (Tian et al., 2014; Neelakantan et al., 2014; Wu & Giles, 2015; Huang et al., 2012; Jauhar et al., 2015). However, identifying the appropriate sense granularity in such models is difficult in practice and in theory (Kilgarriff, 1997; Erk et al., 2013). The second solution, which is the basis of this work, eschews sense inventories (whether latent or explicit) and says that lexical meaning is a function of word and its context (Erk & Pado\u0301, 2008; Kintsch, 2001; Mitchell & Lapata, 2008). While previous work has hinted at the promise of this solution, only a small number of hand-crafted word\u2013context composition functions have been considered thus far in the literature on semantic representation learning. This is surprising given the success of learning composition functions for computing phrase and sentence representations (Socher et al., 2011; Kalchbrenner et al., 2014).\nThere are two central challenges faced by learning to represent words in context. The first is to identifying a suitable function class for the composition function. Such a function must be able to account for the fact that a single word type may have both several completely unreleated meanings as well as a several more or less distinct but still related meanings (Cruse, 2000). For an example of the former, the word plant may refer, depending on context, to a factory or to a living organism that photosynthesizes. For an example of the latter, the word bank may refer to a financial institution or the building housing a financial institution. Since bidirectional RNN-LSTMs have been shown to be able to learn both compositional (Bahdanau et al., 2014) as well as more arbitrary relationships (Ling et al., 2015), we use these as our composition function class (\u00a72).\nThe second challenge is to identify an appropriate supervisory signal that will be used to fit the parameters of the function. Our motivating hypothesis\u2014which follows a long line of work in using parallel data as a source of information about semantics (Bannard & Callison-Burch, 2005; Resnik & Yarowsky, 1999; Diab, 2003; Faruqui & Dyer, 2014; Hermann & Blunsom, 2014)\u2014is that a good representation of a word in context will be one that predicts how that word (in its sentential context) translates into a second language (\u00a73). We show that word-in-context representations can be learned efficiently from pairs of words-in-context and single word translations into a second language which are extracted from parallel corpora using a word alignment model.\nTo evaluate our proposed model and training criterion, we evaluate our learned representations as features in three tasks: supersense tagging, low-resource machine translation (i.e., translation where limited parallel data is available), and a lexical substitution task. Success in each of these requires models that can effectively capturing the meaning of a word in context, and in each, we show our model obtains state-of-the-art performance (\u00a74). Additionally, the feedforward neural net model we use as a baseline for supersense tagging outperforms existing baselines even without our new word-in-context model."}, {"heading": "2 MODEL", "text": "Our model for contextual words is a bidirectional sequence model based on recurrent neural networks (Chan et al., 2015; Bahdanau et al., 2014; 2015, inter alia). Intuitively, this model allows us to condition on arbitrarily long dependencies while having an implicit bias toward more local contexts.\nLet w = (w1, w2, . . . , wn) be the words in a sentence with length n. We also project all words into a fixed d-dimensional vectors x = (x1,x2, . . . ,xn), using a (one-word-per-type) word lookup table.\nThe model encodes each token of the sentence from left to right according to the standard Long-short term memory recurrences:\nit = \u03c3(Wxixt +Whiht\u22121 +Wcict\u22121 + bi)\nft = \u03c3(Wxfxt +Whfht\u22121 +Wcfct\u22121 + bf )\nct = ft \u2299 ct\u22121 + it \u2299 tanh(Wxcxt +Whcht\u22121 + bc)\not = \u03c3(Wxoxt +Whoht\u22121 +Wcoct + bo)\nht = ot \u2299 tanh(ct)\nThis yields a representation \u2212\u2192 ht for each position in the sentence t which can be interpreted as the representation of word with its left context w1, w2, . . . , wt. The same process is repeated from right to left, yielding a vector \u2190\u2212 ht. The concatenation of these two vectors\nht = [ \u2212\u2192 ht; \u2190\u2212 ht],\nis our word-in-context representation."}, {"heading": "2.1 MODEL INTUITION", "text": "Type-level word embeddings must necessarily represent information about multiple senses in a single vector, and our task is to obtain. To obtain a representation of a word in its context, we want to apply functions which mask or scale some dimensions of the vector according to its context. Thus, functions which apply same scaling function even the word and context are different, average and multi-layer perceptron for example, may not be suitable. The input gate in Long-short term memory is considered to be a suitable scaling function which take target word and its context (xt,ht, ct\u22121). Figure 1 show a simplified version of operation to modulate one sense (vegetable plant) from ambiguous type level vector with semantic mask which is conditioned on word and context."}, {"heading": "3 MEANING AND TRANSLATION", "text": "We now require a training objective that provides supervision for learning the parameters of this model. The question we want to answer is: what is a suitable proxy (or \u201cgrounding\u201d) for the mean-\nings of words in context that we can use to construct token-level (rather than type-level) word representations?\nTo illustrate the problem we wish to solve, consider the meaning of the token bank in the following sentences:\n\u2022 I went to the bank to deposit my paycheck.\n\u2022 I went to the river bank to eat some lunch.\nOne very productive strategy for learning semantic word embeddings is to rely on the distributional hypothesis (Harris, 1954), according to which semantically similar items occur in similar contexts. The distributional hypothesis is, furthermore, practically appealing since it enables semantics to be learned from large, unannotated text corpora.\nDespite the empirical success of the distributional hypothesis at obtaining representations of word types, creating a representation of word tokens in terms of context is conceptually unappealing since both the item being embedding and its context potentially share material. One possible solution would be an autoencoding objective, or one might also distinguish between \u201cnarrow\u201d and \u201cwide\u201d context (i.e., one that determines the item being embedding and one that provides supervision).1\nHowever, we instead advocate using an alternative proxy for meaning: how words translate. Consider the two examples from above as they might be translated into French.\n\u2022 Je suis alle\u0301 a\u0300 la banque pour de\u0301poser mon che\u0300que de paie.\n\u2022 Je suis alle\u0301 sur la rive pour le de\u0301jeuner.\nThe homonymous (i.e., having two completely unrelated senses) word bank has been translated into two different words banque and rive in French.\nFinally, while not quite so copious as monolingual corpora, parallel data exist in convenient electronic form in abundance, and this provides a rich resource for learning about the semantics of natural language."}, {"heading": "3.1 OBJECTIVE & PARAMETER LEARNING", "text": "To operationalize our hypothesis that translation provides a good supervisory signal for learning semantic representations, we learn the parameters of source language word type embeddings and the composition function (i.e., the parameters of the bidirectional LSTMs) by using the computed representation to compute the lexical translation probability of a word in context. That is, we use the computed token embedding to define a probability estimate that a source language word et in context c = (e1, . . . , et\u22121, et+1 . . . , en) translates into a second language as f in vocabularyF . i.e., p(f | et, c).\n1For example, Mikolv et al. (2013) showed that short multiword expressions could be embedding by using the \u201cwider\u201d context that they occur in.\nThis is done by performing a softmax over the target vocabulary with the representation of the word ht, as defined in the previous section. That is, we compute\nu = Rht + b \u2032\np(f | et, c) = exp(uf )\u2211\nf \u2032\u2208F exp(uf \u2032) ,\nwhere parameters R and b\u2032 define the projection of the source word with context representation ht onto the target vocabulary F .\nTo obtain pairs of words in context and their lexical translations into a second language, we use unsupervised word alignment techniques (Dyer et al., 2013), to obtain high precision word alignments from a parallel corpus. While modeling alignments as latent variables, or using a soft attention mechanism would be a reasonable alternative, word alignment is fast and the proposed training objective to be easily scaled to large corpora.\nFigure 2 illustrates the pre-training architecture.\nLexical Translation ( FR )"}, {"heading": "3.2 PARAMETER LEARNING", "text": "The model parameters W and b as well as the word projection parameters Ve are first pre-trained with the objective function:\nL = \u2212 \u2211\n(f ,e)\nlog p(f | e, c)\nThat is, we wish to find the parameters that maximize the lexical translation log probability over the whole parallel corpus of lexical translations (f ) of a source word (e) in context (c).\nWhen we want to transfer the model to another supervised task to predict label s \u2208 S for a word e in context c, the final values of the W and b parameters are transferred and formulate a similar model to predict label s. Using the transformation matrix S \u2208 R|S|\u00d7dh and the biases b\u2032\u2032 \u2208 R|S|, we may define the label probability as\nu \u2032 = Sht + b \u2032\u2032\np(s | et, c) = exp(u\u2032s)\u2211\ns\u2032\u2208S exp(u \u2032 s\u2032)\n.\nthe model is training by maximizing the log likelihood of the observed label in the task.\nL\u2032 = \u2212 \u2211\n(s,e)\nlog p(s | e, c) (1)"}, {"heading": "4 EXPERIMENTS", "text": "We now turn to a series of experiments to show the value of learning representations of words in context according to the objective above. Our paradigm will be to pre-train using the objective above the parameters of a word-in-context model, and then use these (without further fine tuning) in downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task."}, {"heading": "4.1 MODEL CONFIGURATION AND PRE-TRAINING", "text": "To pre-train our model, we extracted words (e) in contexts and translations (f ) from the Europerl parallel corpus (Koehn, 2005). We conducted experiments with the following four languages: French (FR), German (DE), Czech (CS), Finnish (FI) which are quite typologically diverse. Table1 shows the numbers of parallel sentences and the numbers of words. For each language pairs, we used 2000 sentences for development and the rest were used for training.\nAfter normal tokenization, we obtained alignments with fast-align tool (Dyer et al., 2013). Since we are modeling single word translations and want high-quality training instances, we run the alignment model in both directions and obtained symmetric alignments by taking intersection between forwards and backward alignments. To control the size of vocabulary, we took 30,000 most common words. For target languages, we removed 10 most common words. The words not in the vocabularies are replaced with \u3008unk\u3009 token. We used sentences which have more than 10 words in a sentence.\nWe used 300 dimension embeddings for source language, and bi-directional LSTMs have 300 hidden units. The trained parameters are source embedding, weights and bias in the model.\nWe randomly initialized source word embeddings sampled from uniform distribution from \u22120.08 to 0.08. All recurrent materices with orthogonal initialization (Saxe et al., 2013), and non-recurrent weights are initialized from scaled uniform distribution (Glorot & Bengio, 2010). Mini-batches of size 128 are used. We used Adam algorithm for optimization (Kingma & Ba, 2014). We trained models with early-stopping. The perplexities on development data for English to French, German, Czech and Finnish are 3.80, 6.49, 6.30, 19.25 respectively."}, {"heading": "4.2 SUPERSENSE TAGGING", "text": "Supersenses can be thought of a generalization of words senses into a universal inventory of semantic types. That is, as the number of word senses tend to be too numerous for existing models to generalize properly with the small amounts of data available, supersenses address this problem by clustering all senses into a tractable set of tags. Table 4.2 show examples of supersense tags and its definition. As such, these are generally used in semantically oriented downstream tasks such as co-reference resolution (O\u2019Connor & Heilman, 2013) and question answering (Pasca & Harabagiu, 2001).\nFollowing previous work, we trained our supersense tagger for nouns and verbs on the Semcor dataset. The Semcor datasets consists of three parts, brown1, brown2, and brownv. We mixed these three parts and trained supersense tagger on randomly split 4/5 of data and the rest were used as a development set. We evaluated our model on the held-out SensEval-3 all-words task (Mihalcea et al.,\n2004), as done in previous work on supersense tagging (Ciaramita & Altun, 2006; Yuret & Yatbaz, 2010). Since some tokens are annotated with two labels in ambiguous cases, we followed the heuristics of only using the first sense in the data as the correct synset/supersense (Ciaramita & Altun, 2006). To extract supersenses from the Semcor data, we used WordNet version 2.0 synsets.\nTo avoid the computational overhead of reading extremely wide contexts, we used sliding window to delimit the range of contexts as in (Collobert et al., 2011), that is, each token wt is embedded using a context window of words wt\u2212n/2, . . . , wt, . . . , wt+n/2. The window size n was fixed to 20.\nWe use the pre-trained parameters and we put a new task specific softmax leyer on top of the hidden units (Fig.2). We updated all parameters including pre-trained parameters. The weights in the softmax layer were initialized from the scaled uniform distribution (Glorot & Bengio, 2010). Minibatches of size 128 were used with the Adam update rule (Kingma & Ba, 2014).\nSince this task has not previously been studied using neural networks, we also report several novel baselines: (1) multi-layer perceptron model which uses a concatenation of a source word type vector and the average of all word type vectors in its context; (2) a forward-only LSTM model; and (3) a bi-directional LSTM with random initialization (rather than cross-lingual pretraining). For fair comparison in terms of the size of word in context representation, we double the hidden unit size of the forward LSTM model."}, {"heading": "4.3 LEXICAL TRANSLATION IN LOW RESOURCE LANGUAGE", "text": "We investigate the benefit to transfer cross lingually pre-trained word-in-context representation to translation in low-resource language. Since low-resource languages do not have enough data to adequate estimate translation probabilities, we hope that we can learn more effective mappings with pre-trained word-in-context embeddings (Chahuneau et al., 2013).\nWe trained lexical translation model, which predict translation of aligned English sentence, for low resource languages, Malagasy and Urdu on top of the pre-trained word in context model. Table 1 shows the numbers of parallel sentences and the number of words. We used a dataset used in (Dou et al., 2014) for Malagasy and the Urdu data we used is a part of NIST MT evaluation in 2008-20122. We used 2000 sentences for development and hold-out test set. We filtered out sentences which have less than 3 words for pre-training and words occur less then 1 time are replaced with \u3008unk\u3009 token.\nWe trained our baseline system with cdec (Dyer et al., 2010) and obtained synchronous context-free grammars rules to translate sentences. We added features, translation probability and log translation probability from our translation model and optimized the parameters of a machine translation system with MIRA, Margin-Infused Relaxed Algorithm (Crammer & Singer, 2003)."}, {"heading": "4.4 LEXICAL SUBSTITUTION", "text": "Lexical substitution is the problem of identifying meaning-preserving substitutes for a target word given a sentential context. The task was introduced in SemEval-2007 (McCarthy & Navigli, 2007) involves both finding the synonyms and disambiguating the context. As such, it is an ideal test case for our representations.\nModels are evaluated on their ability to predict the substitutes in the gold standard of the LS-SE test-set. We evaluated our model on best and best-mode task which evaluate the quality of the best\n2https://catalog.ldc.upenn.edu/LDC2010T21\npredictions. The original task allow to make multiple predictions but we only predict only one substitution following (Melamud et al., 2015). This task is challenging, since it requires to find the best substitutes from entire word vocabulary.\nThe way to make prediction is the following. Given a target word and it\u2019s context, we infer word in context representation of all possible substitutions. Then take one of the most similar words which have highest cosine similarity with target word in context vector as prediction.\nFor our experiments, we used a simple word alignment base candidate generation to reduce inference time. For a target word in English, we collect all possible French translations from word alignment and took English words 90% most frequently aligned to the French words as candidates. We used same candidates for all our experiments including baseline for fair comparison."}, {"heading": "5 RESULT", "text": ""}, {"heading": "5.1 SUPERSENSE TAGGING", "text": "Table 3 shows frequency weighted Precision, Recall F1 score3 on Semcor test set and Senseval3 all-words task. Our bidirectional LSTM model (bi-LSTM) outperformed the first sense heuristic baseline, the perceptron trained Hidden Markov Model proposed in (Ciaramita & Altun, 2006). And our new word-in-context pre-training model result in further improvements with all language pairs. The averaged score of 4 cross lingually pre-trained models, as in bi-LSTM (average), shows significant improvements over bi-LSTM. The model pre-trained with German achieved best result F1 84.1 on senseval3. Additionally, the baseline neural network models outperforms existing baselines even without cross lingual supervision.\n3it can result in an F-score that is not between precision and recall."}, {"heading": "5.2 LEXICAL TRANSLATION IN LOW RESOURCE LANGUAGE", "text": "Table 4 shows results on machine translation in low resource language. We report the averaged BLEU score of 5 runs to avoid optimizer randomness Clark et al. (2011). The result show large improvement on perplexity and consistent improvement on BLEU in all language pairs. The average score of 4 cross lingually trained model improved perplexity by around 3 points and BLEU score by 0.3."}, {"heading": "5.3 LEXICAL SUBSTITUTION", "text": "Table 5 shows results on lexical substitution task. Since our word-in-context representations are build only on Europerl parallel corpora, the baseline system is Skipgram word embedding trained on English side of EN-FR parallel corpora, which is the largest in the corpus. The Skipgram model which take most similar word as prediction is context in-sensitive baseline. Also we compared our results with various context sensitive models, which take arithmetic mean (as in Add and BalAdd) and a geometrical mean (as in Mult and BalMult) of embeddings, proposed by (Melamud et al., 2015). They trained their baseline embeddings (as in Base) on a two billion word web corpus, ukWaC (Ferraresi et al., 2008).\nThe model achieved best measures4 10.63, best mode measure 18.90 with German supervision. And the second best result was obtained with Czech. As for comparison with Melamud et al. (2015), we cannot compare score directly since we used different corpus and candidate generation. We should compare performance gain by taking into account context. Their best model (BalAdd) achieved 0.33 performance gain with context where our model achieved 2.9 performance gain on best evaluation."}, {"heading": "6 DISCUSSION", "text": "We proposed the model to predict lexical translation to build word-in-context representation. Table. 6 shows example of disambiguation with translation model in order of translation pribability. The model correctly disambiguate industrial plant (usine in French), and vegetable plant (plantes in French). Figure 3 shows the effect of pre-trained word-in-context representation for downstream tasks. Pre-trained model start from low perplexities at the first update and converged earlier, in two epochs, for low resource machine translation.\n4evaluation was done by a script provided by the task organizer.\nWe investigated the effect of 4 linguistically diverse language. The results shows the benefit of cross-lingual pre-training in all languages, but overall the model trained with German have stable results and the model trained with Finnish tend to underperform others, especially on lexical substitution task where we do not have supervised fine-tuning process. This is probably because the large vocabulary of Finnish which is two times bigger than German."}, {"heading": "7 RELATED WORK", "text": "Word representation. Distributed word representations were successfully applied to several downstream tasks such as chunking, parsing, sentiment analysis and paraphrase detection. Most of the tasks requires to use not only word representation but representation of phrases or documents. In the previous works, many architectures were proposed to learn and use word representation. In the sequence modeling problems such as BIO chunking, conditional random fields and recurrent neural networks are applied to represent a sequence of word representations (Turian et al., 2010; Mesnil et al., 2013). For classification tasks such as document classification, sentiment analysis, paraphrase detection, summation of word embeddings (Lauly et al., 2014), convolutional neural networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al., 2013; Cheng & Kartsaklis, 2015) were proposed to represent compositionality function of words.\nLearning semantics from parallel data. Previous works show methods to improve word or document level representation by incorporating multilingual context. Faruqui & Dyer (2014) proposed canonical correlation analysis (CCA) based method to improve the quality of type level representation by projecting word representations of translation pairs (obtained by automatic word alignments) to be maximally correlated in common vector space. Hermann & Blunsom (2013) propose compositional vector space model (CVM) to build sentence representation. They represent a sentence as the sum of its word representations and they train word representation by constraining the representations of parallel sentences to be close. Coulmance et al. (2015) shows that predicting context in target language is an effective way to train word representation shared across languages. Hill et al. (2014) investigated the quality of word embedding learned by neural machine translation model and show its benefit on tasks that require modeling word similarity.\nCompositional vector models. Most prior work on compositional vector models has looked primarily at the problem of computing representations of complete phrases rather than specifically words in context. Furthermore, one can learn reasonable generalizations from models that condition on and the generate text using an autoencoding objective (Socher et al., 2011). Dhillon et al. (2012) make the intriguing proposition that left- and right- contexts can be used to supervise each other."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Phil\u00e9mon", "Bengio", "Yoshua"], "venue": "CoRR, abs/1508.04395,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Translating into morphologically rich languages with synthetic phrases", "author": ["Chahuneau", "Victor", "Schlinger", "Eva", "Smith", "Noah A", "Dyer", "Chris"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chahuneau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chahuneau et al\\.", "year": 2013}, {"title": "Listen, attend, and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "CoRR, abs/1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Syntax-aware multi-sense word embeddings for deep compositional models of meaning", "author": ["Cheng", "Jianpeng", "Kartsaklis", "Dimitri"], "venue": "arXiv preprint arXiv:1508.02354,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger", "author": ["Ciaramita", "Massimiliano", "Altun", "Yasemin"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ciaramita et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ciaramita et al\\.", "year": 2006}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Clark", "Jonathan H", "Dyer", "Chris", "Lavie", "Alon", "Smith", "Noah A"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume", "citeRegEx": "Clark et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Trans-gram, fast cross-lingual word-embeddings", "author": ["Coulmance", "Jocelyn", "Marty", "Jean-Marc", "Wenzek", "Guillaume", "Benhalloum", "Amine"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Coulmance et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Coulmance et al\\.", "year": 2015}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Crammer", "Koby", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2003}, {"title": "Meaning in language. An introduction to Semantics and Pragmatics", "author": ["Cruse", "Alan"], "venue": null, "citeRegEx": "Cruse and Alan.,? \\Q2000\\E", "shortCiteRegEx": "Cruse and Alan.", "year": 2000}, {"title": "Two step CCA: A new spectral method for estimating vector models of words", "author": ["Dhillon", "Paramveer S", "Rodu", "Jordan", "Foster", "Dean P", "Ungar", "Lyle H"], "venue": "In Proc. ICML,", "citeRegEx": "Dhillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2012}, {"title": "Word sense disambiguation within a multilingual framework", "author": ["Diab", "Mona Talat"], "venue": "PhD thesis, University of Maryland,", "citeRegEx": "Diab and Talat.,? \\Q2003\\E", "shortCiteRegEx": "Diab and Talat.", "year": 2003}, {"title": "Beyond parallel data: Joint word alignment and decipherment improves machine translation", "author": ["Dou", "Qing", "Vaswani", "Ashish", "Knight", "Kevin"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Dou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dou et al\\.", "year": 2014}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Dyer", "Chris", "Weese", "Jonathan", "Setiawan", "Hendra", "Lopez", "Adam", "Ture", "Ferhan", "Eidelman", "Vladimir", "Ganitkevitch", "Juri", "Blunsom", "Phil", "Resnik", "Philip"], "venue": "In Proceedings of the ACL 2010 System Demonstrations,", "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "A simple, fast, and effective reparameterization of IBM model 2", "author": ["Dyer", "Chris", "Chahuneau", "Victor", "Smith", "Noah A"], "venue": "In Proc. NAACL,", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "A structured vector space model for word meaning in context", "author": ["Erk", "Katrin", "Pad\u00f3", "Sebastian"], "venue": "In Proc. EMNLP,", "citeRegEx": "Erk et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Erk et al\\.", "year": 2008}, {"title": "Measuring word meaning in context", "author": ["Erk", "Katrin", "McCarthy", "Diana", "Gaylord", "Nicholas"], "venue": "Computational Linguistics,", "citeRegEx": "Erk et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Erk et al\\.", "year": 2013}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Manaal", "Dyer", "Chris"], "venue": "In Proceedings of EACL,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Introducing and evaluating ukwac, a very large web-derived corpus of english", "author": ["Ferraresi", "Adriano", "Zanchetta", "Eros", "Baroni", "Marco", "Bernardini", "Silvia"], "venue": "In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google,", "citeRegEx": "Ferraresi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ferraresi et al\\.", "year": 2008}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Multilingual distributed representations without word alignment", "author": ["Hermann", "Karl Moritz", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1312.6173,", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Hermann", "Karl Moritz", "Blunsom", "Phil"], "venue": "In Proc. ACL,", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Embedding word similarity with neural machine translation", "author": ["Hill", "Felix", "Cho", "Kyunghyun", "Jean", "S\u00e9bastien", "Devin", "Coline", "Bengio", "Yoshua"], "venue": "CoRR, abs/1412.6448,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang", "Eric H", "Socher", "Richard", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In Proc. ACL,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Ontologically grounded multi-sense representation learning for semantic vector space models", "author": ["Jauhar", "Sujay K", "Dyer", "Chris", "Hovy", "Eduard"], "venue": "In Proc. NAACL,", "citeRegEx": "Jauhar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Kalchbrenner", "Nal", "Grefenstette", "Edward", "Blunsom", "Phil"], "venue": "In Proc. ACL,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "I don\u2019t believe in word senses", "author": ["Kilgarriff", "Adam"], "venue": "Computers and the Humanities,", "citeRegEx": "Kilgarriff and Adam.,? \\Q1997\\E", "shortCiteRegEx": "Kilgarriff and Adam.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Koehn", "Philipp"], "venue": "In MT summit,", "citeRegEx": "Koehn and Philipp.,? \\Q2005\\E", "shortCiteRegEx": "Koehn and Philipp.", "year": 2005}, {"title": "A solution to Plato\u2019s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge", "author": ["Landauer", "Thomas K", "Dumais", "Susan"], "venue": "Psychological Review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Learning multilingual word representations using a bag-of-words autoencoder", "author": ["Lauly", "Stanislas", "Boulanger", "Alex", "Larochelle", "Hugo"], "venue": "arXiv preprint arXiv:1401.1803,", "citeRegEx": "Lauly et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lauly et al\\.", "year": 2014}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling", "Wang", "Lu\u0131\u0301s", "Tiago", "Marujo", "Astudillo", "Ram\u00f3n Fernandez", "Amir", "Silvio", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel"], "venue": "arXiv preprint arXiv:1508.02096,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Semeval-2007 task 10: English lexical substitution task", "author": ["McCarthy", "Diana", "Navigli", "Roberto"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "McCarthy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McCarthy et al\\.", "year": 2007}, {"title": "A simple word embedding model for lexical substitution", "author": ["Melamud", "Oren", "Levy", "Omer", "Dagan", "Ido"], "venue": "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,", "citeRegEx": "Melamud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2015}, {"title": "Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding", "author": ["Mesnil", "Gr\u00e9goire", "He", "Xiaodong", "Deng", "Li", "Bengio", "Yoshua"], "venue": "In INTERSPEECH,", "citeRegEx": "Mesnil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2013}, {"title": "The senseval-3 english lexical sample task", "author": ["R. Mihalcea", "T. Chklovski", "A. Kilgarriff"], "venue": "In Proceedings of SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,", "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolv", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In Proc. NIPS,", "citeRegEx": "Mikolv et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolv et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Jeff", "Lapata", "Mirella"], "venue": "In Proc. ACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Neelakantan", "Arvind", "Shankar", "Jeevan", "Passos", "Alexandre", "McCallum", "Andrew"], "venue": "In Proc. EMNLP,", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Arkref: a rule-based coreference resolution system", "author": ["O\u2019Connor", "Brendan", "Heilman", "Michael"], "venue": "CoRR, abs/1310.1975,", "citeRegEx": "O.Connor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2013}, {"title": "The informative role of wordnet in open-domain question answering", "author": ["Pasca", "Marius", "Harabagiu", "Sanda"], "venue": "In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources,", "citeRegEx": "Pasca et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Pasca et al\\.", "year": 2001}, {"title": "GloVe: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "In Proc. EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Estimating supersenses with conditional random fields", "author": ["Reichartz", "Frank", "Paa\u00df", "Gerhard"], "venue": "Proceedings of ECMLPKDD,", "citeRegEx": "Reichartz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Reichartz et al\\.", "year": 2008}, {"title": "Distinguishing systems and distinguishing senses: new evaluation methods for word sense disambiguation", "author": ["Resnik", "Philip", "Yarowsky", "David"], "venue": "Natural Language Engineering,", "citeRegEx": "Resnik et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Resnik et al\\.", "year": 1999}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "CoRR, abs/1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Socher", "Richard", "Huang", "Eric H", "Pennington", "Jeffrey", "Ng", "Andrew Y", "Manning", "Chirstopher D"], "venue": "In Proc. NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean Y", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Tian", "Fei", "Dai", "Hanjun", "Bian", "Jiang", "Gao", "Bin", "Zhang", "Rui", "Chen", "Enhong", "Liu", "Tie-Yan"], "venue": "In Proc. COLING,", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian", "Joseph", "Ratinov", "Lev", "Bengio", "Yoshua"], "venue": "In Proc. ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Sense-aware semantic analysis: A multi-prototype word representation model using Wikipedia", "author": ["Wu", "Zhaohui", "Giles", "C. Lee"], "venue": "In Proc. AAAI,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "The noisy channel model for unsupervised word sense disambiguation", "author": ["Yuret", "Deniz", "Yatbaz", "Mehmet Ali"], "venue": "Computational Linguistics,", "citeRegEx": "Yuret et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yuret et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 37, "context": "Distributed representations of words, which represent each word as a vector in a low-dimensional space, can be learned from unannotated text corpora using a variety of techniques (Mikolov et al., 2013; Pennington et al., 2014; Landauer & Dumais, 1997).", "startOffset": 179, "endOffset": 251}, {"referenceID": 43, "context": "Distributed representations of words, which represent each word as a vector in a low-dimensional space, can be learned from unannotated text corpora using a variety of techniques (Mikolov et al., 2013; Pennington et al., 2014; Landauer & Dumais, 1997).", "startOffset": 179, "endOffset": 251}, {"referenceID": 50, "context": "Despite their empirically proven value as a source of features in many downstream applications (Turian et al., 2010), the \u201cone word type, one vector\u201d assumption made by most word representation models is problematic because words may have multiple meanings.", "startOffset": 95, "endOffset": 116}, {"referenceID": 49, "context": "The first to treat each word as a collection of discrete, mutually exclusive senses which are individually represented as vectors (Tian et al., 2014; Neelakantan et al., 2014; Wu & Giles, 2015; Huang et al., 2012; Jauhar et al., 2015).", "startOffset": 130, "endOffset": 234}, {"referenceID": 40, "context": "The first to treat each word as a collection of discrete, mutually exclusive senses which are individually represented as vectors (Tian et al., 2014; Neelakantan et al., 2014; Wu & Giles, 2015; Huang et al., 2012; Jauhar et al., 2015).", "startOffset": 130, "endOffset": 234}, {"referenceID": 24, "context": "The first to treat each word as a collection of discrete, mutually exclusive senses which are individually represented as vectors (Tian et al., 2014; Neelakantan et al., 2014; Wu & Giles, 2015; Huang et al., 2012; Jauhar et al., 2015).", "startOffset": 130, "endOffset": 234}, {"referenceID": 25, "context": "The first to treat each word as a collection of discrete, mutually exclusive senses which are individually represented as vectors (Tian et al., 2014; Neelakantan et al., 2014; Wu & Giles, 2015; Huang et al., 2012; Jauhar et al., 2015).", "startOffset": 130, "endOffset": 234}, {"referenceID": 17, "context": "However, identifying the appropriate sense granularity in such models is difficult in practice and in theory (Kilgarriff, 1997; Erk et al., 2013).", "startOffset": 109, "endOffset": 145}, {"referenceID": 47, "context": "This is surprising given the success of learning composition functions for computing phrase and sentence representations (Socher et al., 2011; Kalchbrenner et al., 2014).", "startOffset": 121, "endOffset": 169}, {"referenceID": 26, "context": "This is surprising given the success of learning composition functions for computing phrase and sentence representations (Socher et al., 2011; Kalchbrenner et al., 2014).", "startOffset": 121, "endOffset": 169}, {"referenceID": 0, "context": "Since bidirectional RNN-LSTMs have been shown to be able to learn both compositional (Bahdanau et al., 2014) as well as more arbitrary relationships (Ling et al.", "startOffset": 85, "endOffset": 108}, {"referenceID": 32, "context": ", 2014) as well as more arbitrary relationships (Ling et al., 2015), we use these as our composition function class (\u00a72).", "startOffset": 48, "endOffset": 67}, {"referenceID": 38, "context": "For example, Mikolv et al. (2013) showed that short multiword expressions could be embedding by using the \u201cwider\u201d context that they occur in.", "startOffset": 13, "endOffset": 34}, {"referenceID": 15, "context": "To obtain pairs of words in context and their lexical translations into a second language, we use unsupervised word alignment techniques (Dyer et al., 2013), to obtain high precision word alignments from a parallel corpus.", "startOffset": 137, "endOffset": 156}, {"referenceID": 15, "context": "After normal tokenization, we obtained alignments with fast-align tool (Dyer et al., 2013).", "startOffset": 71, "endOffset": 90}, {"referenceID": 46, "context": "All recurrent materices with orthogonal initialization (Saxe et al., 2013), and non-recurrent weights are initialized from scaled uniform distribution (Glorot & Bengio, 2010).", "startOffset": 55, "endOffset": 74}, {"referenceID": 7, "context": "To avoid the computational overhead of reading extremely wide contexts, we used sliding window to delimit the range of contexts as in (Collobert et al., 2011), that is, each token wt is embedded using a context window of words wt\u2212n/2, .", "startOffset": 134, "endOffset": 158}, {"referenceID": 2, "context": "Since low-resource languages do not have enough data to adequate estimate translation probabilities, we hope that we can learn more effective mappings with pre-trained word-in-context embeddings (Chahuneau et al., 2013).", "startOffset": 195, "endOffset": 219}, {"referenceID": 13, "context": "We used a dataset used in (Dou et al., 2014) for Malagasy and the Urdu data we used is a part of NIST MT evaluation in 2008-20122.", "startOffset": 26, "endOffset": 44}, {"referenceID": 14, "context": "We trained our baseline system with cdec (Dyer et al., 2010) and obtained synchronous context-free grammars rules to translate sentences.", "startOffset": 41, "endOffset": 60}, {"referenceID": 34, "context": "The original task allow to make multiple predictions but we only predict only one substitution following (Melamud et al., 2015).", "startOffset": 105, "endOffset": 127}, {"referenceID": 6, "context": "We report the averaged BLEU score of 5 runs to avoid optimizer randomness Clark et al. (2011). The result show large improvement on perplexity and consistent improvement on BLEU in all language pairs.", "startOffset": 74, "endOffset": 94}, {"referenceID": 34, "context": "Also we compared our results with various context sensitive models, which take arithmetic mean (as in Add and BalAdd) and a geometrical mean (as in Mult and BalMult) of embeddings, proposed by (Melamud et al., 2015).", "startOffset": 193, "endOffset": 215}, {"referenceID": 19, "context": "They trained their baseline embeddings (as in Base) on a two billion word web corpus, ukWaC (Ferraresi et al., 2008).", "startOffset": 92, "endOffset": 116}, {"referenceID": 19, "context": "They trained their baseline embeddings (as in Base) on a two billion word web corpus, ukWaC (Ferraresi et al., 2008). The model achieved best measures4 10.63, best mode measure 18.90 with German supervision. And the second best result was obtained with Czech. As for comparison with Melamud et al. (2015), we cannot compare score directly since we used different corpus and candidate generation.", "startOffset": 93, "endOffset": 305}, {"referenceID": 50, "context": "In the sequence modeling problems such as BIO chunking, conditional random fields and recurrent neural networks are applied to represent a sequence of word representations (Turian et al., 2010; Mesnil et al., 2013).", "startOffset": 172, "endOffset": 214}, {"referenceID": 35, "context": "In the sequence modeling problems such as BIO chunking, conditional random fields and recurrent neural networks are applied to represent a sequence of word representations (Turian et al., 2010; Mesnil et al., 2013).", "startOffset": 172, "endOffset": 214}, {"referenceID": 31, "context": "For classification tasks such as document classification, sentiment analysis, paraphrase detection, summation of word embeddings (Lauly et al., 2014), convolutional neural networks (Kalchbrenner et al.", "startOffset": 129, "endOffset": 149}, {"referenceID": 26, "context": ", 2014), convolutional neural networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al.", "startOffset": 39, "endOffset": 66}, {"referenceID": 48, "context": ", 2014) and recursive networks (Socher et al., 2013; Cheng & Kartsaklis, 2015) were proposed to represent compositionality function of words.", "startOffset": 31, "endOffset": 78}, {"referenceID": 8, "context": "Coulmance et al. (2015) shows that predicting context in target language is an effective way to train word representation shared across languages.", "startOffset": 0, "endOffset": 24}, {"referenceID": 8, "context": "Coulmance et al. (2015) shows that predicting context in target language is an effective way to train word representation shared across languages. Hill et al. (2014) investigated the quality of word embedding learned by neural machine translation model and show its benefit on tasks that require modeling word similarity.", "startOffset": 0, "endOffset": 166}, {"referenceID": 47, "context": "Furthermore, one can learn reasonable generalizations from models that condition on and the generate text using an autoencoding objective (Socher et al., 2011).", "startOffset": 138, "endOffset": 159}, {"referenceID": 11, "context": "Dhillon et al. (2012) make the intriguing proposition that left- and right- contexts can be used to supervise each other.", "startOffset": 0, "endOffset": 22}], "year": 2015, "abstractText": "We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these.", "creator": "LaTeX with hyperref package"}}}