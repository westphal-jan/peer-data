{"id": "1511.04695", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2015", "title": "An Iterative Reweighted Method for Tucker Decomposition of Incomplete Multiway Tensors", "abstract": "we consider partial result of low - rank decomposition of incomplete multiway components. since many real - function data depended on totally intrinsically low dimensional subspace, tensor low - rank retrieval with missing entries allows success in many different structures problems identified as tensor theory and entropy inpainting. in this respect, we focus on tucker decomposition which resembles an nth - diffusion tensor in terms of simple matrices rank and a core term termed multilinear transform. to exploit the underlying multilinear low - norm structure in tri - dimensional datasets, our generate shallow group - based log - sum penalty functional lets enforce metric sparsity over the core tensor, which yield, dense posterior inverse hence greater core tensor. the concept for shallow decomposition roughly proceeded by iteratively minimizing inverse surrogate function that passes the original objective function, which results despite additional extensive reweighted search. until comparison, to reduce maximal computational complexity, an over - threshold quantum fast computational shrinkage - filtering technique remains adapted and reused in three iterative fisher decomposition. the metric optimization is able also determine the brute complexity ( sv. our. multilinear rank ) in single automatic way. initial results show any contemporary proposed algorithm offers inadequate performance considering a other differential filters.", "histories": [["v1", "Sun, 15 Nov 2015 12:56:36 GMT  (4993kb)", "http://arxiv.org/abs/1511.04695v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["linxiao yang", "jun fang", "hongbin li", "bing zeng"], "accepted": false, "id": "1511.04695"}, "pdf": {"name": "1511.04695.pdf", "metadata": {"source": "CRF", "title": "An Iterative Reweighted Method for Tucker Decomposition of Incomplete Multiway Tensors", "authors": ["Linxiao Yang", "Jun Fang", "Hongbin Li", "Bing Zeng"], "emails": ["Fang@uestc.edu.cn", "bin.Li@stevens.edu", "eezeng@uestc.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n04 69\n5v 1\n[ cs\n.N A\n] 1\n5 N\nov 2\n01 5\nIndex Terms\u2014Tucker decomposition, low rank tensor decomposition, tensor completion, iterative reweighted method.\nI. INTRODUCTION\nMulti-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7]. Tensors (i.e. multiway arrays) provide an effective representation of such data. Tensor decomposition based on low rank approximation is a powerful technique to extract useful information from multiway data as many real-world multiway data are lying on a low dimensional subspace. Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc. Tucker decomposition [14] and CANDECOMP/PARAFAC (CP) decomposition [15] are two\nLinxiao Yang, and Jun Fang are with the National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu 611731, China, Email: JunFang@uestc.edu.cn\nHongbin Li is with the Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ 07030, USA, E-mail: Hongbin.Li@stevens.edu\nBing Zeng is with the School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China, Email: eezeng@uestc.edu.cn\nThis work was supported in part by the National Science Foundation of China under Grant 61428103, and the National Science Foundation under Grant ECCS-1408182.\nwidely used low-rank tensor decompositions. Specifically, CP decomposes a tensor as a sum of rank-one tensors, whereas Tucker is a more general decomposition which involves multilinear operations between a number of factor matrices and a core tensor. CP decomposition can be viewed as a special case of Tucker decomposition with a super-diagonal core tensor. It is generally believed that Tucker decomposition has a better generalization ability than CP decomposition for different types of data [16]. In many applications, only partial observations of the tensor may be available. It is therefore important to develop efficient tensor decomposition methods for incomplete, sparsely observed data where a significant fraction of entries is missing.\nLow-rank decomposition of incomplete multiway tensors has attracted a lot of attention over the past few years and a number of algorithms [17]\u2013[28] were proposed via either optimization techniques or probabilistic model learning. For both CP and Tucker decompositions, the most challenging task is to determine the model complexity (i.e. the rank of the tensor) in the presence of missing entries and noise. It has been shown that determining the CP rank, i.e. the minimum number of rank-one terms in CP decomposition, is an NP-hard problem even for a completely observed tensor [29]. Unfortunately, many existing methods require that the rank of the decomposition is specified a priori. To address this issue, a Bayesian method was proposed in [13] for CP decomposition, where a shrinkage prior called as the multiplicative gamma process (MGP) was employed to adaptively learn a concise representation of the tensor. In [26], a sparsity-inducing Gaussian inverse-Gamma prior was placed over multiple latent factors to achieve automatic rank determination. Besides the above Bayesian methods, an optimization-based CP decomposition method was proposed in [18], [19], where the Frobenius-norm of the factor matrices is used as the rank regularization to determine an appropriate number of component tensors.\nIn addition to the CP rank, another notion of tensor rank is multilinear rank [30], which is defined as the tuple of the ranks of the mode-n unfoldings of the tensor. Multilinear rank is closely related to the Tucker decomposition since the multilinear rank is equivalent to the dimension of the smallest achievable core tensor in Tucker decomposition [31]. To search for a low multilinear rank representation, a tensor nuclear norm, defined as a (weighted) summation of nuclear norms of mode-n unfoldings, was introduced to approximate the multilinear rank. Tensor completion and decomposition can then be accomplished by minimizing the tensor nuclear norm. Specifically, an alternating direction method of multipliers (ADMM) was developed in [17], [23] to minimize the tensor\n2 nuclear norm with missing data, and encouraging results were reported on visual data. The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization. Nevertheless, the tensor nuclear norm, albeit effective, is not necessarily the tightest convex\nenvelope of the multilinear rank [21]. Also, the nuclear normbased methods are sensitive to outliers and work well only if the tensor is exactly low multilinear rank.\nIn this paper, to automatically achieve a concise Tucker representation, we introduce a notion referred to as the order(N \u2212 1) sub-tensor and propose a group log-sum penalty functional to encourage structural sparsity of the core tensor. Specifically, in the log-sum penalty function, elements in every sub-tensor of the core tensor along each mode are grouped together. Minimizing the group log-sum penalty function thus leads to a structured sparse core tensor with only a few nonzero order-(N \u2212 1) sub-tensors along each mode. By removing the zero order-(N \u2212 1) sub-tensors, the core tensor shrinks and a compact Tucker decomposition can be obtained. Note that the log-sum function which behaves like the \u21130-norm is more sparsity-encouraging than the nuclear norm that is \u21131norm applied to the singular values of a matrix. Thus we expect the group log-sum minimization is more effective than the tensor nuclear norm-minimization in finding a concise representation of the tensor. By resorting to a majorizationminimization approach, we develop an iterative reweighted method via iteratively decreasing a surrogate function that majorizes the original log-sum penalty function. The proposed method can determine the model complexity (i.e. multilinear rank) in an automatic way. Also, the over-relaxed monotone fast iterative shrinkage-thresholding technique [34] is adapted and embedded in the iterative reweighted process, which achieves a substantial reduction in computational complexity.\nThe remainder of this paper is organized as follows. Section II provides notations and basics on tensors. The problem of Tucker decomposition with incomplete entries is formulated as an unconstrained optimization problem in Section III. An iterative reweighted method is developed in Section IV for Tucker decomposition of incomplete multiway tensors. In Section V, the over-relaxed monotone fast iterative shrinkagethresholding technique is adapted and integrated with the proposed iterative reweighted method, which results in a significant computational complexity reduction. Simulation results are provided in section VI, followed by concluding remarks in Section VII."}, {"heading": "II. NOTATIONS AND BASICS ON TENSORS", "text": "We first provide a brief review on tensor decompositions. A tensor is the generalization of a matrix to higher dimensions, also known as ways or modes. Vectors and matrices can be viewed as special cases of tensors with one and two modes, respectively. Throughout this paper, we use symbols \u2297 , \u25e6 and \u2217 to denote the Kronecker, outer and Hadamard product, respectively.\nLet X \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN denote an N th order tensor with its (i1, . . . , iN )th entry denoted by Xi1\u00b7\u00b7\u00b7iN . Here the order\nFig. 1. The Tucker decomposition of a three-order tensor.\nN of a tensor is the number of dimensions. Fibers are the higher-order analogue of matrix rows and columns. The moden fibers of X are In-dimensional vectors obtained by fixing every index but in. Slices are two-dimensional sections of a tensor, defined by fixing all but two indices. Unfolding or matricization is an operation that turns a tensor into a matrix. Specifically, the mode-n unfolding of a tensor X , denoted as X(n), arranges the mode-n fibers to be the columns of the resulting matrix. For notational convenience, we also use the notation unfoldn(X ) to denote the unfolding operation along the n-th mode. The n-mode product of X with a matrix A \u2208 RJ\u00d7In is denoted by X \u00d7n A and is of size I1 \u00b7 \u00b7 \u00b7 \u00d7 In\u22121\u00d7J\u00d7In+1\u00d7\u00b7 \u00b7 \u00b7\u00d7IN , with each mode-n fiber multiplied by the matrix A, i.e.\nY = X \u00d7n A \u21d4 Y (n) = AX(n) (1)\nThe CP decomposition decomposes a tensor into a sum of rank-one component tensors, i.e.\nX =\nR\u2211\nr=1\n\u03bbra (1) r \u25e6 a (2) r \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 a (N) r (2)\nwhere a(n)r \u2208 RIn , \u2018\u25e6\u2019 denotes the vector outer product, and R is referred to as the rank of the tensor. Elementwise, we have\nXi1i2\u00b7\u00b7\u00b7iN = R\u2211\nr=1\n\u03bbra (1) i1r a (2) i2r \u00b7 \u00b7 \u00b7 a (N) iNr\n(3)\nThe Tucker decomposition can be considered as a high order principle component analysis. It decomposes a tensor into a core tensor multiplied by a factor matrix along each mode. The Tucker decomposition of an N -th order tensor X can be written as\nX = G \u00d71 A (1) \u00d72 A (2) \u00b7 \u00b7 \u00b7 \u00d7N A (N)\n=\nR1\u2211\nr1=1\nR2\u2211\nr2=2\n\u00b7 \u00b7 \u00b7\nRN\u2211\nrN=1\nGr1r2\u00b7\u00b7\u00b7rNa (1) r1 \u25e6 a(2)r2 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 a (N) rN (4)\nwhere G \u2208 RR1\u00d7R2\u00d7\u00b7\u00b7\u00b7\u00d7RN is the core tensor, and A(n) , [a\n(n) 1 . . . a (n) Rn ] \u2208 RIn\u00d7Rn denotes the factor matrix along the n-th mode (see Fig.1).\nThe inner product of two tensors with the same size is defined as\n\u3008X ,Y\u3009 =\nI1\u2211\ni1=1\nI2\u2211\ni2=1\n\u00b7 \u00b7 \u00b7\nIN\u2211\niN=1\nxi1i2...iN yi1i2...iN (5)\nThe Frobenius norm of a tensor X is square root of the inner product with itself, i.e.\n\u2016X\u2016F = \u3008X ,X \u3009 1 2 (6)\n3 Also, for notational convenience, the sequential Kronecker product of a set of matrices in a reversed order is defined and denoted by \u2297\nn\nA(n) = A(N) \u2297A(N\u22121) \u2297 \u00b7 \u00b7 \u00b7 \u2297A(1)\n\u2297\nn6=k\nA(n) = A(N) \u2297 \u00b7 \u00b7 \u00b7 \u2297A(k+1) \u2297A(k\u22121) \u2297 \u00b7 \u00b7 \u00b7 \u2297A(1)\nAn N th order tensor X multiplied by factor matrices {A(k)}Nk=1 along each mode is denoted by\nX\nN\u220f\nn=1\n\u00d7nA (n) = X \u00d71 A (1) \u00d72 A (2) \u00b7 \u00b7 \u00b7 \u00d7N A (N),\nwhile the tensor X multiplied by the factor matrices along every mode except the k-th mode is denoted as\nX \u220f\nn6=k\n\u00d7nA (n) = X \u00d71 A (1) \u00b7 \u00b7 \u00b7 \u00d7k\u22121 A (k\u22121)\n\u00d7k+1 A (k+1) \u00b7 \u00b7 \u00b7 \u00d7N A (N).\nWith these notations, vectorization and unfolding of a tensor which admits a Tucker decomposition (4) can be expressed as\nvec(X ) = (\u2297\nn\nA(n) ) vec(G) (7)\nunfoldn(X ) = A (n)G(n)\n(\u2297\nk 6=n\nA(k) )T\n(8)"}, {"heading": "III. PROBLEM FORMULATION", "text": "Let Y \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN be an incomplete N th order tensor, with its entry Yi1i2...iN observed if Oi1i2...iN = 1, where O \u2208 {0, 1}I1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN is a binary tensor of the same size as Y and indicates which entries of Y are missing or observed. Given the observed data, our objective is to find a Tucker decomposition which has a minimum model complexity and meanwhile fits the observed data, or to be more precise, seek a Tucker representation such that the data can be represented by a smallest core tensor. Since the dimension of the smallest achievable core tensor is unknown a priori, we need to develop a method that can achieve automatic model determination. To this objective, we first introduce a new notion called as order(N \u2212 1) sub-tensor.\nDefinition: Order-(N \u2212 1) sub-tensor is defined as a new tensor obtained by fixing only one index of the original tensor. Let Z \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN be an N th order tensor. The ith (1 \u2264 i \u2264 In) sub-tensor along the nth mode of Z , denoted as Z(n,i), is an (N \u2212 1)th order tensor of size I1 \u00d7 I2 \u00d7 \u00b7 \u00b7 \u00b7 In\u22121 \u00d7 In+1 \u00b7 \u00b7 \u00b7 \u00d7 IN , and its (j1, . . . , jN\u22121)th entry are given by Xj1,...,jn\u22121,i,jn+1,...,jN\u22121 . For tensors with three modes, i.e. N = 3, order-(N \u2212 1) sub-tensors reduce to slices, although order-(N \u2212 1) sub-tensors are generally different from slices.\nClearly, Z consists of In order-(N \u2212 1) sub-tensors along its nth mode. If some order-(N\u22121) sub-tensors along the nth mode become zero, then the dimension of Z along the nth\nmode is reduced accordingly. Suppose the data tensor Y has a Tucker decomposition\nY = X\nN\u220f\nn=1\n\u00d7nA (n) (9)\nUnfolding Y along the nth mode, we have\nY (n) =A (n)X(n)\n(\u2297\nk 6=n\nA(k) )T\n=\nIn\u2211\ni=1\na (n) \u00b7,i x (n) i,\u00b7\n(\u2297\nk 6=n\nA(k) )T\n(10)\nwhere a(n)\u00b7,i is the ith column of A (n) and x(n)i,\u00b7 is the ith row of X(n). Clearly, x (n) i,\u00b7 is the vectorization of the ith order (N \u2212 1) sub-tensor along the nth mode. If x(n)i,\u00b7 is a zero vector, i.e. the corresponding order-(N \u2212 1) sub-tensor is a zero tensor, both a(n)\u00b7,i and x (n) i,\u00b7 have no contribution to Y and can thus be removed. Inspired by this insight, sparsity can be enforced over each sub-tensor (along each mode) of the core tensor such that the observed data can be represented by a structural sparsest core tensor with only a few nonzero subtensors over all modes. By removing those zero sub-tensors along each mode (the associated columns of the factor matrices are disabled and can be removed as well), the core tensor shrinks to a smaller one and a compact Tucker decomposition can be obtained. The problem can be formulated as\nmin X ,{A(n)}\nN\u2211\nn=1\n\u2016zn\u20160\ns.t. \u2016O \u2217 (Y \u2212X N\u220f\nn=1\n\u00d7nA (n))\u20162F \u2264 \u03b5 (11)\nwhere \u03b5 is an error tolerance parameter related to noise statistics, and zn is an In-dimensional vector with its ith entry given by\nzn,i , \u2016X (n,i)\u2016F (12)\nIt should be noted that since there is usually no knowledge about the size of smallest core tensor, the dimensions of the core tensor are predefined to be the same as the original tensor, i.e. X \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN . The term \u2016zn\u20160 specifies the number of nonzero sub-tensors along the nth mode of tensor X . Since the number of nonzero sub-tensors along the nth mode is equivalent to the dimension of mode-n of the core tensor, the above optimization yields a smallest (in terms of sum of dimensions of all modes) core tensor. The optimization (11), however, is an NP-hard problem. Thus, alternative sparsitypromoting functions which are more computationally efficient in finding the structural sparse core tensor are desirable. In this paper, we consider the use of the log-sum sparsity-encouraging functional. Log-sum penalty function has been extensively used for sparse signal recovery and was shown to be more sparsity-encouraging than the \u21131-norm [35]\u2013[38]. Replacing\n4 the \u21130-norm in (11) with the log-sum functional leads to\nmin X ,{A(n)}\nN\u2211\nn=1\nIn\u2211\ni=1\nlog(\u2016X (n,i)\u2016 2 F + \u01eb)\ns.t. \u2016O \u2217 (Y \u2212X N\u220f\nn=1\n\u00d7nA (n))\u20162F \u2264 \u03b5 (13)\nwhere \u01eb is a small positive parameter to ensure the logarithmic function is well-defined. Note that in our formulation, coefficients are grouped according to sub-tensors and different sub-tensors may have overlapping entries. This is different from the group-LASSO [39], [40] method in which entries are grouped into a number of non-overlapping subsets with sparsity imposed on each subset. Also, to avoid the solution X \u2192 0 and {A(n)} \u2192 \u221e, a Frobenius norm can be imposed on the factor matrices {A(n)}. The optimization (13) can eventually be formulated as an unconstrained optimization problem as follows\nmin X ,{A(n)}\nL(X , {A(n)}) =\nN\u2211\nn=1\nIn\u2211\ni=1\nlog(\u2016X (n,i)\u2016 2 F + \u01eb)\n+ \u03bb1 \u2225\u2225\u2225O \u2217 ( Y \u2212X N\u220f\nn=1\n\u00d7nA (n) )\u2225\u2225\u2225 2\nF + \u03bb2\nN\u2211\nn=1\n\u2016A(n)\u20162F\n(14)\nwhere \u03bb1 is a parameter controlling the tradeoff between the sparsity of the core tensor and the fitting error, and \u03bb2 is a regularization parameter for factor matrices. Their choices will be discussed later in our paper.\nThe above optimization (14) can be viewed as searching for a low multilinear rank representation of the observed data. Multilinear rank, also referred to as n-rank, of an N -order tensor X is defined as the tuple of the ranks of the mode-n unfoldings, i.e.\nn-rank , {rank(X(1)), rank(X(2)), . . . , rank(X(N))} (15)\nIt can be shown that n-rank is equivalent to the dimensions of the smallest achievable core tensor in Tucker decomposition [31]. Therefore the optimization (14) can also be used for recovery of incomplete low n-rank tensors. Existing methods for low n-rank completion employ a tensor nuclearnorm, defined as a (weighted) summation of nuclear-norms of mode-n unfoldings, to approximate the n-rank and achieve a low n-rank representation. Our formulation, instead, uses the Frobenius-norms of order-(N \u2212 1) sub-tensors to promote a low n-rank representation."}, {"heading": "IV. PROPOSED ITERATIVE REWEIGHTED METHOD", "text": "We resort to a bounded optimization approach, also known as the majorization-minimization (MM) approach [41], to solve the optimization (14). The idea of the MM approach is to iteratively minimize a simple surrogate function majorizing the given objective function. It can be shown that through iteratively minimizing the surrogate function, the iterative process yields a non-increasing objective function value and eventually converges to a stationary point of the original objective function.\nTo obtain an appropriate surrogate function for (14), we first find a suitable surrogate function for the log-sum function. With the following inequality\nlog(\u2016X (n,i)\u2016 2 F + \u01eb) \u2264\n\u2016X (n,i)\u2016 2 F + \u01eb \u2016X (t) (n,i)\u2016 2 F + \u01eb\n+ log(\u2016X (t) (n,i)\u2016 2 F + \u01eb)\u2212 1 (16)\nwe arrive at f(X |X (t)) defined in (17) is a surrogate function majorizing the log-sum functional, i.e.\nN\u2211\nn=1\nIn\u2211\ni=1\nlog(\u2016X (n,i)\u2016 2 F + \u01eb) \u2264 f(X |X (t))\nwith the equality attained when X = X (t), where\nf(X |X (t))\n,\u3008X ,D(t) \u2217X \u3009+ N\u2211\nn=1\nIn\u2211\ni=1\nlog(\u2016X (t) (n,i)\u2016 2 F + \u01eb)\u2212\nN\u2211\nn=1\nIn\n(17)\nin which D(t) is a tensor of the same size of X , with its (i1, i2, . . . , iN )th element given by\nD (t) i1i2...iN =\nN\u2211\nn=1\n(\u2016X (t) (n,in) \u20162F + \u01eb) \u22121 (18)\nThus we can readily verify that an appropriate surrogate function majorizing the objective function (14) is given as\nQ(X , {A(n)}|X (t)) = \u03bb1 \u2225\u2225\u2225O \u2217 ( Y \u2212X N\u220f\nn=1\n\u00d7nA (n) )\u2225\u2225\u2225 2\nF\n+ \u3008X ,D(t) \u2217X \u3009+ \u03bb2\nN\u2211\nn=1\n\u2016A(n)\u20162F + c (19)\nwhere c is a constant. That is,\nL(X , {A(n)}) \u2264 Q(X , {A(n)}|X (t)) (20)\nwith the equality attained when X = X (t). Solving (14) now reduces to minimizing the surrogate function (19) iteratively. Minimization of the surrogate function, however, is still difficult since it involves a joint search over the core tensor X and the associated factor matrices {A(n)}Nn=1. Nevertheless, we will show that through iteratively decreasing (not necessarily minimizing) the surrogate function, the iterative process also results in a non-increasing objective function value and eventually converges to a stationary point of L(X , {A(n)}). Decreasing the surrogate function is much easier since we only need to alternatively minimize the surrogate function (19) with respect to each variable while keeping other variables fixed. Details of this alternating procedure are provided below.\nFirst, we minimize the surrogate function (19) with respect to the core tensor X , given the factor matrices {A(n)} fixed. The problem reduces to\nmin X \u03bb1\n\u2225\u2225\u2225O \u2217 ( Y \u2212X N\u220f\nn=1\n\u00d7nA (n) )\u2225\u2225\u2225 2\nF + \u3008X ,D(t) \u2217X \u3009\n(21)\n5 Let x , vec(X ). The above optimization can be expressed as\nmin x \u03bb1\n\u2225\u2225\u2225\u03a3 ( y \u2212 (\u2297\nn\nA(n) ) x )\u2225\u2225\u2225 2\nF + xTD(t)x (22)\nwhere \u03a3 , diag(vec(O)) and D(t) , diag(vec(D(t))). For notational simplicity, define\nH , (\u2297\nn\nA(n) )\n(23)\nThe optimal solution to (22) can be easily obtained as\nx = (HT\u03a3H + \u03bb\u221211 D (t))\u22121HT\u03a3y (24)\nNext, we discuss minimizing the surrogate function (19) with respect to the factor matrix A(n), given that the core tensor X and the rest of factor matrices {A(k)}k 6=n fixed. Ignoring terms independent of A(n) and unfolding the tensor along the nth mode, we arrive at\nmin A(n) \u03bb1\n\u2225\u2225\u2225\u2225O(n) \u2217 ( Y (n) \u2212A (n)X(n) (\u2297\nk 6=n\nA(k) )T)\u2225\u2225\u2225\u2225 2\nF\n+ \u03bb2\u2016A (n)\u20162F (25)\nClearly, the optimization can be decomposed into a set of independent tasks, with each task optimizing each row of A(n). Specifically, let yn,i denote the ith row of Y (n), a (n) i denote the ith row of A(n), and \u03a3ni , diag(on,i), with on,i being the ith row of O(n). The optimization of each row of A(n) can be written as\nmin a\n(n) i\n\u03bb1 \u2225\u2225\u2225\u2225\u03a3 n i ( yn,i \u2212 a (n) i X(n) (\u2297\nk 6=n\nA(k) )T)\u2225\u2225\u2225\u2225 2\nF\n+ \u03bb2\u2016a (n) i \u2016 2 F (26)\nwhose optimal solution can be readily given as\na (n) i = \u03bb1yn,i\u03a3 n i \u03a6(\u03bb1\u03a6 T \u03a3 n i \u03a6+ \u03bb2I) \u22121 (27)\nin which\n\u03a6 ,\n(\u2297\nk 6=n\nA(k) ) XT(n)\nNote that \u03a6 can be more efficiently calculated from unfoldn(X \u220f k 6=n A\n(k)). Thus far we have shown how to minimize the surrogate function (19) with respect to each variable while keeping other variables fixed. Given a current estimate of the core tensor and the associated factor matrices {X (t), {(A(n))(t)}Nn=1}, this alternating procedure is guaranteed to find a new estimate {X (t+1), {(A(n))(t+1)}Nn=1} such that\nQ(X (t+1), {(A(n))(t+1)}|X (t)) \u2264 Q(X (t), {(A(n))(t)}|X (t)) (28)\nIn the following, we further show that the new estimate {X (t+1), {(A(n))(t+1)}Nn=1} results in a non-increasing ob-\njective function value\nL(X (t+1), {(A(n))(t+1)})\n=L(X (t+1), {(A(n))(t+1)})\u2212Q(X (t+1), {(A(n))(t+1)}|X (t))\n+Q(X (t+1), {(A(n))(t+1)}|X (t))\n\u2264L(X (t), {(A(n))(t)})\u2212Q(X (t), {(A(n))(t)}|X (t))\n+Q(X (t+1), {(A(n))(t+1)}|X (t))\n\u2264L(X (t), {(A(n))(t)})\u2212Q(X (t), {(A(n))(t)}|X (t))\n+Q(X (t), {(A(n))(t)}|X (t))\n=L(X (t), {(A(n))(t)}) (29)\nwhere the first inequality follows from the fact that Q(X , {(A(n))}|X (t)) \u2212 L(X , {A(n)}) attains its minimum when X = X (t), and the second inequality comes from (28). We see that through iteratively decreasing (not necessarily minimizing) the surrogate function, the objective function L(X , {A(n)}) is guaranteed to be non-increasing at each iteration.\nFor clarity, we summarize our algorithm as follows.\nIterative Reweighted Algorithm for Tucker Decomposition\n1. Given initial estimates {(A(n))(0)}, X (0) and a preselected regularization parameter \u03bb1 and \u03bb2. 2. At iteration t = 0, 1, . . .: Based on the estimate X\n(t), construct the surrogate function as depicted in (19). Search for a new estimate of {(A(n))(t+1)} and X\n(t+1) via (24) and (27), respectively. 3. Go to Step 2 if \u2016X (t+1) \u2212X (t)\u2016F > \u03b5, where \u03b5 is a\nprescribed tolerance value; otherwise stop.\nRemark: We discuss the computational complexity of the proposed method. The main computational task of our proposed algorithm at each iteration involves calculating a new estimate of X (t) and {A(n)(t)}. Specifically, the update of the core tensor X involves computing an inverse of a ( \u220f n In) \u00d7 ( \u220f\nn In) matrix (cf. (24)), which has a computational complexity of order O( \u220f n I 3 n) and scaling polynomially with the data size. The computational complexity associated with the update of the ith row of A(n) is of order O(I3n + ( \u2211 k 6=n Ik) \u220f k Ik) (cf. (27)), where the term\nO(( \u2211 k 6=n Ik) \u220f\nk Ik) comes from the computation of \u03a6 and scales linearly with the data size, and the term O(I3n) is related to the inverse of an In \u00d7 In matrix. Since all rows of A(n) share a same \u03a6, the computational complexity of updating A(n) is of order O(I4n +( \u2211 k 6=n Ik) \u220f k Ik). We see that the overall computational complexity at each iteration is dominated by O( \u220f n I 3 n), which scales polynomially with the data size, and makes the algorithm unsuitable for many real-world applications involving large dimensions. To address this issue, we resort to a computationally efficient algorithm, namely, an over-relaxed monotone fast iterative shrinkagethresholding algorithm (MFISTA), to solve the optimization (22). A directly calculation of (24) is no longer needed and a significant reduction in computational complexity can be achieved.\n6"}, {"heading": "V. A COMPUTATIONALLY EFFICIENT ITERATIVE REWEIGHTED ALGORITHM", "text": "It is well known that first order methods based on function values and gradient evaluations are often practically most feasible options to solve many large-scale optimization problems. One famous first order method is the fast iterative shrinkagethresholding algorithm (FISTA) [42]. It has a convergence rate of O(1/k2) for the minimization of the sum of a smooth and a possibly nonsmooth convex function, where k denotes the iteration counter. Later on in [34], an over-relaxed monotone FISTA (MFISTA) was proposed to overcome some limitations inherent in the FISTA. Specifically, the over-relaxed MFISTA guarantees the monotome decreasing in the function values, which has been shown to be helpful in many practical applications. In addition, the over-relaxed MFISTA admits a variable stepsize in a broader range than FISTA while keeping the same convergence rate. In the following, we first provide a brief review of the over-relaxed MFISTA, and then discuss how to extend the technique to solve our problem."}, {"heading": "A. Review of Over-Relaxed MFISTA", "text": "Consider the general convex optimization problem:\nmin x F (x) = f(x) + g(x)\nwhere f is a smooth convex function with the Lipschitz continuous gradient L(f), and g is a convex but possibly non-smooth function. The over-relaxed MFISTA scheme is summarized as follows. Given x(0) = w(1), \u03b7(1) = 1, \u03b4 \u2208 (0, 2) and \u03b2 \u2208 (0, (2\u2212 \u03b4)/L(f)], the sequence {x(t)} is given by\nz(t) = prox\u03b2g(w (t) \u2212 \u03b2\u2207f(w(t))) (30) x(t) = argmin{F (z)|z \u2208 {z(t),x(t\u22121)}} (31)\n\u03b7(t+1) = 1 +\n\u221a 1 + 4(\u03b7(t))2\n2 (32)\nw(t+1) = x(t) + \u03b7(t)\n\u03b7(t+1) (z(t) \u2212 x(t)) +\n\u03b7(t) \u2212 1\n\u03b7(t+1) (x(t) \u2212 x(t\u22121))\n+ \u03b7(t)\n\u03b7(t+1) (1\u2212 \u03b4)(w(t) \u2212 z(t)) (33)\nwhere \u2207f(x) denotes the gradient of f(x), and the proximal operator is defined as\nprox\u03b2g(x) : z = argmin z\n{g(z) + 1\n2\u03b2 \u2016z \u2212 x\u201622} (34)\nIt was proved in [34] that the sequence {x(t)} is guaranteed to monotonically decrease the objective function F (x) and the convergence rate is O(1/k2). Since (22) is convex, the overrelaxed MFISTA can be employed to efficiently solve (22)."}, {"heading": "B. Solving (22) via the Over-Relaxed MFISTA", "text": "Consider the optimization (22). Let f(x) and g(x) respectively represent the data fitting and regularization terms, i.e.\nf(x) = \u03bb1 \u2225\u2225\u2225\u03a3 ( y \u2212Hx )\u2225\u2225\u2225 2\nF\ng(x) = xTDx\nRecalling that H is defined in (23). To apply the overrelaxed MFISTA, we need to compute \u2207f(x), prox\u03b2g(x), and determine the value of \u03b2. The gradient of f(x) can be easily computed as\n\u2207f(x) = 2\u03bb1H T \u03a3Hx\u2212 2\u03bb1H T \u03a3y (35)\nwhich can also be expressed in a tensor form as\n\u2207f(X ) = 2\u03bb1\n( O \u2217 ( X N\u220f\nn=1\n\u00d7nA (n) \u2212Y\n)) N\u220f\nn=1\n\u00d7nA (n)T\n(36)\nSuch a tensor representation enables a more efficient computation of \u2207f(x). The proximal operation prox\u03b2g(x) defined in (34) can be obtained as\nz = argmin z\n{g(z) + 1\n2\u03b2 \u2016z \u2212 x\u201622}\n= argmin z\n{zTDz + 1\n2\u03b2 \u2016z \u2212 x\u201622}\n= argmin z\n{zT (D + 1\n2\u03b2 )z \u2212\n1 \u03b2 zTx}\n= (2\u03b2D + I)\u22121x (37)\nNote that since D is a diagonal matrix, the inverse of 2\u03b2D+ I is easy to calculate. We now discuss the choice of \u03b2 in the MFISTA. As mentioned earlier, \u03b2 has to be smaller than (2\u2212\u03b4)/L(f); otherwise convergence of the scheme cannot be guaranteed. Recalling that the Lipschitz continuous gradient L(f) is defined as any constant which satisfies the following inequality\n\u2016\u2207f(x)\u2212\u2207f(y)\u2016 \u2264 L(f)\u2016x\u2212 y\u2016 for every x,y\nwhere \u2016 \u00b7 \u2016 denotes the standard Euclidean norm. Hence it is easy to verify that\nL(f) = 2\u03bb1\u03bbmax(H T \u03a3H) (38)\nis a Lipschitz constant of \u2207f(x), where \u03bbmax(X) denotes the largest eigenvalue of the matrix X . Note that HT\u03a3H is of dimension ( \u220f n In)\u00d7( \u220f n In). Calculation of L(f), therefore, requires tremendous computational efforts. To circumvent this issue, we seek an upper bound of L(f) that is easier to compute. Such an upper bound can be obtained by noticing that \u03a3 is a diagonal matrix with its diagonal element equal to zero or one\nL(f) =2\u03bb1\u03bbmax(H T \u03a3H)\n(a) \u22642\u03bb1\u03bbmax(H TH)\n(b) =2\u03bb1\nN\u220f\nn=1\n\u03bbmax(A (n)TA(n)) , L\u0303 (39)\nwhere (a) follows from the fact that HTH \u2212 HT\u03a3H is positive semi-definite, and (b) comes from the Kronecker product\u2019s properties\nHTH = (\u2297\nn\nA(n) )T (\u2297\nn\nA(n) ) = \u2297\nn\n( A(n)TA(n) )\n7 and\neig (\u2297\nn\n( A(n)TA(n) )) = \u2297\nn\neig(A(n)TA(n))\nin which eig(X) is a vector consisting of the eigenvalues of matrix X . Since (2\u2212 \u03b4)/L\u0303 \u2264 (2\u2212 \u03b4)/L(f), \u03b2 can be chosen from (0, (2 \u2212 \u03b4)/L\u0303], without affecting the convergence rate of the over-relaxed MFISTA. The calculation of L\u0303 is much easier than L(f) as the dimension of the matrix involved in the eigenvalue decomposition has been significantly reduced.\nRemarks: We see that the dominant operation in solving (22) via the over-relaxed MFISTA is the evaluation of gradient (36), which has a computational complexity of order O(( \u2211 n In) \u220f n In) that scales linearly with the data size. Thus a significant reduction in computational complexity is achieved as compared with a direct calculation of (24). In addition, our proposed iterative reweighted method only needs to decrease (not necessarily minimize) the surrogate function at each iteration. Therefore when applying the over-relaxed MFISTA to solve (22), there is no need to wait until convergence is achieved. Only a few iterations are enough since the over-relaxed MFISTA guarantees a monotome decreasing in the function values. This further reduces the computational complexity of the proposed algorithm.\nFor clarity, we now summarize the proposed computationally efficient iterative reweighted algorithm as follows.\nAlgorithm 1 Iterative Re-weighted Algorithm For Incomplete Tensor Decomposition Input: Y , O, \u03b4, \u03bb1 and \u03bb2 Output: X , {A(n)}Nn=1, Y and multilinear rank\n1: Initialize X , {A(n)}, D. 2: while not converge do 3: Calculate L\u0303 using (39) and select \u03b2 from (0, (2\u2212\u03b4)/L\u0303]\n4: Set x(0) = vec(X ) 5: for t = 1 to tmax do 6: Calculate the gradient of f(tensor(x(t))) using (36) and the proximal operation prox\u03b2g(x) using (37) 7: Update x(t) using (30), (31), (32), (33) 8: end for 9: Set X = tensor(x(tmax)) 10: for n = 1 to N do 11: for i = 1 to In do 12: Update a(n)i using (27) 13: end for 14: end for 15: Remove the zero order-(N \u2212 1) sub-tensors of X and corresponding columns of {A(n)}. 16: end while 17: Reconstruct Y using estimated X and {A(n)} 18: Estimate multilinear rank by count the nonzero order-(N\u2212\n1) sub-tensors of estimated X along each mode"}, {"heading": "VI. SIMULATION RESULTS", "text": "In this section, we conduct experiments to illustrate the performance of our proposed iterative reweight Tucker decom-\nposition method (referred to as IRTD). In our simulations, we set \u03b4 = 0.1, \u03b2 = (2 \u2212 \u03b4)/L\u0303(f) and \u03bb2 = 1. In fact, our proposed algorithm is insensitive to the choices of these parameters. The choice of \u03bb1 is more critical than the others, and a suitable choice of \u03bb1 depends on the noise level and the data missing ratio. Empirical results suggest that stable recovery performance can be achieved when \u03bb1 is set in the range [0.5, 2]. The factor matrices and core tensor are initialized by decomposing the observed tensor (the missing elements are set to zero) with high order singular value decomposition [43]. In our algorithm, the over-relaxed MFISTA performs only two iterations to update the core tensor, i.e. tmax = 2. We compare our method with several existing state-of-theart tensor decomposition/completion methods, namely, a CP decomposition-based tensor completion method (also referred to as the low rank tensor imputation (LRTI)) which uses the Frobenius-norm of the factor matrices as the rank regularization [18], a tensor nuclear-norm based tensor completion method [23] which is also referred to as the high accuracy low rank tensor completion (HaLRTC) method, and a Tucker factorization method based on pre-specified multilinear rank [22] which is referred to as the WTucker method. It should be noted that the LRTI requires to set a regularization parameters \u03bb to control the tradeoff between the rank and the data fitting error, the HaLRTC method is unable to provide an explicit multilinear rank estimate, and the WTucker method requires an over-estimated multilinear rank. All the parameters used for competing algorithms are tuned carefully to ensure the best performance is achieved."}, {"heading": "A. Synthetic and Chemometrics Data", "text": "In this subsection, we carry out experiments on synthetic and chemometrics data. Two sets of synthetic data are generated and both of them are 3rd-order tensors of size 32\u00d732\u00d732. The first tensor is generated according to the CP model which is a summation of six equally-weighted rank-one tensors, with all of the factor matrices drawn from a normal distribution. Thus the truth rank is 6 or (6, 6, 6) in a multilinear rank form. The other tensor is generated based on the Tucker decomposition model, with a random core tensor of size (3, 4, 5) multiplied by random factor matrices along each mode. Clearly the groundtruth for the multilinear rank is (3, 4, 5). Two chemometrics data sets are also considered in our simulations. One is the Amino Acid data set of size 5 \u00d7 201 \u00d7 61 and the other is the flow injection data set of size 12\u00d7 100\u00d7 89.\nFor each data set, we consider two cases with 50% or 80% entries missing in our simulations, where the missing entries are randomly selected. The observed entries are corrupted by zero mean Gaussian noise and the signal-to-noise ratio (SNR) is set to 10dB. The tensor reconstruction performance is evaluated by the normalized mean squared error (NMSE) \u2016X \u2212 X\u0302\u2016F /\u2016X\u2016F , where X and X\u0302 denote the true tensor and the estimated one, respectively. The parameter \u03bb for the LRTI is carefully selected as \u03bb = 0.3 for the synthetic data set generated by the Tucker model and \u03bb = 0.2 for the other data sets. The pre-defined multilinear rank for WTucker is\n8\nset to be (12, 12, 12), (6, 8, 10), (5, 10, 10) and (10, 10, 10) for the CP, Tucker, Amino Acid and flow injection dataset, respectively. For our proposed method, we choose \u03bb1 = 0.5 for all data sets. Results are averaged over 10 independent runs. The rank or multilinear rank is estimated as the the most frequently occurring rank or multilinear rank value. The standard deviation of the estimated rank is also reported as an indication of the stability of the inferred rank. Results are shown in Table I.\n\u2022 We observe that the proposed method presents the best recovery accuracy among all algorithms and can reliably estimate the true rank. \u2022 Compared with the CP-decomposition based method LRTI, our proposed method presents a clear performance\nadvantage over the LRTI when synthetic data are generated according to the Tucker model and slightly outperforms the LRTI even when data are generated according to the CP model. \u2022 Our proposed method surpasses the tensor nuclear-norm based method HaLRTC by a big margin, which corroborates our claim that the proposed group log-sum functional is more effective than the tensor nuclear-norm in approximating the multilinear rank. \u2022 Tucker model-based methods such as HaLRTC and WTucker work well only when synthetic data are generated according to the Tucker model, while our proposed method provides decent recovery performance whether data are generated via the Tucker or CP model.\n9\nB. Image Inpainting\nThe goal of image inpainting is to complete an image with missing pixels. For a two-dimensional RGB picture, we can treat it as a three-dimensional tensor. Here we consider imputing an incomplete RGB image via respective algorithms. The benchmark Lena image is used, with 50%, 80% and 90% missing entries considered in our simulations. The recovery accuracy is evaluated by the MSE metric which is defined as MSE = 1\nM \u2016O\u2201 \u2217 (X \u2212 X\u0302 )\u20162F , where X and X\u0302 respectively\ndenote the original normalized image and the estimated one, O\u2201 denotes the complement set of the observed set, i.e. O, and M denotes the number of missing elements. For LRTI, the parameter used to control the tradeoff between the data fitting and rank is carefully selected to 5, 3 and 3 for 50%, 80% and 90% missing entries, respectively. The pre-defined rank for the WTucker is set to (100, 100, 3), (80, 80, 3) and (50, 50, 3) for 50%, 80% and 90% missing entries, respectively. For our proposed method, \u03bb1 is set to 3, 0.5 and 0.3 for 50%, 80% and 90% missing entries, respectively. The observed and recovered images are shown in Fig.2 and MSEs of respective algorithms are shown in Table II. From Table II, we see that the proposed method renders a reliable recovery even with 90% missing entries, while the other two Tuck model-based methods WTucker and HaLRTC may incur a considerable performance degradation when the missing ratio is high."}, {"heading": "VII. CONCLUSIONS", "text": "In this paper, we proposed an iterative reweighted algorithm to decompose an incomplete tensor into a concise Tucker decomposition. To automatically determine the model complexity, we introduced a new notion called order-(N \u2212 1) sub-tensor and introduced a group log-sum penalty on every order-(N \u2212 1) sub-tensors to achieve a structural sparse core tensor. By shrinking the zero order-(N \u2212 1) sub-tensors, the core tensor becomes a smaller one and a compact Tucker decomposition can be obtained. By resorting to the majorizationminimization approach, an iterative reweight algorithm was developed. Also, the over-relaxed monotone fast iterative shrinkage-thresholding technique is adapted and embedded in the iterative reweighted process to reduce the computational complexity. The performance of the proposed method is evaluated using synthetic data and real data. Simulation results\nshow that the proposed method offers competitive performance compared with existing methods."}], "references": [{"title": "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering", "author": ["A. Karatzoglou", "X. Amatriain", "L. Baltrunas", "N. Oliver"], "venue": "Proceedings of the fourth ACM conference on Recommender systems. ACM, 2010, pp. 79\u201386.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Temporal collaborative filtering with Bayesian probabilistic tensor factorization", "author": ["L. Xiong", "X. Chen", "T.-K. Huang", "J. Schneider", "J.G. Carbonell"], "venue": "SDM, vol. 10. SIAM, 2010, pp. 211\u2013222.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Context-aware recommender systems", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "Recommender systems handbook. Springer, 2011, pp. 217\u2013 253.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Leveraging features and networks for probabilistic tensor decomposition", "author": ["P. Rai", "Y. Wang", "L. Carin"], "venue": "AAAI Conference on Artificial Intelligence, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Modelling relational data using bayesian clustered tensor factorization", "author": ["I. Sutskever", "J.B. Tenenbaum", "R.R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2009, pp. 1821\u20131828.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor decomposition of EEG signals: A brief review", "author": ["F. Cong", "Q.-H. Lin", "L.-D. Kuang", "X.-F. Gong", "P. Astikainen", "T. Ristaniemi"], "venue": "Journal of neuroscience methods, vol. 248, pp. 59\u201369, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonnegative matrix and tensor factorizations", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S.I. Amari"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Higher-order SVD-based subspace estimation to improve the parameter estimation accuracy in multidimensional harmonic retrieval problems", "author": ["M. Haardt", "F. Roemer", "G.D. Galdo"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3198\u20133213, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Variational bayesian parafac decomposition for multidimensional harmonic retrieval", "author": ["W. Guo", "W. Yu"], "venue": "2011 IEEE CIE International Conference on Radar (Radar), vol. 2. IEEE, 2011, pp. 1864\u20131867.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "A tensor-based approach for automatic music genre classification", "author": ["E. Benetos", "C. Kotropoulos"], "venue": "2008 16th European Signal Processing Conference. IEEE, 2008, pp. 1\u20134.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonnegative matrix and tensor factorizations: An algorithmic perspective", "author": ["G. Zhou", "A. Cichocki", "Q. Zhao", "S. Xie"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 3, pp. 54\u201365, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Multilinear discriminant analysis for higherorder tensor data classification", "author": ["Q. Li", "D. Schonfeld"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 12, pp. 2524\u20132537, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable Bayesian low-rank decomposition of incomplete multiway tensors", "author": ["P. Rai", "Y. Wang", "S. Guo", "G. Chen", "D. Dunson", "L. Carin"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1800\u20131808.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L.R. Tucker"], "venue": "Psychometrika, vol. 31, no. 3, pp. 279\u2013311, 1966.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1966}, {"title": "PARAFAC. Tutorial and applications", "author": ["R. Bro"], "venue": "Chemometrics and intelligent laboratory systems, vol. 38, no. 2, pp. 149\u2013171, 1997.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Tensor decompositions for signal processing applications: From two-way to multiway component analysis", "author": ["A. Cichocki", "D.P. Mandic", "H.A. Phan", "C.F. Caiafa", "G. Zhou", "Q. Zhao", "L.D. Lathauwer"], "venue": "IEEE Signal Processing Magazine, vol. 32, no. 2, pp. 145\u2013163, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE 12th International Conference on Computer Vision. IEEE, 2009, pp. 2114\u20132121.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Rank regularization and bayesian inference for tensor completion and extrapolation", "author": ["J.A.G. Mateos", "G.B. Giannakis"], "venue": "IEEE Transactions on Signal Processing, no. 22, pp. 5689\u20135703, Nov. 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Subspace learning and imputation for streaming big data matrices and tensors", "author": ["M. Mardani", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Transactions on Signal Processing, vol. 63, no. 10, pp. 2663\u20132677, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalable tensor factorizations for incomplete data", "author": ["E. Acar", "D.M. Dunlavy", "T.G. Kolda", "M. M\u00f8rup"], "venue": "Chemometrics and Intelligent Laboratory Systems, vol. 106, no. 1, pp. 41\u201356, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Simultaneous tensor decomposition and completion using factor priors", "author": ["Y.-L. Chen", "C.-T. Hsu", "H.-Y.M. Liao"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 3, pp. 577\u2013591, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Tucker factorization with missing data with application to low-n-rank tensor completion", "author": ["M.F.A. Juki\u0107"], "venue": "Multidimensional Systems and Signal Processing, vol. 26, pp. 677\u2013692, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 208\u2013220, Jan. 2013.  10", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Generalized higherorder orthogonal iteration for tensor decomposition and completion", "author": ["Y. Liu", "F. Shang", "W. Fan", "J. Cheng", "H. Cheng"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1763\u2013 1771.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor factorization using auxiliary information", "author": ["A. Narita", "K. Hayashi", "R. Tomioka", "H. Kashima"], "venue": "Data Mining and Knowledge Discovery, vol. 25, no. 2, pp. 298\u2013324, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian CP factorization of incomplete tensors with automatic rank determination", "author": ["Q. Zhao", "L. Zhang", "A. Cichocki"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, no. 1, pp. 1\u20131, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian nonparametric models for multiway data analysis", "author": ["Z. Xu", "F. Yan", "Y. Qi"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, no. 2, pp. 475\u2013487, Feb. 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "On the best rank-1 and rank-(r1, r2, . . . ,  rn) approximation of higher-order tensors", "author": ["L.D. Lathauwer", "B.D. Moor", "J. Vandewalle"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 21, no. 4, pp. 1324\u2013 1342, 2000.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "Tensor rank is NP-complete", "author": ["J. H\u00e5stad"], "venue": "Journal of Algorithms, vol. 11, no. 4, pp. 644\u2013654, 1990.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1990}, {"title": "Rank, decomposition, and uniqueness for 3-way and n-way arrays", "author": ["Kruskal", "J. B"], "venue": "Multiway data analysis, vol. 33, pp. 7\u201318, 1989.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1989}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM review, vol. 51, no. 3, pp. 455\u2013500, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "author": ["S. Gandy", "B. Recht", "I. Yamada"], "venue": "Inverse Problems, vol. 27, no. 2, p. 025010, 2011.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Tensor completion via a multi-linear low-n-rank factorization model", "author": ["H. Tan", "B. Cheng", "W. Wang", "Y.-J. Zhang", "B. Ran"], "venue": "Neurocomputing, vol. 133, pp. 161\u2013169, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Over-relaxation of the fast iterative shrinkage-thresholding algorithm with variable stepsize", "author": ["M. Yamagishi", "I. Yamada"], "venue": "Inverse Problems, vol. 27, no. 10, p. 105008, 2011.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Enhancing sparsity by reweighted l1 minimization", "author": ["E.J. Cand\u00e9s", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier analysis and applications, vol. 14, pp. 877\u2013905, 2008.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Iteratively reweighted algorithms for compressive sensing", "author": ["R. Chartrand", "W. Yin"], "venue": "IEEE international conference on acoustics, speech and signal processing, 2008. ICASSP 2008. IEEE, 2008, pp. 3869\u2013 3872.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Iterative reweighted l1 and l2 methods for finding sparse solutions", "author": ["D. Wipf", "S. Nagarajan"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, no. 2, pp. 317\u2013329, 2010.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Exact reconstruction analysis of log-sum minimization for compressed sensing", "author": ["Y. Shen", "J. Fang", "H. Li"], "venue": "IEEE Signal Processing Letters, vol. 20, no. 12, pp. 1223\u20131226, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B (Methodological), pp. 267\u2013288, 1996.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1996}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 68, no. 1, pp. 49\u201367, 2006.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "A tutorial on MM algorithms", "author": ["D.R. Hunter", "K. Lange"], "venue": "The American Statistician, vol. 58, no. 1, pp. 30\u201337, 2004.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM journal on imaging sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "A multilinear singular value decomposition", "author": ["L.D. Lathauwer", "B.D. Moor", "J. Vandewalle"], "venue": "SIAM journal on Matrix Analysis and Applications, vol. 21, no. 4, pp. 1253\u20131278, 2000.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "Multi-dimensional data arise in a variety of applications, such as recommender systems [1]\u2013[3], multirelational networks [4], [5], and brain-computer imaging [6], [7].", "startOffset": 163, "endOffset": 166}, {"referenceID": 7, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 211, "endOffset": 214}, {"referenceID": 8, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 216, "endOffset": 219}, {"referenceID": 9, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 247, "endOffset": 251}, {"referenceID": 11, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 252, "endOffset": 256}, {"referenceID": 2, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 278, "endOffset": 281}, {"referenceID": 12, "context": "Compared with matrix factorization, tensor decomposition can capture the intrinsic multi-dimensional structure of the multiway data, which has led to a substantial performance improvement for harmonic retrieval [8], [9], regression/classification [10]\u2013[12], and data completion [3], [13], etc.", "startOffset": 283, "endOffset": 287}, {"referenceID": 13, "context": "Tucker decomposition [14] and CANDECOMP/PARAFAC (CP) decomposition [15] are two", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Tucker decomposition [14] and CANDECOMP/PARAFAC (CP) decomposition [15] are two", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "It is generally believed that Tucker decomposition has a better generalization ability than CP decomposition for different types of data [16].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "Low-rank decomposition of incomplete multiway tensors has attracted a lot of attention over the past few years and a number of algorithms [17]\u2013[28] were proposed via either optimization techniques or probabilistic model learning.", "startOffset": 138, "endOffset": 142}, {"referenceID": 27, "context": "Low-rank decomposition of incomplete multiway tensors has attracted a lot of attention over the past few years and a number of algorithms [17]\u2013[28] were proposed via either optimization techniques or probabilistic model learning.", "startOffset": 143, "endOffset": 147}, {"referenceID": 28, "context": "the minimum number of rank-one terms in CP decomposition, is an NP-hard problem even for a completely observed tensor [29].", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "To address this issue, a Bayesian method was proposed in [13] for CP decomposition, where a shrinkage prior called as the multiplicative gamma process (MGP) was employed to adaptively learn a concise representation of the tensor.", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "In [26], a sparsity-inducing Gaussian inverse-Gamma prior was placed over multiple latent factors to achieve automatic rank determination.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Besides the above Bayesian methods, an optimization-based CP decomposition method was proposed in [18], [19], where the Frobenius-norm of the factor matrices is used as the rank regularization to determine an appropriate number of component tensors.", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "Besides the above Bayesian methods, an optimization-based CP decomposition method was proposed in [18], [19], where the Frobenius-norm of the factor matrices is used as the rank regularization to determine an appropriate number of component tensors.", "startOffset": 104, "endOffset": 108}, {"referenceID": 29, "context": "In addition to the CP rank, another notion of tensor rank is multilinear rank [30], which is defined as the tuple of the ranks of the mode-n unfoldings of the tensor.", "startOffset": 78, "endOffset": 82}, {"referenceID": 30, "context": "Multilinear rank is closely related to the Tucker decomposition since the multilinear rank is equivalent to the dimension of the smallest achievable core tensor in Tucker decomposition [31].", "startOffset": 185, "endOffset": 189}, {"referenceID": 16, "context": "Specifically, an alternating direction method of multipliers (ADMM) was developed in [17], [23] to minimize the tensor", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "Specifically, an alternating direction method of multipliers (ADMM) was developed in [17], [23] to minimize the tensor", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 86, "endOffset": 90}, {"referenceID": 32, "context": "The success of [17] has inspired a number of subsequent works [21], [22], [24], [25], [32], [33] for tensor completion and decomposition based on tensor nuclear norm minimization.", "startOffset": 92, "endOffset": 96}, {"referenceID": 20, "context": "Nevertheless, the tensor nuclear norm, albeit effective, is not necessarily the tightest convex envelope of the multilinear rank [21].", "startOffset": 129, "endOffset": 133}, {"referenceID": 33, "context": "Also, the over-relaxed monotone fast iterative shrinkage-thresholding technique [34] is adapted and embedded in the iterative reweighted process, which achieves a substantial reduction in computational complexity.", "startOffset": 80, "endOffset": 84}, {"referenceID": 34, "context": "Log-sum penalty function has been extensively used for sparse signal recovery and was shown to be more sparsity-encouraging than the l1-norm [35]\u2013[38].", "startOffset": 141, "endOffset": 145}, {"referenceID": 37, "context": "Log-sum penalty function has been extensively used for sparse signal recovery and was shown to be more sparsity-encouraging than the l1-norm [35]\u2013[38].", "startOffset": 146, "endOffset": 150}, {"referenceID": 38, "context": "This is different from the group-LASSO [39], [40] method in which entries are grouped into a number of non-overlapping subsets with sparsity imposed on each subset.", "startOffset": 39, "endOffset": 43}, {"referenceID": 39, "context": "This is different from the group-LASSO [39], [40] method in which entries are grouped into a number of non-overlapping subsets with sparsity imposed on each subset.", "startOffset": 45, "endOffset": 49}, {"referenceID": 30, "context": "It can be shown that n-rank is equivalent to the dimensions of the smallest achievable core tensor in Tucker decomposition [31].", "startOffset": 123, "endOffset": 127}, {"referenceID": 40, "context": "We resort to a bounded optimization approach, also known as the majorization-minimization (MM) approach [41], to solve the optimization (14).", "startOffset": 104, "endOffset": 108}, {"referenceID": 41, "context": "One famous first order method is the fast iterative shrinkagethresholding algorithm (FISTA) [42].", "startOffset": 92, "endOffset": 96}, {"referenceID": 33, "context": "Later on in [34], an over-relaxed monotone FISTA (MFISTA) was proposed to overcome some limitations inherent in the FISTA.", "startOffset": 12, "endOffset": 16}, {"referenceID": 33, "context": "It was proved in [34] that the sequence {x} is guaranteed to monotonically decrease the objective function F (x) and the convergence rate is O(1/k).", "startOffset": 17, "endOffset": 21}, {"referenceID": 42, "context": "The factor matrices and core tensor are initialized by decomposing the observed tensor (the missing elements are set to zero) with high order singular value decomposition [43].", "startOffset": 171, "endOffset": 175}, {"referenceID": 17, "context": "We compare our method with several existing state-of-theart tensor decomposition/completion methods, namely, a CP decomposition-based tensor completion method (also referred to as the low rank tensor imputation (LRTI)) which uses the Frobenius-norm of the factor matrices as the rank regularization [18], a tensor nuclear-norm based tensor completion method [23] which is also referred to as the high accuracy low rank tensor completion (HaLRTC) method, and a Tucker factorization method based on pre-specified multilinear rank [22] which is referred to as the WTucker method.", "startOffset": 299, "endOffset": 303}, {"referenceID": 22, "context": "We compare our method with several existing state-of-theart tensor decomposition/completion methods, namely, a CP decomposition-based tensor completion method (also referred to as the low rank tensor imputation (LRTI)) which uses the Frobenius-norm of the factor matrices as the rank regularization [18], a tensor nuclear-norm based tensor completion method [23] which is also referred to as the high accuracy low rank tensor completion (HaLRTC) method, and a Tucker factorization method based on pre-specified multilinear rank [22] which is referred to as the WTucker method.", "startOffset": 358, "endOffset": 362}, {"referenceID": 21, "context": "We compare our method with several existing state-of-theart tensor decomposition/completion methods, namely, a CP decomposition-based tensor completion method (also referred to as the low rank tensor imputation (LRTI)) which uses the Frobenius-norm of the factor matrices as the rank regularization [18], a tensor nuclear-norm based tensor completion method [23] which is also referred to as the high accuracy low rank tensor completion (HaLRTC) method, and a Tucker factorization method based on pre-specified multilinear rank [22] which is referred to as the WTucker method.", "startOffset": 528, "endOffset": 532}], "year": 2015, "abstractText": "We consider the problem of low-rank decomposition of incomplete multiway tensors. Since many real-world data lie on an intrinsically low dimensional subspace, tensor low-rank decomposition with missing entries has applications in many data analysis problems such as recommender systems and image inpainting. In this paper, we focus on Tucker decomposition which represents an N th-order tensor in terms of N factor matrices and a core tensor via multilinear operations. To exploit the underlying multilinear low-rank structure in high-dimensional datasets, we propose a group-based log-sum penalty functional to place structural sparsity over the core tensor, which leads to a compact representation with smallest core tensor. The method for Tucker decomposition is developed by iteratively minimizing a surrogate function that majorizes the original objective function, which results in an iterative reweighted process. In addition, to reduce the computational complexity, an over-relaxed monotone fast iterative shrinkage-thresholding technique is adapted and embedded in the iterative reweighted process. The proposed method is able to determine the model complexity (i.e. multilinear rank) in an automatic way. Simulation results show that the proposed algorithm offers competitive performance compared with other existing algorithms.", "creator": "LaTeX with hyperref package"}}}