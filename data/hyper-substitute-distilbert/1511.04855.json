{"id": "1511.04855", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Deep Learning for steganalysis is better than a Rich Model with an Ensemble Classifier, and is natively robust to the cover source-mismatch", "abstract": "since original boss pattern, in europe, most regression approaches use a learning task involving orthogonal systems : partial extraction, such with the rich models ( rm ), determining the vector representation, and use of the ensemble encoding ( ec ) for the recovery step. in 1984, qian op al. uniformly criticized that the algorithms of intrinsic deep learning pattern frequently jointly learns and computes cognitive memory, deemed very successful helping the brain. abandoning this paradigm, we followed - up the study of qian et ar., and show that, applied to intrinsic dynamic reasoning, weak results obtained from conditioned convolutional neural distribution ( st ) produce a fully fledged neural network ( vt ), if primarily understood, surpass the conventional framework of weighted rm including an ec. instead, numerous experiments consistently conducted across order to find calculated best \" shape \" of the cnn. hence, experiments were carried out in the clairvoyant scenario in order only predict the cnn underlying fnn to an rm with an op. autopsy results show russians satisfying 130 % reduction in the sleep phase with our cnn matching fnn. third, but previously also performed in a cover - source market setting. the results yielded better the jerk thinks fnn sounded naturally robust than the behavioral problem. \u2026 addition every statistical assumption, humans possess independently ourselves own internal philosophy of finding cnn, and incorporate links with some some undisclosed ideas, in order better open about alternative results we obtained.", "histories": [["v1", "Mon, 16 Nov 2015 07:59:14 GMT  (361kb,D)", "http://arxiv.org/abs/1511.04855v1", "IS&amp;T. Media Watermarking, Security, and Forensics, Part of IS&amp;T International Symposium on Electronic Imaging, EI'2016, Feb 2015, San Fransisco, United States"]], "COMMENTS": "IS&amp;T. Media Watermarking, Security, and Forensics, Part of IS&amp;T International Symposium on Electronic Imaging, EI'2016, Feb 2015, San Fransisco, United States", "reviews": [], "SUBJECTS": "cs.MM cs.CV cs.LG cs.NE", "authors": ["lionel pibre", "pasquet j\\'er\\^ome", "dino ienco", "marc chaumont"], "accepted": false, "id": "1511.04855"}, "pdf": {"name": "1511.04855.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for steganalysis is better than a Rich Model with an Ensemble Classifier, and is natively robust to the cover source-mismatch", "authors": ["Lionel PIBRE", "J\u00e9r\u00f4me PASQUET", "Dino IENCO", "Marc CHAUMONT"], "emails": ["marc.chaumont}@lirmm.fr"], "sections": [{"heading": null, "text": "Since the BOSS competition, in 2010, most steganalysis approaches use a learning methodology involving two steps: feature extraction, such as the Rich Models (RM), for the image representation, and use of the Ensemble Classifier (EC) for the learning step. In 2015, Qian et al. have shown that the use of a deep learning approach that jointly learns and computes the features, is very promising for the steganalysis.\nIn this paper, we follow-up the study of Qian et al., and show that, due to intrinsic joint minimization, the results obtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural Network (FNN), if well parameterized, surpass the conventional use of a RM with an EC.\nFirst, numerous experiments were conducted in order to find the best \u201dshape\u201d of the CNN. Second, experiments were carried out in the clairvoyant scenario in order to compare the CNN and FNN to an RM with an EC. The results show more than 16% reduction in the classification error with our CNN or FNN. Third, experiments were also performed in a cover-source mismatch setting. The results show that the CNN and FNN are naturally robust to the mismatch problem.\nIn Addition to the experiments, we provide discussions on the internal mechanisms of a CNN, and weave links with some previously stated ideas, in order to understand the impressive results we obtained.\n1.Introduction\nThe state-of-the-art for steganalysis currently consists of using a two-step machine-learning methodology.\nThe first step requires the extraction of features describing the image. Those features must be diverse [1], which means that they should capture the maximum of information modeling the image, and they also should be complete [2], which means that their values should be different between a cover and a stego. The best feature set to represent an image has so far been supplied by Rich Models [3].\nThe second step consists of learning to distinguish cover model from the stego(s) model(s). Depending on the memory or computation requirements, the steganalyst can use an SVM [4], an Ensemble Classifier [5], or a Perceptron [6].\nWhen analyzing the empirical security of an embedding al-\ngorithm in a laboratory environment [7], we select the \u201dclairvoyant scenario\u201d [8], i.e. we suppose that the steganalyst knows the algorithm and the payload size that has been used by the steganographer, and has good knowledge of the cover distribution through a set of images of the same type as those used by the steganographer.\nUntil 2015, in the clairvoyant scenario, the best classifier was the Ensemble Classifier [5]. This classifier is able to treat high dimensional vectors, is easily parallelizable, and has a smaller computational complexity than SVM. Moreover, some improvements have been proposed in order to increase its efficiency, such as the use of embedding probabilities in order to better steganalyze the adaptive algorithms [9], tuning of false alarm probability [10], or treating the cover-source mismatch problem [11], where the best classifier is also the Ensemble Classifier [12].\nYet, in recent years, in different areas, the use of deep learning networks challenges traditional two step approaches (feature extraction, and use of a classifier) [13].\nIn the steganalysis field, Qian et al. [14] proposed, in 2015, to use deep learning to replace the traditional two step approach. In their article, Qian et al. obtained a detection percentage of only 3% to 4% lower than that obtained with the Ensemble Classifier [5], and SRM features [3]. The tested algorithms were HUGO [15], WOW [16], and S-UNIWARD [17], on the BOSSbase database [18]. Those first results were encouraging since the study was only a proof of concept, and because the feature vector dimension did not compare favorably with respect to deep learning. Indeed, the dimension of the feature vector of the last convolution layer (layer 5) provides only 256 features, whereas the SRM (Spatial Rich Models) dimension of the feature vector provides 34671 [3].\nIn this article, we pursue the study of the steganalysis via deep learning. After many months of experiments, we obtained a reduction of more than 16% in the classification error compared to the state of the art. We also found a network that is robust to the cover-source mismatch.\nThe network we built is very different from that of Qian et al. [14]. In Section 2, we review the major concepts of a Convolutional Neural Network. In Section 3, we introduce the experimental settings, describe the \u201dshape\u201d of our best CNNN, and we present steganalysis results in both scenarios: clairvoyant and cover-source mismatch. Finally, in Section 4, we discuss the link\nar X\niv :1\n51 1.\n04 85\n5v 1\n[ cs\n.M M\n] 1\n6 N\nov 2\nbetween the network construction and steganalysis research, and we explain our remarkable results.\n2.Convolutional Neural Network\nNeural networks have been studied since the fifties. Initially, they were proposed to model the brain behavior. In computer science, especially in artificial intelligence, they have been used for 30 years for learning purposes. Until recently [19], neural networks were considered as having a too long learning time, and as being less efficient than modern classifiers.\nRecently, due to recent advances in the neural network field [20], and to the computational power supplied by GPUs, deep learning approaches have been proposed as a natural extension of neural networks, and they are getting popular due to their high classification performance. Deep learning networks are big neural networks that can directly take data as input. In image processing, the network is directly fed with pixels. A deep learning network handles two steps at once (feature extraction and classification). Since 2006 [19], many adjustments have been proposed to improve the robustness and reduce the computational costs.\nIn this paper, we recall the major concepts of a Convolutional Neural Networks (CNN), which is a deep learning network that has proved its efficiency in image classification competitions [13], and that was used by Qian et al. for steganalysis purposes.\nThe learning methodology is similar to the classical one. An image database is needed, with, for each image, its label (i.e. its class). Each image is given as network input; in that case, each pixel value is taken as input of one or many neurons. The network is made of a given number of layers. A layer consists of neurons that take input values, do some computations, and then returns values that are supplied to the next layer.\nF(0) = 1 12  \u22121 2 \u22122 2 \u22121 2 \u22126 8 \u22126 2 \u22122 8 \u221212 8 \u22122 2 \u22126 8 \u22126 2 \u22121 2 \u22122 2 \u22121  (1)\nAs an illustration, Figure 1 gives the network used by Qian et al. For this network, an image of size 256\u00d7256 is first filtered with a high-pass filter whose kernel is denoted F(0), and size is 5\u00d7 5 (see Eq. 1). Note that this preliminary mandatory step is specific to the steganalysis problem. We observed that CNNs do not converge without this preliminary high-pass filtering. Then, the filtered image, of size 252\u00d7 2521, is given to the first layer. In the Qian et al. network (see Figure 1), there are 5 convolution layers.\nLet us now explain more precisely what a layer is in a Convolutional Neuronal Network.\n2.1.Layer\nA layer is made of neurons that take input values, do some computations, and then returns values that are supplied to the next layer. More precisely, inside a layer, computations are done in\n1The filtered image is smaller than the original image because there is no padding.\nthree successive steps: a convolution step (see Section 2.2), the application of an activation function (see Section 2.3), and then a pooling step (see Section 2.4). Note that the outputs of a layer could be considered as a set of images. In CNN terminology, each image is named a feature map.\nIn Fig. 1, representing the Qian et al. network, the first layer generates 16 feature maps, each of size 124\u00d7124. Note that this means that there are 16 filters and thus 16 convolutions which are applied to the input image of size 252\u00d7252. From the second to the fifth layer, there are the same three steps: convolution, activation, and pooling, but this time the convolutions are applied to all feature maps. We discuss in further detail how the convolutions and sub-sampling are fulfilled in the next subsection.\nThe last convolution layer is connected to a fully connected two layer neuronal network. Then, a softmax function is connected to the outputs of the last layer in order to normalize the two outputs delivered by the network between [0,1]. The softmax function gives the predicted probability of belonging to a class, while knowing the weighted outputs from the last layer. Thus, the network delivers two values as output: one giving the probability of classifying into the first class (e.g. the cover class), and the other giving the probability of classifying into the second class (e.g. the stego class). The classification decision is obtained by returning the class with the highest probability.\nA Convolutional Neural Network, similar to a classical neuron network, needs a long learning time in order to tune each unknown parameter. In the case of Qian et al. network, the number of unknown parameters is close to 63 000, which indeed is not very large since convergence requires less than 2 hours with GPU programming on a Nvidia Tesla K80. Learning is achieved with the well known back-propagation algorithm. Roughly speaking, back-propagation of the error is equivalent to a gradient descent which is a well-known function optimization technique.\nNetwork learning can thus be seen as the optimization of a function, with lots of unknown parameters, through the use of a well thought stochastic gradient descent. Due to the huge number of parameters to learn, the neural network needs a database that has a considerable number of examples in order to converge. Moreover, the database examples must be diverse enough to obtain a good generalization of the network.\nLet us now explain each step inside a layer of a Convolutional Neural Network more in detail: convolution, activation, and pooling.\n2.2.Convolution\nFor a given layer and a set of feature map as input, the first processing consist of applying the convolutions.\nFor the first layer, the convolution is trivial since there is only one image as input. Convolution is done between the input image and a filter. In the Qian et al. network, there are 16 filters (see Figure 1). Each filter leads to a filtered image. Then the second (activation function; see Section 2.3), and third (pooling; see Section 2.4) steps are applied, leading to a new image named a feature map. In the Qian et al. network, there are 16 feature maps at the output of the first layer.\nFormally, let I(0) denote the image given to the CNN (note that for the Qian et al. network, image I(0) is a high-pass filtered image; see Section 2 and Figure 1). Let F(l)k denote the k th filter\nfrom layer l = {1, ...,L}, with L beeing the number of convolutional layers, and k \u2208 {1, ...,K(l)}, with K(l) beeing the number of filters of the lth layer (K(l) is also the number of feature map outputs by the lth layer). A convolution from the first layer with the kth filter leads to a filtered image, denoted I\u0303(1)k , such that:\nI\u0303(1)k = I (0) ?F(1)k . (2)\nFigure 2 gives an example of 64 filter kernels obtained with the most efficient network we obtained. Note that the filter looks like oriented band-pass filters.\nFrom the second layer to the last convolution layer, the \u201dconvolution\u201d is less classical since there are K(l\u22121) feature maps (K(l\u22121) images) as input, denoted I(l\u22121)k with k = {1, ...,K\n(l\u22121)}. The \u201dconvolution\u201d that will lead to the kth filtered image, I\u0303(l)k , resulting from the convolution layer numbered l, is in fact the sum of K(l\u22121) convolutions, such that:\nI\u0303(l)k = i=K(l\u22121)\n\u2211 i=1\nI(l\u22121)i ?F (l) k,i , (3)\nwith {F(l)k,i } i=K(l\u22121) i=1 a set of K (l\u22121) filters for a given k value. This operation is quite unusual since each feature map is obtained by a sum of K(l\u22121) convolutions with a different filter for each convolution. There is no similar operation in the classical feature extraction process supplied by Rich Models [3], and to the best of our knowledge, in spatio-frequential decomposition. Figure 3 gives an example of the 1024 filter kernels obtained by the second layer of the most efficient network we obtained.\nRemember that a convolution layer is made of three steps i.e. convolution, activation, and pooling. These three consecutive steps can be summarized by looking at the link between a feature\nmap from a layer to the previous one:\nI(l)k = pool\n( f ( b(l)k + i=K(l\u22121)\n\u2211 i=1\nI(l\u22121)i ?F (l) k,i\n)) , (4)\nwith b(l)k \u2208 R being a scalar used for setting a bias to the convolution, f () being the activation function applied pixel per pixel to the filtered image, and the pooling, pool(), which pools a local neighborhood (see Section 2.4).\nNote that the filter kernels (also referred as weights), and the bias have to be learned and are modified during back-propagation of the error. Thus, for a layer l \u2208 {1, ...,L}, and for a number of filters per layer, K(l), with a convolution kernel made of |F(l)| weights, there is a total number of\nl=L \u2211 l=1 K(l)\u00d7 (1+ |F(l)|)\nunknown parameters to be tuned during back-propagation for the convolutional layer part. As an example, for the Qian et al. network, the number of parameters coming from the five convolution layers is equal to 16 filters \u00d7(1+5\u00d75)+3\u00d7 (1+3\u00d73\u00d716)+ (1+5\u00d75\u00d716)) = 13792 unknown parameters coming from the convolution weights and the bias (and a total of 63456 unknown parameters for the entire network if we include the fully connected layers and the softmax).\nNote that the complexity computation (i.e. the number of multiplications, or the number of addition minus 1) for the convolution layers of the network is close to2:\nl=L \u2211 l=1 K(l)\u00d7|I(l\u22121)|\u00d7 (1+ |F(l)|),\n2The image borders are not always filtered, which actually gives a smaller number of operations.\nwith K(l) being the number of \u201dconvolutions\u201d per layer, |I(l\u22121)|, the size of a feature map from layer l\u22121, and |F(l)| the number of weights for a convolution. Note that this complexity does not take into account the activation operation cost, the pooling operation cost, and the normalization operation cost.\nThose computations are very classical and can be accelerated, for example through a fast convolution in the Fourier domain, or through the use of parallel computation. Thus, when using the network, if the size of the input image, the number of filters, the size of the filters, and the number of layers are not too big, the convolution computation cost, with the pooling and the normalization included, is more or less O(L\u00d7K(0)\u00d7I(0)\u00d7|F(2)|), which is not very high.\nAs an example, for the Qian et al. network, this complexity for the convolution part of the network would give more or less 5\u00d7 16\u00d7 (252\u00d7 252)\u00d7 (3\u00d7 3\u00d7 16) additions or multiplications, which is less than 700 Mega operations. This is very small, for example, for a CPU Intel Core i7 which can compute at more than 50 GigaFLOPS, or for a GPU Nvidia Tesla K80 which can compute at more than one TeraFLOPS. Even when looking at the entire network, the computational cost is not very high. The long learning times are due to the fact that those operations have to be done on a big database, which has to be scanned many times in order that the back-propagation process does converge the network.\n2.3.Activation\nOnce each convolution from a convolution layer has been applied, an activation function, f () (see Eq. 4), is applied to each value of the filtered image, I\u0303(l)k (Eq. 2 and Eq. 3). This function is named the activation function in reference to the notion of binary activation in the first network neuron definition. The activation function may for example be an absolute function f (x) = |x|, a sine function f (x) = sinus(x), a Gaussian function as in the Qian et al. network f (x) = e \u2212x2\n\u03c3 2 , a ReLU (for Rectified Linear Units): f (x) = max(0,x), etc...\nThose functions break the linearity property resulting from the linear filtering done during the convolutions. This is usually an interesting property that is exploited in the Ensemble Classifier through the majority vote [5], and that is also used in the Rich Models with the Min-Max features [3]. The choice of the activation function is linked to the classification problem. For example, Qian et al. proposed to use an unusual Gaussian function. Note that the chosen function should be derivable in order to compute the back-propagation error. The derivative could be more or less computationally costly, which has an impact on the learning time. The choice of the activation function is thus often guided by this computational criteria. During the experiments, we observed that the best results were obtained with the ReLU activation function.\n2.4.Pooling\nThe pooling operation consists of computing the average or maximum on a local neighborhood. In the object classification field, this operation, and especially the use of a maximum, ensures a translation invariance of the features. It was introduced in order to reduce the variance obtained during convolution. Qian et al. propose to use the average operation because the stego noise is\nvery small. We also empirically validated this fact in our experiments. The results obtained using the average outperform the one obtained by the maximum operation.\nMoreover, the pooling is coupled with a sub-sampling operation in order to reduce the size of the obtained feature map in comparison to the size of those of the previous layer. In the article of Qian et al., there is a reduction factor of four between the feature map size of a layer and that from the previous layer. This can then be seen as a classical down-sampling with preliminary low pass filtering. This is useful to reduce the memory occupation in the GPU. Nevertheless, this step is similar to denoising, and from a signal processing point of view, it induces information loss. This pooling step does not seem interesting for a steganalysis, and indeed we experimentally observed that suppressing the pooling operation increased the classification results by more than 8%. In return, suppressing the pooling step gives feature maps of a constant size in all the layers, and this leads to an increase in the computational cost, and an increase in GPU memory consumption.\n3.Experiments\n3.1.The databases and learning settings used\nFor the experiments, we first took the database BOSSBase v1.0 [18] consisting of 10 000 grey-level images of size 512\u00d7 512 coming from 7 different cameras, then we split each image in four in order to obtain 40 000 images of size 256\u00d7256. We named this database the cropped BOSSBase database. In their article, Qian et al. also reduced the images sizes due to the GPU memory limitation. Note that they applied image resizing instead of the image cropping.\nWe also created a second database that we named LIRMMBase3 . The LIRMMBase database consists of 1008 greylevel images, coded on 8 bits, of size 256\u00d7 256. There are 6 cameras, none of them present in BOSSBase, and 168 images per camera. This database allows evaluation of the cover-source mismatch phenomenon. Note that readers may find different versions of the LIRMMBase on its hosting website: the color version, LIRMMBaseColor, with 15320 images, the 512\u00d7512 grey-level image database, LIRMMBase512x512, with 252 images, and the LIRMMBase256x256 database with 1008 grey level images. The images come from well-known databases i.e. Columbia, Dresden, Photex, and Raise databases. Note that we used the same script as that used for the BOSSBase in order to transform the RAW full resolution color images into grey-level images4.\n3 If researchers use this free-access database, they are required to cite as follows: \u201d LIRMMBase: A database built from a mix of Columbia, Dresden, Photex, and Raise databases, and whose images do not come from the same cameras as the BOSSBase database ; For the first time used in [ref]. \u201d L. Pibre, J. Pasquet, D. Ienco, and M. Chaumont, LIRMM Laboratory, Montpellier, France, June 2015, Website: www.lirmm.fr/~chaumont/LIRMMBase.html. [ref] Lionel Pibre, Je\u0301ro\u0302me Pasquet, Dino Ienco, and Marc Chaumont, \u201dDeep Learning for steganalysis is better than a Rich Model with an Ensemble Classifier, and is natively robust to the cover source-mismatch,\u201d in Proceedings of Media Watermarking, Security, and Forensics, Part of IS&T International Symposium on Electronic Imaging, EI\u20192016, San Francisco, California, USA, 14-18 Feb. 2016, 10 pages.\n4The script converts the full resolution RAW color images into RGB images, then resizes the images such that the smaller side is 512 pix-\nIn our experiments, we embeded the messages (using the simulator) with S-UNIWARD [17] at 0.4 bits per pixel. After embedding, the database obtained from BOSSBase consisted of 80 000 images (40 000 covers and 40 000 stegos), and the database from LIRMMBase consisted of 2 016 images (1008 covers and 1008 stegos). We limited our experiments to this payload size due to the high number of computations, and experiments that we have led on the CNN. More than 40 CNN were tested with many parameter variations. Figure 4 illustrates the different values of the probability of error for some of the tested CNN.\nDue to the high number of parameters, it is necessary to have a high number of iterations of the back-propagation process in order for the CNN to converge. In our experiments, the number of times the database is scanned was between 100 and 200. With a learning database consisting of 60 000 images of size 256\u00d7256 this leads to a learning time that is less than one day (sometimes 2 hours, sometimes more, depending on the parameters and the charge on the GPU) with the most efficient double precision GPU card on the market on June 2015, i.e. the Nvidia Tesla K80. It thus takes more than one-month of computation in order to find our \u201dbest\u201d network. For a reproducibility of experiments, the main parameters are given: the \u201dmini-batch\u201d size is 128, the \u201dmoment\u201d is 0.9, the \u201dlearning coefficient\u201d is 0.001 for weights and 0.002 for bias, the \u201dweight decay\u201d is 0.004 for convolutions layers and 0.01 for the fully connected network, the \u201ddrop out\u201d is not activated.\nThe paper essentially demonstrates that a CNN is more efficient than an Ensemble Classifier, or an Ensemble Classifier informed on the selection channel (adaptive steganalysis scenario) [21, 9]. Moreover, the paper also shows that CNN exhibits a surprising invariance property with respect to the cover-source mismatch, which is currently, a phenomenon that we are not fully able to explain. For our defense, the deep learning performances in many other topics are not well explained.\n3.2.The shape of our best CNN\nWe tested many CNN (as shown in Figure 4), and the most efficient network we obtained consisted of only two convolutional layers, followed by a three layer fully connected network. Figure 5 illustrates this network. The input image, of size 256\u00d7 256, is first high-pass filtered with the same filter as Qian et al. (see Eq.\nels long, then crops the images to 512\u00d7 512, and finally converts them to greyscale 8-bit images; the script may be found at www.lirmm.fr/ ~chaumont/LIRMMBase.html.\n1; and discussion in Section 2). The size of the filtered image is thus 252\u00d7252.\nFor the first layer, 64 filters of size 7\u00d7 7 are applied. Figure 2 illustrates those 64 filters after a learning on 60 000 images (30000 covers and their 30000 associated stegos) from the cropped BOSSBase. As already stated, those filters seem to act as oriented band-pass filters. Note that, for memory constraints, convolution is only applied 1 pixel above 2 for lines and columns (the \u201dstride\u201d parameter is equal to 2) which leads to filtered images of size 127\u00d7 1275. Note that in comparison to the Qian et al. network, we drastically increased the number of filters in the first layer and reduced the number of CNN layers. The increase in height is one of the reasons why our network generates better results compared to the Qian et al. architecture. The increase in layer number leads to a loss of information, probably due to the negative impact of the pooling step (sub-sampling).\nAfter 64 convolutions, the ReLU activation function is applied (see Section 2.3); this function forces the values to be positive. Note that the use of a Gaussian activation function, as in the article of Qian et al., does not improve the results. As already discussed, the activation breaks the linearity of the successive convolutions applied during the traverse of the convolutional layers.\nNote that we suppressed pooling because this step was counter-productive. The 64 feature maps returned by the first layer, are thus of size 127\u00d7127. Finally, the last process done by the layer is a normalization of the feature maps applied on each value, i.e on each position (x,y) of I(1)k for each k \u2208 {1, ...,K\n(1)} with K(1) = 64. The normalization is done across the maps, which is useful when using unbounded activation functions such as ReLU6:\nnorm(I(1)k (x,y)) =\nI(1)k (x,y)( 1+ \u03b1size k\u2032=min(K,k\u2212bsize/2c+size) \u2211\nk\u2032=max(0,k\u2212bsize/2c) (I(1)k\u2032 (x,y)) 2 )\u03b2 5For this first layer, the convolution with a filter size 7\u00d77 on an image size of 252\u00d7 252 virtually leads to an image size of 253\u00d7 253, because we selected the padding option and because the stride value of 2 implies image down-sampling, which gives an image size of 127\u00d7127.\n6The normalization is done with a \u201dlocal response normalization layer\u201d. This enables detection of high-frequency features with a high neuron response, while damping responses that are uniformly large in a local neighborhood. This is a type of regularizer that encourages \u201dcompetition\u201d for big activities among nearby groups of neurons. From the Cudaconvnet documentation.\nwith \u03b1 and \u03b2 , and size, the parameter set with default values7. The 64 feature maps of size 127\u00d7127 entering in the second\nlayer are padded in order to obtain feature maps of size 131\u00d7131. Next, 16 \u201dconvolutions\u201d are applied, as explained in Section 2.2. Equation 3 recalls this particular convolution step, where each sum of convolutions can be seen as the research of local signals in each of the feature maps through computation of the correlations between the researched signals and the feature maps. This step, and the normalization steps, probably explain the robustness to the cover-source mismatch that we observed during the experiments.\nFigure 3 illustrates those 16\u00d7 64 filters after a learning on 60 000 images (30000 covers and their 30000 associated stegos) from the cropped BOSSBase. ReLU activation is employed after applying the convolutions with those filters, and then the normalization is performed again. This leads to 16 feature maps of size 127\u00d7 127 each. Note that concatenating all feature maps values leads to a feature vector consisting of 258 064 features, which is 7 times more than the SRM which consists of 34 671 features8\nAfter the two convolution layers, there is a fully connected three layer neural network. The first and the second layers consist of 1000 neurons, and the last layer only has 2 neurons. The operations carried out in the first and second layers are dot products, bias additions, and the applications of the ReLU activation function. The operations performed in the last layer are dot products, bias additions, and then a softmax in order to rescale the output values in [0,1].\n3.3.Clairvoyant scenario\nOur first test is in the clairvoyant scenario. In this scenario, we put forward the hypothesis that the steganalyst knows the embedding algorithm, has good knowledge of the statistical distribution of the image database used by the steganograph, and knows the relative payload size. This scenario almost matches the Kerckhoffs principle. The steganalyst knows all the public parameters (in the clairvoyant scenario, the selection channel is assumed to be inaccessible), and does not know the private parameters such as the embedding secret keys. This scenario is a laboratory scenario used to empirically assess the security of a steganographic embedding algorithm [7].\nOur tests were carried out on the cropped BOSSBase database, which consists of 40 000 grey-level images on 8 bits and whose size is 256\u00d7256. We embed with S-UNIWARD [17] at 0.4 bits per pixels. The obtained set of images is thus made of 80 000 images (40 000 covers and 40 000 stegos). We limited our experiments to a single payload size due to the high number of computations, and the high number of experiments on the CNNs.\nThree steganalysis approaches were evaluated. The first steganalysis was done using Rich Models [3] and an Ensemble Classifier [5]. The Rich Models is the SRM whose dimension is 34671. We will denote this steganalysis, RM+EC, for Rich Models and Ensemble Classifier. The second steganalysis was done using the most efficient CNN we built (see Fig. 5). We denote this steganalysis, CNN, for Convolutional Neural Network. The\n7\u03b1=0.001, \u03b2=0.75, and size=9. 8Note that we also tested a CNN with a feature vector of dimension similar to SRM, by keeping the pooling steps, and 64 filters per layer, and we obtained a probability of error 10% better than with an EC with SRM.\nthird steganalysis was done using the a Fully Connected Neural Network (see Fig. 6). We denote this steganalysis, FNN, for Fully-Connected Neural Network.\nFor each payload size, we conducted 10 tests where, for each test, the learning was done on 60 000 images randomly taken from the 80 000 images, such that the covers and stegos are always paired. The tests were performed on 10 000 images (5000 covers and 5000 associated stegos) randomly taken from the 20 000 remaining images. For the Ensemble Classifier, the decision threshold of each base learner was adjusted to minimize the total detection error under equal priors on the training set9. For CNN, and FNN, the sum of the errors was minimized on the training set, as a gradient descent:\nmin i=NT R\n\u2211 i=1\nl=1 \u2211 l=0 (hl(xi)\u2212 yi)2, (5)\nwith NT R being the number of training images, h0(.) (resp. h1(.)), the CNN outputs giving a classification score for cover (resp. stego), yi \u2208 {0,1} the label (0 for cover, and 1 for stego) for the ith input image xi.\nFor both EC+RM, CNN, and FNN, the probability of error was computed and averaged over the 10 tests. The results are summarized in Table 1.\nRich Models with the Ensemble Classifier gives a probability of error of 24.67%, whereas CNN gives a probability of error of\n9We used two different Ensemble Classifiers. The one given on the Binghamton website10; in that case the learning set only consists of 10 000 images, and the one written in C++ and proposed in the paper [11] with the features selection, and parallelization options; in that case, the learning set consists of 60 000 images. For those two cases, the results were similar, and we only report here the results obtained with the Binghamton code.\nonly 7.4%, and FNN gives a probability of error of 8.75%. There is thus more than 17% improvement by using our CNN, and more than 15% with our FNN. This is a very impressive improvement considering the difficulty to grab percentages on the probability of error in steganalysis.\nA first tentative explanation of the good behavior of the CNN has already been given in the previous Sections.\nOur CNN generates better results than Rich Models associated with an Ensemble Classifier for the following reasons:\ni) the shape of the CNN is well chosen,\nii) the learning process is done through a single global optimization,\niii) the filter kernels are optimized (which is not the case in Rich Models),\niv) there is a high number of filters (64 filters for the first layer) which enriches the diversity,\nv) the second convolutional layer, which comes after what looks like a spatio-frequencial decomposition (obtained via the first layer), seems to act as the research of the presence of signals in all the spatio-frequential bands,\nvi) we eliminated the pooling step (compared to the Qian et al. network) which was counter-productive because it was acting as a down-sampling, and thus leading to information loss.\nNote that a less structured network such as FNN gives impressive results even though it is less efficient than CNN. Note that the unknowns number for CNN is around 259 million (only 29 824 unknowns for the two convolutions layers, and 259 million for the fully connected layers ), and for FNN is around 131 millions.\nWe also carried out an additional experiment that confirmed that the first two CNN layers play a strong role in classification. The experiment first consisted of \u201dcutting\u201d another \u201dsmaller\u201d CNN (with a pooling step) that was previously learned, by only keeping the two convolution layers. The network was then nothing more than a feature extractor; in this experiment there were only 57600 features. Second, this feature extractor was used to extract, on the same learning database, one feature vector per image. Third, an Ensemble Classifier used those feature vectors to learn a model. The probability of error was computed and averaged over 10 tests. We observed that the Ensemble Classifier, that learned with the features coming from the \u201dsmaller\u201d CNN, allowed to reduce the average probability of error by 0.4% w.r.t. the \u201dsmaller\u201d CNN. This indicated that: i) feature extraction (i.e convolution steps) is the most interesting part of CNN, ii) the fully connected classifier part of CNN is not necessarily the best classifier. Note that this \u201dsmaller\u201d CNN allowed us to obtain an improvement of 10% compared to RM+EC.\nWe would also like to comment on the adaptive scenario, i.e adaptive steganalysis [21], also named selection-channel-aware steganalysis [9]. When using adaptive steganalysis, the steganalyst uses an estimation of the modification probability of each pixel, as additional information for learning, in order to distinguish between cover and stego. In that scenario, authors from [9] report a detectability improvement of 1 to 4% with the Ensemble Classifier and SRM for detecting S-UNIWARD on the BossBase\ndatabase. At 0.4 bpp, the improvement was less than 2%. Those results are not comparable with ours since our images are smaller, but the 2% increment is really minor compared to the 17% CNN increment. Considering the most activated sections of the CNN, it seems that the CNN intrinsically guesses the selection channel.\n3.4.Cover-Source Mismatch scenario\nOur second test gave a very interesting results in the case of coversource mismatch. The cover-source mismatch phenomenon occurs when the sources model, obtained during the learning step, differs from the sources that are used by the steganograph. From a geometrical point of view, we can explain this inconsistency problem by the fact that the cloud, describing the images used by the steganalyst, is not located at the same place as the cloud describing the images used by the steganograph. Only a few papers have assessed the cover-source mismatch in practice [12], and [22, 6]. Nevertheless, no satisfactory solution is currently available, even though there have been some attempts to understand the phenomenon [23, 24].\nIn this case, we subsampled the cropped BOSSBase five times, thus obtaining five different training sets. For each training set, we built a CNN, and then we applied the classification model on the LIRMMBase test database (The database is available at http://www.lirmm.fr/~chaumont/LIRMMBase.html). We report the average error probability over the five trials. Note that the cover-source mismatch was present since none of the BOSSBase cameras are in LIRMMBase. Note also that we used the same script as that used for BOSSBase in order to generate the grey-level images. Moreover, the original images we used to build the LIRMMBase, were uncompressed and came from three known databases: Dresden, Raise, and Columbia. The average probability of error, for RM+EC, CNN, and FNN are given in Table 2.\nWe observed that the cover-source mismatch issue seriously affected the performance of the RM+EC classifier. Its results were close to those obtained by a random classifier. In return, CNN showed incredible robustness to the mismatch phenomenon with a 5.16% probability of error. FNN gave similar results with a 5.96% probability of error. Other experiments on BOWSBase [25] also confirmed that CNN is robust to the cover-source mismatch phenomenon, whereas RM+EC is not. BOWSBase is nevertheless not a very practical database since the set of used cameras is unknown, and since some of the cameras have also been used in BOSSBase.\nSurprisingly, the probability of error of CNN for LIRMMBase (5.16%) was lower than that from the cropped BOSSBase (7.4%). This was because there are fewer texture images in LIRMMBase than in the cropped BOSSBase. The LIRMMBase is thus easier to steganalyze. The CNN invariance to cover-source\nmismatch was so good that the mismatch phenomenon was no longer present, and we thus obtained steganalysis results that were more related to the content complexity of the data-base [26] (Textured image databases are harder to steganalysis than homogenous ones).\nThe robustness was probably due to the transformation done by the two first layers (because keeping those layers and then branching an EC gives results similar to those of the whole CNN). Thus the feature extractor due to the first two layers gives features that are robust to the cover-source mismatch. Those features are probably invariant which means that the feature representation obtained after the second layer is not sensitive to the different image statistics. Assuming that the role of the first layer is to decompose the noise signal (if the image is stego, this noise is the stego noise) in a spatio-frequential decomposition, and that the role of the second layer is to detect the presence of particular patterns in the spatio-frequential bands, then this could explain the invariance to the image content and thus the invariance to the cover model.\n4.Further discussion\nIn order to demystify CNN, we already explained that the learning was equivalent to minimization of a function having many unknown parameters with a technique similar to gradient descent. In this Section, we make links with previous research on the topic.\nAn important step of the CNN process is convolution as detailed in Sections 2.2 and 3.3. Learning of the filter kernels is done through minimization of the classification error using the back-propagation procedure. This is thus a simple optimization of filter kernels. This strategy already shown its efficiency in [27]. In that paper, some of the filter kernel values, which were used for computing the feature vector, were obtained through optimization with the downhill simplex algorithm. The objective was to minimize the probability of error given by an Ensemble Classifier. The learning achieved in the convolution layers of a CNN shares the same idea that leads to customized kernels, well suited for steganalysis purpose.\nWhen looking more precisely at the first layer, the kernel seems to act as a multi-band filtering (see Figure 2, where we can see the 64 kernels of size 7\u00d7 7 of our CNN). Some recent articles use such spatio-frequential decomposition (or a projection as in PSRM [28]) in order to compute Rich Models using Gabor filters [29] or DCT filters (DCTR features) [30]. Those filters are used to define projections that will then be used for computing a histogram leading to a feature vector.\nWhen looking more precisely at the second convolution layer, which applies a very unusual convolution approach, nothing similar could be found in recent papers dealing with feature extraction, such as histogram computation [3, 28], non-uniform quantization [31], feature selection [11], dimension reduction [32], etc. Nevertheless something important is occurring in this second layer which allows us to obtain features unsensitive to the cover-source mismatch. By looking to the Equation 3, the role of this second layer looks like searching patterns in multiple bands. The sum of the convolutions from Equation 3 is a way to accumulate presence clues of a searched signal in all the subands. The second layer then outputs a set of maps giving some clues to a signal presence.\nTo close the discussion, we should also add that the last treatment of each layer is a normalization step, and that this type of processing can also be found in papers such as [33] or [10]. This normalization is done in order to obtain, comparable output values for each neuron. We should also mention that further analysis on the activation function is requiered to completely understand its impact. The activation introduced non-linearity, which is also the case in the Ensemble Classifier through the majority vote [5], or in Rich Models with the Min-Max features [3].\n5.Conclusion\nIn this article, we propose to pursue the study of CNNs for steganalysis. We tested more than 40 CNNs and found the right parameters for the steganalysis domain. Instead of using a very deep network, such as that proposed by Qian et al., our experiments led us to use a network that is in height, and only consists two convolutional layers. We replaced the unconventional Gaussian activation function by a more classical ReLU activation function, we suppressed the pooling step that was acting as a down-sampling, and we pre-processed the images by applying high pass filtering before feeding the CNN.\nWe evaluated CNNs in two different scenarios. The first set of tests was done with the clairvoyant scenario. We used the cropped BOSSBase database with embedding with S-UNIWARD at 0.4 bpp. Compared to the state-of-the-art approach, i.e. the Ensemble Classifier with SRM features, CNN and FNN reduce the classification error by a three fold.\nThe second set of tests was done with the cover-source mismatch scenario. We used the BOSSBase with S-UNIWARD embedding at 0.4 bpp for learning, and tests were carried out on the public LIRMMBase database. The cover-source mismatch was fully achieved since the cameras were different from one base to another. The conventional method (RM+EC) totally failed to detect the use of steganography in LIRMMBase since the classification error was 48.29% i.e almost a random classification. Conversely, CNN exhibited natural invariance to the cover-source mismatch with a classification error of 5.16%.\nOur future studies will concentrate on improving some of the network parameters, and on gaining insight into the network behavior. Moreover, further experiments have to be done with different payload sizes, and different algorithms."}, {"heading": "Author Biography", "text": "Lionel PIBRE received his Master\u2019s degree in Computer Science in 2015 from University of Montpellier. His Master\u2019s degree allowed him to study data mining, visualization, machine learning and NLP. He made\nhis internship of Master\u2019s degree in the LIRMM laboratory in Montpellier. During this internship, he worked on steganalysis by Deep Learning. He is currently doing his Ph.D. thesis in the LIRMM laboratory. His work areas are multimedia security (steganography / steganalysis), segmentation and tracking in images.\nJe\u0301ro\u0302me PASQUET received a master degree in computer science from the university of Montpellier II, France, in 2013. He is currently working toward the PhD degree in LIRMM (Montpellier Laboratory\nof Informatics, Robotics and Microelectronics). His research interests are urban objects detection and segmentation in aerial photography.\nDino IENCO received his PhD in Computer Science in 2010 from University of Torino. From 2011 he is researcher at at the Irstea Institute, Montpellier, France. His main topics of research involved clus-\ntering algorithm for textual information, social network analysis, data mining approaches for biological data, supervised and semi-supervised methods for multimedia data (image and document analysis) and graph mining. He is actively working in the field of spatio-temporal data with a major emphasis on remote sensing analysis. In 2009, he visited the Yahoo! Research Lab in Barcelona (Spain) to work on information propagation. He coauthored more than 40 papers in major international conferences (SDM, ECML/PKDD, EMNLP, ECIR, etc.), and journals (DAMI, ACM TKDD, Pattern Recognition, JIIS, etc.).\nMarc CHAUMONT received his Engineer Diploma in Computer Sciences at the INSA of Rennes, France in 1999, his Ph.D. at the IRISA Rennes in 2003, and his HDR (\u201dHabilitation a\u0300 Diriger des Recherches\u201d)\nat the University of Montpellier in 2013. Since September 2005, he is an Assistant Professor in the LIRMM laboratory of Montpellier and the University of Nmes. His research areas are multimedia security (steganography, watermarking, digital forensics, video & image compression) and segmentation & tracking in images and videos. He is member of the TC of IEEE SPS - Information Forensics and Security for the period 2015-2017. He was program chair of ACM IH&MMSec\u20192013. He is reviewer for more than 20 journals (IEEE TIFS, IS&T JEI, ...) and for more than 10 conferences (EI MWSF, IEEE WIFS, ACM IH&MMSec, IEEE ICIP, ...)."}], "references": [{"title": "Breaking HUGO - The Process Discovery", "author": ["J. Fridrich", "J. Kodovsk\u00fd", "V. Holub", "M. Goljan"], "venue": "Proceedings of the 13th International Conference on Information Hiding, IH\u20192011, Prague, Czech Republic, May 2011, vol. 6958 of Lecture Notes in Computer Science, pp. 85\u2013101, Springer.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "On Completeness of Feature Spaces in Blind Steganalysis", "author": ["J. Kodovsk\u00fd", "J. Fridrich"], "venue": "Proceedings of the 10th ACM Workshop on Multimedia and Security, MM&Sec\u20192008, Oxford, United Kingdom, Sept. 2008, pp. 123\u2013132.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Rich Models for Steganalysis of Digital Images", "author": ["J. Fridrich", "J. Kodovsk\u00fd"], "venue": "IEEE Transactions on Information Forensics and Security, TIFS, vol. 7, no. 3, pp. 868\u2013882, June 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "LIBSVM: A Library for Support Vector Machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011, Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Ensemble Classifiers for Steganalysis of Digital Media", "author": ["J. Kodovsk\u00fd", "J. Fridrich", "V. Holub"], "venue": "IEEE Transactions on Information Forensics and Security, TIFS, vol. 7, no. 2, pp. 432\u2013444, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Steganalysis with Mismatched Covers: Do Simple Classifiers Help", "author": ["I. Lubenko", "A.D. Ker"], "venue": "Proceedings of the 14th ACM multimedia and Security Workshop, MM&Sec\u20192012, Coventry, United Kingdom, Sept. 2012, pp. 11\u201318.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Moving Steganography and Steganalysis from the Laboratory into the Real World", "author": ["A.D. Ker", "P. Bas", "R. B\u00f6hme", "R. Cogranne", "S. Craver", "T. Filler", "J. Fridrich", "T. Pevn\u00fd"], "venue": "Proceedings of the 1st ACM Workshop on Information Hiding and Multimedia Security, IH&MMSec\u20192013, Montpellier, France, June 2013, pp. 45\u201358, ACM.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Detecting Messages of Unknown Length", "author": ["T. Pevn\u00fd"], "venue": "Proceedings of SPIE Media Watermarking, Security, and Forensics, Part of IS&T/SPIE 21th Annual Symposium on Electronic Imaging, SPIE\u20192011, San Francisco, California, USA, Feb. 2011, vol. 7880.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Selection-Channel-Aware Rich Model for Steganalysis of Digital Images", "author": ["T. Denemark", "V. Sedighi", "V. Holub", "R. Cogranne", "J. Fridrich"], "venue": "Proceedings of the IEEE International Workshop on Information Forensics and Security, WIFS\u20192014, Atlanta, GA, Dec. 2014, pp. 48\u201353.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Theoretical Model of the FLD Ensemble Classifier Based on Hypothesis Testing Theory", "author": ["R. Cogranne", "T. Denemark", "J. Fridrich"], "venue": "Proceedings of IEEE International Workshop on Information Forensics and Security, WIFS\u20192014, Atlanta, GA, Dec. 2014, pp. 167\u2013172.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Steganalysis by Ensemble Classifiers with Boosting by Regression, and Post-Selection of Features", "author": ["M. Chaumont", "S. Kouider"], "venue": "Proceedings of IEEE International Conference on Image Processing, ICIP\u20192012, Lake Buena Vista (suburb of Orlando), Florida, USA, Sept. 2012, pp. 1133\u2013 1136.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Steganalysis with Cover-Source Mismatch and a Small Learning Database", "author": ["J. Pasquet", "S. Bringay", "M. Chaumont"], "venue": "Proceedings of the 22nd European Signal Processing Conference 2014, EUSIPCO\u20192014, Lisbon, Portugal, Sept. 2014, pp. 2425\u20132429.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, NIPS\u20192012, F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, Eds., pp. 1097\u20131105. Curran Associates, Inc., 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep Learning for Steganalysis via Convolutional Neural Networks", "author": ["Yinlong Qian", "Jing Dong", "Wei Wang", "Tieniu Tan"], "venue": "Proceedings of SPIE Media Watermarking, Security, and Forensics 2015, Part of IS&T/SPIE Annual Symposium on Electronic Imaging, SPIE\u20192015, San Francisco, California, USA, Feb. 2015, vol. 9409, pp. 94090J\u201394090J\u2013 10.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Using High-Dimensional Image Models to Perform Highly Undetectable Steganography", "author": ["T. Pevn\u00fd", "T. Filler", "P. Bas"], "venue": "Proceedings of the 12th International Conference on Information Hiding, IH\u20192010, Calgary, Alberta, Canada, June 2010, vol. 6387 of Lecture Notes in Computer Science, pp. 161\u2013177, Springer.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Designing Steganographic Distortion Using Directional Filters", "author": ["V. Holub", "J. Fridrich"], "venue": "Proceedings of the IEEE International Workshop on Information Forensics and Security, WIFS\u20192012, Tenerife, Spain, Dec. 2012, pp. 234\u2013239.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Universal Distortion Function for Steganography in an Arbitrary Domain", "author": ["V. Holub", "J. Fridrich", "T. Denemark"], "venue": "EURASIP Journal on Information Security, JIS, vol. 2014, no. 1, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Break Our Steganographic System\u2019: The Ins and Outs of Organizing BOSS", "author": ["P. Bas", "T. Filler", "T. Pevn\u00fd"], "venue": "Proceedings of the 13th International Conference on Information Hiding, IH\u20192011, Prague, Czech Republic, May 2011, vol. 6958 of Lecture Notes in Computer Science, pp. 59\u201370, Springer.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Reducing the Dimensionality of Data with Neural Networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, July 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Yoshua Bengio", "Aaron C. Courville", "Pascal Vincent"], "venue": "IEEE Transaction on Pattern Analysis and Machine Intelligence, PAMI, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1828}, {"title": "Adaptive Steganalysis Against WOW Embedding Algorithm", "author": ["Weixuan Tang", "Haodong Li", "Weiqi Luo", "Jiwu Huang"], "venue": "Proceedings of the 2nd ACM Workshop on Information Hiding and Multimedia Security, IH&MMSec\u20192014, Salzburg, Austria, 2014, pp. 91\u201396.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Going from Small to Large Data in Steganalysis", "author": ["I. Lubenko", "A.D. Ker"], "venue": "Proceedings of SPIE Media Watermarking, Security, and Forensics III, Part of IS&T/SPIE 22th Annual Symposium on Electronic Imaging, SPIE\u20192012, San Francisco, California, USA, Feb. 2012, vol. 8303.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "A Mishmash of Methods for Mitigating the Model Mismatch Mess", "author": ["A.D. Ker", "T. Pevn\u00fd"], "venue": "Proceedings of SPIE Media Watermarking, Security, and Forensics, Part of IS&T/SPIE 24th Annual Symposium on Electronic Imaging, SPIE\u20192014, San Francisco, California, USA, Feb. 2014, vol. 9028, pp. 90280I\u201390280I\u201315.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Study of Cover Source Mismatch in Steganalysis and Ways to Mitigate its Impact", "author": ["J. Kodovsk\u00fd", "V. Sedighi", "J. Fridrich"], "venue": "Proceedings of SPIE Media Watermarking, Security, and Forensics, Part of IS&T/SPIE 24th Annual Symposium on Electronic Imaging, SPIE\u20192014, San Francisco, California, USA, Feb. 2014, vol. 9028, pp. 90280J\u201390280J\u2013 12.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "BOWS-2 Contest (Break Our Watermarking System)", "author": ["P. Bas", "T. Furon"], "venue": "2008, Organized between the 17th of July 2007 and the 17th of April 2008. http://bows2.eclille.fr/.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "A Comparative Study of +/-1 Steganalyzers", "author": ["G. Cancelli", "G.J. Do\u00ebrr", "M. Barni", "I.J. Cox"], "venue": "Proceedings of the IEEE 10th Workshop on Multimedia Signal Processing, MMSP\u20192008, 2008, pp. 791\u2013796.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimizing Pixel Predictors for  Steganalysis", "author": ["V. Holub", "J. Fridrich"], "venue": "Proceedings of SPIE Media Watermarking, Security, and Forensics, Part of IS&T/SPIE 22th Annual Symposium on Electronic Imaging, SPIE\u20192012, San Francisco, California, USA, Feb. 2012, vol. 8303, pp. 830309\u2013 830309\u201313.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Random Projections of Residuals as an Alternative to Co-occurrences in Steganalysis", "author": ["V. Holub", "J. Fridrich", "T. Denemark"], "venue": "Proceedings of SPIE Media Watermarking, Security, and Forensics, Part of IS&T/SPIE Annual Symposium on Electronic Imaging, SPIE\u20192013, San Francisco, California, USA, Feb. 2013, vol. 8665, pp. 86650L\u2013 86650L\u201311.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Steganalysis of Adaptive JPEG Steganography Using 2D Gabor Filters", "author": ["Xiaofeng Song", "Fenlin Liu", "Chunfang Yang", "Xiangyang Luo", "Yi Zhang"], "venue": "Proceedings of the 3rd ACM Workshop on Information Hiding and Multimedia Security, IH&MMSec\u201920015, Portland, Oregon, USA, June 2015, pp. 15\u201323.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Low-Complexity Features for JPEG Steganalysis Using Undecimated DCT", "author": ["V. Holub", "J. Fridrich"], "venue": "IEEE Transactions on Information Forensics and Security, TIFS, vol. 10, no. 2, pp. 219\u2013228, Feb 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Co-occurrence Steganalysis in High Dimensions", "author": ["T. Pevn\u00fd"], "venue": "Proceeding of SPIE Media Watermarking, Security, and Forensics, Part of IS&T/SPIE 22th Annual Symposium on Electronic Imaging, SPIE\u20192012, San Francisco, California, USA, Feb. 2012, vol. 8303, pp. 83030B\u2013 83030B\u201313.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "The Challenges of Rich Features in Universal Steganalysis", "author": ["T. Pevn\u00fd", "A.D. Ker"], "venue": "Proceeding of SPIE Media Watermarking, Security, and Forensics, Part of IS&T/SPIE 23th Annual Symposium on Electronic Imaging, SPIE\u20192013, San Francisco, California, USA, Feb. 2013, vol. 8665, pp. 86650M\u201386650M\u201315.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Those features must be diverse [1], which means that they should capture the maximum of information modeling the image, and they also should be complete [2], which means that their values should be different between a cover and a stego.", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "Those features must be diverse [1], which means that they should capture the maximum of information modeling the image, and they also should be complete [2], which means that their values should be different between a cover and a stego.", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "The best feature set to represent an image has so far been supplied by Rich Models [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "Depending on the memory or computation requirements, the steganalyst can use an SVM [4], an Ensemble Classifier [5], or a Perceptron [6].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "Depending on the memory or computation requirements, the steganalyst can use an SVM [4], an Ensemble Classifier [5], or a Perceptron [6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "Depending on the memory or computation requirements, the steganalyst can use an SVM [4], an Ensemble Classifier [5], or a Perceptron [6].", "startOffset": 133, "endOffset": 136}, {"referenceID": 6, "context": "When analyzing the empirical security of an embedding algorithm in a laboratory environment [7], we select the \u201dclairvoyant scenario\u201d [8], i.", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "When analyzing the empirical security of an embedding algorithm in a laboratory environment [7], we select the \u201dclairvoyant scenario\u201d [8], i.", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "Until 2015, in the clairvoyant scenario, the best classifier was the Ensemble Classifier [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 8, "context": "Moreover, some improvements have been proposed in order to increase its efficiency, such as the use of embedding probabilities in order to better steganalyze the adaptive algorithms [9], tuning of false alarm probability [10], or treating the cover-source mismatch problem [11], where the best classifier is also the Ensemble Classifier [12].", "startOffset": 182, "endOffset": 185}, {"referenceID": 9, "context": "Moreover, some improvements have been proposed in order to increase its efficiency, such as the use of embedding probabilities in order to better steganalyze the adaptive algorithms [9], tuning of false alarm probability [10], or treating the cover-source mismatch problem [11], where the best classifier is also the Ensemble Classifier [12].", "startOffset": 221, "endOffset": 225}, {"referenceID": 10, "context": "Moreover, some improvements have been proposed in order to increase its efficiency, such as the use of embedding probabilities in order to better steganalyze the adaptive algorithms [9], tuning of false alarm probability [10], or treating the cover-source mismatch problem [11], where the best classifier is also the Ensemble Classifier [12].", "startOffset": 273, "endOffset": 277}, {"referenceID": 11, "context": "Moreover, some improvements have been proposed in order to increase its efficiency, such as the use of embedding probabilities in order to better steganalyze the adaptive algorithms [9], tuning of false alarm probability [10], or treating the cover-source mismatch problem [11], where the best classifier is also the Ensemble Classifier [12].", "startOffset": 337, "endOffset": 341}, {"referenceID": 12, "context": "Yet, in recent years, in different areas, the use of deep learning networks challenges traditional two step approaches (feature extraction, and use of a classifier) [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 13, "context": "[14] proposed, in 2015, to use deep learning to replace the traditional two step approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "obtained a detection percentage of only 3% to 4% lower than that obtained with the Ensemble Classifier [5], and SRM features [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "obtained a detection percentage of only 3% to 4% lower than that obtained with the Ensemble Classifier [5], and SRM features [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 14, "context": "The tested algorithms were HUGO [15], WOW [16], and S-UNIWARD [17], on the BOSSbase database [18].", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "The tested algorithms were HUGO [15], WOW [16], and S-UNIWARD [17], on the BOSSbase database [18].", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "The tested algorithms were HUGO [15], WOW [16], and S-UNIWARD [17], on the BOSSbase database [18].", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "The tested algorithms were HUGO [15], WOW [16], and S-UNIWARD [17], on the BOSSbase database [18].", "startOffset": 93, "endOffset": 97}, {"referenceID": 2, "context": "Indeed, the dimension of the feature vector of the last convolution layer (layer 5) provides only 256 features, whereas the SRM (Spatial Rich Models) dimension of the feature vector provides 34671 [3].", "startOffset": 197, "endOffset": 200}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Until recently [19], neural networks were considered as having a too long learning time, and as being less efficient than modern classifiers.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "Recently, due to recent advances in the neural network field [20], and to the computational power supplied by GPUs, deep learning approaches have been proposed as a natural extension of neural networks, and they are getting popular due to their high classification performance.", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "Since 2006 [19], many adjustments have been proposed to improve the robustness and reduce the computational costs.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "In this paper, we recall the major concepts of a Convolutional Neural Networks (CNN), which is a deep learning network that has proved its efficiency in image classification competitions [13], and that was used by Qian et al.", "startOffset": 187, "endOffset": 191}, {"referenceID": 0, "context": "Then, a softmax function is connected to the outputs of the last layer in order to normalize the two outputs delivered by the network between [0,1].", "startOffset": 142, "endOffset": 147}, {"referenceID": 13, "context": "Convolutional Neural Network [14].", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "There is no similar operation in the classical feature extraction process supplied by Rich Models [3], and to the best of our knowledge, in spatio-frequential decomposition.", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "This is usually an interesting property that is exploited in the Ensemble Classifier through the majority vote [5], and that is also used in the Rich Models with the Min-Max features [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "This is usually an interesting property that is exploited in the Ensemble Classifier through the majority vote [5], and that is also used in the Rich Models with the Min-Max features [3].", "startOffset": 183, "endOffset": 186}, {"referenceID": 17, "context": "0 [18] consisting of 10 000 grey-level images of size 512\u00d7 512 coming from 7 different cameras, then we split each image in four in order to obtain 40 000 images of size 256\u00d7256.", "startOffset": 2, "endOffset": 6}, {"referenceID": 16, "context": "In our experiments, we embeded the messages (using the simulator) with S-UNIWARD [17] at 0.", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "The paper essentially demonstrates that a CNN is more efficient than an Ensemble Classifier, or an Ensemble Classifier informed on the selection channel (adaptive steganalysis scenario) [21, 9].", "startOffset": 186, "endOffset": 193}, {"referenceID": 8, "context": "The paper essentially demonstrates that a CNN is more efficient than an Ensemble Classifier, or an Ensemble Classifier informed on the selection channel (adaptive steganalysis scenario) [21, 9].", "startOffset": 186, "endOffset": 193}, {"referenceID": 0, "context": "The operations performed in the last layer are dot products, bias additions, and then a softmax in order to rescale the output values in [0,1].", "startOffset": 137, "endOffset": 142}, {"referenceID": 6, "context": "This scenario is a laboratory scenario used to empirically assess the security of a steganographic embedding algorithm [7].", "startOffset": 119, "endOffset": 122}, {"referenceID": 16, "context": "We embed with S-UNIWARD [17] at 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 2, "context": "The first steganalysis was done using Rich Models [3] and an Ensemble Classifier [5].", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "The first steganalysis was done using Rich Models [3] and an Ensemble Classifier [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 10, "context": "The one given on the Binghamton website10; in that case the learning set only consists of 10 000 images, and the one written in C++ and proposed in the paper [11] with the features selection, and parallelization options; in that case, the learning set consists of 60 000 images.", "startOffset": 158, "endOffset": 162}, {"referenceID": 20, "context": "e adaptive steganalysis [21], also named selection-channel-aware steganalysis [9].", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "e adaptive steganalysis [21], also named selection-channel-aware steganalysis [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "In that scenario, authors from [9] report a detectability improvement of 1 to 4% with the Ensemble Classifier and SRM for detecting S-UNIWARD on the BossBase database.", "startOffset": 31, "endOffset": 34}, {"referenceID": 11, "context": "Only a few papers have assessed the cover-source mismatch in practice [12], and [22, 6].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "Only a few papers have assessed the cover-source mismatch in practice [12], and [22, 6].", "startOffset": 80, "endOffset": 87}, {"referenceID": 5, "context": "Only a few papers have assessed the cover-source mismatch in practice [12], and [22, 6].", "startOffset": 80, "endOffset": 87}, {"referenceID": 22, "context": "Nevertheless, no satisfactory solution is currently available, even though there have been some attempts to understand the phenomenon [23, 24].", "startOffset": 134, "endOffset": 142}, {"referenceID": 23, "context": "Nevertheless, no satisfactory solution is currently available, even though there have been some attempts to understand the phenomenon [23, 24].", "startOffset": 134, "endOffset": 142}, {"referenceID": 24, "context": "Other experiments on BOWSBase [25] also confirmed that CNN is robust to the cover-source mismatch phenomenon, whereas RM+EC is not.", "startOffset": 30, "endOffset": 34}, {"referenceID": 25, "context": "mismatch was so good that the mismatch phenomenon was no longer present, and we thus obtained steganalysis results that were more related to the content complexity of the data-base [26] (Textured image databases are harder to steganalysis than homogenous ones).", "startOffset": 181, "endOffset": 185}, {"referenceID": 26, "context": "This strategy already shown its efficiency in [27].", "startOffset": 46, "endOffset": 50}, {"referenceID": 27, "context": "Some recent articles use such spatio-frequential decomposition (or a projection as in PSRM [28]) in order to compute Rich Models using Gabor filters [29] or DCT filters (DCTR features) [30].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "Some recent articles use such spatio-frequential decomposition (or a projection as in PSRM [28]) in order to compute Rich Models using Gabor filters [29] or DCT filters (DCTR features) [30].", "startOffset": 149, "endOffset": 153}, {"referenceID": 29, "context": "Some recent articles use such spatio-frequential decomposition (or a projection as in PSRM [28]) in order to compute Rich Models using Gabor filters [29] or DCT filters (DCTR features) [30].", "startOffset": 185, "endOffset": 189}, {"referenceID": 2, "context": "When looking more precisely at the second convolution layer, which applies a very unusual convolution approach, nothing similar could be found in recent papers dealing with feature extraction, such as histogram computation [3, 28], non-uniform quantization [31], feature selection [11], dimension reduction [32], etc.", "startOffset": 223, "endOffset": 230}, {"referenceID": 27, "context": "When looking more precisely at the second convolution layer, which applies a very unusual convolution approach, nothing similar could be found in recent papers dealing with feature extraction, such as histogram computation [3, 28], non-uniform quantization [31], feature selection [11], dimension reduction [32], etc.", "startOffset": 223, "endOffset": 230}, {"referenceID": 30, "context": "When looking more precisely at the second convolution layer, which applies a very unusual convolution approach, nothing similar could be found in recent papers dealing with feature extraction, such as histogram computation [3, 28], non-uniform quantization [31], feature selection [11], dimension reduction [32], etc.", "startOffset": 257, "endOffset": 261}, {"referenceID": 10, "context": "When looking more precisely at the second convolution layer, which applies a very unusual convolution approach, nothing similar could be found in recent papers dealing with feature extraction, such as histogram computation [3, 28], non-uniform quantization [31], feature selection [11], dimension reduction [32], etc.", "startOffset": 281, "endOffset": 285}, {"referenceID": 31, "context": "When looking more precisely at the second convolution layer, which applies a very unusual convolution approach, nothing similar could be found in recent papers dealing with feature extraction, such as histogram computation [3, 28], non-uniform quantization [31], feature selection [11], dimension reduction [32], etc.", "startOffset": 307, "endOffset": 311}, {"referenceID": 9, "context": "To close the discussion, we should also add that the last treatment of each layer is a normalization step, and that this type of processing can also be found in papers such as [33] or [10].", "startOffset": 184, "endOffset": 188}, {"referenceID": 4, "context": "The activation introduced non-linearity, which is also the case in the Ensemble Classifier through the majority vote [5], or in Rich Models with the Min-Max features [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "The activation introduced non-linearity, which is also the case in the Ensemble Classifier through the majority vote [5], or in Rich Models with the Min-Max features [3].", "startOffset": 166, "endOffset": 169}], "year": 2015, "abstractText": "Since the BOSS competition, in 2010, most steganalysis approaches use a learning methodology involving two steps: feature extraction, such as the Rich Models (RM), for the image representation, and use of the Ensemble Classifier (EC) for the learning step. In 2015, Qian et al. have shown that the use of a deep learning approach that jointly learns and computes the features, is very promising for the steganalysis. In this paper, we follow-up the study of Qian et al., and show that, due to intrinsic joint minimization, the results obtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural Network (FNN), if well parameterized, surpass the conventional use of a RM with an EC. First, numerous experiments were conducted in order to find the best \u201dshape\u201d of the CNN. Second, experiments were carried out in the clairvoyant scenario in order to compare the CNN and FNN to an RM with an EC. The results show more than 16% reduction in the classification error with our CNN or FNN. Third, experiments were also performed in a cover-source mismatch setting. The results show that the CNN and FNN are naturally robust to the mismatch problem. In Addition to the experiments, we provide discussions on the internal mechanisms of a CNN, and weave links with some previously stated ideas, in order to understand the impressive results we obtained.", "creator": "LaTeX with hyperref package"}}}