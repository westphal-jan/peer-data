{"id": "1511.04868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "A Neural Transducer", "abstract": "sequence - y - loop mechanisms have achieved impressive results on various tasks. however, they are unnecessary for tasks customers prefer incremental measurements to justify repeated regarding more data mature. relevance is because software generate an output program conditioned on an inherently identical sequence. see xml paper, vt describes as new model as effectively replicate incremental predictions provided more input arrives, without redoing the entire computation. comparing auto - y - signal simulator, my method performs improved next - day pattern conditioned on cumulative partial correlation sum observed including the greatest variance data. it aims this goal using an encoder recurrent control loop ( rnn ) that computes features meaning the same frame rate as current event, and a transducer robot that operates over blocks yielding input fragments. the transducer continuously extends the sequence produced accordingly far constructing a local sequence - to - sequence model. during integration, our function uses various alignment modules generate supervised sentences for each block. approximate intelligence is typically available amongst parameters published as speech synthesizers, frame recognition in videos, mp3. whereas inference ( decoding ), object search is prohibited ) find the highly acceptable ancestral sequence yielding an input sequence. processing functionality is performed first - at best end scanning a model, if other candidates running the previous execution are interactive and hybrid local sequence - vs - sequence model. since timit, brand online method achieves 19. 8 % api error rate ( sms ). alongside comparison with similar query - to - sequence methods, bits present a bidirectional encoder and gains 18. 63 % per. this delivers 3 - possible best reported sequence - to - sequence model which achieves 17. 6 %. importantly, direct state - to - sequence learning our choice is directly impacted affecting the effect of the tag. every 10 - times replicated transcript, it achieves 20. 9 % with enhanced unidirectional interpretation, compared to 20 % employing the best bidirectional sequence - to - sequence comparisons.", "histories": [["v1", "Mon, 16 Nov 2015 08:53:44 GMT  (762kb,D)", "http://arxiv.org/abs/1511.04868v1", null], ["v2", "Wed, 18 Nov 2015 19:56:58 GMT  (762kb,D)", "http://arxiv.org/abs/1511.04868v2", null], ["v3", "Thu, 19 Nov 2015 19:27:14 GMT  (762kb,D)", "http://arxiv.org/abs/1511.04868v3", null], ["v4", "Thu, 4 Aug 2016 23:31:46 GMT  (434kb,D)", "http://arxiv.org/abs/1511.04868v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["navdeep jaitly", "david sussillo", "quoc v le", "oriol vinyals", "ilya sutskever", "samy bengio"], "accepted": false, "id": "1511.04868"}, "pdf": {"name": "1511.04868.pdf", "metadata": {"source": "CRF", "title": "AN ONLINE SEQUENCE-TO-SEQUENCE MODEL USING PARTIAL CONDITIONING", "authors": ["Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals", "Ilya Sutskeyver", "Samy Bengio"], "emails": ["ndjaitly@google.com", "qvl@google.com", "vinyals@google.com", "ilyasu@google.com", "bengio@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The recently introduced sequence-to-sequence model has shown tremendous success in various tasks that map sequences to sequences (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015a; Chorowski et al., 2014; 2015; Chan et al., 2015; Bahdanau et al., 2015b; Vinyals et al., 2015b;a; Vinyals & Le, 2015). However, this method is unsuitable for tasks where it is important to produce outputs as the input sequence arrives. Speech recognition is an example of such an online task \u2013 users prefer seeing an ongoing transcription of speech over receiving it at the \u201cend\u201d of an utterance. This limitation of the sequence-to-sequence model is due to the fact that ouput predictions are conditioned on the entire input sequence.\nIn this paper, we present a modification to sequence-to-sequence models such that they can produce chunks of outputs (possibly of zero length) as blocks of inputs arrive - thus satisfying the condition of being \u201conline\u201d (see Figure 1(c) for an overview). The model generates outputs for each block by using a transducer RNN that implements a sequence-to-sequence model. The inputs to the transducer RNN come from two sources: the encoder RNN and its own recurrent state. In other words, the transducer RNN generates local extensions to the output sequence, conditioned on the features computed for the block by an encoder RNN and the recurrent state of the transducer RNN at the last step of the previous block. Similar to standard sequence-to-sequence models, conditioning over the\nar X\niv :1\n51 1.\n04 86\n8v 1\n[ cs\n.L G\n] 1\n6 N\nov 2\n01 5\nxi xLx2x1\nyiy2y1\nNeural Network\nyL\n(a) DNN-HMM\nyLyiy2y1\nxLxix2x1\nNeural Network\nyi-1y1<s> yL-1 yL\n</s>\n(b) seq2seq\nhistory of tokens produced in a given block is achieved by feeding in targets of previous steps to the transducer. Conditioning over the history of output tokens is achieved by the transducer, through the use of its recurrent connections across blocks. Conditioning on all input seen so far is achieved through the propagation of information through the recurrent states of the encoder RNN. This partial conditioning on the input sequence seen and output sequence generated so far is what makes the model different from sequence-to-sequence models. Partial conditioning allows this model to produce output tokens as data arrives, instead of waiting until all the input sequence has arrived.\nDuring training, the model assumes that approximate alignments of output tokens to the input sequence are given. The model uses the alignments to group outputs into blocks. An end-of-block symbol (<e>) suffix is added to each local target sequence. This allows the model to learn to terminate the sequence produced for a block. During inference, the emission of the end-of-block symbol causes the transducer to stop generating more tokens for the current block and to move to the next block to generate extensions. The need for approximate alignment is not a big hurdle for many domains. For example, in speech recognition, alignments can be easily generated by bootstrapping from a different model. Our experiments indicate that these alignments do not need to be very precise - the transducer is able to use its recurrent state to deal with outputs being on different sides of block boundaries.\nWe experimented with several variations of sequence-to-sequence models and found attention-based models to be most useful. In our model, the transducer uses attention over the features within each block, to generate the next output token. In order to learn a model of how attention changes from one output step to the next we also experimented with a new attention mechanism that uses a Long Short Term Memory (LSTM) recurrent neural network (Hochreiter & Schmidhuber, 1997). This attention mechanism is different from that of Chorowski et al. (2015) in that, instead of taking a softmax over attention scalars, we input these values into an LSTM-RNN which produces the attention vector as its output. The LSTM can thus learn to modulate attention at one output step, using the attention generated at the previous output step.\nWe applied the model to the phone recognition task on the TIMIT dataset. Using this method, we achieved 19.8% PER on TIMIT using three layers of 250 LSTM cells in both the encoder and the transducer along with the LSTM-based attention model mentioned above. Unlike previous work on sequence-to-sequence models this model uses only unidirectional LSTMs for both encoder and transducer networks, and decoding can be performed online by extending beams from one block of inputs to the next. Further, the model trains relatively easily without careful regularization.\nWhile prior work on sequence-to-sequence models have not reported results on a unidirectional setting, the above performance matches the 19.6% PER achieved by Connectionist Temporal Classification (CTC) with unidirectional LSTMs for the same task (Graves et al., 2013a). However, CTC makes independent predictions at each step (see figure 1(a)), and as such can only produce high accuracy on large vocabulary continuous speech recognition (LVCSR) tasks when it is coupled to a strong language model and pronounciation dictionary (Bahdanau et al., 2015b). Our model is inherently an end-to-end model that can incorporate a language model within its parameters, and should produce accurate results on LVCSR even without strong language models.\nFor a comparison with previously reported sequence-to-sequence results on TIMIT, we used a bidirectional encoder and achieved 18.7% PER compared to the best reported number of 17.6% achieved by Chorowski et al. (2015). 1 Even with the bidirectional encoder, our model is different from the sequence-to-sequence model in that it uses a slightly different training objective. Our objective function is geared towards generating partial outputs by the ends of blocks. The partial outputs are provided to the model using alignment information and may not be the best for state of the art performance. Nevertheless, the comparable performance of these two models in spite of these, and other architectural differences between the models imply that little is lost by training towards partial outputs from blocks."}, {"heading": "2 RELATED WORK", "text": "In the last few years, multiple deep learning architectures have been proposed for speech recognition. Figure 1 shows the comparison of our work against these previous approaches.\nThe predominant methodology for speech recognition until recently was to use a Hidden Markov Model coupled with a Deep Neural Network, a Convolutional Network or a Recurrent Network (Hinton et al., 2012; Abdel-Hamid et al., 2012; Graves et al., 2013b). More recently, a CTC approach was also proposed for this task (Graves et al., 2013a; Hannun et al., 2014; Sak et al., 2015). An important aspect of these approaches is that the model makes predictions at every input time step. A high-level picture of this architecture is shown in Figure 1(a). A weakness of these models is that they typically assume conditional independence between the predictions at each output step.\nSequence-to-sequence models make no such assumptions \u2013 the output sequence is generated by next step prediction, conditioning on the entire input sequence and the partial output sequence generated so far (Chorowski et al., 2014; 2015; Bahdanau et al., 2015b; Chan et al., 2015). Figure 1(b) shows the high-level picture of this architecture. As can be seen from the figure, these models have a significant limitation in that they have to wait until the end of the speech utterance to start decoding. This property makes them unattractive for real time speech recognition.\nConceptually, speech is ordered left-to-right, and localization of relevant frames using attention over the entire input seems to be overkill. Moreover, it has been empirically observed that these models perform significantly worse on longer inputs \u2013 presumably because inacuracies in attention at one step can negatively impact the entire transcripts produced subsequently (Chan et al., 2015). Chorowski et al. (2015) ameliorate (but not eliminate) this problem by introducing a heuristic that requires the attention to move from one step to the next. However, this is still a heuristic, and their published model reports results on inputs from a bidirectional encoder (Schuster & Paliwal, 1997), which prevents the system from being able to do online decoding.\nOur model on the other hand uses alignment information such that it can decode online, extending transcripts at the ends of blocks. Alignments of speech units (phones or words or even phrases) to segments of speech are easily obtain with orthogonal methods such as manual labeling or performing\n1Note that the use of a bidirectional encoder here makes both models offline, since bidirectional features needed by the transducer cannot be computed until the end of the utterance.\nforced alignment with an existing model. Using this information we can train a model that uses the sequence-to-sequence model with partial conditioning on inputs seen and outputs produced, to extend output subsequences as input arrives. The blocked architecture also reduces the impact of the length of the input on the accuracy of the transcript, as each new block is relatively unaffected by the errors in the earlier blocks. Figure 1(c) shows the difference between our method and previous sequence-to-sequence models.\nNote that our model does not require frame level alignments that are used by the DNN/RNN-HMM systems. Instead it requires approximate alignments that determine which tokens to emit for a block. Moreover, although we only performed phone recognition experiments for this paper, like Bahdanau et al. (2015b); Chan et al. (2015) the model can use words or phrases as tokens, and can be trained as an end-to-end model."}, {"heading": "3 METHODS", "text": "In this section we describe the model in more detail. Please refer to Figure 2 for an overview."}, {"heading": "3.1 MODEL", "text": "Let x1\u00b7\u00b7\u00b7L be the input data that is L time steps long, where xi represents the features at input time step i. Correspondingly, let y1\u00b7\u00b7\u00b7S be the one-hot encoding of the target sequence of length S, from a dictionary of K targets. Let W be the block size, i.e., the periodicity with which the transducer\nemits output tokens, and N = d LW e be the number of blocks, eb, b \u2208 1 \u00b7 \u00b7 \u00b7N be the index of the last token emitted in the bth block. Note that e0 = 0 and eN = S. We append the <e> token to each block of outputs. Thus yeb =<e> for each block b.\nWe compute the probability of seeing output sequence y1\u00b7\u00b7\u00b7eb by the end of block b as follows:\np (y1\u00b7\u00b7\u00b7eb |x1\u00b7\u00b7\u00b7bW ) = p (y1\u00b7\u00b7\u00b7e1 |x1\u00b7\u00b7\u00b7W ) b\u220f\nb\u2032=2\np ( y(eb\u2032\u22121+1)\u00b7\u00b7\u00b7e\u2032b |x1\u00b7\u00b7\u00b7b\u2032W , y1\u00b7\u00b7\u00b7eb\u2032\u22121 )\n(1)\nEach of the terms in this equation is itself computed by the chain rule decomposition, i.e., for any block b,\np ( y(eb\u22121+1)\u00b7\u00b7\u00b7eb |x1\u00b7\u00b7\u00b7bW , y1\u00b7\u00b7\u00b7eb\u22121 ) = eb\u220f m=e(b\u22121)+1 p ( ym|x1\u00b7\u00b7\u00b7bW , y1\u00b7\u00b7\u00b7(m\u22121) ) (2)\nThe next step probability terms, p ( ym|x1\u00b7\u00b7\u00b7bW , y1\u00b7\u00b7\u00b7(m\u22121) ) , in Equation 2 are computed by the transducer using the encoding of the input x1\u00b7\u00b7\u00b7bW computed by the encoder, and the label prefix y1\u00b7\u00b7\u00b7(m\u22121) that was input into the transducer, at previous emission steps. We describe this in more detail in the next subsection."}, {"heading": "3.2 NEXT STEP PREDICTION", "text": "We again refer the reader to Figure 2 for this discussion. The example shows a transducer with two hidden layers, with units sm and h\u2032m at output step m. In the figure, the next step prediction is shown for block b. For this block, the index of the first output symbol is m = eb\u22121 + 1, and the index of the last output symbol is m+ 2 (i.e. eb = m+ 2).\nThe transducer computes the next step prediction, using parameters, \u03b8, of the neural network through the following sequence of steps:\nsm = fRNN (sm\u22121, [cm\u22121; ym\u22121] ; \u03b8) (3) cm = fcontext ( sm,h((b\u22121)W+1)\u00b7\u00b7\u00b7bW ; \u03b8 ) (4) h\u2032m = fRNN (h \u2032 m\u22121, [cm; sm] ; \u03b8) (5)\np ( ym|x1\u00b7\u00b7\u00b7bW , y1\u00b7\u00b7\u00b7(m\u22121) ) = fsoftmax (ym;h \u2032 m, \u03b8) (6)\nwhere fRNN (am\u22121,bm; \u03b8) is the recurrent neural network function (such as an LSTM or a sigmoid or tanh RNN) that computes the state vector am for a layer at a step using the recurrent state vector am\u22121 at the last time step, and input bm at the current time step;2 fsoftmax (\u00b7;am; \u03b8) is the softmax distribution computed by a softmax layer, with input vector am; and fcontext ( sm,h((b\u22121)W+1)\u00b7\u00b7\u00b7bW ; \u03b8 ) is the context function, that computes the input to the transducer at output step m from the state sm at the current time step, and the features h((b\u22121)W+1)\u00b7\u00b7\u00b7bW of the encoder for the current input block, b. We experimented with different ways of computing the context vector \u2013 with and without an attention mechanism. These are described subsequently in section 3.3.\nNote that since the encoder is an RNN, h(b\u22121)W \u00b7\u00b7\u00b7bW is actually a function of the entire acoustic input, x1\u00b7\u00b7\u00b7bW so far. Correspondingly, sm is a function of the labels emitted so far, and the entire acoustic signal seen so far.3 Similarly, h\u2032m is a function of the labels emitted so far and the entire acoustic signal seen so far.\n3.3 COMPUTING fcontext\nWe first describe how the context vector is computed by an attention model similar to earlier work (Chorowski et al., 2014; Bahdanau et al., 2015a; Chan et al., 2015). We call this model the MLP-attention model.\n2Note that for LSTM, we would have to additionally factor in cell states from the previous states - we have ignored this in the notation for purpose of clarity. The exact details are easily worked out.\n3For the first output step of a block it includes only the acoustic signal seen until the end of the last block.\nIn this model the context vector cm is in computed in two steps - first a normalized attention vector \u03b1m is computed from the state sm of the transducer and next the hidden states h(b\u22121)W+1\u00b7\u00b7\u00b7bW of the encoder for the current block are linearly combined using \u03b1 and used as the context vector. To compute \u03b1m, a multi-layer perceptron computes a scalar value, emj for each pair of transducer state sm and encoder h(b\u22121)W+j . The attention vector is computed from the scalar values, emj , j = 1 \u00b7 \u00b7 \u00b7W . Formally:\nemj = fattention ( sm,h(b\u22121)W+j ; \u03b8 ) (7)\n\u03b1m = softmax ([e m 1 ; e m 2 ; \u00b7 \u00b7 \u00b7 emW ]) (8)\ncm = W\u2211 j=1 \u03b1mj h(b\u22121)W+j (9)\nWe also experimented with using a simpler model for fattention that computed emj = s T mh(b\u22121)W+j . We refer to this model as DOT-attention model.\nBoth of these attention models have two shortcomings. Firstly there is no explicit mechanism that requires the attention model to move its focus forward, from one output time step to the next. Secondly, the energies computed as inputs to the softmax function, for different input frames j are independent of each other at each time step, and thus cannot modulate (e.g., enhance or suppress) each other, other than through the softmax function.\nWe attempt to address these two shortcomings using a new attention mechanism. In this model, instead of feeding [em1 ; e m 2 ; \u00b7 \u00b7 \u00b7 emW ] into a softmax, we feed them into a recurrent neural network with one hidden layer that outputs the softmax attention vector at each time step. Thus the model should be able to modulate the attention vector both within a time step and across time steps. We refer to this model as LSTM-attention."}, {"heading": "3.4 ADDRESSING END OF BLOCKS", "text": "Since the model only produces a small sequence of output tokens in each block, we have to address the mechanism for shifting the transducer from one block to the next. We experimented with three distinct ways of doing this. In the first approach, we introduced no explicit mechanism for end-of-blocks, hoping that the transducer neural network would implicitly learn a model from the training data. In the second approach we added end-of-block symbols, <e>, to the label sequence to demarcate the end of blocks, and we added this symbol to the target dictionary. Thus the softmax function in Equation 6 implicitly learns to either emit a token, or to move the transducer forward to the next block. In the third approach, we model moving the transducer forward, using a separate logistic function of the attention vector. The target of the logistic function is 0 or 1 depending on whether the current step is the last step in the block or not."}, {"heading": "3.5 TRAINING", "text": "The parameters, \u03b8, of the model are learned by performing stochastic gradient descent (SGD) with momentum over the loss function,\nl (\u03b8;x1\u00b7\u00b7\u00b7L, y1\u00b7\u00b7\u00b7S) = B\u2211 b=1 S\u2211 m=1 log p ( y(eb\u22121+1)\u00b7\u00b7\u00b7eb |x1\u00b7\u00b7\u00b7bW , y1\u00b7\u00b7\u00b7(eb\u22121+1) ) (10)"}, {"heading": "3.6 INFERENCE", "text": "For inference, given the input acoustics x1\u00b7\u00b7\u00b7L, and the model parameters, \u03b8, we find the sequence of labels y1..M that maximizes the probability of the labels, conditioned on the data, i.e.,\ny\u03031\u00b7\u00b7\u00b7S = arg max y1\u00b7\u00b7\u00b7S\u2032 ,e1\u00b7\u00b7\u00b7N N\u2211 b=1 log p ( ye(b\u22121)+1\u00b7\u00b7\u00b7eb |x1\u00b7\u00b7\u00b7bW , y1\u00b7\u00b7\u00b7e(b\u22121) ) (11)\nExact inference in this scheme is computationally expensive because the expression for log probability does not permit decomposition into smaller terms that can be independently computed. Instead,\neach candidate, y1\u00b7S\u2032 , would have to be tested independently, and the best sequence over an exponentially large number of sequences would have to be discovered. Hence, we use a beam search heuristic to find the \u201cbest\u201d set of candidates. To do this, at each output step m, we keep a heap of alternative n best prefixes, and extend each one by one symbol, trying out all the possible alternative extensions, keeping only the best n extensions. Included in the beam search is the act of moving the attention to the next input block. The beam search ends either when the sequence is longer than a pre-specified threshold, or when the end of token symbol is produced at the last block.\nAn alternative way of explaining inference that highlights its online property is as follows. Beam search uses a heap of n candidates at each step. For each new block, we extend each of the candidates from the last block, one symbol at a time, and add it back to the heap. A candidate is no longer extended in a block once it emits the <e> token. When none of the candidates in the heap can be extended any further we move to the next block and repeat the inference step. Note that the candidate sequences can be extended without redoing any of the computations at the earlier blocks by storing the state of the transducer at the last step for each of the candidates."}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": "We used TIMIT, a standard bechmark for speech recognition (Fisher et al., 1986) for our experiments. Log Mel filterbanks were computed every 10ms as inputs to the system. The targets were the 60 phones defined for the TIMIT dataset (the demarcating h# were relabelled as pau and merged with neighbouring pau if one existed). Alignments were generated from Kaldi toolkit with a simple Gaussian Mixture Model-Hidden Markov Model (GMM-HMM) trained on the data. The frame level alignments from Kaldi were converted into block level alignments by assigning each phone in the sequence to the block it was last observed in.\nTo train our model we use stochastic gradient descent with momentum with a batch size of 1 (i.e., one utterance per training step), starting with a learning rate of 0.05, and momentum of 0.9. We reduced the learning rate by a factor of 0.5 every time the average log prob over the validation set decreased.4 The decrease was applied for a maximum of 4 times. The models were trained for 50 epochs and the parameters from the epochs with the best dev set log prob were used for decoding.\nTable 1 shows a comparison of our method against a basic implementation of a sequence-tosequence model that produces outputs for each block independent of the other blocks, and concatenates the produced sequences. Here, the sequence-to-sequence model produces the output conditioned on the state of the encoder at the end of the block. Both models used an encoder with two layers of 250 LSTM cells, without attention. The standard sequence-to-sequence model performs significantly worse than our model \u2013 the recurrent connections of the transducer across blocks are clearly helpful in improving the accuracy of the model.\nFigure 3 shows the impact of block size on the accuracy of the different sequence-to-sequence variations we used. See Section 3.3 for a description of the {DOT,MLP,LSTM}-attention models. All models used a two LSTM layer encoder and a two LSTM layer transducer. As expected, sequenceto-sequence models without attention are very sensitive to the choice of the block size, but with attention, the precise value of the block size becomes less important. Our results also indicate that the LSTM-based attention model generally performs better than the other attention mechanisms we explored. Since this model performed best with W=25, we used this configuration for subsequent experiments.\n4Note the TIMIT provides a validation set, called the dev set. We will use these terms interchangeably.\nWe combined our model with scheduled sampling (Bengio et al., 2015) but found no gains on this task. This is presumably because the regularizing effect of scheduled sampling is observed over longer sequences while our transducer outputs only a small number of tokens per block.\nWe also carried out experiments to see the impact of offsetting the labels from the input, so that the model was required to produce labels only after several time steps of seeing the input. Surprisingly, the model\u2019s accuracy was not impacted much by this choice. However, the model performed slightly worse (approximately 3% relative) if targets that straddled boundaries were randomly assigned to a side based on its length on that side. Thus the model does prefer to receive the relevant input before emitting the target; but beyond that, it does not require additional offset.\nTo improve the PER of the trained models, we used the PER on the validation set at the end of each epoch of training to decide whether to reduce the learning rate or not. We explored the impact of number of layers in the encoder and in the transducer (see Table 2). We found that a three layer encoder coupled to a three layer transducer performed best. Four layer transducers produced results with higher spread in accuracy - possibly because of the more difficult optimization involved. Using this, we were able to achieve a PER of 19.8% on the TIMIT test set. These results could probably be improved with dropout training (Zaremba et al., 2014) or other regularization techniques (Chorowski et al., 2015) but we did not pursue those avenues in this paper.\nFor a comparison with previously published sequence-to-sequence models on this task, we used a three layer bidirectional LSTM encoder with 250 LSTM cells in each direction and achieved a PER of 18.7%. By contrast, the best reported results using previous sequence-to-sequence models are 17.6% (Chorowski et al., 2015). However, this result comes with more careful training techniques than we attempted for this work.\nWe also explored the impact of the length of utterances on PER. Figure 4 shows that the PER of utterances is relatively consistenct across different lengths. This is very different from the results\nreported by sequence-to-sequence methods (Chan et al., 2015) that report strong decrease in accuracy of results as a function of length of utterances. Chorowski et al. (2014) reported a means of measuring this impact, by computing the PER of a dataset made by concatenating repetitions of the same utterance. They reported an increase in PER from 17.6% to 20% on concatenating each utterance ten times. By contrast, we find that our unidirectional online sequence-to-sequence model achieves 20.9% (up from 19.8%) PER on the ten-times replicated dataset. The bidirectional transducer meanwhile goes from 18.7% PER to 19.7%."}, {"heading": "5 DISCUSSION", "text": "Our model simplifies the earlier approach of Chorowski et al. (2014) by using alignment information in the data. However, note that unlike DNN-HMM models, the alignment used here is coarse in nature and does not use precise frame labeling of inputs. Instead, it uses coarse labels that indicate which tokens were emitted by the time a particular block of input was processed. As the alignment corresponds to block level information and not frame level alignments, it could include labels such as what words were verbalized by what block of the input data. The use of a transducer RNN that processes input at a slower frame rate reduces the importance of precise alignments.\nOne of the imporant side-effects of our model using partial conditioning with a blocked transducer is that it naturally alleviates the problem of \u201cloosing attention\u201d suffered by sequence-to-sequence models. Because of this, sequence-to-sequence models perform worse on longer utterances (Chorowski et al., 2015; Chan et al., 2015). This problem is automatically tackled in our model because each new block automatically shifts the attention monotonically forward. Within a block, the model learns to move attention forward from one step to the next, and the attention mechanism rarely suffers, because both the size of a block, and the number of output steps for a block are relatively small. As a result, error in attention in one block, has minimal impact on the predictions at subsequent blocks.\nFinally, we note that increasing the block size, W , so that it is as large as the input utterance makes the model similar to other end-to-end models (Chorowski et al., 2014; Chan et al., 2015)."}, {"heading": "6 CONCLUSION", "text": "We have introduced a new model that uses partial conditioning on inputs to generate output sequences. Using a new attention mechanism, we applied the model to a phone recognition task and showed that is can produce results comparable to the state of the art from sequence-to-sequence models. The model tackles the two main hurdles to the adoption of sequence-to-sequence models as speech recognition systems. Firstly, the model can produce output transcripts as data arrives, without having to redo the entire decoding. Secondly, because of its blocked architecture, problems with attention in one part of the input are unlikely to impact the entire transcript \u2013 reducing the impact of the length of the input sequence on the accuracy of the transcript. Future work will aim to apply this method for end-to-end training on a larger corpus."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank George Dahl, Andrew Senior, Geoffrey Hinton, Erik McDermott, Tara Sainath, Vincent Vanhoucke and the Google Brain team for the help with the project. We thank William Chan for fruitful discussions on sequence-to-sequence models."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["Abdel-Hamid", "Ossama", "Mohamed", "Abdel-rahman", "Jiang", "Hui", "Penn", "Gerald"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2012}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Philemon", "Bengio", "Yoshua"], "venue": "In http://arxiv.org/abs/1508.04395,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Listen, attend and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwen", "Holger", "Bengio", "Yoshua"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Neural Information Processing Systems: Workshop Deep Learning and Representation Learning Workshop,", "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Attention-Based Models for Speech Recognition", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "The darpa speech recognition research database: specifications and status", "author": ["Fisher", "William M", "Doddington", "George R", "Goudie-Marshall", "Kathleen M"], "venue": "In Proc. DARPA Workshop on speech recognition,", "citeRegEx": "Fisher et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 1986}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alan", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Hybrid Speech Recognition with Bidirectional LSTM", "author": ["Graves", "Alex", "Jaitly", "Navdeep", "Mohamed", "Abdel-rahman"], "venue": "In Automatic Speech Recognition and Understanding Workshop,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Deepspeech: Scaling up endto-end speech recognition", "author": ["Hannun", "Awni", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam"], "venue": "In http://arxiv.org/abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N", "Kingsbury", "Brian"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "Jurgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Sak", "Hasim", "Senior", "Andrew W", "Rao", "Kanishka", "Beaufays", "Fran\u00e7oise"], "venue": "CoRR, abs/1507.06947,", "citeRegEx": "Sak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Mike", "Paliwal", "Kuldip K"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In ICML Deep Learning Workshop,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "Lukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In NIPS,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol"], "venue": "CoRR, abs/1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "The recently introduced sequence-to-sequence model has shown tremendous success in various tasks that map sequences to sequences (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015a; Chorowski et al., 2014; 2015; Chan et al., 2015; Bahdanau et al., 2015b; Vinyals et al., 2015b;a; Vinyals & Le, 2015).", "startOffset": 129, "endOffset": 313}, {"referenceID": 5, "context": "The recently introduced sequence-to-sequence model has shown tremendous success in various tasks that map sequences to sequences (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015a; Chorowski et al., 2014; 2015; Chan et al., 2015; Bahdanau et al., 2015b; Vinyals et al., 2015b;a; Vinyals & Le, 2015).", "startOffset": 129, "endOffset": 313}, {"referenceID": 6, "context": "The recently introduced sequence-to-sequence model has shown tremendous success in various tasks that map sequences to sequences (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015a; Chorowski et al., 2014; 2015; Chan et al., 2015; Bahdanau et al., 2015b; Vinyals et al., 2015b;a; Vinyals & Le, 2015).", "startOffset": 129, "endOffset": 313}, {"referenceID": 4, "context": "The recently introduced sequence-to-sequence model has shown tremendous success in various tasks that map sequences to sequences (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015a; Chorowski et al., 2014; 2015; Chan et al., 2015; Bahdanau et al., 2015b; Vinyals et al., 2015b;a; Vinyals & Le, 2015).", "startOffset": 129, "endOffset": 313}, {"referenceID": 4, "context": "(b) Sequence-to-sequence models (Chan et al., 2015; Bahdanau et al., 2015b).", "startOffset": 32, "endOffset": 75}, {"referenceID": 6, "context": "This attention mechanism is different from that of Chorowski et al. (2015) in that, instead of taking a softmax over attention scalars, we input these values into an LSTM-RNN which produces the attention vector as its output.", "startOffset": 51, "endOffset": 75}, {"referenceID": 1, "context": "However, CTC makes independent predictions at each step (see figure 1(a)), and as such can only produce high accuracy on large vocabulary continuous speech recognition (LVCSR) tasks when it is coupled to a strong language model and pronounciation dictionary (Bahdanau et al., 2015b). Our model is inherently an end-to-end model that can incorporate a language model within its parameters, and should produce accurate results on LVCSR even without strong language models. For a comparison with previously reported sequence-to-sequence results on TIMIT, we used a bidirectional encoder and achieved 18.7% PER compared to the best reported number of 17.6% achieved by Chorowski et al. (2015). 1 Even with the bidirectional encoder, our model is different from the sequence-to-sequence model in that it uses a slightly different training objective.", "startOffset": 259, "endOffset": 689}, {"referenceID": 12, "context": "The predominant methodology for speech recognition until recently was to use a Hidden Markov Model coupled with a Deep Neural Network, a Convolutional Network or a Recurrent Network (Hinton et al., 2012; Abdel-Hamid et al., 2012; Graves et al., 2013b).", "startOffset": 182, "endOffset": 251}, {"referenceID": 0, "context": "The predominant methodology for speech recognition until recently was to use a Hidden Markov Model coupled with a Deep Neural Network, a Convolutional Network or a Recurrent Network (Hinton et al., 2012; Abdel-Hamid et al., 2012; Graves et al., 2013b).", "startOffset": 182, "endOffset": 251}, {"referenceID": 11, "context": "More recently, a CTC approach was also proposed for this task (Graves et al., 2013a; Hannun et al., 2014; Sak et al., 2015).", "startOffset": 62, "endOffset": 123}, {"referenceID": 14, "context": "More recently, a CTC approach was also proposed for this task (Graves et al., 2013a; Hannun et al., 2014; Sak et al., 2015).", "startOffset": 62, "endOffset": 123}, {"referenceID": 6, "context": "Sequence-to-sequence models make no such assumptions \u2013 the output sequence is generated by next step prediction, conditioning on the entire input sequence and the partial output sequence generated so far (Chorowski et al., 2014; 2015; Bahdanau et al., 2015b; Chan et al., 2015).", "startOffset": 204, "endOffset": 277}, {"referenceID": 4, "context": "Sequence-to-sequence models make no such assumptions \u2013 the output sequence is generated by next step prediction, conditioning on the entire input sequence and the partial output sequence generated so far (Chorowski et al., 2014; 2015; Bahdanau et al., 2015b; Chan et al., 2015).", "startOffset": 204, "endOffset": 277}, {"referenceID": 4, "context": "Moreover, it has been empirically observed that these models perform significantly worse on longer inputs \u2013 presumably because inacuracies in attention at one step can negatively impact the entire transcripts produced subsequently (Chan et al., 2015).", "startOffset": 231, "endOffset": 250}, {"referenceID": 0, "context": ", 2012; Abdel-Hamid et al., 2012; Graves et al., 2013b). More recently, a CTC approach was also proposed for this task (Graves et al., 2013a; Hannun et al., 2014; Sak et al., 2015). An important aspect of these approaches is that the model makes predictions at every input time step. A high-level picture of this architecture is shown in Figure 1(a). A weakness of these models is that they typically assume conditional independence between the predictions at each output step. Sequence-to-sequence models make no such assumptions \u2013 the output sequence is generated by next step prediction, conditioning on the entire input sequence and the partial output sequence generated so far (Chorowski et al., 2014; 2015; Bahdanau et al., 2015b; Chan et al., 2015). Figure 1(b) shows the high-level picture of this architecture. As can be seen from the figure, these models have a significant limitation in that they have to wait until the end of the speech utterance to start decoding. This property makes them unattractive for real time speech recognition. Conceptually, speech is ordered left-to-right, and localization of relevant frames using attention over the entire input seems to be overkill. Moreover, it has been empirically observed that these models perform significantly worse on longer inputs \u2013 presumably because inacuracies in attention at one step can negatively impact the entire transcripts produced subsequently (Chan et al., 2015). Chorowski et al. (2015) ameliorate (but not eliminate) this problem by introducing a heuristic that requires the attention to move from one step to the next.", "startOffset": 8, "endOffset": 1469}, {"referenceID": 1, "context": "Moreover, although we only performed phone recognition experiments for this paper, like Bahdanau et al. (2015b); Chan et al.", "startOffset": 88, "endOffset": 112}, {"referenceID": 1, "context": "Moreover, although we only performed phone recognition experiments for this paper, like Bahdanau et al. (2015b); Chan et al. (2015) the model can use words or phrases as tokens, and can be trained as an end-to-end model.", "startOffset": 88, "endOffset": 132}, {"referenceID": 6, "context": "We first describe how the context vector is computed by an attention model similar to earlier work (Chorowski et al., 2014; Bahdanau et al., 2015a; Chan et al., 2015).", "startOffset": 99, "endOffset": 166}, {"referenceID": 4, "context": "We first describe how the context vector is computed by an attention model similar to earlier work (Chorowski et al., 2014; Bahdanau et al., 2015a; Chan et al., 2015).", "startOffset": 99, "endOffset": 166}, {"referenceID": 8, "context": "We used TIMIT, a standard bechmark for speech recognition (Fisher et al., 1986) for our experiments.", "startOffset": 58, "endOffset": 79}, {"referenceID": 3, "context": "We combined our model with scheduled sampling (Bengio et al., 2015) but found no gains on this task.", "startOffset": 46, "endOffset": 67}, {"referenceID": 20, "context": "These results could probably be improved with dropout training (Zaremba et al., 2014) or other regularization techniques (Chorowski et al.", "startOffset": 63, "endOffset": 85}, {"referenceID": 7, "context": ", 2014) or other regularization techniques (Chorowski et al., 2015) but we did not pursue those avenues in this paper.", "startOffset": 43, "endOffset": 67}, {"referenceID": 7, "context": "6% (Chorowski et al., 2015).", "startOffset": 3, "endOffset": 27}, {"referenceID": 4, "context": "reported by sequence-to-sequence methods (Chan et al., 2015) that report strong decrease in accuracy of results as a function of length of utterances.", "startOffset": 41, "endOffset": 60}, {"referenceID": 4, "context": "reported by sequence-to-sequence methods (Chan et al., 2015) that report strong decrease in accuracy of results as a function of length of utterances. Chorowski et al. (2014) reported a means of measuring this impact, by computing the PER of a dataset made by concatenating repetitions of the same utterance.", "startOffset": 42, "endOffset": 175}, {"referenceID": 7, "context": "Because of this, sequence-to-sequence models perform worse on longer utterances (Chorowski et al., 2015; Chan et al., 2015).", "startOffset": 80, "endOffset": 123}, {"referenceID": 4, "context": "Because of this, sequence-to-sequence models perform worse on longer utterances (Chorowski et al., 2015; Chan et al., 2015).", "startOffset": 80, "endOffset": 123}, {"referenceID": 6, "context": "Finally, we note that increasing the block size, W , so that it is as large as the input utterance makes the model similar to other end-to-end models (Chorowski et al., 2014; Chan et al., 2015).", "startOffset": 150, "endOffset": 193}, {"referenceID": 4, "context": "Finally, we note that increasing the block size, W , so that it is as large as the input utterance makes the model similar to other end-to-end models (Chorowski et al., 2014; Chan et al., 2015).", "startOffset": 150, "endOffset": 193}, {"referenceID": 5, "context": "Our model simplifies the earlier approach of Chorowski et al. (2014) by using alignment information in the data.", "startOffset": 45, "endOffset": 69}], "year": 2017, "abstractText": "Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a new model that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, our method computes the next-step distribution conditioned on the partial input sequence observed and the partial sequence generated. It accomplishes this goal using an encoder recurrent neural network (RNN) that computes features at the same frame rate as the input, and a transducer RNN that operates over blocks of input steps. The transducer RNN extends the sequence produced so far using a local sequence-to-sequence model. During training, our method uses alignment information to generate supervised targets for each block. Approximate alignment is easily available for tasks such as speech recognition, action recognition in videos, etc. During inference (decoding), beam search is used to find the most likely output sequence for an input sequence. This decoding is performed online at the end of each block, the best candidates from the previous block are extended through the local sequenceto-sequence model. On TIMIT, our online method achieves 19.8% phone error rate (PER). For comparison with published sequence-to-sequence methods, we used a bidirectional encoder and achieved 18.7% PER. This compares favorably to the best reported sequence-to-sequence model which achieves 17.6%. Importantly, unlike sequence-to-sequence models our model is minimally impacted by the length of the input. On 10-times replicated utterances, it achieves 20.9% with a unidirectional model, compared to 20% from the best bidirectional sequence-tosequence models.", "creator": "LaTeX with hyperref package"}}}