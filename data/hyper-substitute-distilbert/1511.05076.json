{"id": "1511.05076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Latent Dirichlet Allocation Based Organisation of Broadcast Media Archives for Deep Neural Network Adaptation", "abstract": "microphone problem presents exciting new concept for the discovery analyzing latent domains supporting diverse speech data, besides the use high frequencies of particular cell networks ( mt ) for specific speech recognition. similar work results on preservation of four - genre acoustic media, which is often only categorised broadly with terms that many coverage profiles including as interactive, news, documentary, fantasy. however, mere matter of acoustic profiles these categories are coarse. instead, still is hoped that pure mixture of resonance domains can better represent the complex and diverse representations forming a tv transmitting, nor therefore lead progressively better and somewhat robust performance. we propose a new method, requiring these latent domains are discovered with latent dirichlet traces, in random acoustic manner. these are arranged as search dnns using the unique binary code ( mp ) representation comprising the measured signal. competitions conducted on a set of bbc tv broadcasts, with wider than 17, million users for training against 47 shows where testing, show consistently detailed determination of pulse - mapped dnns reduces the likelihood up than 29 % relative compared to the current hybrid dnn regions.", "histories": [["v1", "Mon, 16 Nov 2015 18:25:33 GMT  (454kb,D)", "http://arxiv.org/abs/1511.05076v1", "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015), 13-17 Dec 2015, Scottsdale, Arizona, USA"]], "COMMENTS": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015), 13-17 Dec 2015, Scottsdale, Arizona, USA", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mortaza doulaty", "oscar saz", "raymond w m ng", "thomas hain"], "accepted": false, "id": "1511.05076"}, "pdf": {"name": "1511.05076.pdf", "metadata": {"source": "CRF", "title": "LATENT DIRICHLET ALLOCATION BASED ORGANISATION OF BROADCAST MEDIA ARCHIVES FOR DEEP NEURAL NETWORK ADAPTATION", "authors": ["Mortaza Doulaty", "Oscar Saz", "Raymond W. M. Ng", "Thomas Hain"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Latent Dirichlet Allocation, Deep Neural Network Adaptation, Speech Recognition\n1. INTRODUCTION\nStreaming and webcasts are popular in this age of high\u2013 speed internet and mobile networks. With the ever increasing amount of audio\u2013visual media data, the ability to index their contents and search for them is becoming more and more important. For data with speech contents, using Automatic Speech Recognition (ASR) to get the transcripts, is an efficient way to search and browse through thousands of hours of recordings. Error rates for the traditional broadcast news programmes could reach below 10% even in 1990s [1, 2, 3]. However broadcast media is not just limited to clean and read studio speech but also includes other types of multi\u2013genre data with diverse speakers, variety of acoustic and recording conditions and diversity of the topics covered resulting in complex acoustic, lexical and linguistic conditions which is not yet well studied [4].\nThis work was supported by the EPSRC Programme Grant EP/I031022/1 (Natural Speech Technology).\nThe wide variety of conditions in complex broadcast media causes mismatch between training and testing data, and therefore degrades the performance of the speech recognition systems [5]. Adaptation can compensate for this mismatch. For Gaussian Mixture Model/Hidden Markov Model (GMM/HMM) systems several well established adaptation methods exist. However, adaptation of Deep Neural Networks (DNNs) is still a very active research topic. DNN adaptation methods can be divided into these three main categories [6]:\n1. Linear input transformations: this is the most common adaptation method where a linear transformation is applied to either input feature [7], input to the softmax layer [8] or activation of the hidden layers [9]\n2. Retraining: all or some of the model parameters are adapted or trained using the adaptation data [10, 11].\n3. Subspace methods: a speaker/environment subspace is estimated and then neurons\u2019 weights or transformations are computed, based on the subspace representation of the speaker/environment. Principle Component Analysis (PCA) based adaptation approach [12], i-Vector based speaker\u2013aware training [13] or speaker\u2013aware DNNs [14] can be considered as subspace methods.\nBroadcast media is complex in nature. For instance, in a news programme, there are in\u2013studio reporting and live coverage on the scene. Assuming that all content variation from a single show can be described by a single, vaguely\u2013defined domain is unrealistic and also not that helpful to ASR. Nonetheless it is clear that certain show types have very specific characteristics. Being able to assign broadcast media to a mixture of domains can alleviate this problem. Latent Dirichlet Allocation (LDA) is a statistical approach to discover latent variables in a collection of data that is describable with first\u2013 order statistic, in an unsupervised manner [15]. It is mostly used in Natural Language Processing (NLP) for the categorisation of text documents, but it has been used for audio and image processing as well. In audio tasks, LDA has been used for classifying unstructured audio files into onomatopoeic and semantic descriptions with successful results [16]. We have previously used LDA for domain adaptation of GMM/HMM systems [17]. ar X\niv :1\n51 1.\n05 07\n6v 1\n[ cs\n.C L\n] 1\n6 N\nov 2\n01 5\nThis paper builds on this knowledge, and introduces a method on how to use LDA for domain adaptation of hybrid DNNs. Using LDA models, a data class - further referred to as \u201cLDA domain\u201d, is chosen for each utterance. The class information is then provided to the DNN in training. The learning algorithm adjusts the model parameters to exploit these additional information. During testing the same information is supplied. We further refer to this method as Latent\u2013Domain\u2013 aware Training (LDaT). Results shown later in this paper indicate significant improvements of LDaT over baseline and input\u2013adapted DNNs.\nThe following section briefly introduces LDA, followed by a description of acoustic data LDA. In section 4, DNN adaptation using LDA is described. Section 5 describes the experimental setup, followed by discussion and a conclusion.\n2. LATENT DIRICHLET ALLOCATION\nLatent Dirichlet Allocation (LDA) [15] is an unsupervised probabilistic generative model for collections of discrete data. It aims to describe how every item within a collection is generated, assuming that there are a set of latent variables and that each item is modelled as a finite mixture over those latent variables. LDA was originally used for topic modelling of text corpora; however, it is a generic model and can be applied to other tasks, such as object categorisation and localisation in image processing [18], automatic harmonic analysis in music processing [19], acoustic information retrieval in unstructured audio analysis [16] and our previous work for domain adaptation of GMM/HMM systems [17].\nA dataset is defined as a collection of sets where each set is in turn a collection of discrete symbols (in case of topic modelling of text documents, a document is equivalent to a set and words inside a document are equivalent to the discrete symbols). Each set is represented by a V -dimensional vector based on the histogram of the symbols\u2019 table which has size of V . It is assumed that the sets were generated by the following generative process:\n1. For each set dm,m \u2208 {1...M}, choose aK\u2013dimensional latent variable weight vector \u03b8m from the Dirichlet distribution with scaling parameter \u03b1: p(\u03b8m|\u03b1) = Dir(\u03b1)\n2. For each discrete item wn, n \u2208 {1...N} in set dm\n(a) Draw a latent variable zn \u2208 {1...K} from the multinomial distribution p(zn = k|\u03b8m)\n(b) Given the latent variable, draw a symbol from p(wn|zn, \u03b2), where \u03b2 is a V \u00d7K matrix and \u03b2ij = p(wn = i|zn = j, \u03b2)\nIt is assumed that each set can be represented as a bag\u2013of\u2013 symbols - i.e. by first\u2013order statistics, which means any symbol sequence relationship is disregarded. Since speech and\ntext are highly ordered processes this can be an issue. Another assumption is that the dimensionality of the Dirichlet distribution K is fixed and known (and thus the dimensionality of the latent variable z).\nA graphical representation of the LDA model is shown at Figure 1 as a three\u2013level hierarchical Bayesian model. In this model, the only observed variable is w and the rest are all latent. \u03b1 and \u03b2 are dataset level parameters, \u03b8m is a set level variable and zn, wn are symbol level variables. The generative process is described formally as:\np(\u03b8, z,w|\u03b1, \u03b2) = p(\u03b8|\u03b1) N\u220f n=1 p(zn|\u03b8)p(wn|zn, \u03b2) (1)\nThe posterior distribution of the latent variables given the symbols and \u03b1 and \u03b2 parameters is:\np(\u03b8, z|w, \u03b1, \u03b2) = p(\u03b8, z,w|\u03b1, \u03b2) p(w|\u03b1, \u03b2)\n(2)\nComputing p(w|\u03b1, \u03b2) requires some intractable integrals. A reasonable approximate can be acquired using variational approximation, which is shown to work reasonably well in various applications [15]. The approximated posterior distribution is:\nq(\u03b8, z|\u03b3, \u03c6) = q(\u03b8|\u03b3) N\u220f n=1 q(zn|\u03c6n) (3)\nwhere \u03b3 is the Dirichlet parameter that determines \u03b8 and \u03c6 is the parameter for the multinomial that generates the latent variables.\nTraining tries to minimise the Kullback\u2013Leiber Divergence (KLD) between the real and the approximated joint probabilities (equations 2 and 3) [15]:\nargmin \u03b3,\u03c6\nKLD ( q(\u03b8, z|\u03b3, \u03c6) || p(\u03b8, z|w, \u03b1, \u03b2) ) (4)\nOther training methods based on Markov\u2013Chain MonteCarlo are also proposed, like Gibbs sampling method [20].\n3. ACOUSTIC LDA\nAs outlined above, LDA is a model to describe latent factors in sets of discrete symbols [15] which are here interpreted as \u201cdomains\u201d. In order to fit into that concept speech signals need to be converted into such a form. Typically speech is represented using continuous features (e.g. with Mel frequency cepstral coefficients), and has variable length. In our previous\nwork [17] we used Linde\u2013Buzo\u2013Gray vector quantization algorithm [21] to represent each speech frame with a discrete symbol, equivalent to an acoustic word or phone label.\nIn this paper an approach similar to that used in [22] was implemented. A GMM model with V components is trained using all of the training data. The model is then used to get the posterior probabilities of the Gaussian components to represent each frame with index of the Gaussian component with the highest posterior probability. Frames of every speech segment of length T , x = {x1, ...,xt, ...,xT } are represented as:\nx\u0303t = argmax i\nP (Gi|xt) (5)\nwhere Gi (1 \u2264 i \u2264 V ) is the ith Gaussian component. After applying this process to each utterance, each speech segment is represented as {x\u03031, ..., x\u0303t, ..., x\u0303T } where xt is index of the Gaussian component and thus a natural number (1 \u2264 xt \u2264 V ). Here we refer to each speech utterance as an acoustic document. With this information, a fixed length vector x\u0302 = {a1, ..., ai, ..., aV } of size V were constructed to represent the count of every Gaussian component in an acoustic document. This leads to a type of bag\u2013of\u2013sounds representation. The sounds would normally be expected to relate to phones, however given the acoustic diversity of background conditions many other factors may play a role. Once these bag\u2013of\u2013sounds representations of acoustic documents are derived, LDA models can be trained.\n4. LDA\u2013DNN ADAPTATION\nAfter acoustic symbols are established and speech segments are represented as bag\u2013of\u2013sounds, LDA models with designated latent domain sizes are trained using the variational EM algorithm [15]. Hence, the posterior distribution of latent domains (zm) for each utterancem is computed. Since there can be many utterances in the training set, to effectively incorporate domain information in the vast amount of data, each utterance is assigned to only one domain. The assignment is made according to the maximum posterior estimate of domains p(zm).\nThe maximum posterior assumption requires high domain homogeneity for each acoustic document. This can to some degree be controlled by the size of domains. With a large number of domains, the resolution may be too high and the domain homogeneity within one acoustic document may be therefore lowered. On the other hand it is desirable to have a sufficient number of domains such that the variability in shows and between different types of shows are sufficiently covered.\nFinally, domain information derived from the LDA model with K domains is encoded with a K-dimensional one\u2013hot vector called Unique Binary Index Code (UBIC) [14]. UBIC indicates the most likely domain of the utterance using the posterior domain probability. UBIC is then used to augment\nthe input feature vectors. Apart from the extra nodes and connections in the input layer, the DNN architecture is identical to other baseline DNNs which are not domain\u2013aware.\nWith the baseline DNNs, activation of the first layer is:\nv1 = f(W1v0 + b1) (6)\nwhere superscripts denote the layer index, v1 is the activation vector of the first layer, Wi and bi are the weight matrix and bias vector associated with layer i and v0 is the input features. With augmented UBIC in LDaT training this becomes:\nv1LDaT = f ( [ W1vW 1 d ] [v0 d ] + b1LDaT ) = f ( W1vv\n0 +W1dd+ b 1 LDaT\ufe38 \ufe37\ufe37 \ufe38\ndomain specific bias ) (7) where d is the K\u2013dimensional domain assignment vector from the LDA model, W1v is the weigh matrix for the acoustic features and it is initialised from W1 of equation 6. W1s is the weigh matrix for the augmented LDA domain assignment input. Comparing equations 6 and 7 the only difference is in the bias vector where there was a fixed bias before (b1) and now with the augmented LDA domain information, there is a new adapted bias b1d = W 1 dd + b 1 LDaT for each of the LDA domains. This type of adaptation is efficient, since it is implicit in the training process and does not require further adaptation steps [6]. Figure 2 illustrates the DNN architecture with the augmented UBIC code.\n5. EXPERIMENTAL SETUP"}, {"heading": "5.1. Data", "text": "TV broadcasts from the BBC were selected for the experiments. The data is identical to the one defined and provided for the 2015 Multi\u2013Genre Broadcast (MGB) Challenge\n[23, 24, 25]. The shows were chosen to cover the full range of broadcast show types in current TV, and categorised in terms of 8 genres: advice, children\u2019s, comedy, competition, documentary, drama, events and news. Acoustic Model training data was fixed and limited to more than 2,000 shows, broadcast by the BBC during 6 weeks in April and May of 2008. The development data for the task was 47 shows broadcast by the BBC during a week in mid\u2013May 2008. The amount of shows and broadcast time for training and development data is shown in Table 1.\nFor the training data high quality transcription was not available. Instead only the subtitle text broadcast with each show plus an aligned version of the subtitles were available where the time stamps of the subtitles had been corrected in a lightly supervised manner [26]. After this process, the new transcripts for the training shows had two potential problems: first, the subtitle text might not always match the actual spoken words and second, the time boundaries given might have errors arising from the lightly supervised alignment. To alleviate these two problems, only segments with Word Matching Error Rate (WMER) of lower than 40% were used, which yielded around 500h of data. The WMER was a by\u2013product of the semi\u2013supervised alignment process that measures how similar the text in the subtitle matched the output of a lightly supervised ASR system for that segment [26].\nFor the Language Model (LM) subtitles from shows broadcast from 1979 to March 2008, with a total of 650 million words were used to train statistical language models."}, {"heading": "5.2. Baseline", "text": "Initial models were GMM/HMM systems with 13 dimensional PLP [27] features where four neighbouring frames on each side were spliced together to form a 117\u2013dimensional feature vector. Using Linear Discriminant Analysis [28] this feature vector was projected down to 40\u2013dimensional vector and a global Constrained Maximum Likelihood Linear Regression [29] transformation was applied to de\u2013correlate the features. Speaker Adaptive Training (SAT) [30] was performed and then models were discriminatively trained using\nthe Boosted Maximum Mutual Information criterion [31] and used to get the state level alignments for the DNN training. The input to the DNN was 440 dimensional PLP features that were \u00b15 frames to the left/right of the current frame. The network had 6 hidden layers of size 2048 and an output layer of size 6478. The network was initialised using Deep Belief Network [32] pre\u2013training and then trained to optimise per frame Cross Entropy objective function with Stochastic Gradient Descent. A speaker adapted DNN was also trained as the second base\u2013line system using SAT style training. Speaker\u2013based CMLLR transformations were applied to the features to make the inputs of the DNN closer to an average speaker. The Kaldi open\u2013source speech recognition toolkit [33] was used to train the acoustic models.\nFor decoding a 50k lexicon with a highly pruned 3\u2013gram language model was used to generate lattices and then those lattices were re\u2013scored using a 4\u2013gram language model. Both of the language models were trained on the 650M words of the subtitles data using the SRILM toolkit [34]\nTable 2 presents the Word Error Rate (WER) of the development set with baseline models. There is a 19% relative WER reduction from GMM/HMM models to the baseline DNN models as usually expected. Speaker\u2013adapted DNN also yields a further 6% relative WER reduction compared to the un\u2013adapted DNN."}, {"heading": "5.3. LDA\u2013DNN Experiments", "text": "A GMM with 4k Gaussian mixtures is constructed using the mix\u2013up procedure. Using this GMM, the audio frames are mapped to discrete symbols to train the LDA models [35]. With LDA models, we experimented with different number of latent domains, namely 4, 6, 8, 16, 32, 64, 128, 256 and 512.\nFor each of the domain sizes mentioned above, we computed the average domain entropy over all acoustic documents. Entropy increases from 0.76 to 4.06 when domain size increases from 4 to 512. In this experiment, domain sizes 64 and 128 were used. This leveraged the considerations about the homogeneity and sparsity of the discovered domains discussed in section 4.\nApart from selecting an appropriate size of domain, crossagreement data filtering was performed to ensure high domain homogeneity for each acoustic document. A domaintuple with 8192 items was established. These items come from the Cartesian product of the 64\u00d7 128 domain mappings from the two corresponding LDA models. It is assumed that\nthe two LDA models share a significant portion of the domains. If there is a high heterogeneity within an acoustic document, maximum-a-posteriori domain assignment from either or both LDA models will not be accurate, and they would appear in the rare classes in the 8192 domain\u2013tuple items. Histogram pruning based on normalised pairs counts was performed to remove those rare items. The pruning cut\u2013off was determined to result in a target training set size of around 500h, which was comparable to the data amount in our previous baseline experiments. Figure 3 shows the amount of data (in hours) for each of the 64 LDA domains.\nThe baseline DNN systems had an input layer of size 440. That input was expanded by augmenting the LDA inferred domain with one\u2013hot encoding. The new input had the size of 504 (440+64). The new LDA\u2013DNN was trained similarly to the base line DNNs. Table 3 shows the frame classification accuracy of DNNs on a 10% held\u2013out cross\u2013validation set with and without augmenting UBIC vectors.\nTable 4 presents the WER of baseline and adapted models for all of the eight genres. LDaT training reduces the WER from 33.3% to 30.6%, which is even better than speaker adapted DNN (31.4%). Combining speaker adaptation and domain adaptation (SAT+LDaT, linear input transformation for the speaker and bias adaptation for the latent domain) yields 28.9%, which is 13% relative WER reduction compared to the baseline DNN model and 8% relative improvement over the speaker adapted DNN. This also suggests that LDA inferred domains were not speaker clusters (since combining two adaptations still improves the performance). Because of the diverse nature of the data used, WER differs a lot across genres. Namely comedy and drama had the highest errors (43.8% and 45.0% respectively with LDaT+SAT models) showing the difficult nature of these genres. On the other\nhand, news had the lowest WER (14.3%). The WER diversity across the genres was consistent between all of the four models presented in table 4.\n6. LDA DOMAIN ANALYSIS\nIt is of interest to understand how the data is structured by the LDA model. Unfortunately ground truth labelling is only available for words and 8 different genres, and for both labels the quality is highly variable. One would suspect that the words themselves are less important, however acoustic attributes such as the presence of music or laughter may be very informative. Unfortunately such labels are not available. However one can still get some impression on the differences by looking at the raw relationship to genres and differences between individual shows.\nThe domain assignment with the procedure outlined above, is visualised for the training data. The amount of data (time) assigned to an LDA domain is accumulated, for each of the 8 genres. Figure 4 shows the distribution of data for the most important 16 LDA domains (based on duration), across genres. All remaining domains have been subsumed into one group at the top of the figure, for better illustration. In this and the following figure LDA domains are sorted by the amount of data overall. From the graph it is clear that different genres exhibit significantly different LDA domain composition, with significant fine structure. Therefore such domain classification is very useful for genre classification.\nOne can also investigate how the LDA domain assignment varies within a genre, and between genres. In particular multiple episodes of shows are interesting in such analysis as one should expect high similarity due to similar programme structure. We obtained distributions for two sample programmes from two different genres. Figure 5 shows the LDA domain distribution for 8 episodes of Bargain Hunt (competition), followed by a further 8 from Waking the Dead (drama). Again 16+1 domains are displayed. One can observe that the distribution shows similarity within a genre (e.g., similarities of the red region on the lower left corner or the green area on the lower right corner). However between the two genres clear and systematic differences can be observed. One can further observe that more than 50% of each show is typically described by the top 2 or 3 LDA domains, and these differ in case of different genres but agree for the same programme within the genre. This indicates that individual shows are far more consistently described than the accumulated statistic allows to observe.\n7. CONCLUSION\nThis paper introduced a new method, latent-domain-aware training, to adapt Deep neural networks to new domains. The method employs acoustic Latent Dirichlet Allocation to identify acoustically distinctive data clusters. These so-called\nLDA domains are then encoded using one\u2013hot encoding, and used to augment standard input features for DNNs in training and testing. We further introduced coherence data selection to improve classification quality, and presented results on a diverse set of BBC TV broadcasts, with 500h of training and 28h of testing data. Word Error Rate reduction of 13% relative was achieved using the proposed adaptation method, compared to the baseline hybrid DNNs.\nThe proposed method lends itself to several future investigations. In the current LDA domain representation, each domain is described as a point on one of the axes of a high\u2013 dimensional space, where all have same distance from each other. Representing these points differently so that similar domains became closer in that space and verifying how that improves the performance can be an interesting problem to verify as a future work. Newer sets of features, better targeted to describe background acoustic characteristics [36], could also provide an improvement.\n8. DATA ACCESS STATEMENT\nThe audio and subtitle data used for these experiments was distributed as part of the MGB Challenge (mgb-challenge. org) [23] through a licence with the BBC.\n9. REFERENCES\n[1] P. C. Woodland, M. J. F. Gales, D. Pye, and S. J. Young, \u201cBroadcast news transcription using HTK,\u201d in Proc. of ICASSP, Munich, Germany, 1997.\n[2] J. L. Gauvain, L. Lamel, and G. Adda, \u201cThe LIMSI broadcast news transcription system,\u201d Speech Communication, vol. 37, no. 1\u20132, pp. 89\u2013108, 2002.\n[3] M. J. F. Gales, D. Y. Kim, P. C. Woodland, H. Y. Chan, D. Mrva, R. Sinha, and S. E. Tranter, \u201cProgress in the CUHTK broadcast news transcription system,\u201d IEEE Trans. on Audio, Speech and Language Processing, vol. 14, no. 5, pp. 1513\u20131525, 2006.\n[4] P. Lanchantin, P. Bell, M. Gales, T. Hain, X. Liu, Y. Long, J. Quinnell, S. Renals, O. Saz, and M. Seigel, \u201cAutomatic transcription of multi\u2013genre media archives,\u201d in Proc. of SLAM, Marseille, France, 2013.\n[5] M. Doulaty, O. Saz, and T. Hain, \u201cData-selective transfer learning for multi-domain speech recognition,\u201d in Proc. of Interspeech, Dresden, Germany, 2015.\n[6] D. Yu and L. Deng, Automatic Speech Recognition: A Deep Learning Approach, Springer-Verlag, London, UK, 2015.\n[7] V. Abrash, H. Franco, A. Sankar, and M. Cohen, \u201cConnectionist speaker normalization and adaptation,\u201d in Proc. of EuroSpeech, Madrid, Spain, 1995.\n[8] B. Li and K. C. Sim, \u201cComparison of discriminative input and output transformations for speaker adaptation in the hybrid nn/hmm systems,\u201d in Proc. of Interspeech, Makuhari, Japan, 2010.\n[9] R. Gemello, F. Mana, S. Scanzio, P. Laface, and R. De Mori, \u201cLinear hidden transformations for adaptation of hybrid ANN/HMM models,\u201d Speech Communication, vol. 49, no. 10, pp. 827\u2013835, 2007.\n[10] J. Stadermann and G. Rigoll, \u201cTwo-stage speaker adaptation of hybrid tied-posterior acoustic models.,\u201d in Proc. of ICASSP, Philadelphia, USA, 2005.\n[11] R. Doddipatla, M. Hasan, and T. Hain, \u201cSpeaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition,\u201d in Proc. of Interspeech, Singapore, 2014.\n[12] S. Dupont and L. Cheboub, \u201cFast speaker adaptation of artificial neural networks for automatic speech recognition,\u201d in Proc. of ICASSP, Istanbul, Turkey, 2000.\n[13] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny, \u201cSpeaker adaptation of neural network acoustic models using i-Vectors,\u201d in Proc. of ASRU, Olomouc, Czech Republic, 2013.\n[14] Y. Liu, P. Karanasou, and T. Hain, \u201cAn investigation into speaker informed DNN front-end for LVCSR,\u201d in Proc. of ICASSP, Brisbane, Australia, 2015.\n[15] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent Dirichlet Allocation,\u201d Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, 2003.\n[16] S. Kim, S. Narayanan, and S. Sundaram, \u201cAcoustic topic model for audio information retrieval,\u201d in Proc. of WASPAA, New Paltz NY, USA, 2009, pp. 37\u201340.\n[17] M. Doulaty, O. Saz, and T. Hain, \u201cUnsupervised domain discovery using latent dirichlet allocation for acoustic modelling in speech recognition,\u201d in Proc. of Interspeech, Dresden, Germany, 2015.\n[18] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman, \u201cDiscovering objects and their location in images,\u201d in Proc. of ICCV, Beijing, China, 2005.\n[19] D. Hu and L. K. Saul, \u201cA probabilistic topic model for unsupervised learning of musical key-profiles.,\u201d in Proc. of ISMIR, Kobe, Japan, 2009.\n[20] T. L. Griffiths and M. Steyvers, \u201cFinding scientific topics,\u201d Proc. of National Academy of Sciences of the United States of America, vol. 101, pp. 5228\u20135235, 2004.\n[21] A. Gersho and R. M. Gray, Vector quantization and signal compression, Springer Science & Business Media, Berlin, Germany, 1992.\n[22] C. Ni, C. C. Leung, L. Wang, N. F. Chen, and B. Ma, \u201cUnsupervised data selection and word\u2013morph mixed language model for tamil low-resource keyword search,\u201d in Proc. of ICASSP, Brisbane, Australia, 2015.\n[23] P. Bell, M. J. F. Gales, T. Hain, J. Kilgour, P. Lanchantin, X. Liu, A. McParland, S. Renals, O. Saz, M. Webster, and P. Woodland, \u201cThe MGB Challenge: Evaluating multi-genre broadcast media recognition,\u201d in Proc. of ASRU, Arizona, USA, 2015.\n[24] O. Saz, M. Doulaty, S. Deena, R. Milner, R. W. M. Ng, M. Hasan, Y. Liu, and T. Hain, \u201cThe 2015 Sheffield system for transcription of multi\u2013genre broadcast media,\u201d in Proc. of ASRU, Arizona, USA, 2015.\n[25] R. W. M. Ng, M. Doulaty, R. Doddipatla, O. Saz, M. Hasan, T. Hain, W. Aziz, K. Shaf, and L. Specia, \u201cThe USFD spoken language translation system for IWSLT 2014,\u201d in Proc. of IWSLT, Lake Tahoe NV, USA, 2014.\n[26] Y. Long, M. J. F. Gales, P. Lanchantin, X. Liu, M. S. Seigel, and P. C. Woodland, \u201cImproving lightly supervised training for broadcast transcriptions,\u201d in Proc. of Interspeech, Lyon, France, 2013.\n[27] H. Hermansky, \u201cPerceptual linear predictive (PLP) analysis of speech,\u201d the Journal of the Acoustical Society of America, vol. 87, no. 4, pp. 1738\u20131752, 1990.\n[28] R. Haeb-Umbach and H. Ney, \u201cLinear discriminant analysis for improved large vocabulary continuous speech recognition,\u201d in Proc. of ICASSP, San Francisco, USA, 1992.\n[29] M. Gales, \u201cMaximum likelihood linear transformations for HMM-based speech recognition,\u201d Computer Speech & Language, vol. 12, no. 2, pp. 75 \u2013 98, 1998.\n[30] T. Anastasakos, J. McDonough, R. Schwartz, and J. Makhoul, \u201cA compact model for speaker-adaptive training,\u201d in Proc. of ICSLP, Philadelphia, USA, 1996.\n[31] D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran, G. Saon, and K. Visweswariah, \u201cBoosted MMI for model and feature-space discriminative training,\u201d in Proc. of ICASSP, Las Vegas, USA, 2008.\n[32] G. E. Hinton, S. Osindero, and Y.-W. Teh, \u201cA fast learning algorithm for deep belief nets,\u201d Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.\n[33] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely, \u201cThe Kaldi speech recognition toolkit,\u201d in Proc. of ASRU, Hawaii, USA, 2011.\n[34] A. Stolcke, \u201cSrilm-an extensible language modeling toolkit.,\u201d in Proc. of Interspeech, Denver, US, 2002.\n[35] R. Rehurek and P. Sojka, \u201cSoftware framework for topic modelling with large corpora,\u201d in Proc. of LREC, Valletta, Malta, 2010.\n[36] O. Saz, M. Doulaty, and T. Hain, \u201cBackground\u2013tracking acoustic features for genre identification of broadcast shows,\u201d in Proc. of SLT, Lake Tahoe NV, USA, 2014."}], "references": [{"title": "Broadcast news transcription using HTK", "author": ["P.C. Woodland", "M.J.F. Gales", "D. Pye", "S.J. Young"], "venue": "Proc. of ICASSP, Munich, Germany, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "The LIMSI broadcast news transcription system", "author": ["J.L. Gauvain", "L. Lamel", "G. Adda"], "venue": "Speech Communication, vol. 37, no. 1\u20132, pp. 89\u2013108, 2002.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Progress in the CU- HTK broadcast news transcription system", "author": ["M.J.F. Gales", "D.Y. Kim", "P.C. Woodland", "H.Y. Chan", "D. Mrva", "R. Sinha", "S.E. Tranter"], "venue": "IEEE Trans. on Audio, Speech and Language Processing, vol. 14, no. 5, pp. 1513\u20131525, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic transcription of multi\u2013genre media archives", "author": ["P. Lanchantin", "P. Bell", "M. Gales", "T. Hain", "X. Liu", "Y. Long", "J. Quinnell", "S. Renals", "O. Saz", "M. Seigel"], "venue": "Proc. of SLAM, Marseille, France, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Data-selective transfer learning for multi-domain speech recognition", "author": ["M. Doulaty", "O. Saz", "T. Hain"], "venue": "Proc. of Interspeech, Dresden, Germany, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic Speech Recognition: A Deep Learning Approach, Springer-Verlag", "author": ["D. Yu", "L. Deng"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Connectionist speaker normalization and adaptation", "author": ["V. Abrash", "H. Franco", "A. Sankar", "M. Cohen"], "venue": "Proc. of EuroSpeech, Madrid, Spain, 1995.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid nn/hmm systems", "author": ["B. Li", "K.C. Sim"], "venue": "Proc. of Interspeech, Makuhari, Japan, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Linear hidden transformations for adaptation of hybrid ANN/HMM models", "author": ["R. Gemello", "F. Mana", "S. Scanzio", "P. Laface", "R. De Mori"], "venue": "Speech Communication, vol. 49, no. 10, pp. 827\u2013835, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Two-stage speaker adaptation of hybrid tied-posterior acoustic models", "author": ["J. Stadermann", "G. Rigoll"], "venue": "Proc. of ICASSP, Philadelphia, USA, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition", "author": ["R. Doddipatla", "M. Hasan", "T. Hain"], "venue": "Proc. of Interspeech, Singapore, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast speaker adaptation of artificial neural networks for automatic speech recognition", "author": ["S. Dupont", "L. Cheboub"], "venue": "Proc. of ICASSP, Istanbul, Turkey, 2000.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Speaker adaptation of neural network acoustic models using i-Vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "Proc. of ASRU, Olomouc, Czech Republic, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "An investigation into speaker informed DNN front-end for LVCSR", "author": ["Y. Liu", "P. Karanasou", "T. Hain"], "venue": "Proc. of ICASSP, Brisbane, Australia, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Acoustic topic model for audio information retrieval", "author": ["S. Kim", "S. Narayanan", "S. Sundaram"], "venue": "Proc. of WASPAA, New Paltz NY, USA, 2009, pp. 37\u201340.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised domain discovery using latent dirichlet allocation for acoustic modelling in speech recognition", "author": ["M. Doulaty", "O. Saz", "T. Hain"], "venue": "Proc. of Interspeech, Dresden, Germany, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Discovering objects and their location in images", "author": ["J. Sivic", "B.C. Russell", "A.A. Efros", "A. Zisserman", "W.T. Freeman"], "venue": "Proc. of ICCV, Beijing, China, 2005.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "A probabilistic topic model for unsupervised learning of musical key-profiles", "author": ["D. Hu", "L.K. Saul"], "venue": "Proc. of ISMIR, Kobe, Japan, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proc. of National Academy of Sciences of the United States of America, vol. 101, pp. 5228\u20135235, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Vector quantization and signal compression", "author": ["A. Gersho", "R.M. Gray"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "Unsupervised data selection and word\u2013morph mixed language model for tamil low-resource keyword search", "author": ["C. Ni", "C.C. Leung", "L. Wang", "N.F. Chen", "B. Ma"], "venue": "Proc. of ICASSP, Brisbane, Australia, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "The MGB Challenge: Evaluating multi-genre broadcast media recognition", "author": ["P. Bell", "M.J.F. Gales", "T. Hain", "J. Kilgour", "P. Lanchantin", "X. Liu", "A. McParland", "S. Renals", "O. Saz", "M. Webster", "P. Woodland"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "The 2015 Sheffield system for transcription of multi\u2013genre broadcast media", "author": ["O. Saz", "M. Doulaty", "S. Deena", "R. Milner", "R.W.M. Ng", "M. Hasan", "Y. Liu", "T. Hain"], "venue": "Proc. of ASRU, Arizona, USA, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "The USFD spoken language translation system for IWSLT 2014", "author": ["R.W.M. Ng", "M. Doulaty", "R. Doddipatla", "O. Saz", "M. Hasan", "T. Hain", "W. Aziz", "K. Shaf", "L. Specia"], "venue": "Proc. of IWSLT, Lake Tahoe NV, USA, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving lightly supervised training for broadcast transcriptions", "author": ["Y. Long", "M.J.F. Gales", "P. Lanchantin", "X. Liu", "M.S. Seigel", "P.C. Woodland"], "venue": "Proc. of Interspeech, Lyon, France, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Perceptual linear predictive (PLP) analysis of speech", "author": ["H. Hermansky"], "venue": "the Journal of the Acoustical Society of America, vol. 87, no. 4, pp. 1738\u20131752, 1990.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1990}, {"title": "Linear discriminant analysis for improved large vocabulary continuous speech recognition", "author": ["R. Haeb-Umbach", "H. Ney"], "venue": "Proc. of ICASSP, San Francisco, USA, 1992.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1992}, {"title": "Maximum likelihood linear transformations for HMM-based speech recognition", "author": ["M. Gales"], "venue": "Computer Speech & Language, vol. 12, no. 2, pp. 75 \u2013 98, 1998.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "A compact model for speaker-adaptive training", "author": ["T. Anastasakos", "J. McDonough", "R. Schwartz", "J. Makhoul"], "venue": "Proc. of ICSLP, Philadelphia, USA, 1996.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Boosted MMI for model and feature-space discriminative training", "author": ["D. Povey", "D. Kanevsky", "B. Kingsbury", "B. Ramabhadran", "G. Saon", "K. Visweswariah"], "venue": "Proc. of ICASSP, Las Vegas, USA, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "Proc. of ASRU, Hawaii, USA, 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "Proc. of Interspeech, Denver, US, 2002.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Software framework for topic modelling with large corpora", "author": ["R. Rehurek", "P. Sojka"], "venue": "Proc. of LREC, Valletta, Malta, 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Background\u2013tracking acoustic features for genre identification of broadcast shows", "author": ["O. Saz", "M. Doulaty", "T. Hain"], "venue": "Proc. of SLT, Lake Tahoe NV, USA, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Error rates for the traditional broadcast news programmes could reach below 10% even in 1990s [1, 2, 3].", "startOffset": 94, "endOffset": 103}, {"referenceID": 1, "context": "Error rates for the traditional broadcast news programmes could reach below 10% even in 1990s [1, 2, 3].", "startOffset": 94, "endOffset": 103}, {"referenceID": 2, "context": "Error rates for the traditional broadcast news programmes could reach below 10% even in 1990s [1, 2, 3].", "startOffset": 94, "endOffset": 103}, {"referenceID": 3, "context": "However broadcast media is not just limited to clean and read studio speech but also includes other types of multi\u2013genre data with diverse speakers, variety of acoustic and recording conditions and diversity of the topics covered resulting in complex acoustic, lexical and linguistic conditions which is not yet well studied [4].", "startOffset": 325, "endOffset": 328}, {"referenceID": 4, "context": "The wide variety of conditions in complex broadcast media causes mismatch between training and testing data, and therefore degrades the performance of the speech recognition systems [5].", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "DNN adaptation methods can be divided into these three main categories [6]:", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "Linear input transformations: this is the most common adaptation method where a linear transformation is applied to either input feature [7], input to the softmax layer [8] or activation of the hidden layers [9]", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "Linear input transformations: this is the most common adaptation method where a linear transformation is applied to either input feature [7], input to the softmax layer [8] or activation of the hidden layers [9]", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "Linear input transformations: this is the most common adaptation method where a linear transformation is applied to either input feature [7], input to the softmax layer [8] or activation of the hidden layers [9]", "startOffset": 208, "endOffset": 211}, {"referenceID": 9, "context": "Retraining: all or some of the model parameters are adapted or trained using the adaptation data [10, 11].", "startOffset": 97, "endOffset": 105}, {"referenceID": 10, "context": "Retraining: all or some of the model parameters are adapted or trained using the adaptation data [10, 11].", "startOffset": 97, "endOffset": 105}, {"referenceID": 11, "context": "Principle Component Analysis (PCA) based adaptation approach [12], i-Vector based speaker\u2013aware training [13] or speaker\u2013aware DNNs [14] can be considered as subspace methods.", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "Principle Component Analysis (PCA) based adaptation approach [12], i-Vector based speaker\u2013aware training [13] or speaker\u2013aware DNNs [14] can be considered as subspace methods.", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "Principle Component Analysis (PCA) based adaptation approach [12], i-Vector based speaker\u2013aware training [13] or speaker\u2013aware DNNs [14] can be considered as subspace methods.", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "Latent Dirichlet Allocation (LDA) is a statistical approach to discover latent variables in a collection of data that is describable with first\u2013 order statistic, in an unsupervised manner [15].", "startOffset": 188, "endOffset": 192}, {"referenceID": 15, "context": "In audio tasks, LDA has been used for classifying unstructured audio files into onomatopoeic and semantic descriptions with successful results [16].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "We have previously used LDA for domain adaptation of GMM/HMM systems [17].", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "Latent Dirichlet Allocation (LDA) [15] is an unsupervised probabilistic generative model for collections of discrete data.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "LDA was originally used for topic modelling of text corpora; however, it is a generic model and can be applied to other tasks, such as object categorisation and localisation in image processing [18], automatic harmonic analysis in music processing [19], acoustic information retrieval in unstructured audio analysis [16] and our previous work for domain adaptation of GMM/HMM systems [17].", "startOffset": 194, "endOffset": 198}, {"referenceID": 18, "context": "LDA was originally used for topic modelling of text corpora; however, it is a generic model and can be applied to other tasks, such as object categorisation and localisation in image processing [18], automatic harmonic analysis in music processing [19], acoustic information retrieval in unstructured audio analysis [16] and our previous work for domain adaptation of GMM/HMM systems [17].", "startOffset": 248, "endOffset": 252}, {"referenceID": 15, "context": "LDA was originally used for topic modelling of text corpora; however, it is a generic model and can be applied to other tasks, such as object categorisation and localisation in image processing [18], automatic harmonic analysis in music processing [19], acoustic information retrieval in unstructured audio analysis [16] and our previous work for domain adaptation of GMM/HMM systems [17].", "startOffset": 316, "endOffset": 320}, {"referenceID": 16, "context": "LDA was originally used for topic modelling of text corpora; however, it is a generic model and can be applied to other tasks, such as object categorisation and localisation in image processing [18], automatic harmonic analysis in music processing [19], acoustic information retrieval in unstructured audio analysis [16] and our previous work for domain adaptation of GMM/HMM systems [17].", "startOffset": 384, "endOffset": 388}, {"referenceID": 14, "context": "A reasonable approximate can be acquired using variational approximation, which is shown to work reasonably well in various applications [15].", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "Training tries to minimise the Kullback\u2013Leiber Divergence (KLD) between the real and the approximated joint probabilities (equations 2 and 3) [15]:", "startOffset": 142, "endOffset": 146}, {"referenceID": 19, "context": "Other training methods based on Markov\u2013Chain MonteCarlo are also proposed, like Gibbs sampling method [20].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "As outlined above, LDA is a model to describe latent factors in sets of discrete symbols [15] which are here interpreted as \u201cdomains\u201d.", "startOffset": 89, "endOffset": 93}, {"referenceID": 16, "context": "work [17] we used Linde\u2013Buzo\u2013Gray vector quantization algorithm [21] to represent each speech frame with a discrete symbol, equivalent to an acoustic word or phone label.", "startOffset": 5, "endOffset": 9}, {"referenceID": 20, "context": "work [17] we used Linde\u2013Buzo\u2013Gray vector quantization algorithm [21] to represent each speech frame with a discrete symbol, equivalent to an acoustic word or phone label.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "In this paper an approach similar to that used in [22] was implemented.", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "After acoustic symbols are established and speech segments are represented as bag\u2013of\u2013sounds, LDA models with designated latent domain sizes are trained using the variational EM algorithm [15].", "startOffset": 187, "endOffset": 191}, {"referenceID": 13, "context": "Finally, domain information derived from the LDA model with K domains is encoded with a K-dimensional one\u2013hot vector called Unique Binary Index Code (UBIC) [14].", "startOffset": 156, "endOffset": 160}, {"referenceID": 5, "context": "This type of adaptation is efficient, since it is implicit in the training process and does not require further adaptation steps [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 22, "context": "[23, 24, 25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 23, "context": "[23, 24, 25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 24, "context": "[23, 24, 25].", "startOffset": 0, "endOffset": 12}, {"referenceID": 25, "context": "Instead only the subtitle text broadcast with each show plus an aligned version of the subtitles were available where the time stamps of the subtitles had been corrected in a lightly supervised manner [26].", "startOffset": 201, "endOffset": 205}, {"referenceID": 25, "context": "The WMER was a by\u2013product of the semi\u2013supervised alignment process that measures how similar the text in the subtitle matched the output of a lightly supervised ASR system for that segment [26].", "startOffset": 189, "endOffset": 193}, {"referenceID": 26, "context": "Initial models were GMM/HMM systems with 13 dimensional PLP [27] features where four neighbouring frames on each side were spliced together to form a 117\u2013dimensional feature vector.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "Using Linear Discriminant Analysis [28] this feature vector was projected down to 40\u2013dimensional vector and a global Constrained Maximum Likelihood Linear Regression [29] transformation was applied to de\u2013correlate the features.", "startOffset": 35, "endOffset": 39}, {"referenceID": 28, "context": "Using Linear Discriminant Analysis [28] this feature vector was projected down to 40\u2013dimensional vector and a global Constrained Maximum Likelihood Linear Regression [29] transformation was applied to de\u2013correlate the features.", "startOffset": 166, "endOffset": 170}, {"referenceID": 29, "context": "Speaker Adaptive Training (SAT) [30] was performed and then models were discriminatively trained using Table 2.", "startOffset": 32, "endOffset": 36}, {"referenceID": 30, "context": "the Boosted Maximum Mutual Information criterion [31] and used to get the state level alignments for the DNN training.", "startOffset": 49, "endOffset": 53}, {"referenceID": 31, "context": "The network was initialised using Deep Belief Network [32] pre\u2013training and then trained to optimise per frame Cross Entropy objective function with Stochastic Gradient Descent.", "startOffset": 54, "endOffset": 58}, {"referenceID": 32, "context": "The Kaldi open\u2013source speech recognition toolkit [33] was used to train the acoustic models.", "startOffset": 49, "endOffset": 53}, {"referenceID": 33, "context": "Both of the language models were trained on the 650M words of the subtitles data using the SRILM toolkit [34] Table 2 presents the Word Error Rate (WER) of the development set with baseline models.", "startOffset": 105, "endOffset": 109}, {"referenceID": 34, "context": "Using this GMM, the audio frames are mapped to discrete symbols to train the LDA models [35].", "startOffset": 88, "endOffset": 92}, {"referenceID": 35, "context": "Newer sets of features, better targeted to describe background acoustic characteristics [36], could also provide an improvement.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "org) [23] through a licence with the BBC.", "startOffset": 5, "endOffset": 9}], "year": 2015, "abstractText": "This paper presents a new method for the discovery of latent domains in diverse speech data, for the use of adaptation of Deep Neural Networks (DNNs) for Automatic Speech Recognition. Our work focuses on transcription of multi-genre broadcast media, which is often only categorised broadly in terms of high level genres such as sports, news, documentary, etc. However, in terms of acoustic modelling these categories are coarse. Instead, it is expected that a mixture of latent domains can better represent the complex and diverse behaviours within a TV show, and therefore lead to better and more robust performance. We propose a new method, whereby these latent domains are discovered with Latent Dirichlet Allocation, in an unsupervised manner. These are used to adapt DNNs using the Unique Binary Code (UBIC) representation for the LDA domains. Experiments conducted on a set of BBC TV broadcasts, with more than 2,000 shows for training and 47 shows for testing, show that the use of LDA-UBIC DNNs reduces the error up to 13% relative compared to the baseline hybrid DNN models.", "creator": "LaTeX with hyperref package"}}}